Matrix Factorization with Explicit Trust and Distrust Side Information
for Improved Social Recommendation
RANA FORSATI, Shahid Beheshti University and University of Minnesota
MEHRDAD MAHDAVI, Michigan State University
MEHRNOUSH SHAMSFARD, Shahid Beheshti University
MOHAMED SARWAT, University of Minnesota

With the advent of online social networks, recommender systems have became crucial for the success of
many online applications/services due to their significance role in tailoring these applications to user-specific
needs or preferences. Despite their increasing popularity, in general, recommender systems suffer from data
sparsity and cold-start problems. To alleviate these issues, in recent years, there has been an upsurge of
interest in exploiting social information such as trust relations among users along with the rating data to
improve the performance of recommender systems. The main motivation for exploiting trust information in
the recommendation process stems from the observation that the ideas we are exposed to and the choices
we make are significantly influenced by our social context. However, in large user communities, in addition
to trust relations, distrust relations also exist between users. For instance, in Epinions, the concepts of
personal “web of trust” and personal “block list” allow users to categorize their friends based on the quality
of reviews into trusted and distrusted friends, respectively. Hence, it will be interesting to incorporate this
new source of information in recommendation as well. In contrast to the incorporation of trust information
in recommendation which is thriving, the potential of explicitly incorporating distrust relations is almost
unexplored. In this article, we propose a matrix factorization-based model for recommendation in social rating
networks that properly incorporates both trust and distrust relationships aiming to improve the quality of
recommendations and mitigate the data sparsity and cold-start users issues. Through experiments on the
Epinions dataset, we show that our new algorithm outperforms its standard trust-enhanced or distrustenhanced counterparts with respect to accuracy, thereby demonstrating the positive effect that incorporation
of explicit distrust information can have on recommender systems.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and
Retrieval—Information filtering; I.2 [Computing Methodologies]: Artificial Intelligence; I.2.6 [Artificial
Intelligence]: Learning; J.4 [Computer Applications]: Social and Behavioral Sciences
General Terms: Design, Algorithms
Additional Key Words and Phrases: Matrix factorization, recommender systems, social relationships
ACM Reference Format:
Rana Forsati, Mehrdad Mahdavi, Mehrnoush Shamsfard, and Mohamed Sarwat. 2014. Matrix factorization
with explicit trust and distrust side information for improved social recommendation. ACM Trans. Inf. Syst.
32, 4, Article 17 (October 2014), 38 pages.
DOI: http://dx.doi.org/10.1145/2641564

Author’s addresses: R. Forsati (corresponding author) and M. Shamsfard, Natural Language Processing
(NLP) Research Lab, Faculty of Electrical and Computer Engineering, Shahid Beheshti University, G. C.,
Tehran, Iran; M. Mahdavi, Department of Computer Science and Engineering, Michigan State University,
East Lansing, MI; M. Sarwat, Computer Science and Engineering Department, University of Minnesota,
Minneapolis, MN; corresponding author’s email: rana.forsati@gmail.com.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by
others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to
post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
c 2014 ACM 1046-8188/2014/10-ART17 $15.00

DOI: http://dx.doi.org/10.1145/2641564
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17

17:2

R. Forsati et al.

1. INTRODUCTION

The huge amount of information available on the Web has made it increasingly challenging to cope with this information overload and find the most relevant information
one is really interested in. Recommender systems intend to provide users with recommendations of products they might appreciate, taking into account their past ratings,
purchase history, or interest. The recent proliferation of online social networks has further enhanced the need for such systems. Therefore, it is obvious why such systems are
indispensable for the success of many online applications such as Amazon, iTunes, and
Netflix to guide the search process and help users to effectively find the information or
products they are looking for [Miller et al. 2004]. Roughly speaking, the overarching
goal of recommender systems is to identify a subset of items (e.g., products, movies,
books, music, news, and webpages) that are likely to be more interesting to users based
on their interests [Deshpande and Karypis 2004; Wu et al. 2009; Forsati and Meybodi
2010; Bobadilla et al. 2013].
In general, most widely used recommender systems (RS) can be broadly classified
into content-based (CB), collaborative filtering (CF), or hybrid methods [Adomavicius
and Tuzhilin 2005]. In CB recommendation, one tries to recommend items similar
to those a given user preferred in the past. These methods usually rely on external
information, such as explicit item descriptions, user profiles, and/or the appropriate
features extracted from items to analyze item similarity or user preference to provide
recommendation. In contrast, CF recommendation, the most popular method adopted
by contemporary recommender systems, is based on the core assumption that similar
users on similar items express similar interest, and it usually relies on the rating information to build a model out of the rating information in the past without having
access to external information required in CB methods. The hybrid approaches proposed combine both CB- and CF-based recommenders to gain advantages and avoid
certain limitations of each type of systems [Good et al. 1999; Soboroff and Nicholas
1999; Pazzani 1999; Melville et al. 2002; Pavlov and Pennock 2002; Talabeigi et al.
2010; Forsati et al. 2013].
The essence of CF lies in analyzing the neighborhood information of past users and
items’ interactions in the user-item rating matrix to generate personalized recommendations based on the preferences of other users with similar behavior. CF has been
shown to be an effective approach to recommender systems. The advantage of these
types of recommender systems over content-based RS is that the CF-based methods
do not require an explicit representation of the items in terms of features, but is based
only on the judgments/ratings of the users. These CF algorithms are mainly divided
into two main categories [Gu et al. 2010]: memory-based methods (also known as
neighborhood-based methods) [Wang et al. 2006b; Chen et al. 2009] and model-based
methods [Hofmann 2004; Si and Jin 2003; Srebro and Jaakkola 2003; Zhang et al.
2006]. Recently, another direction in CF considers how to combine memory-based and
model-based approaches to take advantage of both types of methods, thereby building
a more accurate hybrid recommender system [Pennock et al. 2000; Xue et al. 2005;
Koren 2008].
The heart of memory-based CF methods is the measurement of similarity based
on ratings of items given by users: either the similarity of users (user-oriented CF)
[Herlocker et al. 1999], the similarity of items (items-oriented CF) [Sarwar et al. 2001],
or combined user-oriented and item-oriented collaborative filtering approaches to overcome the limitations specific to either of them [Wang et al. 2006a]. The user-oriented CF
computes the similarity among users, usually based on user profiles or past behavior,
and seeks consistency in the predictions among similar users [Yu et al. 2004; Hofmann
2004]. The item-oriented CF, on the other hand, allows input of additional item-wise

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:3

information and is also capable of capturing the interactions among them. If the rating
of an item by a user is unavailable, collaborative-filtering methods estimate it by computing a weighted average of known ratings of the items from the most similar users.
Memory-based collaborative filtering is most effective when users have expressed
enough ratings to have common ratings with other users, but it performs poorly for
so-called cold-start users. Cold-start users are new users who have expressed only a few
ratings. Thus, for memory-based CF methods to be effective, large amounts of userrating data are required. Unfortunately, due to the sparsity of the user-item rating
matrix, memory-based methods may fail to correctly identify the most similar users
or items, which in turn decreases the recommender accuracy. Another major issue
that memory-based methods suffer from is the scalability problem. The reason being
essentially the fact that when the number of users and items is very large, which
is common in many real-world applications, the search to identify the k most similar
neighbors of the active user is computationally burdensome. In summary, data sparsity
and non-scalability issues are two main issues current memory-based methods suffer
from.
To overcome the limitations of memory-based methods, model-based approaches have
been proposed, which establish a model using the observed ratings that can interpret
the given data and predict the unknown ratings [Adomavicius and Tuzhilin 2005]. In
contrast to memory-based algorithms, model-based algorithms try to model the users
based on their past ratings and use these models to predict the ratings on unseen items.
In model-based CF, the goal is to employ statistical and machine learning techniques
to learn models from the data and make recommendations based on the learned model.
Methods in this category include aspect model [Hofmann 2004; Si and Jin 2003], clustering methods [Kohrs and Merialdo 1999], Bayesian model [Zhang and Koren 2007],
and low-dimensional linear factor models such as matrix factorization (MF) [Srebro
et al. 2005; Srebro and Jaakkola 2003; Zhang et al. 2006; Salakhutdinov and Mnih
2008b]. Due to its efficiency in handling very huge datasets, matrix factorization-based
methods have become one of the most popular models among the model-based methods, for example, weighted low-rank matrix factorization [Srebro and Jaakkola 2003],
weighted nonnegative matrix factorization (WNMF) [Zhang et al. 2006], maximum
margin matrix factorization (MMMF) [Srebro et al. 2005], and probabilistic matrix factorization (PMF) [Salakhutdinov and Mnih 2008b]. These methods assume that user
preferences can be modeled by only a small number of latent factors [Dasgupta et al.
2002] and all focus on fitting the user-item rating matrix using low-rank approximations only based on the observed ratings. The recommender system we propose in this
article adheres to the model-based factorization paradigm.
Although latent factor models and in particular matrix factorization are able to
generate high-quality recommendations, these techniques also suffer from the data
sparsity problem in real-world scenarios and fail to address users who rated only a
few items. For instance, according to Sarwar et al. [2001], the density of non-missing
ratings in most commercial recommender systems is less than one or even much less.
Therefore, it is unsatisfactory to rely predictions on such small amounts of data, which
becomes more challenging in the presence of large number of users or items. This
observation necessitates tackling the data sparsity problem in an affirmative manner
to be able to generate more accurate recommendations.
One of the most prominent approaches to tackling the data sparsity problem is to
compensate for the lack of information in the rating matrix with other sources of
side information which are available to the recommender system. For example, social
media applications allow users to connect with each other and to interact with items
of interest such as songs, videos, pages, news, and groups. In such networks, the ideas
we are exposed to and the choices we make are significantly influenced by our social
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:4

R. Forsati et al.

context. More specifically, users generally tend to connect with other users due to some
commonalities they share, often reflected in similar interests. Moreover, in many reallife applications it may be the case that only social information about certain users
is available while interaction data between the items and those users has not yet
been observed. Therefore, the social data accumulated in social networks would be a
rich source of information for the recommender system to utilize as side information
to alleviate the data sparsity problem. To accomplish this goal, in recent years, the
trust-based recommender systems became an emerging field to provide users with
personalized item recommendations based on the historical ratings given by users and
the trust relationships among users (e.g., social friends).
Social-enhanced recommendation systems are becoming of greater significance and
practicality with the increased availability of online reviews, ratings, friendship links,
and follower relationships. Moreover, many e-commerce and consumer review websites
provide both reviews of products and a social network structure among the reviewers.
As an example, the e-commerce site Epinions [Guha et al. 2004] asks its users to indicate which reviews/users they trust and use this trust information to rank the reviews
of products. Similar patterns can be found in online communities such as Slashdot in
which millions of users post news and comment daily and are capable of tagging other
users as friends/foes or fans/freaks. Another example is the ski mountaineering site
Moleskiing [Avesani et al. 2005] which enables users to share their opinions about
the snow conditions of the different ski routes and also express how much they trust
the other users. Another well-known example is the FilmTrsut system [Golbeck and
Hendler 2006], an online social network that provides movie rating and review features
to its users. The social networking component of the website requires users to provide
a trust rating for each person they add as a friend. Also users on Wikipedia can vote
for or against the nomination of others to adminship [Burke and Kraut 2008]. These
websites have come to play an important role in guiding users’ opinions on products
and, in many cases, also influence their decisions in buying or not buying the product
or service. The results of experiments in Crandall et al. [2008] and of similar works
confirm that a social network can be exploited to improve the quality of recommendations. From this point of view, traditional recommender systems that ignore the social
structure between users may no longer be suitable.
A fundamental assumption in social-based recommender systems which has been
adopted by almost all of the relevant literature is that if two users have a friendship relation, then the recommendation from his or her friends probably has higher
trustworthiness than strangers. Therefore, the goal becomes how to combine the useritem rating matrix with the social/trust network of a user to boost the accuracy of the
recommendation system and alleviate the sparsity problem. Over the years, several
studies have addressed the issue of the transfer of trust among users in online social
networks. These studies exploit the fact that trust can be passed from one member
to another in a social network, creating trust chains, based on its propagative and
transitive nature.1 Therefore, some recommendation methods fusing social relations
by regularization [Jamali and Ester 2011; Li and Yeung 2009; Ma et al. 2011a; Zhu
et al. 2011] or factorization [Ma et al. 2008, 2011b; Salakhutdinov and Mnih 2008a,
2008b; Srebro and Jaakkola 2003; Salakhutdinov et al. 2007; Rennie and Srebro 2005]
were proposed that exploit trust relations in a social network.

1 We note that while the concept of trust has been studied in many disciplines, including sociology, psychology,
economics, and computer science from different perspectives, the issue of propagation and transitivity have
often been debated in literature, and different authors have reached different conclusions (see e.g., [Sherchan
et al. 2013] for a thorough discussion).

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:5

Also, the results of incorporating trust information in recommender systems is appealing and has been the focus of much researcher in the last few years, but in large
user communities, besides the trust relationship between users, the distrust relationships are also unavoidable. For example, Epinions provided the feature that enables
users to categorize other users in a personal web of trust list based on their quality
as a reviewer. Later on, this feature integrated with the concept of personal block list,
which reflects the members that are distrusted by a particular user. In other words, if
a user encounters a member whose reviews are consistently offensive, inaccurate, or
otherwise low quality, she can add that member to her block list. Therefore, it would be
tempting to investigate whether or not distrust information could be effectively utilized
to boost the accuracy of recommender systems as well.
In contrast to trust information for which there has been a great deal of research,
the potential advantage/disadvantage of explicitly utilizing distrust information is
almost unexplored. Recently, few attempts have been made to explicitly incorporate
the distrust relations in recommendation process [Guha et al. 2004; Ma et al. 2009b;
Victor et al. 2011b, 2013], which demonstrated that the recommender systems can
benefit from the proper incorporation of distrust relations in social networks. However,
despite these positive results, there are some unique challenges involved in distrustenhanced recommender systems. In particular, it has proven challenging to model
distrust propagation in a manner which is both logically consistent and psychologically
plausible. Furthermore, the naive modeling of distrust as negative trust raises a
number of challenges—both algorithmic and philosophical. Finally, it is an open
challenge how to incorporate trust and distrust relations in model-based methods
simultaneously. This article is concerned with these questions and gives an affirmative
solution to challenges involved with distrust-enhanced recommendation. In particular,
the proposed method makes it possible to simultaneously incorporate both trust and
distrust relationships in recommender systems to increase the prediction accuracy. To
the best of our knowledge, this is the first work that models distrust relations into the
matrix factorization problem along with trust relations at the same time.
The main intuition behind the proposed algorithm is that one can interpret the distrust relations between users as dissimilarity in their preferences. In particular, when
a user u distrusts another user v, it indicates that user u disagrees with most of the
opinions issued, or ratings made by user v. Therefore, the latent features of user u
obtained by matrix factorization must be as dissimilar as possible to v’s latent features. In other words, this intuition suggests directly incorporating the distrust into
recommendation by considering distrust as reversing the deviation of latent features.
However, when combined with the trust relations between users, due to the contradictory role of trust and distrust relations in propagating social information in the matrix
factorization process, this idea fails to effectively capture both relations simultaneously. This statement also follows from the preliminary experimental results in Victor
et al. [2011b] for memory-based CF methods that demonstrated regarding distrust as
an indication to reverse deviations in not the right way to incorporate distrust.
To remedy this problem, we settle for a less ambitious goal and propose another
method to facilitate the learning from both types of relations. In particular, we try
to learn latent features in a manner such that the latent features of users who are
distrusted by the user u have a guaranteed minimum dissimilarity gap from the worst
dissimilarity of users who are trusted by user u. By this formulation, we ensure that
when user u agrees on an item with one of his trusted friends, he will disagree on
the same item with his distrusted friends with a minimum predefined margin. We
note that this idea significantly departs from the existing works in distrust-enhanced
memory-based recommender systems [Victor et al. 2011b, 2013] that employ the distrust relations to either filter out or debug the trust relations to reduce the prediction
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:6

R. Forsati et al.

task to a trust-enhanced recommendation. In particular, the proposed method ranks
the latent features of trusted and distrusted friends of each user to reflect the effect of
relation in factorization.
Summary of Contributions. This work makes the following key contributions.
—A matrix factorization-based algorithm for simultaneous incorporation of trust and
distrust relationships in recommender systems. To the best of our knowledge, this is
the first model-based recommender algorithm that is able to leverage both types of
relationships in recommendation.
—An efficient stochastic optimization algorithm to solve the optimization problem
which makes the proposed method scalable to large social networks.
—An empirical investigation of the consistency of the social relationships with rating
information. In particular, we examine to what extent trust and distrust relations
between users are aligned with the ratings they issued on items.
—An exhaustive set of experiments on the Epinions dataset to empirically evaluate the
performance of the proposed algorithm and demonstrate its merits and advantages.
—A detailed comparison of the proposed algorithm to state-of-the-art trust/distrustenhanced memory/model-based recommender systems.
Outline. The rest of this article is organized as follows. In Section 2, we draw connections to and put our work in context of some of the most recent work on social
recommender systems. Section 3 formally introduces the matrix factorization problem,
an optimization-based framework to solve it, and its extension to incorporate the trust
relations between users. The proposed algorithm along with optimization methods are
discussed in Section 4. Section 5 includes our experimental result on the Epinions
dataset which demonstrates the merits of the proposed algorithm in alleviating the
data sparsity problem in rating matrix and generating more accurate recommendations. Finally, Section 6 concludes and discusses a few directions as future work.
2. RELATED WORK ON SOCIAL RECOMMENDATION

Earlier in the introduction, we discussed some of the main lines of research on recommender systems; here, we survey further lines of study that are most directly-related to
our work on social-enhanced recommendation. Many successful algorithms have been
developed over the past few years to incorporate social information in recommender
systems. After reviewing trust-enhanced memory-based approaches, we discuss some
model-based approaches for recommendation in social networks with trust relations.
Finally, we review major approaches in distrust modeling and distrust-enhanced recommender systems.
2.1. Trust-Enhanced Memory-Based Recommendation

Social network data has been widely investigated in the memory-based approaches.
These methods typically explore the social network and find a neighborhood of users
trusted (directly or indirectly) by a user and perform the recommendation by aggregating their ratings. These methods use the transitivity of trust and propagate trust
to indirect neighbors in the social network [Massa and Avesani 2004, 2009; Konstas
et al. 2009; Jamali and Ester 2009, 2010, 2011; Koren et al. 2009].
In Massa and Avesani [2004], a trust-aware collaborative filtering method for recommender systems is proposed. In this work, the collaborative filtering process is informed
by the reputation of users, which is computed by propagating trust. Konstas et al. [2009]
proposed a method based on the random walk algorithm to utilize social connection and
other social annotations to improve recommendation accuracy. However, this method
does not utilize the rating information and is not applicable to constructing a random
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:7

walk graph in real datasets. TidalTrust [Golbeck 2006] performs a modied breadthfirst search in the trust network to compute a prediction. To compute the trust value
between user u and v who are not directly connected, TidalTrust aggregates the trust
value between u’s direct neighbors and v weighted by the direct trust values of u and
its direct neighbors.
MoleTrust [Massa and Avesani 2004, 2005; Zhang and Koren 2007] applies the same
idea as TidalTrust, but MoleTrust considers all the raters up to a fixed maximumdepth given as an input, independent of any specific user and item. The trust metric
in MoleTrust consists of two major steps. First, cycles in trust networks are removed.
Therefore, removing trust cycles beforehand from trust networks can significantly
speed up the proposed algorithm because every user only needs to be visited once to
infer trust values. Second, trust values are calculated based on the obtained directed
acyclic graph by performing a simple graph random walk.
TrustWalker [Jamali and Ester 2009] combines trust-based and item-based recommendation to consider enough ratings without suffering from noisy data. Their experiments show that TrustWalker outperforms other existing memory-based approaches.
Each random walk on the user trust graph returns a predicted rating for user u on
target item i. The probability of stopping is directly proportional to the similarity between the target item and the most similar item j, weighted by the sigmoid function of
step size k. The more the similarity, the greater the probability of stopping and using
the rating on item j as the predicted rating for item i. As the step size increases, the
probability of stopping decreases. Thus ratings by closer friends on similar items are
considered more reliable than ratings on the target item by friends further away.
We note that all these methods are neighborhood-based methods which employ only
heuristic algorithms to generate recommendations. There are several problems with
this approach. The relationship between the trust network and the user-item matrix
has not been studied systematically. Moreover, these methods are not scalable to very
large datasets, since they may need to calculate the pairwise user similarities and
pairwise user trust scores.
2.2. Trust-Enhanced Model-Based Recommendation

Recently, researchers have exploited matrix factorization techniques to learn latent
features for users and items from the observed ratings, and fusing social relations
among users with rating data as will be detailed in Section 3. These methods can be
divided into two types: regularization-based methods and factorization-based methods.
Here we review some existing matrix factorization algorithms that incorporate trust
information in the factorization process.
2.2.1. Regularization-Based Social Recommendation. Regularization-based methods typically add a regularization term to the loss function and minimizes it. Most recently, Ma
et al. [2011a] proposed an idea based on social-regularized matrix factorization to make
recommendation based on social network information. In this approach, the social regularization term is added to the loss function, which measures the difference between
the latent feature vector of a user and those of his friends. A probability model similar
to the model in Ma et al. [2011a] is proposed by Jamali and Ester [2011]. The graph
Laplacian regularization term of social relations is added into the loss function in
Li and Yeung [2009] and minimizes the loss function by alternative projection algorithm. Zhu et al. [2011] used the same model in Li and Yeung [2009] and built
graph Laplacian of social relations using three kinds of kernel functions. In Liu et al.
[2013], the minimization problem is formulated as a low-rank semidefinite optimization
problem.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:8

R. Forsati et al.

2.2.2. Factorization-Based Social Recommendation. In factorization-based methods, social
relationships between users are represented as a social relation matrix, which is factored as well as the rating matrix. The loss function is the weighted sum of the social relation matrix factorization error and the rating matrix factorization error. For instance,
SoRec [Ma et al. 2008] incorporates the social network graph into the probabilistic matrix factorization model by simultaneously factorizing the user-item rating matrix and
the social trust networks by sharing a common latent low-dimensional user feature
matrix [Liu et al. 2013]. The experimental analysis shows that this method generates
better recommendations than the non-social filtering algorithms [Jamali and Ester
2010]. However, the disadvantage of this work is that although users’ social networks
are integrated into the recommender systems by factorizing the social trust graph,
the real-world recommendation processes are not reflected in the model. Two sets of
different feature vectors are assumed for users, which makes the interpretability of
the model very hard [Jamali and Ester 2010; Ma et al. 2009a]. This drawback not only
causes lack of interpretability in the model, but also affects the recommendation qualities. A better model named Social Trust Ensemble (STE) [Ma et al. 2009a] is proposed,
by making the latent features of a user’s direct neighbors affect the rating of the user.
Their method is a linear combination of a basic matrix factorization approach and a social network-based approach. Experiments show that their model outperforms the basic
matrix factorization-based approach and existing trust-based approaches. However, in
their model, the feature vectors of direct neighbors of u affect the ratings of u instead of
affecting the feature vector of u. This model does not handle trust propagation. Another
method for recommendation in social networks has been proposed in Ma et al. [2009b].
This method is not a generative model and defines a loss function to be minimized.
The main disadvantage of this method is that it punishes the users with lots of social
relations more than other users. Finally, SocialMF [Jamali and Ester 2010] is a matrix
factorization-based model which incorporates social influence by making the features
of every user depend on the features of his/her direct neighbors in the social network.
2.3. Distrust-Enhanced Social Recommendation

In contrast to incorporation of trust relations, unfortunately most of the literature on
social recommendation totally ignores the potential of distrust information in boosting
the accuracy of recommendations. In particular, only recently a few work have started
to investigate the rule of distrust information in the recommendation process, both
from theoretical and empirical viewpoints [Guha et al. 2004; Ziegler and Lausen 2005;
Nalluri 2008; Ziegler 2009; Ma et al. 2009b; Wierzowiecki and Wierzbicki 2010; Victor
et al. 2011b, 2011c, 2013; Verbiest et al. 2012]. Although these studies have shown
that distrust information can be plentiful, but there is a significant gap in clear understanding of distrust in recommender systems. The most important reasons for this
shortage are the lack of datasets that contain distrust information and dearth of a
unified consensus on modeling and propagation of distrust.
A formal framework of trust propagation schemes, introducing the formal and computational treatment of distrust propagation, has been developed in Guha et al. [2004].
In an extension of this work, Ziegler [2009] proposed clever adaptations in order to
handle distrust and sinks such as trust decay and normalization. In Wierzowiecki and
Wierzbicki [2010], a trust/distrust propagation algorithm called CloseLook is proposed,
which is capable of using the same kinds of trust propagation as the algorithm proposed
by Guha et al. [2004]. Leskovec et al. [2010a] extended the results of Guha et al. [2004]
using a machine-learning framework (instead of the propagation algorithms based
on an adjacency matrix) to enable the evaluation of the most informative structural
features for the prediction task of positive/negative links in online social networks. A
comprehensive framework that computes trust/distrust estimations for user pairs in
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:9

the network using trust metrics is built in Victor et al. [2011c]: given two users in the
trust network, we can search for a path between them and propagate the trust scores
along this path to obtain an estimation. When more than one path is available, we
may single out the most relevant ones (selection), and aggregation operators can then
be used to combine the propagated trust scores into one final trust score, according to
different trust score propagation operators.
Ma et al. [2009b] was the first seminal work to demonstrate that the incorporation
of distrust information could be beneficial based on a model-based recommender system. In Victor et al. [2011c, 2013], the same question is addressed in memory-based
approaches. In particular, Victor et al. [2013] embarked upon the distrust-enhanced recommendation and showed that with careful incorporation of distrust metric, distrustenhanced recommender systems are able to outperform their trust-only counterparts.
The main rational behind the algorithm proposed in Victor et al. [2013] is to employ
the distrust information to debug or filter out the users’ propagated web of trust. It is
also has been realized that the debugging methods must exhibit a moderate behavior
in order to be effective. Verbiest et al. [2012] addressed the problem of considering the
length of the paths that connect two users for computing trust-distrust between them,
according to the concept of trust decay. This work also introduced several aggregation
strategies for trust scores with variable path lengths.
Finally we note that the aforementioned works try to either model or utilize
trust/distrust information. In recent years, there has been an upsurge of interest in
predicting trust and distrust relations in a social network [Leskovec et al. 2010a;
DuBois et al. 2011; Bachi et al. 2012; Patil et al. 2013]. For instance, Leskovec et al.
[2010a] casts the problem as a sign prediction problem (i.e., +1 for friendship and −1
for opposition) and utilizes machine learning methods to predict the sign of links in the
social network. In DuBois et al. [2011] a new method is presented for computing both
trust and distrust by combining an inference algorithm that relies on a probabilistic
interpretation of trust based on random graphs with a modified spring-embedding algorithm to classify an edge. Another direction of research is to examine the consistency
of social relations with theories in social psychology [Cartwright and Harary 1956;
Leskovec et al. 2010b]. Our work significantly departs from these works on prediction
or consistency analysis of social relations, and aims to effectively incorporate distrust
information in matrix factorization for effective recommendation.
3. MATRIX FACTORIZATION-BASED RECOMMENDER SYSTEMS

This section provides a formal definition of collaborative filtering, the primary recommendation method we are concerned with in this article, followed by solution methods
for low-rank factorization that are proposed in the literature to address the problem.
(See Table I for Common notations and their meanings.)
3.1. Matrix Factorization for Recommendation

In collaborative filtering, we assume that there is a set of n users U = {u1 , . . . , un} and
a set of m items I = {i1 , . . . , im}, where each user ui expresses opinions about a set of
items. In this article, we assume opinions are expressed through an explicit numeric
rating (e.g., scale from one to five), but other rating methods such as hyperlink clicks are
possible as well. We are mainly interested in recommending a set of items for an active
user such that the user has not rated these items before. To this end, we are aimed
at learning a model from the existing ratings, that is, offline phase, and then use the
learned model to generate recommendations for active users, that is, online phase. The
rating information is summarized in an n × m matrix R ∈ Rn×m, 1 ≤ i ≤ n, 1 ≤ j ≤ m,
where the rows correspond to the users and the columns correspond to the items, and
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:10

R. Forsati et al.
Table I. Summary of Notations Consistently Used in the Article and Their Meaning
Symbol

Meaning

U = {u1 , . . . , un}, n
I = {i1 , . . . , im}, m
k
R ∈ Rn×m
R , |R |
U ∈ Rn×k
V ∈ Rm×k
S ∈ {−1, +1}n×n
S , |S |
W ∈ Rn×n
+
N (u) ⊆ [n]
N+ (u) ⊆ [n]
N− (u) ⊆ [n]
D : Rk × Rk → R+

The set of users in system and the number of users
The set of items and the number of items
The dimension of latent features in factorization
The partially observed rating matrix
The set of observed entires in rating matrix and its size
The matrix of latent features for users
The matrix of latent features for items
The social network between n users
The set of extracted triplets from the social relations and its size
The pairwise similarity matrix between users
Neighbors of user u in the social graph
The set of trusted neighbors by user u in the social graph
The set of distrusted neighbors by user u in the social graph
The measurement function used to assess the similarly of latent features

the ( p, q)th entry is the rate given by user up to the item iq . We note that the rating
matrix is partially observed, and it is sparse in most cases.
An efficient and effective approach to recommender systems is to factorize the useritem rating matrix R by a multiplicative of k-rank matrices R ≈ UV , where U ∈ Rn×k
and V ∈ Rm×k utilize the factorized user-specific and item-specific matrices, respectively,
to make further missing data prediction. The main intuition behind a low-dimensional
factor model is that there is only a small number of factors influencing the preferences,
and that a user’s preference vector is determined by how each factor applies to that
user. This low rank assumption makes it possible to effectively recover the missing
entires in the rating matrix from the observed entries. We note that the celebrated
Singular Value Decomposition (SVD) method for factorizing the rating matrix R is
not applicable here due to the fact that the rating matrix is partially available and
we are only allowed to utilize the observed entries in factorization process. There are
two basic formulations to solve this problem: optimization based (see e.g., [Rennie and
Srebro 2005; Liu et al. 2013; Ma et al. 2008; Koren et al. 2009]) and probabilistic [Mnih
and Salakhutdinov 2007]. In the following sections, we first review the optimizationbased framework for matrix factorization and then discuss how it can be extended to
incorporate trust information.
3.2. Optimization-Based Matrix Factorization

Let R be the set of observed ratings in the user-item matrix R ∈ Rn×m, that is,
R = {(i, j) ∈ [n] × [m] : Ri j has been observed},
where n is the number of users and mis the number of items to be rated. In optimizationbased matrix factorization, the goal is to learn the latent matrices U and V by solving
the following optimization problem:
⎡
⎤
 

1
λ
λ
2
U
V

min ⎣L(U, V) =
UF +
VF ⎦ ,
Ri j − Ui,:
(1)
V j,: +
U,V
2
2
2
(i, j)∈R

where  · F is the Frobenius norm of a matrix, that is, AF =

	
 

n
m
i=1

j=1

|Ai j |2 .

The optimization problem in Eq. (1) constitutes of three terms: the first term aims
to minimize the inconsistency between the observed entries and their corresponding
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:11

value obtained by the factorized matrices. The last two terms regularize the latent
matrices for users and items, respectively. The parameters λU and λV are regularization
parameters that are introduced to control the regularization of latent matrices U and
V, respectively. We would like to emphasize that the problem in Eq. (1) is non-convex
jointly in both U and V. However, despite its non-convexity, the formulation in Eq. (1)
is widely used in practical collaborative filtering applications, as the performance is
competitive, or better as compared to trace-norm minimization, while scalability is
much better. For example, as indicated in Koren et al. [2009], to address the Netflix
problem, Eq. (1) has been applied with a fair amount of success to factorize datasets
with 100 million ratings.
3.3. Matrix Factorization with Trust Side Information

Recently it has been shown that just relying on the rating matrix to build a recommender system is not as accurate as expected. The main reason for this claim is the
known cold-start users problem and the sparsity of the rating matrix. Cold-start users
are one of the most important challenges in recommender systems. Since cold-start
users are more dependent on the social network compared to users with more ratings,
the effect of using trust propagation gets more important for cold-start users. Moreover,
in many real-life systems, a very large portion of users do not express any ratings, and
they only participate in the social network. Hence, using only the observed ratings does
not allow us to learn the user features.
One of the most prominent approaches to tacking the data sparsity problem in matrix factorization is to compensate for the lack of information in rating matrix with
other sources of side information which are available to the recommender system. It
has been recently shown that social information, such as trust relationship between
users, is a rich source of side information to compensate for the sparsity. The already
mentioned traditional recommendation techniques are all based on working on the
user-item rating matrix, and ignore the abundant relationships among users. Trustbased recommendation usually involves constructing a trust network where nodes are
users and edges represent the trust placed on them. The goal of a trust-based recommendation system is to generate personalized recommendations by aggregating the
opinions of other users in the trust network. The intuition is that users tend to adopt
items recommended by trusted friends rather than strangers, and that trust is positively and strongly correlated with user preferences. Recommendation techniques that
analyze trust networks were found to provide very accurate and highly personalized
results.
To incorporate the social relations in the optimization problem formulated in Eq. (1),
a few papers [Ma et al. 2009b, 2011a; Jamali and Ester 2011; Liu et al. 2013; Zhu et al.
2011] proposed the social regularization method which aims at keeping the latent
vector of each user similar to his/her neighbors in the social network. The proposed
models force the user feature vectors to be close to those of their neighbors to be able to
learn the latent user features for users with no or very few ratings [Jamali and Ester
2011]. More specifically, the optimization problem becomes
L(U, V) =

2 λU
1  
λV

UF +
VF
V j,: +
Ri j − Ui,:
2
2
2
(i, j)∈R



n 


λS  
1

+
U j,: 
Ui,: − |N (i)|
,
2

i=1 
j∈N (i)

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

(2)

17:12

R. Forsati et al.

where λ S is the social regularization parameter and N (i) is the subset of users who has
relationship with the ith user in the social graph.
The rationale behind this social regularization idea is that every user’s taste is
relatively similar to the average taste of his friends in the social network. We note that
in using this idea, latent features of users indirectly connected in the social network
will be dependent, and hence the trust gets propagated. A more reasonable and realistic
model should treat all friends differently based on how similar they are. Let us assume
the weight of a relationship between two users i and j is captured by Wi j , where
W ∈ Rn×n denotes the social weight matrix. It is easy to extend the model in Eq. (2) to
treat friends differently based on the weight matrix W as
2 λU
1  
λV

UF +
VF
Ri j − Ui,:
L(U, V) =
V j,: +
(3)
2
2
2
(i, j)∈R




n

λS  
j∈N (i) Wi j U j,: 

+
Ui,: − 

.

2
j∈N (i) Wi j 
i=1

An alternative formulation is to regularize each user’s friends individually, resulting
in the following objective function [Ma et al. 2011a]:
2 λU
1  
λV

L(U, V) =
UF +
VF
Ri j − Ui,:
V j,: +
2
2
2
(i, j)∈R

n

2
λS 
+
Wi j Ui,: − U j,:  ,
2
i, j=1

where we simply assumed that for any j ∈
/ N (i), Wi j = 0.
As mentioned earlier, the objective function in L(U, V) is not jointly convex in both
U and V, but it is convex in each of them fixing the other one. Therefore, to find a local
solution, one can stick to the standard gradient descent method to find a solution in an
iterative manner as follows:
Ut+1 ← Ut − ηt ∇U L(U, V)|U=Ut ,V=Vt ,
Vt+1 ← Vt − ηt ∇V L(U, V)|U=Ut ,V=Vt .
4. MATRIX FACTORIZATION WITH TRUST AND DISTRUST SIDE INFORMATION

In this section, we describe the proposed algorithm for social recommendation which
is able to incorporate both trust and distrust relationships in the social network along
with the partially observed rating matrix. We then present two strategies to solve the
derived optimization problem, one based on the gradient descent optimization algorithm which generates more accurate solutions but is computationally cumbersome,
and another based on the stochastic gradient descent method which is computationally
more efficient for large rating and social matrices but suffers from slow convergence
rate.
4.1. Algorithm Description

As already discussed, the vast majority of related work in the field of matrix factorization for recommendation has primarily focused on trust propagation and has
simply ignored the distrust information between users or, intrinsically, is not capable
of exploiting it. Now, we aim at developing a matrix factorization-based model for recommendation in social rating networks to utilize both trust and distrust relationships.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:13

We incorporate the trust/distrust relationship between users in our model to improve
the quality of recommendations. While intuition and experimental evidence indicate
that trust is somewhat transitive, distrust is certainly not transitive. Thus, when we
intend to propagate distrust through a network, questions about transitivity and how
to deal with conflicting information abound.
To inject social influence in our model, the basic idea is to find appropriate latent
features for users such that each user is brought closer to the users she/he trusts and
separated from the users that she/he distrusts and who have different interests. We
note that simply incorporating this idea in matrix factorization by naively penalizing
the similarity of each user’s latent features to his distrusted friends’ latent features
fails to reach the desired goal. The main reason being that distrust is not as transitive
as trust, that is, distrust can not directly replace trust in trust propagation approaches,
and utilizing distrust requires careful consideration (trust is transitive, i.e., if user u
trusts user v and v trusts w, there is a good chance that u will trust w, but distrust is
certainly not transitive, i.e., if u distrusts v and v distrusts w, then w may be closer to
u than v or maybe even farther away). It is noticeable that this statement is consistent
with the preliminary experimental results in Victor et al. [2011b] for memory-based
CF methods that indicate regarding distrust as an indication to reverse deviations in
not the right way to incorporate distrust. Therefore, we pursue another approach to
model the distrust in the recommendation process.
The main intuition behind the proposed framework stems from the observation that
trust relations between users can be treated as agreement on items and distrust relations can be considered as disagreement on items. Then, the question becomes how can
we guarantee that when a user agrees on an item with one of his/her friends, he/she will
disagree on the same item with his/her distrusted friends with a reasonable margin.
We note that this margin should be large enough to make it possible to distinguish
between two types of friends. In terms of latent features, this observation translates to
having a margin between the similarity and dissimilarity of users’ latent features to
his/her trusted and distrusted friends.
Alternatively, one can view the proposed method from the viewpoint of connectivity of
latent features in a properly designated graph. Intuitively, certain features or groups of
features should influence how users connect in the social network, and thus it should
be possible to learn a mapping from features to connectivity in the social network
such that the mapping respects the underlying structure of the social network. In the
basic matrix factorization algorithm for recommendation, we can consider the latent
features as isolated vertices of a graph where there is no connection between nodes.
This can be generalized to the social-enhanced setting by considering the social graph
as the underlying graph between latent features with two types of edges (i.e., trust
and distrust relations correspond to positive and negative edges, respectively). Now
the problem reduces to learning the latent features for each user u such that users
trusted by u in the social network (with positive edges) are close and users which are
distrusted by u (with negative edges) are more distant. Learning latent features in this
manner respects the inherent topology of the social network.
Figure 1 shows an example to illustrate the intuition behind this idea. For ease of
exposition, we only consider the latent features for the user u1 . From the trust network
in Figure 1(a), we can see that user u1 trusts the list of users N+ = {u2 , u4 , u6 , u7 },
and from the distrust network in Figure 1(b), we see that user u1 distrusts the list
of users N− = {u3 , u5 }. The goal is to learn the latent features that obeys two goals:
(i) it minimizes the prediction error on observed entries in the rating matrix, (ii) it
respects the underlying structure of the trust and distrust networks between users.
In Figure 1(d), the latent features are depicted in the Euclidean space from the viewpoint of user u1 . As shown in Figure 1(d), for user u1 , the latent features of his/her
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:14

R. Forsati et al.

Fig. 1. A simple example with seven users {u1 , u2 , . . . , u7 } and six items {i1 , i2 , . . . , i6 } to illustrate the main
intuition behind the proposed algorithm. The inputs of the algorithm are (a) trust network, (b) distrust
network, and (c) partially observed rating matrix R, respectively. As shown in (d) for user u1 , the learned
latent features for all his trusted friends {u2 , u4 , u6 , u7 } are closer to u1 ’s latent features than his distrusted
friends {u3 , u5 } with a margin of 1.

trusted friends N+ lie inside the solid circle centered at u1 , and the latent features of
his/her distrusted friends N− lie outside the dashed circle. The gap between two circles
guarantees that there always exists a safe margin between u1 ’s agreements with his
trusted and distrusted friends. One simple way to impose these constraints on the latent features of users is to generate a set of triplets for any combination of trusted and
distrusted friends (e.g., one such triplet for user u1 can be constructed as (u1 , u2 , u5 ))
and force the margin constraint to hold for all extracted triplets. This ensures that the
minimum margin gap will definitely exist between the latent features of all the trusted
and distrusted friends as desired and makes it possible to incorporate both types of
relationships between users in the matrix factorization.
It is worth mentioning that similar to the social-enhanced recommender systems
previously discussed, the proposed algorithm is also based on hypotheses about the
existence and the correlation of trust/distrust relations and ratings in the data. The
empirical investigation of correlation between social relations and rating information
has been the focus of a bulk of recent research including [Ziegler and Golbeck 2007;
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:15

Patil et al. 2013; Ma 2013], where the results reinforce the hypothesis that ratings
from trusted people count more than those from others and in particular distrusted
neighbors. We have also conducted experiments, as will be detailed in Section 5.5, to empirically investigate the correlation/alignment between social relations and the rating
information issued by users which supports our strategy in exploiting the trust/distrust
relations in matrix factorization.
We now formalize the proposed solution. As the first ingredient, we need a measure
to evaluate the consistency between the latent features of users, that is, the matrix U,
and the trust and distrust constraints existing between users in the social network. To
this end, we introduce a monotonically-increasing convex loss function (z) to measure
the discrepancy between the latent features of different users. Let ui , u j , and uk be
three users in the model such that ui trusts u j but distrusts uk. The main intuition
behind the proposed framework is that the latent features of ui , that is, Ui,: must be
more similar to u j ’s latent features than latent features for user uk. For each such
a triplet, we penalize the objective function by (D(Ui,: , U j,: ) − D(Ui,: , Uk,: )), where the
function D : Rk ×Rk → R+ measures the similarity between two latent vectors assigned
to two different users, and  : R → R+ is a penalty function that is utilized to assess
the violation of latent vectors of trusted and distrusted users. Example loss functions
include hinge loss (z) = max(0, 1 − z) and logistic loss (z) = log(1 + e−z ), which are
widely used convex surrogates of 0-1 loss function in learning community.
Let S denote the set of extracted triplets from the social relations, that is,


S = (i, j, k) ∈ [n] × [n] × [n] : Si j = 1 & Sik = −1 .
Here, a positive relationship means friends or a trusted relationship and a negative
relationship means foes or a distrust relationship. Then, our goal becomes to find a
factorization of matrix R such that the learned latent features of users are consistent
with the constraints in S , where the consistency is reflected in the loss function. This
results in the following optimization problem:
2 λU
1  
λV

L(U, V) =
UF +
VF
Ri j − Ui,:
V j,: +
2
2
2
(i, j)∈R

+

λS
|S |



(D(Ui,: , U j,: ) − D(Ui,: , Uk,: )).

(4)

(i, j,k)∈S

Let us make this general formulation more specific by setting (·) and D(·, ·) to be the
hinge loss and the Euclidian distance, respectively. Under these two assumptions, the
objective can be formulated as
2 λU
1  
λV

L(U, V) =
VF
Ri j − Ui,:
V j,: + UF +
2
2
2
(i, j)∈R



R(U,V)

λS
+
|S |





max 0, 1 − Ui,: − U j,: 2 + Ui,: − Uk,: 2 .

(5)

(i, j,k)∈S

Here the constraints have been written in terms of hinge-losses over triplets, each
consisting of a user, his/her trusted friend, and his/her distrusted friend. Solving the
optimization problem in Eq. (5) outputs the latent features for users and items that
can be utilized to estimate the missing values in the user-item matrix. Comparing the
formulation in Eq. (5) to the existing factorization-based methods discussed earlier
reveals two main features of the proposed formulation. First, it aims to minimize
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:16

R. Forsati et al.

the error on the observed ratings and to respect the inherent structure of the social
network among the users. The trade-off between these two objectives is captured by
the regularization parameter λ S which is required to be tuned effectively.
In a similar way, applying the logistic loss to the general formulation in Eq. (4) yields
the following objective:
2 λU
1  
λV

L(U, V) =
UF +
VF
V j,: +
Ri j − Ui,:
2
2
2
(i, j)∈R

+

λS
|S |






log 1 + exp Ui,: − Uk,: 2 − Ui,: − U j,: )2 .

(6)

(i, j,k)∈S

Remark 4.1. We note that in several applications of recommender systems, besides
the observed ratings, a description of the users and/or the objects through attributes
(e.g., gender, age) or measures of similarity is available that could potentially benefit the
process of recommendation (see, e.g., [Agarwal and Chen 2010] for a few interesting
applications). In that case, it is tempting to take advantage of both known ratings
and descriptions to model the preferences of users. A natural way to incorporate the
available metadata is to kernalize the similarity measure between latent features based
on a positive definite kernel between pairs that can be deduced from the metadata.
More specifically, instead of simply using Euclidian distance as the similarity measure
between latent features in Eq. (5), we can use the kernel matrix K obtained from the
Laplacian of the graph obtained from the metadata to measure the similarity
 


D(Ui,: , U j,: ) = Ui,: − U j,: K Ui,: − U j,: ,


where K = (D − W)−1 , with D as a diagonal matrix with Di,i = nj=1 Wi j . Here, W
captures the pairwise weight between users in the similarity graph between users that
is computed based on the available metadata about users.
Remark 4.2. We would like to emphasize that it is straightforward to generalize the
proposed framework to incorporate similarity and dissimilarity information between
items. What we need is to extract the triplets from the trust/distrust links between
items and repeat the same process we did for users. This will add another term to the
objective in terms of latent features of items V, as shown in the following generalized
formulation:
2 λU
1  
λV

L(U, V) =
UF +
VF
Ri j − Ui,:
V j,: +
2
2
2
(i, j)∈R

+
+

λS
|S |
λI
|I |





max 0, 1 − Ui,: − U j,: 2 + Ui,: − Uk,: 2

(i, j,k)∈S





max 0, 1 − Vi,: − V j,: 2 + Vi,: − Vk,: 2 ,

(i, j,k)∈I

where λ I is the regularization parameter and I is the set of triplets extracted from
the similar/dissimilar links between items. The similarity/dissimilarity links between
items can be constructed according to tags issued by users or associated with items,
and categories. For example, if two items are attached with a same tag, there is a trust
link between them, and otherwise a distrust link. Alternatively, trust/distrust links
can be extracted by measuring similarity/dissimilarity based on the item properties or
profile if provided. This could further improve the accuracy of recommendations.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:17

ALGORITHM 1: GD-based Matrix Factorization with Trust and Distrust Propagation
1: Input: R: partially observed rating matrix, S
2: Output: U and V
3: for t = 1, . . . , T do
4:
Compute the gradients ∇U R(Ut , Vt ) and ∇V R(Ut , Vt ).
5:
Compute ∇U by Eq. 7
6:
Compute ∇V by Eq. 8
7:
Update:
Ut+1 = Ut − ηt ∇U |U=Ut ,V=Vt
Vt+1 = Vt − ηt ∇V |U=Ut ,V=Vt
8: end for
9: return UT +1 and VT +1 .

4.2. Batch Gradient Descent-Based Optimization

In optimization for supervised machine learning, there exist two regimes in which
popular algorithms tend to operate: the stochastic approximation regime, which
samples a small dataset per iteration, typically a single data point, and the batch or
sample average approximation regime, in which larger samples are used to compute an
approximate gradient. The choice between these two extremes outlines the well-known
trade-off between inexpensive noisy steps and expensive but more reliable steps.
Two preliminary examples of these regimes are the Gradient Descent (GD) and the
Stochastic Gradient Descent (SGD) methods, respectively. Both GD and SGD methods
start with some initial point and iteratively update the solution using the gradient
information at intermediate solutions. The main difference is that GD requires a full
gradient information at each iteration, while SGD only requires an unbiased estimate
of the full gradient which can be done by sampling.
We now discuss the application of the GD algorithm for solving the optimization
problem in Eq. (5), as detailed in Algorithm 1. Recall that the objective function is
not jointly convex in both U and V. On the other hand, the objective is convex in one
parameter by fixing the other one. Therefore, we follow an iterative method to minimize
the objective. At each iteration, first by fixing V, we take a step in the direction of the
negative gradient for U and repeat the same process for V by fixing U.
For ease of exposition, we introduce further notation. For any triplet (i, j, k) ∈ S , we
note that Ui,: − U j,: 2 − Ui,: − Uk,: 2 can be written as Tr(CU U), where Tr(·) denotes
the trace of the input matrix and C is a sparse auxiliary matrix defined for each triplet
with all entries equal to zero except: Cik = Cki = C j j = 1 and Ckk = Ci j = C ji = −1.
Having defined this notation, we can write the objective in Eq. (5) as
L(U, V) = R(U, V) +

λV
λS
λU
UF +
VF +
2
2
|S |






max 0, 1 − Tr Cikj U U .

(i, j,k)∈S

where Cikj is the C matrix previously defined which is associated with triplet (i, j, k).
To apply the GD method, we need to compute the gradient of L(U, V) with respect to
U and V, which we denote by ∇U = ∇U L(U, V) and ∇V = ∇V L(U, V), respectively. We
have
∇U = ∇U R(U, V) + λU U −

λS
|S |





k
1[Tr(Cikj U U)<1] UCik
j + UCi j ,

(i, j,k)∈S

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

(7)

17:18

R. Forsati et al.

where 1[·] is the indicator function which takes a value of one if its argument is true,
and zero otherwise. Similarly for ∇V , we have
∇V = ∇V R(U, V) + λV V.

(8)

The main shortcoming of the GD method is its high computational cost per iteration
due to the gradient computation (i.e., step (7)) which is expensive when the size of
social constraints S is large. We note that the size of S can be as large as O(n3 )
by considering all triplets in the social graph. In the next section, we provide an
alternative solution to resolving this issue using the stochastic gradient descent and
mini-batch SGD methods which are more efficient than the GD method in terms of the
computational cost per iteration but with a slow convergence rate in terms of target
approximation error.
4.3. Stochastic and Mini-Batch Optimization

As discussed, when the size of the social network is very large, the size of S may cause
computational problems in solving the optimization problem in Eq. (5) using the GD
method. The reason is essentially the fact that computing the gradient at each iteration
requires going through all the triplets in S , which is infeasible for large networks.
To alleviate this problem, we propose a stochastic gradient-based [Nemirovski et al.
2009] method for solving the optimization problem. The main idea is to choose a
fixed subset of triplets for gradient computation instead of all |S | triplets at each
iteration [Cotter et al. 2011]. More specifically, at each iteration, we sample B triplets
uniformly at random from S to compute the next solution. We note that this strategy
generates unbiased estimates of the true gradient and makes each iteration of the
algorithm computationally more efficient compared to the full gradient counterpart.
In the simplest case, the SGD algorithm, only one triplet is chosen at each iteration
to generate an unbiased estimate of the full gradient. We note that in practice, SGD
is usually implemented based on data shuffling, that is, making the sequence of the
training samples random and then training the model by going through the training
samples one by one. An intermediate solution, known as mini-batch SGD, chooses
a subset of triplets to compute the gradient. The promise is that by selecting more
triplets at each iteration, on one hand the variance of stochastic gradients decreases
promotional to the number of sampled triplets, and on the other hand the algorithm
enjoys the light computational cost of basic SGD method.
The detailed steps of the algorithm are shown in Algorithm 2. The mini-batch SGD
method improves the computational efficiency by grouping multiple constraints into a
mini-batch and only updating the U and V once for each mini-batch. For brevity, we
will refer to this algorithm as Mini-SGD. More specifically, the Mini-SGD algorithm,
instead of computing the full gradient over all triplets, samples B triplets uniformly at
random from S , where 1 ≤ B ≤ |S | is a parameter that needs to be provided to the
algorithm, and computes the stochastic gradient as


λS 
k
1[Tr(Cikj Ut Ut )<1] UCik
∇t =
j + UCi j ,
B
(i, j,k)∈ B

where  B is the set of B sampled triplets from S . We note that



λS
k
1[Tr(Cikj Ut Ut )<1] UCik
E[∇t ] =
j + UCi j ,
|S |
(i, j,k)∈S

that is, ∇t is an unbiased estimate of the full gradient in the right-hand side. When
B = |S |, each iteration handles the original objective function, and Mini-SGD reduces
to the batch GD algorithm. We note that both GD and SGD share the same convergence
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:19

ALGORITHM 2: Mini-SGD-Based Matrix Factorization with Trust and Distrust Propagation
1: Input: R: partially observed rating matrix, S , min batch size B
2: Output: U and V
3: for t = 1, . . . , T do
4:
∇t ← 0
5:
for b = 1, . . . , B do
6:
(i, j, k) ← Sample random triplet from S
7:
if (1 − Ui,: − U j,: )2 + Ui,: − Uk,: 2 > 0) then
8:
∇t ← Ut Cikj U
t
9:
end if
10:
end for
11:
Compute the gradients ∇U R(Ut , Vt ) and ∇V R(Ut , Vt ).
12:
Update:


λS
Ut+1 = Ut − ηt ∇U R(Ut , Vt ) + λU Ut +
∇t
B|S |
13:

Update:



Vt+1 = Vt − ηt ∇V R(Ut , Vt ) + λV Vt

14: end for
15: return UT +1 and VT +1 .

rate in terms of the√number of iterations in expectation for non-smooth optimization
problems (i.e., O(1/ T ) after T iterations), but the SGD method requires much less
running time to convergence compared to the GD method due to the efficiency of its
individual iterations.
5. EXPERIMENTAL RESULTS

In this section, we conduct exhaustive experiments to demonstrate the merits and
advantages of the proposed algorithm. We conduct the experiments on the wellknown Epinions2 dataset, aiming to accomplish and answer the following fundamental
questions.
(1) Prediction accuracy. How does the proposed algorithm perform in comparison to
the state-of- the-art algorithms with/without incorporating trust and distrust relationships between users. Whether or not the trust/distrust social network could
help in making more accurate recommendations?
(2) Correlation of social relations with rating information. To what extent are the
trusted and distrusted friends of a user u aligned with the ratings user u issued
for the reviews written by his friends? A positive answer to this question indicates
that two users will issue similar (dissimilar) ratings if they are connected by a trust
(distrust) relation and prefer to behave similarly.
(3) Model selection. What role do the regularization parameters λ S , λU , and λV play in
the accuracy of the proposed recommender system and what is the best strategy to
tune these parameters?
(4) Handling cold-start users. How does exploiting social relationships in the prediction
process affect the performance of recommendation for cold-start users?
(5) Trading trust for distrust. To what extent can the distrust relations compensate for
the lack of trust relations?
2 http://www.trustlet.org/wiki/Epinions

datasets.

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:20

R. Forsati et al.

(6) Efficiency of optimization. What is the trade-off between accuracy and efficiency by
moving from the gradient descent to the stochastic gradient descent with different
batch sizes?
In the following sections, we intend to answer these questions. We begin by introducing
the dataset we use in our experiments and the metrics we employ to evaluate the
results, followed by the detailed experimental results.
5.1. Dataset Description and Experimental Setup

The Epinions Dataset. We begin by discussing the dataset we have chosen for our
experiments. To evaluate the proposed algorithm on trust and distrust-aware recommendations, we use the Epinions dataset [Guha et al. 2004], a popular e-commerce site
and customer review website where users share opinions on various types of items such
as electronic products, companies, and movies, through writing reviews about them or
assigning a rating to the reviews written by other users. The rating values in Epinions
are discrete values ranging from not helpful (1/5) to most helpful (5/5). These ratings
and reviews could potentially influence future customers when they are about to decide
whether a product is worth buying or a movie is worth watching.
Epinions allows users to evaluate other users based on the quality of their reviews
and to make trust and distrust relations with other users in addition to the ratings.
Every member of Epinions can maintain a “trust” list of people he/she trusts that
is referred to as web of trust (social network with trust relationships) based on the
reviewers with consistent ratings or “distrust” list known as block list (social network
with distrust relationships) for reviewers whose reviews were consistently found to
be inaccurate or low quality. The fact that the dataset contains explicit positive and
negative relations between users makes it very appropriate for studying issues in
trust- and distrust-enhanced recommender systems. Epinions is thus an ideal source
for experiments on social recommendation. We remark that the Epinions dataset only
contains bivalent relations (i.e., contains only full trust and full distrust, and no gradual
statements).
To conduct the coming experiments, we sampled a subset of the Epinions dataset
with n = 121, 240 users and m = 685, 621 different items. The total number of observed
ratings in the sampled dataset is 12,721,437, which approximately includes 0.02% of
all entries in the rating matrix R which demonstrates the sparsity of the rating matrix.
We note that the selected items are the most frequently rated overall. The statistics
of the dataset are given in Table II. The social statistics of the this data source are
summarized in Table III. The frequencies of ratings for users are shown are Table IV.
In the user distrust network, the total number of issued distrust statements is 96,823.
As to the user trust network, the total number of issued trust statements is 481,799.
Experimental Setup. To better evaluate the effect of utilizing the social side information in recommendation accuracy, we employ different amount of training data 90%,
80% , 70%, and 60% to create four different training sets that are increasingly sparse,
but the social network remains the same in all of them. Training data 90%, for example, means we randomly select 90% of the ratings from the sampled Epinions dataset
as the training data to predict the remaining 10% of ratings. The random selection
was carried out five times independently to have a fair comparison. Also, since our
preliminary results on a smaller dataset revealed that hinge loss performs better than
exponential loss, in the rest of experiments, we stick to this loss function. However, we
note that exponential loss is slightly faster in optimizing the corresponding objective
function thanks to its smoothness, but it was negligible considering its worse accuracy
compared to hinge loss. All implementations are in Matlab, and all experiments were
performed on a 4-core 2.0GHZ of a load-free machine with a 12G of RAM.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:21

Table II. Statistics of Sample Data from Epinions Dataset
Used in Our Experiments
Statistic
Number of users
Number of items
Number of ratings
Number of trust relations
Number of distrust relations
Minimum number of ratings by users
Minimum number of ratings for items
Maximum number of ratings by users
Maximum number of ratings for items
Average number of ratings by users
Average number of ratings for items

Quantity
121,240
685,621
12,721,437
481,799
96,823
1
1
148735
945
85.08
15.26

Table III. Maximum and Average Trust and Distrust Relations
for Users in the Sampled Dataset
Statistics

Trust per user

Be Trusted per user

Max
Min
Average

1983
1
4.76

2941
0
4.76

Max
Min
Average

Distrust per user
1188
1
0.91

Be Distrusted per user
429
0
0.91

Table IV. Frequencies of User’s Rating
# of Ratings
# of Users

0–10
4,198,074
(≈33%)

11–20
3,053,144
(≈24%)

21–30
2,289,858
(≈18%)

31–40
1,526,572
(≈12%)

41–50
534,300
(≈4.2%)

51–60
267,150
(≈2.1%)

# of Ratings
# of Users

61–70
157,745
(≈1.24%)

71–80
143,752
(≈1.13%)

81–90
104,315
(≈0.82%)

91–100
43,252
(≈0.34%)

101–200
21,626
(≈0.17%)

201–300
10,686
(≈0.084%)

5.2. Metrics

5.2.1. Metrics for Rating Prediction. We employ two well-known measures, mean absolute
error (MAE) and root mean squared error (RMSE) [Herlocker et al. 2004] to measure the prediction accuracy of the proposed approach in comparison with other basic
collaborative filtering and trust/distrust-enhanced recommendation methods.
MAE is a very appropriate and useful measure for evaluating prediction accuracy in
offline tests [Herlocker et al. 2004; Massa and Avesani 2004]. To calculate MAE, the
predicted rating is compared with the real rating and the difference (in absolute value)
considered as the prediction error. Then, these individual errors are averaged over all
predictions to obtain the overall MAE value. More precisely, let T denote the set of
ratings to be predicted, that is, T = {(i, j) ∈ [n] × [m], Ri j needs to be predicted} and let
R̂ denote the prediction matrix obtained by algorithm after factorization. Then,


(i, j)∈T |Ri j − R̂i j |
,
MAE =
|T |
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:22

R. Forsati et al.

where Ri j is the real rating assigned by user i to item j, and R̂i j is the rating user i
would assign to item j that is predicted by the algorithm.
The RMSE metric is defined as


2
(i, j)∈T (Ri j − R̂i j )
RMSE =
.
|T |
The first measure (MAE) considers every error of equal value, while the second one
(RMSE) emphasizes larger errors. We would like to emphasize that even small improvements in RMSE are considered valuable in the context of recommender systems.
For example, the Netflix prize competition offered $ 1,000,000 reward for a reduction
of the RMSE by 10% [Victor et al. 2013].
5.2.2. Metrics for Evaluating the Correlation of Ratings with Trust/Distrust Relations. As part of
our experiments, we investigate how the explicit trust/distrust relations between users
in the social network are aligned with the implicit trust/distrust relations between
users conveyed from the rating information. We use recall, mean average precision
(MAP) [Manning et al. 2008], and normalized discount cumulative gain (NDCG) to
evaluate the ranking results. Recall is defined as the number of relevant friends divided
by the total number of friends in the social network. Precision is defined as the number
of relevant friends (trusted or distrusted) divided by the number of friends in the social
network. Given a user u, let ri be the relevance score of the friend ranked at position i,
where ri = 1 if the user is relevant to the u and ri = 0 otherwise. Then we can compute
the average precision (AP) as


i ri × Precision@i
.
AP =
# of relevant friends
MAP is the average of AP over all the users in the network.
NDCG is a normalization of the discounted cumulative gain (DCG) measure. DCG
is a weighted sum of the degree of relevancy of the ranked users. The weight is a
decreasing function of the rank (position) of the user, and therefore called discount.
NDCG normalizes DCG by the ideal DCG (IDCG), which is simply the DCG measure
of the best-ranking result. Thus NDCG measure is always a number in [0, 1]. NDCG
at position k is defined as
k

2ri − 1
NDCG@k = Zk
,
log(i + 1)
i=1

where k is also called the scope, which means the number of top-ranked users presented
to the user and Zk is chosen such that the perfect ranking has an NDCG value of 1.
We note that the base of the logarithm does not matter for NDCG, since constant
scaling will cancel out due to normalization. We will assume it is the natural logarithm
throughout this article.
5.3. Model Selection

Tuning of parameters (a.k.a model selection in learning community) is a critical problem in most of the learning problems. In some situations, the learning performance
may drastically vary with different choices of parameters. There are three parameters
in Eq. (5) that play very important roles in the effectivity of the proposed algorithm.
These are λU , λV , and λ S . Between these, λ S controls how much the proposed algorithm
should incorporate the information of the social network in completing the partially
observed rating matrix. In the extreme case, a very small value for λ S , the algorithm
almost forgets that social information exists between the users and only utilizes the
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:23

Fig. 2. Grid search to find the best values for λU and λC on the dataset with 90% of rating information.

observed user-item rating matrix for factorization. On the other hand, if we employ
a very large value for λ S , the social network information will dominate the learning
process, leading to poorer performance. Therefore, in order to not hurt the recommendation performance, we need to find a reasonable value for a social regularization
parameter. To this end, we analyze how the combination of these parameters affect the
recommendation performance.
We conduct a grid search on the potential values of two parameters λ S and λV to find
the combination with the best performance. Figure 2 shows the grid search results for
these parameters on a dataset with 90% of training data, where the optimal prediction
accuracy is achieved at point (14.8, 11) with the optimal RMSE = 1.12. We would
like to emphasize that we have done the cross-validation only for pairs of (λ S , λV ) and
(λ S , λU ), considering (i) the grid search for the triplet (λ S , λU , λV ) is computationally
burdensome, and (ii) our preliminary experiments showed that λV and λU behave
similarly with respect to λ S . Based on the results reported in Figure 2, in the remaining
experiments, we set λ S = 14.8, λV = 11, and λU = 13 when training is performed on
the dataset with 90% rating information. We repeat the same process to find out the
best setting of regularization parameters for other datasets with 80%, 70%, and 60%
rating data as well.
5.4. Baseline Methods

Here we briefly discuss the baseline algorithms against which we intend to compare
the proposed algorithm. The baseline algorithms are chosen from both types of
memory-based and model-based recommender systems with different types of trust
and distrust relations. In particular, we consider the following basic algorithms.
—MF (Matrix Factorization-based Recommender). This is the basic matrix
factorization-based recommender formulated in the optimization problem in Eq. (1),
which does not take social data into account.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:24

R. Forsati et al.

—MF+T (Matrix Factorization with Trust Information). To exploit the trust relations
between users in matrix factorization, Ma et al. [2009b] relied on the fact that the
distance between latent features of users who trust each other must be minimized
and can be formulated as the following objective:
n
1 
min
D(Ui,: , U j,: ),
U 2
i=1 j∈N+ (i)

where N+ (i) is the set of users the ith user trusts in the social network (i.e., Si j = +1).
By employing this intuition in the basic formulation in Eq. (1), Ma et al. [2009b]
solved the following optimization problem:
⎡
⎤
n


 

1
α
λ
λ
2
U
V

min ⎣
Ri j − Ui,:
V j,: +
D(Ui,: , U j,: ) +
UF +
VF ⎦ .
U,V
2
2
2
2
i=1 j∈N+ (i)

(i, j)∈R

—MF+D (Matrix Factorization with Distrust Information). The basic intuition behind
the algorithm proposed in Ma et al. [2009b] to exploit the distrust relations is as
follows: if user ui distrusts user u j , then we can assume that their corresponding
latent features Ui,: and U j,: would have a large distance. As a result, we aim to
maximize the following quantity for all users:
max
U

n
1 
D(Ui,: , U j,: ),
2
i=1 j∈N− (i)

where N− (i) denotes the set of users the ith users distrusts (i.e., Si j = −1). Adding this
term to the basic optimization problem in Eq. (1), we obtain the following optimization
problem:
⎡
⎤
n





1
β
λU
λV
2

min ⎣
Ri j − Ui,:
V j,: −
D(Ui,: , U j,: ) +
UF +
VF ⎦ .
U,V
2
2
2
2
(i, j)∈R

i=1 j∈N− (i)

—MF+TD (Matrix Factorization with Trust and Distrust Information). This algorithm
stands for the algorithm proposed in the present work. We note that there is no algorithm in the literature that exploits both trust and distrust relations in factorization
process simultaneously.
—NB (Neighborhood-Based Recommender). This algorithm is the basic memory-based
recommender algorithm that predicts a rating of a target item i for user u using a
combination of the ratings of neighbors of u (similar users) that already issued a
rating for item i. Formally,


u ∈N (u),Wuu >0 Wuu (Rui − R̄u)


R̂ui = R̄u +
,
(9)
u ∈N (u),Wuu Wuu
where the pairwise weight Wuu between pair of users (u, u ) is calculated by the
Pearson’s correlation coefficient [Herlocker et al. 2004]
—NB+T (Neighborhood with Trust Information) [Massa and Avesani 2004, 2009;
Golbeck 2005]. The basic idea behind the trust-based recommender systems proposed in TidalTrsut [Golbeck 2005] and MoleTrsut [Massa and Avesani 2004] is to
limit the set of neighbors in Eq. (9) to the users who are trusted by user u. The
distinguishing feature of these algorithms is the mechanism of trust propagation to
estimate the trust transitively for all the users. By adapting Eq. (9) to only consider
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

trustworthy neighbors in predicting the new ratings, we obtain


u ∈N+∗ (u),Wuu >0 Wuu (Rui − R̄u)


R̂ui = R̄u +
,
u ∈N+∗ (u),Wuu >0 Wuu

17:25

(10)

where N+∗ (u) is the set of trusted neighbors of u in the social network with propagated
trust relations (when there is no propagation, we have N+∗ (u) = N+ (u)). We note that
instead of Pearson’s correlation coefficient as the weighting schema, we can infer
the weights exploiting the social relation between the users. Since for the dataset
we consider in our experiments, the trust/distrust relations are binary values, the
social-based pairwise distance would be simply the hamming distance between the
binary vector representation of social relations of users. For implementation details,
we refer to Victor et al. [2011a, Chapter 6].
—NB+TD-F (Neighborhood with Trust Information and Distrust Information as Filtration) [Victor et al. 2011b, 2013]. A simple strategy for using distrust relations
in the recommendation is to filter out distrusted users from the list of neighbors in
predicting the ratings. As a result, we adapt Eq. (9) to exclude distrusted users from
the users’ propagated web of trust.
—NB+TD-D (Neighborhood-Based with Trust Information and Integrated Distrust Information) [Victor et al. 2011b, 2013]. In the same spirit as the filtration strategy,
we can use distrust relations to debug the trust relations. More specifically, if user u
trusts user v, v trusts w, and u distrusts w, then the latter distrust relation contradicts the propagation of the trust from u to w and can be excluded from the prediction.
In this method, distrust is used to debug the trust relations.
5.5. On the Consistency of Social Relations and Rating Information

As already mentioned, the Epinions website allows users to write reviews about products and services and to rate reviews written by other users. Epinions also allows
users to define their web of trust, that is, “reviewers whose reviews and ratings have
been consistently found to be valuable” and their block list, that is, “reviewers whose
reviews are found to be consistently inaccurate or not valuable”. Different intuitions
on interpreting these social information will result in different models. The main rationale behind incorporating trust and distrust relations in recommendation process
is to take the trust/distrust relations between users in the social network as the level
of agreement between ratings assigned to reviews by users.3 Therefore, investigating
the consistency or alignment between user ratings (implicit trust) and trust/distrust
relations in the social network (explicit trsut) become an important issue.
Here, we aim to empirically investigate whether or not there is a correlation between
a user’s current trustees/friends or distrusted friends and the ratings that user would
assign to reviews issued by his neighbors. Obviously, if there is no correlation between
the social context of a user and his/her ratings to reviews written by his neighbors,
then the social structure does not provide any advantage to the rating information.
On the other hand, if there exists such a correlation, then the social context could be
supplementary information to compensate for the lack of rating information to boost
the accuracy of recommendations.
The consistency of trust relations and rating information issued by users on the
reviews written by his trustees has been analyzed [Ziegler and Golbeck 2007; Guo et al.
2014]. However, Ziegler and Golbeck [2007] also claimed that social trust (i.e., explicit
trust) and similarity between users based on their issued ratings (i.e., implicit trust) are
3 In

the literature, the similarity between users conveyed from the rating information issued by users and
the direct relation in the social network are usually referred to as implicit and explicit trust, respectively.

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:26

R. Forsati et al.
Table V. Consistency of Implicit and Explicit Trust Relations in the Dataset for Different Ranges of
Ratings Measured in Terms of NDCG, Recall, and MAP
# of Ratings
0–20
21–40
41–60
61–80
≥81

NDCG@10

NDCG@20

Recall@10

Recall@20

Recall@40

MAP

0.083
0.108
0.117
0.120
0.135

0.078
0.103
0.112
0.117
0.126

0.054
0.080
0.083
0.088
0.091

0.092
0.125
0.128
0.132
0.151

0.156
0.198
0.225
0.230
0.253

0.140
0.190
0.208
0.230
0.244

Table VI. Consistency of Implicit and Explicit Distrust Relations in the Dataset for Different Ranges
of Ratings Measured in Terms of NDCG, Recall, and MAP
# of Ratings
0–20
21–40
41–60
61–80
≥81

NDCG@10

NDCG@20

Recall@10

Recall@20

Recall@40

MAP

0.065
0.071
0.082
0.089
0.104

0.057
0.068
0.072
0.078
0.096

0.045
0.060
0.075
0.081
0.087

0.071
0.077
0.085
0.105
0.125

0.132
0.140
0.158
0.164
0.191

0.130
0.134
0.152
0.160
0.183

not the same, and can be used complementary. According to Ma [2013], when comparing
implicit social information with explicit social information, the performance of using
implicit information is slightly worse. We further investigate the same question about
the consistency of distrust relations and ratings issued by users to their distrusted
neighbors. The positive answer to this question can be interpreted as follows. Given
that user u is interested in item i, the chances that v, trusted (distrusted) by u, also likes
this item i is much higher (lower) than for user w not explicitly trusted (distrusted)
by u.
To measure the similarity between users, there are several methods we can borrow
in the literature. In this article, we adopt the most popular approach that is referred to
as the Pearson correlation coefficient (PCC) P : U × U → [−1, +1] [Breese et al. 1998;
Massa and Avesani 2009], which is defined as

m
i=1 (Rui − R̄u)(Rvi − R̄v )
P(u, v) = 	

, ∀u, v ∈ U,

m
m
2×
2
(R
−
R̄
)
(R
−
R̄
)
ui
u
vi
v
j=1
j=1
where R̄u and R̄v are the average of ratings issued by users u and v, respectively. The
PCC measures the extent to which there is a linear relationship between the rating
behaviors of the two users, the extreme values being −1 and 1. The similarity of two
users becomes negative when users have completely diverging ratings. We note that
this quantity can be considered as the implicit trust between users that is conveyed
via ratings given by users.
To conduct this set of experiments, we first group all the users in the training data
set based on the number of ratings, and then measure the prediction accuracies of
different user groups. Users are grouped into five classes: [1, 20), [20, 40), [40, 60),
[60, 80), and >81. In order to have a comprehensive view of the ranking performance,
we present the NDCG, recall, and MAP scores of trust and distrust alignments on the
Epinions dataset in Table V and Table VI, respectively. We note that the dataset we
use in our experiments only contains bivalent trust values, that is, −1 and +1, and it
is not possible to have an ordering on the list of friends (time stamp of relations would
be an option to order the friends, but unfortunately it is not available in our dataset).
To compute the NDCG, we use the ordering of trusted/distrusted friends which yields
the best value.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:27

Table VII. Alignment Rate of Users in Establishing Trust/Distrust Relationships with Future Users in the
Social Network Based on the Majority Vote of Their Current Trusted/Distrusted Friends
Setting

Type of Relation (u ; w)

% of Relations

Alignment Rate (%)

n+ > n−

+
−
+
−
+

48.80
2.54
1.15
8.02
39.49

92.09
8.15
17.88
83.42
−

n+ < n−
n+ = n− > 0 or n+ = n− = 0

Note: The number of trusted friends (+) and distrusted friends (−) are denoted by n+ and n− ,
respectively. Here u denotes the current user and w stands for a future user in the network.

On the positive side, we observe a clear trend of alignment between ratings assigned
by a user and the type of relation he has made in the social network. This observation
coincides with our intuition. Overall, when more ratings are observed for a user, the
similarity calculation process will find more accurate similar or dissimilar neighbors
for this user, since we have more information to represent or interpret this user. Hence,
by increasing the number of ratings, it is conceivable from the results in Tables V
and VI that the alignment between implicit and explicit neighbors becomes better. By
comparing the results in Tables V and VI we can see that trust relations are slightly
better aligned than the distrust relations.
On the negative side, the results show that the NDCG on both types of relations is
small. One explanation for this phenomenon is that the Epinions dataset is not tightly
bound to a specific application. For example, a user may trust or distrust anther user
based on his/her comments on a specific product, but they might have similar taste
on other products. Furthermore, compared to other datasets such as FilmTrusts, the
Epinions dataset is a very sparse dataset, and consequently it is relatively inaccurate
to rely on the rating information to compute the implicit trust relations. Finally, our
approach to distinguishing trust/distrust lists from the rating information is limited
by the PCC trust metric we have utilized. We conjecture that better trust metrics
able to exploit other side information, such as time and interactional information,
would be helpful in distinguishing implicit trusted/distrusted friends, leading to better
alignment between implicit and explicit trust relations.
We also conduct experiments to evaluate the consistency of social network only
based on the trust/distrust relations between users. In particular, we investigate to
what extent a user’s relations are aligned with the opinion of his/her neighbors in
the social network. More specifically, let u be a user who is about to make a trust or
distrust relation to another user v. We assume that n+ number of u’s neighbors trust v
and n− number of u’s neighbors distrust v. We note that in the real dataset, the distrust
relations are hidden. To conduct this set of experiments, we randomly sample 30% of
the relations from the social network and use the remaining 70% to predict the type of
sampled relations4 by majority voting.
Table VII shows the results on the consistency of social relations. We observe that in
all cases there is an alignment between the opinions of a user’s friends and his/her own
relation (92.09% and 83.42% when the majority of friends trust and distrust the target
user, respectively). This might be due to the social influence of people on the social
network; however, it is hard to justify the existence of such a correlation in the Epinions
dataset which includes reviews for a diverse set of products and taste of users. One
interesting observation from the results reported in Table VII is the case where the
number of distrusted users dominates the number of trusted users (i.e., n− > n+ ). While
4A

more realistic way would be to use the time stamp of relations to create the training and test sets.

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:28

R. Forsati et al.

Table VIII. Accuracy of Prediction of Matrix Factorization with Three Different Methods Measured in Terms of
MAE and RMSE Errors
k

% of Training

10

60%
70%
80%
90%

20

60%
70%
80%
90%

MF

MF+T

MF+D

MF+TD

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

Measure

0.9813 ± 0.042
1.6050 ± 0.032
0.9462 ± 0.083
1.5327 ± 0.032
0.9150± 0.022
1.3824 ± 0.032
0.8921 ± 0.025
1.2166 ± 0.017

0.8561 ± 0.032
1.4125 ± 0.022
0.8332 ± 0.092
1.2407 ± 0.063
0.8206 ± 0.041
1.1906 ± 0.042
0.8158 ± 0.016
1.1403 ± 0.027

0.9720 ± 0.038
1.5036 ± 0.040
0.9241 ± 0.012
1.4405 ± 0.023
0.8722 ± 0.034
1.3155 ± 0.026
0.8736 ± 0.053
1.1869 ± 0.049

0.8310 ± 0.016
1.2294 ± 0.086
0.8206 ± 0.023
1.1562 ± 0.043
0.8113 ± 0.032
1.1061 ± 0.021
0.8025 ± 0.014
1.0872 ± 0.020

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

0.9972 ± 0.016
1.6248 ± 0.014
0.9688 ± 0.019
1.5162 ± 0.016
0.9365 ± 0.025
1.4081 ± 0.015
0.9224 ± 0.016
1.2207 ± 0.0 18

0.8431 ± 0.018
1.3904 ± 0.042
0.8342 ± 0.062
1.2722 ± 0.027
0.8172 ± 0.011
1.1853 ± 0.023
0.8128 ± 0.021
1.1402 ± 0.026

0.9746 ± 0.060
1.5423 ± 0.046
0.9350 ± 0.022
1.4540 ± 0.075
0.8705 ± 0.016
1.3591 ± 0.073
0.8805 ± 0.032
1.1933 ± 0.028

0.8475 ± 0.012
1.1837 ± 0.023
0.8290 ± 0.034
1.1452 ± 0.016
0.8129 ± 0.025
1.1049 ± 0.082
0.8096 ± 0.010
1.0851 ± 0.011

Note: The parameter k represents the number of latent features in factorization.

the distrust relations are private to other users, we can see that there is a significant
alignment between a users relation type and his distrusted friends.
5.6. On the Power of Utilizing Social Relationships

We now turn to investigate the effect of utilizing social relationships between users
on the accuracy of recommendations in factorization-based methods. In other words,
we would like to experimentally evaluate whether incorporating distrust can indeed
enhance the trust-based recommendation process. To this end, we run four different
MF (i.e., pure matrix factorization-based algorithm), MF+T (i.e., matrix factorization
with only trust relationships), MF+D (i.e., matrix factorization with only distrust relationships), and MF+TD (i.e., the algorithm proposed here) algorithms on the dataset.
We run the algorithms with k = 10 and k = 20 latent vector dimensions. As mentioned
earlier, different amounts of training data 90%, 80% , 70%, and 60% have been used to
create four different training sets that are increasingly sparse, but the social network
remains the same in all of them. We evaluate all algorithms by both MAE and RMSE
measures.
Table VIII shows the MAE and RMSE errors for the four sampled datasets. First, as
we expected, the performance of all learning algorithms improves with an increasing
number of training data. It is also not surprising to see that the MF+T, MF+D, and
MF+TD algorithms which exploit social side information perform better than the pure
matrix factorization-based MF algorithm. Second, the proposed algorithm outperforms
all other baseline algorithms for all the cases, indicating that it is effective for incorporating both types of social side information in the recommendation. This result by itself
indicates that besides trust relationships in the social network, distrust information is
also a rich source of information and can be utilized in recommendation algorithms. We
note that distrust information needs to be incorporated carefully, as its nature is totally
different from trust information. Finally, it is noticeable that MF+T outperforms the
MF+D algorithm due to a huge number of trust relations to distrust relations in our
dataset. It is also remarkable that users are more likely to be influenced by their friends
to make trust relations than the distrust relations due to the private nature of distrust
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:29

Table IX. Comparison with Other Popular Methods
Method

Parameter (s)

MAE

RMSE

MF
MF+T
MF+D
MF+TD

k = 10 and λU = λV = 5
k = 10, λU = λV = 5 , and α = 1
k = 10, λU = λV = 5 , and β = 10
k = 10, λU = 13, λV = 11 , and λ S = 14.8

0.8921
0.8158
0.8736
0.8025

1.2166
1.1403
1.1852
1.0872

NB
NB+T
NB+TD-F
NB+TD-D

p=1
p = 1 and q = 3
p = 1 and q = 3

0.9381
0.8904
0.8692
0.8728

1.5275
1.3455
1.2455
1.2604

Note: The reported values are the MAE and RMSE on the dataset with 90%
rating information. The values of parameters for each specific algorithm are
included in the second column.

relations in the Epinions dataset. This might lead us to believe that distrust relations
have better quality than trust relations, which requires a deeper investigation to be
verified.
5.7. Comparison to Baseline Algorithms

Another question that is worth investigating is how state-of-the-art approaches perform compared to the method proposed here. To this end, we compare the performance of the MF-TD algorithm with the baseline algorithms introduced in Section 5.4.
Table IX contains the results of our experiments with eight different algorithms on the
dataset with 90% rating data. The second column in the table represents the configuration of parameters used by each algorithm.
When we utilize trust/distrust relations in neighborhood-based algorithms, a crucial
decision we need to make is to which level the propagation must be performed (no propagation corresponds to single-level propagation which only includes direct neighbors).
Let p and q denote the level of propagation for trust and distrust relations, respectively.
Let us first consider the trust propagation to decide the value of p. We note that there
is a trade-off between accuracy and the level of trust propagation: longer propagation
levels results in less accurate trust predictions. This is due to the fact that when we
use longer propagation levels, the further away we are heading from each user, and
consequently decrease the confidence on the predictions. Obviously, this affects the accuracy of the recommendations significantly. As a result, for the trust propagation we
only consider single-level propagation by choosing p = 1 (i.e., N+∗ = N+ ). We also note
that since in the Epinions dataset, a user can not simultaneously trust and distrust
another user, in the neighborhood-based method with distrust relations, the debugging
only makes sense for propagated information. Therefore, we perform a three-level distrust propagation (q = 3) to constitute the set of distrusted users for each users. We
note that the longer the propagation levels, the more often distrust evidence can be
found for a particular user, and hence the less neighbors will be left to participate in
the recommendation process. For factorization-based methods, the value of regularization parameters, that is, λU , λV , and λ S , are determined by the procedure discussed in
Section 5.3.
The results of Table IX reveal some interesting conclusions as summarized here.
—From Table IX, we can observe that for factorization-based methods, incorporating
trust or distrust information boosts the performance of recommendation in terms of
both accuracy measures. This demonstrates the advantages of trust and distrustaware recommendation algorithms. We also can see that both MF+T and MF+D
perform better than the non-social MF, but the performance of MF+T is significantly
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:30

R. Forsati et al.

better than MF+D. As discussed, this observation does not indicate that trust relations are more beneficial than distrust relations, as in our dataset, only 16.7% of
relations are distrust relations. The MF+TD algorithm that is able to employ both
types of relations is significantly better than other algorithms, which demonstrates
the advantages of our proposed method in utilizing trust and distrust relations.
—Looking at the results reported in Table IX, it can immediately be noticed that
the incorporation of trust and distrust information in neighborhood-based methods
decreases the prediction error, but the improvement is not as significant as the
factorization-based methods. We note that for the NB+T method with longer levels
of propagation ( p = 2, 3), our experiments revealed that the accuracy remains almost
the same or has gotten worse on both MAE and RMSE measures, and this is why we
only report the results only for p = 1. In contrast, for distrust propagation, we found
out that q = 3 has a visible impact on the performance of both filtering and debugging
methods. We would like to emphasize that for longer levels of distrust propagation
in the Epinions dataset, that is, q > 4, we found that the size of the set of distrusted
users N−∗ (·) becomes large for most users, which degrades the prediction accuracy. We
also observe another interesting result about the performance of the NB+TD method
with filtering and debugging strategies. We found that although filtering generates
slightly better predictions, NB+TD-F performs almost as well as the NB+TD-D
method. Although this observation does not suggest any of these methods as the
method of choice in incorporating distrust, we believe that the accuracy might differ
from dataset to dataset, and it strongly depends on the propagation/aggregation
strategy.
—Considering the results for both model-based and memory-based methods in
Table IX, we can conclude a few interesting observations. First, we notice that
factorization-based methods with trust/distrust information perform better than the
neighborhood-based methods. Second, the incorporation of trust and distrust relations in matrix factorization has significant improvement compared to improvement
achieved by memory-based methods. Although the type of filtration or debugging
strategy could significantly affect the accuracy of incorporating distrust in memorybased methods, the main shortcoming of these methods comes from the fact that
these algorithms somehow exclude the influence of distrusted users from the rating prediction. This stands in stark contrast to the model proposed in this article
that ranks the neighbors based on the type of relation. This observation necessitates devising better algorithms for propagation and aggregation of trust/distrust
information in memory-based methods.
5.8. Handling Cold-Start Users by Social Side Information

In this section, we demonstrate the use of the social network to further illustrate the
potential of the proposed framework and the relevance of incorporating side information. To do so, as another set of our experiments, we intend to examine the performance
of proposed algorithm on cold-start users. Addressing cold-start users (i.e., users with
few ratings or new users) is very important for the success of recommender systems
due to the huge numbers of this type of users in many real-world systems. As a result, handling cold-start users is one of the main challenges in the existing systems.
To evaluate different algorithms, we randomly select 30%, 20%, 10%, and 5% as the
cold-start users. For cold-start users, we do not include any rating in the training data
and consider all the ratings made by cold-start users as testing data.
Table X shows the performance of the previously mentioned algorithms. As it is
clear from Table X, when the number of cold-start users is low with respect to the
total number of users, say 5% of total users, the affect of the distrust relationships
is negligible in prediction accuracy. But, when the number of cold-start users is high,
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:31

Table X. Accuracy of Handling Cold-Start Users and the Effect of Social Relations
% of Cold-start Users
30%
20%
10%
5%

Measure
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

MF

MF+T

MF+D

MF+TD

0.9923
1.7211
0.9812
1.7088
0.9334
1.4222
0.9134
1.3852

0.8824
1.5562
0.8805
1.4339
0.8477
1.3782
0.8292
1.2921

0.9721
1.6433
0.9505
1.6250
0.9182
1.4006
0.8633
1.3255

0.8533
1.4802
0.8472
1.2630
0.8322
1.2655
0.8280
1.2888

Note: The number of leant features in this experiments is set to k = 10. The first
column shows the number of cold-start users sampled randomly from all users
in the dataset. For the cold-starts users, all the ratings have been excluded from
the training data and used in the evaluation of three different algorithms.

exploiting the trust and distrust relationships significantly improves the performance
of the recommendation. This result is interesting, as it reveals that the lack of rating
information for cold-start and new users can be alleviated by incorporating the social
relations of users, and in particular, both trust and distrust relationships.
5.9. Trading Trust for Distrust Relationships

We also compare the potential benefit of trust relations to distrust relations in the
proposed algorithm. More specifically, we would like to see to what extent the distrust
relations can compensate for the lack of trust relations. We run the proposed algorithm
with the subset of trust and distrust relations and compare it to the algorithm which
only utilizes all of the trust relations. To set up this set of experiments, we randomly
sample a subset of trust relations and gradually increase the amount of distrust relations to see when the effect of distrust information compensates for the effect of missed
trust relations.
We sample 433,619 (approximately 90%) trust relations from the total 481,799 trust
relations and vary the number of distrust relations fed to the proposed algorithm.
Table XI reports the accuracy of the proposed algorithm for different numbers of distrust relations in the datasets. All these samplings have been done uniformly at random. We use 90% of all ratings for training and the remaining 10% for evaluation,
and set the dimension of the latent features to k = 10. As can be concluded from
Table XI, when we feed the proposed algorithm MF+TD with 90% of trust and 50%
of the distrust relations, it reveals very similar behavior to the trust-enhanced matrix factorization-based method MF+T, which only utilizes all the trust relations in
factorization. This result is interesting in the sense that the distrust information between users is as important as the trust information (we note that in this scenario,
the number trust relations excluded from the training is almost same as the number
of distrust relations included). By increasing the number of distrust relations, we can
observe that the accuracy of recommendations increases as expected. In summary, this
set of experiments validates that incorporating distrust relations could indeed enhance
the trust-based recommendation process and could be considered as a rich source of
information to be exploited.
5.10. On the Impact of Batch Size in Stochastic Optimization

As mentioned earlier, directly solving the optimization problem in Eq. (5) using full
gradient descent method requires going through all the triplets in the constraint set
S , which could be computationally expensive due to the huge number of triplets in
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:32

R. Forsati et al.
Table XI. Accuracy of Proposed Algorithm on a Dataset with 39,0257 (≈ 90%) Trust Relations
Sampled Uniformly at Random from All Trust Relations with Varied Number of Distrust Relations
Method

# of Trust Relations

MF+TD

433,619 (≈90%)

# of Distrust Relations
9,682 (≈10%)
19,364 (≈20%)
29,047 (≈30%)
38,729 (≈40%)
48,411 (≈ 50%)
58,093 (≈60%)
67,776 (≈70%)
77,458 (≈80%)
87,140 (≈90%)
96,823 (= 100%)

MF+T

481,799 (=100%)

0

Measure

Accuracy

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

0.8803 ± 0.051
1.2166 ± 0.028
0.8755 ± 0.033
1.1944 ± 0.042
0.8604 ± 0.036
1.1822 ± 0.081
0.8431 ± 0.047
1.1706 ± 0.055
0.8165 ± 0.056
1.1425 ± 0.091
0.8130 ± 0.035
1.1380 ± 0.046
0.8122 ± 0.041
1.1306 ± 0.042
0.8095 ± 0.036
1.1290 ± 0.085
0.8061 ± 0.044
1.1176 ± 0.067
0.8050 ± 0.052
1.1092 ± 0.063

MAE
RMSE

0.8158 ± 0.016
1.1403 ± 0.027

Note: The learning is performed based on 90% of all ratings with k = 10 as the dimension of
latent features.

S . To overcome this efficiency problem, one can turn to the stochastic gradient scent
method which tries to generate unbiased estimates of the gradient at each iteration in
a much cheaper way by sampling a subset of triplets from S .
To accomplish this goal, we perform gradient descent and stochastic gradient descent
to solve the optimization problem in Eq. (5) to find the matrices U and V following the
updating equations derived in Eqs. (7) and (8). At each iteration t, the currently learned
matrices Ut and Vt are used to predict the ratings in the testset. In particular, at each
iteration, we evaluate the RMSE and MAE on the testset and terminate training once
the RMSE and MAE starts increasing or once the maximum number of iterations is
reached. We run the algorithm with latent vectors of dimension k = 10.
We compare the computational efficiency between the proposed algorithm with GD
and mini-batch SGD with different batch sizes. We note that the GD updating rule
can be considered a mini-batch SGD, where the batch size B is deterministically set
to be B = |S |, and simple SGD can be considered a mini-batch SGD with B = 1. We
remark that in contrast to GD method which uses all the triplets in S for gradient
computation at each iteration, for the SGD method—due to uniform sampling over all
tuples in S —some of the tuples may be used more than once and some of the tuples
might never been used for gradient computation.
Figures 3 and 4 show the convergence rate of four different updating rules in terms
of the number of iterations t for two different measures RMSE and RME, respectively. The first algorithm denoted by GD runs the simple full gradient descent iteratively to optimize the objective. The other three algorithms SGD1, SGD2, and SGD3
in the figures use the batch sizes B = 0.1 ∗ |S |, B = 0.2 ∗ | S |, and B = 0.3 ∗ |S |,
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:33

Fig. 3. Comparison of accuracy of prediction in terms of RMSE with GD and SGD with three varied batch
sizes.

respectively. In our experiments, due to very slow convergence of the basic SGD method
with B = 1 in comparison to other fours methods, we simply exclude its result from the
discussion.
In terms of accuracy of predictions, from both Figures 3 and 4, we can conclude that
the GD has the best convergence and SGD3 has the worst convergence in all settings.
This is because, although all four of the algorithms use an unbiased estimate of the
true gradient to update the solution at each iteration, the variance of each stochastic
gradient is proportional to the size of the batch size B. Therefore, for larger values of B,
the variance of stochastic gradients is smaller, and the algorithm convergences faster,
but for smaller values of B, the algorithm suffers from high variance in stochastic
gradients and convergences slowly. We emphasize that this comparison holds for iteration complexity which is different from the computational complexity (running time)
of individual iterations. More specifically, each iteration of GD requires |S | gradient
computations, while for SGD, we only need to perform B  |S | gradient computations.
In summary, SGD has lightweight iteration but requires more iterations to converge.
In contrast, GD takes expensive steps in a much fewer number of iterations. From Figures 3 and 4, it is noticeable that although a large number of iterations is usually needed
to obtain a solution of desirable accuracy using SGD, the lightweight computation per
iteration makes SGD attractive for the optimization problem in Eq. (5) for large number
of users. We also not that for the GD method, the error is a monotonically-decreasing
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:34

R. Forsati et al.

Fig. 4. Comparison of accuracy of prediction in terms of MAE with GD and SGD with three varied batch
sizes.

function it terms of the number of iterations t, but for the SGD-based methods, this
does not hold. This is because although the SGD algorithm is guaranteed to converge to
an optimal solution (at least in expectation), there is no guarantee that the stochastic
gradients provide a descent direction for the objective at each iteration due to the noise
in computing gradients. As a result, for a few iterations, we can see that the objective
increases but finally it convergences as expected.
6. CONCLUSIONS AND FUTURE WORKS

In this article, we have made progress towards making distrust information beneficial
in the social recommendation problem. In particular, we have proposed a framework
based on matrix factorization which is able to incorporate both trust and distrust relationships between users in a factorization algorithm. We experimentally investigated
the potential of distrust as side information to overcome data sparsity and cold-start
problems in traditional recommender systems. In summary, our results showed that
more accurate recommendations can be obtained by incorporating distrust relations,
indicating that distrust information can indeed be beneficial for the recommendation
process.
This work leaves few directions, both theoretically and empirically, as future work.
From an empirical point of view, it would be interesting to extend our model for
weighted social trust and distrust relations. One challenge in this direction is that,
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:35

as far as we know, there is no publicly available dataset that includes weighted (gradual) trust and distrust information. Also, the experimental results we have conducted
on the consistency of social relations with rating information hint at a number of potential enhancements in future work. In particular, it would be interesting to further
examine the correlation between implicit and explicit distrust information. An important challenge in this direction is to develop better metrics to measure the implicit
trust between users, as the simple metrics such as the Pearson correlation coefficient
seem insufficient. Furthermore, since we only consider distrust between users, it would
be easy to generalize our model in the same way to incorporate dissimilarity between
items and investigate how it works in practice. Also, our preliminary results indicated
that hinge loss almost performs better than exponential loss, but from the optimization
viewpoint, exponential loss is more attractive due to its smoothness. So, an interesting
direction would be to use a smoothed version of hinge loss to gain from both optimization efficiency and algorithmic accuracy.
ACKNOWLEDGMENTS
The authors would like to thank the Associate Editor and three anonymous reviewers for their immensely
insightful comments and helpful suggestions on the original version of this article. R. Forsati would also
like to thank Professor Mohamed Mokbel, Department of Computer Science and Engineering, University of
Minnesota, for the opportunity to visit his research group while doing this work.

REFERENCES
Gediminas Adomavicius and Alexander Tuzhilin. 2005. Toward the next generation of recommender systems:
A survey of the state-of-the-art and possible extensions. IEEE Trans. Knowl. Data Eng. 17, 6 (2005),
734–749.
Deepak Agarwal and Bee-Chung Chen. 2010. fLDA: Matrix factorization through latent dirichlet allocation.
In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining. ACM, 91–100.
Paolo Avesani, Paolo Massa, and Roberto Tiella. 2005. A trust-enhanced recommender system application:
Moleskiing. In Proceedings of the ACM Symposium on Applied Computing. 1589–1593.
Giacomo Bachi, Michele Coscia, Anna Monreale, and Fosca Giannotti. 2012. Classifying trust/distrust relationships in online social networks. In International Conference on Privacy, Security, Risk and Trust
(PASSAT) and International Confernece on Social Computing (SocialCom). IEEE, 552–557.
Jesús Bobadilla, Fernando Ortega, Antonio Hernando, and Abraham Gutiérrez. 2013. Recommender systems
survey. Knowl. Based Syst. 46 (2013), 109–132.
John S. Breese, David Heckerman, and Carl Kadie. 1998. Empirical analysis of predictive algorithms for
collaborative filtering. In Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence.
Morgan Kaufmann Publishers Inc., 43–52.
Moira Burke and Robert Kraut. 2008. Mopping up: Modeling Wikipedia promotion decisions. In Proceedings
of the ACM Conference on Computer Supported Cooperative Work. ACM, 27–36.
Dorwin Cartwright and Frank Harary. 1956. Structural balance: A generalization of Heider’s theory. Psychol.
Rev. 63, 5 (1956), 277.
Gang Chen, Fei Wang, and Changshui Zhang. 2009. Collaborative filtering using orthogonal nonnegative
matrix tri-factorization. Inf. Process. Manag. 45, 3 (2009), 368–379.
Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan. 2011. Better mini-batch algorithms via
accelerated gradient methods. In Conference on Neural Information Processing Systems (NIPS). Vol. 24
1647–1655.
David Crandall, Dan Cosley, Daniel Huttenlocher, Jon Kleinberg, and Siddharth Suri. 2008. Feedback effects
between similarity and social influence in online communities. In Proceedings of the 14th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining. ACM, 160–168.
Sanjoy Dasgupta, Michael L. Littman, and David McAllester. 2002. PAC generalization bounds for cotraining. In Proceedings of the Conference on Neural Information Processing Systems. 375–382.
Mukund Deshpande and George Karypis. 2004. Item-based top-n recommendation algorithms. ACM Trans.
Inf. Syst. 22, 1 (2004), 143–177.
Thomas DuBois, Jennifer Golbeck, and Aravind Srinivasan. 2011. Predicting trust and distrust in social
networks. In Proceedings of the IEEE 3rd International Conference on Privacy, Security, Risk and Trust
(PASSAT), and IEEE 3rd International Conference on Social Computing (SocialCom). IEEE, 418–424.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:36

R. Forsati et al.

Rana Forsati, Hanieh Mohammadi Doustdar, Mehrnoush Shamsfard, Andisheh Keikha, and Mohammad
Reza Meybodi. 2013. A fuzzy co-clustering approach for hybrid recommender systems. Int. J. Hybrid
Intell. Syst. 10, 2 (2013), 71–81.
Rana Forsati and Mohammad Reza Meybodi. 2010. Effective page recommendation algorithms based on
distributed learning automata and weighted association rules. Expert Syst. Appl. 37, 2 (2010), 1316–
1330.
Jennifer Golbeck. 2005. Computing and applying trust in web-based social networks. Ph.D. Dissertation,
University of Maryland at College Park.
Jennifer Golbeck. 2006. Generating predictive movie recommendations from trust in social networks. In Proceedings of the 4th International Conference on Trust Management (iTrust). Lecture Notes in Computer
Science, Vol. 3986, Springer, Berlin, 93–104.
Jennifer Golbeck and James Hendler. 2006. Filmtrust: Movie recommendations using trust in web-based
social networks. In Proceedings of the IEEE Consumer Communications and Networking Conference,
Vol. 96. Citeseer.
Nathaniel Good, J. Ben Schafer, Joseph A. Konstan, Al Borchers, Badrul Sarwar, Jon Herlocker, and John
Riedl. 1999. Combining collaborative filtering with personal agents for better recommendations. In Proceedings of the 16th National Conference on Artificial Intelligence and the 11th Innovative Applications
of Artificial Intelligence Conference (AAAI/IAAI). 439–446.
Quanquan Gu, Jie Zhou, and Chris Ding. 2010. Collaborative filtering: Weighted nonnegative matrix factorization incorporating user and item graphs. In Proceedings of the SIAM International Conference on
Data Mining (SDM). 199–210.
R. Guha, Ravi Kumar, Prabhakar Raghavan, and Andrew Tomkins. 2004. Propagation of trust and distrust.
In Proceedings of the 13th International Conference on World Wide Web. ACM, 403–412.
Guibing Guo, Jie Zhang, Daniel Thalmann, Anirban Basu, and Neil Yorke-Smith. 2014. From ratings to
trust: An empirical study of implicit trust in recommender systems. In Proceedings of the 29th ACM
Symposiam on Applied Computing (SAC).
Jonathan L. Herlocker, Joseph A. Konstan, Al Borchers, and John Riedl. 1999. An algorithmic framework
for performing collaborative filtering. In Proceedings of the 22nd Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval. ACM, 230–237.
Jonathan L. Herlocker, Joseph A. Konstan, Loren G. Terveen, and John T. Riedl. 2004. Evaluating collaborative filtering recommender systems. ACM Trans. Inf. Syst. 22, 1 (2004), 5–53.
Thomas Hofmann. 2004. Latent semantic models for collaborative filtering. ACM Trans. Inf. Syst. 22, 1
(2004), 89–115.
Mohsen Jamali and Martin Ester. 2009. TrustWalker: A random walk model for combining trust-based
and item-based recommendation. In Proceedings of the 15th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. ACM, 397–406.
Mohsen Jamali and Martin Ester. 2010. A matrix factorization technique with trust propagation for recommendation in social networks. In Proceedings of the 4th ACM Conference on Recommender Systems.
ACM, 135–142.
Mohsen Jamali and Martin Ester. 2011. A transitivity aware matrix factorization model for recommendation
in social networks. In Proceedings of the 22nd International Joint Conference on Artificial Intelligence,
Vol. 3. AAAI Press, 2644–2649.
Arnd Kohrs and Bernard Merialdo. 1999. Clustering for collaborative filtering applications. In Computational
Intelligence for Modelling, Control & Automation. IOS Press.
Ioannis Konstas, Vassilios Stathopoulos, and Joemon M. Jose. 2009. On social networks and collaborative
recommendation. In Proceedings of the 32nd International ACM SIGIR Conference on Research and
Development in Information Retrieval. ACM, 195–202.
Yehuda Koren. 2008. Factorization meets the neighborhood: A multifaceted collaborative filtering model.
In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining. ACM, 426–434.
Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender
systems. Computer 42, 8 (2009), 30–37.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. 2010a. Predicting positive and negative links in
online social networks. In Proceedings of the 19th International Conference on World Wide Web. ACM,
641–650.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. 2010b. Signed networks in social media. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 1361–1370.
Wu-Jun Li and Dit-Yan Yeung. 2009. Relation regularized matrix factorization. In Proceedings of the 21st
International Conference on Artificial Intelligence (IJCAI’09).
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:37

Juntao Liu, Caihua Wu, and Wenyu Liu. 2013. Bayesian probabilistic matrix factorization with social relations and item contents for recommendation. Decision Support Syst. 55, 3 (June 2013), 838–850.
Hao Ma. 2013. An experimental study on implicit social recommendation. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 73–82.
Hao Ma, Irwin King, and Michael R. Lyu. 2009a. Learning to recommend with social trust ensemble.
In Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in
Information Retrieval. ACM, 203–210.
Hao Ma, Michael R. Lyu, and Irwin King. 2009b. Learning to recommend with trust and distrust relationships. In Proceedings of the 3rd ACM Conference on Recommender Systems. ACM, 189–196.
Hao Ma, Haixuan Yang, Michael R. Lyu, and Irwin King. 2008. SoRec: Social recommendation using probabilistic matrix factorization. In Proceedings of the 17th ACM Conference on Information and Knowledge
Management. ACM, 931–940.
Hao Ma, Dengyong Zhou, Chao Liu, Michael R. Lyu, and Irwin King. 2011a. Recommender systems with
social regularization. In Proceedings of the 4th ACM International Conference on Web Search and Data
Mining. ACM, 287–296.
Hao Ma, Tom Chao Zhou, Michael R. Lyu, and Irwin King. 2011b. Improving recommender systems by
incorporating social contextual information. ACM Trans. Inf. Syst. 29, 2 (2011), 9.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to Information
Retrieval. Vol. 1. Cambridge University Press, Cambridge.
Paolo Massa and Paolo Avesani. 2004. Trust-aware collaborative filtering for recommender systems. In On
the Move to Meaningful Internet Systems 2004: CoopIS, DOA, and ODBASE. Lecture Notes in Computer
Science, Vol. 3290. Springer, 492–508.
Paolo Massa and Paolo Avesani. 2005. Controversial users demand local trust metrics: An experimental study
on Epinions.com community. In Proceedings of the 20th National Conference on Artificial Intelligence
(AAAI’05). 121–126.
Paolo Massa and Paolo Avesani. 2009. Trust metrics in recommender systems. In Computing with Social
Trust. Human-Computer Intercation Series, Springer, 259–285.
Prem Melville, Raymond J. Mooney, and Ramadass Nagarajan. 2002. Content-boosted collaborative filtering
for improved recommendations. In Proceedings of the 18th National Conference on Artificial Intelligence
(AAAI). 187–192.
Bradley N. Miller, Joseph A. Konstan, and John Riedl. 2004. PocketLens: Toward a personal recommender
system. ACM Trans. Inf. Syst. (TOIS) 22, 3 (2004), 437–476.
Andriy Mnih and Ruslan Salakhutdinov. 2007. Probabilistic matrix factorization. In Proceedings of the 21st
Annual Conference on Neural Information Processing Systems (NIPS). 1257–1264.
Uma Nalluri. 2008. Utility of distrust in online recommender systems. Technical Report, Coopstone.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. 2009. Robust stochastic approximation approach to stochastic programming. SIAM J. Optim. 19, 4 (2009), 1574–1609.
Akshay Patil, Golnaz Ghasemiesfeh, Roozbeh Ebrahimi, and Jie Gao. 2013. Quantifying social influence in
epinions. Human 2, 2 (2013).
Dmitry Pavlov and David M. Pennock. 2002. A maximum entropy approach to collaborative filtering in
dynamic, sparse, high-dimensional domains. In Proceedings of the Annual Conference on Neural Information Processing System (NIPS), Vol. 2. 1441–1448.
Michael J. Pazzani. 1999. A framework for collaborative, content-based and demographic filtering. Artif.
Intell. Rev. 13, 5–6 (1999), 393–408.
David M. Pennock, Eric Horvitz, Steve Lawrence, and C. Lee Giles. 2000. Collaborative filtering by personality diagnosis: A hybrid memory-and model-based approach. In Proceedings of the 16th Conference on
Uncertainty in Artificial Intelligence. Morgan Kaufmann Publishers Inc., 473–480.
Jasson D. M. Rennie and Nathan Srebro. 2005. Fast maximum margin matrix factorization for collaborative
prediction. In Proceedings of the 22nd International Conference on Machine Learning. ACM, 713–719.
Ruslan Salakhutdinov and Andriy Mnih. 2008a. Bayesian probabilistic matrix factorization using Markov
chain Monte Carlo. In Proceedings of the 25th International Conference on Machine Learning. ACM,
880–887.
Ruslan Salakhutdinov and Andriy Mnih. 2008b. Probabilistic matrix factorization. In Proceedings of the
22nd Annual Conference on Neural Information Processing Systems. 1257–1264.
Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. 2007. Restricted Boltzmann machines for collaborative filtering. In Proceedings of the 24th International Conference on Machine Learning. ACM,
791–798.

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:38

R. Forsati et al.

Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item-based collaborative filtering
recommendation algorithms. In Proceedings of the 10th International Conference on World Wide Web.
ACM, 285–295.
Wanita Sherchan, Surya Nepal, and Cecile Paris. 2013. A survey of trust in social networks. ACM Comput.
Surv. 45, 4 (2013), 47.
Luo Si and Rong Jin. 2003. Flexible mixture model for collaborative filtering. In Proceedings of the 20th
International Conference on Machine Learning (ICML), Vol. 3. 704–711.
Ian Soboroff and Charles Nicholas. 1999. Combining content and collaboration in text filtering. In Proceedings
of the IJCAI Workshop on Machine Learning for Information Filtering, Vol. 99. 86–91.
Nathan Srebro and Tommi Jaakkola. 2003. Weighted low-rank approximations. In Proceedings of the 20th
International Conference on Machine Learning (ICML), Vol. 3. 720–727.
Nathan Srebro, Jason D. M. Rennie, and Tommi Jaakkola. 2005. Maximum-margin matrix factorization. In
Proceedings of the Conference on Neural Information Processing Systems (NIPS). 1329–1336.
Mojdeh Talabeigi, Rana Forsati, and Mohammad Reza Meybodi. 2010. A hybrid web recommender system
based on cellular learning automata. In Proceedings of the IEEE International Conference on Granular
Computing (GrC). IEEE, 453–458.
Nele Verbiest, Chris Cornelis, Patricia Victor, and Enrique Herrera-Viedma. 2012. Trust and distrust aggregation enhanced with path length incorporation. Fuzzy Sets Syst. 202 (2012), 61–74.
Patricia Victor, Chris Cornelis, Martine De Cock, and Ankur Teredesai. 2011b. Trust- and distrust-based
recommendations for controversial reviews. IEEE Intell. Syst. 26, 1 (2011), 48–55.
Patricia Victor, Chris Cornelis, and Martine De Cock. 2011a. Trust Networks for Recommender Systems.
Atlantis-Computational Intelligence Series, Vol. 4. Springer, Berlin.
Patricia Victor, Chris Cornelis, Martine De Cock, and Enrique Herrera-Viedma. 2011c. Practical aggregation
operators for gradual trust and distrust. Fuzzy Sets Syst. 184, 1 (2011), 126–147.
Patricia Victor, Nele Verbiest, Chris Cornelis, and Martine De Cock. 2013. Enhancing the trust-based recommendation process with explicit distrust. ACM Trans. Web 7, 2 (2013), 6.
Fei Wang, Sheng Ma, Liuzhong Yang, and Tao Li. 2006b. Recommendation on item graphs. In Proceedings
of the 6th International Conference on Data Mining (ICDM’06). IEEE, 1119–1123.
Jun Wang, Arjen P. De Vries, and Marcel J. T. Reinders. 2006a. Unifying user-based and item-based collaborative filtering approaches by similarity fusion. In Proceedings of the 29th Annual International ACM
SIGIR Conference on Research and Development in Information Retrieval. ACM, 501–508.
Grzegorz Wierzowiecki and Adam Wierzbicki. 2010. Efficient and correct trust propagation using closelook.
In Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent
Agent Technology (WI-IAT). Vol. 1. IEEE, 676–681.
Lei Wu, Steven C. H. Hoi, Rong Jin, Jianke Zhu, and Nenghai Yu. 2009. Distance metric learning from
uncertain side information with application to automated photo tagging. In Proceedings of the 17th
ACM International Conference on Multimedia. ACM, 135–144.
Gui-Rong Xue, Chenxi Lin, Qiang Yang, WenSi Xi, Hua-Jun Zeng, Yong Yu, and Zheng Chen. 2005. Scalable
collaborative filtering using cluster-based smoothing. In Proceedings of the 28th Annual International
ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 114–121.
Kai Yu, Anton Schwaighofer, Volker Tresp, Xiaowei Xu, and H.-P. Kriegel. 2004. Probabilistic memory-based
collaborative filtering. IEEE Trans. Knowl. Data Eng. 16, 1 (2004), 56–69.
Sheng Zhang, Weihong Wang, James Ford, and Fillia Makedon. 2006. Learning from incomplete ratings
using non-negative matrix factorization. In Proceedings of the 6th SIAM Conference on Data Mining
(SDM).
Yi Zhang and Jonathan Koren. 2007. Efficient bayesian hierarchical user modeling for recommendation
system. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval. ACM, 47–54.
Jianke Zhu, Hao Ma, Chun Chen, and Jiajun Bu. 2011. Social recommendation using low-rank semidefinite
program. In Proceedings of the 25th AAAI Conference on Artificial Intelligence.
Cai-Nicolas Ziegler. 2009. On propagating interpersonal trust in social networks. In Computing with Social
Trust Human-Computer Interaction Series, Springer, Berlin, 133–168.
Cai-Nicolas Ziegler and Jennifer Golbeck. 2007. Investigating interactions of trust and interest similarity.
Decision Support Syst. 43, 2 (2007), 460–475.
Cai-Nicolas Ziegler and Georg Lausen. 2005. Propagation models for trust and distrust in social networks.
Inf. Syst. Frontiers 7, 4–5 (2005), 337–358.
Received November 2013; revised April, June 2014; accepted June 2014
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Geoinformatica (2013) 17:417448 DOI 10.1007/s10707-012-0164-9

Generic and efficient framework for search trees on flash memory storage systems
Mohamed Sarwat  Mohamed F. Mokbel  Xun Zhou  Suman Nath

Received: 16 February 2012 / Revised: 27 June 2012 / Accepted: 12 July 2012 / Published online: 30 August 2012  Springer Science+Business Media, LLC 2012

Abstract Tree index structures are crucial components in data management systems. Existing tree index structure are designed with the implicit assumption that the underlying external memory storage is the conventional magnetic hard disk drives. This assumption is going to be invalid soon, as flash memory storage is increasingly adopted as the main storage media in mobile devices, digital cameras, embedded sensors, and notebooks. Though it is direct and simple to port existing tree index structures on the flash memory storage, that direct approach does not consider the unique characteristics of flash memory, i.e., slow write operations, and erase-beforeupdate property, which would result in a sub optimal performance. In this paper, we introduce FAST (i.e., Flash-Aware Search Trees) as a generic framework for flashaware tree index structures. FAST distinguishes itself from all previous attempts of flash memory indexing in two aspects: (1) FAST is a generic framework that can be applied to a wide class of data partitioning tree structures including R-tree and its variants, and (2) FAST achieves both ef f iciency and durability of read and write flash operations through memory flushing and crash recovery techniques. Extensive experimental results, based on an actual implementation of FAST inside the GiST

The research of M. Sarwat and M. F. Mokbel is supported in part by the National Science Foundation under Grants IIS-0811998, IIS-0811935, CNS-0708604, IIS-0952977, by a Microsoft Research Gift, and by a seed grant from UMN DTC. M. Sarwat ( )  M. F. Mokbel  X. Zhou Department of Computer Science and Engineering, University of Minnesota - Twin Cities, 200 SE Union Street, Minneapolis, MN 55455, USA e-mail: sarwat@cs.umn.edu M. F. Mokbel e-mail: mokbel@cs.umn.edu X. Zhou e-mail: xun@cs.umn.edu S. Nath Microsoft Research, One Microsoft Way - Redmond, Redmond, WA 98052, USA e-mail: sumann@microsoft.com

418

Geoinformatica (2013) 17:417448

index structure in PostgreSQL, show that FAST achieves better performance than its competitors. Keywords Flash memory  Tree  Spatial  Index structure  Storage  Multi-dimensional  Data  System

1 Introduction Data partitioning tree index structures are crucial components in spatial data management systems, as they are mainly used for efficient spatial data retrieval, hence boosting up query performance. The most common examples of such index structures include B-tree [4], with its variants [10, 27], for one-dimensional indexing, and R-tree [14], with its variants [5, 17, 32, 34], for multi-dimensional indexing. Data partitioning tree index structures are designed with the implicit assumption that the underlying external memory storage is the conventional magnetic hard disk drives, and thus has to account for the mechanical disk movement and its seek and rotational delay costs. This assumption is going to be invalid soon, as flash memory storage is expected to soon prevail in the storage market replacing the magnetic hard disks for many applications [11, 12, 31]. Flash memory storage is increasingly adopted as the main storage media in mobile devices and as a storage alternative in laptops, desktops, and enterprise class servers (e.g., in forms of SSDs) [3, 21, 23, 28, 33]. Recently, several data-intensive applications have started using custom flash cards (e.g., ReMix [19]) with large capacity and access to underlying raw flash chips. Such a popularity of flash is mainly due to its superior characteristics that include smaller size, lighter weight, lower power consumption, shock resistance, lower noise, and faster read performance [16, 18, 20, 22, 29]. Flash memory is block-oriented, i.e., pages are clustered into a set of blocks. Thus, it has fundamentally different characteristics, compared to the conventional pageoriented magnetic disks, especially for the write operations. First, write operations in flash are slower than read operations. Second, random writes are substantially slower than sequential writes. In devices that allow direct access to flash chips (e.g., ReMix [19]), a random write operation updates the contents of an already written part of the block, which requires an expensive block erase operation,1 followed by a sequential write operation on the erased block; an operation termed as erase-before-update [7, 20]. SSDs, which emulate a disk-like interface with a Flash Translation Layer (FTL), also need to internally address flash's erase-before-update property with logging and garbage collection, and hence random writes, especially small random writes, are significantly slower than sequential writes in almost all SSDs [7]. Though it is direct and simple to port existing tree index structures (e.g., R-tree and B-tree) on FTL-equipped flash devices (e.g., SSDs), that direct approach does not consider the unique characteristics of flash memory and therefore would result in a sub-optimal performance due to the random writes encountered by these index structures. To remedy this situation, several approaches have been proposed for

1 In

a typical flash memory, the cost of read, write, and erase operations are 25, 200 and 1,500 s, respectively [3].

Geoinformatica (2013) 17:417448

419

flash-aware index structures that either focus on a specific index structure, and make it a flash-aware, e.g., flash-aware B-tree [30, 36] and R-tree [35], or design brand new index structures specific to the flash storage [2, 24, 25]. Unfortunately, previous works on flash-aware search trees suffer from two major limitations. First, these trees are specialized--they are not flexible enough to support new data types or new ways of partitioning and searching data. For example, FlashDB [30], which is designed to use a B-Tree, does not support R-Tree functionalities. RFTL [35] is designed to work with R-tree, and does not support B-tree functionalities. Thus, if a system needs to support many applications with diverse data partitioning and searching requirements, it needs to have multiple tree data structures. The effort required to implement and maintain multiple such data structures is high. Second, existing flash-aware designs often show trade-offs between efficiency and durability. Many designs sacrifice strict durability guarantee to achieve efficiency [24, 25, 30, 35, 36]. They buffer updates in memory and flush them in batches to amortize the cost of random writes. Such buffering poses the risk that in-memory updates may be lost if the system crashes. On the other hand, several designs achieve strict durability by writing (in a sequential log) all updates to flash [2]. However, this increases the cost of search for many log entries that need to be read from flash in order to access each tree node [30]. In summary, no existing flash-aware tree structure achieves both strict durability and efficiency. In this paper, we address the above two limitations by introducing FAST; a framework for Flash-Aware Search Tree index structures. FAST distinguishes itself from all previous flash-aware approaches in two main aspects: (1) Rather than focusing on a specific index structure or building a new index structure, FAST is a generic framework that can be applied to a wide variety of tree index structures, including B-tree, R-tree along with their variants. Such an important property makes FAST a very attractive solution to database industry as it is practical to port it inside the database engine with minimal disturbance to the engine code. (2) FAST achieves both efficiency and durability in the same design. For efficiency, FAST buffers all the incoming updates in memory while employing an intelligent f lushing policy that evicts selected updates from memory to minimize the cost of writing to the flash storage. In the mean time, FAST guarantees durability by sequentially logging each in-memory update and by employing an efficient crash recovery technique. FAST mainly has four modules, update, search, f lushing, and recovery. The update module is responsible on buffering incoming tree updates in an in-memory data structure, while writing small entries sequentially in a designated flash-resident log file. The search module retrieves requested data from the flash storage and updates it with recent updates stored in memory, if any. The f lushing module is triggered once the memory is full and is responsible on evicting flash blocks from memory to the flash storage to give space for incoming updates. Finally, the recovery module ensures the durability of in-memory updates in case of a system crash. FAST is a generic system approach that neither changes the structure of tree indexes it is applied to, nor changes the search, insert, delete, or update algorithms of these indexes. FAST only changes the way these algorithms reads, or updates the tree nodes in order to make the index structure flash-aware. We have implemented FAST within the GiST framework [15] inside PostgrSQL. As GiST is a generalized index structure, FAST can support any tree index structure that GiST is supporting,

420

Geoinformatica (2013) 17:417448

including one-dimensional tree index structures (e.g., B-tree [4]) and including but not restricted to R-tree [14], R*-tree [5], SS-tree [34], and SR-tree [17], as well as Btree and its variants. In summary, the contributions of this paper can be summarized as follows:   We introduce FAST; a general framework that adapts existing tree index structures to consider and exploit the unique properties of the flash memory storage. We show how to achieve efficiency and durability in the same design. For efficiency, we introduce two f lushing policies that smartly select parts of the main memory buffer to be flushed into the flash storage in a way that amortizes expensive random write operations. We also introduce a crash recovery technique that ensures the durability of update transactions in case of system crash. We give experimental evidence for generality, efficiency, and durability of FAST framework when applied to different data partitioning tree index structures.



The rest of the paper is organized as follows: An overview of Flash Memory storage is given in Section 2. Section 3 highlights related work to FAST. Section 4 gives an overview of FAST along with its data structure. The four modules of FAST, namely, update, search, f lushing, and recovery are discussed in Sections 5 8, respectively. Section 9 gives experimental results. Finally, Section 10 concludes the paper.

2 Flash memory storage overview Figure 1 gives an overview of a typical flash memory storage device. In flash memory, data is stored in an array of flash blocks. Each block contains 64128 pages, where a page is the smallest unit of access. Flash memory supports three types of operations: read, write, and erase. The Erase operation is the most expensive one where it can only be done at the block level and results in setting of all bits within a block to ones. Read is a low latency page level operation and can occur randomly anywhere in the flash memory without incurring any additional cost. Write is also a page level operation and can be performed only once a page has been previously erased since it sets the required bits to zeros. In that sense, writing on a previously erased block is a low latency operation, and termed as a sequential write, while writing on an already written block will result in a block erase operation before the actual write operation, and thus, would incur much higher cost. Current generation flash memory-based storage devices have varying access latencies for each of these operations. On average, compared to read operations, write operations are eight times slower, while erase operations are 60 times slower [6]. The typical access latencies for read, write, and erase operations in flash memory devices are 25, 200 and 1,500 s, respectively [3]. The Flash Translation layer (FTL) [26] is a layer on top of NAND flash memory that makes the flash memory device acts like a virtual disk. The FTL layer receives read and write commands for logical pages addresses from the application layer and converts them to the internal flash memory commands (i.e., read page, write page, erase block) on physical pages/blocks addresses. To emulate disk like in-place update operation for a logical page Plogical , the FTL writes data into a new physical page Pphysical, maintains a mapping between logical pages and physical pages, and marks

Geoinformatica (2013) 17:417448 Fig. 1 Flash memory storage. Grey rectangles represent pages that are contained in blocks represented by dotted rectangles
Logical Page Read Logical Page Write

421

Flash Translation Layer (FTL)
Page Read Block Erase Page Write

NAND Flash Memory

the previous physical location of Pphysical as invalid for future garbage collection. Even though FTL allows existing disk based applications to use flash memory without any modications, it needs to internally deal with flash physical constraint of erasing a block before updating a page in that block. Besides this asymmetric read and write latency issue, a flash memory block can only be erased for a limited number of times (e.g., 105 106 ), after which it acts like a read-only device [3]. FTL employs various wear-leveling techniques to even out the erase counts of different blocks in the flash memory to increase its longevity [8]. However, still early wear-out of flash memory is one of the big concerns in widely deploying flash memory storage devices [31], and thus, it is of essence that flash memory avoids block erases as much as possible. Recent studies show that current FTL schemes are very effective for the workloads with sequential access write patterns. However, for the workloads with random access patterns, these schemes show very poor performance [9]

3 Related work Previous approaches for flash-aware index structures can be classified into two categories: (1) Making an existing specif ic index structure flash-aware, which includes flash-aware B-tree (e.g., FlashDB [30] and BFTL [36]) and flash-aware R-tree (e.g., RFTL [35]). The main idea of these index structures is to save the B-tree (R-tree) operations in a reservation buffer residing on main memory. When the reservation buffer is full, its content is totally flushed to flash memory. For instance, BFTL and RFTL are adding a buffering layer on top of the flash translation layer in order to make B-trees work efficiently on flash devices. (2) Designing brand new onedimensional index structures specific to the flash storage, e.g., the LA-tree [2] and the FD-tree [24, 25]. LA-tree is flash friendly index structure that is intended to replace the B-tree. LA-tree stores the updates in cascaded buffers residing on flash memory and, then empties these buffers dynamically based on the operations workload. FDtree is also a one-dimensional index structure that allows small random writes to

422

Geoinformatica (2013) 17:417448

occur only in a small portion of the tree called the head tree which exists at the top level of the tree. When the capacity of the head tree is exceeded, its entries are merged in batches to subsequent tree levels. In terms of the performance-durability trade-off, previous approaches either: (a) achieve efficiency, yet sacrifice durability, by buffering updates in main memory and flush them in batches to flash memory to amortize the cost of random writes [24, 25, 30, 35, 36]. However, storing updates in memory without taking into account system failures, which leads to durability issue, where in-memory updates may be lost if the system crashes, or (b) achieve durability, yet sacrifice efficiency, by writing all the recent updates in a sequential log file [2], hence retrieving the updates from the log file in case of a system crash. However, doing this increases the cost of search for many log entries that need to be read from flash in order to access each tree node with search and update operations [30]. FAST distinguishes itself from all previous techniques in three main aspects: (1) FAST is a general framework for data-partitioning tree index structures built inside GiST [15]. As GiST is a generalized index structure that can instantiate a wide set of data-partitioning trees that include B-tree [4], R-tree [14], R*-tree [5], SS-tree [34], and SR-tree [17]), FAST can support any tree that GiST is supporting. (2) FAST ensures both the ef f iciency and durability of system transactions where updates are buffered in memory, yet, an efficient crash recovery technique is triggered in case of a system crash to ensure the durability. (3) FAST is not a brand new index structure, hence does not need to replace existing tree indexes. However, it complements the existing tree index structures in database management systems to make them work efficiently on flash storage devices, with much less implementation cost.

4 Fast system overview Figure 2 gives an overview of FAST. The original tree is stored on persistent flash memory storage while recent updates are stored in an in-memory buffer. Both parts need to be combined together to get the most recent version of the tree structure. FAST has four main modules, depicted in bold rectangles, namely, update, search, flushing, and crash recovery. FAST is optimized for both SSDs and raw flash devices. SSDs are the dominant flash device for large database applications. On the other hand, raw flash chips, which are dominant in embedded systems and custom flash cards (e.g., ReMix [19]), are getting popular for data-intensive applications. 4.1 FAST modules In this section, we explain FAST system architecture, along with its four main modules; (1) Update, (2) Search, (3) Flushing, and (4) Crash recovery. The actions of these four modules are triggered through three main events, namely, search queries, data updates, and system restart. Update module Similar to some of the previous research for indexing in flash memory, FAST buffers its recent updates in memory, and flushes them later, in bulk, to the persistent flash storage. However, FAST update module distinguishes itself from previous research in two main aspects: (1) FAST does not store the

Geoinformatica (2013) 17:417448

423

Fig. 2 FAST system architecture

update operations in memory, instead, it stores the results of the update operations in memory, and (2) FAST ensures the durability of update operations by writing small log entries to the persistent storage. These log entries are written sequentially to the flash storage, i.e., very small overhead. Details of the update module will be discussed in Section 5. Search module The search module in FAST answers point and range queries that can be imposed to the underlying tree structure. The main challenge in the search module is that the actual tree structure is split between the flash storage and the memory. Thus, the main responsibility of the search module is to construct the recent image of the tree by integrating the stored tree in flash with the tree updates in memory that did not make it to the flash storage yet. Details of the search module will be discussed in Section 6. Flushing module As the memory resource is limited, it will be filled up with the recent tree updates. In this case, FAST triggers its flushing module that employs a f lushing policy to select some of the in-memory updates and write them, in bulk, into the flash storage. Previous research in flash indexing flush their in-memory updates or log file entries by writing all the memory or log updates once to the flash storage. In contrast, the flushing module in FAST distinguishes itself from previous techniques in two main aspects: (1) FAST employs f lushing policies that smartly selects some of the updates from memory to be flushed to the flash storage in a way that amortizes the expensive cost of the block erase operation over a large set of random write operations, and (2) FAST logs the flushing process using a single log entry written sequentially on the flash storage. Details of the flushing module will be discussed in Section 7. Crash recovery module FAST employs a crash recovery module to ensure the durability of update operations. This is a crucial module in FAST, as only because of this module, we are able to have our updates in memory, and not to worry about any data losses. This is in contrast to previous research in flash indexing that may encounter data losses in case of system crash, e.g., [24, 25, 35, 36]. The crash recovery

424

Geoinformatica (2013) 17:417448

module is mainly responsible on two operations: (1) Once the system restarts after crash, the crash recovery module utilizes the log file entries, written by both the update and flushing modules, to reconstruct the state of the flash storage and inmemory updates just before the crash took place, and (2) maintaining the size of the log file within the allowed limit. As the log space is limited, FAST needs to periodically compact the log entries. Details of this module will be discussed in Section 8. 4.2 FAST design goals FAST avoids the tradeoff of durability and efficiency by using a combination of buffering and logging. Unlike existing efficient-but-not-durable designs [24, 25, 30, 35, 36], FAST uses write-ahead-logging and crash recovery to ensure strict system durability. FAST makes tree updates efficient by buffering write operations in main memory and by employing an intelligent flushing policy that optimizes I/O costs for both SSDs and raw flash devices. Unlike existing durable-but-inefficient solutions [2], FAST does not require reading in-flash log entries for each search/update operation, which makes reading FAST trees efficient. 4.3 FAST data structure Other than the underlying index tree structure stored in the flash memory storage, FAST maintains two main data structures, namely, the Tree Modif ications Table, and Log File, described below. Tree modif ications table This is an in-memory hash table (depicted in Fig. 3) that keeps track of recent tree updates that did not make it to the flash storage yet. Assuming no hashing collisions, each entry in the hash table represents the modification applied to a unique node identifier, and has the form (status, list) where status is either NEW, DEL, or MOD to indicate if this node is newly created, deleted, or just modified, respectively, while list is a pointer to a new node, null, or a list of node modifications based on whether the status is NEW, DEL, or MOD, respectively. For MOD case, each modification in the list is presented by the quadruple (TimeStamp, type, index, value) where TimeStamp represents the time at

Fig. 3 Tree modifications table

Geoinformatica (2013) 17:417448

425

which the update happened, type is either K , P F , or P M , to indicate if the modified entry is the key, a pointer to a flash node, or a pointer to an in-memory node, respectively, while index and value determines the index and the new value for the modified node entry, respectively. In Fig. 3, there are two modifications in nodes A and D, one modification in nodes B and F , while node G is newly created and node H is deleted. Log f ile This is a set of flash memory blocks, reserved for recovery purposes. A log file includes short logs, written sequentially, about insert, delete, update, and flushing operations. Each log entry includes the triple (operation, node_list, modif ication) where operation indicates the type of this log entry as either insert, delete, update, or flush, node_list includes the list of affected nodes by this operation in case of a flush operation, or the only affected node, otherwise, modif ication is similar to the triple (type, index, value), used in the tree modif ications table. All log entries are written sequentially to the flash storage, which has a much lower cost than random writes that call for the erase operation. 4.4 Running example Throughout the rest of this paper, we will use Fig. 4 as a running example where six objects O1 to O6 , depicted by small black circles, are indexed by an R-tree. Then, two objects O7 and O8 , depicted by small white circles, are to be inserted in the same Rtree. Figure 4a depicts the eight objects in the two-dimensional space domain, while Fig. 4b gives the flash-resident R-tree with only the six objects that made it to the

10
O2

O1 O8

8
O3 O5 O4 O7

6 4 2

O6

0

2

4

6

8

10

12

14

(a) 2D Space
A
C Mod Mod Mod Mod 3, K, 2, (5,10,8,7) 1, K, 2, (12,4,14,2)

2, K, 2, O7

B

C

G B

D O1 O2

E O3

F O4 O5 O6

G

D

4, K, 2, O8

(b) R-tree Index

(c) Tree Modifications Table

Fig. 4 Illustrating example for search and update operations in FAST

426 Table 1 Cost analysis parameters Parameter T RM WM RF WF EF Definition

Geoinformatica (2013) 17:417448

The underlying tree index structure to which FAST has been applied The average time to read a node update entry from the tree modifications table The average time to write a node update entry to the tree modifications table The average time to read a tree node from the underlying tree T residing on flash memory The average time to write a tree node to the underlying tree T residing on flash memory The average time to erase a whole block on the flash memory device

flash memory. Finally, Fig. 4c gives the in-memory buffer (tree modif ications table) upon the insertion of O7 and O8 in the tree. 4.5 Operations cost parameters For each FAST module, we analyze the cost model of its main operations, including search, update, flushing, crash recovery and log compaction. To this end, we define the parameters given in Table 1.

5 Tree updates in FAST This section discusses the update operations in FAST, which include inserting a new entry and deleting/updating an existing entry. An update operation to any tree in FAST may result in creating new tree nodes as in the case of splitting operations (i.e., when inserting an element in the tree leads to node overflow), deleting existing tree nodes as in the case of merging operations (i.e., when deleting an element from the tree leads to node underflow), or just modifying existing node keys and/or pointers. Main idea For any update operation (i.e., insert, delete, update) that needs to be applied to the index tree, FAST does not change the underlying insert, delete, or update algorithm for the tree structure it represents. Instead, FAST runs the underlying update algorithm for the tree it represents, with the only exception of writing any changes caused by the update operation in memory instead of the external storage, to be flushed later to the flash storage, and logging the result of the update operation. A main distinguishing characteristic of FAST is that what is buffered in memory, and also written in the log file, is the result of the update operation, not a log of this operation. Algorithm Algorithm 1 gives the pseudo code of inserting an object Obj in FAST. The algorithms for deleting and updating objects are similar in spirit to the insertion algorithm, and thus are omitted from the paper. The algorithm mainly has two steps: (1) Executing the insertion in memory (Line 2 in Algorithm 1). This is basically done by calling the insertion procedure of the underlying tree, e.g., R-tree insertion, with two main differences. First, the insertion operation calls the search operation, discussed later in Section 6, to find where we need to insert our data based on the most recent version of the tree, constructed from main memory recent updates and

Geoinformatica (2013) 17:417448

427

Algorithm 1 Insert an Object in the Tree 1: Function Insert(Obj) /* STEP 1: Executing the Insertion in Memory only */ 2: L  List of modified nodes from the in-memory execution of inserting Obj in the underlying tree /* STEP 2: Buf fering and Logging the Updates */ 3: for each Node N in L do 4: HashEntry  N entry in the Tree Modif ications Table 5: if HashEntry is not NULL then 6: Add the triple (MOD, N , updates in N ) to the log file 7: if the status of HashEntry is MOD then 8: Add the changes in N to the list of changes of HashEntry 9: else 10: Apply the changes in N to the new node of HashEntry 11: end if 12: else 13: HashEntry  Create a new entry for N in the Tree Modif ications Table 14: if N is a newly created node then 15: Add the triple (NEW, N , updates in N ) to the log file 16: Set HashEntry status to NEW, and its pointer to N 17: else 18: Add the triple (MOD, N , updates in N ) to the log file 19: Set HashEntry status to MOD, and its pointer to the list of changes that took place in N 20: end if 21: end if 22: end for

the in-flash tree index structure. Second, the modified or newly created nodes that result back from the insertion operation are not written back to the flash storage, instead, they will be returned to the algorithm in a list L. Notice that the insertion procedure may result in creating new nodes if it encounters a split operation. (2) Buf fering and logging the tree updates (Lines 322 in Algorithm 1). For each modified node N in the list L, we check if there is an entry for N in our in-memory buffer, tree modif ications table. If this is the case, we first add a corresponding log entry that records the changes that took place in N . Then, we either add the changes in N to the list of changes in its entry in the tree modif ications table if this entry status is MOD, or update N entry in the tree modif ications table, if the entry status is NEW. On the other hand, if there is no entry for N in the tree modif ications table, we create such entry, add it to the log file, and fill it according to whether N is a newly created node or a modified one. Example In our running example of Fig. 4, inserting O7 results in modifying two nodes, G and C. Node G needs to have an extra key to hold O7 while node C needs to modify its minimum bounding rectangle that points to G to accommodate its size change. The changes in both nodes are stored in the tree modif ications table depicted

428 Fig. 5 FAST logging and recovery example

Geoinformatica (2013) 17:417448

Log# 1 2 3 4 5

Operation MOD MOD MOD MOD FLUSH

Node C G B D B, C, D

Modification 1, K, 2, (12,4,14,2) 2, K, 2, O7 3, K, 2, (5,10,8,7) 4, K,2, O8 *

(a) FAST Log File
Log# 1 Operation MOD Node G Modification 2, K, 2, O7

(a) FAST Log File after Crash Recovery

in Fig. 4c. The log entries for this operation are depicted in the first two entries of the log file of Fig. 5a. Similarly, inserting O8 results in modifying nodes, D and B Cost analysis For a given update operation U applied to a tree index structure T , let yi,U  {0, 1} represent whether or not node i of T has been modified by U . Let N be the total number of nodes in T at the time U is applied, then the total cost CU of update operation U applied on T is as follows:
N

CU = C Q +
i=0

yi,U  [W F + W M + L]

(1)

The update operation (e.g., insert, delete, modify) requires first a search query Q for a proper leaf node in T . This also takes the same search time C Q as illustrated above. For each updated node i due to applying U , yi,U = 1, and for each of these updates we write a sequential log entry to the log file that each takes W F time. Hence, the total time to write all log entries is equal to iN =0 yi,U  W F . For each updated node i, we also perform a lookup on the tree modifications table to get the entry for node i, which is performed in constant time L. In addition, the total time to write the modifications to all nodes (for which yi,U = 1) in the tree modifications table is N i=0 yi,U  W M . All of the above sums up to give the update cost given in Eq. 1 6 Searching in FAST Given a query Q, the search operation returns those objects indexed by FAST and satisfy Q. The search query Q could be a point query that searches for objects with a specific (point) value, or a range query that searches for objects within a specific range. An important promise of FAST is that it does not change the main search algorithm for any tree it represents. Instead, FAST complements the underlying searching algorithm to consider the latest tree updates stored in memory. Main idea As it is the case for any index tree, the search algorithm starts by fetching the root node from the secondary storage, unless it is already buffered in memory. Then, based on the entries in the root, we find out which tree pointer to follow to

Geoinformatica (2013) 17:417448

429

fetch another node from the next level. The algorithm goes on recursively by fetching nodes from the secondary storage and traversing the tree structure till we either find a node that includes the objects we are searching for or conclude that there are no objects that satisfy the search query. The challenging part here is that the retrieved nodes from the flash storage do not include the recent in-memory stored updates. FAST complements this search algorithm to apply the recent tree updates to each retrieved node from the flash storage. In particular, for each visited node, FAST constructs the latest version of the node by merging the retrieved version from the flash storage with the recent in-memory updates for that node. Algorithm Algorithm 2 gives the pseudo code of the search operation in FAST. The algorithm takes two input parameters, the query Q, which might be a point or range query, and a pointer to the root node R of the tree we want to search in. The output of the algorithm is the list of objects that satisfy the input query Q. Starting from the root node and for each visited node R in the tree, the algorithm mainly goes through two main steps: (1) Constructing the most recent version of R (Line 2 in Algorithm 2). This is mainly to integrate the latest flash-residant version of R with its in-memory stored updates. Algorithm 3 gives the detailed pseudo code for this

Algorithm 2 Searching for an Object indexed by the Tree 1: Function Search(Query Q, Tree Node R) /* STEP 1: Constructing the most recent version of R */ 2: N  RetrieveNode(R) /* STEP 2: Recursive search calls */ 3: if N is non-leaf node then 4: Check each entry E in N . If E satisfies the query Q, invoke Search(Q, E.NodePointer) for the subtree below E 5: else 6: Check each entry E in N . If E satisfies the search query Q, return the object to which E is pointing 7: end if

Algorithm 3 Retrieving a tree node 1: Function RetrieveNode(Tree Node R) 2: FlashNode  Retrieve node R from the flash-resident index tree 3: HashEntry  R's entry in the Tree Modif ications Table 4: if HashEntry is NULL then 5: return FlashNode 6: end if 7: if the status of HashEntry is MOD then 8: FlashNode  FlashNode  All the updates in HashEntry list 9: return FlashNode 10: end if /* We are trying to retrieve either a new or a deleted node */ 11: return the node that HashEntry is pointing to

430

Geoinformatica (2013) 17:417448

step, where initially, we read R from the flash storage. Then, we check if there is an entry for R in the tree modif ications table. If this is not the case, then we know that the version we have read from the flash storage is up-to-date, and we just return it back as the most recent version. On the other hand, if R has an entry in the tree modif ications table, we either apply the changes stored in this entry to R in case the entry status is MOD, or just return the node that this entry is pointing to instead of R. This return value could be null in case the entry status is DEL. (2) Recursive search calls (Lines 37 in Algorithm 2). This step is typical in any tree search algorithm, and it is basically inherited from the underlying tree that FAST is representing. The idea is to check if R is a leaf node or not. If R is a non-leaf node, we will check each entry E in the node. If E satisfies the search query Q, we recursively search in the subtree below E. On the other hand, if R is a leaf node, we will also check each entry E in the node, yet if E satisfies the search query Q, we will return the object to which E is pointing to as an answer to the query. Example Given the range query Q in Fig. 4a, FAST search algorithm will first fetch the root node A stored in flash memory. As there is no entry for A in the tree modif ications table (Fig. 4c), then the version of A stored in flash memory is the most recent one. Then, node C is the next node to be fetched from flash memory by the searching algorithm. As the tree modif ications table has an entry for C with status MOD, the modifications listed in the tree modif ications table for C will be applied to the version of C read from the flash storage. Similarly, the search algorithm will construct the leaf nodes F and G by first fetching them from flash memory, and then reading their recent updates from the tree modif ications table. Finally, the result of this query is { O4 , O5 , O6 , O7 }. Cost analysis For a given search query Q applied to a tree index structure T , let xi, Q  {0, 1} represent whether node i of T is visited or not when issuing query Q. Let Mi, Q be the number of modifications applied to node i and buffered in the tree modifications table at the time Q is issued. Let N be the total number of nodes in T at the time Q is issued, then the total search cost C Q on T is as follows:
N

CQ =
i=0

xi, Q  [ R F + ( Mi, Q  R M ) + L]

(2)

Assuming a range query, the search operation returns a number of objects within the query range. In FAST, when reading a node i from the flash-resident R-tree, we also need to accommodate all the corresponding modifications on i that have been recorded in the tree modifications table. Then, the total cost of reading a node i would thus be ( R F + Tm ) where Tm is the in-memory processing time for each node. For the in-memory processing part, it first takes constant time L to locate the node in the tree modification table , and then takes a linear scan of the list to apply all the modifications. Given that the number of modifications associated with each node is Mi, Q , then Tm = ( Mi, Q  R M ) + L, where Mi, Q is upper bounded by the memory size.

Geoinformatica (2013) 17:417448

431

7 Memory flushing in FAST As discussed in Section 5, the effect of all incoming updates in FAST has to be buffered in memory. As memory is a scarce resource, it will eventually be filled up with incoming updates. In that case, FAST triggers its flushing module, equipped with a f lushing policy, to free some memory space by evicting a selected part of the memory, termed a f lushing unit, to the flash storage. Such flushing is done in a way that amortizes the cost of expensive random write operations over a high number of update operations. In this section, we first define the flushing unit. Then, we discuss the flushing policy used in FAST. Finally, we explain the FAST flushing algorithm. The motivation of having a f lushing policy that flushes only part of the memory is twofold: (1) Clearing the whole memory at once will cause a significant pause to the system due to the need of erasing all the flash blocks that include at least one update record in memory. As a result, it is better to consider clearing only part of the memory in a way that does not really pause the system. In this paper, we present two main flushing policies employed by the system, and we empirically evaluate both of them, (2) Considering that we need to flush only part of the memory, it is crucial to select that part in a way that reduces the overhead of the block erase operation. 7.1 Flushing unit An important design parameter, in FAST, is the size of a f lushing unit, the granularity of consecutive memory space written in the flash storage during each flush operation. Our goal is to find a suitable f lushing unit size that minimizes the average cost of flushing an update operation to the flash storage, denoted as C. The value of C av erage writing cost depends on two factors: C1 = numb ; the average cost per bytes written, er of written b ytes

er of written b ytes ; the number of bytes written per update. This gives C = and C2 = numb numb er of updates C1  C2 . Interestingly, the values of C1 and C2 show opposite behaviors with the increase of the f lushing unit size. First consider C1 . On raw flash devices (e.g., ReMix [19]), for a f lushing unit smaller than a flash block, C1 decreases with the increase of the flushing unit size (see [29] for more detail experiments). This is intuitive, since with a larger f lushing unit, the cost of erasing a block is amortized over more bytes in the flushing unit. The same is also true for SSDs since small random writes introduce large garbage collection overheads, while large random writes approach the performance of sequential writes. Previous work has shown that, on several SSDs including the ones from Samsung, MTron, and Transcend, random write latency per byte increases by 32 when the write size is reduced from 16 KB to 0.5 KB [7]. Even on newer generation SSDs from Intel, we observed an increase of 4 in a similar experimental setup. This suggests that a flushing unit should not be very small, as that would result in a large value of C1 . On the other hand, the value of C2 increases with increasing the size of the f lushing unit. Due to non-uniform updates of tree nodes, a large flushing unit is unlikely to have as dense updates as a small flushing unit. Thus, the larger a f lushing unit is, the less the number of updates per byte is (i.e., the higher the value of C2 is). Another disadvantage of large f lushing unit is that it may cause a significant pause to the system. All these suggest that the f lushing unit should not be very large.

432

Geoinformatica (2013) 17:417448

Deciding the optimal size of a f lushing unit requires finding a sweet spot between the competing costs of C1 and C2 . Our experiments show that for raw flash devices, a f lushing unit of one flash block minimizes the overall cost. For SSDs, a f lushing unit of size 16 KB is a good choice, as it gives a good balance between the values of C1 and C2 . Note that a flushing unit size of 16 KB also matches the optimal size of a tree node, as suggested by Gray et al. [13]. Thus, with a tree of this optimal node size of 16 KB, we can simply flush one node at a time from the memory. 7.2 Flushing policies FAST is designed so that different flushing policies can be plugged in to the system. In the rest of this section, we discuss two main flushing policies adopted by FAST: (1) FAST Flushing Policy, and (2) FAST* Flushing Policy. 7.2.1 FAST f lushing policy The main idea of FAST f lushing policy is to minimize the average cost of writing each update to the underlying flash storage. To that end, FAST flushing policy aims to flush the in-memory tree updates that belong to the f lushing unit that has the highest number of in-memory updates. In that case, the cost of writing the f lushing unit will be amortized among the highest possible number of updates. Moreover, since the maximum number of updates are being flushed out, this frees up the maximum amount of memory used by buffered updates. Finally, as done in the update operations, the flushing operation is logged in the log file to ensure the durability of system transactions. Data structure The flushing policy maintains an in-memory max heap structure, termed FlushHeap, of all f lushing units that have at least one in-memory tree update. The max heap is ordered on the number of in-memory updates for each f lushing unit, and is updated with each incoming tree update. Updates in max heap is O(n), where n is the number of flash blocks with in-memory updates. In the mean time, retrieving the flushing unit with maximum number of updates is an O(1) operation. 7.2.2 FAST* f lushing policy The FAST* f lushing policy is an enhancement over the FAST flushing policy described in Section 7.2.1. FAST* flushing policy takes into account two parameters that helps in deciding which unit must be flushed: (1) Number of updates per flushing unit: It is the same parameter used by the FAST flushing policy explained in Section 7.2.1; which favors the flash unit that has the highest number of updates, and (2) Time stamp of the flushing unit: which represents the last time a flash block has been updated. When deciding which unit needs to be flushed, that parameter gives higher priority to the flushing unit that has the lowest time stamp (i.e., least recently updated). FAST* Flushing Policy employs a Top-1 selection algorithm to select a flushing unit to be evicted to flash memory with the objective of maximizing the number of updates per flushing unit and minimizing the time stamp of the flushing unit. The intuition behind such a policy is that it is sometimes better to keep the block that has the highest number of updates in the tree modifications table (i.e., in memory)

Geoinformatica (2013) 17:417448

433

and not to flush it, especially if that block is expected to receive more updates (i.e., recently updated block). On the other hand, it might be better to flush a flash block that has a bit less number of updates, but it is not expected to be updated frequently (i.e., least recently updated block). Hence, FAST* flushing policy makes that tradeoff between the two parameters in order to amortize the total number of erase operations on flash memory storage systems. 7.3 Flushing algorithm Algorithm 4 gives the pseudo code for flushing tree updates. The algorithm has two main steps: (1) Finding out the list of f lushed tree nodes (Lines 29 in Algorithm 4). This step starts by finding out the victim f lushing unit, MaxUnit, using the flushing policy passed to the algorithm. Then, we scan the tree modif ications table to find all updated tree nodes that belong to MaxUnit. For each such node, we construct the most recent version of the node by retrieving the tree node from the flash storage, and updating it with the in-memory updates. This is done by calling the RetrieveNode(N ) function, given in Algorithm 3. The list of these updated nodes constitute the list of to be flushed nodes, FlushList. (2) Flushing, logging, and cleaning selected tree nodes (Lines 1015 in Algorithm 4). In this step, all nodes in the FlushList are written once to the flash storage. As all these nodes reside in one f lushing unit, this operation would have a minimal cost due to our careful selection of the f lushing unit size. Then, similar to update operations, we log the flushing operation to ensure durability. Finally, all flushed nodes are removed from the tree modif ications table to free memory space for new updates. Algorithm 4 Flushing Tree Updates 1: Function FlushTreeUpdates( FlushPolicy) /* STEP 1: Finding out the list of flushed tree nodes */ 2: FlushList  { } 3: MaxUnit  Retrieve Unit to be Flushed uisng FlushPolicy 4: for each Node N in tree modif ications table do 5: if N  MaxUnit then 6: F  RetrieveNode(N ) 7: FlushList  FlushList  F 8: end if 9: end for /* STEP 2: Flushing, logging, and cleaning selected nodes */ 10: Flush all tree updates  FlushList to a clean flash memory block 11: Add (Flush, All Nodes in FlushList) to the log file 12: Erase the old flash memory block and update the index pointer to refer the new block 13: for each Node F in FlushList do 14: Delete F from the Tree Modif ications Table 15: end for Example In our running example given in Fig. 4, assume that the memory is full, hence FAST triggers its flushing module. Assume also that nodes B, C, and D reside

434

Geoinformatica (2013) 17:417448

in the same f lushing unit B1 , while nodes E, F , and G reside in another f lushing unit B2 . The number of updates in B1 is three as each of nodes B, C and D has been updated once. On the other hand, the number of updates in B2 is one because nodes E and F has no updates at all, and node G has only a single update. Hence, as per FAST flushing policy, MaxUnit is set to B1 , and we will invoke RetrieveNode algorithm for all nodes belonging to B1 (i.e., nodes B, C, and D) to get the most recent version of these nodes and flush them to flash memory. Then, the log entry ( Flush; Nodes B, C, D) is added to the log file (depicted as the last log entry in Fig. 5a). Finally, the entries for nodes B, C, and D are removed from the tree modif ications table. Cost analysis For a given flushing operation F applied to a tree index structure T , Let Pflush be the set of tree nodes that belongs to the block selected to be flushed. Let M p be the number of modifications applied to node p and buffered in the tree modifications table at the time F is applied. Hence, the total cost C F of flushing operation F applied on T is as follows: CF = EF + H +
p  Pflush

[ R F + ( M p  R M ) + W F + L]

(3)

We decide which unit to flush by employing the flushing policy passed to the algorithm. The cost of this in memory operation H varies based on which flushing policy is activated. For each node p  Pflush , we first need to retrieve the node current value saved in flash memory which costs p  Pflush R F , and then lookup the node in the tree modifications table in p  Pflush L. For each node p  Pflush , we read all M p modifications of p that are buffered in the tree modifications table, which sum up to p  Pflush ( M p  R M ). Before we write the new nodes values, we first erase the whole flash block which costs E F time. For each node p  Pflush , we write the flushed node new value, that costs p  Pflush W F . All of the above sum up to give the flushing operation cost given by Eq. 3.

8 Crash recovery and log compaction in FAST As discussed before, FAST heavily relies on storing recent updates in memory, to be flushed later to the flash storage. Although such design efficiently amortizes the expensive random write operations over a large number of updates, it poses another challenge where memory contents may be lost in case of system crash. To avoid such loss of data, FAST employs a crash recovery module that ensures the durability of in-memory updates even if the system crashed. The crash recovery module in FAST mainly relies on the log file entries, written sequentially upon the update and flush operations. In this section, we will first describe the crash recovery module and logging mechanism in FAST. Then, we will follow by discussing the log compaction operation in FAST, which is mainly done to ensure that the log file is within a certain size limit. Log compaction has a very similar operation to the recovery module, and it is crucial to keep up the efficiency of FAST. For simplicity, we will not consider the case of having a system crash during the recovery process, as this can be handled in a similar way to traditional recovery modules in database systems.

Geoinformatica (2013) 17:417448

435

8.1 Recovery The recovery module in FAST is triggered when the system restarts from a crash, with the goal of restoring the state of the system just before the crash took place. The state of the system includes the contents of the in-memory data structure, tree modif ications table, and the flash-resident tree index structure. By doing so, FAST ensures the durability of all non-flushed updates that were stored in memory before crash. Main idea The main idea of the recovery operation is to scan the log file bottomup to be aware of the flushed nodes, i.e., nodes that made their way to the flash storage. During this bottom-up scanning, we also find out the set of operations that need to be replayed to restore the tree modif ications table. Then, the recovery module cleans all the flash blocks, and starts to replay the non-flushed operations in the order of their insertion, i.e., top-down. The replay process includes insertion in the tree modif ications table as well as a new log entry. It is important here to reiterate our assumption that there will be no crash during the recovery process, so, it is safe to keep the list of operations to be replayed in memory. If we will consider a system crash during the recovery process, we might just leave the operations to be replayed in the log, and scan the whole log file again in a top-down manner. In this top-down scan, we will only replay the operations for non-flushed nodes, while writing the new log entries into a clean flash block. The result of the crash recovery module is that the state of the memory will be stored as it was before the system crashes, and the log file will be an exact image of the tree modif ications table. Algorithm Algorithm 5 gives the pseudo code for crash recovery in FAST, which has two main steps: (1) Bottom-Up scan (Lines 211 in Algorithm 5). In this step, FAST scans the log file bottom-up, i.e., in the reverse order of the insertion of log entries. For each log entry L in the log file, if the operation of L is Flush, then we know that all the nodes listed in this entry have already made their way to the flash storage. Thus, we keep track of these nodes in a list, termed FlushedNodes, so that we avoid redoing any updates over any of these nodes later. On the other side, if the operation of L is not Flush, we check if the node in L entry is in the list FlushedNodes. If this is the case, we just ignore this entry as we know that it has made its way to the flash storage. Otherwise, we push this log entry into a stack of operations, termed RedoStack, as it indicates a non-flushed entry at the crash time. At the end of this step, we pass the RedoStack to the second step. (2) Top-Down processing (Lines 1319 in Algorithm 5). At the beginning, we first create a new log file Fnew . Then, this step basically goes through all the entries in the RedoStack in a top-down way, i.e., the order of insertion in the log file. As all these operations were not flushed by the crash time, we just add each operation to the tree modif ications table and add a corresponding log entry in the new Log File Fnew . The reason of doing these operations in a top-down way is to ensure that we have the same order of updates, which is essential in case one node has multiple non-flushed updates. At the end of this step, the tree modif ications table will be exactly the same as it was just before the crash time, while the new log file Fnew will be exactly an image of the tree modif ications table stored in the flash storage. Finally, we change the log file pointer to refer to the new log file Fnew and we finally erase the old log file flash blocks.

436

Geoinformatica (2013) 17:417448

Algorithm 5 Crash Recovery 1: Function RecoverFromCrash() /* STEP 1: Bottom-Up Cleaning */ 2: FlushedNodes   3: for each Log Entry L in the log file in a reverse order do 4: if the operation of L is Flush then 5: FlushedNodes  FlushedNodes  the list of nodes in L 6: else 7: if the node in entry L  / FlushedNodes then 8: Push L into the stack of updates RedoStack 9: end if 10: end if 11: end for /* Phase 2: Top-Down Processing */ 12: Create a new Log File Fnew 13: while RedoStack is not Empty do 14: Op  Pop an update operation from the top of RedoStack 15: Insert the operation Op into the tree modif ications table 16: Add a log entry for Op in the new log file Fnew 17: end while 18: Change the Log File pointer to refer to the new Log File Fnew 19: Clean all the old log entries by erasing the old log file flash blocks

Example In our running example, the log entries of inserting Objects O7 and O8 in Fig. 4 are given as the first four log entries in Fig. 5a. Then, the last log entry in Fig. 5a corresponds to flushing nodes B, C, and D. We assume that the system is crashed just after inserting this flushing operation. Upon restarting the system, the recovery module will be invoked. First, the bottom-up scanning process will be started with the last entry of the log file, where nodes B, C, and D are added to the list FlushedNodes. Then, for the next log entry, i.e., the fourth entry, as the node affected by this entry D is already in the FlushedNodes list, we just ignore this entry, since we are sure that it has made its way to disk. Similarly, we ignore the third log entry for node B. For the second log entry, as the affected node G is not in the FlushedNodes list, we know that this operation did not make it to the storage yet, and we add it to the RedoStack to be redone later. The bottom-up scanning step is concluded by ignoring the first log entry as its affected node C is already flushed, and by wiping out all log entries. Then, the top-down processing step starts with only one entry in the RedoStack that corresponds to node G. This entry will be added to the tree modif ications table and log file. Figure 5b gives the log file after the end of the recovery module which also corresponds to the entries of the tree modif ications table after recovering from failure. Cost analysis For a given crash recovery operation R applied to a tree index structure T , let Z be the set of operations recorded in the log file. Let  (0    1) be the fraction of operations in Z that had been flushed to T before the system fails. Let Spage , Sblock , and Slog be the byte size of the flash page, flash block and flash log

Geoinformatica (2013) 17:417448

437

file, respectively. Hence, the total cost C R of a crash recovery operation R applied on T is as follows: C R = Slog  RF RF + Z    (W M + W F ) + Spage Sblock (4)

As all the Z entries in the log file have to be scanned, then the total cost to scan Slog them is R F  Spage . In addition, only Z   log file operations need to be redone (i.e., written back to the tree modifications table), which results to an additional cost of Z    W M . As all redone operations are written back to memory, an additional cost of logging them is Z    W F . The old log file blocks needs to be erased, which Slog incurs a cost of E F  Sblock . All of the above sums up to give the recovery cost given in Eq. 4. 8.2 Log compaction As FAST log file is a limited resource, it may eventually become full. In this case, FAST triggers a log compaction module that organizes the log file entries for better space utilization. This can be achieved by two space saving techniques: (a) Removing all the log entries of flushed nodes. As these nodes have already made their way to the flash storage, we do not need to keep their log entries anymore, and (b) Packing small log entries in a larger writing unit. Whenever a new log entry is inserted, it mostly has a small size that may occupy a flash page as the smallest writing unit to the flash storage. At the time of compaction, these small entries can be packed together to achieve the maximum possible space utilization. The main idea and algorithm for the log compaction module are almost the same as the ones used for the recovery module, with the exception that the entries in the RedoStack will not be added to the tree modif ications table, yet they will just be written back to the log file, in a more compact way. As in the recovery module, Fig. 5a and b give the log file before and after log compaction, respectively. The log compaction have similar expensive cost as the recovery process. Fortunately, with an appropriate size of log file and memory, it will not be common to call the log compaction module. It is unlikely that the log compaction module will not really compact the log file much. This may take place only for a very small log size and a very large memory size, as there will be a lot of non-flushed operations in memory with their corresponding log entries. Notice that if the memory size is small, there will be a lot of flushing operations, which means that log compaction can always find log entries to be removed. If this unlikely case takes place, we call an emergency f lushing operation where we force flushing all main memory contents to the flash memory persistent storage, and hence clean all the log file contents leaving space for more log entries to be added. Cost analysis The log compaction is almost the same as the crash recovery procedure. The only difference is that records are not redone (written to the tree modifications table). Similar to recovery cost, the log compaction cost CC is as follows: CC = Slog  RF RF + Z    WF + Spage Sblock (5)

438

Geoinformatica (2013) 17:417448

9 Experimental evaluation This section experimentally evaluates the performance of FAST, compared to the state-of-the-art algorithms for one-dimensional and multi-dimensional flash index structures: (1) Lazy Adaptive Tree (LA-tree) [2]: LA-tree is a flash friendly one dimensional index structure that is intended to replace the B-tree. LA-tree stores the updates in cascaded buffers residing on flash memory and, then empties these buffers dynamically based on the operations workload. (2) FD-tree [24, 25]: FD-tree is a one-dimensional index structure that allows small random writes to occur only in a small portion of the tree called the head tree which exists at the top level of the tree. When the capacity of the head tree is exceeded, its entries are merged in batches to subsequent tree levels. (3) RFTL [35]: RFTL is a mutli-dimensional tree index structure that adds a buffering layer on top of the flash translation layer (FTL) in order to make R-trees work efficiently on flash devices. We instantiate B-tree and R-tree instances of FAST using both flushing policies (i.e., FAST flushing policy and FAST* flushing policy), termed FAST-Btree, FAST*Btree, FAST-Rtree, and FAST*-Rtree , respectively, by implementing FAST inside the GiST generalized index structure [15], which is already built inside PostgreSQL [1]. In our experiments, we use two synthetic workloads: (1) Lookup intensive workload (W L ): that includes 80 % search operations and 20 % update operations (i.e., insert, delete, or update). (2) Update intensive workload, (WU ): that includes 20 % search operations and 80 % update operations. Unless mentioned otherwise, we set the number of workload operations to 10 million operations, main memory size to 256 KB (i.e., the amount of memory dedicated to main memory buffer used by FAST), tree index size to 512 MB, and log file size to 10 MB, which means that the default log size is 2 % of the index size. The experiments in this section mainly discuss the effect of varying the memory size, log file size, index size, and number of updates on the performance of FASTBtree, FAST-Rtree, LA-tree, FD-tree, and RFTL. Also, we study the performance of flushing, log compaction, and recovery operations in FAST. In addition, we compare the implementation cost between FAST and its counterparts. Our performance metrics are mainly the number of flash memory erase operations and the average response time. However, in almost all of our experiments, we got a similar trend for both performance measures. Thus, for brevity, we only show the experiments for the number of flash memory erase operations, which is the most expensive operation in flash storage. Although we compare FAST to its counterparts from a performance point of view, however we believe the main contribution of FAST is not in the performance gain. The generic structure and low implementation cost are the main advantages of FAST over specific flash-aware tree index structures. All experiments were run on both raw flash memory storage, and solid state drives (SSDs). For raw flash, we used the raw NAND flash emulator described in [2]. The emulator was populated with exhaustive measurements from a custom-designed Mica2 sensor board with a Toshiba1Gb NAND TC58DVG02A1FT00 flash chip. For SSDs, we used a 32GB MSP-SATA7525032 SSD device. All the experiments were run on a machine with Intel Core2 8400 at 3Ghz with 4GB of RAM running Ubuntu Linux 8.04.

Geoinformatica (2013) 17:417448

439

9.1 Effect of memory size Figure 6 and b give the effect of varying the memory size from 128 KB to 1,024 KB (in a log scale) on the number of erase operations, encountered in FAST-Btree, LAtree, and FD-tree, for workloads W L and WU , respectively. For both workloads and for all memory sizes, FAST-Btree consistently has much lower erase operations than that of the LA-tree. More specifically, Fast-Btree results in having only from half to one third of the erase operations encountered by LA-tree. This is mainly due to the choice of f lushing unit and f lushing policy used in FAST that amortize the block erase operations over a large number of updates. Also, for both experiments, the number of erase operations decreases with the increase of the memory size, which is intuitive as more memory means less frequent need for flushing, and hence less need for block erase operations. The performance of FAST-Btree is slightly better than that of FD-tree, because FD-tree does not employ a crash recovery technique (i.e., no logging overhead). FAST still performs better than FD-tree due to FAST flushing policy that selects the best block to be flushed to flash memory. Although the performance of FD-tree is close to FAST-Btree, however FAST has the edge of being a generic framework which is applied to many tree index structures and needs less work and overhead (in terms of lines of code) to be incorporated in the database engine. Comparing the two workloads against each other, we can see that the workload WU encounters much more erase operations than that of workload W L . This is mainly because WU is an update intensive workload which results in many in-memory updates that need to flushed. FAST*-Btree gives a slightly better performance than FAST-Btree as FAST*-Btree employs a flushing policy that does not only rely on the number of updates per flash block, but also takes into account the last time a flash block has been updated. Hence, FAST*-tree gives a chance for those flash blocks that has higher number of updates to stay in memory if more updates are expected to be applied to these blocks. Figures 7a and b give similar experiments to that of Fig. 6 and b, with the exception that we run the experiments for two-dimensional search and update operations for both the Fast-Rtree and RFTL. To be able to do so, we have adjusted our
90 600

# of erase operations *(103)

80 70 60 50 40 30 20
FAST*Btree FASTBtree LAtree 10 FDtree

# of erase operations *(103)

500 400 300 200 100 0 128 256

FAST*Btree FASTBtree LAtree FDtree

0 128

256

512

1024

512

1024

Memory Size (KB)

Memory Size (KB)

(a) WL
Fig. 6 Effect of memory size on one-dimensional index structure

(b) WU

440
100
FAST*Rtree FASTRtree RFTL

Geoinformatica (2013) 17:417448
900
FAST*-Rtree FAST-Rtree RFTL

# of erase operations *(103)

# of erase operations *(103)

800 700 600 500 400 300 200 100 0 128 256

80 60 40 20

128

256

512

1024

512

1024

Memory Size (KB)

Memory Size (KB)

(a) Spatial-WL
Fig. 7 Effect of memory size on multi-dimensional index structure

(b) Spatial-WU

workload W L and WU to Spatial-W L and Spatial-WU , respectively, which have twodimensional operations instead of the one-dimensional operations used in W L and WU . The result of these experiments have the same trend as the ones done for onedimensional tree structures, where FAST-Rtree has consistently better performance than RFTL in all cases, with around one half to one third of the number of erase operations encountered in RFTL. Similar to the one-dimesnional case, FAST*-Rtree slightly outperforms FAST-Rtree. Comparing the multi-dimensional workload to the one dimensional one shows that the multi-dimensional workload encounters more erase operations which is mainly due to the facts that the update operation may span more nodes. However, even with this, FAST still keeps its performance ratio over its counterparts. The experiments in Figs. 6 and 7 not only shows that FAST has better performance than its counterparts LA-tree, FD-tree and RFTL, but it also shows the power of the FAST framework where it can be applied to both one-dimensional and multidimensional index structures with the same efficiency. In other words, it is not only that FAST is better than LA-tree and FD-tree, but it is also the fact that FAST has the ability to efficiently support multi-dimensional search and update operations in which LA-tree or FD-tree cannot even support. 9.2 Effect of log file size Figure 8 gives the effect of varying the log file size from 10 MB (i.e., 2 % of the index size) to 25 MB (i.e., 5 % of the index size) on the number of erase operations, encountered in FAST-Btree, LA-tree, and FD-tree for workload W L (Fig. 8a) and FAST-Rtree and RFTL for workload Spatial-WU (Fig. 8b). For brevity, we do not show the experiments of FAST-Btree, LA-tree, and FD-tree for workload WU nor the experiment of FAST-Rtree and RFTL for workload Spatial-W L . As can be seen from the figures, the performance of both LA-tee, FD-tree, and RFTL is not affected by the change of the log file size. This is mainly because these three approaches rely on buffering incoming updates, and hence does not make use of any log file. It is interesting, however, to see that the number of erase operations in FAST-Btree and FAST-Rtree significantly decreases with the increase of the log file size, given that the memory size is set to its default value of 256 KB in all experiments. The

Geoinformatica (2013) 17:417448
100
FAST*Btree FASTBtree LAtree FDtree

441
800

# of erase operations *(103)

90 80 70 60 50 40 30 20 10 0 10 15

# of erase operations *(103)

700 600 500 400 300 200 100 0 10 15 20 25
FAST*Rtree FASTRtree RFTL

20

25

Maximum Log File Size (MB)

Maximum Log File Size (MB)

(a) WL
Fig. 8 Effect of FAST log file size

(b) Spatial-WU

justification for this is that with the increase of the log file size, there will be less need for FAST to do log compaction. FAST*-Btree and FAST*-Rtree shows the same trend as FAST-Btree and FAST-Rtree except that they slightly give better performance due to the fact that they apply the FAST* Flushing policy. Revisiting Figs. 6 and 7 in Section 9.1, the number of erase operations encountered in both LA-tree, FD-tree, and RFTL were only coming from flushing buffered updates, while the number of erase operations in FAST were coming from two sources, flushing in-memory updates, and log compaction. Then, the experiment in this section (Fig. 8) shows that a large fraction of the erase operations in FAST is coming from the log compaction operation, which can be significantly reduced with the slight increase of the log file. With this, we can see that FAST achieves close to an order of magnitude less erase operations than its counterparts for both onedimensional and multi-dimensional index structures when having the log file as small as 5 % of the index size, i.e., 25 MB. 9.3 Effect of index size Figure 9 gives the effect of varying the index size from 128 MB to 4 GB (in a log scale) on the number of erase operations, encountered in FAST-Btree, LA-tree, and FD-tree for workload W L (Fig. 9a) and FAST-Rtree and RFTL for workload Spatial-WU (Fig. 9b). Same as in Section 9.2, we omit other workloads for brevity. In all cases, FAST consistently gives much better performance than its counterparts. Both FAST and other index structures have similar trend of a linear increase of the number of erase operations with the increase of the index size. This is mainly because with a larger index, an update operation may end up modifying more nodes in the index hierarchy, or more overlapped nodes in case of multi-dimensional index structures. Moreover, FAST*-Btree and FAST*-Rtree give a bit better performance than FAST-Btree and FAST-Rtree, respectively. This is basically due to the fact that FAST* flushing policy handles the flash memory updates better than the original FAST flushing policy, hence when the index size increase the possibility that more blocks are updated increases leading to such performance gain for both FAST*-Btree and FAST*-Rtree. The take home message from this experiment is that FAST still maintains its performance gain over its counterparts even with the large increase of the index size.

442
# of Erase Operations *(103)
FAST*Btree FASTBtree LAtree FDtree

Geoinformatica (2013) 17:417448
# of Erase Operations *(103)
1200 1000 800 600 400 200 0
FAST*Rtree FASTRtree RFTL

140 120 100 80 60 40 20 0

B 1G

B 1G

12

25

51

B 2G

Fig. 9 Effect of tree index size

9.4 Effect of number of updates Figure 9 gives the effect of varying the number of update operations from one million to 100 millions (in a log scale) on the number of erase operations for both one-dimensional (i.e., FAST-Btree, LA-tree, and FD-tree in Fig. 10a) and multidimensional index structures (i.e., FAST-Rtree and RFTL in Fig. 10b). As we are only interested in update operations, the workload for the experiments in this section is just a stream of incoming update operations, up to 100 million operations. As can be seen from the figure, FAST scales well with the number of updates and still maintains its superior performance over its counterparts from both one-dimensional (LA-tree) and multi-dimensional index structures (RFTL). FAST performs slightly better than FD-tree; this is because FD-tree (one dimensional index structure) is buffering some of the tree updates in memory and flushes them when needed, but FAST applies a flushing policy, which flushes only the block with the highest number of updates. In addition, FAST* slightly outperforms FAST because FAST* flushing policy employs a Top-1 algorithm that maximizes the number of updates per block and minimize the timestamp at which the block has been updated, hence the total amortized update cost in FAST* is less than FAST.
10000 9000 8000 7000 6000 5000 4000 3000 2000 1000 0

12 8M B

25 6M B

51 2M B

B 2G

B 4G

B 4G

8M B

6M B

2M B

Index Size

Index Size

(a) WL

(b) Spatial-WU

# of erase operations *(103)

FAST*Btree FASTBtree LAtree FDtree

# of erase operations *(103)

FAST*Rtree FASTRtree RFTL

10000

1000

1

10

100

1

10

100

# of Updates *(106)

# of Updates *(106)

(a) FAST-Btree
Fig. 10 Effect of number of updates

(b) FAST-Rtree

Geoinformatica (2013) 17:417448

443

9.5 Flushing performance Figure 11 illustrates the performance of the f lushing policy employed by FAST compared to a naive flushing policy, termed f lush-all that just flushes all the memory contents to the flash storage once. We also compare FAST to a random flushing policy, termed Rand-Flush that chooses a block at random and flushed its contents to the flash storage The performance is given with respect to various memory sizes (Fig. 11a) and log file sizes (Fig. 11a). Both experiments were run for FASTBtree under workload WU . Running these experiments for FAST-Rtree and other workloads give similar performance, and thus omitted for brevity. Figure 11a gives the effect of varying the memory size from 128 KB to 1,024 KB on the number of erase operations for flush-all policy, Rand-Flush policy and FAST flushing policy. In all cases, FAST has much lower erase operations than the flushall and Rand-Flush policies, which is about one fourth of the erase operations for a memory size of 512 KB. The main reason behind this gain in FAST is that it amortizes the cost of the block erase operation over a large number of updates, and hence, will free more memory with each flushing operation. On the other side, in the flush-all or Rand-Flush policy, a block may be erased just because it has only one single update in the memory. In this case, although a block is erased, it does not free much memory space. The Rand-Flush policy performance is slightly better than that of flush-all policy because the Rand-Flush flushes only one block and hence keeping all other blocks in memory, which decrease the cost of random writes on these blocks. FAST* flushing policy is better than FAST flushing policy as it better amortizes the update cost. FAST* policy may still keep a block that has the highest number of updates in memory if this block has higher potential to be updated soon, and hence the decreasing the number of erase operations applied to that block. Figure 11b gives a similar experiment to that of Fig. 11a with the exception that we study the effect of changing the log size from 10 MB to 25 MB on the number of erase operations. In all cases, FAST flushing policy is superior, which is intuitive given the above explanation for Fig. 11a. However, an interesting observation from Fig. 11b is that the gain from FAST flushing policy over the flush-all and RandFlush policies increases with the increase of the log file size. This means that FAST flushing policy makes better use of the log file than the flush-all and Rand-Flush policies. A justification for this is as follows: As FAST flushing policy evicts a block
550 500 450 400 350 300 250 200 150 100 50 0 128 600

# of erase operations *(103)

# of erase operations *(103)

FAST*Flush FASTFlush FlushAll RandFlush

500 400 300 200 100 0 10 15

FAST*Flush FASTFlush FlushAll RandFlush

256

512

1024

20

25

Memory Size (KB)

Log Size (MB)

(a) Memory size
Fig. 11 Flushing performance

(b) Log size

444

Geoinformatica (2013) 17:417448

to the storage only if it has high number of updates, the log entry for this flushing operation will include many updated nodes. Then, in the log compaction process, there will be a lot of space for compaction. This would not be the case for the flushall and Rand-Flush policies where a log entry for a flush operation may include only one flushed node. Then, at the time of log compaction, there will be nothing much to compact, which means that the log compaction will be called again. As discussed in Section 9.2 and Fig. 9, log compaction is a major factor in the number of erase operations. Reducing the frequency of log compaction makes FAST flushing policy more superior than the flush-all policy. Moreover, FAST* flushing policy slightly outperforms FAST flushing policy because of the fact that FAST* may prefer to keep the block that has the highest number of updates in memory leading to less erase operations on the flash memory storage. 9.6 Log compaction Figure 12a gives the behavior and frequency of log compaction operations in FAST when running a sequence of 200 thousands update operations for a log file size of 10 MB. The Y axis in this figure gives the size of the filled part of the log file, started as empty. The size is monotonically increasing with having more update operations till it reaches its maximum limit of 10 MB. Then, the log compaction operation is triggered to compact the log file. As can be seen from the figure, the log compaction operation may compact the log file from 20 to 60 % of its capacity, which is very efficient compaction. Another take from this experiment is that we have made only seven log compaction operations for 200 thousands update operations, which means that the log compaction process is not very common, making FAST more efficient even with a large amount of update operations. 9.7 Recovery performance Figure 12b gives the overhead of the recovery process in FAST, which serves also as the overhead of the log compaction process. The overhead of recovery increases linearly with the size increase of the log file contents at the time of crash. This is intuitive as with more log entries in the log file, it will take more time from the FAST

10 8 6 4 2 0

100

Recovery Time (millisec)

90 80 70 60 50 40 30 20 10 1

FAST

Log File Size (MB)

0

50

100

150

200

2

3

4

5

6

7

8

9

Number of Updates So Far *(103)

Log Size (MB)

(a) Log Compaction
Fig. 12 Log compaction and recovery

(b) Recovery

Geoinformatica (2013) 17:417448

445

recovery module to scan this log file, and replay some of its operations to recover the lost main memory contents. However, what we really want to emphasize on in this experiment is that the overhead of recovery is only about 100 ms for a log file that includes 9 MB of log entries. This shows that the recovery overhead is a low price to pay to ensure transaction durability.

10 Conclusion This paper presented FAST; a generic framework for flash-aware data-partitioning tree index structures. FAST distinguishes itself from all previous attempts of flash memory indexing in two aspects: (1) FAST is a generic framework that can be applied to a wide class of tree index structures, and (2) FAST achieves both ef f iciency and durability of read and write flash operations. FAST has four main modules, namely, update, search, f lushing, and recovery. The update module is responsible on buffering incoming tree updates in an in-memory data structure, while writing small entries sequentially in a designated flash-resident log file. The search module retrieves requested data from the flash storage and updates it with recent updates stored in memory, if any. The f lushing module is responsible on evicting flash blocks from memory to the flash storage to give space for incoming updates. Finally, the recovery module ensures the durability of in-memory updates in case of a system crash.

References
1. PostgreSQL. http://www.postgresql.org 2. Agrawal D, Ganesan D, Sitaraman RK, Diao Y, Singh S (2009) Lazy-adaptive tree: an optimized index structure for flash devices. PVLDB 3. Agrawal N, Prabhakaran V, Wobber T, Davis J, Manasse M, Panigrahy R (2008) Design tradeoffs for SSD performance. In: Usenix annual technical conference, USENIX 4. Bayer R, McCreight EM (1972) Organization and maintenance of large ordered indices. Acta Inform 1:173189 5. Beckmann N, Kriegel H-P, Schneider R, Seeger B (1990) The R*-tree: an efficient and robust access method for points and rectangles. In: SIGMOD 6. Birrell A, Isard M, Thacker C, Wobber T (2007) A design for high-performance flash disks. ACM SIGOPS Oper Syst Rev 41(2):8893 7. Bouganim L, Jnsson B, Bonnet P (2009) uFLIP: understanding flash IO patterns. In: CIDR 8. Chang Y-H, Hsieh J-W, Kuo T-W (2007) Endurance enhancement of flash-memory storage systems: an efficient static wear leveling design. In: Proceedings of the annual ACM IEEE Design Automation Conference, DAC, pp 212217 9. Chen S (2009) FlashLogging: exploiting flash devices for synchronous logging performance. In: SIGMOD. New York, NY 10. Comer D (1979) The ubiquitous B-tree. ACM Comput Surv 11(2):121137 11. Gray J (2006) Tape is dead, disk is tape, flash is disk, RAM locality is king. http://research. microsoft.com/gray/talks/Flash_is_Good.ppt. Accessed Dec 2006 12. Gray J, Fitzgerald B (2008) Flash disk opportunity for server applications. ACM Queue 6(4):18 23 13. Gray J, Graefe G (1997) The five-minute rule ten years later, and other computer storage rules of thumb. SIGMOD Rec 26(4):6368 14. Guttman A (1984) R-trees: a dynamic index structure for spatial searching. In: SIGMOD 15. Hellerstein JM, Naughton JF, Pfeffer A (1995) Generalized search trees for database systems. In: VLDB 16. Hutsell W (2007) Solid state storage for the enterprise. Storage Networking Industry Association (SNIA) Tutorial, Fall

446

Geoinformatica (2013) 17:417448

17. Katayama N, Satoh, S (1997) The sr-tree: an index structure for high-dimensional nearest neighbor queries. In: SIGMOD 18. Kim H, Ahn S (2008) BPLRU: a buffer management scheme for improving random writes in flash storage. In: FAST 19. Lavenier D, Xinchun X, Georges G (2006) seed-based genomic sequence comparison using a FPGA/FLASH accelerator. In: ICFPT 20. Lee S, Moon B (2007) Design of flash-based DBMS: an in-page logging approach. In: SIGMOD 21. Lee S-W, Moon B, Park C, Kim J-M, Kim S-W (2008) A case for flash memory SSD in enterprise database applications. In: SIGMOD 22. Lee S-W, Park D-J, sum Chung T, Lee D-H, Park S, Song H-J (2007) A log buffer-based flash translation layer using fully-associate sector translation. TECS 23. Leventhal A (2008) Flash storage today. ACM Queue 6(4):2430 24. Li Y, He B, Luo Q, Yi K (2009) Tree indexing on flash disks. In: ICDE 25. Li Y, He B, Yang RJ, Luo Q, Yi K (2010) Tree indexing on solid state drives. Proceedings of the VLDB Endowment 3(12):11951206 26. Ma D, Feng J, Li G (2011) LazyFTL: A page-level flash translation layer optimized for NAND flash memory. In: SIGMOD 27. McCreight EM (1977) Pagination of B*-trees with variable-length records. Commun ACM 20(9):670674 28. Moshayedi M, Wilkison P (2008) Enterprise SSDs. ACM Queue 6(4):3239 29. Nath S, Gibbons PB (2008) Online maintenance of very large random samples on flash storage. In: VLDB 30. Nath S, Kansal A (2007) Flashdb: dynamic self-tuning database for NAND flash. In: IPSN 31. Reinsel D, Janukowicz J (2008) Datacenter SSDs: solid footing for growth. http://www.samsung. com/us/business/semiconductor/news/downloads/210290.pdf. Accessed Jan 2008 32. Sellis TK, Roussopoulos N, Faloutsos C (1987) The R+-tree: a dynamic index for multidimensional objects. In: VLDB 33. Shah MA, Harizopoulos S, Wiener JL, Graefe G (2008) Fast scans and joins using flash drives. In: International Workshop of Data Managment on New Hardware, DaMoN 34. White DA, Jain R (1996) Similarity indexing with the SS-tree. In: ICDE 35. Wu C, Chang L, Kuo T (2003) An efficient R-tree implementation over flash-memory storage systems. In: GIS 36. Wu C, Kuo T, Chang L (2007) An efficient B-tree layer implementation for flash-memory storage systems. TECS

Mohamed Sarwat is a PhD candidate at the Computer Science and Engineering department, University of Minnesota, where he also received his master's degree in computer science in 2011. His research interest lies in the broad area of Database systems, spatio-temporal databases, distributed graph databases, social networking, cloud computing, large-scale data management, data indexing and storage systems. He has been awarded the University of Minnesota Doctoral Dissertation Fellowship in 2012/2013. He has been a recipient of Best Research Paper Award in the 12th international symposium on spatial and temporal databases 2011.

Geoinformatica (2013) 17:417448

447

Mohamed F. Mokbel is an associate professor in the Department of Computer Science and Engineering, University of Minnesota. His current main research interests focus on providing database and platform support for spatial data, moving objects, and location-based services. Mohamed is the main architect for the PLACE, Casper, and CareDB systems that provide a database support for location-based services, location privacy, and personalization, respectively. His research work has been recognized by two best paper awards at IEEE MASS 2008 and MDM 2009 and by the NSF CAREER award 2010. Mohamed is currently the general co-chair of SSTD 2011 and program cochair for MDM 2011, DMSN 2011, and LBSN 2011. Mohamed was also the proceeding chair of ACM SIGMOD 2010, and the program co-chair for ACM SIGSPATIAL GIS 2008, 2009, and 2010. He serves in the editorial board of IEEE Data Engineering Bulletin, Distributed and Parallel Databases Journal, and Journal of Spatial Information Science. Mohamed is an ACM and IEEE member and a founding member of ACM SIGSPATIAL.

Xun Zhou received his B.Eng., and M.Eng., in Computer Science and Technology from Harbin Institute of Technology, Harbin, China in 2007 and 2009 respectively. He is currently a Ph.D. student in Computer Science at the University of Minnesota, Twin Cities. His research interests include spatiotemporal data mining, spatial databases and Geographical Information Systems (GIS). His current application focus is understanding climate change from data.

448

Geoinformatica (2013) 17:417448

Suman Nath is a researcher in the Sensing and Energy Research Group at Microsoft Research Redmond. He works on various data management problems in mobile and sensing systems. He received his PhD from Carnegie Mellon University in 2005. He has authored 20+ patents (granted or pending), 70+ papers in various computer science conferences and journals, and received Best Paper Awards at BaseNets 2004, USENIX NSDI 2006, IEEE ICDE 2008, and SSTD 2011. At Microsoft, he received the Gold Star Award, which recognizes excellence in leadership and contributions for Microsoft's long-term success.

A Demonstration of GeoSpark: A Cluster Computing Framework for Processing Big Spatial Data
Jia Yu
School of Computing, Informatics, and Decision Systems Engineering Arizona State University Tempe, Arizona 85281 Email: jiayu2@asu.edu

Jinxuan Wu
School of Computing, Informatics, and Decision Systems Engineering Arizona State University Tempe, Arizona 85281 Email: jinxuanw@asu.edu

Mohamed Sarwat
School of Computing, Informatics, and Decision Systems Engineering Arizona State University Tempe, Arizona 85281 Email: msarwat@asu.edu

Abstract--This paper demonstrates G EO S PARK a cluster computing framework for developing and processing large-scale spatial data analytics programs. G EO S PARK consists of three main layers: Apache Spark Layer, Spatial RDD Layer and Spatial Query Processing Layer. Apache Spark Layer provides basic Apache Spark functionalities as regular RDD operations. Spatial RDD Layer consists of three novel Spatial Resilient Distributed Datasets (SRDDs) which extend regular Apache Spark RDD to support geometrical and spatial objects with data partitioning and indexing. Spatial Query Processing Layer executes spatial queries (e.g., Spatial Join) on SRDDs. The dynamic status of SRDDs and spatial operations are visualized by G EO S PARK monitoring map interface. We demonstrate G EO S PARK using three spatial analytics applications (spatial aggregation, autocorrelation and co-location) to show how users can easily define their spatial analytics tasks and efficiently process such tasks on large-scale spatial data at interactive performance.

manage and maintain. The underlying database system must be able to digest Petabytes of spatial data and effectively analyze it.  Challenge II: Fast Analytics. In spatial data analytics applications, users will not tolerate delays introduced by the underlying spatial database system. Instead, the user needs to see useful information quickly. Hence, the underlying spatial data processing system must figure out effective ways to execute spatial analytics in parallel.

I.

I NTRODUCTION

Spatial data includes but is not limited to: weather maps, geological maps, socioeconomic data, vegetation indices, and more. Moreover, novel technology allows hundreds of millions of users to use their mobile devices to access their healthcare information and bank accounts, interact with friends, buy stuff online, search interesting places to visit on-thego, ask for driving directions, and more. In consequence, everything we do on the mobile internet leaves breadcrumbs of spatial digital traces, e.g., geo-tagged tweets, venue checkins. Making sense of such spatial data will be beneficial for several applications that may transform science and society  For example: (1) Socio-Economic Analysis: that includes for example climate change analysis, study of deforestation, population migration, and variation in sea levels, (2) Urban Planning: assisting government in city/regional planning, road network design, and transportation / traffic engineering, and (3) Commerce and Advertisement: e.g., point-of-interest (POI) recommendation services. The aforementioned applications need a powerful data management platform to handle the large volume of spatial data such applications deal with. Challenges to building such platform are as follows:  Challenge I: System Scalability. The massive-scale of available spatial data hinders making sense of it using traditional spatial database management systems. Moreover, large-scale spatial data, besides its tremendous storage footprint, may be extremely difficult to

Existing spatial database systems extend relational database systems with new data types, functions, operators, and index structures to handle spatial operations based on the Open Geospatial Consortium. Even though such systems sort of provide full support for spatial data storage and access, they suffer from a scalability issue. Based upon a relational database system, such systems are not scalable enough to handle largescale analytics over big spatial data. Recent works (e.g., [1], [2]) extend the Hadoop ecosystem to perform spatial analytics at scale. The Hadoop-based approach indeed achieves high scalability. However, these systems though exhibit excellent performance in batch-processing jobs, they show poor performance handling applications that require fast data analysis. Apache Spark [3], on the other hand, is an in-memory cluster computing system. Spark provides a novel data abstraction called resilient distributed datasets (RDDs) [4] that are collections of objects partitioned across a cluster of machines. Each RDD is built using parallelized transformations (filter, join or groupBy) that could be traced back to recover the RDD data. In memory RDDs allow Spark to outperform existing models (MapReduce) by up to two orders of magnitude. Unfortunately, Spark does not provide native support for spatial data and spatial operations. Hence, users need to perform the tedious task of programming their own spatial data processing jobs on top of Spark. This paper demonstrates G EO S PARK 1 [5] an in-memory cluster computing system for processing large-scale spatial data. G EO S PARK extends Apache Spark to support spatial data types and operations. In other words, the system extends the resilient distributed datasets (RDDs) concept to support spatial data. This problem is quite challenging due to the fact that (1) spatial data may be quite complex, e.g., rivers' and cities'
1 GeoSpark

Github repository: https://github.com/Sarwat/GeoSpark

Spatial Query Processing Layer Spatial Range Spatial KNN Spatial Join

Spatial RDD (SRDD) Layer Indexed Spatial RDD Point RDD Rectangle RDD Polygon RDD

Geometrical Operations Library

Fig. 2: SRDD partitioning
...

Apache Spark Layer

Fig. 1: GeoSpark architecture

users to write spatial data analytics programs. The SRDD layer consists of three new RDDs: PointRDD, RectangleRDD and PolygonRDD. One useful Geometrical operations library is also provided for every spatial RDD. Spatial Objects Support. G EO S PARK supports various spatial data input format (e.g., Comma Separated Value, Tab Separated Value and Well-Known Text). Each type of spatial objects is stored in a SRDD, PointRDD, RectangleRDD or PolygonRDD. G EO S PARK provides a set of geometrical operations which is called Geometrical Operations Library. This library natively supports geometrical operations. For example, Overlap(): Finds all of the internal objects which are intersected with others in geometry; MinimumBoundingRectangle(): Finds the minimum bounding rectangles for each object in a Spatial RDD or return a large minimum bounding rectangle which contains all of the internal objects in a Spatial RDD; Union(): Returns the union polygon of all polygons in this RDD. SRDD Partitioning. G EO S PARK automatically partitions all loaded Spatial RDDs by creating one global grid file for data partitioning. The main idea for assigning each element in a Spatial RDD to the same 2-Dimensional spatial grid space is as follows: Firstly, split the spatial space into a number of non-equal grid cells which compose a global grid file. This global grid file has load balanced grids according to presampling techniques. Then traverse each element in the SRDD and assign this element to a grid cell if the element overlaps with this grid cell. If one element intersects with two or more grid cells, then duplicate this element and assign different grid IDs to the copies of this element. Figure 2 depicts tweets in the U.S. at a particular moment, tweets and states are assigned to respective grid cells. SRDD Indexing. Spatial indexes like Quad-Tree and RTree are provided in Spatial IndexRDDs which inherit from Spatial RDDs. Users are able to initialize a Spatial IndexRDD. Moreover, G EO S PARK adaptively decides whether a local spatial index should be created for a certain Spatial IndexRDD partition based on a tradeoff between the indexing overhead (memory and time) on one-hand and the query selectivity as well as the number of spatial objects on the other hand.

geometrical boundaries, (2) spatial (and geometric) operations (e.g., Overlap, MinimumBoundingRectangle, Union) cannot be easily and efficiently expressed using regular RDD transformations and actions. G EO S PARK extends RDDs to form Spatial RDDs (SRDDs) and efficiently partitions SRDD data elements across machines and introduces novel parallelized spatial (geometric operations that follows the Open Geosptial Consortium (OGC) [6] standard) transformations and actions (for SRDD) that provide a more intuitive interface for users to write spatial data analytics programs. Moreover, G EO S PARK extends the SRDD layer to execute spatial queries (e.g., Range query, KNN query, and Join query) on large-scale spatial datasets. The dynamic status of SRDDs and associated queries are visualized by G EO S PARK monitoring interface throughout each entire spatial analytics process. We demonstrate G EO S PARK using three applications: (1) Application 1 uses G EO S PARK to calculate geospatial autocorrelation in a spatial dataset, (2) Application 2 leverages the system to generate a heat map of the San-Francisco trees population, and (3) Application 3 executes a spatial co-location pattern mining with the help of G EO S PARK . II. G EO S PARK
ARCHITECTURE

As depicted in Figure 1, G EO S PARK consists of three main layers: (1) Apache Spark Layer: that consists of regular operations that are natively supported by Apache Spark. These native functions are responsible for loading / saving data from / to persistent storage (e.g., stored on local disk or Hadoop file system HDFS). (2) Spatial Resilient Distributed Dataset (SRDD) Layer (Section II-A). (3) Spatial Query Processing Layer (Section II-B). A. Spatial RDD (SRDD) Layer This layer extends Spark with spatial RDDs (SRDDs) that efficiently partition SRDD data elements across machines and introduces novel parallelized spatial transformations and actions (for SRDD) that provide a more intuitive interface for

B. Spatial Query Processing Layer This layer supports spatial queries (e.g., Range query and Join query) for large-scale spatial datasets. After geometrical objects are stored and processed in the Spatial RDD layer, user may invoke a spatial query provided in Spatial Query Processing Layer. G EO S PARK processes such query and returns the final results to the user. G EO S PARK execution model implements the algorithms proposed by [7] and [8]. To accelerate a spatial query, G EO S PARK leverages the grid partitioned Spatial RDDs, spatial indexing, the fast in-memory computation and DAG scheduler of Apache Spark to parallelize the query execution. Spatial Range Query. G EO S PARK executes the spatial range query algorithm following the execution model: Load target dataset, partition data, create a spatial index on each SRDD partition if necessary, broadcast the query window to each SRDD partition, check the spatial predicate in each partition, and remove spatial objects duplicates that existed due to the data partitioning phase. Spatial Join Query. G EO S PARK executes the parallel spatial join query following the execution model. GeoSpark first partitions the data from the two input SRDDs as well as creates local spatial indexes (if required) for the SRDD which is being queried. Then it joins the two datasets by their keys which are grid IDs. For the spatial objects (from the two SRDDs) that have the same grid ID, GeoSpark calculates their spatial relations. If two elements from two SRDDS are overlapped, they are kept in the final results. The algorithm continues to group the results for each rectangle. The grouped results are in the following format: Rectangle, Point, Point, ... Finally, the algorithm removes the duplicated points and returns the result to other operations or saves the final result to disk. Spatial KNN Query. To process a Spatial KNN query, G EO S PARK uses a heap based top-k algorithm[9], which contains two phases: selection and merge. It takes a partitioned SRDD, a point P and a number k as inputs. To calculate the k nearest objects around point P , in the selection phase, for each SRDD partition G EO S PARK calculates the distances between each object to the given point P , then maintains a local heap by adding or removing elements based on the distances. This heap contains the nearest k objects around the given point P . For IndexedSRDD, the system can utilize the local indexes to reduce the query time. After the selection phase, G EO S PARK merges results from each partition, keeps the nearest k elements that have the shortest distances to P and outputs the result. III. D EMONSTRATION
SCENARIOS

SRDD partition (if it is still alive) on the left pane, she obtains more detailed information from a nested menu such as the data size in this partition, physical machine IP address, CPU and memory utilization. Besides the description of SRDDs, the tool also provides the status of a running spatial program in a progress bar format. By browsing G EO S PARK Monitoring Tool, users can interactively monitor the run time of their entire spatial analytics program. A. Application 1: Spatial Autocorrelation Spatial autocorrelation studies whether neighbor spatial data points might have correlations in some non-spatial attributes. Moran's I and Geary's C are two common coefficients in spatial autocorrelation. Based on them, analysts can determine whether these objects influence each other. These efficients are defined by two specific formulas correspondingly. An important part of these formulas is to find the spatial adjacent matrix. In this matrix, each tuples stands for whether two objects, such as points, rectangles or polygons, are within a specified distance. An application programmer may leverage G EO S PARK SpatialJoinQuery() to calculate the spatial adjacent matrix. Assume one dataset is composed of millions of point objects. The process to find the global adjacent matrix in G EO S PARK is as as follows: (1) Call G EO S PARK PointRDD initialization method to store the dataset in memory. Data partitioning and indexing are also completed by G EO S PARK at this stage. (2) Call G EO S PARK SpatialJoinQuery() in PointRDD. The first parameter is the query point set itself and the second one is the specified distance. (3) Use a new instance of Spatial PairRDD to store the result of Step (2). Step (2) will return the whole point set which has a new column specify the neighbors of each tuple within the distance. The expected schema is like this: Point coordinates (longitude, latitude), neighbor 1 coordinates (longitude, latitude), neighbor 2 coordinates (longitude, latitude), ... (4) Call persistence method in G EO S PARK to persist the resulting PointRDD. B. Application 2: Spatial Aggregation Assume an environmental scientist  studying the relationship between air quality and trees  would like to explore the trees population in San Francisco. A query may leverage the SpatialRangeQuery() provided by G EO S PARK to just return all trees in San Francisco. Alternatively, a heat map (spatial aggregate) that shows the distribution of trees in San Francisco may be also helpful. This spatial aggregate query (i.e., heat map) needs to count all trees at every single region over the map. In the heat map case, in terms of spatial queries, the heat map is a spatial join in which the target set is the tree map in San Francisco and the query area set is a set of San Francisco regions. The region number depends on the display resolution, or granularity, in the heat map. One proper G EO S PARK program is as follows: (3) Use a Spatial PairRDD to store the result of Step (2) which is the count for each polygon. The Spatial PairRDD follows the schema like this: (Polygon, count) such that Polygon represents the boundaries of the spatial region. (4) Call persistence method in Spark to persist the result PolygonRDD.

We demonstrate G EO S PARK using three spatial applications which are described below. G EO S PARK provides a monitoring map interface for system users to visualize and monitor the spatial program dynamically. A screenshot of this tool is provided in Figure 3. The interface allows users to execute Scala code interactively through an integrated Scale shell. Meanwhile, a map on-top of the shell visualizes the SRDDs generated by Scala code. Throughout the entire spatial analytics process, all generated SRDDs are listed on the left side pane of the user interface. When the user clicks on any

Fig. 3: G EO S PARK Monitoring Tool

C. Application 3: Spatial Co-location Spatial co-location is defined as two or more species are often located in a neighborhood relationship. The determination of this co-location pattern may benefit many further scientific researches. Biologists may find symbiotic relationships, mobile carriers can provide proper plans based users' co-location, and advertising agencies are able to place directed advertisements at the center of co-located populations. For instance, one existing co-location pattern is that one kind of tigers always live within a certain distance from one kind of rabbits. Thus we may infer one possible fact that these tigers feed on these rabbits. Some co-efficients are applied to determine the co-location relationship. Ripley's K function [10] is the most common one in real life. It usually executes numerous times iteratively and finds the ideal distance. The calculation of K function also needs the adjacent matrix between two type of objects. As we mentioned in spatial autocorrelation analysis, seeking adjacent matrix may leverage G EO S PARK SpatialJoinQuery(). Programmer are able to follow the same procedure depicted in Spatial Autocorrelation. Furthermore, spatial co-location, different from the previous basic spatial applications, is able to maximize the in memory computation goodness of G EO S PARK . Under G EO S PARK framework, users only need to spend time on loading data, partitioning data, and constructing indexes in the first iteration and then G EO S PARK automatically caches these intermediate data in memory. In the next numerous iterations, users are

able to directly keep mining the co-location pattern using the cache in memory instead of loading and pre-processing data from scratch. R EFERENCES
[1] A. Aji, F. Wang, H. Vo, R. Lee, Q. Liu, X. Zhang, and J. H. Saltz, "Hadoop-GIS: A High Performance Spatial Data Warehousing System over MapReduce," Proceedings of the VLDB Endowment, PVLDB, vol. 6, no. 11, pp. 10091020, 2013. [2] A. Eldawy and M. F. Mokbel, "A demonstration of spatialhadoop: An efficient mapreduce framework for spatial data," Proceedings of the VLDB Endowment, PVLDB, vol. 6, no. 12, pp. 12301233, 2013. [3] "Spark," https://spark.apache.org. [4] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauly, M. J. Franklin, S. Shenker, and I. Stoica, "Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing," in Proceedings of the USENIX Symposium on Networked Systems Design and Implementation, NSDI, 2012, pp. 1528. [5] J. Yu, J. Wu, and M. Sarwat, "Geosaprk: A cluster computing framework for processing large scale spatial data," in Proceedings of ACM SIGSPATIAL GIS, 2015. [6] "Open Geospatial Consortium," http://www.opengeospatial.org/. [7] G. Luo, J. F. Naughton, and C. J. Ellmann, "A non-blocking parallel spatial join algorithm," in Data Engineering, 2002. Proceedings. 18th International Conference on. IEEE, 2002, pp. 697705. [8] X. Zhou, D. J. Abel, and D. Truffet, "Data partitioning for parallel spatial join processing," Geoinformatica, vol. 2, no. 2, pp. 175204, 1998. [9] N. Roussopoulos, S. Kelley, and F. Vincent, "Nearest neighbor queries," in ACM sigmod record, vol. 24, no. 2. ACM, 1995, pp. 7179. [10] B. D. Ripley, Spatial statistics. John Wiley & Sons, 2005, vol. 575.

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/283123965

Methodsforderivingandcalibratingprivacypreservingheatmapsfrommobilesports trackingapplicationdata
ArticleinJournalofTransportGeographySeptember2015
DOI:10.1016/j.jtrangeo.2015.09.001

CITATIONS

READS

8
4authors,including: JuhaOksanen FinnishGeospatialResearchInstitute/Natio...
32PUBLICATIONS333CITATIONS
SEEPROFILE

38

CeciliaBergman FinnishGeospatialResearchInstitute
3PUBLICATIONS10CITATIONS
SEEPROFILE

AllcontentfollowingthispagewasuploadedbyJuhaOksanenon07January2016.

Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Journal of Transport Geography 48 (2015) 135144

Contents lists available at ScienceDirect

Journal of Transport Geography
journal homepage: www.elsevier.com/locate/jtrg

Methods for deriving and calibrating privacy-preserving heat maps from mobile sports tracking application data
Juha Oksanen a,, Cecilia Bergman a, Jani Sainio b, Jan Westerholm b
a b

Department of Geoinformatics and Cartography, Finnish Geospatial Research Institute/National Land Survey of Finland, P.O. Box 84, FI-00521 Helsinki, Finland Faculty of Science and Engineering, bo Akademi University, Joukahainengatan 3-5, FI-20520 Turku, Finland

a r t i c l e

i n f o

a b s t r a c t
Utilization of movement data from mobile sports tracking applications is affected by its inherent biases and sensitivity, which need to be understood when developing value-added services for, e.g., application users and city planners. We have developed a method for generating a privacy-preserving heat map with user diversity (ppDIV), in which the density of trajectories, as well as the diversity of users, is taken into account, thus preventing the bias effects caused by participation inequality. The method is applied to public cycling workouts and compared with privacy-preserving kernel density estimation (ppKDE) focusing only on the density of the recorded trajectories and privacy-preserving user count calculation (ppUCC), which is similar to the quadrat-count of individual application users. An awareness of privacy was introduced to all methods as a data pre-processing step following the principle of k-Anonymity. Calibration results for our heat maps using bicycle counting data gathered by the city of Helsinki are good (R2 N 0.7) and raise high expectations for utilizing heat maps in a city planning context. This is further supported by the diurnal distribution of the workouts indicating that, in addition to sports-oriented cyclists, many utilitarian cyclists are tracking their commutes. However, sports tracking data can only enrich official in-situ counts with its high spatio-temporal resolution and coverage, not replace them.  2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).

Article history: Received 19 December 2014 Received in revised form 3 September 2015 Accepted 5 September 2015 Available online 20 September 2015 Keywords: Cycling Location-based services (LBSs) Urban planning Privacy Big data GIS

1. Introduction Mobile sports tracking applications have become popular among the public audience, and a large number of smartphone users are willing to collect and compare their workouts privately, as well as to share their data within social networks or even publicly, for all application and Internet users. Key factors in this development have been the maturity of sensor technology, such as an accelerometer, digital compass, gyroscope, and GPS (e.g., Lane et al., 2010), available in nearly all recent mid- and top-range smartphones; and well-documented application programming interfaces for third-party developers to create new applications for mobile platforms. The aim of our work is to develop methods to enrich workout data from a mobile sports tracking application to create privacy-preserving information about the most popular places to do sports. While our case study focuses on public cycling workouts collected using Sports Tracker (http://www.sports-tracker.com/), the developed methods can be used for any other sports recorded using any mobile sports tracking application. The approach chosen for this study relies on visual data mining, which utilizes human perception in data exploration, and combines human flexibility, creativity, and knowledge with a computer's storage capacity, computing power, and visualization capabilities
 Corresponding author. Tel.: +358 40 831 4092. E-mail address: juha.oksanen@nls.fi (J. Oksanen).

(Keim, 2002). When integrated into a location-based service (LBS), the result of our analysis replies to the end-user's question "Where have most cyclists continued to from here?" In addition, we investigate the relation of tracking data and in-situ bicycle counting information in order to compare the quality of the derived heat maps, as well as to calibrate the heat maps based on mobile sports tracking application data, for example, for city planning purposes. We use the term "workout" throughout the paper to denote all recorded trajectories, be they recreational/exercise or utilitarian by purpose. The idea of generating heat maps from mobile sports app data to communicate the popularity of sports is not new (e.g., Garmin, 2013; Lin, 2012; Strava, 2014), but less attention has been paid to the methods for making the calculation, and concerns about the appropriate understanding of the heat maps have been raised due to new application areas of heat maps, such as city planning (Maus, 2014a) and analysis of eye-tracking data (Bojko, 2009). When creating heat maps, an obvious surrogate for the popularity of sports is the density of workout trajectories, but other surrogates, such as the number of different people doing sports, can also be used. According to the limited information available on existing heat maps, the one provided by Strava uses the number of GPS points as a pixel value (Mach, 2014), whereas in the heat map offered by Nike+, the value at each pixel represents the number of users (Lin, 2012). As we will show in this paper, the two methods can locally result in very different patterns of bike riding.

http://dx.doi.org/10.1016/j.jtrangeo.2015.09.001 0966-6923/ 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).

136

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135144

The use of heat maps as a representation of the intensity of a phenomenon has its roots in spectrometry (e.g., Moon et al., 2009) and the generation of isarithm and dot density maps (Slocum et al., 2009), but in the context of cartography, a textbook definition of a heat map is still missing (e.g., Trame and Keler, 2011). Heat maps are a common visualization technique in many fields of research where large amounts of data are handled. For example, in humancomputer interaction, "attention heat maps" are a popular tool of visual analysis of eyetracking data (e.g., Blascheck et al., 2014; Bojko, 2009). Related to studies utilizing the increasing volumes of volunteered geographic information (VGI; Goodchild, 2007), heat maps have been used, for example, to reveal attractive/popular places in a region using the density-based spatial clustering of geotagged images (Kisilevich et al., 2010; Kurata, 2012) and videos (Mirkovi et al., 2010), or to visualize spatio-temporal patterns revealed by the distribution of tweets (e.g., Morstatter et al., 2013; Zeile et al., 2012). The coloring of a heat map is typically selected in such way that the interpretation of the intensity differences becomes intuitive. This is expected to happen when warm colors, in terms of color temperature, are used for high intensities of the represented phenomenon and cool colors for low intensities (e.g., Spakov and Miniotas, 2007). In addition to the public audience, interest in mobile tracking applications, and enriching, especially, cycling data collected with them has emerged among city planners (Albergotti, 2014; Charlton et al., 2011; Hood et al., 2011). One of the biggest challenges in a city-planning context regarding non-motorized traffic is the lack of documentation on the usage and the demand figures for, for example, cyclists and pedestrians (Lindsey et al., 2014; NBPD, 2014). Traditional approaches for monitoring cycling traffic have been the use of surveys for qualitative results and different types of manual and automatic in-situ counting for quantitative results (Griffin et al., 2014; NBPD, 2014; Rantala and Luukkonen, 2014). Mobile tracking of cyclists has been seen as an attractive, inexpensive, and dynamic alternative to traditional bicycle data collection (Hudson et al., 2012). An early approach to tracking was the development of dedicated platforms, such as CycleTracks, which was developed at San Francisco Municipal Transportation Agency and has since been used at a number of agencies and municipalities in the US (Charlton et al., 2011; Masoner, 2014). In the UK, another crowdsourcing-based application, Cycle Hackney, is expected to provide a cost-effective way to find out where, especially, utilitarian cycling is taking place (CycleStreets, 2014). Recently, in the city of Oulu, Finland, there has been a development project aiming to create a "smoothness navigator" for cyclists, based on 1000 recorded tracks of people participating in the pilot phase (Poikola, 2014). The problem in dedicated tracking platforms appears to be the limited group of people interested in using them voluntarily (SFMTA, 2013). To overcome this problem the potential of utilizing mobile sports tracking data has been recognized, reflecting the idea of utilizing humans as sensors (Goodchild, 2007) and big data analytics (e.g., Russom, 2011). For example, Oregon's Department of Transportation paid $20,000 to use data from the mobile sports tracking application Strava for a year, containing 400,000 individual bicycle trips, totaling 8 million bicycle kilometers traveled (Estes, 2014; Maus, 2014b). While mobile sports tracking data may not qualify as `big data' regarding its volume  except in the sense "bigger than previously" (Goodchild, 2013)  it shares many characteristics with other social media data, often classified as big data. Many of the characteristics follow from the fact that big data is typically not collected with any specific purpose in mind or not used for its original purpose (Kitchin, 2014). In statistics, random sampling is used to guarantee the representativeness of observations, but in big data analytics, the `sample' is not randomly chosen at all (Goodchild, 2013). Rather, the aim is to use all the data following the principle of exhaustivity in scope (Harford, 2014; Kitchin, 2014). However, considering that only a small and possibly behaviorally biased subset of cyclists use mobile applications to track their routes, the question is how well they represent the whole population of cyclists

(e.g., Maus, 2014a; Rantala and Luukkonen, 2014); i.e., as social media data in general, sports tracking data is affected by self-selection bias (Shearmur, 2015). In addition, mobile tracking applications have their differences with respect to appearance and function, such as the available range of sports (multi-sports or single activity type), and may therefore attract different people. As an example, the cycling and running app Strava has the reputation of being used by more competitive or "serious" cyclists (Zahradnik, 2014) and is also targeting people who identify themselves as "athletes" (Strava, 2014). On the other hand, for example, Sports Tracker (ST, 2014) "want[s] to help people train better, connect through sports, and live healthier, happier lives;" HeiaHeia! focuses on the business-to-business sector and work welfare (Kauppalehti, 2013); and Endomondo (2014) is, in its own words, aiming "to motivate people to get and stay active." It has been estimated that 90% of the cyclists who use Strava are male (Usborne, 2013; Vanderbilt, 2013) and in 2012, 75% of all Endomondo users were men (Endoangela, 2012). Furthermore, participation inequality is a known property of VGI and online communities, according to which 90% of community members are followers and do not contribute to the community, whereas 9% contribute from time to time, and 1% account for most contributions (Nielsen, 2006). Although sports tracking applications do not today represent shared projects where people would track and share their workouts to promote the common good, some typical motivations for contribution in VGI, such as social reward and enhanced personal reputation (Coleman et al., 2009), can be identified within their communities as well. These bias issues introduce a major challenge in using mobile sports app data in a city-planning context. According to Westin's tenet, privacy is an individual's right to have full control over information about themselves, and to decide when, how, and to what extent this information is shared with others (Agrawal et al., 2002). Guaranteeing privacy in LBSs is extremely important, due to the unique characteristics of moving object data (Fung et al., 2010; Montjoye et al., 2013; Verykios et al., 2008). Topics such as anonymization of the original dataset (e.g., Monreale et al., 2010; Pensa et al., 2008), or de-identifying a given LBS-request location (e.g., Bettini et al., 2005; Gedik and Liu, 2004; Gruteser and Liu, 2004), have gained a great deal of attention in trajectory studies but are beyond the scope of this paper. Instead, we approach privacy-preservation from the standpoint of visualization. The idea behind preserving privacy in visualizations is to generalize or otherwise obfuscate data in such a manner that the disclosed data is still useful in the particular case (Andrienko et al., 2008; Fung et al., 2010). Various methods of geographical masking, first introduced by Armstrong et al. (1999), have been developed with the aim of protecting the confidentiality of individual locations by adding stochastic or deterministic noise to the geographic coordinates of the original data points without substantially affecting analytical results or the visual characteristics of the original pattern (Kwan et al., 2004). Spatial aggregation of individual-level data for administrative areas or other areal units that have a population greater than a chosen cutoff value is a common procedure of preserving confidentiality, for example, in censuses where disclosure control has long been an integral part of the process (Armstrong et al., 1999; Kwan et al., 2004; Leitner and Curtis, 2006; Young et al., 2009). Because aggregation can hide important spatial patterns in the data, various alternative geo-masking techniques, such as random perturbation and affine transformation (translate, rotate, and scale), have been introduced to preserve the disaggregated, discrete nature of the original data (Armstrong et al., 1999; Kwan et al., 2004). Although they have been used mainly with georeferenced, sensitive health- and crime-related point data (e.g., Leitner and Curtis, 2006; Kounadi and Leitner, 2015), Krumm (2007) and Seidl et al. (2015) have applied them also to GPS trajectory data. In this study, where it was crucial to prevent re-identification of an individual user and trajectory while providing the heat map viewer accurate information about popular cycling paths in their actual locations on the road network, geo-masking techniques as such were, however, not

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135144

137

adequate. Instead, we followed the principle of k-Anonymity in anonymization of the final heat maps. K-Anonymity was originally developed for record data by Samarati and Sweeney (1998), but has since been extended to spatio-temporal movement data by, for example, Abul et al. (2008), and Terrovitis and Mamoulis (2008). Related to visualization techniques, the method has been previously applied to privacy preservation for parallel coordinates (Dasgupta and Kosara, 2011). K-Anonymity refers to the requirement according to which "each release of data must be such that every combination of values of quasiidentifiers can be indistinctly matched to at least k individuals" (Samarati and Sweeney, 1998). By quasi-identifiers, they mean a set of attributes whose publication needs to be somehow restricted. When bringing the k-Anonymity requirement into the heat map context, our goal was that all details interpretable from the final heat map must be such that the result has been borne out from at least a minimum pre-defined number of application users. This paper presents a novel method for deriving privacy-preserving heat maps from mobile sports application data which takes into account the density of trajectories, as well as the diversity of users, attempting to avoid the bias caused by having few very active sports application users. As a secondary objective, the study presents a method by which heat maps derived from a mobile sports application's cycling data can be calibrated using in-situ field-collected bicycle counting data. With this approach, cities may use the calibrated heat maps as source information for city planning purposes. The remainder of the paper is organized as follows. In Section 2, we describe the data and methods used for deriving privacy-preserving heat maps. In Section 3, we compare the heat maps and discuss the characteristics of different methods, and we also show the relation between our heat maps and real-world in-situ bicycle counting data. Finally, in Section 4, conclusions are drawn and some future directions are pointed out. 2. Materials and methods The data used for our study was obtained from Sports Tracking Technologies Ltd., the inventor of the first mobile sports tracking application, Sports Tracker, for mobile phones (ST, 2014). While the app turns a smart phone into a sports computer, it is also a complete social platform on which application users share their workouts, photos, and experiences of exercises, with their group of friends or even with everyone. The basic unit of recorded data is a workout, which contains information about the user, sport (28 pre-classified sports, such as cycling, walking, and running), and a 4D (x, y, z, and t) coordinate list recorded at approximately 1-second intervals from the start to the finish of the workout. For this study, the data contained only public workouts from the region of Helsinki, and each user identifier was changed into a pseudo-ID at Sports Tracking Technologies Ltd. before the data delivery. The total sample data contained 192,597 workouts, of which a subset of 36,757 workouts collected by 2424 users represented cycling inside our approximately 320 km2 study region. Temporally continuous data was recorded between April 17th 2010 and November 21st 2012, and it consisted of a total of 36,663,190 GPS points. About 82% of the data contained valid time values accepted for further analysis. In a pre-processing phase, the data was systematically thinned to 10-second time intervals, the time data was transformed from POSIX time to Coordinated Universal Time (UTC + 02:00), and coordinates were converted from WGS84 (EPSG:4326) to ETRS-TM35FIN (EPSG:3067) (Anon, 2012). In addition, each point was supplied with information about distance, time, and speed from the previous tracked point. Further filtering of gross errors was done based on thresholds set for distance (b 300 m) and speed (b 50 m/s) between consecutive GPS observations. One characteristic of the data was that the number of people tracking and publishing a large number of workouts was small, and most of the users had tracked less than 10 workouts (Fig. 1), which appears to follow the principle of participation inequality (Nielsen, 2006). In the

Fig. 1. Number of tracked and published cycling workouts per user.

study area and within the time range, 65% of the users had published 5 or fewer workouts, and 87% had published 20 or fewer workouts. While the most active user had published more than 600 workouts, only 3% of the users had published more than 100 workouts. On average, for the sample data, each user had published 15 workouts. Further inspection of the data revealed cyclicity at a number of temporal scales. At an annual level, the popularity of cycling increased sharply during the summer season and peaked in August (Fig. 2a and b). In addition, the popularity of tracking cycling increased steadily from 2010 to 2012. When focusing on the weekly pattern, tracking of cycling was at its highest level on Tuesdays and Wednesdays, while the minimum level was reached on Fridays (Fig. 2c). Peaks in the frequencies of tracked points between 78 AM and 45 PM (Fig. 2d) reveal that many people use Sports Tracker to track their daily commuting trips. The use of pseudoIDs is not an adequate means for preserving the privacy of the application users since location alone may also reveal sensitive personal data (e.g., Bettini et al., 2005; Samarati and Sweeney, 1998). Privacy filtering of the tracked points was done in a preprocessing phase containing: 1) the generation of trajectories from all points, 2) the generation of a user count raster (number of different users on a 10 m grid within a 15 m search radius) from the trajectories, 3) the extraction of user count values at all points, and 4) the generation of trajectories from the subset of points with a user count value higher than a pre-defined threshold (here 5 users). By this method, we were able to filter privacy-preserving trajectories, which contained only the features that were covered by an adequate number of different users. From the filtered cycling trajectories we generated heat maps by a custom ArcGIS tool using three methods, namely privacy-preserving user count calculation (ppUCC), privacy-preserving kernel density estimation (ppKDE), and privacy-preserving kernel density estimation modified with the user diversity index (ppDIV). ppUCC was the simplest of the applied methods, in which the study region was partitioned into sub-regions of equal area and each area got its value, so-called quadrat counts (Bailey and Gatrell, 1995), from the number of users passing the pixel or a larger calculation window (20 m in our case). Thus, using ppUCC, we created a 2D histogram of individual cyclists within the study area. Kernel density estimation (KDE) is a family of methods originally developed to obtain smooth estimates of uni- or multivariate densities from observations (Bailey and Gatrell, 1995), but recently KDE has also been widely used to derive heat maps from data representing moving objects (e.g., Krisp and Peters, 2011; Willems, 2011). The commonly used kernel function is described by Silverman (1986): ppKDEs 
n s-s  1 X i ; K nh i1 h

where h is the width of the calculation window (bandwidth), n is the number of sample points, and K is the kernel function used for smoothing the estimate. Here, a quartic approximation of a Gaussian kernel (Silverman, 1986) was used. Sample points within the radius h are

138

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135144

Fig. 2. Frequencies of tracked points a) daily, b) monthly, c) according to weekday, and d) according to the hour of the day.

represented by si. When KDE is used for line features, such as trajectories, the result of KDE can be thought of as the result of placing a smooth kernel surface on top of the lines. The bandwidth was chosen as 25 m, to generalize the positioning uncertainty of GPS devices, but

simultaneously to preserve the details of the street network along which the cycling has occurred. In ppDIV the result of ppKDE was scaled with the diversity of users at each quadrat. Diversity is a descriptive statistic of a population with a

Fig. 3. Locations of the 89 bicycle counting sites in the city of Helsinki in June 2013 (Hellman, 2013) used in the study. The dashed circle represents a distance zone of 2.5 km from the city center. The map contains data from the Topographic database by the National Land Survey of Finland, 7/2012.

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135144

139

class structure (Junge, 1994). Diversity indices are used when we are interested in quantifying how many different classes there are in our data, and simultaneously how evenly the entities are distributed among those classes. The index used in this study is Simpson's Diversity Index D (McDonald and Dimmick, 2003): Ds  1- X ps2 i

ppDIVs  ppKDEs  Ds where pi is the proportion of user i's trajectories among all trajectories

in the quadrat s. This resulted in an index that was close to 1 when the user distribution was locally uniform, and close to 0 when skewness of the user distribution was locally high. Finally, the heat maps were compared with each other, as well as with the official 2013 bicycle counting data gathered by the city of Helsinki (Hellman, 2013). The challenge in a quantitative comparison of densities represented as heat maps based on different calculation methods is that the information varies in terms of the shape and scale of the density zones. Therefore, point sampling was done by extracting values of ppUCC, ppKDE, and ppDIV at the centroids of the Topographic database's (NLS, 2014) road segments from the Helsinki region for

Fig. 4. Heat maps based on (a) privacy-preserving user count calculation (ppUCC), (b) privacy-preserving kernel density estimation (ppKDE), and (c) privacy-preserving kernel density estimation modified with the user diversity index (ppDIV). Major differences between the methods are found in the highlighted regions A and B. The map contains data from the Topographic database by the National Land Survey of Finland, 7/2012;  OpenStreetMap contributors.

140

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135144

regression analysis purposes. The extraction was done by a nearest neighbor assignment for ppUCC, and by a bilinear interpolation for ppKDE and ppDIV. Manual bicycle counting was done at 94 counting points in early June 2013, during one weekday between 7 AM7 PM, and this was transformed to 24 h counts with a supply from semantically near automatic counting stations (Hellman, 2014). The precise locations of the counting sites were undocumented, but road segments used for the counting were published, and we adjusted the location of the 89 unambiguous points to correspond to the local maxima of the heat maps (Fig. 3). 3. Results and discussion In general, the three methods for deriving heat maps produced similar looking results, but when focusing on the details, some major differences could be found. In the simplest heat map based on ppUCC (Fig. 4a), patterns of the popularity of cycling in the region became clearly visible, but the problem appeared to be a lack of knowledge about the density of the trajectories. For example, in ppUCC, a road segment with 100 workouts recorded by 10 users got the same heat map score as a segment with 10 workouts recorded by 10 users. In ppKDE (Fig. 4b), the density of the trajectories became visible and the previously mentioned limitation of ppUCC was surpassed, but a single very active user may result in significant bias and overestimation of the route section's popularity (Fig. 4a, highlights A and B). In ppDIV (Fig. 4c), we took into account the density of the workout trajectories and the diversity of users, which resulted in a heat map that closely corresponded to ppKDE but that did not suffer from the bias introduced by very active application users. To further compare the methods, regression analysis between ppUCC, ppKDE, and ppDIV was performed on the point sample, based on the centroids of all road segments in the study region. In accordance with our expectations the coefficients of determination for regression models were high (Fig. 5), R2 = 0.830.94 with the highest coefficient being between ppKDE and ppDIV (Fig. 5c), revealing the similarity of the methods in general. Most often, the low number of users was associated with a small number of individual tracks, and a large number of users also indicated a large number of tracks. However, the most interesting features of the regression models were found from a spatial analysis of the residuals of the models. By residual analysis we could find answers to questions such as why a large number of tracks were not always related to a large number of users (Fig. 5a), and where were the places where the impact of taking the diversity of users into account in the calculation of densities of trajectories was the biggest (Fig. 5c). Observations below the regression curve in Fig. 5a revealed road segments where the density of trajectories (ppKDE) was lower than might have been expected according to the number of individual users (ppUCC). When plotting those parts of the observations on the map (Fig. 6a, blue dots), we could see that many of these road segments are important through-roads and entry cycling roads to downtown Helsinki. These also appeared to be routes not

favored by the most active cyclists, by whom we mean cyclists actively tracking their workouts. In the opposite case (Fig. 6a, red dots), the density of trajectories (ppKDE) was higher than predicted based on the number of individual users (ppUCC). These were found from the routes with very active cyclists, either because of the route's popularity among the enthusiastic sports cyclists or because of a very active user using the Sports Tracking application to track daily commuting trips. While the first group is interesting in terms of finding the most popular routes for cycling, the second group clearly displays bias, and it should be possible to filter it out from the results. In a similar manner, analyzing residuals from the regression model between ppKDE and ppDIV revealed the road segments where the diversity of the users had the biggest impact on ppKDE. Again, observations below the regression curve in Fig. 5c revealed road segments where a low diversity of users decreased the density value in the heat map (Fig. 6b, blue dots). In other words, they were the road segments where either a single cyclist or very few active cyclists had caused the largest positive bias in ppKDE. In the opposite case (Fig. 6b, red dots), a high diversity of users had resulted in a relative increase in ppKDE. These road segments appeared to be mostly the same important through-roads and entry cycling roads to downtown Helsinki, highlighted in blue in Fig. 6a. When comparing our heat maps to real-world data, all methods for deriving heat maps performed almost equally well (Fig. 7). Any of the heat maps can be used in predicting 24 h bicycle counting data, keeping in mind the coefficients of determination, R2 = 0.490.50 (p b 0.001). In practice this means that while, in many places, heat maps and realworld counting data had a clear connection, there are places where predictions based on heat maps fail. This may result at least partly from a temporal mismatch of the datasets and fundamental changes occurring in the cycling infrastructure. From the ten largest absolute residuals in the regression model between ppDIV and 24 h counting data (Fig. 7c, red and blue highlighting), we see that in 80% of the residuals, real-world 24 h counting data was greater compared to the predicted value based on ppDIV, and for only 20% the opposite was true. When these ten largest residuals were plotted on a map (Fig. 8), we noticed that the maximum (point 26) was located on Baana, a popular cycle path opened on June 12th 2012. Our data covered the time period from April 17th 2010 to November 21st 2012, which means that only the last 6 months of our data contained cyclists using Baana. A similar fundamental change in cycling infrastructure had taken place near measurement point 13 where the new cycle and pedestrian bridge Aurora was inaugurated in late 2012 (HS, 2012). The other large positive residuals could at least partly be explained by the overall increase in cycling (Hellman, 2013), which has been most significant on main entry cycleways to the city (points 7, 16, 21, 28, and 30). Together with the essential cycle-way through downtown Helsinki (point 27), these might also be locations where the difference between everyday cyclists and cyclists using Sports Tracker to record their workouts is largest. At points 7, 16, 30, and 28, manual bicycle counting data has been collected for opposite lanes, and the data was generalized to a single point. Using this method

Fig. 5. Regression models between (a) ppUCC and ppKDE, (b) ppUCC and ppDIV, and (c) ppKDE and ppDIV.

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135144

141

Fig. 6. Spatial distribution of maximum absolute residuals from the regression models between (a) ppUCC and ppKDE, and (b) ppKDE and ppDIV. The lowest 5th percentile of the residuals is represented with blue dots and the highest 5th percentile with red dots. The map contains data from the Topographic database by the National Land Survey of Finland, 7/2012.

to derive heat maps may be insensitive to the traffic on different lanes and underestimates the popularity indicating problems induced by the size of the calculation window. On the other hand, at point 8, the

manual counting data has been collected only from one side of the road, and a generalization of the result into a single point resulted in a large positive bias in prediction. The other negative residual (point

Fig. 7. Regression models between (a) ppKDE, (b) ppUCC, and (c) ppDIV and 24 h bicycle counting data 2013 from the city of Helsinki. In panel (c), the ten largest positive (red, + sign) and negative (blue,  sign) residuals from the regression model are highlighted. The same labels are used in the map in Fig. 8.

142

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135144

Fig. 8. The ten largest positive (red) and negative (blue) residuals from the regression model between ppDIV and 24 h bicycle counting data (black) in the city of Helsinki (Fig. 7c). The background heat map is based on ppDIV. The map contains data from the Topographic database by the National Land Survey of Finland, 7/2012.

4) might be explained by the construction site of Koivusaari metro station, which affected routes in 2013. When we compared the location of all the above-mentioned positive residuals to the distance zones in Helsinki (Fig. 3), they were all within a 2.5 km radius from the city center. Finally, when the observations resulting in the ten largest residuals were removed from the data (Fig. 9a), the coefficient of determination for predicting the number of cyclists based on ppDIV rose to R2 = 0.76 (p b 0.001). When focusing on the ten largest residuals, and by removing the points where significant changes in cycling infrastructure have occurred (points 13, 26, and 4) and where uncertainty due to a mismatch in in-situ counting and the heat map was the largest (point 8), we also got a high coefficient of determination, R2 = 0.72 (p = 0.15) (Fig. 9b). Furthermore, if the points for Tlnranta (point 21) and Kaisaniemenranta (point 30) were removed, the coefficient of determination rose to R2 = 0.96 (p = 0.20). This clearly indicates that, in our case study, a calibration of heat maps with the absolute number of cyclists outside a 2.5 km radius can be done with moderate accuracy, and calibration of the city center heat map would be best done as a separate processing step. To summarize the methods (Table 1), it appears that the advantages of ppUCC are the simplicity of the calculation and the intuitive quantity of the result, the number of different users per quadrant. In addition, ppUCC automatically reduces the impact of very active cyclists and therefore, for example, diminishes the bias caused by active commuters. The disadvantage of ppUCC is that it ignores the density of trajectories

and therefore brings, for example, mass sports events with a large number of individual athletes into the heat maps. In ppKDE, the density of trajectories is calculated using the well-established kernel density estimation, but individual active cyclists may introduce significant bias into the heat map. In addition, the unit of the heat map is not intuitive, even though in a visual interpretation of the results, the unit of the quantity is noncritical. A novel method introduced in this paper is ppDIV, which combines the best properties of the previously mentioned methods. It takes into account the density of the workout trajectories, as well as the diversity of the users who have tracked the workouts. Therefore, an individual athlete has no significant impact on the resulting heat map, and the computational biases of ppUCC and ppKDE are eliminated. A disadvantage of all the methods is that the results depend on the subjective definition of the size of the calculation window. This decision should be based, on one hand, on the positioning uncertainty of the workout trajectories, and on the other hand, on the desired level of detail of the final results.

4. Conclusions In this paper, we have introduced a privacy-preserving diversity method (ppDIV) for deriving heat maps from mobile sports tracking application data, which takes into account the density of the trajectories and the diversity of the users. The method was applied to Sports Tracker's public cycling workouts and compared with privacypreserving kernel density estimation (ppKDE) and privacy-preserving user count calculation (ppUCC). In addition, we demonstrated a method for calibrating the cycling heat map with in-situ bicycle counting data.
Table 1 Performance summary of the properties of the heat map generation methods (+ = poor, ++ = moderate, +++ = good). ppUCC Simplicity of calculation Sensitivity to density of trajectories Sensitivity to the number of individual users Sensitivity to diversity of users Intuitiveness of the measurement unit of the result Sensitivity to the size of the calculation window Ability to filter out very active users Ability to filter out mass sports events +++ + +++ + +++ + +++ + ppKDE ++ +++ + + + + + + ppDIV + +++ ++ +++ + + +++ +

Fig. 9. Regression models between ppDIV and 24 h bicycle counting data 2013 from the city of Helsinki, when (a) observations in the city center (shown in Fig. 8) have been removed and (b) when focusing only on the observations in the city center. In panel (b), the two largest positive (red, + sign) and negative (blue,  sign) residuals from the regression model are highlighted.

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135144

143

The results show that ppUCC, ppKDE, and ppDIV reveal different aspects of the popularity of cycling, and they all are sensitive in different ways to bias issues in the mobile sports tracking data. The order of superiority between them depends on the requirements set for the final result, but for general purposes, ppDIV offers the most neutral view on the popularity of cycling, and hides the bias issues related to, for example, very active application users. The final added-value application that would inspire and support visual reasoning with the aim of choosing attractive routes for cycling could be further improved by allowing the end-user to filter the heat map based on subjective preferences on, for example, speed and time of day or year. A balance between location privacy and data accuracy is an important but thus far largely uninvestigated topic in the context of heat maps. In our study area, k-Anonymity-based privacy preservation of the heat maps could be seen as a valid technique to guarantee the non-disclosure of individual users without diminishing the value of the service to the end-users. Still, an open question remains whether the value k should be a function of the density of the population or the underlying road network to protect the confidentiality of sensitive locations even in areas where there are small variations in the routes. A similar issue has been discussed in many geographical masking studies that have suggested weighting the displacement distance by population density because in less densely populated areas the risk of re-identification is higher (e.g., Armstrong and Ruggles, 2005; Kwan et al., 2004; Murad et al., 2014; Seidl et al., 2015). When considering the usability of our heat maps from a cityplanning perspective, the chosen approach for big data analytics appears to be promising. Despite the recognized bias issues, due to a selected group of cyclists tracking their workouts, as well as participation inequality, calibration results for our heat maps are surprisingly good. When doing regression modeling between in-situ bicycle counting data and ppDIV heat map scores, by splitting the data based on a 2.5 km distance from the city center, the coefficients of determination rise to R2 N 0.7. Most likely, the coefficients of determination would be even higher when bicycle counting data and the heat map are derived from the same period of time. This clearly shows the potential of utilizing heat maps in a city-planning context by using the in-situ bicycle counting data to get the absolute scale and the heat map for getting a high level of detail and a large spatial coverage of cycling activities. The use of big data from mobile sports apps also offers a chance for significant savings in the investments made in light traffic counting. In the long term this could significantly help to improve cycling infrastructure development and planning. Still, it is worth noting that big data analytics does not replace the need for in-situ bicycle counting since uncalibrated heat maps suffer from the bias issues caused by a behaviorally biased subset of cyclists using mobile applications to track their routes, as well as participation inequality. Acknowledgments We thank Sports Tracking Technologies Ltd. for the possibility to use their public workout data in our research, and Mrs. Susanne Suvanto for assistance in data processing. The study is a part of the research project SUPRA (Revolution of Location-Based Services: Embedded data refinement in Service Processes from Massive Geospatial Datasets) funded by Tekes, the Finnish Funding Agency for Innovation (grants 40261/12 and 40262/12). In addition, Oksanen's funding from the Academy of Finland (grant 251987) is gratefully acknowledged. References
Abul, O., Bonchi, F., Nanni, M., 2008. Never walk alone: uncertainty for anonymity in moving objects databases. Proceedings of the 24th IEEE International Conference on Data Engineering (ICDE'08), Cancun 712 April 2008, pp. 376385. Agrawal, R., Kiernan, J., Srikant, R., Xu, Y., 2002. Hippocratic databases. Proceedings of the 28th International Conference on Very Large Databases (VLDB'02), pp. 143154.

Albergotti, R., 2014. Strava, popular with cyclists and runners, wants to sell its data to urban planners. Digits - Tech News and Analysis from the Wall Street J. (http:// blogs.wsj.com/digits/2014/05/07/strava-popular-with-cyclists-and-runnerswants-to-sell-its-data-to-urban-planners/, accessed 29th July 2014). Andrienko, G., Andrienko, N., Kopanakis, I., Ligtenberg, A., Wrobel, S., 2008. Visual analytics methods for movement data. In: Giannotti, F., Pedreschi, D. (Eds.), Mobility, Data Mining and Privacy -- Geographic Knowledge Discovery. Springer, Berlin, pp. 375410. Anon, 2012. ETRS89 -jrjestelmn liittyvt karttaprojektiot, tasokoordinaatistot ja karttalehtijako (map projections, projected coordinate system, and map sheet division related to ETSR89, in Finnish). Julkishallinnon suosituksia (JHS) 154 (http:// www.jhs-suositukset.fi/suomi/jhs154 (accessed 15th December 2014)). Armstrong, M.P., Ruggles, A.J., 2005. Geographic information technologies and personal privacy. Cartographica 40 (4), 6373. http://dx.doi.org/10.3138/RU65-81R3-0W758V21. Armstrong, M.P., Rushton, G., Zimmerman, D.L., 1999. Geographically masking health data to preserve confidentiality. Stat. Med. 18 (5), 497525. http://dx.doi.org/10.1002/ (SICI)1097-0258(19990315)18:5b497::AID-SIM45N3.0.CO;2-#. Bailey, T.C., Gatrell, A.C., 1995. Interactive Spatial Data Analysis. Pearson Education, Harlow, UK. Bettini, C., Wang, X.S., Jajodia, S., 2005. Protecting privacy against location-based personal identification. In: Jonker, W., Petkovc, M. (Eds.), Proceedings of the 2nd VDLB international conference on Secure Data Management (SDM'05)Lecture Notes in Computer Science 3674. Springer, Berlin, pp. 185199. http://dx.doi.org/10.1007/11552338_ 13. Blascheck, T., Kurzhals, K., Raschke, M., Burch, M., Weiskopf, D., Ertl, T., 2014. State-of-theart of visualization for eye tracking data. In: Borgo, R., Maciejewski, R., Viola, I. (Eds.), State of the Art Report, Eurographics Conference on Visualization (EuroVis'14) (http://www.visus.uni-stuttgart.de/fileadmin/vis/pdf_s_fuer_Publikationen/State-ofthe-Art_of_Visualization_for_Eye_Tracking_Data.pdf, accessed 15th December 2014). Bojko, A., 2009. Informative or misleading? Heatmaps deconstructed. In: Jacko, J.A. (Ed.), HumanComputer Interaction International 2009, Part I. Lecture Notes in Computer Science 5610, pp. 3039. Charlton, B., Hood, J., Sall, E., Schwartz, M., 2011. Bicycle route choice data collection using GPS-enabled smartphones. Transportation Research Board 90th Annual Meeting, Washington DC, 2327 Jan 2011 (10 pp.). Coleman, D.J., Geogiadou, Y., Labonte, J., 2009. Volunteered geographic information: the nature and motivation of produsers. Int. J. Spat. Data Infrastruct. Res 4 (http://ijsdir. jrc.ec.europa.eu/index.php/ijsdir/article/view/140/198, accessed 25th June 2015). CycleStreets, 2014. Cycle Hackney app created by CycleStreets. http://www.cyclestreets. net/blog/2014/07/06/cycle-hackney-app/ (accessed 26th November 2014). Dasgupta, A., Kosara, R., 2011. Adaptive privacy-preserving visualization using parallel coordinates. IEEE Trans. Vis. Comput. Graph. 17 (12), 22412248. Endoangela, 2012. Popular Endomondo sports tracker mobile app hits 10 million user milestone and 320 million miles logged. http://blog.endomondo.com/2012/06/26/ popular-endomondo-sports-tracker-mobile-app-hits-10-million-user-milestoneand-320-million-miles-logged/ (accessed 7th August 2014). Endomondo, 2014. What we do. https://www.endomondo.com/about (accessed 7th August 2014). Estes, A.C., 2014. Why a fitness-tracking app is selling its data to city planners. GIZMODO. http://gizmodo.com/why-a-fitness-tracking-app-is-selling-its-data-to-city-1572964149 (accessed 2nd September 2014). Fung, B.C.M., Wang, K., Chen, R., Yu, P.S., 2010. Privacy-preserving data publishing: a survey of recent developments. ACM Comput. Surv. 42 (4). http://dx.doi.org/10. 1145/1749603.1749605 (Article 14). Garmin, 2013. Garmin connect is heating up with heat maps. http://garmin.blogs.com/ my_weblog/2013/03/garmin-connect-is-heating-up.html#.U6FbGbFABa4 (accessed 16th December 2014). Gedik, B., Liu, L., 2004. A customizable k-anonymity model for protecting location privacy. Technical Report GIT-CERCS-04-15. Georgia Institute of Technology (12 pp., https:// smartech.gatech.edu/xmlui/bitstream/handle/1853/100/git-cercs-04-15.pdf, accessed 14th October 2014). Goodchild, M.F., 2007. Citizens as sensors: the world of volunteered geography. GeoJournal 69, 211221. http://dx.doi.org/10.1007/s10708-007-9111-y. Goodchild, M.F., 2013. The quality of big (geo)data. Dialogues Hum. Geogr. 3 (3), 280284. http://dx.doi.org/10.1177/2043820613513392. Griffin, G., Nordback, K., Gtschi, T., Stolz, E., Kothuri, S., 2014. Monitoring bicyclist and pedestrian travel and behavior -- current research and practice. Transportation Research Circular E-C183 (32 pp., http://onlinepubs.trb.org/onlinepubs/circulars/ ec183.pdf, accessed 26th November 2014). Gruteser, M., Liu, X., 2004. Protecting privacy in continuous location-tracking applications. IEEE Secur. Priv. 2 (2), 2834. Harford, T., 2014. Big data: are we making a big mistake? The Financial Times. http:// www.ft.com/cms/s/2/21a6e7d8-b479-11e3-a09a-00144feabdc0. html#axzz3GqbLWgPw (accessed 22nd October 2014). Hellman, T., 2013. Polkupyrlaskennat Helsingiss 2013 (bicycle counting in Helsinki 2013, in Finnish). Memorandum 28th October 2013. City of Helsinki. City Planning Department, Transportation and Traffic Planning Division (http://www.hel.fi/hel2/ ksv/Aineistot/Liikennesuunnittelu/Liikennetutkimus/pyoralaskennat_2013.pdf, accessed 2nd September 2014). Hellman, T., 2014. Personal Communication, June 13th 2014. Hood, J., Sall, E., Charlton, B., 2011. A GPS-based bicycle route choice model for San Francisco, California. Transp. Lett. 3, 6375. HS, 2012. Neljn miljoonan euron Auroransilta avautui vihdoin ulkoilijoille (four million euro Aurora's bridge finally open for citizens, in Finnish). Helsingin Sanomat, 24th November 2012. http://www.hs.fi/kaupunki/a1305621575898 (accessed 12th November 2014).

144

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135144 in Geographic Information Collection and Analysis (GeoPrivacy'14), ACM, November 47, 2014, Dallas/Fort Worth, TX, USA http://dx.doi.org/10.1145/2675682.2676399. NBPD, 2014. National bicycle and pedestrian documentation project. Alta Planning & Design. Institute of Transportation Engineers (ITE) Pedestrian and Bicycle Council (http://bikepeddocumentation.org/, accessed 12th November 2014). Nielsen, J., 2006. The 90-9-1 Rule for Participation Inequality in Social Media and Online Communities. Nielsen Normal Group. (http://www.nngroup.com/articles/ participation-inequality/, accessed 10th October 2014). NLS, 2014. The Topographic database. National Land Survey of Finland. http://www. maanmittauslaitos.fi/en/digituotteet/topographic-database (accessed 14th October 2014). Pensa, R.G., Monreale, A., Monreale, A., Pinelli, F., Pedreschi, D., 2008. Pattern-preserving k-anonymization of sequences and its application to mobility data mining. In: Bettini, C., Jajodia, S., Samarati, P., Wang, X.S. (Eds.), Proceedings of the 1st International Workshop on Privacy in Location-Based Applications (PiLBA '08), Malaga, Spain, October 9, 2008 (http://ceur-ws.org/Vol-397/paper4.pdf, accessed 16th December 2014). Poikola, A., 2014. Sujuvuusnavigaattorin pilotointi (piloting of the smoothness navigator). Open knowledge Finland. http://bit.ly/sujuvuusnavi_kalvot (accessed 12th November 2014). Rantala, T., Luukkonen, T., 2014. Bicycle and Pedestrian Traffic Monitoring -- Guide to Creating an Indicator Toolbox. Finnish Transport Agency, Planning Department, Helsinki (37 pages, 2 appendices. http://www2.liikennevirasto.fi/julkaisut/pdf8/lts_ 2014-15_kavelyn_pyorailyn_web.pdf, accessed 30th September 2014). Russom, P., 2011. Big data analytics. TDWI Best Practices Report, Fourth Quarter 2011 (35 pp., http://tdwi.org/research/2011/12/sas_best-practices-report-q4-big-dataanalytics.aspx?tc=page0, accessed 14th October 2014). Samarati, P., Sweeney, L., 1998. Protecting privacy when disclosing information: k-anonymity and its enforcement through generalization and suppression. Proceedings of the IEEE Symposium on Research in Security and Privacy (S&P). May 1998, Oakland, CA, pp. 384393 (http://dataprivacylab.org/dataprivacy/projects/ kanonymity/paper3.pdf, accessed 16th December 2014). Seidl, D., Jankowski, P., Tsou, M.-H., 2015. Spatial obfuscation of GPS travel data. CSU Geospatial Review vol. 13 (http://csugis.sfsu.edu/CSU_Geospatial_Review/2015.pdf (accessed 25th June 2015)). SFMTA, 2013. CycleTracks for iPhone and Android. San Francisco County Transport Authority. http://www.sfcta.org/modeling-and-travel-forecasting/cycletracksiphone-and-android (accessed 2nd September 2014). Shearmur, R., 2015. Dazzled by data: big data, the census and urban geography. Urban Geogr. http://dx.doi.org/10.1080/02723638.2015.1050922. Silverman, B.W., 1986. Density Estimation for Statistics and Data Analysis. Chapman & Hall, London. Slocum, T.A., McMaster, R.B., Kessler, F.C., Howard, H.H., 2009. Thematic Cartography and Geovisualisation. Prentice Hall, New Jersey, NJ. Spakov, O., Miniotas, D., 2007. Visualization of eye gaze data using heat maps. Electron. Electr. Eng. 2, 5558. ST, 2014. Introducing Sports Tracker. Sports Tracking Technologies Ltd. (http://www. sports-tracker.com/blog/about/, accessed 20th August 2014). Strava, 2014. About us. Strava Inc. (http://www.strava.com/about, accessed 7th August 2014). Terrovitis, M., Mamoulis, N., 2008. Privacy preservation in the publication of trajectories. Proceedings of the 9th International Conference on Mobile Data Management (MDM '08), 2730 April 2008, pp. 6572. http://dx.doi.org/10.1109/MDM.2008.29. Trame, J., Keler, C., 2011. Exploring the lineage of volunteered geographic information with heat maps. Abstracts of GeoViz 2011, Hamburg, Germany, March 1011, 2011. http://www.geomatik-hamburg.de/geoviz11/abstracts/28_TrameKessler_Abstract_ GeoViz2011.pdf (accessed 12th November 2014). Usborne, S., 2013. Can cycling app Strava change the way we ride? The Independent, 4th July 2013. http://www.independent.co.uk/life-style/gadgets-and-tech/features/cancycling-app-strava-change-the-way-we-ride-8685996.html (accessed 7th August 2014). Vanderbilt, T., 2013. How Strava is changing the way we ride. Outside Magazine, January 2013. http://www.outsideonline.com/fitness/biking/How-Strava-Is-Changing-theWay-We-Ride.html (accessed 7th August 2014). Verykios, V.S., Damiani, M.L., Gkoulalas-Divanis, A., 2008. Privacy and security in spatiotemporal data and trajectories. In: Giannotti, F., Pedreschi, D. (Eds.), Mobility, Data Mining and Privacy, pp. 213240. Willems, C.M.E., 2011. Visualization of vessel traffic (PhD Thesis) Eindhoven University of Technology. Young, C., Martin, D., Skinner, C., 2009. Geographically intelligent disclosure control for flexible aggregation of census data. Int. J. Geogr. Inf. Sci. 23 (4), 457482 (http://dx. doi.org/10.1080/13658810801949835). Zahradnik, F., 2014. Strava Cycling app with unique social and record-keeping features. About.com. http://gps.about.com/od/sportsandfitness/fr/Strava-Cycling-App-Review. htm (accessed 7th August 2014). Zeile, P., Memmel, M., Exner, J.-P., 2012. A new urban sensing and monitoring approach: tagging the city with the RADAR SENSING app. Proceedings REAL CORP 2012 Tagungsband 1416 May 2012, Schwechat (http://programm.corp.at/cdrom2012/ papers2012/CORP2012_104.pdf (26th September 2014)).

Hudson, J.G., Duthie, J.C., Rathod, Y.K., Larsen, K.A., Meyer, J.L., 2012. Using smartphones to collect bicycle travel data in Texas. Final Report of the UTCM Project #11-35-69 (http://utcm.tamu.edu/publications/final_reports/Hudson_11-35-69.pdf, accessed 26th November 2014). Junge, K., 1994. Diversity of ideas about diversity measurement. Scand. J. Psychol. 35 (1), 1626. http://dx.doi.org/10.1111/j.1467-9450.1994.tb00929.x. Kauppalehti, 2013. Suomalainen liikuntasovellus: 100 yrityst 12 maassa (Finnish sports application: 100 companies in 12 countries, in Finnish). Kauppalehti 17th March 2013. http://www.kauppalehti.fi/omayritys/suomalainen+liikuntasovellus+100+ yritysta+12+maassa/201303381093 (accessed 16th December 2014). Keim, D.A., 2002. Information visualization and visual data mining. IEEE Trans. Vis. Comput. Graph. 7 (1), 100107. Kisilevich, S., Mansmann, F., Bak, P., Keim, D., 2010. Where would you go on your next vacation? A framework for visual exploration of attractive places. Second International Conference on Advanced Geographic Information Systems, Applications, and Services (GEOPROCESSING), 2010, pp. 2126. http://dx.doi.org/10.1109/GEOProcessing. 2010.11. Kitchin, R., 2014. Big data, new epistemologies and paradigm shifts. Big Data Soc. 1 (1). http://dx.doi.org/10.1177/2053951714528481. Kounadi, O., Leitner, M., 2015. Defining a threshold value for maximum spatial information loss of masked geo-data. ISPRS Int. J. Geo-Inf. 4 (2), 572590. http://dx.doi.org/ 10.3390/ijgi4020572. Krisp, J.M., Peters, S., 2011. Directed kernel density estimation (DKDE) for time series visualization. Ann. GIS 17 (3), 155162. http://dx.doi.org/10.1080/19475683.2011. 602218. Krumm, J., 2007. Inference attacks on location tracks. In: LaMarca, A., Langheinrich, M., Truong, K.N. (Eds.), Proceedings of the 5th International Conference on Pervasive Computing (Pervasive'07)Lecture Notes in Computer Science 4480. Springer, Berlin, pp. 127143. http://dx.doi.org/10.1007/978-3-540-72037-9_8. Kurata, Y., 2012. Potential-of-interest maps for mobile tourist information services. In: Fuchs, M., Ricci, F., Cantoni, L. (Eds.), Information and Communication Technologies in Tourism 2012. Springer, Vienna, pp. 239248. http://dx.doi.org/10.1007/978-37091-1142-0_21. Kwan, M.P., Casas, I., Schmitz, B., 2004. Protection of geoprivacy and accuracy of spatial information: how effective are geographical masks? Cartographica 39 (2), 1528. http://dx.doi.org/10.3138/X204-4223-57MK-8273. Lane, N.D., Miluzzo, E., Lu, H., Peebles, D., Choudhury, T., Campbell, A.T., 2010. A Survey of mobile phone sensing. IEEE Commun. Mag. 140150 September. Leitner, M., Curtis, A., 2006. A first step towards a framework for presenting the location of confidential point data on maps--results of an empirical perceptual study. Int. J. Geogr. Inf. Sci. 20 (7), 813822. http://dx.doi.org/10.1080/13658810600711261. Lin, D., 2012. How is Nike+ Heat Map calculated? http://howtonike.blogspot.fi/2012/06/ how-is-nike-heat-map-calculated.html (accessed 16th December 2014) Lindsey, G., Nordback, K., Figliozzi, M.A., 2014. Institutionalizing bicycle and pedestrian monitoring programs in three states: progress and challenges. 93rd Annual Meeting of the Transportation Research Board, Washington, DC, January 1216. Mach, P., 2014. What do 220,000,000,000 GPS data points look like? http://engineering. strava.com/global-heatmap/ (accessed 20th August 2014) Masoner, R., 2014. Santa Cruz County: log your bike rides for transportation planning. http://www.cyclelicio.us/2014/santa-cruz-county-log-your-bike-rides-fortransportation-planning/ (accessed 16th December 2014). Maus, J., 2014a. A closer look at Strava's `heat map' for the Portland region. BikePortland.org (Blog Post) (http://bikeportland.org/2014/04/30/a-closer-look-atstravas-heat-map-for-the-portland-region-105280, accessed 29th July 2014). Maus, J., 2014b. ODOT embarks on "big data" project with purchase of Strava dataset. BikePortland.org (Blog Post) (http://bikeportland.org/2014/05/01/odot-embarkson-big-data-project-with-purchase-of-strava-dataset-105375, accessed 29th July 2014). McDonald, D.G., Dimmick, J., 2003. The conceptualization and measurement of diversity. Commun. Res. 30 (1), 6079. http://dx.doi.org/10.1177/0093650202239026. Mirkovi, M., Aulibrk, D., Milisavljevi, S., Crnojevi, V., 2010. Detecting attractive locations using publicly available user-generated video content -- central Serbia case study. Proceedings of 18th Telecommunications Forum (TELFOR'10), Serbia, Belgrade, November 2325, 2010, pp. 10891092. Monreale, A., Andrienko, G., Andrienko, N., Giannotti, F., Pedreschi, D., Rinzivillo, S., Wrobel, S., 2010. Movement data anonymity through generalization. Trans. Data Privacy 3, 91121 (http://www.tdp.cat/issues/tdp.a045a10.pdf, accessed 15th December 2014). Montjoye, Y.-A., Hidalgo, C.A., Verleysen, M., Blondel, V.D., 2013. Unique in the crowd: the privacy bounds of human mobility. Sci. Rep. 3. http://dx.doi.org/10.1038/srep01376. Moon, J.-Y., Jung, H.-J., Hee Moon, M., Chul Chung, B., Ho Choi, M., 2009. Heat-map visualization of gas chromatography-mass spectrometry based quantitative signatures on steroid metabolism. J. Am. Soc. Mass Spectrom. 20 (9), 16261637. http://dx.doi. org/10.1016/j.jasms.2009.04.020. Morstatter, F., Kumar, S., Liu, H., Maciejewski, R., 2013. Understanding Twitter data with TweetXplorer. Proceedings of the 2013 ACM SIG KDD International Conference on Knowledge Discovery and Data Mining (KDD'13), ACM, August 1114, 2013, Chicago, IL, USA, pp. 14821485 http://dx.doi.org/10.1145/2487575.2487703. Murad, A., Hilton, B., Horan, T., Tangenberg, J., 2014. Protecting patient geo-privacy via a triangular displacement geo-masking method. In: Kessler, C., McKenzie, G.D., Kulik, L. (Eds.), Proceedings of the 1st ACM SIGSPATIAL International Workshop on Privacy

View publication stats

Geoinformatica (2013) 17:417–448
DOI 10.1007/s10707-012-0164-9

Generic and efficient framework for search trees
on flash memory storage systems
Mohamed Sarwat · Mohamed F. Mokbel ·
Xun Zhou · Suman Nath

Received: 16 February 2012 / Revised: 27 June 2012 /
Accepted: 12 July 2012 / Published online: 30 August 2012
© Springer Science+Business Media, LLC 2012

Abstract Tree index structures are crucial components in data management systems.
Existing tree index structure are designed with the implicit assumption that the
underlying external memory storage is the conventional magnetic hard disk drives.
This assumption is going to be invalid soon, as flash memory storage is increasingly
adopted as the main storage media in mobile devices, digital cameras, embedded
sensors, and notebooks. Though it is direct and simple to port existing tree index
structures on the flash memory storage, that direct approach does not consider the
unique characteristics of flash memory, i.e., slow write operations, and erase-beforeupdate property, which would result in a sub optimal performance. In this paper, we
introduce FAST (i.e., Flash-Aware Search Trees) as a generic framework for flashaware tree index structures. FAST distinguishes itself from all previous attempts of
flash memory indexing in two aspects: (1) FAST is a generic framework that can
be applied to a wide class of data partitioning tree structures including R-tree and
its variants, and (2) FAST achieves both ef f iciency and durability of read and write
flash operations through memory flushing and crash recovery techniques. Extensive
experimental results, based on an actual implementation of FAST inside the GiST

The research of M. Sarwat and M. F. Mokbel is supported in part by the National Science
Foundation under Grants IIS-0811998, IIS-0811935, CNS-0708604, IIS-0952977,
by a Microsoft Research Gift, and by a seed grant from UMN DTC.
M. Sarwat ( ) · M. F. Mokbel · X. Zhou
Department of Computer Science and Engineering, University of Minnesota - Twin Cities,
200 SE Union Street, Minneapolis, MN 55455, USA
e-mail: sarwat@cs.umn.edu
M. F. Mokbel
e-mail: mokbel@cs.umn.edu
X. Zhou
e-mail: xun@cs.umn.edu
S. Nath
Microsoft Research, One Microsoft Way - Redmond, Redmond, WA 98052, USA
e-mail: sumann@microsoft.com

418

Geoinformatica (2013) 17:417–448

index structure in PostgreSQL, show that FAST achieves better performance than
its competitors.
Keywords Flash memory · Tree · Spatial · Index structure · Storage ·
Multi-dimensional · Data · System

1 Introduction
Data partitioning tree index structures are crucial components in spatial data
management systems, as they are mainly used for efficient spatial data retrieval,
hence boosting up query performance. The most common examples of such index
structures include B-tree [4], with its variants [10, 27], for one-dimensional indexing,
and R-tree [14], with its variants [5, 17, 32, 34], for multi-dimensional indexing. Data
partitioning tree index structures are designed with the implicit assumption that the
underlying external memory storage is the conventional magnetic hard disk drives,
and thus has to account for the mechanical disk movement and its seek and rotational
delay costs. This assumption is going to be invalid soon, as flash memory storage is
expected to soon prevail in the storage market replacing the magnetic hard disks
for many applications [11, 12, 31]. Flash memory storage is increasingly adopted as
the main storage media in mobile devices and as a storage alternative in laptops,
desktops, and enterprise class servers (e.g., in forms of SSDs) [3, 21, 23, 28, 33].
Recently, several data-intensive applications have started using custom flash cards
(e.g., ReMix [19]) with large capacity and access to underlying raw flash chips. Such
a popularity of flash is mainly due to its superior characteristics that include smaller
size, lighter weight, lower power consumption, shock resistance, lower noise, and
faster read performance [16, 18, 20, 22, 29].
Flash memory is block-oriented, i.e., pages are clustered into a set of blocks. Thus,
it has fundamentally different characteristics, compared to the conventional pageoriented magnetic disks, especially for the write operations. First, write operations in
flash are slower than read operations. Second, random writes are substantially slower
than sequential writes. In devices that allow direct access to flash chips (e.g., ReMix
[19]), a random write operation updates the contents of an already written part of the
block, which requires an expensive block erase operation,1 followed by a sequential
write operation on the erased block; an operation termed as erase-before-update [7,
20]. SSDs, which emulate a disk-like interface with a Flash Translation Layer (FTL),
also need to internally address flash’s erase-before-update property with logging and
garbage collection, and hence random writes, especially small random writes, are
significantly slower than sequential writes in almost all SSDs [7].
Though it is direct and simple to port existing tree index structures (e.g., R-tree
and B-tree) on FTL-equipped flash devices (e.g., SSDs), that direct approach does
not consider the unique characteristics of flash memory and therefore would result
in a sub-optimal performance due to the random writes encountered by these index
structures. To remedy this situation, several approaches have been proposed for

a typical flash memory, the cost of read, write, and erase operations are 25, 200 and 1,500 µs,
respectively [3].

1 In

Geoinformatica (2013) 17:417–448

419

flash-aware index structures that either focus on a specific index structure, and make
it a flash-aware, e.g., flash-aware B-tree [30, 36] and R-tree [35], or design brand new
index structures specific to the flash storage [2, 24, 25].
Unfortunately, previous works on flash-aware search trees suffer from two
major limitations. First, these trees are specialized—they are not flexible enough
to support new data types or new ways of partitioning and searching data. For
example, FlashDB [30], which is designed to use a B-Tree, does not support R-Tree
functionalities. RFTL [35] is designed to work with R-tree, and does not support
B-tree functionalities. Thus, if a system needs to support many applications with
diverse data partitioning and searching requirements, it needs to have multiple tree
data structures. The effort required to implement and maintain multiple such data
structures is high.
Second, existing flash-aware designs often show trade-offs between efficiency and
durability. Many designs sacrifice strict durability guarantee to achieve efficiency
[24, 25, 30, 35, 36]. They buffer updates in memory and flush them in batches to
amortize the cost of random writes. Such buffering poses the risk that in-memory
updates may be lost if the system crashes. On the other hand, several designs achieve
strict durability by writing (in a sequential log) all updates to flash [2]. However, this
increases the cost of search for many log entries that need to be read from flash
in order to access each tree node [30]. In summary, no existing flash-aware tree
structure achieves both strict durability and efficiency.
In this paper, we address the above two limitations by introducing FAST; a
framework for Flash-Aware Search Tree index structures. FAST distinguishes itself
from all previous flash-aware approaches in two main aspects: (1) Rather than
focusing on a specific index structure or building a new index structure, FAST is
a generic framework that can be applied to a wide variety of tree index structures,
including B-tree, R-tree along with their variants. Such an important property makes
FAST a very attractive solution to database industry as it is practical to port it inside
the database engine with minimal disturbance to the engine code. (2) FAST achieves
both efficiency and durability in the same design. For efficiency, FAST buffers all
the incoming updates in memory while employing an intelligent f lushing policy that
evicts selected updates from memory to minimize the cost of writing to the flash
storage. In the mean time, FAST guarantees durability by sequentially logging each
in-memory update and by employing an efficient crash recovery technique.
FAST mainly has four modules, update, search, f lushing, and recovery. The update
module is responsible on buffering incoming tree updates in an in-memory data
structure, while writing small entries sequentially in a designated flash-resident log
file. The search module retrieves requested data from the flash storage and updates
it with recent updates stored in memory, if any. The f lushing module is triggered
once the memory is full and is responsible on evicting flash blocks from memory to
the flash storage to give space for incoming updates. Finally, the recovery module
ensures the durability of in-memory updates in case of a system crash.
FAST is a generic system approach that neither changes the structure of tree
indexes it is applied to, nor changes the search, insert, delete, or update algorithms
of these indexes. FAST only changes the way these algorithms reads, or updates the
tree nodes in order to make the index structure flash-aware. We have implemented
FAST within the GiST framework [15] inside PostgrSQL. As GiST is a generalized
index structure, FAST can support any tree index structure that GiST is supporting,

420

Geoinformatica (2013) 17:417–448

including one-dimensional tree index structures (e.g., B-tree [4]) and including but
not restricted to R-tree [14], R*-tree [5], SS-tree [34], and SR-tree [17], as well as Btree and its variants. In summary, the contributions of this paper can be summarized
as follows:
–
–

–

We introduce FAST; a general framework that adapts existing tree index structures to consider and exploit the unique properties of the flash memory storage.
We show how to achieve efficiency and durability in the same design. For
efficiency, we introduce two f lushing policies that smartly select parts of the main
memory buffer to be flushed into the flash storage in a way that amortizes expensive random write operations. We also introduce a crash recovery technique that
ensures the durability of update transactions in case of system crash.
We give experimental evidence for generality, efficiency, and durability of FAST
framework when applied to different data partitioning tree index structures.

The rest of the paper is organized as follows: An overview of Flash Memory
storage is given in Section 2. Section 3 highlights related work to FAST. Section 4
gives an overview of FAST along with its data structure. The four modules of
FAST, namely, update, search, f lushing, and recovery are discussed in Sections 5–
8, respectively. Section 9 gives experimental results. Finally, Section 10 concludes
the paper.

2 Flash memory storage overview
Figure 1 gives an overview of a typical flash memory storage device. In flash memory,
data is stored in an array of flash blocks. Each block contains ≈64–128 pages,
where a page is the smallest unit of access. Flash memory supports three types of
operations: read, write, and erase. The Erase operation is the most expensive one
where it can only be done at the block level and results in setting of all bits within
a block to ones. Read is a low latency page level operation and can occur randomly
anywhere in the flash memory without incurring any additional cost. Write is also a
page level operation and can be performed only once a page has been previously
erased since it sets the required bits to zeros. In that sense, writing on a previously
erased block is a low latency operation, and termed as a sequential write, while
writing on an already written block will result in a block erase operation before the
actual write operation, and thus, would incur much higher cost. Current generation
flash memory-based storage devices have varying access latencies for each of these
operations. On average, compared to read operations, write operations are eight
times slower, while erase operations are 60 times slower [6]. The typical access
latencies for read, write, and erase operations in flash memory devices are 25, 200
and 1,500 µs, respectively [3].
The Flash Translation layer (FTL) [26] is a layer on top of NAND flash memory
that makes the flash memory device acts like a virtual disk. The FTL layer receives
read and write commands for logical pages addresses from the application layer and
converts them to the internal flash memory commands (i.e., read page, write page,
erase block) on physical pages/blocks addresses. To emulate disk like in-place update
operation for a logical page Plogical , the FTL writes data into a new physical page
Pphysical, maintains a mapping between logical pages and physical pages, and marks

Geoinformatica (2013) 17:417–448
Fig. 1 Flash memory storage.
Grey rectangles represent
pages that are contained in
blocks represented by dotted
rectangles

421
Logical Page Read

Logical Page Write

Flash Translation Layer (FTL)
Page Read

Block Erase

Page Write

NAND Flash Memory

the previous physical location of Pphysical as invalid for future garbage collection.
Even though FTL allows existing disk based applications to use flash memory
without any modiÞcations, it needs to internally deal with flash physical constraint
of erasing a block before updating a page in that block. Besides this asymmetric read
and write latency issue, a flash memory block can only be erased for a limited number
of times (e.g., 105 –106 ), after which it acts like a read-only device [3]. FTL employs
various wear-leveling techniques to even out the erase counts of different blocks in
the flash memory to increase its longevity [8]. However, still early wear-out of flash
memory is one of the big concerns in widely deploying flash memory storage devices
[31], and thus, it is of essence that flash memory avoids block erases as much as
possible. Recent studies show that current FTL schemes are very effective for the
workloads with sequential access write patterns. However, for the workloads with
random access patterns, these schemes show very poor performance [9]

3 Related work
Previous approaches for flash-aware index structures can be classified into two categories: (1) Making an existing specif ic index structure flash-aware, which includes
flash-aware B-tree (e.g., FlashDB [30] and BFTL [36]) and flash-aware R-tree (e.g.,
RFTL [35]). The main idea of these index structures is to save the B-tree (R-tree)
operations in a reservation buffer residing on main memory. When the reservation
buffer is full, its content is totally flushed to flash memory. For instance, BFTL and
RFTL are adding a buffering layer on top of the flash translation layer in order
to make B-trees work efficiently on flash devices. (2) Designing brand new onedimensional index structures specific to the flash storage, e.g., the LA-tree [2] and the
FD-tree [24, 25]. LA-tree is flash friendly index structure that is intended to replace
the B-tree. LA-tree stores the updates in cascaded buffers residing on flash memory
and, then empties these buffers dynamically based on the operations workload. FDtree is also a one-dimensional index structure that allows small random writes to

422

Geoinformatica (2013) 17:417–448

occur only in a small portion of the tree called the head tree which exists at the
top level of the tree. When the capacity of the head tree is exceeded, its entries are
merged in batches to subsequent tree levels.
In terms of the performance-durability trade-off, previous approaches either:
(a) achieve efficiency, yet sacrifice durability, by buffering updates in main memory
and flush them in batches to flash memory to amortize the cost of random writes
[24, 25, 30, 35, 36]. However, storing updates in memory without taking into account
system failures, which leads to durability issue, where in-memory updates may be
lost if the system crashes, or (b) achieve durability, yet sacrifice efficiency, by writing
all the recent updates in a sequential log file [2], hence retrieving the updates from
the log file in case of a system crash. However, doing this increases the cost of search
for many log entries that need to be read from flash in order to access each tree node
with search and update operations [30].
FAST distinguishes itself from all previous techniques in three main aspects:
(1) FAST is a general framework for data-partitioning tree index structures built
inside GiST [15]. As GiST is a generalized index structure that can instantiate a wide
set of data-partitioning trees that include B-tree [4], R-tree [14], R*-tree [5], SS-tree
[34], and SR-tree [17]), FAST can support any tree that GiST is supporting. (2) FAST
ensures both the ef f iciency and durability of system transactions where updates are
buffered in memory, yet, an efficient crash recovery technique is triggered in case of
a system crash to ensure the durability. (3) FAST is not a brand new index structure,
hence does not need to replace existing tree indexes. However, it complements the
existing tree index structures in database management systems to make them work
efficiently on flash storage devices, with much less implementation cost.

4 Fast system overview
Figure 2 gives an overview of FAST. The original tree is stored on persistent flash
memory storage while recent updates are stored in an in-memory buffer. Both parts
need to be combined together to get the most recent version of the tree structure.
FAST has four main modules, depicted in bold rectangles, namely, update, search,
flushing, and crash recovery. FAST is optimized for both SSDs and raw flash devices.
SSDs are the dominant flash device for large database applications. On the other
hand, raw flash chips, which are dominant in embedded systems and custom flash
cards (e.g., ReMix [19]), are getting popular for data-intensive applications.
4.1 FAST modules
In this section, we explain FAST system architecture, along with its four main
modules; (1) Update, (2) Search, (3) Flushing, and (4) Crash recovery. The actions of
these four modules are triggered through three main events, namely, search queries,
data updates, and system restart.
Update module Similar to some of the previous research for indexing in flash
memory, FAST buffers its recent updates in memory, and flushes them later, in
bulk, to the persistent flash storage. However, FAST update module distinguishes
itself from previous research in two main aspects: (1) FAST does not store the

Geoinformatica (2013) 17:417–448

423

Fig. 2 FAST system architecture

update operations in memory, instead, it stores the results of the update operations in
memory, and (2) FAST ensures the durability of update operations by writing small
log entries to the persistent storage. These log entries are written sequentially to the
flash storage, i.e., very small overhead. Details of the update module will be discussed
in Section 5.
Search module The search module in FAST answers point and range queries that
can be imposed to the underlying tree structure. The main challenge in the search
module is that the actual tree structure is split between the flash storage and the
memory. Thus, the main responsibility of the search module is to construct the recent
image of the tree by integrating the stored tree in flash with the tree updates in
memory that did not make it to the flash storage yet. Details of the search module
will be discussed in Section 6.
Flushing module As the memory resource is limited, it will be filled up with the
recent tree updates. In this case, FAST triggers its flushing module that employs a
f lushing policy to select some of the in-memory updates and write them, in bulk,
into the flash storage. Previous research in flash indexing flush their in-memory
updates or log file entries by writing all the memory or log updates once to the flash
storage. In contrast, the flushing module in FAST distinguishes itself from previous
techniques in two main aspects: (1) FAST employs f lushing policies that smartly
selects some of the updates from memory to be flushed to the flash storage in a way
that amortizes the expensive cost of the block erase operation over a large set of
random write operations, and (2) FAST logs the flushing process using a single log
entry written sequentially on the flash storage. Details of the flushing module will be
discussed in Section 7.
Crash recovery module FAST employs a crash recovery module to ensure the
durability of update operations. This is a crucial module in FAST, as only because
of this module, we are able to have our updates in memory, and not to worry about
any data losses. This is in contrast to previous research in flash indexing that may
encounter data losses in case of system crash, e.g., [24, 25, 35, 36]. The crash recovery

424

Geoinformatica (2013) 17:417–448

module is mainly responsible on two operations: (1) Once the system restarts after
crash, the crash recovery module utilizes the log file entries, written by both the
update and flushing modules, to reconstruct the state of the flash storage and inmemory updates just before the crash took place, and (2) maintaining the size of
the log file within the allowed limit. As the log space is limited, FAST needs to
periodically compact the log entries. Details of this module will be discussed in
Section 8.
4.2 FAST design goals
FAST avoids the tradeoff of durability and efficiency by using a combination of
buffering and logging. Unlike existing efficient-but-not-durable designs [24, 25, 30,
35, 36], FAST uses write-ahead-logging and crash recovery to ensure strict system
durability. FAST makes tree updates efficient by buffering write operations in main
memory and by employing an intelligent flushing policy that optimizes I/O costs for
both SSDs and raw flash devices. Unlike existing durable-but-inefficient solutions
[2], FAST does not require reading in-flash log entries for each search/update
operation, which makes reading FAST trees efficient.
4.3 FAST data structure
Other than the underlying index tree structure stored in the flash memory storage,
FAST maintains two main data structures, namely, the Tree Modif ications Table,
and Log File, described below.
Tree modif ications table This is an in-memory hash table (depicted in Fig. 3)
that keeps track of recent tree updates that did not make it to the flash storage
yet. Assuming no hashing collisions, each entry in the hash table represents the
modification applied to a unique node identifier, and has the form (status, list)
where status is either NEW, DEL, or MOD to indicate if this node is newly created,
deleted, or just modified, respectively, while list is a pointer to a new node, null,
or a list of node modifications based on whether the status is NEW, DEL, or
MOD, respectively. For MOD case, each modification in the list is presented by the
quadruple (TimeStamp, type, index, value) where TimeStamp represents the time at

Fig. 3 Tree modifications table

Geoinformatica (2013) 17:417–448

425

which the update happened, type is either K, P F , or P M , to indicate if the modified
entry is the key, a pointer to a flash node, or a pointer to an in-memory node,
respectively, while index and value determines the index and the new value for the
modified node entry, respectively. In Fig. 3, there are two modifications in nodes A
and D, one modification in nodes B and F, while node G is newly created and node
H is deleted.
Log f ile This is a set of flash memory blocks, reserved for recovery purposes. A log
file includes short logs, written sequentially, about insert, delete, update, and flushing
operations. Each log entry includes the triple (operation, node_list, modif ication)
where operation indicates the type of this log entry as either insert, delete, update, or
flush, node_list includes the list of affected nodes by this operation in case of a flush
operation, or the only affected node, otherwise, modif ication is similar to the triple
(type, index, value), used in the tree modif ications table. All log entries are written
sequentially to the flash storage, which has a much lower cost than random writes
that call for the erase operation.
4.4 Running example
Throughout the rest of this paper, we will use Fig. 4 as a running example where six
objects O1 to O6 , depicted by small black circles, are indexed by an R-tree. Then, two
objects O7 and O8 , depicted by small white circles, are to be inserted in the same Rtree. Figure 4a depicts the eight objects in the two-dimensional space domain, while
Fig. 4b gives the flash-resident R-tree with only the six objects that made it to the

O1

10

O8

O2

8
O5

O3

6
O7

O4

4

O6

2

0

2

4

6

8

10

12

14

(a) 2D Space
A

1, K, 2, (12,4,14,2)

B

C

D
O1

O2

E

F

O3

O4

G
O5

(b) R-tree Index

C

Mod

G

Mod

B

Mod

D

Mod

O6

2, K, 2, O7

3, K, 2, (5,10,8,7)

4, K, 2, O8

(c) Tree Modifications Table

Fig. 4 Illustrating example for search and update operations in FAST

426

Geoinformatica (2013) 17:417–448

Table 1 Cost analysis parameters
Parameter

Definition

T
RM
WM
RF

The underlying tree index structure to which FAST has been applied
The average time to read a node update entry from the tree modifications table
The average time to write a node update entry to the tree modifications table
The average time to read a tree node from the underlying tree T residing
on flash memory
The average time to write a tree node to the underlying tree T residing
on flash memory
The average time to erase a whole block on the flash memory device

WF
EF

flash memory. Finally, Fig. 4c gives the in-memory buffer (tree modif ications table)
upon the insertion of O7 and O8 in the tree.
4.5 Operations cost parameters
For each FAST module, we analyze the cost model of its main operations, including
search, update, flushing, crash recovery and log compaction. To this end, we define
the parameters given in Table 1.

5 Tree updates in FAST
This section discusses the update operations in FAST, which include inserting a new
entry and deleting/updating an existing entry. An update operation to any tree in
FAST may result in creating new tree nodes as in the case of splitting operations (i.e.,
when inserting an element in the tree leads to node overflow), deleting existing tree
nodes as in the case of merging operations (i.e., when deleting an element from the
tree leads to node underflow), or just modifying existing node keys and/or pointers.
Main idea For any update operation (i.e., insert, delete, update) that needs to
be applied to the index tree, FAST does not change the underlying insert, delete,
or update algorithm for the tree structure it represents. Instead, FAST runs the
underlying update algorithm for the tree it represents, with the only exception
of writing any changes caused by the update operation in memory instead of the
external storage, to be flushed later to the flash storage, and logging the result of
the update operation. A main distinguishing characteristic of FAST is that what
is buffered in memory, and also written in the log file, is the result of the update
operation, not a log of this operation.
Algorithm Algorithm 1 gives the pseudo code of inserting an object Obj in FAST.
The algorithms for deleting and updating objects are similar in spirit to the insertion
algorithm, and thus are omitted from the paper. The algorithm mainly has two steps:
(1) Executing the insertion in memory (Line 2 in Algorithm 1). This is basically
done by calling the insertion procedure of the underlying tree, e.g., R-tree insertion,
with two main differences. First, the insertion operation calls the search operation,
discussed later in Section 6, to find where we need to insert our data based on the
most recent version of the tree, constructed from main memory recent updates and

Geoinformatica (2013) 17:417–448

427

Algorithm 1 Insert an Object in the Tree
1: Function Insert(Obj)
/* STEP 1: Executing the Insertion in Memory only */
2: L ← List of modified nodes from the in-memory execution of inserting Obj in
the underlying tree
/* STEP 2: Buf fering and Logging the Updates */
3: for each Node N in L do
4:
HashEntry ← N entry in the Tree Modif ications Table
5:
if HashEntry is not NULL then
6:
Add the triple (MOD, N , updates in N ) to the log file
7:
if the status of HashEntry is MOD then
8:
Add the changes in N to the list of changes of HashEntry
9:
else
10:
Apply the changes in N to the new node of HashEntry
11:
end if
12:
else
13:
HashEntry ← Create a new entry for N in the Tree Modif ications Table
14:
if N is a newly created node then
15:
Add the triple (NEW, N , updates in N ) to the log file
16:
Set HashEntry status to NEW, and its pointer to N
17:
else
18:
Add the triple (MOD, N , updates in N ) to the log file
19:
Set HashEntry status to MOD, and its pointer to the list of changes that
took place in N
20:
end if
21:
end if
22: end for

the in-flash tree index structure. Second, the modified or newly created nodes that
result back from the insertion operation are not written back to the flash storage,
instead, they will be returned to the algorithm in a list L. Notice that the insertion
procedure may result in creating new nodes if it encounters a split operation.
(2) Buf fering and logging the tree updates (Lines 3–22 in Algorithm 1). For each
modified node N in the list L, we check if there is an entry for N in our in-memory
buffer, tree modif ications table. If this is the case, we first add a corresponding log
entry that records the changes that took place in N . Then, we either add the changes
in N to the list of changes in its entry in the tree modif ications table if this entry status
is MOD, or update N entry in the tree modif ications table, if the entry status is NEW.
On the other hand, if there is no entry for N in the tree modif ications table, we create
such entry, add it to the log file, and fill it according to whether N is a newly created
node or a modified one.
Example In our running example of Fig. 4, inserting O7 results in modifying two
nodes, G and C. Node G needs to have an extra key to hold O7 while node C needs
to modify its minimum bounding rectangle that points to G to accommodate its size
change. The changes in both nodes are stored in the tree modif ications table depicted

428
Fig. 5 FAST logging and
recovery example

Geoinformatica (2013) 17:417–448

Log#

Operation

Node

Modification

1

MOD

C

1, K, 2, (12,4,14,2)

2

MOD

G

2, K, 2, O7

3

MOD

B

3, K, 2, (5,10,8,7)

4

MOD

D

4, K,2, O8

5

FLUSH

B, C, D

*

(a) FAST Log File
Log#
1

Operation

Node

Modification

MOD

G

2, K, 2, O7

(a) FAST Log File after Crash Recovery

in Fig. 4c. The log entries for this operation are depicted in the first two entries of the
log file of Fig. 5a. Similarly, inserting O8 results in modifying nodes, D and B
Cost analysis For a given update operation U applied to a tree index structure T,
let yi,U ∈ {0, 1} represent whether or not node i of T has been modified by U. Let N
be the total number of nodes in T at the time U is applied, then the total cost CU of
update operation U applied on T is as follows:
CU = C Q +

N


yi,U ∗ [W F + W M + L]

(1)

i=0

The update operation (e.g., insert, delete, modify) requires first a search query Q
for a proper leaf node in T. This also takes the same search time C Q as illustrated
above. For each updated node i due to applying U, yi,U = 1, and for each of these
updates we write a sequential log entry to the log file
each takes W F time. Hence,
that
N
the total time to write all log entries is equal to i=0
yi,U ∗ W F . For each updated
node i, we also perform a lookup on the tree modifications table to get the entry for
node i, which is performed in constant time L. In addition, the total time to write
the modifications to all nodes (for which yi,U = 1) in the tree modifications table is
N
i=0 yi,U ∗ W M . All of the above sums up to give the update cost given in Eq. 1
6 Searching in FAST
Given a query Q, the search operation returns those objects indexed by FAST and
satisfy Q. The search query Q could be a point query that searches for objects with
a specific (point) value, or a range query that searches for objects within a specific
range. An important promise of FAST is that it does not change the main search
algorithm for any tree it represents. Instead, FAST complements the underlying
searching algorithm to consider the latest tree updates stored in memory.
Main idea As it is the case for any index tree, the search algorithm starts by fetching
the root node from the secondary storage, unless it is already buffered in memory.
Then, based on the entries in the root, we find out which tree pointer to follow to

Geoinformatica (2013) 17:417–448

429

fetch another node from the next level. The algorithm goes on recursively by fetching
nodes from the secondary storage and traversing the tree structure till we either find
a node that includes the objects we are searching for or conclude that there are no
objects that satisfy the search query. The challenging part here is that the retrieved
nodes from the flash storage do not include the recent in-memory stored updates.
FAST complements this search algorithm to apply the recent tree updates to each
retrieved node from the flash storage. In particular, for each visited node, FAST
constructs the latest version of the node by merging the retrieved version from the
flash storage with the recent in-memory updates for that node.
Algorithm Algorithm 2 gives the pseudo code of the search operation in FAST.
The algorithm takes two input parameters, the query Q, which might be a point or
range query, and a pointer to the root node R of the tree we want to search in. The
output of the algorithm is the list of objects that satisfy the input query Q. Starting
from the root node and for each visited node R in the tree, the algorithm mainly
goes through two main steps: (1) Constructing the most recent version of R (Line 2 in
Algorithm 2). This is mainly to integrate the latest flash-residant version of R with
its in-memory stored updates. Algorithm 3 gives the detailed pseudo code for this

Algorithm 2 Searching for an Object indexed by the Tree
1: Function Search(Query Q, Tree Node R)
/* STEP 1: Constructing the most recent version of R */
2: N ← RetrieveNode(R)
/* STEP 2: Recursive search calls */
3: if N is non-leaf node then
4:
Check each entry E in N . If E satisfies the query Q, invoke Search(Q,
E.NodePointer) for the subtree below E
5: else
6:
Check each entry E in N . If E satisfies the search query Q, return the object
to which E is pointing
7: end if

Algorithm 3 Retrieving a tree node
1: Function RetrieveNode(Tree Node R)
2: FlashNode ← Retrieve node R from the flash-resident index tree
3: HashEntry ← R’s entry in the Tree Modif ications Table
4: if HashEntry is NULL then
5:
return FlashNode
6: end if
7: if the status of HashEntry is MOD then
8:
FlashNode ← FlashNode ∪ All the updates in HashEntry list
9:
return FlashNode
10: end if
/* We are trying to retrieve either a new or a deleted node */
11: return the node that HashEntry is pointing to

430

Geoinformatica (2013) 17:417–448

step, where initially, we read R from the flash storage. Then, we check if there is
an entry for R in the tree modif ications table. If this is not the case, then we know
that the version we have read from the flash storage is up-to-date, and we just return
it back as the most recent version. On the other hand, if R has an entry in the tree
modif ications table, we either apply the changes stored in this entry to R in case the
entry status is MOD, or just return the node that this entry is pointing to instead of R.
This return value could be null in case the entry status is DEL. (2) Recursive search
calls (Lines 3–7 in Algorithm 2). This step is typical in any tree search algorithm, and
it is basically inherited from the underlying tree that FAST is representing. The idea
is to check if R is a leaf node or not. If R is a non-leaf node, we will check each entry
E in the node. If E satisfies the search query Q, we recursively search in the subtree
below E. On the other hand, if R is a leaf node, we will also check each entry E in
the node, yet if E satisfies the search query Q, we will return the object to which E is
pointing to as an answer to the query.
Example Given the range query Q in Fig. 4a, FAST search algorithm will first
fetch the root node A stored in flash memory. As there is no entry for A in the tree
modif ications table (Fig. 4c), then the version of A stored in flash memory is the most
recent one. Then, node C is the next node to be fetched from flash memory by the
searching algorithm. As the tree modif ications table has an entry for C with status
MOD, the modifications listed in the tree modif ications table for C will be applied
to the version of C read from the flash storage. Similarly, the search algorithm will
construct the leaf nodes F and G by first fetching them from flash memory, and then
reading their recent updates from the tree modif ications table. Finally, the result of
this query is {O4 , O5 , O6 , O7 }.
Cost analysis For a given search query Q applied to a tree index structure T, let
xi,Q ∈ {0, 1} represent whether node i of T is visited or not when issuing query Q.
Let Mi,Q be the number of modifications applied to node i and buffered in the tree
modifications table at the time Q is issued. Let N be the total number of nodes in T
at the time Q is issued, then the total search cost C Q on T is as follows:
CQ =

N


xi,Q ∗ [R F + (Mi,Q × R M ) + L]

(2)

i=0

Assuming a range query, the search operation returns a number of objects within
the query range. In FAST, when reading a node i from the flash-resident R-tree, we
also need to accommodate all the corresponding modifications on i that have been
recorded in the tree modifications table. Then, the total cost of reading a node i
would thus be (R F + Tm ) where Tm is the in-memory processing time for each node.
For the in-memory processing part, it first takes constant time L to locate the node
in the tree modification table , and then takes a linear scan of the list to apply all
the modifications. Given that the number of modifications associated with each node
is Mi,Q , then Tm = (Mi,Q × R M ) + L, where Mi,Q is upper bounded by the memory
size.

Geoinformatica (2013) 17:417–448

431

7 Memory flushing in FAST
As discussed in Section 5, the effect of all incoming updates in FAST has to be
buffered in memory. As memory is a scarce resource, it will eventually be filled up
with incoming updates. In that case, FAST triggers its flushing module, equipped
with a f lushing policy, to free some memory space by evicting a selected part of the
memory, termed a f lushing unit, to the flash storage. Such flushing is done in a way
that amortizes the cost of expensive random write operations over a high number of
update operations. In this section, we first define the flushing unit. Then, we discuss
the flushing policy used in FAST. Finally, we explain the FAST flushing algorithm.
The motivation of having a f lushing policy that flushes only part of the memory
is twofold: (1) Clearing the whole memory at once will cause a significant pause to
the system due to the need of erasing all the flash blocks that include at least one
update record in memory. As a result, it is better to consider clearing only part of the
memory in a way that does not really pause the system. In this paper, we present two
main flushing policies employed by the system, and we empirically evaluate both of
them, (2) Considering that we need to flush only part of the memory, it is crucial to
select that part in a way that reduces the overhead of the block erase operation.
7.1 Flushing unit
An important design parameter, in FAST, is the size of a f lushing unit, the granularity
of consecutive memory space written in the flash storage during each flush operation.
Our goal is to find a suitable f lushing unit size that minimizes the average cost of
flushing an update operation to the flash storage, denoted as C. The value of C
average writing cost
depends on two factors: C1 = numb
; the average cost per bytes written,
er of written b ytes

er of written b ytes
; the number of bytes written per update. This gives C =
and C2 = numb
numb er of updates
C1 × C2 .
Interestingly, the values of C1 and C2 show opposite behaviors with the increase
of the f lushing unit size. First consider C1 . On raw flash devices (e.g., ReMix [19]),
for a f lushing unit smaller than a flash block, C1 decreases with the increase of
the flushing unit size (see [29] for more detail experiments). This is intuitive, since
with a larger f lushing unit, the cost of erasing a block is amortized over more bytes
in the flushing unit. The same is also true for SSDs since small random writes
introduce large garbage collection overheads, while large random writes approach
the performance of sequential writes. Previous work has shown that, on several SSDs
including the ones from Samsung, MTron, and Transcend, random write latency per
byte increases by ≈32× when the write size is reduced from 16 KB to 0.5 KB [7]. Even
on newer generation SSDs from Intel, we observed an increase of ≈4× in a similar
experimental setup. This suggests that a flushing unit should not be very small, as that
would result in a large value of C1 . On the other hand, the value of C2 increases with
increasing the size of the f lushing unit. Due to non-uniform updates of tree nodes, a
large flushing unit is unlikely to have as dense updates as a small flushing unit. Thus,
the larger a f lushing unit is, the less the number of updates per byte is (i.e., the higher
the value of C2 is). Another disadvantage of large f lushing unit is that it may cause a
significant pause to the system. All these suggest that the f lushing unit should not be
very large.

432

Geoinformatica (2013) 17:417–448

Deciding the optimal size of a f lushing unit requires finding a sweet spot between
the competing costs of C1 and C2 . Our experiments show that for raw flash devices, a
f lushing unit of one flash block minimizes the overall cost. For SSDs, a f lushing unit
of size 16 KB is a good choice, as it gives a good balance between the values of C1
and C2 . Note that a flushing unit size of 16 KB also matches the optimal size of a tree
node, as suggested by Gray et al. [13]. Thus, with a tree of this optimal node size of
16 KB, we can simply flush one node at a time from the memory.
7.2 Flushing policies
FAST is designed so that different flushing policies can be plugged in to the system.
In the rest of this section, we discuss two main flushing policies adopted by FAST:
(1) FAST Flushing Policy, and (2) FAST* Flushing Policy.
7.2.1 FAST f lushing policy
The main idea of FAST f lushing policy is to minimize the average cost of writing
each update to the underlying flash storage. To that end, FAST flushing policy aims
to flush the in-memory tree updates that belong to the f lushing unit that has the
highest number of in-memory updates. In that case, the cost of writing the f lushing
unit will be amortized among the highest possible number of updates. Moreover,
since the maximum number of updates are being flushed out, this frees up the
maximum amount of memory used by buffered updates. Finally, as done in the
update operations, the flushing operation is logged in the log file to ensure the
durability of system transactions.
Data structure The flushing policy maintains an in-memory max heap structure,
termed FlushHeap, of all f lushing units that have at least one in-memory tree update.
The max heap is ordered on the number of in-memory updates for each f lushing unit,
and is updated with each incoming tree update. Updates in max heap is O(n), where
n is the number of flash blocks with in-memory updates. In the mean time, retrieving
the flushing unit with maximum number of updates is an O(1) operation.
7.2.2 FAST* f lushing policy
The FAST* f lushing policy is an enhancement over the FAST flushing policy
described in Section 7.2.1. FAST* flushing policy takes into account two parameters
that helps in deciding which unit must be flushed: (1) Number of updates per
flushing unit: It is the same parameter used by the FAST flushing policy explained in
Section 7.2.1; which favors the flash unit that has the highest number of updates, and
(2) Time stamp of the flushing unit: which represents the last time a flash block has
been updated. When deciding which unit needs to be flushed, that parameter gives
higher priority to the flushing unit that has the lowest time stamp (i.e., least recently
updated).
FAST* Flushing Policy employs a Top-1 selection algorithm to select a flushing
unit to be evicted to flash memory with the objective of maximizing the number of
updates per flushing unit and minimizing the time stamp of the flushing unit. The
intuition behind such a policy is that it is sometimes better to keep the block that
has the highest number of updates in the tree modifications table (i.e., in memory)

Geoinformatica (2013) 17:417–448

433

and not to flush it, especially if that block is expected to receive more updates
(i.e., recently updated block). On the other hand, it might be better to flush a flash
block that has a bit less number of updates, but it is not expected to be updated
frequently (i.e., least recently updated block). Hence, FAST* flushing policy makes
that tradeoff between the two parameters in order to amortize the total number of
erase operations on flash memory storage systems.
7.3 Flushing algorithm
Algorithm 4 gives the pseudo code for flushing tree updates. The algorithm has two
main steps: (1) Finding out the list of f lushed tree nodes (Lines 2–9 in Algorithm 4).
This step starts by finding out the victim f lushing unit, MaxUnit, using the flushing
policy passed to the algorithm. Then, we scan the tree modif ications table to find
all updated tree nodes that belong to MaxUnit. For each such node, we construct
the most recent version of the node by retrieving the tree node from the flash
storage, and updating it with the in-memory updates. This is done by calling the
RetrieveNode(N ) function, given in Algorithm 3. The list of these updated nodes
constitute the list of to be flushed nodes, FlushList. (2) Flushing, logging, and cleaning
selected tree nodes (Lines 10–15 in Algorithm 4). In this step, all nodes in the
FlushList are written once to the flash storage. As all these nodes reside in one
f lushing unit, this operation would have a minimal cost due to our careful selection
of the f lushing unit size. Then, similar to update operations, we log the flushing
operation to ensure durability. Finally, all flushed nodes are removed from the tree
modif ications table to free memory space for new updates.
Algorithm 4 Flushing Tree Updates
1: Function FlushTreeUpdates(FlushPolicy)
/* STEP 1: Finding out the list of flushed tree nodes */
2: FlushList ← {φ}
3: MaxUnit ← Retrieve Unit to be Flushed uisng FlushPolicy
4: for each Node N in tree modif ications table do
5:
if N ∈ MaxUnit then
6:
F ← RetrieveNode(N )
7:
FlushList ← FlushList ∪ F
8:
end if
9: end for
/* STEP 2: Flushing, logging, and cleaning selected nodes */
10: Flush all tree updates ∈ FlushList to a clean flash memory block
11: Add (Flush, All Nodes in FlushList) to the log file
12: Erase the old flash memory block and update the index pointer to refer the new
block
13: for each Node F in FlushList do
14:
Delete F from the Tree Modif ications Table
15: end for
Example In our running example given in Fig. 4, assume that the memory is full,
hence FAST triggers its flushing module. Assume also that nodes B, C, and D reside

434

Geoinformatica (2013) 17:417–448

in the same f lushing unit B1 , while nodes E, F, and G reside in another f lushing
unit B2 . The number of updates in B1 is three as each of nodes B, C and D has
been updated once. On the other hand, the number of updates in B2 is one because
nodes E and F has no updates at all, and node G has only a single update. Hence,
as per FAST flushing policy, MaxUnit is set to B1 , and we will invoke RetrieveNode
algorithm for all nodes belonging to B1 (i.e., nodes B, C, and D) to get the most
recent version of these nodes and flush them to flash memory. Then, the log entry
(Flush; Nodes B, C, D) is added to the log file (depicted as the last log entry
in Fig. 5a). Finally, the entries for nodes B, C, and D are removed from the tree
modif ications table.
Cost analysis For a given flushing operation F applied to a tree index structure T,
Let Pflush be the set of tree nodes that belongs to the block selected to be flushed.
Let M p be the number of modifications applied to node p and buffered in the tree
modifications table at the time F is applied. Hence, the total cost C F of flushing
operation F applied on T is as follows:

CF = EF + H +
[R F + (M p ∗ R M ) + W F + L]
(3)
p ∈ Pflush

We decide which unit to flush by employing the flushing policy passed to the
algorithm. The cost of this in memory operation H varies based on which flushing
policy is activated. For each node p ∈ Pflush
, we first need to retrieve the node current
value saved in flash memory which
costs
p ∈ Pflush R F , and then lookup the node in

the tree modifications table in
L.
For each node p ∈ Pflush , we read all
p ∈ Pflush
M p modifications
of
p
that
are
buffered
in
the
tree modifications table, which sum

up to p ∈ Pflush (M p ∗ R M ). Before we write the new nodes values, we first erase the
whole flash block which costs
 E F time. For each node p ∈ Pflush , we write the flushed
node new value, that costs p ∈ Pflush W F . All of the above sum up to give the flushing
operation cost given by Eq. 3.

8 Crash recovery and log compaction in FAST
As discussed before, FAST heavily relies on storing recent updates in memory, to
be flushed later to the flash storage. Although such design efficiently amortizes the
expensive random write operations over a large number of updates, it poses another
challenge where memory contents may be lost in case of system crash. To avoid such
loss of data, FAST employs a crash recovery module that ensures the durability of
in-memory updates even if the system crashed. The crash recovery module in FAST
mainly relies on the log file entries, written sequentially upon the update and flush
operations.
In this section, we will first describe the crash recovery module and logging
mechanism in FAST. Then, we will follow by discussing the log compaction operation
in FAST, which is mainly done to ensure that the log file is within a certain size limit.
Log compaction has a very similar operation to the recovery module, and it is crucial
to keep up the efficiency of FAST. For simplicity, we will not consider the case of
having a system crash during the recovery process, as this can be handled in a similar
way to traditional recovery modules in database systems.

Geoinformatica (2013) 17:417–448

435

8.1 Recovery
The recovery module in FAST is triggered when the system restarts from a crash,
with the goal of restoring the state of the system just before the crash took place.
The state of the system includes the contents of the in-memory data structure, tree
modif ications table, and the flash-resident tree index structure. By doing so, FAST
ensures the durability of all non-flushed updates that were stored in memory before
crash.
Main idea The main idea of the recovery operation is to scan the log file bottomup to be aware of the flushed nodes, i.e., nodes that made their way to the flash
storage. During this bottom-up scanning, we also find out the set of operations that
need to be replayed to restore the tree modif ications table. Then, the recovery module
cleans all the flash blocks, and starts to replay the non-flushed operations in the order
of their insertion, i.e., top-down. The replay process includes insertion in the tree
modif ications table as well as a new log entry. It is important here to reiterate our
assumption that there will be no crash during the recovery process, so, it is safe to
keep the list of operations to be replayed in memory. If we will consider a system
crash during the recovery process, we might just leave the operations to be replayed
in the log, and scan the whole log file again in a top-down manner. In this top-down
scan, we will only replay the operations for non-flushed nodes, while writing the new
log entries into a clean flash block. The result of the crash recovery module is that
the state of the memory will be stored as it was before the system crashes, and the
log file will be an exact image of the tree modif ications table.
Algorithm Algorithm 5 gives the pseudo code for crash recovery in FAST, which
has two main steps: (1) Bottom-Up scan (Lines 2–11 in Algorithm 5). In this step,
FAST scans the log file bottom-up, i.e., in the reverse order of the insertion of log
entries. For each log entry L in the log file, if the operation of L is Flush, then
we know that all the nodes listed in this entry have already made their way to the
flash storage. Thus, we keep track of these nodes in a list, termed FlushedNodes,
so that we avoid redoing any updates over any of these nodes later. On the other
side, if the operation of L is not Flush, we check if the node in L entry is in the list
FlushedNodes. If this is the case, we just ignore this entry as we know that it has
made its way to the flash storage. Otherwise, we push this log entry into a stack of
operations, termed RedoStack, as it indicates a non-flushed entry at the crash time.
At the end of this step, we pass the RedoStack to the second step. (2) Top-Down
processing (Lines 13–19 in Algorithm 5). At the beginning, we first create a new log
file Fnew . Then, this step basically goes through all the entries in the RedoStack in a
top-down way, i.e., the order of insertion in the log file. As all these operations were
not flushed by the crash time, we just add each operation to the tree modif ications
table and add a corresponding log entry in the new Log File Fnew . The reason of
doing these operations in a top-down way is to ensure that we have the same order
of updates, which is essential in case one node has multiple non-flushed updates. At
the end of this step, the tree modif ications table will be exactly the same as it was just
before the crash time, while the new log file Fnew will be exactly an image of the tree
modif ications table stored in the flash storage. Finally, we change the log file pointer
to refer to the new log file Fnew and we finally erase the old log file flash blocks.

436

Geoinformatica (2013) 17:417–448

Algorithm 5 Crash Recovery
1: Function RecoverFromCrash()
/* STEP 1: Bottom-Up Cleaning */
2: FlushedNodes ← φ
3: for each Log Entry L in the log file in a reverse order do
4:
if the operation of L is Flush then
5:
FlushedNodes ← FlushedNodes ∪ the list of nodes in L
6:
else
7:
if the node in entry L ∈
/ FlushedNodes then
8:
Push L into the stack of updates RedoStack
9:
end if
10:
end if
11: end for
/* Phase 2: Top-Down Processing */
12: Create a new Log File Fnew
13: while RedoStack is not Empty do
14:
Op ← Pop an update operation from the top of RedoStack
15:
Insert the operation Op into the tree modif ications table
16:
Add a log entry for Op in the new log file Fnew
17: end while
18: Change the Log File pointer to refer to the new Log File Fnew
19: Clean all the old log entries by erasing the old log file flash blocks

Example In our running example, the log entries of inserting Objects O7 and O8
in Fig. 4 are given as the first four log entries in Fig. 5a. Then, the last log entry
in Fig. 5a corresponds to flushing nodes B, C, and D. We assume that the system
is crashed just after inserting this flushing operation. Upon restarting the system,
the recovery module will be invoked. First, the bottom-up scanning process will be
started with the last entry of the log file, where nodes B, C, and D are added to the
list FlushedNodes. Then, for the next log entry, i.e., the fourth entry, as the node
affected by this entry D is already in the FlushedNodes list, we just ignore this entry,
since we are sure that it has made its way to disk. Similarly, we ignore the third log
entry for node B. For the second log entry, as the affected node G is not in the
FlushedNodes list, we know that this operation did not make it to the storage yet,
and we add it to the RedoStack to be redone later. The bottom-up scanning step is
concluded by ignoring the first log entry as its affected node C is already flushed, and
by wiping out all log entries. Then, the top-down processing step starts with only one
entry in the RedoStack that corresponds to node G. This entry will be added to the
tree modif ications table and log file. Figure 5b gives the log file after the end of the
recovery module which also corresponds to the entries of the tree modif ications table
after recovering from failure.
Cost analysis For a given crash recovery operation R applied to a tree index
structure T, let Z be the set of operations recorded in the log file. Let α (0 ≤ α ≤ 1)
be the fraction of operations in Z that had been flushed to T before the system fails.
Let Spage , Sblock , and Slog be the byte size of the flash page, flash block and flash log

Geoinformatica (2013) 17:417–448

437

file, respectively. Hence, the total cost C R of a crash recovery operation R applied
on T is as follows:


RF
RF
+ Z × α × (W M + W F )
+
(4)
C R = Slog ×
Spage
Sblock
As all the Z entries in the log file have to be scanned, then the total cost to scan
Slog
them is R F × Spage
. In addition, only Z × α log file operations need to be redone
(i.e., written back to the tree modifications table), which results to an additional cost
of Z × α × W M . As all redone operations are written back to memory, an additional
cost of logging them is Z × α × W F . The old log file blocks needs to be erased, which
Slog
incurs a cost of E F × Sblock
. All of the above sums up to give the recovery cost given
in Eq. 4.
8.2 Log compaction
As FAST log file is a limited resource, it may eventually become full. In this case,
FAST triggers a log compaction module that organizes the log file entries for better
space utilization. This can be achieved by two space saving techniques: (a) Removing
all the log entries of flushed nodes. As these nodes have already made their way to
the flash storage, we do not need to keep their log entries anymore, and (b) Packing
small log entries in a larger writing unit. Whenever a new log entry is inserted, it
mostly has a small size that may occupy a flash page as the smallest writing unit
to the flash storage. At the time of compaction, these small entries can be packed
together to achieve the maximum possible space utilization.
The main idea and algorithm for the log compaction module are almost the same
as the ones used for the recovery module, with the exception that the entries in the
RedoStack will not be added to the tree modif ications table, yet they will just be
written back to the log file, in a more compact way. As in the recovery module,
Fig. 5a and b give the log file before and after log compaction, respectively. The log
compaction have similar expensive cost as the recovery process. Fortunately, with
an appropriate size of log file and memory, it will not be common to call the log
compaction module.
It is unlikely that the log compaction module will not really compact the log file
much. This may take place only for a very small log size and a very large memory size,
as there will be a lot of non-flushed operations in memory with their corresponding
log entries. Notice that if the memory size is small, there will be a lot of flushing
operations, which means that log compaction can always find log entries to be
removed. If this unlikely case takes place, we call an emergency f lushing operation
where we force flushing all main memory contents to the flash memory persistent
storage, and hence clean all the log file contents leaving space for more log entries to
be added.
Cost analysis The log compaction is almost the same as the crash recovery procedure. The only difference is that records are not redone (written to the tree
modifications table). Similar to recovery cost, the log compaction cost CC is as
follows:


RF
RF
+ Z × α × WF
+
(5)
CC = Slog ×
Spage
Sblock

438

Geoinformatica (2013) 17:417–448

9 Experimental evaluation
This section experimentally evaluates the performance of FAST, compared to the
state-of-the-art algorithms for one-dimensional and multi-dimensional flash index
structures: (1) Lazy Adaptive Tree (LA-tree) [2]: LA-tree is a flash friendly one
dimensional index structure that is intended to replace the B-tree. LA-tree stores
the updates in cascaded buffers residing on flash memory and, then empties these
buffers dynamically based on the operations workload. (2) FD-tree [24, 25]: FD-tree
is a one-dimensional index structure that allows small random writes to occur only
in a small portion of the tree called the head tree which exists at the top level of
the tree. When the capacity of the head tree is exceeded, its entries are merged in
batches to subsequent tree levels. (3) RFTL [35]: RFTL is a mutli-dimensional tree
index structure that adds a buffering layer on top of the flash translation layer (FTL)
in order to make R-trees work efficiently on flash devices.
We instantiate B-tree and R-tree instances of FAST using both flushing policies
(i.e., FAST flushing policy and FAST* flushing policy), termed FAST-Btree, FAST*Btree, FAST-Rtree, and FAST*-Rtree , respectively, by implementing FAST inside
the GiST generalized index structure [15], which is already built inside PostgreSQL
[1]. In our experiments, we use two synthetic workloads: (1) Lookup intensive
workload (W L ): that includes 80 % search operations and 20 % update operations
(i.e., insert, delete, or update). (2) Update intensive workload, (WU ): that includes
20 % search operations and 80 % update operations.
Unless mentioned otherwise, we set the number of workload operations to
10 million operations, main memory size to 256 KB (i.e., the amount of memory
dedicated to main memory buffer used by FAST), tree index size to 512 MB, and log
file size to 10 MB, which means that the default log size is ≈2 % of the index size.
The experiments in this section mainly discuss the effect of varying the memory
size, log file size, index size, and number of updates on the performance of FASTBtree, FAST-Rtree, LA-tree, FD-tree, and RFTL. Also, we study the performance of
flushing, log compaction, and recovery operations in FAST. In addition, we compare
the implementation cost between FAST and its counterparts. Our performance
metrics are mainly the number of flash memory erase operations and the average
response time. However, in almost all of our experiments, we got a similar trend for
both performance measures. Thus, for brevity, we only show the experiments for the
number of flash memory erase operations, which is the most expensive operation in
flash storage. Although we compare FAST to its counterparts from a performance
point of view, however we believe the main contribution of FAST is not in the
performance gain. The generic structure and low implementation cost are the main
advantages of FAST over specific flash-aware tree index structures.
All experiments were run on both raw flash memory storage, and solid state drives
(SSDs). For raw flash, we used the raw NAND flash emulator described in [2].
The emulator was populated with exhaustive measurements from a custom-designed
Mica2 sensor board with a Toshiba1Gb NAND TC58DVG02A1FT00 flash chip. For
SSDs, we used a 32GB MSP-SATA7525032 SSD device. All the experiments were
run on a machine with Intel Core2 8400 at 3Ghz with 4GB of RAM running Ubuntu
Linux 8.04.

Geoinformatica (2013) 17:417–448

439

9.1 Effect of memory size
Figure 6 and b give the effect of varying the memory size from 128 KB to 1,024 KB
(in a log scale) on the number of erase operations, encountered in FAST-Btree, LAtree, and FD-tree, for workloads W L and WU , respectively. For both workloads and
for all memory sizes, FAST-Btree consistently has much lower erase operations than
that of the LA-tree. More specifically, Fast-Btree results in having only from half
to one third of the erase operations encountered by LA-tree. This is mainly due to
the choice of f lushing unit and f lushing policy used in FAST that amortize the block
erase operations over a large number of updates. Also, for both experiments, the
number of erase operations decreases with the increase of the memory size, which is
intuitive as more memory means less frequent need for flushing, and hence less need
for block erase operations.
The performance of FAST-Btree is slightly better than that of FD-tree, because
FD-tree does not employ a crash recovery technique (i.e., no logging overhead).
FAST still performs better than FD-tree due to FAST flushing policy that selects
the best block to be flushed to flash memory. Although the performance of FD-tree
is close to FAST-Btree, however FAST has the edge of being a generic framework
which is applied to many tree index structures and needs less work and overhead
(in terms of lines of code) to be incorporated in the database engine. Comparing
the two workloads against each other, we can see that the workload WU encounters
much more erase operations than that of workload W L . This is mainly because WU
is an update intensive workload which results in many in-memory updates that need
to flushed. FAST*-Btree gives a slightly better performance than FAST-Btree as
FAST*-Btree employs a flushing policy that does not only rely on the number of
updates per flash block, but also takes into account the last time a flash block has
been updated. Hence, FAST*-tree gives a chance for those flash blocks that has
higher number of updates to stay in memory if more updates are expected to be
applied to these blocks.
Figures 7a and b give similar experiments to that of Fig. 6 and b, with the exception
that we run the experiments for two-dimensional search and update operations
for both the Fast-Rtree and RFTL. To be able to do so, we have adjusted our
600

# of erase operations *(103)

# of erase operations *(103)

90
80
70
60
50
40
30

FAST*–Btree
FAST–Btree
LA–tree
10
FD–tree

20

0
128

256

512

1024

FAST*–Btree
FAST–Btree
LA–tree
FD–tree

500
400
300
200
100
0
128

256

512

Memory Size (KB)

Memory Size (KB)

(a) WL

(b) WU

Fig. 6 Effect of memory size on one-dimensional index structure

1024

440

Geoinformatica (2013) 17:417–448
900

FAST*–Rtree
FAST–Rtree
RFTL

80

# of erase operations *(103)

# of erase operations *(103)

100

60
40
20

128

256

512

1024

FAST*-Rtree
FAST-Rtree
RFTL

800
700
600
500
400
300
200
100
0
128

256

512

Memory Size (KB)

Memory Size (KB)

(a) Spatial-WL

(b) Spatial-WU

1024

Fig. 7 Effect of memory size on multi-dimensional index structure

workload W L and WU to Spatial-W L and Spatial-WU , respectively, which have twodimensional operations instead of the one-dimensional operations used in W L and
WU . The result of these experiments have the same trend as the ones done for onedimensional tree structures, where FAST-Rtree has consistently better performance
than RFTL in all cases, with around one half to one third of the number of erase
operations encountered in RFTL. Similar to the one-dimesnional case, FAST*-Rtree
slightly outperforms FAST-Rtree. Comparing the multi-dimensional workload to the
one dimensional one shows that the multi-dimensional workload encounters more
erase operations which is mainly due to the facts that the update operation may span
more nodes. However, even with this, FAST still keeps its performance ratio over its
counterparts.
The experiments in Figs. 6 and 7 not only shows that FAST has better performance
than its counterparts LA-tree, FD-tree and RFTL, but it also shows the power of
the FAST framework where it can be applied to both one-dimensional and multidimensional index structures with the same efficiency. In other words, it is not only
that FAST is better than LA-tree and FD-tree, but it is also the fact that FAST has
the ability to efficiently support multi-dimensional search and update operations in
which LA-tree or FD-tree cannot even support.
9.2 Effect of log file size
Figure 8 gives the effect of varying the log file size from 10 MB (i.e., 2 % of the
index size) to 25 MB (i.e., 5 % of the index size) on the number of erase operations,
encountered in FAST-Btree, LA-tree, and FD-tree for workload W L (Fig. 8a) and
FAST-Rtree and RFTL for workload Spatial-WU (Fig. 8b). For brevity, we do not
show the experiments of FAST-Btree, LA-tree, and FD-tree for workload WU nor
the experiment of FAST-Rtree and RFTL for workload Spatial-W L . As can be seen
from the figures, the performance of both LA-tee, FD-tree, and RFTL is not affected
by the change of the log file size. This is mainly because these three approaches
rely on buffering incoming updates, and hence does not make use of any log file. It
is interesting, however, to see that the number of erase operations in FAST-Btree
and FAST-Rtree significantly decreases with the increase of the log file size, given
that the memory size is set to its default value of 256 KB in all experiments. The

Geoinformatica (2013) 17:417–448
800

FAST*–Btree
FAST–Btree
LA–tree
FD–tree

90
80
70

# of erase operations *(103)

# of erase operations *(103)

100

441

60
50
40
30
20
10
0

10

20

15

25

700
600
500

FAST*–Rtree
FAST–Rtree
RFTL

400
300
200
100
0

10

15

20

Maximum Log File Size (MB)

Maximum Log File Size (MB)

(a) WL

(b) Spatial-WU

25

Fig. 8 Effect of FAST log file size

justification for this is that with the increase of the log file size, there will be less
need for FAST to do log compaction. FAST*-Btree and FAST*-Rtree shows the
same trend as FAST-Btree and FAST-Rtree except that they slightly give better
performance due to the fact that they apply the FAST* Flushing policy.
Revisiting Figs. 6 and 7 in Section 9.1, the number of erase operations encountered
in both LA-tree, FD-tree, and RFTL were only coming from flushing buffered
updates, while the number of erase operations in FAST were coming from two
sources, flushing in-memory updates, and log compaction. Then, the experiment in
this section (Fig. 8) shows that a large fraction of the erase operations in FAST is
coming from the log compaction operation, which can be significantly reduced with
the slight increase of the log file. With this, we can see that FAST achieves close
to an order of magnitude less erase operations than its counterparts for both onedimensional and multi-dimensional index structures when having the log file as small
as 5 % of the index size, i.e., 25 MB.
9.3 Effect of index size
Figure 9 gives the effect of varying the index size from 128 MB to 4 GB (in a log
scale) on the number of erase operations, encountered in FAST-Btree, LA-tree,
and FD-tree for workload W L (Fig. 9a) and FAST-Rtree and RFTL for workload
Spatial-WU (Fig. 9b). Same as in Section 9.2, we omit other workloads for brevity. In
all cases, FAST consistently gives much better performance than its counterparts.
Both FAST and other index structures have similar trend of a linear increase of
the number of erase operations with the increase of the index size. This is mainly
because with a larger index, an update operation may end up modifying more nodes
in the index hierarchy, or more overlapped nodes in case of multi-dimensional index
structures. Moreover, FAST*-Btree and FAST*-Rtree give a bit better performance
than FAST-Btree and FAST-Rtree, respectively. This is basically due to the fact that
FAST* flushing policy handles the flash memory updates better than the original
FAST flushing policy, hence when the index size increase the possibility that more
blocks are updated increases leading to such performance gain for both FAST*-Btree
and FAST*-Rtree. The take home message from this experiment is that FAST still
maintains its performance gain over its counterparts even with the large increase of
the index size.

Geoinformatica (2013) 17:417–448

140

# of Erase Operations *(103)

# of Erase Operations *(103)

442
FAST*–Btree
FAST–Btree
LA–tree
FD–tree

120
100
80
60
40
20
0

1200

FAST*–Rtree
FAST–Rtree
RFTL

1000
800
600
400
200
0

B
4G

B
2G

B

2M

B

6M

B

8M

B
1G

51

25

12

B
4G

B
2G

B

2M

B

6M

B

8M

B
1G

51

25

12

Index Size

Index Size

(a) WL

(b) Spatial-WU

Fig. 9 Effect of tree index size

9.4 Effect of number of updates

10000
9000
8000
7000
6000
5000
4000
3000
2000
1000
0

# of erase operations *(103)

# of erase operations *(103)

Figure 9 gives the effect of varying the number of update operations from one
million to 100 millions (in a log scale) on the number of erase operations for both
one-dimensional (i.e., FAST-Btree, LA-tree, and FD-tree in Fig. 10a) and multidimensional index structures (i.e., FAST-Rtree and RFTL in Fig. 10b). As we are
only interested in update operations, the workload for the experiments in this section
is just a stream of incoming update operations, up to 100 million operations. As
can be seen from the figure, FAST scales well with the number of updates and still
maintains its superior performance over its counterparts from both one-dimensional
(LA-tree) and multi-dimensional index structures (RFTL). FAST performs slightly
better than FD-tree; this is because FD-tree (one dimensional index structure) is
buffering some of the tree updates in memory and flushes them when needed, but
FAST applies a flushing policy, which flushes only the block with the highest number
of updates. In addition, FAST* slightly outperforms FAST because FAST* flushing
policy employs a Top-1 algorithm that maximizes the number of updates per block
and minimize the timestamp at which the block has been updated, hence the total
amortized update cost in FAST* is less than FAST.

FAST*–Btree
FAST–Btree
LA–tree
FD–tree

1

10

100

FAST*–Rtree
FAST–Rtree
RFTL

10000

1000

1

10

# of Updates *(106)

# of Updates *(106)

(a) FAST-Btree

(b) FAST-Rtree

Fig. 10 Effect of number of updates

100

Geoinformatica (2013) 17:417–448

443

9.5 Flushing performance

550
500
450
400
350
300
250
200
150
100
50
0
128

600

FAST*–Flush
FAST–Flush
Flush–All
Rand–Flush

256

512

# of erase operations *(103)

# of erase operations *(103)

Figure 11 illustrates the performance of the f lushing policy employed by FAST
compared to a naive flushing policy, termed f lush-all that just flushes all the memory
contents to the flash storage once. We also compare FAST to a random flushing
policy, termed Rand-Flush that chooses a block at random and flushed its contents
to the flash storage The performance is given with respect to various memory
sizes (Fig. 11a) and log file sizes (Fig. 11a). Both experiments were run for FASTBtree under workload WU . Running these experiments for FAST-Rtree and other
workloads give similar performance, and thus omitted for brevity.
Figure 11a gives the effect of varying the memory size from 128 KB to 1,024 KB
on the number of erase operations for flush-all policy, Rand-Flush policy and FAST
flushing policy. In all cases, FAST has much lower erase operations than the flushall and Rand-Flush policies, which is about one fourth of the erase operations for a
memory size of 512 KB. The main reason behind this gain in FAST is that it amortizes
the cost of the block erase operation over a large number of updates, and hence, will
free more memory with each flushing operation. On the other side, in the flush-all or
Rand-Flush policy, a block may be erased just because it has only one single update in
the memory. In this case, although a block is erased, it does not free much memory
space. The Rand-Flush policy performance is slightly better than that of flush-all
policy because the Rand-Flush flushes only one block and hence keeping all other
blocks in memory, which decrease the cost of random writes on these blocks.
FAST* flushing policy is better than FAST flushing policy as it better amortizes
the update cost. FAST* policy may still keep a block that has the highest number of
updates in memory if this block has higher potential to be updated soon, and hence
the decreasing the number of erase operations applied to that block.
Figure 11b gives a similar experiment to that of Fig. 11a with the exception that
we study the effect of changing the log size from 10 MB to 25 MB on the number
of erase operations. In all cases, FAST flushing policy is superior, which is intuitive
given the above explanation for Fig. 11a. However, an interesting observation from
Fig. 11b is that the gain from FAST flushing policy over the flush-all and RandFlush policies increases with the increase of the log file size. This means that FAST
flushing policy makes better use of the log file than the flush-all and Rand-Flush
policies. A justification for this is as follows: As FAST flushing policy evicts a block

1024

FAST*–Flush
FAST–Flush
Flush–All
Rand–Flush

500
400
300
200
100
0

10

15

20

Memory Size (KB)

Log Size (MB)

(a) Memory size

(b) Log size

Fig. 11 Flushing performance

25

444

Geoinformatica (2013) 17:417–448

to the storage only if it has high number of updates, the log entry for this flushing
operation will include many updated nodes. Then, in the log compaction process,
there will be a lot of space for compaction. This would not be the case for the flushall and Rand-Flush policies where a log entry for a flush operation may include only
one flushed node. Then, at the time of log compaction, there will be nothing much
to compact, which means that the log compaction will be called again. As discussed
in Section 9.2 and Fig. 9, log compaction is a major factor in the number of erase
operations. Reducing the frequency of log compaction makes FAST flushing policy
more superior than the flush-all policy. Moreover, FAST* flushing policy slightly
outperforms FAST flushing policy because of the fact that FAST* may prefer to
keep the block that has the highest number of updates in memory leading to less
erase operations on the flash memory storage.
9.6 Log compaction
Figure 12a gives the behavior and frequency of log compaction operations in FAST
when running a sequence of 200 thousands update operations for a log file size of
10 MB. The Y axis in this figure gives the size of the filled part of the log file, started
as empty. The size is monotonically increasing with having more update operations
till it reaches its maximum limit of 10 MB. Then, the log compaction operation is
triggered to compact the log file. As can be seen from the figure, the log compaction
operation may compact the log file from 20 to 60 % of its capacity, which is very
efficient compaction. Another take from this experiment is that we have made only
seven log compaction operations for 200 thousands update operations, which means
that the log compaction process is not very common, making FAST more efficient
even with a large amount of update operations.
9.7 Recovery performance
Figure 12b gives the overhead of the recovery process in FAST, which serves also
as the overhead of the log compaction process. The overhead of recovery increases
linearly with the size increase of the log file contents at the time of crash. This is
intuitive as with more log entries in the log file, it will take more time from the FAST

100

Recovery Time (millisec)

Log File Size (MB)

10
8
6
4
2
0

0

50

100

150

200

FAST

90
80
70
60
50
40
30
20
10

1

2

3

4

5

6

Number of Updates So Far *(103)

Log Size (MB)

(a) Log Compaction

(b) Recovery

Fig. 12 Log compaction and recovery

7

8

9

Geoinformatica (2013) 17:417–448

445

recovery module to scan this log file, and replay some of its operations to recover the
lost main memory contents. However, what we really want to emphasize on in this
experiment is that the overhead of recovery is only about 100 ms for a log file that
includes 9 MB of log entries. This shows that the recovery overhead is a low price to
pay to ensure transaction durability.

10 Conclusion
This paper presented FAST; a generic framework for flash-aware data-partitioning
tree index structures. FAST distinguishes itself from all previous attempts of flash
memory indexing in two aspects: (1) FAST is a generic framework that can be applied
to a wide class of tree index structures, and (2) FAST achieves both ef f iciency
and durability of read and write flash operations. FAST has four main modules,
namely, update, search, f lushing, and recovery. The update module is responsible
on buffering incoming tree updates in an in-memory data structure, while writing
small entries sequentially in a designated flash-resident log file. The search module
retrieves requested data from the flash storage and updates it with recent updates
stored in memory, if any. The f lushing module is responsible on evicting flash blocks
from memory to the flash storage to give space for incoming updates. Finally, the
recovery module ensures the durability of in-memory updates in case of a system
crash.

References
1. PostgreSQL. http://www.postgresql.org
2. Agrawal D, Ganesan D, Sitaraman RK, Diao Y, Singh S (2009) Lazy-adaptive tree: an optimized
index structure for flash devices. PVLDB
3. Agrawal N, Prabhakaran V, Wobber T, Davis J, Manasse M, Panigrahy R (2008) Design
tradeoffs for SSD performance. In: Usenix annual technical conference, USENIX
4. Bayer R, McCreight EM (1972) Organization and maintenance of large ordered indices. Acta
Inform 1:173–189
5. Beckmann N, Kriegel H-P, Schneider R, Seeger B (1990) The R*-tree: an efficient and robust
access method for points and rectangles. In: SIGMOD
6. Birrell A, Isard M, Thacker C, Wobber T (2007) A design for high-performance flash disks. ACM
SIGOPS Oper Syst Rev 41(2):88–93
7. Bouganim L, Jónsson B, Bonnet P (2009) uFLIP: understanding flash IO patterns. In: CIDR
8. Chang Y-H, Hsieh J-W, Kuo T-W (2007) Endurance enhancement of flash-memory storage
systems: an efficient static wear leveling design. In: Proceedings of the annual ACM IEEE Design
Automation Conference, DAC, pp 212–217
9. Chen S (2009) FlashLogging: exploiting flash devices for synchronous logging performance. In:
SIGMOD. New York, NY
10. Comer D (1979) The ubiquitous B-tree. ACM Comput Surv 11(2):121–137
11. Gray J (2006) Tape is dead, disk is tape, flash is disk, RAM locality is king. http://research.
microsoft.com/∼gray/talks/Flash_is_Good.ppt. Accessed Dec 2006
12. Gray J, Fitzgerald B (2008) Flash disk opportunity for server applications. ACM Queue 6(4):18–
23
13. Gray J, Graefe G (1997) The five-minute rule ten years later, and other computer storage rules
of thumb. SIGMOD Rec 26(4):63–68
14. Guttman A (1984) R-trees: a dynamic index structure for spatial searching. In: SIGMOD
15. Hellerstein JM, Naughton JF, Pfeffer A (1995) Generalized search trees for database systems.
In: VLDB
16. Hutsell W (2007) Solid state storage for the enterprise. Storage Networking Industry Association
(SNIA) Tutorial, Fall

446

Geoinformatica (2013) 17:417–448

17. Katayama N, Satoh, S (1997) The sr-tree: an index structure for high-dimensional nearest neighbor queries. In: SIGMOD
18. Kim H, Ahn S (2008) BPLRU: a buffer management scheme for improving random writes in
flash storage. In: FAST
19. Lavenier D, Xinchun X, Georges G (2006) seed-based genomic sequence comparison using a
FPGA/FLASH accelerator. In: ICFPT
20. Lee S, Moon B (2007) Design of flash-based DBMS: an in-page logging approach. In: SIGMOD
21. Lee S-W, Moon B, Park C, Kim J-M, Kim S-W (2008) A case for flash memory SSD in enterprise
database applications. In: SIGMOD
22. Lee S-W, Park D-J, sum Chung T, Lee D-H, Park S, Song H-J (2007) A log buffer-based flash
translation layer using fully-associate sector translation. TECS
23. Leventhal A (2008) Flash storage today. ACM Queue 6(4):24–30
24. Li Y, He B, Luo Q, Yi K (2009) Tree indexing on flash disks. In: ICDE
25. Li Y, He B, Yang RJ, Luo Q, Yi K (2010) Tree indexing on solid state drives. Proceedings of the
VLDB Endowment 3(1–2):1195–1206
26. Ma D, Feng J, Li G (2011) LazyFTL: A page-level flash translation layer optimized for NAND
flash memory. In: SIGMOD
27. McCreight EM (1977) Pagination of B*-trees with variable-length records. Commun ACM
20(9):670–674
28. Moshayedi M, Wilkison P (2008) Enterprise SSDs. ACM Queue 6(4):32–39
29. Nath S, Gibbons PB (2008) Online maintenance of very large random samples on flash storage.
In: VLDB
30. Nath S, Kansal A (2007) Flashdb: dynamic self-tuning database for NAND flash. In: IPSN
31. Reinsel D, Janukowicz J (2008) Datacenter SSDs: solid footing for growth. http://www.samsung.
com/us/business/semiconductor/news/downloads/210290.pdf. Accessed Jan 2008
32. Sellis TK, Roussopoulos N, Faloutsos C (1987) The R+-tree: a dynamic index for multidimensional objects. In: VLDB
33. Shah MA, Harizopoulos S, Wiener JL, Graefe G (2008) Fast scans and joins using flash drives.
In: International Workshop of Data Managment on New Hardware, DaMoN
34. White DA, Jain R (1996) Similarity indexing with the SS-tree. In: ICDE
35. Wu C, Chang L, Kuo T (2003) An efficient R-tree implementation over flash-memory storage
systems. In: GIS
36. Wu C, Kuo T, Chang L (2007) An efficient B-tree layer implementation for flash-memory storage
systems. TECS

Mohamed Sarwat is a PhD candidate at the Computer Science and Engineering department,
University of Minnesota, where he also received his master’s degree in computer science in 2011. His
research interest lies in the broad area of Database systems, spatio-temporal databases, distributed
graph databases, social networking, cloud computing, large-scale data management, data indexing
and storage systems. He has been awarded the University of Minnesota Doctoral Dissertation
Fellowship in 2012/2013. He has been a recipient of Best Research Paper Award in the 12th
international symposium on spatial and temporal databases 2011.

Geoinformatica (2013) 17:417–448

447

Mohamed F. Mokbel is an associate professor in the Department of Computer Science and Engineering, University of Minnesota. His current main research interests focus on providing database
and platform support for spatial data, moving objects, and location-based services. Mohamed is the
main architect for the PLACE, Casper, and CareDB systems that provide a database support for
location-based services, location privacy, and personalization, respectively. His research work has
been recognized by two best paper awards at IEEE MASS 2008 and MDM 2009 and by the NSF
CAREER award 2010. Mohamed is currently the general co-chair of SSTD 2011 and program cochair for MDM 2011, DMSN 2011, and LBSN 2011. Mohamed was also the proceeding chair of ACM
SIGMOD 2010, and the program co-chair for ACM SIGSPATIAL GIS 2008, 2009, and 2010. He
serves in the editorial board of IEEE Data Engineering Bulletin, Distributed and Parallel Databases
Journal, and Journal of Spatial Information Science. Mohamed is an ACM and IEEE member and a
founding member of ACM SIGSPATIAL.

Xun Zhou received his B.Eng., and M.Eng., in Computer Science and Technology from Harbin
Institute of Technology, Harbin, China in 2007 and 2009 respectively. He is currently a Ph.D. student
in Computer Science at the University of Minnesota, Twin Cities. His research interests include
spatiotemporal data mining, spatial databases and Geographical Information Systems (GIS). His
current application focus is understanding climate change from data.

448

Geoinformatica (2013) 17:417–448

Suman Nath is a researcher in the Sensing and Energy Research Group at Microsoft Research
Redmond. He works on various data management problems in mobile and sensing systems. He
received his PhD from Carnegie Mellon University in 2005. He has authored 20+ patents (granted or
pending), 70+ papers in various computer science conferences and journals, and received Best Paper
Awards at BaseNets 2004, USENIX NSDI 2006, IEEE ICDE 2008, and SSTD 2011. At Microsoft,
he received the Gold Star Award, which recognizes excellence in leadership and contributions for
Microsoft’s long-term success.

arXiv:1604.03234v1 [cs.DB] 12 Apr 2016

Hippo: A Fast, yet Scalable, Database Indexing Approach
Jia Yu

Mohamed Sarwat

Arizona State University
699 S. Mill Avenue, Tempe, AZ

Arizona State University
699 S. Mill Avenue, Tempe, AZ

jiayu2@asu.edu

msarwat@asu.edu
TPC-H
2 GB
20 GB
200 GB

ABSTRACT
+

Even though existing database indexes (e.g., B -Tree) speed
up the query execution, they suffer from two main drawbacks: (1) A database index usually yields 5% to 15% additional storage overhead which results in non-ignorable dollar cost in big data scenarios especially when deployed on
modern storage devices like Solid State Disk (SSD) or NonVolatile Memory (NVM). (2) Maintaining a database index incurs high latency because the DBMS has to find and
update those index pages affected by the underlying table
changes. This paper proposes Hippo a fast, yet scalable,
database indexing approach. Hippo only stores the pointers
of disk pages along with light weight histogram-based summaries. The proposed structure significantly shrinks index
storage and maintenance overhead without compromising
much on query execution performance. Experiments, based
on real Hippo implementation inside PostgreSQL 9.5, using the TPC-H benchmark show that Hippo achieves up to
two orders of magnitude less storage space and up to three
orders of magnitude less maintenance overhead than traditional database indexes, i.e., B+ -Tree. Furthermore, the experiments also show that Hippo achieves comparable query
execution performance to that of the B+ -Tree for various
selectivity factors.

1.

Index size
0.25 GB
2.51 GB
25 GB

HDD
0.04 $/GB

Initialization time
30 sec
500 sec
8000 sec

(a) B+ -Tree overhead
E-HDD
SSD
0.1 $/GB 0.5 $/GB

Insertion time
10 sec
1180 sec
42000 sec
E-SSD
1.4 $/GB

(b) Storage dollar cost

Table 1: Index overhead and storage dollar cost

storage overhead. Even though the storage overhead
may not seem too high in small databases, it results in non-ignorable dollar cost in big data scenarios. Table 1a depicts the storage overhead of a B+ Tree created on the Lineitem table from the TPCH [5] benchmark (database size varies from 2, 20 and
200 GB). Moreover, the storage dollar cost is dramatically amplified when the DBMS is deployed on modern storage devices (e.g., Solid State Drives and NonVolatile Memory) because they are still more than an
order of magnitude expensive than Hard Disk Drives
(HDDs) per unit of storage. Table 1b lists the dollar
cost per storage unit collected from Amazon.com and
NewEgg.com (Enterprise is abbreviated to E). In addition, initializing an index may be a time consuming
process especially when the index is created on a large
database table. Such high initialization overhead may
delay the analysis process (see Table 1a).

INTRODUCTION

A database system (DBMS) often employs an index structure, e.g., B+ -Tree [4], to speed up query execution at the
cost of additional storage and maintenance overhead. A
DBMS user may create an index on one or more attributes
of a database table. A created index allows the DBMS to
quickly locate tuples without having to scan the whole indexed table. Even though existing database indexes significantly improve the query response time, they suffer from
the following drawbacks:

• Maintenance Overhead: A DBMS must update the
index after inserting (deleting) tuples into (from) the
underlying table. Maintaining a database index incurs high latency because the DBMS has to find and
update those index entries affected by the underlying
table changes. For instance, maintaining a B+ -Tree
searches the tree structure and perhaps performs a set
of tree nodes splitting or merging operations. That
requires plenty of disk I/O operations and hence encumbers the time performance of the entire DBMS in
big data scenarios. Table 1a shows the B+ Tree insertion overhead (insert 0.1% records) for the TPC-H
Lineitem table.

• Indexing Overhead: Indexing overhead consists of
two parts - storage and initialization time overhead.
A database index usually yields 5% to 15% additional

Existing approaches that tackle one or more of the aforementioned drawbacks are classified as follows: (1) Compressed indexes: Compressed B+ -Tree approaches [7, 8, 19]
1

reduce index storage overhead but all these methods compromise on query performance due to the additional compression and decompression time. Compressed bitmap indexes also reduce index storage overhead [9, 11, 14] but they
mainly suit low cardinality attributes which are quite rare.
For high cardinality attributes, the storage overhead of compressed bitmap indexes significantly increases [17]. (2) Approximate indexes: Approximate indexing approaches [2, 10,
12] trade query accuracy for storage to produce smaller, yet
fast, index structures. Even though approximate indexes
may shrink the storage size, users cannot rely on their unguaranteed query accuracy in many accuracy-sensitive application scenarios like banking systems or user archive systems. (3) Sparse indexes: A sparse index [3, 13, 15, 16] only
stores pointers which refer to disk pages and value ranges
(min and max values) in each page so that it can save indexing and maintenance overhead. It is generally built on
ordered attributes. For a posed query, it finds value ranges
which cover or overlap the query predicate and then rapidly
inspects the associated few parent table pages one by one
for retrieving truly qualified tuples. However, for unordered
attributes which are much more common, sparse indexes
compromise too much on query performance because they
find numerous qualified value ranges and have to inspect a
large number of pages.
This paper proposes Hippo1 a fast, yet scalable, sparse
database indexing approach. In contrast to existing tree index structures, Hippo stores disk page ranges (each works
as a pointer of one or many pages) instead of tuple pointers
in the indexed table to reduce the storage space occupied by
the index. Unlike existing approximate indexing methods,
Hippo guarantees the query result accuracy by inspecting
possible qualified pages and only emitting those tuples that
satisfy the query predicate. As opposed to existing sparse
indexes, Hippo maintains simplified histograms that represent the data distribution for pages no matter how skew it is,
as the summaries for these pages in each page range. Since
Hippo relies on histograms already created and maintained
by almost every existing DBMS (e.g., PostgreSQL), the system does not exhibit a major additional overhead to create
the index. Hippo also adopts a page grouping technique
that groups contiguous pages into page ranges based on the
similarity of their index key attribute distributions. When a
query is issued on the indexed database table, Hippo leverages the page ranges and page summaries to recognize those
pages for which the internal tuples are guaranteed not to satisfy the query predicates and inspects the remaining pages.
Thus Hippo achieves competitive performance on common
range queries without compromising the accuracy. For data
insertion and deletion, Hippo dispenses with the numerous
disk operations by rapidly locating the affected index entries. Hippo also adaptively decides whether to adopt an
eager or lazy index maintenance strategy to mitigate the
maintenance overhead while ensuring future queries are answered correctly.
We implemented a prototype of Hippo inside PostgreSQL
9.5. Experiments based on the TPC-H benchmark show
that Hippo achieves up to two orders of magnitude less storage space and up to three orders of magnitude less maintenance overhead than traditional database indexes, i.e.,
B+ -Tree. Furthermore, the experiments show that Hippo
1

Create an index

Execute a query
User

Age table
Disk Page #

Query predicate

Internal data

1,2,3,4,5,… 21,22,55,75,77,…

Compare

Bucket
2,3,4,…

(Return)

Bucket
1
2
3
4
5

Age
1 - 20
21 - 40
41 - 60
61 - 90
91 - 120

Age = 55
Compare

(Return)

Bucket
3

Complete height balanced histogram
Filter false positives

Generate partial histograms

Hippo
Bucket
2
3
4

Age
21 - 40
41 - 60
61 - 90

Partial histogram 1
Bucket
2
4
5

Age
21 - 40
61 - 90
91 - 120

Partial histogram 2

Page range Partial histogram Internal data
(Return)
1 - 10
2,3,4
21,22,55,75,77
11 – 25
2,4,5
23,24,62,91,92
26 - 30
1,2,5
11,12,25,101,110
Index Entry 1

…

Bucket
1
2
5

Age
1 - 20
21 - 40
91 - 120

Inspect
Page
1 - 10

Partial histogram 3
Legend: Index Initialization

Database
Index Search

Figure 1: Initialize and search Hippo on age table

achieves comparable query execution performance to that
of the B+ -Tree for various selectivity factors.
The remainder of the paper is structured as follows: In
Section 2, we explain the idea of Hippo and show its structure. We demonstrate how to query Hippo swiftly, build
Hippo from scratch, and maintain Hippo efficiently in Section 3, 4 and 5. In Section 6, we provide useful cost estimation for these three scenarios. Extensive experiments
and related analysis are included in Section 7. We discuss
related work then analyze the drawbacks in existing indexes
in Section 8. Finally, Section 9 concludes the paper.

2. HIPPO OVERVIEW
This section gives an overview of Hippo. A running example that describes a Hippo index built on an age table is
given in Figure 1. The figure’s right part which depicts how
to search Hippo and the left part which shows how to initialize Hippo are explained in Section 3 and 4 respectively.
The main challenges of designing an index are to reduce
the indexing overhead in terms of storage and initialization
time as well as speed up the index maintenance while still
keeping competitive query performance. To achieve that,
an index should possess the following two main properties:
(1) Less Index Entries: For better storage space utilization,
an index should determine and only store the most representative index entries that summarize the key attribute.
Keeping too many index entries inevitably results in high
storage overhead as well as high initialization time. (2) Index Entries Independence: Index entries of a created index
should be independent from each other. In other words, the
range of values that each index entry represents should have
minimal overlap with other index entries. Interdependence
among index entries, like that in a B+ -Tree, may lead to
overlapped tree nodes traverse during query processing and
several cascaded updates during index maintenance.

https://github.com/DataSystemsLab/hippo-postgresql

2

Age = 55

Data Structure. When creating an index, Hippo scans
the indexed table and generates histogram-based summaries
for disk pages based upon the index key attribute. Afterwards, these summaries are stored by Hippo along with
pointers of the pages they summarize. As shown in Figure 1, a Hippo index entry consists of the following two
components (Internal data of pages is given in the figure
only for the ease of understanding):

Bucket
1
2
3
4
5

Age > 55
Bucket
1
2
3
4
5

Age > 55 AND Age < 65

Age
1 - 20
21 - 40
41 - 60
61 - 90
91 - 120

Bucket
1
2
3
4
5

Age
1 - 20
21 - 40
41 - 60
61 - 90
91 - 120

Figure 2: Convert query predicates

• Summarized Page Range: The page range (works
as a pointer) represents the IDs (i.e., address) of the
first and last pages summarized by a certain histogram
based summary. DBMS can load particular pages into
buffer according to their customized IDs. Hippo is able
to summarize more than one contiguous (in terms of
physical storage) pages to reduce the overall index size
to a great extent (e.g., Page 1 - 10, 11 - 25, 26 - 30).
The number of summarized pages (denoted as pages
per partial histogram) in each index entry varies. For
a certain index attribute, some contiguous pages have
very similar content but some are not. Hence, Hippo
adopts a page grouping technique that groups contiguous pages into page ranges based on the similarity of
their index attribute distributions, using the partial
histogram density (explained in Section 4).

predicate. The search process leverages the index structure
to avoid worthless page inspection so that Hippo can achieve
competitive query performance.
Algorithm 1: Hippo index search
Data: A given query predicate and Hippo index
Result: Qualified tuples
1 Create Bitmap a for the given predicate;
2 foreach bucket of the complete histogram do
3
if it is hit by the query predicate then
4
Set the corresponding bit in Bitmap a to 1;
5
end
6 end
7 Create Bitmap b for recording all pages;
8 foreach partial histogram do
9
if it has joint buckets with Bitmap a then
10
Set the corresponding bits of the summarized
pages in Bitmap b to 1;
11
end
12 end
13 foreach page marked as 1 in Bitmap b do
14
Check each tuple in it against the predicate;
15 end

• Histogram-based Page Summary: The page summary in each index entry is a partial histogram
that represents a subset of the complete height balanced histogram buckets (maintained by the underlying DBMS). Each bucket if exists indicates that at
least one of the tuples of this bucket exists in the summarized pages. Each partial histogram represents the
distribution of the data in the summarized contiguous
pages. Since each bucket of a height balanced histogram roughly contains the same number of tuples,
each of them has the same probability to be hit by a
random tuple from the table. Hippo leverages this feature to handle data which has various or even skewed
distribution. To save storage space, only bucket IDs
are kept in partial histograms and partial histograms
are stored in a compressed bitmap format. For instance, the partial histogram of the first Hippo index
entry in Figure 1 is stored as 01110. Each bit, set to 1
or 0, reflects whether the corresponding bucket exists
or not.

3.1 Convert query predicates
The main idea is to check each partial histogram against
the given query predicate for filtering false positives and
so speeding up the query. However, as explained in Section 2, partial histograms are stored in bitmap formats without recording value ranges of buckets. Therefore, there has
to be an additional step to recover the missing information
for each partial histogram on-the-fly or convert the predicate to the bitmap format per query. Obviously, the later
one is more efficient.
Any query predicates for a particular attribute can be
broken down into atomic units: equality query predicate
and range query predicate. Age = 55 is a typical equality
query predicate while age > 55 is a range query predicate.
These unit predicates can be combined together by AND
operator like age > 55 AND age <65.
Each unit predicate is compared with the buckets of the
complete height balanced histogram (retrieving method is
discussed in Section 4). A bucket is hit by a predicate if
the predicate fully contains, overlaps, or is fully contained
by the bucket. Each unit predicate can hit one, at least,
or more buckets. For instance, according to the complete
histogram in Figure 1, bucket 3 whose description is 41 - 60
is hit by age = 55 while bucket 3, 4, and 5 are hit by age
> 55. This strategy is also applicable for the conjunct query
predicates. For a conjunct predicate like age > 55 and age
< 65, only buckets which are hit by all these unit predicates

Main idea. Hippo solves the aforementioned challenges as follows: (1) Each index entry summarizes many
pages and each only stores two page IDs and a compressed
bitmap.(2) Each page of the parent table is only summarized by one Hippo index entry. Hence, any updates that
occur in a certain page only affect a single independent index entry. Finally, during a query, pages whose partial histograms do not have desired buckets are guaranteed not to
satisfy certain query predicates and marked as false positives. Thus Hippo only inspects other pages that probably
satisfies the query predicate and achieves competitive query
performance.

3.

Age
1 - 20
21 - 40
41 - 60
61 - 90
91 - 120

INDEX SEARCH

The search algorithm runs in three main steps: (1) Step 1:
convert query predicates, (2) Step 2: filter false positives and
(3) Step 3: inspect possible qualified pages against the query
3

Age = 55

Partial histogram 1

(bitmap)

(bitmap)

Bucket
0
0
1
0
0

AND
0
0
1
0
0

IDs of possible qualified pages are recorded in a separate
bitmap. Each bit in this bitmap is mapped to the page at
the same position in the parent table. For instance, the bit
at position 1 in the bitmap is mapped to the page ID 1 of the
parent table. The value (1 or 0) of this bit reflects whether
the associate page is a possible qualified page or not.
Hippo has to inspect all of the possible qualified pages
recorded in the bitmap against the query predicate one by
one because every retained page from the previous step is
possible to contain qualified tuples. The only way to inspect
these possible qualified pages is to traverse them and check
each tuple in each page one by one. Qualified tuples are
returned to the DBMSs.
Algorithm 1 shows the three steps of Hippo index search.
The right part of Figure 1 describes how to search Hippo
index using a certain query predicate. Firstly, Hippo finds
query predicate age = 55 hits bucket 3. And the first one of
the three partial histograms nicely contains bucket 3. Thus
only the disk pages 1 - 10 are selected as possible qualified
pages which need further inspection. It is also worth noting
that these partial histograms summarize different number of
pages.

Bucket
0
1
1
1
0

Bitwise AND = 1
Figure 3: Bitwise AND two bitmaps to find joint buckets

simultaneously (the joint bucket 3 and 4) are kept as the final
result and others are directly discarded. Figure 2 shows the
hit buckets of three query predicates. Afterwards, the given
query predicate is converted to a bitmap. Each bit in this
bitmap reflects whether the bucket has the corresponding
ID is hit (1) or not (0). Thus the corresponding bits of all
hit buckets are set to 1.

3.2 Filter false positives
4. INDEX INITIALIZATION

Filtering false positives is the most important step of
Hippo index search. Each Hippo index entry stores a page
range and a summary of several contiguous pages but it is
very possible that none of these pages in the certain index
entry contain the qualified tuples especially for small range
queries. This kind of pages and their associated index entries are false positives. This step is to check each partial
histogram against the converted query predicate, recognize
some false positive pages utmost and finally avoid worthless
page inspection on these pages.
A given query predicate hits one ,at least, or more buckets
of the complete histogram. Pages whose partial histograms
contain the hit buckets (the corresponding bitmap bits are
1) might have qualified tuples, whereas pages whose partial histograms don’t contain these buckets (the corresponding bitmap bits are 0) are guaranteed not to contain qualified tuples. The former kind of pages are possible qualified
pages. In contrast, the later kind of pages are false positives
and excluded from the next step - inspect possible qualified
pages. The straight way to find false positive pages is to
do a nested loop between each partial histogram and the
converted query predicate to find the joint buckets.
Interestingly, because both of partial histograms and the
converted query predicate are in bitmap format, the nested
loop can be accelerated by bitwise ’AND’ing the bytes from
both sides, aka bit-level parallelism. If bitwise ’AND’ing
the two bytes from both sides returns 1, that means there
are joint buckets between the query predicate and the partial histogram. Thus the pages are possible qualified pages.
Figure 3 provides an example of how to perform a bitwise
AND using the same data in Figure 1.

Hippo performs three main steps to initialize itself:
(1) Retrieve a complete histogram, (2) Generate partial histograms, and (3) Group similar pages into page ranges, described as follows.
Algorithm 2: Hippo index initialization
Data: Pages of a parent table
Result: Hippo index
1 Create a working partial histogram (in bitmap format);
2 Set StartPage = 1 and EndPage = 1;
3 foreach page do
4
Find distinct buckets hit by its tuples;
5
Set associated bits to 1 in the partial histogram;
6
if the working partial histogram density > threshold
then
7
Store the partial histogram and the page range
(StartPage and EndPage) as an index entry;
8
Create a new working partial histogram;
9
StartPage = EndPage + 1;
10
EndPage = StartPage;
11
else
12
EndPage = EndPage + 1;
13
end
14 end

4.1 Retrieve a complete histogram
Histograms used in Hippo include a complete height balanced histogram and many partial histograms. A complete
height balanced histogram represents the distribution of all
tuples and already exists in DBMSs. Respectively, a partial
histogram, as a subsection, only contains partial buckets
from the complete histogram. Therefore, for generating any
partial histograms, a complete histogram should be retrieved
at the first priority. Full-fledged functions for retrieving a
complete histogram exist in any DBMSs. Detailed explanation for these functions is omitted in this paper since it is

3.3 Inspect possible qualified pages
The previous step recognizes many false positive pages
and excludes them from possible qualified pages. However,
one fact is that not all false positives can be detected by
the previous step. Possible qualified pages still may contain
false positives and this is why they are called ”possible”.
This step is to inspect the tuples in each possible qualified
pages and retrieve the qualified tuples directly.
4

histograms into one larger partial histogram (in other words,
summarizing more pages within one partial histogram) can
make Hippo more efficient. On the other hand, users may
want to shrink Hippo physically to a greater extent. For
example, if a partial histogram can summarize 10 pages in
one go, the new Hippo size will be much smaller. Grouping
more pages into one page range and summarizing them with
just one partial histogram are expected and practical as well.
Yet, this is not saying that all pages should be grouped
together and summarized by one merged partial histogram.
As more and more pages are summarized, this partial histogram contains more and more buckets until all buckets
from the complete histogram are included. At this moment,
this partial histogram becomes a complete histogram and
covers any possible query predicates. That means this kind
of partial histograms is unable to help Hippo to filter the
false positives and the disk pages summarized by this partial histogram will be always treated as possible qualified
pages.
One strategy is to group a fixed number of contiguous
pages per range/partial histogram. Yet, this strategy is
not suitable if some contiguous pages in a certain area have
much more similar data distribution than other areas. Lacking the awareness of data distribution cannot reduce storage overhead smartly. Under this circumstance, it is better
to let Hippo group more pages together in this area and
group less pages together in other areas dynamically. For
instance, assume original pages per partial histogram is 100.
If there are 1000 out of 10000 disk pages and the tuples in
these 1000 pages are exactly same, a better way to shrink
the index size is to set the P from 100 to 1000 for grouping/summarizing these 1000 pages into one range/partial
histogram and change it back to 100 for other 9000 pages.
A terminology - partial histogram density is introduced
here. The density of a partial histogram is the percentage
of kept buckets in the total buckets of a complete histogram.
The complete histogram has a density value of 1. The definition can be formalized as follows:

not our focus. We also assume that the complete histogram
is not changed at any time because the global distribution
of the parent table will not be affected even if some local
updates are performed.
The resolution of the complete histogram (denoted as histogram resolution) is adjustable. A complete histogram is
considered as higher resolution if it contains more buckets.
The resolution of partial histograms is consistent with their
complete histograms technically. It is apparent that a complete histogram will have larger physical size if it has higher
resolution and, accordingly, the numerous partial histograms
are also physically larger than the low resolution ones. On
the other hand, the histogram resolution also affects Hippo
query time. The cost estimation section will further discuss
this issue.

4.2 Generate partial histograms
A partial histogram only contains some buckets from the
complete histogram. It is used to represent the distribution
of parent tuples in one or many disk pages. In other words,
people can get an approximate overview from the partial
histogram of these pages: What values might lie in these
pages and what do not. These partial histograms are able
to help Hippo to recognize false positives utmost and avoid
worthless page inspection. We explain how to generate a
partial histogram for each disk page in this section.
Generating partial histograms traverses all disk pages of
the parent table from the start to end. For each page, a
nested loop passes through each tuple in this page. The
specified attribute value is extracted from each tuple and
compared with the complete histogram (using a binary
search). Buckets hit by tuples are kept for this page and
then compose a partial histogram. A partial histogram only
contains distinct buckets. For instance, there is a group of
age values like the first entry of Hippo shown in Figure 1:
21, 22, 55, 75, 77. Bucket 2 is hit by 21 and 22, bucket 3 is
hit by 55 and bucket 4 is hit by 77. Therefore, the partial
histogram for these values is just as partial histogram 1 in
Figure 1.
Shrinking the physical size of partial histograms is desirable. The basic idea is to drop all bucket value ranges and
only keep bucket IDs. Hippo in Figure 1 shows the effect.
Actually, as mentioned in Section 2, dropping value range
information does not impact much on the index search. To
further shrink the size, storing bucket IDs in integer type (4
bytes or more) is also considered as an overhead. Bitmap format storage is a better choice to bypass this overhead. Each
partial histogram is stored as a bitmap. Each bit in a bitmap
stands for a bucket at the same position in a complete histogram. Bit value 1 means the associated bucket is hit and
kept in this partial histogram while 0 means the associated
bucket is not included. Bitmap compression is introduced to
Hippo as well. The partial histogram in a bitmap format can
be compressed by any existing compression techniques. The
time of compressing and decompressing partial histograms is
ignorable in contrast to that of inspecting possible qualified
pages.

P artial histogram density =

Bucketspartial histogram
Bucketscomplete histogram

This density has an important phenomenon that, for a group
of contiguous pages, their merged partial histogram density
will be very low if these pages are very similar, vice versa.
Therefore, a partial histogram with a certain density may
summarize more pages if these contiguous pages have similar data, vice versa. Making use of this phenomenon enables Hippo to dynamically group pages and merge partial
histograms into one. In addition, it is understandable that
a lower density partial histogram (summarizes less pages)
has the high probability to be recognized as false positives
so that speed up queries.
User can easily set a same density for all partial histograms as a threshold. Each partial histogram can automatically decide how many pages it should summarize.
Algorithm 2 depicts how to initialize a Hippo and summarize more pages within one partial histogram with the help
of the partial histogram density. The basic idea is that new
pages will not be summarized into a partial histogram if
its density is larger than the threshold and a new partial
histogram will be created for the following pages.
Figure 1’s left part depicts how to initialize a Hippo on
the age table with a partial histogram density 0.6. All of

4.3 Group similar pages into page ranges
Generating a partial histogram per one disk page is as easy
as that in Section 4.2. However, for some contiguous pages
which have similar data, it is a waste of storage. Grouping
them together as many as possible and merging their partial
5

Algorithm 3: Update Hippo for data insertion
Data: A new inserted tuple belongs to Page a
Result: Updated Hippo
1 Find the bucket hit by the inserted tuple;
2 Locate a Hippo index entry which summarizes Page a;
3 if one index entry is located then
4
Retrieve the associated Hippo index entry;
5
Update the retrieved entry if necessary;
6 else
7
Retrieve the Hippo entry summarizes the last page;
8
if the partial histogram density < threshold then
9
Summarize Page a into the retrieved entry;
10
else
11
Summarize Page a into a new entry;
12
end
13 end

Updated Hippo
S	
 t
e #
L

H



Page range Partial histogram Internal data
1 - 10
2,3,4
21,22,55,75,77
Blank space
2 - 3
12
111221111
11  2

124

Mve

132324 29192

Figure 4: Hippo Index Entries Sorted List

time-consuming if every entry is retrieved from disk, deserialized and checked against the target page. Therefore,
a binary search on Hippo index entries is a good choice.
(This search actually leverages the index entries sorted list
explained in Section 5.3.)
Step 3: Update the index entry: If the new tuple
belongs to a new page not summarized by any Hippo index
entries and the density of Hippo partial histogram which
summarizes the last disk page is smaller than the density
threshold set by users, this new page will be summarized
into this partial histogram in the last index entry otherwise
a new partial histogram will be created to summarize this
page and stored in a new Hippo index entry. For a new tuple
belongs to pages already summarized by Hippo, the partial
histogram in the associated index entry will be updated if
the inserted tuple hits a new bucket.
It is worth noting that: (1) Since the compressed bitmaps
of partial histograms may have different size, the updated
index entry may not fit the space left at the old location.
Thus the updated one may be put at the end of Hippo.
(2) After some changes (replacing old or creating new index
entry) in Hippo, the corresponding position of the sorted list
needs to be updated.

the tuples are compared with the complete histogram and
IDs of distinct buckets hit by tuples are generated as partial
histograms along with page range.
So far, as Figure 1 shows, each entry in a Hippo index has the following content: a partial histogram in compressed bitmap format and two integers stand for the first
and last pages summarized by this histogram (summarized
page range). Each entry is serialized and stored on disk.

5.

inte

INDEX MAINTENANCE

Inserting (deleting) tuples into (from) the indexed table
requires maintaining the index to ensure that the DBMS
can retrieve the correct set of tuples that match the query
predicate. However, the overhead of maintaining the index
quite frequently may preclude system scalability. This section explains how Hippo handles updates.

5.2 Data deletion

5.1 Data insertion

The eager update strategy is not highly desired for data
deletion. Hippo still ensures the correctness of queries even
if it doesn’t update itself at all after deleting tuples from a
table. This benefits by inspecting possible qualified pages
in index search. Pages used to have qualified tuples might
be still marked as possible qualified pages but they are discarded after being inspected against the query predicates. A
periodic update or bulk update will be a good choice here.
For data deletion, Hippo adopts a lazy update strategy
that maintains the index after a bulk of delete operations.
In such case, Hippo traverses each index entry from the start
to end. For each index entry, Hippo inspects the header of
each summarized page for seeking notes made by DBMSs
(e.g., PostgreSQL makes notes in page headers if data is
removed from pages). Hippo re-summarizes the entire index
entry instantly within the original page range if data deletion
on one page is detected. The re-summarization follows the
same steps in Section 4. It is worth noting that this updated
Hippo index entry is not leading to the update on the sorted
list because the updated partial histogram, having same or
less buckets, can obtain same or less compress bitmap size
and the new index entry certainly fits the old space.

Hippo should instantly update or check the index at least
after inserting one record into the indexed table. Otherwise, all subsequent queries might miss the newly inserted
tuple since it is not reflected by the index. Therefore, Hippo
adopts an eager update strategy when a new tuple is inserted. Data insertion may change the physical structure
of a table. The new tuple may belong to any pages of the
indexed table. The insertion procedure (See Algorithm 3)
performs the following steps: (1) Find buckets hit by the
new tuple, (2) Locate the affected index entry, and (3) Update the index entry if necessary.
Step 1: Find buckets hit by the new tuple: Similar
with some steps of generating partial histogram in index
initialization, after retrieving the complete histogram, the
newly inserted tuple is checked against it using a binary
search and a bucket hit by this new tuple is found.
Step 2: Locate the affected index entry: The new
tuple has to belong to one page in this table. This page may
be a new one which has not been summarized by any partial
histograms before or an old one which has been summarized.
However, because the numbers of pages summarized by each
histogram are different, searching Hippo index entry to find
the one contains this target page is inevitable. From the
perspective of disk storage, in a Hippo, all partial histograms
are stored on disk in a serialized format. It will be extremely

5.3 Index Entries Sorted List
When a new tuple is inserted, Hippo executes a fast binary
search (according to the page IDs) to locate the affected
6

Term
H
D
P
T
Card
pageCard
SF

Qy  ate 1

Definition
Complete histogram resolution which means the
number of buckets in this complete histogram
Partial histogram density which is an user supplied threshold
Pages summarized by one partial histogram for a
certain attribute
Tuples summarized by one partial histogram for
a certain attribute
Cardinality (the total number of tuples) of the
indexed table
Number of tuples in each page of the indexed table
Query selectivity factor =

Query output
Query input

Miss…
Partial histogram
O
Qy  ate 2

Figure 5: Visualize how to filter false positives

The first step of Hippo index search is to traverse Hippo
index entries. Pages in each index entry are likely to be
selected for further inspection if their associated partial histogram has joint buckets with the query predicate. Determining the probability of having joint buckets contributes
to the query time cost estimation.
For the ease of presentation, Figure 5 visualizes the procedure of filtering false positives according to their partial
histograms. Partial histogram density (D) of this index is
0.2. The complete histogram constitutes of 10 buckets in
total (H = 10). Assume the indexed table’s tuples follow an
uniform distribution based upon the key attribute. Let the
query selectivity factor (SF ) be 20%. In Figure 5, buckets
hit by the query predicates and the partial histogram are
represented in a bitmap format. According to this figure,
the partial histogram misses a query predicate if the highlighted area of the predicate falls into the blank area of the
partial histogram, whereas a partial histogram is selected if
the predicate does not fall completely into the blank area of
the histogram. In other words, the probability of a partial
histogram having joint buckets with a predicate depends on
how likely a predicate doesn’t fall into the blank area of
a partial histogram. The probability is determined by the
formula given below (The terms are defined in Table 2):

* 100%

Table 2: Notations used in Cost Estimation

index entry and then updates it. Since the index entries are
not guaranteed to be sorted based on the page IDs (noted in
data insertion section), an auxiliary structure for recording
the sorted order is introduced to Hippo.
The sorted list is initialized after all steps in Section 4
with the original order of index entries and put at the first
several index pages of Hippo. During the entire Hippo life
time, the sorted list maintains a list of pointers of Hippo
index entries in the ascending order of page IDs. Actually
each pointer represents the fixed size physical address of
an index entry and these addresses can be used to retrieve
index entries directly. That way, the premise of a binary
search has been satisfied. Figure 4 depicts the Hippo index
entries sorted list. Index entry 2 in Figure 1 has a new
bucket ID 1 due to a newly inserted tuple in its internal
data and hence this entry becomes the last index entry in
Figure 4. The sorted list is still able to record the ascending
order and help Hippo to perform a binary search on the
index entries. In addition, such sorted list leads to slight
additional maintenance overhead: Some index updates need
to modify the affected pointers in the sorted list to reflect
the new physical addresses.

6.

P rob = (Buckets hit by a query predicate) ∗ D
= (SF ∗ H) ∗ D

(1)

To be precise, P rob follows a piecewise function as follows:

COST ESTIMATION

P rob =

This section gives a detailed cost estimation of Hippo.
We first provide an accurate query time cost model which
assists the DBMS query optimizer in picking an efficient
query execution plan. Estimating the storage overhead of
an index can also facilitate better disk space management
and planning. Index initialization certainly consumes a large
chunk of time. Similarly, index maintenance can present a
significant time overhead in any write-intensive application.
Both of them should be carefully estimated.
Table 2 summarizes the main notations we use to derive
the cost model. Given a database table R with Card number
of tuples (i.e., cardinality) and average number of tuples per
disk page equal to pageCard, a user may create a Hippo
index on attribute ai of R. When initializing the index,
Hippo sets the complete histogram resolution to H (it has
H buckets in total) and the partial histogram density to D.
Assume that each Hippo index entry summarizes P indexed
table pages (in terms of pages)/ T tuples (in terms of tuples).
P and T vary for each index entry. Queries executed against
the index have average selectivity factor SF .

(

(SF ∗ H) ∗ D
1

1
S∗H 6 D
1
SF ∗ H > D

SF ∗ H ∈ {1, 2, 3, 4, ...}
SF ∗ H should be no smaller than 1 no matter how small
SF is. Because the query predicate at least hits one bucket
of the complete histogram. Therefore, the probability in
Figure 5 is 20% × 10 × 0.2 = 40%. That means pages summarized by each index entry have 40% probability to be selected as possible qualified pages. Given the aforementioned
discussion, we observe the following:
Observation 1: When SF and H are fixed, the smaller
D is, the smaller P rob is.
Observation 2: When H and D are fixed, the smaller
SF is, the smaller P rob is.
Observation 3: When SF and D are fixed, the smaller
H is, the smaller P rob is.
In fact, the probability given above is equal to the percentage of inspected tuples in all tuples. In addition, considering
that Hippo index entries are much less than the inspected
tuples of the parent table, the total query time cost estimation is mainly decided by the time spent on inspecting
possible qualified pages. Thus, the query time cost estimation (in terms of disk I/O) can be concluded as follows:

6.1 Query time
7

Query time = (P rob ∗ Card)

(2)

Hippo index entries =

If we substitute P rob with its piecewise function, the
query time cost is as follows:

=

Card
T
1
H ∗ (H
+

(5)
Card
+ ... +

1
H−1

1
)
H−D∗H+1

(6)
Query time =

(

(SF ∗ H) ∗ D ∗ Card
Card

SF ∗ H 6
SF ∗ H >

1
D
1
D

D ∈ [ pageCard
, 1]
H
Some observations can be obtained from Formula 6:
Observation 1 For a certain H, the higher D there is,
the less Hippo index entries there are.
Observation 2 For a certain D, the higher H there is,
the less Hippo index entries there are. Meanwhile, the size
of each Hippo index entry is increasing with the growth of
the complete histogram resolution.
Index initialization time hinges on the number of disk I/Os
because it takes much more time than memory I/Os. In
general, the initialization time is composed of two parts:
retrieve parent tuples one by one and write index entry to
disk one by one. Accordingly, Hippo initialization time can
be deduced as follows:

SF ∗ H ∈ {1, 2, 3, 4, ...}

6.2 Indexing overhead
Indexing overhead which consists of storage overhead and
initialization time highly hinges on the number of index entries in an index. The more index entries there are, the more
disk writes and storage space an index costs. B+ -Tree and
other indexes take huge disk space and time for storing their
substantial nodes one by one.
The first problem in estimating the number of Hippo index
entries is that: how many disk pages (P ) are summarized by
one partial histogram in general? Or, how many tuples (T )
are checked against the complete histogram for generating
one partial histogram? Interestingly, this problem is very
similar with Coupon Collector’s Problem[6]. This problem
can be described like that: ”A vending machine sells H types
of coupons (a complete histogram with H buckets). Alice
is purchasing coupons from this machine. Each time (each
tuple) she can get a random type coupon (a bucket) but
she might already have a same one. Alice keeps purchasing
until she gets D ∗ H types of coupons (distinct buckets).
How many times (T ) does she need to purchase?”
Therefore, the expectations of T and P are determined by
the following formulas (The terms are defined in Table 2):
H
H
H
H
+
+
+ ... +
H
H −1
H −2
H −D∗H +1
1
1
1
+ ... +
)
=H ∗( +
H
H −1
H −D∗H +1
T
P =
pageCard

Hippo initialization time = Card + Hippo index entries
(7)
The number of Hippo index entries mentioned in the formula
above can be substituted by its mathematical expectation
in Formula 6.

6.3 Maintenance time
Data insertion. Hippo updates itself eagerly for data
insertion so that this operation is relatively time sensitive.
There are five steps cost disk I/Os in this update: retrieve
the complete histogram, locate associated Hippo index entry, retrieve the associated index entry, update the index
entry (if necessary) and update the mapped sorted list element. It is not hard to conclude that locating the associated index entry completes in log(Hippo index entries) I/O
times, whereas other four steps are able to accomplish their
assignments in constant I/O times. Thus the data insertion
time cost estimation model is summarized as follows under
different conditions:

T =

(3)
(4)

, 1]
D ∈ [ pageCard
H
The product of D ∗ H is the actual number of buckets
in each partial histogram. This value should be no smaller
than the tuples per disk page (pageCard) in case that each
tuple in a certain page hit one unique bucket.
For instance, in a Hippo, the complete histogram has 1000
buckets in total and the partial histogram density is 0.1. The
105.3
expectation of T and P will be 105.3 and pageCard
respectively. That means each partial histogram may summarize
105.3
pages under this circumstance. In another exampageCard
ple, if the total number of buckets is 10000 and the density
2230
is 0.2, T and P will be 2230 and pageCard
correspondingly.
After being aware of the expectation of the number of P ,
it is not hard to deduce the approximate number of index
entries in a Hippo. Thus the estimation of Hippo index
entries number is Formula 5. If we substitute T with their
mathematical expectations in Formula 3 and Formula 5 will
be changed to Formula 6. Hippo index size is equal to the
product of the number of index entries and the size of one
entry which roughly depends on each partial histogram size
(in compressed bitmap format).

Data insert time = log(Hippo index entries) + 4

(8)

Hippo index entries mentioned in Formula 8 can be substituted by its mathematical estimation in Formula 6.
Data deletion. Hippo updates itself lazily for data deletion so that it is hard to finalize a general estimation model.
However, it is recommended that do not update Hippo for
data deletion too frequently because Hippo will re-traverse
and re-summarize all disk pages summarized by one Hippo
index entry once it detects that one disk page has data deletion. This algorithm is more suitable for bulk deletion and
lazy update strategy.

7. EXPERIMENTS
This section provides extensive experiments of Hippo
along with reasonable analysis for supporting insights discussed before. For the ease of testing, Hippo has been implemented into PostgreSQL 9.5 kernel. All the experiments
are completed on PostgreSQL.
8

2
1
0

25x

25x

9

B+-Tree

Hippo

2.8x

6
3
2x

1.5x

0
2
20
200
TPC-H workload size (GB)

x 10000000

25x

Insertion time (ms)

Hippo

x 1000000

B+-Tree

Initialization time (ms)

x 10000

Index Size (MB)

3

5

Hippo

1200x

2
1
0
2
20
200
TPC-H workload size (GB)

2
20
200
TPC-H workload size (GB)

(a) Index size

B+-Tree

(c) Index insertion time

(b) Index initialization time

2
1
0

5

B+-Tree

Hippo

4
3
2
1
0

x 1000000

Hippo

Query time (ms)

B+-Tree

x 100000

5

Query time (ms)

x 10000

Query time (ms)

Figure 6: Index overhead on different TPC-H workload size
5

B+-Tree

Hippo

4
3
2
1
0

0.001%
Query selectivity (%)

0.01%
0.1%
1%
Query selectivity (%)

(a) 2 GB

(b) 20 GB

0.001%

0.01%
0.1%
1%
Query selectivity (%)

(c) 200 GB

Figure 7: Index query time on different TPC-H workload size
It is also worth noting that the final Hippo implementation in PostgreSQL has some slight differences from the
details above caused by some platform-dependent features
as follows:
Automatically inspect pages: Hippo only records possible qualified page IDs in a tid bitmap format and returns it
to the kernel. PostgreSQL will automatically inspect pages
and check tuples against query predicates.
Store the complete histogram on disk: Compared
with other disk operations, retrieving the complete histogram from PostgreSQL system cache is relatively slow so
that Hippo stores it on disk and executes a binary search on
it when query or update for data insertion and deletion. It
is better to rebuild Hippo index if there is a huge change of
the parent attribute’s histogram.
Vacuum tables to physically delete data: PostgreSQL DELETE command does not really remove data
from disk unless a VACUUM command is called automatically or manually. Thus Hippo will update itself for data
deletion when a VACUUM command is called.

Datasets and Workload. We use TPC-H workload in
the experiments with different scale factors (2, 20, 200). The
corresponding dataset sizes are 2 GB, 20 GB and 200 GB.
All TPC-H data follows an uniform distribution. We use
the largest table of TPC-H workload - Lineitem table in
most experiments and it has three corresponding sizes: 1.3
GB, 13.8 GB and 142 GB. We compare the query time of
Hippo with B+ -Tree through different query selectivity factors (0.001%, 0.01%, 0.1% and 1%). In addition, we also
test the two indexes using TPC-H standard queries 6, 15
and 20. We use TPC-H standard refresh operation (insert
0.1% new tuples into the DBMS) to test the maintenance
overhead of B+ -Tree and Hippo.
Experimental Setup. The test machine has 8 CPUs
(3.5 GHz per core), 32 GB memory, and 2 TB magnetic
disk with PostgreSQL 9.5 installed. Unless mentioned otherwise, Hippo sets the default partial histogram density to
20% and the default histogram resolution to 400. The impact of parameters is also discussed.

7.1 Implementation Details

7.2 Pre-tune Hippo parameters

We have implemented a prototype of Hippo inside the core
kernel of PostgreSQL 9.5 as one of the main index access
methods by leveraging the underlying interfaces which include but not limited to ”ambuild”, ”amgetbitmap”, ”aminsert” and ”amvacuumcleanup”. A database user is able to
create and query a Hippo index as follows:

Hippo is a flexible index which can be tuned by the
database user to perfectly fit his specific scenarios. There are
two parameters, partial histogram density D (Default value
is 20%) and complete histogram resolution H (Default value
is 400), discussed in this section. Referring to the estimation
before, both of them have impacts on index size, initialization time, and query time. For these experiments, we build
Hippo and B+ -tree on ”partkey” attribute in Lineitem table
of 200 GB TPC-H workload. As mentioned in Introduction,
B+ -Tree has 25 GB index size at this time.

CREATE INDEX hippo_idx ON lineitem USING hippo(partkey)
SELECT * FROM lineitem
WHERE partkey > 1000 AND partkey < 2000

7.2.1 Impact of partial histogram densities

DROP INDEX hippo_idx

9

3
2
1
0

B+-Tree

Hippo

4
3
2
1
0
400
1600
Hippo histogram resolution

Figure 8: Partial histogram density

Figure 9: Histogram resolution

Density (D)
Resolution (R)

6

B+-Tree

Hippo

4
2
0

20%
40%
Hippo partial histogram density (%)

Parameter
Default

x 1000000

4

5

Query time (ms)

Hippo

x 1000000

B+-Tree

Query time (ms)

x 1000000

Query time (ms)

5

Value
D=20% R=400

Size
1012 MB

Initial. time
2765 sec

40%

680 MB

2724 sec

80%

145 MB

2695 sec

800

822 MB

2762 sec

1600

710 MB

2760 sec

Q6
-

Figure 10: TPC-H standard queries

impact on the index size and initialization time is given in
Table 3 and the impact on query time is depicted in Figure 9.
As Table 3 illustrates, with the growth of histogram resolution, Hippo size reduces moderately. The explanation
is that Hippo which has higher histogram resolution consists of less partial histograms and each partial histogram in
this Hippo may summarize more pages but the partial histogram (in bitmap format) has larger physical size because
the bitmap has to store more bits.
As Figure 9 shows, the query time of three Hippos varies
with the growth of histogram resolution. This is because for
the large histogram resolution, the query predicate may hit
more buckets so that this Hippo is more likely to overlap
with query predicates and result in more pages are selected
as possible qualified pages. At this selectivity factor, Hippo
which has histogram resolution 400 is just a little bit worse
than B+ -Tree in terms of query time.

Table 3: Parameters affect Hippo indexing overhead

Hippo introduces a terminology ”partial histogram density” to dynamically control the number of pages summarized by one partial histogram. Based on the discussion
before, the partial histogram density may affect Hippo size,
initialization time and query time. The following experiment compares the default Hippo density (20%) with two
different densities (40% and 80%) and tests their query time
with selectivity factor 0.1%. According to the discussion in
Section 6.2, partial histograms under the three different density setting may summarize around 2 pages, 5 pages and 17
pages respectively (if one page contains 50 tuples). Thus it
can be estimated that the index size of 20% density Hippo
is around 2 times of 40% density Hippo and 8 times of 80%
density Hippo. The impact of the density on Hippo size and
initialization time is described in Table 3 and the impact on
query time is described in Figure 8.
It can be observed that as we increase the density, Hippo
indexing overhead decreases as expected (up to two orders
of magnitude smaller than B+ -Tree in terms of storage) because Hippo is able to summarize more pages per partial
histogram and write less index entries on disk. Similarly,
Hippo which has higher density costs more query time because it is more likely to overlap with query predicates and
result in more pages are selected as possible qualified pages.
At this selectivity factor, Hippo which has density 20% is
just a little bit worse than B+ -Tree in terms of query time.

7.3 Compare Hippo to B+ -Tree
This section compares Hippo with B+ -Tree in terms of
indexing overhead (index size and initialization time), index
maintenance overhead and index query time. To further illustrate the advantages of Hippo, we also compare these indexes using TPC-H standard queries. Hippo tested in this
section uses the default setting which has histogram resolution 400 and partial histogram density 20%.

7.3.1 Indexing overhead
The following experiment builds B-Tree and Hippo on
attribute ”partkey” in Lineitem table of TPC-H workload
(2 GB, 20 GB and 200 GB) and measures their indexing
overhead including index size and index initialization time.
Hippo only stores disk page pointers along with their summaries so that it may have much less index entries in contrast
with B+ -Tree. Thus it is not difficult to understand that
Hippo remains an index size which is lower than B+ -Tree.
In addition, referring to the discussion in the initialization
time estimation model, Hippo initialization time should be
far less than B+ -Tree because B+ -Tree has numerous nodes
to be written to disk.
As Figure 6a illustrates, the index size increases with the
growth of data size. The index size of Hippo is around
25 times smaller than that of B+ -Tree on all workload
sizes. Thus Hippo significantly reduces the storage overhead. Moreover, as Figure 6b shows, Hippo index initialization is at least 1.5x faster that of B+ -Tree.

7.2.2 Impact of histogram resolutions
Each partial histogram of Hippo is composed of some
buckets from the complete histogram. The number of buckets in this complete histogram represents the histogram resolution. The more buckets there are, the higher resolution
the complete histogram has. According to the discussion
before, the histogram resolution may affect index size, initialization time and query time. The following experiment
compares the default Hippo histogram resolution (400) with
two different histogram resolutions (800 and 1600) and tests
their query time with selectivity factor 0.1%. The density

7.3.2 Index maintenance overhead
Hippo updates itself eagerly after inserting a tuple into the
parent table. This eager update strategy for data insertion
10

is also adopted by B+ -Tree so that the two indexes can be
compared together. In terms of update time complexity, B+ Tree has approximate log(Card) and Hippo has (log(Hippo
index entries) + 4). Thus it can be predicted that, for inserting same percentage of tuples, while the update time of
Hippo and B+ -Tree is increasing with the growth of data
size. Hippo will take much less time to update itself than
B+ -Tree because Card is much larger than the number of
Hippo index entries. And also the difference of update
time between Hippo and B+ -Tree will be larger on larger
workload. The experiment uses TPC-H Lineitem table and
creates B+ -Tree and Hippo on attribute ”partkey”. Afterwards, TPC-H refresh transaction which inserts 0.1% new
tuples into Lineitem table is executed. The insertion time
of the indexes is compared in Figure 6c.
As Figure 6c shows, the two indexes take more time to
update on large workload. And also the difference between B+ -Tree and Hippo is more obvious (1200x) on the
largest workload as expected. This is because B+ -Tree
spends much more time on searching proper tuple insert
location (log(Card)) and its update time is increasing with
the growth of TPC-H workload.
Hippo updates itself lazily after deleting data which means
it updates itself after many data deletions occur. In contrast,
B+ -Tree takes an eager update strategy which has around
log(Card) update time cost. It may not make much sense
to compare the two indexes for data deletion.

Index
type

Fast
Query

Guaranteed
Accuracy

Low
Storage

Fast
Maintenance

B+ -Tree

✓

✓

✗

✗

Compressed

✗

✓

✓

✗

Approximate

✓

✗

✓

✗

Sparse

✗

✓

✓

✓

Hippo

✓

✓

✓

✓

Table 4: Compared Indexing Approaches

tor is 0.1%. Thus we find three TPC-H queries which have
typical range queries on ”l shipdate” attribute (Query 6, 15
and 20) and set the range query selectivity factor to 0.1%
which means one week. The query plans of the three queries
are described as follows:
Query 6 This query has a very simple plan. It firstly
performs an index search on Lineitem table using one of
the candidate indexes, then filters the returned values and
finally aggregates the values to calculate the result.
Query 15 This query builds a sub-view beforehand and
embeds it into the main query twice. The range query which
leverages the candidate indexes is a part of the sub-view.
Query 20 The candidate indexes are invoked in a subquery. Then range query results are sorted and aggregated
for calculation. The result is cached into memory and used
the upper level query.
As Figure 10 depicts, Hippo consumes similar query time
with B+ -Tree on Query 6, 15 and 20. The difference between the two indexes is more obvious on Query 15 because
this query invokes the range query twice. Therefore, we
may conclude that Hippo may achieve almost similar query
performance with B+ -Tree at the 25 times smaller storage
overhead when the query selectivity factor is 0.1%.

7.3.3 Impact of query selectivity factors
In this experiment, the query selectivity factors used for
B+ -Tree and Hippo are 0.001%, 0.01%, 0.1% and 1%. According to the query time cost estimation of Hippo, the corresponding query time costs in this experiment are 0.2Card,
0.2Card, 0.2Card and 0.8Card. Therefore, it can be predicted that there will be a great time gap between the first
three Hippo queries and the last one Hippo query. On the
other hand, B+ -Tree should be faster than Hippo at low
query selectivity factor like 0.001% but the difference between the two indexes should be narrowed with the growth
of query selectivity factors.
The result in Figure 7 perfectly matches our predication:
the last Hippo query consumes much more time than the
first three queries. Among them, query time of 0.1% selectivity factor query is a little higher than the first two because
it returns more query results which costs more to retrieve.
Both indexes cost more time on queries with the decreasing of query selectivity factors. B+ -Tree has almost similar
query time with Hippo at 0.1% query selectivity factor. It
is worth noting that B+ -Tree consumes 25 times more storage than Hippo. Therefore, we may conclude that Hippo
makes a well tradeoff between query time and index storage
overhead on medium query selectivity factors like 0.1% so
that, under this scenario, Hippo is a good substitution for
B+ -Tree if the database user is sensitive to aforementioned
index overhead.

8. RELATED WORK
Table 4 summarizes state-of-the-art database index structures in terms of query time, accuracy, storage overhead and
maintenance overhead.
Tree Index Structures: B+ -Tree is the most commonly
used type of indexes. The basic idea can be summarized as
follows: For a non-leaf node, the value of its left child node
must be smaller than that of its right child node. Each leaf
node points to the physical address of the original tuple.
With the help of this structure, searching B+ -Tree can be
completed in one binary search time scope. The excellent
query performance of B+ -Tree and other tree like indexes
is benefited by their well designed structures which consist
of many non-leaf nodes for quick searching and leaf nodes
for fast accessing parent tuples. This feature incurs two
inevitable drawbacks: (1) Storing plenty of nodes costs a
huge chunk of disk storage. As shown in Section 1, it results
in non-ignorable dollar cost and huge initialization time in
big data scenarios. (2) Index maintenance is extremely timeconsuming. For any insertions or deletions occur on parent
table, tree like indexes firstly have to traverse themselves
for finding proper update locations and then split, merge or
re-order one or more nodes which are out of date.
Compressed Index Structures: Compressed indexes
try to drop some repeated index information as much as
possible beforehand for saving space and recover it as fast
as possible upon queries from users but they all have guaranteed query accuracy. These techniques are applied to tree

7.3.4 TPC-H queries
To further explore the query performance of Hippo in the
real business decision support, we compare Hippo with B+ Tree using TPC-H standard queries. Both of the two indexes are built on ”l shipdate” attribute in Lineitem table
of 200 GB workload. As discussed before, Hippo costs similar query time with B+ -Tree when the query selectivity fac11

10. REFERENCES

indexes [8, 9] and bitmap indexes [7, 11, 14, 19] (low cardinality and read-only datasets). Though compressed indexes
are storage economy, they require additional time for compressing beforehand and decompressing on-the-fly. Compromising on the time of initialization, query and maintenance
is not desirable in many time-sensitive scenarios. Hippo on
the other hand reduces the storage overhead by dropping redundancy tuple pointers and hence still achieves competitive
query response time.
Approximate Index Structures: Approximate indexes [2, 10, 12] give up the query accuracy and only store
some representative information of parent tables for saving
indexing and maintenance overhead and improving query
performance. They propose many efficient statistics algorithms to figure out the most representative information
which is worth to be stored. In addition, some people focus on approximate query processing (AQP)[1, 18] which
relies on data sampling and error bar estimating to accelerate query speed directly. However, trading query accuracy
makes them applicable to limited scenarios. On the other
hand, Hippo, though still reduces the storage overhead, only
returns exact answer that match the query predicate.
Sparse Index Structures: Sparse index (denoted as
Zone Map Index in IBM Data Warehouse[3], Data Pack
structure in Infobright[13], Block Range Index in PostgreSQL[15], and Storage Index in Oracle Exadata[16]) is
a simple index structure implemented by many popular
DBMS in recent years. Sparse index only stores pointers
which point to disk pages of parent tables and value ranges
(min and max values) in each page so that it can save indexing and maintenance overhead. It is generally built on
ordered attributes. For a posed query, it finds value ranges
which cover or overlap the query predicate and then rapidly
inspects the associated few parent table pages one by one
for retrieving truly qualified tuples. However, for most real
life attributes which have unordered data, sparse index has
to spend lots of time on page scanning because the stored
value ranges (min and max values) may cover most query
predicates and encumber the page inspection. Therefore, an
efficient yet concise page summarizing method (i.e., Hippo)
instead of simple value ranges is highly desirable.

9.

[1] S. Agarwal, H. Milner, A. Kleiner, A. Talwalkar,
M. Jordan, S. Madden, B. Mozafari, and I. Stoica.
Knowing when you’re wrong: building fast and
reliable approximate query processing systems. In
SIGMOD, pages 481–492. ACM, 2014.
[2] M. Athanassoulis and A. Ailamaki. Bf-tree:
Approximate tree indexing. In VLDB, pages
1881–1892. VLDB Endowment, 2014.
[3] C. Bontempo and G. Zagelow. The ibm data
warehouse architecture. CACM, 41(9):38–48, 1998.
[4] D. Comer. Ubiquitous b-tree. CSUR, 11(2):121–137,
1979.
[5] T. P. P. Council. Tpc-h benchmark specification.
Published at http://www. tcp. org/hspec. html, 2008.
[6] P. Flajolet, D. Gardy, and L. Thimonier. Birthday
paradox, coupon collectors, caching algorithms and
self-organizing search. Discrete Applied Mathematics,
39(3):207–229, 1992.
[7] F. Fusco, M. P. Stoecklin, and M. Vlachos. Net-fli:
on-the-fly compression, archiving and indexing of
streaming network traffic. VLDB J., 3(1-2):1382–1393,
2010.
[8] J. Goldstein, R. Ramakrishnan, and U. Shaft.
Compressing relations and indexes. In ICDE, pages
370–379. IEEE, 1998.
[9] G. Guzun, G. Canahuate, D. Chiu, and J. Sawin. A
tunable compression framework for bitmap indices. In
ICDE, pages 484–495. IEEE, 2014.
[10] M. E. Houle and J. Sakuma. Fast approximate
similarity search in extremely high-dimensional data
sets. In ICDE, pages 619–630. IEEE, 2005.
[11] D. Lemire, O. Kaser, and K. Aouiche. Sorting
improves word-aligned bitmap indexes. Data &
Knowledge Engineering, 69(1):3–28, 2010.
[12] Y. Sakurai, M. Yoshikawa, S. Uemura, H. Kojima,
et al. The a-tree: An index structure for
high-dimensional spaces using relative approximation.
In VLDB, pages 5–16. VLDB Endowment, 2000.
[13] D. Ślezak and V. Eastwood. Data warehouse
technology by infobright. In SIGMOD, pages 841–846.
ACM, 2009.
[14] K. Stockinger and K. Wu. Bitmap indices for data
warehouses. Data Warehouses and OLAP: Concepts,
Architectures and Solutions, page 57, 2006.
[15] M. Stonebraker and L. A. Rowe. The design of
postgres. In SIGMOD, pages 340–355. ACM, 1986.
[16] R. Weiss. A technical overview of the oracle exadata
database machine and exadata storage server. Oracle
White Paper. Oracle Corporation, Redwood Shores,
2012.
[17] K. Wu, E. Otoo, and A. Shoshani. On the performance
of bitmap indices for high cardinality attributes. In
VLDB, pages 24–35. VLDB Endowment, 2004.
[18] K. Zeng, S. Gao, B. Mozafari, and C. Zaniolo. The
analytical bootstrap: a new method for fast error
estimation in approximate query processing. In
SIGMOD, pages 277–288. ACM, 2014.
[19] M. Zukowski, S. Heman, N. Nes, and P. Boncz.
Super-scalar ram-cpu cache compression. In ICDE,
pages 59–59. IEEE, 2006.

CONCLUSION

The paper introduces Hippo a sparse indexing approach
that efficiently and accurately answers database queries
while occupying up to two orders of magnitude less storage
overhead than de-facto database indexes, i.e., B+ -tree. To
achieve that, Hippo stores pointers of pages instead of tuples
in the indexed table to reduce the storage space occupied
by the index. Furthermore, Hippo maintains histograms,
which represent the data distribution for one or more pages,
as the summaries for these pages. This structure significantly shrinks index storage footprint without compromising much on performance of common analytics queries, i.e.,
TPC-H workload. Moreover, Hippo achieves about three
orders of magnitudes less maintenance overhead compared
to the B+ -tree. Such performance benefits make Hippo a
very promising alternative to index data in big data application scenarios. Furthermore, the simplicity of the proposed
structure makes it practical for database systems vendors to
adopt Hippo as an alternative indexing technique. In the
future, we plan to adapt Hippo to support more complex
data types, e.g., spatial data, unstructured data.
12

Matrix Factorization with Explicit Trust and Distrust Relationships
Rana Forsati
Shahid Beheshti University, G.C., Tehran, Iran

Mehrdad Mahdavi
Michigan State University, Michigan, USA

r_forsati@sbu.ac.ir

mahdavim@cse.msu.edu

Mehrnoush Shamsfard
Shahid Beheshti University, G.C., Tehran, Iran

Mohamed Sarwat
University of Minnesota, Minneapolis, USA

m_shams@sbu.ac.ir

sarwat@cs.umn.edu

arXiv:1408.0325v1 [cs.SI] 2 Aug 2014

Abstract With the advent of online social networks, recommender systems have became crucial for the success of many online applications/services due to their significance role in tailoring these applications to user-specific needs or preferences. Despite their increasing popularity, in general recommender systems suffer from the data sparsity and the cold-start problems. To alleviate these issues, in recent years there has been an upsurge of interest in exploiting social information such as trust relations among users along with the rating data to improve the performance of recommender systems. The main motivation for exploiting trust information in recommendation process stems from the observation that the ideas we are exposed to and the choices we make are significantly influenced by our social context. However, in large user communities, in addition to trust relations, the distrust relations also exist between users. For instance, in Epinions the concepts of personal "web of trust" and personal "block list" allow users to categorize their friends based on the quality of reviews into trusted and distrusted friends, respectively. Hence, it will be interesting to incorporate this new source of information in recommendation as well. In contrast to the incorporation of trust information in recommendation which is thriving, the potential of explicitly incorporating distrust relations is almost unexplored. In this paper, we propose a matrix factorization based model for recommendation in social rating networks that properly incorporates both trust and distrust relationships aiming to improve the quality of recommendations and mitigate the data sparsity and the cold-start users issues. Through experiments on the Epinions data set, we show that our new algorithm outperforms its standard trust-enhanced or distrust-enhanced counterparts with respect to accuracy, thereby demonstrating the positive effect that incorporation of explicit distrust information can have on recommender systems.

1 Introduction
The huge amount of information available on the Web has made it increasingly challenging to cope with this information overload and find the most relevant information one is really interested in. Recommender systems intend to provide users with recommendations of products they might appreciate, taking into account their past ratings, purchase history, or interest. The recent proliferation of online social networks have further enhanced the need for such systems. Therefore, it is obvious why such systems are indispensable for the success of many online applications such as Amazon, iTunes and Netflix to guide the search process and help users to effectively find the information or products they are looking for [49]. Roughly speaking, the overarching goal of recommender systems is to identify a subset of items (e.g. products, movies, books, music, news, and web pages) that are likely to be more interesting to users based on their interests [13, 76, 16, 5]. In general, most widely used recommender systems (RS) can be broadly classified into contentbased (CB), collaborative filtering (CF), or hybrid methods [1]. In CB recommendation, one tries to recommend items similar to those a given user preferred in the past. These methods usually rely on the external information such as explicit item descriptions, user profiles, and/or the appropriate features extracted from items to analyze item similarity or user preference to provide recommendation.

1

In contrast, CF recommendation, the most popular method adopted by contemporary recommender systems, is based on the core assumption that similar users on similar items express similar interest, and it usually relies on the rating information to build a model out of the rating information in the past without having access to external information required in CB methods. The hybrid approaches were proposed that combine both CB and CF based recommenders to gain advantages and avoid certain limitations of each type of systems [20, 64, 55, 48, 54, 67, 15]. The essence of CF lies in analyzing the neighborhood information of past users and items' interactions in the user-item rating matrix to generate personalized recommendations based on the preferences of other users with similar behavior. CF has been shown to be an effective approach to recommender systems. The advantage of these types of recommender systems over content-based RS is that the CF based methods do not require an explicit representation of the items in terms of features, but it is based only on the judgments/ratings of the users. These CF algorithms are mainly divided into two main categories [21]: memory-based methods (also known as neighborhood-based methods) [73, 9] and model-based methods [26, 63, 65, 79]. Recently, another direction in CF considers how to combine memory-based and model-based approaches to take advantage of both types of methods, thereby building a more accurate hybrid recommender system [56, 77, 32]. The heart of memory-based CF methods is the measurement of similarity based on ratings of items given by users: either the similarity of users (user-oriented CF) [24], the similarity of items (itemsoriented CF) [61], or combined user-oriented and item-oriented collaborative filtering approaches to overcome the limitations specific to either of them [74]. The user-oriented CF computes the similarity among users, usually based on user profiles or past behavior, and seeks consistency in the predictions among similar users [78, 26]. The item-oriented CF, on the other hand, allows input of additional itemwise information and is also capable of capturing the interactions among them. If the rating of an item by a user is unavailable, collaborative-filtering methods estimate it by computing a weighted average of known ratings of the items from the most similar users. Memory-based collaborative filtering is most effective when users have expressed enough ratings to have common ratings with other users, but it performs poorly for so-called cold-start users. Coldstart users are new users who have expressed only a few ratings. Thus, for memory based CF methods to be effective, large amount of user-rating data are required. Unfortunately, due to the sparsity of the user-item rating matrix, memory-based methods may fail to correctly identify the most similar users or items, which in turn decreases the recommender accuracy. Another major issue that memory-based methods suffer from is the scalability problem. The reason is essentially the fact that when the number of users and items are very large, which is common in many real world applications, the search to identify k most similar neighbors of the active user is computationally burdensome. In summary, data sparsity and non-scalability issues are two main issues current memory based methods suffer from. To overcome the limitations of memory-based methods, model-based approaches have been proposed, which establish a model using the observed ratings that can interpret the given data and predict the unknown ratings [1]. In contrast to the memory-based algorithms, model-based algorithms try to model the users based on their past ratings and use these models to predict the ratings on unseen items. In model-based CF the goal is to employ statistical and machine learning techniques to learn models from the data and make recommendations based on the learned model. Methods in this category include aspect model [26, 63], clustering methods [30], Bayesian model [80], and low dimensional linear factor models such as matrix factorization (MF) [66, 65, 79, 59]. Due to its efficiency in handling very huge data sets, matrix factorization based methods have become one of the most popular models among the model-based methods, e.g. weighted low rank matrix factorization [65], weighted nonnegative matrix factorization (WNMF) [79], maximum margin matrix factorization (MMMF) [66] and probabilistic matrix factorization (PMF) [59]. These methods assume that user preferences can be modeled by only a small number of latent factors [12] and all focus on fitting the user-item rating matrix using low-rank approximations only based on the observed ratings. The recommender system we will propose in this paper adhere to the model-based factorization paradigm. Although latent factor models and in particular matrix factorization are able to generate high quality recommendations, these techniques also suffer from the data sparsity problem in real-world scenarios and fail to address users who rated only a few items. For instance, according to [61], the density of non-missing ratings in most commercial recommender systems is less than one or even much less. Therefore, it is unsatisfactory to rely predictions on such small amount of data which becomes more challenging in the presence of large number of users or items. This observation necessitates tackling the 2

data sparsity problem in an affirmative manner to be able to generate more accurate recommendations. One of the most prominent approaches to tackle the data sparsity problem is to compensate for the lack of information in rating matrix with other sources of side information which are available to the recommender system. For example, social media applications allow users to connect with each other and to interact with items of interest such as songs, videos, pages, news, and groups. In such networks the ideas we are exposed to and the choices we make are significantly influenced by our social context. More specifically, users generally tend to connect with other users due to some commonalities they share, often reflected in similar interests. Moreover, in many real-life applications it may be the case that only social information about certain users is available while interaction data between the items and those users has not yet been observed. Therefore, the social data accumulated in social networks would be a rich source of information for the recommender system to utilize as side information to alleviate the data sparsity problem. To accomplish this goal, in recent years the trust-based recommender systems became an emerging field to provide users personalized item recommendations based on the historical ratings given by users and the trust relationships among users (e.g., social friends). Social-enhanced recommendation systems are becoming of greater significance and practicality with the increased availability of online reviews, ratings, friendship links, and follower relationships. Moreover, many e-commerce and consumer review websites provide both reviews of products and a social network structure among the reviewers. As an example, the e-commerce site Epinions [22] asks its users to indicate which reviews/users they trust and use these trust information to rank the reviews of products. Similar patterns can be found in online communities such as Slashdot in which millions of users post news and comment daily and are capable of tagging other users as friends/foes or fans/freaks. Another example is the ski mountaineering site Moleskiing [3] which enables users to share their opinions about the snow conditions of the different ski routes and also express how much they trust the other users. Another well-known example is the FilmTrsut system [19], an online social network that provides movie rating and review features to its users. The social networking component of the website requires users to provide a trust rating for each person they add as a friend. Also users on Wikipedia can vote for or against the nomination of others to adminship [7]. These websites have come to play an important role in guiding users' opinions on products and in many cases also influence their decisions in buying or not buying the product or service. The results of experiments in [11] and of similar works confirm that a social network can be exploited to improve the quality of recommendations. From this point of view, traditional recommender systems that ignore the social structure between users may no longer be suitable. A fundamental assumption in social based recommender systems which has been adopted by almost all of the relevant literature is that if two users have friendship relation, then the recommendation from his or her friends probably has higher trustworthiness than strangers. Therefore the goal becomes how to combine the user-item rating matrix with the social/trust network of a user to boost the accuracy of recommendation system and alleviate the sparsity problem. Over the years, several studies have addressed the issue of the transfer of trust among users in online social networks. These studies exploit the fact that trust can be passed from one member to another in a social network, creating trust chains, based on its propagative and transitive nature 1 . Therefore, some recommendation methods fusing social relations by regularization [29, 36, 42, 81] or factorization [41, 43, 59, 58, 65, 60, 57] were proposed that exploit the trust relations in the social network. Also, the results of incorporating the trust information in recommender systems is appealing and has been the focus of many researchers in the last few years, but, in large user communities, besides the trust relationship between users, the distrust relationships are also unavoidable. For example, Epinions provided the feature that enables users to categorize other users in a personal web of trust list based on their quality as a reviewer. Later on, this feature integrated with the concept of personal block list, which reflects the members that are distrusted by a particular user. In other words, if a user encounters a member whose reviews are consistently offensive, inaccurate, or otherwise low quality, she can add that member to her block list. Therefore, it would be tempting to investigate whether or not distrust information could be effectively utilized to boost the accuracy of recommender systems as well. In contrast to trust information for which there has been a great research, the potential advantage/disadvantage of explicitly utilizing distrust information is almost unexplored. Recently, few
1 We note that while the concept of trust has been studied in many disciplines including sociology, psychology, economics, and computer science from different perspectives, but the issue of propagation and transitivity have often been debated in literature and different authors have reached different conclusions (see for example [62] for a thorough discussion)

3

attempts have been made to explicitly incorporate the distrust relations in recommendation process [22, 40, 69, 72], which demonstrated that the recommender systems can benefit from the proper incorporation of distrust relations in social networks. However, despite these positive results, there are some unique challenges involved in distrust-enhanced recommender systems. In particular, it has proven challenging to model distrust propagation in a manner which is both logically consistent and psychologically plausible. Furthermore, the naive modeling of distrust as negative trust raises a number of challenges- both algorithmic and philosophical. Finally, it is an open challenge how to incorporate trust and distrust relations in model-based methods simultaneously. This paper is concerned with these questions and gives an affirmative solution to challenges involved with distrust-enhanced recommendation. In particular, the proposed method makes it possible to simultaneously incorporate both trust and distrust relationships in recommender systems to increase the prediction accuracy. To the best of our knowledge, this is the first work that models distrust relations into the matrix factorization problem along with trust relations at the same time. The main intuition behind the proposed algorithm is that one can interpret the distrust relations between users as the dissimilarity in their preferences. In particular, when a user u distrusts another user v , it indicates that user u disagrees with most of the opinions issued, or ratings made by user v . Therefore, the latent features of user u obtained by matrix factorization must be as dissimilar as possible to v 's latent features. In other words, this intuition suggests to directly incorporate the distrust into recommendation by considering distrust as reversing the deviation of latent features. However, when combined with the trust relations between users, due to the contradictory role of trust and distrust relations in propagating social information in the matrix factorization process, this idea fails to effectively capture both relations simultaneously. This statement also follows from the preliminary experimental results in [69] for memory-based CF methods that demonstrated regarding distrust as an indication to reverse deviations in not the right way to incorporate distrust. To remedy this problem, we settle to a less ambitious goal and propose another method to facilitate the learning from both types of relations. In particular, we try to learn latent features in a manner that the latent features of users who are distrusted by the user u have a guaranteed minimum dissimilarity gap from the worst dissimilarity of users who are trusted by user u . By this formulation, we ensure that when user u agrees on an item with one of his trusted friends, he/she will disagree on the same item with his distrusted friends with a minimum predefined margin. We note that this idea significantly departs from the existing works in distrust-enhanced memory based recommender systems [69, 72], that employ the distrust relations to either filter out or debug the trust relations to reduce the prediction task to a trust-enhanced recommendation. In particular, the proposed method ranks the latent features of trusted and distrusted friends of each user to reflect the effect of relation in factorization. Summary of Contributions This work makes the following key contributions:  A matrix factorization based algorithm for simultaneous incorporation of trust and distrust relationships in recommender systems. To the best of our knowledge, this is the first model-based recommender algorithm that is able to leverage both types of relationships in recommendation.  An efficient stochastic optimization algorithm to solve the optimization problem which makes the proposed method scalable to large social networks.  An empirical investigation of the consistency of the social relationships with rating information. In particular, we examine to what extent trust and distrust relations between users are aligned with the ratings they issued on items.  An exhaustive set of experiments on Epinions data set to empirically evaluate the performance of the proposed algorithm and demonstrate its merits and advantages.  A detailed comparison of the proposed algorithm to the state-of-the-art trust/distrust enhanced memory/model based recommender systems. Outline The rest of this paper is organized as follows. In Section 2 we draw connections to and put our work in context of some of the most recent work on social recommender systems. Section 3 formally

4

introduces the matrix factorization problem, an optimization based framework to solve it, and its extension to incorporate the trust relations between users. The proposed algorithm along with optimization methods are discussed in Section 4. Section 5 includes our experimental result on Epinions data set which demonstrates the merits of the proposed algorithm in alleviating data sparsity problem in rating matrix and generating more accurate recommendations. Finally, Section 6 concludes the paper and discusses few directions as future work.

2 Related Work on Social Recommendation
Earlier in the introduction, we discussed some of the main lines of research on recommender system; here, we survey further lines of study that are most directly related to our work on social-enhanced recommendation. Many successful algorithms have been developed over the past few years to incorporate social information in recommender systems. After reviewing trust-enhanced memory-based approaches, we discuss some model-based approaches for recommendation in social networks with trust relations. Finally, we review major approaches in distrust modeling and distrust-enhanced recommender systems.

2.1 Trust Enhanced Memory-based Recommendation
Social network data has been widely investigated in the memory-based approaches. These methods typically explore the social network and find a neighborhood of users trusted (directly or indirectly) by a user and perform the recommendation by aggregating their ratings. These methods use the transitivity of trust and propagate trust to indirect neighbors in the social network [45, 47, 31, 27, 29, 28, 33]. In [45], a trust-aware collaborative filtering method for recommender systems is proposed. In this work, the collaborative filtering process is informed by the reputation of users, which is computed by propagating trust. [31] proposed a method based on the random walk algorithm to utilize social connection and other social annotations to improve recommendation accuracy. However, this method does not utilize the rating information and is not applicable to constructing a random walk graph in real data sets. TidalTrust [18] performs a modied breadth first search in the trust network to compute a prediction. To compute the trust value between user u and v who are not directly connected, TidalTrust aggregates the trust value between u 's direct neighbors and v weighted by the direct trust values of u and its direct neighbors. MoleTrust [45, 46, 80] does the same idea as TidalTrust, but MoleTrust considers all the raters up to a fixed maximum-depth given as an input, independent of any specific user and item. The trust metric in MoleTrust consists of two major steps. First, cycles in trust networks are removed. Therefore, removing trust cycles beforehand from trust networks can significantly speed up the proposed algorithm because every user only needs to be visited once to infer trust values. Second, trust values are calculated based on the obtained directed acyclic graph by performing a simple graph random walk: TrustWalker [27] combines trust-based and item-based recommendation to consider enough ratings without suffering from noisy data. Their experiments show that TrustWalker outperforms other existing memory based approaches. Each random walk on the user trust graph returns a predicted rating for user u on target item i . The probability of stopping is directly proportional to the similarity between the target item and the most similar item j , weighted by the sigmoid function of step size k . The more the similarity, the greater the probability of stopping and using the rating on item j as the predicted rating for item i . As the step size increases, the probability of stopping decreases. Thus ratings by closer friends on similar items are considered more reliable than ratings on the target item by friends further away. We note that all these methods are neighborhood-based methods which employ only heuristic algorithms to generate recommendations. There are several problems with this approach. The relationship between the trust network and the user-item matrix has not been studied systematically. Moreover, these methods are not scalable to very large data sets since they may need to calculate the pairwise user similarities and pairwise user trust scores.

5

2.2 Trust Enhanced Model-based Recommendation
Recently, researchers exploited matrix factorization techniques to learn latent features for users and items from the observed ratings and fusing social relations among users with rating data as will be detailed in Section 3. These methods can be divided into two types: regularization-based methods and factorization-based methods. Here we review some existing matrix factorization algorithms that incorporate trust information in the factorization process. 2.2.1 Regularization based Social Recommendation Regularization based methods typically add regularization term to the loss function and minimize it. Most recently, Ma [42] proposed an idea based on social regularized matrix factorization to make recommendation based on social network information. In this approach, the social regularization term is added to the loss function, which measures the difference between the latent feature vector of a user and those of his friends. The probability model similar to the model in [42] is proposed by Jamali [29]. The graph Laplacian regularization term of social relations is added into the loss function in [36] and minimizes the loss function by alternative projection algorithm. Zhu et a l. [81] used the same model in [36] and built graph Laplacian of social relations using three kinds of kernel functions. In [37], the minimization problem is formulated as a low-rank semidefinite optimization problem. 2.2.2 Factorization based Social Recommendation In factorization-based methods, social relationship between users are represented as social relation matrix, which is factored as well as the rating matrix. The loss function is the weighted sum of the social relation matrix factorization error and the rating matrix factorization error. For instance, SoRec [41] incorporates the social network graph into probabilistic matrix factorization model by simultaneously factorizing the user-item rating matrix and the social trust networks by sharing a common latent lowdimensional user feature matrix [37]. The experimental analysis shows that this method generates better recommendations than the non-social filtering algorithms [28]. However, the disadvantage of this work is that although the users social network is integrated into the recommender systems by factorizing the social trust graph, the real world recommendation processes are not reflected in the model. Two sets of different feature vectors are assumed for users which makes the interpretability of the model very hard [28, 39]. This drawback not only causes lack of interpretability in the model, but also affects the recommendation qualities. A better model named Social Trust Ensemble (STE) [39] is proposed by the same authors, by making the latent features of a user's direct neighbors affect the rating of the user. Their method is a linear combination of basic matrix factorization approach and a social network based approach. Experiments show that their model outperforms the basic matrix factorization based approach and existing trust based approaches. However, in their model, the feature vectors of direct neighbors of u affect the ratings of u instead of affecting the feature vector of u . This model does not handle trust propagation. Another method for recommendation in social networks has been proposed in [40]. This method is not a generative model and defines a loss function to be minimized. The main disadvantage of this method is that it punishes the users with lots of social relations more than other users. Finally, SocialMF [28] is a matrix factorization based model which incorporates social influence by making the features of every user depend on the features of his/her direct neighbors in the social network.

2.3 Distrust Enhanced Social Recommendation
In contrast to incorporation of trust relations, unfortunately most of the literature on social recommendation totally ignore the potential of distrust information in boosting the accuracy of recommendations. In particular, only recently few work started to investigate the rule of distrust information in recommendation process both from theoretical and empirical viewpoints [22, 84, 51, 82, 40, 75, 69, 71, 68, 72]. Although these studies have shown that distrust information can be plentiful, but there is a significant gap in clear understanding of distrust in recommender systems. The most important reasons for this shortage are the lack of data sets that contain distrust information and dearth of a unified consensus on modeling and propagation of distrust.

6

Table 1: Summary of notations consistently used in the paper and their meaning. Symbol Meaning U = {u 1 ,    , u n }, n I = {i 1 ,    , i m }, m k R  Rn m  R , | R | U  Rn k V  Rm k S  {-1, +1}n n S , |S | n W  Rn + N (u )  [ n ] N + (u )  [ n ] N - (u )  [ n ] D : Rk  Rk  R+ The set of users in system and the number of users The set of items and the number of items The dimension of latent features in factorization The partially observed rating matrix The set of observed entires in rating matrix and its size The matrix of latent features for users The matrix of latent features for items The social network between n users The set of extracted triplets from the social relations and its size The pairwise similarity matrix between users Neighbors of user u in the social graph The set of trusted neighbors by user u in the social graph The set of distrusted neighbors by user u in the social graph The measurement function used to assess the similarly of latent features

A formal framework of trust propagation schemes, introducing the formal and computational treatment of distrust propagation has been developed in [22]. In an extension of this work, [82] proposed clever adaptations in order to handle distrust and sinks such as trust decay and normalization. In [75], a trust/distrust propagation algorithm called CloseLook is proposed, which is capable of using the same kinds of trust propagation as the algorithm proposed by [22]. [34] extended the results by [22] using a machine-learning framework (instead of the propagation algorithms based on an adjacency matrix) to enable the evaluation of the most informative structural features for the prediction task of positive/negative links in online social networks. A comprehensive framework that computes trust/distrust estimations for user pairs in the network using trust metrics is build in [71]: given two users in the trust network, we can search for a path between them and propagate the trust scores along this path to obtain an estimation. When more than one path is available, we may single out the most relevant ones (selection), and aggregation operators can then be used to combine the propagated trust scores into one final trust score, according to different trust score propagation operators. [40] was the first seminal work to demonstrate that the incorporation of distrust information could be beneficial based on a model-based recommender system. In [71] and [72] the same question is addressed in memory-based approaches. In particular, [72] embarked upon the distrust-enhanced recommendation and showed that with careful incorporation of distrust metric, distrust-enhanced recommender systems are able to outperform their trust-only counterparts. The main rational behind the algorithm proposed in [72] is to employ the distrust information to debug or filter out the users' propagated web of trust. It is also has been realized that the debugging methods must exhibit a moderate behavior in order to be effective. [68] addressed the problem of considering the length of the paths that connect two users for computing trust-distrust between them, according to the concept of trust decay. This work also introduced several aggregation strategies for trust scores with variable path lengths Finally we note that the aforementioned works try to either model or utilize the trust/distrust information. In recent years there has been an upsurge of interest in predicting the trust and distrust relations in a social network [34, 14, 4, 53]. For instance, [34] casts the problem as a sign prediction problem (i.e., +1 for friendship and -1 for opposition) and utilizes machine learning methods to predict the sign of links in the social network. In [14] a new method is presented for computing both trust and distrust by combining an inference algorithm that relies on a probabilistic interpretation of trust based on random graphs with a modified spring-embedding algorithm to classify an edge. Another direction of research is to examine the consistency of social relations with theories in social psychology [8, 35]. Our work significantly departs from these works on prediction or consistency analysis of social relations, and aims to effectively incorporate the distrust information in matrix factorization for effective recommendation.

7

3 Matrix Factorization based Recommender Systems
This section provides a formal definition of collaborative filtering, the primary recommendation method we are concerned with in this paper, followed by solution methods for low-rank factorization that are proposed in the literature to address the problem.

3.1 Matrix Factorization for Recommendation
In collaborative filtering we assume that there is a set of n users U = {u 1 ,    , u n } and a set of m items I = {i 1 ,    , i m } where each user u i expresses opinions about a set of items. In this paper, we assume opinions are expressed through an explicit numeric rating (e.g., scale from one to five), but other rating methods such as hyperlink clicks are possible as well. We are mainly interested in recommending a set of items for an active user such that the user has not rated these items before. To this end, we are aimed at learning a model from the existing ratings, i.e., offline phase, and then use the learned model to generate recommendations for active users, i.e., online phase. The rating information is summarized in an n  m matrix R  Rn m , 1  i  n , 1  j  m where the rows correspond to the users and the columns correspond to the items and (p , q )th entry is the rate given by user u p to the item i q . We note that the rating matrix is partially observed and it is sparse in most cases. An efficient and effective approach to recommender systems is to factorize the user-item rating matrix R by a multiplicative of k -rank matrices R  UV , where U  Rn k and V  Rm k utilize the factorized user-specific and item-specific matrices, respectively, to make further missing data prediction. The main intuition behind a low-dimensional factor model is that there is only a small number of factors influencing the preferences, and that a user's preference vector is determined by how each factor applies to that user. This low rank assumption makes it possible to effectively recover the missing entires in the rating matrix from the observed entries. We note that the celebrated Singular Value Decomposition (SVD) method to factorize the rating matrix R is not applicable here due to the fact that the rating matrix is partially available and we are only allowed to utilize the observed entries in factorization process. There are two basic formulations to solve this problem: these are optimization based (see e.g., [57, 37, 41, 33]) and probabilistic [50]. In the following subsections, we first review the optimization based framework for matrix factorization and then discuss how it can be extended to incorporate trust information.

3.2 Optimization based Matrix Factorization
Let R be the set of observed ratings in the user-item matrix R  Rn m , i.e., R = {(i , j )  [n ]  [m ] : R i j has been observed}, where n is the number of users and m is the number of items to be rated. In optimization based matrix factorization, the goal is to learn the latent matrices U and V by solving the following optimization problem: min L (U, V) =
U,V

1 R i j - Ui ,: V j ,: 2 (i , j )R
F

2

+

U U 2
n i =1

F+

V V 2

F

,

(1)

where 

F

is the Frobenius norm of a matrix, i.e, A

=

m 2 j =1 | A i j | .

The optimization prob-

lem in (1) constitutes of three terms: the first term aims to minimize the inconsistency between the observed entries and their corresponding value obtained by the factorized matrices. The last two terms regularize the latent matrices for users and items, respectively. The parameters U and V are regularization parameters that are introduced to control the regularization of latent matrices U and V, respectively. We would like to emphasize that the problem in (1) is non-convex jointly in both U and V. However, despite its non-convexity, the formulation in (1) is widely used in practical collaborative filtering applications as the performance is competitive or better as compared to trace-norm minimization, while scalability is much better. For example, as indicated in [33], to address the Netflix problem, (1) has been applied with a fair amount of success to factorize data sets with 100 million ratings.

8

3.3 Matrix Factorization with Trust Side Information
Recently it has been shown that just relying on the rating matrix to build a recommender system is not as accurate as expected. The main reason for this claim is the known cold-start users problem and the sparsity of rating matrix. Cold-start users are one of the most important challenges in recommender systems. Since cold-start users are more dependent on the social network compared to users with more ratings, the effect of using trust propagation gets more important for cold-start users. Moreover, in many real life systems a very large portion of users do not express any ratings, and they only participate in the social network. Hence, using only the observed ratings does not allow to learn the user features. One of the most prominent approaches to tackle the data sparsity problem in matrix factorization is to compensate the lack of information in rating matrix with other sources of side information which are available to the recommender system. It has been recently shown that social information such as trust relationship between users is a rich source of side information to compensate for the sparsity. The above mentioned traditional recommendation techniques are all based on working on the user-item rating matrix, and ignore the abundant relationships among users. Trust-based recommendation usually involves constructing a trust network where nodes are users and edges represent the trust placed on them. The goal of a trust-based recommendation system is to generate personalized recommendations by aggregating the opinions of other users in the trust network. The intuition is that users tend to adopt items recommended by trusted friends rather than strangers, and that trust is positively and strongly correlated with user preferences. Recommendation techniques that analyze trust networks were found to provide very accurate and highly personalized results. To incorporate the social relations in the optimization problem formulated in (1), few papers [40, 29, 42, 37, 81] proposed the social regularization method which aims at keeping the latent vector of each user similar to his/her neighbors in the social network. The proposed models force the user feature vectors to be close to those of their neighbors to be able to learn the latent user features for users with no or very few ratings [29]. More specifically, the optimization problem becomes as: L (U, V) = 1 R i j - Ui ,: V j ,: 2 (i , j )R + S 2
n 2

+

U U 2

F+

V V 2

F

(2)

Ui ,: -
i =1

1 | N (i )|

j N (i )

U j ,: ,

where S is the social regularization parameter and N (i ) is the subset of users who has relationship with i th user in the social graph. The rationale behind social regularization idea is that every user's taste is relatively similar to the average taste of his friends in the social network. We note that using this idea, latent features of users indirectly connected in the social network will be dependent and hence the trust gets propagated. A more reasonable and realistic model should treat all friends differently based on how similar they are. Let assume the weight of relationship between two users i and j is captured by Wi j where W  Rn n demotes the social weight matrix. It is easy to extend the model in (2) to treat friends differently based on the weight matrix W as: L (U, V) = 1 R i j - Ui ,: V j ,: 2 (i , j )R + S 2
n 2

+

U U 2

F+

V V 2

F

(3)

Ui ,: -
i =1

j N (i ) Wi j U j ,: j N (i ) Wi j

An alternative formulation is to regularize each users' fiends individually, resulting in the following objective function [42]: L (U, V) = 1 R i j - Ui ,: V j ,: 2 (i , j )R + S 2
n 2

+

U U 2
2

F+

V V 2

F

Wi j Ui ,: - U j ,:
i , j =1

.

where we simply assumed that for any j  N (i ), Wi j = 0. 9

As mentioned earlier, the objective function in L (U, V) is not jointly convex in both U and V but it is convex in each of them fixing the other one. Therefore, to find a local solution one can stick to the standard gradient descent method to find a solution in an iterative manner as follows: Ut +1  Ut -  t U L (U, V)|U=Ut ,V=Vt , Vt +1  Vt -  t V L (U, V)|U=Ut ,V=Vt .

4 Matrix Factorization with Trust and Distrust Side Information
In this section we describe the proposed algorithm for social recommendation which is able to incorporate both trust and distrust relationships in the social network along with the partially observed rating matrix. We then present two strategies to solve the derived optimization problem, one based on the gradient descent optimization algorithm which generates more accurate solutions but it is computationally cumbersome, and another based on the stochastic gradient descent method which is computationally more efficient for large rating and social matrices but suffers from slow convergence rate.

4.1 Algorithm Description
As discussed before, the vast majority of related work in the field of matrix factorization for recommendation has primarily focussed on trust propagation and simply ignore the distrust information between users, or intrinsically, are not capable of exploiting it. Now, we aim at developing a matrix factorization based model for recommendation in social rating networks to utilize both trust and distrust relationships. We incorporate the trust/distrust relationship between users in our model to improve the quality of recommendations. While intuition and experimental evidence indicate that trust is somewhat transitive, distrust is certainly not transitive. Thus, when we intend to propagate distrust through a network, questions about transitivity and how to deal with conflicting information abound. To inject social influence in our model, the basic idea is to find appropriate latent features for users such that each user is brought closer to the users she/he trusts and separated apart from the users that she/he distrusts and have different interests. We note that simply incorporating this idea in matrix factorization by naively penalizing the similarity of each user's latent features to his distrusted friends' latent features fails to reach the desired goal. The main reason is that distrust is not as transitive as trust, i.e. distrust can not directly replace trust in trust propagation approaches and utilizing distrust requires careful consideration (trust is transitive, i.e., if user u trusts user v and v trusts w , there is a good chance that u will trust w , but distrust is certainly not transitive, i.e., if u distrusts v and v distrusts w , then w may be closer to u than v or maybe even farther away). It is noticeable that this statement is consistent with the preliminary experimental results in [69] for memory-based CF methods that indicate regarding distrust as an indication to reverse deviations in not the right way to incorporate distrust. Therefore we pursue another approach to model the distrust in recommendation process. The main intuition behind the proposed framework stems from the observation that the trust relations between users can be treated as agreement on items and distrust relations can be considered as disagreement on items. Then, the question becomes how can we guarantee when a user agrees on an item with one of his/her friends, he/she will disagree on the same item with his/her distrusted friends with a reasonable margin. We note that this margin should be large enough to make it possible to distinguish between two types of friends. In terms of latent features, this observation translates to having a margin between the similarity and dissimilarity of users' latent features to his/her trusted and distrusted friends. Alternatively, one can view the proposed method from the viewpoint of connectivity of latent features in a properly designated graph. Intuitively, certain features or groups of features should influence how users connect in the social network, and thus it should be possible to learn a mapping from features to connectivity in the social network such that the mapping respects the underlying structure of the social network. In the basic matrix factorization algorithm for recommendation, we can consider the latent features as isolated vertices of a graph where there is no connection between nodes. This can be generalized to the social-enhanced setting by considering the social graph as the underlying graph between latent features with two types of edges (i.e., trust and distrust relations correspond to positive

10

(a) User trust netwrok

(b) User distrust netwrok

(c) Partially observed rating matrix

(d) Illustration of learned latent features

Figure 1: A simple example with seven users {u 1 , u 2 ,    , u 7 } and six items {i 1 , i 2 ,    , i 6 } to illustrate the main intuition behind the proposed algorithm. The inputs of the algorithm are (a) trust network, (b) distrust network, and (c) partially observed rating matrix R, respectively. As shown in (d) for user u 1 , the learned latent features for all his trusted friends {u 2 , u 4 , u 6 , u 7 } are closer to u 1 's latent features than his distrusted friends {u 3 , u 5 } with a margin of 1. and negative edges, respectively). Now the problem reduces to learning the latent features for each user u such that users trusted by u in the social network (with positive edges) are close and users which are distrusted by u (with negative edges) are more distant. Learning latent features in this manner respects the inherent topology of the social network. Figure 1 shows an example to illustrate the intuition behind the mentioned idea. For ease of exposition, we only consider the latent features for the user u 1 . From the trust network in Figure 1 (a) we can see that user u 1 trusts the list of users N+ = {u 2 , u 4 , u 6 , u 7 } and from the distrust network in Figure 1 (b) we see that user u 1 distrusts the list of users N- = {u 3 , u 5 }. The goal is to learn the latent features that obeys two goals, i) it minimizes the prediction error on observed entries in the rating matrix, ii) it respects the underlying structure of the trust and distrust networks between users. In Figure 1 (d) the latent features are depicted in the Euclidean space from the viewpoint of user u 1 . As shown in Figure 1 (d), for user u 1 , the latent features of his/her trusted friends N+ lie inside the solid circle centered at u 1 and the latent features of his/her distrusted friends N- lie outside the dashed circle. The gap between two circles guarantees that always there exists a safe margin between u 1 's agreements with his trusted and distrusted friends. One simple way to impose these constraints on the latent features of users is to generate a set of triplets for any combination of trusted and distrusted friends ( e.g., one such triplet for user u 1 can be constructed as (u 1 , u 2 , u 5 )) and force the margin constraint to hold for all extracted triplets. This ensures that the minimum margin gap will definitely exist between the latent features of all the trusted and distrusted friends as desired and makes it possible to incorporate both types of

11

relationships between users in the matrix factorization. It is worthy to mention that similar to the social-enhanced recommender systems discussed before, the proposed algorithm is also based on hypotheses about the existence and the correlation of trust/distrust relations and ratings in the data. The empirical investigation of correlation between social relations and rating information has been the focus of a bulk of recent research including [83, 53, 38], where the results reinforce the hypothesis that ratings from trusted people count more than those from others and in particular distrusted neighbors. We have also conducted experiments as will be detailed in Subsection 5.5, to empirically investigate the correlation/alignment between social relations and the rating information issued by users which supports our strategy in exploiting the trust/distrust relations in matrix factorization. We now formalize the proposed solution. As the first ingredient, we need a measure to evaluate the consistency between the latent features of users, i.e., the matrix U, and the trust and distrust constraints existing between users in the social network. To this end, we introduce a monotonically increasing convex loss function (z ) to measure the discrepancy between the latent features of different users. Let u i , u j , and u k be three users in the model such that u i trusts u j but distrusts u k . The main intuition behind the proposed framework is that the latent features of u i , i.e., Ui ,: must be more similar to u j 's latent features than latent features for user u k . For each such a triplet we penalize the objective function by (D (Ui ,: , U j ,: ) - D (Ui ,: , Uk ,: )) where the function D : Rk  Rk  R+ measures the similarity between two latent vectors assigned to two different users, and : R  R+ is a penalty function that is utilized to assess the violation of latent vectors of trusted and distrusted users. Example loss functions include hinge loss (z ) = max(0, 1- z ) and logistic loss (z ) = log(1+e -z ) which are widely used convex surrogate of 0-1 loss function in learning community. Let S denote the set of extracted triplets from the social relations, i.e., S = (i , j , k )  [n ]  [n ]  [n ] : S i j = 1 & S i k = -1 . Here, a positive relationship means friends or a trusted relationship and a negative relationship means foes or a distrust relationship. Then, our goal becomes to find a factorization of matrix R such that the learned latent features of users are consistent with the constraints in S where the consistency is reflected in the loss function. This results in the following optimization problem: L (U, V) = 1 R i j - Ui ,: V j ,: 2 (i , j )R +
2

+

U U 2

F+

V V 2

F

S (D (Ui ,: , U j ,: ) - D (Ui ,: , Uk ,: )). |S | (i , j ,k )S

(4)

Let us make the above general formulation more specific by setting () and D (, ) to be the hinge loss and the Euclidian distance, respectively. Under these two assumptions, the objective can be formulated as: L (U, V) = 1 R i j - Ui ,: V j ,: 2 (i , j )R
R (U,V) 2

+

U U 2

F+

V V 2

F

S + max 0, 1 - Ui ,: - U j ,: |S | (i , j ,k )S

2

+ Ui ,: - Uk ,:

2

.

(5)

Here the constraints have been written in terms of hinge-losses over triplets, each consisting of a user, his/her trusted friend and his/her distrusted friend. Solving the optimization problem in (5) outputs the latent features for users and items that can utilized to estimate the missing values in the user-item matrix. Comparing the formulation in (5) to the existing factorization-based methods discussed earlier reveals two main features of the proposed formulation. First, it aims to minimize the error on the observed ratings and to respect the inherent structure of the social network among the users. The tradeoff between these two objectives is captured by the regularization parameter S which is required to be tuned effectively.

12

Algorithm 1 GD based Matrix Factorization with Trust and Distrust Propagation 1: Input: R: partially observed rating matrix, S 2: Output: U and V 3: for t = 1, . . . , T do 4: Compute the gradients U R (Ut , Vt ) and V R (Ut , Vt ). 5: Compute U by Eq. 7 6: Compute V by Eq. 8 7: Update: Ut +1 = Ut -  t U |U=Ut ,V=Vt Vt +1 = Vt -  t V |U=Ut ,V=Vt
8: 9:

end for return UT +1 and VT +1 .

In a similar way, applying the logistic loss to the general formulation in (4) yields the following objective: L (U, V) = 1 R i j - Ui ,: V j ,: 2 (i , j )R +
2

+

U U 2

F+

V V 2
2

F

S log 1 + exp Ui ,: - Uk ,: |S | (i , j ,k )S

- Ui ,: - U j ,: )

2

.

(6)

Remark 1. We note that in several applications of recommender systems, besides the observed ratings, a description of the users and/or the objects through attributes (e.g., gender, age) or measures of similarity is available that could potentially benefit the process of recommendation (see e.g. [2] for few interesting applications). In that case it is tempting to take advantage of both known ratings and descriptions to model the preferences of users. A natural way to incorporate the available meta-data is to kernalize the similarity measure between latent features based on a positive definite kernel between pairs that can be deduced from the meta-data. More specifically, instead of simply using Euclidian distance as the similarity measure between latent features in (5), we can use the kernel matrix K obtained from the Laplacian of the graph obtained from the meta-data to measure the similarity as: D (Ui ,: , U j ,: ) = Ui ,: - U j ,: K Ui ,: - U j ,: ,

where K = (D - W)-1 , with D as a diagonal matrix with D i ,i = n j =1 Wi j . Here W captures the pairwise weight between users in the similarity graph between users that is computed based on the available metadata about users. Remark 2. We would like to emphasize that it is straightforward to generalize the proposed framework to incorporate similarity and dissimilarity information between items. What we need is to extract the triplets from the trust/distrust links between items and repeat the same process we did for users. This will add another term to the objective in terms of latent features of items V as shown in the following generalized formulation: L (U, V) = 1 R i j - Ui ,: V j ,: 2 (i , j )R + +
2

+

U U 2

F+

V V 2
2

F

S max 0, 1 - Ui ,: - U j ,: |S | (i , j ,k )S I max 0, 1 - Vi ,: - V j ,: |I | (i , j ,k )I
2

+ Ui ,: - Uk ,:
2

2

+ Vi ,: - Vk ,:

,

where I is the regularization parameter and I is the set of triplets extracted from the similar/dissimilar links between items. The similarity/dissimilarity links between items can be constructed according to tags issued by users or associated with items, and categories. For example, if two items are attached with a 13

same tag, there is a trust link between them and otherwise distrust link. Alternatively, trust/distrust links can be extracted by measuring similarity/dissimilarity based on the item properties or profile if provided. This can further improve the accuracy of recommendations.

4.2 Batch Gradient Descent based Optimization
In optimization for supervised machine learning, there exist two regimes in which popular algorithms tend to operate: the stochastic approximation regime, which samples a small data set per iteration, typically a single data point, and the batch or sample average approximation regime, in which larger samples are used to compute an approximate gradient. The choice between these two extremes outlines the well-known tradeoff between inexpensive noisy steps and expensive but more reliable steps. Two preliminary examples of these regimes are the Gradient Descent (GD) and the Stochastic Gradient Descent (SGD) methods, respectively. Both GD and SGD methods starts with some initial point, and iteratively updates the solution using the gradient information at intermediate solutions. The main difference is that GD requires a full gradient information at each iteration while SGD only requires an unbiased estimate of the full gradient which can be done by sampling We now discuss the application of GD algorithm to solve the optimization problem in (5) as detailed in Algorithm 1. Recall that the objective function is not jointly convex in both U and V. On the other hand, the objective is convex in one parameter by fixing the other one. Therefore, we follow an iterative method to minimize the objective. At each iteration, first by fixing V, we take a step in the direction of the negative gradient for U and repeat the same process for V by fixing U. For the ease of exposition, we introduce further notation. For any triplet (i , j , k )  S we note that the Ui ,: - U j ,: 2 - Ui ,: - Uk ,: 2 can be written as Tr(CU U) where Tr() denotes the trace of the input matrix and C is a sparse auxiliary matrix defined for each triplet with all entries equal to zero except: Ci k = Cki = C j j = 1 and Ckk = Ci j = C j i = -1. Having defined this notation, we can write the objective in (5) as: U U 2 V V 2 S max 0, 1 - Tr(Ck i j U U) . |S | (i , j ,k )S

L (U, V) = R (U, V) +

F+

F+

is the C matrix defined above which is associated with triplet (i , j , k ). To apply the GD where Ck ij method, we need to compute the gradient of L (U, V) with respect to U and V which we denote by U = U L (U, V) and V = V L (U, V), respectively. We have: S 1 k |S | (i , j ,k )S [Tr(Ci j U

U = U R (U, V) + U U -

k U)<1] (UCi j

+ UCk ij)

(7)

where 1[] is the indicator function which takes a value of one if its argument is true, and zero otherwise. Similarly for V we have: V = V R (U, V) + V V (8)

The main shortcoming of GD method is its high computational cost per iteration due to the gradient computation (i.e., step (7)) which is expensive when the size of social constraints S is large. We note that the size of S can be as large as O (n 3 ) by considering all triplets in the social graph. In the next subsection we provide an alternative solution to resolve this issue using the stochastic gradient descent and mini-batch SGD methods which are more efficient than the GD method in terms of the computational cost per iteration but with a slow convergence rate in terms of target approximation error.

4.3 Stochastic and Mini-batch Optimization
As discussed above, when the size of social network is very large, the size of S may cause computational problems in solving the optimization problem in (5) using GD method. The reason is essentially the fact that computing the gradient at each iteration requires to go through all the triplets in S which is infeasible for large networks. To alleviate this problem we propose a stochastic gradient based [52]

14

Algorithm 2 Mini-SGD based Matrix Factorization with Trust and Distrust Propagation 1: Input: R: partially observed rating matrix, S , min batch size B 2: Output: U and V 3: for t = 1, . . . , T do 4: t  0 5: for b = 1, . . . , B do 6: (i , j , k )  Sample random triplet from S 7: if (1 - Ui ,: - U j ,: ) 2 + Ui ,: - Uk ,: 2 > 0) then 8:  t  U t Ck U ij t 9: end if 10: end for 11: Compute the gradients U R (Ut , Vt ) and V R (Ut , Vt ). 12: Update: S t Ut +1 = Ut -  t U R (Ut , Vt ) + U Ut + B | S |
13:

Update: Vt +1 = Vt -  t (V R (Ut , Vt ) + V Vt ) end for return UT +1 and VT +1 .

14: 15:

method to solve the optimization problem. The main idea is to choose a fixed subset of triplets for gradient computation instead of all |S | triplets at each iteration [10]. More specifically, at each iteration, we sample B triplets uniformly at random from S to compute the next solution. We note that this strategy generates unbiased estimates of the true gradient and makes each iteration of algorithm computationally more efficient compared to the full gradient counterpart. In the simplest case, SGD algorithm, only one triplet is chosen at each iteration to generate an unbiased estimate of the full gradient. We note that in practice SGD is usually implemented based on data shuffling, i.e., making the sequence of the training samples random and then training the model by going through the training samples one by one. An intermediate solution, known as mini-batch SGD, chooses a subset of triplets to compute the gradient. The promise is that by selecting more triplets at each iteration, on one hand the variance of stochastic gradients decreases promotional to the number of sampled triplets, and on the other hand the algorithm enjoys the light computational cost of basic SGD method. The detailed steps of the algorithm are shown in Algorithm 2. The mini-batch SGD method improves the computational efficiency by grouping multiple constraints into a mini-batch and only updating the U and V once for each mini-batch. For brevity, we will refer to this algorithm as Mini-SGD. More specifically, the Mini-SGD algorithm, instead of computing the full gradient over all triplets, samples B triplets uniformly at random from S where 1  B  |S | is a parameter that needs to be provided to the algorithm, and computes the stochastic gradient as: t = S B 1[Tr(Ck
k Ut Ut )<1] (UCi j

(i , j ,k )B

ij

+ UCk ij)

where B is the set of B sampled triplets from S . We note that E[  t ] = S 1 k |S | (i , j ,k )S [Tr(Ci j Ut
k Ut )<1] (UCi j

+ UCk i j ),

i.e., t is an unbiased estimate of the full gradient in the right hand side. When B = |S |, each iteration handles the original objective function and Mini-SGD reduces to the batch GD algorithm. We note that both GD and SGD share the same convergence rate in terms of number of iterations in expectation for non-smooth optimization problems (i.e., O (1/ T ) after T iterations), but SGD method requires much less running time to convergence compared to the GD method due to the efficiency of its individual iterations.

15

5 Experimental Results
In this section, we conduct exhaustive experiments to demonstrate the merits and advantages of the proposed algorithm. We conduct the experiments on the well-known Epinions 2 data set, aiming to accomplish and answer the following fundamental questions: 1. Prediction accuracy: How does the proposed algorithm perform in comparison to the stateof- the-art algorithms with/without incorporating trust and distrust relationships between users. Whether or not the trust/distrust social network could help in making more accurate recommendations? 2. Correlation of social relations with rating information: To what extent, the trusted and distrusted friends of a user u are aligned with the ratings the user u issued for the reviews written by his friends? A positive answer to this question indicates that two users will issue similar (dissimilar) ratings if they are connected by a trust (distrust) relation and prefer to behave similarly. 3. Model selection: What role do the regularization parameters S , U and V play in the accuracy of the proposed recommender system and what is the best strategy to tune these parameters? 4. Handling cold-start users: How does exploiting social relationships in prediction process affect the performance of recommendation for cold-start users? 5. Trading trust for distrust: To what extent the distrust relations can compensate for the lack of trust relations? 6. Efficiency of optimization: What is the trade-off between accuracy and efficiency by moving from the gradient descent to the stochastic gradient descent with different batch sizes? In the following subsections, we intend to answer these questions. We begin by introducing the data set we use in our experiemnts and the metrics we employ to evaluate the results, followed by the detailed experimental results.

5.1 Data Set Description and Experimental Setup
The Epinions data set We begin by discussing the data set we have chosen for our experiments. To evaluate the proposed algorithm on trust and distrust-aware recommendations, we use the Epinions data set [22], a popular e-commerce site and customer review website where users share opinions on various types of items such as electronic products, companies, and movies, through writing reviews about them or assigning a rating to the reviews written by other users. The rating values in Epinions are discrete values ranging from not helpful (1/5) to most helpful (5/5). These ratings and reviews would potentially influence future customers when they are about to decide whether a product is worth buying or a movie is worth watching. Epinions allows users to evaluate other users based on the quality of their reviews, and to make trust and distrust relations with other users in addition to the ratings. Every member of Epinions can maintain a "trust" list of people he/she trusts that is referred to as web of trust (social network with trust relationships) based on the reviewers with consistent ratings or "distrust" list known as block list (social network with distrust relationships) that presents reviewers whose reviews were consistently found to be inaccurate or low quality. The fact that the data set contains explicit positive and negative relations between users makes it very appropriate to study issues in trust- and distrust-enhanced recommender systems. Epinions is thus an ideal source for experiments on social recommendation. We remark that the Epinions data set only contains bivalent relations (i.e., contains only full trust and full distrust, and no gradual statements). To conduct the coming experiments, we sampled a subset of Epinions data set with n = 121, 240 users and m = 685, 621 different items. The total number of observed ratings in the sampled data set is 12,721,437 which approximately includes 0.02% of all entries in the rating matrix R which demonstrates the sparsity of the rating matrix. We note that the selected items are the most frequently rated overall. The statistics of the data set is given in Table 2. The social statistics of the this data source is summarized
2 http://www.trustlet.org/wiki/Epinions_datasets

16

Table 2: Statistics of sample data from Epinions data set used in our experiments. Statistic Quantity Number of users 121,240 Number of items 685,621 Number of ratings 12,721,437 Number of trust relations 481,799 Number of distrust relations 96,823 Minimum number of ratings by users 1 Minimum number of ratings for items 1 Maximum number of ratings by users 148735 Maximum number of ratings for items 945 Average number of ratings by users 85.08 Average number of ratings for items 15.26

Table 3: Maximum and average trust and distrust relations for users in the sampled data set. Statistics Trust per user Be Trusted per user Max Min Average Max Min Average 1983 1 4.76 Distrust per user 1188 1 0.91 2941 0 4.76 Be Distrusted per user 429 0 0.91

in Table 3. The frequencies of ratings for users is shown are Table 4. In the user distrust network, the total number of issued distrust statements is 96,823. As to the user trust network, the total number of issued trust statements is 481,799. Experimental setup To better evaluate the effect of utilizing the social side information in recommendation accuracy, we employ different amount of training data 90%, 80% , 70% and 60% to create four different training sets that are increasingly sparse but the social network remains the same in all of them. Training data 90%, for example, means we randomly select 90% of the ratings from the sampled Epinions data set as the training data to predict the remaining 10% of ratings. The random selection was carried out 5 times independently to have a fair comparison. Also, since our preliminary results on a smaller data set revealed that the hinge loss performs better than the exponential loss, in the rest of experiments we stick to this loss function. However, we note the exponential loss is slightly faster in optimizing the corresponding objective function thanks to its smoothness, but it was negligible considering its worse accuracy compared to the hinge loss. All implementations are in Matlab, and all experiments were performed on a 4-core 2.0 GHZ of a load-free machine with a 12G of RAM.

5.2 Metrics
5.2.1 Metrics for rating prediction We employ two well-known measures, the Mean Absolute Error (MAE) and the Root Mean Squared Error (RMSE) [25] to measure the prediction accuracy of the proposed approach in comparison with other basic collaborative filtering and trust/distrust-enhanced recommendation methods. MAE is very appropriate and useful measure for evaluating prediction accuracy in offline tests [25, 45]. To calculate MAE, the predicted rating is compared with the real rating and the difference (in absolute value) considered as the prediction error. Then, these individual errors are averaged over all predictions to obtain the overall MAE value. More precisely, let T denote the set of ratings to be predicted,

17

Table 4: The frequencies of user's rating. # of Ratings # of Users # of Ratings # of Users 0-10 4,198,074 ( 33%) 61-70 157,745 ( 1.24%) 11-20 3,053,144 ( 24%) 71-80 143,752 ( 1.13%) 21-30 2,289,858 ( 18%) 81-90 104,315 ( 0.82%) 31-40 1,526,572 ( 12%) 91-100 43,252 ( 0.34%) 41-50 534,300 ( 4.2%) 101-200 21,626 ( 0.17%)

267,1

2 10,68

^ denote the prediction matrix obtained by i.e., T = {(i , j )  [n ]  [m ], R i j needs to be predicted} and let R algorithm after factorization. Then, MAE =
(i , j )T

^i j | |R i j - R

|T |

,

^i j is the rating user i would assign where R i j is the real rating assigned by the user i to the item j , and R to the item j that is predicted by the algorithm . The RMSE metric is defined as:
(i , j )T

RMSE =

^i j Ri j - R |T |

2

.

The first measure (MAE) considers every error of equal value, while the second one (RMSE) emphasizes larger errors. We would like to emphasize that even small improvements in RMSE are considered valuable in the context of recommender systems. For example, the Netflix prize competition offered a 1,000,000 reward for a reduction of the RMSE by 10% [72]. 5.2.2 Metrics for evaluating the correlation of ratings with trust/distrust relations As part of our experiments, we investigate how the explicit trust/distrust relations between users in the social network are aligned with the implicit trust/distrust relations between users conveyed from the rating information. We use recall, Mean Average Precision (MAP) [44] and Normalized Discount Cumulative Gain (NDCG) to evaluate the ranking results. Recall is defined as the number of relevant friends divided by the total number of friends in the social network. Precision is defined as the number of relevant friends (trusted or distrusted) divided by the number of friends in the social network. Given a user u , let r i be the relevance score of the friend ranked at position i , where r i = 1 if the user is relevant to the u and r i = 0 otherwise. Then we can compute the Average Precision (AP) as AP =
i ri

 Precision@i

# of relevant friends

.

MAP is the average of AP over all the users in the network. NDCG is a normalization of the Discounted Cumulative Gain (DCG) measure. DCG is a weighted sum of the degree of relevancy of the ranked users. The weight is a decreasing function of the rank (position) of the user, and therefore called discount. NDCG normalizes DCG by the Ideal DCG (IDCG), which is simply the DCG measure of the best ranking result. Thus NDCG measure is always a number in [0, 1]. NDCG at position k is defined as: NDCG@k = Zk 2r i - 1 i =1 log(i + 1)
k

where k is also called the scope, which means the number of top-ranked users presented to the user and Zk is chosen such that the perfect ranking has a NDCG value of 1. We note that the base of the logarithm does not matter for NDCG, since constant scaling will cancel out due to normalization. We will assume it is the natural logarithm throughout this paper.

18

Figure 2: Grid Search to find the best values for U and C on the data set with 90% of rating information.

5.3 Model Selection
Tuning of parameters (a.k.a model selection in learning community) is a critical problem in most of the learning problems. In some situations, the learning performance may drastically vary with different choices of the parameters. There are three parameters in objective (5) that play very important role in the effectivity of the proposed algorithm. These are U , V , and S . Between these, S controls how much the proposed algorithm should incorporate the information of the social network in completing the partially observed rating matrix. In the extreme case, a very small value for S , the algorithm almost forgets the social information exists between the users and only utilizes the observed user-item rating matrix for factorization. On the other hand, if we employ a very large value for S , the social network information will dominate the learning process, leading to a poorer performance. Therefore, in order to not hurt the recommendation performance, we need to find a reasonable value for social regularization parameter. To this end, we analyze how the combination of these parameters affect the recommendation performance. We conduct a grid search on the potential values of two parameters S and V to find the combination with best performance. Figure 2 shows the grid search results for these parameters on data set with 90% of training data where the optimal prediction accuracy is achieved at point (14.8, 11) with the optimal RMSE = 1.12. We would like to emphasize that we have done the cross validation for only pairs of (S , V ) and (S , U ) because, (i) considering the grid search for the triplet (S , U , V ) is computationally burdensome, (ii) and our preliminary experiments showed that V and U behave similarly with respect to S . Based on the results reported in Figure 2, in the remaining experiments, we set S = 14.8, V = 11, and U = 13 when the training is performed on the data set with 90% of rating information. We repeat the same process to find out the best setting of regularization parameters for other data sets with 80%, 70%, and 60% of rating data as well.

5.4 Baseline Methods
Here we briefly discuss the baseline algorithms that we intend to compare the proposed algorithm. The baseline algorithms are chosen from both types of memory-based and model-based recommender systems with different types of trust and distrust relations. In particular, we consider the following basic algorithms:  MF (matrix factorization based recommender): this is the basic matrix factorization based rec19

ommender formulated in the optimization problem in (1) which does not take the social data into account.  MF+T (matrix factorization with trust information): to exploit the trust relations between users in matrix factorization, [40] relied on the fact that the distance between latent features of users who trust each other must be minimized that can be formulated as the following objective: min
U

1 n D (Ui ,: , U j ,: ), 2 i =1 j N+ (i )

where N+ (i ) is the set of users the i th user trusts in the social network (i.e., S i j = +1). By employing this intuition in the basic formulation in (1), [40] solves the following optimization problem: min
U,V

1 R i j - Ui ,: V j ,: 2 (i , j )R

2

+

 n U D (Ui ,: , U j ,: ) + U 2 i =1 j N+ (i ) 2

F+

V V 2

F

.

 MF+D (matrix factorization with distrust information): the basic intuition behind the algorithm proposed in [40] to exploit the distrust relations is as follows: if user u i distrusts user u j , then we can assume that their corresponding latent features Ui ,: and U j ,: would have a large distance. As a result we aim to maximize the following quantity for all users: max
U

1 n D (Ui ,: , U j ,: ), 2 i =1 j N- (i )

where N- (i ) denotes the set of users the i th users distrusts (i.e, S i j = -1). Adding this term to the basic optimization problem in (1) we obtain the following optimization problem: min
U,V

1 R i j - Ui ,: V j ,: 2 (i , j )R

2

-

U  n D (Ui ,: , U j ,: ) + U 2 i =1 j N- (i ) 2

F+

V V 2

F

.

 MF+TD (matrix factorization with trust and distrust information): this algorithm stands for the algorithm proposed in the present work. We note that there is no algorithm in the literature that exploits both trust and distrust relations in factorization process simultaneously.  NB (neighborhood-based recommender): this algorithm is the basic memory-based recommender algorithm that predicts a rating of a target item i for user u using a combination of the ratings of neighbors of u (similar users) that already issued a rating for item i . Formally, ^ui = R u + R
u N (u ),Wuu >0

u ) Wuu (R ui - R Wuu , (9)

u N (u ),Wuu

where the pairwise weight Wuu between pair of users (u , u ) is calculated by Pearson's correlation coefficient [25]  NB+T (neighborhood with trust information) [45, 17, 47]: the basic idea behind the trust based recommender systems proposed in TidalTrsut [17] and MoleTrsut [45] is to limit the set of neighbors in (9) to the users who are trusted by user u . The distinguishing feature of these algorithms is the mechanism of trust propagation to estimate the trust transitively for all the users. By adapting (9) to only consider trustworthy neighbors in predicting the new ratings we obtain: ^ui = R u + R
 (u ),W u N+ uu >0

u ) Wuu (R ui - R Wuu , (10)

 (u ),W u N+ uu >0

 where N+ (u ) is the set of trusted neighbors of u in the social network with propagated trust rela tions (when there is no propagation we have N+ (u ) = N+ (u )). We note that instead of Pearson's correlation coefficient as the wighting schema, we can infer the weights exploiting the social relation between the users. Since for the data set we consider in our experiments, the trust/distrust relations are binary values, the social based pairwise distance would be simply the hamming distance between the binary vector representation of social relations of users. For implementation details we refer to [70, Chapter 6].

20

Table 5: The consistency of implicit and explicit trust relations in the data set for different ranges of ratings measured in terms of NDCG, recall, and MAP . # of Ratings NDCG@10 NDCG@20 Recall@10 Recall@20 Recall@40 MAP 0-20 21-40 41-60 61-80  81 0.083 0.108 0.117 0.120 0.135 0.078 0.103 0.112 0.117 0.126 0.054 0.080 0.083 0.088 0.091 0.092 0.125 0.128 0.132 0.151 0.156 0.198 0.225 0.230 0.253 0.140 0.190 0.208 0.230 0.244

Table 6: The consistency of implicit and explicit distrust relations in the data set for different ranges of ratings measured in terms of NDCG, recall, and MAP . # of Ratings NDCG@10 NDCG@20 Recall@10 Recall@20 Recall@40 MAP 0-20 21-40 41-60 61-80  81 0.065 0.071 0.082 0.089 0.104 0.057 0.068 0.072 0.078 0.096 0.045 0.060 0.075 0.081 0.087 0.071 0.077 0.085 0.105 0.125 0.132 0.140 0.158 0.164 0.191 0.130 0.134 0.152 0.160 0.183

 NB+TD-F (neighborhood with trust information and distrust information as filtration) [69, 72]: a simple strategy to use distrust relations in the recommendation is to filter out distrusted users from the list of neighbors in predicting the ratings. As a result, we adapt (9) to exclude distrusted users from the users' propagated web of trust.  NB+TD-D (neighborhood-based with trust information and integrated distrust information) [69, 72]: in the same spirit as the filtration strategy, we can use distrust relations to debug the trust relations. More specifically, if user u trusts user v , v trusts w , and u distrusts w , then the latter distrust relation contradicts the propagation of the trust from u to w and can be excluded from the prediction. In this method distrust is used to debug the trust relations.

5.5 On the Consistency of Social Relations and Rating Information
As already mentioned, the Epinions website allows users to write reviews about products and services and to rate reviews written by other users. Epinions also allows users to define their web of trust, i.e. "reviewers whose reviews and ratings have been consistently found to be valuable" and their block list, i.e. "reviewers whose reviews are found to be consistently inaccurate or not valuable. Different intuitions on interpreting these social information will result in different models. The main rational behind incorporating trust and distrust relations in recommendation process is to take the trust/distrust relations between users in the social network as the level of agreement between ratings assigned to reviews by users 3 . Therefore, investigating the consistency or alignment between user ratings (implicit trust) and trust/distrust relations in the social network (explicit trsut) become an important issue. Here, we aim to empirically investigate whether or not there is a correlation between a user's current trustees/friends or distrusted friends and the ratings that user would assign to reviews issued by his neighbors. Obviously, if there is no correlation between social context of a user and his/her ratings to reviews written by his neighbors, then the social structure does not provide any advantage to the rating information. On the other hand, if there exists such a correlation, then the social context could be supplementary information to compensate for the lack of rating information to boost the accuracy of recommendations. The consistency of trust relations and rating information issued by users on the reviews written by his trustees has been analyzed in [83, 23]. However, [83] also claimed that social trust (i.e., explicit trust) and similarity between users based on their issued ratings (i.e., implicit trust) are not the same,
3 In the literature the similarity between users conveyed from the rating information issued by users and the direct relation in the social network are usually referred to as the implicit and the explicit trust, respectively.

21

Table 7: The alignment rate of users in establishing trust/distrust relationships with future users in the social network based on the majority vote of their current trusted/distrusted friends. The number of trusted friends (+) and distrusted friends (-) are denoted by n + and n - , respectively. Here u denotes the current user and w stands for a future user in the network. Setting Type of Relation (u w ) % of Relations Alignment Rate (%) n+ > n- n+ < n- n + = n - > 0 or n + = n - = 0 + + + 48.80 2.54 1.15 8.02 39.49 92.09 8.15 17.88 83.42 -

and can be used complementary. According to [38], when comparing implicit social information with explicit social information, the performance of using implicit information is slightly worse. We further investigate the same question about the consistency of distrust relations and ratings issued by users to their distrusted neighbors. The positive answer to this question can be interpreted as follows. Given that user u is interested in item i , the chances that v , trusted (distrusted) by u , also likes this item i is much higher (lower) than for user w not explicitly trusted (distrusted) by u . To measure the similarity between users, there are several methods we can borrow in the literature. In this paper, we adopt the most popular approach that is referred to as Pearson correlation coefficient (PCC) P : U  U  [-1, +1] [6, 47], which is defined as: P (u , v ) =
m i =1 (R ui m j =1 (R ui

u )(R vi - R v ) -R
m j =1 (R vi

 u )2  -R

 v )2 -R

, u , v  U ,

u and R v are the average of ratings issued by users u and v , respectively. The PCC measures where R the extent to which there is a linear relationship between the rating behaviors of the two users, the extreme values being -1 and 1. The similarity of two users becomes negative when users have completely diverging ratings. We note that this quantity can be considered as the implicit trust between users that is conveyed via ratings given by users. To conduct this set of experiments, we first group all the users in the training data set based on the number of ratings, and then measure the prediction accuracies of different user groups. Users are grouped into five classes: "[1, 20)", "[20, 40)", "[40, 60)", "[60, 80)", and "> 81 ". In order to have a comprehensive view of the ranking performance, we present the NDCG, recall and MAP scores of trust and distrust alignments on the Epinions data set in Table 5 and Table 6, respectively. We note that the data set we use in our experiments only contains bivalent trust values, i.e., -1 and +1, and it is not possible to have an ordering on the list of friends (timestamp of relations would be an option to order the friends but unfortunately it is not available in our data set). To compute the NDCG, we use the ordering of trusted/distrusted friends which yields the best value. On the positive side, we observe a clear trend of alignment between ratings assigned by a user and the type of relation he has made in the social network. This observation coincides with our intuition. Overall, when more ratings are observed for a user, the similarity calculation process will find more accurate similar or dissimilar neighbors for this user since we have more information to represent or interpret this user. Hence, by increasing the number of ratings, It is conceivable from the results in Tables 5 and 6 that the alignment between implicit and explicit neighbors becomes better. By comparing the results in Tables 5 and 6 we can see that trust relations are slightly better aligned than the distrust relations. On the negative side, the results show that the NDCG on both types of relations is small. One explanation for this phenomenon is that the Epinions data set is not tightly bound to a specific application. For example, a user may trust or distrust anther user based on his/her comments on a specific product but they might have similar taste on other products. Furthermore, compared to other data sets such as FilmTrusts, the Epinions data set is very sparse data set, and consequently it is relatively inaccurate to rely on the rating information to compute the implicit trust relations. Finally, our approach to distinguish trust/distrust lists from the rating information is limited by the PCC trust metric we have utilized. We conjecture that better trust metrics that is able to exploit other side information such as time and in22

Table 8: The accuracy of prediction of matrix factorization with three different methods measured in terms of MAE and RMSE errors. The parameter k represents the number of latent features in factorization. k % of Training Measure MF MF+T MF+D MF+TD 10 60% 70% 80% 90% 20 60% 70% 80% 90% MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE 0.9813  0.042 1.6050  0.032 0.9462  0.083 1.5327  0.032 0.9150 0.022 1.3824  0.032 0.8921  0.025 1.2166  0.017 0.9972  0.016 1.6248  0.014 0.9688  0.019 1.5162  0.016 0.9365  0.025 1.4081  0.015 0.9224  0.016 1.2207  0.0 18 0.8561  0.032 1.4125  0.022 0.8332  0.092 1.2407  0.063 0.8206  0.041 1.1906  0.042 0.8158  0.016 1.1403  0.027 0.8431  0.018 1.3904  0.042 0.8342  0.062 1.2722  0.027 0.8172  0.011 1.1853  0.023 0.8128  0.021 1.1402  0.026 0.9720  0.038 1.5036  0.040 0.9241  0.012 1.4405  0.023 0.8722  0.034 1.3155  0.026 0.8736  0.053 1.1869  0.049 0.9746  0.060 1.5423  0.046 0.9350  0.022 1.4540  0.075 0.8705  0.016 1.3591  0.073 0.8805  0.032 1.1933  0.028 0.8310  0.016 1.2294  0.086 0.8206  0.023 1.1562  0.043 0.8113  0.032 1.1061  0.021 0.8025  0.014 1.0872  0.020 0.8475  0.012 1.1837  0.023 0.8290  0.034 1.1452  0.016 0.8129  0.025 1.1049  0.082 0.8096  0.010 1.0851  0.011

teractional information would be helpful in distinguishing implicit trusted/distrusted friends, leading to better alignment between implicit and explicit trust relations. We also conduct experiments to evaluate the consistency of social network only based on the trust/distrust relations between users. In particular, we investigate to what extent a users' relations are aligned with the opinion of his/her neighbors in the social network. More specifically, let u be a user who is about to make a trust or distrust relation to another user v . We assume that n + number of u 's neighbors trust v and n - number of u 's neighbors distrust v . We note that in the real data set the distrust relations are hidden. To conduct this set of experiments, we randomly sample 30% of the relations from the social network and use the remaining 70% to predict the type of sampled relations 4 by majority voting. Table 7 shows the results on the consistency of social relations. We observe that in all cases there is an alignment between the opinions of users' friends and his/her own relation (92.09% and 83.42% when the majority of friends trust and distrust the target user, respectively). This might be due to social influence of people on social network, however, it is hard to justify the existence of such a correlation in Epinions data set which includes reviews for diverse set of products and taste of users. One interesting observation from the results reported in Table 7 is the case where the number of distrusted users dominates the number of trusted users (i.e., n - > n + ). While the distrust relations are private to other users, but we can see that there is a significant alignment between users's relation type and his distrusted friends.

5.6 On the Power of Utilizing Social Relationships
We now turn to investigate the effect of utilizing social relationships between users on the accuracy of recommendations in factorization-based methods. In other words, we would like to experimentally evaluate whether incorporating distrust can indeed enhance the trust-based recommendation process. To this end, we run four different MF (i.e., pure matrix factorization based algorithm), MF+T (i.e., matrix factorization with only trust relationships), MF+D (i.e., matrix factorization with only distrust relationships), and MF+TD (i.e., the algorithm proposed here) algorithms on the data set. We run the algorithms with k = 10 and k = 20 latent vector dimensions. As mentioned earlier, different amount of training data 90%, 80% , 70% and 60% has been used to create four different training sets that are increasingly sparse but the social network remains the same in all of them. We evaluate all algorithms by both MAE and
4 A more realistic way would be to use the timestamp of relations to create the training and test sets.

23

RMSE measures. Table 8 shows the MAE and RMSE errors for the four sampled data sets. First, as we expected, the performance of all learning algorithms improves with an increasing number of training data. It is also not surprising to see that the MF+T, MF+D, and MF+TD algorithms which exploit social side information perform better than the pure matrix factorization based MF algorithm. Second, the proposed algorithm outperforms all other baseline algorithms for all the cases, indicating that it is effective to incorporate both types of social side information in recommendation. This result by itself indicates that besides trust relationships in the social network, the distrust information is also a rich source of information and can be utilized in recommendation algorithms. We note that distrust information needs to be incorporated carefully as its nature is totally different from trust information. Finally, it is noticeable that the MF+T outperforms the MF+D algorithm due to huge number of trust relations to distrust relations in our data set. It is also remarkable that users are more likely to be influenced by their friends to make trust relations than the distrust relations due to the private nature of distrust relations in Epinions data set. This might lead us to believe that distrust relations have better quality than trust relations which requires a deeper investigation to be verified.

5.7 Comparison to Baseline Algorithms
Another question that is worthy of investigation is how state-of-the-art approaches perform compared to the method proposed in this paper. To this end, we compare the performance of the MF-TD algorithm with the baseline algorithms introduced in Subsection 5.4. Table 9 contains the results of our experiments with eight different algorithms on the data set with 90% of rating data. The second column in the table represents the configuration of parameters used by each algorithm. When we utilize trust/distrust relations in neighborhood-based algorithms, a crucial decision we need to make is to which level the propagation must be performed (no propagation corresponds to the single level propagation which only includes direct neighbors). Let p and q denote the level of propagation for trust and distrust relations, respectively. Let us first consider the trust propagation to decide the value of p . We note that there is a tradeoff between accuracy and the level of trust propagation: the longer propagation levels results in less accurate trust predictions. This is due the fact that when we use longer propagation levels, the further away we are heading from each user, and consequently decrease the confidence on the predictions. Obviously this affects the accuracy of the recommendations significantly. As a result, for the trust propagation we only consider single level propagation by choosing p = 1  (i.e, N+ = N+ ). We also note that since in the Epinions data set a user can not simultaneously trust and distrust another user, in the neighborhood-based method with distrust relations, the debugging only makes sense for propagated information. Therefore, we perform a three level distrust propagation (q = 3) to constitute the set of distrusted users for each users. We note that the longer the propagation levels, the more often distrust evidence can be found for a particular user, and hence the less neighbors will be left to participate in the recommendation process. For factorization based methods, the value of regularization parameters, i.e., U , V , and S , are determined by the procedure discussed in Subsection 5.3. The results of Table 9 reveal some interesting conclusions as summarized below:  From Table 9, we can observe that for factorization-based methods, incorporating trust or distrust information boost the performance of recommendation in terms of both accuracy measures. This demonstrates the advantages of trust and distrust-aware recommendation algorithms. We also can see that both MF+T and MF+D perform better than the non-social MF but the performance of MF+T is significantly better than MF+D. As discussed before, this observation does not indicate that the trust relations are more beneficial than the distrust relations as in our data set only 16.7% of relations are distrust relations. The MF+TD algorithm that is able to employ both types of relations is significantly better than other algorithms that demonstrates the advantages of proposed method to utilize trust and distrust relations.  Looking at the results reported in Table 9, it can immediately be noticed that the incorporation of trust and distrust information in neighborhood-based methods decreases the prediction error but the improvement is not as significant as the factorization based methods. We note that for the NB+T method with longer levels of propagation (p = 2, 3), our experiments revealed that the accuracy remains almost same or gotten worse on both MAE and RMSE measures and this is 24

Table 9: Comparison with other popular methods. The reported values are the MAE and RMSE on the data set with 90% of rating information. The values of parameters for each specific algorithm is included in the second column. Method Parameter (s) MAE RMSE MF MF+T MF+D MF+TD NB NB+T NB+TD-F NB+TD-D k = 10 and U = V = 5 k = 10, U = V = 5 , and  = 1 k = 10, U = V = 5 , and  = 10 k = 10, U = 13, V = 11 , and S = 14.8 p =1 p = 1 and q = 3 p = 1 and q = 3 0.8921 0.8158 0.8736 0.8025 0.9381 0.8904 0.8692 0.8728 1.2166 1.1403 1.1852 1.0872 1.5275 1.3455 1.2455 1.2604

Table 10: The accuracy of handling cold-start users and the effect of social relations. The number of leant features in this experiments is set to k = 10. The first column shows the number of cold-start users sampled randomly from all users in the data set. For the cold-starts users all the ratings have been excluded from the training data and used in the evaluation of three different algorithms. % of Cold-start Users Measure MF MF+T MF+D MF+TD 30% 20% 10% 5% MAE RMSE MAE RMSE MAE RMSE MAE RMSE 0.9923 1.7211 0.9812 1.7088 0.9334 1.4222 0.9134 1.3852 0.8824 1.5562 0.8805 1.4339 0.8477 1.3782 0.8292 1.2921 0.9721 1.6433 0.9505 1.6250 0.9182 1.4006 0.8633 1.3255 0.8533 1.4802 0.8472 1.2630 0.8322 1.2655 0.8280 1.2888

why we only report the results only for p = 1. In contrast, for distrust propagation we found out that q = 3 has a visible impact on the performance of both filtering and debugging methods. We would like to emphasize that for longer levels of distrust propagation in Epinions data set, i.e.,  q > 4, we found that the size of the set of distrusted users N- () becomes large for most of users which degrades the prediction accuracy. We also observe another interesting result about the performance of NB+TD method with filtering and debugging strategies. We found that although filtering generates slightly better predictions, NB+TD-F performs almost as good as the NB+TDD method. Although this observation does not suggest any of these methods as the method of choice in incorporating distrust, we believe that the accuracy might differ from data set to data set and it strongly depends on the propagation/aggregation strategy.  Considering the results for both model-based and memory-based methods in Table 9, we can conclude few interesting observations. First, we notice that factorization-based methods with trust/distrust information perform better than the neighborhood based methods. Second, the incorporation of trust and distrust relations in matrix factorization has significant improvement compared to improvement achieved by memory-based methods. Although the type of filtration or debugging strategy could significantly affect the accuracy of incorporating distrust in memorybased methods, but the main shortcoming of these methods comes from the fact that these algorithms somehow exclude the influence of distrusted users from the rating prediction. This stands in stark contrast to the model proposed in this paper that ranks the neighbors based on the type of relation. This observation necessitates to devise better algorithms for propagation and aggregation of trust/distrust information in memory-based methods.

25

Table 11: The accuracy of proposed algorithm on a data set with 390257 ( 90%) trust relations sampled uniformly at random from all trust relations with varied number of distrust relations. The learning is performed based on 90% of all ratings with k = 10 as the dimension of latent features. Method # of Trust Relations # of Distrust Relations Measure Accuracy MF+TD 433,619 ( 90%) 9,682 ( 10%) 19,364 ( 20%) 29,047 ( 30%) 38,729 ( 40%) 48,411 ( 50%) 58,093 ( 60%) 67,776 ( 70%) 77,458 ( 80%) 87,140 ( 90%) 96,823 (= 100%) MF+T 481,799 (= 100%) 0 MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE 0.8803  0.051 1.2166  0.028 0.8755  0.033 1.1944  0.042 0.8604  0.036 1.1822  0.081 0.8431  0.047 1.1706 0.055 0.8165 0.056 1.1425 0.091 0.8130 0.035 1.1380 0.046 0.8122  0.041 1.1306  0.042 0.8095  0.036 1.1290  0.085 0.8061  0.044 1.1176  0.067 0.8050  0.052 1.1092  0.063 0.8158  0.016 1.1403  0.027

5.8 Handling Cold-start Users by Social Side Information
In this subsection, we demonstrate the use of social network to further illustrate the potential of proposed framework and the relevance of incorporating side information. To do so, as another set of our experiments, we intend to examine the performance of proposed algorithm on clod-start users. Addressing cold-start users (i.e., users with few ratings or new users) is a very important for the success of recommender systems due to huge number of this type of users in many real world systems. As a result, handling cold-start users is one the main challenges in existing systems. To evaluate different algorithms we randomly select 30%, 20%, 10%, and 5% as the cold-start users. For cold-start users, we do not include any rating in the training data and consider all the ratings made by cold-start users as testing data. Table 10 shows the performance of above mentioned algorithms. As it is clear from the Table 10, when the number of cold-start users is low with respect to the total number of users, say 5% of total users, the affect of distrust relationships is negligible in prediction accuracy. But, when the number of cold-start users is high, exploiting the trust and distrust relationships significantly improve the performance of recommendation. This result is interesting as it reveals that the lack of rating information for cold-start and new users can be alleviated by incorporating the social relations of users, and in particular both trust and distrust relationships.

5.9 Trading Trust for Distrust Relationships
We also compare the potential benefit of trust relations to distrust relations in the proposed algorithm. More specifically, we would like to see in what extent the distrust relations can compensate for the lack of trust relations. We run the proposed algorithm with the subset of trust and distrust relations and compare it to the algorithm which only utilizes all of the trust relations. To setup this set of experiments, we randomly sample a subset of trust relations and gradually increase the amount of distrust relations to see when the effect of distrust information compensate the effect of missed trust relations.

26

We sample 433,619 (approximately 90%) trust relations from the total 481,799 trust relations and vary the number of distrust relations and feed to the proposed algorithm. Table 11 reports the accuracy of proposed algorithm for different number of distrust relations in the data sets. All these samplings have been done uniformly at random. We use 90% of all ratings for training and the remaining 10% for evaluation, and set the dimension of latent features to k = 10. As it can be concluded from Table 11, when we feed the proposed algorithm MF+TD with 90% of trust and 50% of the distrust relations, it reveals very similar behavior to the trust-enhanced matrix factorization based method MF+T, which only utilizes all the trust relations in factorization. This result is interesting in the sense that the distrust information between users is as important as the trust information (we note that in this scenario the number trust relations excluded from the training is almost same as the number of distrust relations included). By increasing the number of distrust relations we can observe that the accuracy of recommendations increases as expected. In summary, this set of experiments validates that incorporating distrust relations can indeed enhance the trust-based recommendation process and could be considered as a rich source of information to be exploited.

5.10 On the Impact of Batch Size in Stochastic Optimization
As mentioned earlier in the paper, directly solving the optimization problem in (5) using full gradient descent method requires to go through all the triplets in the constraint set S which could be computationally expensive due to the huge number of triplets in S . To overcome this efficiency problem, one can turn to stochastic gradient scent method which tries to generate unbiased estimates of the gradient at each iteration in a much cheeper way by sampling a subset of triplets from S . To accomplish this goal, we perform gradient descent and stochastic gradient descent to solve the optimization problem in (5) to find the matrices U and V following the updating equations derived in (7) and (8). At each iteration t , the currently learned matrices Ut and Vt are used to predict the ratings in the test set. In particular, at each iteration, we evaluate the RMSE and MAE on the test set, and terminate training once the RMSE and MAE starts increasing, or the maximum number of iterations is reached. We run the algorithm with latent vectors of dimension k = 10. We compare the computational efficiency between proposed algorithm with GD and mini-batch SGD with different batch sizes. We note that the GD updating rule can be considered as min-batch SGD where the batch size B is deterministically set to be B = |S | and simple SGD can be considered as mini-batch SGD with B = 1. We remark that in contrast to GD method which uses all the triplets in S for gradient computation at each iteration, for SGD method due to uniform sampling over all tuples in S , some of the tuples may be used more than once and some of the tuples might never been used for gradient computation. Figures 3 and 4 show the convergence rate of four different updating rules in terms of the number of iterations t for two different measures RMSE and RME, respectively. The first algorithm denoted by GD runs the simple full gradient descent iteratively to optimize the objective. The other three algorithms named SGD1, SGD2, and SGD3 in the figures use the batch sizes of B = 0.1  |S |, B = 0.2  |S |, and B = 0.3  |S |, respectively. In our experiments, due to very slow convergence of the basic SGD method with B = 1 in comparison to other fours methods, we simply exclude its result from the discussion. In terms of accuracy of predictions, from both Figures 3 and 4, we can conclude that the GD has the best convergence and SGD3 has the worst convergence in all settings. This is because, although all of the four algorithms use an unbiased estimate of the true gradient to update the solution at each iteration, but the variance of each stochastic gradient is proportional to the size of the batch size B . Therefore, for larger values of B , the variance of stochastic gradients is smaller and the algorithm convergences faster, but, for smaller values of B the algorithm suffers from high variance in stochastic gradients and convergences slowly. We emphasize that this comparison holds for iteration complexity which is different from the computational complexity (running time) of individual iterations. More specifically, each iteration of GD requires |S | gradient computations, while for SGD we only need to perform B |S | gradient computations. In summary, SGD has lightweight iteration but requires more iterations to converge. In contrast, GD takes expensive steps in much less number of iterations. From Figures 3 and 4, it is noticeable that although a large number of iterations is usually needed to obtain a solution of desirable accuracy using SGD, the lightweight computation per iteration makes SGD attractive for the optimization problem in (5) for large number of users. We also not that for the GD method, the error is a monotonically decreasing function it terms of number of iterations t , but for the SGD

27

based methods this does not hold. This is because although SGD algorithm is guaranteed to converge to an optimal solution (at least in expectation), but there is no guarantee that the stochastic gradients provide a descent direction for the objective at each iteration due to the noise in computing gradients. As a result, for few iterations we can see that the objective increases but finally it convergences as expected.

6 Conclusions and Future Works
In this paper, we have made a progress towards making distrust information beneficial in social recommendation problem. In particular, we have proposed a framework based on the matrix factorization which is able to incorporate both trust and distrust relationships between users in factorization algorithm. We experimentally investigated the potential of distrust as a side information to overcome the data sparsity and cold-start problems in traditional recommender systems. In summary, our results showed that more accurate recommendations can be obtained by incorporating distrust relations, indicating that distrust information can indeed be beneficial for the recommendation process. This work leaves few directions, both theoretically and empirically, as future work. From an empirical point of view, it would be interesting to extend our model for weighted social trust and distrust relations. One challenge in this direction is that, as far as we know, there is no publicly available data set that includes weighted (gradual) trust and distrust information. Also, the experimental results we have conducted on the consistency of social relations with rating information hint at a number of potential enhancements in future work. In particular, it would be interesting to further examine the correlation between implicit and explicit distrust information. An important challenge in this direction is to develop better metrics to measure the implicit trust between users as the simple metrics such as Pearson correlation coefficient seem to be insufficient. Furthermore, since we only consider the distrust between users, it would be easy to generalize our model in the same way to incorporate dissimilarity between items and investigate how it works in practice. Also, our preliminary results indicated that hinge loss almost performs better than the exponential loss, but from the optimization viewpoint, the exponential loss is more attractive due to its smoothness. So, an interesting direction would be to use a smoothed version of the hinge loss to gain from both optimization efficiency and algorithmic accuracy.

References
[1] Gediminas Adomavicius and Alexander Tuzhilin. Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions. IEEE Transactions on Knowledge and Data Engineering, 17(6):734749, 2005. [2] Deepak Agarwal and Bee-Chung Chen. flda: matrix factorization through latent dirichlet allocation. In Proceedings of the third ACM international conference on Web search and data mining, pages 91100. ACM, 2010. [3] Paolo Avesani, Paolo Massa, and Roberto Tiella. A trust-enhanced recommender system application: Moleskiing. In Proceedings of the 2005 ACM symposium on Applied computing, pages 1589 1593, 2005. [4] Giacomo Bachi, Michele Coscia, Anna Monreale, and Fosca Giannotti. Classifying trust/distrust relationships in online social networks. In Privacy, Security, Risk and Trust (PASSAT), 2012 International Conference on and 2012 International Confernece on Social Computing (SocialCom), pages 552557. IEEE, 2012. [5] Jess Bobadilla, Fernando Ortega, Antonio Hernando, and Abraham Gutirrez. Recommender systems survey. Knowledge-Based Systems, 46:109132, 2013. [6] John S Breese, David Heckerman, and Carl Kadie. Empirical analysis of predictive algorithms for collaborative filtering. In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence, pages 4352. Morgan Kaufmann Publishers Inc., 1998.

28

[7] Moira Burke and Robert Kraut. Mopping up: modeling wikipedia promotion decisions. In Proceedings of the 2008 ACM conference on Computer supported cooperative work, pages 2736. ACM, 2008. [8] Dorwin Cartwright and Frank Harary. Structural balance: a generalization of heider's theory. Psychological review, 63(5):277, 1956. [9] Gang Chen, Fei Wang, and Changshui Zhang. Collaborative filtering using orthogonal nonnegative matrix tri-factorization. Information Processing & Management, 45(3):368379, 2009. [10] Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan. Better mini-batch algorithms via accelerated gradient methods. In NIPS, volume 24, pages 16471655, 2011. [11] David Crandall, Dan Cosley, Daniel Huttenlocher, Jon Kleinberg, and Siddharth Suri. Feedback effects between similarity and social influence in online communities. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 160168. ACM, 2008. [12] Sanjoy Dasgupta, Michael L Littman, and David McAllester. Pac generalization bounds for cotraining. Advances in neural information processing systems, 1:375382, 2002. [13] Mukund Deshpande and George Karypis. Item-based top-n recommendation algorithms. ACM Transactions on Information Systems (TOIS), 22(1):143177, 2004. [14] Thomas DuBois, Jennifer Golbeck, and Aravind Srinivasan. Predicting trust and distrust in social networks. In Privacy, security, risk and trust (passat), 2011 ieee third international conference on and 2011 ieee third international conference on social computing (socialcom), pages 418424. IEEE, 2011. [15] Rana Forsati, Hanieh Mohammadi Doustdar, Mehrnoush Shamsfard, Andisheh Keikha, and Mohammad Reza Meybodi. A fuzzy co-clustering approach for hybrid recommender systems. International Journal of Hybrid Intelligent Systems, 10(2):7181, 2013. [16] Rana Forsati and Mohammad Reza Meybodi. Effective page recommendation algorithms based on distributed learning automata and weighted association rules. Expert Systems with Applications, 37(2):13161330, 2010. [17] Jennifer Golbeck. Computing and applying trust in web-based social networks. PhD thesis, 2005. [18] Jennifer Golbeck. Generating predictive movie recommendations from trust in social networks. Springer, 2006. [19] Jennifer Golbeck and James Hendler. Filmtrust: Movie recommendations using trust in web-based social networks. In Proceedings of the IEEE Consumer communications and networking conference, volume 96. Citeseer, 2006. [20] Nathaniel Good, J Ben Schafer, Joseph A Konstan, Al Borchers, Badrul Sarwar, Jon Herlocker, and John Riedl. Combining collaborative filtering with personal agents for better recommendations. In AAAI/IAAI, pages 439446, 1999. [21] Quanquan Gu, Jie Zhou, and Chris Ding. Collaborative filtering: Weighted nonnegative matrix factorization incorporating user and item graphs. In SIAM SDM, pages 199210, 2010. [22] R. Guha, Ravi Kumar, Prabhakar Raghavan, and Andrew Tomkins. Propagation of trust and distrust. In Proceedings of the 13th International Conference on World Wide Web, pages 403412. ACM, 2004. [23] Guibing Guo, Jie Zhang, Daniel Thalmann, Anirban Basu, and Neil Yorke-Smith. From ratings to trust: an empirical study of implicit trust in recommender systems. In SAC, 2014. [24] Jonathan L Herlocker, Joseph A Konstan, Al Borchers, and John Riedl. An algorithmic framework for performing collaborative filtering. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 230237. ACM, 1999. 29

[25] Jonathan L Herlocker, Joseph A Konstan, Loren G Terveen, and John T Riedl. Evaluating collaborative filtering recommender systems. ACM Transactions on Information Systems (TOIS), 22(1):553, 2004. [26] Thomas Hofmann. Latent semantic models for collaborative filtering. ACM Transactions on Information Systems (TOIS), 22(1):89115, 2004. [27] Mohsen Jamali and Martin Ester. Trustwalker: a random walk model for combining trust-based and item-based recommendation. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 397406. ACM, 2009. [28] Mohsen Jamali and Martin Ester. A matrix factorization technique with trust propagation for recommendation in social networks. In Proceedings of the fourth ACM conference on Recommender systems, pages 135142. ACM, 2010. [29] Mohsen Jamali and Martin Ester. A transitivity aware matrix factorization model for recommendation in social networks. In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence-Volume Volume Three, pages 26442649. AAAI Press, 2011. [30] Arnd Kohrs and Bernard Merialdo. Clustering for collaborative filtering applications. In In Computational Intelligence for Modelling, Control & Automation. IOS. Citeseer, 1999. [31] Ioannis Konstas, Vassilios Stathopoulos, and Joemon M Jose. On social networks and collaborative recommendation. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 195202. ACM, 2009. [32] Yehuda Koren. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 426434. ACM, 2008. [33] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):3037, 2009. [34] Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. Predicting positive and negative links in online social networks. In Proceedings of the 19th international conference on World wide web, pages 641650. ACM, 2010. [35] Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. Signed networks in social media. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 13611370. ACM, 2010. [36] Wu-Jun Li and Dit-Yan Yeung. Relation regularized matrix factorization. IJCAI-09, 2009. [37] Juntao Liu, Caihua Wu, and Wenyu Liu. Bayesian probabilistic matrix factorization with social relations and item contents for recommendation. Decision Support Systems, 2013. [38] Hao Ma. An experimental study on implicit social recommendation. In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval, pages 7382. ACM, 2013. [39] Hao Ma, Irwin King, and Michael R Lyu. Learning to recommend with social trust ensemble. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 203210. ACM, 2009. [40] Hao Ma, Michael R Lyu, and Irwin King. Learning to recommend with trust and distrust relationships. In Proceedings of the third ACM conference on Recommender systems, pages 189196. ACM, 2009. [41] Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin King. Sorec: social recommendation using probabilistic matrix factorization. In Proceedings of the 17th ACM conference on Information and knowledge management, pages 931940. ACM, 2008.

30

[42] Hao Ma, Dengyong Zhou, Chao Liu, Michael R Lyu, and Irwin King. Recommender systems with social regularization. In Proceedings of the fourth ACM international conference on Web search and data mining, pages 287296. ACM, 2011. [43] Hao Ma, Tom Chao Zhou, Michael R Lyu, and Irwin King. Improving recommender systems by incorporating social contextual information. ACM Transactions on Information Systems (TOIS), 29(2):9, 2011. [44] Christopher D Manning, Prabhakar Raghavan, and Hinrich Schtze. Introduction to information retrieval, volume 1. Cambridge university press Cambridge, 2008. [45] Paolo Massa and Paolo Avesani. Trust-aware collaborative filtering for recommender systems. In On the Move to Meaningful Internet Systems 2004: CoopIS, DOA, and ODBASE, pages 492508. Springer, 2004. [46] Paolo Massa and Paolo Avesani. Controversial users demand local trust metrics: An experimental study on epinions. com community. In Proceedings of the National Conference on artificial Intelligence, volume 20, page 121. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2005. [47] Paolo Massa and Paolo Avesani. Trust metrics in recommender systems. In Computing with Social Trust, pages 259285. Springer, 2009. [48] Prem Melville, Raymond J Mooney, and Ramadass Nagarajan. Content-boosted collaborative filtering for improved recommendations. In AAAI/IAAI, pages 187192, 2002. [49] Bradley N Miller, Joseph A Konstan, and John Riedl. Pocketlens: Toward a personal recommender system. ACM Transactions on Information Systems (TOIS), 22(3):437476, 2004. [50] Andriy Mnih and Ruslan Salakhutdinov. Probabilistic matrix factorization. In Advances in neural information processing systems, pages 12571264, 2007. [51] Uma Nalluri. Utility of distrust in online recommender systems. Technical report, 2008. [52] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574 1609, 2009. [53] Akshay Patil, Golnaz Ghasemiesfeh, Roozbeh Ebrahimi, and Jie Gao. Quantifying social influence in epinions. HUMAN, 2(2):pp67, 2013. [54] Dmitry Pavlov and David M Pennock. A maximum entropy approach to collaborative filtering in dynamic, sparse, high-dimensional domains. In NIPS, volume 2, pages 14411448, 2002. [55] Michael J Pazzani. A framework for collaborative, content-based and demographic filtering. Artificial Intelligence Review, 13(5-6):393408, 1999. [56] David M Pennock, Eric Horvitz, Steve Lawrence, and C Lee Giles. Collaborative filtering by personality diagnosis: A hybrid memory-and model-based approach. In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence, pages 473480. Morgan Kaufmann Publishers Inc., 2000. [57] Jasson DM Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative prediction. In Proceedings of the 22nd international conference on Machine learning, pages 713719. ACM, 2005. [58] Ruslan Salakhutdinov and Andriy Mnih. Bayesian probabilistic matrix factorization using markov chain monte carlo. In Proceedings of the 25th international conference on Machine learning, pages 880887. ACM, 2008. [59] Ruslan Salakhutdinov and Andriy Mnih. Probabilistic matrix factorization. Advances in neural information processing systems, 20:12571264, 2008. 31

[60] Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. Restricted boltzmann machines for collaborative filtering. In Proceedings of the 24th international conference on Machine learning, pages 791798. ACM, 2007. [61] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. Item-based collaborative filtering recommendation algorithms. In Proceedings of the 10th international conference on World Wide Web, pages 285295. ACM, 2001. [62] Wanita Sherchan, Surya Nepal, and Cecile Paris. A survey of trust in social networks. ACM Computing Surveys (CSUR), 45(4):47, 2013. [63] Luo Si and Rong Jin. Flexible mixture model for collaborative filtering. In ICML, volume 3, pages 704711, 2003. [64] Ian Soboroff and Charles Nicholas. Combining content and collaboration in text filtering. In Proceedings of the IJCAI, volume 99, pages 8691, 1999. [65] Nathan Srebro, Tommi Jaakkola, et al. Weighted low-rank approximations. In ICML, volume 3, pages 720727, 2003. [66] Nathan Srebro, Jason DM Rennie, and Tommi Jaakkola. Maximum-margin matrix factorization. Advances in neural information processing systems, 17(5):13291336, 2005. [67] Mojdeh Talabeigi, Rana Forsati, and Mohammad Reza Meybodi. A hybrid web recommender system based on cellular learning automata. In Granular Computing (GrC), 2010 IEEE International Conference on, pages 453458. IEEE, 2010. [68] Nele Verbiest, Chris Cornelis, Patricia Victor, and Enrique Herrera-Viedma. Trust and distrust aggregation enhanced with path length incorporation. Fuzzy Sets and Systems, 202:6174, 2012. [69] Patricia Victor, Chris Cornelis, Martine De Cock, and Ankur Teredesai. Trust- and distrust-based recommendations for controversial reviews. IEEE Intelligent Systems, 26(1):4855, 2011. [70] Patricia Victor, Chris Cornelis, and Martine De Cock. Trust networks for recommender systems, volume 4. Springer, 2011. [71] Patricia Victor, Chris Cornelis, Martine De Cock, and Enrique Herrera-Viedma. Practical aggregation operators for gradual trust and distrust. Fuzzy Sets and Systems, 184(1):126147, 2011. [72] Patricia Victor, Nele Verbiest, Chris Cornelis, and Martine De Cock. Enhancing the trust-based recommendation process with explicit distrust. ACM Transactions on the Web (TWEB), 7(2):6, 2013. [73] Fei Wang, Sheng Ma, Liuzhong Yang, and Tao Li. Recommendation on item graphs. In Data Mining, 2006. ICDM'06. Sixth International Conference on, pages 11191123. IEEE, 2006. [74] Jun Wang, Arjen P De Vries, and Marcel JT Reinders. Unifying user-based and item-based collaborative filtering approaches by similarity fusion. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501508. ACM, 2006. [75] Grzegorz Wierzowiecki and Adam Wierzbicki. Efficient and correct trust propagation using closelook. In Web Intelligence and Intelligent Agent Technology (WI-IAT), 2010 IEEE/WIC/ACM International Conference on, volume 1, pages 676681. IEEE, 2010. [76] Lei Wu, Steven CH Hoi, Rong Jin, Jianke Zhu, and Nenghai Yu. Distance metric learning from uncertain side information with application to automated photo tagging. In Proceedings of the 17th ACM international conference on Multimedia, pages 135144. ACM, 2009. [77] Gui-Rong Xue, Chenxi Lin, Qiang Yang, WenSi Xi, Hua-Jun Zeng, Yong Yu, and Zheng Chen. Scalable collaborative filtering using cluster-based smoothing. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 114121. ACM, 2005. 32

[78] Kai Yu, Anton Schwaighofer, Volker Tresp, Xiaowei Xu, and H-P Kriegel. Probabilistic memorybased collaborative filtering. Knowledge and Data Engineering, IEEE Transactions on, 16(1):5669, 2004. [79] Sheng Zhang, Weihong Wang, James Ford, and Fillia Makedon. Learning from incomplete ratings using non-negative matrix factorization. SIAM, 2006. [80] Yi Zhang and Jonathan Koren. Efficient bayesian hierarchical user modeling for recommendation system. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 4754. ACM, 2007. [81] Jianke Zhu, Hao Ma, Chun Chen, and Jiajun Bu. Social recommendation using low-rank semidefinite program. In Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011. [82] Cai-Nicolas Ziegler. On propagating interpersonal trust in social networks. In Computing with Social Trust, pages 133168. Springer, 2009. [83] Cai-Nicolas Ziegler and Jennifer Golbeck. Investigating interactions of trust and interest similarity. Decision Support Systems, 43(2):460475, 2007. [84] Cai-Nicolas Ziegler and Georg Lausen. Propagation models for trust and distrust in social networks. Information Systems Frontiers, 7(4-5):337358, 2005.

33

2 1.9 1.8 1.7 1.6 1.5 1.4 1.3 1.2 1.1 1 0
GD S G D 1 S G D 2 S G D 3

2 1.9 1.8 1.7 1.6 1.5 1.4 1.3 1.2 1.1 1 50 100 150 It e r a t i o n N u m b e r 200 250 0
GD S G D 1 S G D 2 S G D 3

RMSE Err o r

RMSE Err o r

50

100 150 It e r a t i o n N u m b e r

200

250

(a) 60% of Training Data

(b) 70% of Training Data

2 1.9 1.8 1.7 1.6 1.5 1.4 1.3 1.2 1.1 1 0
GD S G D 1 S G D 2 S G D 3

2 1.9 1.8 1.7 1.6 1.5 1.4 1.3 1.2 1.1 1 50 100 150 It e r a t i o n N u m b e r 200 250 0
GD S G D 1 S G D 2 S G D 3

RMSE Err o r

RMSE Err o r

50

100 150 It e r a t i o n N u m b e r

200

250

(c) 80% of Training Data

(d) 90% of Training Data

Figure 3: Comparison of accuracy of prediction in terms of RMSE with GD and SGD with three varied batch sizes.

34

1

1

0 . 95

0 . 95

0.9 M A E Err o r M A E Err o r
GD S G D 1 S G D 2 S G D 3

0.9

0 . 85

0 . 85

0.8

0.8

0 . 75

0 . 75

0.7 0

0.7 50 100 150 It e r a t i o n N u m b e r 200 250 0

GD S G D 1 S G D 2 S G D 3

50

100 150 It e r a t i o n N u m b e r

200

250

(a) 60% of Training Data

(b) 70% of Training Data

1

1

0 . 95

0 . 95

0.9 RMSE Err o r RMSE Err o r
GD S G D 1 S G D 2 S G D 3

0.9

0 . 85

0 . 85

0.8

0.8

0 . 75

0 . 75

0.7 0

0.7 50 100 150 It e r a t i o n N u m b e r 200 250 0

GD S G D 1 S G D 2 S G D 3

50

100 150 It e r a t i o n N u m b e r

200

250

(c) 80% of Training Data

(d) 90% of Training Data

Figure 4: Comparison of accuracy of prediction in terms of MAE with GD and SGD with three varied batch sizes.

35

GeoSpark: A Cluster Computing Framework for
Processing Large-Scale Spatial Data
Jia Yu

Jinxuan Wu

Mohamed Sarwat

School of Computing, Informatics,
and Decision Systems Engineering,
Arizona State University
699 S. Mill Avenue, Tempe, AZ

School of Computing, Informatics,
and Decision Systems Engineering,
Arizona State University
699 S. Mill Avenue, Tempe, AZ

School of Computing, Informatics,
and Decision Systems Engineering,
Arizona State University
699 S. Mill Avenue, Tempe, AZ

jiayu2@asu.edu

jinxuanw@asu.edu

msarwat@asu.edu

ABSTRACT
This paper introduces GeoSpark an in-memory cluster
computing framework for processing large-scale spatial data.
GeoSpark consists of three layers: Apache Spark Layer,
Spatial RDD Layer and Spatial Query Processing Layer.
Apache Spark Layer provides basic Spark functionalities
that include loading / storing data to disk as well as regular RDD operations. Spatial RDD Layer consists of three
novel Spatial Resilient Distributed Datasets (SRDDs) which
extend regular Apache Spark RDDs to support geometrical
and spatial objects. GeoSpark provides a geometrical operations library that accesses Spatial RDDs to perform basic
geometrical operations (e.g., Overlap, Intersect). System
users can leverage the newly deﬁned SRDDs to eﬀectively
develop spatial data processing programs in Spark. The
Spatial Query Processing Layer eﬃciently executes spatial
query processing algorithms (e.g., Spatial Range, Join, KNN
query) on SRDDs. GeoSpark also allows users to create a
spatial index (e.g., R-tree, Quad-tree ) that boosts spatial
data processing performance in each SRDD partition. Preliminary experiments show that GeoSpark achieves better
run time performance than its Hadoop-based counterparts
(e.g., SpatialHadoop).

Categories and Subject Descriptors
H.2.4 [DATABASE MANAGEMENT]: Systems—Distributed databases; H.2.8 [DATABASE MANAGEMENT]: Database Applications—Spatial databases and
GIS

Keywords
Cluster computing; Large-scale data; Spatial data

1. INTRODUCTION
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from Permissions@acm.org.
SIGSPATIAL’15 November 03-06, 2015, Bellevue, WA, USA
Copyright 2015 ACM ISBN 978-1-4503-3967-4/15/11 $15.00.
http://dx.doi.org/10.1145/2820783.2820860

The volume of available spatial data increased tremendously. Such data includes but not limited to: weather
maps, socioeconomic data, vegetation indices, and more.
Moreover, novel technology allows hundreds of millions of
users to use their mobile devices to access their healthcare
information and bank accounts, interact with friends, buy
stuﬀ online, search interesting places to visit on-the-go, ask
for driving directions, and more. Making sense of such spatial data will be beneﬁcial for several applications that may
transform science and society. Challenges to building such
platform are as follows: Challenge I: System Scalability. The
underlying database system must be able to digest Petabytes
of spatial data, eﬀectively stores it, and allows applications
to eﬃciently retrieve it when necessary. Challenge II: Interactive Performance. The underlying spatial data processing
system must ﬁgure out eﬀective ways to process user’s request in a sub-second response time.
Apache Spark is an in-memory cluster computing system.
Spark provides a novel data abstraction called resilient distributed datasets (RDDs) [9] that are collections of objects
partitioned across a cluster of machines. Each RDD is built
using parallelized transformations (ﬁlter, join or groupBy)
that could be traced back to recover the RDD data. In
memory RDDs allow Spark to outperform existing models
(MapReduce). Unfortunately, Spark does not provide support for spatial data and operations. Hence, users need to
perform the tedious task of programming their own spatial
data processing jobs on top of Spark.
This paper introduces GeoSpark 1 an in-memory cluster computing system for processing large-scale spatial data.
GeoSpark extends the core of Apache Spark to support
spatial data types, indexes, and operations. In other words,
the system extends the resilient distributed datasets (RDDs)
concept to support spatial data. The key contributions of
this paper are as follows: (1) GeoSpark as a full-ﬂedged
cluster computing framework to load, process, and analyze
large-scale spatial data in Apache Spark. (2) A set of out-ofthe-box Spatial Resilient Distributed Dataset (SRDD) types
(e.g., Point RDD and Polygon RDD) that provide in house
support for geometrical and distance operations. SRDDS
provides an Application Programming Interface (API) for
Apache Spark programmers to easily develop their spatial
analysis programs. (3) Spatial data indexing strategies that
partition the input Spatial RDD using a grid structure and
assign grids to machines for parallel execution. GeoSpark
1

GeoSpark website: http://geospark.datasyslab.org













	
	












Figure 2: SRDD partitioning

	






Figure 1: GeoSpark Overview

also adaptively decides whether a spatial index needs to be
created locally on a Spatial RDD partition to strike a balance between the run time performance and memory/cpu
utilization in the cluster. Experiments show that GeoSpark
achieves better run time performance than its Hadoop-based
counterparts (e.g., SpatialHadoop).
The rest of this paper is organized as follows. Section 2
highlights the related work. GeoSpark architecture is
given in Section 3. Preliminary experiments that evaluate
GeoSpark are given in Section 4. Finally, Section 5 concludes the paper.

2. BACKGROUND AND RELATED WORK
Spatial Database Systems. Spatial database operations are vital for spatial analysis and spatial data mining.
Spatial range queries inquire about certain spatial objects
exist in a certain area (e.g., Return all parks in Phoenix).
Spatial join queries are queries that combine two datasets
or more with a spatial predicate, such as distance relations
(e.g., ﬁnd the parks that have rivers in Phoenix). Spatial
k-Nearest Neighbors queries ﬁnd the k nearest objects to a
given spatial object (e.g., show the 10 nearby restaurants).
Spatial query processing algorithms usually make use of spatial indexes to reduce the query latency. For instance, RTree [3] provides an eﬃcient data partitioning strategy to
eﬃciently index spatial data. Its key idea is that group
nearby objects and put them in the next higher level node
of the tree. Quad-Tree [8] is also a spatial index that recursively divides a two-dimensional space into four quadrants.
Parallel and Distributed Spatial Data Processing.
As the development of distributed data processing system,
more and more people in geospatial area direct their attention to deal with massive geospatial data with distributed
frameworks. Hadoop-GIS [1] utilizes global partition indexing and customizable on demand local spatial indexing
to achieve eﬃcient query processing. SpatialHadoop [2], a
comprehensive extension to Hadoop, has native support for
spatial data by modifying the underlying code of Hadoop.
MD-HBase [6] extends HBase, a non-relational database

runs on top of Hadoop, to support multidimensional indexes
which allows for eﬃcient retrieval of points using range and
kNN queries. Parallel SECONDO [4] combines Hadoop with
SECONDO, a database which can handle non-standard data
types, like spatial data, usually not supported by standard
systems. Although these systems have well-developed functions, all of them are implemented on Hadoop framework.
That means they cannot avoid the disadvantages of Hadoop,
especially a large number of reads and writes on disks.

3.

GEOSPARK ARCHITECTURE

As depicted in Figure 1, GeoSpark consists of three main
layers: (1) Apache Spark Layer: that consists of regular
operations that are natively supported by Apache Spark.
These native functions are responsible for loading / saving
data from / to persistent storage (e.g., stored on local disk or
Hadoop ﬁle system HDFS). (2) Spatial Resilient Distributed
Dataset (SRDD) Layer (Section 3.1). (3) Spatial Query Processing Layer (Section 3.2).

3.1

Spatial RDD (SRDD) Layer

This layer extends Spark with spatial RDDs (SRDDs)
that eﬃciently partition SRDD data elements across machines and introduces novel parallelized spatial transformations and actions (for SRDD) that provide a more intuitive
interface for users to write spatial data analytics programs.
The SRDD layer consists of three new RDDs: PointRDD,
RectangleRDD and PolygonRDD. One useful Geometrical
operations library is also provided for every spatial RDD.
Spatial Objects Support. GeoSpark supports various
spatial data input format (e.g., Comma Separated Value,
Tab Separated Value and Well-Known Text). Each type
of spatial objects is stored in a SRDD, PointRDD, RectangleRDD or PolygonRDD. GeoSpark provides a set of
geometrical operations which is called Geometrical Operations Library. This library natively supports geometrical
operations. For example, Overlap(): Finds all of the internal objects which are intersected with others in geometry;
MinimumBoundingRectangle(): Finds the minimum bounding rectangles for each object in a Spatial RDD or return
a large minimum bounding rectangle which contains all of
the internal objects in a Spatial RDD; Union(): Returns the
union polygon of all polygons in this RDD.
SRDD Partitioning. GeoSpark automatically partitions all loaded Spatial RDDs by creating one global grid
ﬁle for data partitioning. The main idea for assigning each
element in a Spatial RDD to the same 2-Dimensional spatial
grid space is as follows: Firstly, split the spatial space into a




	

  







Figure 3: Query execution model

number of equal geographical size grid cells which compose
a global grid ﬁle. Then traverse each element in the SRDD
and assign this element to a grid cell if the element overlaps with this grid cell. If one element intersects with two
or more grid cells, then duplicate this element and assign
diﬀerent grid IDs to the copies of this element. Figure 2 depicts tweets in the U.S. at a particular moment, tweets and
states are assigned to respective grid cells.
SRDD Indexing. Spatial indexes like Quad-Tree and
R-Tree are provided in Spatial IndexRDDs which inherit
from Spatial RDDs. Users are able to initialize a Spatial IndexRDD. Moreover, GeoSpark adaptively decides whether
a local spatial index should be created for a certain Spatial IndexRDD partition based on a tradeoﬀ between the
indexing overhead (memory and time) on one-hand and the
query selectivity as well as the number of spatial objects on
the other hand.

3.2 Spatial Query Processing Layer
This layer supports spatial queries (e.g., Range query and
Join query) for large-scale spatial datasets. After geometrical objects are stored and processed in the Spatial RDD
layer, user may invoke a spatial query provided in Spatial
Query Processing Layer. GeoSpark processes such query
and returns the ﬁnal results to the user. Figure 3 gives the
general execution model followed by GeoSpark . This execution model implements the algorithms proposed by [5]
and [10]. To accelerate a spatial query, GeoSpark leverages the grid partitioned Spatial RDDs, spatial indexing, the
fast in-memory computation and DAG scheduler of Apache
Spark to parallelize the query execution.
Spatial Range Query. GeoSpark executes the spatial
range query algorithm following the execution model: Load
target dataset, partition data, create a spatial index on each
SRDD partition if necessary, broadcast the query window
to each SRDD partition, check the spatial predicate in each
partition, and remove spatial objects duplicates that existed
due to the data partitioning phase.
Spatial Join Query. GeoSpark executes the parallel
spatial join query following the execution model. GeoSpark
ﬁrst partitions the data from the two input SRDDs as well
as creates local spatial indexes (if required) for the SRDD
which is being queried. Then it joins the two datasets by
their keys which are grid IDs. For the spatial objects (from
the two SRDDs) that have the same grid ID, GeoSpark calculates their spatial relations. If two elements from two
SRDDS are overlapped, they are kept in the ﬁnal results.
The algorithm continues to group the results for each rectangle. The grouped results are in the following format: Rect-

angle, Point, Point, ... Finally, the algorithm removes the
duplicated points and returns the result to other operations
or saves the ﬁnal result to disk.
Spatial KNN Query. To process a Spatial KNN query,
GeoSpark uses a heap based top-k algorithm[7], which contains two phases: selection and merge. It takes a partitioned
SRDD, a point P and a number k as inputs. To calculate
the k nearest objects around point P , in the selection phase,
for each SRDD partition GeoSpark calculates the distances
between each object to the given point P , then maintains a
local heap by adding or removing elements based on the distances. This heap contains the nearest k objects around the
given point P . For IndexedSRDD, the system can utilize the
local indexes to reduce the query time. After the selection
phase, GeoSpark merges results from each partition, keeps
the nearest k elements that have the shortest distances to P
and outputs the result.

4.

EXPERIMENTS

This section provides preliminary experimental evaluation that studies the run time performance of the
following large-scale spatial data processing systems:
(1) GeoSpark_NoIndex | QuadTree | RTree: GeoSpark
approach without spatial index, with spatial Quad-Tree
or R-Tree index. In these approaches, data is partitioned according grids. Required spatial indexes are created on each partition after data partitioned. (2) SpatialHadoop_NoIndex | RTree: SpatialHadoop approach without spatial index or with spatial R-Tree index.
Experimental Setup. Our cluster setting on Amazon
EC2 is as follows: (1) CPU per worker: 8 Intel Xeon Processors operating at 2.5 GHz with Turbo up to 3.3 GHz.
(2) Memory per worker: 61 GB in total and 50 GB registered memory in Spark and Hadoop. (3) Storage per worker:
Amazon general purpose SSD. We deploy Ganglia, a scalable
distributed monitoring system for high performance computing systems such as clusters, on our Amazon EC2 experimental cluster.
Datasets. We use three real spatial datasets extracted
from TIGER ﬁles in our experiments: Zcta510 1.5 GB
dataset, Areawater 6.5 GB dataset and Edges 62 GB
dataset. They contain all the cities, all the lakes and all
the meaningful boundaries in the US in rectangle format
correspondingly. All of the datasets are preprocessed by
SpatialHadoop and are open to the public on its website [2].

4.1

Impact of Data Size

This section compares GeoSpark on TIGER Areawater
6.5 GB dataset with TIGER Edges 62 GB dataset as well as
SpatialHadoop. They are tested on 16 nodes cluster. Their
performance are shown in Figure 5. As depicted in Figure 5, GeoSpark and SpatialHadoop cost more run time
on the large dataset than that on the small one. However,
GeoSpark achieves much better run time performance than
SpatialHadoop in both datasets. This superiority is more
obvious on the small dataset. The reason is that GeoSpark
can cache more percentage of the intermediate data in memory on the small scale input than that on the large one. That
accelerates the processing speed.

4.2

Performance of Spatial Iterative Analysis

Spatial co-location pattern recognition is deﬁned as two
or more species are ofter located in a neighborhood rela-

1
3
5
7

9
11
13
15
17

d o u b l e t h r e s h o l d = THRESHOLD;
double baseDistance = 1 . 0 ;
double I n t e r v a l D i s t a n c e = 0 . 5 ;
i n t c o u n t e r =0;
double CoL oc at io n C oef f i cie n t =0.0;
// I n i t i a l i z e IndexedPointRDD
IndexedPointRDD t a r g e t =
new IndexedPointRDD ( SparkContext ,
DatasetLocation ) ;
// I t e r a t i v e Adja cency Matrix C a l c u l a t i o n
w h i l e ( C o L o c a t i o n C o e f f i c i e n t >t h r e s h o l d ) {
PairRDD glbAdjMat =
t a r g e t . S p a t i a l J o i n Q u e r y ( t a r g e t , WITHIN,
baseDistance + counter ∗ IntervalDistance ) ;
C o L o c a t i o n C o e f f i c i e n t=
C a l c u l a t e C o L o c a t i o n ( glbAdjMat ) ;
c o u n t e r ++;
}
r e t u r n b a s e D i s t a n c e + ( c o u n t e r −1) ∗
IntervalDistance ;

Figure 4: Adjacency Matrix (Java code) in GeoSpark

Figure 6: Run Time Performance for Spatial Co-location
Pattern Recognition
indexing and query processing algorithms in Apache Spark
to eﬃciently analyze spatial data at scale. Experiments
on data sizes and spatial analysis show that GeoSpark
achieves better run time performance than its MapReducebased counterparts (e.g., SpatialHadoop). The proposed
ideas are packaged into an open source software artifact.
In the future, we envision GeoSpark to be used by Earth
and Space Scientists, Geographers, Politicians, Commercial
Institutions to analyze spatial data at scale. We also expect
the scientiﬁc community will contribute to GeoSpark and
add new functionalities on top-of it that serve novel spatial
data analysis applications.

6.
Figure 5: Run Time Performance for Spatial Join Over Different Spatial Datasets

tionship. It usually executes multiple times to form a 2dimension curve for observation. This calculation needs the
adjacent matrix between two type of objects which is the
result of a join query. Sample code for ﬁnding adjacent matrix is given in Figure 4. We iteratively query GeoSpark
SRDDs two times with diﬀerent distances which can be
deﬁned as neighborhood relationships in adjacent matrix.
Since SpatialHadoop doesn’t natively support iterative jobs,
we have to run SpatialHadoop_RTree two times for a reasonable comparison. We use the ﬁrst point column in both
of TIGER Zcta 1.5 GB dataset and TIGER Edges 62 GB
dataset and join them.
As shown in Figure 6, GeoSpark outperforms SpatialHadoop in spatial co-location. And their performances are
also improved when we increase the number of machines per
cluster. GeoSpark only costs the quarter time of SpatialHadoop. The main reason behind is that GeoSpark caches
these datasets in memory with SRDDs automatically after
loads from the storage system. The iterative jobs like spatial co-location can invoke these SRDDs multiple times from
memory without any data transformation and data loading. SpatialHadoop has to read and transform the original
datasets again and again.

5. CONCLUSION AND FUTURE WORK
This paper introduced GeoSpark an in-memory cluster
computing framework for processing large-scale spatial data.
GeoSpark provides an API for Apache Spark programmers to easily develop spatial analysis applications. Moreover, GeoSpark provides native support for spatial data

REFERENCES

[1] A. Aji, F. Wang, H. Vo, R. Lee, Q. Liu, X. Zhang, and
J. H. Saltz. Hadoop-GIS: A High Performance Spatial
Data Warehousing System over MapReduce. PVLDB,
6(11):1009–1020, 2013.
[2] A. Eldawy and M. F. Mokbel. A demonstration of
spatialhadoop: An eﬃcient mapreduce framework for
spatial data. PVLDB, 6(12):1230–1233, 2013.
[3] A. Guttman. R-trees: a dynamic index structure for
spatial searching. In SIGMOD, 1984.
[4] J. Lu and R. H. Guting. Parallel Secondo: Boosting
Database Engines with Hadoop. In ICPADS, pages
738 –743, 2012.
[5] G. Luo, J. F. Naughton, and C. J. Ellmann. A
non-blocking parallel spatial join algorithm. In Data
Engineering, 2002. Proceedings. 18th International
Conference on, pages 697–705. IEEE, 2002.
[6] S. Nishimura, S. Das, D. Agrawal, and A. E. Abbadi.
MD-Hbase: A Scalable Multi-dimensional Data
Infrastructure for Location Aware Services. In MDM,
pages 7–16, 2011.
[7] N. Roussopoulos, S. Kelley, and F. Vincent. Nearest
neighbor queries. In ACM SIGMOD record,
volume 24, pages 71–79. ACM, 1995.
[8] H. Samet. The quadtree and related hierarchical data
structures. ACM Computing Surveys (CSUR),
16(2):187–260, 1984.
[9] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma,
M. McCauly, M. J. Franklin, S. Shenker, and I. Stoica.
Resilient Distributed Datasets: A Fault-Tolerant
Abstraction for In-Memory Cluster Computing. In
NSDI, pages 15–28, 2012.
[10] X. Zhou, D. J. Abel, and D. Truﬀet. Data partitioning
for parallel spatial join processing. Geoinformatica,
2(2):175–204, 1998.

Yuhan Sun

Mohamed Sarwat

CIDSE
Arizona State University
Tempe, AZ 85287-9309
Email: Yuhan.Sun.1@asu.edu

CIDSE
Arizona State University
Tempe, AZ 85287-9309
Email: msarwat@asu.edu

I. I NTRODUCTION
Graphs are widely used to model data in many application domains, including social networking, citation network
analysis, studying biological function of genes, and brain
simulation. A graph contains a set of vertices and a set of
edges that connect these vertices. Each graph vertex or edge
may possess a set of properties (aka. attributes). Thanks to the
wide spread use of GPS-enabled devices, many applications
assign a spatial attribute to a vertex (e.g., geo-tagged social
media). Figure 1 depicts an example of a social graph that
has two types of vertices: Person and Venue and two types
of edges: Follow and Like. Vertices with type Person
have two properties (i.e., attributes): name and age. Vertices
with type Venue have two properties: name and spatial
location. A spatial location attribute represents the spatial
location of the entity (i.e., Venue) represented by such vertex.
In Figure 1, vertices {e, f, g, h, i} are spatial vertices which
represent venues.

d

w

llo

Fo

Person

c

Follow

b

a
Like

Abstract—Graphs are widely used to model data in many
application domains. Thanks to the wide spread use of GPSenabled devices, many applications assign a spatial attribute
to graph vertices (e.g., geo-tagged social media). Users may
issue a Reachability Query with Spatial Range Predicate (abbr.
RangeReach). RangeReach finds whether an input vertex
can reach any spatial vertex that lies within an input spatial
range. An example of a RangeReach query is: Given a social
graph, find whether Alice can reach any of the venues located
within the geographical area of Arizona State University. The
paper proposes G EO R EACH an approach that adds spatial data
awareness to a graph database management system (GDBMS).
G EO R EACH allows efficient execution of RangeReach queries,
yet without compromising a lot on the overall system scalability
(measured in terms of storage size and initialization/maintenance
time). To achieve that, G EO R EACH is equipped with a lightweight data structure, namely SPA-Graph, that augments the
underlying graph data with spatial indexing directories. When a
RangeReach query is issued, the system employs a prunedgraph traversal approach. Experiments based on real system
implementation inside Neo4j proves that G EO R EACH exhibits
up to two orders of magnitude better query response time and
up to four times less storage than the state-of-the-art spatial and
reachability indexing approaches.

l

Like

arXiv:1603.05355v1 [cs.DB] 17 Mar 2016

GeoReach: An Efficient Approach for Evaluating
Graph Reachability Queries with Spatial Range
Predicates

j

h

k

a: {name: Alice, age: 19}
b: {name: Dan, age: 20}
c: {name: Carol, age: 35}
d: {name: Bob, age: 25}
j: {name: Kate, age: 18}
k: {name: Mat, age: 23}
l: {name: Katharine, age:21}

g
e

i
f

P
R

Venue
e: {name: Pita Jungle}
f :{name: Chipotle}
g: {name: Sushi 101}
h: {name: Subway}
i: {name: McDonald's}

Fig. 1: Location-Aware Social Graph

Graph Database Management Systems (GDBMSs) emerged
as a prominent NoSQL approach to store, query, and analyze
graph data [15], [8], [25], [24], [28]. Using a GDBMS,
users can pose reachability analysis queries like: (i) Find out
whether two vertices in the graph are reachable, e.g., Are Alice
(vertex a) and Katharine (vertex l) reachable in the social
graph given in Figure 1. (ii) Search for graph paths that match
a given regular language expression representing predicates on
graph elements, e.g., Find all venues that Alice’s Followees
and/or her Followees’ Followees also liked. Similarly, users
may issue a Reachability Query with Spatial Range Predicate
(abbr. RangeReach). A RangeReach query takes as input a
graph vertex v and a spatial range R and returns true only if v
can reach any spatial vertex (that possesses a spatial attribute)
which lies within the extent of R (formal definition is given in
Section II). An example of a RangeReach query is: Find out
whether Alice can reach any of the Venues located within the
geographical area of Arizona State University (depicted as a
dotted red rectangle R in Figure 1). As given in Figure 1, The
answer to this query is true since Alice can reach Sushi 101
(vertex g) which is located within R. Another query example
is to find out whether Katharine can reach any of the venues
located within R. The answer to this query is false due to the

fact that the only venue reachable from Katharine, Subway
(vertex h), is not located within R.
There are several straightforward approaches to execute
a RangeReach query: (1) Traversal Approach: The naive
approach traverses the graph, checks whether each visited
vertex is a spatial vertex and returns true as the answer if
the vertex’s spatial attribute lies within the input query range
R. This approach yields no storage/maintenance overhead
since no pre-computed data structure is maintained. However,
the Traversal approach may lead to high query response
time since the algorithm may traverse the whole graph to
answer the query. (2) Transitive Closure (TC) Approach: this
approach leverages the pre-computed transitive closure [27]
of the graph to retrieve all vertices that are reachable from v
and returns true if at least one spatial vertex (located in the
spatial range R) that is reachable from v. The TC approach
achieves the lowest query response time, however it needs
to pre-compute (and maintain) the graph transitive closure
which is deemed notoriously infeasible especially for largescale graphs. (3) Spatial-Reachability Indexing (SpaReach)
Approach: uses a spatial index [3], [22] to locate all spatial
vertices VR that lie within the spatial range R and then uses
a reachability index [35] to find out whether v can reach any
vertex in VR . SpaReach achieves better query response time
than the Traversal approach but it still needs to necessarily
probe the reachability index for spatial vertices that may never
be reached from the v. Moreover, SpaReach has to store and
maintain two index structures which may preclude the system
scalability.
In this paper, we propose G EO R EACH, a scalable and
time-efficient approach that answers graph reachability queries
with spatial range predicates (RangeReach). G EO R EACH is
equipped with a light-weight data structure, namely SPAGraph, that augments the underlying graph data with spatial
indexing directories. When a RangeReach query is issued,
the system employs a pruned-graph traversal approach. As
opposed to the SpaReach approach, G EO R EACH leverages
the Spa-Graph’ s auxiliary spatial indexing information to
alternate between spatial filtering and graph traversal and early
prunes those graph paths that are guaranteed: (a) not to reach
any spatial vertex or (b) to only reach spatial vertices that
outside the input spatial range query. As opposed to the TC
and SpaReach approaches, G EO R EACH decides the amount of
spatial indexing entries (attached to the graph) that strikes a
balance between query processing efficiency on one hand and
scalability (in terms of storage overhead) on the other hand. In
summary, the main contributions of this paper are as follows:
• To the best of the authors’ knowledge, the paper is the
first that formally motivates and defines RangeReach,
a novel graph query that enriches classic graph reachability analysis queries with spatial range predicates.
RangeReach finds out whether an input graph vertex can
reach any spatial vertex that lies within an input spatial
range.
• The paper proposes G EO R EACH a generic approach that
adds spatial data awareness to an existing GDBMS.

Notation
G = {V, E}
Vvout
Vvin
RF (v)
VS
RFS (v)
n
m
v1 ❀ v2
MBR(P )

Description
A graph G with a set of vertices V and set of edges E
The set of vertices that can be reached via a direct edge
from a vertex v
The set of vertices that can reach (via a direct edge) vertex
v
The set of vertices that are reachable from (via any number
of edges) vertex v
The set of spatial vertices in G such that VS ⊆ V
The set of spatial vertices that are reachable from (via any
number of edges) vertex v
The cardinality of V (n = |V |); the number of vertices in
G
The cardinality of E (m = |E|); the number of edges in G
v2 is reachable from v1 via connected path in G (such that
both v1 and v2 ∈ V )
Minimum bounding rectangle of a set of spatial polygons
P (e.g., points, rectangles)

TABLE I: Notations.

G EO R EACH allows efficient execution of RangeReach
queries issued on a GDBMS, yet without compromising
a lot on the overall system scalability (measured in terms
of storage size and initialization/maintenance time).
1
• The paper experimentally evaluates G EO R EACH
using
real graph datasets based on a system implementation
inside Neo4j (an open source graph database system).
The experiments show that G EO R EACH exhibits up to
two orders of magnitude better query response time and
occupies up to four times less storage than the state-ofthe-art spatial and reachability indexing approaches.
The rest of the paper is organized as follows: Section II lays
out the preliminary background and related work. The SPAGraph data structure, G EO R EACH query processing, initialization and maintenance algorithms are explained in Sections III
to V. Section VI experimentally evaluates the performance of
G EO R EACH. Finally, Section VII concludes the paper.
II. P RELIMINARIES

AND

BACKGROUND

This section highlights the necessary background and related research work. Table I summarizes the main notations in
the paper.
A. Preliminaries
Graph Data. G EO R EACH deals with a directed property
graph G = (V, E) where (1) V is a set of vertices such that
each vertex has a set of properties (attributes) and (2) E is a set
of edges in which every edge can be represented as a tuple of
two vertices v1 and v2 (v1 , v2 ∈ V ). The set of spatial vertices
VS ⊆ V such that each v ∈ VS has a spatial attribute (property)
v.spatial. The spatial attribute v.spatial may be a geometrical
point, rectangle, or a polygon. For ease of presentation, we
assume that a spatial attribute of spatial vertex is represented
by a point. Figure 1 depicts an example of a directed property
graph. Spatial Vertices VS are represented by black colored
circles and are located in a two-dimensional planer space while
white colored circles represent regular vertices that do not
1 https://github.com/DataSystemsLab/GeoGraphDB–Neo4j

possess a spatial attribute. Arrows indicate directions of edges
in the graph.
Graph Reachability (v1 ❀ v2 ). Given two vertices v1 and v2
in a graph G, v1 can reach v2 (v1 ❀ v2 ) or in other words v2
is reachable from v1 if and only if there is at least one graph
path from v1 to v2 . For example, in Figure 1, vertex a can
reach vertex f through the graph path a->c->i->f so it can
be represented as a ❀ f . On the other hand, c cannot reach
h.
Reachability with Spatial Range Predicate (RangeReach).
RangeReach queries find whether a graph vertex can reach a
specific spatial region (range) R. Given a vertex v ∈ V in a
Graph G and a spatial range R, RangeReach can be described
as follows:

RangeReach(v, R) =


true













f alse

if ∃ v ′ such that
(1) v ′ ∈ VS
(2) v ′ .spatial lies within R
(3) v ❀ v ′
Otherwise.
(1)

As given in Equation 1, if any spatial vertex v ′ ∈ VS that
lies within the extent of the spatial range R is reachable from
the input vertex v, then RangeReach(v, R) returns true (i.e.,
v ❀ R). For example, in Figure 1, RangeReach(a, R) = true
since a can reach at least one spatial vertex f in R. However,
RangeReach(l, R) = false since l can merely reach a spatial
vertex h which is not located in R. Vertex d cannot reach R
since it cannot reach any vertex.
B. Related Work
This section presents previous work on reachability indexes,
spatial indexes, and straightforward solutions to processing
graph reachability queries with spatial range predicates (RangeReach).
Reachability Index. Existing solutions to processing graph
reachability queries (u ❀ v) can be divided into three
categories [35]: (1) Pruned Graph Traversal [6], [30], [34]:
These approaches pre-compute some auxiliary reachability
information offline. When a query is issued, the query processing algorithm traverses the graph using a classic traversal
algorithm, e.g., Depth First Search (DFS) or Breadth First
Search (BFS), and leverages the pre-computed reachability
information to prune the search space. (2) Transitive closure
retrieval [1], [7], [17], [18], [27], [31], [32]: this approach
pre-computes the transitive closure of a graph offline and compresses it to reduce its storage footprint. When a query u ❀ v
is posed, the transitive closure of the source vertex u is fetched
and decomposed. Then the query processing algorithm checks
whether the terminal vertex v lies in the transitive closure of u.
and (3) Two-Hop label matching [5], [8], [10], [11], [12], [26]:
The two-hop label matching approach assigns each vertex v in
the graph an out-label set Lout (v) and an in-label set Lin (v).
When a reachability query is answered, the algorithm decides
that u ❀ v if and only if Lout (v) ∩ Lin (v) 6= ∅. Since the

two label sets do not contain all in and out vertices, size of
the reachability index reduces.
Spatial Index. A spatial index [21], [23], [29] is used
for efficient retrieval of either multi-dimensional objects (e.g.,
hx,yi coordinates of an object location) or objects with spatial
extents, e.g., polygon areas represented by their minimum
boundary rectangles (MBR). Spatial index structures can be
broadly classified to hierarchical (i.e., tree-based) and nonhierarchical index structures. Hierarchical tree-based spatial
index structures can be classified into another two broad
categories: (a) the class of data-partitioning trees, also known
as the class of Grow-and-Post trees [20], which refers to the
class of hierarchical data structures that basically extend the
B-tree index structure [2], [13] to support multi-dimensional
and spatial objects. The main idea is to recursively partition
the spatial data based on a spatial proximity clustering, which
means that the spatial clusters may overlap. Examples of
spatial index structures in this category include R-tree [16] and
R*-tree [3]. (b) the class of space-partitioning trees that refers
to the class of hierarchical data structures that recursively
decomposes the space into disjoint partitions. Examples of
spatial index structures in this category include the Quadtree [14] and k-d tree [4].
Spatial Data in Graphs. Some existing graph database
systems, e.g., Neo4j, allow users to define spatial properties on
graph elements. However, these systems do not provide native
support for RangeReach queries. Hence, users need to create
both a spatial index and a reachability index to efficiently
answer a RangeReach queries (drawbacks of this approach
are given in the following section). On the other hand, existing
research work [19] extends the RDF data with spatial data to
support RDF queries with spatial predicates (including range
and spatial join). However, such technique is limited to RDF
and not general graph databases. It also does not provide an
efficient solution to handle reachability queries.
C. Straightforward Solutions
There are three main straightforward approaches to process
a RangeReach query, described as follows:
Approach I: Graph Traversal. This approach executes
a spatial reachability query using a classical graph traversal
algorithm like DFS (Depth First Search) or BFS (Breadth
First Search). When RangeReach(v, R) is invoked, the system
traverses the graph from the starting vertex v. For each visited
vertex, the algorithm checks whether it is a spatial vertex and
returns true as the query answer if the vertex’s location lies
within the input query range R because the requirement of
spatial reachability is satisfied and hence v ❀ R. Otherwise,
the algorithm keeps traversing the graph. If all vertices that v
can reach do not lie in R, that means v cannot reach R.
Approach II: Transitive Closure (TC). This approach precomputes the transitive closure of the graph and stores it as an
adjacency matrix in the database. Transitive closure of a graph
stores the connectivity component of the graph which can be
used to answer reachability query in constant time. Since the
final result will be determined by spatial vertices, only spatial

vertices are stored. When RangeReach(v, R) is invoked, the
system retrieves all spatial vertices that are reachable from v
by means of the transitive closure. The system then returns
true if at least one spatial vertex that is reachable from v is
also located in the spatial range R.
Approach III: SpaReach. This approach constructs two
indexes a-priori: (1) A Spatial Index: that indexes all spatial
vertices in the graph and (2) A Reachability Index: that indexes
the reachability information of all vertices in the graph. When
a RangeReach query is issued, the system first takes advantage
of the spatial index to locate all spatial vertices VR that
lie within the spatial range R. For each vertex v ′ ∈ VR ,
a reachability query against the reachability index is issued
to test whether v can reach v ′ . For example, to answer
RangeReach(a, R) in Figure 2, spatial index is exploited first
to retrieve all spatial vertices that are located in R. From the
range query result, it can be known that g, i and f are located
in rectangle R. Then graph reachability index is accessed to
determine whether a can reach any located-in vertex. Hence, it
is obvious RangeReach(a, R) = true by using this approach.
Critique. The Graph Traversal approach yields no storage/maintenance overhead since no pre-computed data structure is maintained. However, the traversal approach may lead
to high query response time (O(m) where m is the number
of edges in the graph) since the algorithm may traverse the
whole graph to answer the query. The TC approach needs
to pre-compute (and maintain) the graph transitive closure
which is deemed notoriously infeasible especially for largescale graphs. The transitive closure computation is O(kn3 ) or
O(nm) and the TC storage overhead is O(kn2 ) where n is
total number of vertices and k is the ratio of spatial vertices
to the total number of vertices in the graph. To answer a
RangeReach query, the TC approach takes O(kn) time since it
checks whether each reachable spatial vertex in the transitive
closure is located within the query rectangle. On the other
hand, SpaReach builds a reachability index, which is a timeconsuming step, in O(n3 ) [32] time. The storage overhead
of a spatial index is O(n) and that of a reachability index
is O(nm1/2 ). To store the two indices, the overall storage
overhead is O(nm1/2 ). Storage cost of this approach is far less
than TC approach but still not small enough to accommodate
large-scale graphs. The query time complexity of a spatial
index is O(kn) while that of reachability index is m1/2 . But
for a graph reachability query, checking is demanded for each
spatial vertex in the result set generated by the range query.
Hence, cost of second step reachability query is O(knm1/2 ).
The total cost should be O(knm1/2 ). Query performance
of Spa-Reach is highly impacted by the size of the query
rectangle since the query rectangle determines how many
spatial vertices are located in the region. In Figure 1, query
rectangle R overlaps with three spatial vertices. For example,
to answer RangeReach(l, R), all three vertices {f, g, i} will
be checked against the reachability index to decide whether
any of them is reachable from l and in fact neither of them
is reachable. In a large graph, a query rectangle will possibly
contain a large number of vertices. That will definitely lead to

d
c
b

a
l

1

2
h

5

4

3

RMBR(j)

6

8

7 g

L0

e

13

Q

10

9
14

f

12

11

15

i

16

17
19

B-vertex:
a: true
d: false
f: false
h: false
k: false

k

j

18

L1

20

21

R-vertex:
j: RMBR(j)
G-vertex:
b: {2, 19}
c: {12, 14}
e: {14}
g: {12, 14}
i: {14}
l: {2}

L2

Fig. 2: SPA-Graph Overview

high unreasonable high query response time.
III. O UR A PPROACH : G EO R EACH
In this section, we give an overview of G EO R EACH an efficient and scalable approach for executing graph reachability
queries with spatial range predicates.
A. Data Structure
In this section, we explain how G EO R EACH augments a
graph structure with spatial indexing entries to form what we
call SPatially-Augmented Graph (SPA-Graph). To be generic,
G EO R EACHstores the newly added spatial indexing entries
the same way other properties are stored in a graph database
system. The structure of a SPA-Graph is similar to that of
the original graph except that each vertex v ∈ V in a SPAGraph G = {V, E} stores spatial reachability information. A
SPA-Graph has three different types of vertices, described as
follows:
•

•

B-Vertex: a B-Vertex v (v ∈ V ) stores an extra bit (i.e.,
boolean), called Spatial Reachability Bit (abbr. GeoB)
that determines whether v can reach any spatial vertex
(u ∈ VS ) in the graph. GeoB of a vertex v is set to 1
(i.e., true) in case v can reach at least one spatial vertex
in the graph and reset to 0 (i.e., false) otherwise.
R-Vertex: an R-Vertex v (v ∈ V ) stores an additional attribute, namely Reachability Minimum Bounding
Rectangle (abbr. RMBR(v)). RMBR(v) represents the
minimum bounding rectangle MBR(S) (represented by
a top-left and a lower-right corner point) that encloses all
spatial polygons which represent all spatial vertices S that
are reachable from vertex v (RMBR(v) = MBR(RFS (v)),
RFS (v) = {u|v ❀ u, u ∈ VS }).

G-Vertex: a G-Vertex v stores a list of spatial grid cells,
called the reachability grid list (abbr. ReachGrid(v)). Each
grid cell C in ReachGrid(v) belongs to a hierarchical grid
data structure that splits the total physical space into n
spatial grid cells. Each spatial vertex u ∈ VS will be
assigned a unique cell ID (k ∈ [1, n]) in case u is located
within the extents of cell k, noted as Grid(u) = k. Each
cell C ∈ ReachGrid(v) contains at least one spatial vertex
that is reachable from v (ReachGrid(v) = ∪ Grid(u),
{u|v ❀ u, u ∈ VS }).
Lemma 3.1: Let v (v ∈ V ) be a vertex in a SPA-Graph
G = {V, E} and Vvout be the set of vertices that can be
reached via a direct edge from a vertex v. The reachability
minimum bounding rectangle of v (RMBR(v)) is equivalent
to the minimum bounding rectangle that encloses all its outedge neighbors Vvout and their reachability minimum bounding
rectangles. RMBR(v) = MBRv′ ∈Vvout (RMBR(v ′ ), v ′ .spatial).
Proof: Based on the reachability definition, the set of
reachable vertices RF (v) from a vertex v is equal to the union
of the set of vertices that is reached from v via a direct edge
′
(Vvout ) and all vertices that are reached from each vertex v ∈
Vvout . Hence, the set (RFS (v)) of reachable spatial vertices
from v is given in Equation 2.
•

RFS (v) =
′

[

′

(v ′ ∪ RFS (v ))

(2)

v ∈Vvout

And since RMBR(v) = MBR(RFS (v)), then the the reachability minimum bounding rectangle of v is as follows:
RM BR(v) = M BR(
′

[

′

(v ′ ∪ RFS (v ))))
(3)

v ∈Vvout
′

′

= M BRv′ ∈Vvout (RM BR(v ), v .spatial)

That concludes the proof.
Lemma 3.2: The set of reachable spatial grid cells from a
given vertex v is equal to the union of all spatial grid cells
reached from its all its out-edge neighbors and grid cells that
contain the spatial neighbors
ReachGrid(v) =

[

(ReachGrid(v ′ ) ∪ Grid(v ′ ))

(4)

v ′ ∈Vvout

Proof: Similar to that of Lemma III-A.
Example. Figure 2 gives an example of a SPA-Graph. GeoB
of vertex b is set to 1 (true) since b can reach three spatial
vertices e, f and h. GeoB for d is 0 since d cannot reach any
spatial vertex in the graph. Figure 2 also gives an example of a
Reachability Minimum Bounding Rectangle RMBR of vertex
j (i.e., RMBR(j)). All reachable spatial vertices from j are g,
i, h and f . Figure 2 also depicts an example of ReachGrid.
There are three layers of grids, denoted as L0 , L1 , L2 from
top to bottom. The uppermost layer L0 is split into 4 × 4
grid cells; each cell is assigned a unique id from 1 to 16.
We denote grid cell with id 1 as G1 for brevity. The middle
layer gird L1 is split into four cells G17 to G20 . Each cell
in L1 covers four times larger space than each cell in L0 .
G17 in L1 covers exactly the same area of G1 , G2 , G5 , G6

Algorithm 1 Reachability Query with Spatial Range Predicate
1: Function R ANGER EACH(v, R)
2: if v is a spatial vertex and v.spatial Lie In R then return true
3: Terminate ← true
4: if v is a B-vertex then
5:
if GeoB(v) = true then Terminate ← false
6: else if v is a R-vertex then
7:
if R full contains RMBR(v) then return true
8:
if R no overlap with RMBR(v) then return false
9:
Terminate ← false
10: else if v is a G-vertex then
11:
for each grid Gi ∈ ReachGrid(v) do
12:
if R fully contains Gi then return true
13:
Gi partially overlaps with R then Terminate ← false
14: if Terminate = false then
15:
for each vertex v ′ ∈ Vvout do
16:
if R ANGER EACH(v ′ , R) = true then return true
17: return false

in L0 . The bottom layer L2 contains only a single grid cell
which covers all four grids in L1 and represents the whole
physical space. All spatial vertices reachable from vertex a
are located in G2 , G7 , G9 , G12 and G14 , respectively. Hence,
ReachGrid(a) can be {2, 7, 9, 12, 14}. Notice that vertex e
and f are both located in G9 and G14 covered by G19 in
ReachGrid(a) can be replaced by G19 . Then, ReachGrid(a)
= {2, 7, 12, 19}. In fact, there exist more options to represent
ReachGrid(a), such as {17, 18, 19, 20} or {21} by merging
into only a single grid cell in L2 . When we look into
ReachGrid of connected vertices, for instance g, ReachGrid(g)
is {12, 14} and ReachGrid(i) is {14}. It is easy to verify that
ReachGrid(g) is ReachGrid(i)∪Grid(i.spatial), which accords
with lemma 3.2.
SPA-Graph Intuition. The main idea behind the SPAGraph is to leverage the spatial reachability bit, reachability
minimum bounding rectangle and reachability grid list stored
in a B-Vertex, R-Vertex or a G-Vertex to prune graph paths
that are guaranteed (or not) to satisfy both the spatial range
predicate and the reachability condition. That way, G EO R E ACH cuts down the number of traversed graph vertices and
edges and hence significantly reduce the overall latency of a
RangeReach query.
B. Query Processing
This section explains the RangeReach query processing
algorithm. The main objective is to visit as less graph vertices
and edges as possible to reduce the overall query latency.
The query processing algorithm accelerates the SPA-Graph
traversal procedure by pruning those graph paths that are
guaranteed (or not) to satisfy the spatial reachability constraint.
Algorithm 1 gives pseudocode for query processing. The
algorithm takes as input a graph vertex v and query rectangle
R. It then starts traversing the graph starting from v. For
each visited vertex v, three cases might happen, explained as
follows:
Case I (B-vertex): In case GeoB is false, a B-vertex
cannot reach any spatial vertex and hence the algorithm stops
traversing all graph paths after this vertex. Otherwise, further
traversal from current B-vertex is required when GeoB value
is true. Line 4 to 5 in algorithm 1 is for processing such case.

RMBR

A

R

I

A
I

RMBR

Query Rectangle

RMBR
R

Query Rectangle

(a) No Overlap

(x1,y1)

(x1,y1)

B

B
e

RMBR

(b) Lie In

(x2,y2)

(x2,y2)

e
f

RMBR

Q

(a)

Query
Rectangle

(b)

A
I

RMBR

f

Q

Query Rectangle

(c) Partially Covered By

R
(x1,y1)

B

RMBR

e
(x2,y2)

Fig. 3: Relationships between RMBR and a query rectangle
f

Q

Case II (R-vertex): For a visited R-vertex u, there are three
conditions that may happen (see figure 3). They are the case
from line 6 to 9 in algorithm 1:
• Case II.A: RMBR(u) lies within the query rectangle (see
Figure 3b). In such case, the algorithm terminates and
returns true as the answer to the query since there must
exist at least a spatial vertex that is reachable from v.
• Case II.B: The spatial query region R does not overlap
with RMBR(u) (see Figure 3a). Since all reachable spatial
vertices of u must lie inside RMBR(u), there is no
reachable vertex can be located in the query rectangle.
As a result, graph paths originating at u can be pruned.
• Case III.C: RMBR(u) is partially covered by the query
rectangle (see Figure 3c). In this case, the algorithm keeps
traversing the graph by fetching the set of vertices Vvout
that can be reached via a direct edge from v.
Case III (G-vertex): For a G-vertex u, it store many
reachable grids from u. Actually, it can be regarded as many
smaller RMBRs. So three cases may also happen. Algorithm 1
line 13 to 18 is for such case. Three cases will happen are
explained as follows:
• Case III.A: The query rectangle R fully contains any
grid cell in ReachGrid(u). In such case, the algorithms
terminates and returns true as the query answer.
• Case III.B: The query rectangle have no overlap with all
grids in ReachGrid(u). This case means that v cannot
reach any grids overlapped with R. Then we never
traverse from v and this search branch is pruned.
• Case III.C: If the query rectangle fully contains none
of the reachable grid and partially overlap with any
reachable grid, it corresponds to Partially Covered By
case for RMBR. So further traversal is performed.
Figure 2 gives an example of RangeReach that finds
whether vertex a can reach query rectangle Q (the shaded one
in figure 2). At the beginning of the traversal, the algorithm
checks the category of a. In case, It is a B-vertex and its GeoB
value is true, the algorithm recursively traverses out-edge
neighbors of a and perform recursive checking. Therefore,
the algorithm retrieves vertices b, c, d and j. For vertex b,

(c)

Fig. 4: R-vertex Pruning Power
it is a G-vertex and its reachable grids are G2 and G19 . G19
cover the range of four grids in L0 . They are G9 , G10 , G13
and G14 . The spatial range is merely partially covered by
Q (Case III.C), hence it is possible for b to reach Q. We
cannot make an assured decision in this step so b is recorded
for future traversal. Another neighbor is c. ReachGrid(c) is
{12, 14} which means that G12 and G14 are reachable from
c. G14 lies in Q (Case III.A). In such case, since a ❀ c, we
can conclude that a ❀ R. The algorithm then halts the graph
traversal at this step and returns true as the query answer.
IV. SPA-G RAPH A NALYSIS
This section analyzes each SPA-Graph vertex type rom two
perspectives: (1) Storage Overhead: the amount of storage
overhead that each vertex type adds to the system (2) Pruning
Power: the probability that the query processing algorithm
terminates when a vertex of such type is visited during the
graph traversal.
B-vertex. When visiting a B-Vertex, in case GeoB is false,
the query processing algorithm prunes all subsequent graph
paths originated at such vertex. That is due to the fact that such
vertex cannot reach any spatial vertex in the graph. Otherwise,
the query processing algorithm continues traversing the graph.
As a result, pruned power of a B-vertex lies in the condition
that GeoB is false. For a given graph, number of vertices that
can reach any space is a certain value. So probability that a
vertex can reach any spatial vertex is denoted as Ptrue . This is
also the probability of a B-vertex whose GeoB value is true.
Probability of a B-vertex whose GeoB value is false, denoted
as Pf alse , will be 1 − Ptrue . To sum up, pruned power of a
B-vertex is 1 − Ptrue or Pf alse
R-vertex. When an R-vertex is visited, the condition
whether the vertex can reach any space still exists. If the
R-vertex cannot reach any space, we assign the R-vertex a
specific value to represent it(e.g. set coordinates of RMBR’s

bottom-left point bigger than that of the top-right point). In
this case, pruned power of a R-vertex will be the same with
a B-vertex, which is Pf alse . Otherwise, when the R-vertex
can reach some space, it will be more complex. Because
information of RMBR and query rectangle R have some
impact on the pruned power of this R-vertex. The algorithm
stops traversing the graph in both the No Overlap and Lie
In cases depicted in Figures 3a and 3b. Figure 4 shows the
two cases that R-vertex will stop the traversal. In Figure 4,
width and height of the total 2D space are denoted as A and
B. Assume that the query rectangle can be located anywhere
in the space with equal probability. We use (x1 , y1 ) and
(x2 , y2 ) to represent the RMBR’s top-left corner and lowerright point coordinates, respectively. Then all possible areas
where top-left vertex of query rectangle Q should be part
of the total space, denoted as I (see the shadowed area in
the figure. Its area is determined by size of query rectangle.
Denote width and height of Q are e and f , then area of I,
AI = (A − e) × (B − f ).
First, we estimate probability of No Overlap case. Figure 4a
shows one case of No Overlap. If the query rectangle Q do
not overlap with RMBR, top-left vertex of Q must lie outside
rectangle R which is forms the overlap region (drawn with
solid line in Figure 4b). Area of R (denoted as AR ) is obviously determined by the RMBR location and size of Q. It can
be easily observed that AR = (x2 −(x1 −e))×(y2 −(y1 −f )).
Another possible case is demonstrated in Figure 4b. In such
case, if we calculate R in the same way, range of R will
exceeds area of I which contains all possible locations. As a
result, AR = AI in this case. As we can see, area of overlap
region is determined by the range of R and I altogether.
Then we can have a general representation of the overlap area
AOverlap = (min(A − e, x2 ) − max(0, x1 − e)) × (min(B −
f, y2 )−max(0, x2 −f ). The No Overlap area is AI −AOverlap
and the probability of having a No Overlap case is calculated
as follows:
PN oOverlap =

AOverlap
AI − AOverlap
=1−
.
AI
AI

(5)

Figure 4c depicts the Lie In case. When top-left vertex of
Q lies in region R, then such Lie In case will happen. To
ensure that R exists, it is necessary that e > (x2 − x1 ) and
f > (y2 −y1 ). If it is not, then probability of such case must be
0. If this requirement is satisfied, then AR = (x1 − (x2 − e))×
(y1 − (y2 − f )). Recall what is met in the above-mentioned
case, R may exceed the area of I. Similarly, more general
area should be AR = (min(A − e, x1 ) − max(0, x1 − (x2 −
e))) × (min(B − f, y1 ) − max(0, y1 − (y2 − f ))). Probability
R
of such case should be A
AI . To sum up, we have
PLieIn =

(

AR
AI

0

e > (x2 − x1 ) and f > (y2 − y1 )
else

(6)

After we sum up all conditional probabilities based on
Ptrue and Pf alse , pruning power of an R-vertex is equal to

Algorithm 2 G EO R EACH Initialization Algorithm
1:
2:
3:
4:
5:
6:
7:
8:
9:

Function INITIALIZE(Graph G = {V, E})
/*PHASE I: SPA-Graph Vertex Initialization */
for each Vertex v ∈ V according their sequence in topology do
InitializeVertex(G, v, MAX_REACH_GRIDS, MAX_RMBR)
/* PHASE II: Reachable Grid Cells Merging */
for each G-vertex v do
for each layer Li from L1 to Lbottom do
for each grid cell Gi in Li do
if Number of reachable grids in corresponding region in Li−1 is larger
than MERGE_COUNT then
10:
Add Gi in Li into ReachGrid(v)
11:
Remove reachable grid cells that are covered by Gi in higher layers

(PN oOverlap +PLieIn )×Ptrue +Pf alse . Evidently, the pruning
power of an R-vertex is more powerful than a B-vertex. When
the storage overhead of an R-vertex is considered, coordinates
of RMBR’s top-left and lower-right vertices should be stored.
Thus its storage will be at least four bytes depending on the
spatial data precision. That means the storage overhead of a
G-Vertex is always higher than that of a B-Vertex.
G-vertex. For a high resolution grid, it is of no doubt that a
G-vertex possesses a high pruning power. However, this comes
at the cost of higher storage overhead because more grid cells
occupies more space. When a G-vertex is compared with an
R-vertex, the area of an R-vertex is much larger than a grid. In
this case, an R-vertex can be seen as a a simplified G-vertex
for which the grid cell size is equal to that of RMBR. One
extreme case of R-vertex is that the vertex can reach only one
spatial vertex. In such case, RMBR is location of the reachable
spatial vertex. Such R-vertex can still be counted as a G-vertex
whose grid size x → 0. According the rule, it should be with
higher storage overhead and more accuracy. Actually, storing it
as a G-vertex will cost an integer while any R-vertex requires
storage for four float or even double number.
V. I NITIALIZATION & M AINTENANCE
This section describes the SPA-Graph initialization algorithm. The G EO R EACH initialization algorithm (Pseudocode
is given in Algorithm 2) takes as input a graph Graph
G = {V, E} and runs in two main phases: (1) Phase I:
SPA-Graph Vertex Type Initialization: this phase leverages the
tradeoff between query response time and storage overhead
explained in Section IV to determine the type of each vertex.
(2) Phase II: Reachable Grid Cells Merging: This step further
reduces the storage overhead of each G-Vertex in the SPAGraph by merging a set of grid cells into a single grid cell.
Details of each phase are described in Section V-A and V-B
A. SPA-Graph Vertex Type Initialization
To determine the type of each vertex, the initialization
algorithm takes into account the following system parameters:
• MAX_RMBR: This parameter represents a threshold that
limits space area of each RMBR. If a vertex v is
an R-vertex, area of RMBR(v) cannot be larger than
MAX_RMBR. Otherwise, v will be degraded to a B-vertex.
• MAX_REACH_GRIDS: This parameter sets up the maximum number of grid cells in each ReachGrid. If a vertex
v is a G-vertex, number of grid cells in ReachGrid(v)

Algorithm 3 SPA-Graph Vertex Initialization Algorithm

Algorithm 4 Maintain R-vertex

1: Function INITIALIZE V ERTEX(Graph G = {V, E}, Vertex v)
2: Type ← InitializeType(v)
3: switch (Type)
4: case B-vertex:
5:
Set v B-vertex and GeoB(v) = true
6: case G-vertex:
7:
ReachGrid(v) ← ∅
8:
for each Vertex v ′ ∈ Vvout do
9:
Maintain-GVertex(v, v ′ )
10:
if Number of grids in ReachGrid(v) ¿ MAX_REACH_GRIDS then
11:
Set v R-vertex and break
12:
Type ← R-vertex
13:
if Number of grids in ReachGrid(v) = 0 then
14:
Set v B-vertex, GeoB(v) ← false and break
15: case R-vertex:
16:
RMBR(v) ← ∅
17:
for each Vertex v ′ ∈ Vvout do
18:
Maintain-RVertex(v, v ′ )
19:
if Area(RMBR(v)) ¿ MAX_RMBR then
20:
Set v B-vertex, GeoB(v) ← true and break
21: end switch

1: Function M AINTAIN -RV ERTEX(From-side vertex v, To-side vertex v′ )
2: switch (Type of v′ )
3: case B-vertex:
4:
if GeoB(v ′ ) = true then
5:
Set v ′ B-vertex and GeoB(v) ← true
6:
else if RMBR(v) fully contains MBR(v ′ .spatial) then
7:
return false
8:
else
9:
RMBR(v) ← MBR(RMBR(v), v ′ .spatial)
10: case R-vertex:
11:
if RMBR(v) fully contains MBR(RMBR(v ′ ), v ′ .spatial) then
12:
return false
13:
else
14:
RMBR(v) ← MBR(RMBR(v), RMBR(v ′ ), v ′ .spatial)
15: case G-vertex:
16:
if RMBR(v) fully contains MBR(RMBR(v ′ ), v ′ .spatial) then
17:
return false
18:
else
19:
RMBR(v) ← MBR(RMBR(v), RMBR(v ′ ), v ′ .spatial)
20: end switch
21: return true

cannot exceed MAX_REACH_GRIDS. Otherwise, v will
be degraded to an R-vertex.
Algorithm 3 gives the pseudocode of the vertex initialization
algorithm. Vertices are processed based on their topological
sequence in the graph. For each vertex, the algorithm first
determines the initial vertex type using the InitializeType
function (pseudocode omitted for brevity). For a vertex v,
categories of vertex v ′ ({v ′ | v ′ ∈ Vvout }) will be checked.
If there is any B-vertex v ′ with GeoB(v ′ ) = true, v is directly
initialized to a B-vertex with GeoB(v) = true. Otherwise, if
there is any R-vertex, the function will return an R-vertex type,
which means that v is initialized to R-vertex. If either of the
above happens, the function returns G-vertex type. Based on
the initial vertex type, the algorithm may encounter one of the
following three cases:
Case I (B-vertex): The algorithm directly sets v as a Bvertex and GeoB(v) = true because there must exist one outedge neighbor v ′ of v such that GeoB(v ′ ) = true.
Case III (R-vertex): For each v ′ (v ′ ∈ Vvout ), the algorithm calls the Maintain-RVertex algorithm. Algorithm 4
shows the pseudocode of the Maintain-RVertex algorithm.
Maintain-RVertex aggregates RMBR information. After each
aggregation step, area of RMBR(v) will be compared with
MAX_RMBR: In case the area of RMBR(v) is larger than
MAX_RMBR, the algorithm sets v to be a B-vertex with a true
GeoBvalue and terminates. When v ′ is either a G-vertex or
an R-vertex, the algorithm uses the new bounding rectangle
returned from MBR(RMBR(v), RMBR(v ′ ), v ′ .spatial) to
update the current RMBR(v). The algorithm calculates the
RMBR of a G-vertex in case III. In case v ′ is a B-vertex,
GeoB(v ′ ) must be reset to false. The algorithm then updates
RMBR(v) to MBR(RMBR(v), v.spatial).
Case II (G-vertex): For each vertex v ′ (v ′ ∈ Vvout ),
Maintain-GVertex (pseudocode omitted for the sake of space)
is invoked to calculate the ReachGrid of v ′ . In case v ′
is a B-vertex with GeoB(v ′ ) = false and v ′ is a spatial
vertex, the grid cell that contains the location of v ′ will be
added into ReachGrid(v). If v ′ is a G-vertex, all grid cells

in ReachGrid(v ′ ) and Grid(v ′ .spatial) will be added into
ReachGrid(v). It does not matter whether v ′ is a spatial vertex
or not. If v ′ is not a spatial vertex, Grid(v ′ .spatial) is ∅.
After accumulating information from each neighbor v ′ , the
algorithm changes the type of v to R-vertex immediately in
case the number of reachable grid cells in ReachGrid(v) is
larger than MAX_REACH_GRIDS. Therefore, the algorithm
sets the Type to R-vertex since RMBR(v) should be calculated
for possible future usage, e.g. RMBR of in-edge neighbors of
v(it will be shown in R-vertex case).
Example. Figure 2 depicts a SPA-Graph with MAX_RMBR
= 0.8A and MAX_REACH_GRIDS = 4, where A is area of the
whole space. Each vertex is attached with some information
and affiliated to one category of G EO R EACH index. Their
affiliations are listed in the figure. It is obvious that those
vertices which cannot reach any spatial vertices will be stored
as B-vertex and have a false boolean GeoB value to represent
such condition. Vertices d, f , h, i, j and k are assigned a
false value. Other vertices are G-vertex initially. ReachGrid(a)
= {2, 7, 9, 12, 14}, ReachGrid(b) = {2, 9, 14}, ReachGrid(c)
= {12, 14}, ReachGrid(e) = {14}, ReachGrid(g) = {12, 14},
ReachGrid(i) = {14}, ReachGrid(j) = {2, 7, 12, 14} and
ReachGrid(l) = {2}. Because of MAX_REACH_GRIDS, some
of them will be degraded to an R-vertex. Number of reachable grids in ReachGrid(a) and ReachGrid(j) are 4 and
5, respectively. Both of them are larger than or equal to
MERGE_COUNT. They will be degraded to R-vertex first. Then
area of their RMBR are compared with MAX_RMBR. Area
of RMBR(a) is apparently over 80% of the total space area.
According to MAX_RMBR, a is stored as a B-vertex with a true
value while j is stored as an R-vertex with an RMBR.
B. Reachable Grid Cells Merging
After the type of each vertex is decided, the initialization
algorithm performs the reachable grid cells merging phase
(lines 5 to 11 in Algorithm 2). In this phase, the algorithm
merges adjacent grid cells to reduce the overall storage overhead of each G-Vertex. To achieve that, the algorithm assumes
a system parameter, namely MERGE_COUNT. This parameter

Algorithm 5 Maintain B-vertex
1: Function M AINTAIN -BV ERTEX(From-side vertex v, To-side vertex v′ )
2: if GeoB(v) = true then
3:
return false
4: else
5:
switch (Type of v ′ )
6:
case B-vertex:
7:
if GeoB(v ′ ) = true then
8:
GeoB(v) ← true
9:
else if v ′ .spatial 6= NULL then
10:
ReachGrid(v) ← Grid(v ′ .spatial)
11:
else
12:
return false
13:
case R-vertex:
14:
RMBR(v) ← MBR(RMBR(v ′ ), v ′ .spatial)
15:
case G-vertex:
16:
ReachGrid(v) ← ReachGrid(v ′ )∪Grid(v ′ .spatial)
17:
end switch
18: return true

determines how G EO R EACH merges spatially adjacent grid
cells according to MERGE_COUNT. In each spatial region with
four grid cells, the number of reachable grid cells should not
be less than MERGE_COUNT. Otherwise, we merge the four
grid cells into a single grid cell in the lower layer.
For each G-vertex v, all grid cells in grid cell layers L1 to
Lbottom are checked. When a grid cell Gi in Li is processed,
four grid cells in Li−1 that cover the same space with Gi will
be accessed. If number of reachable grid cells is larger than or
equal to MERGE_COUNT, Gi should be added in ReachGrid(v)
first. Then all grid cells covered by Gi in layers from L0 to
Li−1 should be removed. In order to achieve that, a recursive
approach is implemented as follows. For each grid cell in Li−1
that is reachable from v, the algorithm directly remove it from
ReachGrid(v). The removal stops at this grid in this layer. No
recursive checking is required on grid cells in higher layers for
which the space is covered by the reachable grid cell. Since
all those reachable grid cells have been removed already. For
those grid cells that are not reachable from v, the algorithm
cannot assure that they do not cover some reachable grids in
a higher layer. Hence, the recursive removal is invoked until
the algorithm reaches the highest layer or other reachable grid
cells are visited.
The SPA-Graph in Figure 2 has a MERGE_COUNT set to 2.
There is no merging in e, i and l because their ReachGrids
contain only one grid. The rest are b, c and g. In ReachGrid(b),
for each grid in L1 , we make the MERGE_COUNT checking.
G17 covers four grids G1 , G2 , G5 and G6 in L0 . In such
four-grids region, only G2 is reachable from b. The merging
will not happen in G17 . It is the same case in G18 and G20 .
However, there are two grids, G9 and G14 covered by G19
in L1 . As a result, the two grids in L0 will be removed from
ReachGrid(b) with G19 being added instead. For the grid G21
in L2 , the same checking in L1 will be performed. Since, only
G19 is reachable, no merging happens. Finally, ReachGrid(b)
= {2, 19}. Similarly, we can have ReachGrid(c) = {12, 14}
and ReachGrid(g) = {12, 14} where no merging occurs.
C. SPA-Graph Maintenance
When the structure of a graph is updated, i.e., adding or
deleting edges and/or vertices, G EO R EACHneeds to maintain

the SPA-Graph structure accordingly. Moreover, when the
spatial attribute of a vertex changes, G EO R EACHmay need to
maintain the RMBRand/or ReachGridproperties of that vertex
and other connected vertices as well. As a matter of fact, all
graph updates can be simulated as a combination of adding
and/or deleting a set of edges.
Adding an edge. When an edge is added to the graph,
the directly-influenced vertices are those that are connected
to another vertex by the newly added edge. The spatial
reachability information of the to-side vertex will not be
influenced by the new edge. Based upon Lemmas III-A
and 3.2, the spatial reachability information, i.e., RMBRor
ReachGrid, of the to-side vertex should be modified based
on the the from-side vertex. On the other hand, the fromside vertex may remain the same or change. In the former
case, there is no recursive updates required for the in-edge
neighbors of the from-side vertex. Otherwise, the recursive
updates are performed in the reverse direction until no change
occurs or there is no more in-edge neighbor. A queue Q will
be exploited to track the updated vertices. When Q is not
empty, which means there are still some in-edge neighbors
waiting for updates, the algorithm retrieves the next vertex
in the queue. For such vertex, all its in-edge neighbors are
updated by using the reachability information stored on this
vertex. Updated neighbors will then be pushed into the queue.
The algorithm halts when the queue is empty. Depending on
category of the from-side vertex, corresponding maintenance
functions, including Maintain-BVertex, Maintain-RVertex and
Maintain-GVertex are used to update the newly added spatial
reachability information.
Algorithm 5 is used when the from-side vertex is a B-vertex.
In algorithm 5, if the from-side vertex v is already a B-vertex
with GeoB(v) = true. The added edge will never cause any
change on v. Hence a false value is returned. In case GeoB(v)
= false, the algorithm considers type of the to-side vertex v ′ .
•

•

•

B-vertex. If GeoB(v ′ ) = true, it is no doubt that GeoB(v)
will be set to true and a true value will be returned.
Otherwise, the algorithm checks whether v ′ is spatial.
If it is, ReachGrid(v) is updated with Grid(v ′ spatial).
Otherwise, the algorithm returns false because v is not
changed.
R-vertex. In such case, it is certain that v will be updated
to an R-vertex. The algorithm merely updates RMBR(v)
with MBR(RMBR(v ′ ), v ′ .spatial).
G-vertex. It is similar to the R-vertex case. Type of v ′ can
decide that v should be a G-vertex and the algorithm updates ReachGrid(v) with ReachGrid(v ′)∪Grid(v ′ .spatial)

Maintain-BVertex and Maintain-RVertex are what we use
in the initialization. However, there is a new condition that
should be taken into consideration. When the from-side vertex
v is an R-vertex and the to-side vertex v ′ is a G-vertex, the
algorithm needs to update the RMBR(v) with ReachGrid(v ′ ).
Under such circumstance, first a dummy RMBR(v ′ ) will be
constructed using ReachGrid(v ′ ). Although it is not the exact
RMBR of v ′ , it is still precise. Error of the width and height

will not be greater than size of a grid cell. No matter what
function is invoked to update the from-side vertex, G EO R E ACH takes into account the system parameters MAX_RMBR and
MAX_REACH_GRIDS are checked on RMBR and ReachGrid,
respectively.
Deleting an edge. When an edge is removed, the to-side
vertex will be not impacted by the deleting which is the same
with adding an edge. To maintain the correctness of spatial
reachability information stored on the from-side vertex, the
only way is to reinitialize its spatial reachability information
according to all its current out-edge neighbors. If its structure
is different from the original state due to the deleting, the
structure of all its in-edge neighbors will be rebuilt recursively.
A queue Q is used to keep track of the changed vertices. The
way G EO R EACHmaintains the queue and the operations on
each vertex in the queue are similar to the AddEdge procedure.
Maintenance cost of deleting an edge will be O(kn3 ) because
the whole G EO R EACH index may be reinitialized.
VI. E XPERIMENTAL E VALUATION
In this section, we present a comprehensive experimental evaluation of G EO R EACH performance. We compare the
following approaches: GeoMT0, GeoMT2, GeoMT3, GeoP,
GeoRMBR and SpaReach. GeoMT0, GeoMT2 and GeoMT3 are approaches that store only ReachGrid by setting MAX_REACH_GRIDS to the total number of grids in
the space and MAX_RMBR to A where A represent the
area of the whole 2D space. Their difference lies in the
value of MERGE_COUNT. GeoMT0 is an approach where
MERGE_COUNT is 0. In such approach, no higher layer
grids are merged. MERGE_COUNT is set to 2 and 3 respectively in GeoMT2 and GeoMT3. GeoP is an approach in
which MERGE_COUNT = 0, MAX_REACH_GRIDS = 200 and
MAX_RMBR = A. In such approach, reachable grids in ReachGrid will not be merged. If the number of reachable grids
of ReachGrid(v) is larger than 200 then v will be degraded
to an R-vertex. Since MAX_RMBR = A, there will be no Bvertex. In GeoRMBR, MAX_REACH_GRIDS = 0, MAX_RMBR
= A, hence only RMBR s are stored. In all ReachGrid related
approaches, the total space is split into 128 × 128 pieces in
the highest grid layer. SpaReach approach is implemented with
both spatial index and reachability index. Graph structure is
stored in Neo4j graph database. Reachability index is stored as
attributes of each graph vertex in Neo4j database. Reachability
index we use is proposed in [33]. Spatial index used SpaReach
approaches is implemented by gist index in postgresql. To
integrate Neo4j and postgresql databases, for each vertex in
the graph, we assign it an id to uniquely identify it.
Experimental Environment. The source code for evaluating query response time is implemented in Java and compiled
with java-7-openjdk-amd64. Source codes of index construction are implemented in c++ and complied using g++ 4.8.4.
Gist index is constructed automatically by using command
line in Postgresql shell. All evaluation experiments are run
on a computer with an 3.60GHz CPU, 32GB RAM running
Ubuntu 14.04 Linux OS.

TABLE II: Graph Datasets (K = 103 )
Dataset
citeseerx
go-uniprot
patent
uniprot22m
uniprot100m
uniprot150m

|V |
6540K
6968K
3775K
1595K
16087K
25038K

|E|
15011K
34770K
16519K
1595K
16087K
25038K

davg
2.30
4.99
4.38
1.00
1.00
1.00

l
59
21
32
4
9
10

Datasets. We evaluate the performance of our methods
using six real datasets [9], [33] (see Table II). Number of
vertices and edges are listed in column |V | and |E|. Column
davg and l are average degree of vertices and length of the
longest path in the graph, respectively. Citeseerx and patent
are real life citation graphs extracted from CiteSeerx2 and US
patents3 [33]. Go-uniprot is a graph generated from Gene Ontology and annotation files from Uniprot4 [33]. Uniprot22m,
uniprot100m and uniprot150m are RDF graphs from UniProt
database [33]. The aforementioned datasets represent graphs
that possess no spatial attributes. For each graph, we simulate
spatial data by assigning a spatial location to a subset of
the graph vertices. During the experiments, we change the
ratio of spatial vertices to the total number of vertices from
20% to 80%. During the experiments, we vary the spatial
distribution to be: uniform, zipf, and clustered distributions.
Unless mentioned otherwise, the number of spatial clusters is
set to 4 by default.
A. Query Response Time
In this section, we fist compare the query response time
performance of SpaReach to our GeoP approach. Afterwards,
we change tunable parameters in G EO R EACH to evaluate
influence of these thresholds. For each dataset, we change the
spatial selectivity of the input query rectangle from 0.0001 to
0.1. For each query spatial selectivity, we randomly generate
500 queries by randomly selecting 500 random vertices and
500 random spatial locations of the query rectangle. The
reported query response time is calculated as the average time
taken to answer the 500 queries.
Figure 5 depicts the query response time of GeoP and
SpaReach on four datasets. 80% of vertices in the graph
are spatial and they are randomly-distributed in space. For
brevity, we omit the results of the other two datasets, i.e.,
uniprot22m and uniprot100m, since they have almost the same
graph structure and exihibit the same performance. As it turns
out In Figure 5, GeoP outperforms SpaReach for any query
spatial selectivity in uniprot150m, go-uniprot and citeseerx.
For these datasets, SpaReach approach cost more time when
query selectivity increases. When we increasing the query
range size, the range query step tends to return a larger number
of spatial vertices. Hence, the graph reachability checking
step has to check more spatial vertices. Figure 5c and 5d
show similar experiment results. In conclusion, GeoP is much
2 http://citeseer.ist.psu.edu/
3 http://snap.stanford.edu/data/
4 http://www.uniprot.org/

query time (sec)

GeoP

SpaReach

query time (sec)

GeoP

SpaReach

query time (sec)

GeoP

SpaReach

query time (sec)

10000

100

10

100

100

10

1

10

1

1

0.1

1

0.01
0.0001

0.001

0.01

0.1

0.1
0.0001

0.001

0.01

0.1

0.01
0.0001

0.001

0.01

0.1

0.1
0.0001

0.001

GeoP

0.01

Query range

Query range

Query range

Query range

(a) uniprot150m

(b) patent

(c) go-uniprot

(d) citeseerx

SpaReach

0.1

Fig. 5: Query response time (80% spatial vertex ratio, randomly-distributed spatial data, and spatial selectivity ranging from 0.0001 to 0.1)
more query-efficient in relatively sparse graphs. Patent dataset
is the densest graph with richer reachability information.
Figure 5b indicates that even when spatial selectivity set to
0.0001, GeoP can achieve almost the same performance as
SpaReach. When spatial selectivity increases, GeoP outperforms SpaReach again. In a denser graph, the performance
difference between the two approaches is smaller than in
sparse graphs especially when the spatial selectivity is low.
Table III compares the query response time of all our approaches for the uniprot150m, patent go-uniprot and citeseerx
datasets with randomly distributed spatial vertices and spatial
ratio of 80%. In uniprot150m, all our approaches almost have
the same performance. The same pattern happens with the
uniprot22m, uniprot100m and go-uniprot datasets. So we use
uniprot150m as a representative.
For the patent graph with random-distributed spatial vertices
and spatial ratio of 20%, query efficiency difference can be
easily caught. GeoMT0 keeps information of exact reachable
grids of every vertex which brings us fast query speed, but
also the highest storage overhead. RMBR stores general spatial
boundary of reachable vertices which is the most scalable.
However, such approach spend the most time in answering the
query. Since GeoMT3 is an approach that MERGE_COUNT is
set to 3, just few grids in GeoMT3 are merged. As a result,
its query time is merely little bit longer than GeoMT0. There
are more grids getting merged in GeoMT2 than in GeoMT3.
Inaccuracy caused by more integration lowers efficiency of
GeoMT3 in query. GeoP is combination of ReachGrid and
RMBR. Its query efficiency is lower than GeoMT0 and better
than GeoRMBR. In this case, GeoMT2 outperforms GeoP. But
it is not always the case. By tuning MAX_REACH_GRIDS to
a larger number, GeoP can be more efficient in query.
In citeseerx, GeoMT0 keeps the best performance as expected. Performance of GeoP is in between GeoMT0 and
GeoRMBR as what is shown in patent. But GeoMT2 and
GeoMT3 reveal almost the same efficiency and they are worse
than GeoRMBR. Distinct polarized graph structure accounts
for the abnormal appearance. In citeseerx, all vertices can be
divided into two groups. One group consists of vertices that
cannot reach any vertex. The other group contains a what
we call center vertex. The center vertex has huge number of
out-edge neighbor vertices and is connected by huge number
of vertices as well. Because the center vertex can reach that
many vertices, it can reach nearly all grid cells in space. As

a result, vertices that can reach the center vertex can also
reach all grid cells in space. So no matter what value is
MAX_REACH_GRIDS, reachable grids in ReachGrid of these
vertices will be merged into only one grid in a lower layer
until to the bottom layer which is the whole space. Then such
ReachGrid can merely function as a GeoB which owns poorer
locality than RMBR.
B. Storage Overhead
Figure 6a gives the storage overhead of all approaches
for the uniprot150m dataset. In this experiment, the spatial
vertices are randomly distributed in space. Since uniprot22m
and uniprot100m share the same pattern with uniprot150m
(even spatial distribution of vertices varies), they are not shown
in the figure. The experiments show that G EO R EACH and
all its variants require less storage overhead than SpaReach
because of the additional overhead introduced by the spatial index. When there are less spatial vertices, SpaReach
obviously occupies less space because size of spatial index
lessens. However, SpaReach always requires more storage
than any other approaches. Storage overhead of G EO R EACH
approaches shows a two-stages pattern which means it is either
very high (ratio = 0.8, 0.6 and 0.4) or very low (ratio = 0.2).
The reason is as follows. These graphs are sparse and almost
all vertices reach the same vertex. This vertex cannot reach any
other vertex. Let us call it an end vertex. If the end vertex is
a spatial vertex, then all vertices that can reach the end vertex
will keep their spatial reachability information (no matter what
category they are) in storage. But if it is not, majority of
vertices will store nothing for spatial reachability information.
GeoMT0 and GeoP are of almost the same index size because
of sparsity and end-point phenomenon in these graphs. Such
characteristic causes that almost each vertex can just reach
only one grid which makes MAX_REACH_GRIDS invalid in
approach GeoMT0 (number of reachable grids is always less
than MAX_REACH_GRIDS) which makes GeoMT0 and GeoP
have nearly the same size. For similar reason, MERGE_COUNT
becomes invalid in these datasets which makes GeoMT2 and
GeoMT3 share the same index size with GeoMT0 and GeoP.
We also find out that index size of GeoRMBR is slightly larger
than GeoMT0 approaches. Intuitively, RMBR should be more
scalable than ReachGrid. But most of the vertices in these
three graphs can reach only one grid. In GeoRMBR, for each
vertex that have reachable spatial vertices, we assign an RMBR

TABLE III: Query Response Time in three datasets, 80% spatial vertex ratio, and spatial selectivity ranging from 0.0001 to 0.1
Selectivity
0.0001
0.001
0.01
0.1
Index size

MT0
68
65
66
69

GeoMT0
GeoP

(MB)
2400

uniprot150m
MT3 GeoP
67
66
78
66
65
65
65
75

MT2
68
77
66
65

GeoMT2
GeoRMBR

GeoMT3
SpaReach

Index size

RMBR
66
65
65
66
GeoMT0
GeoP

(MB)
6000

MT0
643
168
87
51

GeoMT2
GeoRMBR

patent
MT3
741
185
98
59

MT2
762
258
143
108

GeoMT3
SpaReach

Index size

GeoP
1570
559
217
155

GeoMT0
GeoP

(MB)
750

RMBR
2991
1965
915
348
GeoMT2
GeoRMBR

MT0
202
34
32
33

GeoMT3
SpaReach

Index size

4000

500

800

800

2000

250

400

0
0.8

0.6

0.4

0.2

0
0.8

0.6

0.4

0.2

GeoMT0
GeoP

(MB)
1200

1600

0

citeseerx
MT3 GeoP
203
210
471
207
410
189
399
160

MT2
212
460
408
399

GeoMT2
GeoRMBR

RMBR
234
215
200
183
GeoMT3
SpaReach

0
0.8

0.6

0.4

0.2

0.8

0.6

0.4

ratio

ratio

ratio

ratio

(a) uniprot150m

(b) patent

(c) go-uniprot

(d) citeseerx

0.2

Fig. 6: Storage Overhead (Randomly distributed, spatial vertex ratio from 0.8 to 0.2)
index size(MB)

Random

Clustered

Zipf

2400
1800

index size(MB)

Random

Clustered

Zipf

index size(MB)

Random

Clustered

Zipf

index size(MB)

1800

7 

6

1200



400

600

2 

200

Random

Clustered

Zipf

1200
600
0

0

0
GeoMT2

GeoP

GeoRMBR SpaReach

(a) uniprot150m

GeoMT2

GeoP

GeoRMBR SpaReach

(b) patent

0
GeoMT2

GeoP

GeoRMBR SpaReach

(c) go-uniprot

GeoMT2

GeoP

GeoRMBR SpaReach

(d) citeseerx

Fig. 7: Storage Overhead for varying spatial data distribution (randomly, cluster and zipf distributed) and 0.8 spatial vertex ratio)
which will be stored as coordinates of RMBR’s top-left and
lower-right points. It is more scalable to store one grid id than
two coordinates. So when a graph is highly sparse, index size
of GeoMT0 is possible to be less than GeoRMBR.
Figure 6c shows that in go-uniprot all G EO R EACH approaches performs better than SpaReach. When we compare
all the G EO R EACH approaches, GeoMT0, GeoMT2 and GeoMT3 lead to almost the same storage overhead. That happens
due to the fact that go-uniprot is a very sparse graph. A
vertex can only reach few grids in the whole space. Grid
cells in ReachGrid can hardly be spatially adjacent to each
other which causes no integration. The graph sparsity makes
the number of reachable grids in ReachGrid always less than
MAX_REACH_GRIDS which leads to less R-vertices and more
G-vertices. In consequence, go-uniprot, GeoMT0, GeoMT2,
GeoMT3 and GeoP lead to the same storage overhead. It
is rational that GeoRMBR requires the least storage because
RMBR occupies less storage than ReachGrid.
When graphs are denser, results become more complex.
Figure 6b shows index size of different approaches in patent
dataset with randomly-distributed spatial vertices. GeoRMBR
and GeoP, take the first and the second least storage and
are far less than other approaches because both of them
use RMBR which is more scalable. GeoMT0 takes the most
storage in all spatial ratios for that ReachGrid takes high
storage overhead. GeoMT2 and GeoMT3 require less storage
than GeoMT0 because spatially-adjacent reachable grids in

GeoMT0 are merged which brings us scalability. GeoMT3
are more scalable than GeoMT2 because MERGE_COUNT in
GeoMT2 is 2 which causes more integration. There are three
approaches, GeoMT3, GeoP and GeoRMBR, that outperform
SpaReach approach. By tuning parameters in G EO R EACH, we
are able achieve different performance in storage overhead and
can also outperform SpaReach.
Figure 6d depicts index size of all approaches in citeseerx
with randomly distributed spatial vertices. Spatial vertices ratio
ranges from 0.8 to 0.2. All G EO R EACH approaches outperform
SpaReach except for one outlier when spatial vertices ratio
is 0.2. GeoMT0 consumes huge storage. This is caused by
the center vertex which is above-mentioned. Recall that large
proportion of ReachGrid contains almost all grids in space.
After bitmap compression, it will cause low storage overhead.
This is why when spatial vertices ratio is 0.8, 0.6 and 0.4,
GeoMT0 consumes small size of index. When the ratio is 0.2,
there are less spatial vertices. Although graph structure does
not change, the center vertex reach less spatial vertices and
less grids. Then the bitmap compression brings no advantage
in storage overhead.
Figure 7 shows the impact of spatial data distribution on
the storage cost. GeoMT0, GeoMT2 and GeoMT3 are all
ReachGrid-based approaches. Spatial data distribution of vertices influences all approaches the same way. For all datasets,
SpaReach is not influenced by the spatial data distribution.
SpaReach consists of two sections: (1) The reachability index

size is determined by graph structure and (2) The spatial
index size is directly determined by number of spatial vertices. Hence, SpaReach exhibits the same storage overhead
for different spatial data distributions. When spatial vertices
distribution varies, GeoRMBR also keeps stable storage overhead. This is due to the fact that the storage overhead for
each RMBR is a constant and the number of stored RMBR
s is determined by the graph structure and spatial vertices
ratio, and not by the spatial vertices distribution. Spatial data
distribution can only influence the shape of each RMBR.
Figure 7a shows that each approach in G EO R EACH keeps
the same storage overhead under different distributions in
uniprot150m. As mentioned before, GeoMT0, GeoMT2, GeoMT3 and GeoP actually represent the same data structure
since there is only a single reachable grid in ReachGrid. When
there is only one grid reachable, varying the spatial distribution
becomes invalid for all approaches which use ReachGrid.
Figure 7b and 7c shows that the storage overhead introduced by ReachGrid-based approaches decreases when spatial
vertices become more congested. Randomly distributed spatial
data is the least congested while zipf distributed is the most.
The number of reachable spatial vertices from each vertex do
not change but these reachable spatial vertices become more
concentrated in space. This leads to less reachable grids in
ReachGrid.
Figure 7d shows that when spatial vertices are more congested, ReachGrid based approaches, i.e., GeoMT0, GeoMT2
and GeoMT3, tend to be less scalable. Recall that citeseerx
dataset is a polarized graph with a center vertex. One group
contains vertices that can reach huge number of vertices (about
200,000) due to the center vertex. When spatial vertices are
more concentrated and that will lead to more storage overhead.
C. Initialization time
In this section, we evaluate the index initialization time
for all considered approaches. For brevity, we only show the
performance results for four datasets, uniprot150m, patent,
go-uniprot and citeseerx, since uniprot22m, uniprot100m and
uniprot150m datasets exhibit the same performance. Figure 8a
shows that SpaReach requires much more construction time
than the other approaches under all spatial ratios. Although
these graphs are sparse, they contain large number of vertices. This characteristic causes huge overhead in constructing
a spatial index which dominates the initialization time in
SpaReach. Hence, SpaReach takes much more time than all
other approaches. However, the SpaReach initialization time
decreases when decreasing the number spatial vertices since
the spatial index building step deals with less spatial vertices
in such case. However, SpaReach remains the worst even when
the spatial vertex ratio is set to 20%.
Figures 8b and 8d gives the initialization time for both the
patent and citeseerx datasets, respectively. GeoRMBR takes
significantly less initialization time compared to all other
approaches. GeoP takes less time than the rest of approaches
because it is ReachGrid of partial vertices whose number of
reachable grids are less than MAX_REACH_GRIDS that are

calculated. In most cases, GeoMT0 can achieve almost equal
or better performance compared to SpaReach while GeoMT2
and GeoMT3 requires more time due to the integration of
adjacent reachable grids. To sum up, GeoRMBR and GeoP
perform much better than SpaReach in initialization even
in very dense graphs. GeoMT0 can keep almost the same
performance with SpaReach approach.
Figure 8c shows the initialization time for all six approaches on the go-uniprot dataset. Both RMBR approaches,
i.e., GeoRMBR and GeoP, still outperform SpaReach. This
is due to the fact that a spatial index constitutes a high
proportion of SpaReach initialization time. As opposed to the
uniprot150m case, the smaller performance gap between initializing GeoRMBR and SpaReach in go-uniprot.is explained
as follows. The size of go-uniprotis far less than uniprot150m
which decreases the spatial index initialization cost. As a
result, the index construction time in SpaReach is less than
that in uniprot150m. Since this graph has more reachability
information, all G EO R EACH approaches require more time
than in uniprot150m. It is conjunction of G EO R EACH and
SpaReach index size changes that causes the smaller gap.
VII. C ONCLUSION
This paper describes G EO R EACH a novel approach that
evaluates graph reachability queries and spatial range predicates side-by-side. G EO R EACH extends the functionality of a
given graph database management system with light-weight
spatial indexing entries to efficiently prune the graph traversal
based on spatial constraints. G EO R EACH allows users to
tune the system performance to achieve both efficiency and
scalability. Based on extensive experiments, we show that
G EO R EACH can be scalable and query-efficient than existing spatial and reachability indexing approaches in relatively
sparse graphs. Even in rather dense graphs, our approach
can outperform existing approaches in storage overhead and
initialization time and still achieves faster query response
time. In the future, we plan to study we plan to study
the extensibility of G EO R EACH to support different spatial
predicates. Furthermore, we aim to extend the framework
to support a distributed system environment. Last but not
least, we also plan to study the applicability of G EO R EACH
to various application domains including: Spatial Influence
Maximization, Location and Social-Aware Recommendation,
and Location-Aware Citation Network Analysis.
R EFERENCES
[1] R. Agrawal, A. Borgida, and H. V. Jagadish. Efficient Management of
Transitive Relationships in Large Data and Knowledge Bases. ACM,
1989.
[2] R. Bayer and E. M. McCreight. Organization and Maintenance of Large
Ordered Indices. Acta Informatica, 1(3):173–89, 1972.
[3] N. Beckmann, H.-P. Kriegel, R. Schneider, and B. Seeger. The R*-Tree:
An Efficient and Robust Access Method for Points and Rectangles. In
SIGMOD, pages 322–331, May 1990.
[4] J. L. Bentley. Multidimensional Binary Search Trees Used for Associative Searching. Communications of the ACM, CACM, 18(9):509–517,
1975.
[5] J. Cai and C. K. Poon. Path-hop: efficiently indexing large graphs for
reachability queries. In CIKM, pages 119–128. ACM, 2010.

Ix 	me
(sec)
480

GeoMT0
GeoP

GeoMT2
GeoRMBR

GeoMT
SpaReach

Index me

GeoMT0
GeoP

GeoMT2
GeoRMBR

GeoMT3
SpaReach

Index me

GeoMT0
GeoP

GeoMT2
GeoRMBR

GeoMT3 x me
(sec)
SpaReach
00

5


(sec)
150

320

00

100

200

1

15


50

100

0
0.8

0.

0.4

0.2

(sec)

0

08

0
0.6

0.4

0.2

08

0
0.6

0.4

0.2

08

GeoMT0
GeoP

GeoMT2
GeoRMBR

0.

0

ratio

ratio

ratio

ratio

(a) uniprot150

(b) patent

(c) go-uniprot

(d) citeseerx

GeoMT
SpaReach

02

Fig. 8: Initialization time (Randomly distributed, spatial vertex ratio from 0.8 to 0.2)

[6] L. Chen, A. Gupta, and M. E. Kurul. Stack-based algorithms for pattern
matching on dags. In VLDB, pages 493–504. VLDB Endowment, 2005.
[7] Y. Chen and Y. Chen. An efficient algorithm for answering graph
reachability queries. In ICDE, pages 893–902. IEEE, 2008.
[8] J. Cheng, S. Huang, H. Wu, and A. W.-C. Fu. Tf-label: a topologicalfolding labeling scheme for reachability querying in a large graph. In
SIGMOD, pages 193–204. ACM, 2013.
[9] J. Cheng, S. Huang, H. Wu, and A. W.-C. Fu. Tf-label: a topologicalfolding labeling scheme for reachability querying in a large graph. In
SIGMOD, pages 193–204. ACM, 2013.
[10] J. Cheng, Z. Shang, H. Cheng, H. Wang, and J. X. Yu. K-reach: who
is in your small world. PVLDB, 5(11):1292–1303, 2012.
[11] J. Cheng, J. X. Yu, X. Lin, H. Wang, and P. S. Yu. Fast computing
reachability labelings for large graphs with high compression rate. In
EDBT, pages 193–204. ACM, 2008.
[12] E. Cohen, E. Halperin, H. Kaplan, and U. Zwick. Reachability and
distance queries via 2-hop labels. SIAM Journal on Computing,
32(5):1338–1355, 2003.
[13] D. Comer. The Ubiquitous B-Tree. ACM Computing Surveys, 11(2):121–
137, 1979.
[14] R. A. Finkel and J. L. Bentley. Quad trees: A Data Structure for Retrieval
of Composite Keys. Acta Informatica, 4(1):1–9, 1974.
[15] J. E. Gonzalez, Y. Low, H. Gu, D. Bickson, and C. Guestrin. GraphChi:
Large-Scale Graph Computation on Just a PC. In OSDI, 2012.
[16] A. Guttman. R-Trees: A Dynamic Index Structure For Spatial Searching.
In SIGMOD, 1984.
[17] H. Jagadish. A compression technique to materialize transitive closure.
TODS, 15(4):558–598, 1990.
[18] R. Jin, Y. Xiang, N. Ruan, and H. Wang. Efficiently answering
reachability queries on very large directed graphs. In SIGMOD, pages
595–608. ACM, 2008.
[19] J. Liagouris, N. Mamoulis, P. Bouros, and M. Terrovitis. An effective
encoding scheme for spatial RDF data. PVLDB, 7(12):1271–1282, 2014.
[20] D. B. Lomet. Grow and Post Index Trees: Roles, Techniques and Future
Potential. In SSD, pages 183–206, Aug. 1991.
[21] P. Rigaux, M. Scholl, and A. Voisard. Spatial Databases with Application to GIS. Morgan Kaufmann, 2002.
[22] H. Samet. The Design and Analysis of Spatial Data Structures. AddisonWesley, 1990.
[23] H. Samet. Foundations of Multidimensional and Metric Data Structures.
Morgan Kaufmann, 2006.
[24] M. Sarwat, S. Elnikety, Y. He, and G. Kliot. Horton: Online Query
Execution Engine for Large Distributed Graphs. In ICDE, 2012.
[25] M. Sarwat, S. Elnikety, Y. He, and M. F. Mokbel. Horton+: A Distributed
System for Processing Declarative Reachability Queries over Partitioned
Graphs. PVLDB, 6(14):1918–1929, 2013.
[26] R. Schenkel, A. Theobald, and G. Weikum. Hopi: An efficient connection index for complex xml document collections. In EDBT, pages
237–255. Springer, 2004.
[27] S. Seufert, A. Anand, S. Bedathur, and G. Weikum. Ferrari: Flexible
and efficient reachability range assignment for graph indexing. In ICDE,
pages 1009–1020. IEEE, 2013.
[28] B. Shao, H. Wang, and Y. Li. Trinity: A Distributed Graph Engine on
a Memory Cloud. In SIGMOD, 2013.
[29] S. Shekhar and S. Chawla. Spatial Databases: A Tour. Prentice Hall,
2003.
[30] S. Trißl and U. Leser. Fast and practical indexing and querying of very
large graphs. In SIGMOD, pages 845–856. ACM, 2007.

[31] S. J. van Schaik and O. de Moor. A memory efficient reachability data
structure through bit vector compression. In SIGMOD, pages 913–924.
ACM, 2011.
[32] H. Wang, H. He, J. Yang, P. S. Yu, and J. X. Yu. Dual labeling:
Answering graph reachability queries in constant time. In ICDE, pages
75–75. IEEE, 2006.
[33] Y. Yano, T. Akiba, Y. Iwata, and Y. Yoshida. Fast and scalable
reachability queries on graphs by pruned labeling with landmarks and
paths. In CIKM, pages 1601–1606. ACM, 2013.
[34] H. Yildirim, V. Chaoji, and M. J. Zaki. Grail: Scalable reachability index
for large graphs. PVLDB, 3(1-2):276–284, 2010.
[35] A. D. Zhu, W. Lin, S. Wang, and X. Xiao. Reachability queries on large
dynamic graphs: a total order approach. In SIGMOD, pages 1323–1334.
ACM, 2014.

arXiv:1408.0325v1 [cs.SI] 2 Aug 2014

Matrix Factorization with Explicit Trust and Distrust
Relationships
Rana Forsati

Mehrdad Mahdavi

Shahid Beheshti University, G.C., Tehran, Iran

Michigan State University, Michigan, USA

r_forsati@sbu.ac.ir

mahdavim@cse.msu.edu

Mehrnoush Shamsfard

Mohamed Sarwat

Shahid Beheshti University, G.C., Tehran, Iran

University of Minnesota, Minneapolis, USA

m_shams@sbu.ac.ir

sarwat@cs.umn.edu

Abstract
With the advent of online social networks, recommender systems have became crucial for the
success of many online applications/services due to their significance role in tailoring these applications to user-specific needs or preferences. Despite their increasing popularity, in general recommender systems suffer from the data sparsity and the cold-start problems. To alleviate these issues,
in recent years there has been an upsurge of interest in exploiting social information such as trust
relations among users along with the rating data to improve the performance of recommender systems. The main motivation for exploiting trust information in recommendation process stems from
the observation that the ideas we are exposed to and the choices we make are significantly influenced by our social context. However, in large user communities, in addition to trust relations, the
distrust relations also exist between users. For instance, in Epinions the concepts of personal "web
of trust" and personal "block list" allow users to categorize their friends based on the quality of reviews into trusted and distrusted friends, respectively. Hence, it will be interesting to incorporate
this new source of information in recommendation as well. In contrast to the incorporation of trust
information in recommendation which is thriving, the potential of explicitly incorporating distrust
relations is almost unexplored. In this paper, we propose a matrix factorization based model for
recommendation in social rating networks that properly incorporates both trust and distrust relationships aiming to improve the quality of recommendations and mitigate the data sparsity and the
cold-start users issues. Through experiments on the Epinions data set, we show that our new algorithm outperforms its standard trust-enhanced or distrust-enhanced counterparts with respect to
accuracy, thereby demonstrating the positive effect that incorporation of explicit distrust information
can have on recommender systems.

1 Introduction
The huge amount of information available on the Web has made it increasingly challenging to cope
with this information overload and find the most relevant information one is really interested in. Recommender systems intend to provide users with recommendations of products they might appreciate,
taking into account their past ratings, purchase history, or interest. The recent proliferation of online
social networks have further enhanced the need for such systems. Therefore, it is obvious why such systems are indispensable for the success of many online applications such as Amazon, iTunes and Netflix
to guide the search process and help users to effectively find the information or products they are looking for [49]. Roughly speaking, the overarching goal of recommender systems is to identify a subset of
items (e.g. products, movies, books, music, news, and web pages) that are likely to be more interesting
to users based on their interests [13, 76, 16, 5].
In general, most widely used recommender systems (RS) can be broadly classified into contentbased (CB), collaborative filtering (CF), or hybrid methods [1]. In CB recommendation, one tries to
recommend items similar to those a given user preferred in the past. These methods usually rely on
the external information such as explicit item descriptions, user profiles, and/or the appropriate features extracted from items to analyze item similarity or user preference to provide recommendation.

1

In contrast, CF recommendation, the most popular method adopted by contemporary recommender
systems, is based on the core assumption that similar users on similar items express similar interest,
and it usually relies on the rating information to build a model out of the rating information in the past
without having access to external information required in CB methods. The hybrid approaches were
proposed that combine both CB and CF based recommenders to gain advantages and avoid certain
limitations of each type of systems [20, 64, 55, 48, 54, 67, 15].
The essence of CF lies in analyzing the neighborhood information of past users and items’ interactions in the user-item rating matrix to generate personalized recommendations based on the preferences of other users with similar behavior. CF has been shown to be an effective approach to recommender systems. The advantage of these types of recommender systems over content-based RS is that
the CF based methods do not require an explicit representation of the items in terms of features, but it
is based only on the judgments/ratings of the users. These CF algorithms are mainly divided into two
main categories [21]: memory-based methods (also known as neighborhood-based methods) [73, 9]
and model-based methods [26, 63, 65, 79]. Recently, another direction in CF considers how to combine memory-based and model-based approaches to take advantage of both types of methods, thereby
building a more accurate hybrid recommender system [56, 77, 32].
The heart of memory-based CF methods is the measurement of similarity based on ratings of items
given by users: either the similarity of users (user-oriented CF) [24], the similarity of items (itemsoriented CF) [61], or combined user-oriented and item-oriented collaborative filtering approaches to
overcome the limitations specific to either of them [74]. The user-oriented CF computes the similarity
among users, usually based on user profiles or past behavior, and seeks consistency in the predictions
among similar users [78, 26]. The item-oriented CF, on the other hand, allows input of additional itemwise information and is also capable of capturing the interactions among them. If the rating of an item
by a user is unavailable, collaborative-filtering methods estimate it by computing a weighted average of
known ratings of the items from the most similar users.
Memory-based collaborative filtering is most effective when users have expressed enough ratings
to have common ratings with other users, but it performs poorly for so-called cold-start users. Coldstart users are new users who have expressed only a few ratings. Thus, for memory based CF methods
to be effective, large amount of user-rating data are required. Unfortunately, due to the sparsity of the
user-item rating matrix, memory-based methods may fail to correctly identify the most similar users
or items, which in turn decreases the recommender accuracy. Another major issue that memory-based
methods suffer from is the scalability problem. The reason is essentially the fact that when the number of users and items are very large, which is common in many real world applications, the search to
identify k most similar neighbors of the active user is computationally burdensome. In summary, data
sparsity and non-scalability issues are two main issues current memory based methods suffer from.
To overcome the limitations of memory-based methods, model-based approaches have been proposed, which establish a model using the observed ratings that can interpret the given data and predict
the unknown ratings [1]. In contrast to the memory-based algorithms, model-based algorithms try
to model the users based on their past ratings and use these models to predict the ratings on unseen
items. In model-based CF the goal is to employ statistical and machine learning techniques to learn
models from the data and make recommendations based on the learned model. Methods in this category include aspect model [26, 63], clustering methods [30], Bayesian model [80], and low dimensional
linear factor models such as matrix factorization (MF) [66, 65, 79, 59]. Due to its efficiency in handling very huge data sets, matrix factorization based methods have become one of the most popular
models among the model-based methods, e.g. weighted low rank matrix factorization [65], weighted
nonnegative matrix factorization (WNMF) [79], maximum margin matrix factorization (MMMF) [66]
and probabilistic matrix factorization (PMF) [59]. These methods assume that user preferences can be
modeled by only a small number of latent factors [12] and all focus on fitting the user-item rating matrix
using low-rank approximations only based on the observed ratings. The recommender system we will
propose in this paper adhere to the model-based factorization paradigm.
Although latent factor models and in particular matrix factorization are able to generate high quality
recommendations, these techniques also suffer from the data sparsity problem in real-world scenarios
and fail to address users who rated only a few items. For instance, according to [61], the density of
non-missing ratings in most commercial recommender systems is less than one or even much less.
Therefore, it is unsatisfactory to rely predictions on such small amount of data which becomes more
challenging in the presence of large number of users or items. This observation necessitates tackling the
2

data sparsity problem in an affirmative manner to be able to generate more accurate recommendations.
One of the most prominent approaches to tackle the data sparsity problem is to compensate for the
lack of information in rating matrix with other sources of side information which are available to the
recommender system. For example, social media applications allow users to connect with each other
and to interact with items of interest such as songs, videos, pages, news, and groups. In such networks
the ideas we are exposed to and the choices we make are significantly influenced by our social context.
More specifically, users generally tend to connect with other users due to some commonalities they
share, often reflected in similar interests. Moreover, in many real-life applications it may be the case
that only social information about certain users is available while interaction data between the items
and those users has not yet been observed. Therefore, the social data accumulated in social networks
would be a rich source of information for the recommender system to utilize as side information to alleviate the data sparsity problem. To accomplish this goal, in recent years the trust-based recommender
systems became an emerging field to provide users personalized item recommendations based on the
historical ratings given by users and the trust relationships among users (e.g., social friends).
Social-enhanced recommendation systems are becoming of greater significance and practicality
with the increased availability of online reviews, ratings, friendship links, and follower relationships.
Moreover, many e-commerce and consumer review websites provide both reviews of products and a
social network structure among the reviewers. As an example, the e-commerce site Epinions [22] asks
its users to indicate which reviews/users they trust and use these trust information to rank the reviews
of products. Similar patterns can be found in online communities such as Slashdot in which millions of users post news and comment daily and are capable of tagging other users as friends/foes or
fans/freaks. Another example is the ski mountaineering site Moleskiing [3] which enables users to
share their opinions about the snow conditions of the different ski routes and also express how much
they trust the other users. Another well-known example is the FilmTrsut system [19], an online social
network that provides movie rating and review features to its users. The social networking component
of the website requires users to provide a trust rating for each person they add as a friend. Also users
on Wikipedia can vote for or against the nomination of others to adminship [7]. These websites have
come to play an important role in guiding users’ opinions on products and in many cases also influence their decisions in buying or not buying the product or service. The results of experiments in [11]
and of similar works confirm that a social network can be exploited to improve the quality of recommendations. From this point of view, traditional recommender systems that ignore the social structure
between users may no longer be suitable.
A fundamental assumption in social based recommender systems which has been adopted by almost all of the relevant literature is that if two users have friendship relation, then the recommendation
from his or her friends probably has higher trustworthiness than strangers. Therefore the goal becomes
how to combine the user-item rating matrix with the social/trust network of a user to boost the accuracy of recommendation system and alleviate the sparsity problem. Over the years, several studies have
addressed the issue of the transfer of trust among users in online social networks. These studies exploit
the fact that trust can be passed from one member to another in a social network, creating trust chains,
based on its propagative and transitive nature 1 . Therefore, some recommendation methods fusing social relations by regularization [29, 36, 42, 81] or factorization [41, 43, 59, 58, 65, 60, 57] were proposed
that exploit the trust relations in the social network.
Also, the results of incorporating the trust information in recommender systems is appealing and
has been the focus of many researchers in the last few years, but, in large user communities, besides the
trust relationship between users, the distrust relationships are also unavoidable. For example, Epinions
provided the feature that enables users to categorize other users in a personal web of trust list based
on their quality as a reviewer. Later on, this feature integrated with the concept of personal block list,
which reflects the members that are distrusted by a particular user. In other words, if a user encounters
a member whose reviews are consistently offensive, inaccurate, or otherwise low quality, she can add
that member to her block list. Therefore, it would be tempting to investigate whether or not distrust
information could be effectively utilized to boost the accuracy of recommender systems as well.
In contrast to trust information for which there has been a great research, the potential advantage/disadvantage of explicitly utilizing distrust information is almost unexplored. Recently, few
1 We note that while the concept of trust has been studied in many disciplines including sociology, psychology, economics, and
computer science from different perspectives, but the issue of propagation and transitivity have often been debated in literature
and different authors have reached different conclusions (see for example [62] for a thorough discussion)

3

attempts have been made to explicitly incorporate the distrust relations in recommendation process [22, 40, 69, 72], which demonstrated that the recommender systems can benefit from the proper
incorporation of distrust relations in social networks. However, despite these positive results, there
are some unique challenges involved in distrust-enhanced recommender systems. In particular, it has
proven challenging to model distrust propagation in a manner which is both logically consistent and
psychologically plausible. Furthermore, the naive modeling of distrust as negative trust raises a number of challenges- both algorithmic and philosophical. Finally, it is an open challenge how to incorporate trust and distrust relations in model-based methods simultaneously. This paper is concerned with
these questions and gives an affirmative solution to challenges involved with distrust-enhanced recommendation. In particular, the proposed method makes it possible to simultaneously incorporate both
trust and distrust relationships in recommender systems to increase the prediction accuracy. To the
best of our knowledge, this is the first work that models distrust relations into the matrix factorization
problem along with trust relations at the same time.
The main intuition behind the proposed algorithm is that one can interpret the distrust relations
between users as the dissimilarity in their preferences. In particular, when a user u distrusts another
user v, it indicates that user u disagrees with most of the opinions issued, or ratings made by user v.
Therefore, the latent features of user u obtained by matrix factorization must be as dissimilar as possible to v’s latent features. In other words, this intuition suggests to directly incorporate the distrust into
recommendation by considering distrust as reversing the deviation of latent features. However, when
combined with the trust relations between users, due to the contradictory role of trust and distrust relations in propagating social information in the matrix factorization process, this idea fails to effectively
capture both relations simultaneously. This statement also follows from the preliminary experimental
results in [69] for memory-based CF methods that demonstrated regarding distrust as an indication to
reverse deviations in not the right way to incorporate distrust.
To remedy this problem, we settle to a less ambitious goal and propose another method to facilitate
the learning from both types of relations. In particular, we try to learn latent features in a manner that
the latent features of users who are distrusted by the user u have a guaranteed minimum dissimilarity
gap from the worst dissimilarity of users who are trusted by user u. By this formulation, we ensure that
when user u agrees on an item with one of his trusted friends, he/she will disagree on the same item
with his distrusted friends with a minimum predefined margin. We note that this idea significantly
departs from the existing works in distrust-enhanced memory based recommender systems [69, 72],
that employ the distrust relations to either filter out or debug the trust relations to reduce the prediction
task to a trust-enhanced recommendation. In particular, the proposed method ranks the latent features
of trusted and distrusted friends of each user to reflect the effect of relation in factorization.
Summary of Contributions This work makes the following key contributions:
• A matrix factorization based algorithm for simultaneous incorporation of trust and distrust relationships in recommender systems. To the best of our knowledge, this is the first model-based
recommender algorithm that is able to leverage both types of relationships in recommendation.
• An efficient stochastic optimization algorithm to solve the optimization problem which makes
the proposed method scalable to large social networks.
• An empirical investigation of the consistency of the social relationships with rating information.
In particular, we examine to what extent trust and distrust relations between users are aligned
with the ratings they issued on items.
• An exhaustive set of experiments on Epinions data set to empirically evaluate the performance of
the proposed algorithm and demonstrate its merits and advantages.
• A detailed comparison of the proposed algorithm to the state-of-the-art trust/distrust enhanced
memory/model based recommender systems.
Outline The rest of this paper is organized as follows. In Section 2 we draw connections to and put our
work in context of some of the most recent work on social recommender systems. Section 3 formally

4

introduces the matrix factorization problem, an optimization based framework to solve it, and its extension to incorporate the trust relations between users. The proposed algorithm along with optimization
methods are discussed in Section 4. Section 5 includes our experimental result on Epinions data set
which demonstrates the merits of the proposed algorithm in alleviating data sparsity problem in rating
matrix and generating more accurate recommendations. Finally, Section 6 concludes the paper and
discusses few directions as future work.

2 Related Work on Social Recommendation
Earlier in the introduction, we discussed some of the main lines of research on recommender system;
here, we survey further lines of study that are most directly related to our work on social-enhanced
recommendation. Many successful algorithms have been developed over the past few years to incorporate social information in recommender systems. After reviewing trust-enhanced memory-based
approaches, we discuss some model-based approaches for recommendation in social networks with
trust relations. Finally, we review major approaches in distrust modeling and distrust-enhanced recommender systems.

2.1 Trust Enhanced Memory-based Recommendation
Social network data has been widely investigated in the memory-based approaches. These methods
typically explore the social network and find a neighborhood of users trusted (directly or indirectly) by a
user and perform the recommendation by aggregating their ratings. These methods use the transitivity
of trust and propagate trust to indirect neighbors in the social network [45, 47, 31, 27, 29, 28, 33].
In [45], a trust-aware collaborative filtering method for recommender systems is proposed. In this
work, the collaborative filtering process is informed by the reputation of users, which is computed by
propagating trust. [31] proposed a method based on the random walk algorithm to utilize social connection and other social annotations to improve recommendation accuracy. However, this method
does not utilize the rating information and is not applicable to constructing a random walk graph in
real data sets. TidalTrust [18] performs a modiÞed breadth first search in the trust network to compute
a prediction. To compute the trust value between user u and v who are not directly connected, TidalTrust aggregates the trust value between u’s direct neighbors and v weighted by the direct trust values
of u and its direct neighbors.
MoleTrust [45, 46, 80] does the same idea as TidalTrust, but MoleTrust considers all the raters up to a
fixed maximum-depth given as an input, independent of any specific user and item. The trust metric in
MoleTrust consists of two major steps. First, cycles in trust networks are removed. Therefore, removing
trust cycles beforehand from trust networks can significantly speed up the proposed algorithm because
every user only needs to be visited once to infer trust values. Second, trust values are calculated based
on the obtained directed acyclic graph by performing a simple graph random walk:
TrustWalker [27] combines trust-based and item-based recommendation to consider enough ratings without suffering from noisy data. Their experiments show that TrustWalker outperforms other
existing memory based approaches. Each random walk on the user trust graph returns a predicted
rating for user u on target item i . The probability of stopping is directly proportional to the similarity
between the target item and the most similar item j , weighted by the sigmoid function of step size k.
The more the similarity, the greater the probability of stopping and using the rating on item j as the
predicted rating for item i . As the step size increases, the probability of stopping decreases. Thus ratings by closer friends on similar items are considered more reliable than ratings on the target item by
friends further away.
We note that all these methods are neighborhood-based methods which employ only heuristic algorithms to generate recommendations. There are several problems with this approach. The relationship
between the trust network and the user-item matrix has not been studied systematically. Moreover,
these methods are not scalable to very large data sets since they may need to calculate the pairwise
user similarities and pairwise user trust scores.

5

2.2 Trust Enhanced Model-based Recommendation
Recently, researchers exploited matrix factorization techniques to learn latent features for users and
items from the observed ratings and fusing social relations among users with rating data as will be
detailed in Section 3. These methods can be divided into two types: regularization-based methods
and factorization-based methods. Here we review some existing matrix factorization algorithms that
incorporate trust information in the factorization process.
2.2.1 Regularization based Social Recommendation
Regularization based methods typically add regularization term to the loss function and minimize it.
Most recently, Ma [42] proposed an idea based on social regularized matrix factorization to make recommendation based on social network information. In this approach, the social regularization term is
added to the loss function, which measures the difference between the latent feature vector of a user
and those of his friends. The probability model similar to the model in [42] is proposed by Jamali [29].
The graph Laplacian regularization term of social relations is added into the loss function in [36] and
minimizes the loss function by alternative projection algorithm. Zhu et a l. [81] used the same model
in [36] and built graph Laplacian of social relations using three kinds of kernel functions. In [37], the
minimization problem is formulated as a low-rank semidefinite optimization problem.
2.2.2 Factorization based Social Recommendation
In factorization-based methods, social relationship between users are represented as social relation
matrix, which is factored as well as the rating matrix. The loss function is the weighted sum of the social
relation matrix factorization error and the rating matrix factorization error. For instance, SoRec [41]
incorporates the social network graph into probabilistic matrix factorization model by simultaneously
factorizing the user-item rating matrix and the social trust networks by sharing a common latent lowdimensional user feature matrix [37]. The experimental analysis shows that this method generates better recommendations than the non-social filtering algorithms [28]. However, the disadvantage of this
work is that although the usersÕ social network is integrated into the recommender systems by factorizing the social trust graph, the real world recommendation processes are not reflected in the model.
Two sets of different feature vectors are assumed for users which makes the interpretability of the model
very hard [28, 39]. This drawback not only causes lack of interpretability in the model, but also affects
the recommendation qualities. A better model named Social Trust Ensemble (STE) [39] is proposed
by the same authors, by making the latent features of a user’s direct neighbors affect the rating of the
user. Their method is a linear combination of basic matrix factorization approach and a social network
based approach. Experiments show that their model outperforms the basic matrix factorization based
approach and existing trust based approaches. However, in their model, the feature vectors of direct
neighbors of u affect the ratings of u instead of affecting the feature vector of u. This model does not
handle trust propagation. Another method for recommendation in social networks has been proposed
in [40]. This method is not a generative model and defines a loss function to be minimized. The main
disadvantage of this method is that it punishes the users with lots of social relations more than other
users. Finally, SocialMF [28] is a matrix factorization based model which incorporates social influence
by making the features of every user depend on the features of his/her direct neighbors in the social
network.

2.3 Distrust Enhanced Social Recommendation
In contrast to incorporation of trust relations, unfortunately most of the literature on social recommendation totally ignore the potential of distrust information in boosting the accuracy of recommendations. In particular, only recently few work started to investigate the rule of distrust information in
recommendation process both from theoretical and empirical viewpoints [22, 84, 51, 82, 40, 75, 69, 71,
68, 72]. Although these studies have shown that distrust information can be plentiful, but there is a
significant gap in clear understanding of distrust in recommender systems. The most important reasons for this shortage are the lack of data sets that contain distrust information and dearth of a unified
consensus on modeling and propagation of distrust.

6

Table 1: Summary of notations consistently used in the paper and their meaning.
Symbol
Meaning
U = {u 1 , · · · , u n }, n
I = {i 1 , · · · , i m }, m
k
R ∈ Rn×m
ΩR , |ΩR |
U ∈ Rn×k
V ∈ Rm×k
S ∈ {−1, +1}n×n
ΩS , |ΩS |
W ∈ Rn×n
+
N (u) ⊆ [n]
N+ (u) ⊆ [n]
N− (u) ⊆ [n]
D : Rk × Rk → R+

The set of users in system and the number of users
The set of items and the number of items
The dimension of latent features in factorization
The partially observed rating matrix
The set of observed entires in rating matrix and its size
The matrix of latent features for users
The matrix of latent features for items
The social network between n users
The set of extracted triplets from the social relations and its size
The pairwise similarity matrix between users
Neighbors of user u in the social graph
The set of trusted neighbors by user u in the social graph
The set of distrusted neighbors by user u in the social graph
The measurement function used to assess the similarly of latent features

A formal framework of trust propagation schemes, introducing the formal and computational treatment of distrust propagation has been developed in [22]. In an extension of this work, [82] proposed
clever adaptations in order to handle distrust and sinks such as trust decay and normalization. In [75], a
trust/distrust propagation algorithm called CloseLook is proposed, which is capable of using the same
kinds of trust propagation as the algorithm proposed by [22]. [34] extended the results by [22] using a
machine-learning framework (instead of the propagation algorithms based on an adjacency matrix)
to enable the evaluation of the most informative structural features for the prediction task of positive/negative links in online social networks. A comprehensive framework that computes trust/distrust
estimations for user pairs in the network using trust metrics is build in [71]: given two users in the trust
network, we can search for a path between them and propagate the trust scores along this path to obtain an estimation. When more than one path is available, we may single out the most relevant ones
(selection), and aggregation operators can then be used to combine the propagated trust scores into
one final trust score, according to different trust score propagation operators.
[40] was the first seminal work to demonstrate that the incorporation of distrust information could
be beneficial based on a model-based recommender system. In [71] and [72] the same question is
addressed in memory-based approaches. In particular, [72] embarked upon the distrust-enhanced recommendation and showed that with careful incorporation of distrust metric, distrust-enhanced recommender systems are able to outperform their trust-only counterparts. The main rational behind the
algorithm proposed in [72] is to employ the distrust information to debug or filter out the users’ propagated web of trust. It is also has been realized that the debugging methods must exhibit a moderate
behavior in order to be effective. [68] addressed the problem of considering the length of the paths that
connect two users for computing trust-distrust between them, according to the concept of trust decay.
This work also introduced several aggregation strategies for trust scores with variable path lengths
Finally we note that the aforementioned works try to either model or utilize the trust/distrust information. In recent years there has been an upsurge of interest in predicting the trust and distrust
relations in a social network [34, 14, 4, 53]. For instance, [34] casts the problem as a sign prediction
problem (i.e., +1 for friendship and -1 for opposition) and utilizes machine learning methods to predict
the sign of links in the social network. In [14] a new method is presented for computing both trust and
distrust by combining an inference algorithm that relies on a probabilistic interpretation of trust based
on random graphs with a modified spring-embedding algorithm to classify an edge. Another direction
of research is to examine the consistency of social relations with theories in social psychology [8, 35].
Our work significantly departs from these works on prediction or consistency analysis of social relations, and aims to effectively incorporate the distrust information in matrix factorization for effective
recommendation.

7

3 Matrix Factorization based Recommender Systems
This section provides a formal definition of collaborative filtering, the primary recommendation
method we are concerned with in this paper, followed by solution methods for low-rank factorization
that are proposed in the literature to address the problem.

3.1 Matrix Factorization for Recommendation
In collaborative filtering we assume that there is a set of n users U = {u 1 , · · · , u n } and a set of m items
I = {i 1 , · · · , i m } where each user u i expresses opinions about a set of items. In this paper, we assume
opinions are expressed through an explicit numeric rating (e.g., scale from one to five), but other rating
methods such as hyperlink clicks are possible as well. We are mainly interested in recommending a
set of items for an active user such that the user has not rated these items before. To this end, we are
aimed at learning a model from the existing ratings, i.e., offline phase, and then use the learned model to
generate recommendations for active users, i.e., online phase. The rating information is summarized in
an n × m matrix R ∈ Rn×m , 1 ≤ i ≤ n, 1 ≤ j ≤ m where the rows correspond to the users and the columns
correspond to the items and (p, q)th entry is the rate given by user u p to the item i q . We note that the
rating matrix is partially observed and it is sparse in most cases.
An efficient and effective approach to recommender systems is to factorize the user-item rating
matrix R by a multiplicative of k-rank matrices R ≈ UV> , where U ∈ Rn×k and V ∈ Rm×k utilize the
factorized user-specific and item-specific matrices, respectively, to make further missing data prediction. The main intuition behind a low-dimensional factor model is that there is only a small number of
factors influencing the preferences, and that a user’s preference vector is determined by how each factor applies to that user. This low rank assumption makes it possible to effectively recover the missing
entires in the rating matrix from the observed entries. We note that the celebrated Singular Value Decomposition (SVD) method to factorize the rating matrix R is not applicable here due to the fact that the
rating matrix is partially available and we are only allowed to utilize the observed entries in factorization process. There are two basic formulations to solve this problem: these are optimization based (see
e.g., [57, 37, 41, 33]) and probabilistic [50]. In the following subsections, we first review the optimization
based framework for matrix factorization and then discuss how it can be extended to incorporate trust
information.

3.2 Optimization based Matrix Factorization
Let ΩR be the set of observed ratings in the user-item matrix R ∈ Rn×m , i.e.,
ΩR = {(i , j ) ∈ [n] × [m] : R i j has been observed},
where n is the number of users and m is the number of items to be rated. In optimization based matrix
factorization, the goal is to learn the latent matrices U and V by solving the following optimization
problem:
#
"
´2 λ
λV
1 X ³
U
>
R i j −Ui ,: V j ,: +
kUkF +
kVkF ,
(1)
min L (U, V) =
U,V
2 (i , j )∈ΩR
2
2
where k · kF is the Frobenius norm of a matrix, i.e, kAkF =

qP P
n
m
i =1

j =1 |A i j |

2.

The optimization prob-

lem in (1) constitutes of three terms: the first term aims to minimize the inconsistency between the
observed entries and their corresponding value obtained by the factorized matrices. The last two terms
regularize the latent matrices for users and items, respectively. The parameters λU and λV are regularization parameters that are introduced to control the regularization of latent matrices U and V,
respectively. We would like to emphasize that the problem in (1) is non-convex jointly in both U and V.
However, despite its non-convexity, the formulation in (1) is widely used in practical collaborative filtering applications as the performance is competitive or better as compared to trace-norm minimization,
while scalability is much better. For example, as indicated in [33], to address the Netflix problem, (1)
has been applied with a fair amount of success to factorize data sets with 100 million ratings.

8

3.3 Matrix Factorization with Trust Side Information
Recently it has been shown that just relying on the rating matrix to build a recommender system is
not as accurate as expected. The main reason for this claim is the known cold-start users problem and
the sparsity of rating matrix. Cold-start users are one of the most important challenges in recommender
systems. Since cold-start users are more dependent on the social network compared to users with more
ratings, the effect of using trust propagation gets more important for cold-start users. Moreover, in
many real life systems a very large portion of users do not express any ratings, and they only participate
in the social network. Hence, using only the observed ratings does not allow to learn the user features.
One of the most prominent approaches to tackle the data sparsity problem in matrix factorization
is to compensate the lack of information in rating matrix with other sources of side information which
are available to the recommender system. It has been recently shown that social information such as
trust relationship between users is a rich source of side information to compensate for the sparsity. The
above mentioned traditional recommendation techniques are all based on working on the user-item
rating matrix, and ignore the abundant relationships among users. Trust-based recommendation usually involves constructing a trust network where nodes are users and edges represent the trust placed on
them. The goal of a trust-based recommendation system is to generate personalized recommendations
by aggregating the opinions of other users in the trust network. The intuition is that users tend to adopt
items recommended by trusted friends rather than strangers, and that trust is positively and strongly
correlated with user preferences. Recommendation techniques that analyze trust networks were found
to provide very accurate and highly personalized results.
To incorporate the social relations in the optimization problem formulated in (1), few papers [40, 29,
42, 37, 81] proposed the social regularization method which aims at keeping the latent vector of each
user similar to his/her neighbors in the social network. The proposed models force the user feature
vectors to be close to those of their neighbors to be able to learn the latent user features for users with
no or very few ratings [29]. More specifically, the optimization problem becomes as:
L (U, V) =

´2 λ
λV
1 X ³
U
kUkF +
kVkF
R i j −Ui>,: V j ,: +
2 (i , j )∈ΩR
2
2
°
°
°
n °
X
λS X
1
°
°
U j ,: °,
+
°Ui ,: −
°
2 i =1 °
|N (i )| j ∈N (i )

(2)

where λS is the social regularization parameter and N (i ) is the subset of users who has relationship
with i th user in the social graph.
The rationale behind social regularization idea is that every user’s taste is relatively similar to the
average taste of his friends in the social network. We note that using this idea, latent features of users
indirectly connected in the social network will be dependent and hence the trust gets propagated. A
more reasonable and realistic model should treat all friends differently based on how similar they are.
Let assume the weight of relationship between two users i and j is captured by Wi j where W ∈ Rn×n
demotes the social weight matrix. It is easy to extend the model in (2) to treat friends differently based
on the weight matrix W as:
L (U, V) =

´2 λ
1 X ³
λV
U
R i j −Ui>,: V j ,: +
kUkF +
kVkF
2 (i , j )∈ΩR
2
2
°
°
P
°
n °
λS X
j ∈N (i ) Wi j U j ,: °
°
+
°
°Ui ,: − P
°
2 i =1 °
j ∈N (i ) Wi j

(3)

An alternative formulation is to regularize each users’ fiends individually, resulting in the following objective function [42]:
L (U, V) =

´2 λ
1 X ³
λV
U
R i j −Ui>,: V j ,: +
kUkF +
kVkF
2 (i , j )∈ΩR
2
2
+

n
°
°2
λS X
Wi j °Ui ,: −U j ,: ° .
2 i , j =1

where we simply assumed that for any j ∉ N (i ), Wi j = 0.
9

As mentioned earlier, the objective function in L (U, V) is not jointly convex in both U and V but it
is convex in each of them fixing the other one. Therefore, to find a local solution one can stick to the
standard gradient descent method to find a solution in an iterative manner as follows:
Ut +1 ← Ut − η t ∇U L (U, V)|U=Ut ,V=Vt ,
Vt +1 ← Vt − η t ∇V L (U, V)|U=Ut ,V=Vt .

4 Matrix Factorization with Trust and Distrust Side Information
In this section we describe the proposed algorithm for social recommendation which is able to incorporate both trust and distrust relationships in the social network along with the partially observed rating
matrix. We then present two strategies to solve the derived optimization problem, one based on the
gradient descent optimization algorithm which generates more accurate solutions but it is computationally cumbersome, and another based on the stochastic gradient descent method which is computationally more efficient for large rating and social matrices but suffers from slow convergence rate.

4.1 Algorithm Description
As discussed before, the vast majority of related work in the field of matrix factorization for recommendation has primarily focussed on trust propagation and simply ignore the distrust information between
users, or intrinsically, are not capable of exploiting it. Now, we aim at developing a matrix factorization
based model for recommendation in social rating networks to utilize both trust and distrust relationships. We incorporate the trust/distrust relationship between users in our model to improve the quality
of recommendations. While intuition and experimental evidence indicate that trust is somewhat transitive, distrust is certainly not transitive. Thus, when we intend to propagate distrust through a network,
questions about transitivity and how to deal with conflicting information abound.
To inject social influence in our model, the basic idea is to find appropriate latent features for users
such that each user is brought closer to the users she/he trusts and separated apart from the users
that she/he distrusts and have different interests. We note that simply incorporating this idea in matrix
factorization by naively penalizing the similarity of each user’s latent features to his distrusted friends’
latent features fails to reach the desired goal. The main reason is that distrust is not as transitive as trust,
i.e. distrust can not directly replace trust in trust propagation approaches and utilizing distrust requires
careful consideration (trust is transitive, i.e., if user u trusts user v and v trusts w, there is a good chance
that u will trust w, but distrust is certainly not transitive, i.e., if u distrusts v and v distrusts w, then w
may be closer to u than v or maybe even farther away). It is noticeable that this statement is consistent
with the preliminary experimental results in [69] for memory-based CF methods that indicate regarding
distrust as an indication to reverse deviations in not the right way to incorporate distrust. Therefore we
pursue another approach to model the distrust in recommendation process.
The main intuition behind the proposed framework stems from the observation that the trust relations between users can be treated as agreement on items and distrust relations can be considered as
disagreement on items. Then, the question becomes how can we guarantee when a user agrees on an
item with one of his/her friends, he/she will disagree on the same item with his/her distrusted friends
with a reasonable margin. We note that this margin should be large enough to make it possible to distinguish between two types of friends. In terms of latent features, this observation translates to having
a margin between the similarity and dissimilarity of users’ latent features to his/her trusted and distrusted friends.
Alternatively, one can view the proposed method from the viewpoint of connectivity of latent features in a properly designated graph. Intuitively, certain features or groups of features should influence
how users connect in the social network, and thus it should be possible to learn a mapping from features to connectivity in the social network such that the mapping respects the underlying structure of
the social network. In the basic matrix factorization algorithm for recommendation, we can consider
the latent features as isolated vertices of a graph where there is no connection between nodes. This can
be generalized to the social-enhanced setting by considering the social graph as the underlying graph
between latent features with two types of edges (i.e., trust and distrust relations correspond to positive

10

(a) User trust netwrok

(b) User distrust netwrok

(c) Partially observed rating matrix

(d) Illustration of learned latent features

Figure 1: A simple example with seven users {u 1 , u 2 , · · · , u 7 } and six items {i 1 , i 2 , · · · , i 6 } to illustrate the
main intuition behind the proposed algorithm. The inputs of the algorithm are (a) trust network, (b)
distrust network, and (c) partially observed rating matrix R, respectively. As shown in (d) for user u 1 ,
the learned latent features for all his trusted friends {u 2 , u 4 , u 6 , u 7 } are closer to u 1 ’s latent features than
his distrusted friends {u 3 , u 5 } with a margin of 1.
and negative edges, respectively). Now the problem reduces to learning the latent features for each user
u such that users trusted by u in the social network (with positive edges) are close and users which are
distrusted by u (with negative edges) are more distant. Learning latent features in this manner respects
the inherent topology of the social network.
Figure 1 shows an example to illustrate the intuition behind the mentioned idea. For ease of exposition, we only consider the latent features for the user u 1 . From the trust network in Figure 1 (a) we
can see that user u 1 trusts the list of users N+ = {u 2 , u 4 , u 6 , u 7 } and from the distrust network in Figure 1
(b) we see that user u 1 distrusts the list of users N− = {u 3 , u 5 }. The goal is to learn the latent features
that obeys two goals, i) it minimizes the prediction error on observed entries in the rating matrix, ii) it
respects the underlying structure of the trust and distrust networks between users. In Figure 1 (d) the
latent features are depicted in the Euclidean space from the viewpoint of user u 1 . As shown in Figure 1
(d), for user u 1 , the latent features of his/her trusted friends N+ lie inside the solid circle centered at u 1
and the latent features of his/her distrusted friends N− lie outside the dashed circle. The gap between
two circles guarantees that always there exists a safe margin between u 1 ’s agreements with his trusted
and distrusted friends. One simple way to impose these constraints on the latent features of users is to
generate a set of triplets for any combination of trusted and distrusted friends ( e.g., one such triplet
for user u 1 can be constructed as (u 1 , u 2 , u 5 )) and force the margin constraint to hold for all extracted
triplets. This ensures that the minimum margin gap will definitely exist between the latent features
of all the trusted and distrusted friends as desired and makes it possible to incorporate both types of

11

relationships between users in the matrix factorization.
It is worthy to mention that similar to the social-enhanced recommender systems discussed before, the proposed algorithm is also based on hypotheses about the existence and the correlation of
trust/distrust relations and ratings in the data. The empirical investigation of correlation between social
relations and rating information has been the focus of a bulk of recent research including [83, 53, 38],
where the results reinforce the hypothesis that ratings from trusted people count more than those from
others and in particular distrusted neighbors. We have also conducted experiments as will be detailed
in Subsection 5.5, to empirically investigate the correlation/alignment between social relations and the
rating information issued by users which supports our strategy in exploiting the trust/distrust relations
in matrix factorization.
We now formalize the proposed solution. As the first ingredient, we need a measure to evaluate the
consistency between the latent features of users, i.e., the matrix U, and the trust and distrust constraints
existing between users in the social network. To this end, we introduce a monotonically increasing
convex loss function `(z) to measure the discrepancy between the latent features of different users. Let
u i , u j , and u k be three users in the model such that u i trusts u j but distrusts u k . The main intuition
behind the proposed framework is that the latent features of u i , i.e., Ui ,: must be more similar to u j ’s
latent features than latent features for user u k . For each such a triplet we penalize the objective function
by `(D(Ui ,: ,U j ,: ) − D(Ui ,: ,Uk,: )) where the function D : Rk × Rk 7→ R+ measures the similarity between
two latent vectors assigned to two different users, and ` : R 7→ R+ is a penalty function that is utilized
to assess the violation of latent vectors of trusted and distrusted users. Example loss functions include
hinge loss `(z) = max(0, 1−z) and logistic loss `(z) = log(1+e −z ) which are widely used convex surrogate
of 0-1 loss function in learning community.
Let ΩS denote the set of extracted triplets from the social relations, i.e.,
©
ª
ΩS = (i , j , k) ∈ [n] × [n] × [n] : S i j = 1 & S i k = −1 .
Here, a positive relationship means friends or a trusted relationship and a negative relationship means
foes or a distrust relationship. Then, our goal becomes to find a factorization of matrix R such that
the learned latent features of users are consistent with the constraints in ΩS where the consistency is
reflected in the loss function. This results in the following optimization problem:
L (U, V) =

´2 λ
λV
1 X ³
U
kUkF +
kVkF
R i j −Ui>,: V j ,: +
2 (i , j )∈ΩR
2
2
+

X
λS
`(D(Ui ,: ,U j ,: ) − D(Ui ,: ,Uk,: )).
|ΩS | (i , j ,k)∈ΩS

(4)

Let us make the above general formulation more specific by setting `(·) and D(·, ·) to be the hinge loss
and the Euclidian distance, respectively. Under these two assumptions, the objective can be formulated
as:
L (U, V) =

´2 λ
1 X ³
λV
U
R i j −Ui>,: V j ,: +
kUkF +
kVkF
2 (i , j )∈ΩR
2
2
{z
}
|
R(U,V)

X
¢
¡
λS
+
max 0, 1 − kUi ,: −U j ,: k2 + kUi ,: −Uk,: k2 .
|ΩS | (i , j ,k)∈ΩS

(5)

Here the constraints have been written in terms of hinge-losses over triplets, each consisting of a user,
his/her trusted friend and his/her distrusted friend. Solving the optimization problem in (5) outputs
the latent features for users and items that can utilized to estimate the missing values in the user-item
matrix. Comparing the formulation in (5) to the existing factorization-based methods discussed earlier reveals two main features of the proposed formulation. First, it aims to minimize the error on the
observed ratings and to respect the inherent structure of the social network among the users. The tradeoff between these two objectives is captured by the regularization parameter λS which is required to be
tuned effectively.

12

Algorithm 1 GD based Matrix Factorization with Trust and Distrust Propagation
1: Input: R: partially observed rating matrix, ΩS
2: Output: U and V
3: for t = 1, . . . , T do
4:
Compute the gradients ∇U R(Ut , Vt ) and ∇V R(Ut , Vt ).
5:
Compute ∇U by Eq. 7
6:
Compute ∇V by Eq. 8
7:
Update:
Ut +1 = Ut − η t ∇U |U=Ut ,V=Vt
Vt +1 = Vt − η t ∇V |U=Ut ,V=Vt
8:
9:

end for
return UT +1 and VT +1 .

In a similar way, applying the logistic loss to the general formulation in (4) yields the following objective:
L (U, V) =

´2 λ
1 X ³
λV
U
R i j −Ui>,: V j ,: +
kUkF +
kVkF
2 (i , j )∈ΩR
2
2
+

X
¡
¡
¢¢
λS
log 1 + exp kUi ,: −Uk,: k2 − kUi ,: −U j ,: )k2 .
|ΩS | (i , j ,k)∈ΩS

(6)

Remark 1. We note that in several applications of recommender systems, besides the observed ratings, a
description of the users and/or the objects through attributes (e.g., gender, age) or measures of similarity
is available that could potentially benefit the process of recommendation (see e.g. [2] for few interesting
applications). In that case it is tempting to take advantage of both known ratings and descriptions to
model the preferences of users. A natural way to incorporate the available meta-data is to kernalize the
similarity measure between latent features based on a positive definite kernel between pairs that can be
deduced from the meta-data. More specifically, instead of simply using Euclidian distance as the similarity measure between latent features in (5), we can use the kernel matrix K obtained from the Laplacian of
the graph obtained from the meta-data to measure the similarity as:
¢
¡
¢> ¡
D(Ui ,: ,U j ,: ) = Ui ,: −U j ,: K Ui ,: −U j ,: ,
P
where K = (D − W)−1 , with D as a diagonal matrix with D i ,i = nj=1 Wi j . Here W captures the pairwise
weight between users in the similarity graph between users that is computed based on the available metadata about users.
Remark 2. We would like to emphasize that it is straightforward to generalize the proposed framework
to incorporate similarity and dissimilarity information between items. What we need is to extract the
triplets from the trust/distrust links between items and repeat the same process we did for users. This
will add another term to the objective in terms of latent features of items V as shown in the following
generalized formulation:
L (U, V) =

´2 λ
λV
1 X ³
U
R i j −Ui>,: V j ,: +
kUkF +
kVkF
2 (i , j )∈ΩR
2
2
+

X
¡
¢
λS
max 0, 1 − kUi ,: −U j ,: k2 + kUi ,: −Uk,: k2
|ΩS | (i , j ,k)∈ΩS

+

X
¡
¢
λI
max 0, 1 − kVi ,: − V j ,: k2 + kVi ,: − Vk,: k2 ,
|ΩI | (i , j ,k)∈ΩI

where λI is the regularization parameter and ΩI is the set of triplets extracted from the similar/dissimilar
links between items. The similarity/dissimilarity links between items can be constructed according to tags
issued by users or associated with items, and categories. For example, if two items are attached with a
13

same tag, there is a trust link between them and otherwise distrust link. Alternatively, trust/distrust links
can be extracted by measuring similarity/dissimilarity based on the item properties or profile if provided.
This can further improve the accuracy of recommendations.

4.2 Batch Gradient Descent based Optimization
In optimization for supervised machine learning, there exist two regimes in which popular algorithms
tend to operate: the stochastic approximation regime, which samples a small data set per iteration,
typically a single data point, and the batch or sample average approximation regime, in which larger
samples are used to compute an approximate gradient. The choice between these two extremes outlines the well-known tradeoff between inexpensive noisy steps and expensive but more reliable steps.
Two preliminary examples of these regimes are the Gradient Descent (GD) and the Stochastic Gradient
Descent (SGD) methods, respectively. Both GD and SGD methods starts with some initial point, and
iteratively updates the solution using the gradient information at intermediate solutions. The main
difference is that GD requires a full gradient information at each iteration while SGD only requires an
unbiased estimate of the full gradient which can be done by sampling
We now discuss the application of GD algorithm to solve the optimization problem in (5) as detailed
in Algorithm 1. Recall that the objective function is not jointly convex in both U and V. On the other
hand, the objective is convex in one parameter by fixing the other one. Therefore, we follow an iterative
method to minimize the objective. At each iteration, first by fixing V, we take a step in the direction of
the negative gradient for U and repeat the same process for V by fixing U.
For the ease of exposition, we introduce further notation. For any triplet (i , j , k) ∈ ΩS we note that
the kUi ,: −U j ,: k2 − kUi ,: −Uk,: k2 can be written as Tr(CU> U) where Tr(·) denotes the trace of the input
matrix and C is a sparse auxiliary matrix defined for each triplet with all entries equal to zero except:
Ci k = Cki = C j j = 1 and Ckk = Ci j = C j i = −1. Having defined this notation, we can write the objective
in (5) as:

L (U, V) = R(U, V) +

´
³
X
λV
λS
λU
kUkF +
kVkF +
max 0, 1 − Tr(Ckij U> U) .
2
2
|ΩS | (i , j ,k)∈ΩS

where Ckij is the C matrix defined above which is associated with triplet (i , j , k). To apply the GD
method, we need to compute the gradient of L (U, V) with respect to U and V which we denote by
∇U = ∇U L (U, V) and ∇V = ∇V L (U, V), respectively. We have:

∇U = ∇U R(U, V) + λU U −

X
λS
k
1
(UCk>
k
>
i j + UCi j )
|ΩS | (i , j ,k)∈ΩS [Tr(Ci j U U)<1]

(7)

where 1[·] is the indicator function which takes a value of one if its argument is true, and zero otherwise.
Similarly for ∇V we have:
∇V = ∇V R(U, V) + λV V

(8)

The main shortcoming of GD method is its high computational cost per iteration due to the gradient
computation (i.e., step (7)) which is expensive when the size of social constraints ΩS is large. We note
that the size of ΩS can be as large as O(n 3 ) by considering all triplets in the social graph. In the next subsection we provide an alternative solution to resolve this issue using the stochastic gradient descent and
mini-batch SGD methods which are more efficient than the GD method in terms of the computational
cost per iteration but with a slow convergence rate in terms of target approximation error.

4.3 Stochastic and Mini-batch Optimization
As discussed above, when the size of social network is very large, the size of ΩS may cause computational problems in solving the optimization problem in (5) using GD method. The reason is essentially
the fact that computing the gradient at each iteration requires to go through all the triplets in ΩS which
is infeasible for large networks. To alleviate this problem we propose a stochastic gradient based [52]

14

Algorithm 2 Mini-SGD based Matrix Factorization with Trust and Distrust Propagation
1: Input: R: partially observed rating matrix, ΩS , min batch size B
2: Output: U and V
3: for t = 1, . . . , T do
4:
∇t ← 0
5:
for b = 1, . . . , B do
6:
(i , j , k) ← Sample random triplet from ΩS
7:
if (1 − kUi ,: −U j ,: )k2 + kUi ,: −Uk,: k2 > 0) then
8:
∇t ← Ut Ckij U>
t
9:
end if
10:
end for
11:
Compute the gradients ∇U R(Ut , Vt ) and ∇V R(Ut , Vt ).
12:
Update:
¶
µ
λS
∇t
Ut +1 = Ut − η t ∇U R(Ut , Vt ) + λU Ut +
B |ΩS |
13:

Update:
Vt +1 = Vt − η t (∇V R(Ut , Vt ) + λV Vt )

14:
15:

end for
return UT +1 and VT +1 .

method to solve the optimization problem. The main idea is to choose a fixed subset of triplets for
gradient computation instead of all |ΩS | triplets at each iteration [10]. More specifically, at each iteration, we sample B triplets uniformly at random from ΩS to compute the next solution. We note that
this strategy generates unbiased estimates of the true gradient and makes each iteration of algorithm
computationally more efficient compared to the full gradient counterpart. In the simplest case, SGD
algorithm, only one triplet is chosen at each iteration to generate an unbiased estimate of the full gradient. We note that in practice SGD is usually implemented based on data shuffling, i.e., making the
sequence of the training samples random and then training the model by going through the training
samples one by one. An intermediate solution, known as mini-batch SGD, chooses a subset of triplets
to compute the gradient. The promise is that by selecting more triplets at each iteration, on one hand
the variance of stochastic gradients decreases promotional to the number of sampled triplets, and on
the other hand the algorithm enjoys the light computational cost of basic SGD method.
The detailed steps of the algorithm are shown in Algorithm 2. The mini-batch SGD method improves the computational efficiency by grouping multiple constraints into a mini-batch and only updating the U and V once for each mini-batch. For brevity, we will refer to this algorithm as Mini-SGD.
More specifically, the Mini-SGD algorithm, instead of computing the full gradient over all triplets, samples B triplets uniformly at random from ΩS where 1 ≤ B ≤ |ΩS | is a parameter that needs to be provided
to the algorithm, and computes the stochastic gradient as:
∇t =

λS
B

X
(i , j ,k)∈ΩB

1[Tr(Ck

ij

k
(UCk>
i j + UCi j )
U>
t Ut )<1]

where ΩB is the set of B sampled triplets from ΩS . We note that
E[∇t ] =

X
λS
k
1
(UCk>
k
>
i j + UCi j ),
|ΩS | (i , j ,k)∈ΩS [Tr(Ci j Ut Ut )<1]

i.e., ∇t is an unbiased estimate of the full gradient in the right hand side. When B = |ΩS |, each iteration
handles the original objective function and Mini-SGD reduces to the batch GD algorithm. We note that
both GD and SGD share the same convergenceprate in terms of number of iterations in expectation for
non-smooth optimization problems (i.e., O(1/ T ) after T iterations), but SGD method requires much
less running time to convergence compared to the GD method due to the efficiency of its individual
iterations.

15

5 Experimental Results
In this section, we conduct exhaustive experiments to demonstrate the merits and advantages of the
proposed algorithm. We conduct the experiments on the well-known Epinions 2 data set, aiming to
accomplish and answer the following fundamental questions:
1. Prediction accuracy: How does the proposed algorithm perform in comparison to the stateof- the-art algorithms with/without incorporating trust and distrust relationships between users.
Whether or not the trust/distrust social network could help in making more accurate recommendations?
2. Correlation of social relations with rating information: To what extent, the trusted and distrusted friends of a user u are aligned with the ratings the user u issued for the reviews written by
his friends? A positive answer to this question indicates that two users will issue similar (dissimilar) ratings if they are connected by a trust (distrust) relation and prefer to behave similarly.
3. Model selection: What role do the regularization parameters λS , λU and λV play in the accuracy
of the proposed recommender system and what is the best strategy to tune these parameters?
4. Handling cold-start users: How does exploiting social relationships in prediction process affect
the performance of recommendation for cold-start users?
5. Trading trust for distrust: To what extent the distrust relations can compensate for the lack of
trust relations?
6. Efficiency of optimization: What is the trade-off between accuracy and efficiency by moving
from the gradient descent to the stochastic gradient descent with different batch sizes?
In the following subsections, we intend to answer these questions. We begin by introducing the data set
we use in our experiemnts and the metrics we employ to evaluate the results, followed by the detailed
experimental results.

5.1 Data Set Description and Experimental Setup
The Epinions data set We begin by discussing the data set we have chosen for our experiments. To
evaluate the proposed algorithm on trust and distrust-aware recommendations, we use the Epinions
data set [22], a popular e-commerce site and customer review website where users share opinions on
various types of items such as electronic products, companies, and movies, through writing reviews
about them or assigning a rating to the reviews written by other users. The rating values in Epinions are
discrete values ranging from Ònot helpfulÓ (1/5) to Òmost helpfulÓ (5/5). These ratings and reviews
would potentially influence future customers when they are about to decide whether a product is worth
buying or a movie is worth watching.
Epinions allows users to evaluate other users based on the quality of their reviews, and to make
trust and distrust relations with other users in addition to the ratings. Every member of Epinions can
maintain a "trust" list of people he/she trusts that is referred to as web of trust (social network with trust
relationships) based on the reviewers with consistent ratings or "distrust" list known as block list (social
network with distrust relationships) that presents reviewers whose reviews were consistently found to
be inaccurate or low quality. The fact that the data set contains explicit positive and negative relations
between users makes it very appropriate to study issues in trust- and distrust-enhanced recommender
systems. Epinions is thus an ideal source for experiments on social recommendation. We remark that
the Epinions data set only contains bivalent relations (i.e., contains only full trust and full distrust, and
no gradual statements).
To conduct the coming experiments, we sampled a subset of Epinions data set with n = 121, 240
users and m = 685, 621 different items. The total number of observed ratings in the sampled data set is
12,721,437 which approximately includes 0.02% of all entries in the rating matrix R which demonstrates
the sparsity of the rating matrix. We note that the selected items are the most frequently rated overall.
The statistics of the data set is given in Table 2. The social statistics of the this data source is summarized
2 http://www.trustlet.org/wiki/Epinions_datasets

16

Table 2: Statistics of sample data from Epinions data set used in our experiments.
Statistic
Quantity
Number of users
121,240
Number of items
685,621
Number of ratings
12,721,437
Number of trust relations
481,799
Number of distrust relations
96,823
Minimum number of ratings by users
1
Minimum number of ratings for items
1
Maximum number of ratings by users
148735
Maximum number of ratings for items
945
Average number of ratings by users
85.08
Average number of ratings for items
15.26

Table 3: Maximum and average trust and distrust relations for users in the sampled data set.
Statistics
Trust per user
Be Trusted per user
Max
Min
Average

1983
1
4.76

2941
0
4.76

Max
Min
Average

Distrust per user
1188
1
0.91

Be Distrusted per user
429
0
0.91

in Table 3. The frequencies of ratings for users is shown are Table 4. In the user distrust network, the
total number of issued distrust statements is 96,823. As to the user trust network, the total number of
issued trust statements is 481,799.
Experimental setup To better evaluate the effect of utilizing the social side information in recommendation accuracy, we employ different amount of training data 90%, 80% , 70% and 60% to create
four different training sets that are increasingly sparse but the social network remains the same in all of
them. Training data 90%, for example, means we randomly select 90% of the ratings from the sampled
Epinions data set as the training data to predict the remaining 10% of ratings. The random selection
was carried out 5 times independently to have a fair comparison. Also, since our preliminary results
on a smaller data set revealed that the hinge loss performs better than the exponential loss, in the rest
of experiments we stick to this loss function. However, we note the exponential loss is slightly faster in
optimizing the corresponding objective function thanks to its smoothness, but it was negligible considering its worse accuracy compared to the hinge loss. All implementations are in Matlab, and all
experiments were performed on a 4-core 2.0 GHZ of a load-free machine with a 12G of RAM.

5.2 Metrics
5.2.1 Metrics for rating prediction
We employ two well-known measures, the Mean Absolute Error (MAE) and the Root Mean Squared
Error (RMSE) [25] to measure the prediction accuracy of the proposed approach in comparison with
other basic collaborative filtering and trust/distrust-enhanced recommendation methods.
MAE is very appropriate and useful measure for evaluating prediction accuracy in offline tests [25,
45]. To calculate MAE, the predicted rating is compared with the real rating and the difference (in absolute value) considered as the prediction error. Then, these individual errors are averaged over all predictions to obtain the overall MAE value. More precisely, let T denote the set of ratings to be predicted,

17

Table 4: The frequencies of user’s rating.
# of Ratings
# of Users

0-10
4,198,074 (≈ 33%)

11-20
3,053,144 (≈ 24%)

21-30
2,289,858 (≈ 18%)

31-40
1,526,572 (≈ 12%)

41-50
534,300 (≈ 4.2%)

267,1

# of Ratings
# of Users

61-70
157,745 (≈ 1.24%)

71-80
143,752 (≈ 1.13%)

81-90
104,315 (≈ 0.82%)

91-100
43,252 (≈ 0.34%)

101-200
21,626 (≈ 0.17%)

2
10,68

i.e., T = {(i , j ) ∈ [n]×[m], R i j needs to be predicted} and let R̂ denote the prediction matrix obtained by
algorithm after factorization. Then,
P
MAE =

(i , j )∈T

|R i j − R̂ i j |

|T |

,

where R i j is the real rating assigned by the user i to the item j , and R̂ i j is the rating user i would assign
to the item j that is predicted by the algorithm .
The RMSE metric is defined as:
v
uP
¡
¢2
u
t (i , j )∈T R i j − R̂ i j
.
RMSE =
|T |
The first measure (MAE) considers every error of equal value, while the second one (RMSE) emphasizes larger errors. We would like to emphasize that even small improvements in RMSE are considered
valuable in the context of recommender systems. For example, the Netflix prize competition offered a
1,000,000 reward for a reduction of the RMSE by 10% [72].
5.2.2 Metrics for evaluating the correlation of ratings with trust/distrust relations
As part of our experiments, we investigate how the explicit trust/distrust relations between users in
the social network are aligned with the implicit trust/distrust relations between users conveyed from
the rating information. We use recall, Mean Average Precision (MAP) [44] and Normalized Discount
Cumulative Gain (NDCG) to evaluate the ranking results. Recall is defined as the number of relevant
friends divided by the total number of friends in the social network. Precision is defined as the number
of relevant friends (trusted or distrusted) divided by the number of friends in the social network. Given
a user u, let r i be the relevance score of the friend ranked at position i , where r i = 1 if the user is relevant
to the u and r i = 0 otherwise. Then we can compute the Average Precision (AP) as
P
r i × Precision@i
.
AP = i
# of relevant friends
MAP is the average of AP over all the users in the network.
NDCG is a normalization of the Discounted Cumulative Gain (DCG) measure. DCG is a weighted
sum of the degree of relevancy of the ranked users. The weight is a decreasing function of the rank
(position) of the user, and therefore called discount. NDCG normalizes DCG by the Ideal DCG (IDCG),
which is simply the DCG measure of the best ranking result. Thus NDCG measure is always a number
in [0, 1]. NDCG at position k is defined as:
NDCG@k = Zk

k
X
2r i − 1
i =1 log(i + 1)

where k is also called the scope, which means the number of top-ranked users presented to the user
and Zk is chosen such that the perfect ranking has a NDCG value of 1. We note that the base of the
logarithm does not matter for NDCG, since constant scaling will cancel out due to normalization. We
will assume it is the natural logarithm throughout this paper.

18

Figure 2: Grid Search to find the best values for λU and λC on the data set with 90% of rating information.

5.3 Model Selection
Tuning of parameters (a.k.a model selection in learning community) is a critical problem in most of the
learning problems. In some situations, the learning performance may drastically vary with different
choices of the parameters. There are three parameters in objective (5) that play very important role in
the effectivity of the proposed algorithm. These are λU , λV , and λS . Between these, λS controls how
much the proposed algorithm should incorporate the information of the social network in completing
the partially observed rating matrix. In the extreme case, a very small value for λS , the algorithm almost forgets the social information exists between the users and only utilizes the observed user-item
rating matrix for factorization. On the other hand, if we employ a very large value for λS , the social
network information will dominate the learning process, leading to a poorer performance. Therefore,
in order to not hurt the recommendation performance, we need to find a reasonable value for social
regularization parameter. To this end, we analyze how the combination of these parameters affect the
recommendation performance.
We conduct a grid search on the potential values of two parameters λS and λV to find the combination with best performance. Figure 2 shows the grid search results for these parameters on data set with
90% of training data where the optimal prediction accuracy is achieved at point (14.8, 11) with the optimal RMSE = 1.12. We would like to emphasize that we have done the cross validation for only pairs of
(λS , λV ) and (λS , λU ) because, (i) considering the grid search for the triplet (λS , λU , λV ) is computationally burdensome, (ii) and our preliminary experiments showed that λV and λU behave similarly with
respect to λS . Based on the results reported in Figure 2, in the remaining experiments, we set λS = 14.8,
λV = 11, and λU = 13 when the training is performed on the data set with 90% of rating information.
We repeat the same process to find out the best setting of regularization parameters for other data sets
with 80%, 70%, and 60% of rating data as well.

5.4 Baseline Methods
Here we briefly discuss the baseline algorithms that we intend to compare the proposed algorithm.
The baseline algorithms are chosen from both types of memory-based and model-based recommender
systems with different types of trust and distrust relations. In particular, we consider the following basic
algorithms:
• MF (matrix factorization based recommender): this is the basic matrix factorization based rec19

ommender formulated in the optimization problem in (1) which does not take the social data
into account.
• MF+T (matrix factorization with trust information): to exploit the trust relations between users
in matrix factorization, [40] relied on the fact that the distance between latent features of users
who trust each other must be minimized that can be formulated as the following objective:
min
U

n
X
1X
D(Ui ,: ,U j ,: ),
2 i =1 j ∈N+ (i )

where N+ (i ) is the set of users the i th user trusts in the social network (i.e., S i j = +1). By employing this intuition in the basic formulation in (1), [40] solves the following optimization problem:
"
#
´2 α X
n
X
1 X ³
λU
λV
>
min
R i j −Ui ,: V j ,: +
D(Ui ,: ,U j ,: ) +
kUkF +
kVkF .
U,V 2 (i , j )∈Ω
2 i =1 j ∈N+ (i )
2
2
R
• MF+D (matrix factorization with distrust information): the basic intuition behind the algorithm
proposed in [40] to exploit the distrust relations is as follows: if user u i distrusts user u j , then we
can assume that their corresponding latent features Ui ,: and U j ,: would have a large distance. As
a result we aim to maximize the following quantity for all users:
max
U

n
X
1X
D(Ui ,: ,U j ,: ),
2 i =1 j ∈N− (i )

where N− (i ) denotes the set of users the i th users distrusts (i.e, S i j = −1). Adding this term to the
basic optimization problem in (1) we obtain the following optimization problem:
"
#
´2 β X
n
X
1 X ³
λU
λV
>
min
D(Ui ,: ,U j ,: ) +
kUkF +
kVkF .
R i j −Ui ,: V j ,: −
U,V 2 (i , j )∈Ω
2 i =1 j ∈N− (i )
2
2
R
• MF+TD (matrix factorization with trust and distrust information): this algorithm stands for the
algorithm proposed in the present work. We note that there is no algorithm in the literature that
exploits both trust and distrust relations in factorization process simultaneously.
• NB (neighborhood-based recommender): this algorithm is the basic memory-based recommender algorithm that predicts a rating of a target item i for user u using a combination of the
ratings of neighbors of u (similar users) that already issued a rating for item i . Formally,
P
W 0 (R ui − R̄ u )
u 0 ∈N (u),Wuu 0 >0 uu
R̂ ui = R̄ u +
,
(9)
P
Wuu 0
u 0 ∈N (u),W 0
uu

where the pairwise weight Wuu 0 between pair of users (u, u 0 ) is calculated by Pearson’s correlation
coefficient [25]
• NB+T (neighborhood with trust information) [45, 17, 47]: the basic idea behind the trust based
recommender systems proposed in TidalTrsut [17] and MoleTrsut [45] is to limit the set of neighbors in (9) to the users who are trusted by user u. The distinguishing feature of these algorithms
is the mechanism of trust propagation to estimate the trust transitively for all the users. By adapting (9) to only consider trustworthy neighbors in predicting the new ratings we obtain:
P
W 0 (R ui − R̄ u )
u 0 ∈N+∗ (u),Wuu 0 >0 uu
,
(10)
R̂ ui = R̄ u +
P
W 0
u 0 ∈N ∗ (u),W 0 >0 uu
+

uu

where N+∗ (u) is the set of trusted neighbors of u in the social network with propagated trust relations (when there is no propagation we have N+∗ (u) = N+ (u)). We note that instead of Pearson’s
correlation coefficient as the wighting schema, we can infer the weights exploiting the social relation between the users. Since for the data set we consider in our experiments, the trust/distrust
relations are binary values, the social based pairwise distance would be simply the hamming distance between the binary vector representation of social relations of users. For implementation
details we refer to [70, Chapter 6].
20

Table 5: The consistency of implicit and explicit trust relations in the data set for different ranges of
ratings measured in terms of NDCG, recall, and MAP.
# of Ratings
NDCG@10 NDCG@20
Recall@10 Recall@20 Recall@40
MAP
0-20
21-40
41-60
61-80
≥ 81

0.083
0.108
0.117
0.120
0.135

0.078
0.103
0.112
0.117
0.126

0.054
0.080
0.083
0.088
0.091

0.092
0.125
0.128
0.132
0.151

0.156
0.198
0.225
0.230
0.253

0.140
0.190
0.208
0.230
0.244

Table 6: The consistency of implicit and explicit distrust relations in the data set for different ranges of
ratings measured in terms of NDCG, recall, and MAP.
# of Ratings
NDCG@10 NDCG@20
Recall@10 Recall@20 Recall@40
MAP
0-20
21-40
41-60
61-80
≥ 81

0.065
0.071
0.082
0.089
0.104

0.057
0.068
0.072
0.078
0.096

0.045
0.060
0.075
0.081
0.087

0.071
0.077
0.085
0.105
0.125

0.132
0.140
0.158
0.164
0.191

0.130
0.134
0.152
0.160
0.183

• NB+TD-F (neighborhood with trust information and distrust information as filtration) [69, 72]:
a simple strategy to use distrust relations in the recommendation is to filter out distrusted users
from the list of neighbors in predicting the ratings. As a result, we adapt (9) to exclude distrusted
users from the users’ propagated web of trust.
• NB+TD-D (neighborhood-based with trust information and integrated distrust information) [69,
72]: in the same spirit as the filtration strategy, we can use distrust relations to debug the trust
relations. More specifically, if user u trusts user v, v trusts w, and u distrusts w, then the latter
distrust relation contradicts the propagation of the trust from u to w and can be excluded from
the prediction. In this method distrust is used to debug the trust relations.

5.5 On the Consistency of Social Relations and Rating Information
As already mentioned, the Epinions website allows users to write reviews about products and services
and to rate reviews written by other users. Epinions also allows users to define their web of trust, i.e.
"reviewers whose reviews and ratings have been consistently found to be valuable" and their block list,
i.e. "reviewers whose reviews are found to be consistently inaccurate or not valuableÓ. Different intuitions on interpreting these social information will result in different models. The main rational behind
incorporating trust and distrust relations in recommendation process is to take the trust/distrust relations between users in the social network as the level of agreement between ratings assigned to reviews
by users 3 . Therefore, investigating the consistency or alignment between user ratings (implicit trust)
and trust/distrust relations in the social network (explicit trsut) become an important issue.
Here, we aim to empirically investigate whether or not there is a correlation between a user’s current
trustees/friends or distrusted friends and the ratings that user would assign to reviews issued by his
neighbors. Obviously, if there is no correlation between social context of a user and his/her ratings to
reviews written by his neighbors, then the social structure does not provide any advantage to the rating
information. On the other hand, if there exists such a correlation, then the social context could be
supplementary information to compensate for the lack of rating information to boost the accuracy of
recommendations.
The consistency of trust relations and rating information issued by users on the reviews written
by his trustees has been analyzed in [83, 23]. However, [83] also claimed that social trust (i.e., explicit
trust) and similarity between users based on their issued ratings (i.e., implicit trust) are not the same,
3 In the literature the similarity between users conveyed from the rating information issued by users and the direct relation in
the social network are usually referred to as the implicit and the explicit trust, respectively.

21

Table 7: The alignment rate of users in establishing trust/distrust relationships with future users in the
social network based on the majority vote of their current trusted/distrusted friends. The number of
trusted friends (+) and distrusted friends (-) are denoted by n + and n − , respectively. Here u denotes the
current user and w stands for a future user in the network.
Setting
Type of Relation (u
w) % of Relations Alignment Rate (%)
n+ > n−
n+ < n−
n + = n − > 0 or n + = n − = 0

+
+
+

48.80
2.54
1.15
8.02
39.49

92.09
8.15
17.88
83.42
-

and can be used complementary. According to [38], when comparing implicit social information with
explicit social information, the performance of using implicit information is slightly worse. We further
investigate the same question about the consistency of distrust relations and ratings issued by users to
their distrusted neighbors. The positive answer to this question can be interpreted as follows. Given
that user u is interested in item i , the chances that v, trusted (distrusted) by u, also likes this item i is
much higher (lower) than for user w not explicitly trusted (distrusted) by u.
To measure the similarity between users, there are several methods we can borrow in the literature.
In this paper, we adopt the most popular approach that is referred to as Pearson correlation coefficient
(PCC) P : U × U 7→ [−1, +1] [6, 47], which is defined as:
Pm
i =1 (R ui − R̄ u )(R vi − R̄ v )
, ∀u, v ∈ U ,
P (u, v) = qP
Pm
m
2
2
j =1 (R ui − R̄ u ) × j =1 (R vi − R̄ v )
where R̄ u and R̄ v are the average of ratings issued by users u and v, respectively. The PCC measures
the extent to which there is a linear relationship between the rating behaviors of the two users, the extreme values being -1 and 1. The similarity of two users becomes negative when users have completely
diverging ratings. We note that this quantity can be considered as the implicit trust between users that
is conveyed via ratings given by users.
To conduct this set of experiments, we first group all the users in the training data set based on
the number of ratings, and then measure the prediction accuracies of different user groups. Users are
grouped into five classes: "[1, 20)", "[20, 40)", "[40, 60)", "[60, 80)", and "> 81 ". In order to have a
comprehensive view of the ranking performance, we present the NDCG, recall and MAP scores of trust
and distrust alignments on the Epinions data set in Table 5 and Table 6, respectively. We note that
the data set we use in our experiments only contains bivalent trust values, i.e., -1 and +1, and it is not
possible to have an ordering on the list of friends (timestamp of relations would be an option to order
the friends but unfortunately it is not available in our data set). To compute the NDCG, we use the
ordering of trusted/distrusted friends which yields the best value.
On the positive side, we observe a clear trend of alignment between ratings assigned by a user and
the type of relation he has made in the social network. This observation coincides with our intuition.
Overall, when more ratings are observed for a user, the similarity calculation process will find more
accurate similar or dissimilar neighbors for this user since we have more information to represent or
interpret this user. Hence, by increasing the number of ratings, It is conceivable from the results in Tables 5 and 6 that the alignment between implicit and explicit neighbors becomes better. By comparing
the results in Tables 5 and 6 we can see that trust relations are slightly better aligned than the distrust
relations.
On the negative side, the results show that the NDCG on both types of relations is small. One explanation for this phenomenon is that the Epinions data set is not tightly bound to a specific application.
For example, a user may trust or distrust anther user based on his/her comments on a specific product
but they might have similar taste on other products. Furthermore, compared to other data sets such as
FilmTrusts, the Epinions data set is very sparse data set, and consequently it is relatively inaccurate to
rely on the rating information to compute the implicit trust relations. Finally, our approach to distinguish trust/distrust lists from the rating information is limited by the PCC trust metric we have utilized.
We conjecture that better trust metrics that is able to exploit other side information such as time and in22

Table 8: The accuracy of prediction of matrix factorization with three different methods measured in
terms of MAE and RMSE errors. The parameter k represents the number of latent features in factorization.
k
% of Training Measure
MF
MF+T
MF+D
MF+TD
10

60%
70%
80%
90%

20

60%
70%
80%
90%

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

0.9813 ± 0.042
1.6050 ± 0.032
0.9462 ± 0.083
1.5327 ± 0.032
0.9150± 0.022
1.3824 ± 0.032
0.8921 ± 0.025
1.2166 ± 0.017

0.8561 ± 0.032
1.4125 ± 0.022
0.8332 ± 0.092
1.2407 ± 0.063
0.8206 ± 0.041
1.1906 ± 0.042
0.8158 ± 0.016
1.1403 ± 0.027

0.9720 ± 0.038
1.5036 ± 0.040
0.9241 ± 0.012
1.4405 ± 0.023
0.8722 ± 0.034
1.3155 ± 0.026
0.8736 ± 0.053
1.1869 ± 0.049

0.8310 ± 0.016
1.2294 ± 0.086
0.8206 ± 0.023
1.1562 ± 0.043
0.8113 ± 0.032
1.1061 ± 0.021
0.8025 ± 0.014
1.0872 ± 0.020

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

0.9972 ± 0.016
1.6248 ± 0.014
0.9688 ± 0.019
1.5162 ± 0.016
0.9365 ± 0.025
1.4081 ± 0.015
0.9224 ± 0.016
1.2207 ± 0.0 18

0.8431 ± 0.018
1.3904 ± 0.042
0.8342 ± 0.062
1.2722 ± 0.027
0.8172 ± 0.011
1.1853 ± 0.023
0.8128 ± 0.021
1.1402 ± 0.026

0.9746 ± 0.060
1.5423 ± 0.046
0.9350 ± 0.022
1.4540 ± 0.075
0.8705 ± 0.016
1.3591 ± 0.073
0.8805 ± 0.032
1.1933 ± 0.028

0.8475 ± 0.012
1.1837 ± 0.023
0.8290 ± 0.034
1.1452 ± 0.016
0.8129 ± 0.025
1.1049 ± 0.082
0.8096 ± 0.010
1.0851 ± 0.011

teractional information would be helpful in distinguishing implicit trusted/distrusted friends, leading
to better alignment between implicit and explicit trust relations.
We also conduct experiments to evaluate the consistency of social network only based on the
trust/distrust relations between users. In particular, we investigate to what extent a users’ relations
are aligned with the opinion of his/her neighbors in the social network. More specifically, let u be a
user who is about to make a trust or distrust relation to another user v. We assume that n + number
of u’s neighbors trust v and n − number of u’s neighbors distrust v. We note that in the real data set
the distrust relations are hidden. To conduct this set of experiments, we randomly sample 30% of the
relations from the social network and use the remaining 70% to predict the type of sampled relations 4
by majority voting.
Table 7 shows the results on the consistency of social relations. We observe that in all cases there
is an alignment between the opinions of users’ friends and his/her own relation (92.09% and 83.42%
when the majority of friends trust and distrust the target user, respectively). This might be due to social
influence of people on social network, however, it is hard to justify the existence of such a correlation in
Epinions data set which includes reviews for diverse set of products and taste of users. One interesting
observation from the results reported in Table 7 is the case where the number of distrusted users dominates the number of trusted users (i.e., n − > n + ). While the distrust relations are private to other users,
but we can see that there is a significant alignment between users’s relation type and his distrusted
friends.

5.6 On the Power of Utilizing Social Relationships
We now turn to investigate the effect of utilizing social relationships between users on the accuracy
of recommendations in factorization-based methods. In other words, we would like to experimentally
evaluate whether incorporating distrust can indeed enhance the trust-based recommendation process.
To this end, we run four different MF (i.e., pure matrix factorization based algorithm), MF+T (i.e., matrix
factorization with only trust relationships), MF+D (i.e., matrix factorization with only distrust relationships), and MF+TD (i.e., the algorithm proposed here) algorithms on the data set. We run the algorithms
with k = 10 and k = 20 latent vector dimensions. As mentioned earlier, different amount of training data
90%, 80% , 70% and 60% has been used to create four different training sets that are increasingly sparse
but the social network remains the same in all of them. We evaluate all algorithms by both MAE and
4 A more realistic way would be to use the timestamp of relations to create the training and test sets.

23

RMSE measures.
Table 8 shows the MAE and RMSE errors for the four sampled data sets. First, as we expected, the
performance of all learning algorithms improves with an increasing number of training data. It is also
not surprising to see that the MF+T, MF+D, and MF+TD algorithms which exploit social side information perform better than the pure matrix factorization based MF algorithm. Second, the proposed
algorithm outperforms all other baseline algorithms for all the cases, indicating that it is effective to incorporate both types of social side information in recommendation. This result by itself indicates that
besides trust relationships in the social network, the distrust information is also a rich source of information and can be utilized in recommendation algorithms. We note that distrust information needs to
be incorporated carefully as its nature is totally different from trust information. Finally, it is noticeable
that the MF+T outperforms the MF+D algorithm due to huge number of trust relations to distrust relations in our data set. It is also remarkable that users are more likely to be influenced by their friends
to make trust relations than the distrust relations due to the private nature of distrust relations in Epinions data set. This might lead us to believe that distrust relations have better quality than trust relations
which requires a deeper investigation to be verified.

5.7 Comparison to Baseline Algorithms
Another question that is worthy of investigation is how state-of-the-art approaches perform compared
to the method proposed in this paper. To this end, we compare the performance of the MF-TD algorithm with the baseline algorithms introduced in Subsection 5.4. Table 9 contains the results of our
experiments with eight different algorithms on the data set with 90% of rating data. The second column
in the table represents the configuration of parameters used by each algorithm.
When we utilize trust/distrust relations in neighborhood-based algorithms, a crucial decision we
need to make is to which level the propagation must be performed (no propagation corresponds to the
single level propagation which only includes direct neighbors). Let p and q denote the level of propagation for trust and distrust relations, respectively. Let us first consider the trust propagation to decide
the value of p. We note that there is a tradeoff between accuracy and the level of trust propagation: the
longer propagation levels results in less accurate trust predictions. This is due the fact that when we use
longer propagation levels, the further away we are heading from each user, and consequently decrease
the confidence on the predictions. Obviously this affects the accuracy of the recommendations significantly. As a result, for the trust propagation we only consider single level propagation by choosing p = 1
(i.e, N+∗ = N+ ). We also note that since in the Epinions data set a user can not simultaneously trust
and distrust another user, in the neighborhood-based method with distrust relations, the debugging
only makes sense for propagated information. Therefore, we perform a three level distrust propagation
(q = 3) to constitute the set of distrusted users for each users. We note that the longer the propagation
levels, the more often distrust evidence can be found for a particular user, and hence the less neighbors will be left to participate in the recommendation process. For factorization based methods, the
value of regularization parameters, i.e., λU , λV , and λS , are determined by the procedure discussed in
Subsection 5.3.
The results of Table 9 reveal some interesting conclusions as summarized below:
• From Table 9, we can observe that for factorization-based methods, incorporating trust or distrust
information boost the performance of recommendation in terms of both accuracy measures. This
demonstrates the advantages of trust and distrust-aware recommendation algorithms. We also
can see that both MF+T and MF+D perform better than the non-social MF but the performance
of MF+T is significantly better than MF+D. As discussed before, this observation does not indicate
that the trust relations are more beneficial than the distrust relations as in our data set only 16.7%
of relations are distrust relations. The MF+TD algorithm that is able to employ both types of relations is significantly better than other algorithms that demonstrates the advantages of proposed
method to utilize trust and distrust relations.
• Looking at the results reported in Table 9, it can immediately be noticed that the incorporation
of trust and distrust information in neighborhood-based methods decreases the prediction error
but the improvement is not as significant as the factorization based methods. We note that for
the NB+T method with longer levels of propagation (p = 2, 3), our experiments revealed that the
accuracy remains almost same or gotten worse on both MAE and RMSE measures and this is
24

Table 9: Comparison with other popular methods. The reported values are the MAE and RMSE on the
data set with 90% of rating information. The values of parameters for each specific algorithm is included
in the second column.
Method
Parameter (s)
MAE
RMSE
MF
MF+T
MF+D
MF+TD

k = 10 and λU = λV = 5
k = 10, λU = λV = 5 , and α = 1
k = 10, λU = λV = 5 , and β = 10
k = 10, λU = 13, λV = 11 , and λS = 14.8

0.8921
0.8158
0.8736
0.8025

1.2166
1.1403
1.1852
1.0872

NB
NB+T
NB+TD-F
NB+TD-D

p =1
p = 1 and q = 3
p = 1 and q = 3

0.9381
0.8904
0.8692
0.8728

1.5275
1.3455
1.2455
1.2604

Table 10: The accuracy of handling cold-start users and the effect of social relations. The number of
leant features in this experiments is set to k = 10. The first column shows the number of cold-start
users sampled randomly from all users in the data set. For the cold-starts users all the ratings have
been excluded from the training data and used in the evaluation of three different algorithms.
% of Cold-start Users Measure
MF
MF+T MF+D MF+TD
30%
20%
10%
5%

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

0.9923
1.7211
0.9812
1.7088
0.9334
1.4222
0.9134
1.3852

0.8824
1.5562
0.8805
1.4339
0.8477
1.3782
0.8292
1.2921

0.9721
1.6433
0.9505
1.6250
0.9182
1.4006
0.8633
1.3255

0.8533
1.4802
0.8472
1.2630
0.8322
1.2655
0.8280
1.2888

why we only report the results only for p = 1. In contrast, for distrust propagation we found out
that q = 3 has a visible impact on the performance of both filtering and debugging methods. We
would like to emphasize that for longer levels of distrust propagation in Epinions data set, i.e.,
q > 4, we found that the size of the set of distrusted users N−∗ (·) becomes large for most of users
which degrades the prediction accuracy. We also observe another interesting result about the
performance of NB+TD method with filtering and debugging strategies. We found that although
filtering generates slightly better predictions, NB+TD-F performs almost as good as the NB+TDD method. Although this observation does not suggest any of these methods as the method of
choice in incorporating distrust, we believe that the accuracy might differ from data set to data
set and it strongly depends on the propagation/aggregation strategy.
• Considering the results for both model-based and memory-based methods in Table 9, we can
conclude few interesting observations. First, we notice that factorization-based methods with
trust/distrust information perform better than the neighborhood based methods. Second, the
incorporation of trust and distrust relations in matrix factorization has significant improvement
compared to improvement achieved by memory-based methods. Although the type of filtration
or debugging strategy could significantly affect the accuracy of incorporating distrust in memorybased methods, but the main shortcoming of these methods comes from the fact that these algorithms somehow exclude the influence of distrusted users from the rating prediction. This stands
in stark contrast to the model proposed in this paper that ranks the neighbors based on the type
of relation. This observation necessitates to devise better algorithms for propagation and aggregation of trust/distrust information in memory-based methods.

25

Table 11: The accuracy of proposed algorithm on a data set with 390257 (≈ 90%) trust relations sampled
uniformly at random from all trust relations with varied number of distrust relations. The learning is
performed based on 90% of all ratings with k = 10 as the dimension of latent features.
Method # of Trust Relations # of Distrust Relations Measure
Accuracy
MF+TD

433,619 (≈ 90%)

9,682 (≈ 10%)
19,364 (≈ 20%)
29,047 (≈ 30%)
38,729 (≈ 40%)
48,411 (≈ 50%)
58,093 (≈ 60%)
67,776 (≈ 70%)
77,458 (≈ 80%)
87,140 (≈ 90%)
96,823 (= 100%)

MF+T

481,799 (= 100%)

0

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

0.8803 ± 0.051
1.2166 ± 0.028
0.8755 ± 0.033
1.1944 ± 0.042
0.8604 ± 0.036
1.1822 ± 0.081
0.8431 ± 0.047
1.1706± 0.055
0.8165± 0.056
1.1425± 0.091
0.8130± 0.035
1.1380± 0.046
0.8122 ± 0.041
1.1306 ± 0.042
0.8095 ± 0.036
1.1290 ± 0.085
0.8061 ± 0.044
1.1176 ± 0.067
0.8050 ± 0.052
1.1092 ± 0.063

MAE
RMSE

0.8158 ± 0.016
1.1403 ± 0.027

5.8 Handling Cold-start Users by Social Side Information
In this subsection, we demonstrate the use of social network to further illustrate the potential of proposed framework and the relevance of incorporating side information. To do so, as another set of our
experiments, we intend to examine the performance of proposed algorithm on clod-start users. Addressing cold-start users (i.e., users with few ratings or new users) is a very important for the success
of recommender systems due to huge number of this type of users in many real world systems. As a
result, handling cold-start users is one the main challenges in existing systems. To evaluate different
algorithms we randomly select 30%, 20%, 10%, and 5% as the cold-start users. For cold-start users, we
do not include any rating in the training data and consider all the ratings made by cold-start users as
testing data.
Table 10 shows the performance of above mentioned algorithms. As it is clear from the Table 10,
when the number of cold-start users is low with respect to the total number of users, say 5% of total
users, the affect of distrust relationships is negligible in prediction accuracy. But, when the number of
cold-start users is high, exploiting the trust and distrust relationships significantly improve the performance of recommendation. This result is interesting as it reveals that the lack of rating information for
cold-start and new users can be alleviated by incorporating the social relations of users, and in particular both trust and distrust relationships.

5.9 Trading Trust for Distrust Relationships
We also compare the potential benefit of trust relations to distrust relations in the proposed algorithm.
More specifically, we would like to see in what extent the distrust relations can compensate for the
lack of trust relations. We run the proposed algorithm with the subset of trust and distrust relations and
compare it to the algorithm which only utilizes all of the trust relations. To setup this set of experiments,
we randomly sample a subset of trust relations and gradually increase the amount of distrust relations
to see when the effect of distrust information compensate the effect of missed trust relations.

26

We sample 433,619 (approximately 90%) trust relations from the total 481,799 trust relations and
vary the number of distrust relations and feed to the proposed algorithm. Table 11 reports the accuracy
of proposed algorithm for different number of distrust relations in the data sets. All these samplings
have been done uniformly at random. We use 90% of all ratings for training and the remaining 10% for
evaluation, and set the dimension of latent features to k = 10. As it can be concluded from Table 11,
when we feed the proposed algorithm MF+TD with 90% of trust and 50% of the distrust relations, it
reveals very similar behavior to the trust-enhanced matrix factorization based method MF+T, which
only utilizes all the trust relations in factorization. This result is interesting in the sense that the distrust
information between users is as important as the trust information (we note that in this scenario the
number trust relations excluded from the training is almost same as the number of distrust relations
included). By increasing the number of distrust relations we can observe that the accuracy of recommendations increases as expected. In summary, this set of experiments validates that incorporating
distrust relations can indeed enhance the trust-based recommendation process and could be considered as a rich source of information to be exploited.

5.10 On the Impact of Batch Size in Stochastic Optimization
As mentioned earlier in the paper, directly solving the optimization problem in (5) using full gradient
descent method requires to go through all the triplets in the constraint set ΩS which could be computationally expensive due to the huge number of triplets in ΩS . To overcome this efficiency problem, one
can turn to stochastic gradient scent method which tries to generate unbiased estimates of the gradient
at each iteration in a much cheeper way by sampling a subset of triplets from ΩS .
To accomplish this goal, we perform gradient descent and stochastic gradient descent to solve the
optimization problem in (5) to find the matrices U and V following the updating equations derived in (7)
and (8). At each iteration t , the currently learned matrices Ut and Vt are used to predict the ratings in
the test set. In particular, at each iteration, we evaluate the RMSE and MAE on the test set, and terminate
training once the RMSE and MAE starts increasing, or the maximum number of iterations is reached.
We run the algorithm with latent vectors of dimension k = 10.
We compare the computational efficiency between proposed algorithm with GD and mini-batch
SGD with different batch sizes. We note that the GD updating rule can be considered as min-batch
SGD where the batch size B is deterministically set to be B = |ΩS | and simple SGD can be considered as
mini-batch SGD with B = 1. We remark that in contrast to GD method which uses all the triplets in ΩS
for gradient computation at each iteration, for SGD method due to uniform sampling over all tuples in
ΩS , some of the tuples may be used more than once and some of the tuples might never been used for
gradient computation.
Figures 3 and 4 show the convergence rate of four different updating rules in terms of the number of
iterations t for two different measures RMSE and RME, respectively. The first algorithm denoted by GD
runs the simple full gradient descent iteratively to optimize the objective. The other three algorithms
named SGD1, SGD2, and SGD3 in the figures use the batch sizes of B = 0.1 ∗ |ΩS |, B = 0.2 ∗ |ΩS |, and
B = 0.3 ∗ |ΩS |, respectively. In our experiments, due to very slow convergence of the basic SGD method
with B = 1 in comparison to other fours methods, we simply exclude its result from the discussion.
In terms of accuracy of predictions, from both Figures 3 and 4, we can conclude that the GD has
the best convergence and SGD3 has the worst convergence in all settings. This is because, although
all of the four algorithms use an unbiased estimate of the true gradient to update the solution at each
iteration, but the variance of each stochastic gradient is proportional to the size of the batch size B .
Therefore, for larger values of B , the variance of stochastic gradients is smaller and the algorithm convergences faster, but, for smaller values of B the algorithm suffers from high variance in stochastic
gradients and convergences slowly. We emphasize that this comparison holds for iteration complexity which is different from the computational complexity (running time) of individual iterations. More
specifically, each iteration of GD requires |ΩS | gradient computations, while for SGD we only need to
perform B ¿ |ΩS | gradient computations. In summary, SGD has lightweight iteration but requires more
iterations to converge. In contrast, GD takes expensive steps in much less number of iterations. From
Figures 3 and 4, it is noticeable that although a large number of iterations is usually needed to obtain a
solution of desirable accuracy using SGD, the lightweight computation per iteration makes SGD attractive for the optimization problem in (5) for large number of users. We also not that for the GD method,
the error is a monotonically decreasing function it terms of number of iterations t , but for the SGD

27

based methods this does not hold. This is because although SGD algorithm is guaranteed to converge
to an optimal solution (at least in expectation), but there is no guarantee that the stochastic gradients
provide a descent direction for the objective at each iteration due to the noise in computing gradients.
As a result, for few iterations we can see that the objective increases but finally it convergences as expected.

6 Conclusions and Future Works
In this paper, we have made a progress towards making distrust information beneficial in social recommendation problem. In particular, we have proposed a framework based on the matrix factorization
which is able to incorporate both trust and distrust relationships between users in factorization algorithm. We experimentally investigated the potential of distrust as a side information to overcome the
data sparsity and cold-start problems in traditional recommender systems. In summary, our results
showed that more accurate recommendations can be obtained by incorporating distrust relations, indicating that distrust information can indeed be beneficial for the recommendation process.
This work leaves few directions, both theoretically and empirically, as future work. From an empirical point of view, it would be interesting to extend our model for weighted social trust and distrust
relations. One challenge in this direction is that, as far as we know, there is no publicly available data set
that includes weighted (gradual) trust and distrust information. Also, the experimental results we have
conducted on the consistency of social relations with rating information hint at a number of potential
enhancements in future work. In particular, it would be interesting to further examine the correlation between implicit and explicit distrust information. An important challenge in this direction is to
develop better metrics to measure the implicit trust between users as the simple metrics such as Pearson correlation coefficient seem to be insufficient. Furthermore, since we only consider the distrust
between users, it would be easy to generalize our model in the same way to incorporate dissimilarity
between items and investigate how it works in practice. Also, our preliminary results indicated that
hinge loss almost performs better than the exponential loss, but from the optimization viewpoint, the
exponential loss is more attractive due to its smoothness. So, an interesting direction would be to use a
smoothed version of the hinge loss to gain from both optimization efficiency and algorithmic accuracy.

References
[1] Gediminas Adomavicius and Alexander Tuzhilin. Toward the next generation of recommender
systems: A survey of the state-of-the-art and possible extensions. IEEE Transactions on Knowledge
and Data Engineering, 17(6):734–749, 2005.
[2] Deepak Agarwal and Bee-Chung Chen. flda: matrix factorization through latent dirichlet allocation. In Proceedings of the third ACM international conference on Web search and data mining,
pages 91–100. ACM, 2010.
[3] Paolo Avesani, Paolo Massa, and Roberto Tiella. A trust-enhanced recommender system application: Moleskiing. In Proceedings of the 2005 ACM symposium on Applied computing, pages 1589–
1593, 2005.
[4] Giacomo Bachi, Michele Coscia, Anna Monreale, and Fosca Giannotti. Classifying trust/distrust
relationships in online social networks. In Privacy, Security, Risk and Trust (PASSAT), 2012 International Conference on and 2012 International Confernece on Social Computing (SocialCom), pages
552–557. IEEE, 2012.
[5] Jesús Bobadilla, Fernando Ortega, Antonio Hernando, and Abraham Gutiérrez. Recommender
systems survey. Knowledge-Based Systems, 46:109–132, 2013.
[6] John S Breese, David Heckerman, and Carl Kadie. Empirical analysis of predictive algorithms for
collaborative filtering. In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence, pages 43–52. Morgan Kaufmann Publishers Inc., 1998.

28

[7] Moira Burke and Robert Kraut. Mopping up: modeling wikipedia promotion decisions. In Proceedings of the 2008 ACM conference on Computer supported cooperative work, pages 27–36. ACM,
2008.
[8] Dorwin Cartwright and Frank Harary. Structural balance: a generalization of heider’s theory. Psychological review, 63(5):277, 1956.
[9] Gang Chen, Fei Wang, and Changshui Zhang. Collaborative filtering using orthogonal nonnegative
matrix tri-factorization. Information Processing & Management, 45(3):368–379, 2009.
[10] Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan. Better mini-batch algorithms via
accelerated gradient methods. In NIPS, volume 24, pages 1647–1655, 2011.
[11] David Crandall, Dan Cosley, Daniel Huttenlocher, Jon Kleinberg, and Siddharth Suri. Feedback
effects between similarity and social influence in online communities. In Proceedings of the 14th
ACM SIGKDD international conference on Knowledge discovery and data mining, pages 160–168.
ACM, 2008.
[12] Sanjoy Dasgupta, Michael L Littman, and David McAllester. Pac generalization bounds for cotraining. Advances in neural information processing systems, 1:375–382, 2002.
[13] Mukund Deshpande and George Karypis. Item-based top-n recommendation algorithms. ACM
Transactions on Information Systems (TOIS), 22(1):143–177, 2004.
[14] Thomas DuBois, Jennifer Golbeck, and Aravind Srinivasan. Predicting trust and distrust in social
networks. In Privacy, security, risk and trust (passat), 2011 ieee third international conference on
and 2011 ieee third international conference on social computing (socialcom), pages 418–424. IEEE,
2011.
[15] Rana Forsati, Hanieh Mohammadi Doustdar, Mehrnoush Shamsfard, Andisheh Keikha, and Mohammad Reza Meybodi. A fuzzy co-clustering approach for hybrid recommender systems. International Journal of Hybrid Intelligent Systems, 10(2):71–81, 2013.
[16] Rana Forsati and Mohammad Reza Meybodi. Effective page recommendation algorithms based on
distributed learning automata and weighted association rules. Expert Systems with Applications,
37(2):1316–1330, 2010.
[17] Jennifer Golbeck. Computing and applying trust in web-based social networks. PhD thesis, 2005.
[18] Jennifer Golbeck. Generating predictive movie recommendations from trust in social networks.
Springer, 2006.
[19] Jennifer Golbeck and James Hendler. Filmtrust: Movie recommendations using trust in web-based
social networks. In Proceedings of the IEEE Consumer communications and networking conference,
volume 96. Citeseer, 2006.
[20] Nathaniel Good, J Ben Schafer, Joseph A Konstan, Al Borchers, Badrul Sarwar, Jon Herlocker, and
John Riedl. Combining collaborative filtering with personal agents for better recommendations.
In AAAI/IAAI, pages 439–446, 1999.
[21] Quanquan Gu, Jie Zhou, and Chris Ding. Collaborative filtering: Weighted nonnegative matrix
factorization incorporating user and item graphs. In SIAM SDM, pages 199–210, 2010.
[22] R. Guha, Ravi Kumar, Prabhakar Raghavan, and Andrew Tomkins. Propagation of trust and distrust. In Proceedings of the 13th International Conference on World Wide Web, pages 403–412. ACM,
2004.
[23] Guibing Guo, Jie Zhang, Daniel Thalmann, Anirban Basu, and Neil Yorke-Smith. From ratings to
trust: an empirical study of implicit trust in recommender systems. In SAC, 2014.
[24] Jonathan L Herlocker, Joseph A Konstan, Al Borchers, and John Riedl. An algorithmic framework
for performing collaborative filtering. In Proceedings of the 22nd annual international ACM SIGIR
conference on Research and development in information retrieval, pages 230–237. ACM, 1999.
29

[25] Jonathan L Herlocker, Joseph A Konstan, Loren G Terveen, and John T Riedl. Evaluating collaborative filtering recommender systems. ACM Transactions on Information Systems (TOIS), 22(1):5–53,
2004.
[26] Thomas Hofmann. Latent semantic models for collaborative filtering. ACM Transactions on Information Systems (TOIS), 22(1):89–115, 2004.
[27] Mohsen Jamali and Martin Ester. Trustwalker: a random walk model for combining trust-based
and item-based recommendation. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 397–406. ACM, 2009.
[28] Mohsen Jamali and Martin Ester. A matrix factorization technique with trust propagation for recommendation in social networks. In Proceedings of the fourth ACM conference on Recommender
systems, pages 135–142. ACM, 2010.
[29] Mohsen Jamali and Martin Ester. A transitivity aware matrix factorization model for recommendation in social networks. In Proceedings of the Twenty-Second international joint conference on
Artificial Intelligence-Volume Volume Three, pages 2644–2649. AAAI Press, 2011.
[30] Arnd Kohrs and Bernard Merialdo. Clustering for collaborative filtering applications. In In Computational Intelligence for Modelling, Control & Automation. IOS. Citeseer, 1999.
[31] Ioannis Konstas, Vassilios Stathopoulos, and Joemon M Jose. On social networks and collaborative
recommendation. In Proceedings of the 32nd international ACM SIGIR conference on Research and
development in information retrieval, pages 195–202. ACM, 2009.
[32] Yehuda Koren. Factorization meets the neighborhood: a multifaceted collaborative filtering
model. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery
and data mining, pages 426–434. ACM, 2008.
[33] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender
systems. Computer, 42(8):30–37, 2009.
[34] Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. Predicting positive and negative links in
online social networks. In Proceedings of the 19th international conference on World wide web,
pages 641–650. ACM, 2010.
[35] Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. Signed networks in social media. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 1361–1370.
ACM, 2010.
[36] Wu-Jun Li and Dit-Yan Yeung. Relation regularized matrix factorization. IJCAI-09, 2009.
[37] Juntao Liu, Caihua Wu, and Wenyu Liu. Bayesian probabilistic matrix factorization with social
relations and item contents for recommendation. Decision Support Systems, 2013.
[38] Hao Ma. An experimental study on implicit social recommendation. In Proceedings of the 36th
international ACM SIGIR conference on Research and development in information retrieval, pages
73–82. ACM, 2013.
[39] Hao Ma, Irwin King, and Michael R Lyu. Learning to recommend with social trust ensemble. In
Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 203–210. ACM, 2009.
[40] Hao Ma, Michael R Lyu, and Irwin King. Learning to recommend with trust and distrust relationships. In Proceedings of the third ACM conference on Recommender systems, pages 189–196. ACM,
2009.
[41] Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin King. Sorec: social recommendation using probabilistic matrix factorization. In Proceedings of the 17th ACM conference on Information and knowledge management, pages 931–940. ACM, 2008.

30

[42] Hao Ma, Dengyong Zhou, Chao Liu, Michael R Lyu, and Irwin King. Recommender systems with
social regularization. In Proceedings of the fourth ACM international conference on Web search and
data mining, pages 287–296. ACM, 2011.
[43] Hao Ma, Tom Chao Zhou, Michael R Lyu, and Irwin King. Improving recommender systems by
incorporating social contextual information. ACM Transactions on Information Systems (TOIS),
29(2):9, 2011.
[44] Christopher D Manning, Prabhakar Raghavan, and Hinrich Schütze. Introduction to information
retrieval, volume 1. Cambridge university press Cambridge, 2008.
[45] Paolo Massa and Paolo Avesani. Trust-aware collaborative filtering for recommender systems. In
On the Move to Meaningful Internet Systems 2004: CoopIS, DOA, and ODBASE, pages 492–508.
Springer, 2004.
[46] Paolo Massa and Paolo Avesani. Controversial users demand local trust metrics: An experimental
study on epinions. com community. In Proceedings of the National Conference on artificial Intelligence, volume 20, page 121. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999,
2005.
[47] Paolo Massa and Paolo Avesani. Trust metrics in recommender systems. In Computing with Social
Trust, pages 259–285. Springer, 2009.
[48] Prem Melville, Raymond J Mooney, and Ramadass Nagarajan. Content-boosted collaborative filtering for improved recommendations. In AAAI/IAAI, pages 187–192, 2002.
[49] Bradley N Miller, Joseph A Konstan, and John Riedl. Pocketlens: Toward a personal recommender
system. ACM Transactions on Information Systems (TOIS), 22(3):437–476, 2004.
[50] Andriy Mnih and Ruslan Salakhutdinov. Probabilistic matrix factorization. In Advances in neural
information processing systems, pages 1257–1264, 2007.
[51] Uma Nalluri. Utility of distrust in online recommender systems. Technical report, 2008.
[52] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–
1609, 2009.
[53] Akshay Patil, Golnaz Ghasemiesfeh, Roozbeh Ebrahimi, and Jie Gao. Quantifying social influence
in epinions. HUMAN, 2(2):pp–67, 2013.
[54] Dmitry Pavlov and David M Pennock. A maximum entropy approach to collaborative filtering in
dynamic, sparse, high-dimensional domains. In NIPS, volume 2, pages 1441–1448, 2002.
[55] Michael J Pazzani. A framework for collaborative, content-based and demographic filtering. Artificial Intelligence Review, 13(5-6):393–408, 1999.
[56] David M Pennock, Eric Horvitz, Steve Lawrence, and C Lee Giles. Collaborative filtering by personality diagnosis: A hybrid memory-and model-based approach. In Proceedings of the Sixteenth
conference on Uncertainty in artificial intelligence, pages 473–480. Morgan Kaufmann Publishers
Inc., 2000.
[57] Jasson DM Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative prediction. In Proceedings of the 22nd international conference on Machine learning, pages
713–719. ACM, 2005.
[58] Ruslan Salakhutdinov and Andriy Mnih. Bayesian probabilistic matrix factorization using markov
chain monte carlo. In Proceedings of the 25th international conference on Machine learning, pages
880–887. ACM, 2008.
[59] Ruslan Salakhutdinov and Andriy Mnih. Probabilistic matrix factorization. Advances in neural
information processing systems, 20:1257–1264, 2008.
31

[60] Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. Restricted boltzmann machines for
collaborative filtering. In Proceedings of the 24th international conference on Machine learning,
pages 791–798. ACM, 2007.
[61] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. Item-based collaborative filtering
recommendation algorithms. In Proceedings of the 10th international conference on World Wide
Web, pages 285–295. ACM, 2001.
[62] Wanita Sherchan, Surya Nepal, and Cecile Paris. A survey of trust in social networks. ACM Computing Surveys (CSUR), 45(4):47, 2013.
[63] Luo Si and Rong Jin. Flexible mixture model for collaborative filtering. In ICML, volume 3, pages
704–711, 2003.
[64] Ian Soboroff and Charles Nicholas. Combining content and collaboration in text filtering. In Proceedings of the IJCAI, volume 99, pages 86–91, 1999.
[65] Nathan Srebro, Tommi Jaakkola, et al. Weighted low-rank approximations. In ICML, volume 3,
pages 720–727, 2003.
[66] Nathan Srebro, Jason DM Rennie, and Tommi Jaakkola. Maximum-margin matrix factorization.
Advances in neural information processing systems, 17(5):1329–1336, 2005.
[67] Mojdeh Talabeigi, Rana Forsati, and Mohammad Reza Meybodi. A hybrid web recommender system based on cellular learning automata. In Granular Computing (GrC), 2010 IEEE International
Conference on, pages 453–458. IEEE, 2010.
[68] Nele Verbiest, Chris Cornelis, Patricia Victor, and Enrique Herrera-Viedma. Trust and distrust aggregation enhanced with path length incorporation. Fuzzy Sets and Systems, 202:61–74, 2012.
[69] Patricia Victor, Chris Cornelis, Martine De Cock, and Ankur Teredesai. Trust- and distrust-based
recommendations for controversial reviews. IEEE Intelligent Systems, 26(1):48–55, 2011.
[70] Patricia Victor, Chris Cornelis, and Martine De Cock. Trust networks for recommender systems,
volume 4. Springer, 2011.
[71] Patricia Victor, Chris Cornelis, Martine De Cock, and Enrique Herrera-Viedma. Practical aggregation operators for gradual trust and distrust. Fuzzy Sets and Systems, 184(1):126–147, 2011.
[72] Patricia Victor, Nele Verbiest, Chris Cornelis, and Martine De Cock. Enhancing the trust-based
recommendation process with explicit distrust. ACM Transactions on the Web (TWEB), 7(2):6, 2013.
[73] Fei Wang, Sheng Ma, Liuzhong Yang, and Tao Li. Recommendation on item graphs. In Data Mining, 2006. ICDM’06. Sixth International Conference on, pages 1119–1123. IEEE, 2006.
[74] Jun Wang, Arjen P De Vries, and Marcel JT Reinders. Unifying user-based and item-based collaborative filtering approaches by similarity fusion. In Proceedings of the 29th annual international
ACM SIGIR conference on Research and development in information retrieval, pages 501–508. ACM,
2006.
[75] Grzegorz Wierzowiecki and Adam Wierzbicki. Efficient and correct trust propagation using
closelook. In Web Intelligence and Intelligent Agent Technology (WI-IAT), 2010 IEEE/WIC/ACM International Conference on, volume 1, pages 676–681. IEEE, 2010.
[76] Lei Wu, Steven CH Hoi, Rong Jin, Jianke Zhu, and Nenghai Yu. Distance metric learning from
uncertain side information with application to automated photo tagging. In Proceedings of the
17th ACM international conference on Multimedia, pages 135–144. ACM, 2009.
[77] Gui-Rong Xue, Chenxi Lin, Qiang Yang, WenSi Xi, Hua-Jun Zeng, Yong Yu, and Zheng Chen. Scalable collaborative filtering using cluster-based smoothing. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages
114–121. ACM, 2005.
32

[78] Kai Yu, Anton Schwaighofer, Volker Tresp, Xiaowei Xu, and H-P Kriegel. Probabilistic memorybased collaborative filtering. Knowledge and Data Engineering, IEEE Transactions on, 16(1):56–69,
2004.
[79] Sheng Zhang, Weihong Wang, James Ford, and Fillia Makedon. Learning from incomplete ratings
using non-negative matrix factorization. SIAM, 2006.
[80] Yi Zhang and Jonathan Koren. Efficient bayesian hierarchical user modeling for recommendation
system. In Proceedings of the 30th annual international ACM SIGIR conference on Research and
development in information retrieval, pages 47–54. ACM, 2007.
[81] Jianke Zhu, Hao Ma, Chun Chen, and Jiajun Bu. Social recommendation using low-rank semidefinite program. In Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011.
[82] Cai-Nicolas Ziegler. On propagating interpersonal trust in social networks. In Computing with
Social Trust, pages 133–168. Springer, 2009.
[83] Cai-Nicolas Ziegler and Jennifer Golbeck. Investigating interactions of trust and interest similarity.
Decision Support Systems, 43(2):460–475, 2007.
[84] Cai-Nicolas Ziegler and Georg Lausen. Propagation models for trust and distrust in social networks. Information Systems Frontiers, 7(4-5):337–358, 2005.

33

2
1.9

1.8

1.8

1.7

1.7

1.6

1.6

RMSE Error

RMSE Error

2
1.9

1.5
1.4
1.3

1.5
1.4
1.3

1.2

1.2
GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

1.1
1
0

GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

1.1
1
50

100
150
It e r a tio n N u m b e r

200

250

0

50

200

250

200

250

(b) 70% of Training Data

2

2

1.9

1.9

1.8

1.8

1.7

1.7

1.6

1.6

RMSE Error

RMSE Error

(a) 60% of Training Data

100
150
It e r a tio n N u m b e r

1.5
1.4
1.3

1.5
1.4
1.3

1.2

1.2
GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

1.1
1
0

GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

1.1
1
50

100
150
It e r a tio n N u m b e r

200

250

0

(c) 80% of Training Data

50

100
150
It e r a tio n N u m b e r

(d) 90% of Training Data

Figure 3: Comparison of accuracy of prediction in terms of RMSE with GD and SGD with three varied
batch sizes.

34

1

0.95

0.95

0.9

0.9
M A E Error

M A E Error

1

0.85

0.8

0.85

0.8

0.75

0.75

GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

0.7
0

GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

0.7
50

100
150
It e r a tio n N u m b e r

200

250

0

50

200

250

200

250

(b) 70% of Training Data

1

1

0.95

0.95

0.9

0.9
RMSE Error

RMSE Error

(a) 60% of Training Data

100
150
It e r a tio n N u m b e r

0.85

0.8

0.85

0.8

0.75

0.75

GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

0.7
0

GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

0.7
50

100
150
It e r a tio n N u m b e r

200

250

(c) 80% of Training Data

0

50

100
150
It e r a tio n N u m b e r

(d) 90% of Training Data

Figure 4: Comparison of accuracy of prediction in terms of MAE with GD and SGD with three varied
batch sizes.

35

Yuhan Sun

Mohamed Sarwat

CIDSE
Arizona State University
Tempe, AZ 85287-9309
Email: Yuhan.Sun.1@asu.edu

CIDSE
Arizona State University
Tempe, AZ 85287-9309
Email: msarwat@asu.edu

I. I NTRODUCTION
Graphs are widely used to model data in many application domains, including social networking, citation network
analysis, studying biological function of genes, and brain
simulation. A graph contains a set of vertices and a set of
edges that connect these vertices. Each graph vertex or edge
may possess a set of properties (aka. attributes). Thanks to the
wide spread use of GPS-enabled devices, many applications
assign a spatial attribute to a vertex (e.g., geo-tagged social
media). Figure 1 depicts an example of a social graph that
has two types of vertices: Person and Venue and two types
of edges: Follow and Like. Vertices with type Person
have two properties (i.e., attributes): name and age. Vertices
with type Venue have two properties: name and spatial
location. A spatial location attribute represents the spatial
location of the entity (i.e., Venue) represented by such vertex.
In Figure 1, vertices {e, f, g, h, i} are spatial vertices which
represent venues.

d

w

llo

Fo

Person

c

Follow

b

a
Like

Abstract—Graphs are widely used to model data in many
application domains. Thanks to the wide spread use of GPSenabled devices, many applications assign a spatial attribute
to graph vertices (e.g., geo-tagged social media). Users may
issue a Reachability Query with Spatial Range Predicate (abbr.
RangeReach). RangeReach finds whether an input vertex
can reach any spatial vertex that lies within an input spatial
range. An example of a RangeReach query is: Given a social
graph, find whether Alice can reach any of the venues located
within the geographical area of Arizona State University. The
paper proposes G EO R EACH an approach that adds spatial data
awareness to a graph database management system (GDBMS).
G EO R EACH allows efficient execution of RangeReach queries,
yet without compromising a lot on the overall system scalability
(measured in terms of storage size and initialization/maintenance
time). To achieve that, G EO R EACH is equipped with a lightweight data structure, namely SPA-Graph, that augments the
underlying graph data with spatial indexing directories. When a
RangeReach query is issued, the system employs a prunedgraph traversal approach. Experiments based on real system
implementation inside Neo4j proves that G EO R EACH exhibits
up to two orders of magnitude better query response time and
up to four times less storage than the state-of-the-art spatial and
reachability indexing approaches.

l

Like

arXiv:1603.05355v1 [cs.DB] 17 Mar 2016

GeoReach: An Efficient Approach for Evaluating
Graph Reachability Queries with Spatial Range
Predicates

j

h

k

a: {name: Alice, age: 19}
b: {name: Dan, age: 20}
c: {name: Carol, age: 35}
d: {name: Bob, age: 25}
j: {name: Kate, age: 18}
k: {name: Mat, age: 23}
l: {name: Katharine, age:21}

g
e

i
f

P
R

Venue
e: {name: Pita Jungle}
f :{name: Chipotle}
g: {name: Sushi 101}
h: {name: Subway}
i: {name: McDonald's}

Fig. 1: Location-Aware Social Graph

Graph Database Management Systems (GDBMSs) emerged
as a prominent NoSQL approach to store, query, and analyze
graph data [15], [8], [25], [24], [28]. Using a GDBMS,
users can pose reachability analysis queries like: (i) Find out
whether two vertices in the graph are reachable, e.g., Are Alice
(vertex a) and Katharine (vertex l) reachable in the social
graph given in Figure 1. (ii) Search for graph paths that match
a given regular language expression representing predicates on
graph elements, e.g., Find all venues that Alice’s Followees
and/or her Followees’ Followees also liked. Similarly, users
may issue a Reachability Query with Spatial Range Predicate
(abbr. RangeReach). A RangeReach query takes as input a
graph vertex v and a spatial range R and returns true only if v
can reach any spatial vertex (that possesses a spatial attribute)
which lies within the extent of R (formal definition is given in
Section II). An example of a RangeReach query is: Find out
whether Alice can reach any of the Venues located within the
geographical area of Arizona State University (depicted as a
dotted red rectangle R in Figure 1). As given in Figure 1, The
answer to this query is true since Alice can reach Sushi 101
(vertex g) which is located within R. Another query example
is to find out whether Katharine can reach any of the venues
located within R. The answer to this query is false due to the

fact that the only venue reachable from Katharine, Subway
(vertex h), is not located within R.
There are several straightforward approaches to execute
a RangeReach query: (1) Traversal Approach: The naive
approach traverses the graph, checks whether each visited
vertex is a spatial vertex and returns true as the answer if
the vertex’s spatial attribute lies within the input query range
R. This approach yields no storage/maintenance overhead
since no pre-computed data structure is maintained. However,
the Traversal approach may lead to high query response
time since the algorithm may traverse the whole graph to
answer the query. (2) Transitive Closure (TC) Approach: this
approach leverages the pre-computed transitive closure [27]
of the graph to retrieve all vertices that are reachable from v
and returns true if at least one spatial vertex (located in the
spatial range R) that is reachable from v. The TC approach
achieves the lowest query response time, however it needs
to pre-compute (and maintain) the graph transitive closure
which is deemed notoriously infeasible especially for largescale graphs. (3) Spatial-Reachability Indexing (SpaReach)
Approach: uses a spatial index [3], [22] to locate all spatial
vertices VR that lie within the spatial range R and then uses
a reachability index [35] to find out whether v can reach any
vertex in VR . SpaReach achieves better query response time
than the Traversal approach but it still needs to necessarily
probe the reachability index for spatial vertices that may never
be reached from the v. Moreover, SpaReach has to store and
maintain two index structures which may preclude the system
scalability.
In this paper, we propose G EO R EACH, a scalable and
time-efficient approach that answers graph reachability queries
with spatial range predicates (RangeReach). G EO R EACH is
equipped with a light-weight data structure, namely SPAGraph, that augments the underlying graph data with spatial
indexing directories. When a RangeReach query is issued,
the system employs a pruned-graph traversal approach. As
opposed to the SpaReach approach, G EO R EACH leverages
the Spa-Graph’ s auxiliary spatial indexing information to
alternate between spatial filtering and graph traversal and early
prunes those graph paths that are guaranteed: (a) not to reach
any spatial vertex or (b) to only reach spatial vertices that
outside the input spatial range query. As opposed to the TC
and SpaReach approaches, G EO R EACH decides the amount of
spatial indexing entries (attached to the graph) that strikes a
balance between query processing efficiency on one hand and
scalability (in terms of storage overhead) on the other hand. In
summary, the main contributions of this paper are as follows:
• To the best of the authors’ knowledge, the paper is the
first that formally motivates and defines RangeReach,
a novel graph query that enriches classic graph reachability analysis queries with spatial range predicates.
RangeReach finds out whether an input graph vertex can
reach any spatial vertex that lies within an input spatial
range.
• The paper proposes G EO R EACH a generic approach that
adds spatial data awareness to an existing GDBMS.

Notation
G = {V, E}
Vvout
Vvin
RF (v)
VS
RFS (v)
n
m
v1 ❀ v2
MBR(P )

Description
A graph G with a set of vertices V and set of edges E
The set of vertices that can be reached via a direct edge
from a vertex v
The set of vertices that can reach (via a direct edge) vertex
v
The set of vertices that are reachable from (via any number
of edges) vertex v
The set of spatial vertices in G such that VS ⊆ V
The set of spatial vertices that are reachable from (via any
number of edges) vertex v
The cardinality of V (n = |V |); the number of vertices in
G
The cardinality of E (m = |E|); the number of edges in G
v2 is reachable from v1 via connected path in G (such that
both v1 and v2 ∈ V )
Minimum bounding rectangle of a set of spatial polygons
P (e.g., points, rectangles)

TABLE I: Notations.

G EO R EACH allows efficient execution of RangeReach
queries issued on a GDBMS, yet without compromising
a lot on the overall system scalability (measured in terms
of storage size and initialization/maintenance time).
1
• The paper experimentally evaluates G EO R EACH
using
real graph datasets based on a system implementation
inside Neo4j (an open source graph database system).
The experiments show that G EO R EACH exhibits up to
two orders of magnitude better query response time and
occupies up to four times less storage than the state-ofthe-art spatial and reachability indexing approaches.
The rest of the paper is organized as follows: Section II lays
out the preliminary background and related work. The SPAGraph data structure, G EO R EACH query processing, initialization and maintenance algorithms are explained in Sections III
to V. Section VI experimentally evaluates the performance of
G EO R EACH. Finally, Section VII concludes the paper.
II. P RELIMINARIES

AND

BACKGROUND

This section highlights the necessary background and related research work. Table I summarizes the main notations in
the paper.
A. Preliminaries
Graph Data. G EO R EACH deals with a directed property
graph G = (V, E) where (1) V is a set of vertices such that
each vertex has a set of properties (attributes) and (2) E is a set
of edges in which every edge can be represented as a tuple of
two vertices v1 and v2 (v1 , v2 ∈ V ). The set of spatial vertices
VS ⊆ V such that each v ∈ VS has a spatial attribute (property)
v.spatial. The spatial attribute v.spatial may be a geometrical
point, rectangle, or a polygon. For ease of presentation, we
assume that a spatial attribute of spatial vertex is represented
by a point. Figure 1 depicts an example of a directed property
graph. Spatial Vertices VS are represented by black colored
circles and are located in a two-dimensional planer space while
white colored circles represent regular vertices that do not
1 https://github.com/DataSystemsLab/GeoGraphDB–Neo4j

possess a spatial attribute. Arrows indicate directions of edges
in the graph.
Graph Reachability (v1 ❀ v2 ). Given two vertices v1 and v2
in a graph G, v1 can reach v2 (v1 ❀ v2 ) or in other words v2
is reachable from v1 if and only if there is at least one graph
path from v1 to v2 . For example, in Figure 1, vertex a can
reach vertex f through the graph path a->c->i->f so it can
be represented as a ❀ f . On the other hand, c cannot reach
h.
Reachability with Spatial Range Predicate (RangeReach).
RangeReach queries find whether a graph vertex can reach a
specific spatial region (range) R. Given a vertex v ∈ V in a
Graph G and a spatial range R, RangeReach can be described
as follows:

RangeReach(v, R) =


true













f alse

if ∃ v ′ such that
(1) v ′ ∈ VS
(2) v ′ .spatial lies within R
(3) v ❀ v ′
Otherwise.
(1)

As given in Equation 1, if any spatial vertex v ′ ∈ VS that
lies within the extent of the spatial range R is reachable from
the input vertex v, then RangeReach(v, R) returns true (i.e.,
v ❀ R). For example, in Figure 1, RangeReach(a, R) = true
since a can reach at least one spatial vertex f in R. However,
RangeReach(l, R) = false since l can merely reach a spatial
vertex h which is not located in R. Vertex d cannot reach R
since it cannot reach any vertex.
B. Related Work
This section presents previous work on reachability indexes,
spatial indexes, and straightforward solutions to processing
graph reachability queries with spatial range predicates (RangeReach).
Reachability Index. Existing solutions to processing graph
reachability queries (u ❀ v) can be divided into three
categories [35]: (1) Pruned Graph Traversal [6], [30], [34]:
These approaches pre-compute some auxiliary reachability
information offline. When a query is issued, the query processing algorithm traverses the graph using a classic traversal
algorithm, e.g., Depth First Search (DFS) or Breadth First
Search (BFS), and leverages the pre-computed reachability
information to prune the search space. (2) Transitive closure
retrieval [1], [7], [17], [18], [27], [31], [32]: this approach
pre-computes the transitive closure of a graph offline and compresses it to reduce its storage footprint. When a query u ❀ v
is posed, the transitive closure of the source vertex u is fetched
and decomposed. Then the query processing algorithm checks
whether the terminal vertex v lies in the transitive closure of u.
and (3) Two-Hop label matching [5], [8], [10], [11], [12], [26]:
The two-hop label matching approach assigns each vertex v in
the graph an out-label set Lout (v) and an in-label set Lin (v).
When a reachability query is answered, the algorithm decides
that u ❀ v if and only if Lout (v) ∩ Lin (v) 6= ∅. Since the

two label sets do not contain all in and out vertices, size of
the reachability index reduces.
Spatial Index. A spatial index [21], [23], [29] is used
for efficient retrieval of either multi-dimensional objects (e.g.,
hx,yi coordinates of an object location) or objects with spatial
extents, e.g., polygon areas represented by their minimum
boundary rectangles (MBR). Spatial index structures can be
broadly classified to hierarchical (i.e., tree-based) and nonhierarchical index structures. Hierarchical tree-based spatial
index structures can be classified into another two broad
categories: (a) the class of data-partitioning trees, also known
as the class of Grow-and-Post trees [20], which refers to the
class of hierarchical data structures that basically extend the
B-tree index structure [2], [13] to support multi-dimensional
and spatial objects. The main idea is to recursively partition
the spatial data based on a spatial proximity clustering, which
means that the spatial clusters may overlap. Examples of
spatial index structures in this category include R-tree [16] and
R*-tree [3]. (b) the class of space-partitioning trees that refers
to the class of hierarchical data structures that recursively
decomposes the space into disjoint partitions. Examples of
spatial index structures in this category include the Quadtree [14] and k-d tree [4].
Spatial Data in Graphs. Some existing graph database
systems, e.g., Neo4j, allow users to define spatial properties on
graph elements. However, these systems do not provide native
support for RangeReach queries. Hence, users need to create
both a spatial index and a reachability index to efficiently
answer a RangeReach queries (drawbacks of this approach
are given in the following section). On the other hand, existing
research work [19] extends the RDF data with spatial data to
support RDF queries with spatial predicates (including range
and spatial join). However, such technique is limited to RDF
and not general graph databases. It also does not provide an
efficient solution to handle reachability queries.
C. Straightforward Solutions
There are three main straightforward approaches to process
a RangeReach query, described as follows:
Approach I: Graph Traversal. This approach executes
a spatial reachability query using a classical graph traversal
algorithm like DFS (Depth First Search) or BFS (Breadth
First Search). When RangeReach(v, R) is invoked, the system
traverses the graph from the starting vertex v. For each visited
vertex, the algorithm checks whether it is a spatial vertex and
returns true as the query answer if the vertex’s location lies
within the input query range R because the requirement of
spatial reachability is satisfied and hence v ❀ R. Otherwise,
the algorithm keeps traversing the graph. If all vertices that v
can reach do not lie in R, that means v cannot reach R.
Approach II: Transitive Closure (TC). This approach precomputes the transitive closure of the graph and stores it as an
adjacency matrix in the database. Transitive closure of a graph
stores the connectivity component of the graph which can be
used to answer reachability query in constant time. Since the
final result will be determined by spatial vertices, only spatial

vertices are stored. When RangeReach(v, R) is invoked, the
system retrieves all spatial vertices that are reachable from v
by means of the transitive closure. The system then returns
true if at least one spatial vertex that is reachable from v is
also located in the spatial range R.
Approach III: SpaReach. This approach constructs two
indexes a-priori: (1) A Spatial Index: that indexes all spatial
vertices in the graph and (2) A Reachability Index: that indexes
the reachability information of all vertices in the graph. When
a RangeReach query is issued, the system first takes advantage
of the spatial index to locate all spatial vertices VR that
lie within the spatial range R. For each vertex v ′ ∈ VR ,
a reachability query against the reachability index is issued
to test whether v can reach v ′ . For example, to answer
RangeReach(a, R) in Figure 2, spatial index is exploited first
to retrieve all spatial vertices that are located in R. From the
range query result, it can be known that g, i and f are located
in rectangle R. Then graph reachability index is accessed to
determine whether a can reach any located-in vertex. Hence, it
is obvious RangeReach(a, R) = true by using this approach.
Critique. The Graph Traversal approach yields no storage/maintenance overhead since no pre-computed data structure is maintained. However, the traversal approach may lead
to high query response time (O(m) where m is the number
of edges in the graph) since the algorithm may traverse the
whole graph to answer the query. The TC approach needs
to pre-compute (and maintain) the graph transitive closure
which is deemed notoriously infeasible especially for largescale graphs. The transitive closure computation is O(kn3 ) or
O(nm) and the TC storage overhead is O(kn2 ) where n is
total number of vertices and k is the ratio of spatial vertices
to the total number of vertices in the graph. To answer a
RangeReach query, the TC approach takes O(kn) time since it
checks whether each reachable spatial vertex in the transitive
closure is located within the query rectangle. On the other
hand, SpaReach builds a reachability index, which is a timeconsuming step, in O(n3 ) [32] time. The storage overhead
of a spatial index is O(n) and that of a reachability index
is O(nm1/2 ). To store the two indices, the overall storage
overhead is O(nm1/2 ). Storage cost of this approach is far less
than TC approach but still not small enough to accommodate
large-scale graphs. The query time complexity of a spatial
index is O(kn) while that of reachability index is m1/2 . But
for a graph reachability query, checking is demanded for each
spatial vertex in the result set generated by the range query.
Hence, cost of second step reachability query is O(knm1/2 ).
The total cost should be O(knm1/2 ). Query performance
of Spa-Reach is highly impacted by the size of the query
rectangle since the query rectangle determines how many
spatial vertices are located in the region. In Figure 1, query
rectangle R overlaps with three spatial vertices. For example,
to answer RangeReach(l, R), all three vertices {f, g, i} will
be checked against the reachability index to decide whether
any of them is reachable from l and in fact neither of them
is reachable. In a large graph, a query rectangle will possibly
contain a large number of vertices. That will definitely lead to

d
c
b

a
l

1

2
h

5

4

3

RMBR(j)

6

8

7 g

L0

e

13

Q

10

9
14

f

12

11

15

i

16

17
19

B-vertex:
a: true
d: false
f: false
h: false
k: false

k

j

18

L1

20

21

R-vertex:
j: RMBR(j)
G-vertex:
b: {2, 19}
c: {12, 14}
e: {14}
g: {12, 14}
i: {14}
l: {2}

L2

Fig. 2: SPA-Graph Overview

high unreasonable high query response time.
III. O UR A PPROACH : G EO R EACH
In this section, we give an overview of G EO R EACH an efficient and scalable approach for executing graph reachability
queries with spatial range predicates.
A. Data Structure
In this section, we explain how G EO R EACH augments a
graph structure with spatial indexing entries to form what we
call SPatially-Augmented Graph (SPA-Graph). To be generic,
G EO R EACHstores the newly added spatial indexing entries
the same way other properties are stored in a graph database
system. The structure of a SPA-Graph is similar to that of
the original graph except that each vertex v ∈ V in a SPAGraph G = {V, E} stores spatial reachability information. A
SPA-Graph has three different types of vertices, described as
follows:
•

•

B-Vertex: a B-Vertex v (v ∈ V ) stores an extra bit (i.e.,
boolean), called Spatial Reachability Bit (abbr. GeoB)
that determines whether v can reach any spatial vertex
(u ∈ VS ) in the graph. GeoB of a vertex v is set to 1
(i.e., true) in case v can reach at least one spatial vertex
in the graph and reset to 0 (i.e., false) otherwise.
R-Vertex: an R-Vertex v (v ∈ V ) stores an additional attribute, namely Reachability Minimum Bounding
Rectangle (abbr. RMBR(v)). RMBR(v) represents the
minimum bounding rectangle MBR(S) (represented by
a top-left and a lower-right corner point) that encloses all
spatial polygons which represent all spatial vertices S that
are reachable from vertex v (RMBR(v) = MBR(RFS (v)),
RFS (v) = {u|v ❀ u, u ∈ VS }).

G-Vertex: a G-Vertex v stores a list of spatial grid cells,
called the reachability grid list (abbr. ReachGrid(v)). Each
grid cell C in ReachGrid(v) belongs to a hierarchical grid
data structure that splits the total physical space into n
spatial grid cells. Each spatial vertex u ∈ VS will be
assigned a unique cell ID (k ∈ [1, n]) in case u is located
within the extents of cell k, noted as Grid(u) = k. Each
cell C ∈ ReachGrid(v) contains at least one spatial vertex
that is reachable from v (ReachGrid(v) = ∪ Grid(u),
{u|v ❀ u, u ∈ VS }).
Lemma 3.1: Let v (v ∈ V ) be a vertex in a SPA-Graph
G = {V, E} and Vvout be the set of vertices that can be
reached via a direct edge from a vertex v. The reachability
minimum bounding rectangle of v (RMBR(v)) is equivalent
to the minimum bounding rectangle that encloses all its outedge neighbors Vvout and their reachability minimum bounding
rectangles. RMBR(v) = MBRv′ ∈Vvout (RMBR(v ′ ), v ′ .spatial).
Proof: Based on the reachability definition, the set of
reachable vertices RF (v) from a vertex v is equal to the union
of the set of vertices that is reached from v via a direct edge
′
(Vvout ) and all vertices that are reached from each vertex v ∈
Vvout . Hence, the set (RFS (v)) of reachable spatial vertices
from v is given in Equation 2.
•

RFS (v) =
′

[

′

(v ′ ∪ RFS (v ))

(2)

v ∈Vvout

And since RMBR(v) = MBR(RFS (v)), then the the reachability minimum bounding rectangle of v is as follows:
RM BR(v) = M BR(
′

[

′

(v ′ ∪ RFS (v ))))
(3)

v ∈Vvout
′

′

= M BRv′ ∈Vvout (RM BR(v ), v .spatial)

That concludes the proof.
Lemma 3.2: The set of reachable spatial grid cells from a
given vertex v is equal to the union of all spatial grid cells
reached from its all its out-edge neighbors and grid cells that
contain the spatial neighbors
ReachGrid(v) =

[

(ReachGrid(v ′ ) ∪ Grid(v ′ ))

(4)

v ′ ∈Vvout

Proof: Similar to that of Lemma III-A.
Example. Figure 2 gives an example of a SPA-Graph. GeoB
of vertex b is set to 1 (true) since b can reach three spatial
vertices e, f and h. GeoB for d is 0 since d cannot reach any
spatial vertex in the graph. Figure 2 also gives an example of a
Reachability Minimum Bounding Rectangle RMBR of vertex
j (i.e., RMBR(j)). All reachable spatial vertices from j are g,
i, h and f . Figure 2 also depicts an example of ReachGrid.
There are three layers of grids, denoted as L0 , L1 , L2 from
top to bottom. The uppermost layer L0 is split into 4 × 4
grid cells; each cell is assigned a unique id from 1 to 16.
We denote grid cell with id 1 as G1 for brevity. The middle
layer gird L1 is split into four cells G17 to G20 . Each cell
in L1 covers four times larger space than each cell in L0 .
G17 in L1 covers exactly the same area of G1 , G2 , G5 , G6

Algorithm 1 Reachability Query with Spatial Range Predicate
1: Function R ANGER EACH(v, R)
2: if v is a spatial vertex and v.spatial Lie In R then return true
3: Terminate ← true
4: if v is a B-vertex then
5:
if GeoB(v) = true then Terminate ← false
6: else if v is a R-vertex then
7:
if R full contains RMBR(v) then return true
8:
if R no overlap with RMBR(v) then return false
9:
Terminate ← false
10: else if v is a G-vertex then
11:
for each grid Gi ∈ ReachGrid(v) do
12:
if R fully contains Gi then return true
13:
Gi partially overlaps with R then Terminate ← false
14: if Terminate = false then
15:
for each vertex v ′ ∈ Vvout do
16:
if R ANGER EACH(v ′ , R) = true then return true
17: return false

in L0 . The bottom layer L2 contains only a single grid cell
which covers all four grids in L1 and represents the whole
physical space. All spatial vertices reachable from vertex a
are located in G2 , G7 , G9 , G12 and G14 , respectively. Hence,
ReachGrid(a) can be {2, 7, 9, 12, 14}. Notice that vertex e
and f are both located in G9 and G14 covered by G19 in
ReachGrid(a) can be replaced by G19 . Then, ReachGrid(a)
= {2, 7, 12, 19}. In fact, there exist more options to represent
ReachGrid(a), such as {17, 18, 19, 20} or {21} by merging
into only a single grid cell in L2 . When we look into
ReachGrid of connected vertices, for instance g, ReachGrid(g)
is {12, 14} and ReachGrid(i) is {14}. It is easy to verify that
ReachGrid(g) is ReachGrid(i)∪Grid(i.spatial), which accords
with lemma 3.2.
SPA-Graph Intuition. The main idea behind the SPAGraph is to leverage the spatial reachability bit, reachability
minimum bounding rectangle and reachability grid list stored
in a B-Vertex, R-Vertex or a G-Vertex to prune graph paths
that are guaranteed (or not) to satisfy both the spatial range
predicate and the reachability condition. That way, G EO R E ACH cuts down the number of traversed graph vertices and
edges and hence significantly reduce the overall latency of a
RangeReach query.
B. Query Processing
This section explains the RangeReach query processing
algorithm. The main objective is to visit as less graph vertices
and edges as possible to reduce the overall query latency.
The query processing algorithm accelerates the SPA-Graph
traversal procedure by pruning those graph paths that are
guaranteed (or not) to satisfy the spatial reachability constraint.
Algorithm 1 gives pseudocode for query processing. The
algorithm takes as input a graph vertex v and query rectangle
R. It then starts traversing the graph starting from v. For
each visited vertex v, three cases might happen, explained as
follows:
Case I (B-vertex): In case GeoB is false, a B-vertex
cannot reach any spatial vertex and hence the algorithm stops
traversing all graph paths after this vertex. Otherwise, further
traversal from current B-vertex is required when GeoB value
is true. Line 4 to 5 in algorithm 1 is for processing such case.

RMBR

A

R

I

A
I

RMBR

Query Rectangle

RMBR
R

Query Rectangle

(a) No Overlap

(x1,y1)

(x1,y1)

B

B
e

RMBR

(b) Lie In

(x2,y2)

(x2,y2)

e
f

RMBR

Q

(a)

Query
Rectangle

(b)

A
I

RMBR

f

Q

Query Rectangle

(c) Partially Covered By

R
(x1,y1)

B

RMBR

e
(x2,y2)

Fig. 3: Relationships between RMBR and a query rectangle
f

Q

Case II (R-vertex): For a visited R-vertex u, there are three
conditions that may happen (see figure 3). They are the case
from line 6 to 9 in algorithm 1:
• Case II.A: RMBR(u) lies within the query rectangle (see
Figure 3b). In such case, the algorithm terminates and
returns true as the answer to the query since there must
exist at least a spatial vertex that is reachable from v.
• Case II.B: The spatial query region R does not overlap
with RMBR(u) (see Figure 3a). Since all reachable spatial
vertices of u must lie inside RMBR(u), there is no
reachable vertex can be located in the query rectangle.
As a result, graph paths originating at u can be pruned.
• Case III.C: RMBR(u) is partially covered by the query
rectangle (see Figure 3c). In this case, the algorithm keeps
traversing the graph by fetching the set of vertices Vvout
that can be reached via a direct edge from v.
Case III (G-vertex): For a G-vertex u, it store many
reachable grids from u. Actually, it can be regarded as many
smaller RMBRs. So three cases may also happen. Algorithm 1
line 13 to 18 is for such case. Three cases will happen are
explained as follows:
• Case III.A: The query rectangle R fully contains any
grid cell in ReachGrid(u). In such case, the algorithms
terminates and returns true as the query answer.
• Case III.B: The query rectangle have no overlap with all
grids in ReachGrid(u). This case means that v cannot
reach any grids overlapped with R. Then we never
traverse from v and this search branch is pruned.
• Case III.C: If the query rectangle fully contains none
of the reachable grid and partially overlap with any
reachable grid, it corresponds to Partially Covered By
case for RMBR. So further traversal is performed.
Figure 2 gives an example of RangeReach that finds
whether vertex a can reach query rectangle Q (the shaded one
in figure 2). At the beginning of the traversal, the algorithm
checks the category of a. In case, It is a B-vertex and its GeoB
value is true, the algorithm recursively traverses out-edge
neighbors of a and perform recursive checking. Therefore,
the algorithm retrieves vertices b, c, d and j. For vertex b,

(c)

Fig. 4: R-vertex Pruning Power
it is a G-vertex and its reachable grids are G2 and G19 . G19
cover the range of four grids in L0 . They are G9 , G10 , G13
and G14 . The spatial range is merely partially covered by
Q (Case III.C), hence it is possible for b to reach Q. We
cannot make an assured decision in this step so b is recorded
for future traversal. Another neighbor is c. ReachGrid(c) is
{12, 14} which means that G12 and G14 are reachable from
c. G14 lies in Q (Case III.A). In such case, since a ❀ c, we
can conclude that a ❀ R. The algorithm then halts the graph
traversal at this step and returns true as the query answer.
IV. SPA-G RAPH A NALYSIS
This section analyzes each SPA-Graph vertex type rom two
perspectives: (1) Storage Overhead: the amount of storage
overhead that each vertex type adds to the system (2) Pruning
Power: the probability that the query processing algorithm
terminates when a vertex of such type is visited during the
graph traversal.
B-vertex. When visiting a B-Vertex, in case GeoB is false,
the query processing algorithm prunes all subsequent graph
paths originated at such vertex. That is due to the fact that such
vertex cannot reach any spatial vertex in the graph. Otherwise,
the query processing algorithm continues traversing the graph.
As a result, pruned power of a B-vertex lies in the condition
that GeoB is false. For a given graph, number of vertices that
can reach any space is a certain value. So probability that a
vertex can reach any spatial vertex is denoted as Ptrue . This is
also the probability of a B-vertex whose GeoB value is true.
Probability of a B-vertex whose GeoB value is false, denoted
as Pf alse , will be 1 − Ptrue . To sum up, pruned power of a
B-vertex is 1 − Ptrue or Pf alse
R-vertex. When an R-vertex is visited, the condition
whether the vertex can reach any space still exists. If the
R-vertex cannot reach any space, we assign the R-vertex a
specific value to represent it(e.g. set coordinates of RMBR’s

bottom-left point bigger than that of the top-right point). In
this case, pruned power of a R-vertex will be the same with
a B-vertex, which is Pf alse . Otherwise, when the R-vertex
can reach some space, it will be more complex. Because
information of RMBR and query rectangle R have some
impact on the pruned power of this R-vertex. The algorithm
stops traversing the graph in both the No Overlap and Lie
In cases depicted in Figures 3a and 3b. Figure 4 shows the
two cases that R-vertex will stop the traversal. In Figure 4,
width and height of the total 2D space are denoted as A and
B. Assume that the query rectangle can be located anywhere
in the space with equal probability. We use (x1 , y1 ) and
(x2 , y2 ) to represent the RMBR’s top-left corner and lowerright point coordinates, respectively. Then all possible areas
where top-left vertex of query rectangle Q should be part
of the total space, denoted as I (see the shadowed area in
the figure. Its area is determined by size of query rectangle.
Denote width and height of Q are e and f , then area of I,
AI = (A − e) × (B − f ).
First, we estimate probability of No Overlap case. Figure 4a
shows one case of No Overlap. If the query rectangle Q do
not overlap with RMBR, top-left vertex of Q must lie outside
rectangle R which is forms the overlap region (drawn with
solid line in Figure 4b). Area of R (denoted as AR ) is obviously determined by the RMBR location and size of Q. It can
be easily observed that AR = (x2 −(x1 −e))×(y2 −(y1 −f )).
Another possible case is demonstrated in Figure 4b. In such
case, if we calculate R in the same way, range of R will
exceeds area of I which contains all possible locations. As a
result, AR = AI in this case. As we can see, area of overlap
region is determined by the range of R and I altogether.
Then we can have a general representation of the overlap area
AOverlap = (min(A − e, x2 ) − max(0, x1 − e)) × (min(B −
f, y2 )−max(0, x2 −f ). The No Overlap area is AI −AOverlap
and the probability of having a No Overlap case is calculated
as follows:
PN oOverlap =

AOverlap
AI − AOverlap
=1−
.
AI
AI

(5)

Figure 4c depicts the Lie In case. When top-left vertex of
Q lies in region R, then such Lie In case will happen. To
ensure that R exists, it is necessary that e > (x2 − x1 ) and
f > (y2 −y1 ). If it is not, then probability of such case must be
0. If this requirement is satisfied, then AR = (x1 − (x2 − e))×
(y1 − (y2 − f )). Recall what is met in the above-mentioned
case, R may exceed the area of I. Similarly, more general
area should be AR = (min(A − e, x1 ) − max(0, x1 − (x2 −
e))) × (min(B − f, y1 ) − max(0, y1 − (y2 − f ))). Probability
R
of such case should be A
AI . To sum up, we have
PLieIn =

(

AR
AI

0

e > (x2 − x1 ) and f > (y2 − y1 )
else

(6)

After we sum up all conditional probabilities based on
Ptrue and Pf alse , pruning power of an R-vertex is equal to

Algorithm 2 G EO R EACH Initialization Algorithm
1:
2:
3:
4:
5:
6:
7:
8:
9:

Function INITIALIZE(Graph G = {V, E})
/*PHASE I: SPA-Graph Vertex Initialization */
for each Vertex v ∈ V according their sequence in topology do
InitializeVertex(G, v, MAX_REACH_GRIDS, MAX_RMBR)
/* PHASE II: Reachable Grid Cells Merging */
for each G-vertex v do
for each layer Li from L1 to Lbottom do
for each grid cell Gi in Li do
if Number of reachable grids in corresponding region in Li−1 is larger
than MERGE_COUNT then
10:
Add Gi in Li into ReachGrid(v)
11:
Remove reachable grid cells that are covered by Gi in higher layers

(PN oOverlap +PLieIn )×Ptrue +Pf alse . Evidently, the pruning
power of an R-vertex is more powerful than a B-vertex. When
the storage overhead of an R-vertex is considered, coordinates
of RMBR’s top-left and lower-right vertices should be stored.
Thus its storage will be at least four bytes depending on the
spatial data precision. That means the storage overhead of a
G-Vertex is always higher than that of a B-Vertex.
G-vertex. For a high resolution grid, it is of no doubt that a
G-vertex possesses a high pruning power. However, this comes
at the cost of higher storage overhead because more grid cells
occupies more space. When a G-vertex is compared with an
R-vertex, the area of an R-vertex is much larger than a grid. In
this case, an R-vertex can be seen as a a simplified G-vertex
for which the grid cell size is equal to that of RMBR. One
extreme case of R-vertex is that the vertex can reach only one
spatial vertex. In such case, RMBR is location of the reachable
spatial vertex. Such R-vertex can still be counted as a G-vertex
whose grid size x → 0. According the rule, it should be with
higher storage overhead and more accuracy. Actually, storing it
as a G-vertex will cost an integer while any R-vertex requires
storage for four float or even double number.
V. I NITIALIZATION & M AINTENANCE
This section describes the SPA-Graph initialization algorithm. The G EO R EACH initialization algorithm (Pseudocode
is given in Algorithm 2) takes as input a graph Graph
G = {V, E} and runs in two main phases: (1) Phase I:
SPA-Graph Vertex Type Initialization: this phase leverages the
tradeoff between query response time and storage overhead
explained in Section IV to determine the type of each vertex.
(2) Phase II: Reachable Grid Cells Merging: This step further
reduces the storage overhead of each G-Vertex in the SPAGraph by merging a set of grid cells into a single grid cell.
Details of each phase are described in Section V-A and V-B
A. SPA-Graph Vertex Type Initialization
To determine the type of each vertex, the initialization
algorithm takes into account the following system parameters:
• MAX_RMBR: This parameter represents a threshold that
limits space area of each RMBR. If a vertex v is
an R-vertex, area of RMBR(v) cannot be larger than
MAX_RMBR. Otherwise, v will be degraded to a B-vertex.
• MAX_REACH_GRIDS: This parameter sets up the maximum number of grid cells in each ReachGrid. If a vertex
v is a G-vertex, number of grid cells in ReachGrid(v)

Algorithm 3 SPA-Graph Vertex Initialization Algorithm

Algorithm 4 Maintain R-vertex

1: Function INITIALIZE V ERTEX(Graph G = {V, E}, Vertex v)
2: Type ← InitializeType(v)
3: switch (Type)
4: case B-vertex:
5:
Set v B-vertex and GeoB(v) = true
6: case G-vertex:
7:
ReachGrid(v) ← ∅
8:
for each Vertex v ′ ∈ Vvout do
9:
Maintain-GVertex(v, v ′ )
10:
if Number of grids in ReachGrid(v) ¿ MAX_REACH_GRIDS then
11:
Set v R-vertex and break
12:
Type ← R-vertex
13:
if Number of grids in ReachGrid(v) = 0 then
14:
Set v B-vertex, GeoB(v) ← false and break
15: case R-vertex:
16:
RMBR(v) ← ∅
17:
for each Vertex v ′ ∈ Vvout do
18:
Maintain-RVertex(v, v ′ )
19:
if Area(RMBR(v)) ¿ MAX_RMBR then
20:
Set v B-vertex, GeoB(v) ← true and break
21: end switch

1: Function M AINTAIN -RV ERTEX(From-side vertex v, To-side vertex v′ )
2: switch (Type of v′ )
3: case B-vertex:
4:
if GeoB(v ′ ) = true then
5:
Set v ′ B-vertex and GeoB(v) ← true
6:
else if RMBR(v) fully contains MBR(v ′ .spatial) then
7:
return false
8:
else
9:
RMBR(v) ← MBR(RMBR(v), v ′ .spatial)
10: case R-vertex:
11:
if RMBR(v) fully contains MBR(RMBR(v ′ ), v ′ .spatial) then
12:
return false
13:
else
14:
RMBR(v) ← MBR(RMBR(v), RMBR(v ′ ), v ′ .spatial)
15: case G-vertex:
16:
if RMBR(v) fully contains MBR(RMBR(v ′ ), v ′ .spatial) then
17:
return false
18:
else
19:
RMBR(v) ← MBR(RMBR(v), RMBR(v ′ ), v ′ .spatial)
20: end switch
21: return true

cannot exceed MAX_REACH_GRIDS. Otherwise, v will
be degraded to an R-vertex.
Algorithm 3 gives the pseudocode of the vertex initialization
algorithm. Vertices are processed based on their topological
sequence in the graph. For each vertex, the algorithm first
determines the initial vertex type using the InitializeType
function (pseudocode omitted for brevity). For a vertex v,
categories of vertex v ′ ({v ′ | v ′ ∈ Vvout }) will be checked.
If there is any B-vertex v ′ with GeoB(v ′ ) = true, v is directly
initialized to a B-vertex with GeoB(v) = true. Otherwise, if
there is any R-vertex, the function will return an R-vertex type,
which means that v is initialized to R-vertex. If either of the
above happens, the function returns G-vertex type. Based on
the initial vertex type, the algorithm may encounter one of the
following three cases:
Case I (B-vertex): The algorithm directly sets v as a Bvertex and GeoB(v) = true because there must exist one outedge neighbor v ′ of v such that GeoB(v ′ ) = true.
Case III (R-vertex): For each v ′ (v ′ ∈ Vvout ), the algorithm calls the Maintain-RVertex algorithm. Algorithm 4
shows the pseudocode of the Maintain-RVertex algorithm.
Maintain-RVertex aggregates RMBR information. After each
aggregation step, area of RMBR(v) will be compared with
MAX_RMBR: In case the area of RMBR(v) is larger than
MAX_RMBR, the algorithm sets v to be a B-vertex with a true
GeoBvalue and terminates. When v ′ is either a G-vertex or
an R-vertex, the algorithm uses the new bounding rectangle
returned from MBR(RMBR(v), RMBR(v ′ ), v ′ .spatial) to
update the current RMBR(v). The algorithm calculates the
RMBR of a G-vertex in case III. In case v ′ is a B-vertex,
GeoB(v ′ ) must be reset to false. The algorithm then updates
RMBR(v) to MBR(RMBR(v), v.spatial).
Case II (G-vertex): For each vertex v ′ (v ′ ∈ Vvout ),
Maintain-GVertex (pseudocode omitted for the sake of space)
is invoked to calculate the ReachGrid of v ′ . In case v ′
is a B-vertex with GeoB(v ′ ) = false and v ′ is a spatial
vertex, the grid cell that contains the location of v ′ will be
added into ReachGrid(v). If v ′ is a G-vertex, all grid cells

in ReachGrid(v ′ ) and Grid(v ′ .spatial) will be added into
ReachGrid(v). It does not matter whether v ′ is a spatial vertex
or not. If v ′ is not a spatial vertex, Grid(v ′ .spatial) is ∅.
After accumulating information from each neighbor v ′ , the
algorithm changes the type of v to R-vertex immediately in
case the number of reachable grid cells in ReachGrid(v) is
larger than MAX_REACH_GRIDS. Therefore, the algorithm
sets the Type to R-vertex since RMBR(v) should be calculated
for possible future usage, e.g. RMBR of in-edge neighbors of
v(it will be shown in R-vertex case).
Example. Figure 2 depicts a SPA-Graph with MAX_RMBR
= 0.8A and MAX_REACH_GRIDS = 4, where A is area of the
whole space. Each vertex is attached with some information
and affiliated to one category of G EO R EACH index. Their
affiliations are listed in the figure. It is obvious that those
vertices which cannot reach any spatial vertices will be stored
as B-vertex and have a false boolean GeoB value to represent
such condition. Vertices d, f , h, i, j and k are assigned a
false value. Other vertices are G-vertex initially. ReachGrid(a)
= {2, 7, 9, 12, 14}, ReachGrid(b) = {2, 9, 14}, ReachGrid(c)
= {12, 14}, ReachGrid(e) = {14}, ReachGrid(g) = {12, 14},
ReachGrid(i) = {14}, ReachGrid(j) = {2, 7, 12, 14} and
ReachGrid(l) = {2}. Because of MAX_REACH_GRIDS, some
of them will be degraded to an R-vertex. Number of reachable grids in ReachGrid(a) and ReachGrid(j) are 4 and
5, respectively. Both of them are larger than or equal to
MERGE_COUNT. They will be degraded to R-vertex first. Then
area of their RMBR are compared with MAX_RMBR. Area
of RMBR(a) is apparently over 80% of the total space area.
According to MAX_RMBR, a is stored as a B-vertex with a true
value while j is stored as an R-vertex with an RMBR.
B. Reachable Grid Cells Merging
After the type of each vertex is decided, the initialization
algorithm performs the reachable grid cells merging phase
(lines 5 to 11 in Algorithm 2). In this phase, the algorithm
merges adjacent grid cells to reduce the overall storage overhead of each G-Vertex. To achieve that, the algorithm assumes
a system parameter, namely MERGE_COUNT. This parameter

Algorithm 5 Maintain B-vertex
1: Function M AINTAIN -BV ERTEX(From-side vertex v, To-side vertex v′ )
2: if GeoB(v) = true then
3:
return false
4: else
5:
switch (Type of v ′ )
6:
case B-vertex:
7:
if GeoB(v ′ ) = true then
8:
GeoB(v) ← true
9:
else if v ′ .spatial 6= NULL then
10:
ReachGrid(v) ← Grid(v ′ .spatial)
11:
else
12:
return false
13:
case R-vertex:
14:
RMBR(v) ← MBR(RMBR(v ′ ), v ′ .spatial)
15:
case G-vertex:
16:
ReachGrid(v) ← ReachGrid(v ′ )∪Grid(v ′ .spatial)
17:
end switch
18: return true

determines how G EO R EACH merges spatially adjacent grid
cells according to MERGE_COUNT. In each spatial region with
four grid cells, the number of reachable grid cells should not
be less than MERGE_COUNT. Otherwise, we merge the four
grid cells into a single grid cell in the lower layer.
For each G-vertex v, all grid cells in grid cell layers L1 to
Lbottom are checked. When a grid cell Gi in Li is processed,
four grid cells in Li−1 that cover the same space with Gi will
be accessed. If number of reachable grid cells is larger than or
equal to MERGE_COUNT, Gi should be added in ReachGrid(v)
first. Then all grid cells covered by Gi in layers from L0 to
Li−1 should be removed. In order to achieve that, a recursive
approach is implemented as follows. For each grid cell in Li−1
that is reachable from v, the algorithm directly remove it from
ReachGrid(v). The removal stops at this grid in this layer. No
recursive checking is required on grid cells in higher layers for
which the space is covered by the reachable grid cell. Since
all those reachable grid cells have been removed already. For
those grid cells that are not reachable from v, the algorithm
cannot assure that they do not cover some reachable grids in
a higher layer. Hence, the recursive removal is invoked until
the algorithm reaches the highest layer or other reachable grid
cells are visited.
The SPA-Graph in Figure 2 has a MERGE_COUNT set to 2.
There is no merging in e, i and l because their ReachGrids
contain only one grid. The rest are b, c and g. In ReachGrid(b),
for each grid in L1 , we make the MERGE_COUNT checking.
G17 covers four grids G1 , G2 , G5 and G6 in L0 . In such
four-grids region, only G2 is reachable from b. The merging
will not happen in G17 . It is the same case in G18 and G20 .
However, there are two grids, G9 and G14 covered by G19
in L1 . As a result, the two grids in L0 will be removed from
ReachGrid(b) with G19 being added instead. For the grid G21
in L2 , the same checking in L1 will be performed. Since, only
G19 is reachable, no merging happens. Finally, ReachGrid(b)
= {2, 19}. Similarly, we can have ReachGrid(c) = {12, 14}
and ReachGrid(g) = {12, 14} where no merging occurs.
C. SPA-Graph Maintenance
When the structure of a graph is updated, i.e., adding or
deleting edges and/or vertices, G EO R EACHneeds to maintain

the SPA-Graph structure accordingly. Moreover, when the
spatial attribute of a vertex changes, G EO R EACHmay need to
maintain the RMBRand/or ReachGridproperties of that vertex
and other connected vertices as well. As a matter of fact, all
graph updates can be simulated as a combination of adding
and/or deleting a set of edges.
Adding an edge. When an edge is added to the graph,
the directly-influenced vertices are those that are connected
to another vertex by the newly added edge. The spatial
reachability information of the to-side vertex will not be
influenced by the new edge. Based upon Lemmas III-A
and 3.2, the spatial reachability information, i.e., RMBRor
ReachGrid, of the to-side vertex should be modified based
on the the from-side vertex. On the other hand, the fromside vertex may remain the same or change. In the former
case, there is no recursive updates required for the in-edge
neighbors of the from-side vertex. Otherwise, the recursive
updates are performed in the reverse direction until no change
occurs or there is no more in-edge neighbor. A queue Q will
be exploited to track the updated vertices. When Q is not
empty, which means there are still some in-edge neighbors
waiting for updates, the algorithm retrieves the next vertex
in the queue. For such vertex, all its in-edge neighbors are
updated by using the reachability information stored on this
vertex. Updated neighbors will then be pushed into the queue.
The algorithm halts when the queue is empty. Depending on
category of the from-side vertex, corresponding maintenance
functions, including Maintain-BVertex, Maintain-RVertex and
Maintain-GVertex are used to update the newly added spatial
reachability information.
Algorithm 5 is used when the from-side vertex is a B-vertex.
In algorithm 5, if the from-side vertex v is already a B-vertex
with GeoB(v) = true. The added edge will never cause any
change on v. Hence a false value is returned. In case GeoB(v)
= false, the algorithm considers type of the to-side vertex v ′ .
•

•

•

B-vertex. If GeoB(v ′ ) = true, it is no doubt that GeoB(v)
will be set to true and a true value will be returned.
Otherwise, the algorithm checks whether v ′ is spatial.
If it is, ReachGrid(v) is updated with Grid(v ′ spatial).
Otherwise, the algorithm returns false because v is not
changed.
R-vertex. In such case, it is certain that v will be updated
to an R-vertex. The algorithm merely updates RMBR(v)
with MBR(RMBR(v ′ ), v ′ .spatial).
G-vertex. It is similar to the R-vertex case. Type of v ′ can
decide that v should be a G-vertex and the algorithm updates ReachGrid(v) with ReachGrid(v ′)∪Grid(v ′ .spatial)

Maintain-BVertex and Maintain-RVertex are what we use
in the initialization. However, there is a new condition that
should be taken into consideration. When the from-side vertex
v is an R-vertex and the to-side vertex v ′ is a G-vertex, the
algorithm needs to update the RMBR(v) with ReachGrid(v ′ ).
Under such circumstance, first a dummy RMBR(v ′ ) will be
constructed using ReachGrid(v ′ ). Although it is not the exact
RMBR of v ′ , it is still precise. Error of the width and height

will not be greater than size of a grid cell. No matter what
function is invoked to update the from-side vertex, G EO R E ACH takes into account the system parameters MAX_RMBR and
MAX_REACH_GRIDS are checked on RMBR and ReachGrid,
respectively.
Deleting an edge. When an edge is removed, the to-side
vertex will be not impacted by the deleting which is the same
with adding an edge. To maintain the correctness of spatial
reachability information stored on the from-side vertex, the
only way is to reinitialize its spatial reachability information
according to all its current out-edge neighbors. If its structure
is different from the original state due to the deleting, the
structure of all its in-edge neighbors will be rebuilt recursively.
A queue Q is used to keep track of the changed vertices. The
way G EO R EACHmaintains the queue and the operations on
each vertex in the queue are similar to the AddEdge procedure.
Maintenance cost of deleting an edge will be O(kn3 ) because
the whole G EO R EACH index may be reinitialized.
VI. E XPERIMENTAL E VALUATION
In this section, we present a comprehensive experimental evaluation of G EO R EACH performance. We compare the
following approaches: GeoMT0, GeoMT2, GeoMT3, GeoP,
GeoRMBR and SpaReach. GeoMT0, GeoMT2 and GeoMT3 are approaches that store only ReachGrid by setting MAX_REACH_GRIDS to the total number of grids in
the space and MAX_RMBR to A where A represent the
area of the whole 2D space. Their difference lies in the
value of MERGE_COUNT. GeoMT0 is an approach where
MERGE_COUNT is 0. In such approach, no higher layer
grids are merged. MERGE_COUNT is set to 2 and 3 respectively in GeoMT2 and GeoMT3. GeoP is an approach in
which MERGE_COUNT = 0, MAX_REACH_GRIDS = 200 and
MAX_RMBR = A. In such approach, reachable grids in ReachGrid will not be merged. If the number of reachable grids
of ReachGrid(v) is larger than 200 then v will be degraded
to an R-vertex. Since MAX_RMBR = A, there will be no Bvertex. In GeoRMBR, MAX_REACH_GRIDS = 0, MAX_RMBR
= A, hence only RMBR s are stored. In all ReachGrid related
approaches, the total space is split into 128 × 128 pieces in
the highest grid layer. SpaReach approach is implemented with
both spatial index and reachability index. Graph structure is
stored in Neo4j graph database. Reachability index is stored as
attributes of each graph vertex in Neo4j database. Reachability
index we use is proposed in [33]. Spatial index used SpaReach
approaches is implemented by gist index in postgresql. To
integrate Neo4j and postgresql databases, for each vertex in
the graph, we assign it an id to uniquely identify it.
Experimental Environment. The source code for evaluating query response time is implemented in Java and compiled
with java-7-openjdk-amd64. Source codes of index construction are implemented in c++ and complied using g++ 4.8.4.
Gist index is constructed automatically by using command
line in Postgresql shell. All evaluation experiments are run
on a computer with an 3.60GHz CPU, 32GB RAM running
Ubuntu 14.04 Linux OS.

TABLE II: Graph Datasets (K = 103 )
Dataset
citeseerx
go-uniprot
patent
uniprot22m
uniprot100m
uniprot150m

|V |
6540K
6968K
3775K
1595K
16087K
25038K

|E|
15011K
34770K
16519K
1595K
16087K
25038K

davg
2.30
4.99
4.38
1.00
1.00
1.00

l
59
21
32
4
9
10

Datasets. We evaluate the performance of our methods
using six real datasets [9], [33] (see Table II). Number of
vertices and edges are listed in column |V | and |E|. Column
davg and l are average degree of vertices and length of the
longest path in the graph, respectively. Citeseerx and patent
are real life citation graphs extracted from CiteSeerx2 and US
patents3 [33]. Go-uniprot is a graph generated from Gene Ontology and annotation files from Uniprot4 [33]. Uniprot22m,
uniprot100m and uniprot150m are RDF graphs from UniProt
database [33]. The aforementioned datasets represent graphs
that possess no spatial attributes. For each graph, we simulate
spatial data by assigning a spatial location to a subset of
the graph vertices. During the experiments, we change the
ratio of spatial vertices to the total number of vertices from
20% to 80%. During the experiments, we vary the spatial
distribution to be: uniform, zipf, and clustered distributions.
Unless mentioned otherwise, the number of spatial clusters is
set to 4 by default.
A. Query Response Time
In this section, we fist compare the query response time
performance of SpaReach to our GeoP approach. Afterwards,
we change tunable parameters in G EO R EACH to evaluate
influence of these thresholds. For each dataset, we change the
spatial selectivity of the input query rectangle from 0.0001 to
0.1. For each query spatial selectivity, we randomly generate
500 queries by randomly selecting 500 random vertices and
500 random spatial locations of the query rectangle. The
reported query response time is calculated as the average time
taken to answer the 500 queries.
Figure 5 depicts the query response time of GeoP and
SpaReach on four datasets. 80% of vertices in the graph
are spatial and they are randomly-distributed in space. For
brevity, we omit the results of the other two datasets, i.e.,
uniprot22m and uniprot100m, since they have almost the same
graph structure and exihibit the same performance. As it turns
out In Figure 5, GeoP outperforms SpaReach for any query
spatial selectivity in uniprot150m, go-uniprot and citeseerx.
For these datasets, SpaReach approach cost more time when
query selectivity increases. When we increasing the query
range size, the range query step tends to return a larger number
of spatial vertices. Hence, the graph reachability checking
step has to check more spatial vertices. Figure 5c and 5d
show similar experiment results. In conclusion, GeoP is much
2 http://citeseer.ist.psu.edu/
3 http://snap.stanford.edu/data/
4 http://www.uniprot.org/

query time (sec)

GeoP

SpaReach

query time (sec)

GeoP

SpaReach

query time (sec)

GeoP

SpaReach

query time (sec)

10000

100

10

100

100

10

1

10

1

1

0.1

1

0.01
0.0001

0.001

0.01

0.1

0.1
0.0001

0.001

0.01

0.1

0.01
0.0001

0.001

0.01

0.1

0.1
0.0001

0.001

GeoP

0.01

Query range

Query range

Query range

Query range

(a) uniprot150m

(b) patent

(c) go-uniprot

(d) citeseerx

SpaReach

0.1

Fig. 5: Query response time (80% spatial vertex ratio, randomly-distributed spatial data, and spatial selectivity ranging from 0.0001 to 0.1)
more query-efficient in relatively sparse graphs. Patent dataset
is the densest graph with richer reachability information.
Figure 5b indicates that even when spatial selectivity set to
0.0001, GeoP can achieve almost the same performance as
SpaReach. When spatial selectivity increases, GeoP outperforms SpaReach again. In a denser graph, the performance
difference between the two approaches is smaller than in
sparse graphs especially when the spatial selectivity is low.
Table III compares the query response time of all our approaches for the uniprot150m, patent go-uniprot and citeseerx
datasets with randomly distributed spatial vertices and spatial
ratio of 80%. In uniprot150m, all our approaches almost have
the same performance. The same pattern happens with the
uniprot22m, uniprot100m and go-uniprot datasets. So we use
uniprot150m as a representative.
For the patent graph with random-distributed spatial vertices
and spatial ratio of 20%, query efficiency difference can be
easily caught. GeoMT0 keeps information of exact reachable
grids of every vertex which brings us fast query speed, but
also the highest storage overhead. RMBR stores general spatial
boundary of reachable vertices which is the most scalable.
However, such approach spend the most time in answering the
query. Since GeoMT3 is an approach that MERGE_COUNT is
set to 3, just few grids in GeoMT3 are merged. As a result,
its query time is merely little bit longer than GeoMT0. There
are more grids getting merged in GeoMT2 than in GeoMT3.
Inaccuracy caused by more integration lowers efficiency of
GeoMT3 in query. GeoP is combination of ReachGrid and
RMBR. Its query efficiency is lower than GeoMT0 and better
than GeoRMBR. In this case, GeoMT2 outperforms GeoP. But
it is not always the case. By tuning MAX_REACH_GRIDS to
a larger number, GeoP can be more efficient in query.
In citeseerx, GeoMT0 keeps the best performance as expected. Performance of GeoP is in between GeoMT0 and
GeoRMBR as what is shown in patent. But GeoMT2 and
GeoMT3 reveal almost the same efficiency and they are worse
than GeoRMBR. Distinct polarized graph structure accounts
for the abnormal appearance. In citeseerx, all vertices can be
divided into two groups. One group consists of vertices that
cannot reach any vertex. The other group contains a what
we call center vertex. The center vertex has huge number of
out-edge neighbor vertices and is connected by huge number
of vertices as well. Because the center vertex can reach that
many vertices, it can reach nearly all grid cells in space. As

a result, vertices that can reach the center vertex can also
reach all grid cells in space. So no matter what value is
MAX_REACH_GRIDS, reachable grids in ReachGrid of these
vertices will be merged into only one grid in a lower layer
until to the bottom layer which is the whole space. Then such
ReachGrid can merely function as a GeoB which owns poorer
locality than RMBR.
B. Storage Overhead
Figure 6a gives the storage overhead of all approaches
for the uniprot150m dataset. In this experiment, the spatial
vertices are randomly distributed in space. Since uniprot22m
and uniprot100m share the same pattern with uniprot150m
(even spatial distribution of vertices varies), they are not shown
in the figure. The experiments show that G EO R EACH and
all its variants require less storage overhead than SpaReach
because of the additional overhead introduced by the spatial index. When there are less spatial vertices, SpaReach
obviously occupies less space because size of spatial index
lessens. However, SpaReach always requires more storage
than any other approaches. Storage overhead of G EO R EACH
approaches shows a two-stages pattern which means it is either
very high (ratio = 0.8, 0.6 and 0.4) or very low (ratio = 0.2).
The reason is as follows. These graphs are sparse and almost
all vertices reach the same vertex. This vertex cannot reach any
other vertex. Let us call it an end vertex. If the end vertex is
a spatial vertex, then all vertices that can reach the end vertex
will keep their spatial reachability information (no matter what
category they are) in storage. But if it is not, majority of
vertices will store nothing for spatial reachability information.
GeoMT0 and GeoP are of almost the same index size because
of sparsity and end-point phenomenon in these graphs. Such
characteristic causes that almost each vertex can just reach
only one grid which makes MAX_REACH_GRIDS invalid in
approach GeoMT0 (number of reachable grids is always less
than MAX_REACH_GRIDS) which makes GeoMT0 and GeoP
have nearly the same size. For similar reason, MERGE_COUNT
becomes invalid in these datasets which makes GeoMT2 and
GeoMT3 share the same index size with GeoMT0 and GeoP.
We also find out that index size of GeoRMBR is slightly larger
than GeoMT0 approaches. Intuitively, RMBR should be more
scalable than ReachGrid. But most of the vertices in these
three graphs can reach only one grid. In GeoRMBR, for each
vertex that have reachable spatial vertices, we assign an RMBR

TABLE III: Query Response Time in three datasets, 80% spatial vertex ratio, and spatial selectivity ranging from 0.0001 to 0.1
Selectivity
0.0001
0.001
0.01
0.1
Index size

MT0
68
65
66
69

GeoMT0
GeoP

(MB)
2400

uniprot150m
MT3 GeoP
67
66
78
66
65
65
65
75

MT2
68
77
66
65

GeoMT2
GeoRMBR

GeoMT3
SpaReach

Index size

RMBR
66
65
65
66
GeoMT0
GeoP

(MB)
6000

MT0
643
168
87
51

GeoMT2
GeoRMBR

patent
MT3
741
185
98
59

MT2
762
258
143
108

GeoMT3
SpaReach

Index size

GeoP
1570
559
217
155

GeoMT0
GeoP

(MB)
750

RMBR
2991
1965
915
348
GeoMT2
GeoRMBR

MT0
202
34
32
33

GeoMT3
SpaReach

Index size

4000

500

800

800

2000

250

400

0
0.8

0.6

0.4

0.2

0
0.8

0.6

0.4

0.2

GeoMT0
GeoP

(MB)
1200

1600

0

citeseerx
MT3 GeoP
203
210
471
207
410
189
399
160

MT2
212
460
408
399

GeoMT2
GeoRMBR

RMBR
234
215
200
183
GeoMT3
SpaReach

0
0.8

0.6

0.4

0.2

0.8

0.6

0.4

ratio

ratio

ratio

ratio

(a) uniprot150m

(b) patent

(c) go-uniprot

(d) citeseerx

0.2

Fig. 6: Storage Overhead (Randomly distributed, spatial vertex ratio from 0.8 to 0.2)
index size(MB)

Random

Clustered

Zipf

2400
1800

index size(MB)

Random

Clustered

Zipf

index size(MB)

Random

Clustered

Zipf

index size(MB)

1800

7 

6

1200



400

600

2 

200

Random

Clustered

Zipf

1200
600
0

0

0
GeoMT2

GeoP

GeoRMBR SpaReach

(a) uniprot150m

GeoMT2

GeoP

GeoRMBR SpaReach

(b) patent

0
GeoMT2

GeoP

GeoRMBR SpaReach

(c) go-uniprot

GeoMT2

GeoP

GeoRMBR SpaReach

(d) citeseerx

Fig. 7: Storage Overhead for varying spatial data distribution (randomly, cluster and zipf distributed) and 0.8 spatial vertex ratio)
which will be stored as coordinates of RMBR’s top-left and
lower-right points. It is more scalable to store one grid id than
two coordinates. So when a graph is highly sparse, index size
of GeoMT0 is possible to be less than GeoRMBR.
Figure 6c shows that in go-uniprot all G EO R EACH approaches performs better than SpaReach. When we compare
all the G EO R EACH approaches, GeoMT0, GeoMT2 and GeoMT3 lead to almost the same storage overhead. That happens
due to the fact that go-uniprot is a very sparse graph. A
vertex can only reach few grids in the whole space. Grid
cells in ReachGrid can hardly be spatially adjacent to each
other which causes no integration. The graph sparsity makes
the number of reachable grids in ReachGrid always less than
MAX_REACH_GRIDS which leads to less R-vertices and more
G-vertices. In consequence, go-uniprot, GeoMT0, GeoMT2,
GeoMT3 and GeoP lead to the same storage overhead. It
is rational that GeoRMBR requires the least storage because
RMBR occupies less storage than ReachGrid.
When graphs are denser, results become more complex.
Figure 6b shows index size of different approaches in patent
dataset with randomly-distributed spatial vertices. GeoRMBR
and GeoP, take the first and the second least storage and
are far less than other approaches because both of them
use RMBR which is more scalable. GeoMT0 takes the most
storage in all spatial ratios for that ReachGrid takes high
storage overhead. GeoMT2 and GeoMT3 require less storage
than GeoMT0 because spatially-adjacent reachable grids in

GeoMT0 are merged which brings us scalability. GeoMT3
are more scalable than GeoMT2 because MERGE_COUNT in
GeoMT2 is 2 which causes more integration. There are three
approaches, GeoMT3, GeoP and GeoRMBR, that outperform
SpaReach approach. By tuning parameters in G EO R EACH, we
are able achieve different performance in storage overhead and
can also outperform SpaReach.
Figure 6d depicts index size of all approaches in citeseerx
with randomly distributed spatial vertices. Spatial vertices ratio
ranges from 0.8 to 0.2. All G EO R EACH approaches outperform
SpaReach except for one outlier when spatial vertices ratio
is 0.2. GeoMT0 consumes huge storage. This is caused by
the center vertex which is above-mentioned. Recall that large
proportion of ReachGrid contains almost all grids in space.
After bitmap compression, it will cause low storage overhead.
This is why when spatial vertices ratio is 0.8, 0.6 and 0.4,
GeoMT0 consumes small size of index. When the ratio is 0.2,
there are less spatial vertices. Although graph structure does
not change, the center vertex reach less spatial vertices and
less grids. Then the bitmap compression brings no advantage
in storage overhead.
Figure 7 shows the impact of spatial data distribution on
the storage cost. GeoMT0, GeoMT2 and GeoMT3 are all
ReachGrid-based approaches. Spatial data distribution of vertices influences all approaches the same way. For all datasets,
SpaReach is not influenced by the spatial data distribution.
SpaReach consists of two sections: (1) The reachability index

size is determined by graph structure and (2) The spatial
index size is directly determined by number of spatial vertices. Hence, SpaReach exhibits the same storage overhead
for different spatial data distributions. When spatial vertices
distribution varies, GeoRMBR also keeps stable storage overhead. This is due to the fact that the storage overhead for
each RMBR is a constant and the number of stored RMBR
s is determined by the graph structure and spatial vertices
ratio, and not by the spatial vertices distribution. Spatial data
distribution can only influence the shape of each RMBR.
Figure 7a shows that each approach in G EO R EACH keeps
the same storage overhead under different distributions in
uniprot150m. As mentioned before, GeoMT0, GeoMT2, GeoMT3 and GeoP actually represent the same data structure
since there is only a single reachable grid in ReachGrid. When
there is only one grid reachable, varying the spatial distribution
becomes invalid for all approaches which use ReachGrid.
Figure 7b and 7c shows that the storage overhead introduced by ReachGrid-based approaches decreases when spatial
vertices become more congested. Randomly distributed spatial
data is the least congested while zipf distributed is the most.
The number of reachable spatial vertices from each vertex do
not change but these reachable spatial vertices become more
concentrated in space. This leads to less reachable grids in
ReachGrid.
Figure 7d shows that when spatial vertices are more congested, ReachGrid based approaches, i.e., GeoMT0, GeoMT2
and GeoMT3, tend to be less scalable. Recall that citeseerx
dataset is a polarized graph with a center vertex. One group
contains vertices that can reach huge number of vertices (about
200,000) due to the center vertex. When spatial vertices are
more concentrated and that will lead to more storage overhead.
C. Initialization time
In this section, we evaluate the index initialization time
for all considered approaches. For brevity, we only show the
performance results for four datasets, uniprot150m, patent,
go-uniprot and citeseerx, since uniprot22m, uniprot100m and
uniprot150m datasets exhibit the same performance. Figure 8a
shows that SpaReach requires much more construction time
than the other approaches under all spatial ratios. Although
these graphs are sparse, they contain large number of vertices. This characteristic causes huge overhead in constructing
a spatial index which dominates the initialization time in
SpaReach. Hence, SpaReach takes much more time than all
other approaches. However, the SpaReach initialization time
decreases when decreasing the number spatial vertices since
the spatial index building step deals with less spatial vertices
in such case. However, SpaReach remains the worst even when
the spatial vertex ratio is set to 20%.
Figures 8b and 8d gives the initialization time for both the
patent and citeseerx datasets, respectively. GeoRMBR takes
significantly less initialization time compared to all other
approaches. GeoP takes less time than the rest of approaches
because it is ReachGrid of partial vertices whose number of
reachable grids are less than MAX_REACH_GRIDS that are

calculated. In most cases, GeoMT0 can achieve almost equal
or better performance compared to SpaReach while GeoMT2
and GeoMT3 requires more time due to the integration of
adjacent reachable grids. To sum up, GeoRMBR and GeoP
perform much better than SpaReach in initialization even
in very dense graphs. GeoMT0 can keep almost the same
performance with SpaReach approach.
Figure 8c shows the initialization time for all six approaches on the go-uniprot dataset. Both RMBR approaches,
i.e., GeoRMBR and GeoP, still outperform SpaReach. This
is due to the fact that a spatial index constitutes a high
proportion of SpaReach initialization time. As opposed to the
uniprot150m case, the smaller performance gap between initializing GeoRMBR and SpaReach in go-uniprot.is explained
as follows. The size of go-uniprotis far less than uniprot150m
which decreases the spatial index initialization cost. As a
result, the index construction time in SpaReach is less than
that in uniprot150m. Since this graph has more reachability
information, all G EO R EACH approaches require more time
than in uniprot150m. It is conjunction of G EO R EACH and
SpaReach index size changes that causes the smaller gap.
VII. C ONCLUSION
This paper describes G EO R EACH a novel approach that
evaluates graph reachability queries and spatial range predicates side-by-side. G EO R EACH extends the functionality of a
given graph database management system with light-weight
spatial indexing entries to efficiently prune the graph traversal
based on spatial constraints. G EO R EACH allows users to
tune the system performance to achieve both efficiency and
scalability. Based on extensive experiments, we show that
G EO R EACH can be scalable and query-efficient than existing spatial and reachability indexing approaches in relatively
sparse graphs. Even in rather dense graphs, our approach
can outperform existing approaches in storage overhead and
initialization time and still achieves faster query response
time. In the future, we plan to study we plan to study
the extensibility of G EO R EACH to support different spatial
predicates. Furthermore, we aim to extend the framework
to support a distributed system environment. Last but not
least, we also plan to study the applicability of G EO R EACH
to various application domains including: Spatial Influence
Maximization, Location and Social-Aware Recommendation,
and Location-Aware Citation Network Analysis.
R EFERENCES
[1] R. Agrawal, A. Borgida, and H. V. Jagadish. Efficient Management of
Transitive Relationships in Large Data and Knowledge Bases. ACM,
1989.
[2] R. Bayer and E. M. McCreight. Organization and Maintenance of Large
Ordered Indices. Acta Informatica, 1(3):173–89, 1972.
[3] N. Beckmann, H.-P. Kriegel, R. Schneider, and B. Seeger. The R*-Tree:
An Efficient and Robust Access Method for Points and Rectangles. In
SIGMOD, pages 322–331, May 1990.
[4] J. L. Bentley. Multidimensional Binary Search Trees Used for Associative Searching. Communications of the ACM, CACM, 18(9):509–517,
1975.
[5] J. Cai and C. K. Poon. Path-hop: efficiently indexing large graphs for
reachability queries. In CIKM, pages 119–128. ACM, 2010.

Ix 	me
(sec)
480

GeoMT0
GeoP

GeoMT2
GeoRMBR

GeoMT
SpaReach

Index me

GeoMT0
GeoP

GeoMT2
GeoRMBR

GeoMT3
SpaReach

Index me

GeoMT0
GeoP

GeoMT2
GeoRMBR

GeoMT3 x me
(sec)
SpaReach
00

5


(sec)
150

320

00

100

200

1

15


50

100

0
0.8

0.

0.4

0.2

(sec)

0

08

0
0.6

0.4

0.2

08

0
0.6

0.4

0.2

08

GeoMT0
GeoP

GeoMT2
GeoRMBR

0.

0

ratio

ratio

ratio

ratio

(a) uniprot150

(b) patent

(c) go-uniprot

(d) citeseerx

GeoMT
SpaReach

02

Fig. 8: Initialization time (Randomly distributed, spatial vertex ratio from 0.8 to 0.2)

[6] L. Chen, A. Gupta, and M. E. Kurul. Stack-based algorithms for pattern
matching on dags. In VLDB, pages 493–504. VLDB Endowment, 2005.
[7] Y. Chen and Y. Chen. An efficient algorithm for answering graph
reachability queries. In ICDE, pages 893–902. IEEE, 2008.
[8] J. Cheng, S. Huang, H. Wu, and A. W.-C. Fu. Tf-label: a topologicalfolding labeling scheme for reachability querying in a large graph. In
SIGMOD, pages 193–204. ACM, 2013.
[9] J. Cheng, S. Huang, H. Wu, and A. W.-C. Fu. Tf-label: a topologicalfolding labeling scheme for reachability querying in a large graph. In
SIGMOD, pages 193–204. ACM, 2013.
[10] J. Cheng, Z. Shang, H. Cheng, H. Wang, and J. X. Yu. K-reach: who
is in your small world. PVLDB, 5(11):1292–1303, 2012.
[11] J. Cheng, J. X. Yu, X. Lin, H. Wang, and P. S. Yu. Fast computing
reachability labelings for large graphs with high compression rate. In
EDBT, pages 193–204. ACM, 2008.
[12] E. Cohen, E. Halperin, H. Kaplan, and U. Zwick. Reachability and
distance queries via 2-hop labels. SIAM Journal on Computing,
32(5):1338–1355, 2003.
[13] D. Comer. The Ubiquitous B-Tree. ACM Computing Surveys, 11(2):121–
137, 1979.
[14] R. A. Finkel and J. L. Bentley. Quad trees: A Data Structure for Retrieval
of Composite Keys. Acta Informatica, 4(1):1–9, 1974.
[15] J. E. Gonzalez, Y. Low, H. Gu, D. Bickson, and C. Guestrin. GraphChi:
Large-Scale Graph Computation on Just a PC. In OSDI, 2012.
[16] A. Guttman. R-Trees: A Dynamic Index Structure For Spatial Searching.
In SIGMOD, 1984.
[17] H. Jagadish. A compression technique to materialize transitive closure.
TODS, 15(4):558–598, 1990.
[18] R. Jin, Y. Xiang, N. Ruan, and H. Wang. Efficiently answering
reachability queries on very large directed graphs. In SIGMOD, pages
595–608. ACM, 2008.
[19] J. Liagouris, N. Mamoulis, P. Bouros, and M. Terrovitis. An effective
encoding scheme for spatial RDF data. PVLDB, 7(12):1271–1282, 2014.
[20] D. B. Lomet. Grow and Post Index Trees: Roles, Techniques and Future
Potential. In SSD, pages 183–206, Aug. 1991.
[21] P. Rigaux, M. Scholl, and A. Voisard. Spatial Databases with Application to GIS. Morgan Kaufmann, 2002.
[22] H. Samet. The Design and Analysis of Spatial Data Structures. AddisonWesley, 1990.
[23] H. Samet. Foundations of Multidimensional and Metric Data Structures.
Morgan Kaufmann, 2006.
[24] M. Sarwat, S. Elnikety, Y. He, and G. Kliot. Horton: Online Query
Execution Engine for Large Distributed Graphs. In ICDE, 2012.
[25] M. Sarwat, S. Elnikety, Y. He, and M. F. Mokbel. Horton+: A Distributed
System for Processing Declarative Reachability Queries over Partitioned
Graphs. PVLDB, 6(14):1918–1929, 2013.
[26] R. Schenkel, A. Theobald, and G. Weikum. Hopi: An efficient connection index for complex xml document collections. In EDBT, pages
237–255. Springer, 2004.
[27] S. Seufert, A. Anand, S. Bedathur, and G. Weikum. Ferrari: Flexible
and efficient reachability range assignment for graph indexing. In ICDE,
pages 1009–1020. IEEE, 2013.
[28] B. Shao, H. Wang, and Y. Li. Trinity: A Distributed Graph Engine on
a Memory Cloud. In SIGMOD, 2013.
[29] S. Shekhar and S. Chawla. Spatial Databases: A Tour. Prentice Hall,
2003.
[30] S. Trißl and U. Leser. Fast and practical indexing and querying of very
large graphs. In SIGMOD, pages 845–856. ACM, 2007.

[31] S. J. van Schaik and O. de Moor. A memory efficient reachability data
structure through bit vector compression. In SIGMOD, pages 913–924.
ACM, 2011.
[32] H. Wang, H. He, J. Yang, P. S. Yu, and J. X. Yu. Dual labeling:
Answering graph reachability queries in constant time. In ICDE, pages
75–75. IEEE, 2006.
[33] Y. Yano, T. Akiba, Y. Iwata, and Y. Yoshida. Fast and scalable
reachability queries on graphs by pruned labeling with landmarks and
paths. In CIKM, pages 1601–1606. ACM, 2013.
[34] H. Yildirim, V. Chaoji, and M. J. Zaki. Grail: Scalable reachability index
for large graphs. PVLDB, 3(1-2):276–284, 2010.
[35] A. D. Zhu, W. Lin, S. Wang, and X. Xiao. Reachability queries on large
dynamic graphs: a total order approach. In SIGMOD, pages 1323–1334.
ACM, 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information
for Improved Social Recommendation
RANA FORSATI, Shahid Beheshti University and University of Minnesota
MEHRDAD MAHDAVI, Michigan State University
MEHRNOUSH SHAMSFARD, Shahid Beheshti University
MOHAMED SARWAT, University of Minnesota

With the advent of online social networks, recommender systems have became crucial for the success of
many online applications/services due to their significance role in tailoring these applications to user-specific
needs or preferences. Despite their increasing popularity, in general, recommender systems suffer from data
sparsity and cold-start problems. To alleviate these issues, in recent years, there has been an upsurge of
interest in exploiting social information such as trust relations among users along with the rating data to
improve the performance of recommender systems. The main motivation for exploiting trust information in
the recommendation process stems from the observation that the ideas we are exposed to and the choices
we make are significantly influenced by our social context. However, in large user communities, in addition
to trust relations, distrust relations also exist between users. For instance, in Epinions, the concepts of
personal “web of trust” and personal “block list” allow users to categorize their friends based on the quality
of reviews into trusted and distrusted friends, respectively. Hence, it will be interesting to incorporate this
new source of information in recommendation as well. In contrast to the incorporation of trust information
in recommendation which is thriving, the potential of explicitly incorporating distrust relations is almost
unexplored. In this article, we propose a matrix factorization-based model for recommendation in social rating
networks that properly incorporates both trust and distrust relationships aiming to improve the quality of
recommendations and mitigate the data sparsity and cold-start users issues. Through experiments on the
Epinions dataset, we show that our new algorithm outperforms its standard trust-enhanced or distrustenhanced counterparts with respect to accuracy, thereby demonstrating the positive effect that incorporation
of explicit distrust information can have on recommender systems.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and
Retrieval—Information filtering; I.2 [Computing Methodologies]: Artificial Intelligence; I.2.6 [Artificial
Intelligence]: Learning; J.4 [Computer Applications]: Social and Behavioral Sciences
General Terms: Design, Algorithms
Additional Key Words and Phrases: Matrix factorization, recommender systems, social relationships
ACM Reference Format:
Rana Forsati, Mehrdad Mahdavi, Mehrnoush Shamsfard, and Mohamed Sarwat. 2014. Matrix factorization
with explicit trust and distrust side information for improved social recommendation. ACM Trans. Inf. Syst.
32, 4, Article 17 (October 2014), 38 pages.
DOI: http://dx.doi.org/10.1145/2641564

Author’s addresses: R. Forsati (corresponding author) and M. Shamsfard, Natural Language Processing
(NLP) Research Lab, Faculty of Electrical and Computer Engineering, Shahid Beheshti University, G. C.,
Tehran, Iran; M. Mahdavi, Department of Computer Science and Engineering, Michigan State University,
East Lansing, MI; M. Sarwat, Computer Science and Engineering Department, University of Minnesota,
Minneapolis, MN; corresponding author’s email: rana.forsati@gmail.com.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by
others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to
post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
c 2014 ACM 1046-8188/2014/10-ART17 $15.00

DOI: http://dx.doi.org/10.1145/2641564
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17

17:2

R. Forsati et al.

1. INTRODUCTION

The huge amount of information available on the Web has made it increasingly challenging to cope with this information overload and find the most relevant information
one is really interested in. Recommender systems intend to provide users with recommendations of products they might appreciate, taking into account their past ratings,
purchase history, or interest. The recent proliferation of online social networks has further enhanced the need for such systems. Therefore, it is obvious why such systems are
indispensable for the success of many online applications such as Amazon, iTunes, and
Netflix to guide the search process and help users to effectively find the information or
products they are looking for [Miller et al. 2004]. Roughly speaking, the overarching
goal of recommender systems is to identify a subset of items (e.g., products, movies,
books, music, news, and webpages) that are likely to be more interesting to users based
on their interests [Deshpande and Karypis 2004; Wu et al. 2009; Forsati and Meybodi
2010; Bobadilla et al. 2013].
In general, most widely used recommender systems (RS) can be broadly classified
into content-based (CB), collaborative filtering (CF), or hybrid methods [Adomavicius
and Tuzhilin 2005]. In CB recommendation, one tries to recommend items similar
to those a given user preferred in the past. These methods usually rely on external
information, such as explicit item descriptions, user profiles, and/or the appropriate
features extracted from items to analyze item similarity or user preference to provide
recommendation. In contrast, CF recommendation, the most popular method adopted
by contemporary recommender systems, is based on the core assumption that similar
users on similar items express similar interest, and it usually relies on the rating information to build a model out of the rating information in the past without having
access to external information required in CB methods. The hybrid approaches proposed combine both CB- and CF-based recommenders to gain advantages and avoid
certain limitations of each type of systems [Good et al. 1999; Soboroff and Nicholas
1999; Pazzani 1999; Melville et al. 2002; Pavlov and Pennock 2002; Talabeigi et al.
2010; Forsati et al. 2013].
The essence of CF lies in analyzing the neighborhood information of past users and
items’ interactions in the user-item rating matrix to generate personalized recommendations based on the preferences of other users with similar behavior. CF has been
shown to be an effective approach to recommender systems. The advantage of these
types of recommender systems over content-based RS is that the CF-based methods
do not require an explicit representation of the items in terms of features, but is based
only on the judgments/ratings of the users. These CF algorithms are mainly divided
into two main categories [Gu et al. 2010]: memory-based methods (also known as
neighborhood-based methods) [Wang et al. 2006b; Chen et al. 2009] and model-based
methods [Hofmann 2004; Si and Jin 2003; Srebro and Jaakkola 2003; Zhang et al.
2006]. Recently, another direction in CF considers how to combine memory-based and
model-based approaches to take advantage of both types of methods, thereby building
a more accurate hybrid recommender system [Pennock et al. 2000; Xue et al. 2005;
Koren 2008].
The heart of memory-based CF methods is the measurement of similarity based
on ratings of items given by users: either the similarity of users (user-oriented CF)
[Herlocker et al. 1999], the similarity of items (items-oriented CF) [Sarwar et al. 2001],
or combined user-oriented and item-oriented collaborative filtering approaches to overcome the limitations specific to either of them [Wang et al. 2006a]. The user-oriented CF
computes the similarity among users, usually based on user profiles or past behavior,
and seeks consistency in the predictions among similar users [Yu et al. 2004; Hofmann
2004]. The item-oriented CF, on the other hand, allows input of additional item-wise

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:3

information and is also capable of capturing the interactions among them. If the rating
of an item by a user is unavailable, collaborative-filtering methods estimate it by computing a weighted average of known ratings of the items from the most similar users.
Memory-based collaborative filtering is most effective when users have expressed
enough ratings to have common ratings with other users, but it performs poorly for
so-called cold-start users. Cold-start users are new users who have expressed only a few
ratings. Thus, for memory-based CF methods to be effective, large amounts of userrating data are required. Unfortunately, due to the sparsity of the user-item rating
matrix, memory-based methods may fail to correctly identify the most similar users
or items, which in turn decreases the recommender accuracy. Another major issue
that memory-based methods suffer from is the scalability problem. The reason being
essentially the fact that when the number of users and items is very large, which
is common in many real-world applications, the search to identify the k most similar
neighbors of the active user is computationally burdensome. In summary, data sparsity
and non-scalability issues are two main issues current memory-based methods suffer
from.
To overcome the limitations of memory-based methods, model-based approaches have
been proposed, which establish a model using the observed ratings that can interpret
the given data and predict the unknown ratings [Adomavicius and Tuzhilin 2005]. In
contrast to memory-based algorithms, model-based algorithms try to model the users
based on their past ratings and use these models to predict the ratings on unseen items.
In model-based CF, the goal is to employ statistical and machine learning techniques
to learn models from the data and make recommendations based on the learned model.
Methods in this category include aspect model [Hofmann 2004; Si and Jin 2003], clustering methods [Kohrs and Merialdo 1999], Bayesian model [Zhang and Koren 2007],
and low-dimensional linear factor models such as matrix factorization (MF) [Srebro
et al. 2005; Srebro and Jaakkola 2003; Zhang et al. 2006; Salakhutdinov and Mnih
2008b]. Due to its efficiency in handling very huge datasets, matrix factorization-based
methods have become one of the most popular models among the model-based methods, for example, weighted low-rank matrix factorization [Srebro and Jaakkola 2003],
weighted nonnegative matrix factorization (WNMF) [Zhang et al. 2006], maximum
margin matrix factorization (MMMF) [Srebro et al. 2005], and probabilistic matrix factorization (PMF) [Salakhutdinov and Mnih 2008b]. These methods assume that user
preferences can be modeled by only a small number of latent factors [Dasgupta et al.
2002] and all focus on fitting the user-item rating matrix using low-rank approximations only based on the observed ratings. The recommender system we propose in this
article adheres to the model-based factorization paradigm.
Although latent factor models and in particular matrix factorization are able to
generate high-quality recommendations, these techniques also suffer from the data
sparsity problem in real-world scenarios and fail to address users who rated only a
few items. For instance, according to Sarwar et al. [2001], the density of non-missing
ratings in most commercial recommender systems is less than one or even much less.
Therefore, it is unsatisfactory to rely predictions on such small amounts of data, which
becomes more challenging in the presence of large number of users or items. This
observation necessitates tackling the data sparsity problem in an affirmative manner
to be able to generate more accurate recommendations.
One of the most prominent approaches to tackling the data sparsity problem is to
compensate for the lack of information in the rating matrix with other sources of
side information which are available to the recommender system. For example, social
media applications allow users to connect with each other and to interact with items
of interest such as songs, videos, pages, news, and groups. In such networks, the ideas
we are exposed to and the choices we make are significantly influenced by our social
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:4

R. Forsati et al.

context. More specifically, users generally tend to connect with other users due to some
commonalities they share, often reflected in similar interests. Moreover, in many reallife applications it may be the case that only social information about certain users
is available while interaction data between the items and those users has not yet
been observed. Therefore, the social data accumulated in social networks would be a
rich source of information for the recommender system to utilize as side information
to alleviate the data sparsity problem. To accomplish this goal, in recent years, the
trust-based recommender systems became an emerging field to provide users with
personalized item recommendations based on the historical ratings given by users and
the trust relationships among users (e.g., social friends).
Social-enhanced recommendation systems are becoming of greater significance and
practicality with the increased availability of online reviews, ratings, friendship links,
and follower relationships. Moreover, many e-commerce and consumer review websites
provide both reviews of products and a social network structure among the reviewers.
As an example, the e-commerce site Epinions [Guha et al. 2004] asks its users to indicate which reviews/users they trust and use this trust information to rank the reviews
of products. Similar patterns can be found in online communities such as Slashdot in
which millions of users post news and comment daily and are capable of tagging other
users as friends/foes or fans/freaks. Another example is the ski mountaineering site
Moleskiing [Avesani et al. 2005] which enables users to share their opinions about
the snow conditions of the different ski routes and also express how much they trust
the other users. Another well-known example is the FilmTrsut system [Golbeck and
Hendler 2006], an online social network that provides movie rating and review features
to its users. The social networking component of the website requires users to provide
a trust rating for each person they add as a friend. Also users on Wikipedia can vote
for or against the nomination of others to adminship [Burke and Kraut 2008]. These
websites have come to play an important role in guiding users’ opinions on products
and, in many cases, also influence their decisions in buying or not buying the product
or service. The results of experiments in Crandall et al. [2008] and of similar works
confirm that a social network can be exploited to improve the quality of recommendations. From this point of view, traditional recommender systems that ignore the social
structure between users may no longer be suitable.
A fundamental assumption in social-based recommender systems which has been
adopted by almost all of the relevant literature is that if two users have a friendship relation, then the recommendation from his or her friends probably has higher
trustworthiness than strangers. Therefore, the goal becomes how to combine the useritem rating matrix with the social/trust network of a user to boost the accuracy of the
recommendation system and alleviate the sparsity problem. Over the years, several
studies have addressed the issue of the transfer of trust among users in online social
networks. These studies exploit the fact that trust can be passed from one member
to another in a social network, creating trust chains, based on its propagative and
transitive nature.1 Therefore, some recommendation methods fusing social relations
by regularization [Jamali and Ester 2011; Li and Yeung 2009; Ma et al. 2011a; Zhu
et al. 2011] or factorization [Ma et al. 2008, 2011b; Salakhutdinov and Mnih 2008a,
2008b; Srebro and Jaakkola 2003; Salakhutdinov et al. 2007; Rennie and Srebro 2005]
were proposed that exploit trust relations in a social network.

1 We note that while the concept of trust has been studied in many disciplines, including sociology, psychology,
economics, and computer science from different perspectives, the issue of propagation and transitivity have
often been debated in literature, and different authors have reached different conclusions (see e.g., [Sherchan
et al. 2013] for a thorough discussion).

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:5

Also, the results of incorporating trust information in recommender systems is appealing and has been the focus of much researcher in the last few years, but in large
user communities, besides the trust relationship between users, the distrust relationships are also unavoidable. For example, Epinions provided the feature that enables
users to categorize other users in a personal web of trust list based on their quality
as a reviewer. Later on, this feature integrated with the concept of personal block list,
which reflects the members that are distrusted by a particular user. In other words, if
a user encounters a member whose reviews are consistently offensive, inaccurate, or
otherwise low quality, she can add that member to her block list. Therefore, it would be
tempting to investigate whether or not distrust information could be effectively utilized
to boost the accuracy of recommender systems as well.
In contrast to trust information for which there has been a great deal of research,
the potential advantage/disadvantage of explicitly utilizing distrust information is
almost unexplored. Recently, few attempts have been made to explicitly incorporate
the distrust relations in recommendation process [Guha et al. 2004; Ma et al. 2009b;
Victor et al. 2011b, 2013], which demonstrated that the recommender systems can
benefit from the proper incorporation of distrust relations in social networks. However,
despite these positive results, there are some unique challenges involved in distrustenhanced recommender systems. In particular, it has proven challenging to model
distrust propagation in a manner which is both logically consistent and psychologically
plausible. Furthermore, the naive modeling of distrust as negative trust raises a
number of challenges—both algorithmic and philosophical. Finally, it is an open
challenge how to incorporate trust and distrust relations in model-based methods
simultaneously. This article is concerned with these questions and gives an affirmative
solution to challenges involved with distrust-enhanced recommendation. In particular,
the proposed method makes it possible to simultaneously incorporate both trust and
distrust relationships in recommender systems to increase the prediction accuracy. To
the best of our knowledge, this is the first work that models distrust relations into the
matrix factorization problem along with trust relations at the same time.
The main intuition behind the proposed algorithm is that one can interpret the distrust relations between users as dissimilarity in their preferences. In particular, when
a user u distrusts another user v, it indicates that user u disagrees with most of the
opinions issued, or ratings made by user v. Therefore, the latent features of user u
obtained by matrix factorization must be as dissimilar as possible to v’s latent features. In other words, this intuition suggests directly incorporating the distrust into
recommendation by considering distrust as reversing the deviation of latent features.
However, when combined with the trust relations between users, due to the contradictory role of trust and distrust relations in propagating social information in the matrix
factorization process, this idea fails to effectively capture both relations simultaneously. This statement also follows from the preliminary experimental results in Victor
et al. [2011b] for memory-based CF methods that demonstrated regarding distrust as
an indication to reverse deviations in not the right way to incorporate distrust.
To remedy this problem, we settle for a less ambitious goal and propose another
method to facilitate the learning from both types of relations. In particular, we try
to learn latent features in a manner such that the latent features of users who are
distrusted by the user u have a guaranteed minimum dissimilarity gap from the worst
dissimilarity of users who are trusted by user u. By this formulation, we ensure that
when user u agrees on an item with one of his trusted friends, he will disagree on
the same item with his distrusted friends with a minimum predefined margin. We
note that this idea significantly departs from the existing works in distrust-enhanced
memory-based recommender systems [Victor et al. 2011b, 2013] that employ the distrust relations to either filter out or debug the trust relations to reduce the prediction
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:6

R. Forsati et al.

task to a trust-enhanced recommendation. In particular, the proposed method ranks
the latent features of trusted and distrusted friends of each user to reflect the effect of
relation in factorization.
Summary of Contributions. This work makes the following key contributions.
—A matrix factorization-based algorithm for simultaneous incorporation of trust and
distrust relationships in recommender systems. To the best of our knowledge, this is
the first model-based recommender algorithm that is able to leverage both types of
relationships in recommendation.
—An efficient stochastic optimization algorithm to solve the optimization problem
which makes the proposed method scalable to large social networks.
—An empirical investigation of the consistency of the social relationships with rating
information. In particular, we examine to what extent trust and distrust relations
between users are aligned with the ratings they issued on items.
—An exhaustive set of experiments on the Epinions dataset to empirically evaluate the
performance of the proposed algorithm and demonstrate its merits and advantages.
—A detailed comparison of the proposed algorithm to state-of-the-art trust/distrustenhanced memory/model-based recommender systems.
Outline. The rest of this article is organized as follows. In Section 2, we draw connections to and put our work in context of some of the most recent work on social
recommender systems. Section 3 formally introduces the matrix factorization problem,
an optimization-based framework to solve it, and its extension to incorporate the trust
relations between users. The proposed algorithm along with optimization methods are
discussed in Section 4. Section 5 includes our experimental result on the Epinions
dataset which demonstrates the merits of the proposed algorithm in alleviating the
data sparsity problem in rating matrix and generating more accurate recommendations. Finally, Section 6 concludes and discusses a few directions as future work.
2. RELATED WORK ON SOCIAL RECOMMENDATION

Earlier in the introduction, we discussed some of the main lines of research on recommender systems; here, we survey further lines of study that are most directly-related to
our work on social-enhanced recommendation. Many successful algorithms have been
developed over the past few years to incorporate social information in recommender
systems. After reviewing trust-enhanced memory-based approaches, we discuss some
model-based approaches for recommendation in social networks with trust relations.
Finally, we review major approaches in distrust modeling and distrust-enhanced recommender systems.
2.1. Trust-Enhanced Memory-Based Recommendation

Social network data has been widely investigated in the memory-based approaches.
These methods typically explore the social network and find a neighborhood of users
trusted (directly or indirectly) by a user and perform the recommendation by aggregating their ratings. These methods use the transitivity of trust and propagate trust
to indirect neighbors in the social network [Massa and Avesani 2004, 2009; Konstas
et al. 2009; Jamali and Ester 2009, 2010, 2011; Koren et al. 2009].
In Massa and Avesani [2004], a trust-aware collaborative filtering method for recommender systems is proposed. In this work, the collaborative filtering process is informed
by the reputation of users, which is computed by propagating trust. Konstas et al. [2009]
proposed a method based on the random walk algorithm to utilize social connection and
other social annotations to improve recommendation accuracy. However, this method
does not utilize the rating information and is not applicable to constructing a random
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:7

walk graph in real datasets. TidalTrust [Golbeck 2006] performs a modied breadthfirst search in the trust network to compute a prediction. To compute the trust value
between user u and v who are not directly connected, TidalTrust aggregates the trust
value between u’s direct neighbors and v weighted by the direct trust values of u and
its direct neighbors.
MoleTrust [Massa and Avesani 2004, 2005; Zhang and Koren 2007] applies the same
idea as TidalTrust, but MoleTrust considers all the raters up to a fixed maximumdepth given as an input, independent of any specific user and item. The trust metric
in MoleTrust consists of two major steps. First, cycles in trust networks are removed.
Therefore, removing trust cycles beforehand from trust networks can significantly
speed up the proposed algorithm because every user only needs to be visited once to
infer trust values. Second, trust values are calculated based on the obtained directed
acyclic graph by performing a simple graph random walk.
TrustWalker [Jamali and Ester 2009] combines trust-based and item-based recommendation to consider enough ratings without suffering from noisy data. Their experiments show that TrustWalker outperforms other existing memory-based approaches.
Each random walk on the user trust graph returns a predicted rating for user u on
target item i. The probability of stopping is directly proportional to the similarity between the target item and the most similar item j, weighted by the sigmoid function of
step size k. The more the similarity, the greater the probability of stopping and using
the rating on item j as the predicted rating for item i. As the step size increases, the
probability of stopping decreases. Thus ratings by closer friends on similar items are
considered more reliable than ratings on the target item by friends further away.
We note that all these methods are neighborhood-based methods which employ only
heuristic algorithms to generate recommendations. There are several problems with
this approach. The relationship between the trust network and the user-item matrix
has not been studied systematically. Moreover, these methods are not scalable to very
large datasets, since they may need to calculate the pairwise user similarities and
pairwise user trust scores.
2.2. Trust-Enhanced Model-Based Recommendation

Recently, researchers have exploited matrix factorization techniques to learn latent
features for users and items from the observed ratings, and fusing social relations
among users with rating data as will be detailed in Section 3. These methods can be
divided into two types: regularization-based methods and factorization-based methods.
Here we review some existing matrix factorization algorithms that incorporate trust
information in the factorization process.
2.2.1. Regularization-Based Social Recommendation. Regularization-based methods typically add a regularization term to the loss function and minimizes it. Most recently, Ma
et al. [2011a] proposed an idea based on social-regularized matrix factorization to make
recommendation based on social network information. In this approach, the social regularization term is added to the loss function, which measures the difference between
the latent feature vector of a user and those of his friends. A probability model similar
to the model in Ma et al. [2011a] is proposed by Jamali and Ester [2011]. The graph
Laplacian regularization term of social relations is added into the loss function in
Li and Yeung [2009] and minimizes the loss function by alternative projection algorithm. Zhu et al. [2011] used the same model in Li and Yeung [2009] and built
graph Laplacian of social relations using three kinds of kernel functions. In Liu et al.
[2013], the minimization problem is formulated as a low-rank semidefinite optimization
problem.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:8

R. Forsati et al.

2.2.2. Factorization-Based Social Recommendation. In factorization-based methods, social
relationships between users are represented as a social relation matrix, which is factored as well as the rating matrix. The loss function is the weighted sum of the social relation matrix factorization error and the rating matrix factorization error. For instance,
SoRec [Ma et al. 2008] incorporates the social network graph into the probabilistic matrix factorization model by simultaneously factorizing the user-item rating matrix and
the social trust networks by sharing a common latent low-dimensional user feature
matrix [Liu et al. 2013]. The experimental analysis shows that this method generates
better recommendations than the non-social filtering algorithms [Jamali and Ester
2010]. However, the disadvantage of this work is that although users’ social networks
are integrated into the recommender systems by factorizing the social trust graph,
the real-world recommendation processes are not reflected in the model. Two sets of
different feature vectors are assumed for users, which makes the interpretability of
the model very hard [Jamali and Ester 2010; Ma et al. 2009a]. This drawback not only
causes lack of interpretability in the model, but also affects the recommendation qualities. A better model named Social Trust Ensemble (STE) [Ma et al. 2009a] is proposed,
by making the latent features of a user’s direct neighbors affect the rating of the user.
Their method is a linear combination of a basic matrix factorization approach and a social network-based approach. Experiments show that their model outperforms the basic
matrix factorization-based approach and existing trust-based approaches. However, in
their model, the feature vectors of direct neighbors of u affect the ratings of u instead of
affecting the feature vector of u. This model does not handle trust propagation. Another
method for recommendation in social networks has been proposed in Ma et al. [2009b].
This method is not a generative model and defines a loss function to be minimized.
The main disadvantage of this method is that it punishes the users with lots of social
relations more than other users. Finally, SocialMF [Jamali and Ester 2010] is a matrix
factorization-based model which incorporates social influence by making the features
of every user depend on the features of his/her direct neighbors in the social network.
2.3. Distrust-Enhanced Social Recommendation

In contrast to incorporation of trust relations, unfortunately most of the literature on
social recommendation totally ignores the potential of distrust information in boosting
the accuracy of recommendations. In particular, only recently a few work have started
to investigate the rule of distrust information in the recommendation process, both
from theoretical and empirical viewpoints [Guha et al. 2004; Ziegler and Lausen 2005;
Nalluri 2008; Ziegler 2009; Ma et al. 2009b; Wierzowiecki and Wierzbicki 2010; Victor
et al. 2011b, 2011c, 2013; Verbiest et al. 2012]. Although these studies have shown
that distrust information can be plentiful, but there is a significant gap in clear understanding of distrust in recommender systems. The most important reasons for this
shortage are the lack of datasets that contain distrust information and dearth of a
unified consensus on modeling and propagation of distrust.
A formal framework of trust propagation schemes, introducing the formal and computational treatment of distrust propagation, has been developed in Guha et al. [2004].
In an extension of this work, Ziegler [2009] proposed clever adaptations in order to
handle distrust and sinks such as trust decay and normalization. In Wierzowiecki and
Wierzbicki [2010], a trust/distrust propagation algorithm called CloseLook is proposed,
which is capable of using the same kinds of trust propagation as the algorithm proposed
by Guha et al. [2004]. Leskovec et al. [2010a] extended the results of Guha et al. [2004]
using a machine-learning framework (instead of the propagation algorithms based
on an adjacency matrix) to enable the evaluation of the most informative structural
features for the prediction task of positive/negative links in online social networks. A
comprehensive framework that computes trust/distrust estimations for user pairs in
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:9

the network using trust metrics is built in Victor et al. [2011c]: given two users in the
trust network, we can search for a path between them and propagate the trust scores
along this path to obtain an estimation. When more than one path is available, we
may single out the most relevant ones (selection), and aggregation operators can then
be used to combine the propagated trust scores into one final trust score, according to
different trust score propagation operators.
Ma et al. [2009b] was the first seminal work to demonstrate that the incorporation
of distrust information could be beneficial based on a model-based recommender system. In Victor et al. [2011c, 2013], the same question is addressed in memory-based
approaches. In particular, Victor et al. [2013] embarked upon the distrust-enhanced recommendation and showed that with careful incorporation of distrust metric, distrustenhanced recommender systems are able to outperform their trust-only counterparts.
The main rational behind the algorithm proposed in Victor et al. [2013] is to employ
the distrust information to debug or filter out the users’ propagated web of trust. It is
also has been realized that the debugging methods must exhibit a moderate behavior
in order to be effective. Verbiest et al. [2012] addressed the problem of considering the
length of the paths that connect two users for computing trust-distrust between them,
according to the concept of trust decay. This work also introduced several aggregation
strategies for trust scores with variable path lengths.
Finally we note that the aforementioned works try to either model or utilize
trust/distrust information. In recent years, there has been an upsurge of interest in
predicting trust and distrust relations in a social network [Leskovec et al. 2010a;
DuBois et al. 2011; Bachi et al. 2012; Patil et al. 2013]. For instance, Leskovec et al.
[2010a] casts the problem as a sign prediction problem (i.e., +1 for friendship and −1
for opposition) and utilizes machine learning methods to predict the sign of links in the
social network. In DuBois et al. [2011] a new method is presented for computing both
trust and distrust by combining an inference algorithm that relies on a probabilistic
interpretation of trust based on random graphs with a modified spring-embedding algorithm to classify an edge. Another direction of research is to examine the consistency
of social relations with theories in social psychology [Cartwright and Harary 1956;
Leskovec et al. 2010b]. Our work significantly departs from these works on prediction
or consistency analysis of social relations, and aims to effectively incorporate distrust
information in matrix factorization for effective recommendation.
3. MATRIX FACTORIZATION-BASED RECOMMENDER SYSTEMS

This section provides a formal definition of collaborative filtering, the primary recommendation method we are concerned with in this article, followed by solution methods
for low-rank factorization that are proposed in the literature to address the problem.
(See Table I for Common notations and their meanings.)
3.1. Matrix Factorization for Recommendation

In collaborative filtering, we assume that there is a set of n users U = {u1 , . . . , un} and
a set of m items I = {i1 , . . . , im}, where each user ui expresses opinions about a set of
items. In this article, we assume opinions are expressed through an explicit numeric
rating (e.g., scale from one to five), but other rating methods such as hyperlink clicks are
possible as well. We are mainly interested in recommending a set of items for an active
user such that the user has not rated these items before. To this end, we are aimed
at learning a model from the existing ratings, that is, offline phase, and then use the
learned model to generate recommendations for active users, that is, online phase. The
rating information is summarized in an n × m matrix R ∈ Rn×m, 1 ≤ i ≤ n, 1 ≤ j ≤ m,
where the rows correspond to the users and the columns correspond to the items, and
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:10

R. Forsati et al.
Table I. Summary of Notations Consistently Used in the Article and Their Meaning
Symbol

Meaning

U = {u1 , . . . , un}, n
I = {i1 , . . . , im}, m
k
R ∈ Rn×m
R , |R |
U ∈ Rn×k
V ∈ Rm×k
S ∈ {−1, +1}n×n
S , |S |
W ∈ Rn×n
+
N (u) ⊆ [n]
N+ (u) ⊆ [n]
N− (u) ⊆ [n]
D : Rk × Rk → R+

The set of users in system and the number of users
The set of items and the number of items
The dimension of latent features in factorization
The partially observed rating matrix
The set of observed entires in rating matrix and its size
The matrix of latent features for users
The matrix of latent features for items
The social network between n users
The set of extracted triplets from the social relations and its size
The pairwise similarity matrix between users
Neighbors of user u in the social graph
The set of trusted neighbors by user u in the social graph
The set of distrusted neighbors by user u in the social graph
The measurement function used to assess the similarly of latent features

the ( p, q)th entry is the rate given by user up to the item iq . We note that the rating
matrix is partially observed, and it is sparse in most cases.
An efficient and effective approach to recommender systems is to factorize the useritem rating matrix R by a multiplicative of k-rank matrices R ≈ UV , where U ∈ Rn×k
and V ∈ Rm×k utilize the factorized user-specific and item-specific matrices, respectively,
to make further missing data prediction. The main intuition behind a low-dimensional
factor model is that there is only a small number of factors influencing the preferences,
and that a user’s preference vector is determined by how each factor applies to that
user. This low rank assumption makes it possible to effectively recover the missing
entires in the rating matrix from the observed entries. We note that the celebrated
Singular Value Decomposition (SVD) method for factorizing the rating matrix R is
not applicable here due to the fact that the rating matrix is partially available and
we are only allowed to utilize the observed entries in factorization process. There are
two basic formulations to solve this problem: optimization based (see e.g., [Rennie and
Srebro 2005; Liu et al. 2013; Ma et al. 2008; Koren et al. 2009]) and probabilistic [Mnih
and Salakhutdinov 2007]. In the following sections, we first review the optimizationbased framework for matrix factorization and then discuss how it can be extended to
incorporate trust information.
3.2. Optimization-Based Matrix Factorization

Let R be the set of observed ratings in the user-item matrix R ∈ Rn×m, that is,
R = {(i, j) ∈ [n] × [m] : Ri j has been observed},
where n is the number of users and mis the number of items to be rated. In optimizationbased matrix factorization, the goal is to learn the latent matrices U and V by solving
the following optimization problem:
⎡
⎤
 

1
λ
λ
2
U
V

min ⎣L(U, V) =
UF +
VF ⎦ ,
Ri j − Ui,:
(1)
V j,: +
U,V
2
2
2
(i, j)∈R

where  · F is the Frobenius norm of a matrix, that is, AF =

	
 

n
m
i=1

j=1

|Ai j |2 .

The optimization problem in Eq. (1) constitutes of three terms: the first term aims
to minimize the inconsistency between the observed entries and their corresponding
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:11

value obtained by the factorized matrices. The last two terms regularize the latent
matrices for users and items, respectively. The parameters λU and λV are regularization
parameters that are introduced to control the regularization of latent matrices U and
V, respectively. We would like to emphasize that the problem in Eq. (1) is non-convex
jointly in both U and V. However, despite its non-convexity, the formulation in Eq. (1)
is widely used in practical collaborative filtering applications, as the performance is
competitive, or better as compared to trace-norm minimization, while scalability is
much better. For example, as indicated in Koren et al. [2009], to address the Netflix
problem, Eq. (1) has been applied with a fair amount of success to factorize datasets
with 100 million ratings.
3.3. Matrix Factorization with Trust Side Information

Recently it has been shown that just relying on the rating matrix to build a recommender system is not as accurate as expected. The main reason for this claim is the
known cold-start users problem and the sparsity of the rating matrix. Cold-start users
are one of the most important challenges in recommender systems. Since cold-start
users are more dependent on the social network compared to users with more ratings,
the effect of using trust propagation gets more important for cold-start users. Moreover,
in many real-life systems, a very large portion of users do not express any ratings, and
they only participate in the social network. Hence, using only the observed ratings does
not allow us to learn the user features.
One of the most prominent approaches to tacking the data sparsity problem in matrix factorization is to compensate for the lack of information in rating matrix with
other sources of side information which are available to the recommender system. It
has been recently shown that social information, such as trust relationship between
users, is a rich source of side information to compensate for the sparsity. The already
mentioned traditional recommendation techniques are all based on working on the
user-item rating matrix, and ignore the abundant relationships among users. Trustbased recommendation usually involves constructing a trust network where nodes are
users and edges represent the trust placed on them. The goal of a trust-based recommendation system is to generate personalized recommendations by aggregating the
opinions of other users in the trust network. The intuition is that users tend to adopt
items recommended by trusted friends rather than strangers, and that trust is positively and strongly correlated with user preferences. Recommendation techniques that
analyze trust networks were found to provide very accurate and highly personalized
results.
To incorporate the social relations in the optimization problem formulated in Eq. (1),
a few papers [Ma et al. 2009b, 2011a; Jamali and Ester 2011; Liu et al. 2013; Zhu et al.
2011] proposed the social regularization method which aims at keeping the latent
vector of each user similar to his/her neighbors in the social network. The proposed
models force the user feature vectors to be close to those of their neighbors to be able to
learn the latent user features for users with no or very few ratings [Jamali and Ester
2011]. More specifically, the optimization problem becomes
L(U, V) =

2 λU
1  
λV

UF +
VF
V j,: +
Ri j − Ui,:
2
2
2
(i, j)∈R



n 


λS  
1

+
U j,: 
Ui,: − |N (i)|
,
2

i=1 
j∈N (i)

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

(2)

17:12

R. Forsati et al.

where λ S is the social regularization parameter and N (i) is the subset of users who has
relationship with the ith user in the social graph.
The rationale behind this social regularization idea is that every user’s taste is
relatively similar to the average taste of his friends in the social network. We note that
in using this idea, latent features of users indirectly connected in the social network
will be dependent, and hence the trust gets propagated. A more reasonable and realistic
model should treat all friends differently based on how similar they are. Let us assume
the weight of a relationship between two users i and j is captured by Wi j , where
W ∈ Rn×n denotes the social weight matrix. It is easy to extend the model in Eq. (2) to
treat friends differently based on the weight matrix W as
2 λU
1  
λV

UF +
VF
Ri j − Ui,:
L(U, V) =
V j,: +
(3)
2
2
2
(i, j)∈R




n

λS  
j∈N (i) Wi j U j,: 

+
Ui,: − 

.

2
j∈N (i) Wi j 
i=1

An alternative formulation is to regularize each user’s friends individually, resulting
in the following objective function [Ma et al. 2011a]:
2 λU
1  
λV

L(U, V) =
UF +
VF
Ri j − Ui,:
V j,: +
2
2
2
(i, j)∈R

n

2
λS 
+
Wi j Ui,: − U j,:  ,
2
i, j=1

where we simply assumed that for any j ∈
/ N (i), Wi j = 0.
As mentioned earlier, the objective function in L(U, V) is not jointly convex in both
U and V, but it is convex in each of them fixing the other one. Therefore, to find a local
solution, one can stick to the standard gradient descent method to find a solution in an
iterative manner as follows:
Ut+1 ← Ut − ηt ∇U L(U, V)|U=Ut ,V=Vt ,
Vt+1 ← Vt − ηt ∇V L(U, V)|U=Ut ,V=Vt .
4. MATRIX FACTORIZATION WITH TRUST AND DISTRUST SIDE INFORMATION

In this section, we describe the proposed algorithm for social recommendation which
is able to incorporate both trust and distrust relationships in the social network along
with the partially observed rating matrix. We then present two strategies to solve the
derived optimization problem, one based on the gradient descent optimization algorithm which generates more accurate solutions but is computationally cumbersome,
and another based on the stochastic gradient descent method which is computationally
more efficient for large rating and social matrices but suffers from slow convergence
rate.
4.1. Algorithm Description

As already discussed, the vast majority of related work in the field of matrix factorization for recommendation has primarily focused on trust propagation and has
simply ignored the distrust information between users or, intrinsically, is not capable
of exploiting it. Now, we aim at developing a matrix factorization-based model for recommendation in social rating networks to utilize both trust and distrust relationships.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:13

We incorporate the trust/distrust relationship between users in our model to improve
the quality of recommendations. While intuition and experimental evidence indicate
that trust is somewhat transitive, distrust is certainly not transitive. Thus, when we
intend to propagate distrust through a network, questions about transitivity and how
to deal with conflicting information abound.
To inject social influence in our model, the basic idea is to find appropriate latent
features for users such that each user is brought closer to the users she/he trusts and
separated from the users that she/he distrusts and who have different interests. We
note that simply incorporating this idea in matrix factorization by naively penalizing
the similarity of each user’s latent features to his distrusted friends’ latent features
fails to reach the desired goal. The main reason being that distrust is not as transitive
as trust, that is, distrust can not directly replace trust in trust propagation approaches,
and utilizing distrust requires careful consideration (trust is transitive, i.e., if user u
trusts user v and v trusts w, there is a good chance that u will trust w, but distrust is
certainly not transitive, i.e., if u distrusts v and v distrusts w, then w may be closer to
u than v or maybe even farther away). It is noticeable that this statement is consistent
with the preliminary experimental results in Victor et al. [2011b] for memory-based
CF methods that indicate regarding distrust as an indication to reverse deviations in
not the right way to incorporate distrust. Therefore, we pursue another approach to
model the distrust in the recommendation process.
The main intuition behind the proposed framework stems from the observation that
trust relations between users can be treated as agreement on items and distrust relations can be considered as disagreement on items. Then, the question becomes how can
we guarantee that when a user agrees on an item with one of his/her friends, he/she will
disagree on the same item with his/her distrusted friends with a reasonable margin.
We note that this margin should be large enough to make it possible to distinguish
between two types of friends. In terms of latent features, this observation translates to
having a margin between the similarity and dissimilarity of users’ latent features to
his/her trusted and distrusted friends.
Alternatively, one can view the proposed method from the viewpoint of connectivity of
latent features in a properly designated graph. Intuitively, certain features or groups of
features should influence how users connect in the social network, and thus it should
be possible to learn a mapping from features to connectivity in the social network
such that the mapping respects the underlying structure of the social network. In the
basic matrix factorization algorithm for recommendation, we can consider the latent
features as isolated vertices of a graph where there is no connection between nodes.
This can be generalized to the social-enhanced setting by considering the social graph
as the underlying graph between latent features with two types of edges (i.e., trust
and distrust relations correspond to positive and negative edges, respectively). Now
the problem reduces to learning the latent features for each user u such that users
trusted by u in the social network (with positive edges) are close and users which are
distrusted by u (with negative edges) are more distant. Learning latent features in this
manner respects the inherent topology of the social network.
Figure 1 shows an example to illustrate the intuition behind this idea. For ease of
exposition, we only consider the latent features for the user u1 . From the trust network
in Figure 1(a), we can see that user u1 trusts the list of users N+ = {u2 , u4 , u6 , u7 },
and from the distrust network in Figure 1(b), we see that user u1 distrusts the list
of users N− = {u3 , u5 }. The goal is to learn the latent features that obeys two goals:
(i) it minimizes the prediction error on observed entries in the rating matrix, (ii) it
respects the underlying structure of the trust and distrust networks between users.
In Figure 1(d), the latent features are depicted in the Euclidean space from the viewpoint of user u1 . As shown in Figure 1(d), for user u1 , the latent features of his/her
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:14

R. Forsati et al.

Fig. 1. A simple example with seven users {u1 , u2 , . . . , u7 } and six items {i1 , i2 , . . . , i6 } to illustrate the main
intuition behind the proposed algorithm. The inputs of the algorithm are (a) trust network, (b) distrust
network, and (c) partially observed rating matrix R, respectively. As shown in (d) for user u1 , the learned
latent features for all his trusted friends {u2 , u4 , u6 , u7 } are closer to u1 ’s latent features than his distrusted
friends {u3 , u5 } with a margin of 1.

trusted friends N+ lie inside the solid circle centered at u1 , and the latent features of
his/her distrusted friends N− lie outside the dashed circle. The gap between two circles
guarantees that there always exists a safe margin between u1 ’s agreements with his
trusted and distrusted friends. One simple way to impose these constraints on the latent features of users is to generate a set of triplets for any combination of trusted and
distrusted friends (e.g., one such triplet for user u1 can be constructed as (u1 , u2 , u5 ))
and force the margin constraint to hold for all extracted triplets. This ensures that the
minimum margin gap will definitely exist between the latent features of all the trusted
and distrusted friends as desired and makes it possible to incorporate both types of
relationships between users in the matrix factorization.
It is worth mentioning that similar to the social-enhanced recommender systems
previously discussed, the proposed algorithm is also based on hypotheses about the
existence and the correlation of trust/distrust relations and ratings in the data. The
empirical investigation of correlation between social relations and rating information
has been the focus of a bulk of recent research including [Ziegler and Golbeck 2007;
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:15

Patil et al. 2013; Ma 2013], where the results reinforce the hypothesis that ratings
from trusted people count more than those from others and in particular distrusted
neighbors. We have also conducted experiments, as will be detailed in Section 5.5, to empirically investigate the correlation/alignment between social relations and the rating
information issued by users which supports our strategy in exploiting the trust/distrust
relations in matrix factorization.
We now formalize the proposed solution. As the first ingredient, we need a measure
to evaluate the consistency between the latent features of users, that is, the matrix U,
and the trust and distrust constraints existing between users in the social network. To
this end, we introduce a monotonically-increasing convex loss function (z) to measure
the discrepancy between the latent features of different users. Let ui , u j , and uk be
three users in the model such that ui trusts u j but distrusts uk. The main intuition
behind the proposed framework is that the latent features of ui , that is, Ui,: must be
more similar to u j ’s latent features than latent features for user uk. For each such
a triplet, we penalize the objective function by (D(Ui,: , U j,: ) − D(Ui,: , Uk,: )), where the
function D : Rk ×Rk → R+ measures the similarity between two latent vectors assigned
to two different users, and  : R → R+ is a penalty function that is utilized to assess
the violation of latent vectors of trusted and distrusted users. Example loss functions
include hinge loss (z) = max(0, 1 − z) and logistic loss (z) = log(1 + e−z ), which are
widely used convex surrogates of 0-1 loss function in learning community.
Let S denote the set of extracted triplets from the social relations, that is,


S = (i, j, k) ∈ [n] × [n] × [n] : Si j = 1 & Sik = −1 .
Here, a positive relationship means friends or a trusted relationship and a negative
relationship means foes or a distrust relationship. Then, our goal becomes to find a
factorization of matrix R such that the learned latent features of users are consistent
with the constraints in S , where the consistency is reflected in the loss function. This
results in the following optimization problem:
2 λU
1  
λV

L(U, V) =
UF +
VF
Ri j − Ui,:
V j,: +
2
2
2
(i, j)∈R

+

λS
|S |



(D(Ui,: , U j,: ) − D(Ui,: , Uk,: )).

(4)

(i, j,k)∈S

Let us make this general formulation more specific by setting (·) and D(·, ·) to be the
hinge loss and the Euclidian distance, respectively. Under these two assumptions, the
objective can be formulated as
2 λU
1  
λV

L(U, V) =
VF
Ri j − Ui,:
V j,: + UF +
2
2
2
(i, j)∈R



R(U,V)

λS
+
|S |





max 0, 1 − Ui,: − U j,: 2 + Ui,: − Uk,: 2 .

(5)

(i, j,k)∈S

Here the constraints have been written in terms of hinge-losses over triplets, each
consisting of a user, his/her trusted friend, and his/her distrusted friend. Solving the
optimization problem in Eq. (5) outputs the latent features for users and items that
can be utilized to estimate the missing values in the user-item matrix. Comparing the
formulation in Eq. (5) to the existing factorization-based methods discussed earlier
reveals two main features of the proposed formulation. First, it aims to minimize
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:16

R. Forsati et al.

the error on the observed ratings and to respect the inherent structure of the social
network among the users. The trade-off between these two objectives is captured by
the regularization parameter λ S which is required to be tuned effectively.
In a similar way, applying the logistic loss to the general formulation in Eq. (4) yields
the following objective:
2 λU
1  
λV

L(U, V) =
UF +
VF
V j,: +
Ri j − Ui,:
2
2
2
(i, j)∈R

+

λS
|S |






log 1 + exp Ui,: − Uk,: 2 − Ui,: − U j,: )2 .

(6)

(i, j,k)∈S

Remark 4.1. We note that in several applications of recommender systems, besides
the observed ratings, a description of the users and/or the objects through attributes
(e.g., gender, age) or measures of similarity is available that could potentially benefit the
process of recommendation (see, e.g., [Agarwal and Chen 2010] for a few interesting
applications). In that case, it is tempting to take advantage of both known ratings
and descriptions to model the preferences of users. A natural way to incorporate the
available metadata is to kernalize the similarity measure between latent features based
on a positive definite kernel between pairs that can be deduced from the metadata.
More specifically, instead of simply using Euclidian distance as the similarity measure
between latent features in Eq. (5), we can use the kernel matrix K obtained from the
Laplacian of the graph obtained from the metadata to measure the similarity
 


D(Ui,: , U j,: ) = Ui,: − U j,: K Ui,: − U j,: ,


where K = (D − W)−1 , with D as a diagonal matrix with Di,i = nj=1 Wi j . Here, W
captures the pairwise weight between users in the similarity graph between users that
is computed based on the available metadata about users.
Remark 4.2. We would like to emphasize that it is straightforward to generalize the
proposed framework to incorporate similarity and dissimilarity information between
items. What we need is to extract the triplets from the trust/distrust links between
items and repeat the same process we did for users. This will add another term to the
objective in terms of latent features of items V, as shown in the following generalized
formulation:
2 λU
1  
λV

L(U, V) =
UF +
VF
Ri j − Ui,:
V j,: +
2
2
2
(i, j)∈R

+
+

λS
|S |
λI
|I |





max 0, 1 − Ui,: − U j,: 2 + Ui,: − Uk,: 2

(i, j,k)∈S





max 0, 1 − Vi,: − V j,: 2 + Vi,: − Vk,: 2 ,

(i, j,k)∈I

where λ I is the regularization parameter and I is the set of triplets extracted from
the similar/dissimilar links between items. The similarity/dissimilarity links between
items can be constructed according to tags issued by users or associated with items,
and categories. For example, if two items are attached with a same tag, there is a trust
link between them, and otherwise a distrust link. Alternatively, trust/distrust links
can be extracted by measuring similarity/dissimilarity based on the item properties or
profile if provided. This could further improve the accuracy of recommendations.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:17

ALGORITHM 1: GD-based Matrix Factorization with Trust and Distrust Propagation
1: Input: R: partially observed rating matrix, S
2: Output: U and V
3: for t = 1, . . . , T do
4:
Compute the gradients ∇U R(Ut , Vt ) and ∇V R(Ut , Vt ).
5:
Compute ∇U by Eq. 7
6:
Compute ∇V by Eq. 8
7:
Update:
Ut+1 = Ut − ηt ∇U |U=Ut ,V=Vt
Vt+1 = Vt − ηt ∇V |U=Ut ,V=Vt
8: end for
9: return UT +1 and VT +1 .

4.2. Batch Gradient Descent-Based Optimization

In optimization for supervised machine learning, there exist two regimes in which
popular algorithms tend to operate: the stochastic approximation regime, which
samples a small dataset per iteration, typically a single data point, and the batch or
sample average approximation regime, in which larger samples are used to compute an
approximate gradient. The choice between these two extremes outlines the well-known
trade-off between inexpensive noisy steps and expensive but more reliable steps.
Two preliminary examples of these regimes are the Gradient Descent (GD) and the
Stochastic Gradient Descent (SGD) methods, respectively. Both GD and SGD methods
start with some initial point and iteratively update the solution using the gradient
information at intermediate solutions. The main difference is that GD requires a full
gradient information at each iteration, while SGD only requires an unbiased estimate
of the full gradient which can be done by sampling.
We now discuss the application of the GD algorithm for solving the optimization
problem in Eq. (5), as detailed in Algorithm 1. Recall that the objective function is
not jointly convex in both U and V. On the other hand, the objective is convex in one
parameter by fixing the other one. Therefore, we follow an iterative method to minimize
the objective. At each iteration, first by fixing V, we take a step in the direction of the
negative gradient for U and repeat the same process for V by fixing U.
For ease of exposition, we introduce further notation. For any triplet (i, j, k) ∈ S , we
note that Ui,: − U j,: 2 − Ui,: − Uk,: 2 can be written as Tr(CU U), where Tr(·) denotes
the trace of the input matrix and C is a sparse auxiliary matrix defined for each triplet
with all entries equal to zero except: Cik = Cki = C j j = 1 and Ckk = Ci j = C ji = −1.
Having defined this notation, we can write the objective in Eq. (5) as
L(U, V) = R(U, V) +

λV
λS
λU
UF +
VF +
2
2
|S |






max 0, 1 − Tr Cikj U U .

(i, j,k)∈S

where Cikj is the C matrix previously defined which is associated with triplet (i, j, k).
To apply the GD method, we need to compute the gradient of L(U, V) with respect to
U and V, which we denote by ∇U = ∇U L(U, V) and ∇V = ∇V L(U, V), respectively. We
have
∇U = ∇U R(U, V) + λU U −

λS
|S |





k
1[Tr(Cikj U U)<1] UCik
j + UCi j ,

(i, j,k)∈S

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

(7)

17:18

R. Forsati et al.

where 1[·] is the indicator function which takes a value of one if its argument is true,
and zero otherwise. Similarly for ∇V , we have
∇V = ∇V R(U, V) + λV V.

(8)

The main shortcoming of the GD method is its high computational cost per iteration
due to the gradient computation (i.e., step (7)) which is expensive when the size of
social constraints S is large. We note that the size of S can be as large as O(n3 )
by considering all triplets in the social graph. In the next section, we provide an
alternative solution to resolving this issue using the stochastic gradient descent and
mini-batch SGD methods which are more efficient than the GD method in terms of the
computational cost per iteration but with a slow convergence rate in terms of target
approximation error.
4.3. Stochastic and Mini-Batch Optimization

As discussed, when the size of the social network is very large, the size of S may cause
computational problems in solving the optimization problem in Eq. (5) using the GD
method. The reason is essentially the fact that computing the gradient at each iteration
requires going through all the triplets in S , which is infeasible for large networks.
To alleviate this problem, we propose a stochastic gradient-based [Nemirovski et al.
2009] method for solving the optimization problem. The main idea is to choose a
fixed subset of triplets for gradient computation instead of all |S | triplets at each
iteration [Cotter et al. 2011]. More specifically, at each iteration, we sample B triplets
uniformly at random from S to compute the next solution. We note that this strategy
generates unbiased estimates of the true gradient and makes each iteration of the
algorithm computationally more efficient compared to the full gradient counterpart.
In the simplest case, the SGD algorithm, only one triplet is chosen at each iteration
to generate an unbiased estimate of the full gradient. We note that in practice, SGD
is usually implemented based on data shuffling, that is, making the sequence of the
training samples random and then training the model by going through the training
samples one by one. An intermediate solution, known as mini-batch SGD, chooses
a subset of triplets to compute the gradient. The promise is that by selecting more
triplets at each iteration, on one hand the variance of stochastic gradients decreases
promotional to the number of sampled triplets, and on the other hand the algorithm
enjoys the light computational cost of basic SGD method.
The detailed steps of the algorithm are shown in Algorithm 2. The mini-batch SGD
method improves the computational efficiency by grouping multiple constraints into a
mini-batch and only updating the U and V once for each mini-batch. For brevity, we
will refer to this algorithm as Mini-SGD. More specifically, the Mini-SGD algorithm,
instead of computing the full gradient over all triplets, samples B triplets uniformly at
random from S , where 1 ≤ B ≤ |S | is a parameter that needs to be provided to the
algorithm, and computes the stochastic gradient as


λS 
k
1[Tr(Cikj Ut Ut )<1] UCik
∇t =
j + UCi j ,
B
(i, j,k)∈ B

where  B is the set of B sampled triplets from S . We note that



λS
k
1[Tr(Cikj Ut Ut )<1] UCik
E[∇t ] =
j + UCi j ,
|S |
(i, j,k)∈S

that is, ∇t is an unbiased estimate of the full gradient in the right-hand side. When
B = |S |, each iteration handles the original objective function, and Mini-SGD reduces
to the batch GD algorithm. We note that both GD and SGD share the same convergence
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:19

ALGORITHM 2: Mini-SGD-Based Matrix Factorization with Trust and Distrust Propagation
1: Input: R: partially observed rating matrix, S , min batch size B
2: Output: U and V
3: for t = 1, . . . , T do
4:
∇t ← 0
5:
for b = 1, . . . , B do
6:
(i, j, k) ← Sample random triplet from S
7:
if (1 − Ui,: − U j,: )2 + Ui,: − Uk,: 2 > 0) then
8:
∇t ← Ut Cikj U
t
9:
end if
10:
end for
11:
Compute the gradients ∇U R(Ut , Vt ) and ∇V R(Ut , Vt ).
12:
Update:


λS
Ut+1 = Ut − ηt ∇U R(Ut , Vt ) + λU Ut +
∇t
B|S |
13:

Update:



Vt+1 = Vt − ηt ∇V R(Ut , Vt ) + λV Vt

14: end for
15: return UT +1 and VT +1 .

rate in terms of the√number of iterations in expectation for non-smooth optimization
problems (i.e., O(1/ T ) after T iterations), but the SGD method requires much less
running time to convergence compared to the GD method due to the efficiency of its
individual iterations.
5. EXPERIMENTAL RESULTS

In this section, we conduct exhaustive experiments to demonstrate the merits and
advantages of the proposed algorithm. We conduct the experiments on the wellknown Epinions2 dataset, aiming to accomplish and answer the following fundamental
questions.
(1) Prediction accuracy. How does the proposed algorithm perform in comparison to
the state-of- the-art algorithms with/without incorporating trust and distrust relationships between users. Whether or not the trust/distrust social network could
help in making more accurate recommendations?
(2) Correlation of social relations with rating information. To what extent are the
trusted and distrusted friends of a user u aligned with the ratings user u issued
for the reviews written by his friends? A positive answer to this question indicates
that two users will issue similar (dissimilar) ratings if they are connected by a trust
(distrust) relation and prefer to behave similarly.
(3) Model selection. What role do the regularization parameters λ S , λU , and λV play in
the accuracy of the proposed recommender system and what is the best strategy to
tune these parameters?
(4) Handling cold-start users. How does exploiting social relationships in the prediction
process affect the performance of recommendation for cold-start users?
(5) Trading trust for distrust. To what extent can the distrust relations compensate for
the lack of trust relations?
2 http://www.trustlet.org/wiki/Epinions

datasets.

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:20

R. Forsati et al.

(6) Efficiency of optimization. What is the trade-off between accuracy and efficiency by
moving from the gradient descent to the stochastic gradient descent with different
batch sizes?
In the following sections, we intend to answer these questions. We begin by introducing
the dataset we use in our experiments and the metrics we employ to evaluate the
results, followed by the detailed experimental results.
5.1. Dataset Description and Experimental Setup

The Epinions Dataset. We begin by discussing the dataset we have chosen for our
experiments. To evaluate the proposed algorithm on trust and distrust-aware recommendations, we use the Epinions dataset [Guha et al. 2004], a popular e-commerce site
and customer review website where users share opinions on various types of items such
as electronic products, companies, and movies, through writing reviews about them or
assigning a rating to the reviews written by other users. The rating values in Epinions
are discrete values ranging from not helpful (1/5) to most helpful (5/5). These ratings
and reviews could potentially influence future customers when they are about to decide
whether a product is worth buying or a movie is worth watching.
Epinions allows users to evaluate other users based on the quality of their reviews
and to make trust and distrust relations with other users in addition to the ratings.
Every member of Epinions can maintain a “trust” list of people he/she trusts that
is referred to as web of trust (social network with trust relationships) based on the
reviewers with consistent ratings or “distrust” list known as block list (social network
with distrust relationships) for reviewers whose reviews were consistently found to
be inaccurate or low quality. The fact that the dataset contains explicit positive and
negative relations between users makes it very appropriate for studying issues in
trust- and distrust-enhanced recommender systems. Epinions is thus an ideal source
for experiments on social recommendation. We remark that the Epinions dataset only
contains bivalent relations (i.e., contains only full trust and full distrust, and no gradual
statements).
To conduct the coming experiments, we sampled a subset of the Epinions dataset
with n = 121, 240 users and m = 685, 621 different items. The total number of observed
ratings in the sampled dataset is 12,721,437, which approximately includes 0.02% of
all entries in the rating matrix R which demonstrates the sparsity of the rating matrix.
We note that the selected items are the most frequently rated overall. The statistics
of the dataset are given in Table II. The social statistics of the this data source are
summarized in Table III. The frequencies of ratings for users are shown are Table IV.
In the user distrust network, the total number of issued distrust statements is 96,823.
As to the user trust network, the total number of issued trust statements is 481,799.
Experimental Setup. To better evaluate the effect of utilizing the social side information in recommendation accuracy, we employ different amount of training data 90%,
80% , 70%, and 60% to create four different training sets that are increasingly sparse,
but the social network remains the same in all of them. Training data 90%, for example, means we randomly select 90% of the ratings from the sampled Epinions dataset
as the training data to predict the remaining 10% of ratings. The random selection
was carried out five times independently to have a fair comparison. Also, since our
preliminary results on a smaller dataset revealed that hinge loss performs better than
exponential loss, in the rest of experiments, we stick to this loss function. However, we
note that exponential loss is slightly faster in optimizing the corresponding objective
function thanks to its smoothness, but it was negligible considering its worse accuracy
compared to hinge loss. All implementations are in Matlab, and all experiments were
performed on a 4-core 2.0GHZ of a load-free machine with a 12G of RAM.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:21

Table II. Statistics of Sample Data from Epinions Dataset
Used in Our Experiments
Statistic
Number of users
Number of items
Number of ratings
Number of trust relations
Number of distrust relations
Minimum number of ratings by users
Minimum number of ratings for items
Maximum number of ratings by users
Maximum number of ratings for items
Average number of ratings by users
Average number of ratings for items

Quantity
121,240
685,621
12,721,437
481,799
96,823
1
1
148735
945
85.08
15.26

Table III. Maximum and Average Trust and Distrust Relations
for Users in the Sampled Dataset
Statistics

Trust per user

Be Trusted per user

Max
Min
Average

1983
1
4.76

2941
0
4.76

Max
Min
Average

Distrust per user
1188
1
0.91

Be Distrusted per user
429
0
0.91

Table IV. Frequencies of User’s Rating
# of Ratings
# of Users

0–10
4,198,074
(≈33%)

11–20
3,053,144
(≈24%)

21–30
2,289,858
(≈18%)

31–40
1,526,572
(≈12%)

41–50
534,300
(≈4.2%)

51–60
267,150
(≈2.1%)

# of Ratings
# of Users

61–70
157,745
(≈1.24%)

71–80
143,752
(≈1.13%)

81–90
104,315
(≈0.82%)

91–100
43,252
(≈0.34%)

101–200
21,626
(≈0.17%)

201–300
10,686
(≈0.084%)

5.2. Metrics

5.2.1. Metrics for Rating Prediction. We employ two well-known measures, mean absolute
error (MAE) and root mean squared error (RMSE) [Herlocker et al. 2004] to measure the prediction accuracy of the proposed approach in comparison with other basic
collaborative filtering and trust/distrust-enhanced recommendation methods.
MAE is a very appropriate and useful measure for evaluating prediction accuracy in
offline tests [Herlocker et al. 2004; Massa and Avesani 2004]. To calculate MAE, the
predicted rating is compared with the real rating and the difference (in absolute value)
considered as the prediction error. Then, these individual errors are averaged over all
predictions to obtain the overall MAE value. More precisely, let T denote the set of
ratings to be predicted, that is, T = {(i, j) ∈ [n] × [m], Ri j needs to be predicted} and let
R̂ denote the prediction matrix obtained by algorithm after factorization. Then,


(i, j)∈T |Ri j − R̂i j |
,
MAE =
|T |
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:22

R. Forsati et al.

where Ri j is the real rating assigned by user i to item j, and R̂i j is the rating user i
would assign to item j that is predicted by the algorithm.
The RMSE metric is defined as


2
(i, j)∈T (Ri j − R̂i j )
RMSE =
.
|T |
The first measure (MAE) considers every error of equal value, while the second one
(RMSE) emphasizes larger errors. We would like to emphasize that even small improvements in RMSE are considered valuable in the context of recommender systems.
For example, the Netflix prize competition offered $ 1,000,000 reward for a reduction
of the RMSE by 10% [Victor et al. 2013].
5.2.2. Metrics for Evaluating the Correlation of Ratings with Trust/Distrust Relations. As part of
our experiments, we investigate how the explicit trust/distrust relations between users
in the social network are aligned with the implicit trust/distrust relations between
users conveyed from the rating information. We use recall, mean average precision
(MAP) [Manning et al. 2008], and normalized discount cumulative gain (NDCG) to
evaluate the ranking results. Recall is defined as the number of relevant friends divided
by the total number of friends in the social network. Precision is defined as the number
of relevant friends (trusted or distrusted) divided by the number of friends in the social
network. Given a user u, let ri be the relevance score of the friend ranked at position i,
where ri = 1 if the user is relevant to the u and ri = 0 otherwise. Then we can compute
the average precision (AP) as


i ri × Precision@i
.
AP =
# of relevant friends
MAP is the average of AP over all the users in the network.
NDCG is a normalization of the discounted cumulative gain (DCG) measure. DCG
is a weighted sum of the degree of relevancy of the ranked users. The weight is a
decreasing function of the rank (position) of the user, and therefore called discount.
NDCG normalizes DCG by the ideal DCG (IDCG), which is simply the DCG measure
of the best-ranking result. Thus NDCG measure is always a number in [0, 1]. NDCG
at position k is defined as
k

2ri − 1
NDCG@k = Zk
,
log(i + 1)
i=1

where k is also called the scope, which means the number of top-ranked users presented
to the user and Zk is chosen such that the perfect ranking has an NDCG value of 1.
We note that the base of the logarithm does not matter for NDCG, since constant
scaling will cancel out due to normalization. We will assume it is the natural logarithm
throughout this article.
5.3. Model Selection

Tuning of parameters (a.k.a model selection in learning community) is a critical problem in most of the learning problems. In some situations, the learning performance
may drastically vary with different choices of parameters. There are three parameters
in Eq. (5) that play very important roles in the effectivity of the proposed algorithm.
These are λU , λV , and λ S . Between these, λ S controls how much the proposed algorithm
should incorporate the information of the social network in completing the partially
observed rating matrix. In the extreme case, a very small value for λ S , the algorithm
almost forgets that social information exists between the users and only utilizes the
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:23

Fig. 2. Grid search to find the best values for λU and λC on the dataset with 90% of rating information.

observed user-item rating matrix for factorization. On the other hand, if we employ
a very large value for λ S , the social network information will dominate the learning
process, leading to poorer performance. Therefore, in order to not hurt the recommendation performance, we need to find a reasonable value for a social regularization
parameter. To this end, we analyze how the combination of these parameters affect the
recommendation performance.
We conduct a grid search on the potential values of two parameters λ S and λV to find
the combination with the best performance. Figure 2 shows the grid search results for
these parameters on a dataset with 90% of training data, where the optimal prediction
accuracy is achieved at point (14.8, 11) with the optimal RMSE = 1.12. We would
like to emphasize that we have done the cross-validation only for pairs of (λ S , λV ) and
(λ S , λU ), considering (i) the grid search for the triplet (λ S , λU , λV ) is computationally
burdensome, and (ii) our preliminary experiments showed that λV and λU behave
similarly with respect to λ S . Based on the results reported in Figure 2, in the remaining
experiments, we set λ S = 14.8, λV = 11, and λU = 13 when training is performed on
the dataset with 90% rating information. We repeat the same process to find out the
best setting of regularization parameters for other datasets with 80%, 70%, and 60%
rating data as well.
5.4. Baseline Methods

Here we briefly discuss the baseline algorithms against which we intend to compare
the proposed algorithm. The baseline algorithms are chosen from both types of
memory-based and model-based recommender systems with different types of trust
and distrust relations. In particular, we consider the following basic algorithms.
—MF (Matrix Factorization-based Recommender). This is the basic matrix
factorization-based recommender formulated in the optimization problem in Eq. (1),
which does not take social data into account.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:24

R. Forsati et al.

—MF+T (Matrix Factorization with Trust Information). To exploit the trust relations
between users in matrix factorization, Ma et al. [2009b] relied on the fact that the
distance between latent features of users who trust each other must be minimized
and can be formulated as the following objective:
n
1 
min
D(Ui,: , U j,: ),
U 2
i=1 j∈N+ (i)

where N+ (i) is the set of users the ith user trusts in the social network (i.e., Si j = +1).
By employing this intuition in the basic formulation in Eq. (1), Ma et al. [2009b]
solved the following optimization problem:
⎡
⎤
n


 

1
α
λ
λ
2
U
V

min ⎣
Ri j − Ui,:
V j,: +
D(Ui,: , U j,: ) +
UF +
VF ⎦ .
U,V
2
2
2
2
i=1 j∈N+ (i)

(i, j)∈R

—MF+D (Matrix Factorization with Distrust Information). The basic intuition behind
the algorithm proposed in Ma et al. [2009b] to exploit the distrust relations is as
follows: if user ui distrusts user u j , then we can assume that their corresponding
latent features Ui,: and U j,: would have a large distance. As a result, we aim to
maximize the following quantity for all users:
max
U

n
1 
D(Ui,: , U j,: ),
2
i=1 j∈N− (i)

where N− (i) denotes the set of users the ith users distrusts (i.e., Si j = −1). Adding this
term to the basic optimization problem in Eq. (1), we obtain the following optimization
problem:
⎡
⎤
n





1
β
λU
λV
2

min ⎣
Ri j − Ui,:
V j,: −
D(Ui,: , U j,: ) +
UF +
VF ⎦ .
U,V
2
2
2
2
(i, j)∈R

i=1 j∈N− (i)

—MF+TD (Matrix Factorization with Trust and Distrust Information). This algorithm
stands for the algorithm proposed in the present work. We note that there is no algorithm in the literature that exploits both trust and distrust relations in factorization
process simultaneously.
—NB (Neighborhood-Based Recommender). This algorithm is the basic memory-based
recommender algorithm that predicts a rating of a target item i for user u using a
combination of the ratings of neighbors of u (similar users) that already issued a
rating for item i. Formally,


u ∈N (u),Wuu >0 Wuu (Rui − R̄u)


R̂ui = R̄u +
,
(9)
u ∈N (u),Wuu Wuu
where the pairwise weight Wuu between pair of users (u, u ) is calculated by the
Pearson’s correlation coefficient [Herlocker et al. 2004]
—NB+T (Neighborhood with Trust Information) [Massa and Avesani 2004, 2009;
Golbeck 2005]. The basic idea behind the trust-based recommender systems proposed in TidalTrsut [Golbeck 2005] and MoleTrsut [Massa and Avesani 2004] is to
limit the set of neighbors in Eq. (9) to the users who are trusted by user u. The
distinguishing feature of these algorithms is the mechanism of trust propagation to
estimate the trust transitively for all the users. By adapting Eq. (9) to only consider
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

trustworthy neighbors in predicting the new ratings, we obtain


u ∈N+∗ (u),Wuu >0 Wuu (Rui − R̄u)


R̂ui = R̄u +
,
u ∈N+∗ (u),Wuu >0 Wuu

17:25

(10)

where N+∗ (u) is the set of trusted neighbors of u in the social network with propagated
trust relations (when there is no propagation, we have N+∗ (u) = N+ (u)). We note that
instead of Pearson’s correlation coefficient as the weighting schema, we can infer
the weights exploiting the social relation between the users. Since for the dataset
we consider in our experiments, the trust/distrust relations are binary values, the
social-based pairwise distance would be simply the hamming distance between the
binary vector representation of social relations of users. For implementation details,
we refer to Victor et al. [2011a, Chapter 6].
—NB+TD-F (Neighborhood with Trust Information and Distrust Information as Filtration) [Victor et al. 2011b, 2013]. A simple strategy for using distrust relations
in the recommendation is to filter out distrusted users from the list of neighbors in
predicting the ratings. As a result, we adapt Eq. (9) to exclude distrusted users from
the users’ propagated web of trust.
—NB+TD-D (Neighborhood-Based with Trust Information and Integrated Distrust Information) [Victor et al. 2011b, 2013]. In the same spirit as the filtration strategy,
we can use distrust relations to debug the trust relations. More specifically, if user u
trusts user v, v trusts w, and u distrusts w, then the latter distrust relation contradicts the propagation of the trust from u to w and can be excluded from the prediction.
In this method, distrust is used to debug the trust relations.
5.5. On the Consistency of Social Relations and Rating Information

As already mentioned, the Epinions website allows users to write reviews about products and services and to rate reviews written by other users. Epinions also allows
users to define their web of trust, that is, “reviewers whose reviews and ratings have
been consistently found to be valuable” and their block list, that is, “reviewers whose
reviews are found to be consistently inaccurate or not valuable”. Different intuitions
on interpreting these social information will result in different models. The main rationale behind incorporating trust and distrust relations in recommendation process
is to take the trust/distrust relations between users in the social network as the level
of agreement between ratings assigned to reviews by users.3 Therefore, investigating
the consistency or alignment between user ratings (implicit trust) and trust/distrust
relations in the social network (explicit trsut) become an important issue.
Here, we aim to empirically investigate whether or not there is a correlation between
a user’s current trustees/friends or distrusted friends and the ratings that user would
assign to reviews issued by his neighbors. Obviously, if there is no correlation between
the social context of a user and his/her ratings to reviews written by his neighbors,
then the social structure does not provide any advantage to the rating information.
On the other hand, if there exists such a correlation, then the social context could be
supplementary information to compensate for the lack of rating information to boost
the accuracy of recommendations.
The consistency of trust relations and rating information issued by users on the
reviews written by his trustees has been analyzed [Ziegler and Golbeck 2007; Guo et al.
2014]. However, Ziegler and Golbeck [2007] also claimed that social trust (i.e., explicit
trust) and similarity between users based on their issued ratings (i.e., implicit trust) are
3 In

the literature, the similarity between users conveyed from the rating information issued by users and
the direct relation in the social network are usually referred to as implicit and explicit trust, respectively.

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:26

R. Forsati et al.
Table V. Consistency of Implicit and Explicit Trust Relations in the Dataset for Different Ranges of
Ratings Measured in Terms of NDCG, Recall, and MAP
# of Ratings
0–20
21–40
41–60
61–80
≥81

NDCG@10

NDCG@20

Recall@10

Recall@20

Recall@40

MAP

0.083
0.108
0.117
0.120
0.135

0.078
0.103
0.112
0.117
0.126

0.054
0.080
0.083
0.088
0.091

0.092
0.125
0.128
0.132
0.151

0.156
0.198
0.225
0.230
0.253

0.140
0.190
0.208
0.230
0.244

Table VI. Consistency of Implicit and Explicit Distrust Relations in the Dataset for Different Ranges
of Ratings Measured in Terms of NDCG, Recall, and MAP
# of Ratings
0–20
21–40
41–60
61–80
≥81

NDCG@10

NDCG@20

Recall@10

Recall@20

Recall@40

MAP

0.065
0.071
0.082
0.089
0.104

0.057
0.068
0.072
0.078
0.096

0.045
0.060
0.075
0.081
0.087

0.071
0.077
0.085
0.105
0.125

0.132
0.140
0.158
0.164
0.191

0.130
0.134
0.152
0.160
0.183

not the same, and can be used complementary. According to Ma [2013], when comparing
implicit social information with explicit social information, the performance of using
implicit information is slightly worse. We further investigate the same question about
the consistency of distrust relations and ratings issued by users to their distrusted
neighbors. The positive answer to this question can be interpreted as follows. Given
that user u is interested in item i, the chances that v, trusted (distrusted) by u, also likes
this item i is much higher (lower) than for user w not explicitly trusted (distrusted)
by u.
To measure the similarity between users, there are several methods we can borrow
in the literature. In this article, we adopt the most popular approach that is referred to
as the Pearson correlation coefficient (PCC) P : U × U → [−1, +1] [Breese et al. 1998;
Massa and Avesani 2009], which is defined as

m
i=1 (Rui − R̄u)(Rvi − R̄v )
P(u, v) = 	

, ∀u, v ∈ U,

m
m
2×
2
(R
−
R̄
)
(R
−
R̄
)
ui
u
vi
v
j=1
j=1
where R̄u and R̄v are the average of ratings issued by users u and v, respectively. The
PCC measures the extent to which there is a linear relationship between the rating
behaviors of the two users, the extreme values being −1 and 1. The similarity of two
users becomes negative when users have completely diverging ratings. We note that
this quantity can be considered as the implicit trust between users that is conveyed
via ratings given by users.
To conduct this set of experiments, we first group all the users in the training data
set based on the number of ratings, and then measure the prediction accuracies of
different user groups. Users are grouped into five classes: [1, 20), [20, 40), [40, 60),
[60, 80), and >81. In order to have a comprehensive view of the ranking performance,
we present the NDCG, recall, and MAP scores of trust and distrust alignments on the
Epinions dataset in Table V and Table VI, respectively. We note that the dataset we
use in our experiments only contains bivalent trust values, that is, −1 and +1, and it
is not possible to have an ordering on the list of friends (time stamp of relations would
be an option to order the friends, but unfortunately it is not available in our dataset).
To compute the NDCG, we use the ordering of trusted/distrusted friends which yields
the best value.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:27

Table VII. Alignment Rate of Users in Establishing Trust/Distrust Relationships with Future Users in the
Social Network Based on the Majority Vote of Their Current Trusted/Distrusted Friends
Setting

Type of Relation (u ; w)

% of Relations

Alignment Rate (%)

n+ > n−

+
−
+
−
+

48.80
2.54
1.15
8.02
39.49

92.09
8.15
17.88
83.42
−

n+ < n−
n+ = n− > 0 or n+ = n− = 0

Note: The number of trusted friends (+) and distrusted friends (−) are denoted by n+ and n− ,
respectively. Here u denotes the current user and w stands for a future user in the network.

On the positive side, we observe a clear trend of alignment between ratings assigned
by a user and the type of relation he has made in the social network. This observation
coincides with our intuition. Overall, when more ratings are observed for a user, the
similarity calculation process will find more accurate similar or dissimilar neighbors
for this user, since we have more information to represent or interpret this user. Hence,
by increasing the number of ratings, it is conceivable from the results in Tables V
and VI that the alignment between implicit and explicit neighbors becomes better. By
comparing the results in Tables V and VI we can see that trust relations are slightly
better aligned than the distrust relations.
On the negative side, the results show that the NDCG on both types of relations is
small. One explanation for this phenomenon is that the Epinions dataset is not tightly
bound to a specific application. For example, a user may trust or distrust anther user
based on his/her comments on a specific product, but they might have similar taste
on other products. Furthermore, compared to other datasets such as FilmTrusts, the
Epinions dataset is a very sparse dataset, and consequently it is relatively inaccurate
to rely on the rating information to compute the implicit trust relations. Finally, our
approach to distinguishing trust/distrust lists from the rating information is limited
by the PCC trust metric we have utilized. We conjecture that better trust metrics
able to exploit other side information, such as time and interactional information,
would be helpful in distinguishing implicit trusted/distrusted friends, leading to better
alignment between implicit and explicit trust relations.
We also conduct experiments to evaluate the consistency of social network only
based on the trust/distrust relations between users. In particular, we investigate to
what extent a user’s relations are aligned with the opinion of his/her neighbors in
the social network. More specifically, let u be a user who is about to make a trust or
distrust relation to another user v. We assume that n+ number of u’s neighbors trust v
and n− number of u’s neighbors distrust v. We note that in the real dataset, the distrust
relations are hidden. To conduct this set of experiments, we randomly sample 30% of
the relations from the social network and use the remaining 70% to predict the type of
sampled relations4 by majority voting.
Table VII shows the results on the consistency of social relations. We observe that in
all cases there is an alignment between the opinions of a user’s friends and his/her own
relation (92.09% and 83.42% when the majority of friends trust and distrust the target
user, respectively). This might be due to the social influence of people on the social
network; however, it is hard to justify the existence of such a correlation in the Epinions
dataset which includes reviews for a diverse set of products and taste of users. One
interesting observation from the results reported in Table VII is the case where the
number of distrusted users dominates the number of trusted users (i.e., n− > n+ ). While
4A

more realistic way would be to use the time stamp of relations to create the training and test sets.

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:28

R. Forsati et al.

Table VIII. Accuracy of Prediction of Matrix Factorization with Three Different Methods Measured in Terms of
MAE and RMSE Errors
k

% of Training

10

60%
70%
80%
90%

20

60%
70%
80%
90%

MF

MF+T

MF+D

MF+TD

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

Measure

0.9813 ± 0.042
1.6050 ± 0.032
0.9462 ± 0.083
1.5327 ± 0.032
0.9150± 0.022
1.3824 ± 0.032
0.8921 ± 0.025
1.2166 ± 0.017

0.8561 ± 0.032
1.4125 ± 0.022
0.8332 ± 0.092
1.2407 ± 0.063
0.8206 ± 0.041
1.1906 ± 0.042
0.8158 ± 0.016
1.1403 ± 0.027

0.9720 ± 0.038
1.5036 ± 0.040
0.9241 ± 0.012
1.4405 ± 0.023
0.8722 ± 0.034
1.3155 ± 0.026
0.8736 ± 0.053
1.1869 ± 0.049

0.8310 ± 0.016
1.2294 ± 0.086
0.8206 ± 0.023
1.1562 ± 0.043
0.8113 ± 0.032
1.1061 ± 0.021
0.8025 ± 0.014
1.0872 ± 0.020

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

0.9972 ± 0.016
1.6248 ± 0.014
0.9688 ± 0.019
1.5162 ± 0.016
0.9365 ± 0.025
1.4081 ± 0.015
0.9224 ± 0.016
1.2207 ± 0.0 18

0.8431 ± 0.018
1.3904 ± 0.042
0.8342 ± 0.062
1.2722 ± 0.027
0.8172 ± 0.011
1.1853 ± 0.023
0.8128 ± 0.021
1.1402 ± 0.026

0.9746 ± 0.060
1.5423 ± 0.046
0.9350 ± 0.022
1.4540 ± 0.075
0.8705 ± 0.016
1.3591 ± 0.073
0.8805 ± 0.032
1.1933 ± 0.028

0.8475 ± 0.012
1.1837 ± 0.023
0.8290 ± 0.034
1.1452 ± 0.016
0.8129 ± 0.025
1.1049 ± 0.082
0.8096 ± 0.010
1.0851 ± 0.011

Note: The parameter k represents the number of latent features in factorization.

the distrust relations are private to other users, we can see that there is a significant
alignment between a users relation type and his distrusted friends.
5.6. On the Power of Utilizing Social Relationships

We now turn to investigate the effect of utilizing social relationships between users
on the accuracy of recommendations in factorization-based methods. In other words,
we would like to experimentally evaluate whether incorporating distrust can indeed
enhance the trust-based recommendation process. To this end, we run four different
MF (i.e., pure matrix factorization-based algorithm), MF+T (i.e., matrix factorization
with only trust relationships), MF+D (i.e., matrix factorization with only distrust relationships), and MF+TD (i.e., the algorithm proposed here) algorithms on the dataset.
We run the algorithms with k = 10 and k = 20 latent vector dimensions. As mentioned
earlier, different amounts of training data 90%, 80% , 70%, and 60% have been used to
create four different training sets that are increasingly sparse, but the social network
remains the same in all of them. We evaluate all algorithms by both MAE and RMSE
measures.
Table VIII shows the MAE and RMSE errors for the four sampled datasets. First, as
we expected, the performance of all learning algorithms improves with an increasing
number of training data. It is also not surprising to see that the MF+T, MF+D, and
MF+TD algorithms which exploit social side information perform better than the pure
matrix factorization-based MF algorithm. Second, the proposed algorithm outperforms
all other baseline algorithms for all the cases, indicating that it is effective for incorporating both types of social side information in the recommendation. This result by itself
indicates that besides trust relationships in the social network, distrust information is
also a rich source of information and can be utilized in recommendation algorithms. We
note that distrust information needs to be incorporated carefully, as its nature is totally
different from trust information. Finally, it is noticeable that MF+T outperforms the
MF+D algorithm due to a huge number of trust relations to distrust relations in our
dataset. It is also remarkable that users are more likely to be influenced by their friends
to make trust relations than the distrust relations due to the private nature of distrust
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:29

Table IX. Comparison with Other Popular Methods
Method

Parameter (s)

MAE

RMSE

MF
MF+T
MF+D
MF+TD

k = 10 and λU = λV = 5
k = 10, λU = λV = 5 , and α = 1
k = 10, λU = λV = 5 , and β = 10
k = 10, λU = 13, λV = 11 , and λ S = 14.8

0.8921
0.8158
0.8736
0.8025

1.2166
1.1403
1.1852
1.0872

NB
NB+T
NB+TD-F
NB+TD-D

p=1
p = 1 and q = 3
p = 1 and q = 3

0.9381
0.8904
0.8692
0.8728

1.5275
1.3455
1.2455
1.2604

Note: The reported values are the MAE and RMSE on the dataset with 90%
rating information. The values of parameters for each specific algorithm are
included in the second column.

relations in the Epinions dataset. This might lead us to believe that distrust relations
have better quality than trust relations, which requires a deeper investigation to be
verified.
5.7. Comparison to Baseline Algorithms

Another question that is worth investigating is how state-of-the-art approaches perform compared to the method proposed here. To this end, we compare the performance of the MF-TD algorithm with the baseline algorithms introduced in Section 5.4.
Table IX contains the results of our experiments with eight different algorithms on the
dataset with 90% rating data. The second column in the table represents the configuration of parameters used by each algorithm.
When we utilize trust/distrust relations in neighborhood-based algorithms, a crucial
decision we need to make is to which level the propagation must be performed (no propagation corresponds to single-level propagation which only includes direct neighbors).
Let p and q denote the level of propagation for trust and distrust relations, respectively.
Let us first consider the trust propagation to decide the value of p. We note that there
is a trade-off between accuracy and the level of trust propagation: longer propagation
levels results in less accurate trust predictions. This is due to the fact that when we
use longer propagation levels, the further away we are heading from each user, and
consequently decrease the confidence on the predictions. Obviously, this affects the accuracy of the recommendations significantly. As a result, for the trust propagation we
only consider single-level propagation by choosing p = 1 (i.e., N+∗ = N+ ). We also note
that since in the Epinions dataset, a user can not simultaneously trust and distrust
another user, in the neighborhood-based method with distrust relations, the debugging
only makes sense for propagated information. Therefore, we perform a three-level distrust propagation (q = 3) to constitute the set of distrusted users for each users. We
note that the longer the propagation levels, the more often distrust evidence can be
found for a particular user, and hence the less neighbors will be left to participate in
the recommendation process. For factorization-based methods, the value of regularization parameters, that is, λU , λV , and λ S , are determined by the procedure discussed in
Section 5.3.
The results of Table IX reveal some interesting conclusions as summarized here.
—From Table IX, we can observe that for factorization-based methods, incorporating
trust or distrust information boosts the performance of recommendation in terms of
both accuracy measures. This demonstrates the advantages of trust and distrustaware recommendation algorithms. We also can see that both MF+T and MF+D
perform better than the non-social MF, but the performance of MF+T is significantly
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:30

R. Forsati et al.

better than MF+D. As discussed, this observation does not indicate that trust relations are more beneficial than distrust relations, as in our dataset, only 16.7% of
relations are distrust relations. The MF+TD algorithm that is able to employ both
types of relations is significantly better than other algorithms, which demonstrates
the advantages of our proposed method in utilizing trust and distrust relations.
—Looking at the results reported in Table IX, it can immediately be noticed that
the incorporation of trust and distrust information in neighborhood-based methods
decreases the prediction error, but the improvement is not as significant as the
factorization-based methods. We note that for the NB+T method with longer levels
of propagation ( p = 2, 3), our experiments revealed that the accuracy remains almost
the same or has gotten worse on both MAE and RMSE measures, and this is why we
only report the results only for p = 1. In contrast, for distrust propagation, we found
out that q = 3 has a visible impact on the performance of both filtering and debugging
methods. We would like to emphasize that for longer levels of distrust propagation
in the Epinions dataset, that is, q > 4, we found that the size of the set of distrusted
users N−∗ (·) becomes large for most users, which degrades the prediction accuracy. We
also observe another interesting result about the performance of the NB+TD method
with filtering and debugging strategies. We found that although filtering generates
slightly better predictions, NB+TD-F performs almost as well as the NB+TD-D
method. Although this observation does not suggest any of these methods as the
method of choice in incorporating distrust, we believe that the accuracy might differ
from dataset to dataset, and it strongly depends on the propagation/aggregation
strategy.
—Considering the results for both model-based and memory-based methods in
Table IX, we can conclude a few interesting observations. First, we notice that
factorization-based methods with trust/distrust information perform better than the
neighborhood-based methods. Second, the incorporation of trust and distrust relations in matrix factorization has significant improvement compared to improvement
achieved by memory-based methods. Although the type of filtration or debugging
strategy could significantly affect the accuracy of incorporating distrust in memorybased methods, the main shortcoming of these methods comes from the fact that
these algorithms somehow exclude the influence of distrusted users from the rating prediction. This stands in stark contrast to the model proposed in this article
that ranks the neighbors based on the type of relation. This observation necessitates devising better algorithms for propagation and aggregation of trust/distrust
information in memory-based methods.
5.8. Handling Cold-Start Users by Social Side Information

In this section, we demonstrate the use of the social network to further illustrate the
potential of the proposed framework and the relevance of incorporating side information. To do so, as another set of our experiments, we intend to examine the performance
of proposed algorithm on cold-start users. Addressing cold-start users (i.e., users with
few ratings or new users) is very important for the success of recommender systems
due to the huge numbers of this type of users in many real-world systems. As a result, handling cold-start users is one of the main challenges in the existing systems.
To evaluate different algorithms, we randomly select 30%, 20%, 10%, and 5% as the
cold-start users. For cold-start users, we do not include any rating in the training data
and consider all the ratings made by cold-start users as testing data.
Table X shows the performance of the previously mentioned algorithms. As it is
clear from Table X, when the number of cold-start users is low with respect to the
total number of users, say 5% of total users, the affect of the distrust relationships
is negligible in prediction accuracy. But, when the number of cold-start users is high,
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:31

Table X. Accuracy of Handling Cold-Start Users and the Effect of Social Relations
% of Cold-start Users
30%
20%
10%
5%

Measure
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

MF

MF+T

MF+D

MF+TD

0.9923
1.7211
0.9812
1.7088
0.9334
1.4222
0.9134
1.3852

0.8824
1.5562
0.8805
1.4339
0.8477
1.3782
0.8292
1.2921

0.9721
1.6433
0.9505
1.6250
0.9182
1.4006
0.8633
1.3255

0.8533
1.4802
0.8472
1.2630
0.8322
1.2655
0.8280
1.2888

Note: The number of leant features in this experiments is set to k = 10. The first
column shows the number of cold-start users sampled randomly from all users
in the dataset. For the cold-starts users, all the ratings have been excluded from
the training data and used in the evaluation of three different algorithms.

exploiting the trust and distrust relationships significantly improves the performance
of the recommendation. This result is interesting, as it reveals that the lack of rating
information for cold-start and new users can be alleviated by incorporating the social
relations of users, and in particular, both trust and distrust relationships.
5.9. Trading Trust for Distrust Relationships

We also compare the potential benefit of trust relations to distrust relations in the
proposed algorithm. More specifically, we would like to see to what extent the distrust
relations can compensate for the lack of trust relations. We run the proposed algorithm
with the subset of trust and distrust relations and compare it to the algorithm which
only utilizes all of the trust relations. To set up this set of experiments, we randomly
sample a subset of trust relations and gradually increase the amount of distrust relations to see when the effect of distrust information compensates for the effect of missed
trust relations.
We sample 433,619 (approximately 90%) trust relations from the total 481,799 trust
relations and vary the number of distrust relations fed to the proposed algorithm.
Table XI reports the accuracy of the proposed algorithm for different numbers of distrust relations in the datasets. All these samplings have been done uniformly at random. We use 90% of all ratings for training and the remaining 10% for evaluation,
and set the dimension of the latent features to k = 10. As can be concluded from
Table XI, when we feed the proposed algorithm MF+TD with 90% of trust and 50%
of the distrust relations, it reveals very similar behavior to the trust-enhanced matrix factorization-based method MF+T, which only utilizes all the trust relations in
factorization. This result is interesting in the sense that the distrust information between users is as important as the trust information (we note that in this scenario,
the number trust relations excluded from the training is almost same as the number
of distrust relations included). By increasing the number of distrust relations, we can
observe that the accuracy of recommendations increases as expected. In summary, this
set of experiments validates that incorporating distrust relations could indeed enhance
the trust-based recommendation process and could be considered as a rich source of
information to be exploited.
5.10. On the Impact of Batch Size in Stochastic Optimization

As mentioned earlier, directly solving the optimization problem in Eq. (5) using full
gradient descent method requires going through all the triplets in the constraint set
S , which could be computationally expensive due to the huge number of triplets in
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:32

R. Forsati et al.
Table XI. Accuracy of Proposed Algorithm on a Dataset with 39,0257 (≈ 90%) Trust Relations
Sampled Uniformly at Random from All Trust Relations with Varied Number of Distrust Relations
Method

# of Trust Relations

MF+TD

433,619 (≈90%)

# of Distrust Relations
9,682 (≈10%)
19,364 (≈20%)
29,047 (≈30%)
38,729 (≈40%)
48,411 (≈ 50%)
58,093 (≈60%)
67,776 (≈70%)
77,458 (≈80%)
87,140 (≈90%)
96,823 (= 100%)

MF+T

481,799 (=100%)

0

Measure

Accuracy

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

0.8803 ± 0.051
1.2166 ± 0.028
0.8755 ± 0.033
1.1944 ± 0.042
0.8604 ± 0.036
1.1822 ± 0.081
0.8431 ± 0.047
1.1706 ± 0.055
0.8165 ± 0.056
1.1425 ± 0.091
0.8130 ± 0.035
1.1380 ± 0.046
0.8122 ± 0.041
1.1306 ± 0.042
0.8095 ± 0.036
1.1290 ± 0.085
0.8061 ± 0.044
1.1176 ± 0.067
0.8050 ± 0.052
1.1092 ± 0.063

MAE
RMSE

0.8158 ± 0.016
1.1403 ± 0.027

Note: The learning is performed based on 90% of all ratings with k = 10 as the dimension of
latent features.

S . To overcome this efficiency problem, one can turn to the stochastic gradient scent
method which tries to generate unbiased estimates of the gradient at each iteration in
a much cheaper way by sampling a subset of triplets from S .
To accomplish this goal, we perform gradient descent and stochastic gradient descent
to solve the optimization problem in Eq. (5) to find the matrices U and V following the
updating equations derived in Eqs. (7) and (8). At each iteration t, the currently learned
matrices Ut and Vt are used to predict the ratings in the testset. In particular, at each
iteration, we evaluate the RMSE and MAE on the testset and terminate training once
the RMSE and MAE starts increasing or once the maximum number of iterations is
reached. We run the algorithm with latent vectors of dimension k = 10.
We compare the computational efficiency between the proposed algorithm with GD
and mini-batch SGD with different batch sizes. We note that the GD updating rule
can be considered a mini-batch SGD, where the batch size B is deterministically set
to be B = |S |, and simple SGD can be considered a mini-batch SGD with B = 1. We
remark that in contrast to GD method which uses all the triplets in S for gradient
computation at each iteration, for the SGD method—due to uniform sampling over all
tuples in S —some of the tuples may be used more than once and some of the tuples
might never been used for gradient computation.
Figures 3 and 4 show the convergence rate of four different updating rules in terms
of the number of iterations t for two different measures RMSE and RME, respectively. The first algorithm denoted by GD runs the simple full gradient descent iteratively to optimize the objective. The other three algorithms SGD1, SGD2, and SGD3
in the figures use the batch sizes B = 0.1 ∗ |S |, B = 0.2 ∗ | S |, and B = 0.3 ∗ |S |,
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:33

Fig. 3. Comparison of accuracy of prediction in terms of RMSE with GD and SGD with three varied batch
sizes.

respectively. In our experiments, due to very slow convergence of the basic SGD method
with B = 1 in comparison to other fours methods, we simply exclude its result from the
discussion.
In terms of accuracy of predictions, from both Figures 3 and 4, we can conclude that
the GD has the best convergence and SGD3 has the worst convergence in all settings.
This is because, although all four of the algorithms use an unbiased estimate of the
true gradient to update the solution at each iteration, the variance of each stochastic
gradient is proportional to the size of the batch size B. Therefore, for larger values of B,
the variance of stochastic gradients is smaller, and the algorithm convergences faster,
but for smaller values of B, the algorithm suffers from high variance in stochastic
gradients and convergences slowly. We emphasize that this comparison holds for iteration complexity which is different from the computational complexity (running time)
of individual iterations. More specifically, each iteration of GD requires |S | gradient
computations, while for SGD, we only need to perform B  |S | gradient computations.
In summary, SGD has lightweight iteration but requires more iterations to converge.
In contrast, GD takes expensive steps in a much fewer number of iterations. From Figures 3 and 4, it is noticeable that although a large number of iterations is usually needed
to obtain a solution of desirable accuracy using SGD, the lightweight computation per
iteration makes SGD attractive for the optimization problem in Eq. (5) for large number
of users. We also not that for the GD method, the error is a monotonically-decreasing
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:34

R. Forsati et al.

Fig. 4. Comparison of accuracy of prediction in terms of MAE with GD and SGD with three varied batch
sizes.

function it terms of the number of iterations t, but for the SGD-based methods, this
does not hold. This is because although the SGD algorithm is guaranteed to converge to
an optimal solution (at least in expectation), there is no guarantee that the stochastic
gradients provide a descent direction for the objective at each iteration due to the noise
in computing gradients. As a result, for a few iterations, we can see that the objective
increases but finally it convergences as expected.
6. CONCLUSIONS AND FUTURE WORKS

In this article, we have made progress towards making distrust information beneficial
in the social recommendation problem. In particular, we have proposed a framework
based on matrix factorization which is able to incorporate both trust and distrust relationships between users in a factorization algorithm. We experimentally investigated
the potential of distrust as side information to overcome data sparsity and cold-start
problems in traditional recommender systems. In summary, our results showed that
more accurate recommendations can be obtained by incorporating distrust relations,
indicating that distrust information can indeed be beneficial for the recommendation
process.
This work leaves few directions, both theoretically and empirically, as future work.
From an empirical point of view, it would be interesting to extend our model for
weighted social trust and distrust relations. One challenge in this direction is that,
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:35

as far as we know, there is no publicly available dataset that includes weighted (gradual) trust and distrust information. Also, the experimental results we have conducted
on the consistency of social relations with rating information hint at a number of potential enhancements in future work. In particular, it would be interesting to further
examine the correlation between implicit and explicit distrust information. An important challenge in this direction is to develop better metrics to measure the implicit
trust between users, as the simple metrics such as the Pearson correlation coefficient
seem insufficient. Furthermore, since we only consider distrust between users, it would
be easy to generalize our model in the same way to incorporate dissimilarity between
items and investigate how it works in practice. Also, our preliminary results indicated
that hinge loss almost performs better than exponential loss, but from the optimization
viewpoint, exponential loss is more attractive due to its smoothness. So, an interesting
direction would be to use a smoothed version of hinge loss to gain from both optimization efficiency and algorithmic accuracy.
ACKNOWLEDGMENTS
The authors would like to thank the Associate Editor and three anonymous reviewers for their immensely
insightful comments and helpful suggestions on the original version of this article. R. Forsati would also
like to thank Professor Mohamed Mokbel, Department of Computer Science and Engineering, University of
Minnesota, for the opportunity to visit his research group while doing this work.

REFERENCES
Gediminas Adomavicius and Alexander Tuzhilin. 2005. Toward the next generation of recommender systems:
A survey of the state-of-the-art and possible extensions. IEEE Trans. Knowl. Data Eng. 17, 6 (2005),
734–749.
Deepak Agarwal and Bee-Chung Chen. 2010. fLDA: Matrix factorization through latent dirichlet allocation.
In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining. ACM, 91–100.
Paolo Avesani, Paolo Massa, and Roberto Tiella. 2005. A trust-enhanced recommender system application:
Moleskiing. In Proceedings of the ACM Symposium on Applied Computing. 1589–1593.
Giacomo Bachi, Michele Coscia, Anna Monreale, and Fosca Giannotti. 2012. Classifying trust/distrust relationships in online social networks. In International Conference on Privacy, Security, Risk and Trust
(PASSAT) and International Confernece on Social Computing (SocialCom). IEEE, 552–557.
Jesús Bobadilla, Fernando Ortega, Antonio Hernando, and Abraham Gutiérrez. 2013. Recommender systems
survey. Knowl. Based Syst. 46 (2013), 109–132.
John S. Breese, David Heckerman, and Carl Kadie. 1998. Empirical analysis of predictive algorithms for
collaborative filtering. In Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence.
Morgan Kaufmann Publishers Inc., 43–52.
Moira Burke and Robert Kraut. 2008. Mopping up: Modeling Wikipedia promotion decisions. In Proceedings
of the ACM Conference on Computer Supported Cooperative Work. ACM, 27–36.
Dorwin Cartwright and Frank Harary. 1956. Structural balance: A generalization of Heider’s theory. Psychol.
Rev. 63, 5 (1956), 277.
Gang Chen, Fei Wang, and Changshui Zhang. 2009. Collaborative filtering using orthogonal nonnegative
matrix tri-factorization. Inf. Process. Manag. 45, 3 (2009), 368–379.
Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan. 2011. Better mini-batch algorithms via
accelerated gradient methods. In Conference on Neural Information Processing Systems (NIPS). Vol. 24
1647–1655.
David Crandall, Dan Cosley, Daniel Huttenlocher, Jon Kleinberg, and Siddharth Suri. 2008. Feedback effects
between similarity and social influence in online communities. In Proceedings of the 14th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining. ACM, 160–168.
Sanjoy Dasgupta, Michael L. Littman, and David McAllester. 2002. PAC generalization bounds for cotraining. In Proceedings of the Conference on Neural Information Processing Systems. 375–382.
Mukund Deshpande and George Karypis. 2004. Item-based top-n recommendation algorithms. ACM Trans.
Inf. Syst. 22, 1 (2004), 143–177.
Thomas DuBois, Jennifer Golbeck, and Aravind Srinivasan. 2011. Predicting trust and distrust in social
networks. In Proceedings of the IEEE 3rd International Conference on Privacy, Security, Risk and Trust
(PASSAT), and IEEE 3rd International Conference on Social Computing (SocialCom). IEEE, 418–424.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:36

R. Forsati et al.

Rana Forsati, Hanieh Mohammadi Doustdar, Mehrnoush Shamsfard, Andisheh Keikha, and Mohammad
Reza Meybodi. 2013. A fuzzy co-clustering approach for hybrid recommender systems. Int. J. Hybrid
Intell. Syst. 10, 2 (2013), 71–81.
Rana Forsati and Mohammad Reza Meybodi. 2010. Effective page recommendation algorithms based on
distributed learning automata and weighted association rules. Expert Syst. Appl. 37, 2 (2010), 1316–
1330.
Jennifer Golbeck. 2005. Computing and applying trust in web-based social networks. Ph.D. Dissertation,
University of Maryland at College Park.
Jennifer Golbeck. 2006. Generating predictive movie recommendations from trust in social networks. In Proceedings of the 4th International Conference on Trust Management (iTrust). Lecture Notes in Computer
Science, Vol. 3986, Springer, Berlin, 93–104.
Jennifer Golbeck and James Hendler. 2006. Filmtrust: Movie recommendations using trust in web-based
social networks. In Proceedings of the IEEE Consumer Communications and Networking Conference,
Vol. 96. Citeseer.
Nathaniel Good, J. Ben Schafer, Joseph A. Konstan, Al Borchers, Badrul Sarwar, Jon Herlocker, and John
Riedl. 1999. Combining collaborative filtering with personal agents for better recommendations. In Proceedings of the 16th National Conference on Artificial Intelligence and the 11th Innovative Applications
of Artificial Intelligence Conference (AAAI/IAAI). 439–446.
Quanquan Gu, Jie Zhou, and Chris Ding. 2010. Collaborative filtering: Weighted nonnegative matrix factorization incorporating user and item graphs. In Proceedings of the SIAM International Conference on
Data Mining (SDM). 199–210.
R. Guha, Ravi Kumar, Prabhakar Raghavan, and Andrew Tomkins. 2004. Propagation of trust and distrust.
In Proceedings of the 13th International Conference on World Wide Web. ACM, 403–412.
Guibing Guo, Jie Zhang, Daniel Thalmann, Anirban Basu, and Neil Yorke-Smith. 2014. From ratings to
trust: An empirical study of implicit trust in recommender systems. In Proceedings of the 29th ACM
Symposiam on Applied Computing (SAC).
Jonathan L. Herlocker, Joseph A. Konstan, Al Borchers, and John Riedl. 1999. An algorithmic framework
for performing collaborative filtering. In Proceedings of the 22nd Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval. ACM, 230–237.
Jonathan L. Herlocker, Joseph A. Konstan, Loren G. Terveen, and John T. Riedl. 2004. Evaluating collaborative filtering recommender systems. ACM Trans. Inf. Syst. 22, 1 (2004), 5–53.
Thomas Hofmann. 2004. Latent semantic models for collaborative filtering. ACM Trans. Inf. Syst. 22, 1
(2004), 89–115.
Mohsen Jamali and Martin Ester. 2009. TrustWalker: A random walk model for combining trust-based
and item-based recommendation. In Proceedings of the 15th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. ACM, 397–406.
Mohsen Jamali and Martin Ester. 2010. A matrix factorization technique with trust propagation for recommendation in social networks. In Proceedings of the 4th ACM Conference on Recommender Systems.
ACM, 135–142.
Mohsen Jamali and Martin Ester. 2011. A transitivity aware matrix factorization model for recommendation
in social networks. In Proceedings of the 22nd International Joint Conference on Artificial Intelligence,
Vol. 3. AAAI Press, 2644–2649.
Arnd Kohrs and Bernard Merialdo. 1999. Clustering for collaborative filtering applications. In Computational
Intelligence for Modelling, Control & Automation. IOS Press.
Ioannis Konstas, Vassilios Stathopoulos, and Joemon M. Jose. 2009. On social networks and collaborative
recommendation. In Proceedings of the 32nd International ACM SIGIR Conference on Research and
Development in Information Retrieval. ACM, 195–202.
Yehuda Koren. 2008. Factorization meets the neighborhood: A multifaceted collaborative filtering model.
In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining. ACM, 426–434.
Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender
systems. Computer 42, 8 (2009), 30–37.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. 2010a. Predicting positive and negative links in
online social networks. In Proceedings of the 19th International Conference on World Wide Web. ACM,
641–650.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. 2010b. Signed networks in social media. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 1361–1370.
Wu-Jun Li and Dit-Yan Yeung. 2009. Relation regularized matrix factorization. In Proceedings of the 21st
International Conference on Artificial Intelligence (IJCAI’09).
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:37

Juntao Liu, Caihua Wu, and Wenyu Liu. 2013. Bayesian probabilistic matrix factorization with social relations and item contents for recommendation. Decision Support Syst. 55, 3 (June 2013), 838–850.
Hao Ma. 2013. An experimental study on implicit social recommendation. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 73–82.
Hao Ma, Irwin King, and Michael R. Lyu. 2009a. Learning to recommend with social trust ensemble.
In Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in
Information Retrieval. ACM, 203–210.
Hao Ma, Michael R. Lyu, and Irwin King. 2009b. Learning to recommend with trust and distrust relationships. In Proceedings of the 3rd ACM Conference on Recommender Systems. ACM, 189–196.
Hao Ma, Haixuan Yang, Michael R. Lyu, and Irwin King. 2008. SoRec: Social recommendation using probabilistic matrix factorization. In Proceedings of the 17th ACM Conference on Information and Knowledge
Management. ACM, 931–940.
Hao Ma, Dengyong Zhou, Chao Liu, Michael R. Lyu, and Irwin King. 2011a. Recommender systems with
social regularization. In Proceedings of the 4th ACM International Conference on Web Search and Data
Mining. ACM, 287–296.
Hao Ma, Tom Chao Zhou, Michael R. Lyu, and Irwin King. 2011b. Improving recommender systems by
incorporating social contextual information. ACM Trans. Inf. Syst. 29, 2 (2011), 9.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to Information
Retrieval. Vol. 1. Cambridge University Press, Cambridge.
Paolo Massa and Paolo Avesani. 2004. Trust-aware collaborative filtering for recommender systems. In On
the Move to Meaningful Internet Systems 2004: CoopIS, DOA, and ODBASE. Lecture Notes in Computer
Science, Vol. 3290. Springer, 492–508.
Paolo Massa and Paolo Avesani. 2005. Controversial users demand local trust metrics: An experimental study
on Epinions.com community. In Proceedings of the 20th National Conference on Artificial Intelligence
(AAAI’05). 121–126.
Paolo Massa and Paolo Avesani. 2009. Trust metrics in recommender systems. In Computing with Social
Trust. Human-Computer Intercation Series, Springer, 259–285.
Prem Melville, Raymond J. Mooney, and Ramadass Nagarajan. 2002. Content-boosted collaborative filtering
for improved recommendations. In Proceedings of the 18th National Conference on Artificial Intelligence
(AAAI). 187–192.
Bradley N. Miller, Joseph A. Konstan, and John Riedl. 2004. PocketLens: Toward a personal recommender
system. ACM Trans. Inf. Syst. (TOIS) 22, 3 (2004), 437–476.
Andriy Mnih and Ruslan Salakhutdinov. 2007. Probabilistic matrix factorization. In Proceedings of the 21st
Annual Conference on Neural Information Processing Systems (NIPS). 1257–1264.
Uma Nalluri. 2008. Utility of distrust in online recommender systems. Technical Report, Coopstone.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. 2009. Robust stochastic approximation approach to stochastic programming. SIAM J. Optim. 19, 4 (2009), 1574–1609.
Akshay Patil, Golnaz Ghasemiesfeh, Roozbeh Ebrahimi, and Jie Gao. 2013. Quantifying social influence in
epinions. Human 2, 2 (2013).
Dmitry Pavlov and David M. Pennock. 2002. A maximum entropy approach to collaborative filtering in
dynamic, sparse, high-dimensional domains. In Proceedings of the Annual Conference on Neural Information Processing System (NIPS), Vol. 2. 1441–1448.
Michael J. Pazzani. 1999. A framework for collaborative, content-based and demographic filtering. Artif.
Intell. Rev. 13, 5–6 (1999), 393–408.
David M. Pennock, Eric Horvitz, Steve Lawrence, and C. Lee Giles. 2000. Collaborative filtering by personality diagnosis: A hybrid memory-and model-based approach. In Proceedings of the 16th Conference on
Uncertainty in Artificial Intelligence. Morgan Kaufmann Publishers Inc., 473–480.
Jasson D. M. Rennie and Nathan Srebro. 2005. Fast maximum margin matrix factorization for collaborative
prediction. In Proceedings of the 22nd International Conference on Machine Learning. ACM, 713–719.
Ruslan Salakhutdinov and Andriy Mnih. 2008a. Bayesian probabilistic matrix factorization using Markov
chain Monte Carlo. In Proceedings of the 25th International Conference on Machine Learning. ACM,
880–887.
Ruslan Salakhutdinov and Andriy Mnih. 2008b. Probabilistic matrix factorization. In Proceedings of the
22nd Annual Conference on Neural Information Processing Systems. 1257–1264.
Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. 2007. Restricted Boltzmann machines for collaborative filtering. In Proceedings of the 24th International Conference on Machine Learning. ACM,
791–798.

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:38

R. Forsati et al.

Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item-based collaborative filtering
recommendation algorithms. In Proceedings of the 10th International Conference on World Wide Web.
ACM, 285–295.
Wanita Sherchan, Surya Nepal, and Cecile Paris. 2013. A survey of trust in social networks. ACM Comput.
Surv. 45, 4 (2013), 47.
Luo Si and Rong Jin. 2003. Flexible mixture model for collaborative filtering. In Proceedings of the 20th
International Conference on Machine Learning (ICML), Vol. 3. 704–711.
Ian Soboroff and Charles Nicholas. 1999. Combining content and collaboration in text filtering. In Proceedings
of the IJCAI Workshop on Machine Learning for Information Filtering, Vol. 99. 86–91.
Nathan Srebro and Tommi Jaakkola. 2003. Weighted low-rank approximations. In Proceedings of the 20th
International Conference on Machine Learning (ICML), Vol. 3. 720–727.
Nathan Srebro, Jason D. M. Rennie, and Tommi Jaakkola. 2005. Maximum-margin matrix factorization. In
Proceedings of the Conference on Neural Information Processing Systems (NIPS). 1329–1336.
Mojdeh Talabeigi, Rana Forsati, and Mohammad Reza Meybodi. 2010. A hybrid web recommender system
based on cellular learning automata. In Proceedings of the IEEE International Conference on Granular
Computing (GrC). IEEE, 453–458.
Nele Verbiest, Chris Cornelis, Patricia Victor, and Enrique Herrera-Viedma. 2012. Trust and distrust aggregation enhanced with path length incorporation. Fuzzy Sets Syst. 202 (2012), 61–74.
Patricia Victor, Chris Cornelis, Martine De Cock, and Ankur Teredesai. 2011b. Trust- and distrust-based
recommendations for controversial reviews. IEEE Intell. Syst. 26, 1 (2011), 48–55.
Patricia Victor, Chris Cornelis, and Martine De Cock. 2011a. Trust Networks for Recommender Systems.
Atlantis-Computational Intelligence Series, Vol. 4. Springer, Berlin.
Patricia Victor, Chris Cornelis, Martine De Cock, and Enrique Herrera-Viedma. 2011c. Practical aggregation
operators for gradual trust and distrust. Fuzzy Sets Syst. 184, 1 (2011), 126–147.
Patricia Victor, Nele Verbiest, Chris Cornelis, and Martine De Cock. 2013. Enhancing the trust-based recommendation process with explicit distrust. ACM Trans. Web 7, 2 (2013), 6.
Fei Wang, Sheng Ma, Liuzhong Yang, and Tao Li. 2006b. Recommendation on item graphs. In Proceedings
of the 6th International Conference on Data Mining (ICDM’06). IEEE, 1119–1123.
Jun Wang, Arjen P. De Vries, and Marcel J. T. Reinders. 2006a. Unifying user-based and item-based collaborative filtering approaches by similarity fusion. In Proceedings of the 29th Annual International ACM
SIGIR Conference on Research and Development in Information Retrieval. ACM, 501–508.
Grzegorz Wierzowiecki and Adam Wierzbicki. 2010. Efficient and correct trust propagation using closelook.
In Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent
Agent Technology (WI-IAT). Vol. 1. IEEE, 676–681.
Lei Wu, Steven C. H. Hoi, Rong Jin, Jianke Zhu, and Nenghai Yu. 2009. Distance metric learning from
uncertain side information with application to automated photo tagging. In Proceedings of the 17th
ACM International Conference on Multimedia. ACM, 135–144.
Gui-Rong Xue, Chenxi Lin, Qiang Yang, WenSi Xi, Hua-Jun Zeng, Yong Yu, and Zheng Chen. 2005. Scalable
collaborative filtering using cluster-based smoothing. In Proceedings of the 28th Annual International
ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 114–121.
Kai Yu, Anton Schwaighofer, Volker Tresp, Xiaowei Xu, and H.-P. Kriegel. 2004. Probabilistic memory-based
collaborative filtering. IEEE Trans. Knowl. Data Eng. 16, 1 (2004), 56–69.
Sheng Zhang, Weihong Wang, James Ford, and Fillia Makedon. 2006. Learning from incomplete ratings
using non-negative matrix factorization. In Proceedings of the 6th SIAM Conference on Data Mining
(SDM).
Yi Zhang and Jonathan Koren. 2007. Efficient bayesian hierarchical user modeling for recommendation
system. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval. ACM, 47–54.
Jianke Zhu, Hao Ma, Chun Chen, and Jiajun Bu. 2011. Social recommendation using low-rank semidefinite
program. In Proceedings of the 25th AAAI Conference on Artificial Intelligence.
Cai-Nicolas Ziegler. 2009. On propagating interpersonal trust in social networks. In Computing with Social
Trust Human-Computer Interaction Series, Springer, Berlin, 133–168.
Cai-Nicolas Ziegler and Jennifer Golbeck. 2007. Investigating interactions of trust and interest similarity.
Decision Support Syst. 43, 2 (2007), 460–475.
Cai-Nicolas Ziegler and Georg Lausen. 2005. Propagation models for trust and distrust in social networks.
Inf. Syst. Frontiers 7, 4–5 (2005), 337–358.
Received November 2013; revised April, June 2014; accepted June 2014
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Geoinformatica (2013) 17:417448 DOI 10.1007/s10707-012-0164-9

Generic and efficient framework for search trees on flash memory storage systems
Mohamed Sarwat  Mohamed F. Mokbel  Xun Zhou  Suman Nath

Received: 16 February 2012 / Revised: 27 June 2012 / Accepted: 12 July 2012 / Published online: 30 August 2012  Springer Science+Business Media, LLC 2012

Abstract Tree index structures are crucial components in data management systems. Existing tree index structure are designed with the implicit assumption that the underlying external memory storage is the conventional magnetic hard disk drives. This assumption is going to be invalid soon, as flash memory storage is increasingly adopted as the main storage media in mobile devices, digital cameras, embedded sensors, and notebooks. Though it is direct and simple to port existing tree index structures on the flash memory storage, that direct approach does not consider the unique characteristics of flash memory, i.e., slow write operations, and erase-beforeupdate property, which would result in a sub optimal performance. In this paper, we introduce FAST (i.e., Flash-Aware Search Trees) as a generic framework for flashaware tree index structures. FAST distinguishes itself from all previous attempts of flash memory indexing in two aspects: (1) FAST is a generic framework that can be applied to a wide class of data partitioning tree structures including R-tree and its variants, and (2) FAST achieves both ef f iciency and durability of read and write flash operations through memory flushing and crash recovery techniques. Extensive experimental results, based on an actual implementation of FAST inside the GiST

The research of M. Sarwat and M. F. Mokbel is supported in part by the National Science Foundation under Grants IIS-0811998, IIS-0811935, CNS-0708604, IIS-0952977, by a Microsoft Research Gift, and by a seed grant from UMN DTC. M. Sarwat ( )  M. F. Mokbel  X. Zhou Department of Computer Science and Engineering, University of Minnesota - Twin Cities, 200 SE Union Street, Minneapolis, MN 55455, USA e-mail: sarwat@cs.umn.edu M. F. Mokbel e-mail: mokbel@cs.umn.edu X. Zhou e-mail: xun@cs.umn.edu S. Nath Microsoft Research, One Microsoft Way - Redmond, Redmond, WA 98052, USA e-mail: sumann@microsoft.com

418

Geoinformatica (2013) 17:417448

index structure in PostgreSQL, show that FAST achieves better performance than its competitors. Keywords Flash memory  Tree  Spatial  Index structure  Storage  Multi-dimensional  Data  System

1 Introduction Data partitioning tree index structures are crucial components in spatial data management systems, as they are mainly used for efficient spatial data retrieval, hence boosting up query performance. The most common examples of such index structures include B-tree [4], with its variants [10, 27], for one-dimensional indexing, and R-tree [14], with its variants [5, 17, 32, 34], for multi-dimensional indexing. Data partitioning tree index structures are designed with the implicit assumption that the underlying external memory storage is the conventional magnetic hard disk drives, and thus has to account for the mechanical disk movement and its seek and rotational delay costs. This assumption is going to be invalid soon, as flash memory storage is expected to soon prevail in the storage market replacing the magnetic hard disks for many applications [11, 12, 31]. Flash memory storage is increasingly adopted as the main storage media in mobile devices and as a storage alternative in laptops, desktops, and enterprise class servers (e.g., in forms of SSDs) [3, 21, 23, 28, 33]. Recently, several data-intensive applications have started using custom flash cards (e.g., ReMix [19]) with large capacity and access to underlying raw flash chips. Such a popularity of flash is mainly due to its superior characteristics that include smaller size, lighter weight, lower power consumption, shock resistance, lower noise, and faster read performance [16, 18, 20, 22, 29]. Flash memory is block-oriented, i.e., pages are clustered into a set of blocks. Thus, it has fundamentally different characteristics, compared to the conventional pageoriented magnetic disks, especially for the write operations. First, write operations in flash are slower than read operations. Second, random writes are substantially slower than sequential writes. In devices that allow direct access to flash chips (e.g., ReMix [19]), a random write operation updates the contents of an already written part of the block, which requires an expensive block erase operation,1 followed by a sequential write operation on the erased block; an operation termed as erase-before-update [7, 20]. SSDs, which emulate a disk-like interface with a Flash Translation Layer (FTL), also need to internally address flash's erase-before-update property with logging and garbage collection, and hence random writes, especially small random writes, are significantly slower than sequential writes in almost all SSDs [7]. Though it is direct and simple to port existing tree index structures (e.g., R-tree and B-tree) on FTL-equipped flash devices (e.g., SSDs), that direct approach does not consider the unique characteristics of flash memory and therefore would result in a sub-optimal performance due to the random writes encountered by these index structures. To remedy this situation, several approaches have been proposed for

1 In

a typical flash memory, the cost of read, write, and erase operations are 25, 200 and 1,500 s, respectively [3].

Geoinformatica (2013) 17:417448

419

flash-aware index structures that either focus on a specific index structure, and make it a flash-aware, e.g., flash-aware B-tree [30, 36] and R-tree [35], or design brand new index structures specific to the flash storage [2, 24, 25]. Unfortunately, previous works on flash-aware search trees suffer from two major limitations. First, these trees are specialized--they are not flexible enough to support new data types or new ways of partitioning and searching data. For example, FlashDB [30], which is designed to use a B-Tree, does not support R-Tree functionalities. RFTL [35] is designed to work with R-tree, and does not support B-tree functionalities. Thus, if a system needs to support many applications with diverse data partitioning and searching requirements, it needs to have multiple tree data structures. The effort required to implement and maintain multiple such data structures is high. Second, existing flash-aware designs often show trade-offs between efficiency and durability. Many designs sacrifice strict durability guarantee to achieve efficiency [24, 25, 30, 35, 36]. They buffer updates in memory and flush them in batches to amortize the cost of random writes. Such buffering poses the risk that in-memory updates may be lost if the system crashes. On the other hand, several designs achieve strict durability by writing (in a sequential log) all updates to flash [2]. However, this increases the cost of search for many log entries that need to be read from flash in order to access each tree node [30]. In summary, no existing flash-aware tree structure achieves both strict durability and efficiency. In this paper, we address the above two limitations by introducing FAST; a framework for Flash-Aware Search Tree index structures. FAST distinguishes itself from all previous flash-aware approaches in two main aspects: (1) Rather than focusing on a specific index structure or building a new index structure, FAST is a generic framework that can be applied to a wide variety of tree index structures, including B-tree, R-tree along with their variants. Such an important property makes FAST a very attractive solution to database industry as it is practical to port it inside the database engine with minimal disturbance to the engine code. (2) FAST achieves both efficiency and durability in the same design. For efficiency, FAST buffers all the incoming updates in memory while employing an intelligent f lushing policy that evicts selected updates from memory to minimize the cost of writing to the flash storage. In the mean time, FAST guarantees durability by sequentially logging each in-memory update and by employing an efficient crash recovery technique. FAST mainly has four modules, update, search, f lushing, and recovery. The update module is responsible on buffering incoming tree updates in an in-memory data structure, while writing small entries sequentially in a designated flash-resident log file. The search module retrieves requested data from the flash storage and updates it with recent updates stored in memory, if any. The f lushing module is triggered once the memory is full and is responsible on evicting flash blocks from memory to the flash storage to give space for incoming updates. Finally, the recovery module ensures the durability of in-memory updates in case of a system crash. FAST is a generic system approach that neither changes the structure of tree indexes it is applied to, nor changes the search, insert, delete, or update algorithms of these indexes. FAST only changes the way these algorithms reads, or updates the tree nodes in order to make the index structure flash-aware. We have implemented FAST within the GiST framework [15] inside PostgrSQL. As GiST is a generalized index structure, FAST can support any tree index structure that GiST is supporting,

420

Geoinformatica (2013) 17:417448

including one-dimensional tree index structures (e.g., B-tree [4]) and including but not restricted to R-tree [14], R*-tree [5], SS-tree [34], and SR-tree [17], as well as Btree and its variants. In summary, the contributions of this paper can be summarized as follows:   We introduce FAST; a general framework that adapts existing tree index structures to consider and exploit the unique properties of the flash memory storage. We show how to achieve efficiency and durability in the same design. For efficiency, we introduce two f lushing policies that smartly select parts of the main memory buffer to be flushed into the flash storage in a way that amortizes expensive random write operations. We also introduce a crash recovery technique that ensures the durability of update transactions in case of system crash. We give experimental evidence for generality, efficiency, and durability of FAST framework when applied to different data partitioning tree index structures.



The rest of the paper is organized as follows: An overview of Flash Memory storage is given in Section 2. Section 3 highlights related work to FAST. Section 4 gives an overview of FAST along with its data structure. The four modules of FAST, namely, update, search, f lushing, and recovery are discussed in Sections 5 8, respectively. Section 9 gives experimental results. Finally, Section 10 concludes the paper.

2 Flash memory storage overview Figure 1 gives an overview of a typical flash memory storage device. In flash memory, data is stored in an array of flash blocks. Each block contains 64128 pages, where a page is the smallest unit of access. Flash memory supports three types of operations: read, write, and erase. The Erase operation is the most expensive one where it can only be done at the block level and results in setting of all bits within a block to ones. Read is a low latency page level operation and can occur randomly anywhere in the flash memory without incurring any additional cost. Write is also a page level operation and can be performed only once a page has been previously erased since it sets the required bits to zeros. In that sense, writing on a previously erased block is a low latency operation, and termed as a sequential write, while writing on an already written block will result in a block erase operation before the actual write operation, and thus, would incur much higher cost. Current generation flash memory-based storage devices have varying access latencies for each of these operations. On average, compared to read operations, write operations are eight times slower, while erase operations are 60 times slower [6]. The typical access latencies for read, write, and erase operations in flash memory devices are 25, 200 and 1,500 s, respectively [3]. The Flash Translation layer (FTL) [26] is a layer on top of NAND flash memory that makes the flash memory device acts like a virtual disk. The FTL layer receives read and write commands for logical pages addresses from the application layer and converts them to the internal flash memory commands (i.e., read page, write page, erase block) on physical pages/blocks addresses. To emulate disk like in-place update operation for a logical page Plogical , the FTL writes data into a new physical page Pphysical, maintains a mapping between logical pages and physical pages, and marks

Geoinformatica (2013) 17:417448 Fig. 1 Flash memory storage. Grey rectangles represent pages that are contained in blocks represented by dotted rectangles
Logical Page Read Logical Page Write

421

Flash Translation Layer (FTL)
Page Read Block Erase Page Write

NAND Flash Memory

the previous physical location of Pphysical as invalid for future garbage collection. Even though FTL allows existing disk based applications to use flash memory without any modications, it needs to internally deal with flash physical constraint of erasing a block before updating a page in that block. Besides this asymmetric read and write latency issue, a flash memory block can only be erased for a limited number of times (e.g., 105 106 ), after which it acts like a read-only device [3]. FTL employs various wear-leveling techniques to even out the erase counts of different blocks in the flash memory to increase its longevity [8]. However, still early wear-out of flash memory is one of the big concerns in widely deploying flash memory storage devices [31], and thus, it is of essence that flash memory avoids block erases as much as possible. Recent studies show that current FTL schemes are very effective for the workloads with sequential access write patterns. However, for the workloads with random access patterns, these schemes show very poor performance [9]

3 Related work Previous approaches for flash-aware index structures can be classified into two categories: (1) Making an existing specif ic index structure flash-aware, which includes flash-aware B-tree (e.g., FlashDB [30] and BFTL [36]) and flash-aware R-tree (e.g., RFTL [35]). The main idea of these index structures is to save the B-tree (R-tree) operations in a reservation buffer residing on main memory. When the reservation buffer is full, its content is totally flushed to flash memory. For instance, BFTL and RFTL are adding a buffering layer on top of the flash translation layer in order to make B-trees work efficiently on flash devices. (2) Designing brand new onedimensional index structures specific to the flash storage, e.g., the LA-tree [2] and the FD-tree [24, 25]. LA-tree is flash friendly index structure that is intended to replace the B-tree. LA-tree stores the updates in cascaded buffers residing on flash memory and, then empties these buffers dynamically based on the operations workload. FDtree is also a one-dimensional index structure that allows small random writes to

422

Geoinformatica (2013) 17:417448

occur only in a small portion of the tree called the head tree which exists at the top level of the tree. When the capacity of the head tree is exceeded, its entries are merged in batches to subsequent tree levels. In terms of the performance-durability trade-off, previous approaches either: (a) achieve efficiency, yet sacrifice durability, by buffering updates in main memory and flush them in batches to flash memory to amortize the cost of random writes [24, 25, 30, 35, 36]. However, storing updates in memory without taking into account system failures, which leads to durability issue, where in-memory updates may be lost if the system crashes, or (b) achieve durability, yet sacrifice efficiency, by writing all the recent updates in a sequential log file [2], hence retrieving the updates from the log file in case of a system crash. However, doing this increases the cost of search for many log entries that need to be read from flash in order to access each tree node with search and update operations [30]. FAST distinguishes itself from all previous techniques in three main aspects: (1) FAST is a general framework for data-partitioning tree index structures built inside GiST [15]. As GiST is a generalized index structure that can instantiate a wide set of data-partitioning trees that include B-tree [4], R-tree [14], R*-tree [5], SS-tree [34], and SR-tree [17]), FAST can support any tree that GiST is supporting. (2) FAST ensures both the ef f iciency and durability of system transactions where updates are buffered in memory, yet, an efficient crash recovery technique is triggered in case of a system crash to ensure the durability. (3) FAST is not a brand new index structure, hence does not need to replace existing tree indexes. However, it complements the existing tree index structures in database management systems to make them work efficiently on flash storage devices, with much less implementation cost.

4 Fast system overview Figure 2 gives an overview of FAST. The original tree is stored on persistent flash memory storage while recent updates are stored in an in-memory buffer. Both parts need to be combined together to get the most recent version of the tree structure. FAST has four main modules, depicted in bold rectangles, namely, update, search, flushing, and crash recovery. FAST is optimized for both SSDs and raw flash devices. SSDs are the dominant flash device for large database applications. On the other hand, raw flash chips, which are dominant in embedded systems and custom flash cards (e.g., ReMix [19]), are getting popular for data-intensive applications. 4.1 FAST modules In this section, we explain FAST system architecture, along with its four main modules; (1) Update, (2) Search, (3) Flushing, and (4) Crash recovery. The actions of these four modules are triggered through three main events, namely, search queries, data updates, and system restart. Update module Similar to some of the previous research for indexing in flash memory, FAST buffers its recent updates in memory, and flushes them later, in bulk, to the persistent flash storage. However, FAST update module distinguishes itself from previous research in two main aspects: (1) FAST does not store the

Geoinformatica (2013) 17:417448

423

Fig. 2 FAST system architecture

update operations in memory, instead, it stores the results of the update operations in memory, and (2) FAST ensures the durability of update operations by writing small log entries to the persistent storage. These log entries are written sequentially to the flash storage, i.e., very small overhead. Details of the update module will be discussed in Section 5. Search module The search module in FAST answers point and range queries that can be imposed to the underlying tree structure. The main challenge in the search module is that the actual tree structure is split between the flash storage and the memory. Thus, the main responsibility of the search module is to construct the recent image of the tree by integrating the stored tree in flash with the tree updates in memory that did not make it to the flash storage yet. Details of the search module will be discussed in Section 6. Flushing module As the memory resource is limited, it will be filled up with the recent tree updates. In this case, FAST triggers its flushing module that employs a f lushing policy to select some of the in-memory updates and write them, in bulk, into the flash storage. Previous research in flash indexing flush their in-memory updates or log file entries by writing all the memory or log updates once to the flash storage. In contrast, the flushing module in FAST distinguishes itself from previous techniques in two main aspects: (1) FAST employs f lushing policies that smartly selects some of the updates from memory to be flushed to the flash storage in a way that amortizes the expensive cost of the block erase operation over a large set of random write operations, and (2) FAST logs the flushing process using a single log entry written sequentially on the flash storage. Details of the flushing module will be discussed in Section 7. Crash recovery module FAST employs a crash recovery module to ensure the durability of update operations. This is a crucial module in FAST, as only because of this module, we are able to have our updates in memory, and not to worry about any data losses. This is in contrast to previous research in flash indexing that may encounter data losses in case of system crash, e.g., [24, 25, 35, 36]. The crash recovery

424

Geoinformatica (2013) 17:417448

module is mainly responsible on two operations: (1) Once the system restarts after crash, the crash recovery module utilizes the log file entries, written by both the update and flushing modules, to reconstruct the state of the flash storage and inmemory updates just before the crash took place, and (2) maintaining the size of the log file within the allowed limit. As the log space is limited, FAST needs to periodically compact the log entries. Details of this module will be discussed in Section 8. 4.2 FAST design goals FAST avoids the tradeoff of durability and efficiency by using a combination of buffering and logging. Unlike existing efficient-but-not-durable designs [24, 25, 30, 35, 36], FAST uses write-ahead-logging and crash recovery to ensure strict system durability. FAST makes tree updates efficient by buffering write operations in main memory and by employing an intelligent flushing policy that optimizes I/O costs for both SSDs and raw flash devices. Unlike existing durable-but-inefficient solutions [2], FAST does not require reading in-flash log entries for each search/update operation, which makes reading FAST trees efficient. 4.3 FAST data structure Other than the underlying index tree structure stored in the flash memory storage, FAST maintains two main data structures, namely, the Tree Modif ications Table, and Log File, described below. Tree modif ications table This is an in-memory hash table (depicted in Fig. 3) that keeps track of recent tree updates that did not make it to the flash storage yet. Assuming no hashing collisions, each entry in the hash table represents the modification applied to a unique node identifier, and has the form (status, list) where status is either NEW, DEL, or MOD to indicate if this node is newly created, deleted, or just modified, respectively, while list is a pointer to a new node, null, or a list of node modifications based on whether the status is NEW, DEL, or MOD, respectively. For MOD case, each modification in the list is presented by the quadruple (TimeStamp, type, index, value) where TimeStamp represents the time at

Fig. 3 Tree modifications table

Geoinformatica (2013) 17:417448

425

which the update happened, type is either K , P F , or P M , to indicate if the modified entry is the key, a pointer to a flash node, or a pointer to an in-memory node, respectively, while index and value determines the index and the new value for the modified node entry, respectively. In Fig. 3, there are two modifications in nodes A and D, one modification in nodes B and F , while node G is newly created and node H is deleted. Log f ile This is a set of flash memory blocks, reserved for recovery purposes. A log file includes short logs, written sequentially, about insert, delete, update, and flushing operations. Each log entry includes the triple (operation, node_list, modif ication) where operation indicates the type of this log entry as either insert, delete, update, or flush, node_list includes the list of affected nodes by this operation in case of a flush operation, or the only affected node, otherwise, modif ication is similar to the triple (type, index, value), used in the tree modif ications table. All log entries are written sequentially to the flash storage, which has a much lower cost than random writes that call for the erase operation. 4.4 Running example Throughout the rest of this paper, we will use Fig. 4 as a running example where six objects O1 to O6 , depicted by small black circles, are indexed by an R-tree. Then, two objects O7 and O8 , depicted by small white circles, are to be inserted in the same Rtree. Figure 4a depicts the eight objects in the two-dimensional space domain, while Fig. 4b gives the flash-resident R-tree with only the six objects that made it to the

10
O2

O1 O8

8
O3 O5 O4 O7

6 4 2

O6

0

2

4

6

8

10

12

14

(a) 2D Space
A
C Mod Mod Mod Mod 3, K, 2, (5,10,8,7) 1, K, 2, (12,4,14,2)

2, K, 2, O7

B

C

G B

D O1 O2

E O3

F O4 O5 O6

G

D

4, K, 2, O8

(b) R-tree Index

(c) Tree Modifications Table

Fig. 4 Illustrating example for search and update operations in FAST

426 Table 1 Cost analysis parameters Parameter T RM WM RF WF EF Definition

Geoinformatica (2013) 17:417448

The underlying tree index structure to which FAST has been applied The average time to read a node update entry from the tree modifications table The average time to write a node update entry to the tree modifications table The average time to read a tree node from the underlying tree T residing on flash memory The average time to write a tree node to the underlying tree T residing on flash memory The average time to erase a whole block on the flash memory device

flash memory. Finally, Fig. 4c gives the in-memory buffer (tree modif ications table) upon the insertion of O7 and O8 in the tree. 4.5 Operations cost parameters For each FAST module, we analyze the cost model of its main operations, including search, update, flushing, crash recovery and log compaction. To this end, we define the parameters given in Table 1.

5 Tree updates in FAST This section discusses the update operations in FAST, which include inserting a new entry and deleting/updating an existing entry. An update operation to any tree in FAST may result in creating new tree nodes as in the case of splitting operations (i.e., when inserting an element in the tree leads to node overflow), deleting existing tree nodes as in the case of merging operations (i.e., when deleting an element from the tree leads to node underflow), or just modifying existing node keys and/or pointers. Main idea For any update operation (i.e., insert, delete, update) that needs to be applied to the index tree, FAST does not change the underlying insert, delete, or update algorithm for the tree structure it represents. Instead, FAST runs the underlying update algorithm for the tree it represents, with the only exception of writing any changes caused by the update operation in memory instead of the external storage, to be flushed later to the flash storage, and logging the result of the update operation. A main distinguishing characteristic of FAST is that what is buffered in memory, and also written in the log file, is the result of the update operation, not a log of this operation. Algorithm Algorithm 1 gives the pseudo code of inserting an object Obj in FAST. The algorithms for deleting and updating objects are similar in spirit to the insertion algorithm, and thus are omitted from the paper. The algorithm mainly has two steps: (1) Executing the insertion in memory (Line 2 in Algorithm 1). This is basically done by calling the insertion procedure of the underlying tree, e.g., R-tree insertion, with two main differences. First, the insertion operation calls the search operation, discussed later in Section 6, to find where we need to insert our data based on the most recent version of the tree, constructed from main memory recent updates and

Geoinformatica (2013) 17:417448

427

Algorithm 1 Insert an Object in the Tree 1: Function Insert(Obj) /* STEP 1: Executing the Insertion in Memory only */ 2: L  List of modified nodes from the in-memory execution of inserting Obj in the underlying tree /* STEP 2: Buf fering and Logging the Updates */ 3: for each Node N in L do 4: HashEntry  N entry in the Tree Modif ications Table 5: if HashEntry is not NULL then 6: Add the triple (MOD, N , updates in N ) to the log file 7: if the status of HashEntry is MOD then 8: Add the changes in N to the list of changes of HashEntry 9: else 10: Apply the changes in N to the new node of HashEntry 11: end if 12: else 13: HashEntry  Create a new entry for N in the Tree Modif ications Table 14: if N is a newly created node then 15: Add the triple (NEW, N , updates in N ) to the log file 16: Set HashEntry status to NEW, and its pointer to N 17: else 18: Add the triple (MOD, N , updates in N ) to the log file 19: Set HashEntry status to MOD, and its pointer to the list of changes that took place in N 20: end if 21: end if 22: end for

the in-flash tree index structure. Second, the modified or newly created nodes that result back from the insertion operation are not written back to the flash storage, instead, they will be returned to the algorithm in a list L. Notice that the insertion procedure may result in creating new nodes if it encounters a split operation. (2) Buf fering and logging the tree updates (Lines 322 in Algorithm 1). For each modified node N in the list L, we check if there is an entry for N in our in-memory buffer, tree modif ications table. If this is the case, we first add a corresponding log entry that records the changes that took place in N . Then, we either add the changes in N to the list of changes in its entry in the tree modif ications table if this entry status is MOD, or update N entry in the tree modif ications table, if the entry status is NEW. On the other hand, if there is no entry for N in the tree modif ications table, we create such entry, add it to the log file, and fill it according to whether N is a newly created node or a modified one. Example In our running example of Fig. 4, inserting O7 results in modifying two nodes, G and C. Node G needs to have an extra key to hold O7 while node C needs to modify its minimum bounding rectangle that points to G to accommodate its size change. The changes in both nodes are stored in the tree modif ications table depicted

428 Fig. 5 FAST logging and recovery example

Geoinformatica (2013) 17:417448

Log# 1 2 3 4 5

Operation MOD MOD MOD MOD FLUSH

Node C G B D B, C, D

Modification 1, K, 2, (12,4,14,2) 2, K, 2, O7 3, K, 2, (5,10,8,7) 4, K,2, O8 *

(a) FAST Log File
Log# 1 Operation MOD Node G Modification 2, K, 2, O7

(a) FAST Log File after Crash Recovery

in Fig. 4c. The log entries for this operation are depicted in the first two entries of the log file of Fig. 5a. Similarly, inserting O8 results in modifying nodes, D and B Cost analysis For a given update operation U applied to a tree index structure T , let yi,U  {0, 1} represent whether or not node i of T has been modified by U . Let N be the total number of nodes in T at the time U is applied, then the total cost CU of update operation U applied on T is as follows:
N

CU = C Q +
i=0

yi,U  [W F + W M + L]

(1)

The update operation (e.g., insert, delete, modify) requires first a search query Q for a proper leaf node in T . This also takes the same search time C Q as illustrated above. For each updated node i due to applying U , yi,U = 1, and for each of these updates we write a sequential log entry to the log file that each takes W F time. Hence, the total time to write all log entries is equal to iN =0 yi,U  W F . For each updated node i, we also perform a lookup on the tree modifications table to get the entry for node i, which is performed in constant time L. In addition, the total time to write the modifications to all nodes (for which yi,U = 1) in the tree modifications table is N i=0 yi,U  W M . All of the above sums up to give the update cost given in Eq. 1 6 Searching in FAST Given a query Q, the search operation returns those objects indexed by FAST and satisfy Q. The search query Q could be a point query that searches for objects with a specific (point) value, or a range query that searches for objects within a specific range. An important promise of FAST is that it does not change the main search algorithm for any tree it represents. Instead, FAST complements the underlying searching algorithm to consider the latest tree updates stored in memory. Main idea As it is the case for any index tree, the search algorithm starts by fetching the root node from the secondary storage, unless it is already buffered in memory. Then, based on the entries in the root, we find out which tree pointer to follow to

Geoinformatica (2013) 17:417448

429

fetch another node from the next level. The algorithm goes on recursively by fetching nodes from the secondary storage and traversing the tree structure till we either find a node that includes the objects we are searching for or conclude that there are no objects that satisfy the search query. The challenging part here is that the retrieved nodes from the flash storage do not include the recent in-memory stored updates. FAST complements this search algorithm to apply the recent tree updates to each retrieved node from the flash storage. In particular, for each visited node, FAST constructs the latest version of the node by merging the retrieved version from the flash storage with the recent in-memory updates for that node. Algorithm Algorithm 2 gives the pseudo code of the search operation in FAST. The algorithm takes two input parameters, the query Q, which might be a point or range query, and a pointer to the root node R of the tree we want to search in. The output of the algorithm is the list of objects that satisfy the input query Q. Starting from the root node and for each visited node R in the tree, the algorithm mainly goes through two main steps: (1) Constructing the most recent version of R (Line 2 in Algorithm 2). This is mainly to integrate the latest flash-residant version of R with its in-memory stored updates. Algorithm 3 gives the detailed pseudo code for this

Algorithm 2 Searching for an Object indexed by the Tree 1: Function Search(Query Q, Tree Node R) /* STEP 1: Constructing the most recent version of R */ 2: N  RetrieveNode(R) /* STEP 2: Recursive search calls */ 3: if N is non-leaf node then 4: Check each entry E in N . If E satisfies the query Q, invoke Search(Q, E.NodePointer) for the subtree below E 5: else 6: Check each entry E in N . If E satisfies the search query Q, return the object to which E is pointing 7: end if

Algorithm 3 Retrieving a tree node 1: Function RetrieveNode(Tree Node R) 2: FlashNode  Retrieve node R from the flash-resident index tree 3: HashEntry  R's entry in the Tree Modif ications Table 4: if HashEntry is NULL then 5: return FlashNode 6: end if 7: if the status of HashEntry is MOD then 8: FlashNode  FlashNode  All the updates in HashEntry list 9: return FlashNode 10: end if /* We are trying to retrieve either a new or a deleted node */ 11: return the node that HashEntry is pointing to

430

Geoinformatica (2013) 17:417448

step, where initially, we read R from the flash storage. Then, we check if there is an entry for R in the tree modif ications table. If this is not the case, then we know that the version we have read from the flash storage is up-to-date, and we just return it back as the most recent version. On the other hand, if R has an entry in the tree modif ications table, we either apply the changes stored in this entry to R in case the entry status is MOD, or just return the node that this entry is pointing to instead of R. This return value could be null in case the entry status is DEL. (2) Recursive search calls (Lines 37 in Algorithm 2). This step is typical in any tree search algorithm, and it is basically inherited from the underlying tree that FAST is representing. The idea is to check if R is a leaf node or not. If R is a non-leaf node, we will check each entry E in the node. If E satisfies the search query Q, we recursively search in the subtree below E. On the other hand, if R is a leaf node, we will also check each entry E in the node, yet if E satisfies the search query Q, we will return the object to which E is pointing to as an answer to the query. Example Given the range query Q in Fig. 4a, FAST search algorithm will first fetch the root node A stored in flash memory. As there is no entry for A in the tree modif ications table (Fig. 4c), then the version of A stored in flash memory is the most recent one. Then, node C is the next node to be fetched from flash memory by the searching algorithm. As the tree modif ications table has an entry for C with status MOD, the modifications listed in the tree modif ications table for C will be applied to the version of C read from the flash storage. Similarly, the search algorithm will construct the leaf nodes F and G by first fetching them from flash memory, and then reading their recent updates from the tree modif ications table. Finally, the result of this query is { O4 , O5 , O6 , O7 }. Cost analysis For a given search query Q applied to a tree index structure T , let xi, Q  {0, 1} represent whether node i of T is visited or not when issuing query Q. Let Mi, Q be the number of modifications applied to node i and buffered in the tree modifications table at the time Q is issued. Let N be the total number of nodes in T at the time Q is issued, then the total search cost C Q on T is as follows:
N

CQ =
i=0

xi, Q  [ R F + ( Mi, Q  R M ) + L]

(2)

Assuming a range query, the search operation returns a number of objects within the query range. In FAST, when reading a node i from the flash-resident R-tree, we also need to accommodate all the corresponding modifications on i that have been recorded in the tree modifications table. Then, the total cost of reading a node i would thus be ( R F + Tm ) where Tm is the in-memory processing time for each node. For the in-memory processing part, it first takes constant time L to locate the node in the tree modification table , and then takes a linear scan of the list to apply all the modifications. Given that the number of modifications associated with each node is Mi, Q , then Tm = ( Mi, Q  R M ) + L, where Mi, Q is upper bounded by the memory size.

Geoinformatica (2013) 17:417448

431

7 Memory flushing in FAST As discussed in Section 5, the effect of all incoming updates in FAST has to be buffered in memory. As memory is a scarce resource, it will eventually be filled up with incoming updates. In that case, FAST triggers its flushing module, equipped with a f lushing policy, to free some memory space by evicting a selected part of the memory, termed a f lushing unit, to the flash storage. Such flushing is done in a way that amortizes the cost of expensive random write operations over a high number of update operations. In this section, we first define the flushing unit. Then, we discuss the flushing policy used in FAST. Finally, we explain the FAST flushing algorithm. The motivation of having a f lushing policy that flushes only part of the memory is twofold: (1) Clearing the whole memory at once will cause a significant pause to the system due to the need of erasing all the flash blocks that include at least one update record in memory. As a result, it is better to consider clearing only part of the memory in a way that does not really pause the system. In this paper, we present two main flushing policies employed by the system, and we empirically evaluate both of them, (2) Considering that we need to flush only part of the memory, it is crucial to select that part in a way that reduces the overhead of the block erase operation. 7.1 Flushing unit An important design parameter, in FAST, is the size of a f lushing unit, the granularity of consecutive memory space written in the flash storage during each flush operation. Our goal is to find a suitable f lushing unit size that minimizes the average cost of flushing an update operation to the flash storage, denoted as C. The value of C av erage writing cost depends on two factors: C1 = numb ; the average cost per bytes written, er of written b ytes

er of written b ytes ; the number of bytes written per update. This gives C = and C2 = numb numb er of updates C1  C2 . Interestingly, the values of C1 and C2 show opposite behaviors with the increase of the f lushing unit size. First consider C1 . On raw flash devices (e.g., ReMix [19]), for a f lushing unit smaller than a flash block, C1 decreases with the increase of the flushing unit size (see [29] for more detail experiments). This is intuitive, since with a larger f lushing unit, the cost of erasing a block is amortized over more bytes in the flushing unit. The same is also true for SSDs since small random writes introduce large garbage collection overheads, while large random writes approach the performance of sequential writes. Previous work has shown that, on several SSDs including the ones from Samsung, MTron, and Transcend, random write latency per byte increases by 32 when the write size is reduced from 16 KB to 0.5 KB [7]. Even on newer generation SSDs from Intel, we observed an increase of 4 in a similar experimental setup. This suggests that a flushing unit should not be very small, as that would result in a large value of C1 . On the other hand, the value of C2 increases with increasing the size of the f lushing unit. Due to non-uniform updates of tree nodes, a large flushing unit is unlikely to have as dense updates as a small flushing unit. Thus, the larger a f lushing unit is, the less the number of updates per byte is (i.e., the higher the value of C2 is). Another disadvantage of large f lushing unit is that it may cause a significant pause to the system. All these suggest that the f lushing unit should not be very large.

432

Geoinformatica (2013) 17:417448

Deciding the optimal size of a f lushing unit requires finding a sweet spot between the competing costs of C1 and C2 . Our experiments show that for raw flash devices, a f lushing unit of one flash block minimizes the overall cost. For SSDs, a f lushing unit of size 16 KB is a good choice, as it gives a good balance between the values of C1 and C2 . Note that a flushing unit size of 16 KB also matches the optimal size of a tree node, as suggested by Gray et al. [13]. Thus, with a tree of this optimal node size of 16 KB, we can simply flush one node at a time from the memory. 7.2 Flushing policies FAST is designed so that different flushing policies can be plugged in to the system. In the rest of this section, we discuss two main flushing policies adopted by FAST: (1) FAST Flushing Policy, and (2) FAST* Flushing Policy. 7.2.1 FAST f lushing policy The main idea of FAST f lushing policy is to minimize the average cost of writing each update to the underlying flash storage. To that end, FAST flushing policy aims to flush the in-memory tree updates that belong to the f lushing unit that has the highest number of in-memory updates. In that case, the cost of writing the f lushing unit will be amortized among the highest possible number of updates. Moreover, since the maximum number of updates are being flushed out, this frees up the maximum amount of memory used by buffered updates. Finally, as done in the update operations, the flushing operation is logged in the log file to ensure the durability of system transactions. Data structure The flushing policy maintains an in-memory max heap structure, termed FlushHeap, of all f lushing units that have at least one in-memory tree update. The max heap is ordered on the number of in-memory updates for each f lushing unit, and is updated with each incoming tree update. Updates in max heap is O(n), where n is the number of flash blocks with in-memory updates. In the mean time, retrieving the flushing unit with maximum number of updates is an O(1) operation. 7.2.2 FAST* f lushing policy The FAST* f lushing policy is an enhancement over the FAST flushing policy described in Section 7.2.1. FAST* flushing policy takes into account two parameters that helps in deciding which unit must be flushed: (1) Number of updates per flushing unit: It is the same parameter used by the FAST flushing policy explained in Section 7.2.1; which favors the flash unit that has the highest number of updates, and (2) Time stamp of the flushing unit: which represents the last time a flash block has been updated. When deciding which unit needs to be flushed, that parameter gives higher priority to the flushing unit that has the lowest time stamp (i.e., least recently updated). FAST* Flushing Policy employs a Top-1 selection algorithm to select a flushing unit to be evicted to flash memory with the objective of maximizing the number of updates per flushing unit and minimizing the time stamp of the flushing unit. The intuition behind such a policy is that it is sometimes better to keep the block that has the highest number of updates in the tree modifications table (i.e., in memory)

Geoinformatica (2013) 17:417448

433

and not to flush it, especially if that block is expected to receive more updates (i.e., recently updated block). On the other hand, it might be better to flush a flash block that has a bit less number of updates, but it is not expected to be updated frequently (i.e., least recently updated block). Hence, FAST* flushing policy makes that tradeoff between the two parameters in order to amortize the total number of erase operations on flash memory storage systems. 7.3 Flushing algorithm Algorithm 4 gives the pseudo code for flushing tree updates. The algorithm has two main steps: (1) Finding out the list of f lushed tree nodes (Lines 29 in Algorithm 4). This step starts by finding out the victim f lushing unit, MaxUnit, using the flushing policy passed to the algorithm. Then, we scan the tree modif ications table to find all updated tree nodes that belong to MaxUnit. For each such node, we construct the most recent version of the node by retrieving the tree node from the flash storage, and updating it with the in-memory updates. This is done by calling the RetrieveNode(N ) function, given in Algorithm 3. The list of these updated nodes constitute the list of to be flushed nodes, FlushList. (2) Flushing, logging, and cleaning selected tree nodes (Lines 1015 in Algorithm 4). In this step, all nodes in the FlushList are written once to the flash storage. As all these nodes reside in one f lushing unit, this operation would have a minimal cost due to our careful selection of the f lushing unit size. Then, similar to update operations, we log the flushing operation to ensure durability. Finally, all flushed nodes are removed from the tree modif ications table to free memory space for new updates. Algorithm 4 Flushing Tree Updates 1: Function FlushTreeUpdates( FlushPolicy) /* STEP 1: Finding out the list of flushed tree nodes */ 2: FlushList  { } 3: MaxUnit  Retrieve Unit to be Flushed uisng FlushPolicy 4: for each Node N in tree modif ications table do 5: if N  MaxUnit then 6: F  RetrieveNode(N ) 7: FlushList  FlushList  F 8: end if 9: end for /* STEP 2: Flushing, logging, and cleaning selected nodes */ 10: Flush all tree updates  FlushList to a clean flash memory block 11: Add (Flush, All Nodes in FlushList) to the log file 12: Erase the old flash memory block and update the index pointer to refer the new block 13: for each Node F in FlushList do 14: Delete F from the Tree Modif ications Table 15: end for Example In our running example given in Fig. 4, assume that the memory is full, hence FAST triggers its flushing module. Assume also that nodes B, C, and D reside

434

Geoinformatica (2013) 17:417448

in the same f lushing unit B1 , while nodes E, F , and G reside in another f lushing unit B2 . The number of updates in B1 is three as each of nodes B, C and D has been updated once. On the other hand, the number of updates in B2 is one because nodes E and F has no updates at all, and node G has only a single update. Hence, as per FAST flushing policy, MaxUnit is set to B1 , and we will invoke RetrieveNode algorithm for all nodes belonging to B1 (i.e., nodes B, C, and D) to get the most recent version of these nodes and flush them to flash memory. Then, the log entry ( Flush; Nodes B, C, D) is added to the log file (depicted as the last log entry in Fig. 5a). Finally, the entries for nodes B, C, and D are removed from the tree modif ications table. Cost analysis For a given flushing operation F applied to a tree index structure T , Let Pflush be the set of tree nodes that belongs to the block selected to be flushed. Let M p be the number of modifications applied to node p and buffered in the tree modifications table at the time F is applied. Hence, the total cost C F of flushing operation F applied on T is as follows: CF = EF + H +
p  Pflush

[ R F + ( M p  R M ) + W F + L]

(3)

We decide which unit to flush by employing the flushing policy passed to the algorithm. The cost of this in memory operation H varies based on which flushing policy is activated. For each node p  Pflush , we first need to retrieve the node current value saved in flash memory which costs p  Pflush R F , and then lookup the node in the tree modifications table in p  Pflush L. For each node p  Pflush , we read all M p modifications of p that are buffered in the tree modifications table, which sum up to p  Pflush ( M p  R M ). Before we write the new nodes values, we first erase the whole flash block which costs E F time. For each node p  Pflush , we write the flushed node new value, that costs p  Pflush W F . All of the above sum up to give the flushing operation cost given by Eq. 3.

8 Crash recovery and log compaction in FAST As discussed before, FAST heavily relies on storing recent updates in memory, to be flushed later to the flash storage. Although such design efficiently amortizes the expensive random write operations over a large number of updates, it poses another challenge where memory contents may be lost in case of system crash. To avoid such loss of data, FAST employs a crash recovery module that ensures the durability of in-memory updates even if the system crashed. The crash recovery module in FAST mainly relies on the log file entries, written sequentially upon the update and flush operations. In this section, we will first describe the crash recovery module and logging mechanism in FAST. Then, we will follow by discussing the log compaction operation in FAST, which is mainly done to ensure that the log file is within a certain size limit. Log compaction has a very similar operation to the recovery module, and it is crucial to keep up the efficiency of FAST. For simplicity, we will not consider the case of having a system crash during the recovery process, as this can be handled in a similar way to traditional recovery modules in database systems.

Geoinformatica (2013) 17:417448

435

8.1 Recovery The recovery module in FAST is triggered when the system restarts from a crash, with the goal of restoring the state of the system just before the crash took place. The state of the system includes the contents of the in-memory data structure, tree modif ications table, and the flash-resident tree index structure. By doing so, FAST ensures the durability of all non-flushed updates that were stored in memory before crash. Main idea The main idea of the recovery operation is to scan the log file bottomup to be aware of the flushed nodes, i.e., nodes that made their way to the flash storage. During this bottom-up scanning, we also find out the set of operations that need to be replayed to restore the tree modif ications table. Then, the recovery module cleans all the flash blocks, and starts to replay the non-flushed operations in the order of their insertion, i.e., top-down. The replay process includes insertion in the tree modif ications table as well as a new log entry. It is important here to reiterate our assumption that there will be no crash during the recovery process, so, it is safe to keep the list of operations to be replayed in memory. If we will consider a system crash during the recovery process, we might just leave the operations to be replayed in the log, and scan the whole log file again in a top-down manner. In this top-down scan, we will only replay the operations for non-flushed nodes, while writing the new log entries into a clean flash block. The result of the crash recovery module is that the state of the memory will be stored as it was before the system crashes, and the log file will be an exact image of the tree modif ications table. Algorithm Algorithm 5 gives the pseudo code for crash recovery in FAST, which has two main steps: (1) Bottom-Up scan (Lines 211 in Algorithm 5). In this step, FAST scans the log file bottom-up, i.e., in the reverse order of the insertion of log entries. For each log entry L in the log file, if the operation of L is Flush, then we know that all the nodes listed in this entry have already made their way to the flash storage. Thus, we keep track of these nodes in a list, termed FlushedNodes, so that we avoid redoing any updates over any of these nodes later. On the other side, if the operation of L is not Flush, we check if the node in L entry is in the list FlushedNodes. If this is the case, we just ignore this entry as we know that it has made its way to the flash storage. Otherwise, we push this log entry into a stack of operations, termed RedoStack, as it indicates a non-flushed entry at the crash time. At the end of this step, we pass the RedoStack to the second step. (2) Top-Down processing (Lines 1319 in Algorithm 5). At the beginning, we first create a new log file Fnew . Then, this step basically goes through all the entries in the RedoStack in a top-down way, i.e., the order of insertion in the log file. As all these operations were not flushed by the crash time, we just add each operation to the tree modif ications table and add a corresponding log entry in the new Log File Fnew . The reason of doing these operations in a top-down way is to ensure that we have the same order of updates, which is essential in case one node has multiple non-flushed updates. At the end of this step, the tree modif ications table will be exactly the same as it was just before the crash time, while the new log file Fnew will be exactly an image of the tree modif ications table stored in the flash storage. Finally, we change the log file pointer to refer to the new log file Fnew and we finally erase the old log file flash blocks.

436

Geoinformatica (2013) 17:417448

Algorithm 5 Crash Recovery 1: Function RecoverFromCrash() /* STEP 1: Bottom-Up Cleaning */ 2: FlushedNodes   3: for each Log Entry L in the log file in a reverse order do 4: if the operation of L is Flush then 5: FlushedNodes  FlushedNodes  the list of nodes in L 6: else 7: if the node in entry L  / FlushedNodes then 8: Push L into the stack of updates RedoStack 9: end if 10: end if 11: end for /* Phase 2: Top-Down Processing */ 12: Create a new Log File Fnew 13: while RedoStack is not Empty do 14: Op  Pop an update operation from the top of RedoStack 15: Insert the operation Op into the tree modif ications table 16: Add a log entry for Op in the new log file Fnew 17: end while 18: Change the Log File pointer to refer to the new Log File Fnew 19: Clean all the old log entries by erasing the old log file flash blocks

Example In our running example, the log entries of inserting Objects O7 and O8 in Fig. 4 are given as the first four log entries in Fig. 5a. Then, the last log entry in Fig. 5a corresponds to flushing nodes B, C, and D. We assume that the system is crashed just after inserting this flushing operation. Upon restarting the system, the recovery module will be invoked. First, the bottom-up scanning process will be started with the last entry of the log file, where nodes B, C, and D are added to the list FlushedNodes. Then, for the next log entry, i.e., the fourth entry, as the node affected by this entry D is already in the FlushedNodes list, we just ignore this entry, since we are sure that it has made its way to disk. Similarly, we ignore the third log entry for node B. For the second log entry, as the affected node G is not in the FlushedNodes list, we know that this operation did not make it to the storage yet, and we add it to the RedoStack to be redone later. The bottom-up scanning step is concluded by ignoring the first log entry as its affected node C is already flushed, and by wiping out all log entries. Then, the top-down processing step starts with only one entry in the RedoStack that corresponds to node G. This entry will be added to the tree modif ications table and log file. Figure 5b gives the log file after the end of the recovery module which also corresponds to the entries of the tree modif ications table after recovering from failure. Cost analysis For a given crash recovery operation R applied to a tree index structure T , let Z be the set of operations recorded in the log file. Let  (0    1) be the fraction of operations in Z that had been flushed to T before the system fails. Let Spage , Sblock , and Slog be the byte size of the flash page, flash block and flash log

Geoinformatica (2013) 17:417448

437

file, respectively. Hence, the total cost C R of a crash recovery operation R applied on T is as follows: C R = Slog  RF RF + Z    (W M + W F ) + Spage Sblock (4)

As all the Z entries in the log file have to be scanned, then the total cost to scan Slog them is R F  Spage . In addition, only Z   log file operations need to be redone (i.e., written back to the tree modifications table), which results to an additional cost of Z    W M . As all redone operations are written back to memory, an additional cost of logging them is Z    W F . The old log file blocks needs to be erased, which Slog incurs a cost of E F  Sblock . All of the above sums up to give the recovery cost given in Eq. 4. 8.2 Log compaction As FAST log file is a limited resource, it may eventually become full. In this case, FAST triggers a log compaction module that organizes the log file entries for better space utilization. This can be achieved by two space saving techniques: (a) Removing all the log entries of flushed nodes. As these nodes have already made their way to the flash storage, we do not need to keep their log entries anymore, and (b) Packing small log entries in a larger writing unit. Whenever a new log entry is inserted, it mostly has a small size that may occupy a flash page as the smallest writing unit to the flash storage. At the time of compaction, these small entries can be packed together to achieve the maximum possible space utilization. The main idea and algorithm for the log compaction module are almost the same as the ones used for the recovery module, with the exception that the entries in the RedoStack will not be added to the tree modif ications table, yet they will just be written back to the log file, in a more compact way. As in the recovery module, Fig. 5a and b give the log file before and after log compaction, respectively. The log compaction have similar expensive cost as the recovery process. Fortunately, with an appropriate size of log file and memory, it will not be common to call the log compaction module. It is unlikely that the log compaction module will not really compact the log file much. This may take place only for a very small log size and a very large memory size, as there will be a lot of non-flushed operations in memory with their corresponding log entries. Notice that if the memory size is small, there will be a lot of flushing operations, which means that log compaction can always find log entries to be removed. If this unlikely case takes place, we call an emergency f lushing operation where we force flushing all main memory contents to the flash memory persistent storage, and hence clean all the log file contents leaving space for more log entries to be added. Cost analysis The log compaction is almost the same as the crash recovery procedure. The only difference is that records are not redone (written to the tree modifications table). Similar to recovery cost, the log compaction cost CC is as follows: CC = Slog  RF RF + Z    WF + Spage Sblock (5)

438

Geoinformatica (2013) 17:417448

9 Experimental evaluation This section experimentally evaluates the performance of FAST, compared to the state-of-the-art algorithms for one-dimensional and multi-dimensional flash index structures: (1) Lazy Adaptive Tree (LA-tree) [2]: LA-tree is a flash friendly one dimensional index structure that is intended to replace the B-tree. LA-tree stores the updates in cascaded buffers residing on flash memory and, then empties these buffers dynamically based on the operations workload. (2) FD-tree [24, 25]: FD-tree is a one-dimensional index structure that allows small random writes to occur only in a small portion of the tree called the head tree which exists at the top level of the tree. When the capacity of the head tree is exceeded, its entries are merged in batches to subsequent tree levels. (3) RFTL [35]: RFTL is a mutli-dimensional tree index structure that adds a buffering layer on top of the flash translation layer (FTL) in order to make R-trees work efficiently on flash devices. We instantiate B-tree and R-tree instances of FAST using both flushing policies (i.e., FAST flushing policy and FAST* flushing policy), termed FAST-Btree, FAST*Btree, FAST-Rtree, and FAST*-Rtree , respectively, by implementing FAST inside the GiST generalized index structure [15], which is already built inside PostgreSQL [1]. In our experiments, we use two synthetic workloads: (1) Lookup intensive workload (W L ): that includes 80 % search operations and 20 % update operations (i.e., insert, delete, or update). (2) Update intensive workload, (WU ): that includes 20 % search operations and 80 % update operations. Unless mentioned otherwise, we set the number of workload operations to 10 million operations, main memory size to 256 KB (i.e., the amount of memory dedicated to main memory buffer used by FAST), tree index size to 512 MB, and log file size to 10 MB, which means that the default log size is 2 % of the index size. The experiments in this section mainly discuss the effect of varying the memory size, log file size, index size, and number of updates on the performance of FASTBtree, FAST-Rtree, LA-tree, FD-tree, and RFTL. Also, we study the performance of flushing, log compaction, and recovery operations in FAST. In addition, we compare the implementation cost between FAST and its counterparts. Our performance metrics are mainly the number of flash memory erase operations and the average response time. However, in almost all of our experiments, we got a similar trend for both performance measures. Thus, for brevity, we only show the experiments for the number of flash memory erase operations, which is the most expensive operation in flash storage. Although we compare FAST to its counterparts from a performance point of view, however we believe the main contribution of FAST is not in the performance gain. The generic structure and low implementation cost are the main advantages of FAST over specific flash-aware tree index structures. All experiments were run on both raw flash memory storage, and solid state drives (SSDs). For raw flash, we used the raw NAND flash emulator described in [2]. The emulator was populated with exhaustive measurements from a custom-designed Mica2 sensor board with a Toshiba1Gb NAND TC58DVG02A1FT00 flash chip. For SSDs, we used a 32GB MSP-SATA7525032 SSD device. All the experiments were run on a machine with Intel Core2 8400 at 3Ghz with 4GB of RAM running Ubuntu Linux 8.04.

Geoinformatica (2013) 17:417448

439

9.1 Effect of memory size Figure 6 and b give the effect of varying the memory size from 128 KB to 1,024 KB (in a log scale) on the number of erase operations, encountered in FAST-Btree, LAtree, and FD-tree, for workloads W L and WU , respectively. For both workloads and for all memory sizes, FAST-Btree consistently has much lower erase operations than that of the LA-tree. More specifically, Fast-Btree results in having only from half to one third of the erase operations encountered by LA-tree. This is mainly due to the choice of f lushing unit and f lushing policy used in FAST that amortize the block erase operations over a large number of updates. Also, for both experiments, the number of erase operations decreases with the increase of the memory size, which is intuitive as more memory means less frequent need for flushing, and hence less need for block erase operations. The performance of FAST-Btree is slightly better than that of FD-tree, because FD-tree does not employ a crash recovery technique (i.e., no logging overhead). FAST still performs better than FD-tree due to FAST flushing policy that selects the best block to be flushed to flash memory. Although the performance of FD-tree is close to FAST-Btree, however FAST has the edge of being a generic framework which is applied to many tree index structures and needs less work and overhead (in terms of lines of code) to be incorporated in the database engine. Comparing the two workloads against each other, we can see that the workload WU encounters much more erase operations than that of workload W L . This is mainly because WU is an update intensive workload which results in many in-memory updates that need to flushed. FAST*-Btree gives a slightly better performance than FAST-Btree as FAST*-Btree employs a flushing policy that does not only rely on the number of updates per flash block, but also takes into account the last time a flash block has been updated. Hence, FAST*-tree gives a chance for those flash blocks that has higher number of updates to stay in memory if more updates are expected to be applied to these blocks. Figures 7a and b give similar experiments to that of Fig. 6 and b, with the exception that we run the experiments for two-dimensional search and update operations for both the Fast-Rtree and RFTL. To be able to do so, we have adjusted our
90 600

# of erase operations *(103)

80 70 60 50 40 30 20
FAST*Btree FASTBtree LAtree 10 FDtree

# of erase operations *(103)

500 400 300 200 100 0 128 256

FAST*Btree FASTBtree LAtree FDtree

0 128

256

512

1024

512

1024

Memory Size (KB)

Memory Size (KB)

(a) WL
Fig. 6 Effect of memory size on one-dimensional index structure

(b) WU

440
100
FAST*Rtree FASTRtree RFTL

Geoinformatica (2013) 17:417448
900
FAST*-Rtree FAST-Rtree RFTL

# of erase operations *(103)

# of erase operations *(103)

800 700 600 500 400 300 200 100 0 128 256

80 60 40 20

128

256

512

1024

512

1024

Memory Size (KB)

Memory Size (KB)

(a) Spatial-WL
Fig. 7 Effect of memory size on multi-dimensional index structure

(b) Spatial-WU

workload W L and WU to Spatial-W L and Spatial-WU , respectively, which have twodimensional operations instead of the one-dimensional operations used in W L and WU . The result of these experiments have the same trend as the ones done for onedimensional tree structures, where FAST-Rtree has consistently better performance than RFTL in all cases, with around one half to one third of the number of erase operations encountered in RFTL. Similar to the one-dimesnional case, FAST*-Rtree slightly outperforms FAST-Rtree. Comparing the multi-dimensional workload to the one dimensional one shows that the multi-dimensional workload encounters more erase operations which is mainly due to the facts that the update operation may span more nodes. However, even with this, FAST still keeps its performance ratio over its counterparts. The experiments in Figs. 6 and 7 not only shows that FAST has better performance than its counterparts LA-tree, FD-tree and RFTL, but it also shows the power of the FAST framework where it can be applied to both one-dimensional and multidimensional index structures with the same efficiency. In other words, it is not only that FAST is better than LA-tree and FD-tree, but it is also the fact that FAST has the ability to efficiently support multi-dimensional search and update operations in which LA-tree or FD-tree cannot even support. 9.2 Effect of log file size Figure 8 gives the effect of varying the log file size from 10 MB (i.e., 2 % of the index size) to 25 MB (i.e., 5 % of the index size) on the number of erase operations, encountered in FAST-Btree, LA-tree, and FD-tree for workload W L (Fig. 8a) and FAST-Rtree and RFTL for workload Spatial-WU (Fig. 8b). For brevity, we do not show the experiments of FAST-Btree, LA-tree, and FD-tree for workload WU nor the experiment of FAST-Rtree and RFTL for workload Spatial-W L . As can be seen from the figures, the performance of both LA-tee, FD-tree, and RFTL is not affected by the change of the log file size. This is mainly because these three approaches rely on buffering incoming updates, and hence does not make use of any log file. It is interesting, however, to see that the number of erase operations in FAST-Btree and FAST-Rtree significantly decreases with the increase of the log file size, given that the memory size is set to its default value of 256 KB in all experiments. The

Geoinformatica (2013) 17:417448
100
FAST*Btree FASTBtree LAtree FDtree

441
800

# of erase operations *(103)

90 80 70 60 50 40 30 20 10 0 10 15

# of erase operations *(103)

700 600 500 400 300 200 100 0 10 15 20 25
FAST*Rtree FASTRtree RFTL

20

25

Maximum Log File Size (MB)

Maximum Log File Size (MB)

(a) WL
Fig. 8 Effect of FAST log file size

(b) Spatial-WU

justification for this is that with the increase of the log file size, there will be less need for FAST to do log compaction. FAST*-Btree and FAST*-Rtree shows the same trend as FAST-Btree and FAST-Rtree except that they slightly give better performance due to the fact that they apply the FAST* Flushing policy. Revisiting Figs. 6 and 7 in Section 9.1, the number of erase operations encountered in both LA-tree, FD-tree, and RFTL were only coming from flushing buffered updates, while the number of erase operations in FAST were coming from two sources, flushing in-memory updates, and log compaction. Then, the experiment in this section (Fig. 8) shows that a large fraction of the erase operations in FAST is coming from the log compaction operation, which can be significantly reduced with the slight increase of the log file. With this, we can see that FAST achieves close to an order of magnitude less erase operations than its counterparts for both onedimensional and multi-dimensional index structures when having the log file as small as 5 % of the index size, i.e., 25 MB. 9.3 Effect of index size Figure 9 gives the effect of varying the index size from 128 MB to 4 GB (in a log scale) on the number of erase operations, encountered in FAST-Btree, LA-tree, and FD-tree for workload W L (Fig. 9a) and FAST-Rtree and RFTL for workload Spatial-WU (Fig. 9b). Same as in Section 9.2, we omit other workloads for brevity. In all cases, FAST consistently gives much better performance than its counterparts. Both FAST and other index structures have similar trend of a linear increase of the number of erase operations with the increase of the index size. This is mainly because with a larger index, an update operation may end up modifying more nodes in the index hierarchy, or more overlapped nodes in case of multi-dimensional index structures. Moreover, FAST*-Btree and FAST*-Rtree give a bit better performance than FAST-Btree and FAST-Rtree, respectively. This is basically due to the fact that FAST* flushing policy handles the flash memory updates better than the original FAST flushing policy, hence when the index size increase the possibility that more blocks are updated increases leading to such performance gain for both FAST*-Btree and FAST*-Rtree. The take home message from this experiment is that FAST still maintains its performance gain over its counterparts even with the large increase of the index size.

442
# of Erase Operations *(103)
FAST*Btree FASTBtree LAtree FDtree

Geoinformatica (2013) 17:417448
# of Erase Operations *(103)
1200 1000 800 600 400 200 0
FAST*Rtree FASTRtree RFTL

140 120 100 80 60 40 20 0

B 1G

B 1G

12

25

51

B 2G

Fig. 9 Effect of tree index size

9.4 Effect of number of updates Figure 9 gives the effect of varying the number of update operations from one million to 100 millions (in a log scale) on the number of erase operations for both one-dimensional (i.e., FAST-Btree, LA-tree, and FD-tree in Fig. 10a) and multidimensional index structures (i.e., FAST-Rtree and RFTL in Fig. 10b). As we are only interested in update operations, the workload for the experiments in this section is just a stream of incoming update operations, up to 100 million operations. As can be seen from the figure, FAST scales well with the number of updates and still maintains its superior performance over its counterparts from both one-dimensional (LA-tree) and multi-dimensional index structures (RFTL). FAST performs slightly better than FD-tree; this is because FD-tree (one dimensional index structure) is buffering some of the tree updates in memory and flushes them when needed, but FAST applies a flushing policy, which flushes only the block with the highest number of updates. In addition, FAST* slightly outperforms FAST because FAST* flushing policy employs a Top-1 algorithm that maximizes the number of updates per block and minimize the timestamp at which the block has been updated, hence the total amortized update cost in FAST* is less than FAST.
10000 9000 8000 7000 6000 5000 4000 3000 2000 1000 0

12 8M B

25 6M B

51 2M B

B 2G

B 4G

B 4G

8M B

6M B

2M B

Index Size

Index Size

(a) WL

(b) Spatial-WU

# of erase operations *(103)

FAST*Btree FASTBtree LAtree FDtree

# of erase operations *(103)

FAST*Rtree FASTRtree RFTL

10000

1000

1

10

100

1

10

100

# of Updates *(106)

# of Updates *(106)

(a) FAST-Btree
Fig. 10 Effect of number of updates

(b) FAST-Rtree

Geoinformatica (2013) 17:417448

443

9.5 Flushing performance Figure 11 illustrates the performance of the f lushing policy employed by FAST compared to a naive flushing policy, termed f lush-all that just flushes all the memory contents to the flash storage once. We also compare FAST to a random flushing policy, termed Rand-Flush that chooses a block at random and flushed its contents to the flash storage The performance is given with respect to various memory sizes (Fig. 11a) and log file sizes (Fig. 11a). Both experiments were run for FASTBtree under workload WU . Running these experiments for FAST-Rtree and other workloads give similar performance, and thus omitted for brevity. Figure 11a gives the effect of varying the memory size from 128 KB to 1,024 KB on the number of erase operations for flush-all policy, Rand-Flush policy and FAST flushing policy. In all cases, FAST has much lower erase operations than the flushall and Rand-Flush policies, which is about one fourth of the erase operations for a memory size of 512 KB. The main reason behind this gain in FAST is that it amortizes the cost of the block erase operation over a large number of updates, and hence, will free more memory with each flushing operation. On the other side, in the flush-all or Rand-Flush policy, a block may be erased just because it has only one single update in the memory. In this case, although a block is erased, it does not free much memory space. The Rand-Flush policy performance is slightly better than that of flush-all policy because the Rand-Flush flushes only one block and hence keeping all other blocks in memory, which decrease the cost of random writes on these blocks. FAST* flushing policy is better than FAST flushing policy as it better amortizes the update cost. FAST* policy may still keep a block that has the highest number of updates in memory if this block has higher potential to be updated soon, and hence the decreasing the number of erase operations applied to that block. Figure 11b gives a similar experiment to that of Fig. 11a with the exception that we study the effect of changing the log size from 10 MB to 25 MB on the number of erase operations. In all cases, FAST flushing policy is superior, which is intuitive given the above explanation for Fig. 11a. However, an interesting observation from Fig. 11b is that the gain from FAST flushing policy over the flush-all and RandFlush policies increases with the increase of the log file size. This means that FAST flushing policy makes better use of the log file than the flush-all and Rand-Flush policies. A justification for this is as follows: As FAST flushing policy evicts a block
550 500 450 400 350 300 250 200 150 100 50 0 128 600

# of erase operations *(103)

# of erase operations *(103)

FAST*Flush FASTFlush FlushAll RandFlush

500 400 300 200 100 0 10 15

FAST*Flush FASTFlush FlushAll RandFlush

256

512

1024

20

25

Memory Size (KB)

Log Size (MB)

(a) Memory size
Fig. 11 Flushing performance

(b) Log size

444

Geoinformatica (2013) 17:417448

to the storage only if it has high number of updates, the log entry for this flushing operation will include many updated nodes. Then, in the log compaction process, there will be a lot of space for compaction. This would not be the case for the flushall and Rand-Flush policies where a log entry for a flush operation may include only one flushed node. Then, at the time of log compaction, there will be nothing much to compact, which means that the log compaction will be called again. As discussed in Section 9.2 and Fig. 9, log compaction is a major factor in the number of erase operations. Reducing the frequency of log compaction makes FAST flushing policy more superior than the flush-all policy. Moreover, FAST* flushing policy slightly outperforms FAST flushing policy because of the fact that FAST* may prefer to keep the block that has the highest number of updates in memory leading to less erase operations on the flash memory storage. 9.6 Log compaction Figure 12a gives the behavior and frequency of log compaction operations in FAST when running a sequence of 200 thousands update operations for a log file size of 10 MB. The Y axis in this figure gives the size of the filled part of the log file, started as empty. The size is monotonically increasing with having more update operations till it reaches its maximum limit of 10 MB. Then, the log compaction operation is triggered to compact the log file. As can be seen from the figure, the log compaction operation may compact the log file from 20 to 60 % of its capacity, which is very efficient compaction. Another take from this experiment is that we have made only seven log compaction operations for 200 thousands update operations, which means that the log compaction process is not very common, making FAST more efficient even with a large amount of update operations. 9.7 Recovery performance Figure 12b gives the overhead of the recovery process in FAST, which serves also as the overhead of the log compaction process. The overhead of recovery increases linearly with the size increase of the log file contents at the time of crash. This is intuitive as with more log entries in the log file, it will take more time from the FAST

10 8 6 4 2 0

100

Recovery Time (millisec)

90 80 70 60 50 40 30 20 10 1

FAST

Log File Size (MB)

0

50

100

150

200

2

3

4

5

6

7

8

9

Number of Updates So Far *(103)

Log Size (MB)

(a) Log Compaction
Fig. 12 Log compaction and recovery

(b) Recovery

Geoinformatica (2013) 17:417448

445

recovery module to scan this log file, and replay some of its operations to recover the lost main memory contents. However, what we really want to emphasize on in this experiment is that the overhead of recovery is only about 100 ms for a log file that includes 9 MB of log entries. This shows that the recovery overhead is a low price to pay to ensure transaction durability.

10 Conclusion This paper presented FAST; a generic framework for flash-aware data-partitioning tree index structures. FAST distinguishes itself from all previous attempts of flash memory indexing in two aspects: (1) FAST is a generic framework that can be applied to a wide class of tree index structures, and (2) FAST achieves both ef f iciency and durability of read and write flash operations. FAST has four main modules, namely, update, search, f lushing, and recovery. The update module is responsible on buffering incoming tree updates in an in-memory data structure, while writing small entries sequentially in a designated flash-resident log file. The search module retrieves requested data from the flash storage and updates it with recent updates stored in memory, if any. The f lushing module is responsible on evicting flash blocks from memory to the flash storage to give space for incoming updates. Finally, the recovery module ensures the durability of in-memory updates in case of a system crash.

References
1. PostgreSQL. http://www.postgresql.org 2. Agrawal D, Ganesan D, Sitaraman RK, Diao Y, Singh S (2009) Lazy-adaptive tree: an optimized index structure for flash devices. PVLDB 3. Agrawal N, Prabhakaran V, Wobber T, Davis J, Manasse M, Panigrahy R (2008) Design tradeoffs for SSD performance. In: Usenix annual technical conference, USENIX 4. Bayer R, McCreight EM (1972) Organization and maintenance of large ordered indices. Acta Inform 1:173189 5. Beckmann N, Kriegel H-P, Schneider R, Seeger B (1990) The R*-tree: an efficient and robust access method for points and rectangles. In: SIGMOD 6. Birrell A, Isard M, Thacker C, Wobber T (2007) A design for high-performance flash disks. ACM SIGOPS Oper Syst Rev 41(2):8893 7. Bouganim L, Jnsson B, Bonnet P (2009) uFLIP: understanding flash IO patterns. In: CIDR 8. Chang Y-H, Hsieh J-W, Kuo T-W (2007) Endurance enhancement of flash-memory storage systems: an efficient static wear leveling design. In: Proceedings of the annual ACM IEEE Design Automation Conference, DAC, pp 212217 9. Chen S (2009) FlashLogging: exploiting flash devices for synchronous logging performance. In: SIGMOD. New York, NY 10. Comer D (1979) The ubiquitous B-tree. ACM Comput Surv 11(2):121137 11. Gray J (2006) Tape is dead, disk is tape, flash is disk, RAM locality is king. http://research. microsoft.com/gray/talks/Flash_is_Good.ppt. Accessed Dec 2006 12. Gray J, Fitzgerald B (2008) Flash disk opportunity for server applications. ACM Queue 6(4):18 23 13. Gray J, Graefe G (1997) The five-minute rule ten years later, and other computer storage rules of thumb. SIGMOD Rec 26(4):6368 14. Guttman A (1984) R-trees: a dynamic index structure for spatial searching. In: SIGMOD 15. Hellerstein JM, Naughton JF, Pfeffer A (1995) Generalized search trees for database systems. In: VLDB 16. Hutsell W (2007) Solid state storage for the enterprise. Storage Networking Industry Association (SNIA) Tutorial, Fall

446

Geoinformatica (2013) 17:417448

17. Katayama N, Satoh, S (1997) The sr-tree: an index structure for high-dimensional nearest neighbor queries. In: SIGMOD 18. Kim H, Ahn S (2008) BPLRU: a buffer management scheme for improving random writes in flash storage. In: FAST 19. Lavenier D, Xinchun X, Georges G (2006) seed-based genomic sequence comparison using a FPGA/FLASH accelerator. In: ICFPT 20. Lee S, Moon B (2007) Design of flash-based DBMS: an in-page logging approach. In: SIGMOD 21. Lee S-W, Moon B, Park C, Kim J-M, Kim S-W (2008) A case for flash memory SSD in enterprise database applications. In: SIGMOD 22. Lee S-W, Park D-J, sum Chung T, Lee D-H, Park S, Song H-J (2007) A log buffer-based flash translation layer using fully-associate sector translation. TECS 23. Leventhal A (2008) Flash storage today. ACM Queue 6(4):2430 24. Li Y, He B, Luo Q, Yi K (2009) Tree indexing on flash disks. In: ICDE 25. Li Y, He B, Yang RJ, Luo Q, Yi K (2010) Tree indexing on solid state drives. Proceedings of the VLDB Endowment 3(12):11951206 26. Ma D, Feng J, Li G (2011) LazyFTL: A page-level flash translation layer optimized for NAND flash memory. In: SIGMOD 27. McCreight EM (1977) Pagination of B*-trees with variable-length records. Commun ACM 20(9):670674 28. Moshayedi M, Wilkison P (2008) Enterprise SSDs. ACM Queue 6(4):3239 29. Nath S, Gibbons PB (2008) Online maintenance of very large random samples on flash storage. In: VLDB 30. Nath S, Kansal A (2007) Flashdb: dynamic self-tuning database for NAND flash. In: IPSN 31. Reinsel D, Janukowicz J (2008) Datacenter SSDs: solid footing for growth. http://www.samsung. com/us/business/semiconductor/news/downloads/210290.pdf. Accessed Jan 2008 32. Sellis TK, Roussopoulos N, Faloutsos C (1987) The R+-tree: a dynamic index for multidimensional objects. In: VLDB 33. Shah MA, Harizopoulos S, Wiener JL, Graefe G (2008) Fast scans and joins using flash drives. In: International Workshop of Data Managment on New Hardware, DaMoN 34. White DA, Jain R (1996) Similarity indexing with the SS-tree. In: ICDE 35. Wu C, Chang L, Kuo T (2003) An efficient R-tree implementation over flash-memory storage systems. In: GIS 36. Wu C, Kuo T, Chang L (2007) An efficient B-tree layer implementation for flash-memory storage systems. TECS

Mohamed Sarwat is a PhD candidate at the Computer Science and Engineering department, University of Minnesota, where he also received his master's degree in computer science in 2011. His research interest lies in the broad area of Database systems, spatio-temporal databases, distributed graph databases, social networking, cloud computing, large-scale data management, data indexing and storage systems. He has been awarded the University of Minnesota Doctoral Dissertation Fellowship in 2012/2013. He has been a recipient of Best Research Paper Award in the 12th international symposium on spatial and temporal databases 2011.

Geoinformatica (2013) 17:417448

447

Mohamed F. Mokbel is an associate professor in the Department of Computer Science and Engineering, University of Minnesota. His current main research interests focus on providing database and platform support for spatial data, moving objects, and location-based services. Mohamed is the main architect for the PLACE, Casper, and CareDB systems that provide a database support for location-based services, location privacy, and personalization, respectively. His research work has been recognized by two best paper awards at IEEE MASS 2008 and MDM 2009 and by the NSF CAREER award 2010. Mohamed is currently the general co-chair of SSTD 2011 and program cochair for MDM 2011, DMSN 2011, and LBSN 2011. Mohamed was also the proceeding chair of ACM SIGMOD 2010, and the program co-chair for ACM SIGSPATIAL GIS 2008, 2009, and 2010. He serves in the editorial board of IEEE Data Engineering Bulletin, Distributed and Parallel Databases Journal, and Journal of Spatial Information Science. Mohamed is an ACM and IEEE member and a founding member of ACM SIGSPATIAL.

Xun Zhou received his B.Eng., and M.Eng., in Computer Science and Technology from Harbin Institute of Technology, Harbin, China in 2007 and 2009 respectively. He is currently a Ph.D. student in Computer Science at the University of Minnesota, Twin Cities. His research interests include spatiotemporal data mining, spatial databases and Geographical Information Systems (GIS). His current application focus is understanding climate change from data.

448

Geoinformatica (2013) 17:417448

Suman Nath is a researcher in the Sensing and Energy Research Group at Microsoft Research Redmond. He works on various data management problems in mobile and sensing systems. He received his PhD from Carnegie Mellon University in 2005. He has authored 20+ patents (granted or pending), 70+ papers in various computer science conferences and journals, and received Best Paper Awards at BaseNets 2004, USENIX NSDI 2006, IEEE ICDE 2008, and SSTD 2011. At Microsoft, he received the Gold Star Award, which recognizes excellence in leadership and contributions for Microsoft's long-term success.

A Demonstration of GeoSpark: A Cluster Computing Framework for Processing Big Spatial Data
Jia Yu
School of Computing, Informatics, and Decision Systems Engineering Arizona State University Tempe, Arizona 85281 Email: jiayu2@asu.edu

Jinxuan Wu
School of Computing, Informatics, and Decision Systems Engineering Arizona State University Tempe, Arizona 85281 Email: jinxuanw@asu.edu

Mohamed Sarwat
School of Computing, Informatics, and Decision Systems Engineering Arizona State University Tempe, Arizona 85281 Email: msarwat@asu.edu

Abstract--This paper demonstrates G EO S PARK a cluster computing framework for developing and processing large-scale spatial data analytics programs. G EO S PARK consists of three main layers: Apache Spark Layer, Spatial RDD Layer and Spatial Query Processing Layer. Apache Spark Layer provides basic Apache Spark functionalities as regular RDD operations. Spatial RDD Layer consists of three novel Spatial Resilient Distributed Datasets (SRDDs) which extend regular Apache Spark RDD to support geometrical and spatial objects with data partitioning and indexing. Spatial Query Processing Layer executes spatial queries (e.g., Spatial Join) on SRDDs. The dynamic status of SRDDs and spatial operations are visualized by G EO S PARK monitoring map interface. We demonstrate G EO S PARK using three spatial analytics applications (spatial aggregation, autocorrelation and co-location) to show how users can easily define their spatial analytics tasks and efficiently process such tasks on large-scale spatial data at interactive performance.

manage and maintain. The underlying database system must be able to digest Petabytes of spatial data and effectively analyze it.  Challenge II: Fast Analytics. In spatial data analytics applications, users will not tolerate delays introduced by the underlying spatial database system. Instead, the user needs to see useful information quickly. Hence, the underlying spatial data processing system must figure out effective ways to execute spatial analytics in parallel.

I.

I NTRODUCTION

Spatial data includes but is not limited to: weather maps, geological maps, socioeconomic data, vegetation indices, and more. Moreover, novel technology allows hundreds of millions of users to use their mobile devices to access their healthcare information and bank accounts, interact with friends, buy stuff online, search interesting places to visit on-thego, ask for driving directions, and more. In consequence, everything we do on the mobile internet leaves breadcrumbs of spatial digital traces, e.g., geo-tagged tweets, venue checkins. Making sense of such spatial data will be beneficial for several applications that may transform science and society  For example: (1) Socio-Economic Analysis: that includes for example climate change analysis, study of deforestation, population migration, and variation in sea levels, (2) Urban Planning: assisting government in city/regional planning, road network design, and transportation / traffic engineering, and (3) Commerce and Advertisement: e.g., point-of-interest (POI) recommendation services. The aforementioned applications need a powerful data management platform to handle the large volume of spatial data such applications deal with. Challenges to building such platform are as follows:  Challenge I: System Scalability. The massive-scale of available spatial data hinders making sense of it using traditional spatial database management systems. Moreover, large-scale spatial data, besides its tremendous storage footprint, may be extremely difficult to

Existing spatial database systems extend relational database systems with new data types, functions, operators, and index structures to handle spatial operations based on the Open Geospatial Consortium. Even though such systems sort of provide full support for spatial data storage and access, they suffer from a scalability issue. Based upon a relational database system, such systems are not scalable enough to handle largescale analytics over big spatial data. Recent works (e.g., [1], [2]) extend the Hadoop ecosystem to perform spatial analytics at scale. The Hadoop-based approach indeed achieves high scalability. However, these systems though exhibit excellent performance in batch-processing jobs, they show poor performance handling applications that require fast data analysis. Apache Spark [3], on the other hand, is an in-memory cluster computing system. Spark provides a novel data abstraction called resilient distributed datasets (RDDs) [4] that are collections of objects partitioned across a cluster of machines. Each RDD is built using parallelized transformations (filter, join or groupBy) that could be traced back to recover the RDD data. In memory RDDs allow Spark to outperform existing models (MapReduce) by up to two orders of magnitude. Unfortunately, Spark does not provide native support for spatial data and spatial operations. Hence, users need to perform the tedious task of programming their own spatial data processing jobs on top of Spark. This paper demonstrates G EO S PARK 1 [5] an in-memory cluster computing system for processing large-scale spatial data. G EO S PARK extends Apache Spark to support spatial data types and operations. In other words, the system extends the resilient distributed datasets (RDDs) concept to support spatial data. This problem is quite challenging due to the fact that (1) spatial data may be quite complex, e.g., rivers' and cities'
1 GeoSpark

Github repository: https://github.com/Sarwat/GeoSpark

Spatial Query Processing Layer Spatial Range Spatial KNN Spatial Join

Spatial RDD (SRDD) Layer Indexed Spatial RDD Point RDD Rectangle RDD Polygon RDD

Geometrical Operations Library

Fig. 2: SRDD partitioning
...

Apache Spark Layer

Fig. 1: GeoSpark architecture

users to write spatial data analytics programs. The SRDD layer consists of three new RDDs: PointRDD, RectangleRDD and PolygonRDD. One useful Geometrical operations library is also provided for every spatial RDD. Spatial Objects Support. G EO S PARK supports various spatial data input format (e.g., Comma Separated Value, Tab Separated Value and Well-Known Text). Each type of spatial objects is stored in a SRDD, PointRDD, RectangleRDD or PolygonRDD. G EO S PARK provides a set of geometrical operations which is called Geometrical Operations Library. This library natively supports geometrical operations. For example, Overlap(): Finds all of the internal objects which are intersected with others in geometry; MinimumBoundingRectangle(): Finds the minimum bounding rectangles for each object in a Spatial RDD or return a large minimum bounding rectangle which contains all of the internal objects in a Spatial RDD; Union(): Returns the union polygon of all polygons in this RDD. SRDD Partitioning. G EO S PARK automatically partitions all loaded Spatial RDDs by creating one global grid file for data partitioning. The main idea for assigning each element in a Spatial RDD to the same 2-Dimensional spatial grid space is as follows: Firstly, split the spatial space into a number of non-equal grid cells which compose a global grid file. This global grid file has load balanced grids according to presampling techniques. Then traverse each element in the SRDD and assign this element to a grid cell if the element overlaps with this grid cell. If one element intersects with two or more grid cells, then duplicate this element and assign different grid IDs to the copies of this element. Figure 2 depicts tweets in the U.S. at a particular moment, tweets and states are assigned to respective grid cells. SRDD Indexing. Spatial indexes like Quad-Tree and RTree are provided in Spatial IndexRDDs which inherit from Spatial RDDs. Users are able to initialize a Spatial IndexRDD. Moreover, G EO S PARK adaptively decides whether a local spatial index should be created for a certain Spatial IndexRDD partition based on a tradeoff between the indexing overhead (memory and time) on one-hand and the query selectivity as well as the number of spatial objects on the other hand.

geometrical boundaries, (2) spatial (and geometric) operations (e.g., Overlap, MinimumBoundingRectangle, Union) cannot be easily and efficiently expressed using regular RDD transformations and actions. G EO S PARK extends RDDs to form Spatial RDDs (SRDDs) and efficiently partitions SRDD data elements across machines and introduces novel parallelized spatial (geometric operations that follows the Open Geosptial Consortium (OGC) [6] standard) transformations and actions (for SRDD) that provide a more intuitive interface for users to write spatial data analytics programs. Moreover, G EO S PARK extends the SRDD layer to execute spatial queries (e.g., Range query, KNN query, and Join query) on large-scale spatial datasets. The dynamic status of SRDDs and associated queries are visualized by G EO S PARK monitoring interface throughout each entire spatial analytics process. We demonstrate G EO S PARK using three applications: (1) Application 1 uses G EO S PARK to calculate geospatial autocorrelation in a spatial dataset, (2) Application 2 leverages the system to generate a heat map of the San-Francisco trees population, and (3) Application 3 executes a spatial co-location pattern mining with the help of G EO S PARK . II. G EO S PARK
ARCHITECTURE

As depicted in Figure 1, G EO S PARK consists of three main layers: (1) Apache Spark Layer: that consists of regular operations that are natively supported by Apache Spark. These native functions are responsible for loading / saving data from / to persistent storage (e.g., stored on local disk or Hadoop file system HDFS). (2) Spatial Resilient Distributed Dataset (SRDD) Layer (Section II-A). (3) Spatial Query Processing Layer (Section II-B). A. Spatial RDD (SRDD) Layer This layer extends Spark with spatial RDDs (SRDDs) that efficiently partition SRDD data elements across machines and introduces novel parallelized spatial transformations and actions (for SRDD) that provide a more intuitive interface for

B. Spatial Query Processing Layer This layer supports spatial queries (e.g., Range query and Join query) for large-scale spatial datasets. After geometrical objects are stored and processed in the Spatial RDD layer, user may invoke a spatial query provided in Spatial Query Processing Layer. G EO S PARK processes such query and returns the final results to the user. G EO S PARK execution model implements the algorithms proposed by [7] and [8]. To accelerate a spatial query, G EO S PARK leverages the grid partitioned Spatial RDDs, spatial indexing, the fast in-memory computation and DAG scheduler of Apache Spark to parallelize the query execution. Spatial Range Query. G EO S PARK executes the spatial range query algorithm following the execution model: Load target dataset, partition data, create a spatial index on each SRDD partition if necessary, broadcast the query window to each SRDD partition, check the spatial predicate in each partition, and remove spatial objects duplicates that existed due to the data partitioning phase. Spatial Join Query. G EO S PARK executes the parallel spatial join query following the execution model. GeoSpark first partitions the data from the two input SRDDs as well as creates local spatial indexes (if required) for the SRDD which is being queried. Then it joins the two datasets by their keys which are grid IDs. For the spatial objects (from the two SRDDs) that have the same grid ID, GeoSpark calculates their spatial relations. If two elements from two SRDDS are overlapped, they are kept in the final results. The algorithm continues to group the results for each rectangle. The grouped results are in the following format: Rectangle, Point, Point, ... Finally, the algorithm removes the duplicated points and returns the result to other operations or saves the final result to disk. Spatial KNN Query. To process a Spatial KNN query, G EO S PARK uses a heap based top-k algorithm[9], which contains two phases: selection and merge. It takes a partitioned SRDD, a point P and a number k as inputs. To calculate the k nearest objects around point P , in the selection phase, for each SRDD partition G EO S PARK calculates the distances between each object to the given point P , then maintains a local heap by adding or removing elements based on the distances. This heap contains the nearest k objects around the given point P . For IndexedSRDD, the system can utilize the local indexes to reduce the query time. After the selection phase, G EO S PARK merges results from each partition, keeps the nearest k elements that have the shortest distances to P and outputs the result. III. D EMONSTRATION
SCENARIOS

SRDD partition (if it is still alive) on the left pane, she obtains more detailed information from a nested menu such as the data size in this partition, physical machine IP address, CPU and memory utilization. Besides the description of SRDDs, the tool also provides the status of a running spatial program in a progress bar format. By browsing G EO S PARK Monitoring Tool, users can interactively monitor the run time of their entire spatial analytics program. A. Application 1: Spatial Autocorrelation Spatial autocorrelation studies whether neighbor spatial data points might have correlations in some non-spatial attributes. Moran's I and Geary's C are two common coefficients in spatial autocorrelation. Based on them, analysts can determine whether these objects influence each other. These efficients are defined by two specific formulas correspondingly. An important part of these formulas is to find the spatial adjacent matrix. In this matrix, each tuples stands for whether two objects, such as points, rectangles or polygons, are within a specified distance. An application programmer may leverage G EO S PARK SpatialJoinQuery() to calculate the spatial adjacent matrix. Assume one dataset is composed of millions of point objects. The process to find the global adjacent matrix in G EO S PARK is as as follows: (1) Call G EO S PARK PointRDD initialization method to store the dataset in memory. Data partitioning and indexing are also completed by G EO S PARK at this stage. (2) Call G EO S PARK SpatialJoinQuery() in PointRDD. The first parameter is the query point set itself and the second one is the specified distance. (3) Use a new instance of Spatial PairRDD to store the result of Step (2). Step (2) will return the whole point set which has a new column specify the neighbors of each tuple within the distance. The expected schema is like this: Point coordinates (longitude, latitude), neighbor 1 coordinates (longitude, latitude), neighbor 2 coordinates (longitude, latitude), ... (4) Call persistence method in G EO S PARK to persist the resulting PointRDD. B. Application 2: Spatial Aggregation Assume an environmental scientist  studying the relationship between air quality and trees  would like to explore the trees population in San Francisco. A query may leverage the SpatialRangeQuery() provided by G EO S PARK to just return all trees in San Francisco. Alternatively, a heat map (spatial aggregate) that shows the distribution of trees in San Francisco may be also helpful. This spatial aggregate query (i.e., heat map) needs to count all trees at every single region over the map. In the heat map case, in terms of spatial queries, the heat map is a spatial join in which the target set is the tree map in San Francisco and the query area set is a set of San Francisco regions. The region number depends on the display resolution, or granularity, in the heat map. One proper G EO S PARK program is as follows: (3) Use a Spatial PairRDD to store the result of Step (2) which is the count for each polygon. The Spatial PairRDD follows the schema like this: (Polygon, count) such that Polygon represents the boundaries of the spatial region. (4) Call persistence method in Spark to persist the result PolygonRDD.

We demonstrate G EO S PARK using three spatial applications which are described below. G EO S PARK provides a monitoring map interface for system users to visualize and monitor the spatial program dynamically. A screenshot of this tool is provided in Figure 3. The interface allows users to execute Scala code interactively through an integrated Scale shell. Meanwhile, a map on-top of the shell visualizes the SRDDs generated by Scala code. Throughout the entire spatial analytics process, all generated SRDDs are listed on the left side pane of the user interface. When the user clicks on any

Fig. 3: G EO S PARK Monitoring Tool

C. Application 3: Spatial Co-location Spatial co-location is defined as two or more species are often located in a neighborhood relationship. The determination of this co-location pattern may benefit many further scientific researches. Biologists may find symbiotic relationships, mobile carriers can provide proper plans based users' co-location, and advertising agencies are able to place directed advertisements at the center of co-located populations. For instance, one existing co-location pattern is that one kind of tigers always live within a certain distance from one kind of rabbits. Thus we may infer one possible fact that these tigers feed on these rabbits. Some co-efficients are applied to determine the co-location relationship. Ripley's K function [10] is the most common one in real life. It usually executes numerous times iteratively and finds the ideal distance. The calculation of K function also needs the adjacent matrix between two type of objects. As we mentioned in spatial autocorrelation analysis, seeking adjacent matrix may leverage G EO S PARK SpatialJoinQuery(). Programmer are able to follow the same procedure depicted in Spatial Autocorrelation. Furthermore, spatial co-location, different from the previous basic spatial applications, is able to maximize the in memory computation goodness of G EO S PARK . Under G EO S PARK framework, users only need to spend time on loading data, partitioning data, and constructing indexes in the first iteration and then G EO S PARK automatically caches these intermediate data in memory. In the next numerous iterations, users are

able to directly keep mining the co-location pattern using the cache in memory instead of loading and pre-processing data from scratch. R EFERENCES
[1] A. Aji, F. Wang, H. Vo, R. Lee, Q. Liu, X. Zhang, and J. H. Saltz, "Hadoop-GIS: A High Performance Spatial Data Warehousing System over MapReduce," Proceedings of the VLDB Endowment, PVLDB, vol. 6, no. 11, pp. 10091020, 2013. [2] A. Eldawy and M. F. Mokbel, "A demonstration of spatialhadoop: An efficient mapreduce framework for spatial data," Proceedings of the VLDB Endowment, PVLDB, vol. 6, no. 12, pp. 12301233, 2013. [3] "Spark," https://spark.apache.org. [4] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauly, M. J. Franklin, S. Shenker, and I. Stoica, "Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing," in Proceedings of the USENIX Symposium on Networked Systems Design and Implementation, NSDI, 2012, pp. 1528. [5] J. Yu, J. Wu, and M. Sarwat, "Geosaprk: A cluster computing framework for processing large scale spatial data," in Proceedings of ACM SIGSPATIAL GIS, 2015. [6] "Open Geospatial Consortium," http://www.opengeospatial.org/. [7] G. Luo, J. F. Naughton, and C. J. Ellmann, "A non-blocking parallel spatial join algorithm," in Data Engineering, 2002. Proceedings. 18th International Conference on. IEEE, 2002, pp. 697705. [8] X. Zhou, D. J. Abel, and D. Truffet, "Data partitioning for parallel spatial join processing," Geoinformatica, vol. 2, no. 2, pp. 175204, 1998. [9] N. Roussopoulos, S. Kelley, and F. Vincent, "Nearest neighbor queries," in ACM sigmod record, vol. 24, no. 2. ACM, 1995, pp. 7179. [10] B. D. Ripley, Spatial statistics. John Wiley & Sons, 2005, vol. 575.

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/283123965

Methodsforderivingandcalibratingprivacypreservingheatmapsfrommobilesports trackingapplicationdata
ArticleinJournalofTransportGeographySeptember2015
DOI:10.1016/j.jtrangeo.2015.09.001

CITATIONS

READS

8
4authors,including: JuhaOksanen FinnishGeospatialResearchInstitute/Natio...
32PUBLICATIONS333CITATIONS
SEEPROFILE

38

CeciliaBergman FinnishGeospatialResearchInstitute
3PUBLICATIONS10CITATIONS
SEEPROFILE

AllcontentfollowingthispagewasuploadedbyJuhaOksanenon07January2016.

Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Journal of Transport Geography 48 (2015) 135144

Contents lists available at ScienceDirect

Journal of Transport Geography
journal homepage: www.elsevier.com/locate/jtrg

Methods for deriving and calibrating privacy-preserving heat maps from mobile sports tracking application data
Juha Oksanen a,, Cecilia Bergman a, Jani Sainio b, Jan Westerholm b
a b

Department of Geoinformatics and Cartography, Finnish Geospatial Research Institute/National Land Survey of Finland, P.O. Box 84, FI-00521 Helsinki, Finland Faculty of Science and Engineering, bo Akademi University, Joukahainengatan 3-5, FI-20520 Turku, Finland

a r t i c l e

i n f o

a b s t r a c t
Utilization of movement data from mobile sports tracking applications is affected by its inherent biases and sensitivity, which need to be understood when developing value-added services for, e.g., application users and city planners. We have developed a method for generating a privacy-preserving heat map with user diversity (ppDIV), in which the density of trajectories, as well as the diversity of users, is taken into account, thus preventing the bias effects caused by participation inequality. The method is applied to public cycling workouts and compared with privacy-preserving kernel density estimation (ppKDE) focusing only on the density of the recorded trajectories and privacy-preserving user count calculation (ppUCC), which is similar to the quadrat-count of individual application users. An awareness of privacy was introduced to all methods as a data pre-processing step following the principle of k-Anonymity. Calibration results for our heat maps using bicycle counting data gathered by the city of Helsinki are good (R2 N 0.7) and raise high expectations for utilizing heat maps in a city planning context. This is further supported by the diurnal distribution of the workouts indicating that, in addition to sports-oriented cyclists, many utilitarian cyclists are tracking their commutes. However, sports tracking data can only enrich official in-situ counts with its high spatio-temporal resolution and coverage, not replace them.  2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).

Article history: Received 19 December 2014 Received in revised form 3 September 2015 Accepted 5 September 2015 Available online 20 September 2015 Keywords: Cycling Location-based services (LBSs) Urban planning Privacy Big data GIS

1. Introduction Mobile sports tracking applications have become popular among the public audience, and a large number of smartphone users are willing to collect and compare their workouts privately, as well as to share their data within social networks or even publicly, for all application and Internet users. Key factors in this development have been the maturity of sensor technology, such as an accelerometer, digital compass, gyroscope, and GPS (e.g., Lane et al., 2010), available in nearly all recent mid- and top-range smartphones; and well-documented application programming interfaces for third-party developers to create new applications for mobile platforms. The aim of our work is to develop methods to enrich workout data from a mobile sports tracking application to create privacy-preserving information about the most popular places to do sports. While our case study focuses on public cycling workouts collected using Sports Tracker (http://www.sports-tracker.com/), the developed methods can be used for any other sports recorded using any mobile sports tracking application. The approach chosen for this study relies on visual data mining, which utilizes human perception in data exploration, and combines human flexibility, creativity, and knowledge with a computer's storage capacity, computing power, and visualization capabilities
 Corresponding author. Tel.: +358 40 831 4092. E-mail address: juha.oksanen@nls.fi (J. Oksanen).

(Keim, 2002). When integrated into a location-based service (LBS), the result of our analysis replies to the end-user's question "Where have most cyclists continued to from here?" In addition, we investigate the relation of tracking data and in-situ bicycle counting information in order to compare the quality of the derived heat maps, as well as to calibrate the heat maps based on mobile sports tracking application data, for example, for city planning purposes. We use the term "workout" throughout the paper to denote all recorded trajectories, be they recreational/exercise or utilitarian by purpose. The idea of generating heat maps from mobile sports app data to communicate the popularity of sports is not new (e.g., Garmin, 2013; Lin, 2012; Strava, 2014), but less attention has been paid to the methods for making the calculation, and concerns about the appropriate understanding of the heat maps have been raised due to new application areas of heat maps, such as city planning (Maus, 2014a) and analysis of eye-tracking data (Bojko, 2009). When creating heat maps, an obvious surrogate for the popularity of sports is the density of workout trajectories, but other surrogates, such as the number of different people doing sports, can also be used. According to the limited information available on existing heat maps, the one provided by Strava uses the number of GPS points as a pixel value (Mach, 2014), whereas in the heat map offered by Nike+, the value at each pixel represents the number of users (Lin, 2012). As we will show in this paper, the two methods can locally result in very different patterns of bike riding.

http://dx.doi.org/10.1016/j.jtrangeo.2015.09.001 0966-6923/ 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).

136

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135144

The use of heat maps as a representation of the intensity of a phenomenon has its roots in spectrometry (e.g., Moon et al., 2009) and the generation of isarithm and dot density maps (Slocum et al., 2009), but in the context of cartography, a textbook definition of a heat map is still missing (e.g., Trame and Keler, 2011). Heat maps are a common visualization technique in many fields of research where large amounts of data are handled. For example, in humancomputer interaction, "attention heat maps" are a popular tool of visual analysis of eyetracking data (e.g., Blascheck et al., 2014; Bojko, 2009). Related to studies utilizing the increasing volumes of volunteered geographic information (VGI; Goodchild, 2007), heat maps have been used, for example, to reveal attractive/popular places in a region using the density-based spatial clustering of geotagged images (Kisilevich et al., 2010; Kurata, 2012) and videos (Mirkovi et al., 2010), or to visualize spatio-temporal patterns revealed by the distribution of tweets (e.g., Morstatter et al., 2013; Zeile et al., 2012). The coloring of a heat map is typically selected in such way that the interpretation of the intensity differences becomes intuitive. This is expected to happen when warm colors, in terms of color temperature, are used for high intensities of the represented phenomenon and cool colors for low intensities (e.g., Spakov and Miniotas, 2007). In addition to the public audience, interest in mobile tracking applications, and enriching, especially, cycling data collected with them has emerged among city planners (Albergotti, 2014; Charlton et al., 2011; Hood et al., 2011). One of the biggest challenges in a city-planning context regarding non-motorized traffic is the lack of documentation on the usage and the demand figures for, for example, cyclists and pedestrians (Lindsey et al., 2014; NBPD, 2014). Traditional approaches for monitoring cycling traffic have been the use of surveys for qualitative results and different types of manual and automatic in-situ counting for quantitative results (Griffin et al., 2014; NBPD, 2014; Rantala and Luukkonen, 2014). Mobile tracking of cyclists has been seen as an attractive, inexpensive, and dynamic alternative to traditional bicycle data collection (Hudson et al., 2012). An early approach to tracking was the development of dedicated platforms, such as CycleTracks, which was developed at San Francisco Municipal Transportation Agency and has since been used at a number of agencies and municipalities in the US (Charlton et al., 2011; Masoner, 2014). In the UK, another crowdsourcing-based application, Cycle Hackney, is expected to provide a cost-effective way to find out where, especially, utilitarian cycling is taking place (CycleStreets, 2014). Recently, in the city of Oulu, Finland, there has been a development project aiming to create a "smoothness navigator" for cyclists, based on 1000 recorded tracks of people participating in the pilot phase (Poikola, 2014). The problem in dedicated tracking platforms appears to be the limited group of people interested in using them voluntarily (SFMTA, 2013). To overcome this problem the potential of utilizing mobile sports tracking data has been recognized, reflecting the idea of utilizing humans as sensors (Goodchild, 2007) and big data analytics (e.g., Russom, 2011). For example, Oregon's Department of Transportation paid $20,000 to use data from the mobile sports tracking application Strava for a year, containing 400,000 individual bicycle trips, totaling 8 million bicycle kilometers traveled (Estes, 2014; Maus, 2014b). While mobile sports tracking data may not qualify as `big data' regarding its volume  except in the sense "bigger than previously" (Goodchild, 2013)  it shares many characteristics with other social media data, often classified as big data. Many of the characteristics follow from the fact that big data is typically not collected with any specific purpose in mind or not used for its original purpose (Kitchin, 2014). In statistics, random sampling is used to guarantee the representativeness of observations, but in big data analytics, the `sample' is not randomly chosen at all (Goodchild, 2013). Rather, the aim is to use all the data following the principle of exhaustivity in scope (Harford, 2014; Kitchin, 2014). However, considering that only a small and possibly behaviorally biased subset of cyclists use mobile applications to track their routes, the question is how well they represent the whole population of cyclists

(e.g., Maus, 2014a; Rantala and Luukkonen, 2014); i.e., as social media data in general, sports tracking data is affected by self-selection bias (Shearmur, 2015). In addition, mobile tracking applications have their differences with respect to appearance and function, such as the available range of sports (multi-sports or single activity type), and may therefore attract different people. As an example, the cycling and running app Strava has the reputation of being used by more competitive or "serious" cyclists (Zahradnik, 2014) and is also targeting people who identify themselves as "athletes" (Strava, 2014). On the other hand, for example, Sports Tracker (ST, 2014) "want[s] to help people train better, connect through sports, and live healthier, happier lives;" HeiaHeia! focuses on the business-to-business sector and work welfare (Kauppalehti, 2013); and Endomondo (2014) is, in its own words, aiming "to motivate people to get and stay active." It has been estimated that 90% of the cyclists who use Strava are male (Usborne, 2013; Vanderbilt, 2013) and in 2012, 75% of all Endomondo users were men (Endoangela, 2012). Furthermore, participation inequality is a known property of VGI and online communities, according to which 90% of community members are followers and do not contribute to the community, whereas 9% contribute from time to time, and 1% account for most contributions (Nielsen, 2006). Although sports tracking applications do not today represent shared projects where people would track and share their workouts to promote the common good, some typical motivations for contribution in VGI, such as social reward and enhanced personal reputation (Coleman et al., 2009), can be identified within their communities as well. These bias issues introduce a major challenge in using mobile sports app data in a city-planning context. According to Westin's tenet, privacy is an individual's right to have full control over information about themselves, and to decide when, how, and to what extent this information is shared with others (Agrawal et al., 2002). Guaranteeing privacy in LBSs is extremely important, due to the unique characteristics of moving object data (Fung et al., 2010; Montjoye et al., 2013; Verykios et al., 2008). Topics such as anonymization of the original dataset (e.g., Monreale et al., 2010; Pensa et al., 2008), or de-identifying a given LBS-request location (e.g., Bettini et al., 2005; Gedik and Liu, 2004; Gruteser and Liu, 2004), have gained a great deal of attention in trajectory studies but are beyond the scope of this paper. Instead, we approach privacy-preservation from the standpoint of visualization. The idea behind preserving privacy in visualizations is to generalize or otherwise obfuscate data in such a manner that the disclosed data is still useful in the particular case (Andrienko et al., 2008; Fung et al., 2010). Various methods of geographical masking, first introduced by Armstrong et al. (1999), have been developed with the aim of protecting the confidentiality of individual locations by adding stochastic or deterministic noise to the geographic coordinates of the original data points without substantially affecting analytical results or the visual characteristics of the original pattern (Kwan et al., 2004). Spatial aggregation of individual-level data for administrative areas or other areal units that have a population greater than a chosen cutoff value is a common procedure of preserving confidentiality, for example, in censuses where disclosure control has long been an integral part of the process (Armstrong et al., 1999; Kwan et al., 2004; Leitner and Curtis, 2006; Young et al., 2009). Because aggregation can hide important spatial patterns in the data, various alternative geo-masking techniques, such as random perturbation and affine transformation (translate, rotate, and scale), have been introduced to preserve the disaggregated, discrete nature of the original data (Armstrong et al., 1999; Kwan et al., 2004). Although they have been used mainly with georeferenced, sensitive health- and crime-related point data (e.g., Leitner and Curtis, 2006; Kounadi and Leitner, 2015), Krumm (2007) and Seidl et al. (2015) have applied them also to GPS trajectory data. In this study, where it was crucial to prevent re-identification of an individual user and trajectory while providing the heat map viewer accurate information about popular cycling paths in their actual locations on the road network, geo-masking techniques as such were, however, not

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135144

137

adequate. Instead, we followed the principle of k-Anonymity in anonymization of the final heat maps. K-Anonymity was originally developed for record data by Samarati and Sweeney (1998), but has since been extended to spatio-temporal movement data by, for example, Abul et al. (2008), and Terrovitis and Mamoulis (2008). Related to visualization techniques, the method has been previously applied to privacy preservation for parallel coordinates (Dasgupta and Kosara, 2011). K-Anonymity refers to the requirement according to which "each release of data must be such that every combination of values of quasiidentifiers can be indistinctly matched to at least k individuals" (Samarati and Sweeney, 1998). By quasi-identifiers, they mean a set of attributes whose publication needs to be somehow restricted. When bringing the k-Anonymity requirement into the heat map context, our goal was that all details interpretable from the final heat map must be such that the result has been borne out from at least a minimum pre-defined number of application users. This paper presents a novel method for deriving privacy-preserving heat maps from mobile sports application data which takes into account the density of trajectories, as well as the diversity of users, attempting to avoid the bias caused by having few very active sports application users. As a secondary objective, the study presents a method by which heat maps derived from a mobile sports application's cycling data can be calibrated using in-situ field-collected bicycle counting data. With this approach, cities may use the calibrated heat maps as source information for city planning purposes. The remainder of the paper is organized as follows. In Section 2, we describe the data and methods used for deriving privacy-preserving heat maps. In Section 3, we compare the heat maps and discuss the characteristics of different methods, and we also show the relation between our heat maps and real-world in-situ bicycle counting data. Finally, in Section 4, conclusions are drawn and some future directions are pointed out. 2. Materials and methods The data used for our study was obtained from Sports Tracking Technologies Ltd., the inventor of the first mobile sports tracking application, Sports Tracker, for mobile phones (ST, 2014). While the app turns a smart phone into a sports computer, it is also a complete social platform on which application users share their workouts, photos, and experiences of exercises, with their group of friends or even with everyone. The basic unit of recorded data is a workout, which contains information about the user, sport (28 pre-classified sports, such as cycling, walking, and running), and a 4D (x, y, z, and t) coordinate list recorded at approximately 1-second intervals from the start to the finish of the workout. For this study, the data contained only public workouts from the region of Helsinki, and each user identifier was changed into a pseudo-ID at Sports Tracking Technologies Ltd. before the data delivery. The total sample data contained 192,597 workouts, of which a subset of 36,757 workouts collected by 2424 users represented cycling inside our approximately 320 km2 study region. Temporally continuous data was recorded between April 17th 2010 and November 21st 2012, and it consisted of a total of 36,663,190 GPS points. About 82% of the data contained valid time values accepted for further analysis. In a pre-processing phase, the data was systematically thinned to 10-second time intervals, the time data was transformed from POSIX time to Coordinated Universal Time (UTC + 02:00), and coordinates were converted from WGS84 (EPSG:4326) to ETRS-TM35FIN (EPSG:3067) (Anon, 2012). In addition, each point was supplied with information about distance, time, and speed from the previous tracked point. Further filtering of gross errors was done based on thresholds set for distance (b 300 m) and speed (b 50 m/s) between consecutive GPS observations. One characteristic of the data was that the number of people tracking and publishing a large number of workouts was small, and most of the users had tracked less than 10 workouts (Fig. 1), which appears to follow the principle of participation inequality (Nielsen, 2006). In the

Fig. 1. Number of tracked and published cycling workouts per user.

study area and within the time range, 65% of the users had published 5 or fewer workouts, and 87% had published 20 or fewer workouts. While the most active user had published more than 600 workouts, only 3% of the users had published more than 100 workouts. On average, for the sample data, each user had published 15 workouts. Further inspection of the data revealed cyclicity at a number of temporal scales. At an annual level, the popularity of cycling increased sharply during the summer season and peaked in August (Fig. 2a and b). In addition, the popularity of tracking cycling increased steadily from 2010 to 2012. When focusing on the weekly pattern, tracking of cycling was at its highest level on Tuesdays and Wednesdays, while the minimum level was reached on Fridays (Fig. 2c). Peaks in the frequencies of tracked points between 78 AM and 45 PM (Fig. 2d) reveal that many people use Sports Tracker to track their daily commuting trips. The use of pseudoIDs is not an adequate means for preserving the privacy of the application users since location alone may also reveal sensitive personal data (e.g., Bettini et al., 2005; Samarati and Sweeney, 1998). Privacy filtering of the tracked points was done in a preprocessing phase containing: 1) the generation of trajectories from all points, 2) the generation of a user count raster (number of different users on a 10 m grid within a 15 m search radius) from the trajectories, 3) the extraction of user count values at all points, and 4) the generation of trajectories from the subset of points with a user count value higher than a pre-defined threshold (here 5 users). By this method, we were able to filter privacy-preserving trajectories, which contained only the features that were covered by an adequate number of different users. From the filtered cycling trajectories we generated heat maps by a custom ArcGIS tool using three methods, namely privacy-preserving user count calculation (ppUCC), privacy-preserving kernel density estimation (ppKDE), and privacy-preserving kernel density estimation modified with the user diversity index (ppDIV). ppUCC was the simplest of the applied methods, in which the study region was partitioned into sub-regions of equal area and each area got its value, so-called quadrat counts (Bailey and Gatrell, 1995), from the number of users passing the pixel or a larger calculation window (20 m in our case). Thus, using ppUCC, we created a 2D histogram of individual cyclists within the study area. Kernel density estimation (KDE) is a family of methods originally developed to obtain smooth estimates of uni- or multivariate densities from observations (Bailey and Gatrell, 1995), but recently KDE has also been widely used to derive heat maps from data representing moving objects (e.g., Krisp and Peters, 2011; Willems, 2011). The commonly used kernel function is described by Silverman (1986): ppKDEs 
n s-s  1 X i ; K nh i1 h

where h is the width of the calculation window (bandwidth), n is the number of sample points, and K is the kernel function used for smoothing the estimate. Here, a quartic approximation of a Gaussian kernel (Silverman, 1986) was used. Sample points within the radius h are

138

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135144

Fig. 2. Frequencies of tracked points a) daily, b) monthly, c) according to weekday, and d) according to the hour of the day.

represented by si. When KDE is used for line features, such as trajectories, the result of KDE can be thought of as the result of placing a smooth kernel surface on top of the lines. The bandwidth was chosen as 25 m, to generalize the positioning uncertainty of GPS devices, but

simultaneously to preserve the details of the street network along which the cycling has occurred. In ppDIV the result of ppKDE was scaled with the diversity of users at each quadrat. Diversity is a descriptive statistic of a population with a

Fig. 3. Locations of the 89 bicycle counting sites in the city of Helsinki in June 2013 (Hellman, 2013) used in the study. The dashed circle represents a distance zone of 2.5 km from the city center. The map contains data from the Topographic database by the National Land Survey of Finland, 7/2012.

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135144

139

class structure (Junge, 1994). Diversity indices are used when we are interested in quantifying how many different classes there are in our data, and simultaneously how evenly the entities are distributed among those classes. The index used in this study is Simpson's Diversity Index D (McDonald and Dimmick, 2003): Ds  1- X ps2 i

ppDIVs  ppKDEs  Ds where pi is the proportion of user i's trajectories among all trajectories

in the quadrat s. This resulted in an index that was close to 1 when the user distribution was locally uniform, and close to 0 when skewness of the user distribution was locally high. Finally, the heat maps were compared with each other, as well as with the official 2013 bicycle counting data gathered by the city of Helsinki (Hellman, 2013). The challenge in a quantitative comparison of densities represented as heat maps based on different calculation methods is that the information varies in terms of the shape and scale of the density zones. Therefore, point sampling was done by extracting values of ppUCC, ppKDE, and ppDIV at the centroids of the Topographic database's (NLS, 2014) road segments from the Helsinki region for

Fig. 4. Heat maps based on (a) privacy-preserving user count calculation (ppUCC), (b) privacy-preserving kernel density estimation (ppKDE), and (c) privacy-preserving kernel density estimation modified with the user diversity index (ppDIV). Major differences between the methods are found in the highlighted regions A and B. The map contains data from the Topographic database by the National Land Survey of Finland, 7/2012;  OpenStreetMap contributors.

140

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135144

regression analysis purposes. The extraction was done by a nearest neighbor assignment for ppUCC, and by a bilinear interpolation for ppKDE and ppDIV. Manual bicycle counting was done at 94 counting points in early June 2013, during one weekday between 7 AM7 PM, and this was transformed to 24 h counts with a supply from semantically near automatic counting stations (Hellman, 2014). The precise locations of the counting sites were undocumented, but road segments used for the counting were published, and we adjusted the location of the 89 unambiguous points to correspond to the local maxima of the heat maps (Fig. 3). 3. Results and discussion In general, the three methods for deriving heat maps produced similar looking results, but when focusing on the details, some major differences could be found. In the simplest heat map based on ppUCC (Fig. 4a), patterns of the popularity of cycling in the region became clearly visible, but the problem appeared to be a lack of knowledge about the density of the trajectories. For example, in ppUCC, a road segment with 100 workouts recorded by 10 users got the same heat map score as a segment with 10 workouts recorded by 10 users. In ppKDE (Fig. 4b), the density of the trajectories became visible and the previously mentioned limitation of ppUCC was surpassed, but a single very active user may result in significant bias and overestimation of the route section's popularity (Fig. 4a, highlights A and B). In ppDIV (Fig. 4c), we took into account the density of the workout trajectories and the diversity of users, which resulted in a heat map that closely corresponded to ppKDE but that did not suffer from the bias introduced by very active application users. To further compare the methods, regression analysis between ppUCC, ppKDE, and ppDIV was performed on the point sample, based on the centroids of all road segments in the study region. In accordance with our expectations the coefficients of determination for regression models were high (Fig. 5), R2 = 0.830.94 with the highest coefficient being between ppKDE and ppDIV (Fig. 5c), revealing the similarity of the methods in general. Most often, the low number of users was associated with a small number of individual tracks, and a large number of users also indicated a large number of tracks. However, the most interesting features of the regression models were found from a spatial analysis of the residuals of the models. By residual analysis we could find answers to questions such as why a large number of tracks were not always related to a large number of users (Fig. 5a), and where were the places where the impact of taking the diversity of users into account in the calculation of densities of trajectories was the biggest (Fig. 5c). Observations below the regression curve in Fig. 5a revealed road segments where the density of trajectories (ppKDE) was lower than might have been expected according to the number of individual users (ppUCC). When plotting those parts of the observations on the map (Fig. 6a, blue dots), we could see that many of these road segments are important through-roads and entry cycling roads to downtown Helsinki. These also appeared to be routes not

favored by the most active cyclists, by whom we mean cyclists actively tracking their workouts. In the opposite case (Fig. 6a, red dots), the density of trajectories (ppKDE) was higher than predicted based on the number of individual users (ppUCC). These were found from the routes with very active cyclists, either because of the route's popularity among the enthusiastic sports cyclists or because of a very active user using the Sports Tracking application to track daily commuting trips. While the first group is interesting in terms of finding the most popular routes for cycling, the second group clearly displays bias, and it should be possible to filter it out from the results. In a similar manner, analyzing residuals from the regression model between ppKDE and ppDIV revealed the road segments where the diversity of the users had the biggest impact on ppKDE. Again, observations below the regression curve in Fig. 5c revealed road segments where a low diversity of users decreased the density value in the heat map (Fig. 6b, blue dots). In other words, they were the road segments where either a single cyclist or very few active cyclists had caused the largest positive bias in ppKDE. In the opposite case (Fig. 6b, red dots), a high diversity of users had resulted in a relative increase in ppKDE. These road segments appeared to be mostly the same important through-roads and entry cycling roads to downtown Helsinki, highlighted in blue in Fig. 6a. When comparing our heat maps to real-world data, all methods for deriving heat maps performed almost equally well (Fig. 7). Any of the heat maps can be used in predicting 24 h bicycle counting data, keeping in mind the coefficients of determination, R2 = 0.490.50 (p b 0.001). In practice this means that while, in many places, heat maps and realworld counting data had a clear connection, there are places where predictions based on heat maps fail. This may result at least partly from a temporal mismatch of the datasets and fundamental changes occurring in the cycling infrastructure. From the ten largest absolute residuals in the regression model between ppDIV and 24 h counting data (Fig. 7c, red and blue highlighting), we see that in 80% of the residuals, real-world 24 h counting data was greater compared to the predicted value based on ppDIV, and for only 20% the opposite was true. When these ten largest residuals were plotted on a map (Fig. 8), we noticed that the maximum (point 26) was located on Baana, a popular cycle path opened on June 12th 2012. Our data covered the time period from April 17th 2010 to November 21st 2012, which means that only the last 6 months of our data contained cyclists using Baana. A similar fundamental change in cycling infrastructure had taken place near measurement point 13 where the new cycle and pedestrian bridge Aurora was inaugurated in late 2012 (HS, 2012). The other large positive residuals could at least partly be explained by the overall increase in cycling (Hellman, 2013), which has been most significant on main entry cycleways to the city (points 7, 16, 21, 28, and 30). Together with the essential cycle-way through downtown Helsinki (point 27), these might also be locations where the difference between everyday cyclists and cyclists using Sports Tracker to record their workouts is largest. At points 7, 16, 30, and 28, manual bicycle counting data has been collected for opposite lanes, and the data was generalized to a single point. Using this method

Fig. 5. Regression models between (a) ppUCC and ppKDE, (b) ppUCC and ppDIV, and (c) ppKDE and ppDIV.

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135144

141

Fig. 6. Spatial distribution of maximum absolute residuals from the regression models between (a) ppUCC and ppKDE, and (b) ppKDE and ppDIV. The lowest 5th percentile of the residuals is represented with blue dots and the highest 5th percentile with red dots. The map contains data from the Topographic database by the National Land Survey of Finland, 7/2012.

to derive heat maps may be insensitive to the traffic on different lanes and underestimates the popularity indicating problems induced by the size of the calculation window. On the other hand, at point 8, the

manual counting data has been collected only from one side of the road, and a generalization of the result into a single point resulted in a large positive bias in prediction. The other negative residual (point

Fig. 7. Regression models between (a) ppKDE, (b) ppUCC, and (c) ppDIV and 24 h bicycle counting data 2013 from the city of Helsinki. In panel (c), the ten largest positive (red, + sign) and negative (blue,  sign) residuals from the regression model are highlighted. The same labels are used in the map in Fig. 8.

142

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135144

Fig. 8. The ten largest positive (red) and negative (blue) residuals from the regression model between ppDIV and 24 h bicycle counting data (black) in the city of Helsinki (Fig. 7c). The background heat map is based on ppDIV. The map contains data from the Topographic database by the National Land Survey of Finland, 7/2012.

4) might be explained by the construction site of Koivusaari metro station, which affected routes in 2013. When we compared the location of all the above-mentioned positive residuals to the distance zones in Helsinki (Fig. 3), they were all within a 2.5 km radius from the city center. Finally, when the observations resulting in the ten largest residuals were removed from the data (Fig. 9a), the coefficient of determination for predicting the number of cyclists based on ppDIV rose to R2 = 0.76 (p b 0.001). When focusing on the ten largest residuals, and by removing the points where significant changes in cycling infrastructure have occurred (points 13, 26, and 4) and where uncertainty due to a mismatch in in-situ counting and the heat map was the largest (point 8), we also got a high coefficient of determination, R2 = 0.72 (p = 0.15) (Fig. 9b). Furthermore, if the points for Tlnranta (point 21) and Kaisaniemenranta (point 30) were removed, the coefficient of determination rose to R2 = 0.96 (p = 0.20). This clearly indicates that, in our case study, a calibration of heat maps with the absolute number of cyclists outside a 2.5 km radius can be done with moderate accuracy, and calibration of the city center heat map would be best done as a separate processing step. To summarize the methods (Table 1), it appears that the advantages of ppUCC are the simplicity of the calculation and the intuitive quantity of the result, the number of different users per quadrant. In addition, ppUCC automatically reduces the impact of very active cyclists and therefore, for example, diminishes the bias caused by active commuters. The disadvantage of ppUCC is that it ignores the density of trajectories

and therefore brings, for example, mass sports events with a large number of individual athletes into the heat maps. In ppKDE, the density of trajectories is calculated using the well-established kernel density estimation, but individual active cyclists may introduce significant bias into the heat map. In addition, the unit of the heat map is not intuitive, even though in a visual interpretation of the results, the unit of the quantity is noncritical. A novel method introduced in this paper is ppDIV, which combines the best properties of the previously mentioned methods. It takes into account the density of the workout trajectories, as well as the diversity of the users who have tracked the workouts. Therefore, an individual athlete has no significant impact on the resulting heat map, and the computational biases of ppUCC and ppKDE are eliminated. A disadvantage of all the methods is that the results depend on the subjective definition of the size of the calculation window. This decision should be based, on one hand, on the positioning uncertainty of the workout trajectories, and on the other hand, on the desired level of detail of the final results.

4. Conclusions In this paper, we have introduced a privacy-preserving diversity method (ppDIV) for deriving heat maps from mobile sports tracking application data, which takes into account the density of the trajectories and the diversity of the users. The method was applied to Sports Tracker's public cycling workouts and compared with privacypreserving kernel density estimation (ppKDE) and privacy-preserving user count calculation (ppUCC). In addition, we demonstrated a method for calibrating the cycling heat map with in-situ bicycle counting data.
Table 1 Performance summary of the properties of the heat map generation methods (+ = poor, ++ = moderate, +++ = good). ppUCC Simplicity of calculation Sensitivity to density of trajectories Sensitivity to the number of individual users Sensitivity to diversity of users Intuitiveness of the measurement unit of the result Sensitivity to the size of the calculation window Ability to filter out very active users Ability to filter out mass sports events +++ + +++ + +++ + +++ + ppKDE ++ +++ + + + + + + ppDIV + +++ ++ +++ + + +++ +

Fig. 9. Regression models between ppDIV and 24 h bicycle counting data 2013 from the city of Helsinki, when (a) observations in the city center (shown in Fig. 8) have been removed and (b) when focusing only on the observations in the city center. In panel (b), the two largest positive (red, + sign) and negative (blue,  sign) residuals from the regression model are highlighted.

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135144

143

The results show that ppUCC, ppKDE, and ppDIV reveal different aspects of the popularity of cycling, and they all are sensitive in different ways to bias issues in the mobile sports tracking data. The order of superiority between them depends on the requirements set for the final result, but for general purposes, ppDIV offers the most neutral view on the popularity of cycling, and hides the bias issues related to, for example, very active application users. The final added-value application that would inspire and support visual reasoning with the aim of choosing attractive routes for cycling could be further improved by allowing the end-user to filter the heat map based on subjective preferences on, for example, speed and time of day or year. A balance between location privacy and data accuracy is an important but thus far largely uninvestigated topic in the context of heat maps. In our study area, k-Anonymity-based privacy preservation of the heat maps could be seen as a valid technique to guarantee the non-disclosure of individual users without diminishing the value of the service to the end-users. Still, an open question remains whether the value k should be a function of the density of the population or the underlying road network to protect the confidentiality of sensitive locations even in areas where there are small variations in the routes. A similar issue has been discussed in many geographical masking studies that have suggested weighting the displacement distance by population density because in less densely populated areas the risk of re-identification is higher (e.g., Armstrong and Ruggles, 2005; Kwan et al., 2004; Murad et al., 2014; Seidl et al., 2015). When considering the usability of our heat maps from a cityplanning perspective, the chosen approach for big data analytics appears to be promising. Despite the recognized bias issues, due to a selected group of cyclists tracking their workouts, as well as participation inequality, calibration results for our heat maps are surprisingly good. When doing regression modeling between in-situ bicycle counting data and ppDIV heat map scores, by splitting the data based on a 2.5 km distance from the city center, the coefficients of determination rise to R2 N 0.7. Most likely, the coefficients of determination would be even higher when bicycle counting data and the heat map are derived from the same period of time. This clearly shows the potential of utilizing heat maps in a city-planning context by using the in-situ bicycle counting data to get the absolute scale and the heat map for getting a high level of detail and a large spatial coverage of cycling activities. The use of big data from mobile sports apps also offers a chance for significant savings in the investments made in light traffic counting. In the long term this could significantly help to improve cycling infrastructure development and planning. Still, it is worth noting that big data analytics does not replace the need for in-situ bicycle counting since uncalibrated heat maps suffer from the bias issues caused by a behaviorally biased subset of cyclists using mobile applications to track their routes, as well as participation inequality. Acknowledgments We thank Sports Tracking Technologies Ltd. for the possibility to use their public workout data in our research, and Mrs. Susanne Suvanto for assistance in data processing. The study is a part of the research project SUPRA (Revolution of Location-Based Services: Embedded data refinement in Service Processes from Massive Geospatial Datasets) funded by Tekes, the Finnish Funding Agency for Innovation (grants 40261/12 and 40262/12). In addition, Oksanen's funding from the Academy of Finland (grant 251987) is gratefully acknowledged. References
Abul, O., Bonchi, F., Nanni, M., 2008. Never walk alone: uncertainty for anonymity in moving objects databases. Proceedings of the 24th IEEE International Conference on Data Engineering (ICDE'08), Cancun 712 April 2008, pp. 376385. Agrawal, R., Kiernan, J., Srikant, R., Xu, Y., 2002. Hippocratic databases. Proceedings of the 28th International Conference on Very Large Databases (VLDB'02), pp. 143154.

Albergotti, R., 2014. Strava, popular with cyclists and runners, wants to sell its data to urban planners. Digits - Tech News and Analysis from the Wall Street J. (http:// blogs.wsj.com/digits/2014/05/07/strava-popular-with-cyclists-and-runnerswants-to-sell-its-data-to-urban-planners/, accessed 29th July 2014). Andrienko, G., Andrienko, N., Kopanakis, I., Ligtenberg, A., Wrobel, S., 2008. Visual analytics methods for movement data. In: Giannotti, F., Pedreschi, D. (Eds.), Mobility, Data Mining and Privacy -- Geographic Knowledge Discovery. Springer, Berlin, pp. 375410. Anon, 2012. ETRS89 -jrjestelmn liittyvt karttaprojektiot, tasokoordinaatistot ja karttalehtijako (map projections, projected coordinate system, and map sheet division related to ETSR89, in Finnish). Julkishallinnon suosituksia (JHS) 154 (http:// www.jhs-suositukset.fi/suomi/jhs154 (accessed 15th December 2014)). Armstrong, M.P., Ruggles, A.J., 2005. Geographic information technologies and personal privacy. Cartographica 40 (4), 6373. http://dx.doi.org/10.3138/RU65-81R3-0W758V21. Armstrong, M.P., Rushton, G., Zimmerman, D.L., 1999. Geographically masking health data to preserve confidentiality. Stat. Med. 18 (5), 497525. http://dx.doi.org/10.1002/ (SICI)1097-0258(19990315)18:5b497::AID-SIM45N3.0.CO;2-#. Bailey, T.C., Gatrell, A.C., 1995. Interactive Spatial Data Analysis. Pearson Education, Harlow, UK. Bettini, C., Wang, X.S., Jajodia, S., 2005. Protecting privacy against location-based personal identification. In: Jonker, W., Petkovc, M. (Eds.), Proceedings of the 2nd VDLB international conference on Secure Data Management (SDM'05)Lecture Notes in Computer Science 3674. Springer, Berlin, pp. 185199. http://dx.doi.org/10.1007/11552338_ 13. Blascheck, T., Kurzhals, K., Raschke, M., Burch, M., Weiskopf, D., Ertl, T., 2014. State-of-theart of visualization for eye tracking data. In: Borgo, R., Maciejewski, R., Viola, I. (Eds.), State of the Art Report, Eurographics Conference on Visualization (EuroVis'14) (http://www.visus.uni-stuttgart.de/fileadmin/vis/pdf_s_fuer_Publikationen/State-ofthe-Art_of_Visualization_for_Eye_Tracking_Data.pdf, accessed 15th December 2014). Bojko, A., 2009. Informative or misleading? Heatmaps deconstructed. In: Jacko, J.A. (Ed.), HumanComputer Interaction International 2009, Part I. Lecture Notes in Computer Science 5610, pp. 3039. Charlton, B., Hood, J., Sall, E., Schwartz, M., 2011. Bicycle route choice data collection using GPS-enabled smartphones. Transportation Research Board 90th Annual Meeting, Washington DC, 2327 Jan 2011 (10 pp.). Coleman, D.J., Geogiadou, Y., Labonte, J., 2009. Volunteered geographic information: the nature and motivation of produsers. Int. J. Spat. Data Infrastruct. Res 4 (http://ijsdir. jrc.ec.europa.eu/index.php/ijsdir/article/view/140/198, accessed 25th June 2015). CycleStreets, 2014. Cycle Hackney app created by CycleStreets. http://www.cyclestreets. net/blog/2014/07/06/cycle-hackney-app/ (accessed 26th November 2014). Dasgupta, A., Kosara, R., 2011. Adaptive privacy-preserving visualization using parallel coordinates. IEEE Trans. Vis. Comput. Graph. 17 (12), 22412248. Endoangela, 2012. Popular Endomondo sports tracker mobile app hits 10 million user milestone and 320 million miles logged. http://blog.endomondo.com/2012/06/26/ popular-endomondo-sports-tracker-mobile-app-hits-10-million-user-milestoneand-320-million-miles-logged/ (accessed 7th August 2014). Endomondo, 2014. What we do. https://www.endomondo.com/about (accessed 7th August 2014). Estes, A.C., 2014. Why a fitness-tracking app is selling its data to city planners. GIZMODO. http://gizmodo.com/why-a-fitness-tracking-app-is-selling-its-data-to-city-1572964149 (accessed 2nd September 2014). Fung, B.C.M., Wang, K., Chen, R., Yu, P.S., 2010. Privacy-preserving data publishing: a survey of recent developments. ACM Comput. Surv. 42 (4). http://dx.doi.org/10. 1145/1749603.1749605 (Article 14). Garmin, 2013. Garmin connect is heating up with heat maps. http://garmin.blogs.com/ my_weblog/2013/03/garmin-connect-is-heating-up.html#.U6FbGbFABa4 (accessed 16th December 2014). Gedik, B., Liu, L., 2004. A customizable k-anonymity model for protecting location privacy. Technical Report GIT-CERCS-04-15. Georgia Institute of Technology (12 pp., https:// smartech.gatech.edu/xmlui/bitstream/handle/1853/100/git-cercs-04-15.pdf, accessed 14th October 2014). Goodchild, M.F., 2007. Citizens as sensors: the world of volunteered geography. GeoJournal 69, 211221. http://dx.doi.org/10.1007/s10708-007-9111-y. Goodchild, M.F., 2013. The quality of big (geo)data. Dialogues Hum. Geogr. 3 (3), 280284. http://dx.doi.org/10.1177/2043820613513392. Griffin, G., Nordback, K., Gtschi, T., Stolz, E., Kothuri, S., 2014. Monitoring bicyclist and pedestrian travel and behavior -- current research and practice. Transportation Research Circular E-C183 (32 pp., http://onlinepubs.trb.org/onlinepubs/circulars/ ec183.pdf, accessed 26th November 2014). Gruteser, M., Liu, X., 2004. Protecting privacy in continuous location-tracking applications. IEEE Secur. Priv. 2 (2), 2834. Harford, T., 2014. Big data: are we making a big mistake? The Financial Times. http:// www.ft.com/cms/s/2/21a6e7d8-b479-11e3-a09a-00144feabdc0. html#axzz3GqbLWgPw (accessed 22nd October 2014). Hellman, T., 2013. Polkupyrlaskennat Helsingiss 2013 (bicycle counting in Helsinki 2013, in Finnish). Memorandum 28th October 2013. City of Helsinki. City Planning Department, Transportation and Traffic Planning Division (http://www.hel.fi/hel2/ ksv/Aineistot/Liikennesuunnittelu/Liikennetutkimus/pyoralaskennat_2013.pdf, accessed 2nd September 2014). Hellman, T., 2014. Personal Communication, June 13th 2014. Hood, J., Sall, E., Charlton, B., 2011. A GPS-based bicycle route choice model for San Francisco, California. Transp. Lett. 3, 6375. HS, 2012. Neljn miljoonan euron Auroransilta avautui vihdoin ulkoilijoille (four million euro Aurora's bridge finally open for citizens, in Finnish). Helsingin Sanomat, 24th November 2012. http://www.hs.fi/kaupunki/a1305621575898 (accessed 12th November 2014).

144

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135144 in Geographic Information Collection and Analysis (GeoPrivacy'14), ACM, November 47, 2014, Dallas/Fort Worth, TX, USA http://dx.doi.org/10.1145/2675682.2676399. NBPD, 2014. National bicycle and pedestrian documentation project. Alta Planning & Design. Institute of Transportation Engineers (ITE) Pedestrian and Bicycle Council (http://bikepeddocumentation.org/, accessed 12th November 2014). Nielsen, J., 2006. The 90-9-1 Rule for Participation Inequality in Social Media and Online Communities. Nielsen Normal Group. (http://www.nngroup.com/articles/ participation-inequality/, accessed 10th October 2014). NLS, 2014. The Topographic database. National Land Survey of Finland. http://www. maanmittauslaitos.fi/en/digituotteet/topographic-database (accessed 14th October 2014). Pensa, R.G., Monreale, A., Monreale, A., Pinelli, F., Pedreschi, D., 2008. Pattern-preserving k-anonymization of sequences and its application to mobility data mining. In: Bettini, C., Jajodia, S., Samarati, P., Wang, X.S. (Eds.), Proceedings of the 1st International Workshop on Privacy in Location-Based Applications (PiLBA '08), Malaga, Spain, October 9, 2008 (http://ceur-ws.org/Vol-397/paper4.pdf, accessed 16th December 2014). Poikola, A., 2014. Sujuvuusnavigaattorin pilotointi (piloting of the smoothness navigator). Open knowledge Finland. http://bit.ly/sujuvuusnavi_kalvot (accessed 12th November 2014). Rantala, T., Luukkonen, T., 2014. Bicycle and Pedestrian Traffic Monitoring -- Guide to Creating an Indicator Toolbox. Finnish Transport Agency, Planning Department, Helsinki (37 pages, 2 appendices. http://www2.liikennevirasto.fi/julkaisut/pdf8/lts_ 2014-15_kavelyn_pyorailyn_web.pdf, accessed 30th September 2014). Russom, P., 2011. Big data analytics. TDWI Best Practices Report, Fourth Quarter 2011 (35 pp., http://tdwi.org/research/2011/12/sas_best-practices-report-q4-big-dataanalytics.aspx?tc=page0, accessed 14th October 2014). Samarati, P., Sweeney, L., 1998. Protecting privacy when disclosing information: k-anonymity and its enforcement through generalization and suppression. Proceedings of the IEEE Symposium on Research in Security and Privacy (S&P). May 1998, Oakland, CA, pp. 384393 (http://dataprivacylab.org/dataprivacy/projects/ kanonymity/paper3.pdf, accessed 16th December 2014). Seidl, D., Jankowski, P., Tsou, M.-H., 2015. Spatial obfuscation of GPS travel data. CSU Geospatial Review vol. 13 (http://csugis.sfsu.edu/CSU_Geospatial_Review/2015.pdf (accessed 25th June 2015)). SFMTA, 2013. CycleTracks for iPhone and Android. San Francisco County Transport Authority. http://www.sfcta.org/modeling-and-travel-forecasting/cycletracksiphone-and-android (accessed 2nd September 2014). Shearmur, R., 2015. Dazzled by data: big data, the census and urban geography. Urban Geogr. http://dx.doi.org/10.1080/02723638.2015.1050922. Silverman, B.W., 1986. Density Estimation for Statistics and Data Analysis. Chapman & Hall, London. Slocum, T.A., McMaster, R.B., Kessler, F.C., Howard, H.H., 2009. Thematic Cartography and Geovisualisation. Prentice Hall, New Jersey, NJ. Spakov, O., Miniotas, D., 2007. Visualization of eye gaze data using heat maps. Electron. Electr. Eng. 2, 5558. ST, 2014. Introducing Sports Tracker. Sports Tracking Technologies Ltd. (http://www. sports-tracker.com/blog/about/, accessed 20th August 2014). Strava, 2014. About us. Strava Inc. (http://www.strava.com/about, accessed 7th August 2014). Terrovitis, M., Mamoulis, N., 2008. Privacy preservation in the publication of trajectories. Proceedings of the 9th International Conference on Mobile Data Management (MDM '08), 2730 April 2008, pp. 6572. http://dx.doi.org/10.1109/MDM.2008.29. Trame, J., Keler, C., 2011. Exploring the lineage of volunteered geographic information with heat maps. Abstracts of GeoViz 2011, Hamburg, Germany, March 1011, 2011. http://www.geomatik-hamburg.de/geoviz11/abstracts/28_TrameKessler_Abstract_ GeoViz2011.pdf (accessed 12th November 2014). Usborne, S., 2013. Can cycling app Strava change the way we ride? The Independent, 4th July 2013. http://www.independent.co.uk/life-style/gadgets-and-tech/features/cancycling-app-strava-change-the-way-we-ride-8685996.html (accessed 7th August 2014). Vanderbilt, T., 2013. How Strava is changing the way we ride. Outside Magazine, January 2013. http://www.outsideonline.com/fitness/biking/How-Strava-Is-Changing-theWay-We-Ride.html (accessed 7th August 2014). Verykios, V.S., Damiani, M.L., Gkoulalas-Divanis, A., 2008. Privacy and security in spatiotemporal data and trajectories. In: Giannotti, F., Pedreschi, D. (Eds.), Mobility, Data Mining and Privacy, pp. 213240. Willems, C.M.E., 2011. Visualization of vessel traffic (PhD Thesis) Eindhoven University of Technology. Young, C., Martin, D., Skinner, C., 2009. Geographically intelligent disclosure control for flexible aggregation of census data. Int. J. Geogr. Inf. Sci. 23 (4), 457482 (http://dx. doi.org/10.1080/13658810801949835). Zahradnik, F., 2014. Strava Cycling app with unique social and record-keeping features. About.com. http://gps.about.com/od/sportsandfitness/fr/Strava-Cycling-App-Review. htm (accessed 7th August 2014). Zeile, P., Memmel, M., Exner, J.-P., 2012. A new urban sensing and monitoring approach: tagging the city with the RADAR SENSING app. Proceedings REAL CORP 2012 Tagungsband 1416 May 2012, Schwechat (http://programm.corp.at/cdrom2012/ papers2012/CORP2012_104.pdf (26th September 2014)).

Hudson, J.G., Duthie, J.C., Rathod, Y.K., Larsen, K.A., Meyer, J.L., 2012. Using smartphones to collect bicycle travel data in Texas. Final Report of the UTCM Project #11-35-69 (http://utcm.tamu.edu/publications/final_reports/Hudson_11-35-69.pdf, accessed 26th November 2014). Junge, K., 1994. Diversity of ideas about diversity measurement. Scand. J. Psychol. 35 (1), 1626. http://dx.doi.org/10.1111/j.1467-9450.1994.tb00929.x. Kauppalehti, 2013. Suomalainen liikuntasovellus: 100 yrityst 12 maassa (Finnish sports application: 100 companies in 12 countries, in Finnish). Kauppalehti 17th March 2013. http://www.kauppalehti.fi/omayritys/suomalainen+liikuntasovellus+100+ yritysta+12+maassa/201303381093 (accessed 16th December 2014). Keim, D.A., 2002. Information visualization and visual data mining. IEEE Trans. Vis. Comput. Graph. 7 (1), 100107. Kisilevich, S., Mansmann, F., Bak, P., Keim, D., 2010. Where would you go on your next vacation? A framework for visual exploration of attractive places. Second International Conference on Advanced Geographic Information Systems, Applications, and Services (GEOPROCESSING), 2010, pp. 2126. http://dx.doi.org/10.1109/GEOProcessing. 2010.11. Kitchin, R., 2014. Big data, new epistemologies and paradigm shifts. Big Data Soc. 1 (1). http://dx.doi.org/10.1177/2053951714528481. Kounadi, O., Leitner, M., 2015. Defining a threshold value for maximum spatial information loss of masked geo-data. ISPRS Int. J. Geo-Inf. 4 (2), 572590. http://dx.doi.org/ 10.3390/ijgi4020572. Krisp, J.M., Peters, S., 2011. Directed kernel density estimation (DKDE) for time series visualization. Ann. GIS 17 (3), 155162. http://dx.doi.org/10.1080/19475683.2011. 602218. Krumm, J., 2007. Inference attacks on location tracks. In: LaMarca, A., Langheinrich, M., Truong, K.N. (Eds.), Proceedings of the 5th International Conference on Pervasive Computing (Pervasive'07)Lecture Notes in Computer Science 4480. Springer, Berlin, pp. 127143. http://dx.doi.org/10.1007/978-3-540-72037-9_8. Kurata, Y., 2012. Potential-of-interest maps for mobile tourist information services. In: Fuchs, M., Ricci, F., Cantoni, L. (Eds.), Information and Communication Technologies in Tourism 2012. Springer, Vienna, pp. 239248. http://dx.doi.org/10.1007/978-37091-1142-0_21. Kwan, M.P., Casas, I., Schmitz, B., 2004. Protection of geoprivacy and accuracy of spatial information: how effective are geographical masks? Cartographica 39 (2), 1528. http://dx.doi.org/10.3138/X204-4223-57MK-8273. Lane, N.D., Miluzzo, E., Lu, H., Peebles, D., Choudhury, T., Campbell, A.T., 2010. A Survey of mobile phone sensing. IEEE Commun. Mag. 140150 September. Leitner, M., Curtis, A., 2006. A first step towards a framework for presenting the location of confidential point data on maps--results of an empirical perceptual study. Int. J. Geogr. Inf. Sci. 20 (7), 813822. http://dx.doi.org/10.1080/13658810600711261. Lin, D., 2012. How is Nike+ Heat Map calculated? http://howtonike.blogspot.fi/2012/06/ how-is-nike-heat-map-calculated.html (accessed 16th December 2014) Lindsey, G., Nordback, K., Figliozzi, M.A., 2014. Institutionalizing bicycle and pedestrian monitoring programs in three states: progress and challenges. 93rd Annual Meeting of the Transportation Research Board, Washington, DC, January 1216. Mach, P., 2014. What do 220,000,000,000 GPS data points look like? http://engineering. strava.com/global-heatmap/ (accessed 20th August 2014) Masoner, R., 2014. Santa Cruz County: log your bike rides for transportation planning. http://www.cyclelicio.us/2014/santa-cruz-county-log-your-bike-rides-fortransportation-planning/ (accessed 16th December 2014). Maus, J., 2014a. A closer look at Strava's `heat map' for the Portland region. BikePortland.org (Blog Post) (http://bikeportland.org/2014/04/30/a-closer-look-atstravas-heat-map-for-the-portland-region-105280, accessed 29th July 2014). Maus, J., 2014b. ODOT embarks on "big data" project with purchase of Strava dataset. BikePortland.org (Blog Post) (http://bikeportland.org/2014/05/01/odot-embarkson-big-data-project-with-purchase-of-strava-dataset-105375, accessed 29th July 2014). McDonald, D.G., Dimmick, J., 2003. The conceptualization and measurement of diversity. Commun. Res. 30 (1), 6079. http://dx.doi.org/10.1177/0093650202239026. Mirkovi, M., Aulibrk, D., Milisavljevi, S., Crnojevi, V., 2010. Detecting attractive locations using publicly available user-generated video content -- central Serbia case study. Proceedings of 18th Telecommunications Forum (TELFOR'10), Serbia, Belgrade, November 2325, 2010, pp. 10891092. Monreale, A., Andrienko, G., Andrienko, N., Giannotti, F., Pedreschi, D., Rinzivillo, S., Wrobel, S., 2010. Movement data anonymity through generalization. Trans. Data Privacy 3, 91121 (http://www.tdp.cat/issues/tdp.a045a10.pdf, accessed 15th December 2014). Montjoye, Y.-A., Hidalgo, C.A., Verleysen, M., Blondel, V.D., 2013. Unique in the crowd: the privacy bounds of human mobility. Sci. Rep. 3. http://dx.doi.org/10.1038/srep01376. Moon, J.-Y., Jung, H.-J., Hee Moon, M., Chul Chung, B., Ho Choi, M., 2009. Heat-map visualization of gas chromatography-mass spectrometry based quantitative signatures on steroid metabolism. J. Am. Soc. Mass Spectrom. 20 (9), 16261637. http://dx.doi. org/10.1016/j.jasms.2009.04.020. Morstatter, F., Kumar, S., Liu, H., Maciejewski, R., 2013. Understanding Twitter data with TweetXplorer. Proceedings of the 2013 ACM SIG KDD International Conference on Knowledge Discovery and Data Mining (KDD'13), ACM, August 1114, 2013, Chicago, IL, USA, pp. 14821485 http://dx.doi.org/10.1145/2487575.2487703. Murad, A., Hilton, B., Horan, T., Tangenberg, J., 2014. Protecting patient geo-privacy via a triangular displacement geo-masking method. In: Kessler, C., McKenzie, G.D., Kulik, L. (Eds.), Proceedings of the 1st ACM SIGSPATIAL International Workshop on Privacy

View publication stats

Geoinformatica (2013) 17:417–448
DOI 10.1007/s10707-012-0164-9

Generic and efficient framework for search trees
on flash memory storage systems
Mohamed Sarwat · Mohamed F. Mokbel ·
Xun Zhou · Suman Nath

Received: 16 February 2012 / Revised: 27 June 2012 /
Accepted: 12 July 2012 / Published online: 30 August 2012
© Springer Science+Business Media, LLC 2012

Abstract Tree index structures are crucial components in data management systems.
Existing tree index structure are designed with the implicit assumption that the
underlying external memory storage is the conventional magnetic hard disk drives.
This assumption is going to be invalid soon, as flash memory storage is increasingly
adopted as the main storage media in mobile devices, digital cameras, embedded
sensors, and notebooks. Though it is direct and simple to port existing tree index
structures on the flash memory storage, that direct approach does not consider the
unique characteristics of flash memory, i.e., slow write operations, and erase-beforeupdate property, which would result in a sub optimal performance. In this paper, we
introduce FAST (i.e., Flash-Aware Search Trees) as a generic framework for flashaware tree index structures. FAST distinguishes itself from all previous attempts of
flash memory indexing in two aspects: (1) FAST is a generic framework that can
be applied to a wide class of data partitioning tree structures including R-tree and
its variants, and (2) FAST achieves both ef f iciency and durability of read and write
flash operations through memory flushing and crash recovery techniques. Extensive
experimental results, based on an actual implementation of FAST inside the GiST

The research of M. Sarwat and M. F. Mokbel is supported in part by the National Science
Foundation under Grants IIS-0811998, IIS-0811935, CNS-0708604, IIS-0952977,
by a Microsoft Research Gift, and by a seed grant from UMN DTC.
M. Sarwat ( ) · M. F. Mokbel · X. Zhou
Department of Computer Science and Engineering, University of Minnesota - Twin Cities,
200 SE Union Street, Minneapolis, MN 55455, USA
e-mail: sarwat@cs.umn.edu
M. F. Mokbel
e-mail: mokbel@cs.umn.edu
X. Zhou
e-mail: xun@cs.umn.edu
S. Nath
Microsoft Research, One Microsoft Way - Redmond, Redmond, WA 98052, USA
e-mail: sumann@microsoft.com

418

Geoinformatica (2013) 17:417–448

index structure in PostgreSQL, show that FAST achieves better performance than
its competitors.
Keywords Flash memory · Tree · Spatial · Index structure · Storage ·
Multi-dimensional · Data · System

1 Introduction
Data partitioning tree index structures are crucial components in spatial data
management systems, as they are mainly used for efficient spatial data retrieval,
hence boosting up query performance. The most common examples of such index
structures include B-tree [4], with its variants [10, 27], for one-dimensional indexing,
and R-tree [14], with its variants [5, 17, 32, 34], for multi-dimensional indexing. Data
partitioning tree index structures are designed with the implicit assumption that the
underlying external memory storage is the conventional magnetic hard disk drives,
and thus has to account for the mechanical disk movement and its seek and rotational
delay costs. This assumption is going to be invalid soon, as flash memory storage is
expected to soon prevail in the storage market replacing the magnetic hard disks
for many applications [11, 12, 31]. Flash memory storage is increasingly adopted as
the main storage media in mobile devices and as a storage alternative in laptops,
desktops, and enterprise class servers (e.g., in forms of SSDs) [3, 21, 23, 28, 33].
Recently, several data-intensive applications have started using custom flash cards
(e.g., ReMix [19]) with large capacity and access to underlying raw flash chips. Such
a popularity of flash is mainly due to its superior characteristics that include smaller
size, lighter weight, lower power consumption, shock resistance, lower noise, and
faster read performance [16, 18, 20, 22, 29].
Flash memory is block-oriented, i.e., pages are clustered into a set of blocks. Thus,
it has fundamentally different characteristics, compared to the conventional pageoriented magnetic disks, especially for the write operations. First, write operations in
flash are slower than read operations. Second, random writes are substantially slower
than sequential writes. In devices that allow direct access to flash chips (e.g., ReMix
[19]), a random write operation updates the contents of an already written part of the
block, which requires an expensive block erase operation,1 followed by a sequential
write operation on the erased block; an operation termed as erase-before-update [7,
20]. SSDs, which emulate a disk-like interface with a Flash Translation Layer (FTL),
also need to internally address flash’s erase-before-update property with logging and
garbage collection, and hence random writes, especially small random writes, are
significantly slower than sequential writes in almost all SSDs [7].
Though it is direct and simple to port existing tree index structures (e.g., R-tree
and B-tree) on FTL-equipped flash devices (e.g., SSDs), that direct approach does
not consider the unique characteristics of flash memory and therefore would result
in a sub-optimal performance due to the random writes encountered by these index
structures. To remedy this situation, several approaches have been proposed for

a typical flash memory, the cost of read, write, and erase operations are 25, 200 and 1,500 µs,
respectively [3].

1 In

Geoinformatica (2013) 17:417–448

419

flash-aware index structures that either focus on a specific index structure, and make
it a flash-aware, e.g., flash-aware B-tree [30, 36] and R-tree [35], or design brand new
index structures specific to the flash storage [2, 24, 25].
Unfortunately, previous works on flash-aware search trees suffer from two
major limitations. First, these trees are specialized—they are not flexible enough
to support new data types or new ways of partitioning and searching data. For
example, FlashDB [30], which is designed to use a B-Tree, does not support R-Tree
functionalities. RFTL [35] is designed to work with R-tree, and does not support
B-tree functionalities. Thus, if a system needs to support many applications with
diverse data partitioning and searching requirements, it needs to have multiple tree
data structures. The effort required to implement and maintain multiple such data
structures is high.
Second, existing flash-aware designs often show trade-offs between efficiency and
durability. Many designs sacrifice strict durability guarantee to achieve efficiency
[24, 25, 30, 35, 36]. They buffer updates in memory and flush them in batches to
amortize the cost of random writes. Such buffering poses the risk that in-memory
updates may be lost if the system crashes. On the other hand, several designs achieve
strict durability by writing (in a sequential log) all updates to flash [2]. However, this
increases the cost of search for many log entries that need to be read from flash
in order to access each tree node [30]. In summary, no existing flash-aware tree
structure achieves both strict durability and efficiency.
In this paper, we address the above two limitations by introducing FAST; a
framework for Flash-Aware Search Tree index structures. FAST distinguishes itself
from all previous flash-aware approaches in two main aspects: (1) Rather than
focusing on a specific index structure or building a new index structure, FAST is
a generic framework that can be applied to a wide variety of tree index structures,
including B-tree, R-tree along with their variants. Such an important property makes
FAST a very attractive solution to database industry as it is practical to port it inside
the database engine with minimal disturbance to the engine code. (2) FAST achieves
both efficiency and durability in the same design. For efficiency, FAST buffers all
the incoming updates in memory while employing an intelligent f lushing policy that
evicts selected updates from memory to minimize the cost of writing to the flash
storage. In the mean time, FAST guarantees durability by sequentially logging each
in-memory update and by employing an efficient crash recovery technique.
FAST mainly has four modules, update, search, f lushing, and recovery. The update
module is responsible on buffering incoming tree updates in an in-memory data
structure, while writing small entries sequentially in a designated flash-resident log
file. The search module retrieves requested data from the flash storage and updates
it with recent updates stored in memory, if any. The f lushing module is triggered
once the memory is full and is responsible on evicting flash blocks from memory to
the flash storage to give space for incoming updates. Finally, the recovery module
ensures the durability of in-memory updates in case of a system crash.
FAST is a generic system approach that neither changes the structure of tree
indexes it is applied to, nor changes the search, insert, delete, or update algorithms
of these indexes. FAST only changes the way these algorithms reads, or updates the
tree nodes in order to make the index structure flash-aware. We have implemented
FAST within the GiST framework [15] inside PostgrSQL. As GiST is a generalized
index structure, FAST can support any tree index structure that GiST is supporting,

420

Geoinformatica (2013) 17:417–448

including one-dimensional tree index structures (e.g., B-tree [4]) and including but
not restricted to R-tree [14], R*-tree [5], SS-tree [34], and SR-tree [17], as well as Btree and its variants. In summary, the contributions of this paper can be summarized
as follows:
–
–

–

We introduce FAST; a general framework that adapts existing tree index structures to consider and exploit the unique properties of the flash memory storage.
We show how to achieve efficiency and durability in the same design. For
efficiency, we introduce two f lushing policies that smartly select parts of the main
memory buffer to be flushed into the flash storage in a way that amortizes expensive random write operations. We also introduce a crash recovery technique that
ensures the durability of update transactions in case of system crash.
We give experimental evidence for generality, efficiency, and durability of FAST
framework when applied to different data partitioning tree index structures.

The rest of the paper is organized as follows: An overview of Flash Memory
storage is given in Section 2. Section 3 highlights related work to FAST. Section 4
gives an overview of FAST along with its data structure. The four modules of
FAST, namely, update, search, f lushing, and recovery are discussed in Sections 5–
8, respectively. Section 9 gives experimental results. Finally, Section 10 concludes
the paper.

2 Flash memory storage overview
Figure 1 gives an overview of a typical flash memory storage device. In flash memory,
data is stored in an array of flash blocks. Each block contains ≈64–128 pages,
where a page is the smallest unit of access. Flash memory supports three types of
operations: read, write, and erase. The Erase operation is the most expensive one
where it can only be done at the block level and results in setting of all bits within
a block to ones. Read is a low latency page level operation and can occur randomly
anywhere in the flash memory without incurring any additional cost. Write is also a
page level operation and can be performed only once a page has been previously
erased since it sets the required bits to zeros. In that sense, writing on a previously
erased block is a low latency operation, and termed as a sequential write, while
writing on an already written block will result in a block erase operation before the
actual write operation, and thus, would incur much higher cost. Current generation
flash memory-based storage devices have varying access latencies for each of these
operations. On average, compared to read operations, write operations are eight
times slower, while erase operations are 60 times slower [6]. The typical access
latencies for read, write, and erase operations in flash memory devices are 25, 200
and 1,500 µs, respectively [3].
The Flash Translation layer (FTL) [26] is a layer on top of NAND flash memory
that makes the flash memory device acts like a virtual disk. The FTL layer receives
read and write commands for logical pages addresses from the application layer and
converts them to the internal flash memory commands (i.e., read page, write page,
erase block) on physical pages/blocks addresses. To emulate disk like in-place update
operation for a logical page Plogical , the FTL writes data into a new physical page
Pphysical, maintains a mapping between logical pages and physical pages, and marks

Geoinformatica (2013) 17:417–448
Fig. 1 Flash memory storage.
Grey rectangles represent
pages that are contained in
blocks represented by dotted
rectangles

421
Logical Page Read

Logical Page Write

Flash Translation Layer (FTL)
Page Read

Block Erase

Page Write

NAND Flash Memory

the previous physical location of Pphysical as invalid for future garbage collection.
Even though FTL allows existing disk based applications to use flash memory
without any modiÞcations, it needs to internally deal with flash physical constraint
of erasing a block before updating a page in that block. Besides this asymmetric read
and write latency issue, a flash memory block can only be erased for a limited number
of times (e.g., 105 –106 ), after which it acts like a read-only device [3]. FTL employs
various wear-leveling techniques to even out the erase counts of different blocks in
the flash memory to increase its longevity [8]. However, still early wear-out of flash
memory is one of the big concerns in widely deploying flash memory storage devices
[31], and thus, it is of essence that flash memory avoids block erases as much as
possible. Recent studies show that current FTL schemes are very effective for the
workloads with sequential access write patterns. However, for the workloads with
random access patterns, these schemes show very poor performance [9]

3 Related work
Previous approaches for flash-aware index structures can be classified into two categories: (1) Making an existing specif ic index structure flash-aware, which includes
flash-aware B-tree (e.g., FlashDB [30] and BFTL [36]) and flash-aware R-tree (e.g.,
RFTL [35]). The main idea of these index structures is to save the B-tree (R-tree)
operations in a reservation buffer residing on main memory. When the reservation
buffer is full, its content is totally flushed to flash memory. For instance, BFTL and
RFTL are adding a buffering layer on top of the flash translation layer in order
to make B-trees work efficiently on flash devices. (2) Designing brand new onedimensional index structures specific to the flash storage, e.g., the LA-tree [2] and the
FD-tree [24, 25]. LA-tree is flash friendly index structure that is intended to replace
the B-tree. LA-tree stores the updates in cascaded buffers residing on flash memory
and, then empties these buffers dynamically based on the operations workload. FDtree is also a one-dimensional index structure that allows small random writes to

422

Geoinformatica (2013) 17:417–448

occur only in a small portion of the tree called the head tree which exists at the
top level of the tree. When the capacity of the head tree is exceeded, its entries are
merged in batches to subsequent tree levels.
In terms of the performance-durability trade-off, previous approaches either:
(a) achieve efficiency, yet sacrifice durability, by buffering updates in main memory
and flush them in batches to flash memory to amortize the cost of random writes
[24, 25, 30, 35, 36]. However, storing updates in memory without taking into account
system failures, which leads to durability issue, where in-memory updates may be
lost if the system crashes, or (b) achieve durability, yet sacrifice efficiency, by writing
all the recent updates in a sequential log file [2], hence retrieving the updates from
the log file in case of a system crash. However, doing this increases the cost of search
for many log entries that need to be read from flash in order to access each tree node
with search and update operations [30].
FAST distinguishes itself from all previous techniques in three main aspects:
(1) FAST is a general framework for data-partitioning tree index structures built
inside GiST [15]. As GiST is a generalized index structure that can instantiate a wide
set of data-partitioning trees that include B-tree [4], R-tree [14], R*-tree [5], SS-tree
[34], and SR-tree [17]), FAST can support any tree that GiST is supporting. (2) FAST
ensures both the ef f iciency and durability of system transactions where updates are
buffered in memory, yet, an efficient crash recovery technique is triggered in case of
a system crash to ensure the durability. (3) FAST is not a brand new index structure,
hence does not need to replace existing tree indexes. However, it complements the
existing tree index structures in database management systems to make them work
efficiently on flash storage devices, with much less implementation cost.

4 Fast system overview
Figure 2 gives an overview of FAST. The original tree is stored on persistent flash
memory storage while recent updates are stored in an in-memory buffer. Both parts
need to be combined together to get the most recent version of the tree structure.
FAST has four main modules, depicted in bold rectangles, namely, update, search,
flushing, and crash recovery. FAST is optimized for both SSDs and raw flash devices.
SSDs are the dominant flash device for large database applications. On the other
hand, raw flash chips, which are dominant in embedded systems and custom flash
cards (e.g., ReMix [19]), are getting popular for data-intensive applications.
4.1 FAST modules
In this section, we explain FAST system architecture, along with its four main
modules; (1) Update, (2) Search, (3) Flushing, and (4) Crash recovery. The actions of
these four modules are triggered through three main events, namely, search queries,
data updates, and system restart.
Update module Similar to some of the previous research for indexing in flash
memory, FAST buffers its recent updates in memory, and flushes them later, in
bulk, to the persistent flash storage. However, FAST update module distinguishes
itself from previous research in two main aspects: (1) FAST does not store the

Geoinformatica (2013) 17:417–448

423

Fig. 2 FAST system architecture

update operations in memory, instead, it stores the results of the update operations in
memory, and (2) FAST ensures the durability of update operations by writing small
log entries to the persistent storage. These log entries are written sequentially to the
flash storage, i.e., very small overhead. Details of the update module will be discussed
in Section 5.
Search module The search module in FAST answers point and range queries that
can be imposed to the underlying tree structure. The main challenge in the search
module is that the actual tree structure is split between the flash storage and the
memory. Thus, the main responsibility of the search module is to construct the recent
image of the tree by integrating the stored tree in flash with the tree updates in
memory that did not make it to the flash storage yet. Details of the search module
will be discussed in Section 6.
Flushing module As the memory resource is limited, it will be filled up with the
recent tree updates. In this case, FAST triggers its flushing module that employs a
f lushing policy to select some of the in-memory updates and write them, in bulk,
into the flash storage. Previous research in flash indexing flush their in-memory
updates or log file entries by writing all the memory or log updates once to the flash
storage. In contrast, the flushing module in FAST distinguishes itself from previous
techniques in two main aspects: (1) FAST employs f lushing policies that smartly
selects some of the updates from memory to be flushed to the flash storage in a way
that amortizes the expensive cost of the block erase operation over a large set of
random write operations, and (2) FAST logs the flushing process using a single log
entry written sequentially on the flash storage. Details of the flushing module will be
discussed in Section 7.
Crash recovery module FAST employs a crash recovery module to ensure the
durability of update operations. This is a crucial module in FAST, as only because
of this module, we are able to have our updates in memory, and not to worry about
any data losses. This is in contrast to previous research in flash indexing that may
encounter data losses in case of system crash, e.g., [24, 25, 35, 36]. The crash recovery

424

Geoinformatica (2013) 17:417–448

module is mainly responsible on two operations: (1) Once the system restarts after
crash, the crash recovery module utilizes the log file entries, written by both the
update and flushing modules, to reconstruct the state of the flash storage and inmemory updates just before the crash took place, and (2) maintaining the size of
the log file within the allowed limit. As the log space is limited, FAST needs to
periodically compact the log entries. Details of this module will be discussed in
Section 8.
4.2 FAST design goals
FAST avoids the tradeoff of durability and efficiency by using a combination of
buffering and logging. Unlike existing efficient-but-not-durable designs [24, 25, 30,
35, 36], FAST uses write-ahead-logging and crash recovery to ensure strict system
durability. FAST makes tree updates efficient by buffering write operations in main
memory and by employing an intelligent flushing policy that optimizes I/O costs for
both SSDs and raw flash devices. Unlike existing durable-but-inefficient solutions
[2], FAST does not require reading in-flash log entries for each search/update
operation, which makes reading FAST trees efficient.
4.3 FAST data structure
Other than the underlying index tree structure stored in the flash memory storage,
FAST maintains two main data structures, namely, the Tree Modif ications Table,
and Log File, described below.
Tree modif ications table This is an in-memory hash table (depicted in Fig. 3)
that keeps track of recent tree updates that did not make it to the flash storage
yet. Assuming no hashing collisions, each entry in the hash table represents the
modification applied to a unique node identifier, and has the form (status, list)
where status is either NEW, DEL, or MOD to indicate if this node is newly created,
deleted, or just modified, respectively, while list is a pointer to a new node, null,
or a list of node modifications based on whether the status is NEW, DEL, or
MOD, respectively. For MOD case, each modification in the list is presented by the
quadruple (TimeStamp, type, index, value) where TimeStamp represents the time at

Fig. 3 Tree modifications table

Geoinformatica (2013) 17:417–448

425

which the update happened, type is either K, P F , or P M , to indicate if the modified
entry is the key, a pointer to a flash node, or a pointer to an in-memory node,
respectively, while index and value determines the index and the new value for the
modified node entry, respectively. In Fig. 3, there are two modifications in nodes A
and D, one modification in nodes B and F, while node G is newly created and node
H is deleted.
Log f ile This is a set of flash memory blocks, reserved for recovery purposes. A log
file includes short logs, written sequentially, about insert, delete, update, and flushing
operations. Each log entry includes the triple (operation, node_list, modif ication)
where operation indicates the type of this log entry as either insert, delete, update, or
flush, node_list includes the list of affected nodes by this operation in case of a flush
operation, or the only affected node, otherwise, modif ication is similar to the triple
(type, index, value), used in the tree modif ications table. All log entries are written
sequentially to the flash storage, which has a much lower cost than random writes
that call for the erase operation.
4.4 Running example
Throughout the rest of this paper, we will use Fig. 4 as a running example where six
objects O1 to O6 , depicted by small black circles, are indexed by an R-tree. Then, two
objects O7 and O8 , depicted by small white circles, are to be inserted in the same Rtree. Figure 4a depicts the eight objects in the two-dimensional space domain, while
Fig. 4b gives the flash-resident R-tree with only the six objects that made it to the

O1

10

O8

O2

8
O5

O3

6
O7

O4

4

O6

2

0

2

4

6

8

10

12

14

(a) 2D Space
A

1, K, 2, (12,4,14,2)

B

C

D
O1

O2

E

F

O3

O4

G
O5

(b) R-tree Index

C

Mod

G

Mod

B

Mod

D

Mod

O6

2, K, 2, O7

3, K, 2, (5,10,8,7)

4, K, 2, O8

(c) Tree Modifications Table

Fig. 4 Illustrating example for search and update operations in FAST

426

Geoinformatica (2013) 17:417–448

Table 1 Cost analysis parameters
Parameter

Definition

T
RM
WM
RF

The underlying tree index structure to which FAST has been applied
The average time to read a node update entry from the tree modifications table
The average time to write a node update entry to the tree modifications table
The average time to read a tree node from the underlying tree T residing
on flash memory
The average time to write a tree node to the underlying tree T residing
on flash memory
The average time to erase a whole block on the flash memory device

WF
EF

flash memory. Finally, Fig. 4c gives the in-memory buffer (tree modif ications table)
upon the insertion of O7 and O8 in the tree.
4.5 Operations cost parameters
For each FAST module, we analyze the cost model of its main operations, including
search, update, flushing, crash recovery and log compaction. To this end, we define
the parameters given in Table 1.

5 Tree updates in FAST
This section discusses the update operations in FAST, which include inserting a new
entry and deleting/updating an existing entry. An update operation to any tree in
FAST may result in creating new tree nodes as in the case of splitting operations (i.e.,
when inserting an element in the tree leads to node overflow), deleting existing tree
nodes as in the case of merging operations (i.e., when deleting an element from the
tree leads to node underflow), or just modifying existing node keys and/or pointers.
Main idea For any update operation (i.e., insert, delete, update) that needs to
be applied to the index tree, FAST does not change the underlying insert, delete,
or update algorithm for the tree structure it represents. Instead, FAST runs the
underlying update algorithm for the tree it represents, with the only exception
of writing any changes caused by the update operation in memory instead of the
external storage, to be flushed later to the flash storage, and logging the result of
the update operation. A main distinguishing characteristic of FAST is that what
is buffered in memory, and also written in the log file, is the result of the update
operation, not a log of this operation.
Algorithm Algorithm 1 gives the pseudo code of inserting an object Obj in FAST.
The algorithms for deleting and updating objects are similar in spirit to the insertion
algorithm, and thus are omitted from the paper. The algorithm mainly has two steps:
(1) Executing the insertion in memory (Line 2 in Algorithm 1). This is basically
done by calling the insertion procedure of the underlying tree, e.g., R-tree insertion,
with two main differences. First, the insertion operation calls the search operation,
discussed later in Section 6, to find where we need to insert our data based on the
most recent version of the tree, constructed from main memory recent updates and

Geoinformatica (2013) 17:417–448

427

Algorithm 1 Insert an Object in the Tree
1: Function Insert(Obj)
/* STEP 1: Executing the Insertion in Memory only */
2: L ← List of modified nodes from the in-memory execution of inserting Obj in
the underlying tree
/* STEP 2: Buf fering and Logging the Updates */
3: for each Node N in L do
4:
HashEntry ← N entry in the Tree Modif ications Table
5:
if HashEntry is not NULL then
6:
Add the triple (MOD, N , updates in N ) to the log file
7:
if the status of HashEntry is MOD then
8:
Add the changes in N to the list of changes of HashEntry
9:
else
10:
Apply the changes in N to the new node of HashEntry
11:
end if
12:
else
13:
HashEntry ← Create a new entry for N in the Tree Modif ications Table
14:
if N is a newly created node then
15:
Add the triple (NEW, N , updates in N ) to the log file
16:
Set HashEntry status to NEW, and its pointer to N
17:
else
18:
Add the triple (MOD, N , updates in N ) to the log file
19:
Set HashEntry status to MOD, and its pointer to the list of changes that
took place in N
20:
end if
21:
end if
22: end for

the in-flash tree index structure. Second, the modified or newly created nodes that
result back from the insertion operation are not written back to the flash storage,
instead, they will be returned to the algorithm in a list L. Notice that the insertion
procedure may result in creating new nodes if it encounters a split operation.
(2) Buf fering and logging the tree updates (Lines 3–22 in Algorithm 1). For each
modified node N in the list L, we check if there is an entry for N in our in-memory
buffer, tree modif ications table. If this is the case, we first add a corresponding log
entry that records the changes that took place in N . Then, we either add the changes
in N to the list of changes in its entry in the tree modif ications table if this entry status
is MOD, or update N entry in the tree modif ications table, if the entry status is NEW.
On the other hand, if there is no entry for N in the tree modif ications table, we create
such entry, add it to the log file, and fill it according to whether N is a newly created
node or a modified one.
Example In our running example of Fig. 4, inserting O7 results in modifying two
nodes, G and C. Node G needs to have an extra key to hold O7 while node C needs
to modify its minimum bounding rectangle that points to G to accommodate its size
change. The changes in both nodes are stored in the tree modif ications table depicted

428
Fig. 5 FAST logging and
recovery example

Geoinformatica (2013) 17:417–448

Log#

Operation

Node

Modification

1

MOD

C

1, K, 2, (12,4,14,2)

2

MOD

G

2, K, 2, O7

3

MOD

B

3, K, 2, (5,10,8,7)

4

MOD

D

4, K,2, O8

5

FLUSH

B, C, D

*

(a) FAST Log File
Log#
1

Operation

Node

Modification

MOD

G

2, K, 2, O7

(a) FAST Log File after Crash Recovery

in Fig. 4c. The log entries for this operation are depicted in the first two entries of the
log file of Fig. 5a. Similarly, inserting O8 results in modifying nodes, D and B
Cost analysis For a given update operation U applied to a tree index structure T,
let yi,U ∈ {0, 1} represent whether or not node i of T has been modified by U. Let N
be the total number of nodes in T at the time U is applied, then the total cost CU of
update operation U applied on T is as follows:
CU = C Q +

N


yi,U ∗ [W F + W M + L]

(1)

i=0

The update operation (e.g., insert, delete, modify) requires first a search query Q
for a proper leaf node in T. This also takes the same search time C Q as illustrated
above. For each updated node i due to applying U, yi,U = 1, and for each of these
updates we write a sequential log entry to the log file
each takes W F time. Hence,
that
N
the total time to write all log entries is equal to i=0
yi,U ∗ W F . For each updated
node i, we also perform a lookup on the tree modifications table to get the entry for
node i, which is performed in constant time L. In addition, the total time to write
the modifications to all nodes (for which yi,U = 1) in the tree modifications table is
N
i=0 yi,U ∗ W M . All of the above sums up to give the update cost given in Eq. 1
6 Searching in FAST
Given a query Q, the search operation returns those objects indexed by FAST and
satisfy Q. The search query Q could be a point query that searches for objects with
a specific (point) value, or a range query that searches for objects within a specific
range. An important promise of FAST is that it does not change the main search
algorithm for any tree it represents. Instead, FAST complements the underlying
searching algorithm to consider the latest tree updates stored in memory.
Main idea As it is the case for any index tree, the search algorithm starts by fetching
the root node from the secondary storage, unless it is already buffered in memory.
Then, based on the entries in the root, we find out which tree pointer to follow to

Geoinformatica (2013) 17:417–448

429

fetch another node from the next level. The algorithm goes on recursively by fetching
nodes from the secondary storage and traversing the tree structure till we either find
a node that includes the objects we are searching for or conclude that there are no
objects that satisfy the search query. The challenging part here is that the retrieved
nodes from the flash storage do not include the recent in-memory stored updates.
FAST complements this search algorithm to apply the recent tree updates to each
retrieved node from the flash storage. In particular, for each visited node, FAST
constructs the latest version of the node by merging the retrieved version from the
flash storage with the recent in-memory updates for that node.
Algorithm Algorithm 2 gives the pseudo code of the search operation in FAST.
The algorithm takes two input parameters, the query Q, which might be a point or
range query, and a pointer to the root node R of the tree we want to search in. The
output of the algorithm is the list of objects that satisfy the input query Q. Starting
from the root node and for each visited node R in the tree, the algorithm mainly
goes through two main steps: (1) Constructing the most recent version of R (Line 2 in
Algorithm 2). This is mainly to integrate the latest flash-residant version of R with
its in-memory stored updates. Algorithm 3 gives the detailed pseudo code for this

Algorithm 2 Searching for an Object indexed by the Tree
1: Function Search(Query Q, Tree Node R)
/* STEP 1: Constructing the most recent version of R */
2: N ← RetrieveNode(R)
/* STEP 2: Recursive search calls */
3: if N is non-leaf node then
4:
Check each entry E in N . If E satisfies the query Q, invoke Search(Q,
E.NodePointer) for the subtree below E
5: else
6:
Check each entry E in N . If E satisfies the search query Q, return the object
to which E is pointing
7: end if

Algorithm 3 Retrieving a tree node
1: Function RetrieveNode(Tree Node R)
2: FlashNode ← Retrieve node R from the flash-resident index tree
3: HashEntry ← R’s entry in the Tree Modif ications Table
4: if HashEntry is NULL then
5:
return FlashNode
6: end if
7: if the status of HashEntry is MOD then
8:
FlashNode ← FlashNode ∪ All the updates in HashEntry list
9:
return FlashNode
10: end if
/* We are trying to retrieve either a new or a deleted node */
11: return the node that HashEntry is pointing to

430

Geoinformatica (2013) 17:417–448

step, where initially, we read R from the flash storage. Then, we check if there is
an entry for R in the tree modif ications table. If this is not the case, then we know
that the version we have read from the flash storage is up-to-date, and we just return
it back as the most recent version. On the other hand, if R has an entry in the tree
modif ications table, we either apply the changes stored in this entry to R in case the
entry status is MOD, or just return the node that this entry is pointing to instead of R.
This return value could be null in case the entry status is DEL. (2) Recursive search
calls (Lines 3–7 in Algorithm 2). This step is typical in any tree search algorithm, and
it is basically inherited from the underlying tree that FAST is representing. The idea
is to check if R is a leaf node or not. If R is a non-leaf node, we will check each entry
E in the node. If E satisfies the search query Q, we recursively search in the subtree
below E. On the other hand, if R is a leaf node, we will also check each entry E in
the node, yet if E satisfies the search query Q, we will return the object to which E is
pointing to as an answer to the query.
Example Given the range query Q in Fig. 4a, FAST search algorithm will first
fetch the root node A stored in flash memory. As there is no entry for A in the tree
modif ications table (Fig. 4c), then the version of A stored in flash memory is the most
recent one. Then, node C is the next node to be fetched from flash memory by the
searching algorithm. As the tree modif ications table has an entry for C with status
MOD, the modifications listed in the tree modif ications table for C will be applied
to the version of C read from the flash storage. Similarly, the search algorithm will
construct the leaf nodes F and G by first fetching them from flash memory, and then
reading their recent updates from the tree modif ications table. Finally, the result of
this query is {O4 , O5 , O6 , O7 }.
Cost analysis For a given search query Q applied to a tree index structure T, let
xi,Q ∈ {0, 1} represent whether node i of T is visited or not when issuing query Q.
Let Mi,Q be the number of modifications applied to node i and buffered in the tree
modifications table at the time Q is issued. Let N be the total number of nodes in T
at the time Q is issued, then the total search cost C Q on T is as follows:
CQ =

N


xi,Q ∗ [R F + (Mi,Q × R M ) + L]

(2)

i=0

Assuming a range query, the search operation returns a number of objects within
the query range. In FAST, when reading a node i from the flash-resident R-tree, we
also need to accommodate all the corresponding modifications on i that have been
recorded in the tree modifications table. Then, the total cost of reading a node i
would thus be (R F + Tm ) where Tm is the in-memory processing time for each node.
For the in-memory processing part, it first takes constant time L to locate the node
in the tree modification table , and then takes a linear scan of the list to apply all
the modifications. Given that the number of modifications associated with each node
is Mi,Q , then Tm = (Mi,Q × R M ) + L, where Mi,Q is upper bounded by the memory
size.

Geoinformatica (2013) 17:417–448

431

7 Memory flushing in FAST
As discussed in Section 5, the effect of all incoming updates in FAST has to be
buffered in memory. As memory is a scarce resource, it will eventually be filled up
with incoming updates. In that case, FAST triggers its flushing module, equipped
with a f lushing policy, to free some memory space by evicting a selected part of the
memory, termed a f lushing unit, to the flash storage. Such flushing is done in a way
that amortizes the cost of expensive random write operations over a high number of
update operations. In this section, we first define the flushing unit. Then, we discuss
the flushing policy used in FAST. Finally, we explain the FAST flushing algorithm.
The motivation of having a f lushing policy that flushes only part of the memory
is twofold: (1) Clearing the whole memory at once will cause a significant pause to
the system due to the need of erasing all the flash blocks that include at least one
update record in memory. As a result, it is better to consider clearing only part of the
memory in a way that does not really pause the system. In this paper, we present two
main flushing policies employed by the system, and we empirically evaluate both of
them, (2) Considering that we need to flush only part of the memory, it is crucial to
select that part in a way that reduces the overhead of the block erase operation.
7.1 Flushing unit
An important design parameter, in FAST, is the size of a f lushing unit, the granularity
of consecutive memory space written in the flash storage during each flush operation.
Our goal is to find a suitable f lushing unit size that minimizes the average cost of
flushing an update operation to the flash storage, denoted as C. The value of C
average writing cost
depends on two factors: C1 = numb
; the average cost per bytes written,
er of written b ytes

er of written b ytes
; the number of bytes written per update. This gives C =
and C2 = numb
numb er of updates
C1 × C2 .
Interestingly, the values of C1 and C2 show opposite behaviors with the increase
of the f lushing unit size. First consider C1 . On raw flash devices (e.g., ReMix [19]),
for a f lushing unit smaller than a flash block, C1 decreases with the increase of
the flushing unit size (see [29] for more detail experiments). This is intuitive, since
with a larger f lushing unit, the cost of erasing a block is amortized over more bytes
in the flushing unit. The same is also true for SSDs since small random writes
introduce large garbage collection overheads, while large random writes approach
the performance of sequential writes. Previous work has shown that, on several SSDs
including the ones from Samsung, MTron, and Transcend, random write latency per
byte increases by ≈32× when the write size is reduced from 16 KB to 0.5 KB [7]. Even
on newer generation SSDs from Intel, we observed an increase of ≈4× in a similar
experimental setup. This suggests that a flushing unit should not be very small, as that
would result in a large value of C1 . On the other hand, the value of C2 increases with
increasing the size of the f lushing unit. Due to non-uniform updates of tree nodes, a
large flushing unit is unlikely to have as dense updates as a small flushing unit. Thus,
the larger a f lushing unit is, the less the number of updates per byte is (i.e., the higher
the value of C2 is). Another disadvantage of large f lushing unit is that it may cause a
significant pause to the system. All these suggest that the f lushing unit should not be
very large.

432

Geoinformatica (2013) 17:417–448

Deciding the optimal size of a f lushing unit requires finding a sweet spot between
the competing costs of C1 and C2 . Our experiments show that for raw flash devices, a
f lushing unit of one flash block minimizes the overall cost. For SSDs, a f lushing unit
of size 16 KB is a good choice, as it gives a good balance between the values of C1
and C2 . Note that a flushing unit size of 16 KB also matches the optimal size of a tree
node, as suggested by Gray et al. [13]. Thus, with a tree of this optimal node size of
16 KB, we can simply flush one node at a time from the memory.
7.2 Flushing policies
FAST is designed so that different flushing policies can be plugged in to the system.
In the rest of this section, we discuss two main flushing policies adopted by FAST:
(1) FAST Flushing Policy, and (2) FAST* Flushing Policy.
7.2.1 FAST f lushing policy
The main idea of FAST f lushing policy is to minimize the average cost of writing
each update to the underlying flash storage. To that end, FAST flushing policy aims
to flush the in-memory tree updates that belong to the f lushing unit that has the
highest number of in-memory updates. In that case, the cost of writing the f lushing
unit will be amortized among the highest possible number of updates. Moreover,
since the maximum number of updates are being flushed out, this frees up the
maximum amount of memory used by buffered updates. Finally, as done in the
update operations, the flushing operation is logged in the log file to ensure the
durability of system transactions.
Data structure The flushing policy maintains an in-memory max heap structure,
termed FlushHeap, of all f lushing units that have at least one in-memory tree update.
The max heap is ordered on the number of in-memory updates for each f lushing unit,
and is updated with each incoming tree update. Updates in max heap is O(n), where
n is the number of flash blocks with in-memory updates. In the mean time, retrieving
the flushing unit with maximum number of updates is an O(1) operation.
7.2.2 FAST* f lushing policy
The FAST* f lushing policy is an enhancement over the FAST flushing policy
described in Section 7.2.1. FAST* flushing policy takes into account two parameters
that helps in deciding which unit must be flushed: (1) Number of updates per
flushing unit: It is the same parameter used by the FAST flushing policy explained in
Section 7.2.1; which favors the flash unit that has the highest number of updates, and
(2) Time stamp of the flushing unit: which represents the last time a flash block has
been updated. When deciding which unit needs to be flushed, that parameter gives
higher priority to the flushing unit that has the lowest time stamp (i.e., least recently
updated).
FAST* Flushing Policy employs a Top-1 selection algorithm to select a flushing
unit to be evicted to flash memory with the objective of maximizing the number of
updates per flushing unit and minimizing the time stamp of the flushing unit. The
intuition behind such a policy is that it is sometimes better to keep the block that
has the highest number of updates in the tree modifications table (i.e., in memory)

Geoinformatica (2013) 17:417–448

433

and not to flush it, especially if that block is expected to receive more updates
(i.e., recently updated block). On the other hand, it might be better to flush a flash
block that has a bit less number of updates, but it is not expected to be updated
frequently (i.e., least recently updated block). Hence, FAST* flushing policy makes
that tradeoff between the two parameters in order to amortize the total number of
erase operations on flash memory storage systems.
7.3 Flushing algorithm
Algorithm 4 gives the pseudo code for flushing tree updates. The algorithm has two
main steps: (1) Finding out the list of f lushed tree nodes (Lines 2–9 in Algorithm 4).
This step starts by finding out the victim f lushing unit, MaxUnit, using the flushing
policy passed to the algorithm. Then, we scan the tree modif ications table to find
all updated tree nodes that belong to MaxUnit. For each such node, we construct
the most recent version of the node by retrieving the tree node from the flash
storage, and updating it with the in-memory updates. This is done by calling the
RetrieveNode(N ) function, given in Algorithm 3. The list of these updated nodes
constitute the list of to be flushed nodes, FlushList. (2) Flushing, logging, and cleaning
selected tree nodes (Lines 10–15 in Algorithm 4). In this step, all nodes in the
FlushList are written once to the flash storage. As all these nodes reside in one
f lushing unit, this operation would have a minimal cost due to our careful selection
of the f lushing unit size. Then, similar to update operations, we log the flushing
operation to ensure durability. Finally, all flushed nodes are removed from the tree
modif ications table to free memory space for new updates.
Algorithm 4 Flushing Tree Updates
1: Function FlushTreeUpdates(FlushPolicy)
/* STEP 1: Finding out the list of flushed tree nodes */
2: FlushList ← {φ}
3: MaxUnit ← Retrieve Unit to be Flushed uisng FlushPolicy
4: for each Node N in tree modif ications table do
5:
if N ∈ MaxUnit then
6:
F ← RetrieveNode(N )
7:
FlushList ← FlushList ∪ F
8:
end if
9: end for
/* STEP 2: Flushing, logging, and cleaning selected nodes */
10: Flush all tree updates ∈ FlushList to a clean flash memory block
11: Add (Flush, All Nodes in FlushList) to the log file
12: Erase the old flash memory block and update the index pointer to refer the new
block
13: for each Node F in FlushList do
14:
Delete F from the Tree Modif ications Table
15: end for
Example In our running example given in Fig. 4, assume that the memory is full,
hence FAST triggers its flushing module. Assume also that nodes B, C, and D reside

434

Geoinformatica (2013) 17:417–448

in the same f lushing unit B1 , while nodes E, F, and G reside in another f lushing
unit B2 . The number of updates in B1 is three as each of nodes B, C and D has
been updated once. On the other hand, the number of updates in B2 is one because
nodes E and F has no updates at all, and node G has only a single update. Hence,
as per FAST flushing policy, MaxUnit is set to B1 , and we will invoke RetrieveNode
algorithm for all nodes belonging to B1 (i.e., nodes B, C, and D) to get the most
recent version of these nodes and flush them to flash memory. Then, the log entry
(Flush; Nodes B, C, D) is added to the log file (depicted as the last log entry
in Fig. 5a). Finally, the entries for nodes B, C, and D are removed from the tree
modif ications table.
Cost analysis For a given flushing operation F applied to a tree index structure T,
Let Pflush be the set of tree nodes that belongs to the block selected to be flushed.
Let M p be the number of modifications applied to node p and buffered in the tree
modifications table at the time F is applied. Hence, the total cost C F of flushing
operation F applied on T is as follows:

CF = EF + H +
[R F + (M p ∗ R M ) + W F + L]
(3)
p ∈ Pflush

We decide which unit to flush by employing the flushing policy passed to the
algorithm. The cost of this in memory operation H varies based on which flushing
policy is activated. For each node p ∈ Pflush
, we first need to retrieve the node current
value saved in flash memory which
costs
p ∈ Pflush R F , and then lookup the node in

the tree modifications table in
L.
For each node p ∈ Pflush , we read all
p ∈ Pflush
M p modifications
of
p
that
are
buffered
in
the
tree modifications table, which sum

up to p ∈ Pflush (M p ∗ R M ). Before we write the new nodes values, we first erase the
whole flash block which costs
 E F time. For each node p ∈ Pflush , we write the flushed
node new value, that costs p ∈ Pflush W F . All of the above sum up to give the flushing
operation cost given by Eq. 3.

8 Crash recovery and log compaction in FAST
As discussed before, FAST heavily relies on storing recent updates in memory, to
be flushed later to the flash storage. Although such design efficiently amortizes the
expensive random write operations over a large number of updates, it poses another
challenge where memory contents may be lost in case of system crash. To avoid such
loss of data, FAST employs a crash recovery module that ensures the durability of
in-memory updates even if the system crashed. The crash recovery module in FAST
mainly relies on the log file entries, written sequentially upon the update and flush
operations.
In this section, we will first describe the crash recovery module and logging
mechanism in FAST. Then, we will follow by discussing the log compaction operation
in FAST, which is mainly done to ensure that the log file is within a certain size limit.
Log compaction has a very similar operation to the recovery module, and it is crucial
to keep up the efficiency of FAST. For simplicity, we will not consider the case of
having a system crash during the recovery process, as this can be handled in a similar
way to traditional recovery modules in database systems.

Geoinformatica (2013) 17:417–448

435

8.1 Recovery
The recovery module in FAST is triggered when the system restarts from a crash,
with the goal of restoring the state of the system just before the crash took place.
The state of the system includes the contents of the in-memory data structure, tree
modif ications table, and the flash-resident tree index structure. By doing so, FAST
ensures the durability of all non-flushed updates that were stored in memory before
crash.
Main idea The main idea of the recovery operation is to scan the log file bottomup to be aware of the flushed nodes, i.e., nodes that made their way to the flash
storage. During this bottom-up scanning, we also find out the set of operations that
need to be replayed to restore the tree modif ications table. Then, the recovery module
cleans all the flash blocks, and starts to replay the non-flushed operations in the order
of their insertion, i.e., top-down. The replay process includes insertion in the tree
modif ications table as well as a new log entry. It is important here to reiterate our
assumption that there will be no crash during the recovery process, so, it is safe to
keep the list of operations to be replayed in memory. If we will consider a system
crash during the recovery process, we might just leave the operations to be replayed
in the log, and scan the whole log file again in a top-down manner. In this top-down
scan, we will only replay the operations for non-flushed nodes, while writing the new
log entries into a clean flash block. The result of the crash recovery module is that
the state of the memory will be stored as it was before the system crashes, and the
log file will be an exact image of the tree modif ications table.
Algorithm Algorithm 5 gives the pseudo code for crash recovery in FAST, which
has two main steps: (1) Bottom-Up scan (Lines 2–11 in Algorithm 5). In this step,
FAST scans the log file bottom-up, i.e., in the reverse order of the insertion of log
entries. For each log entry L in the log file, if the operation of L is Flush, then
we know that all the nodes listed in this entry have already made their way to the
flash storage. Thus, we keep track of these nodes in a list, termed FlushedNodes,
so that we avoid redoing any updates over any of these nodes later. On the other
side, if the operation of L is not Flush, we check if the node in L entry is in the list
FlushedNodes. If this is the case, we just ignore this entry as we know that it has
made its way to the flash storage. Otherwise, we push this log entry into a stack of
operations, termed RedoStack, as it indicates a non-flushed entry at the crash time.
At the end of this step, we pass the RedoStack to the second step. (2) Top-Down
processing (Lines 13–19 in Algorithm 5). At the beginning, we first create a new log
file Fnew . Then, this step basically goes through all the entries in the RedoStack in a
top-down way, i.e., the order of insertion in the log file. As all these operations were
not flushed by the crash time, we just add each operation to the tree modif ications
table and add a corresponding log entry in the new Log File Fnew . The reason of
doing these operations in a top-down way is to ensure that we have the same order
of updates, which is essential in case one node has multiple non-flushed updates. At
the end of this step, the tree modif ications table will be exactly the same as it was just
before the crash time, while the new log file Fnew will be exactly an image of the tree
modif ications table stored in the flash storage. Finally, we change the log file pointer
to refer to the new log file Fnew and we finally erase the old log file flash blocks.

436

Geoinformatica (2013) 17:417–448

Algorithm 5 Crash Recovery
1: Function RecoverFromCrash()
/* STEP 1: Bottom-Up Cleaning */
2: FlushedNodes ← φ
3: for each Log Entry L in the log file in a reverse order do
4:
if the operation of L is Flush then
5:
FlushedNodes ← FlushedNodes ∪ the list of nodes in L
6:
else
7:
if the node in entry L ∈
/ FlushedNodes then
8:
Push L into the stack of updates RedoStack
9:
end if
10:
end if
11: end for
/* Phase 2: Top-Down Processing */
12: Create a new Log File Fnew
13: while RedoStack is not Empty do
14:
Op ← Pop an update operation from the top of RedoStack
15:
Insert the operation Op into the tree modif ications table
16:
Add a log entry for Op in the new log file Fnew
17: end while
18: Change the Log File pointer to refer to the new Log File Fnew
19: Clean all the old log entries by erasing the old log file flash blocks

Example In our running example, the log entries of inserting Objects O7 and O8
in Fig. 4 are given as the first four log entries in Fig. 5a. Then, the last log entry
in Fig. 5a corresponds to flushing nodes B, C, and D. We assume that the system
is crashed just after inserting this flushing operation. Upon restarting the system,
the recovery module will be invoked. First, the bottom-up scanning process will be
started with the last entry of the log file, where nodes B, C, and D are added to the
list FlushedNodes. Then, for the next log entry, i.e., the fourth entry, as the node
affected by this entry D is already in the FlushedNodes list, we just ignore this entry,
since we are sure that it has made its way to disk. Similarly, we ignore the third log
entry for node B. For the second log entry, as the affected node G is not in the
FlushedNodes list, we know that this operation did not make it to the storage yet,
and we add it to the RedoStack to be redone later. The bottom-up scanning step is
concluded by ignoring the first log entry as its affected node C is already flushed, and
by wiping out all log entries. Then, the top-down processing step starts with only one
entry in the RedoStack that corresponds to node G. This entry will be added to the
tree modif ications table and log file. Figure 5b gives the log file after the end of the
recovery module which also corresponds to the entries of the tree modif ications table
after recovering from failure.
Cost analysis For a given crash recovery operation R applied to a tree index
structure T, let Z be the set of operations recorded in the log file. Let α (0 ≤ α ≤ 1)
be the fraction of operations in Z that had been flushed to T before the system fails.
Let Spage , Sblock , and Slog be the byte size of the flash page, flash block and flash log

Geoinformatica (2013) 17:417–448

437

file, respectively. Hence, the total cost C R of a crash recovery operation R applied
on T is as follows:


RF
RF
+ Z × α × (W M + W F )
+
(4)
C R = Slog ×
Spage
Sblock
As all the Z entries in the log file have to be scanned, then the total cost to scan
Slog
them is R F × Spage
. In addition, only Z × α log file operations need to be redone
(i.e., written back to the tree modifications table), which results to an additional cost
of Z × α × W M . As all redone operations are written back to memory, an additional
cost of logging them is Z × α × W F . The old log file blocks needs to be erased, which
Slog
incurs a cost of E F × Sblock
. All of the above sums up to give the recovery cost given
in Eq. 4.
8.2 Log compaction
As FAST log file is a limited resource, it may eventually become full. In this case,
FAST triggers a log compaction module that organizes the log file entries for better
space utilization. This can be achieved by two space saving techniques: (a) Removing
all the log entries of flushed nodes. As these nodes have already made their way to
the flash storage, we do not need to keep their log entries anymore, and (b) Packing
small log entries in a larger writing unit. Whenever a new log entry is inserted, it
mostly has a small size that may occupy a flash page as the smallest writing unit
to the flash storage. At the time of compaction, these small entries can be packed
together to achieve the maximum possible space utilization.
The main idea and algorithm for the log compaction module are almost the same
as the ones used for the recovery module, with the exception that the entries in the
RedoStack will not be added to the tree modif ications table, yet they will just be
written back to the log file, in a more compact way. As in the recovery module,
Fig. 5a and b give the log file before and after log compaction, respectively. The log
compaction have similar expensive cost as the recovery process. Fortunately, with
an appropriate size of log file and memory, it will not be common to call the log
compaction module.
It is unlikely that the log compaction module will not really compact the log file
much. This may take place only for a very small log size and a very large memory size,
as there will be a lot of non-flushed operations in memory with their corresponding
log entries. Notice that if the memory size is small, there will be a lot of flushing
operations, which means that log compaction can always find log entries to be
removed. If this unlikely case takes place, we call an emergency f lushing operation
where we force flushing all main memory contents to the flash memory persistent
storage, and hence clean all the log file contents leaving space for more log entries to
be added.
Cost analysis The log compaction is almost the same as the crash recovery procedure. The only difference is that records are not redone (written to the tree
modifications table). Similar to recovery cost, the log compaction cost CC is as
follows:


RF
RF
+ Z × α × WF
+
(5)
CC = Slog ×
Spage
Sblock

438

Geoinformatica (2013) 17:417–448

9 Experimental evaluation
This section experimentally evaluates the performance of FAST, compared to the
state-of-the-art algorithms for one-dimensional and multi-dimensional flash index
structures: (1) Lazy Adaptive Tree (LA-tree) [2]: LA-tree is a flash friendly one
dimensional index structure that is intended to replace the B-tree. LA-tree stores
the updates in cascaded buffers residing on flash memory and, then empties these
buffers dynamically based on the operations workload. (2) FD-tree [24, 25]: FD-tree
is a one-dimensional index structure that allows small random writes to occur only
in a small portion of the tree called the head tree which exists at the top level of
the tree. When the capacity of the head tree is exceeded, its entries are merged in
batches to subsequent tree levels. (3) RFTL [35]: RFTL is a mutli-dimensional tree
index structure that adds a buffering layer on top of the flash translation layer (FTL)
in order to make R-trees work efficiently on flash devices.
We instantiate B-tree and R-tree instances of FAST using both flushing policies
(i.e., FAST flushing policy and FAST* flushing policy), termed FAST-Btree, FAST*Btree, FAST-Rtree, and FAST*-Rtree , respectively, by implementing FAST inside
the GiST generalized index structure [15], which is already built inside PostgreSQL
[1]. In our experiments, we use two synthetic workloads: (1) Lookup intensive
workload (W L ): that includes 80 % search operations and 20 % update operations
(i.e., insert, delete, or update). (2) Update intensive workload, (WU ): that includes
20 % search operations and 80 % update operations.
Unless mentioned otherwise, we set the number of workload operations to
10 million operations, main memory size to 256 KB (i.e., the amount of memory
dedicated to main memory buffer used by FAST), tree index size to 512 MB, and log
file size to 10 MB, which means that the default log size is ≈2 % of the index size.
The experiments in this section mainly discuss the effect of varying the memory
size, log file size, index size, and number of updates on the performance of FASTBtree, FAST-Rtree, LA-tree, FD-tree, and RFTL. Also, we study the performance of
flushing, log compaction, and recovery operations in FAST. In addition, we compare
the implementation cost between FAST and its counterparts. Our performance
metrics are mainly the number of flash memory erase operations and the average
response time. However, in almost all of our experiments, we got a similar trend for
both performance measures. Thus, for brevity, we only show the experiments for the
number of flash memory erase operations, which is the most expensive operation in
flash storage. Although we compare FAST to its counterparts from a performance
point of view, however we believe the main contribution of FAST is not in the
performance gain. The generic structure and low implementation cost are the main
advantages of FAST over specific flash-aware tree index structures.
All experiments were run on both raw flash memory storage, and solid state drives
(SSDs). For raw flash, we used the raw NAND flash emulator described in [2].
The emulator was populated with exhaustive measurements from a custom-designed
Mica2 sensor board with a Toshiba1Gb NAND TC58DVG02A1FT00 flash chip. For
SSDs, we used a 32GB MSP-SATA7525032 SSD device. All the experiments were
run on a machine with Intel Core2 8400 at 3Ghz with 4GB of RAM running Ubuntu
Linux 8.04.

Geoinformatica (2013) 17:417–448

439

9.1 Effect of memory size
Figure 6 and b give the effect of varying the memory size from 128 KB to 1,024 KB
(in a log scale) on the number of erase operations, encountered in FAST-Btree, LAtree, and FD-tree, for workloads W L and WU , respectively. For both workloads and
for all memory sizes, FAST-Btree consistently has much lower erase operations than
that of the LA-tree. More specifically, Fast-Btree results in having only from half
to one third of the erase operations encountered by LA-tree. This is mainly due to
the choice of f lushing unit and f lushing policy used in FAST that amortize the block
erase operations over a large number of updates. Also, for both experiments, the
number of erase operations decreases with the increase of the memory size, which is
intuitive as more memory means less frequent need for flushing, and hence less need
for block erase operations.
The performance of FAST-Btree is slightly better than that of FD-tree, because
FD-tree does not employ a crash recovery technique (i.e., no logging overhead).
FAST still performs better than FD-tree due to FAST flushing policy that selects
the best block to be flushed to flash memory. Although the performance of FD-tree
is close to FAST-Btree, however FAST has the edge of being a generic framework
which is applied to many tree index structures and needs less work and overhead
(in terms of lines of code) to be incorporated in the database engine. Comparing
the two workloads against each other, we can see that the workload WU encounters
much more erase operations than that of workload W L . This is mainly because WU
is an update intensive workload which results in many in-memory updates that need
to flushed. FAST*-Btree gives a slightly better performance than FAST-Btree as
FAST*-Btree employs a flushing policy that does not only rely on the number of
updates per flash block, but also takes into account the last time a flash block has
been updated. Hence, FAST*-tree gives a chance for those flash blocks that has
higher number of updates to stay in memory if more updates are expected to be
applied to these blocks.
Figures 7a and b give similar experiments to that of Fig. 6 and b, with the exception
that we run the experiments for two-dimensional search and update operations
for both the Fast-Rtree and RFTL. To be able to do so, we have adjusted our
600

# of erase operations *(103)

# of erase operations *(103)

90
80
70
60
50
40
30

FAST*–Btree
FAST–Btree
LA–tree
10
FD–tree

20

0
128

256

512

1024

FAST*–Btree
FAST–Btree
LA–tree
FD–tree

500
400
300
200
100
0
128

256

512

Memory Size (KB)

Memory Size (KB)

(a) WL

(b) WU

Fig. 6 Effect of memory size on one-dimensional index structure

1024

440

Geoinformatica (2013) 17:417–448
900

FAST*–Rtree
FAST–Rtree
RFTL

80

# of erase operations *(103)

# of erase operations *(103)

100

60
40
20

128

256

512

1024

FAST*-Rtree
FAST-Rtree
RFTL

800
700
600
500
400
300
200
100
0
128

256

512

Memory Size (KB)

Memory Size (KB)

(a) Spatial-WL

(b) Spatial-WU

1024

Fig. 7 Effect of memory size on multi-dimensional index structure

workload W L and WU to Spatial-W L and Spatial-WU , respectively, which have twodimensional operations instead of the one-dimensional operations used in W L and
WU . The result of these experiments have the same trend as the ones done for onedimensional tree structures, where FAST-Rtree has consistently better performance
than RFTL in all cases, with around one half to one third of the number of erase
operations encountered in RFTL. Similar to the one-dimesnional case, FAST*-Rtree
slightly outperforms FAST-Rtree. Comparing the multi-dimensional workload to the
one dimensional one shows that the multi-dimensional workload encounters more
erase operations which is mainly due to the facts that the update operation may span
more nodes. However, even with this, FAST still keeps its performance ratio over its
counterparts.
The experiments in Figs. 6 and 7 not only shows that FAST has better performance
than its counterparts LA-tree, FD-tree and RFTL, but it also shows the power of
the FAST framework where it can be applied to both one-dimensional and multidimensional index structures with the same efficiency. In other words, it is not only
that FAST is better than LA-tree and FD-tree, but it is also the fact that FAST has
the ability to efficiently support multi-dimensional search and update operations in
which LA-tree or FD-tree cannot even support.
9.2 Effect of log file size
Figure 8 gives the effect of varying the log file size from 10 MB (i.e., 2 % of the
index size) to 25 MB (i.e., 5 % of the index size) on the number of erase operations,
encountered in FAST-Btree, LA-tree, and FD-tree for workload W L (Fig. 8a) and
FAST-Rtree and RFTL for workload Spatial-WU (Fig. 8b). For brevity, we do not
show the experiments of FAST-Btree, LA-tree, and FD-tree for workload WU nor
the experiment of FAST-Rtree and RFTL for workload Spatial-W L . As can be seen
from the figures, the performance of both LA-tee, FD-tree, and RFTL is not affected
by the change of the log file size. This is mainly because these three approaches
rely on buffering incoming updates, and hence does not make use of any log file. It
is interesting, however, to see that the number of erase operations in FAST-Btree
and FAST-Rtree significantly decreases with the increase of the log file size, given
that the memory size is set to its default value of 256 KB in all experiments. The

Geoinformatica (2013) 17:417–448
800

FAST*–Btree
FAST–Btree
LA–tree
FD–tree

90
80
70

# of erase operations *(103)

# of erase operations *(103)

100

441

60
50
40
30
20
10
0

10

20

15

25

700
600
500

FAST*–Rtree
FAST–Rtree
RFTL

400
300
200
100
0

10

15

20

Maximum Log File Size (MB)

Maximum Log File Size (MB)

(a) WL

(b) Spatial-WU

25

Fig. 8 Effect of FAST log file size

justification for this is that with the increase of the log file size, there will be less
need for FAST to do log compaction. FAST*-Btree and FAST*-Rtree shows the
same trend as FAST-Btree and FAST-Rtree except that they slightly give better
performance due to the fact that they apply the FAST* Flushing policy.
Revisiting Figs. 6 and 7 in Section 9.1, the number of erase operations encountered
in both LA-tree, FD-tree, and RFTL were only coming from flushing buffered
updates, while the number of erase operations in FAST were coming from two
sources, flushing in-memory updates, and log compaction. Then, the experiment in
this section (Fig. 8) shows that a large fraction of the erase operations in FAST is
coming from the log compaction operation, which can be significantly reduced with
the slight increase of the log file. With this, we can see that FAST achieves close
to an order of magnitude less erase operations than its counterparts for both onedimensional and multi-dimensional index structures when having the log file as small
as 5 % of the index size, i.e., 25 MB.
9.3 Effect of index size
Figure 9 gives the effect of varying the index size from 128 MB to 4 GB (in a log
scale) on the number of erase operations, encountered in FAST-Btree, LA-tree,
and FD-tree for workload W L (Fig. 9a) and FAST-Rtree and RFTL for workload
Spatial-WU (Fig. 9b). Same as in Section 9.2, we omit other workloads for brevity. In
all cases, FAST consistently gives much better performance than its counterparts.
Both FAST and other index structures have similar trend of a linear increase of
the number of erase operations with the increase of the index size. This is mainly
because with a larger index, an update operation may end up modifying more nodes
in the index hierarchy, or more overlapped nodes in case of multi-dimensional index
structures. Moreover, FAST*-Btree and FAST*-Rtree give a bit better performance
than FAST-Btree and FAST-Rtree, respectively. This is basically due to the fact that
FAST* flushing policy handles the flash memory updates better than the original
FAST flushing policy, hence when the index size increase the possibility that more
blocks are updated increases leading to such performance gain for both FAST*-Btree
and FAST*-Rtree. The take home message from this experiment is that FAST still
maintains its performance gain over its counterparts even with the large increase of
the index size.

Geoinformatica (2013) 17:417–448

140

# of Erase Operations *(103)

# of Erase Operations *(103)

442
FAST*–Btree
FAST–Btree
LA–tree
FD–tree

120
100
80
60
40
20
0

1200

FAST*–Rtree
FAST–Rtree
RFTL

1000
800
600
400
200
0

B
4G

B
2G

B

2M

B

6M

B

8M

B
1G

51

25

12

B
4G

B
2G

B

2M

B

6M

B

8M

B
1G

51

25

12

Index Size

Index Size

(a) WL

(b) Spatial-WU

Fig. 9 Effect of tree index size

9.4 Effect of number of updates

10000
9000
8000
7000
6000
5000
4000
3000
2000
1000
0

# of erase operations *(103)

# of erase operations *(103)

Figure 9 gives the effect of varying the number of update operations from one
million to 100 millions (in a log scale) on the number of erase operations for both
one-dimensional (i.e., FAST-Btree, LA-tree, and FD-tree in Fig. 10a) and multidimensional index structures (i.e., FAST-Rtree and RFTL in Fig. 10b). As we are
only interested in update operations, the workload for the experiments in this section
is just a stream of incoming update operations, up to 100 million operations. As
can be seen from the figure, FAST scales well with the number of updates and still
maintains its superior performance over its counterparts from both one-dimensional
(LA-tree) and multi-dimensional index structures (RFTL). FAST performs slightly
better than FD-tree; this is because FD-tree (one dimensional index structure) is
buffering some of the tree updates in memory and flushes them when needed, but
FAST applies a flushing policy, which flushes only the block with the highest number
of updates. In addition, FAST* slightly outperforms FAST because FAST* flushing
policy employs a Top-1 algorithm that maximizes the number of updates per block
and minimize the timestamp at which the block has been updated, hence the total
amortized update cost in FAST* is less than FAST.

FAST*–Btree
FAST–Btree
LA–tree
FD–tree

1

10

100

FAST*–Rtree
FAST–Rtree
RFTL

10000

1000

1

10

# of Updates *(106)

# of Updates *(106)

(a) FAST-Btree

(b) FAST-Rtree

Fig. 10 Effect of number of updates

100

Geoinformatica (2013) 17:417–448

443

9.5 Flushing performance

550
500
450
400
350
300
250
200
150
100
50
0
128

600

FAST*–Flush
FAST–Flush
Flush–All
Rand–Flush

256

512

# of erase operations *(103)

# of erase operations *(103)

Figure 11 illustrates the performance of the f lushing policy employed by FAST
compared to a naive flushing policy, termed f lush-all that just flushes all the memory
contents to the flash storage once. We also compare FAST to a random flushing
policy, termed Rand-Flush that chooses a block at random and flushed its contents
to the flash storage The performance is given with respect to various memory
sizes (Fig. 11a) and log file sizes (Fig. 11a). Both experiments were run for FASTBtree under workload WU . Running these experiments for FAST-Rtree and other
workloads give similar performance, and thus omitted for brevity.
Figure 11a gives the effect of varying the memory size from 128 KB to 1,024 KB
on the number of erase operations for flush-all policy, Rand-Flush policy and FAST
flushing policy. In all cases, FAST has much lower erase operations than the flushall and Rand-Flush policies, which is about one fourth of the erase operations for a
memory size of 512 KB. The main reason behind this gain in FAST is that it amortizes
the cost of the block erase operation over a large number of updates, and hence, will
free more memory with each flushing operation. On the other side, in the flush-all or
Rand-Flush policy, a block may be erased just because it has only one single update in
the memory. In this case, although a block is erased, it does not free much memory
space. The Rand-Flush policy performance is slightly better than that of flush-all
policy because the Rand-Flush flushes only one block and hence keeping all other
blocks in memory, which decrease the cost of random writes on these blocks.
FAST* flushing policy is better than FAST flushing policy as it better amortizes
the update cost. FAST* policy may still keep a block that has the highest number of
updates in memory if this block has higher potential to be updated soon, and hence
the decreasing the number of erase operations applied to that block.
Figure 11b gives a similar experiment to that of Fig. 11a with the exception that
we study the effect of changing the log size from 10 MB to 25 MB on the number
of erase operations. In all cases, FAST flushing policy is superior, which is intuitive
given the above explanation for Fig. 11a. However, an interesting observation from
Fig. 11b is that the gain from FAST flushing policy over the flush-all and RandFlush policies increases with the increase of the log file size. This means that FAST
flushing policy makes better use of the log file than the flush-all and Rand-Flush
policies. A justification for this is as follows: As FAST flushing policy evicts a block

1024

FAST*–Flush
FAST–Flush
Flush–All
Rand–Flush

500
400
300
200
100
0

10

15

20

Memory Size (KB)

Log Size (MB)

(a) Memory size

(b) Log size

Fig. 11 Flushing performance

25

444

Geoinformatica (2013) 17:417–448

to the storage only if it has high number of updates, the log entry for this flushing
operation will include many updated nodes. Then, in the log compaction process,
there will be a lot of space for compaction. This would not be the case for the flushall and Rand-Flush policies where a log entry for a flush operation may include only
one flushed node. Then, at the time of log compaction, there will be nothing much
to compact, which means that the log compaction will be called again. As discussed
in Section 9.2 and Fig. 9, log compaction is a major factor in the number of erase
operations. Reducing the frequency of log compaction makes FAST flushing policy
more superior than the flush-all policy. Moreover, FAST* flushing policy slightly
outperforms FAST flushing policy because of the fact that FAST* may prefer to
keep the block that has the highest number of updates in memory leading to less
erase operations on the flash memory storage.
9.6 Log compaction
Figure 12a gives the behavior and frequency of log compaction operations in FAST
when running a sequence of 200 thousands update operations for a log file size of
10 MB. The Y axis in this figure gives the size of the filled part of the log file, started
as empty. The size is monotonically increasing with having more update operations
till it reaches its maximum limit of 10 MB. Then, the log compaction operation is
triggered to compact the log file. As can be seen from the figure, the log compaction
operation may compact the log file from 20 to 60 % of its capacity, which is very
efficient compaction. Another take from this experiment is that we have made only
seven log compaction operations for 200 thousands update operations, which means
that the log compaction process is not very common, making FAST more efficient
even with a large amount of update operations.
9.7 Recovery performance
Figure 12b gives the overhead of the recovery process in FAST, which serves also
as the overhead of the log compaction process. The overhead of recovery increases
linearly with the size increase of the log file contents at the time of crash. This is
intuitive as with more log entries in the log file, it will take more time from the FAST

100

Recovery Time (millisec)

Log File Size (MB)

10
8
6
4
2
0

0

50

100

150

200

FAST

90
80
70
60
50
40
30
20
10

1

2

3

4

5

6

Number of Updates So Far *(103)

Log Size (MB)

(a) Log Compaction

(b) Recovery

Fig. 12 Log compaction and recovery

7

8

9

Geoinformatica (2013) 17:417–448

445

recovery module to scan this log file, and replay some of its operations to recover the
lost main memory contents. However, what we really want to emphasize on in this
experiment is that the overhead of recovery is only about 100 ms for a log file that
includes 9 MB of log entries. This shows that the recovery overhead is a low price to
pay to ensure transaction durability.

10 Conclusion
This paper presented FAST; a generic framework for flash-aware data-partitioning
tree index structures. FAST distinguishes itself from all previous attempts of flash
memory indexing in two aspects: (1) FAST is a generic framework that can be applied
to a wide class of tree index structures, and (2) FAST achieves both ef f iciency
and durability of read and write flash operations. FAST has four main modules,
namely, update, search, f lushing, and recovery. The update module is responsible
on buffering incoming tree updates in an in-memory data structure, while writing
small entries sequentially in a designated flash-resident log file. The search module
retrieves requested data from the flash storage and updates it with recent updates
stored in memory, if any. The f lushing module is responsible on evicting flash blocks
from memory to the flash storage to give space for incoming updates. Finally, the
recovery module ensures the durability of in-memory updates in case of a system
crash.

References
1. PostgreSQL. http://www.postgresql.org
2. Agrawal D, Ganesan D, Sitaraman RK, Diao Y, Singh S (2009) Lazy-adaptive tree: an optimized
index structure for flash devices. PVLDB
3. Agrawal N, Prabhakaran V, Wobber T, Davis J, Manasse M, Panigrahy R (2008) Design
tradeoffs for SSD performance. In: Usenix annual technical conference, USENIX
4. Bayer R, McCreight EM (1972) Organization and maintenance of large ordered indices. Acta
Inform 1:173–189
5. Beckmann N, Kriegel H-P, Schneider R, Seeger B (1990) The R*-tree: an efficient and robust
access method for points and rectangles. In: SIGMOD
6. Birrell A, Isard M, Thacker C, Wobber T (2007) A design for high-performance flash disks. ACM
SIGOPS Oper Syst Rev 41(2):88–93
7. Bouganim L, Jónsson B, Bonnet P (2009) uFLIP: understanding flash IO patterns. In: CIDR
8. Chang Y-H, Hsieh J-W, Kuo T-W (2007) Endurance enhancement of flash-memory storage
systems: an efficient static wear leveling design. In: Proceedings of the annual ACM IEEE Design
Automation Conference, DAC, pp 212–217
9. Chen S (2009) FlashLogging: exploiting flash devices for synchronous logging performance. In:
SIGMOD. New York, NY
10. Comer D (1979) The ubiquitous B-tree. ACM Comput Surv 11(2):121–137
11. Gray J (2006) Tape is dead, disk is tape, flash is disk, RAM locality is king. http://research.
microsoft.com/∼gray/talks/Flash_is_Good.ppt. Accessed Dec 2006
12. Gray J, Fitzgerald B (2008) Flash disk opportunity for server applications. ACM Queue 6(4):18–
23
13. Gray J, Graefe G (1997) The five-minute rule ten years later, and other computer storage rules
of thumb. SIGMOD Rec 26(4):63–68
14. Guttman A (1984) R-trees: a dynamic index structure for spatial searching. In: SIGMOD
15. Hellerstein JM, Naughton JF, Pfeffer A (1995) Generalized search trees for database systems.
In: VLDB
16. Hutsell W (2007) Solid state storage for the enterprise. Storage Networking Industry Association
(SNIA) Tutorial, Fall

446

Geoinformatica (2013) 17:417–448

17. Katayama N, Satoh, S (1997) The sr-tree: an index structure for high-dimensional nearest neighbor queries. In: SIGMOD
18. Kim H, Ahn S (2008) BPLRU: a buffer management scheme for improving random writes in
flash storage. In: FAST
19. Lavenier D, Xinchun X, Georges G (2006) seed-based genomic sequence comparison using a
FPGA/FLASH accelerator. In: ICFPT
20. Lee S, Moon B (2007) Design of flash-based DBMS: an in-page logging approach. In: SIGMOD
21. Lee S-W, Moon B, Park C, Kim J-M, Kim S-W (2008) A case for flash memory SSD in enterprise
database applications. In: SIGMOD
22. Lee S-W, Park D-J, sum Chung T, Lee D-H, Park S, Song H-J (2007) A log buffer-based flash
translation layer using fully-associate sector translation. TECS
23. Leventhal A (2008) Flash storage today. ACM Queue 6(4):24–30
24. Li Y, He B, Luo Q, Yi K (2009) Tree indexing on flash disks. In: ICDE
25. Li Y, He B, Yang RJ, Luo Q, Yi K (2010) Tree indexing on solid state drives. Proceedings of the
VLDB Endowment 3(1–2):1195–1206
26. Ma D, Feng J, Li G (2011) LazyFTL: A page-level flash translation layer optimized for NAND
flash memory. In: SIGMOD
27. McCreight EM (1977) Pagination of B*-trees with variable-length records. Commun ACM
20(9):670–674
28. Moshayedi M, Wilkison P (2008) Enterprise SSDs. ACM Queue 6(4):32–39
29. Nath S, Gibbons PB (2008) Online maintenance of very large random samples on flash storage.
In: VLDB
30. Nath S, Kansal A (2007) Flashdb: dynamic self-tuning database for NAND flash. In: IPSN
31. Reinsel D, Janukowicz J (2008) Datacenter SSDs: solid footing for growth. http://www.samsung.
com/us/business/semiconductor/news/downloads/210290.pdf. Accessed Jan 2008
32. Sellis TK, Roussopoulos N, Faloutsos C (1987) The R+-tree: a dynamic index for multidimensional objects. In: VLDB
33. Shah MA, Harizopoulos S, Wiener JL, Graefe G (2008) Fast scans and joins using flash drives.
In: International Workshop of Data Managment on New Hardware, DaMoN
34. White DA, Jain R (1996) Similarity indexing with the SS-tree. In: ICDE
35. Wu C, Chang L, Kuo T (2003) An efficient R-tree implementation over flash-memory storage
systems. In: GIS
36. Wu C, Kuo T, Chang L (2007) An efficient B-tree layer implementation for flash-memory storage
systems. TECS

Mohamed Sarwat is a PhD candidate at the Computer Science and Engineering department,
University of Minnesota, where he also received his master’s degree in computer science in 2011. His
research interest lies in the broad area of Database systems, spatio-temporal databases, distributed
graph databases, social networking, cloud computing, large-scale data management, data indexing
and storage systems. He has been awarded the University of Minnesota Doctoral Dissertation
Fellowship in 2012/2013. He has been a recipient of Best Research Paper Award in the 12th
international symposium on spatial and temporal databases 2011.

Geoinformatica (2013) 17:417–448

447

Mohamed F. Mokbel is an associate professor in the Department of Computer Science and Engineering, University of Minnesota. His current main research interests focus on providing database
and platform support for spatial data, moving objects, and location-based services. Mohamed is the
main architect for the PLACE, Casper, and CareDB systems that provide a database support for
location-based services, location privacy, and personalization, respectively. His research work has
been recognized by two best paper awards at IEEE MASS 2008 and MDM 2009 and by the NSF
CAREER award 2010. Mohamed is currently the general co-chair of SSTD 2011 and program cochair for MDM 2011, DMSN 2011, and LBSN 2011. Mohamed was also the proceeding chair of ACM
SIGMOD 2010, and the program co-chair for ACM SIGSPATIAL GIS 2008, 2009, and 2010. He
serves in the editorial board of IEEE Data Engineering Bulletin, Distributed and Parallel Databases
Journal, and Journal of Spatial Information Science. Mohamed is an ACM and IEEE member and a
founding member of ACM SIGSPATIAL.

Xun Zhou received his B.Eng., and M.Eng., in Computer Science and Technology from Harbin
Institute of Technology, Harbin, China in 2007 and 2009 respectively. He is currently a Ph.D. student
in Computer Science at the University of Minnesota, Twin Cities. His research interests include
spatiotemporal data mining, spatial databases and Geographical Information Systems (GIS). His
current application focus is understanding climate change from data.

448

Geoinformatica (2013) 17:417–448

Suman Nath is a researcher in the Sensing and Energy Research Group at Microsoft Research
Redmond. He works on various data management problems in mobile and sensing systems. He
received his PhD from Carnegie Mellon University in 2005. He has authored 20+ patents (granted or
pending), 70+ papers in various computer science conferences and journals, and received Best Paper
Awards at BaseNets 2004, USENIX NSDI 2006, IEEE ICDE 2008, and SSTD 2011. At Microsoft,
he received the Gold Star Award, which recognizes excellence in leadership and contributions for
Microsoft’s long-term success.

arXiv:1604.03234v1 [cs.DB] 12 Apr 2016

Hippo: A Fast, yet Scalable, Database Indexing Approach
Jia Yu

Mohamed Sarwat

Arizona State University
699 S. Mill Avenue, Tempe, AZ

Arizona State University
699 S. Mill Avenue, Tempe, AZ

jiayu2@asu.edu

msarwat@asu.edu
TPC-H
2 GB
20 GB
200 GB

ABSTRACT
+

Even though existing database indexes (e.g., B -Tree) speed
up the query execution, they suffer from two main drawbacks: (1) A database index usually yields 5% to 15% additional storage overhead which results in non-ignorable dollar cost in big data scenarios especially when deployed on
modern storage devices like Solid State Disk (SSD) or NonVolatile Memory (NVM). (2) Maintaining a database index incurs high latency because the DBMS has to find and
update those index pages affected by the underlying table
changes. This paper proposes Hippo a fast, yet scalable,
database indexing approach. Hippo only stores the pointers
of disk pages along with light weight histogram-based summaries. The proposed structure significantly shrinks index
storage and maintenance overhead without compromising
much on query execution performance. Experiments, based
on real Hippo implementation inside PostgreSQL 9.5, using the TPC-H benchmark show that Hippo achieves up to
two orders of magnitude less storage space and up to three
orders of magnitude less maintenance overhead than traditional database indexes, i.e., B+ -Tree. Furthermore, the experiments also show that Hippo achieves comparable query
execution performance to that of the B+ -Tree for various
selectivity factors.

1.

Index size
0.25 GB
2.51 GB
25 GB

HDD
0.04 $/GB

Initialization time
30 sec
500 sec
8000 sec

(a) B+ -Tree overhead
E-HDD
SSD
0.1 $/GB 0.5 $/GB

Insertion time
10 sec
1180 sec
42000 sec
E-SSD
1.4 $/GB

(b) Storage dollar cost

Table 1: Index overhead and storage dollar cost

storage overhead. Even though the storage overhead
may not seem too high in small databases, it results in non-ignorable dollar cost in big data scenarios. Table 1a depicts the storage overhead of a B+ Tree created on the Lineitem table from the TPCH [5] benchmark (database size varies from 2, 20 and
200 GB). Moreover, the storage dollar cost is dramatically amplified when the DBMS is deployed on modern storage devices (e.g., Solid State Drives and NonVolatile Memory) because they are still more than an
order of magnitude expensive than Hard Disk Drives
(HDDs) per unit of storage. Table 1b lists the dollar
cost per storage unit collected from Amazon.com and
NewEgg.com (Enterprise is abbreviated to E). In addition, initializing an index may be a time consuming
process especially when the index is created on a large
database table. Such high initialization overhead may
delay the analysis process (see Table 1a).

INTRODUCTION

A database system (DBMS) often employs an index structure, e.g., B+ -Tree [4], to speed up query execution at the
cost of additional storage and maintenance overhead. A
DBMS user may create an index on one or more attributes
of a database table. A created index allows the DBMS to
quickly locate tuples without having to scan the whole indexed table. Even though existing database indexes significantly improve the query response time, they suffer from
the following drawbacks:

• Maintenance Overhead: A DBMS must update the
index after inserting (deleting) tuples into (from) the
underlying table. Maintaining a database index incurs high latency because the DBMS has to find and
update those index entries affected by the underlying
table changes. For instance, maintaining a B+ -Tree
searches the tree structure and perhaps performs a set
of tree nodes splitting or merging operations. That
requires plenty of disk I/O operations and hence encumbers the time performance of the entire DBMS in
big data scenarios. Table 1a shows the B+ Tree insertion overhead (insert 0.1% records) for the TPC-H
Lineitem table.

• Indexing Overhead: Indexing overhead consists of
two parts - storage and initialization time overhead.
A database index usually yields 5% to 15% additional

Existing approaches that tackle one or more of the aforementioned drawbacks are classified as follows: (1) Compressed indexes: Compressed B+ -Tree approaches [7, 8, 19]
1

reduce index storage overhead but all these methods compromise on query performance due to the additional compression and decompression time. Compressed bitmap indexes also reduce index storage overhead [9, 11, 14] but they
mainly suit low cardinality attributes which are quite rare.
For high cardinality attributes, the storage overhead of compressed bitmap indexes significantly increases [17]. (2) Approximate indexes: Approximate indexing approaches [2, 10,
12] trade query accuracy for storage to produce smaller, yet
fast, index structures. Even though approximate indexes
may shrink the storage size, users cannot rely on their unguaranteed query accuracy in many accuracy-sensitive application scenarios like banking systems or user archive systems. (3) Sparse indexes: A sparse index [3, 13, 15, 16] only
stores pointers which refer to disk pages and value ranges
(min and max values) in each page so that it can save indexing and maintenance overhead. It is generally built on
ordered attributes. For a posed query, it finds value ranges
which cover or overlap the query predicate and then rapidly
inspects the associated few parent table pages one by one
for retrieving truly qualified tuples. However, for unordered
attributes which are much more common, sparse indexes
compromise too much on query performance because they
find numerous qualified value ranges and have to inspect a
large number of pages.
This paper proposes Hippo1 a fast, yet scalable, sparse
database indexing approach. In contrast to existing tree index structures, Hippo stores disk page ranges (each works
as a pointer of one or many pages) instead of tuple pointers
in the indexed table to reduce the storage space occupied by
the index. Unlike existing approximate indexing methods,
Hippo guarantees the query result accuracy by inspecting
possible qualified pages and only emitting those tuples that
satisfy the query predicate. As opposed to existing sparse
indexes, Hippo maintains simplified histograms that represent the data distribution for pages no matter how skew it is,
as the summaries for these pages in each page range. Since
Hippo relies on histograms already created and maintained
by almost every existing DBMS (e.g., PostgreSQL), the system does not exhibit a major additional overhead to create
the index. Hippo also adopts a page grouping technique
that groups contiguous pages into page ranges based on the
similarity of their index key attribute distributions. When a
query is issued on the indexed database table, Hippo leverages the page ranges and page summaries to recognize those
pages for which the internal tuples are guaranteed not to satisfy the query predicates and inspects the remaining pages.
Thus Hippo achieves competitive performance on common
range queries without compromising the accuracy. For data
insertion and deletion, Hippo dispenses with the numerous
disk operations by rapidly locating the affected index entries. Hippo also adaptively decides whether to adopt an
eager or lazy index maintenance strategy to mitigate the
maintenance overhead while ensuring future queries are answered correctly.
We implemented a prototype of Hippo inside PostgreSQL
9.5. Experiments based on the TPC-H benchmark show
that Hippo achieves up to two orders of magnitude less storage space and up to three orders of magnitude less maintenance overhead than traditional database indexes, i.e.,
B+ -Tree. Furthermore, the experiments show that Hippo
1

Create an index

Execute a query
User

Age table
Disk Page #

Query predicate

Internal data

1,2,3,4,5,… 21,22,55,75,77,…

Compare

Bucket
2,3,4,…

(Return)

Bucket
1
2
3
4
5

Age
1 - 20
21 - 40
41 - 60
61 - 90
91 - 120

Age = 55
Compare

(Return)

Bucket
3

Complete height balanced histogram
Filter false positives

Generate partial histograms

Hippo
Bucket
2
3
4

Age
21 - 40
41 - 60
61 - 90

Partial histogram 1
Bucket
2
4
5

Age
21 - 40
61 - 90
91 - 120

Partial histogram 2

Page range Partial histogram Internal data
(Return)
1 - 10
2,3,4
21,22,55,75,77
11 – 25
2,4,5
23,24,62,91,92
26 - 30
1,2,5
11,12,25,101,110
Index Entry 1

…

Bucket
1
2
5

Age
1 - 20
21 - 40
91 - 120

Inspect
Page
1 - 10

Partial histogram 3
Legend: Index Initialization

Database
Index Search

Figure 1: Initialize and search Hippo on age table

achieves comparable query execution performance to that
of the B+ -Tree for various selectivity factors.
The remainder of the paper is structured as follows: In
Section 2, we explain the idea of Hippo and show its structure. We demonstrate how to query Hippo swiftly, build
Hippo from scratch, and maintain Hippo efficiently in Section 3, 4 and 5. In Section 6, we provide useful cost estimation for these three scenarios. Extensive experiments
and related analysis are included in Section 7. We discuss
related work then analyze the drawbacks in existing indexes
in Section 8. Finally, Section 9 concludes the paper.

2. HIPPO OVERVIEW
This section gives an overview of Hippo. A running example that describes a Hippo index built on an age table is
given in Figure 1. The figure’s right part which depicts how
to search Hippo and the left part which shows how to initialize Hippo are explained in Section 3 and 4 respectively.
The main challenges of designing an index are to reduce
the indexing overhead in terms of storage and initialization
time as well as speed up the index maintenance while still
keeping competitive query performance. To achieve that,
an index should possess the following two main properties:
(1) Less Index Entries: For better storage space utilization,
an index should determine and only store the most representative index entries that summarize the key attribute.
Keeping too many index entries inevitably results in high
storage overhead as well as high initialization time. (2) Index Entries Independence: Index entries of a created index
should be independent from each other. In other words, the
range of values that each index entry represents should have
minimal overlap with other index entries. Interdependence
among index entries, like that in a B+ -Tree, may lead to
overlapped tree nodes traverse during query processing and
several cascaded updates during index maintenance.

https://github.com/DataSystemsLab/hippo-postgresql

2

Age = 55

Data Structure. When creating an index, Hippo scans
the indexed table and generates histogram-based summaries
for disk pages based upon the index key attribute. Afterwards, these summaries are stored by Hippo along with
pointers of the pages they summarize. As shown in Figure 1, a Hippo index entry consists of the following two
components (Internal data of pages is given in the figure
only for the ease of understanding):

Bucket
1
2
3
4
5

Age > 55
Bucket
1
2
3
4
5

Age > 55 AND Age < 65

Age
1 - 20
21 - 40
41 - 60
61 - 90
91 - 120

Bucket
1
2
3
4
5

Age
1 - 20
21 - 40
41 - 60
61 - 90
91 - 120

Figure 2: Convert query predicates

• Summarized Page Range: The page range (works
as a pointer) represents the IDs (i.e., address) of the
first and last pages summarized by a certain histogram
based summary. DBMS can load particular pages into
buffer according to their customized IDs. Hippo is able
to summarize more than one contiguous (in terms of
physical storage) pages to reduce the overall index size
to a great extent (e.g., Page 1 - 10, 11 - 25, 26 - 30).
The number of summarized pages (denoted as pages
per partial histogram) in each index entry varies. For
a certain index attribute, some contiguous pages have
very similar content but some are not. Hence, Hippo
adopts a page grouping technique that groups contiguous pages into page ranges based on the similarity of
their index attribute distributions, using the partial
histogram density (explained in Section 4).

predicate. The search process leverages the index structure
to avoid worthless page inspection so that Hippo can achieve
competitive query performance.
Algorithm 1: Hippo index search
Data: A given query predicate and Hippo index
Result: Qualified tuples
1 Create Bitmap a for the given predicate;
2 foreach bucket of the complete histogram do
3
if it is hit by the query predicate then
4
Set the corresponding bit in Bitmap a to 1;
5
end
6 end
7 Create Bitmap b for recording all pages;
8 foreach partial histogram do
9
if it has joint buckets with Bitmap a then
10
Set the corresponding bits of the summarized
pages in Bitmap b to 1;
11
end
12 end
13 foreach page marked as 1 in Bitmap b do
14
Check each tuple in it against the predicate;
15 end

• Histogram-based Page Summary: The page summary in each index entry is a partial histogram
that represents a subset of the complete height balanced histogram buckets (maintained by the underlying DBMS). Each bucket if exists indicates that at
least one of the tuples of this bucket exists in the summarized pages. Each partial histogram represents the
distribution of the data in the summarized contiguous
pages. Since each bucket of a height balanced histogram roughly contains the same number of tuples,
each of them has the same probability to be hit by a
random tuple from the table. Hippo leverages this feature to handle data which has various or even skewed
distribution. To save storage space, only bucket IDs
are kept in partial histograms and partial histograms
are stored in a compressed bitmap format. For instance, the partial histogram of the first Hippo index
entry in Figure 1 is stored as 01110. Each bit, set to 1
or 0, reflects whether the corresponding bucket exists
or not.

3.1 Convert query predicates
The main idea is to check each partial histogram against
the given query predicate for filtering false positives and
so speeding up the query. However, as explained in Section 2, partial histograms are stored in bitmap formats without recording value ranges of buckets. Therefore, there has
to be an additional step to recover the missing information
for each partial histogram on-the-fly or convert the predicate to the bitmap format per query. Obviously, the later
one is more efficient.
Any query predicates for a particular attribute can be
broken down into atomic units: equality query predicate
and range query predicate. Age = 55 is a typical equality
query predicate while age > 55 is a range query predicate.
These unit predicates can be combined together by AND
operator like age > 55 AND age <65.
Each unit predicate is compared with the buckets of the
complete height balanced histogram (retrieving method is
discussed in Section 4). A bucket is hit by a predicate if
the predicate fully contains, overlaps, or is fully contained
by the bucket. Each unit predicate can hit one, at least,
or more buckets. For instance, according to the complete
histogram in Figure 1, bucket 3 whose description is 41 - 60
is hit by age = 55 while bucket 3, 4, and 5 are hit by age
> 55. This strategy is also applicable for the conjunct query
predicates. For a conjunct predicate like age > 55 and age
< 65, only buckets which are hit by all these unit predicates

Main idea. Hippo solves the aforementioned challenges as follows: (1) Each index entry summarizes many
pages and each only stores two page IDs and a compressed
bitmap.(2) Each page of the parent table is only summarized by one Hippo index entry. Hence, any updates that
occur in a certain page only affect a single independent index entry. Finally, during a query, pages whose partial histograms do not have desired buckets are guaranteed not to
satisfy certain query predicates and marked as false positives. Thus Hippo only inspects other pages that probably
satisfies the query predicate and achieves competitive query
performance.

3.

Age
1 - 20
21 - 40
41 - 60
61 - 90
91 - 120

INDEX SEARCH

The search algorithm runs in three main steps: (1) Step 1:
convert query predicates, (2) Step 2: filter false positives and
(3) Step 3: inspect possible qualified pages against the query
3

Age = 55

Partial histogram 1

(bitmap)

(bitmap)

Bucket
0
0
1
0
0

AND
0
0
1
0
0

IDs of possible qualified pages are recorded in a separate
bitmap. Each bit in this bitmap is mapped to the page at
the same position in the parent table. For instance, the bit
at position 1 in the bitmap is mapped to the page ID 1 of the
parent table. The value (1 or 0) of this bit reflects whether
the associate page is a possible qualified page or not.
Hippo has to inspect all of the possible qualified pages
recorded in the bitmap against the query predicate one by
one because every retained page from the previous step is
possible to contain qualified tuples. The only way to inspect
these possible qualified pages is to traverse them and check
each tuple in each page one by one. Qualified tuples are
returned to the DBMSs.
Algorithm 1 shows the three steps of Hippo index search.
The right part of Figure 1 describes how to search Hippo
index using a certain query predicate. Firstly, Hippo finds
query predicate age = 55 hits bucket 3. And the first one of
the three partial histograms nicely contains bucket 3. Thus
only the disk pages 1 - 10 are selected as possible qualified
pages which need further inspection. It is also worth noting
that these partial histograms summarize different number of
pages.

Bucket
0
1
1
1
0

Bitwise AND = 1
Figure 3: Bitwise AND two bitmaps to find joint buckets

simultaneously (the joint bucket 3 and 4) are kept as the final
result and others are directly discarded. Figure 2 shows the
hit buckets of three query predicates. Afterwards, the given
query predicate is converted to a bitmap. Each bit in this
bitmap reflects whether the bucket has the corresponding
ID is hit (1) or not (0). Thus the corresponding bits of all
hit buckets are set to 1.

3.2 Filter false positives
4. INDEX INITIALIZATION

Filtering false positives is the most important step of
Hippo index search. Each Hippo index entry stores a page
range and a summary of several contiguous pages but it is
very possible that none of these pages in the certain index
entry contain the qualified tuples especially for small range
queries. This kind of pages and their associated index entries are false positives. This step is to check each partial
histogram against the converted query predicate, recognize
some false positive pages utmost and finally avoid worthless
page inspection on these pages.
A given query predicate hits one ,at least, or more buckets
of the complete histogram. Pages whose partial histograms
contain the hit buckets (the corresponding bitmap bits are
1) might have qualified tuples, whereas pages whose partial histograms don’t contain these buckets (the corresponding bitmap bits are 0) are guaranteed not to contain qualified tuples. The former kind of pages are possible qualified
pages. In contrast, the later kind of pages are false positives
and excluded from the next step - inspect possible qualified
pages. The straight way to find false positive pages is to
do a nested loop between each partial histogram and the
converted query predicate to find the joint buckets.
Interestingly, because both of partial histograms and the
converted query predicate are in bitmap format, the nested
loop can be accelerated by bitwise ’AND’ing the bytes from
both sides, aka bit-level parallelism. If bitwise ’AND’ing
the two bytes from both sides returns 1, that means there
are joint buckets between the query predicate and the partial histogram. Thus the pages are possible qualified pages.
Figure 3 provides an example of how to perform a bitwise
AND using the same data in Figure 1.

Hippo performs three main steps to initialize itself:
(1) Retrieve a complete histogram, (2) Generate partial histograms, and (3) Group similar pages into page ranges, described as follows.
Algorithm 2: Hippo index initialization
Data: Pages of a parent table
Result: Hippo index
1 Create a working partial histogram (in bitmap format);
2 Set StartPage = 1 and EndPage = 1;
3 foreach page do
4
Find distinct buckets hit by its tuples;
5
Set associated bits to 1 in the partial histogram;
6
if the working partial histogram density > threshold
then
7
Store the partial histogram and the page range
(StartPage and EndPage) as an index entry;
8
Create a new working partial histogram;
9
StartPage = EndPage + 1;
10
EndPage = StartPage;
11
else
12
EndPage = EndPage + 1;
13
end
14 end

4.1 Retrieve a complete histogram
Histograms used in Hippo include a complete height balanced histogram and many partial histograms. A complete
height balanced histogram represents the distribution of all
tuples and already exists in DBMSs. Respectively, a partial
histogram, as a subsection, only contains partial buckets
from the complete histogram. Therefore, for generating any
partial histograms, a complete histogram should be retrieved
at the first priority. Full-fledged functions for retrieving a
complete histogram exist in any DBMSs. Detailed explanation for these functions is omitted in this paper since it is

3.3 Inspect possible qualified pages
The previous step recognizes many false positive pages
and excludes them from possible qualified pages. However,
one fact is that not all false positives can be detected by
the previous step. Possible qualified pages still may contain
false positives and this is why they are called ”possible”.
This step is to inspect the tuples in each possible qualified
pages and retrieve the qualified tuples directly.
4

histograms into one larger partial histogram (in other words,
summarizing more pages within one partial histogram) can
make Hippo more efficient. On the other hand, users may
want to shrink Hippo physically to a greater extent. For
example, if a partial histogram can summarize 10 pages in
one go, the new Hippo size will be much smaller. Grouping
more pages into one page range and summarizing them with
just one partial histogram are expected and practical as well.
Yet, this is not saying that all pages should be grouped
together and summarized by one merged partial histogram.
As more and more pages are summarized, this partial histogram contains more and more buckets until all buckets
from the complete histogram are included. At this moment,
this partial histogram becomes a complete histogram and
covers any possible query predicates. That means this kind
of partial histograms is unable to help Hippo to filter the
false positives and the disk pages summarized by this partial histogram will be always treated as possible qualified
pages.
One strategy is to group a fixed number of contiguous
pages per range/partial histogram. Yet, this strategy is
not suitable if some contiguous pages in a certain area have
much more similar data distribution than other areas. Lacking the awareness of data distribution cannot reduce storage overhead smartly. Under this circumstance, it is better
to let Hippo group more pages together in this area and
group less pages together in other areas dynamically. For
instance, assume original pages per partial histogram is 100.
If there are 1000 out of 10000 disk pages and the tuples in
these 1000 pages are exactly same, a better way to shrink
the index size is to set the P from 100 to 1000 for grouping/summarizing these 1000 pages into one range/partial
histogram and change it back to 100 for other 9000 pages.
A terminology - partial histogram density is introduced
here. The density of a partial histogram is the percentage
of kept buckets in the total buckets of a complete histogram.
The complete histogram has a density value of 1. The definition can be formalized as follows:

not our focus. We also assume that the complete histogram
is not changed at any time because the global distribution
of the parent table will not be affected even if some local
updates are performed.
The resolution of the complete histogram (denoted as histogram resolution) is adjustable. A complete histogram is
considered as higher resolution if it contains more buckets.
The resolution of partial histograms is consistent with their
complete histograms technically. It is apparent that a complete histogram will have larger physical size if it has higher
resolution and, accordingly, the numerous partial histograms
are also physically larger than the low resolution ones. On
the other hand, the histogram resolution also affects Hippo
query time. The cost estimation section will further discuss
this issue.

4.2 Generate partial histograms
A partial histogram only contains some buckets from the
complete histogram. It is used to represent the distribution
of parent tuples in one or many disk pages. In other words,
people can get an approximate overview from the partial
histogram of these pages: What values might lie in these
pages and what do not. These partial histograms are able
to help Hippo to recognize false positives utmost and avoid
worthless page inspection. We explain how to generate a
partial histogram for each disk page in this section.
Generating partial histograms traverses all disk pages of
the parent table from the start to end. For each page, a
nested loop passes through each tuple in this page. The
specified attribute value is extracted from each tuple and
compared with the complete histogram (using a binary
search). Buckets hit by tuples are kept for this page and
then compose a partial histogram. A partial histogram only
contains distinct buckets. For instance, there is a group of
age values like the first entry of Hippo shown in Figure 1:
21, 22, 55, 75, 77. Bucket 2 is hit by 21 and 22, bucket 3 is
hit by 55 and bucket 4 is hit by 77. Therefore, the partial
histogram for these values is just as partial histogram 1 in
Figure 1.
Shrinking the physical size of partial histograms is desirable. The basic idea is to drop all bucket value ranges and
only keep bucket IDs. Hippo in Figure 1 shows the effect.
Actually, as mentioned in Section 2, dropping value range
information does not impact much on the index search. To
further shrink the size, storing bucket IDs in integer type (4
bytes or more) is also considered as an overhead. Bitmap format storage is a better choice to bypass this overhead. Each
partial histogram is stored as a bitmap. Each bit in a bitmap
stands for a bucket at the same position in a complete histogram. Bit value 1 means the associated bucket is hit and
kept in this partial histogram while 0 means the associated
bucket is not included. Bitmap compression is introduced to
Hippo as well. The partial histogram in a bitmap format can
be compressed by any existing compression techniques. The
time of compressing and decompressing partial histograms is
ignorable in contrast to that of inspecting possible qualified
pages.

P artial histogram density =

Bucketspartial histogram
Bucketscomplete histogram

This density has an important phenomenon that, for a group
of contiguous pages, their merged partial histogram density
will be very low if these pages are very similar, vice versa.
Therefore, a partial histogram with a certain density may
summarize more pages if these contiguous pages have similar data, vice versa. Making use of this phenomenon enables Hippo to dynamically group pages and merge partial
histograms into one. In addition, it is understandable that
a lower density partial histogram (summarizes less pages)
has the high probability to be recognized as false positives
so that speed up queries.
User can easily set a same density for all partial histograms as a threshold. Each partial histogram can automatically decide how many pages it should summarize.
Algorithm 2 depicts how to initialize a Hippo and summarize more pages within one partial histogram with the help
of the partial histogram density. The basic idea is that new
pages will not be summarized into a partial histogram if
its density is larger than the threshold and a new partial
histogram will be created for the following pages.
Figure 1’s left part depicts how to initialize a Hippo on
the age table with a partial histogram density 0.6. All of

4.3 Group similar pages into page ranges
Generating a partial histogram per one disk page is as easy
as that in Section 4.2. However, for some contiguous pages
which have similar data, it is a waste of storage. Grouping
them together as many as possible and merging their partial
5

Algorithm 3: Update Hippo for data insertion
Data: A new inserted tuple belongs to Page a
Result: Updated Hippo
1 Find the bucket hit by the inserted tuple;
2 Locate a Hippo index entry which summarizes Page a;
3 if one index entry is located then
4
Retrieve the associated Hippo index entry;
5
Update the retrieved entry if necessary;
6 else
7
Retrieve the Hippo entry summarizes the last page;
8
if the partial histogram density < threshold then
9
Summarize Page a into the retrieved entry;
10
else
11
Summarize Page a into a new entry;
12
end
13 end

Updated Hippo
S	
 t
e #
L

H



Page range Partial histogram Internal data
1 - 10
2,3,4
21,22,55,75,77
Blank space
2 - 3
12
111221111
11  2

124

Mve

132324 29192

Figure 4: Hippo Index Entries Sorted List

time-consuming if every entry is retrieved from disk, deserialized and checked against the target page. Therefore,
a binary search on Hippo index entries is a good choice.
(This search actually leverages the index entries sorted list
explained in Section 5.3.)
Step 3: Update the index entry: If the new tuple
belongs to a new page not summarized by any Hippo index
entries and the density of Hippo partial histogram which
summarizes the last disk page is smaller than the density
threshold set by users, this new page will be summarized
into this partial histogram in the last index entry otherwise
a new partial histogram will be created to summarize this
page and stored in a new Hippo index entry. For a new tuple
belongs to pages already summarized by Hippo, the partial
histogram in the associated index entry will be updated if
the inserted tuple hits a new bucket.
It is worth noting that: (1) Since the compressed bitmaps
of partial histograms may have different size, the updated
index entry may not fit the space left at the old location.
Thus the updated one may be put at the end of Hippo.
(2) After some changes (replacing old or creating new index
entry) in Hippo, the corresponding position of the sorted list
needs to be updated.

the tuples are compared with the complete histogram and
IDs of distinct buckets hit by tuples are generated as partial
histograms along with page range.
So far, as Figure 1 shows, each entry in a Hippo index has the following content: a partial histogram in compressed bitmap format and two integers stand for the first
and last pages summarized by this histogram (summarized
page range). Each entry is serialized and stored on disk.

5.

inte

INDEX MAINTENANCE

Inserting (deleting) tuples into (from) the indexed table
requires maintaining the index to ensure that the DBMS
can retrieve the correct set of tuples that match the query
predicate. However, the overhead of maintaining the index
quite frequently may preclude system scalability. This section explains how Hippo handles updates.

5.2 Data deletion

5.1 Data insertion

The eager update strategy is not highly desired for data
deletion. Hippo still ensures the correctness of queries even
if it doesn’t update itself at all after deleting tuples from a
table. This benefits by inspecting possible qualified pages
in index search. Pages used to have qualified tuples might
be still marked as possible qualified pages but they are discarded after being inspected against the query predicates. A
periodic update or bulk update will be a good choice here.
For data deletion, Hippo adopts a lazy update strategy
that maintains the index after a bulk of delete operations.
In such case, Hippo traverses each index entry from the start
to end. For each index entry, Hippo inspects the header of
each summarized page for seeking notes made by DBMSs
(e.g., PostgreSQL makes notes in page headers if data is
removed from pages). Hippo re-summarizes the entire index
entry instantly within the original page range if data deletion
on one page is detected. The re-summarization follows the
same steps in Section 4. It is worth noting that this updated
Hippo index entry is not leading to the update on the sorted
list because the updated partial histogram, having same or
less buckets, can obtain same or less compress bitmap size
and the new index entry certainly fits the old space.

Hippo should instantly update or check the index at least
after inserting one record into the indexed table. Otherwise, all subsequent queries might miss the newly inserted
tuple since it is not reflected by the index. Therefore, Hippo
adopts an eager update strategy when a new tuple is inserted. Data insertion may change the physical structure
of a table. The new tuple may belong to any pages of the
indexed table. The insertion procedure (See Algorithm 3)
performs the following steps: (1) Find buckets hit by the
new tuple, (2) Locate the affected index entry, and (3) Update the index entry if necessary.
Step 1: Find buckets hit by the new tuple: Similar
with some steps of generating partial histogram in index
initialization, after retrieving the complete histogram, the
newly inserted tuple is checked against it using a binary
search and a bucket hit by this new tuple is found.
Step 2: Locate the affected index entry: The new
tuple has to belong to one page in this table. This page may
be a new one which has not been summarized by any partial
histograms before or an old one which has been summarized.
However, because the numbers of pages summarized by each
histogram are different, searching Hippo index entry to find
the one contains this target page is inevitable. From the
perspective of disk storage, in a Hippo, all partial histograms
are stored on disk in a serialized format. It will be extremely

5.3 Index Entries Sorted List
When a new tuple is inserted, Hippo executes a fast binary
search (according to the page IDs) to locate the affected
6

Term
H
D
P
T
Card
pageCard
SF

Qy  ate 1

Definition
Complete histogram resolution which means the
number of buckets in this complete histogram
Partial histogram density which is an user supplied threshold
Pages summarized by one partial histogram for a
certain attribute
Tuples summarized by one partial histogram for
a certain attribute
Cardinality (the total number of tuples) of the
indexed table
Number of tuples in each page of the indexed table
Query selectivity factor =

Query output
Query input

Miss…
Partial histogram
O
Qy  ate 2

Figure 5: Visualize how to filter false positives

The first step of Hippo index search is to traverse Hippo
index entries. Pages in each index entry are likely to be
selected for further inspection if their associated partial histogram has joint buckets with the query predicate. Determining the probability of having joint buckets contributes
to the query time cost estimation.
For the ease of presentation, Figure 5 visualizes the procedure of filtering false positives according to their partial
histograms. Partial histogram density (D) of this index is
0.2. The complete histogram constitutes of 10 buckets in
total (H = 10). Assume the indexed table’s tuples follow an
uniform distribution based upon the key attribute. Let the
query selectivity factor (SF ) be 20%. In Figure 5, buckets
hit by the query predicates and the partial histogram are
represented in a bitmap format. According to this figure,
the partial histogram misses a query predicate if the highlighted area of the predicate falls into the blank area of the
partial histogram, whereas a partial histogram is selected if
the predicate does not fall completely into the blank area of
the histogram. In other words, the probability of a partial
histogram having joint buckets with a predicate depends on
how likely a predicate doesn’t fall into the blank area of
a partial histogram. The probability is determined by the
formula given below (The terms are defined in Table 2):

* 100%

Table 2: Notations used in Cost Estimation

index entry and then updates it. Since the index entries are
not guaranteed to be sorted based on the page IDs (noted in
data insertion section), an auxiliary structure for recording
the sorted order is introduced to Hippo.
The sorted list is initialized after all steps in Section 4
with the original order of index entries and put at the first
several index pages of Hippo. During the entire Hippo life
time, the sorted list maintains a list of pointers of Hippo
index entries in the ascending order of page IDs. Actually
each pointer represents the fixed size physical address of
an index entry and these addresses can be used to retrieve
index entries directly. That way, the premise of a binary
search has been satisfied. Figure 4 depicts the Hippo index
entries sorted list. Index entry 2 in Figure 1 has a new
bucket ID 1 due to a newly inserted tuple in its internal
data and hence this entry becomes the last index entry in
Figure 4. The sorted list is still able to record the ascending
order and help Hippo to perform a binary search on the
index entries. In addition, such sorted list leads to slight
additional maintenance overhead: Some index updates need
to modify the affected pointers in the sorted list to reflect
the new physical addresses.

6.

P rob = (Buckets hit by a query predicate) ∗ D
= (SF ∗ H) ∗ D

(1)

To be precise, P rob follows a piecewise function as follows:

COST ESTIMATION

P rob =

This section gives a detailed cost estimation of Hippo.
We first provide an accurate query time cost model which
assists the DBMS query optimizer in picking an efficient
query execution plan. Estimating the storage overhead of
an index can also facilitate better disk space management
and planning. Index initialization certainly consumes a large
chunk of time. Similarly, index maintenance can present a
significant time overhead in any write-intensive application.
Both of them should be carefully estimated.
Table 2 summarizes the main notations we use to derive
the cost model. Given a database table R with Card number
of tuples (i.e., cardinality) and average number of tuples per
disk page equal to pageCard, a user may create a Hippo
index on attribute ai of R. When initializing the index,
Hippo sets the complete histogram resolution to H (it has
H buckets in total) and the partial histogram density to D.
Assume that each Hippo index entry summarizes P indexed
table pages (in terms of pages)/ T tuples (in terms of tuples).
P and T vary for each index entry. Queries executed against
the index have average selectivity factor SF .

(

(SF ∗ H) ∗ D
1

1
S∗H 6 D
1
SF ∗ H > D

SF ∗ H ∈ {1, 2, 3, 4, ...}
SF ∗ H should be no smaller than 1 no matter how small
SF is. Because the query predicate at least hits one bucket
of the complete histogram. Therefore, the probability in
Figure 5 is 20% × 10 × 0.2 = 40%. That means pages summarized by each index entry have 40% probability to be selected as possible qualified pages. Given the aforementioned
discussion, we observe the following:
Observation 1: When SF and H are fixed, the smaller
D is, the smaller P rob is.
Observation 2: When H and D are fixed, the smaller
SF is, the smaller P rob is.
Observation 3: When SF and D are fixed, the smaller
H is, the smaller P rob is.
In fact, the probability given above is equal to the percentage of inspected tuples in all tuples. In addition, considering
that Hippo index entries are much less than the inspected
tuples of the parent table, the total query time cost estimation is mainly decided by the time spent on inspecting
possible qualified pages. Thus, the query time cost estimation (in terms of disk I/O) can be concluded as follows:

6.1 Query time
7

Query time = (P rob ∗ Card)

(2)

Hippo index entries =

If we substitute P rob with its piecewise function, the
query time cost is as follows:

=

Card
T
1
H ∗ (H
+

(5)
Card
+ ... +

1
H−1

1
)
H−D∗H+1

(6)
Query time =

(

(SF ∗ H) ∗ D ∗ Card
Card

SF ∗ H 6
SF ∗ H >

1
D
1
D

D ∈ [ pageCard
, 1]
H
Some observations can be obtained from Formula 6:
Observation 1 For a certain H, the higher D there is,
the less Hippo index entries there are.
Observation 2 For a certain D, the higher H there is,
the less Hippo index entries there are. Meanwhile, the size
of each Hippo index entry is increasing with the growth of
the complete histogram resolution.
Index initialization time hinges on the number of disk I/Os
because it takes much more time than memory I/Os. In
general, the initialization time is composed of two parts:
retrieve parent tuples one by one and write index entry to
disk one by one. Accordingly, Hippo initialization time can
be deduced as follows:

SF ∗ H ∈ {1, 2, 3, 4, ...}

6.2 Indexing overhead
Indexing overhead which consists of storage overhead and
initialization time highly hinges on the number of index entries in an index. The more index entries there are, the more
disk writes and storage space an index costs. B+ -Tree and
other indexes take huge disk space and time for storing their
substantial nodes one by one.
The first problem in estimating the number of Hippo index
entries is that: how many disk pages (P ) are summarized by
one partial histogram in general? Or, how many tuples (T )
are checked against the complete histogram for generating
one partial histogram? Interestingly, this problem is very
similar with Coupon Collector’s Problem[6]. This problem
can be described like that: ”A vending machine sells H types
of coupons (a complete histogram with H buckets). Alice
is purchasing coupons from this machine. Each time (each
tuple) she can get a random type coupon (a bucket) but
she might already have a same one. Alice keeps purchasing
until she gets D ∗ H types of coupons (distinct buckets).
How many times (T ) does she need to purchase?”
Therefore, the expectations of T and P are determined by
the following formulas (The terms are defined in Table 2):
H
H
H
H
+
+
+ ... +
H
H −1
H −2
H −D∗H +1
1
1
1
+ ... +
)
=H ∗( +
H
H −1
H −D∗H +1
T
P =
pageCard

Hippo initialization time = Card + Hippo index entries
(7)
The number of Hippo index entries mentioned in the formula
above can be substituted by its mathematical expectation
in Formula 6.

6.3 Maintenance time
Data insertion. Hippo updates itself eagerly for data
insertion so that this operation is relatively time sensitive.
There are five steps cost disk I/Os in this update: retrieve
the complete histogram, locate associated Hippo index entry, retrieve the associated index entry, update the index
entry (if necessary) and update the mapped sorted list element. It is not hard to conclude that locating the associated index entry completes in log(Hippo index entries) I/O
times, whereas other four steps are able to accomplish their
assignments in constant I/O times. Thus the data insertion
time cost estimation model is summarized as follows under
different conditions:

T =

(3)
(4)

, 1]
D ∈ [ pageCard
H
The product of D ∗ H is the actual number of buckets
in each partial histogram. This value should be no smaller
than the tuples per disk page (pageCard) in case that each
tuple in a certain page hit one unique bucket.
For instance, in a Hippo, the complete histogram has 1000
buckets in total and the partial histogram density is 0.1. The
105.3
expectation of T and P will be 105.3 and pageCard
respectively. That means each partial histogram may summarize
105.3
pages under this circumstance. In another exampageCard
ple, if the total number of buckets is 10000 and the density
2230
is 0.2, T and P will be 2230 and pageCard
correspondingly.
After being aware of the expectation of the number of P ,
it is not hard to deduce the approximate number of index
entries in a Hippo. Thus the estimation of Hippo index
entries number is Formula 5. If we substitute T with their
mathematical expectations in Formula 3 and Formula 5 will
be changed to Formula 6. Hippo index size is equal to the
product of the number of index entries and the size of one
entry which roughly depends on each partial histogram size
(in compressed bitmap format).

Data insert time = log(Hippo index entries) + 4

(8)

Hippo index entries mentioned in Formula 8 can be substituted by its mathematical estimation in Formula 6.
Data deletion. Hippo updates itself lazily for data deletion so that it is hard to finalize a general estimation model.
However, it is recommended that do not update Hippo for
data deletion too frequently because Hippo will re-traverse
and re-summarize all disk pages summarized by one Hippo
index entry once it detects that one disk page has data deletion. This algorithm is more suitable for bulk deletion and
lazy update strategy.

7. EXPERIMENTS
This section provides extensive experiments of Hippo
along with reasonable analysis for supporting insights discussed before. For the ease of testing, Hippo has been implemented into PostgreSQL 9.5 kernel. All the experiments
are completed on PostgreSQL.
8

2
1
0

25x

25x

9

B+-Tree

Hippo

2.8x

6
3
2x

1.5x

0
2
20
200
TPC-H workload size (GB)

x 10000000

25x

Insertion time (ms)

Hippo

x 1000000

B+-Tree

Initialization time (ms)

x 10000

Index Size (MB)

3

5

Hippo

1200x

2
1
0
2
20
200
TPC-H workload size (GB)

2
20
200
TPC-H workload size (GB)

(a) Index size

B+-Tree

(c) Index insertion time

(b) Index initialization time

2
1
0

5

B+-Tree

Hippo

4
3
2
1
0

x 1000000

Hippo

Query time (ms)

B+-Tree

x 100000

5

Query time (ms)

x 10000

Query time (ms)

Figure 6: Index overhead on different TPC-H workload size
5

B+-Tree

Hippo

4
3
2
1
0

0.001%
Query selectivity (%)

0.01%
0.1%
1%
Query selectivity (%)

(a) 2 GB

(b) 20 GB

0.001%

0.01%
0.1%
1%
Query selectivity (%)

(c) 200 GB

Figure 7: Index query time on different TPC-H workload size
It is also worth noting that the final Hippo implementation in PostgreSQL has some slight differences from the
details above caused by some platform-dependent features
as follows:
Automatically inspect pages: Hippo only records possible qualified page IDs in a tid bitmap format and returns it
to the kernel. PostgreSQL will automatically inspect pages
and check tuples against query predicates.
Store the complete histogram on disk: Compared
with other disk operations, retrieving the complete histogram from PostgreSQL system cache is relatively slow so
that Hippo stores it on disk and executes a binary search on
it when query or update for data insertion and deletion. It
is better to rebuild Hippo index if there is a huge change of
the parent attribute’s histogram.
Vacuum tables to physically delete data: PostgreSQL DELETE command does not really remove data
from disk unless a VACUUM command is called automatically or manually. Thus Hippo will update itself for data
deletion when a VACUUM command is called.

Datasets and Workload. We use TPC-H workload in
the experiments with different scale factors (2, 20, 200). The
corresponding dataset sizes are 2 GB, 20 GB and 200 GB.
All TPC-H data follows an uniform distribution. We use
the largest table of TPC-H workload - Lineitem table in
most experiments and it has three corresponding sizes: 1.3
GB, 13.8 GB and 142 GB. We compare the query time of
Hippo with B+ -Tree through different query selectivity factors (0.001%, 0.01%, 0.1% and 1%). In addition, we also
test the two indexes using TPC-H standard queries 6, 15
and 20. We use TPC-H standard refresh operation (insert
0.1% new tuples into the DBMS) to test the maintenance
overhead of B+ -Tree and Hippo.
Experimental Setup. The test machine has 8 CPUs
(3.5 GHz per core), 32 GB memory, and 2 TB magnetic
disk with PostgreSQL 9.5 installed. Unless mentioned otherwise, Hippo sets the default partial histogram density to
20% and the default histogram resolution to 400. The impact of parameters is also discussed.

7.1 Implementation Details

7.2 Pre-tune Hippo parameters

We have implemented a prototype of Hippo inside the core
kernel of PostgreSQL 9.5 as one of the main index access
methods by leveraging the underlying interfaces which include but not limited to ”ambuild”, ”amgetbitmap”, ”aminsert” and ”amvacuumcleanup”. A database user is able to
create and query a Hippo index as follows:

Hippo is a flexible index which can be tuned by the
database user to perfectly fit his specific scenarios. There are
two parameters, partial histogram density D (Default value
is 20%) and complete histogram resolution H (Default value
is 400), discussed in this section. Referring to the estimation
before, both of them have impacts on index size, initialization time, and query time. For these experiments, we build
Hippo and B+ -tree on ”partkey” attribute in Lineitem table
of 200 GB TPC-H workload. As mentioned in Introduction,
B+ -Tree has 25 GB index size at this time.

CREATE INDEX hippo_idx ON lineitem USING hippo(partkey)
SELECT * FROM lineitem
WHERE partkey > 1000 AND partkey < 2000

7.2.1 Impact of partial histogram densities

DROP INDEX hippo_idx

9

3
2
1
0

B+-Tree

Hippo

4
3
2
1
0
400
1600
Hippo histogram resolution

Figure 8: Partial histogram density

Figure 9: Histogram resolution

Density (D)
Resolution (R)

6

B+-Tree

Hippo

4
2
0

20%
40%
Hippo partial histogram density (%)

Parameter
Default

x 1000000

4

5

Query time (ms)

Hippo

x 1000000

B+-Tree

Query time (ms)

x 1000000

Query time (ms)

5

Value
D=20% R=400

Size
1012 MB

Initial. time
2765 sec

40%

680 MB

2724 sec

80%

145 MB

2695 sec

800

822 MB

2762 sec

1600

710 MB

2760 sec

Q6
-

Figure 10: TPC-H standard queries

impact on the index size and initialization time is given in
Table 3 and the impact on query time is depicted in Figure 9.
As Table 3 illustrates, with the growth of histogram resolution, Hippo size reduces moderately. The explanation
is that Hippo which has higher histogram resolution consists of less partial histograms and each partial histogram in
this Hippo may summarize more pages but the partial histogram (in bitmap format) has larger physical size because
the bitmap has to store more bits.
As Figure 9 shows, the query time of three Hippos varies
with the growth of histogram resolution. This is because for
the large histogram resolution, the query predicate may hit
more buckets so that this Hippo is more likely to overlap
with query predicates and result in more pages are selected
as possible qualified pages. At this selectivity factor, Hippo
which has histogram resolution 400 is just a little bit worse
than B+ -Tree in terms of query time.

Table 3: Parameters affect Hippo indexing overhead

Hippo introduces a terminology ”partial histogram density” to dynamically control the number of pages summarized by one partial histogram. Based on the discussion
before, the partial histogram density may affect Hippo size,
initialization time and query time. The following experiment compares the default Hippo density (20%) with two
different densities (40% and 80%) and tests their query time
with selectivity factor 0.1%. According to the discussion in
Section 6.2, partial histograms under the three different density setting may summarize around 2 pages, 5 pages and 17
pages respectively (if one page contains 50 tuples). Thus it
can be estimated that the index size of 20% density Hippo
is around 2 times of 40% density Hippo and 8 times of 80%
density Hippo. The impact of the density on Hippo size and
initialization time is described in Table 3 and the impact on
query time is described in Figure 8.
It can be observed that as we increase the density, Hippo
indexing overhead decreases as expected (up to two orders
of magnitude smaller than B+ -Tree in terms of storage) because Hippo is able to summarize more pages per partial
histogram and write less index entries on disk. Similarly,
Hippo which has higher density costs more query time because it is more likely to overlap with query predicates and
result in more pages are selected as possible qualified pages.
At this selectivity factor, Hippo which has density 20% is
just a little bit worse than B+ -Tree in terms of query time.

7.3 Compare Hippo to B+ -Tree
This section compares Hippo with B+ -Tree in terms of
indexing overhead (index size and initialization time), index
maintenance overhead and index query time. To further illustrate the advantages of Hippo, we also compare these indexes using TPC-H standard queries. Hippo tested in this
section uses the default setting which has histogram resolution 400 and partial histogram density 20%.

7.3.1 Indexing overhead
The following experiment builds B-Tree and Hippo on
attribute ”partkey” in Lineitem table of TPC-H workload
(2 GB, 20 GB and 200 GB) and measures their indexing
overhead including index size and index initialization time.
Hippo only stores disk page pointers along with their summaries so that it may have much less index entries in contrast
with B+ -Tree. Thus it is not difficult to understand that
Hippo remains an index size which is lower than B+ -Tree.
In addition, referring to the discussion in the initialization
time estimation model, Hippo initialization time should be
far less than B+ -Tree because B+ -Tree has numerous nodes
to be written to disk.
As Figure 6a illustrates, the index size increases with the
growth of data size. The index size of Hippo is around
25 times smaller than that of B+ -Tree on all workload
sizes. Thus Hippo significantly reduces the storage overhead. Moreover, as Figure 6b shows, Hippo index initialization is at least 1.5x faster that of B+ -Tree.

7.2.2 Impact of histogram resolutions
Each partial histogram of Hippo is composed of some
buckets from the complete histogram. The number of buckets in this complete histogram represents the histogram resolution. The more buckets there are, the higher resolution
the complete histogram has. According to the discussion
before, the histogram resolution may affect index size, initialization time and query time. The following experiment
compares the default Hippo histogram resolution (400) with
two different histogram resolutions (800 and 1600) and tests
their query time with selectivity factor 0.1%. The density

7.3.2 Index maintenance overhead
Hippo updates itself eagerly after inserting a tuple into the
parent table. This eager update strategy for data insertion
10

is also adopted by B+ -Tree so that the two indexes can be
compared together. In terms of update time complexity, B+ Tree has approximate log(Card) and Hippo has (log(Hippo
index entries) + 4). Thus it can be predicted that, for inserting same percentage of tuples, while the update time of
Hippo and B+ -Tree is increasing with the growth of data
size. Hippo will take much less time to update itself than
B+ -Tree because Card is much larger than the number of
Hippo index entries. And also the difference of update
time between Hippo and B+ -Tree will be larger on larger
workload. The experiment uses TPC-H Lineitem table and
creates B+ -Tree and Hippo on attribute ”partkey”. Afterwards, TPC-H refresh transaction which inserts 0.1% new
tuples into Lineitem table is executed. The insertion time
of the indexes is compared in Figure 6c.
As Figure 6c shows, the two indexes take more time to
update on large workload. And also the difference between B+ -Tree and Hippo is more obvious (1200x) on the
largest workload as expected. This is because B+ -Tree
spends much more time on searching proper tuple insert
location (log(Card)) and its update time is increasing with
the growth of TPC-H workload.
Hippo updates itself lazily after deleting data which means
it updates itself after many data deletions occur. In contrast,
B+ -Tree takes an eager update strategy which has around
log(Card) update time cost. It may not make much sense
to compare the two indexes for data deletion.

Index
type

Fast
Query

Guaranteed
Accuracy

Low
Storage

Fast
Maintenance

B+ -Tree

✓

✓

✗

✗

Compressed

✗

✓

✓

✗

Approximate

✓

✗

✓

✗

Sparse

✗

✓

✓

✓

Hippo

✓

✓

✓

✓

Table 4: Compared Indexing Approaches

tor is 0.1%. Thus we find three TPC-H queries which have
typical range queries on ”l shipdate” attribute (Query 6, 15
and 20) and set the range query selectivity factor to 0.1%
which means one week. The query plans of the three queries
are described as follows:
Query 6 This query has a very simple plan. It firstly
performs an index search on Lineitem table using one of
the candidate indexes, then filters the returned values and
finally aggregates the values to calculate the result.
Query 15 This query builds a sub-view beforehand and
embeds it into the main query twice. The range query which
leverages the candidate indexes is a part of the sub-view.
Query 20 The candidate indexes are invoked in a subquery. Then range query results are sorted and aggregated
for calculation. The result is cached into memory and used
the upper level query.
As Figure 10 depicts, Hippo consumes similar query time
with B+ -Tree on Query 6, 15 and 20. The difference between the two indexes is more obvious on Query 15 because
this query invokes the range query twice. Therefore, we
may conclude that Hippo may achieve almost similar query
performance with B+ -Tree at the 25 times smaller storage
overhead when the query selectivity factor is 0.1%.

7.3.3 Impact of query selectivity factors
In this experiment, the query selectivity factors used for
B+ -Tree and Hippo are 0.001%, 0.01%, 0.1% and 1%. According to the query time cost estimation of Hippo, the corresponding query time costs in this experiment are 0.2Card,
0.2Card, 0.2Card and 0.8Card. Therefore, it can be predicted that there will be a great time gap between the first
three Hippo queries and the last one Hippo query. On the
other hand, B+ -Tree should be faster than Hippo at low
query selectivity factor like 0.001% but the difference between the two indexes should be narrowed with the growth
of query selectivity factors.
The result in Figure 7 perfectly matches our predication:
the last Hippo query consumes much more time than the
first three queries. Among them, query time of 0.1% selectivity factor query is a little higher than the first two because
it returns more query results which costs more to retrieve.
Both indexes cost more time on queries with the decreasing of query selectivity factors. B+ -Tree has almost similar
query time with Hippo at 0.1% query selectivity factor. It
is worth noting that B+ -Tree consumes 25 times more storage than Hippo. Therefore, we may conclude that Hippo
makes a well tradeoff between query time and index storage
overhead on medium query selectivity factors like 0.1% so
that, under this scenario, Hippo is a good substitution for
B+ -Tree if the database user is sensitive to aforementioned
index overhead.

8. RELATED WORK
Table 4 summarizes state-of-the-art database index structures in terms of query time, accuracy, storage overhead and
maintenance overhead.
Tree Index Structures: B+ -Tree is the most commonly
used type of indexes. The basic idea can be summarized as
follows: For a non-leaf node, the value of its left child node
must be smaller than that of its right child node. Each leaf
node points to the physical address of the original tuple.
With the help of this structure, searching B+ -Tree can be
completed in one binary search time scope. The excellent
query performance of B+ -Tree and other tree like indexes
is benefited by their well designed structures which consist
of many non-leaf nodes for quick searching and leaf nodes
for fast accessing parent tuples. This feature incurs two
inevitable drawbacks: (1) Storing plenty of nodes costs a
huge chunk of disk storage. As shown in Section 1, it results
in non-ignorable dollar cost and huge initialization time in
big data scenarios. (2) Index maintenance is extremely timeconsuming. For any insertions or deletions occur on parent
table, tree like indexes firstly have to traverse themselves
for finding proper update locations and then split, merge or
re-order one or more nodes which are out of date.
Compressed Index Structures: Compressed indexes
try to drop some repeated index information as much as
possible beforehand for saving space and recover it as fast
as possible upon queries from users but they all have guaranteed query accuracy. These techniques are applied to tree

7.3.4 TPC-H queries
To further explore the query performance of Hippo in the
real business decision support, we compare Hippo with B+ Tree using TPC-H standard queries. Both of the two indexes are built on ”l shipdate” attribute in Lineitem table
of 200 GB workload. As discussed before, Hippo costs similar query time with B+ -Tree when the query selectivity fac11

10. REFERENCES

indexes [8, 9] and bitmap indexes [7, 11, 14, 19] (low cardinality and read-only datasets). Though compressed indexes
are storage economy, they require additional time for compressing beforehand and decompressing on-the-fly. Compromising on the time of initialization, query and maintenance
is not desirable in many time-sensitive scenarios. Hippo on
the other hand reduces the storage overhead by dropping redundancy tuple pointers and hence still achieves competitive
query response time.
Approximate Index Structures: Approximate indexes [2, 10, 12] give up the query accuracy and only store
some representative information of parent tables for saving
indexing and maintenance overhead and improving query
performance. They propose many efficient statistics algorithms to figure out the most representative information
which is worth to be stored. In addition, some people focus on approximate query processing (AQP)[1, 18] which
relies on data sampling and error bar estimating to accelerate query speed directly. However, trading query accuracy
makes them applicable to limited scenarios. On the other
hand, Hippo, though still reduces the storage overhead, only
returns exact answer that match the query predicate.
Sparse Index Structures: Sparse index (denoted as
Zone Map Index in IBM Data Warehouse[3], Data Pack
structure in Infobright[13], Block Range Index in PostgreSQL[15], and Storage Index in Oracle Exadata[16]) is
a simple index structure implemented by many popular
DBMS in recent years. Sparse index only stores pointers
which point to disk pages of parent tables and value ranges
(min and max values) in each page so that it can save indexing and maintenance overhead. It is generally built on
ordered attributes. For a posed query, it finds value ranges
which cover or overlap the query predicate and then rapidly
inspects the associated few parent table pages one by one
for retrieving truly qualified tuples. However, for most real
life attributes which have unordered data, sparse index has
to spend lots of time on page scanning because the stored
value ranges (min and max values) may cover most query
predicates and encumber the page inspection. Therefore, an
efficient yet concise page summarizing method (i.e., Hippo)
instead of simple value ranges is highly desirable.

9.

[1] S. Agarwal, H. Milner, A. Kleiner, A. Talwalkar,
M. Jordan, S. Madden, B. Mozafari, and I. Stoica.
Knowing when you’re wrong: building fast and
reliable approximate query processing systems. In
SIGMOD, pages 481–492. ACM, 2014.
[2] M. Athanassoulis and A. Ailamaki. Bf-tree:
Approximate tree indexing. In VLDB, pages
1881–1892. VLDB Endowment, 2014.
[3] C. Bontempo and G. Zagelow. The ibm data
warehouse architecture. CACM, 41(9):38–48, 1998.
[4] D. Comer. Ubiquitous b-tree. CSUR, 11(2):121–137,
1979.
[5] T. P. P. Council. Tpc-h benchmark specification.
Published at http://www. tcp. org/hspec. html, 2008.
[6] P. Flajolet, D. Gardy, and L. Thimonier. Birthday
paradox, coupon collectors, caching algorithms and
self-organizing search. Discrete Applied Mathematics,
39(3):207–229, 1992.
[7] F. Fusco, M. P. Stoecklin, and M. Vlachos. Net-fli:
on-the-fly compression, archiving and indexing of
streaming network traffic. VLDB J., 3(1-2):1382–1393,
2010.
[8] J. Goldstein, R. Ramakrishnan, and U. Shaft.
Compressing relations and indexes. In ICDE, pages
370–379. IEEE, 1998.
[9] G. Guzun, G. Canahuate, D. Chiu, and J. Sawin. A
tunable compression framework for bitmap indices. In
ICDE, pages 484–495. IEEE, 2014.
[10] M. E. Houle and J. Sakuma. Fast approximate
similarity search in extremely high-dimensional data
sets. In ICDE, pages 619–630. IEEE, 2005.
[11] D. Lemire, O. Kaser, and K. Aouiche. Sorting
improves word-aligned bitmap indexes. Data &
Knowledge Engineering, 69(1):3–28, 2010.
[12] Y. Sakurai, M. Yoshikawa, S. Uemura, H. Kojima,
et al. The a-tree: An index structure for
high-dimensional spaces using relative approximation.
In VLDB, pages 5–16. VLDB Endowment, 2000.
[13] D. Ślezak and V. Eastwood. Data warehouse
technology by infobright. In SIGMOD, pages 841–846.
ACM, 2009.
[14] K. Stockinger and K. Wu. Bitmap indices for data
warehouses. Data Warehouses and OLAP: Concepts,
Architectures and Solutions, page 57, 2006.
[15] M. Stonebraker and L. A. Rowe. The design of
postgres. In SIGMOD, pages 340–355. ACM, 1986.
[16] R. Weiss. A technical overview of the oracle exadata
database machine and exadata storage server. Oracle
White Paper. Oracle Corporation, Redwood Shores,
2012.
[17] K. Wu, E. Otoo, and A. Shoshani. On the performance
of bitmap indices for high cardinality attributes. In
VLDB, pages 24–35. VLDB Endowment, 2004.
[18] K. Zeng, S. Gao, B. Mozafari, and C. Zaniolo. The
analytical bootstrap: a new method for fast error
estimation in approximate query processing. In
SIGMOD, pages 277–288. ACM, 2014.
[19] M. Zukowski, S. Heman, N. Nes, and P. Boncz.
Super-scalar ram-cpu cache compression. In ICDE,
pages 59–59. IEEE, 2006.

CONCLUSION

The paper introduces Hippo a sparse indexing approach
that efficiently and accurately answers database queries
while occupying up to two orders of magnitude less storage
overhead than de-facto database indexes, i.e., B+ -tree. To
achieve that, Hippo stores pointers of pages instead of tuples
in the indexed table to reduce the storage space occupied
by the index. Furthermore, Hippo maintains histograms,
which represent the data distribution for one or more pages,
as the summaries for these pages. This structure significantly shrinks index storage footprint without compromising much on performance of common analytics queries, i.e.,
TPC-H workload. Moreover, Hippo achieves about three
orders of magnitudes less maintenance overhead compared
to the B+ -tree. Such performance benefits make Hippo a
very promising alternative to index data in big data application scenarios. Furthermore, the simplicity of the proposed
structure makes it practical for database systems vendors to
adopt Hippo as an alternative indexing technique. In the
future, we plan to adapt Hippo to support more complex
data types, e.g., spatial data, unstructured data.
12

Matrix Factorization with Explicit Trust and Distrust Relationships
Rana Forsati
Shahid Beheshti University, G.C., Tehran, Iran

Mehrdad Mahdavi
Michigan State University, Michigan, USA

r_forsati@sbu.ac.ir

mahdavim@cse.msu.edu

Mehrnoush Shamsfard
Shahid Beheshti University, G.C., Tehran, Iran

Mohamed Sarwat
University of Minnesota, Minneapolis, USA

m_shams@sbu.ac.ir

sarwat@cs.umn.edu

arXiv:1408.0325v1 [cs.SI] 2 Aug 2014

Abstract With the advent of online social networks, recommender systems have became crucial for the success of many online applications/services due to their significance role in tailoring these applications to user-specific needs or preferences. Despite their increasing popularity, in general recommender systems suffer from the data sparsity and the cold-start problems. To alleviate these issues, in recent years there has been an upsurge of interest in exploiting social information such as trust relations among users along with the rating data to improve the performance of recommender systems. The main motivation for exploiting trust information in recommendation process stems from the observation that the ideas we are exposed to and the choices we make are significantly influenced by our social context. However, in large user communities, in addition to trust relations, the distrust relations also exist between users. For instance, in Epinions the concepts of personal "web of trust" and personal "block list" allow users to categorize their friends based on the quality of reviews into trusted and distrusted friends, respectively. Hence, it will be interesting to incorporate this new source of information in recommendation as well. In contrast to the incorporation of trust information in recommendation which is thriving, the potential of explicitly incorporating distrust relations is almost unexplored. In this paper, we propose a matrix factorization based model for recommendation in social rating networks that properly incorporates both trust and distrust relationships aiming to improve the quality of recommendations and mitigate the data sparsity and the cold-start users issues. Through experiments on the Epinions data set, we show that our new algorithm outperforms its standard trust-enhanced or distrust-enhanced counterparts with respect to accuracy, thereby demonstrating the positive effect that incorporation of explicit distrust information can have on recommender systems.

1 Introduction
The huge amount of information available on the Web has made it increasingly challenging to cope with this information overload and find the most relevant information one is really interested in. Recommender systems intend to provide users with recommendations of products they might appreciate, taking into account their past ratings, purchase history, or interest. The recent proliferation of online social networks have further enhanced the need for such systems. Therefore, it is obvious why such systems are indispensable for the success of many online applications such as Amazon, iTunes and Netflix to guide the search process and help users to effectively find the information or products they are looking for [49]. Roughly speaking, the overarching goal of recommender systems is to identify a subset of items (e.g. products, movies, books, music, news, and web pages) that are likely to be more interesting to users based on their interests [13, 76, 16, 5]. In general, most widely used recommender systems (RS) can be broadly classified into contentbased (CB), collaborative filtering (CF), or hybrid methods [1]. In CB recommendation, one tries to recommend items similar to those a given user preferred in the past. These methods usually rely on the external information such as explicit item descriptions, user profiles, and/or the appropriate features extracted from items to analyze item similarity or user preference to provide recommendation.

1

In contrast, CF recommendation, the most popular method adopted by contemporary recommender systems, is based on the core assumption that similar users on similar items express similar interest, and it usually relies on the rating information to build a model out of the rating information in the past without having access to external information required in CB methods. The hybrid approaches were proposed that combine both CB and CF based recommenders to gain advantages and avoid certain limitations of each type of systems [20, 64, 55, 48, 54, 67, 15]. The essence of CF lies in analyzing the neighborhood information of past users and items' interactions in the user-item rating matrix to generate personalized recommendations based on the preferences of other users with similar behavior. CF has been shown to be an effective approach to recommender systems. The advantage of these types of recommender systems over content-based RS is that the CF based methods do not require an explicit representation of the items in terms of features, but it is based only on the judgments/ratings of the users. These CF algorithms are mainly divided into two main categories [21]: memory-based methods (also known as neighborhood-based methods) [73, 9] and model-based methods [26, 63, 65, 79]. Recently, another direction in CF considers how to combine memory-based and model-based approaches to take advantage of both types of methods, thereby building a more accurate hybrid recommender system [56, 77, 32]. The heart of memory-based CF methods is the measurement of similarity based on ratings of items given by users: either the similarity of users (user-oriented CF) [24], the similarity of items (itemsoriented CF) [61], or combined user-oriented and item-oriented collaborative filtering approaches to overcome the limitations specific to either of them [74]. The user-oriented CF computes the similarity among users, usually based on user profiles or past behavior, and seeks consistency in the predictions among similar users [78, 26]. The item-oriented CF, on the other hand, allows input of additional itemwise information and is also capable of capturing the interactions among them. If the rating of an item by a user is unavailable, collaborative-filtering methods estimate it by computing a weighted average of known ratings of the items from the most similar users. Memory-based collaborative filtering is most effective when users have expressed enough ratings to have common ratings with other users, but it performs poorly for so-called cold-start users. Coldstart users are new users who have expressed only a few ratings. Thus, for memory based CF methods to be effective, large amount of user-rating data are required. Unfortunately, due to the sparsity of the user-item rating matrix, memory-based methods may fail to correctly identify the most similar users or items, which in turn decreases the recommender accuracy. Another major issue that memory-based methods suffer from is the scalability problem. The reason is essentially the fact that when the number of users and items are very large, which is common in many real world applications, the search to identify k most similar neighbors of the active user is computationally burdensome. In summary, data sparsity and non-scalability issues are two main issues current memory based methods suffer from. To overcome the limitations of memory-based methods, model-based approaches have been proposed, which establish a model using the observed ratings that can interpret the given data and predict the unknown ratings [1]. In contrast to the memory-based algorithms, model-based algorithms try to model the users based on their past ratings and use these models to predict the ratings on unseen items. In model-based CF the goal is to employ statistical and machine learning techniques to learn models from the data and make recommendations based on the learned model. Methods in this category include aspect model [26, 63], clustering methods [30], Bayesian model [80], and low dimensional linear factor models such as matrix factorization (MF) [66, 65, 79, 59]. Due to its efficiency in handling very huge data sets, matrix factorization based methods have become one of the most popular models among the model-based methods, e.g. weighted low rank matrix factorization [65], weighted nonnegative matrix factorization (WNMF) [79], maximum margin matrix factorization (MMMF) [66] and probabilistic matrix factorization (PMF) [59]. These methods assume that user preferences can be modeled by only a small number of latent factors [12] and all focus on fitting the user-item rating matrix using low-rank approximations only based on the observed ratings. The recommender system we will propose in this paper adhere to the model-based factorization paradigm. Although latent factor models and in particular matrix factorization are able to generate high quality recommendations, these techniques also suffer from the data sparsity problem in real-world scenarios and fail to address users who rated only a few items. For instance, according to [61], the density of non-missing ratings in most commercial recommender systems is less than one or even much less. Therefore, it is unsatisfactory to rely predictions on such small amount of data which becomes more challenging in the presence of large number of users or items. This observation necessitates tackling the 2

data sparsity problem in an affirmative manner to be able to generate more accurate recommendations. One of the most prominent approaches to tackle the data sparsity problem is to compensate for the lack of information in rating matrix with other sources of side information which are available to the recommender system. For example, social media applications allow users to connect with each other and to interact with items of interest such as songs, videos, pages, news, and groups. In such networks the ideas we are exposed to and the choices we make are significantly influenced by our social context. More specifically, users generally tend to connect with other users due to some commonalities they share, often reflected in similar interests. Moreover, in many real-life applications it may be the case that only social information about certain users is available while interaction data between the items and those users has not yet been observed. Therefore, the social data accumulated in social networks would be a rich source of information for the recommender system to utilize as side information to alleviate the data sparsity problem. To accomplish this goal, in recent years the trust-based recommender systems became an emerging field to provide users personalized item recommendations based on the historical ratings given by users and the trust relationships among users (e.g., social friends). Social-enhanced recommendation systems are becoming of greater significance and practicality with the increased availability of online reviews, ratings, friendship links, and follower relationships. Moreover, many e-commerce and consumer review websites provide both reviews of products and a social network structure among the reviewers. As an example, the e-commerce site Epinions [22] asks its users to indicate which reviews/users they trust and use these trust information to rank the reviews of products. Similar patterns can be found in online communities such as Slashdot in which millions of users post news and comment daily and are capable of tagging other users as friends/foes or fans/freaks. Another example is the ski mountaineering site Moleskiing [3] which enables users to share their opinions about the snow conditions of the different ski routes and also express how much they trust the other users. Another well-known example is the FilmTrsut system [19], an online social network that provides movie rating and review features to its users. The social networking component of the website requires users to provide a trust rating for each person they add as a friend. Also users on Wikipedia can vote for or against the nomination of others to adminship [7]. These websites have come to play an important role in guiding users' opinions on products and in many cases also influence their decisions in buying or not buying the product or service. The results of experiments in [11] and of similar works confirm that a social network can be exploited to improve the quality of recommendations. From this point of view, traditional recommender systems that ignore the social structure between users may no longer be suitable. A fundamental assumption in social based recommender systems which has been adopted by almost all of the relevant literature is that if two users have friendship relation, then the recommendation from his or her friends probably has higher trustworthiness than strangers. Therefore the goal becomes how to combine the user-item rating matrix with the social/trust network of a user to boost the accuracy of recommendation system and alleviate the sparsity problem. Over the years, several studies have addressed the issue of the transfer of trust among users in online social networks. These studies exploit the fact that trust can be passed from one member to another in a social network, creating trust chains, based on its propagative and transitive nature 1 . Therefore, some recommendation methods fusing social relations by regularization [29, 36, 42, 81] or factorization [41, 43, 59, 58, 65, 60, 57] were proposed that exploit the trust relations in the social network. Also, the results of incorporating the trust information in recommender systems is appealing and has been the focus of many researchers in the last few years, but, in large user communities, besides the trust relationship between users, the distrust relationships are also unavoidable. For example, Epinions provided the feature that enables users to categorize other users in a personal web of trust list based on their quality as a reviewer. Later on, this feature integrated with the concept of personal block list, which reflects the members that are distrusted by a particular user. In other words, if a user encounters a member whose reviews are consistently offensive, inaccurate, or otherwise low quality, she can add that member to her block list. Therefore, it would be tempting to investigate whether or not distrust information could be effectively utilized to boost the accuracy of recommender systems as well. In contrast to trust information for which there has been a great research, the potential advantage/disadvantage of explicitly utilizing distrust information is almost unexplored. Recently, few
1 We note that while the concept of trust has been studied in many disciplines including sociology, psychology, economics, and computer science from different perspectives, but the issue of propagation and transitivity have often been debated in literature and different authors have reached different conclusions (see for example [62] for a thorough discussion)

3

attempts have been made to explicitly incorporate the distrust relations in recommendation process [22, 40, 69, 72], which demonstrated that the recommender systems can benefit from the proper incorporation of distrust relations in social networks. However, despite these positive results, there are some unique challenges involved in distrust-enhanced recommender systems. In particular, it has proven challenging to model distrust propagation in a manner which is both logically consistent and psychologically plausible. Furthermore, the naive modeling of distrust as negative trust raises a number of challenges- both algorithmic and philosophical. Finally, it is an open challenge how to incorporate trust and distrust relations in model-based methods simultaneously. This paper is concerned with these questions and gives an affirmative solution to challenges involved with distrust-enhanced recommendation. In particular, the proposed method makes it possible to simultaneously incorporate both trust and distrust relationships in recommender systems to increase the prediction accuracy. To the best of our knowledge, this is the first work that models distrust relations into the matrix factorization problem along with trust relations at the same time. The main intuition behind the proposed algorithm is that one can interpret the distrust relations between users as the dissimilarity in their preferences. In particular, when a user u distrusts another user v , it indicates that user u disagrees with most of the opinions issued, or ratings made by user v . Therefore, the latent features of user u obtained by matrix factorization must be as dissimilar as possible to v 's latent features. In other words, this intuition suggests to directly incorporate the distrust into recommendation by considering distrust as reversing the deviation of latent features. However, when combined with the trust relations between users, due to the contradictory role of trust and distrust relations in propagating social information in the matrix factorization process, this idea fails to effectively capture both relations simultaneously. This statement also follows from the preliminary experimental results in [69] for memory-based CF methods that demonstrated regarding distrust as an indication to reverse deviations in not the right way to incorporate distrust. To remedy this problem, we settle to a less ambitious goal and propose another method to facilitate the learning from both types of relations. In particular, we try to learn latent features in a manner that the latent features of users who are distrusted by the user u have a guaranteed minimum dissimilarity gap from the worst dissimilarity of users who are trusted by user u . By this formulation, we ensure that when user u agrees on an item with one of his trusted friends, he/she will disagree on the same item with his distrusted friends with a minimum predefined margin. We note that this idea significantly departs from the existing works in distrust-enhanced memory based recommender systems [69, 72], that employ the distrust relations to either filter out or debug the trust relations to reduce the prediction task to a trust-enhanced recommendation. In particular, the proposed method ranks the latent features of trusted and distrusted friends of each user to reflect the effect of relation in factorization. Summary of Contributions This work makes the following key contributions:  A matrix factorization based algorithm for simultaneous incorporation of trust and distrust relationships in recommender systems. To the best of our knowledge, this is the first model-based recommender algorithm that is able to leverage both types of relationships in recommendation.  An efficient stochastic optimization algorithm to solve the optimization problem which makes the proposed method scalable to large social networks.  An empirical investigation of the consistency of the social relationships with rating information. In particular, we examine to what extent trust and distrust relations between users are aligned with the ratings they issued on items.  An exhaustive set of experiments on Epinions data set to empirically evaluate the performance of the proposed algorithm and demonstrate its merits and advantages.  A detailed comparison of the proposed algorithm to the state-of-the-art trust/distrust enhanced memory/model based recommender systems. Outline The rest of this paper is organized as follows. In Section 2 we draw connections to and put our work in context of some of the most recent work on social recommender systems. Section 3 formally

4

introduces the matrix factorization problem, an optimization based framework to solve it, and its extension to incorporate the trust relations between users. The proposed algorithm along with optimization methods are discussed in Section 4. Section 5 includes our experimental result on Epinions data set which demonstrates the merits of the proposed algorithm in alleviating data sparsity problem in rating matrix and generating more accurate recommendations. Finally, Section 6 concludes the paper and discusses few directions as future work.

2 Related Work on Social Recommendation
Earlier in the introduction, we discussed some of the main lines of research on recommender system; here, we survey further lines of study that are most directly related to our work on social-enhanced recommendation. Many successful algorithms have been developed over the past few years to incorporate social information in recommender systems. After reviewing trust-enhanced memory-based approaches, we discuss some model-based approaches for recommendation in social networks with trust relations. Finally, we review major approaches in distrust modeling and distrust-enhanced recommender systems.

2.1 Trust Enhanced Memory-based Recommendation
Social network data has been widely investigated in the memory-based approaches. These methods typically explore the social network and find a neighborhood of users trusted (directly or indirectly) by a user and perform the recommendation by aggregating their ratings. These methods use the transitivity of trust and propagate trust to indirect neighbors in the social network [45, 47, 31, 27, 29, 28, 33]. In [45], a trust-aware collaborative filtering method for recommender systems is proposed. In this work, the collaborative filtering process is informed by the reputation of users, which is computed by propagating trust. [31] proposed a method based on the random walk algorithm to utilize social connection and other social annotations to improve recommendation accuracy. However, this method does not utilize the rating information and is not applicable to constructing a random walk graph in real data sets. TidalTrust [18] performs a modied breadth first search in the trust network to compute a prediction. To compute the trust value between user u and v who are not directly connected, TidalTrust aggregates the trust value between u 's direct neighbors and v weighted by the direct trust values of u and its direct neighbors. MoleTrust [45, 46, 80] does the same idea as TidalTrust, but MoleTrust considers all the raters up to a fixed maximum-depth given as an input, independent of any specific user and item. The trust metric in MoleTrust consists of two major steps. First, cycles in trust networks are removed. Therefore, removing trust cycles beforehand from trust networks can significantly speed up the proposed algorithm because every user only needs to be visited once to infer trust values. Second, trust values are calculated based on the obtained directed acyclic graph by performing a simple graph random walk: TrustWalker [27] combines trust-based and item-based recommendation to consider enough ratings without suffering from noisy data. Their experiments show that TrustWalker outperforms other existing memory based approaches. Each random walk on the user trust graph returns a predicted rating for user u on target item i . The probability of stopping is directly proportional to the similarity between the target item and the most similar item j , weighted by the sigmoid function of step size k . The more the similarity, the greater the probability of stopping and using the rating on item j as the predicted rating for item i . As the step size increases, the probability of stopping decreases. Thus ratings by closer friends on similar items are considered more reliable than ratings on the target item by friends further away. We note that all these methods are neighborhood-based methods which employ only heuristic algorithms to generate recommendations. There are several problems with this approach. The relationship between the trust network and the user-item matrix has not been studied systematically. Moreover, these methods are not scalable to very large data sets since they may need to calculate the pairwise user similarities and pairwise user trust scores.

5

2.2 Trust Enhanced Model-based Recommendation
Recently, researchers exploited matrix factorization techniques to learn latent features for users and items from the observed ratings and fusing social relations among users with rating data as will be detailed in Section 3. These methods can be divided into two types: regularization-based methods and factorization-based methods. Here we review some existing matrix factorization algorithms that incorporate trust information in the factorization process. 2.2.1 Regularization based Social Recommendation Regularization based methods typically add regularization term to the loss function and minimize it. Most recently, Ma [42] proposed an idea based on social regularized matrix factorization to make recommendation based on social network information. In this approach, the social regularization term is added to the loss function, which measures the difference between the latent feature vector of a user and those of his friends. The probability model similar to the model in [42] is proposed by Jamali [29]. The graph Laplacian regularization term of social relations is added into the loss function in [36] and minimizes the loss function by alternative projection algorithm. Zhu et a l. [81] used the same model in [36] and built graph Laplacian of social relations using three kinds of kernel functions. In [37], the minimization problem is formulated as a low-rank semidefinite optimization problem. 2.2.2 Factorization based Social Recommendation In factorization-based methods, social relationship between users are represented as social relation matrix, which is factored as well as the rating matrix. The loss function is the weighted sum of the social relation matrix factorization error and the rating matrix factorization error. For instance, SoRec [41] incorporates the social network graph into probabilistic matrix factorization model by simultaneously factorizing the user-item rating matrix and the social trust networks by sharing a common latent lowdimensional user feature matrix [37]. The experimental analysis shows that this method generates better recommendations than the non-social filtering algorithms [28]. However, the disadvantage of this work is that although the users social network is integrated into the recommender systems by factorizing the social trust graph, the real world recommendation processes are not reflected in the model. Two sets of different feature vectors are assumed for users which makes the interpretability of the model very hard [28, 39]. This drawback not only causes lack of interpretability in the model, but also affects the recommendation qualities. A better model named Social Trust Ensemble (STE) [39] is proposed by the same authors, by making the latent features of a user's direct neighbors affect the rating of the user. Their method is a linear combination of basic matrix factorization approach and a social network based approach. Experiments show that their model outperforms the basic matrix factorization based approach and existing trust based approaches. However, in their model, the feature vectors of direct neighbors of u affect the ratings of u instead of affecting the feature vector of u . This model does not handle trust propagation. Another method for recommendation in social networks has been proposed in [40]. This method is not a generative model and defines a loss function to be minimized. The main disadvantage of this method is that it punishes the users with lots of social relations more than other users. Finally, SocialMF [28] is a matrix factorization based model which incorporates social influence by making the features of every user depend on the features of his/her direct neighbors in the social network.

2.3 Distrust Enhanced Social Recommendation
In contrast to incorporation of trust relations, unfortunately most of the literature on social recommendation totally ignore the potential of distrust information in boosting the accuracy of recommendations. In particular, only recently few work started to investigate the rule of distrust information in recommendation process both from theoretical and empirical viewpoints [22, 84, 51, 82, 40, 75, 69, 71, 68, 72]. Although these studies have shown that distrust information can be plentiful, but there is a significant gap in clear understanding of distrust in recommender systems. The most important reasons for this shortage are the lack of data sets that contain distrust information and dearth of a unified consensus on modeling and propagation of distrust.

6

Table 1: Summary of notations consistently used in the paper and their meaning. Symbol Meaning U = {u 1 ,    , u n }, n I = {i 1 ,    , i m }, m k R  Rn m  R , | R | U  Rn k V  Rm k S  {-1, +1}n n S , |S | n W  Rn + N (u )  [ n ] N + (u )  [ n ] N - (u )  [ n ] D : Rk  Rk  R+ The set of users in system and the number of users The set of items and the number of items The dimension of latent features in factorization The partially observed rating matrix The set of observed entires in rating matrix and its size The matrix of latent features for users The matrix of latent features for items The social network between n users The set of extracted triplets from the social relations and its size The pairwise similarity matrix between users Neighbors of user u in the social graph The set of trusted neighbors by user u in the social graph The set of distrusted neighbors by user u in the social graph The measurement function used to assess the similarly of latent features

A formal framework of trust propagation schemes, introducing the formal and computational treatment of distrust propagation has been developed in [22]. In an extension of this work, [82] proposed clever adaptations in order to handle distrust and sinks such as trust decay and normalization. In [75], a trust/distrust propagation algorithm called CloseLook is proposed, which is capable of using the same kinds of trust propagation as the algorithm proposed by [22]. [34] extended the results by [22] using a machine-learning framework (instead of the propagation algorithms based on an adjacency matrix) to enable the evaluation of the most informative structural features for the prediction task of positive/negative links in online social networks. A comprehensive framework that computes trust/distrust estimations for user pairs in the network using trust metrics is build in [71]: given two users in the trust network, we can search for a path between them and propagate the trust scores along this path to obtain an estimation. When more than one path is available, we may single out the most relevant ones (selection), and aggregation operators can then be used to combine the propagated trust scores into one final trust score, according to different trust score propagation operators. [40] was the first seminal work to demonstrate that the incorporation of distrust information could be beneficial based on a model-based recommender system. In [71] and [72] the same question is addressed in memory-based approaches. In particular, [72] embarked upon the distrust-enhanced recommendation and showed that with careful incorporation of distrust metric, distrust-enhanced recommender systems are able to outperform their trust-only counterparts. The main rational behind the algorithm proposed in [72] is to employ the distrust information to debug or filter out the users' propagated web of trust. It is also has been realized that the debugging methods must exhibit a moderate behavior in order to be effective. [68] addressed the problem of considering the length of the paths that connect two users for computing trust-distrust between them, according to the concept of trust decay. This work also introduced several aggregation strategies for trust scores with variable path lengths Finally we note that the aforementioned works try to either model or utilize the trust/distrust information. In recent years there has been an upsurge of interest in predicting the trust and distrust relations in a social network [34, 14, 4, 53]. For instance, [34] casts the problem as a sign prediction problem (i.e., +1 for friendship and -1 for opposition) and utilizes machine learning methods to predict the sign of links in the social network. In [14] a new method is presented for computing both trust and distrust by combining an inference algorithm that relies on a probabilistic interpretation of trust based on random graphs with a modified spring-embedding algorithm to classify an edge. Another direction of research is to examine the consistency of social relations with theories in social psychology [8, 35]. Our work significantly departs from these works on prediction or consistency analysis of social relations, and aims to effectively incorporate the distrust information in matrix factorization for effective recommendation.

7

3 Matrix Factorization based Recommender Systems
This section provides a formal definition of collaborative filtering, the primary recommendation method we are concerned with in this paper, followed by solution methods for low-rank factorization that are proposed in the literature to address the problem.

3.1 Matrix Factorization for Recommendation
In collaborative filtering we assume that there is a set of n users U = {u 1 ,    , u n } and a set of m items I = {i 1 ,    , i m } where each user u i expresses opinions about a set of items. In this paper, we assume opinions are expressed through an explicit numeric rating (e.g., scale from one to five), but other rating methods such as hyperlink clicks are possible as well. We are mainly interested in recommending a set of items for an active user such that the user has not rated these items before. To this end, we are aimed at learning a model from the existing ratings, i.e., offline phase, and then use the learned model to generate recommendations for active users, i.e., online phase. The rating information is summarized in an n  m matrix R  Rn m , 1  i  n , 1  j  m where the rows correspond to the users and the columns correspond to the items and (p , q )th entry is the rate given by user u p to the item i q . We note that the rating matrix is partially observed and it is sparse in most cases. An efficient and effective approach to recommender systems is to factorize the user-item rating matrix R by a multiplicative of k -rank matrices R  UV , where U  Rn k and V  Rm k utilize the factorized user-specific and item-specific matrices, respectively, to make further missing data prediction. The main intuition behind a low-dimensional factor model is that there is only a small number of factors influencing the preferences, and that a user's preference vector is determined by how each factor applies to that user. This low rank assumption makes it possible to effectively recover the missing entires in the rating matrix from the observed entries. We note that the celebrated Singular Value Decomposition (SVD) method to factorize the rating matrix R is not applicable here due to the fact that the rating matrix is partially available and we are only allowed to utilize the observed entries in factorization process. There are two basic formulations to solve this problem: these are optimization based (see e.g., [57, 37, 41, 33]) and probabilistic [50]. In the following subsections, we first review the optimization based framework for matrix factorization and then discuss how it can be extended to incorporate trust information.

3.2 Optimization based Matrix Factorization
Let R be the set of observed ratings in the user-item matrix R  Rn m , i.e., R = {(i , j )  [n ]  [m ] : R i j has been observed}, where n is the number of users and m is the number of items to be rated. In optimization based matrix factorization, the goal is to learn the latent matrices U and V by solving the following optimization problem: min L (U, V) =
U,V

1 R i j - Ui ,: V j ,: 2 (i , j )R
F

2

+

U U 2
n i =1

F+

V V 2

F

,

(1)

where 

F

is the Frobenius norm of a matrix, i.e, A

=

m 2 j =1 | A i j | .

The optimization prob-

lem in (1) constitutes of three terms: the first term aims to minimize the inconsistency between the observed entries and their corresponding value obtained by the factorized matrices. The last two terms regularize the latent matrices for users and items, respectively. The parameters U and V are regularization parameters that are introduced to control the regularization of latent matrices U and V, respectively. We would like to emphasize that the problem in (1) is non-convex jointly in both U and V. However, despite its non-convexity, the formulation in (1) is widely used in practical collaborative filtering applications as the performance is competitive or better as compared to trace-norm minimization, while scalability is much better. For example, as indicated in [33], to address the Netflix problem, (1) has been applied with a fair amount of success to factorize data sets with 100 million ratings.

8

3.3 Matrix Factorization with Trust Side Information
Recently it has been shown that just relying on the rating matrix to build a recommender system is not as accurate as expected. The main reason for this claim is the known cold-start users problem and the sparsity of rating matrix. Cold-start users are one of the most important challenges in recommender systems. Since cold-start users are more dependent on the social network compared to users with more ratings, the effect of using trust propagation gets more important for cold-start users. Moreover, in many real life systems a very large portion of users do not express any ratings, and they only participate in the social network. Hence, using only the observed ratings does not allow to learn the user features. One of the most prominent approaches to tackle the data sparsity problem in matrix factorization is to compensate the lack of information in rating matrix with other sources of side information which are available to the recommender system. It has been recently shown that social information such as trust relationship between users is a rich source of side information to compensate for the sparsity. The above mentioned traditional recommendation techniques are all based on working on the user-item rating matrix, and ignore the abundant relationships among users. Trust-based recommendation usually involves constructing a trust network where nodes are users and edges represent the trust placed on them. The goal of a trust-based recommendation system is to generate personalized recommendations by aggregating the opinions of other users in the trust network. The intuition is that users tend to adopt items recommended by trusted friends rather than strangers, and that trust is positively and strongly correlated with user preferences. Recommendation techniques that analyze trust networks were found to provide very accurate and highly personalized results. To incorporate the social relations in the optimization problem formulated in (1), few papers [40, 29, 42, 37, 81] proposed the social regularization method which aims at keeping the latent vector of each user similar to his/her neighbors in the social network. The proposed models force the user feature vectors to be close to those of their neighbors to be able to learn the latent user features for users with no or very few ratings [29]. More specifically, the optimization problem becomes as: L (U, V) = 1 R i j - Ui ,: V j ,: 2 (i , j )R + S 2
n 2

+

U U 2

F+

V V 2

F

(2)

Ui ,: -
i =1

1 | N (i )|

j N (i )

U j ,: ,

where S is the social regularization parameter and N (i ) is the subset of users who has relationship with i th user in the social graph. The rationale behind social regularization idea is that every user's taste is relatively similar to the average taste of his friends in the social network. We note that using this idea, latent features of users indirectly connected in the social network will be dependent and hence the trust gets propagated. A more reasonable and realistic model should treat all friends differently based on how similar they are. Let assume the weight of relationship between two users i and j is captured by Wi j where W  Rn n demotes the social weight matrix. It is easy to extend the model in (2) to treat friends differently based on the weight matrix W as: L (U, V) = 1 R i j - Ui ,: V j ,: 2 (i , j )R + S 2
n 2

+

U U 2

F+

V V 2

F

(3)

Ui ,: -
i =1

j N (i ) Wi j U j ,: j N (i ) Wi j

An alternative formulation is to regularize each users' fiends individually, resulting in the following objective function [42]: L (U, V) = 1 R i j - Ui ,: V j ,: 2 (i , j )R + S 2
n 2

+

U U 2
2

F+

V V 2

F

Wi j Ui ,: - U j ,:
i , j =1

.

where we simply assumed that for any j  N (i ), Wi j = 0. 9

As mentioned earlier, the objective function in L (U, V) is not jointly convex in both U and V but it is convex in each of them fixing the other one. Therefore, to find a local solution one can stick to the standard gradient descent method to find a solution in an iterative manner as follows: Ut +1  Ut -  t U L (U, V)|U=Ut ,V=Vt , Vt +1  Vt -  t V L (U, V)|U=Ut ,V=Vt .

4 Matrix Factorization with Trust and Distrust Side Information
In this section we describe the proposed algorithm for social recommendation which is able to incorporate both trust and distrust relationships in the social network along with the partially observed rating matrix. We then present two strategies to solve the derived optimization problem, one based on the gradient descent optimization algorithm which generates more accurate solutions but it is computationally cumbersome, and another based on the stochastic gradient descent method which is computationally more efficient for large rating and social matrices but suffers from slow convergence rate.

4.1 Algorithm Description
As discussed before, the vast majority of related work in the field of matrix factorization for recommendation has primarily focussed on trust propagation and simply ignore the distrust information between users, or intrinsically, are not capable of exploiting it. Now, we aim at developing a matrix factorization based model for recommendation in social rating networks to utilize both trust and distrust relationships. We incorporate the trust/distrust relationship between users in our model to improve the quality of recommendations. While intuition and experimental evidence indicate that trust is somewhat transitive, distrust is certainly not transitive. Thus, when we intend to propagate distrust through a network, questions about transitivity and how to deal with conflicting information abound. To inject social influence in our model, the basic idea is to find appropriate latent features for users such that each user is brought closer to the users she/he trusts and separated apart from the users that she/he distrusts and have different interests. We note that simply incorporating this idea in matrix factorization by naively penalizing the similarity of each user's latent features to his distrusted friends' latent features fails to reach the desired goal. The main reason is that distrust is not as transitive as trust, i.e. distrust can not directly replace trust in trust propagation approaches and utilizing distrust requires careful consideration (trust is transitive, i.e., if user u trusts user v and v trusts w , there is a good chance that u will trust w , but distrust is certainly not transitive, i.e., if u distrusts v and v distrusts w , then w may be closer to u than v or maybe even farther away). It is noticeable that this statement is consistent with the preliminary experimental results in [69] for memory-based CF methods that indicate regarding distrust as an indication to reverse deviations in not the right way to incorporate distrust. Therefore we pursue another approach to model the distrust in recommendation process. The main intuition behind the proposed framework stems from the observation that the trust relations between users can be treated as agreement on items and distrust relations can be considered as disagreement on items. Then, the question becomes how can we guarantee when a user agrees on an item with one of his/her friends, he/she will disagree on the same item with his/her distrusted friends with a reasonable margin. We note that this margin should be large enough to make it possible to distinguish between two types of friends. In terms of latent features, this observation translates to having a margin between the similarity and dissimilarity of users' latent features to his/her trusted and distrusted friends. Alternatively, one can view the proposed method from the viewpoint of connectivity of latent features in a properly designated graph. Intuitively, certain features or groups of features should influence how users connect in the social network, and thus it should be possible to learn a mapping from features to connectivity in the social network such that the mapping respects the underlying structure of the social network. In the basic matrix factorization algorithm for recommendation, we can consider the latent features as isolated vertices of a graph where there is no connection between nodes. This can be generalized to the social-enhanced setting by considering the social graph as the underlying graph between latent features with two types of edges (i.e., trust and distrust relations correspond to positive

10

(a) User trust netwrok

(b) User distrust netwrok

(c) Partially observed rating matrix

(d) Illustration of learned latent features

Figure 1: A simple example with seven users {u 1 , u 2 ,    , u 7 } and six items {i 1 , i 2 ,    , i 6 } to illustrate the main intuition behind the proposed algorithm. The inputs of the algorithm are (a) trust network, (b) distrust network, and (c) partially observed rating matrix R, respectively. As shown in (d) for user u 1 , the learned latent features for all his trusted friends {u 2 , u 4 , u 6 , u 7 } are closer to u 1 's latent features than his distrusted friends {u 3 , u 5 } with a margin of 1. and negative edges, respectively). Now the problem reduces to learning the latent features for each user u such that users trusted by u in the social network (with positive edges) are close and users which are distrusted by u (with negative edges) are more distant. Learning latent features in this manner respects the inherent topology of the social network. Figure 1 shows an example to illustrate the intuition behind the mentioned idea. For ease of exposition, we only consider the latent features for the user u 1 . From the trust network in Figure 1 (a) we can see that user u 1 trusts the list of users N+ = {u 2 , u 4 , u 6 , u 7 } and from the distrust network in Figure 1 (b) we see that user u 1 distrusts the list of users N- = {u 3 , u 5 }. The goal is to learn the latent features that obeys two goals, i) it minimizes the prediction error on observed entries in the rating matrix, ii) it respects the underlying structure of the trust and distrust networks between users. In Figure 1 (d) the latent features are depicted in the Euclidean space from the viewpoint of user u 1 . As shown in Figure 1 (d), for user u 1 , the latent features of his/her trusted friends N+ lie inside the solid circle centered at u 1 and the latent features of his/her distrusted friends N- lie outside the dashed circle. The gap between two circles guarantees that always there exists a safe margin between u 1 's agreements with his trusted and distrusted friends. One simple way to impose these constraints on the latent features of users is to generate a set of triplets for any combination of trusted and distrusted friends ( e.g., one such triplet for user u 1 can be constructed as (u 1 , u 2 , u 5 )) and force the margin constraint to hold for all extracted triplets. This ensures that the minimum margin gap will definitely exist between the latent features of all the trusted and distrusted friends as desired and makes it possible to incorporate both types of

11

relationships between users in the matrix factorization. It is worthy to mention that similar to the social-enhanced recommender systems discussed before, the proposed algorithm is also based on hypotheses about the existence and the correlation of trust/distrust relations and ratings in the data. The empirical investigation of correlation between social relations and rating information has been the focus of a bulk of recent research including [83, 53, 38], where the results reinforce the hypothesis that ratings from trusted people count more than those from others and in particular distrusted neighbors. We have also conducted experiments as will be detailed in Subsection 5.5, to empirically investigate the correlation/alignment between social relations and the rating information issued by users which supports our strategy in exploiting the trust/distrust relations in matrix factorization. We now formalize the proposed solution. As the first ingredient, we need a measure to evaluate the consistency between the latent features of users, i.e., the matrix U, and the trust and distrust constraints existing between users in the social network. To this end, we introduce a monotonically increasing convex loss function (z ) to measure the discrepancy between the latent features of different users. Let u i , u j , and u k be three users in the model such that u i trusts u j but distrusts u k . The main intuition behind the proposed framework is that the latent features of u i , i.e., Ui ,: must be more similar to u j 's latent features than latent features for user u k . For each such a triplet we penalize the objective function by (D (Ui ,: , U j ,: ) - D (Ui ,: , Uk ,: )) where the function D : Rk  Rk  R+ measures the similarity between two latent vectors assigned to two different users, and : R  R+ is a penalty function that is utilized to assess the violation of latent vectors of trusted and distrusted users. Example loss functions include hinge loss (z ) = max(0, 1- z ) and logistic loss (z ) = log(1+e -z ) which are widely used convex surrogate of 0-1 loss function in learning community. Let S denote the set of extracted triplets from the social relations, i.e., S = (i , j , k )  [n ]  [n ]  [n ] : S i j = 1 & S i k = -1 . Here, a positive relationship means friends or a trusted relationship and a negative relationship means foes or a distrust relationship. Then, our goal becomes to find a factorization of matrix R such that the learned latent features of users are consistent with the constraints in S where the consistency is reflected in the loss function. This results in the following optimization problem: L (U, V) = 1 R i j - Ui ,: V j ,: 2 (i , j )R +
2

+

U U 2

F+

V V 2

F

S (D (Ui ,: , U j ,: ) - D (Ui ,: , Uk ,: )). |S | (i , j ,k )S

(4)

Let us make the above general formulation more specific by setting () and D (, ) to be the hinge loss and the Euclidian distance, respectively. Under these two assumptions, the objective can be formulated as: L (U, V) = 1 R i j - Ui ,: V j ,: 2 (i , j )R
R (U,V) 2

+

U U 2

F+

V V 2

F

S + max 0, 1 - Ui ,: - U j ,: |S | (i , j ,k )S

2

+ Ui ,: - Uk ,:

2

.

(5)

Here the constraints have been written in terms of hinge-losses over triplets, each consisting of a user, his/her trusted friend and his/her distrusted friend. Solving the optimization problem in (5) outputs the latent features for users and items that can utilized to estimate the missing values in the user-item matrix. Comparing the formulation in (5) to the existing factorization-based methods discussed earlier reveals two main features of the proposed formulation. First, it aims to minimize the error on the observed ratings and to respect the inherent structure of the social network among the users. The tradeoff between these two objectives is captured by the regularization parameter S which is required to be tuned effectively.

12

Algorithm 1 GD based Matrix Factorization with Trust and Distrust Propagation 1: Input: R: partially observed rating matrix, S 2: Output: U and V 3: for t = 1, . . . , T do 4: Compute the gradients U R (Ut , Vt ) and V R (Ut , Vt ). 5: Compute U by Eq. 7 6: Compute V by Eq. 8 7: Update: Ut +1 = Ut -  t U |U=Ut ,V=Vt Vt +1 = Vt -  t V |U=Ut ,V=Vt
8: 9:

end for return UT +1 and VT +1 .

In a similar way, applying the logistic loss to the general formulation in (4) yields the following objective: L (U, V) = 1 R i j - Ui ,: V j ,: 2 (i , j )R +
2

+

U U 2

F+

V V 2
2

F

S log 1 + exp Ui ,: - Uk ,: |S | (i , j ,k )S

- Ui ,: - U j ,: )

2

.

(6)

Remark 1. We note that in several applications of recommender systems, besides the observed ratings, a description of the users and/or the objects through attributes (e.g., gender, age) or measures of similarity is available that could potentially benefit the process of recommendation (see e.g. [2] for few interesting applications). In that case it is tempting to take advantage of both known ratings and descriptions to model the preferences of users. A natural way to incorporate the available meta-data is to kernalize the similarity measure between latent features based on a positive definite kernel between pairs that can be deduced from the meta-data. More specifically, instead of simply using Euclidian distance as the similarity measure between latent features in (5), we can use the kernel matrix K obtained from the Laplacian of the graph obtained from the meta-data to measure the similarity as: D (Ui ,: , U j ,: ) = Ui ,: - U j ,: K Ui ,: - U j ,: ,

where K = (D - W)-1 , with D as a diagonal matrix with D i ,i = n j =1 Wi j . Here W captures the pairwise weight between users in the similarity graph between users that is computed based on the available metadata about users. Remark 2. We would like to emphasize that it is straightforward to generalize the proposed framework to incorporate similarity and dissimilarity information between items. What we need is to extract the triplets from the trust/distrust links between items and repeat the same process we did for users. This will add another term to the objective in terms of latent features of items V as shown in the following generalized formulation: L (U, V) = 1 R i j - Ui ,: V j ,: 2 (i , j )R + +
2

+

U U 2

F+

V V 2
2

F

S max 0, 1 - Ui ,: - U j ,: |S | (i , j ,k )S I max 0, 1 - Vi ,: - V j ,: |I | (i , j ,k )I
2

+ Ui ,: - Uk ,:
2

2

+ Vi ,: - Vk ,:

,

where I is the regularization parameter and I is the set of triplets extracted from the similar/dissimilar links between items. The similarity/dissimilarity links between items can be constructed according to tags issued by users or associated with items, and categories. For example, if two items are attached with a 13

same tag, there is a trust link between them and otherwise distrust link. Alternatively, trust/distrust links can be extracted by measuring similarity/dissimilarity based on the item properties or profile if provided. This can further improve the accuracy of recommendations.

4.2 Batch Gradient Descent based Optimization
In optimization for supervised machine learning, there exist two regimes in which popular algorithms tend to operate: the stochastic approximation regime, which samples a small data set per iteration, typically a single data point, and the batch or sample average approximation regime, in which larger samples are used to compute an approximate gradient. The choice between these two extremes outlines the well-known tradeoff between inexpensive noisy steps and expensive but more reliable steps. Two preliminary examples of these regimes are the Gradient Descent (GD) and the Stochastic Gradient Descent (SGD) methods, respectively. Both GD and SGD methods starts with some initial point, and iteratively updates the solution using the gradient information at intermediate solutions. The main difference is that GD requires a full gradient information at each iteration while SGD only requires an unbiased estimate of the full gradient which can be done by sampling We now discuss the application of GD algorithm to solve the optimization problem in (5) as detailed in Algorithm 1. Recall that the objective function is not jointly convex in both U and V. On the other hand, the objective is convex in one parameter by fixing the other one. Therefore, we follow an iterative method to minimize the objective. At each iteration, first by fixing V, we take a step in the direction of the negative gradient for U and repeat the same process for V by fixing U. For the ease of exposition, we introduce further notation. For any triplet (i , j , k )  S we note that the Ui ,: - U j ,: 2 - Ui ,: - Uk ,: 2 can be written as Tr(CU U) where Tr() denotes the trace of the input matrix and C is a sparse auxiliary matrix defined for each triplet with all entries equal to zero except: Ci k = Cki = C j j = 1 and Ckk = Ci j = C j i = -1. Having defined this notation, we can write the objective in (5) as: U U 2 V V 2 S max 0, 1 - Tr(Ck i j U U) . |S | (i , j ,k )S

L (U, V) = R (U, V) +

F+

F+

is the C matrix defined above which is associated with triplet (i , j , k ). To apply the GD where Ck ij method, we need to compute the gradient of L (U, V) with respect to U and V which we denote by U = U L (U, V) and V = V L (U, V), respectively. We have: S 1 k |S | (i , j ,k )S [Tr(Ci j U

U = U R (U, V) + U U -

k U)<1] (UCi j

+ UCk ij)

(7)

where 1[] is the indicator function which takes a value of one if its argument is true, and zero otherwise. Similarly for V we have: V = V R (U, V) + V V (8)

The main shortcoming of GD method is its high computational cost per iteration due to the gradient computation (i.e., step (7)) which is expensive when the size of social constraints S is large. We note that the size of S can be as large as O (n 3 ) by considering all triplets in the social graph. In the next subsection we provide an alternative solution to resolve this issue using the stochastic gradient descent and mini-batch SGD methods which are more efficient than the GD method in terms of the computational cost per iteration but with a slow convergence rate in terms of target approximation error.

4.3 Stochastic and Mini-batch Optimization
As discussed above, when the size of social network is very large, the size of S may cause computational problems in solving the optimization problem in (5) using GD method. The reason is essentially the fact that computing the gradient at each iteration requires to go through all the triplets in S which is infeasible for large networks. To alleviate this problem we propose a stochastic gradient based [52]

14

Algorithm 2 Mini-SGD based Matrix Factorization with Trust and Distrust Propagation 1: Input: R: partially observed rating matrix, S , min batch size B 2: Output: U and V 3: for t = 1, . . . , T do 4: t  0 5: for b = 1, . . . , B do 6: (i , j , k )  Sample random triplet from S 7: if (1 - Ui ,: - U j ,: ) 2 + Ui ,: - Uk ,: 2 > 0) then 8:  t  U t Ck U ij t 9: end if 10: end for 11: Compute the gradients U R (Ut , Vt ) and V R (Ut , Vt ). 12: Update: S t Ut +1 = Ut -  t U R (Ut , Vt ) + U Ut + B | S |
13:

Update: Vt +1 = Vt -  t (V R (Ut , Vt ) + V Vt ) end for return UT +1 and VT +1 .

14: 15:

method to solve the optimization problem. The main idea is to choose a fixed subset of triplets for gradient computation instead of all |S | triplets at each iteration [10]. More specifically, at each iteration, we sample B triplets uniformly at random from S to compute the next solution. We note that this strategy generates unbiased estimates of the true gradient and makes each iteration of algorithm computationally more efficient compared to the full gradient counterpart. In the simplest case, SGD algorithm, only one triplet is chosen at each iteration to generate an unbiased estimate of the full gradient. We note that in practice SGD is usually implemented based on data shuffling, i.e., making the sequence of the training samples random and then training the model by going through the training samples one by one. An intermediate solution, known as mini-batch SGD, chooses a subset of triplets to compute the gradient. The promise is that by selecting more triplets at each iteration, on one hand the variance of stochastic gradients decreases promotional to the number of sampled triplets, and on the other hand the algorithm enjoys the light computational cost of basic SGD method. The detailed steps of the algorithm are shown in Algorithm 2. The mini-batch SGD method improves the computational efficiency by grouping multiple constraints into a mini-batch and only updating the U and V once for each mini-batch. For brevity, we will refer to this algorithm as Mini-SGD. More specifically, the Mini-SGD algorithm, instead of computing the full gradient over all triplets, samples B triplets uniformly at random from S where 1  B  |S | is a parameter that needs to be provided to the algorithm, and computes the stochastic gradient as: t = S B 1[Tr(Ck
k Ut Ut )<1] (UCi j

(i , j ,k )B

ij

+ UCk ij)

where B is the set of B sampled triplets from S . We note that E[  t ] = S 1 k |S | (i , j ,k )S [Tr(Ci j Ut
k Ut )<1] (UCi j

+ UCk i j ),

i.e., t is an unbiased estimate of the full gradient in the right hand side. When B = |S |, each iteration handles the original objective function and Mini-SGD reduces to the batch GD algorithm. We note that both GD and SGD share the same convergence rate in terms of number of iterations in expectation for non-smooth optimization problems (i.e., O (1/ T ) after T iterations), but SGD method requires much less running time to convergence compared to the GD method due to the efficiency of its individual iterations.

15

5 Experimental Results
In this section, we conduct exhaustive experiments to demonstrate the merits and advantages of the proposed algorithm. We conduct the experiments on the well-known Epinions 2 data set, aiming to accomplish and answer the following fundamental questions: 1. Prediction accuracy: How does the proposed algorithm perform in comparison to the stateof- the-art algorithms with/without incorporating trust and distrust relationships between users. Whether or not the trust/distrust social network could help in making more accurate recommendations? 2. Correlation of social relations with rating information: To what extent, the trusted and distrusted friends of a user u are aligned with the ratings the user u issued for the reviews written by his friends? A positive answer to this question indicates that two users will issue similar (dissimilar) ratings if they are connected by a trust (distrust) relation and prefer to behave similarly. 3. Model selection: What role do the regularization parameters S , U and V play in the accuracy of the proposed recommender system and what is the best strategy to tune these parameters? 4. Handling cold-start users: How does exploiting social relationships in prediction process affect the performance of recommendation for cold-start users? 5. Trading trust for distrust: To what extent the distrust relations can compensate for the lack of trust relations? 6. Efficiency of optimization: What is the trade-off between accuracy and efficiency by moving from the gradient descent to the stochastic gradient descent with different batch sizes? In the following subsections, we intend to answer these questions. We begin by introducing the data set we use in our experiemnts and the metrics we employ to evaluate the results, followed by the detailed experimental results.

5.1 Data Set Description and Experimental Setup
The Epinions data set We begin by discussing the data set we have chosen for our experiments. To evaluate the proposed algorithm on trust and distrust-aware recommendations, we use the Epinions data set [22], a popular e-commerce site and customer review website where users share opinions on various types of items such as electronic products, companies, and movies, through writing reviews about them or assigning a rating to the reviews written by other users. The rating values in Epinions are discrete values ranging from not helpful (1/5) to most helpful (5/5). These ratings and reviews would potentially influence future customers when they are about to decide whether a product is worth buying or a movie is worth watching. Epinions allows users to evaluate other users based on the quality of their reviews, and to make trust and distrust relations with other users in addition to the ratings. Every member of Epinions can maintain a "trust" list of people he/she trusts that is referred to as web of trust (social network with trust relationships) based on the reviewers with consistent ratings or "distrust" list known as block list (social network with distrust relationships) that presents reviewers whose reviews were consistently found to be inaccurate or low quality. The fact that the data set contains explicit positive and negative relations between users makes it very appropriate to study issues in trust- and distrust-enhanced recommender systems. Epinions is thus an ideal source for experiments on social recommendation. We remark that the Epinions data set only contains bivalent relations (i.e., contains only full trust and full distrust, and no gradual statements). To conduct the coming experiments, we sampled a subset of Epinions data set with n = 121, 240 users and m = 685, 621 different items. The total number of observed ratings in the sampled data set is 12,721,437 which approximately includes 0.02% of all entries in the rating matrix R which demonstrates the sparsity of the rating matrix. We note that the selected items are the most frequently rated overall. The statistics of the data set is given in Table 2. The social statistics of the this data source is summarized
2 http://www.trustlet.org/wiki/Epinions_datasets

16

Table 2: Statistics of sample data from Epinions data set used in our experiments. Statistic Quantity Number of users 121,240 Number of items 685,621 Number of ratings 12,721,437 Number of trust relations 481,799 Number of distrust relations 96,823 Minimum number of ratings by users 1 Minimum number of ratings for items 1 Maximum number of ratings by users 148735 Maximum number of ratings for items 945 Average number of ratings by users 85.08 Average number of ratings for items 15.26

Table 3: Maximum and average trust and distrust relations for users in the sampled data set. Statistics Trust per user Be Trusted per user Max Min Average Max Min Average 1983 1 4.76 Distrust per user 1188 1 0.91 2941 0 4.76 Be Distrusted per user 429 0 0.91

in Table 3. The frequencies of ratings for users is shown are Table 4. In the user distrust network, the total number of issued distrust statements is 96,823. As to the user trust network, the total number of issued trust statements is 481,799. Experimental setup To better evaluate the effect of utilizing the social side information in recommendation accuracy, we employ different amount of training data 90%, 80% , 70% and 60% to create four different training sets that are increasingly sparse but the social network remains the same in all of them. Training data 90%, for example, means we randomly select 90% of the ratings from the sampled Epinions data set as the training data to predict the remaining 10% of ratings. The random selection was carried out 5 times independently to have a fair comparison. Also, since our preliminary results on a smaller data set revealed that the hinge loss performs better than the exponential loss, in the rest of experiments we stick to this loss function. However, we note the exponential loss is slightly faster in optimizing the corresponding objective function thanks to its smoothness, but it was negligible considering its worse accuracy compared to the hinge loss. All implementations are in Matlab, and all experiments were performed on a 4-core 2.0 GHZ of a load-free machine with a 12G of RAM.

5.2 Metrics
5.2.1 Metrics for rating prediction We employ two well-known measures, the Mean Absolute Error (MAE) and the Root Mean Squared Error (RMSE) [25] to measure the prediction accuracy of the proposed approach in comparison with other basic collaborative filtering and trust/distrust-enhanced recommendation methods. MAE is very appropriate and useful measure for evaluating prediction accuracy in offline tests [25, 45]. To calculate MAE, the predicted rating is compared with the real rating and the difference (in absolute value) considered as the prediction error. Then, these individual errors are averaged over all predictions to obtain the overall MAE value. More precisely, let T denote the set of ratings to be predicted,

17

Table 4: The frequencies of user's rating. # of Ratings # of Users # of Ratings # of Users 0-10 4,198,074 ( 33%) 61-70 157,745 ( 1.24%) 11-20 3,053,144 ( 24%) 71-80 143,752 ( 1.13%) 21-30 2,289,858 ( 18%) 81-90 104,315 ( 0.82%) 31-40 1,526,572 ( 12%) 91-100 43,252 ( 0.34%) 41-50 534,300 ( 4.2%) 101-200 21,626 ( 0.17%)

267,1

2 10,68

^ denote the prediction matrix obtained by i.e., T = {(i , j )  [n ]  [m ], R i j needs to be predicted} and let R algorithm after factorization. Then, MAE =
(i , j )T

^i j | |R i j - R

|T |

,

^i j is the rating user i would assign where R i j is the real rating assigned by the user i to the item j , and R to the item j that is predicted by the algorithm . The RMSE metric is defined as:
(i , j )T

RMSE =

^i j Ri j - R |T |

2

.

The first measure (MAE) considers every error of equal value, while the second one (RMSE) emphasizes larger errors. We would like to emphasize that even small improvements in RMSE are considered valuable in the context of recommender systems. For example, the Netflix prize competition offered a 1,000,000 reward for a reduction of the RMSE by 10% [72]. 5.2.2 Metrics for evaluating the correlation of ratings with trust/distrust relations As part of our experiments, we investigate how the explicit trust/distrust relations between users in the social network are aligned with the implicit trust/distrust relations between users conveyed from the rating information. We use recall, Mean Average Precision (MAP) [44] and Normalized Discount Cumulative Gain (NDCG) to evaluate the ranking results. Recall is defined as the number of relevant friends divided by the total number of friends in the social network. Precision is defined as the number of relevant friends (trusted or distrusted) divided by the number of friends in the social network. Given a user u , let r i be the relevance score of the friend ranked at position i , where r i = 1 if the user is relevant to the u and r i = 0 otherwise. Then we can compute the Average Precision (AP) as AP =
i ri

 Precision@i

# of relevant friends

.

MAP is the average of AP over all the users in the network. NDCG is a normalization of the Discounted Cumulative Gain (DCG) measure. DCG is a weighted sum of the degree of relevancy of the ranked users. The weight is a decreasing function of the rank (position) of the user, and therefore called discount. NDCG normalizes DCG by the Ideal DCG (IDCG), which is simply the DCG measure of the best ranking result. Thus NDCG measure is always a number in [0, 1]. NDCG at position k is defined as: NDCG@k = Zk 2r i - 1 i =1 log(i + 1)
k

where k is also called the scope, which means the number of top-ranked users presented to the user and Zk is chosen such that the perfect ranking has a NDCG value of 1. We note that the base of the logarithm does not matter for NDCG, since constant scaling will cancel out due to normalization. We will assume it is the natural logarithm throughout this paper.

18

Figure 2: Grid Search to find the best values for U and C on the data set with 90% of rating information.

5.3 Model Selection
Tuning of parameters (a.k.a model selection in learning community) is a critical problem in most of the learning problems. In some situations, the learning performance may drastically vary with different choices of the parameters. There are three parameters in objective (5) that play very important role in the effectivity of the proposed algorithm. These are U , V , and S . Between these, S controls how much the proposed algorithm should incorporate the information of the social network in completing the partially observed rating matrix. In the extreme case, a very small value for S , the algorithm almost forgets the social information exists between the users and only utilizes the observed user-item rating matrix for factorization. On the other hand, if we employ a very large value for S , the social network information will dominate the learning process, leading to a poorer performance. Therefore, in order to not hurt the recommendation performance, we need to find a reasonable value for social regularization parameter. To this end, we analyze how the combination of these parameters affect the recommendation performance. We conduct a grid search on the potential values of two parameters S and V to find the combination with best performance. Figure 2 shows the grid search results for these parameters on data set with 90% of training data where the optimal prediction accuracy is achieved at point (14.8, 11) with the optimal RMSE = 1.12. We would like to emphasize that we have done the cross validation for only pairs of (S , V ) and (S , U ) because, (i) considering the grid search for the triplet (S , U , V ) is computationally burdensome, (ii) and our preliminary experiments showed that V and U behave similarly with respect to S . Based on the results reported in Figure 2, in the remaining experiments, we set S = 14.8, V = 11, and U = 13 when the training is performed on the data set with 90% of rating information. We repeat the same process to find out the best setting of regularization parameters for other data sets with 80%, 70%, and 60% of rating data as well.

5.4 Baseline Methods
Here we briefly discuss the baseline algorithms that we intend to compare the proposed algorithm. The baseline algorithms are chosen from both types of memory-based and model-based recommender systems with different types of trust and distrust relations. In particular, we consider the following basic algorithms:  MF (matrix factorization based recommender): this is the basic matrix factorization based rec19

ommender formulated in the optimization problem in (1) which does not take the social data into account.  MF+T (matrix factorization with trust information): to exploit the trust relations between users in matrix factorization, [40] relied on the fact that the distance between latent features of users who trust each other must be minimized that can be formulated as the following objective: min
U

1 n D (Ui ,: , U j ,: ), 2 i =1 j N+ (i )

where N+ (i ) is the set of users the i th user trusts in the social network (i.e., S i j = +1). By employing this intuition in the basic formulation in (1), [40] solves the following optimization problem: min
U,V

1 R i j - Ui ,: V j ,: 2 (i , j )R

2

+

 n U D (Ui ,: , U j ,: ) + U 2 i =1 j N+ (i ) 2

F+

V V 2

F

.

 MF+D (matrix factorization with distrust information): the basic intuition behind the algorithm proposed in [40] to exploit the distrust relations is as follows: if user u i distrusts user u j , then we can assume that their corresponding latent features Ui ,: and U j ,: would have a large distance. As a result we aim to maximize the following quantity for all users: max
U

1 n D (Ui ,: , U j ,: ), 2 i =1 j N- (i )

where N- (i ) denotes the set of users the i th users distrusts (i.e, S i j = -1). Adding this term to the basic optimization problem in (1) we obtain the following optimization problem: min
U,V

1 R i j - Ui ,: V j ,: 2 (i , j )R

2

-

U  n D (Ui ,: , U j ,: ) + U 2 i =1 j N- (i ) 2

F+

V V 2

F

.

 MF+TD (matrix factorization with trust and distrust information): this algorithm stands for the algorithm proposed in the present work. We note that there is no algorithm in the literature that exploits both trust and distrust relations in factorization process simultaneously.  NB (neighborhood-based recommender): this algorithm is the basic memory-based recommender algorithm that predicts a rating of a target item i for user u using a combination of the ratings of neighbors of u (similar users) that already issued a rating for item i . Formally, ^ui = R u + R
u N (u ),Wuu >0

u ) Wuu (R ui - R Wuu , (9)

u N (u ),Wuu

where the pairwise weight Wuu between pair of users (u , u ) is calculated by Pearson's correlation coefficient [25]  NB+T (neighborhood with trust information) [45, 17, 47]: the basic idea behind the trust based recommender systems proposed in TidalTrsut [17] and MoleTrsut [45] is to limit the set of neighbors in (9) to the users who are trusted by user u . The distinguishing feature of these algorithms is the mechanism of trust propagation to estimate the trust transitively for all the users. By adapting (9) to only consider trustworthy neighbors in predicting the new ratings we obtain: ^ui = R u + R
 (u ),W u N+ uu >0

u ) Wuu (R ui - R Wuu , (10)

 (u ),W u N+ uu >0

 where N+ (u ) is the set of trusted neighbors of u in the social network with propagated trust rela tions (when there is no propagation we have N+ (u ) = N+ (u )). We note that instead of Pearson's correlation coefficient as the wighting schema, we can infer the weights exploiting the social relation between the users. Since for the data set we consider in our experiments, the trust/distrust relations are binary values, the social based pairwise distance would be simply the hamming distance between the binary vector representation of social relations of users. For implementation details we refer to [70, Chapter 6].

20

Table 5: The consistency of implicit and explicit trust relations in the data set for different ranges of ratings measured in terms of NDCG, recall, and MAP . # of Ratings NDCG@10 NDCG@20 Recall@10 Recall@20 Recall@40 MAP 0-20 21-40 41-60 61-80  81 0.083 0.108 0.117 0.120 0.135 0.078 0.103 0.112 0.117 0.126 0.054 0.080 0.083 0.088 0.091 0.092 0.125 0.128 0.132 0.151 0.156 0.198 0.225 0.230 0.253 0.140 0.190 0.208 0.230 0.244

Table 6: The consistency of implicit and explicit distrust relations in the data set for different ranges of ratings measured in terms of NDCG, recall, and MAP . # of Ratings NDCG@10 NDCG@20 Recall@10 Recall@20 Recall@40 MAP 0-20 21-40 41-60 61-80  81 0.065 0.071 0.082 0.089 0.104 0.057 0.068 0.072 0.078 0.096 0.045 0.060 0.075 0.081 0.087 0.071 0.077 0.085 0.105 0.125 0.132 0.140 0.158 0.164 0.191 0.130 0.134 0.152 0.160 0.183

 NB+TD-F (neighborhood with trust information and distrust information as filtration) [69, 72]: a simple strategy to use distrust relations in the recommendation is to filter out distrusted users from the list of neighbors in predicting the ratings. As a result, we adapt (9) to exclude distrusted users from the users' propagated web of trust.  NB+TD-D (neighborhood-based with trust information and integrated distrust information) [69, 72]: in the same spirit as the filtration strategy, we can use distrust relations to debug the trust relations. More specifically, if user u trusts user v , v trusts w , and u distrusts w , then the latter distrust relation contradicts the propagation of the trust from u to w and can be excluded from the prediction. In this method distrust is used to debug the trust relations.

5.5 On the Consistency of Social Relations and Rating Information
As already mentioned, the Epinions website allows users to write reviews about products and services and to rate reviews written by other users. Epinions also allows users to define their web of trust, i.e. "reviewers whose reviews and ratings have been consistently found to be valuable" and their block list, i.e. "reviewers whose reviews are found to be consistently inaccurate or not valuable. Different intuitions on interpreting these social information will result in different models. The main rational behind incorporating trust and distrust relations in recommendation process is to take the trust/distrust relations between users in the social network as the level of agreement between ratings assigned to reviews by users 3 . Therefore, investigating the consistency or alignment between user ratings (implicit trust) and trust/distrust relations in the social network (explicit trsut) become an important issue. Here, we aim to empirically investigate whether or not there is a correlation between a user's current trustees/friends or distrusted friends and the ratings that user would assign to reviews issued by his neighbors. Obviously, if there is no correlation between social context of a user and his/her ratings to reviews written by his neighbors, then the social structure does not provide any advantage to the rating information. On the other hand, if there exists such a correlation, then the social context could be supplementary information to compensate for the lack of rating information to boost the accuracy of recommendations. The consistency of trust relations and rating information issued by users on the reviews written by his trustees has been analyzed in [83, 23]. However, [83] also claimed that social trust (i.e., explicit trust) and similarity between users based on their issued ratings (i.e., implicit trust) are not the same,
3 In the literature the similarity between users conveyed from the rating information issued by users and the direct relation in the social network are usually referred to as the implicit and the explicit trust, respectively.

21

Table 7: The alignment rate of users in establishing trust/distrust relationships with future users in the social network based on the majority vote of their current trusted/distrusted friends. The number of trusted friends (+) and distrusted friends (-) are denoted by n + and n - , respectively. Here u denotes the current user and w stands for a future user in the network. Setting Type of Relation (u w ) % of Relations Alignment Rate (%) n+ > n- n+ < n- n + = n - > 0 or n + = n - = 0 + + + 48.80 2.54 1.15 8.02 39.49 92.09 8.15 17.88 83.42 -

and can be used complementary. According to [38], when comparing implicit social information with explicit social information, the performance of using implicit information is slightly worse. We further investigate the same question about the consistency of distrust relations and ratings issued by users to their distrusted neighbors. The positive answer to this question can be interpreted as follows. Given that user u is interested in item i , the chances that v , trusted (distrusted) by u , also likes this item i is much higher (lower) than for user w not explicitly trusted (distrusted) by u . To measure the similarity between users, there are several methods we can borrow in the literature. In this paper, we adopt the most popular approach that is referred to as Pearson correlation coefficient (PCC) P : U  U  [-1, +1] [6, 47], which is defined as: P (u , v ) =
m i =1 (R ui m j =1 (R ui

u )(R vi - R v ) -R
m j =1 (R vi

 u )2  -R

 v )2 -R

, u , v  U ,

u and R v are the average of ratings issued by users u and v , respectively. The PCC measures where R the extent to which there is a linear relationship between the rating behaviors of the two users, the extreme values being -1 and 1. The similarity of two users becomes negative when users have completely diverging ratings. We note that this quantity can be considered as the implicit trust between users that is conveyed via ratings given by users. To conduct this set of experiments, we first group all the users in the training data set based on the number of ratings, and then measure the prediction accuracies of different user groups. Users are grouped into five classes: "[1, 20)", "[20, 40)", "[40, 60)", "[60, 80)", and "> 81 ". In order to have a comprehensive view of the ranking performance, we present the NDCG, recall and MAP scores of trust and distrust alignments on the Epinions data set in Table 5 and Table 6, respectively. We note that the data set we use in our experiments only contains bivalent trust values, i.e., -1 and +1, and it is not possible to have an ordering on the list of friends (timestamp of relations would be an option to order the friends but unfortunately it is not available in our data set). To compute the NDCG, we use the ordering of trusted/distrusted friends which yields the best value. On the positive side, we observe a clear trend of alignment between ratings assigned by a user and the type of relation he has made in the social network. This observation coincides with our intuition. Overall, when more ratings are observed for a user, the similarity calculation process will find more accurate similar or dissimilar neighbors for this user since we have more information to represent or interpret this user. Hence, by increasing the number of ratings, It is conceivable from the results in Tables 5 and 6 that the alignment between implicit and explicit neighbors becomes better. By comparing the results in Tables 5 and 6 we can see that trust relations are slightly better aligned than the distrust relations. On the negative side, the results show that the NDCG on both types of relations is small. One explanation for this phenomenon is that the Epinions data set is not tightly bound to a specific application. For example, a user may trust or distrust anther user based on his/her comments on a specific product but they might have similar taste on other products. Furthermore, compared to other data sets such as FilmTrusts, the Epinions data set is very sparse data set, and consequently it is relatively inaccurate to rely on the rating information to compute the implicit trust relations. Finally, our approach to distinguish trust/distrust lists from the rating information is limited by the PCC trust metric we have utilized. We conjecture that better trust metrics that is able to exploit other side information such as time and in22

Table 8: The accuracy of prediction of matrix factorization with three different methods measured in terms of MAE and RMSE errors. The parameter k represents the number of latent features in factorization. k % of Training Measure MF MF+T MF+D MF+TD 10 60% 70% 80% 90% 20 60% 70% 80% 90% MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE 0.9813  0.042 1.6050  0.032 0.9462  0.083 1.5327  0.032 0.9150 0.022 1.3824  0.032 0.8921  0.025 1.2166  0.017 0.9972  0.016 1.6248  0.014 0.9688  0.019 1.5162  0.016 0.9365  0.025 1.4081  0.015 0.9224  0.016 1.2207  0.0 18 0.8561  0.032 1.4125  0.022 0.8332  0.092 1.2407  0.063 0.8206  0.041 1.1906  0.042 0.8158  0.016 1.1403  0.027 0.8431  0.018 1.3904  0.042 0.8342  0.062 1.2722  0.027 0.8172  0.011 1.1853  0.023 0.8128  0.021 1.1402  0.026 0.9720  0.038 1.5036  0.040 0.9241  0.012 1.4405  0.023 0.8722  0.034 1.3155  0.026 0.8736  0.053 1.1869  0.049 0.9746  0.060 1.5423  0.046 0.9350  0.022 1.4540  0.075 0.8705  0.016 1.3591  0.073 0.8805  0.032 1.1933  0.028 0.8310  0.016 1.2294  0.086 0.8206  0.023 1.1562  0.043 0.8113  0.032 1.1061  0.021 0.8025  0.014 1.0872  0.020 0.8475  0.012 1.1837  0.023 0.8290  0.034 1.1452  0.016 0.8129  0.025 1.1049  0.082 0.8096  0.010 1.0851  0.011

teractional information would be helpful in distinguishing implicit trusted/distrusted friends, leading to better alignment between implicit and explicit trust relations. We also conduct experiments to evaluate the consistency of social network only based on the trust/distrust relations between users. In particular, we investigate to what extent a users' relations are aligned with the opinion of his/her neighbors in the social network. More specifically, let u be a user who is about to make a trust or distrust relation to another user v . We assume that n + number of u 's neighbors trust v and n - number of u 's neighbors distrust v . We note that in the real data set the distrust relations are hidden. To conduct this set of experiments, we randomly sample 30% of the relations from the social network and use the remaining 70% to predict the type of sampled relations 4 by majority voting. Table 7 shows the results on the consistency of social relations. We observe that in all cases there is an alignment between the opinions of users' friends and his/her own relation (92.09% and 83.42% when the majority of friends trust and distrust the target user, respectively). This might be due to social influence of people on social network, however, it is hard to justify the existence of such a correlation in Epinions data set which includes reviews for diverse set of products and taste of users. One interesting observation from the results reported in Table 7 is the case where the number of distrusted users dominates the number of trusted users (i.e., n - > n + ). While the distrust relations are private to other users, but we can see that there is a significant alignment between users's relation type and his distrusted friends.

5.6 On the Power of Utilizing Social Relationships
We now turn to investigate the effect of utilizing social relationships between users on the accuracy of recommendations in factorization-based methods. In other words, we would like to experimentally evaluate whether incorporating distrust can indeed enhance the trust-based recommendation process. To this end, we run four different MF (i.e., pure matrix factorization based algorithm), MF+T (i.e., matrix factorization with only trust relationships), MF+D (i.e., matrix factorization with only distrust relationships), and MF+TD (i.e., the algorithm proposed here) algorithms on the data set. We run the algorithms with k = 10 and k = 20 latent vector dimensions. As mentioned earlier, different amount of training data 90%, 80% , 70% and 60% has been used to create four different training sets that are increasingly sparse but the social network remains the same in all of them. We evaluate all algorithms by both MAE and
4 A more realistic way would be to use the timestamp of relations to create the training and test sets.

23

RMSE measures. Table 8 shows the MAE and RMSE errors for the four sampled data sets. First, as we expected, the performance of all learning algorithms improves with an increasing number of training data. It is also not surprising to see that the MF+T, MF+D, and MF+TD algorithms which exploit social side information perform better than the pure matrix factorization based MF algorithm. Second, the proposed algorithm outperforms all other baseline algorithms for all the cases, indicating that it is effective to incorporate both types of social side information in recommendation. This result by itself indicates that besides trust relationships in the social network, the distrust information is also a rich source of information and can be utilized in recommendation algorithms. We note that distrust information needs to be incorporated carefully as its nature is totally different from trust information. Finally, it is noticeable that the MF+T outperforms the MF+D algorithm due to huge number of trust relations to distrust relations in our data set. It is also remarkable that users are more likely to be influenced by their friends to make trust relations than the distrust relations due to the private nature of distrust relations in Epinions data set. This might lead us to believe that distrust relations have better quality than trust relations which requires a deeper investigation to be verified.

5.7 Comparison to Baseline Algorithms
Another question that is worthy of investigation is how state-of-the-art approaches perform compared to the method proposed in this paper. To this end, we compare the performance of the MF-TD algorithm with the baseline algorithms introduced in Subsection 5.4. Table 9 contains the results of our experiments with eight different algorithms on the data set with 90% of rating data. The second column in the table represents the configuration of parameters used by each algorithm. When we utilize trust/distrust relations in neighborhood-based algorithms, a crucial decision we need to make is to which level the propagation must be performed (no propagation corresponds to the single level propagation which only includes direct neighbors). Let p and q denote the level of propagation for trust and distrust relations, respectively. Let us first consider the trust propagation to decide the value of p . We note that there is a tradeoff between accuracy and the level of trust propagation: the longer propagation levels results in less accurate trust predictions. This is due the fact that when we use longer propagation levels, the further away we are heading from each user, and consequently decrease the confidence on the predictions. Obviously this affects the accuracy of the recommendations significantly. As a result, for the trust propagation we only consider single level propagation by choosing p = 1  (i.e, N+ = N+ ). We also note that since in the Epinions data set a user can not simultaneously trust and distrust another user, in the neighborhood-based method with distrust relations, the debugging only makes sense for propagated information. Therefore, we perform a three level distrust propagation (q = 3) to constitute the set of distrusted users for each users. We note that the longer the propagation levels, the more often distrust evidence can be found for a particular user, and hence the less neighbors will be left to participate in the recommendation process. For factorization based methods, the value of regularization parameters, i.e., U , V , and S , are determined by the procedure discussed in Subsection 5.3. The results of Table 9 reveal some interesting conclusions as summarized below:  From Table 9, we can observe that for factorization-based methods, incorporating trust or distrust information boost the performance of recommendation in terms of both accuracy measures. This demonstrates the advantages of trust and distrust-aware recommendation algorithms. We also can see that both MF+T and MF+D perform better than the non-social MF but the performance of MF+T is significantly better than MF+D. As discussed before, this observation does not indicate that the trust relations are more beneficial than the distrust relations as in our data set only 16.7% of relations are distrust relations. The MF+TD algorithm that is able to employ both types of relations is significantly better than other algorithms that demonstrates the advantages of proposed method to utilize trust and distrust relations.  Looking at the results reported in Table 9, it can immediately be noticed that the incorporation of trust and distrust information in neighborhood-based methods decreases the prediction error but the improvement is not as significant as the factorization based methods. We note that for the NB+T method with longer levels of propagation (p = 2, 3), our experiments revealed that the accuracy remains almost same or gotten worse on both MAE and RMSE measures and this is 24

Table 9: Comparison with other popular methods. The reported values are the MAE and RMSE on the data set with 90% of rating information. The values of parameters for each specific algorithm is included in the second column. Method Parameter (s) MAE RMSE MF MF+T MF+D MF+TD NB NB+T NB+TD-F NB+TD-D k = 10 and U = V = 5 k = 10, U = V = 5 , and  = 1 k = 10, U = V = 5 , and  = 10 k = 10, U = 13, V = 11 , and S = 14.8 p =1 p = 1 and q = 3 p = 1 and q = 3 0.8921 0.8158 0.8736 0.8025 0.9381 0.8904 0.8692 0.8728 1.2166 1.1403 1.1852 1.0872 1.5275 1.3455 1.2455 1.2604

Table 10: The accuracy of handling cold-start users and the effect of social relations. The number of leant features in this experiments is set to k = 10. The first column shows the number of cold-start users sampled randomly from all users in the data set. For the cold-starts users all the ratings have been excluded from the training data and used in the evaluation of three different algorithms. % of Cold-start Users Measure MF MF+T MF+D MF+TD 30% 20% 10% 5% MAE RMSE MAE RMSE MAE RMSE MAE RMSE 0.9923 1.7211 0.9812 1.7088 0.9334 1.4222 0.9134 1.3852 0.8824 1.5562 0.8805 1.4339 0.8477 1.3782 0.8292 1.2921 0.9721 1.6433 0.9505 1.6250 0.9182 1.4006 0.8633 1.3255 0.8533 1.4802 0.8472 1.2630 0.8322 1.2655 0.8280 1.2888

why we only report the results only for p = 1. In contrast, for distrust propagation we found out that q = 3 has a visible impact on the performance of both filtering and debugging methods. We would like to emphasize that for longer levels of distrust propagation in Epinions data set, i.e.,  q > 4, we found that the size of the set of distrusted users N- () becomes large for most of users which degrades the prediction accuracy. We also observe another interesting result about the performance of NB+TD method with filtering and debugging strategies. We found that although filtering generates slightly better predictions, NB+TD-F performs almost as good as the NB+TDD method. Although this observation does not suggest any of these methods as the method of choice in incorporating distrust, we believe that the accuracy might differ from data set to data set and it strongly depends on the propagation/aggregation strategy.  Considering the results for both model-based and memory-based methods in Table 9, we can conclude few interesting observations. First, we notice that factorization-based methods with trust/distrust information perform better than the neighborhood based methods. Second, the incorporation of trust and distrust relations in matrix factorization has significant improvement compared to improvement achieved by memory-based methods. Although the type of filtration or debugging strategy could significantly affect the accuracy of incorporating distrust in memorybased methods, but the main shortcoming of these methods comes from the fact that these algorithms somehow exclude the influence of distrusted users from the rating prediction. This stands in stark contrast to the model proposed in this paper that ranks the neighbors based on the type of relation. This observation necessitates to devise better algorithms for propagation and aggregation of trust/distrust information in memory-based methods.

25

Table 11: The accuracy of proposed algorithm on a data set with 390257 ( 90%) trust relations sampled uniformly at random from all trust relations with varied number of distrust relations. The learning is performed based on 90% of all ratings with k = 10 as the dimension of latent features. Method # of Trust Relations # of Distrust Relations Measure Accuracy MF+TD 433,619 ( 90%) 9,682 ( 10%) 19,364 ( 20%) 29,047 ( 30%) 38,729 ( 40%) 48,411 ( 50%) 58,093 ( 60%) 67,776 ( 70%) 77,458 ( 80%) 87,140 ( 90%) 96,823 (= 100%) MF+T 481,799 (= 100%) 0 MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE 0.8803  0.051 1.2166  0.028 0.8755  0.033 1.1944  0.042 0.8604  0.036 1.1822  0.081 0.8431  0.047 1.1706 0.055 0.8165 0.056 1.1425 0.091 0.8130 0.035 1.1380 0.046 0.8122  0.041 1.1306  0.042 0.8095  0.036 1.1290  0.085 0.8061  0.044 1.1176  0.067 0.8050  0.052 1.1092  0.063 0.8158  0.016 1.1403  0.027

5.8 Handling Cold-start Users by Social Side Information
In this subsection, we demonstrate the use of social network to further illustrate the potential of proposed framework and the relevance of incorporating side information. To do so, as another set of our experiments, we intend to examine the performance of proposed algorithm on clod-start users. Addressing cold-start users (i.e., users with few ratings or new users) is a very important for the success of recommender systems due to huge number of this type of users in many real world systems. As a result, handling cold-start users is one the main challenges in existing systems. To evaluate different algorithms we randomly select 30%, 20%, 10%, and 5% as the cold-start users. For cold-start users, we do not include any rating in the training data and consider all the ratings made by cold-start users as testing data. Table 10 shows the performance of above mentioned algorithms. As it is clear from the Table 10, when the number of cold-start users is low with respect to the total number of users, say 5% of total users, the affect of distrust relationships is negligible in prediction accuracy. But, when the number of cold-start users is high, exploiting the trust and distrust relationships significantly improve the performance of recommendation. This result is interesting as it reveals that the lack of rating information for cold-start and new users can be alleviated by incorporating the social relations of users, and in particular both trust and distrust relationships.

5.9 Trading Trust for Distrust Relationships
We also compare the potential benefit of trust relations to distrust relations in the proposed algorithm. More specifically, we would like to see in what extent the distrust relations can compensate for the lack of trust relations. We run the proposed algorithm with the subset of trust and distrust relations and compare it to the algorithm which only utilizes all of the trust relations. To setup this set of experiments, we randomly sample a subset of trust relations and gradually increase the amount of distrust relations to see when the effect of distrust information compensate the effect of missed trust relations.

26

We sample 433,619 (approximately 90%) trust relations from the total 481,799 trust relations and vary the number of distrust relations and feed to the proposed algorithm. Table 11 reports the accuracy of proposed algorithm for different number of distrust relations in the data sets. All these samplings have been done uniformly at random. We use 90% of all ratings for training and the remaining 10% for evaluation, and set the dimension of latent features to k = 10. As it can be concluded from Table 11, when we feed the proposed algorithm MF+TD with 90% of trust and 50% of the distrust relations, it reveals very similar behavior to the trust-enhanced matrix factorization based method MF+T, which only utilizes all the trust relations in factorization. This result is interesting in the sense that the distrust information between users is as important as the trust information (we note that in this scenario the number trust relations excluded from the training is almost same as the number of distrust relations included). By increasing the number of distrust relations we can observe that the accuracy of recommendations increases as expected. In summary, this set of experiments validates that incorporating distrust relations can indeed enhance the trust-based recommendation process and could be considered as a rich source of information to be exploited.

5.10 On the Impact of Batch Size in Stochastic Optimization
As mentioned earlier in the paper, directly solving the optimization problem in (5) using full gradient descent method requires to go through all the triplets in the constraint set S which could be computationally expensive due to the huge number of triplets in S . To overcome this efficiency problem, one can turn to stochastic gradient scent method which tries to generate unbiased estimates of the gradient at each iteration in a much cheeper way by sampling a subset of triplets from S . To accomplish this goal, we perform gradient descent and stochastic gradient descent to solve the optimization problem in (5) to find the matrices U and V following the updating equations derived in (7) and (8). At each iteration t , the currently learned matrices Ut and Vt are used to predict the ratings in the test set. In particular, at each iteration, we evaluate the RMSE and MAE on the test set, and terminate training once the RMSE and MAE starts increasing, or the maximum number of iterations is reached. We run the algorithm with latent vectors of dimension k = 10. We compare the computational efficiency between proposed algorithm with GD and mini-batch SGD with different batch sizes. We note that the GD updating rule can be considered as min-batch SGD where the batch size B is deterministically set to be B = |S | and simple SGD can be considered as mini-batch SGD with B = 1. We remark that in contrast to GD method which uses all the triplets in S for gradient computation at each iteration, for SGD method due to uniform sampling over all tuples in S , some of the tuples may be used more than once and some of the tuples might never been used for gradient computation. Figures 3 and 4 show the convergence rate of four different updating rules in terms of the number of iterations t for two different measures RMSE and RME, respectively. The first algorithm denoted by GD runs the simple full gradient descent iteratively to optimize the objective. The other three algorithms named SGD1, SGD2, and SGD3 in the figures use the batch sizes of B = 0.1  |S |, B = 0.2  |S |, and B = 0.3  |S |, respectively. In our experiments, due to very slow convergence of the basic SGD method with B = 1 in comparison to other fours methods, we simply exclude its result from the discussion. In terms of accuracy of predictions, from both Figures 3 and 4, we can conclude that the GD has the best convergence and SGD3 has the worst convergence in all settings. This is because, although all of the four algorithms use an unbiased estimate of the true gradient to update the solution at each iteration, but the variance of each stochastic gradient is proportional to the size of the batch size B . Therefore, for larger values of B , the variance of stochastic gradients is smaller and the algorithm convergences faster, but, for smaller values of B the algorithm suffers from high variance in stochastic gradients and convergences slowly. We emphasize that this comparison holds for iteration complexity which is different from the computational complexity (running time) of individual iterations. More specifically, each iteration of GD requires |S | gradient computations, while for SGD we only need to perform B |S | gradient computations. In summary, SGD has lightweight iteration but requires more iterations to converge. In contrast, GD takes expensive steps in much less number of iterations. From Figures 3 and 4, it is noticeable that although a large number of iterations is usually needed to obtain a solution of desirable accuracy using SGD, the lightweight computation per iteration makes SGD attractive for the optimization problem in (5) for large number of users. We also not that for the GD method, the error is a monotonically decreasing function it terms of number of iterations t , but for the SGD

27

based methods this does not hold. This is because although SGD algorithm is guaranteed to converge to an optimal solution (at least in expectation), but there is no guarantee that the stochastic gradients provide a descent direction for the objective at each iteration due to the noise in computing gradients. As a result, for few iterations we can see that the objective increases but finally it convergences as expected.

6 Conclusions and Future Works
In this paper, we have made a progress towards making distrust information beneficial in social recommendation problem. In particular, we have proposed a framework based on the matrix factorization which is able to incorporate both trust and distrust relationships between users in factorization algorithm. We experimentally investigated the potential of distrust as a side information to overcome the data sparsity and cold-start problems in traditional recommender systems. In summary, our results showed that more accurate recommendations can be obtained by incorporating distrust relations, indicating that distrust information can indeed be beneficial for the recommendation process. This work leaves few directions, both theoretically and empirically, as future work. From an empirical point of view, it would be interesting to extend our model for weighted social trust and distrust relations. One challenge in this direction is that, as far as we know, there is no publicly available data set that includes weighted (gradual) trust and distrust information. Also, the experimental results we have conducted on the consistency of social relations with rating information hint at a number of potential enhancements in future work. In particular, it would be interesting to further examine the correlation between implicit and explicit distrust information. An important challenge in this direction is to develop better metrics to measure the implicit trust between users as the simple metrics such as Pearson correlation coefficient seem to be insufficient. Furthermore, since we only consider the distrust between users, it would be easy to generalize our model in the same way to incorporate dissimilarity between items and investigate how it works in practice. Also, our preliminary results indicated that hinge loss almost performs better than the exponential loss, but from the optimization viewpoint, the exponential loss is more attractive due to its smoothness. So, an interesting direction would be to use a smoothed version of the hinge loss to gain from both optimization efficiency and algorithmic accuracy.

References
[1] Gediminas Adomavicius and Alexander Tuzhilin. Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions. IEEE Transactions on Knowledge and Data Engineering, 17(6):734749, 2005. [2] Deepak Agarwal and Bee-Chung Chen. flda: matrix factorization through latent dirichlet allocation. In Proceedings of the third ACM international conference on Web search and data mining, pages 91100. ACM, 2010. [3] Paolo Avesani, Paolo Massa, and Roberto Tiella. A trust-enhanced recommender system application: Moleskiing. In Proceedings of the 2005 ACM symposium on Applied computing, pages 1589 1593, 2005. [4] Giacomo Bachi, Michele Coscia, Anna Monreale, and Fosca Giannotti. Classifying trust/distrust relationships in online social networks. In Privacy, Security, Risk and Trust (PASSAT), 2012 International Conference on and 2012 International Confernece on Social Computing (SocialCom), pages 552557. IEEE, 2012. [5] Jess Bobadilla, Fernando Ortega, Antonio Hernando, and Abraham Gutirrez. Recommender systems survey. Knowledge-Based Systems, 46:109132, 2013. [6] John S Breese, David Heckerman, and Carl Kadie. Empirical analysis of predictive algorithms for collaborative filtering. In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence, pages 4352. Morgan Kaufmann Publishers Inc., 1998.

28

[7] Moira Burke and Robert Kraut. Mopping up: modeling wikipedia promotion decisions. In Proceedings of the 2008 ACM conference on Computer supported cooperative work, pages 2736. ACM, 2008. [8] Dorwin Cartwright and Frank Harary. Structural balance: a generalization of heider's theory. Psychological review, 63(5):277, 1956. [9] Gang Chen, Fei Wang, and Changshui Zhang. Collaborative filtering using orthogonal nonnegative matrix tri-factorization. Information Processing & Management, 45(3):368379, 2009. [10] Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan. Better mini-batch algorithms via accelerated gradient methods. In NIPS, volume 24, pages 16471655, 2011. [11] David Crandall, Dan Cosley, Daniel Huttenlocher, Jon Kleinberg, and Siddharth Suri. Feedback effects between similarity and social influence in online communities. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 160168. ACM, 2008. [12] Sanjoy Dasgupta, Michael L Littman, and David McAllester. Pac generalization bounds for cotraining. Advances in neural information processing systems, 1:375382, 2002. [13] Mukund Deshpande and George Karypis. Item-based top-n recommendation algorithms. ACM Transactions on Information Systems (TOIS), 22(1):143177, 2004. [14] Thomas DuBois, Jennifer Golbeck, and Aravind Srinivasan. Predicting trust and distrust in social networks. In Privacy, security, risk and trust (passat), 2011 ieee third international conference on and 2011 ieee third international conference on social computing (socialcom), pages 418424. IEEE, 2011. [15] Rana Forsati, Hanieh Mohammadi Doustdar, Mehrnoush Shamsfard, Andisheh Keikha, and Mohammad Reza Meybodi. A fuzzy co-clustering approach for hybrid recommender systems. International Journal of Hybrid Intelligent Systems, 10(2):7181, 2013. [16] Rana Forsati and Mohammad Reza Meybodi. Effective page recommendation algorithms based on distributed learning automata and weighted association rules. Expert Systems with Applications, 37(2):13161330, 2010. [17] Jennifer Golbeck. Computing and applying trust in web-based social networks. PhD thesis, 2005. [18] Jennifer Golbeck. Generating predictive movie recommendations from trust in social networks. Springer, 2006. [19] Jennifer Golbeck and James Hendler. Filmtrust: Movie recommendations using trust in web-based social networks. In Proceedings of the IEEE Consumer communications and networking conference, volume 96. Citeseer, 2006. [20] Nathaniel Good, J Ben Schafer, Joseph A Konstan, Al Borchers, Badrul Sarwar, Jon Herlocker, and John Riedl. Combining collaborative filtering with personal agents for better recommendations. In AAAI/IAAI, pages 439446, 1999. [21] Quanquan Gu, Jie Zhou, and Chris Ding. Collaborative filtering: Weighted nonnegative matrix factorization incorporating user and item graphs. In SIAM SDM, pages 199210, 2010. [22] R. Guha, Ravi Kumar, Prabhakar Raghavan, and Andrew Tomkins. Propagation of trust and distrust. In Proceedings of the 13th International Conference on World Wide Web, pages 403412. ACM, 2004. [23] Guibing Guo, Jie Zhang, Daniel Thalmann, Anirban Basu, and Neil Yorke-Smith. From ratings to trust: an empirical study of implicit trust in recommender systems. In SAC, 2014. [24] Jonathan L Herlocker, Joseph A Konstan, Al Borchers, and John Riedl. An algorithmic framework for performing collaborative filtering. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 230237. ACM, 1999. 29

[25] Jonathan L Herlocker, Joseph A Konstan, Loren G Terveen, and John T Riedl. Evaluating collaborative filtering recommender systems. ACM Transactions on Information Systems (TOIS), 22(1):553, 2004. [26] Thomas Hofmann. Latent semantic models for collaborative filtering. ACM Transactions on Information Systems (TOIS), 22(1):89115, 2004. [27] Mohsen Jamali and Martin Ester. Trustwalker: a random walk model for combining trust-based and item-based recommendation. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 397406. ACM, 2009. [28] Mohsen Jamali and Martin Ester. A matrix factorization technique with trust propagation for recommendation in social networks. In Proceedings of the fourth ACM conference on Recommender systems, pages 135142. ACM, 2010. [29] Mohsen Jamali and Martin Ester. A transitivity aware matrix factorization model for recommendation in social networks. In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence-Volume Volume Three, pages 26442649. AAAI Press, 2011. [30] Arnd Kohrs and Bernard Merialdo. Clustering for collaborative filtering applications. In In Computational Intelligence for Modelling, Control & Automation. IOS. Citeseer, 1999. [31] Ioannis Konstas, Vassilios Stathopoulos, and Joemon M Jose. On social networks and collaborative recommendation. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 195202. ACM, 2009. [32] Yehuda Koren. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 426434. ACM, 2008. [33] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):3037, 2009. [34] Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. Predicting positive and negative links in online social networks. In Proceedings of the 19th international conference on World wide web, pages 641650. ACM, 2010. [35] Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. Signed networks in social media. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 13611370. ACM, 2010. [36] Wu-Jun Li and Dit-Yan Yeung. Relation regularized matrix factorization. IJCAI-09, 2009. [37] Juntao Liu, Caihua Wu, and Wenyu Liu. Bayesian probabilistic matrix factorization with social relations and item contents for recommendation. Decision Support Systems, 2013. [38] Hao Ma. An experimental study on implicit social recommendation. In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval, pages 7382. ACM, 2013. [39] Hao Ma, Irwin King, and Michael R Lyu. Learning to recommend with social trust ensemble. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 203210. ACM, 2009. [40] Hao Ma, Michael R Lyu, and Irwin King. Learning to recommend with trust and distrust relationships. In Proceedings of the third ACM conference on Recommender systems, pages 189196. ACM, 2009. [41] Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin King. Sorec: social recommendation using probabilistic matrix factorization. In Proceedings of the 17th ACM conference on Information and knowledge management, pages 931940. ACM, 2008.

30

[42] Hao Ma, Dengyong Zhou, Chao Liu, Michael R Lyu, and Irwin King. Recommender systems with social regularization. In Proceedings of the fourth ACM international conference on Web search and data mining, pages 287296. ACM, 2011. [43] Hao Ma, Tom Chao Zhou, Michael R Lyu, and Irwin King. Improving recommender systems by incorporating social contextual information. ACM Transactions on Information Systems (TOIS), 29(2):9, 2011. [44] Christopher D Manning, Prabhakar Raghavan, and Hinrich Schtze. Introduction to information retrieval, volume 1. Cambridge university press Cambridge, 2008. [45] Paolo Massa and Paolo Avesani. Trust-aware collaborative filtering for recommender systems. In On the Move to Meaningful Internet Systems 2004: CoopIS, DOA, and ODBASE, pages 492508. Springer, 2004. [46] Paolo Massa and Paolo Avesani. Controversial users demand local trust metrics: An experimental study on epinions. com community. In Proceedings of the National Conference on artificial Intelligence, volume 20, page 121. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2005. [47] Paolo Massa and Paolo Avesani. Trust metrics in recommender systems. In Computing with Social Trust, pages 259285. Springer, 2009. [48] Prem Melville, Raymond J Mooney, and Ramadass Nagarajan. Content-boosted collaborative filtering for improved recommendations. In AAAI/IAAI, pages 187192, 2002. [49] Bradley N Miller, Joseph A Konstan, and John Riedl. Pocketlens: Toward a personal recommender system. ACM Transactions on Information Systems (TOIS), 22(3):437476, 2004. [50] Andriy Mnih and Ruslan Salakhutdinov. Probabilistic matrix factorization. In Advances in neural information processing systems, pages 12571264, 2007. [51] Uma Nalluri. Utility of distrust in online recommender systems. Technical report, 2008. [52] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574 1609, 2009. [53] Akshay Patil, Golnaz Ghasemiesfeh, Roozbeh Ebrahimi, and Jie Gao. Quantifying social influence in epinions. HUMAN, 2(2):pp67, 2013. [54] Dmitry Pavlov and David M Pennock. A maximum entropy approach to collaborative filtering in dynamic, sparse, high-dimensional domains. In NIPS, volume 2, pages 14411448, 2002. [55] Michael J Pazzani. A framework for collaborative, content-based and demographic filtering. Artificial Intelligence Review, 13(5-6):393408, 1999. [56] David M Pennock, Eric Horvitz, Steve Lawrence, and C Lee Giles. Collaborative filtering by personality diagnosis: A hybrid memory-and model-based approach. In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence, pages 473480. Morgan Kaufmann Publishers Inc., 2000. [57] Jasson DM Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative prediction. In Proceedings of the 22nd international conference on Machine learning, pages 713719. ACM, 2005. [58] Ruslan Salakhutdinov and Andriy Mnih. Bayesian probabilistic matrix factorization using markov chain monte carlo. In Proceedings of the 25th international conference on Machine learning, pages 880887. ACM, 2008. [59] Ruslan Salakhutdinov and Andriy Mnih. Probabilistic matrix factorization. Advances in neural information processing systems, 20:12571264, 2008. 31

[60] Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. Restricted boltzmann machines for collaborative filtering. In Proceedings of the 24th international conference on Machine learning, pages 791798. ACM, 2007. [61] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. Item-based collaborative filtering recommendation algorithms. In Proceedings of the 10th international conference on World Wide Web, pages 285295. ACM, 2001. [62] Wanita Sherchan, Surya Nepal, and Cecile Paris. A survey of trust in social networks. ACM Computing Surveys (CSUR), 45(4):47, 2013. [63] Luo Si and Rong Jin. Flexible mixture model for collaborative filtering. In ICML, volume 3, pages 704711, 2003. [64] Ian Soboroff and Charles Nicholas. Combining content and collaboration in text filtering. In Proceedings of the IJCAI, volume 99, pages 8691, 1999. [65] Nathan Srebro, Tommi Jaakkola, et al. Weighted low-rank approximations. In ICML, volume 3, pages 720727, 2003. [66] Nathan Srebro, Jason DM Rennie, and Tommi Jaakkola. Maximum-margin matrix factorization. Advances in neural information processing systems, 17(5):13291336, 2005. [67] Mojdeh Talabeigi, Rana Forsati, and Mohammad Reza Meybodi. A hybrid web recommender system based on cellular learning automata. In Granular Computing (GrC), 2010 IEEE International Conference on, pages 453458. IEEE, 2010. [68] Nele Verbiest, Chris Cornelis, Patricia Victor, and Enrique Herrera-Viedma. Trust and distrust aggregation enhanced with path length incorporation. Fuzzy Sets and Systems, 202:6174, 2012. [69] Patricia Victor, Chris Cornelis, Martine De Cock, and Ankur Teredesai. Trust- and distrust-based recommendations for controversial reviews. IEEE Intelligent Systems, 26(1):4855, 2011. [70] Patricia Victor, Chris Cornelis, and Martine De Cock. Trust networks for recommender systems, volume 4. Springer, 2011. [71] Patricia Victor, Chris Cornelis, Martine De Cock, and Enrique Herrera-Viedma. Practical aggregation operators for gradual trust and distrust. Fuzzy Sets and Systems, 184(1):126147, 2011. [72] Patricia Victor, Nele Verbiest, Chris Cornelis, and Martine De Cock. Enhancing the trust-based recommendation process with explicit distrust. ACM Transactions on the Web (TWEB), 7(2):6, 2013. [73] Fei Wang, Sheng Ma, Liuzhong Yang, and Tao Li. Recommendation on item graphs. In Data Mining, 2006. ICDM'06. Sixth International Conference on, pages 11191123. IEEE, 2006. [74] Jun Wang, Arjen P De Vries, and Marcel JT Reinders. Unifying user-based and item-based collaborative filtering approaches by similarity fusion. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501508. ACM, 2006. [75] Grzegorz Wierzowiecki and Adam Wierzbicki. Efficient and correct trust propagation using closelook. In Web Intelligence and Intelligent Agent Technology (WI-IAT), 2010 IEEE/WIC/ACM International Conference on, volume 1, pages 676681. IEEE, 2010. [76] Lei Wu, Steven CH Hoi, Rong Jin, Jianke Zhu, and Nenghai Yu. Distance metric learning from uncertain side information with application to automated photo tagging. In Proceedings of the 17th ACM international conference on Multimedia, pages 135144. ACM, 2009. [77] Gui-Rong Xue, Chenxi Lin, Qiang Yang, WenSi Xi, Hua-Jun Zeng, Yong Yu, and Zheng Chen. Scalable collaborative filtering using cluster-based smoothing. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 114121. ACM, 2005. 32

[78] Kai Yu, Anton Schwaighofer, Volker Tresp, Xiaowei Xu, and H-P Kriegel. Probabilistic memorybased collaborative filtering. Knowledge and Data Engineering, IEEE Transactions on, 16(1):5669, 2004. [79] Sheng Zhang, Weihong Wang, James Ford, and Fillia Makedon. Learning from incomplete ratings using non-negative matrix factorization. SIAM, 2006. [80] Yi Zhang and Jonathan Koren. Efficient bayesian hierarchical user modeling for recommendation system. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 4754. ACM, 2007. [81] Jianke Zhu, Hao Ma, Chun Chen, and Jiajun Bu. Social recommendation using low-rank semidefinite program. In Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011. [82] Cai-Nicolas Ziegler. On propagating interpersonal trust in social networks. In Computing with Social Trust, pages 133168. Springer, 2009. [83] Cai-Nicolas Ziegler and Jennifer Golbeck. Investigating interactions of trust and interest similarity. Decision Support Systems, 43(2):460475, 2007. [84] Cai-Nicolas Ziegler and Georg Lausen. Propagation models for trust and distrust in social networks. Information Systems Frontiers, 7(4-5):337358, 2005.

33

2 1.9 1.8 1.7 1.6 1.5 1.4 1.3 1.2 1.1 1 0
GD S G D 1 S G D 2 S G D 3

2 1.9 1.8 1.7 1.6 1.5 1.4 1.3 1.2 1.1 1 50 100 150 It e r a t i o n N u m b e r 200 250 0
GD S G D 1 S G D 2 S G D 3

RMSE Err o r

RMSE Err o r

50

100 150 It e r a t i o n N u m b e r

200

250

(a) 60% of Training Data

(b) 70% of Training Data

2 1.9 1.8 1.7 1.6 1.5 1.4 1.3 1.2 1.1 1 0
GD S G D 1 S G D 2 S G D 3

2 1.9 1.8 1.7 1.6 1.5 1.4 1.3 1.2 1.1 1 50 100 150 It e r a t i o n N u m b e r 200 250 0
GD S G D 1 S G D 2 S G D 3

RMSE Err o r

RMSE Err o r

50

100 150 It e r a t i o n N u m b e r

200

250

(c) 80% of Training Data

(d) 90% of Training Data

Figure 3: Comparison of accuracy of prediction in terms of RMSE with GD and SGD with three varied batch sizes.

34

1

1

0 . 95

0 . 95

0.9 M A E Err o r M A E Err o r
GD S G D 1 S G D 2 S G D 3

0.9

0 . 85

0 . 85

0.8

0.8

0 . 75

0 . 75

0.7 0

0.7 50 100 150 It e r a t i o n N u m b e r 200 250 0

GD S G D 1 S G D 2 S G D 3

50

100 150 It e r a t i o n N u m b e r

200

250

(a) 60% of Training Data

(b) 70% of Training Data

1

1

0 . 95

0 . 95

0.9 RMSE Err o r RMSE Err o r
GD S G D 1 S G D 2 S G D 3

0.9

0 . 85

0 . 85

0.8

0.8

0 . 75

0 . 75

0.7 0

0.7 50 100 150 It e r a t i o n N u m b e r 200 250 0

GD S G D 1 S G D 2 S G D 3

50

100 150 It e r a t i o n N u m b e r

200

250

(c) 80% of Training Data

(d) 90% of Training Data

Figure 4: Comparison of accuracy of prediction in terms of MAE with GD and SGD with three varied batch sizes.

35

GeoSpark: A Cluster Computing Framework for
Processing Large-Scale Spatial Data
Jia Yu

Jinxuan Wu

Mohamed Sarwat

School of Computing, Informatics,
and Decision Systems Engineering,
Arizona State University
699 S. Mill Avenue, Tempe, AZ

School of Computing, Informatics,
and Decision Systems Engineering,
Arizona State University
699 S. Mill Avenue, Tempe, AZ

School of Computing, Informatics,
and Decision Systems Engineering,
Arizona State University
699 S. Mill Avenue, Tempe, AZ

jiayu2@asu.edu

jinxuanw@asu.edu

msarwat@asu.edu

ABSTRACT
This paper introduces GeoSpark an in-memory cluster
computing framework for processing large-scale spatial data.
GeoSpark consists of three layers: Apache Spark Layer,
Spatial RDD Layer and Spatial Query Processing Layer.
Apache Spark Layer provides basic Spark functionalities
that include loading / storing data to disk as well as regular RDD operations. Spatial RDD Layer consists of three
novel Spatial Resilient Distributed Datasets (SRDDs) which
extend regular Apache Spark RDDs to support geometrical
and spatial objects. GeoSpark provides a geometrical operations library that accesses Spatial RDDs to perform basic
geometrical operations (e.g., Overlap, Intersect). System
users can leverage the newly deﬁned SRDDs to eﬀectively
develop spatial data processing programs in Spark. The
Spatial Query Processing Layer eﬃciently executes spatial
query processing algorithms (e.g., Spatial Range, Join, KNN
query) on SRDDs. GeoSpark also allows users to create a
spatial index (e.g., R-tree, Quad-tree ) that boosts spatial
data processing performance in each SRDD partition. Preliminary experiments show that GeoSpark achieves better
run time performance than its Hadoop-based counterparts
(e.g., SpatialHadoop).

Categories and Subject Descriptors
H.2.4 [DATABASE MANAGEMENT]: Systems—Distributed databases; H.2.8 [DATABASE MANAGEMENT]: Database Applications—Spatial databases and
GIS

Keywords
Cluster computing; Large-scale data; Spatial data

1. INTRODUCTION
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from Permissions@acm.org.
SIGSPATIAL’15 November 03-06, 2015, Bellevue, WA, USA
Copyright 2015 ACM ISBN 978-1-4503-3967-4/15/11 $15.00.
http://dx.doi.org/10.1145/2820783.2820860

The volume of available spatial data increased tremendously. Such data includes but not limited to: weather
maps, socioeconomic data, vegetation indices, and more.
Moreover, novel technology allows hundreds of millions of
users to use their mobile devices to access their healthcare
information and bank accounts, interact with friends, buy
stuﬀ online, search interesting places to visit on-the-go, ask
for driving directions, and more. Making sense of such spatial data will be beneﬁcial for several applications that may
transform science and society. Challenges to building such
platform are as follows: Challenge I: System Scalability. The
underlying database system must be able to digest Petabytes
of spatial data, eﬀectively stores it, and allows applications
to eﬃciently retrieve it when necessary. Challenge II: Interactive Performance. The underlying spatial data processing
system must ﬁgure out eﬀective ways to process user’s request in a sub-second response time.
Apache Spark is an in-memory cluster computing system.
Spark provides a novel data abstraction called resilient distributed datasets (RDDs) [9] that are collections of objects
partitioned across a cluster of machines. Each RDD is built
using parallelized transformations (ﬁlter, join or groupBy)
that could be traced back to recover the RDD data. In
memory RDDs allow Spark to outperform existing models
(MapReduce). Unfortunately, Spark does not provide support for spatial data and operations. Hence, users need to
perform the tedious task of programming their own spatial
data processing jobs on top of Spark.
This paper introduces GeoSpark 1 an in-memory cluster computing system for processing large-scale spatial data.
GeoSpark extends the core of Apache Spark to support
spatial data types, indexes, and operations. In other words,
the system extends the resilient distributed datasets (RDDs)
concept to support spatial data. The key contributions of
this paper are as follows: (1) GeoSpark as a full-ﬂedged
cluster computing framework to load, process, and analyze
large-scale spatial data in Apache Spark. (2) A set of out-ofthe-box Spatial Resilient Distributed Dataset (SRDD) types
(e.g., Point RDD and Polygon RDD) that provide in house
support for geometrical and distance operations. SRDDS
provides an Application Programming Interface (API) for
Apache Spark programmers to easily develop their spatial
analysis programs. (3) Spatial data indexing strategies that
partition the input Spatial RDD using a grid structure and
assign grids to machines for parallel execution. GeoSpark
1

GeoSpark website: http://geospark.datasyslab.org













	
	












Figure 2: SRDD partitioning

	






Figure 1: GeoSpark Overview

also adaptively decides whether a spatial index needs to be
created locally on a Spatial RDD partition to strike a balance between the run time performance and memory/cpu
utilization in the cluster. Experiments show that GeoSpark
achieves better run time performance than its Hadoop-based
counterparts (e.g., SpatialHadoop).
The rest of this paper is organized as follows. Section 2
highlights the related work. GeoSpark architecture is
given in Section 3. Preliminary experiments that evaluate
GeoSpark are given in Section 4. Finally, Section 5 concludes the paper.

2. BACKGROUND AND RELATED WORK
Spatial Database Systems. Spatial database operations are vital for spatial analysis and spatial data mining.
Spatial range queries inquire about certain spatial objects
exist in a certain area (e.g., Return all parks in Phoenix).
Spatial join queries are queries that combine two datasets
or more with a spatial predicate, such as distance relations
(e.g., ﬁnd the parks that have rivers in Phoenix). Spatial
k-Nearest Neighbors queries ﬁnd the k nearest objects to a
given spatial object (e.g., show the 10 nearby restaurants).
Spatial query processing algorithms usually make use of spatial indexes to reduce the query latency. For instance, RTree [3] provides an eﬃcient data partitioning strategy to
eﬃciently index spatial data. Its key idea is that group
nearby objects and put them in the next higher level node
of the tree. Quad-Tree [8] is also a spatial index that recursively divides a two-dimensional space into four quadrants.
Parallel and Distributed Spatial Data Processing.
As the development of distributed data processing system,
more and more people in geospatial area direct their attention to deal with massive geospatial data with distributed
frameworks. Hadoop-GIS [1] utilizes global partition indexing and customizable on demand local spatial indexing
to achieve eﬃcient query processing. SpatialHadoop [2], a
comprehensive extension to Hadoop, has native support for
spatial data by modifying the underlying code of Hadoop.
MD-HBase [6] extends HBase, a non-relational database

runs on top of Hadoop, to support multidimensional indexes
which allows for eﬃcient retrieval of points using range and
kNN queries. Parallel SECONDO [4] combines Hadoop with
SECONDO, a database which can handle non-standard data
types, like spatial data, usually not supported by standard
systems. Although these systems have well-developed functions, all of them are implemented on Hadoop framework.
That means they cannot avoid the disadvantages of Hadoop,
especially a large number of reads and writes on disks.

3.

GEOSPARK ARCHITECTURE

As depicted in Figure 1, GeoSpark consists of three main
layers: (1) Apache Spark Layer: that consists of regular
operations that are natively supported by Apache Spark.
These native functions are responsible for loading / saving
data from / to persistent storage (e.g., stored on local disk or
Hadoop ﬁle system HDFS). (2) Spatial Resilient Distributed
Dataset (SRDD) Layer (Section 3.1). (3) Spatial Query Processing Layer (Section 3.2).

3.1

Spatial RDD (SRDD) Layer

This layer extends Spark with spatial RDDs (SRDDs)
that eﬃciently partition SRDD data elements across machines and introduces novel parallelized spatial transformations and actions (for SRDD) that provide a more intuitive
interface for users to write spatial data analytics programs.
The SRDD layer consists of three new RDDs: PointRDD,
RectangleRDD and PolygonRDD. One useful Geometrical
operations library is also provided for every spatial RDD.
Spatial Objects Support. GeoSpark supports various
spatial data input format (e.g., Comma Separated Value,
Tab Separated Value and Well-Known Text). Each type
of spatial objects is stored in a SRDD, PointRDD, RectangleRDD or PolygonRDD. GeoSpark provides a set of
geometrical operations which is called Geometrical Operations Library. This library natively supports geometrical
operations. For example, Overlap(): Finds all of the internal objects which are intersected with others in geometry;
MinimumBoundingRectangle(): Finds the minimum bounding rectangles for each object in a Spatial RDD or return
a large minimum bounding rectangle which contains all of
the internal objects in a Spatial RDD; Union(): Returns the
union polygon of all polygons in this RDD.
SRDD Partitioning. GeoSpark automatically partitions all loaded Spatial RDDs by creating one global grid
ﬁle for data partitioning. The main idea for assigning each
element in a Spatial RDD to the same 2-Dimensional spatial
grid space is as follows: Firstly, split the spatial space into a




	

  







Figure 3: Query execution model

number of equal geographical size grid cells which compose
a global grid ﬁle. Then traverse each element in the SRDD
and assign this element to a grid cell if the element overlaps with this grid cell. If one element intersects with two
or more grid cells, then duplicate this element and assign
diﬀerent grid IDs to the copies of this element. Figure 2 depicts tweets in the U.S. at a particular moment, tweets and
states are assigned to respective grid cells.
SRDD Indexing. Spatial indexes like Quad-Tree and
R-Tree are provided in Spatial IndexRDDs which inherit
from Spatial RDDs. Users are able to initialize a Spatial IndexRDD. Moreover, GeoSpark adaptively decides whether
a local spatial index should be created for a certain Spatial IndexRDD partition based on a tradeoﬀ between the
indexing overhead (memory and time) on one-hand and the
query selectivity as well as the number of spatial objects on
the other hand.

3.2 Spatial Query Processing Layer
This layer supports spatial queries (e.g., Range query and
Join query) for large-scale spatial datasets. After geometrical objects are stored and processed in the Spatial RDD
layer, user may invoke a spatial query provided in Spatial
Query Processing Layer. GeoSpark processes such query
and returns the ﬁnal results to the user. Figure 3 gives the
general execution model followed by GeoSpark . This execution model implements the algorithms proposed by [5]
and [10]. To accelerate a spatial query, GeoSpark leverages the grid partitioned Spatial RDDs, spatial indexing, the
fast in-memory computation and DAG scheduler of Apache
Spark to parallelize the query execution.
Spatial Range Query. GeoSpark executes the spatial
range query algorithm following the execution model: Load
target dataset, partition data, create a spatial index on each
SRDD partition if necessary, broadcast the query window
to each SRDD partition, check the spatial predicate in each
partition, and remove spatial objects duplicates that existed
due to the data partitioning phase.
Spatial Join Query. GeoSpark executes the parallel
spatial join query following the execution model. GeoSpark
ﬁrst partitions the data from the two input SRDDs as well
as creates local spatial indexes (if required) for the SRDD
which is being queried. Then it joins the two datasets by
their keys which are grid IDs. For the spatial objects (from
the two SRDDs) that have the same grid ID, GeoSpark calculates their spatial relations. If two elements from two
SRDDS are overlapped, they are kept in the ﬁnal results.
The algorithm continues to group the results for each rectangle. The grouped results are in the following format: Rect-

angle, Point, Point, ... Finally, the algorithm removes the
duplicated points and returns the result to other operations
or saves the ﬁnal result to disk.
Spatial KNN Query. To process a Spatial KNN query,
GeoSpark uses a heap based top-k algorithm[7], which contains two phases: selection and merge. It takes a partitioned
SRDD, a point P and a number k as inputs. To calculate
the k nearest objects around point P , in the selection phase,
for each SRDD partition GeoSpark calculates the distances
between each object to the given point P , then maintains a
local heap by adding or removing elements based on the distances. This heap contains the nearest k objects around the
given point P . For IndexedSRDD, the system can utilize the
local indexes to reduce the query time. After the selection
phase, GeoSpark merges results from each partition, keeps
the nearest k elements that have the shortest distances to P
and outputs the result.

4.

EXPERIMENTS

This section provides preliminary experimental evaluation that studies the run time performance of the
following large-scale spatial data processing systems:
(1) GeoSpark_NoIndex | QuadTree | RTree: GeoSpark
approach without spatial index, with spatial Quad-Tree
or R-Tree index. In these approaches, data is partitioned according grids. Required spatial indexes are created on each partition after data partitioned. (2) SpatialHadoop_NoIndex | RTree: SpatialHadoop approach without spatial index or with spatial R-Tree index.
Experimental Setup. Our cluster setting on Amazon
EC2 is as follows: (1) CPU per worker: 8 Intel Xeon Processors operating at 2.5 GHz with Turbo up to 3.3 GHz.
(2) Memory per worker: 61 GB in total and 50 GB registered memory in Spark and Hadoop. (3) Storage per worker:
Amazon general purpose SSD. We deploy Ganglia, a scalable
distributed monitoring system for high performance computing systems such as clusters, on our Amazon EC2 experimental cluster.
Datasets. We use three real spatial datasets extracted
from TIGER ﬁles in our experiments: Zcta510 1.5 GB
dataset, Areawater 6.5 GB dataset and Edges 62 GB
dataset. They contain all the cities, all the lakes and all
the meaningful boundaries in the US in rectangle format
correspondingly. All of the datasets are preprocessed by
SpatialHadoop and are open to the public on its website [2].

4.1

Impact of Data Size

This section compares GeoSpark on TIGER Areawater
6.5 GB dataset with TIGER Edges 62 GB dataset as well as
SpatialHadoop. They are tested on 16 nodes cluster. Their
performance are shown in Figure 5. As depicted in Figure 5, GeoSpark and SpatialHadoop cost more run time
on the large dataset than that on the small one. However,
GeoSpark achieves much better run time performance than
SpatialHadoop in both datasets. This superiority is more
obvious on the small dataset. The reason is that GeoSpark
can cache more percentage of the intermediate data in memory on the small scale input than that on the large one. That
accelerates the processing speed.

4.2

Performance of Spatial Iterative Analysis

Spatial co-location pattern recognition is deﬁned as two
or more species are ofter located in a neighborhood rela-

1
3
5
7

9
11
13
15
17

d o u b l e t h r e s h o l d = THRESHOLD;
double baseDistance = 1 . 0 ;
double I n t e r v a l D i s t a n c e = 0 . 5 ;
i n t c o u n t e r =0;
double CoL oc at io n C oef f i cie n t =0.0;
// I n i t i a l i z e IndexedPointRDD
IndexedPointRDD t a r g e t =
new IndexedPointRDD ( SparkContext ,
DatasetLocation ) ;
// I t e r a t i v e Adja cency Matrix C a l c u l a t i o n
w h i l e ( C o L o c a t i o n C o e f f i c i e n t >t h r e s h o l d ) {
PairRDD glbAdjMat =
t a r g e t . S p a t i a l J o i n Q u e r y ( t a r g e t , WITHIN,
baseDistance + counter ∗ IntervalDistance ) ;
C o L o c a t i o n C o e f f i c i e n t=
C a l c u l a t e C o L o c a t i o n ( glbAdjMat ) ;
c o u n t e r ++;
}
r e t u r n b a s e D i s t a n c e + ( c o u n t e r −1) ∗
IntervalDistance ;

Figure 4: Adjacency Matrix (Java code) in GeoSpark

Figure 6: Run Time Performance for Spatial Co-location
Pattern Recognition
indexing and query processing algorithms in Apache Spark
to eﬃciently analyze spatial data at scale. Experiments
on data sizes and spatial analysis show that GeoSpark
achieves better run time performance than its MapReducebased counterparts (e.g., SpatialHadoop). The proposed
ideas are packaged into an open source software artifact.
In the future, we envision GeoSpark to be used by Earth
and Space Scientists, Geographers, Politicians, Commercial
Institutions to analyze spatial data at scale. We also expect
the scientiﬁc community will contribute to GeoSpark and
add new functionalities on top-of it that serve novel spatial
data analysis applications.

6.
Figure 5: Run Time Performance for Spatial Join Over Different Spatial Datasets

tionship. It usually executes multiple times to form a 2dimension curve for observation. This calculation needs the
adjacent matrix between two type of objects which is the
result of a join query. Sample code for ﬁnding adjacent matrix is given in Figure 4. We iteratively query GeoSpark
SRDDs two times with diﬀerent distances which can be
deﬁned as neighborhood relationships in adjacent matrix.
Since SpatialHadoop doesn’t natively support iterative jobs,
we have to run SpatialHadoop_RTree two times for a reasonable comparison. We use the ﬁrst point column in both
of TIGER Zcta 1.5 GB dataset and TIGER Edges 62 GB
dataset and join them.
As shown in Figure 6, GeoSpark outperforms SpatialHadoop in spatial co-location. And their performances are
also improved when we increase the number of machines per
cluster. GeoSpark only costs the quarter time of SpatialHadoop. The main reason behind is that GeoSpark caches
these datasets in memory with SRDDs automatically after
loads from the storage system. The iterative jobs like spatial co-location can invoke these SRDDs multiple times from
memory without any data transformation and data loading. SpatialHadoop has to read and transform the original
datasets again and again.

5. CONCLUSION AND FUTURE WORK
This paper introduced GeoSpark an in-memory cluster
computing framework for processing large-scale spatial data.
GeoSpark provides an API for Apache Spark programmers to easily develop spatial analysis applications. Moreover, GeoSpark provides native support for spatial data

REFERENCES

[1] A. Aji, F. Wang, H. Vo, R. Lee, Q. Liu, X. Zhang, and
J. H. Saltz. Hadoop-GIS: A High Performance Spatial
Data Warehousing System over MapReduce. PVLDB,
6(11):1009–1020, 2013.
[2] A. Eldawy and M. F. Mokbel. A demonstration of
spatialhadoop: An eﬃcient mapreduce framework for
spatial data. PVLDB, 6(12):1230–1233, 2013.
[3] A. Guttman. R-trees: a dynamic index structure for
spatial searching. In SIGMOD, 1984.
[4] J. Lu and R. H. Guting. Parallel Secondo: Boosting
Database Engines with Hadoop. In ICPADS, pages
738 –743, 2012.
[5] G. Luo, J. F. Naughton, and C. J. Ellmann. A
non-blocking parallel spatial join algorithm. In Data
Engineering, 2002. Proceedings. 18th International
Conference on, pages 697–705. IEEE, 2002.
[6] S. Nishimura, S. Das, D. Agrawal, and A. E. Abbadi.
MD-Hbase: A Scalable Multi-dimensional Data
Infrastructure for Location Aware Services. In MDM,
pages 7–16, 2011.
[7] N. Roussopoulos, S. Kelley, and F. Vincent. Nearest
neighbor queries. In ACM SIGMOD record,
volume 24, pages 71–79. ACM, 1995.
[8] H. Samet. The quadtree and related hierarchical data
structures. ACM Computing Surveys (CSUR),
16(2):187–260, 1984.
[9] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma,
M. McCauly, M. J. Franklin, S. Shenker, and I. Stoica.
Resilient Distributed Datasets: A Fault-Tolerant
Abstraction for In-Memory Cluster Computing. In
NSDI, pages 15–28, 2012.
[10] X. Zhou, D. J. Abel, and D. Truﬀet. Data partitioning
for parallel spatial join processing. Geoinformatica,
2(2):175–204, 1998.

Yuhan Sun

Mohamed Sarwat

CIDSE
Arizona State University
Tempe, AZ 85287-9309
Email: Yuhan.Sun.1@asu.edu

CIDSE
Arizona State University
Tempe, AZ 85287-9309
Email: msarwat@asu.edu

I. I NTRODUCTION
Graphs are widely used to model data in many application domains, including social networking, citation network
analysis, studying biological function of genes, and brain
simulation. A graph contains a set of vertices and a set of
edges that connect these vertices. Each graph vertex or edge
may possess a set of properties (aka. attributes). Thanks to the
wide spread use of GPS-enabled devices, many applications
assign a spatial attribute to a vertex (e.g., geo-tagged social
media). Figure 1 depicts an example of a social graph that
has two types of vertices: Person and Venue and two types
of edges: Follow and Like. Vertices with type Person
have two properties (i.e., attributes): name and age. Vertices
with type Venue have two properties: name and spatial
location. A spatial location attribute represents the spatial
location of the entity (i.e., Venue) represented by such vertex.
In Figure 1, vertices {e, f, g, h, i} are spatial vertices which
represent venues.

d

w

llo

Fo

Person

c

Follow

b

a
Like

Abstract—Graphs are widely used to model data in many
application domains. Thanks to the wide spread use of GPSenabled devices, many applications assign a spatial attribute
to graph vertices (e.g., geo-tagged social media). Users may
issue a Reachability Query with Spatial Range Predicate (abbr.
RangeReach). RangeReach finds whether an input vertex
can reach any spatial vertex that lies within an input spatial
range. An example of a RangeReach query is: Given a social
graph, find whether Alice can reach any of the venues located
within the geographical area of Arizona State University. The
paper proposes G EO R EACH an approach that adds spatial data
awareness to a graph database management system (GDBMS).
G EO R EACH allows efficient execution of RangeReach queries,
yet without compromising a lot on the overall system scalability
(measured in terms of storage size and initialization/maintenance
time). To achieve that, G EO R EACH is equipped with a lightweight data structure, namely SPA-Graph, that augments the
underlying graph data with spatial indexing directories. When a
RangeReach query is issued, the system employs a prunedgraph traversal approach. Experiments based on real system
implementation inside Neo4j proves that G EO R EACH exhibits
up to two orders of magnitude better query response time and
up to four times less storage than the state-of-the-art spatial and
reachability indexing approaches.

l

Like

arXiv:1603.05355v1 [cs.DB] 17 Mar 2016

GeoReach: An Efficient Approach for Evaluating
Graph Reachability Queries with Spatial Range
Predicates

j

h

k

a: {name: Alice, age: 19}
b: {name: Dan, age: 20}
c: {name: Carol, age: 35}
d: {name: Bob, age: 25}
j: {name: Kate, age: 18}
k: {name: Mat, age: 23}
l: {name: Katharine, age:21}

g
e

i
f

P
R

Venue
e: {name: Pita Jungle}
f :{name: Chipotle}
g: {name: Sushi 101}
h: {name: Subway}
i: {name: McDonald's}

Fig. 1: Location-Aware Social Graph

Graph Database Management Systems (GDBMSs) emerged
as a prominent NoSQL approach to store, query, and analyze
graph data [15], [8], [25], [24], [28]. Using a GDBMS,
users can pose reachability analysis queries like: (i) Find out
whether two vertices in the graph are reachable, e.g., Are Alice
(vertex a) and Katharine (vertex l) reachable in the social
graph given in Figure 1. (ii) Search for graph paths that match
a given regular language expression representing predicates on
graph elements, e.g., Find all venues that Alice’s Followees
and/or her Followees’ Followees also liked. Similarly, users
may issue a Reachability Query with Spatial Range Predicate
(abbr. RangeReach). A RangeReach query takes as input a
graph vertex v and a spatial range R and returns true only if v
can reach any spatial vertex (that possesses a spatial attribute)
which lies within the extent of R (formal definition is given in
Section II). An example of a RangeReach query is: Find out
whether Alice can reach any of the Venues located within the
geographical area of Arizona State University (depicted as a
dotted red rectangle R in Figure 1). As given in Figure 1, The
answer to this query is true since Alice can reach Sushi 101
(vertex g) which is located within R. Another query example
is to find out whether Katharine can reach any of the venues
located within R. The answer to this query is false due to the

fact that the only venue reachable from Katharine, Subway
(vertex h), is not located within R.
There are several straightforward approaches to execute
a RangeReach query: (1) Traversal Approach: The naive
approach traverses the graph, checks whether each visited
vertex is a spatial vertex and returns true as the answer if
the vertex’s spatial attribute lies within the input query range
R. This approach yields no storage/maintenance overhead
since no pre-computed data structure is maintained. However,
the Traversal approach may lead to high query response
time since the algorithm may traverse the whole graph to
answer the query. (2) Transitive Closure (TC) Approach: this
approach leverages the pre-computed transitive closure [27]
of the graph to retrieve all vertices that are reachable from v
and returns true if at least one spatial vertex (located in the
spatial range R) that is reachable from v. The TC approach
achieves the lowest query response time, however it needs
to pre-compute (and maintain) the graph transitive closure
which is deemed notoriously infeasible especially for largescale graphs. (3) Spatial-Reachability Indexing (SpaReach)
Approach: uses a spatial index [3], [22] to locate all spatial
vertices VR that lie within the spatial range R and then uses
a reachability index [35] to find out whether v can reach any
vertex in VR . SpaReach achieves better query response time
than the Traversal approach but it still needs to necessarily
probe the reachability index for spatial vertices that may never
be reached from the v. Moreover, SpaReach has to store and
maintain two index structures which may preclude the system
scalability.
In this paper, we propose G EO R EACH, a scalable and
time-efficient approach that answers graph reachability queries
with spatial range predicates (RangeReach). G EO R EACH is
equipped with a light-weight data structure, namely SPAGraph, that augments the underlying graph data with spatial
indexing directories. When a RangeReach query is issued,
the system employs a pruned-graph traversal approach. As
opposed to the SpaReach approach, G EO R EACH leverages
the Spa-Graph’ s auxiliary spatial indexing information to
alternate between spatial filtering and graph traversal and early
prunes those graph paths that are guaranteed: (a) not to reach
any spatial vertex or (b) to only reach spatial vertices that
outside the input spatial range query. As opposed to the TC
and SpaReach approaches, G EO R EACH decides the amount of
spatial indexing entries (attached to the graph) that strikes a
balance between query processing efficiency on one hand and
scalability (in terms of storage overhead) on the other hand. In
summary, the main contributions of this paper are as follows:
• To the best of the authors’ knowledge, the paper is the
first that formally motivates and defines RangeReach,
a novel graph query that enriches classic graph reachability analysis queries with spatial range predicates.
RangeReach finds out whether an input graph vertex can
reach any spatial vertex that lies within an input spatial
range.
• The paper proposes G EO R EACH a generic approach that
adds spatial data awareness to an existing GDBMS.

Notation
G = {V, E}
Vvout
Vvin
RF (v)
VS
RFS (v)
n
m
v1 ❀ v2
MBR(P )

Description
A graph G with a set of vertices V and set of edges E
The set of vertices that can be reached via a direct edge
from a vertex v
The set of vertices that can reach (via a direct edge) vertex
v
The set of vertices that are reachable from (via any number
of edges) vertex v
The set of spatial vertices in G such that VS ⊆ V
The set of spatial vertices that are reachable from (via any
number of edges) vertex v
The cardinality of V (n = |V |); the number of vertices in
G
The cardinality of E (m = |E|); the number of edges in G
v2 is reachable from v1 via connected path in G (such that
both v1 and v2 ∈ V )
Minimum bounding rectangle of a set of spatial polygons
P (e.g., points, rectangles)

TABLE I: Notations.

G EO R EACH allows efficient execution of RangeReach
queries issued on a GDBMS, yet without compromising
a lot on the overall system scalability (measured in terms
of storage size and initialization/maintenance time).
1
• The paper experimentally evaluates G EO R EACH
using
real graph datasets based on a system implementation
inside Neo4j (an open source graph database system).
The experiments show that G EO R EACH exhibits up to
two orders of magnitude better query response time and
occupies up to four times less storage than the state-ofthe-art spatial and reachability indexing approaches.
The rest of the paper is organized as follows: Section II lays
out the preliminary background and related work. The SPAGraph data structure, G EO R EACH query processing, initialization and maintenance algorithms are explained in Sections III
to V. Section VI experimentally evaluates the performance of
G EO R EACH. Finally, Section VII concludes the paper.
II. P RELIMINARIES

AND

BACKGROUND

This section highlights the necessary background and related research work. Table I summarizes the main notations in
the paper.
A. Preliminaries
Graph Data. G EO R EACH deals with a directed property
graph G = (V, E) where (1) V is a set of vertices such that
each vertex has a set of properties (attributes) and (2) E is a set
of edges in which every edge can be represented as a tuple of
two vertices v1 and v2 (v1 , v2 ∈ V ). The set of spatial vertices
VS ⊆ V such that each v ∈ VS has a spatial attribute (property)
v.spatial. The spatial attribute v.spatial may be a geometrical
point, rectangle, or a polygon. For ease of presentation, we
assume that a spatial attribute of spatial vertex is represented
by a point. Figure 1 depicts an example of a directed property
graph. Spatial Vertices VS are represented by black colored
circles and are located in a two-dimensional planer space while
white colored circles represent regular vertices that do not
1 https://github.com/DataSystemsLab/GeoGraphDB–Neo4j

possess a spatial attribute. Arrows indicate directions of edges
in the graph.
Graph Reachability (v1 ❀ v2 ). Given two vertices v1 and v2
in a graph G, v1 can reach v2 (v1 ❀ v2 ) or in other words v2
is reachable from v1 if and only if there is at least one graph
path from v1 to v2 . For example, in Figure 1, vertex a can
reach vertex f through the graph path a->c->i->f so it can
be represented as a ❀ f . On the other hand, c cannot reach
h.
Reachability with Spatial Range Predicate (RangeReach).
RangeReach queries find whether a graph vertex can reach a
specific spatial region (range) R. Given a vertex v ∈ V in a
Graph G and a spatial range R, RangeReach can be described
as follows:

RangeReach(v, R) =


true













f alse

if ∃ v ′ such that
(1) v ′ ∈ VS
(2) v ′ .spatial lies within R
(3) v ❀ v ′
Otherwise.
(1)

As given in Equation 1, if any spatial vertex v ′ ∈ VS that
lies within the extent of the spatial range R is reachable from
the input vertex v, then RangeReach(v, R) returns true (i.e.,
v ❀ R). For example, in Figure 1, RangeReach(a, R) = true
since a can reach at least one spatial vertex f in R. However,
RangeReach(l, R) = false since l can merely reach a spatial
vertex h which is not located in R. Vertex d cannot reach R
since it cannot reach any vertex.
B. Related Work
This section presents previous work on reachability indexes,
spatial indexes, and straightforward solutions to processing
graph reachability queries with spatial range predicates (RangeReach).
Reachability Index. Existing solutions to processing graph
reachability queries (u ❀ v) can be divided into three
categories [35]: (1) Pruned Graph Traversal [6], [30], [34]:
These approaches pre-compute some auxiliary reachability
information offline. When a query is issued, the query processing algorithm traverses the graph using a classic traversal
algorithm, e.g., Depth First Search (DFS) or Breadth First
Search (BFS), and leverages the pre-computed reachability
information to prune the search space. (2) Transitive closure
retrieval [1], [7], [17], [18], [27], [31], [32]: this approach
pre-computes the transitive closure of a graph offline and compresses it to reduce its storage footprint. When a query u ❀ v
is posed, the transitive closure of the source vertex u is fetched
and decomposed. Then the query processing algorithm checks
whether the terminal vertex v lies in the transitive closure of u.
and (3) Two-Hop label matching [5], [8], [10], [11], [12], [26]:
The two-hop label matching approach assigns each vertex v in
the graph an out-label set Lout (v) and an in-label set Lin (v).
When a reachability query is answered, the algorithm decides
that u ❀ v if and only if Lout (v) ∩ Lin (v) 6= ∅. Since the

two label sets do not contain all in and out vertices, size of
the reachability index reduces.
Spatial Index. A spatial index [21], [23], [29] is used
for efficient retrieval of either multi-dimensional objects (e.g.,
hx,yi coordinates of an object location) or objects with spatial
extents, e.g., polygon areas represented by their minimum
boundary rectangles (MBR). Spatial index structures can be
broadly classified to hierarchical (i.e., tree-based) and nonhierarchical index structures. Hierarchical tree-based spatial
index structures can be classified into another two broad
categories: (a) the class of data-partitioning trees, also known
as the class of Grow-and-Post trees [20], which refers to the
class of hierarchical data structures that basically extend the
B-tree index structure [2], [13] to support multi-dimensional
and spatial objects. The main idea is to recursively partition
the spatial data based on a spatial proximity clustering, which
means that the spatial clusters may overlap. Examples of
spatial index structures in this category include R-tree [16] and
R*-tree [3]. (b) the class of space-partitioning trees that refers
to the class of hierarchical data structures that recursively
decomposes the space into disjoint partitions. Examples of
spatial index structures in this category include the Quadtree [14] and k-d tree [4].
Spatial Data in Graphs. Some existing graph database
systems, e.g., Neo4j, allow users to define spatial properties on
graph elements. However, these systems do not provide native
support for RangeReach queries. Hence, users need to create
both a spatial index and a reachability index to efficiently
answer a RangeReach queries (drawbacks of this approach
are given in the following section). On the other hand, existing
research work [19] extends the RDF data with spatial data to
support RDF queries with spatial predicates (including range
and spatial join). However, such technique is limited to RDF
and not general graph databases. It also does not provide an
efficient solution to handle reachability queries.
C. Straightforward Solutions
There are three main straightforward approaches to process
a RangeReach query, described as follows:
Approach I: Graph Traversal. This approach executes
a spatial reachability query using a classical graph traversal
algorithm like DFS (Depth First Search) or BFS (Breadth
First Search). When RangeReach(v, R) is invoked, the system
traverses the graph from the starting vertex v. For each visited
vertex, the algorithm checks whether it is a spatial vertex and
returns true as the query answer if the vertex’s location lies
within the input query range R because the requirement of
spatial reachability is satisfied and hence v ❀ R. Otherwise,
the algorithm keeps traversing the graph. If all vertices that v
can reach do not lie in R, that means v cannot reach R.
Approach II: Transitive Closure (TC). This approach precomputes the transitive closure of the graph and stores it as an
adjacency matrix in the database. Transitive closure of a graph
stores the connectivity component of the graph which can be
used to answer reachability query in constant time. Since the
final result will be determined by spatial vertices, only spatial

vertices are stored. When RangeReach(v, R) is invoked, the
system retrieves all spatial vertices that are reachable from v
by means of the transitive closure. The system then returns
true if at least one spatial vertex that is reachable from v is
also located in the spatial range R.
Approach III: SpaReach. This approach constructs two
indexes a-priori: (1) A Spatial Index: that indexes all spatial
vertices in the graph and (2) A Reachability Index: that indexes
the reachability information of all vertices in the graph. When
a RangeReach query is issued, the system first takes advantage
of the spatial index to locate all spatial vertices VR that
lie within the spatial range R. For each vertex v ′ ∈ VR ,
a reachability query against the reachability index is issued
to test whether v can reach v ′ . For example, to answer
RangeReach(a, R) in Figure 2, spatial index is exploited first
to retrieve all spatial vertices that are located in R. From the
range query result, it can be known that g, i and f are located
in rectangle R. Then graph reachability index is accessed to
determine whether a can reach any located-in vertex. Hence, it
is obvious RangeReach(a, R) = true by using this approach.
Critique. The Graph Traversal approach yields no storage/maintenance overhead since no pre-computed data structure is maintained. However, the traversal approach may lead
to high query response time (O(m) where m is the number
of edges in the graph) since the algorithm may traverse the
whole graph to answer the query. The TC approach needs
to pre-compute (and maintain) the graph transitive closure
which is deemed notoriously infeasible especially for largescale graphs. The transitive closure computation is O(kn3 ) or
O(nm) and the TC storage overhead is O(kn2 ) where n is
total number of vertices and k is the ratio of spatial vertices
to the total number of vertices in the graph. To answer a
RangeReach query, the TC approach takes O(kn) time since it
checks whether each reachable spatial vertex in the transitive
closure is located within the query rectangle. On the other
hand, SpaReach builds a reachability index, which is a timeconsuming step, in O(n3 ) [32] time. The storage overhead
of a spatial index is O(n) and that of a reachability index
is O(nm1/2 ). To store the two indices, the overall storage
overhead is O(nm1/2 ). Storage cost of this approach is far less
than TC approach but still not small enough to accommodate
large-scale graphs. The query time complexity of a spatial
index is O(kn) while that of reachability index is m1/2 . But
for a graph reachability query, checking is demanded for each
spatial vertex in the result set generated by the range query.
Hence, cost of second step reachability query is O(knm1/2 ).
The total cost should be O(knm1/2 ). Query performance
of Spa-Reach is highly impacted by the size of the query
rectangle since the query rectangle determines how many
spatial vertices are located in the region. In Figure 1, query
rectangle R overlaps with three spatial vertices. For example,
to answer RangeReach(l, R), all three vertices {f, g, i} will
be checked against the reachability index to decide whether
any of them is reachable from l and in fact neither of them
is reachable. In a large graph, a query rectangle will possibly
contain a large number of vertices. That will definitely lead to

d
c
b

a
l

1

2
h

5

4

3

RMBR(j)

6

8

7 g

L0

e

13

Q

10

9
14

f

12

11

15

i

16

17
19

B-vertex:
a: true
d: false
f: false
h: false
k: false

k

j

18

L1

20

21

R-vertex:
j: RMBR(j)
G-vertex:
b: {2, 19}
c: {12, 14}
e: {14}
g: {12, 14}
i: {14}
l: {2}

L2

Fig. 2: SPA-Graph Overview

high unreasonable high query response time.
III. O UR A PPROACH : G EO R EACH
In this section, we give an overview of G EO R EACH an efficient and scalable approach for executing graph reachability
queries with spatial range predicates.
A. Data Structure
In this section, we explain how G EO R EACH augments a
graph structure with spatial indexing entries to form what we
call SPatially-Augmented Graph (SPA-Graph). To be generic,
G EO R EACHstores the newly added spatial indexing entries
the same way other properties are stored in a graph database
system. The structure of a SPA-Graph is similar to that of
the original graph except that each vertex v ∈ V in a SPAGraph G = {V, E} stores spatial reachability information. A
SPA-Graph has three different types of vertices, described as
follows:
•

•

B-Vertex: a B-Vertex v (v ∈ V ) stores an extra bit (i.e.,
boolean), called Spatial Reachability Bit (abbr. GeoB)
that determines whether v can reach any spatial vertex
(u ∈ VS ) in the graph. GeoB of a vertex v is set to 1
(i.e., true) in case v can reach at least one spatial vertex
in the graph and reset to 0 (i.e., false) otherwise.
R-Vertex: an R-Vertex v (v ∈ V ) stores an additional attribute, namely Reachability Minimum Bounding
Rectangle (abbr. RMBR(v)). RMBR(v) represents the
minimum bounding rectangle MBR(S) (represented by
a top-left and a lower-right corner point) that encloses all
spatial polygons which represent all spatial vertices S that
are reachable from vertex v (RMBR(v) = MBR(RFS (v)),
RFS (v) = {u|v ❀ u, u ∈ VS }).

G-Vertex: a G-Vertex v stores a list of spatial grid cells,
called the reachability grid list (abbr. ReachGrid(v)). Each
grid cell C in ReachGrid(v) belongs to a hierarchical grid
data structure that splits the total physical space into n
spatial grid cells. Each spatial vertex u ∈ VS will be
assigned a unique cell ID (k ∈ [1, n]) in case u is located
within the extents of cell k, noted as Grid(u) = k. Each
cell C ∈ ReachGrid(v) contains at least one spatial vertex
that is reachable from v (ReachGrid(v) = ∪ Grid(u),
{u|v ❀ u, u ∈ VS }).
Lemma 3.1: Let v (v ∈ V ) be a vertex in a SPA-Graph
G = {V, E} and Vvout be the set of vertices that can be
reached via a direct edge from a vertex v. The reachability
minimum bounding rectangle of v (RMBR(v)) is equivalent
to the minimum bounding rectangle that encloses all its outedge neighbors Vvout and their reachability minimum bounding
rectangles. RMBR(v) = MBRv′ ∈Vvout (RMBR(v ′ ), v ′ .spatial).
Proof: Based on the reachability definition, the set of
reachable vertices RF (v) from a vertex v is equal to the union
of the set of vertices that is reached from v via a direct edge
′
(Vvout ) and all vertices that are reached from each vertex v ∈
Vvout . Hence, the set (RFS (v)) of reachable spatial vertices
from v is given in Equation 2.
•

RFS (v) =
′

[

′

(v ′ ∪ RFS (v ))

(2)

v ∈Vvout

And since RMBR(v) = MBR(RFS (v)), then the the reachability minimum bounding rectangle of v is as follows:
RM BR(v) = M BR(
′

[

′

(v ′ ∪ RFS (v ))))
(3)

v ∈Vvout
′

′

= M BRv′ ∈Vvout (RM BR(v ), v .spatial)

That concludes the proof.
Lemma 3.2: The set of reachable spatial grid cells from a
given vertex v is equal to the union of all spatial grid cells
reached from its all its out-edge neighbors and grid cells that
contain the spatial neighbors
ReachGrid(v) =

[

(ReachGrid(v ′ ) ∪ Grid(v ′ ))

(4)

v ′ ∈Vvout

Proof: Similar to that of Lemma III-A.
Example. Figure 2 gives an example of a SPA-Graph. GeoB
of vertex b is set to 1 (true) since b can reach three spatial
vertices e, f and h. GeoB for d is 0 since d cannot reach any
spatial vertex in the graph. Figure 2 also gives an example of a
Reachability Minimum Bounding Rectangle RMBR of vertex
j (i.e., RMBR(j)). All reachable spatial vertices from j are g,
i, h and f . Figure 2 also depicts an example of ReachGrid.
There are three layers of grids, denoted as L0 , L1 , L2 from
top to bottom. The uppermost layer L0 is split into 4 × 4
grid cells; each cell is assigned a unique id from 1 to 16.
We denote grid cell with id 1 as G1 for brevity. The middle
layer gird L1 is split into four cells G17 to G20 . Each cell
in L1 covers four times larger space than each cell in L0 .
G17 in L1 covers exactly the same area of G1 , G2 , G5 , G6

Algorithm 1 Reachability Query with Spatial Range Predicate
1: Function R ANGER EACH(v, R)
2: if v is a spatial vertex and v.spatial Lie In R then return true
3: Terminate ← true
4: if v is a B-vertex then
5:
if GeoB(v) = true then Terminate ← false
6: else if v is a R-vertex then
7:
if R full contains RMBR(v) then return true
8:
if R no overlap with RMBR(v) then return false
9:
Terminate ← false
10: else if v is a G-vertex then
11:
for each grid Gi ∈ ReachGrid(v) do
12:
if R fully contains Gi then return true
13:
Gi partially overlaps with R then Terminate ← false
14: if Terminate = false then
15:
for each vertex v ′ ∈ Vvout do
16:
if R ANGER EACH(v ′ , R) = true then return true
17: return false

in L0 . The bottom layer L2 contains only a single grid cell
which covers all four grids in L1 and represents the whole
physical space. All spatial vertices reachable from vertex a
are located in G2 , G7 , G9 , G12 and G14 , respectively. Hence,
ReachGrid(a) can be {2, 7, 9, 12, 14}. Notice that vertex e
and f are both located in G9 and G14 covered by G19 in
ReachGrid(a) can be replaced by G19 . Then, ReachGrid(a)
= {2, 7, 12, 19}. In fact, there exist more options to represent
ReachGrid(a), such as {17, 18, 19, 20} or {21} by merging
into only a single grid cell in L2 . When we look into
ReachGrid of connected vertices, for instance g, ReachGrid(g)
is {12, 14} and ReachGrid(i) is {14}. It is easy to verify that
ReachGrid(g) is ReachGrid(i)∪Grid(i.spatial), which accords
with lemma 3.2.
SPA-Graph Intuition. The main idea behind the SPAGraph is to leverage the spatial reachability bit, reachability
minimum bounding rectangle and reachability grid list stored
in a B-Vertex, R-Vertex or a G-Vertex to prune graph paths
that are guaranteed (or not) to satisfy both the spatial range
predicate and the reachability condition. That way, G EO R E ACH cuts down the number of traversed graph vertices and
edges and hence significantly reduce the overall latency of a
RangeReach query.
B. Query Processing
This section explains the RangeReach query processing
algorithm. The main objective is to visit as less graph vertices
and edges as possible to reduce the overall query latency.
The query processing algorithm accelerates the SPA-Graph
traversal procedure by pruning those graph paths that are
guaranteed (or not) to satisfy the spatial reachability constraint.
Algorithm 1 gives pseudocode for query processing. The
algorithm takes as input a graph vertex v and query rectangle
R. It then starts traversing the graph starting from v. For
each visited vertex v, three cases might happen, explained as
follows:
Case I (B-vertex): In case GeoB is false, a B-vertex
cannot reach any spatial vertex and hence the algorithm stops
traversing all graph paths after this vertex. Otherwise, further
traversal from current B-vertex is required when GeoB value
is true. Line 4 to 5 in algorithm 1 is for processing such case.

RMBR

A

R

I

A
I

RMBR

Query Rectangle

RMBR
R

Query Rectangle

(a) No Overlap

(x1,y1)

(x1,y1)

B

B
e

RMBR

(b) Lie In

(x2,y2)

(x2,y2)

e
f

RMBR

Q

(a)

Query
Rectangle

(b)

A
I

RMBR

f

Q

Query Rectangle

(c) Partially Covered By

R
(x1,y1)

B

RMBR

e
(x2,y2)

Fig. 3: Relationships between RMBR and a query rectangle
f

Q

Case II (R-vertex): For a visited R-vertex u, there are three
conditions that may happen (see figure 3). They are the case
from line 6 to 9 in algorithm 1:
• Case II.A: RMBR(u) lies within the query rectangle (see
Figure 3b). In such case, the algorithm terminates and
returns true as the answer to the query since there must
exist at least a spatial vertex that is reachable from v.
• Case II.B: The spatial query region R does not overlap
with RMBR(u) (see Figure 3a). Since all reachable spatial
vertices of u must lie inside RMBR(u), there is no
reachable vertex can be located in the query rectangle.
As a result, graph paths originating at u can be pruned.
• Case III.C: RMBR(u) is partially covered by the query
rectangle (see Figure 3c). In this case, the algorithm keeps
traversing the graph by fetching the set of vertices Vvout
that can be reached via a direct edge from v.
Case III (G-vertex): For a G-vertex u, it store many
reachable grids from u. Actually, it can be regarded as many
smaller RMBRs. So three cases may also happen. Algorithm 1
line 13 to 18 is for such case. Three cases will happen are
explained as follows:
• Case III.A: The query rectangle R fully contains any
grid cell in ReachGrid(u). In such case, the algorithms
terminates and returns true as the query answer.
• Case III.B: The query rectangle have no overlap with all
grids in ReachGrid(u). This case means that v cannot
reach any grids overlapped with R. Then we never
traverse from v and this search branch is pruned.
• Case III.C: If the query rectangle fully contains none
of the reachable grid and partially overlap with any
reachable grid, it corresponds to Partially Covered By
case for RMBR. So further traversal is performed.
Figure 2 gives an example of RangeReach that finds
whether vertex a can reach query rectangle Q (the shaded one
in figure 2). At the beginning of the traversal, the algorithm
checks the category of a. In case, It is a B-vertex and its GeoB
value is true, the algorithm recursively traverses out-edge
neighbors of a and perform recursive checking. Therefore,
the algorithm retrieves vertices b, c, d and j. For vertex b,

(c)

Fig. 4: R-vertex Pruning Power
it is a G-vertex and its reachable grids are G2 and G19 . G19
cover the range of four grids in L0 . They are G9 , G10 , G13
and G14 . The spatial range is merely partially covered by
Q (Case III.C), hence it is possible for b to reach Q. We
cannot make an assured decision in this step so b is recorded
for future traversal. Another neighbor is c. ReachGrid(c) is
{12, 14} which means that G12 and G14 are reachable from
c. G14 lies in Q (Case III.A). In such case, since a ❀ c, we
can conclude that a ❀ R. The algorithm then halts the graph
traversal at this step and returns true as the query answer.
IV. SPA-G RAPH A NALYSIS
This section analyzes each SPA-Graph vertex type rom two
perspectives: (1) Storage Overhead: the amount of storage
overhead that each vertex type adds to the system (2) Pruning
Power: the probability that the query processing algorithm
terminates when a vertex of such type is visited during the
graph traversal.
B-vertex. When visiting a B-Vertex, in case GeoB is false,
the query processing algorithm prunes all subsequent graph
paths originated at such vertex. That is due to the fact that such
vertex cannot reach any spatial vertex in the graph. Otherwise,
the query processing algorithm continues traversing the graph.
As a result, pruned power of a B-vertex lies in the condition
that GeoB is false. For a given graph, number of vertices that
can reach any space is a certain value. So probability that a
vertex can reach any spatial vertex is denoted as Ptrue . This is
also the probability of a B-vertex whose GeoB value is true.
Probability of a B-vertex whose GeoB value is false, denoted
as Pf alse , will be 1 − Ptrue . To sum up, pruned power of a
B-vertex is 1 − Ptrue or Pf alse
R-vertex. When an R-vertex is visited, the condition
whether the vertex can reach any space still exists. If the
R-vertex cannot reach any space, we assign the R-vertex a
specific value to represent it(e.g. set coordinates of RMBR’s

bottom-left point bigger than that of the top-right point). In
this case, pruned power of a R-vertex will be the same with
a B-vertex, which is Pf alse . Otherwise, when the R-vertex
can reach some space, it will be more complex. Because
information of RMBR and query rectangle R have some
impact on the pruned power of this R-vertex. The algorithm
stops traversing the graph in both the No Overlap and Lie
In cases depicted in Figures 3a and 3b. Figure 4 shows the
two cases that R-vertex will stop the traversal. In Figure 4,
width and height of the total 2D space are denoted as A and
B. Assume that the query rectangle can be located anywhere
in the space with equal probability. We use (x1 , y1 ) and
(x2 , y2 ) to represent the RMBR’s top-left corner and lowerright point coordinates, respectively. Then all possible areas
where top-left vertex of query rectangle Q should be part
of the total space, denoted as I (see the shadowed area in
the figure. Its area is determined by size of query rectangle.
Denote width and height of Q are e and f , then area of I,
AI = (A − e) × (B − f ).
First, we estimate probability of No Overlap case. Figure 4a
shows one case of No Overlap. If the query rectangle Q do
not overlap with RMBR, top-left vertex of Q must lie outside
rectangle R which is forms the overlap region (drawn with
solid line in Figure 4b). Area of R (denoted as AR ) is obviously determined by the RMBR location and size of Q. It can
be easily observed that AR = (x2 −(x1 −e))×(y2 −(y1 −f )).
Another possible case is demonstrated in Figure 4b. In such
case, if we calculate R in the same way, range of R will
exceeds area of I which contains all possible locations. As a
result, AR = AI in this case. As we can see, area of overlap
region is determined by the range of R and I altogether.
Then we can have a general representation of the overlap area
AOverlap = (min(A − e, x2 ) − max(0, x1 − e)) × (min(B −
f, y2 )−max(0, x2 −f ). The No Overlap area is AI −AOverlap
and the probability of having a No Overlap case is calculated
as follows:
PN oOverlap =

AOverlap
AI − AOverlap
=1−
.
AI
AI

(5)

Figure 4c depicts the Lie In case. When top-left vertex of
Q lies in region R, then such Lie In case will happen. To
ensure that R exists, it is necessary that e > (x2 − x1 ) and
f > (y2 −y1 ). If it is not, then probability of such case must be
0. If this requirement is satisfied, then AR = (x1 − (x2 − e))×
(y1 − (y2 − f )). Recall what is met in the above-mentioned
case, R may exceed the area of I. Similarly, more general
area should be AR = (min(A − e, x1 ) − max(0, x1 − (x2 −
e))) × (min(B − f, y1 ) − max(0, y1 − (y2 − f ))). Probability
R
of such case should be A
AI . To sum up, we have
PLieIn =

(

AR
AI

0

e > (x2 − x1 ) and f > (y2 − y1 )
else

(6)

After we sum up all conditional probabilities based on
Ptrue and Pf alse , pruning power of an R-vertex is equal to

Algorithm 2 G EO R EACH Initialization Algorithm
1:
2:
3:
4:
5:
6:
7:
8:
9:

Function INITIALIZE(Graph G = {V, E})
/*PHASE I: SPA-Graph Vertex Initialization */
for each Vertex v ∈ V according their sequence in topology do
InitializeVertex(G, v, MAX_REACH_GRIDS, MAX_RMBR)
/* PHASE II: Reachable Grid Cells Merging */
for each G-vertex v do
for each layer Li from L1 to Lbottom do
for each grid cell Gi in Li do
if Number of reachable grids in corresponding region in Li−1 is larger
than MERGE_COUNT then
10:
Add Gi in Li into ReachGrid(v)
11:
Remove reachable grid cells that are covered by Gi in higher layers

(PN oOverlap +PLieIn )×Ptrue +Pf alse . Evidently, the pruning
power of an R-vertex is more powerful than a B-vertex. When
the storage overhead of an R-vertex is considered, coordinates
of RMBR’s top-left and lower-right vertices should be stored.
Thus its storage will be at least four bytes depending on the
spatial data precision. That means the storage overhead of a
G-Vertex is always higher than that of a B-Vertex.
G-vertex. For a high resolution grid, it is of no doubt that a
G-vertex possesses a high pruning power. However, this comes
at the cost of higher storage overhead because more grid cells
occupies more space. When a G-vertex is compared with an
R-vertex, the area of an R-vertex is much larger than a grid. In
this case, an R-vertex can be seen as a a simplified G-vertex
for which the grid cell size is equal to that of RMBR. One
extreme case of R-vertex is that the vertex can reach only one
spatial vertex. In such case, RMBR is location of the reachable
spatial vertex. Such R-vertex can still be counted as a G-vertex
whose grid size x → 0. According the rule, it should be with
higher storage overhead and more accuracy. Actually, storing it
as a G-vertex will cost an integer while any R-vertex requires
storage for four float or even double number.
V. I NITIALIZATION & M AINTENANCE
This section describes the SPA-Graph initialization algorithm. The G EO R EACH initialization algorithm (Pseudocode
is given in Algorithm 2) takes as input a graph Graph
G = {V, E} and runs in two main phases: (1) Phase I:
SPA-Graph Vertex Type Initialization: this phase leverages the
tradeoff between query response time and storage overhead
explained in Section IV to determine the type of each vertex.
(2) Phase II: Reachable Grid Cells Merging: This step further
reduces the storage overhead of each G-Vertex in the SPAGraph by merging a set of grid cells into a single grid cell.
Details of each phase are described in Section V-A and V-B
A. SPA-Graph Vertex Type Initialization
To determine the type of each vertex, the initialization
algorithm takes into account the following system parameters:
• MAX_RMBR: This parameter represents a threshold that
limits space area of each RMBR. If a vertex v is
an R-vertex, area of RMBR(v) cannot be larger than
MAX_RMBR. Otherwise, v will be degraded to a B-vertex.
• MAX_REACH_GRIDS: This parameter sets up the maximum number of grid cells in each ReachGrid. If a vertex
v is a G-vertex, number of grid cells in ReachGrid(v)

Algorithm 3 SPA-Graph Vertex Initialization Algorithm

Algorithm 4 Maintain R-vertex

1: Function INITIALIZE V ERTEX(Graph G = {V, E}, Vertex v)
2: Type ← InitializeType(v)
3: switch (Type)
4: case B-vertex:
5:
Set v B-vertex and GeoB(v) = true
6: case G-vertex:
7:
ReachGrid(v) ← ∅
8:
for each Vertex v ′ ∈ Vvout do
9:
Maintain-GVertex(v, v ′ )
10:
if Number of grids in ReachGrid(v) ¿ MAX_REACH_GRIDS then
11:
Set v R-vertex and break
12:
Type ← R-vertex
13:
if Number of grids in ReachGrid(v) = 0 then
14:
Set v B-vertex, GeoB(v) ← false and break
15: case R-vertex:
16:
RMBR(v) ← ∅
17:
for each Vertex v ′ ∈ Vvout do
18:
Maintain-RVertex(v, v ′ )
19:
if Area(RMBR(v)) ¿ MAX_RMBR then
20:
Set v B-vertex, GeoB(v) ← true and break
21: end switch

1: Function M AINTAIN -RV ERTEX(From-side vertex v, To-side vertex v′ )
2: switch (Type of v′ )
3: case B-vertex:
4:
if GeoB(v ′ ) = true then
5:
Set v ′ B-vertex and GeoB(v) ← true
6:
else if RMBR(v) fully contains MBR(v ′ .spatial) then
7:
return false
8:
else
9:
RMBR(v) ← MBR(RMBR(v), v ′ .spatial)
10: case R-vertex:
11:
if RMBR(v) fully contains MBR(RMBR(v ′ ), v ′ .spatial) then
12:
return false
13:
else
14:
RMBR(v) ← MBR(RMBR(v), RMBR(v ′ ), v ′ .spatial)
15: case G-vertex:
16:
if RMBR(v) fully contains MBR(RMBR(v ′ ), v ′ .spatial) then
17:
return false
18:
else
19:
RMBR(v) ← MBR(RMBR(v), RMBR(v ′ ), v ′ .spatial)
20: end switch
21: return true

cannot exceed MAX_REACH_GRIDS. Otherwise, v will
be degraded to an R-vertex.
Algorithm 3 gives the pseudocode of the vertex initialization
algorithm. Vertices are processed based on their topological
sequence in the graph. For each vertex, the algorithm first
determines the initial vertex type using the InitializeType
function (pseudocode omitted for brevity). For a vertex v,
categories of vertex v ′ ({v ′ | v ′ ∈ Vvout }) will be checked.
If there is any B-vertex v ′ with GeoB(v ′ ) = true, v is directly
initialized to a B-vertex with GeoB(v) = true. Otherwise, if
there is any R-vertex, the function will return an R-vertex type,
which means that v is initialized to R-vertex. If either of the
above happens, the function returns G-vertex type. Based on
the initial vertex type, the algorithm may encounter one of the
following three cases:
Case I (B-vertex): The algorithm directly sets v as a Bvertex and GeoB(v) = true because there must exist one outedge neighbor v ′ of v such that GeoB(v ′ ) = true.
Case III (R-vertex): For each v ′ (v ′ ∈ Vvout ), the algorithm calls the Maintain-RVertex algorithm. Algorithm 4
shows the pseudocode of the Maintain-RVertex algorithm.
Maintain-RVertex aggregates RMBR information. After each
aggregation step, area of RMBR(v) will be compared with
MAX_RMBR: In case the area of RMBR(v) is larger than
MAX_RMBR, the algorithm sets v to be a B-vertex with a true
GeoBvalue and terminates. When v ′ is either a G-vertex or
an R-vertex, the algorithm uses the new bounding rectangle
returned from MBR(RMBR(v), RMBR(v ′ ), v ′ .spatial) to
update the current RMBR(v). The algorithm calculates the
RMBR of a G-vertex in case III. In case v ′ is a B-vertex,
GeoB(v ′ ) must be reset to false. The algorithm then updates
RMBR(v) to MBR(RMBR(v), v.spatial).
Case II (G-vertex): For each vertex v ′ (v ′ ∈ Vvout ),
Maintain-GVertex (pseudocode omitted for the sake of space)
is invoked to calculate the ReachGrid of v ′ . In case v ′
is a B-vertex with GeoB(v ′ ) = false and v ′ is a spatial
vertex, the grid cell that contains the location of v ′ will be
added into ReachGrid(v). If v ′ is a G-vertex, all grid cells

in ReachGrid(v ′ ) and Grid(v ′ .spatial) will be added into
ReachGrid(v). It does not matter whether v ′ is a spatial vertex
or not. If v ′ is not a spatial vertex, Grid(v ′ .spatial) is ∅.
After accumulating information from each neighbor v ′ , the
algorithm changes the type of v to R-vertex immediately in
case the number of reachable grid cells in ReachGrid(v) is
larger than MAX_REACH_GRIDS. Therefore, the algorithm
sets the Type to R-vertex since RMBR(v) should be calculated
for possible future usage, e.g. RMBR of in-edge neighbors of
v(it will be shown in R-vertex case).
Example. Figure 2 depicts a SPA-Graph with MAX_RMBR
= 0.8A and MAX_REACH_GRIDS = 4, where A is area of the
whole space. Each vertex is attached with some information
and affiliated to one category of G EO R EACH index. Their
affiliations are listed in the figure. It is obvious that those
vertices which cannot reach any spatial vertices will be stored
as B-vertex and have a false boolean GeoB value to represent
such condition. Vertices d, f , h, i, j and k are assigned a
false value. Other vertices are G-vertex initially. ReachGrid(a)
= {2, 7, 9, 12, 14}, ReachGrid(b) = {2, 9, 14}, ReachGrid(c)
= {12, 14}, ReachGrid(e) = {14}, ReachGrid(g) = {12, 14},
ReachGrid(i) = {14}, ReachGrid(j) = {2, 7, 12, 14} and
ReachGrid(l) = {2}. Because of MAX_REACH_GRIDS, some
of them will be degraded to an R-vertex. Number of reachable grids in ReachGrid(a) and ReachGrid(j) are 4 and
5, respectively. Both of them are larger than or equal to
MERGE_COUNT. They will be degraded to R-vertex first. Then
area of their RMBR are compared with MAX_RMBR. Area
of RMBR(a) is apparently over 80% of the total space area.
According to MAX_RMBR, a is stored as a B-vertex with a true
value while j is stored as an R-vertex with an RMBR.
B. Reachable Grid Cells Merging
After the type of each vertex is decided, the initialization
algorithm performs the reachable grid cells merging phase
(lines 5 to 11 in Algorithm 2). In this phase, the algorithm
merges adjacent grid cells to reduce the overall storage overhead of each G-Vertex. To achieve that, the algorithm assumes
a system parameter, namely MERGE_COUNT. This parameter

Algorithm 5 Maintain B-vertex
1: Function M AINTAIN -BV ERTEX(From-side vertex v, To-side vertex v′ )
2: if GeoB(v) = true then
3:
return false
4: else
5:
switch (Type of v ′ )
6:
case B-vertex:
7:
if GeoB(v ′ ) = true then
8:
GeoB(v) ← true
9:
else if v ′ .spatial 6= NULL then
10:
ReachGrid(v) ← Grid(v ′ .spatial)
11:
else
12:
return false
13:
case R-vertex:
14:
RMBR(v) ← MBR(RMBR(v ′ ), v ′ .spatial)
15:
case G-vertex:
16:
ReachGrid(v) ← ReachGrid(v ′ )∪Grid(v ′ .spatial)
17:
end switch
18: return true

determines how G EO R EACH merges spatially adjacent grid
cells according to MERGE_COUNT. In each spatial region with
four grid cells, the number of reachable grid cells should not
be less than MERGE_COUNT. Otherwise, we merge the four
grid cells into a single grid cell in the lower layer.
For each G-vertex v, all grid cells in grid cell layers L1 to
Lbottom are checked. When a grid cell Gi in Li is processed,
four grid cells in Li−1 that cover the same space with Gi will
be accessed. If number of reachable grid cells is larger than or
equal to MERGE_COUNT, Gi should be added in ReachGrid(v)
first. Then all grid cells covered by Gi in layers from L0 to
Li−1 should be removed. In order to achieve that, a recursive
approach is implemented as follows. For each grid cell in Li−1
that is reachable from v, the algorithm directly remove it from
ReachGrid(v). The removal stops at this grid in this layer. No
recursive checking is required on grid cells in higher layers for
which the space is covered by the reachable grid cell. Since
all those reachable grid cells have been removed already. For
those grid cells that are not reachable from v, the algorithm
cannot assure that they do not cover some reachable grids in
a higher layer. Hence, the recursive removal is invoked until
the algorithm reaches the highest layer or other reachable grid
cells are visited.
The SPA-Graph in Figure 2 has a MERGE_COUNT set to 2.
There is no merging in e, i and l because their ReachGrids
contain only one grid. The rest are b, c and g. In ReachGrid(b),
for each grid in L1 , we make the MERGE_COUNT checking.
G17 covers four grids G1 , G2 , G5 and G6 in L0 . In such
four-grids region, only G2 is reachable from b. The merging
will not happen in G17 . It is the same case in G18 and G20 .
However, there are two grids, G9 and G14 covered by G19
in L1 . As a result, the two grids in L0 will be removed from
ReachGrid(b) with G19 being added instead. For the grid G21
in L2 , the same checking in L1 will be performed. Since, only
G19 is reachable, no merging happens. Finally, ReachGrid(b)
= {2, 19}. Similarly, we can have ReachGrid(c) = {12, 14}
and ReachGrid(g) = {12, 14} where no merging occurs.
C. SPA-Graph Maintenance
When the structure of a graph is updated, i.e., adding or
deleting edges and/or vertices, G EO R EACHneeds to maintain

the SPA-Graph structure accordingly. Moreover, when the
spatial attribute of a vertex changes, G EO R EACHmay need to
maintain the RMBRand/or ReachGridproperties of that vertex
and other connected vertices as well. As a matter of fact, all
graph updates can be simulated as a combination of adding
and/or deleting a set of edges.
Adding an edge. When an edge is added to the graph,
the directly-influenced vertices are those that are connected
to another vertex by the newly added edge. The spatial
reachability information of the to-side vertex will not be
influenced by the new edge. Based upon Lemmas III-A
and 3.2, the spatial reachability information, i.e., RMBRor
ReachGrid, of the to-side vertex should be modified based
on the the from-side vertex. On the other hand, the fromside vertex may remain the same or change. In the former
case, there is no recursive updates required for the in-edge
neighbors of the from-side vertex. Otherwise, the recursive
updates are performed in the reverse direction until no change
occurs or there is no more in-edge neighbor. A queue Q will
be exploited to track the updated vertices. When Q is not
empty, which means there are still some in-edge neighbors
waiting for updates, the algorithm retrieves the next vertex
in the queue. For such vertex, all its in-edge neighbors are
updated by using the reachability information stored on this
vertex. Updated neighbors will then be pushed into the queue.
The algorithm halts when the queue is empty. Depending on
category of the from-side vertex, corresponding maintenance
functions, including Maintain-BVertex, Maintain-RVertex and
Maintain-GVertex are used to update the newly added spatial
reachability information.
Algorithm 5 is used when the from-side vertex is a B-vertex.
In algorithm 5, if the from-side vertex v is already a B-vertex
with GeoB(v) = true. The added edge will never cause any
change on v. Hence a false value is returned. In case GeoB(v)
= false, the algorithm considers type of the to-side vertex v ′ .
•

•

•

B-vertex. If GeoB(v ′ ) = true, it is no doubt that GeoB(v)
will be set to true and a true value will be returned.
Otherwise, the algorithm checks whether v ′ is spatial.
If it is, ReachGrid(v) is updated with Grid(v ′ spatial).
Otherwise, the algorithm returns false because v is not
changed.
R-vertex. In such case, it is certain that v will be updated
to an R-vertex. The algorithm merely updates RMBR(v)
with MBR(RMBR(v ′ ), v ′ .spatial).
G-vertex. It is similar to the R-vertex case. Type of v ′ can
decide that v should be a G-vertex and the algorithm updates ReachGrid(v) with ReachGrid(v ′)∪Grid(v ′ .spatial)

Maintain-BVertex and Maintain-RVertex are what we use
in the initialization. However, there is a new condition that
should be taken into consideration. When the from-side vertex
v is an R-vertex and the to-side vertex v ′ is a G-vertex, the
algorithm needs to update the RMBR(v) with ReachGrid(v ′ ).
Under such circumstance, first a dummy RMBR(v ′ ) will be
constructed using ReachGrid(v ′ ). Although it is not the exact
RMBR of v ′ , it is still precise. Error of the width and height

will not be greater than size of a grid cell. No matter what
function is invoked to update the from-side vertex, G EO R E ACH takes into account the system parameters MAX_RMBR and
MAX_REACH_GRIDS are checked on RMBR and ReachGrid,
respectively.
Deleting an edge. When an edge is removed, the to-side
vertex will be not impacted by the deleting which is the same
with adding an edge. To maintain the correctness of spatial
reachability information stored on the from-side vertex, the
only way is to reinitialize its spatial reachability information
according to all its current out-edge neighbors. If its structure
is different from the original state due to the deleting, the
structure of all its in-edge neighbors will be rebuilt recursively.
A queue Q is used to keep track of the changed vertices. The
way G EO R EACHmaintains the queue and the operations on
each vertex in the queue are similar to the AddEdge procedure.
Maintenance cost of deleting an edge will be O(kn3 ) because
the whole G EO R EACH index may be reinitialized.
VI. E XPERIMENTAL E VALUATION
In this section, we present a comprehensive experimental evaluation of G EO R EACH performance. We compare the
following approaches: GeoMT0, GeoMT2, GeoMT3, GeoP,
GeoRMBR and SpaReach. GeoMT0, GeoMT2 and GeoMT3 are approaches that store only ReachGrid by setting MAX_REACH_GRIDS to the total number of grids in
the space and MAX_RMBR to A where A represent the
area of the whole 2D space. Their difference lies in the
value of MERGE_COUNT. GeoMT0 is an approach where
MERGE_COUNT is 0. In such approach, no higher layer
grids are merged. MERGE_COUNT is set to 2 and 3 respectively in GeoMT2 and GeoMT3. GeoP is an approach in
which MERGE_COUNT = 0, MAX_REACH_GRIDS = 200 and
MAX_RMBR = A. In such approach, reachable grids in ReachGrid will not be merged. If the number of reachable grids
of ReachGrid(v) is larger than 200 then v will be degraded
to an R-vertex. Since MAX_RMBR = A, there will be no Bvertex. In GeoRMBR, MAX_REACH_GRIDS = 0, MAX_RMBR
= A, hence only RMBR s are stored. In all ReachGrid related
approaches, the total space is split into 128 × 128 pieces in
the highest grid layer. SpaReach approach is implemented with
both spatial index and reachability index. Graph structure is
stored in Neo4j graph database. Reachability index is stored as
attributes of each graph vertex in Neo4j database. Reachability
index we use is proposed in [33]. Spatial index used SpaReach
approaches is implemented by gist index in postgresql. To
integrate Neo4j and postgresql databases, for each vertex in
the graph, we assign it an id to uniquely identify it.
Experimental Environment. The source code for evaluating query response time is implemented in Java and compiled
with java-7-openjdk-amd64. Source codes of index construction are implemented in c++ and complied using g++ 4.8.4.
Gist index is constructed automatically by using command
line in Postgresql shell. All evaluation experiments are run
on a computer with an 3.60GHz CPU, 32GB RAM running
Ubuntu 14.04 Linux OS.

TABLE II: Graph Datasets (K = 103 )
Dataset
citeseerx
go-uniprot
patent
uniprot22m
uniprot100m
uniprot150m

|V |
6540K
6968K
3775K
1595K
16087K
25038K

|E|
15011K
34770K
16519K
1595K
16087K
25038K

davg
2.30
4.99
4.38
1.00
1.00
1.00

l
59
21
32
4
9
10

Datasets. We evaluate the performance of our methods
using six real datasets [9], [33] (see Table II). Number of
vertices and edges are listed in column |V | and |E|. Column
davg and l are average degree of vertices and length of the
longest path in the graph, respectively. Citeseerx and patent
are real life citation graphs extracted from CiteSeerx2 and US
patents3 [33]. Go-uniprot is a graph generated from Gene Ontology and annotation files from Uniprot4 [33]. Uniprot22m,
uniprot100m and uniprot150m are RDF graphs from UniProt
database [33]. The aforementioned datasets represent graphs
that possess no spatial attributes. For each graph, we simulate
spatial data by assigning a spatial location to a subset of
the graph vertices. During the experiments, we change the
ratio of spatial vertices to the total number of vertices from
20% to 80%. During the experiments, we vary the spatial
distribution to be: uniform, zipf, and clustered distributions.
Unless mentioned otherwise, the number of spatial clusters is
set to 4 by default.
A. Query Response Time
In this section, we fist compare the query response time
performance of SpaReach to our GeoP approach. Afterwards,
we change tunable parameters in G EO R EACH to evaluate
influence of these thresholds. For each dataset, we change the
spatial selectivity of the input query rectangle from 0.0001 to
0.1. For each query spatial selectivity, we randomly generate
500 queries by randomly selecting 500 random vertices and
500 random spatial locations of the query rectangle. The
reported query response time is calculated as the average time
taken to answer the 500 queries.
Figure 5 depicts the query response time of GeoP and
SpaReach on four datasets. 80% of vertices in the graph
are spatial and they are randomly-distributed in space. For
brevity, we omit the results of the other two datasets, i.e.,
uniprot22m and uniprot100m, since they have almost the same
graph structure and exihibit the same performance. As it turns
out In Figure 5, GeoP outperforms SpaReach for any query
spatial selectivity in uniprot150m, go-uniprot and citeseerx.
For these datasets, SpaReach approach cost more time when
query selectivity increases. When we increasing the query
range size, the range query step tends to return a larger number
of spatial vertices. Hence, the graph reachability checking
step has to check more spatial vertices. Figure 5c and 5d
show similar experiment results. In conclusion, GeoP is much
2 http://citeseer.ist.psu.edu/
3 http://snap.stanford.edu/data/
4 http://www.uniprot.org/

query time (sec)

GeoP

SpaReach

query time (sec)

GeoP

SpaReach

query time (sec)

GeoP

SpaReach

query time (sec)

10000

100

10

100

100

10

1

10

1

1

0.1

1

0.01
0.0001

0.001

0.01

0.1

0.1
0.0001

0.001

0.01

0.1

0.01
0.0001

0.001

0.01

0.1

0.1
0.0001

0.001

GeoP

0.01

Query range

Query range

Query range

Query range

(a) uniprot150m

(b) patent

(c) go-uniprot

(d) citeseerx

SpaReach

0.1

Fig. 5: Query response time (80% spatial vertex ratio, randomly-distributed spatial data, and spatial selectivity ranging from 0.0001 to 0.1)
more query-efficient in relatively sparse graphs. Patent dataset
is the densest graph with richer reachability information.
Figure 5b indicates that even when spatial selectivity set to
0.0001, GeoP can achieve almost the same performance as
SpaReach. When spatial selectivity increases, GeoP outperforms SpaReach again. In a denser graph, the performance
difference between the two approaches is smaller than in
sparse graphs especially when the spatial selectivity is low.
Table III compares the query response time of all our approaches for the uniprot150m, patent go-uniprot and citeseerx
datasets with randomly distributed spatial vertices and spatial
ratio of 80%. In uniprot150m, all our approaches almost have
the same performance. The same pattern happens with the
uniprot22m, uniprot100m and go-uniprot datasets. So we use
uniprot150m as a representative.
For the patent graph with random-distributed spatial vertices
and spatial ratio of 20%, query efficiency difference can be
easily caught. GeoMT0 keeps information of exact reachable
grids of every vertex which brings us fast query speed, but
also the highest storage overhead. RMBR stores general spatial
boundary of reachable vertices which is the most scalable.
However, such approach spend the most time in answering the
query. Since GeoMT3 is an approach that MERGE_COUNT is
set to 3, just few grids in GeoMT3 are merged. As a result,
its query time is merely little bit longer than GeoMT0. There
are more grids getting merged in GeoMT2 than in GeoMT3.
Inaccuracy caused by more integration lowers efficiency of
GeoMT3 in query. GeoP is combination of ReachGrid and
RMBR. Its query efficiency is lower than GeoMT0 and better
than GeoRMBR. In this case, GeoMT2 outperforms GeoP. But
it is not always the case. By tuning MAX_REACH_GRIDS to
a larger number, GeoP can be more efficient in query.
In citeseerx, GeoMT0 keeps the best performance as expected. Performance of GeoP is in between GeoMT0 and
GeoRMBR as what is shown in patent. But GeoMT2 and
GeoMT3 reveal almost the same efficiency and they are worse
than GeoRMBR. Distinct polarized graph structure accounts
for the abnormal appearance. In citeseerx, all vertices can be
divided into two groups. One group consists of vertices that
cannot reach any vertex. The other group contains a what
we call center vertex. The center vertex has huge number of
out-edge neighbor vertices and is connected by huge number
of vertices as well. Because the center vertex can reach that
many vertices, it can reach nearly all grid cells in space. As

a result, vertices that can reach the center vertex can also
reach all grid cells in space. So no matter what value is
MAX_REACH_GRIDS, reachable grids in ReachGrid of these
vertices will be merged into only one grid in a lower layer
until to the bottom layer which is the whole space. Then such
ReachGrid can merely function as a GeoB which owns poorer
locality than RMBR.
B. Storage Overhead
Figure 6a gives the storage overhead of all approaches
for the uniprot150m dataset. In this experiment, the spatial
vertices are randomly distributed in space. Since uniprot22m
and uniprot100m share the same pattern with uniprot150m
(even spatial distribution of vertices varies), they are not shown
in the figure. The experiments show that G EO R EACH and
all its variants require less storage overhead than SpaReach
because of the additional overhead introduced by the spatial index. When there are less spatial vertices, SpaReach
obviously occupies less space because size of spatial index
lessens. However, SpaReach always requires more storage
than any other approaches. Storage overhead of G EO R EACH
approaches shows a two-stages pattern which means it is either
very high (ratio = 0.8, 0.6 and 0.4) or very low (ratio = 0.2).
The reason is as follows. These graphs are sparse and almost
all vertices reach the same vertex. This vertex cannot reach any
other vertex. Let us call it an end vertex. If the end vertex is
a spatial vertex, then all vertices that can reach the end vertex
will keep their spatial reachability information (no matter what
category they are) in storage. But if it is not, majority of
vertices will store nothing for spatial reachability information.
GeoMT0 and GeoP are of almost the same index size because
of sparsity and end-point phenomenon in these graphs. Such
characteristic causes that almost each vertex can just reach
only one grid which makes MAX_REACH_GRIDS invalid in
approach GeoMT0 (number of reachable grids is always less
than MAX_REACH_GRIDS) which makes GeoMT0 and GeoP
have nearly the same size. For similar reason, MERGE_COUNT
becomes invalid in these datasets which makes GeoMT2 and
GeoMT3 share the same index size with GeoMT0 and GeoP.
We also find out that index size of GeoRMBR is slightly larger
than GeoMT0 approaches. Intuitively, RMBR should be more
scalable than ReachGrid. But most of the vertices in these
three graphs can reach only one grid. In GeoRMBR, for each
vertex that have reachable spatial vertices, we assign an RMBR

TABLE III: Query Response Time in three datasets, 80% spatial vertex ratio, and spatial selectivity ranging from 0.0001 to 0.1
Selectivity
0.0001
0.001
0.01
0.1
Index size

MT0
68
65
66
69

GeoMT0
GeoP

(MB)
2400

uniprot150m
MT3 GeoP
67
66
78
66
65
65
65
75

MT2
68
77
66
65

GeoMT2
GeoRMBR

GeoMT3
SpaReach

Index size

RMBR
66
65
65
66
GeoMT0
GeoP

(MB)
6000

MT0
643
168
87
51

GeoMT2
GeoRMBR

patent
MT3
741
185
98
59

MT2
762
258
143
108

GeoMT3
SpaReach

Index size

GeoP
1570
559
217
155

GeoMT0
GeoP

(MB)
750

RMBR
2991
1965
915
348
GeoMT2
GeoRMBR

MT0
202
34
32
33

GeoMT3
SpaReach

Index size

4000

500

800

800

2000

250

400

0
0.8

0.6

0.4

0.2

0
0.8

0.6

0.4

0.2

GeoMT0
GeoP

(MB)
1200

1600

0

citeseerx
MT3 GeoP
203
210
471
207
410
189
399
160

MT2
212
460
408
399

GeoMT2
GeoRMBR

RMBR
234
215
200
183
GeoMT3
SpaReach

0
0.8

0.6

0.4

0.2

0.8

0.6

0.4

ratio

ratio

ratio

ratio

(a) uniprot150m

(b) patent

(c) go-uniprot

(d) citeseerx

0.2

Fig. 6: Storage Overhead (Randomly distributed, spatial vertex ratio from 0.8 to 0.2)
index size(MB)

Random

Clustered

Zipf

2400
1800

index size(MB)

Random

Clustered

Zipf

index size(MB)

Random

Clustered

Zipf

index size(MB)

1800

7 

6

1200



400

600

2 

200

Random

Clustered

Zipf

1200
600
0

0

0
GeoMT2

GeoP

GeoRMBR SpaReach

(a) uniprot150m

GeoMT2

GeoP

GeoRMBR SpaReach

(b) patent

0
GeoMT2

GeoP

GeoRMBR SpaReach

(c) go-uniprot

GeoMT2

GeoP

GeoRMBR SpaReach

(d) citeseerx

Fig. 7: Storage Overhead for varying spatial data distribution (randomly, cluster and zipf distributed) and 0.8 spatial vertex ratio)
which will be stored as coordinates of RMBR’s top-left and
lower-right points. It is more scalable to store one grid id than
two coordinates. So when a graph is highly sparse, index size
of GeoMT0 is possible to be less than GeoRMBR.
Figure 6c shows that in go-uniprot all G EO R EACH approaches performs better than SpaReach. When we compare
all the G EO R EACH approaches, GeoMT0, GeoMT2 and GeoMT3 lead to almost the same storage overhead. That happens
due to the fact that go-uniprot is a very sparse graph. A
vertex can only reach few grids in the whole space. Grid
cells in ReachGrid can hardly be spatially adjacent to each
other which causes no integration. The graph sparsity makes
the number of reachable grids in ReachGrid always less than
MAX_REACH_GRIDS which leads to less R-vertices and more
G-vertices. In consequence, go-uniprot, GeoMT0, GeoMT2,
GeoMT3 and GeoP lead to the same storage overhead. It
is rational that GeoRMBR requires the least storage because
RMBR occupies less storage than ReachGrid.
When graphs are denser, results become more complex.
Figure 6b shows index size of different approaches in patent
dataset with randomly-distributed spatial vertices. GeoRMBR
and GeoP, take the first and the second least storage and
are far less than other approaches because both of them
use RMBR which is more scalable. GeoMT0 takes the most
storage in all spatial ratios for that ReachGrid takes high
storage overhead. GeoMT2 and GeoMT3 require less storage
than GeoMT0 because spatially-adjacent reachable grids in

GeoMT0 are merged which brings us scalability. GeoMT3
are more scalable than GeoMT2 because MERGE_COUNT in
GeoMT2 is 2 which causes more integration. There are three
approaches, GeoMT3, GeoP and GeoRMBR, that outperform
SpaReach approach. By tuning parameters in G EO R EACH, we
are able achieve different performance in storage overhead and
can also outperform SpaReach.
Figure 6d depicts index size of all approaches in citeseerx
with randomly distributed spatial vertices. Spatial vertices ratio
ranges from 0.8 to 0.2. All G EO R EACH approaches outperform
SpaReach except for one outlier when spatial vertices ratio
is 0.2. GeoMT0 consumes huge storage. This is caused by
the center vertex which is above-mentioned. Recall that large
proportion of ReachGrid contains almost all grids in space.
After bitmap compression, it will cause low storage overhead.
This is why when spatial vertices ratio is 0.8, 0.6 and 0.4,
GeoMT0 consumes small size of index. When the ratio is 0.2,
there are less spatial vertices. Although graph structure does
not change, the center vertex reach less spatial vertices and
less grids. Then the bitmap compression brings no advantage
in storage overhead.
Figure 7 shows the impact of spatial data distribution on
the storage cost. GeoMT0, GeoMT2 and GeoMT3 are all
ReachGrid-based approaches. Spatial data distribution of vertices influences all approaches the same way. For all datasets,
SpaReach is not influenced by the spatial data distribution.
SpaReach consists of two sections: (1) The reachability index

size is determined by graph structure and (2) The spatial
index size is directly determined by number of spatial vertices. Hence, SpaReach exhibits the same storage overhead
for different spatial data distributions. When spatial vertices
distribution varies, GeoRMBR also keeps stable storage overhead. This is due to the fact that the storage overhead for
each RMBR is a constant and the number of stored RMBR
s is determined by the graph structure and spatial vertices
ratio, and not by the spatial vertices distribution. Spatial data
distribution can only influence the shape of each RMBR.
Figure 7a shows that each approach in G EO R EACH keeps
the same storage overhead under different distributions in
uniprot150m. As mentioned before, GeoMT0, GeoMT2, GeoMT3 and GeoP actually represent the same data structure
since there is only a single reachable grid in ReachGrid. When
there is only one grid reachable, varying the spatial distribution
becomes invalid for all approaches which use ReachGrid.
Figure 7b and 7c shows that the storage overhead introduced by ReachGrid-based approaches decreases when spatial
vertices become more congested. Randomly distributed spatial
data is the least congested while zipf distributed is the most.
The number of reachable spatial vertices from each vertex do
not change but these reachable spatial vertices become more
concentrated in space. This leads to less reachable grids in
ReachGrid.
Figure 7d shows that when spatial vertices are more congested, ReachGrid based approaches, i.e., GeoMT0, GeoMT2
and GeoMT3, tend to be less scalable. Recall that citeseerx
dataset is a polarized graph with a center vertex. One group
contains vertices that can reach huge number of vertices (about
200,000) due to the center vertex. When spatial vertices are
more concentrated and that will lead to more storage overhead.
C. Initialization time
In this section, we evaluate the index initialization time
for all considered approaches. For brevity, we only show the
performance results for four datasets, uniprot150m, patent,
go-uniprot and citeseerx, since uniprot22m, uniprot100m and
uniprot150m datasets exhibit the same performance. Figure 8a
shows that SpaReach requires much more construction time
than the other approaches under all spatial ratios. Although
these graphs are sparse, they contain large number of vertices. This characteristic causes huge overhead in constructing
a spatial index which dominates the initialization time in
SpaReach. Hence, SpaReach takes much more time than all
other approaches. However, the SpaReach initialization time
decreases when decreasing the number spatial vertices since
the spatial index building step deals with less spatial vertices
in such case. However, SpaReach remains the worst even when
the spatial vertex ratio is set to 20%.
Figures 8b and 8d gives the initialization time for both the
patent and citeseerx datasets, respectively. GeoRMBR takes
significantly less initialization time compared to all other
approaches. GeoP takes less time than the rest of approaches
because it is ReachGrid of partial vertices whose number of
reachable grids are less than MAX_REACH_GRIDS that are

calculated. In most cases, GeoMT0 can achieve almost equal
or better performance compared to SpaReach while GeoMT2
and GeoMT3 requires more time due to the integration of
adjacent reachable grids. To sum up, GeoRMBR and GeoP
perform much better than SpaReach in initialization even
in very dense graphs. GeoMT0 can keep almost the same
performance with SpaReach approach.
Figure 8c shows the initialization time for all six approaches on the go-uniprot dataset. Both RMBR approaches,
i.e., GeoRMBR and GeoP, still outperform SpaReach. This
is due to the fact that a spatial index constitutes a high
proportion of SpaReach initialization time. As opposed to the
uniprot150m case, the smaller performance gap between initializing GeoRMBR and SpaReach in go-uniprot.is explained
as follows. The size of go-uniprotis far less than uniprot150m
which decreases the spatial index initialization cost. As a
result, the index construction time in SpaReach is less than
that in uniprot150m. Since this graph has more reachability
information, all G EO R EACH approaches require more time
than in uniprot150m. It is conjunction of G EO R EACH and
SpaReach index size changes that causes the smaller gap.
VII. C ONCLUSION
This paper describes G EO R EACH a novel approach that
evaluates graph reachability queries and spatial range predicates side-by-side. G EO R EACH extends the functionality of a
given graph database management system with light-weight
spatial indexing entries to efficiently prune the graph traversal
based on spatial constraints. G EO R EACH allows users to
tune the system performance to achieve both efficiency and
scalability. Based on extensive experiments, we show that
G EO R EACH can be scalable and query-efficient than existing spatial and reachability indexing approaches in relatively
sparse graphs. Even in rather dense graphs, our approach
can outperform existing approaches in storage overhead and
initialization time and still achieves faster query response
time. In the future, we plan to study we plan to study
the extensibility of G EO R EACH to support different spatial
predicates. Furthermore, we aim to extend the framework
to support a distributed system environment. Last but not
least, we also plan to study the applicability of G EO R EACH
to various application domains including: Spatial Influence
Maximization, Location and Social-Aware Recommendation,
and Location-Aware Citation Network Analysis.
R EFERENCES
[1] R. Agrawal, A. Borgida, and H. V. Jagadish. Efficient Management of
Transitive Relationships in Large Data and Knowledge Bases. ACM,
1989.
[2] R. Bayer and E. M. McCreight. Organization and Maintenance of Large
Ordered Indices. Acta Informatica, 1(3):173–89, 1972.
[3] N. Beckmann, H.-P. Kriegel, R. Schneider, and B. Seeger. The R*-Tree:
An Efficient and Robust Access Method for Points and Rectangles. In
SIGMOD, pages 322–331, May 1990.
[4] J. L. Bentley. Multidimensional Binary Search Trees Used for Associative Searching. Communications of the ACM, CACM, 18(9):509–517,
1975.
[5] J. Cai and C. K. Poon. Path-hop: efficiently indexing large graphs for
reachability queries. In CIKM, pages 119–128. ACM, 2010.

Ix 	me
(sec)
480

GeoMT0
GeoP

GeoMT2
GeoRMBR

GeoMT
SpaReach

Index me

GeoMT0
GeoP

GeoMT2
GeoRMBR

GeoMT3
SpaReach

Index me

GeoMT0
GeoP

GeoMT2
GeoRMBR

GeoMT3 x me
(sec)
SpaReach
00

5


(sec)
150

320

00

100

200

1

15


50

100

0
0.8

0.

0.4

0.2

(sec)

0

08

0
0.6

0.4

0.2

08

0
0.6

0.4

0.2

08

GeoMT0
GeoP

GeoMT2
GeoRMBR

0.

0

ratio

ratio

ratio

ratio

(a) uniprot150

(b) patent

(c) go-uniprot

(d) citeseerx

GeoMT
SpaReach

02

Fig. 8: Initialization time (Randomly distributed, spatial vertex ratio from 0.8 to 0.2)

[6] L. Chen, A. Gupta, and M. E. Kurul. Stack-based algorithms for pattern
matching on dags. In VLDB, pages 493–504. VLDB Endowment, 2005.
[7] Y. Chen and Y. Chen. An efficient algorithm for answering graph
reachability queries. In ICDE, pages 893–902. IEEE, 2008.
[8] J. Cheng, S. Huang, H. Wu, and A. W.-C. Fu. Tf-label: a topologicalfolding labeling scheme for reachability querying in a large graph. In
SIGMOD, pages 193–204. ACM, 2013.
[9] J. Cheng, S. Huang, H. Wu, and A. W.-C. Fu. Tf-label: a topologicalfolding labeling scheme for reachability querying in a large graph. In
SIGMOD, pages 193–204. ACM, 2013.
[10] J. Cheng, Z. Shang, H. Cheng, H. Wang, and J. X. Yu. K-reach: who
is in your small world. PVLDB, 5(11):1292–1303, 2012.
[11] J. Cheng, J. X. Yu, X. Lin, H. Wang, and P. S. Yu. Fast computing
reachability labelings for large graphs with high compression rate. In
EDBT, pages 193–204. ACM, 2008.
[12] E. Cohen, E. Halperin, H. Kaplan, and U. Zwick. Reachability and
distance queries via 2-hop labels. SIAM Journal on Computing,
32(5):1338–1355, 2003.
[13] D. Comer. The Ubiquitous B-Tree. ACM Computing Surveys, 11(2):121–
137, 1979.
[14] R. A. Finkel and J. L. Bentley. Quad trees: A Data Structure for Retrieval
of Composite Keys. Acta Informatica, 4(1):1–9, 1974.
[15] J. E. Gonzalez, Y. Low, H. Gu, D. Bickson, and C. Guestrin. GraphChi:
Large-Scale Graph Computation on Just a PC. In OSDI, 2012.
[16] A. Guttman. R-Trees: A Dynamic Index Structure For Spatial Searching.
In SIGMOD, 1984.
[17] H. Jagadish. A compression technique to materialize transitive closure.
TODS, 15(4):558–598, 1990.
[18] R. Jin, Y. Xiang, N. Ruan, and H. Wang. Efficiently answering
reachability queries on very large directed graphs. In SIGMOD, pages
595–608. ACM, 2008.
[19] J. Liagouris, N. Mamoulis, P. Bouros, and M. Terrovitis. An effective
encoding scheme for spatial RDF data. PVLDB, 7(12):1271–1282, 2014.
[20] D. B. Lomet. Grow and Post Index Trees: Roles, Techniques and Future
Potential. In SSD, pages 183–206, Aug. 1991.
[21] P. Rigaux, M. Scholl, and A. Voisard. Spatial Databases with Application to GIS. Morgan Kaufmann, 2002.
[22] H. Samet. The Design and Analysis of Spatial Data Structures. AddisonWesley, 1990.
[23] H. Samet. Foundations of Multidimensional and Metric Data Structures.
Morgan Kaufmann, 2006.
[24] M. Sarwat, S. Elnikety, Y. He, and G. Kliot. Horton: Online Query
Execution Engine for Large Distributed Graphs. In ICDE, 2012.
[25] M. Sarwat, S. Elnikety, Y. He, and M. F. Mokbel. Horton+: A Distributed
System for Processing Declarative Reachability Queries over Partitioned
Graphs. PVLDB, 6(14):1918–1929, 2013.
[26] R. Schenkel, A. Theobald, and G. Weikum. Hopi: An efficient connection index for complex xml document collections. In EDBT, pages
237–255. Springer, 2004.
[27] S. Seufert, A. Anand, S. Bedathur, and G. Weikum. Ferrari: Flexible
and efficient reachability range assignment for graph indexing. In ICDE,
pages 1009–1020. IEEE, 2013.
[28] B. Shao, H. Wang, and Y. Li. Trinity: A Distributed Graph Engine on
a Memory Cloud. In SIGMOD, 2013.
[29] S. Shekhar and S. Chawla. Spatial Databases: A Tour. Prentice Hall,
2003.
[30] S. Trißl and U. Leser. Fast and practical indexing and querying of very
large graphs. In SIGMOD, pages 845–856. ACM, 2007.

[31] S. J. van Schaik and O. de Moor. A memory efficient reachability data
structure through bit vector compression. In SIGMOD, pages 913–924.
ACM, 2011.
[32] H. Wang, H. He, J. Yang, P. S. Yu, and J. X. Yu. Dual labeling:
Answering graph reachability queries in constant time. In ICDE, pages
75–75. IEEE, 2006.
[33] Y. Yano, T. Akiba, Y. Iwata, and Y. Yoshida. Fast and scalable
reachability queries on graphs by pruned labeling with landmarks and
paths. In CIKM, pages 1601–1606. ACM, 2013.
[34] H. Yildirim, V. Chaoji, and M. J. Zaki. Grail: Scalable reachability index
for large graphs. PVLDB, 3(1-2):276–284, 2010.
[35] A. D. Zhu, W. Lin, S. Wang, and X. Xiao. Reachability queries on large
dynamic graphs: a total order approach. In SIGMOD, pages 1323–1334.
ACM, 2014.

arXiv:1408.0325v1 [cs.SI] 2 Aug 2014

Matrix Factorization with Explicit Trust and Distrust
Relationships
Rana Forsati

Mehrdad Mahdavi

Shahid Beheshti University, G.C., Tehran, Iran

Michigan State University, Michigan, USA

r_forsati@sbu.ac.ir

mahdavim@cse.msu.edu

Mehrnoush Shamsfard

Mohamed Sarwat

Shahid Beheshti University, G.C., Tehran, Iran

University of Minnesota, Minneapolis, USA

m_shams@sbu.ac.ir

sarwat@cs.umn.edu

Abstract
With the advent of online social networks, recommender systems have became crucial for the
success of many online applications/services due to their significance role in tailoring these applications to user-specific needs or preferences. Despite their increasing popularity, in general recommender systems suffer from the data sparsity and the cold-start problems. To alleviate these issues,
in recent years there has been an upsurge of interest in exploiting social information such as trust
relations among users along with the rating data to improve the performance of recommender systems. The main motivation for exploiting trust information in recommendation process stems from
the observation that the ideas we are exposed to and the choices we make are significantly influenced by our social context. However, in large user communities, in addition to trust relations, the
distrust relations also exist between users. For instance, in Epinions the concepts of personal "web
of trust" and personal "block list" allow users to categorize their friends based on the quality of reviews into trusted and distrusted friends, respectively. Hence, it will be interesting to incorporate
this new source of information in recommendation as well. In contrast to the incorporation of trust
information in recommendation which is thriving, the potential of explicitly incorporating distrust
relations is almost unexplored. In this paper, we propose a matrix factorization based model for
recommendation in social rating networks that properly incorporates both trust and distrust relationships aiming to improve the quality of recommendations and mitigate the data sparsity and the
cold-start users issues. Through experiments on the Epinions data set, we show that our new algorithm outperforms its standard trust-enhanced or distrust-enhanced counterparts with respect to
accuracy, thereby demonstrating the positive effect that incorporation of explicit distrust information
can have on recommender systems.

1 Introduction
The huge amount of information available on the Web has made it increasingly challenging to cope
with this information overload and find the most relevant information one is really interested in. Recommender systems intend to provide users with recommendations of products they might appreciate,
taking into account their past ratings, purchase history, or interest. The recent proliferation of online
social networks have further enhanced the need for such systems. Therefore, it is obvious why such systems are indispensable for the success of many online applications such as Amazon, iTunes and Netflix
to guide the search process and help users to effectively find the information or products they are looking for [49]. Roughly speaking, the overarching goal of recommender systems is to identify a subset of
items (e.g. products, movies, books, music, news, and web pages) that are likely to be more interesting
to users based on their interests [13, 76, 16, 5].
In general, most widely used recommender systems (RS) can be broadly classified into contentbased (CB), collaborative filtering (CF), or hybrid methods [1]. In CB recommendation, one tries to
recommend items similar to those a given user preferred in the past. These methods usually rely on
the external information such as explicit item descriptions, user profiles, and/or the appropriate features extracted from items to analyze item similarity or user preference to provide recommendation.

1

In contrast, CF recommendation, the most popular method adopted by contemporary recommender
systems, is based on the core assumption that similar users on similar items express similar interest,
and it usually relies on the rating information to build a model out of the rating information in the past
without having access to external information required in CB methods. The hybrid approaches were
proposed that combine both CB and CF based recommenders to gain advantages and avoid certain
limitations of each type of systems [20, 64, 55, 48, 54, 67, 15].
The essence of CF lies in analyzing the neighborhood information of past users and items’ interactions in the user-item rating matrix to generate personalized recommendations based on the preferences of other users with similar behavior. CF has been shown to be an effective approach to recommender systems. The advantage of these types of recommender systems over content-based RS is that
the CF based methods do not require an explicit representation of the items in terms of features, but it
is based only on the judgments/ratings of the users. These CF algorithms are mainly divided into two
main categories [21]: memory-based methods (also known as neighborhood-based methods) [73, 9]
and model-based methods [26, 63, 65, 79]. Recently, another direction in CF considers how to combine memory-based and model-based approaches to take advantage of both types of methods, thereby
building a more accurate hybrid recommender system [56, 77, 32].
The heart of memory-based CF methods is the measurement of similarity based on ratings of items
given by users: either the similarity of users (user-oriented CF) [24], the similarity of items (itemsoriented CF) [61], or combined user-oriented and item-oriented collaborative filtering approaches to
overcome the limitations specific to either of them [74]. The user-oriented CF computes the similarity
among users, usually based on user profiles or past behavior, and seeks consistency in the predictions
among similar users [78, 26]. The item-oriented CF, on the other hand, allows input of additional itemwise information and is also capable of capturing the interactions among them. If the rating of an item
by a user is unavailable, collaborative-filtering methods estimate it by computing a weighted average of
known ratings of the items from the most similar users.
Memory-based collaborative filtering is most effective when users have expressed enough ratings
to have common ratings with other users, but it performs poorly for so-called cold-start users. Coldstart users are new users who have expressed only a few ratings. Thus, for memory based CF methods
to be effective, large amount of user-rating data are required. Unfortunately, due to the sparsity of the
user-item rating matrix, memory-based methods may fail to correctly identify the most similar users
or items, which in turn decreases the recommender accuracy. Another major issue that memory-based
methods suffer from is the scalability problem. The reason is essentially the fact that when the number of users and items are very large, which is common in many real world applications, the search to
identify k most similar neighbors of the active user is computationally burdensome. In summary, data
sparsity and non-scalability issues are two main issues current memory based methods suffer from.
To overcome the limitations of memory-based methods, model-based approaches have been proposed, which establish a model using the observed ratings that can interpret the given data and predict
the unknown ratings [1]. In contrast to the memory-based algorithms, model-based algorithms try
to model the users based on their past ratings and use these models to predict the ratings on unseen
items. In model-based CF the goal is to employ statistical and machine learning techniques to learn
models from the data and make recommendations based on the learned model. Methods in this category include aspect model [26, 63], clustering methods [30], Bayesian model [80], and low dimensional
linear factor models such as matrix factorization (MF) [66, 65, 79, 59]. Due to its efficiency in handling very huge data sets, matrix factorization based methods have become one of the most popular
models among the model-based methods, e.g. weighted low rank matrix factorization [65], weighted
nonnegative matrix factorization (WNMF) [79], maximum margin matrix factorization (MMMF) [66]
and probabilistic matrix factorization (PMF) [59]. These methods assume that user preferences can be
modeled by only a small number of latent factors [12] and all focus on fitting the user-item rating matrix
using low-rank approximations only based on the observed ratings. The recommender system we will
propose in this paper adhere to the model-based factorization paradigm.
Although latent factor models and in particular matrix factorization are able to generate high quality
recommendations, these techniques also suffer from the data sparsity problem in real-world scenarios
and fail to address users who rated only a few items. For instance, according to [61], the density of
non-missing ratings in most commercial recommender systems is less than one or even much less.
Therefore, it is unsatisfactory to rely predictions on such small amount of data which becomes more
challenging in the presence of large number of users or items. This observation necessitates tackling the
2

data sparsity problem in an affirmative manner to be able to generate more accurate recommendations.
One of the most prominent approaches to tackle the data sparsity problem is to compensate for the
lack of information in rating matrix with other sources of side information which are available to the
recommender system. For example, social media applications allow users to connect with each other
and to interact with items of interest such as songs, videos, pages, news, and groups. In such networks
the ideas we are exposed to and the choices we make are significantly influenced by our social context.
More specifically, users generally tend to connect with other users due to some commonalities they
share, often reflected in similar interests. Moreover, in many real-life applications it may be the case
that only social information about certain users is available while interaction data between the items
and those users has not yet been observed. Therefore, the social data accumulated in social networks
would be a rich source of information for the recommender system to utilize as side information to alleviate the data sparsity problem. To accomplish this goal, in recent years the trust-based recommender
systems became an emerging field to provide users personalized item recommendations based on the
historical ratings given by users and the trust relationships among users (e.g., social friends).
Social-enhanced recommendation systems are becoming of greater significance and practicality
with the increased availability of online reviews, ratings, friendship links, and follower relationships.
Moreover, many e-commerce and consumer review websites provide both reviews of products and a
social network structure among the reviewers. As an example, the e-commerce site Epinions [22] asks
its users to indicate which reviews/users they trust and use these trust information to rank the reviews
of products. Similar patterns can be found in online communities such as Slashdot in which millions of users post news and comment daily and are capable of tagging other users as friends/foes or
fans/freaks. Another example is the ski mountaineering site Moleskiing [3] which enables users to
share their opinions about the snow conditions of the different ski routes and also express how much
they trust the other users. Another well-known example is the FilmTrsut system [19], an online social
network that provides movie rating and review features to its users. The social networking component
of the website requires users to provide a trust rating for each person they add as a friend. Also users
on Wikipedia can vote for or against the nomination of others to adminship [7]. These websites have
come to play an important role in guiding users’ opinions on products and in many cases also influence their decisions in buying or not buying the product or service. The results of experiments in [11]
and of similar works confirm that a social network can be exploited to improve the quality of recommendations. From this point of view, traditional recommender systems that ignore the social structure
between users may no longer be suitable.
A fundamental assumption in social based recommender systems which has been adopted by almost all of the relevant literature is that if two users have friendship relation, then the recommendation
from his or her friends probably has higher trustworthiness than strangers. Therefore the goal becomes
how to combine the user-item rating matrix with the social/trust network of a user to boost the accuracy of recommendation system and alleviate the sparsity problem. Over the years, several studies have
addressed the issue of the transfer of trust among users in online social networks. These studies exploit
the fact that trust can be passed from one member to another in a social network, creating trust chains,
based on its propagative and transitive nature 1 . Therefore, some recommendation methods fusing social relations by regularization [29, 36, 42, 81] or factorization [41, 43, 59, 58, 65, 60, 57] were proposed
that exploit the trust relations in the social network.
Also, the results of incorporating the trust information in recommender systems is appealing and
has been the focus of many researchers in the last few years, but, in large user communities, besides the
trust relationship between users, the distrust relationships are also unavoidable. For example, Epinions
provided the feature that enables users to categorize other users in a personal web of trust list based
on their quality as a reviewer. Later on, this feature integrated with the concept of personal block list,
which reflects the members that are distrusted by a particular user. In other words, if a user encounters
a member whose reviews are consistently offensive, inaccurate, or otherwise low quality, she can add
that member to her block list. Therefore, it would be tempting to investigate whether or not distrust
information could be effectively utilized to boost the accuracy of recommender systems as well.
In contrast to trust information for which there has been a great research, the potential advantage/disadvantage of explicitly utilizing distrust information is almost unexplored. Recently, few
1 We note that while the concept of trust has been studied in many disciplines including sociology, psychology, economics, and
computer science from different perspectives, but the issue of propagation and transitivity have often been debated in literature
and different authors have reached different conclusions (see for example [62] for a thorough discussion)

3

attempts have been made to explicitly incorporate the distrust relations in recommendation process [22, 40, 69, 72], which demonstrated that the recommender systems can benefit from the proper
incorporation of distrust relations in social networks. However, despite these positive results, there
are some unique challenges involved in distrust-enhanced recommender systems. In particular, it has
proven challenging to model distrust propagation in a manner which is both logically consistent and
psychologically plausible. Furthermore, the naive modeling of distrust as negative trust raises a number of challenges- both algorithmic and philosophical. Finally, it is an open challenge how to incorporate trust and distrust relations in model-based methods simultaneously. This paper is concerned with
these questions and gives an affirmative solution to challenges involved with distrust-enhanced recommendation. In particular, the proposed method makes it possible to simultaneously incorporate both
trust and distrust relationships in recommender systems to increase the prediction accuracy. To the
best of our knowledge, this is the first work that models distrust relations into the matrix factorization
problem along with trust relations at the same time.
The main intuition behind the proposed algorithm is that one can interpret the distrust relations
between users as the dissimilarity in their preferences. In particular, when a user u distrusts another
user v, it indicates that user u disagrees with most of the opinions issued, or ratings made by user v.
Therefore, the latent features of user u obtained by matrix factorization must be as dissimilar as possible to v’s latent features. In other words, this intuition suggests to directly incorporate the distrust into
recommendation by considering distrust as reversing the deviation of latent features. However, when
combined with the trust relations between users, due to the contradictory role of trust and distrust relations in propagating social information in the matrix factorization process, this idea fails to effectively
capture both relations simultaneously. This statement also follows from the preliminary experimental
results in [69] for memory-based CF methods that demonstrated regarding distrust as an indication to
reverse deviations in not the right way to incorporate distrust.
To remedy this problem, we settle to a less ambitious goal and propose another method to facilitate
the learning from both types of relations. In particular, we try to learn latent features in a manner that
the latent features of users who are distrusted by the user u have a guaranteed minimum dissimilarity
gap from the worst dissimilarity of users who are trusted by user u. By this formulation, we ensure that
when user u agrees on an item with one of his trusted friends, he/she will disagree on the same item
with his distrusted friends with a minimum predefined margin. We note that this idea significantly
departs from the existing works in distrust-enhanced memory based recommender systems [69, 72],
that employ the distrust relations to either filter out or debug the trust relations to reduce the prediction
task to a trust-enhanced recommendation. In particular, the proposed method ranks the latent features
of trusted and distrusted friends of each user to reflect the effect of relation in factorization.
Summary of Contributions This work makes the following key contributions:
• A matrix factorization based algorithm for simultaneous incorporation of trust and distrust relationships in recommender systems. To the best of our knowledge, this is the first model-based
recommender algorithm that is able to leverage both types of relationships in recommendation.
• An efficient stochastic optimization algorithm to solve the optimization problem which makes
the proposed method scalable to large social networks.
• An empirical investigation of the consistency of the social relationships with rating information.
In particular, we examine to what extent trust and distrust relations between users are aligned
with the ratings they issued on items.
• An exhaustive set of experiments on Epinions data set to empirically evaluate the performance of
the proposed algorithm and demonstrate its merits and advantages.
• A detailed comparison of the proposed algorithm to the state-of-the-art trust/distrust enhanced
memory/model based recommender systems.
Outline The rest of this paper is organized as follows. In Section 2 we draw connections to and put our
work in context of some of the most recent work on social recommender systems. Section 3 formally

4

introduces the matrix factorization problem, an optimization based framework to solve it, and its extension to incorporate the trust relations between users. The proposed algorithm along with optimization
methods are discussed in Section 4. Section 5 includes our experimental result on Epinions data set
which demonstrates the merits of the proposed algorithm in alleviating data sparsity problem in rating
matrix and generating more accurate recommendations. Finally, Section 6 concludes the paper and
discusses few directions as future work.

2 Related Work on Social Recommendation
Earlier in the introduction, we discussed some of the main lines of research on recommender system;
here, we survey further lines of study that are most directly related to our work on social-enhanced
recommendation. Many successful algorithms have been developed over the past few years to incorporate social information in recommender systems. After reviewing trust-enhanced memory-based
approaches, we discuss some model-based approaches for recommendation in social networks with
trust relations. Finally, we review major approaches in distrust modeling and distrust-enhanced recommender systems.

2.1 Trust Enhanced Memory-based Recommendation
Social network data has been widely investigated in the memory-based approaches. These methods
typically explore the social network and find a neighborhood of users trusted (directly or indirectly) by a
user and perform the recommendation by aggregating their ratings. These methods use the transitivity
of trust and propagate trust to indirect neighbors in the social network [45, 47, 31, 27, 29, 28, 33].
In [45], a trust-aware collaborative filtering method for recommender systems is proposed. In this
work, the collaborative filtering process is informed by the reputation of users, which is computed by
propagating trust. [31] proposed a method based on the random walk algorithm to utilize social connection and other social annotations to improve recommendation accuracy. However, this method
does not utilize the rating information and is not applicable to constructing a random walk graph in
real data sets. TidalTrust [18] performs a modiÞed breadth first search in the trust network to compute
a prediction. To compute the trust value between user u and v who are not directly connected, TidalTrust aggregates the trust value between u’s direct neighbors and v weighted by the direct trust values
of u and its direct neighbors.
MoleTrust [45, 46, 80] does the same idea as TidalTrust, but MoleTrust considers all the raters up to a
fixed maximum-depth given as an input, independent of any specific user and item. The trust metric in
MoleTrust consists of two major steps. First, cycles in trust networks are removed. Therefore, removing
trust cycles beforehand from trust networks can significantly speed up the proposed algorithm because
every user only needs to be visited once to infer trust values. Second, trust values are calculated based
on the obtained directed acyclic graph by performing a simple graph random walk:
TrustWalker [27] combines trust-based and item-based recommendation to consider enough ratings without suffering from noisy data. Their experiments show that TrustWalker outperforms other
existing memory based approaches. Each random walk on the user trust graph returns a predicted
rating for user u on target item i . The probability of stopping is directly proportional to the similarity
between the target item and the most similar item j , weighted by the sigmoid function of step size k.
The more the similarity, the greater the probability of stopping and using the rating on item j as the
predicted rating for item i . As the step size increases, the probability of stopping decreases. Thus ratings by closer friends on similar items are considered more reliable than ratings on the target item by
friends further away.
We note that all these methods are neighborhood-based methods which employ only heuristic algorithms to generate recommendations. There are several problems with this approach. The relationship
between the trust network and the user-item matrix has not been studied systematically. Moreover,
these methods are not scalable to very large data sets since they may need to calculate the pairwise
user similarities and pairwise user trust scores.

5

2.2 Trust Enhanced Model-based Recommendation
Recently, researchers exploited matrix factorization techniques to learn latent features for users and
items from the observed ratings and fusing social relations among users with rating data as will be
detailed in Section 3. These methods can be divided into two types: regularization-based methods
and factorization-based methods. Here we review some existing matrix factorization algorithms that
incorporate trust information in the factorization process.
2.2.1 Regularization based Social Recommendation
Regularization based methods typically add regularization term to the loss function and minimize it.
Most recently, Ma [42] proposed an idea based on social regularized matrix factorization to make recommendation based on social network information. In this approach, the social regularization term is
added to the loss function, which measures the difference between the latent feature vector of a user
and those of his friends. The probability model similar to the model in [42] is proposed by Jamali [29].
The graph Laplacian regularization term of social relations is added into the loss function in [36] and
minimizes the loss function by alternative projection algorithm. Zhu et a l. [81] used the same model
in [36] and built graph Laplacian of social relations using three kinds of kernel functions. In [37], the
minimization problem is formulated as a low-rank semidefinite optimization problem.
2.2.2 Factorization based Social Recommendation
In factorization-based methods, social relationship between users are represented as social relation
matrix, which is factored as well as the rating matrix. The loss function is the weighted sum of the social
relation matrix factorization error and the rating matrix factorization error. For instance, SoRec [41]
incorporates the social network graph into probabilistic matrix factorization model by simultaneously
factorizing the user-item rating matrix and the social trust networks by sharing a common latent lowdimensional user feature matrix [37]. The experimental analysis shows that this method generates better recommendations than the non-social filtering algorithms [28]. However, the disadvantage of this
work is that although the usersÕ social network is integrated into the recommender systems by factorizing the social trust graph, the real world recommendation processes are not reflected in the model.
Two sets of different feature vectors are assumed for users which makes the interpretability of the model
very hard [28, 39]. This drawback not only causes lack of interpretability in the model, but also affects
the recommendation qualities. A better model named Social Trust Ensemble (STE) [39] is proposed
by the same authors, by making the latent features of a user’s direct neighbors affect the rating of the
user. Their method is a linear combination of basic matrix factorization approach and a social network
based approach. Experiments show that their model outperforms the basic matrix factorization based
approach and existing trust based approaches. However, in their model, the feature vectors of direct
neighbors of u affect the ratings of u instead of affecting the feature vector of u. This model does not
handle trust propagation. Another method for recommendation in social networks has been proposed
in [40]. This method is not a generative model and defines a loss function to be minimized. The main
disadvantage of this method is that it punishes the users with lots of social relations more than other
users. Finally, SocialMF [28] is a matrix factorization based model which incorporates social influence
by making the features of every user depend on the features of his/her direct neighbors in the social
network.

2.3 Distrust Enhanced Social Recommendation
In contrast to incorporation of trust relations, unfortunately most of the literature on social recommendation totally ignore the potential of distrust information in boosting the accuracy of recommendations. In particular, only recently few work started to investigate the rule of distrust information in
recommendation process both from theoretical and empirical viewpoints [22, 84, 51, 82, 40, 75, 69, 71,
68, 72]. Although these studies have shown that distrust information can be plentiful, but there is a
significant gap in clear understanding of distrust in recommender systems. The most important reasons for this shortage are the lack of data sets that contain distrust information and dearth of a unified
consensus on modeling and propagation of distrust.

6

Table 1: Summary of notations consistently used in the paper and their meaning.
Symbol
Meaning
U = {u 1 , · · · , u n }, n
I = {i 1 , · · · , i m }, m
k
R ∈ Rn×m
ΩR , |ΩR |
U ∈ Rn×k
V ∈ Rm×k
S ∈ {−1, +1}n×n
ΩS , |ΩS |
W ∈ Rn×n
+
N (u) ⊆ [n]
N+ (u) ⊆ [n]
N− (u) ⊆ [n]
D : Rk × Rk → R+

The set of users in system and the number of users
The set of items and the number of items
The dimension of latent features in factorization
The partially observed rating matrix
The set of observed entires in rating matrix and its size
The matrix of latent features for users
The matrix of latent features for items
The social network between n users
The set of extracted triplets from the social relations and its size
The pairwise similarity matrix between users
Neighbors of user u in the social graph
The set of trusted neighbors by user u in the social graph
The set of distrusted neighbors by user u in the social graph
The measurement function used to assess the similarly of latent features

A formal framework of trust propagation schemes, introducing the formal and computational treatment of distrust propagation has been developed in [22]. In an extension of this work, [82] proposed
clever adaptations in order to handle distrust and sinks such as trust decay and normalization. In [75], a
trust/distrust propagation algorithm called CloseLook is proposed, which is capable of using the same
kinds of trust propagation as the algorithm proposed by [22]. [34] extended the results by [22] using a
machine-learning framework (instead of the propagation algorithms based on an adjacency matrix)
to enable the evaluation of the most informative structural features for the prediction task of positive/negative links in online social networks. A comprehensive framework that computes trust/distrust
estimations for user pairs in the network using trust metrics is build in [71]: given two users in the trust
network, we can search for a path between them and propagate the trust scores along this path to obtain an estimation. When more than one path is available, we may single out the most relevant ones
(selection), and aggregation operators can then be used to combine the propagated trust scores into
one final trust score, according to different trust score propagation operators.
[40] was the first seminal work to demonstrate that the incorporation of distrust information could
be beneficial based on a model-based recommender system. In [71] and [72] the same question is
addressed in memory-based approaches. In particular, [72] embarked upon the distrust-enhanced recommendation and showed that with careful incorporation of distrust metric, distrust-enhanced recommender systems are able to outperform their trust-only counterparts. The main rational behind the
algorithm proposed in [72] is to employ the distrust information to debug or filter out the users’ propagated web of trust. It is also has been realized that the debugging methods must exhibit a moderate
behavior in order to be effective. [68] addressed the problem of considering the length of the paths that
connect two users for computing trust-distrust between them, according to the concept of trust decay.
This work also introduced several aggregation strategies for trust scores with variable path lengths
Finally we note that the aforementioned works try to either model or utilize the trust/distrust information. In recent years there has been an upsurge of interest in predicting the trust and distrust
relations in a social network [34, 14, 4, 53]. For instance, [34] casts the problem as a sign prediction
problem (i.e., +1 for friendship and -1 for opposition) and utilizes machine learning methods to predict
the sign of links in the social network. In [14] a new method is presented for computing both trust and
distrust by combining an inference algorithm that relies on a probabilistic interpretation of trust based
on random graphs with a modified spring-embedding algorithm to classify an edge. Another direction
of research is to examine the consistency of social relations with theories in social psychology [8, 35].
Our work significantly departs from these works on prediction or consistency analysis of social relations, and aims to effectively incorporate the distrust information in matrix factorization for effective
recommendation.

7

3 Matrix Factorization based Recommender Systems
This section provides a formal definition of collaborative filtering, the primary recommendation
method we are concerned with in this paper, followed by solution methods for low-rank factorization
that are proposed in the literature to address the problem.

3.1 Matrix Factorization for Recommendation
In collaborative filtering we assume that there is a set of n users U = {u 1 , · · · , u n } and a set of m items
I = {i 1 , · · · , i m } where each user u i expresses opinions about a set of items. In this paper, we assume
opinions are expressed through an explicit numeric rating (e.g., scale from one to five), but other rating
methods such as hyperlink clicks are possible as well. We are mainly interested in recommending a
set of items for an active user such that the user has not rated these items before. To this end, we are
aimed at learning a model from the existing ratings, i.e., offline phase, and then use the learned model to
generate recommendations for active users, i.e., online phase. The rating information is summarized in
an n × m matrix R ∈ Rn×m , 1 ≤ i ≤ n, 1 ≤ j ≤ m where the rows correspond to the users and the columns
correspond to the items and (p, q)th entry is the rate given by user u p to the item i q . We note that the
rating matrix is partially observed and it is sparse in most cases.
An efficient and effective approach to recommender systems is to factorize the user-item rating
matrix R by a multiplicative of k-rank matrices R ≈ UV> , where U ∈ Rn×k and V ∈ Rm×k utilize the
factorized user-specific and item-specific matrices, respectively, to make further missing data prediction. The main intuition behind a low-dimensional factor model is that there is only a small number of
factors influencing the preferences, and that a user’s preference vector is determined by how each factor applies to that user. This low rank assumption makes it possible to effectively recover the missing
entires in the rating matrix from the observed entries. We note that the celebrated Singular Value Decomposition (SVD) method to factorize the rating matrix R is not applicable here due to the fact that the
rating matrix is partially available and we are only allowed to utilize the observed entries in factorization process. There are two basic formulations to solve this problem: these are optimization based (see
e.g., [57, 37, 41, 33]) and probabilistic [50]. In the following subsections, we first review the optimization
based framework for matrix factorization and then discuss how it can be extended to incorporate trust
information.

3.2 Optimization based Matrix Factorization
Let ΩR be the set of observed ratings in the user-item matrix R ∈ Rn×m , i.e.,
ΩR = {(i , j ) ∈ [n] × [m] : R i j has been observed},
where n is the number of users and m is the number of items to be rated. In optimization based matrix
factorization, the goal is to learn the latent matrices U and V by solving the following optimization
problem:
#
"
´2 λ
λV
1 X ³
U
>
R i j −Ui ,: V j ,: +
kUkF +
kVkF ,
(1)
min L (U, V) =
U,V
2 (i , j )∈ΩR
2
2
where k · kF is the Frobenius norm of a matrix, i.e, kAkF =

qP P
n
m
i =1

j =1 |A i j |

2.

The optimization prob-

lem in (1) constitutes of three terms: the first term aims to minimize the inconsistency between the
observed entries and their corresponding value obtained by the factorized matrices. The last two terms
regularize the latent matrices for users and items, respectively. The parameters λU and λV are regularization parameters that are introduced to control the regularization of latent matrices U and V,
respectively. We would like to emphasize that the problem in (1) is non-convex jointly in both U and V.
However, despite its non-convexity, the formulation in (1) is widely used in practical collaborative filtering applications as the performance is competitive or better as compared to trace-norm minimization,
while scalability is much better. For example, as indicated in [33], to address the Netflix problem, (1)
has been applied with a fair amount of success to factorize data sets with 100 million ratings.

8

3.3 Matrix Factorization with Trust Side Information
Recently it has been shown that just relying on the rating matrix to build a recommender system is
not as accurate as expected. The main reason for this claim is the known cold-start users problem and
the sparsity of rating matrix. Cold-start users are one of the most important challenges in recommender
systems. Since cold-start users are more dependent on the social network compared to users with more
ratings, the effect of using trust propagation gets more important for cold-start users. Moreover, in
many real life systems a very large portion of users do not express any ratings, and they only participate
in the social network. Hence, using only the observed ratings does not allow to learn the user features.
One of the most prominent approaches to tackle the data sparsity problem in matrix factorization
is to compensate the lack of information in rating matrix with other sources of side information which
are available to the recommender system. It has been recently shown that social information such as
trust relationship between users is a rich source of side information to compensate for the sparsity. The
above mentioned traditional recommendation techniques are all based on working on the user-item
rating matrix, and ignore the abundant relationships among users. Trust-based recommendation usually involves constructing a trust network where nodes are users and edges represent the trust placed on
them. The goal of a trust-based recommendation system is to generate personalized recommendations
by aggregating the opinions of other users in the trust network. The intuition is that users tend to adopt
items recommended by trusted friends rather than strangers, and that trust is positively and strongly
correlated with user preferences. Recommendation techniques that analyze trust networks were found
to provide very accurate and highly personalized results.
To incorporate the social relations in the optimization problem formulated in (1), few papers [40, 29,
42, 37, 81] proposed the social regularization method which aims at keeping the latent vector of each
user similar to his/her neighbors in the social network. The proposed models force the user feature
vectors to be close to those of their neighbors to be able to learn the latent user features for users with
no or very few ratings [29]. More specifically, the optimization problem becomes as:
L (U, V) =

´2 λ
λV
1 X ³
U
kUkF +
kVkF
R i j −Ui>,: V j ,: +
2 (i , j )∈ΩR
2
2
°
°
°
n °
X
λS X
1
°
°
U j ,: °,
+
°Ui ,: −
°
2 i =1 °
|N (i )| j ∈N (i )

(2)

where λS is the social regularization parameter and N (i ) is the subset of users who has relationship
with i th user in the social graph.
The rationale behind social regularization idea is that every user’s taste is relatively similar to the
average taste of his friends in the social network. We note that using this idea, latent features of users
indirectly connected in the social network will be dependent and hence the trust gets propagated. A
more reasonable and realistic model should treat all friends differently based on how similar they are.
Let assume the weight of relationship between two users i and j is captured by Wi j where W ∈ Rn×n
demotes the social weight matrix. It is easy to extend the model in (2) to treat friends differently based
on the weight matrix W as:
L (U, V) =

´2 λ
1 X ³
λV
U
R i j −Ui>,: V j ,: +
kUkF +
kVkF
2 (i , j )∈ΩR
2
2
°
°
P
°
n °
λS X
j ∈N (i ) Wi j U j ,: °
°
+
°
°Ui ,: − P
°
2 i =1 °
j ∈N (i ) Wi j

(3)

An alternative formulation is to regularize each users’ fiends individually, resulting in the following objective function [42]:
L (U, V) =

´2 λ
1 X ³
λV
U
R i j −Ui>,: V j ,: +
kUkF +
kVkF
2 (i , j )∈ΩR
2
2
+

n
°
°2
λS X
Wi j °Ui ,: −U j ,: ° .
2 i , j =1

where we simply assumed that for any j ∉ N (i ), Wi j = 0.
9

As mentioned earlier, the objective function in L (U, V) is not jointly convex in both U and V but it
is convex in each of them fixing the other one. Therefore, to find a local solution one can stick to the
standard gradient descent method to find a solution in an iterative manner as follows:
Ut +1 ← Ut − η t ∇U L (U, V)|U=Ut ,V=Vt ,
Vt +1 ← Vt − η t ∇V L (U, V)|U=Ut ,V=Vt .

4 Matrix Factorization with Trust and Distrust Side Information
In this section we describe the proposed algorithm for social recommendation which is able to incorporate both trust and distrust relationships in the social network along with the partially observed rating
matrix. We then present two strategies to solve the derived optimization problem, one based on the
gradient descent optimization algorithm which generates more accurate solutions but it is computationally cumbersome, and another based on the stochastic gradient descent method which is computationally more efficient for large rating and social matrices but suffers from slow convergence rate.

4.1 Algorithm Description
As discussed before, the vast majority of related work in the field of matrix factorization for recommendation has primarily focussed on trust propagation and simply ignore the distrust information between
users, or intrinsically, are not capable of exploiting it. Now, we aim at developing a matrix factorization
based model for recommendation in social rating networks to utilize both trust and distrust relationships. We incorporate the trust/distrust relationship between users in our model to improve the quality
of recommendations. While intuition and experimental evidence indicate that trust is somewhat transitive, distrust is certainly not transitive. Thus, when we intend to propagate distrust through a network,
questions about transitivity and how to deal with conflicting information abound.
To inject social influence in our model, the basic idea is to find appropriate latent features for users
such that each user is brought closer to the users she/he trusts and separated apart from the users
that she/he distrusts and have different interests. We note that simply incorporating this idea in matrix
factorization by naively penalizing the similarity of each user’s latent features to his distrusted friends’
latent features fails to reach the desired goal. The main reason is that distrust is not as transitive as trust,
i.e. distrust can not directly replace trust in trust propagation approaches and utilizing distrust requires
careful consideration (trust is transitive, i.e., if user u trusts user v and v trusts w, there is a good chance
that u will trust w, but distrust is certainly not transitive, i.e., if u distrusts v and v distrusts w, then w
may be closer to u than v or maybe even farther away). It is noticeable that this statement is consistent
with the preliminary experimental results in [69] for memory-based CF methods that indicate regarding
distrust as an indication to reverse deviations in not the right way to incorporate distrust. Therefore we
pursue another approach to model the distrust in recommendation process.
The main intuition behind the proposed framework stems from the observation that the trust relations between users can be treated as agreement on items and distrust relations can be considered as
disagreement on items. Then, the question becomes how can we guarantee when a user agrees on an
item with one of his/her friends, he/she will disagree on the same item with his/her distrusted friends
with a reasonable margin. We note that this margin should be large enough to make it possible to distinguish between two types of friends. In terms of latent features, this observation translates to having
a margin between the similarity and dissimilarity of users’ latent features to his/her trusted and distrusted friends.
Alternatively, one can view the proposed method from the viewpoint of connectivity of latent features in a properly designated graph. Intuitively, certain features or groups of features should influence
how users connect in the social network, and thus it should be possible to learn a mapping from features to connectivity in the social network such that the mapping respects the underlying structure of
the social network. In the basic matrix factorization algorithm for recommendation, we can consider
the latent features as isolated vertices of a graph where there is no connection between nodes. This can
be generalized to the social-enhanced setting by considering the social graph as the underlying graph
between latent features with two types of edges (i.e., trust and distrust relations correspond to positive

10

(a) User trust netwrok

(b) User distrust netwrok

(c) Partially observed rating matrix

(d) Illustration of learned latent features

Figure 1: A simple example with seven users {u 1 , u 2 , · · · , u 7 } and six items {i 1 , i 2 , · · · , i 6 } to illustrate the
main intuition behind the proposed algorithm. The inputs of the algorithm are (a) trust network, (b)
distrust network, and (c) partially observed rating matrix R, respectively. As shown in (d) for user u 1 ,
the learned latent features for all his trusted friends {u 2 , u 4 , u 6 , u 7 } are closer to u 1 ’s latent features than
his distrusted friends {u 3 , u 5 } with a margin of 1.
and negative edges, respectively). Now the problem reduces to learning the latent features for each user
u such that users trusted by u in the social network (with positive edges) are close and users which are
distrusted by u (with negative edges) are more distant. Learning latent features in this manner respects
the inherent topology of the social network.
Figure 1 shows an example to illustrate the intuition behind the mentioned idea. For ease of exposition, we only consider the latent features for the user u 1 . From the trust network in Figure 1 (a) we
can see that user u 1 trusts the list of users N+ = {u 2 , u 4 , u 6 , u 7 } and from the distrust network in Figure 1
(b) we see that user u 1 distrusts the list of users N− = {u 3 , u 5 }. The goal is to learn the latent features
that obeys two goals, i) it minimizes the prediction error on observed entries in the rating matrix, ii) it
respects the underlying structure of the trust and distrust networks between users. In Figure 1 (d) the
latent features are depicted in the Euclidean space from the viewpoint of user u 1 . As shown in Figure 1
(d), for user u 1 , the latent features of his/her trusted friends N+ lie inside the solid circle centered at u 1
and the latent features of his/her distrusted friends N− lie outside the dashed circle. The gap between
two circles guarantees that always there exists a safe margin between u 1 ’s agreements with his trusted
and distrusted friends. One simple way to impose these constraints on the latent features of users is to
generate a set of triplets for any combination of trusted and distrusted friends ( e.g., one such triplet
for user u 1 can be constructed as (u 1 , u 2 , u 5 )) and force the margin constraint to hold for all extracted
triplets. This ensures that the minimum margin gap will definitely exist between the latent features
of all the trusted and distrusted friends as desired and makes it possible to incorporate both types of

11

relationships between users in the matrix factorization.
It is worthy to mention that similar to the social-enhanced recommender systems discussed before, the proposed algorithm is also based on hypotheses about the existence and the correlation of
trust/distrust relations and ratings in the data. The empirical investigation of correlation between social
relations and rating information has been the focus of a bulk of recent research including [83, 53, 38],
where the results reinforce the hypothesis that ratings from trusted people count more than those from
others and in particular distrusted neighbors. We have also conducted experiments as will be detailed
in Subsection 5.5, to empirically investigate the correlation/alignment between social relations and the
rating information issued by users which supports our strategy in exploiting the trust/distrust relations
in matrix factorization.
We now formalize the proposed solution. As the first ingredient, we need a measure to evaluate the
consistency between the latent features of users, i.e., the matrix U, and the trust and distrust constraints
existing between users in the social network. To this end, we introduce a monotonically increasing
convex loss function `(z) to measure the discrepancy between the latent features of different users. Let
u i , u j , and u k be three users in the model such that u i trusts u j but distrusts u k . The main intuition
behind the proposed framework is that the latent features of u i , i.e., Ui ,: must be more similar to u j ’s
latent features than latent features for user u k . For each such a triplet we penalize the objective function
by `(D(Ui ,: ,U j ,: ) − D(Ui ,: ,Uk,: )) where the function D : Rk × Rk 7→ R+ measures the similarity between
two latent vectors assigned to two different users, and ` : R 7→ R+ is a penalty function that is utilized
to assess the violation of latent vectors of trusted and distrusted users. Example loss functions include
hinge loss `(z) = max(0, 1−z) and logistic loss `(z) = log(1+e −z ) which are widely used convex surrogate
of 0-1 loss function in learning community.
Let ΩS denote the set of extracted triplets from the social relations, i.e.,
©
ª
ΩS = (i , j , k) ∈ [n] × [n] × [n] : S i j = 1 & S i k = −1 .
Here, a positive relationship means friends or a trusted relationship and a negative relationship means
foes or a distrust relationship. Then, our goal becomes to find a factorization of matrix R such that
the learned latent features of users are consistent with the constraints in ΩS where the consistency is
reflected in the loss function. This results in the following optimization problem:
L (U, V) =

´2 λ
λV
1 X ³
U
kUkF +
kVkF
R i j −Ui>,: V j ,: +
2 (i , j )∈ΩR
2
2
+

X
λS
`(D(Ui ,: ,U j ,: ) − D(Ui ,: ,Uk,: )).
|ΩS | (i , j ,k)∈ΩS

(4)

Let us make the above general formulation more specific by setting `(·) and D(·, ·) to be the hinge loss
and the Euclidian distance, respectively. Under these two assumptions, the objective can be formulated
as:
L (U, V) =

´2 λ
1 X ³
λV
U
R i j −Ui>,: V j ,: +
kUkF +
kVkF
2 (i , j )∈ΩR
2
2
{z
}
|
R(U,V)

X
¢
¡
λS
+
max 0, 1 − kUi ,: −U j ,: k2 + kUi ,: −Uk,: k2 .
|ΩS | (i , j ,k)∈ΩS

(5)

Here the constraints have been written in terms of hinge-losses over triplets, each consisting of a user,
his/her trusted friend and his/her distrusted friend. Solving the optimization problem in (5) outputs
the latent features for users and items that can utilized to estimate the missing values in the user-item
matrix. Comparing the formulation in (5) to the existing factorization-based methods discussed earlier reveals two main features of the proposed formulation. First, it aims to minimize the error on the
observed ratings and to respect the inherent structure of the social network among the users. The tradeoff between these two objectives is captured by the regularization parameter λS which is required to be
tuned effectively.

12

Algorithm 1 GD based Matrix Factorization with Trust and Distrust Propagation
1: Input: R: partially observed rating matrix, ΩS
2: Output: U and V
3: for t = 1, . . . , T do
4:
Compute the gradients ∇U R(Ut , Vt ) and ∇V R(Ut , Vt ).
5:
Compute ∇U by Eq. 7
6:
Compute ∇V by Eq. 8
7:
Update:
Ut +1 = Ut − η t ∇U |U=Ut ,V=Vt
Vt +1 = Vt − η t ∇V |U=Ut ,V=Vt
8:
9:

end for
return UT +1 and VT +1 .

In a similar way, applying the logistic loss to the general formulation in (4) yields the following objective:
L (U, V) =

´2 λ
1 X ³
λV
U
R i j −Ui>,: V j ,: +
kUkF +
kVkF
2 (i , j )∈ΩR
2
2
+

X
¡
¡
¢¢
λS
log 1 + exp kUi ,: −Uk,: k2 − kUi ,: −U j ,: )k2 .
|ΩS | (i , j ,k)∈ΩS

(6)

Remark 1. We note that in several applications of recommender systems, besides the observed ratings, a
description of the users and/or the objects through attributes (e.g., gender, age) or measures of similarity
is available that could potentially benefit the process of recommendation (see e.g. [2] for few interesting
applications). In that case it is tempting to take advantage of both known ratings and descriptions to
model the preferences of users. A natural way to incorporate the available meta-data is to kernalize the
similarity measure between latent features based on a positive definite kernel between pairs that can be
deduced from the meta-data. More specifically, instead of simply using Euclidian distance as the similarity measure between latent features in (5), we can use the kernel matrix K obtained from the Laplacian of
the graph obtained from the meta-data to measure the similarity as:
¢
¡
¢> ¡
D(Ui ,: ,U j ,: ) = Ui ,: −U j ,: K Ui ,: −U j ,: ,
P
where K = (D − W)−1 , with D as a diagonal matrix with D i ,i = nj=1 Wi j . Here W captures the pairwise
weight between users in the similarity graph between users that is computed based on the available metadata about users.
Remark 2. We would like to emphasize that it is straightforward to generalize the proposed framework
to incorporate similarity and dissimilarity information between items. What we need is to extract the
triplets from the trust/distrust links between items and repeat the same process we did for users. This
will add another term to the objective in terms of latent features of items V as shown in the following
generalized formulation:
L (U, V) =

´2 λ
λV
1 X ³
U
R i j −Ui>,: V j ,: +
kUkF +
kVkF
2 (i , j )∈ΩR
2
2
+

X
¡
¢
λS
max 0, 1 − kUi ,: −U j ,: k2 + kUi ,: −Uk,: k2
|ΩS | (i , j ,k)∈ΩS

+

X
¡
¢
λI
max 0, 1 − kVi ,: − V j ,: k2 + kVi ,: − Vk,: k2 ,
|ΩI | (i , j ,k)∈ΩI

where λI is the regularization parameter and ΩI is the set of triplets extracted from the similar/dissimilar
links between items. The similarity/dissimilarity links between items can be constructed according to tags
issued by users or associated with items, and categories. For example, if two items are attached with a
13

same tag, there is a trust link between them and otherwise distrust link. Alternatively, trust/distrust links
can be extracted by measuring similarity/dissimilarity based on the item properties or profile if provided.
This can further improve the accuracy of recommendations.

4.2 Batch Gradient Descent based Optimization
In optimization for supervised machine learning, there exist two regimes in which popular algorithms
tend to operate: the stochastic approximation regime, which samples a small data set per iteration,
typically a single data point, and the batch or sample average approximation regime, in which larger
samples are used to compute an approximate gradient. The choice between these two extremes outlines the well-known tradeoff between inexpensive noisy steps and expensive but more reliable steps.
Two preliminary examples of these regimes are the Gradient Descent (GD) and the Stochastic Gradient
Descent (SGD) methods, respectively. Both GD and SGD methods starts with some initial point, and
iteratively updates the solution using the gradient information at intermediate solutions. The main
difference is that GD requires a full gradient information at each iteration while SGD only requires an
unbiased estimate of the full gradient which can be done by sampling
We now discuss the application of GD algorithm to solve the optimization problem in (5) as detailed
in Algorithm 1. Recall that the objective function is not jointly convex in both U and V. On the other
hand, the objective is convex in one parameter by fixing the other one. Therefore, we follow an iterative
method to minimize the objective. At each iteration, first by fixing V, we take a step in the direction of
the negative gradient for U and repeat the same process for V by fixing U.
For the ease of exposition, we introduce further notation. For any triplet (i , j , k) ∈ ΩS we note that
the kUi ,: −U j ,: k2 − kUi ,: −Uk,: k2 can be written as Tr(CU> U) where Tr(·) denotes the trace of the input
matrix and C is a sparse auxiliary matrix defined for each triplet with all entries equal to zero except:
Ci k = Cki = C j j = 1 and Ckk = Ci j = C j i = −1. Having defined this notation, we can write the objective
in (5) as:

L (U, V) = R(U, V) +

´
³
X
λV
λS
λU
kUkF +
kVkF +
max 0, 1 − Tr(Ckij U> U) .
2
2
|ΩS | (i , j ,k)∈ΩS

where Ckij is the C matrix defined above which is associated with triplet (i , j , k). To apply the GD
method, we need to compute the gradient of L (U, V) with respect to U and V which we denote by
∇U = ∇U L (U, V) and ∇V = ∇V L (U, V), respectively. We have:

∇U = ∇U R(U, V) + λU U −

X
λS
k
1
(UCk>
k
>
i j + UCi j )
|ΩS | (i , j ,k)∈ΩS [Tr(Ci j U U)<1]

(7)

where 1[·] is the indicator function which takes a value of one if its argument is true, and zero otherwise.
Similarly for ∇V we have:
∇V = ∇V R(U, V) + λV V

(8)

The main shortcoming of GD method is its high computational cost per iteration due to the gradient
computation (i.e., step (7)) which is expensive when the size of social constraints ΩS is large. We note
that the size of ΩS can be as large as O(n 3 ) by considering all triplets in the social graph. In the next subsection we provide an alternative solution to resolve this issue using the stochastic gradient descent and
mini-batch SGD methods which are more efficient than the GD method in terms of the computational
cost per iteration but with a slow convergence rate in terms of target approximation error.

4.3 Stochastic and Mini-batch Optimization
As discussed above, when the size of social network is very large, the size of ΩS may cause computational problems in solving the optimization problem in (5) using GD method. The reason is essentially
the fact that computing the gradient at each iteration requires to go through all the triplets in ΩS which
is infeasible for large networks. To alleviate this problem we propose a stochastic gradient based [52]

14

Algorithm 2 Mini-SGD based Matrix Factorization with Trust and Distrust Propagation
1: Input: R: partially observed rating matrix, ΩS , min batch size B
2: Output: U and V
3: for t = 1, . . . , T do
4:
∇t ← 0
5:
for b = 1, . . . , B do
6:
(i , j , k) ← Sample random triplet from ΩS
7:
if (1 − kUi ,: −U j ,: )k2 + kUi ,: −Uk,: k2 > 0) then
8:
∇t ← Ut Ckij U>
t
9:
end if
10:
end for
11:
Compute the gradients ∇U R(Ut , Vt ) and ∇V R(Ut , Vt ).
12:
Update:
¶
µ
λS
∇t
Ut +1 = Ut − η t ∇U R(Ut , Vt ) + λU Ut +
B |ΩS |
13:

Update:
Vt +1 = Vt − η t (∇V R(Ut , Vt ) + λV Vt )

14:
15:

end for
return UT +1 and VT +1 .

method to solve the optimization problem. The main idea is to choose a fixed subset of triplets for
gradient computation instead of all |ΩS | triplets at each iteration [10]. More specifically, at each iteration, we sample B triplets uniformly at random from ΩS to compute the next solution. We note that
this strategy generates unbiased estimates of the true gradient and makes each iteration of algorithm
computationally more efficient compared to the full gradient counterpart. In the simplest case, SGD
algorithm, only one triplet is chosen at each iteration to generate an unbiased estimate of the full gradient. We note that in practice SGD is usually implemented based on data shuffling, i.e., making the
sequence of the training samples random and then training the model by going through the training
samples one by one. An intermediate solution, known as mini-batch SGD, chooses a subset of triplets
to compute the gradient. The promise is that by selecting more triplets at each iteration, on one hand
the variance of stochastic gradients decreases promotional to the number of sampled triplets, and on
the other hand the algorithm enjoys the light computational cost of basic SGD method.
The detailed steps of the algorithm are shown in Algorithm 2. The mini-batch SGD method improves the computational efficiency by grouping multiple constraints into a mini-batch and only updating the U and V once for each mini-batch. For brevity, we will refer to this algorithm as Mini-SGD.
More specifically, the Mini-SGD algorithm, instead of computing the full gradient over all triplets, samples B triplets uniformly at random from ΩS where 1 ≤ B ≤ |ΩS | is a parameter that needs to be provided
to the algorithm, and computes the stochastic gradient as:
∇t =

λS
B

X
(i , j ,k)∈ΩB

1[Tr(Ck

ij

k
(UCk>
i j + UCi j )
U>
t Ut )<1]

where ΩB is the set of B sampled triplets from ΩS . We note that
E[∇t ] =

X
λS
k
1
(UCk>
k
>
i j + UCi j ),
|ΩS | (i , j ,k)∈ΩS [Tr(Ci j Ut Ut )<1]

i.e., ∇t is an unbiased estimate of the full gradient in the right hand side. When B = |ΩS |, each iteration
handles the original objective function and Mini-SGD reduces to the batch GD algorithm. We note that
both GD and SGD share the same convergenceprate in terms of number of iterations in expectation for
non-smooth optimization problems (i.e., O(1/ T ) after T iterations), but SGD method requires much
less running time to convergence compared to the GD method due to the efficiency of its individual
iterations.

15

5 Experimental Results
In this section, we conduct exhaustive experiments to demonstrate the merits and advantages of the
proposed algorithm. We conduct the experiments on the well-known Epinions 2 data set, aiming to
accomplish and answer the following fundamental questions:
1. Prediction accuracy: How does the proposed algorithm perform in comparison to the stateof- the-art algorithms with/without incorporating trust and distrust relationships between users.
Whether or not the trust/distrust social network could help in making more accurate recommendations?
2. Correlation of social relations with rating information: To what extent, the trusted and distrusted friends of a user u are aligned with the ratings the user u issued for the reviews written by
his friends? A positive answer to this question indicates that two users will issue similar (dissimilar) ratings if they are connected by a trust (distrust) relation and prefer to behave similarly.
3. Model selection: What role do the regularization parameters λS , λU and λV play in the accuracy
of the proposed recommender system and what is the best strategy to tune these parameters?
4. Handling cold-start users: How does exploiting social relationships in prediction process affect
the performance of recommendation for cold-start users?
5. Trading trust for distrust: To what extent the distrust relations can compensate for the lack of
trust relations?
6. Efficiency of optimization: What is the trade-off between accuracy and efficiency by moving
from the gradient descent to the stochastic gradient descent with different batch sizes?
In the following subsections, we intend to answer these questions. We begin by introducing the data set
we use in our experiemnts and the metrics we employ to evaluate the results, followed by the detailed
experimental results.

5.1 Data Set Description and Experimental Setup
The Epinions data set We begin by discussing the data set we have chosen for our experiments. To
evaluate the proposed algorithm on trust and distrust-aware recommendations, we use the Epinions
data set [22], a popular e-commerce site and customer review website where users share opinions on
various types of items such as electronic products, companies, and movies, through writing reviews
about them or assigning a rating to the reviews written by other users. The rating values in Epinions are
discrete values ranging from Ònot helpfulÓ (1/5) to Òmost helpfulÓ (5/5). These ratings and reviews
would potentially influence future customers when they are about to decide whether a product is worth
buying or a movie is worth watching.
Epinions allows users to evaluate other users based on the quality of their reviews, and to make
trust and distrust relations with other users in addition to the ratings. Every member of Epinions can
maintain a "trust" list of people he/she trusts that is referred to as web of trust (social network with trust
relationships) based on the reviewers with consistent ratings or "distrust" list known as block list (social
network with distrust relationships) that presents reviewers whose reviews were consistently found to
be inaccurate or low quality. The fact that the data set contains explicit positive and negative relations
between users makes it very appropriate to study issues in trust- and distrust-enhanced recommender
systems. Epinions is thus an ideal source for experiments on social recommendation. We remark that
the Epinions data set only contains bivalent relations (i.e., contains only full trust and full distrust, and
no gradual statements).
To conduct the coming experiments, we sampled a subset of Epinions data set with n = 121, 240
users and m = 685, 621 different items. The total number of observed ratings in the sampled data set is
12,721,437 which approximately includes 0.02% of all entries in the rating matrix R which demonstrates
the sparsity of the rating matrix. We note that the selected items are the most frequently rated overall.
The statistics of the data set is given in Table 2. The social statistics of the this data source is summarized
2 http://www.trustlet.org/wiki/Epinions_datasets

16

Table 2: Statistics of sample data from Epinions data set used in our experiments.
Statistic
Quantity
Number of users
121,240
Number of items
685,621
Number of ratings
12,721,437
Number of trust relations
481,799
Number of distrust relations
96,823
Minimum number of ratings by users
1
Minimum number of ratings for items
1
Maximum number of ratings by users
148735
Maximum number of ratings for items
945
Average number of ratings by users
85.08
Average number of ratings for items
15.26

Table 3: Maximum and average trust and distrust relations for users in the sampled data set.
Statistics
Trust per user
Be Trusted per user
Max
Min
Average

1983
1
4.76

2941
0
4.76

Max
Min
Average

Distrust per user
1188
1
0.91

Be Distrusted per user
429
0
0.91

in Table 3. The frequencies of ratings for users is shown are Table 4. In the user distrust network, the
total number of issued distrust statements is 96,823. As to the user trust network, the total number of
issued trust statements is 481,799.
Experimental setup To better evaluate the effect of utilizing the social side information in recommendation accuracy, we employ different amount of training data 90%, 80% , 70% and 60% to create
four different training sets that are increasingly sparse but the social network remains the same in all of
them. Training data 90%, for example, means we randomly select 90% of the ratings from the sampled
Epinions data set as the training data to predict the remaining 10% of ratings. The random selection
was carried out 5 times independently to have a fair comparison. Also, since our preliminary results
on a smaller data set revealed that the hinge loss performs better than the exponential loss, in the rest
of experiments we stick to this loss function. However, we note the exponential loss is slightly faster in
optimizing the corresponding objective function thanks to its smoothness, but it was negligible considering its worse accuracy compared to the hinge loss. All implementations are in Matlab, and all
experiments were performed on a 4-core 2.0 GHZ of a load-free machine with a 12G of RAM.

5.2 Metrics
5.2.1 Metrics for rating prediction
We employ two well-known measures, the Mean Absolute Error (MAE) and the Root Mean Squared
Error (RMSE) [25] to measure the prediction accuracy of the proposed approach in comparison with
other basic collaborative filtering and trust/distrust-enhanced recommendation methods.
MAE is very appropriate and useful measure for evaluating prediction accuracy in offline tests [25,
45]. To calculate MAE, the predicted rating is compared with the real rating and the difference (in absolute value) considered as the prediction error. Then, these individual errors are averaged over all predictions to obtain the overall MAE value. More precisely, let T denote the set of ratings to be predicted,

17

Table 4: The frequencies of user’s rating.
# of Ratings
# of Users

0-10
4,198,074 (≈ 33%)

11-20
3,053,144 (≈ 24%)

21-30
2,289,858 (≈ 18%)

31-40
1,526,572 (≈ 12%)

41-50
534,300 (≈ 4.2%)

267,1

# of Ratings
# of Users

61-70
157,745 (≈ 1.24%)

71-80
143,752 (≈ 1.13%)

81-90
104,315 (≈ 0.82%)

91-100
43,252 (≈ 0.34%)

101-200
21,626 (≈ 0.17%)

2
10,68

i.e., T = {(i , j ) ∈ [n]×[m], R i j needs to be predicted} and let R̂ denote the prediction matrix obtained by
algorithm after factorization. Then,
P
MAE =

(i , j )∈T

|R i j − R̂ i j |

|T |

,

where R i j is the real rating assigned by the user i to the item j , and R̂ i j is the rating user i would assign
to the item j that is predicted by the algorithm .
The RMSE metric is defined as:
v
uP
¡
¢2
u
t (i , j )∈T R i j − R̂ i j
.
RMSE =
|T |
The first measure (MAE) considers every error of equal value, while the second one (RMSE) emphasizes larger errors. We would like to emphasize that even small improvements in RMSE are considered
valuable in the context of recommender systems. For example, the Netflix prize competition offered a
1,000,000 reward for a reduction of the RMSE by 10% [72].
5.2.2 Metrics for evaluating the correlation of ratings with trust/distrust relations
As part of our experiments, we investigate how the explicit trust/distrust relations between users in
the social network are aligned with the implicit trust/distrust relations between users conveyed from
the rating information. We use recall, Mean Average Precision (MAP) [44] and Normalized Discount
Cumulative Gain (NDCG) to evaluate the ranking results. Recall is defined as the number of relevant
friends divided by the total number of friends in the social network. Precision is defined as the number
of relevant friends (trusted or distrusted) divided by the number of friends in the social network. Given
a user u, let r i be the relevance score of the friend ranked at position i , where r i = 1 if the user is relevant
to the u and r i = 0 otherwise. Then we can compute the Average Precision (AP) as
P
r i × Precision@i
.
AP = i
# of relevant friends
MAP is the average of AP over all the users in the network.
NDCG is a normalization of the Discounted Cumulative Gain (DCG) measure. DCG is a weighted
sum of the degree of relevancy of the ranked users. The weight is a decreasing function of the rank
(position) of the user, and therefore called discount. NDCG normalizes DCG by the Ideal DCG (IDCG),
which is simply the DCG measure of the best ranking result. Thus NDCG measure is always a number
in [0, 1]. NDCG at position k is defined as:
NDCG@k = Zk

k
X
2r i − 1
i =1 log(i + 1)

where k is also called the scope, which means the number of top-ranked users presented to the user
and Zk is chosen such that the perfect ranking has a NDCG value of 1. We note that the base of the
logarithm does not matter for NDCG, since constant scaling will cancel out due to normalization. We
will assume it is the natural logarithm throughout this paper.

18

Figure 2: Grid Search to find the best values for λU and λC on the data set with 90% of rating information.

5.3 Model Selection
Tuning of parameters (a.k.a model selection in learning community) is a critical problem in most of the
learning problems. In some situations, the learning performance may drastically vary with different
choices of the parameters. There are three parameters in objective (5) that play very important role in
the effectivity of the proposed algorithm. These are λU , λV , and λS . Between these, λS controls how
much the proposed algorithm should incorporate the information of the social network in completing
the partially observed rating matrix. In the extreme case, a very small value for λS , the algorithm almost forgets the social information exists between the users and only utilizes the observed user-item
rating matrix for factorization. On the other hand, if we employ a very large value for λS , the social
network information will dominate the learning process, leading to a poorer performance. Therefore,
in order to not hurt the recommendation performance, we need to find a reasonable value for social
regularization parameter. To this end, we analyze how the combination of these parameters affect the
recommendation performance.
We conduct a grid search on the potential values of two parameters λS and λV to find the combination with best performance. Figure 2 shows the grid search results for these parameters on data set with
90% of training data where the optimal prediction accuracy is achieved at point (14.8, 11) with the optimal RMSE = 1.12. We would like to emphasize that we have done the cross validation for only pairs of
(λS , λV ) and (λS , λU ) because, (i) considering the grid search for the triplet (λS , λU , λV ) is computationally burdensome, (ii) and our preliminary experiments showed that λV and λU behave similarly with
respect to λS . Based on the results reported in Figure 2, in the remaining experiments, we set λS = 14.8,
λV = 11, and λU = 13 when the training is performed on the data set with 90% of rating information.
We repeat the same process to find out the best setting of regularization parameters for other data sets
with 80%, 70%, and 60% of rating data as well.

5.4 Baseline Methods
Here we briefly discuss the baseline algorithms that we intend to compare the proposed algorithm.
The baseline algorithms are chosen from both types of memory-based and model-based recommender
systems with different types of trust and distrust relations. In particular, we consider the following basic
algorithms:
• MF (matrix factorization based recommender): this is the basic matrix factorization based rec19

ommender formulated in the optimization problem in (1) which does not take the social data
into account.
• MF+T (matrix factorization with trust information): to exploit the trust relations between users
in matrix factorization, [40] relied on the fact that the distance between latent features of users
who trust each other must be minimized that can be formulated as the following objective:
min
U

n
X
1X
D(Ui ,: ,U j ,: ),
2 i =1 j ∈N+ (i )

where N+ (i ) is the set of users the i th user trusts in the social network (i.e., S i j = +1). By employing this intuition in the basic formulation in (1), [40] solves the following optimization problem:
"
#
´2 α X
n
X
1 X ³
λU
λV
>
min
R i j −Ui ,: V j ,: +
D(Ui ,: ,U j ,: ) +
kUkF +
kVkF .
U,V 2 (i , j )∈Ω
2 i =1 j ∈N+ (i )
2
2
R
• MF+D (matrix factorization with distrust information): the basic intuition behind the algorithm
proposed in [40] to exploit the distrust relations is as follows: if user u i distrusts user u j , then we
can assume that their corresponding latent features Ui ,: and U j ,: would have a large distance. As
a result we aim to maximize the following quantity for all users:
max
U

n
X
1X
D(Ui ,: ,U j ,: ),
2 i =1 j ∈N− (i )

where N− (i ) denotes the set of users the i th users distrusts (i.e, S i j = −1). Adding this term to the
basic optimization problem in (1) we obtain the following optimization problem:
"
#
´2 β X
n
X
1 X ³
λU
λV
>
min
D(Ui ,: ,U j ,: ) +
kUkF +
kVkF .
R i j −Ui ,: V j ,: −
U,V 2 (i , j )∈Ω
2 i =1 j ∈N− (i )
2
2
R
• MF+TD (matrix factorization with trust and distrust information): this algorithm stands for the
algorithm proposed in the present work. We note that there is no algorithm in the literature that
exploits both trust and distrust relations in factorization process simultaneously.
• NB (neighborhood-based recommender): this algorithm is the basic memory-based recommender algorithm that predicts a rating of a target item i for user u using a combination of the
ratings of neighbors of u (similar users) that already issued a rating for item i . Formally,
P
W 0 (R ui − R̄ u )
u 0 ∈N (u),Wuu 0 >0 uu
R̂ ui = R̄ u +
,
(9)
P
Wuu 0
u 0 ∈N (u),W 0
uu

where the pairwise weight Wuu 0 between pair of users (u, u 0 ) is calculated by Pearson’s correlation
coefficient [25]
• NB+T (neighborhood with trust information) [45, 17, 47]: the basic idea behind the trust based
recommender systems proposed in TidalTrsut [17] and MoleTrsut [45] is to limit the set of neighbors in (9) to the users who are trusted by user u. The distinguishing feature of these algorithms
is the mechanism of trust propagation to estimate the trust transitively for all the users. By adapting (9) to only consider trustworthy neighbors in predicting the new ratings we obtain:
P
W 0 (R ui − R̄ u )
u 0 ∈N+∗ (u),Wuu 0 >0 uu
,
(10)
R̂ ui = R̄ u +
P
W 0
u 0 ∈N ∗ (u),W 0 >0 uu
+

uu

where N+∗ (u) is the set of trusted neighbors of u in the social network with propagated trust relations (when there is no propagation we have N+∗ (u) = N+ (u)). We note that instead of Pearson’s
correlation coefficient as the wighting schema, we can infer the weights exploiting the social relation between the users. Since for the data set we consider in our experiments, the trust/distrust
relations are binary values, the social based pairwise distance would be simply the hamming distance between the binary vector representation of social relations of users. For implementation
details we refer to [70, Chapter 6].
20

Table 5: The consistency of implicit and explicit trust relations in the data set for different ranges of
ratings measured in terms of NDCG, recall, and MAP.
# of Ratings
NDCG@10 NDCG@20
Recall@10 Recall@20 Recall@40
MAP
0-20
21-40
41-60
61-80
≥ 81

0.083
0.108
0.117
0.120
0.135

0.078
0.103
0.112
0.117
0.126

0.054
0.080
0.083
0.088
0.091

0.092
0.125
0.128
0.132
0.151

0.156
0.198
0.225
0.230
0.253

0.140
0.190
0.208
0.230
0.244

Table 6: The consistency of implicit and explicit distrust relations in the data set for different ranges of
ratings measured in terms of NDCG, recall, and MAP.
# of Ratings
NDCG@10 NDCG@20
Recall@10 Recall@20 Recall@40
MAP
0-20
21-40
41-60
61-80
≥ 81

0.065
0.071
0.082
0.089
0.104

0.057
0.068
0.072
0.078
0.096

0.045
0.060
0.075
0.081
0.087

0.071
0.077
0.085
0.105
0.125

0.132
0.140
0.158
0.164
0.191

0.130
0.134
0.152
0.160
0.183

• NB+TD-F (neighborhood with trust information and distrust information as filtration) [69, 72]:
a simple strategy to use distrust relations in the recommendation is to filter out distrusted users
from the list of neighbors in predicting the ratings. As a result, we adapt (9) to exclude distrusted
users from the users’ propagated web of trust.
• NB+TD-D (neighborhood-based with trust information and integrated distrust information) [69,
72]: in the same spirit as the filtration strategy, we can use distrust relations to debug the trust
relations. More specifically, if user u trusts user v, v trusts w, and u distrusts w, then the latter
distrust relation contradicts the propagation of the trust from u to w and can be excluded from
the prediction. In this method distrust is used to debug the trust relations.

5.5 On the Consistency of Social Relations and Rating Information
As already mentioned, the Epinions website allows users to write reviews about products and services
and to rate reviews written by other users. Epinions also allows users to define their web of trust, i.e.
"reviewers whose reviews and ratings have been consistently found to be valuable" and their block list,
i.e. "reviewers whose reviews are found to be consistently inaccurate or not valuableÓ. Different intuitions on interpreting these social information will result in different models. The main rational behind
incorporating trust and distrust relations in recommendation process is to take the trust/distrust relations between users in the social network as the level of agreement between ratings assigned to reviews
by users 3 . Therefore, investigating the consistency or alignment between user ratings (implicit trust)
and trust/distrust relations in the social network (explicit trsut) become an important issue.
Here, we aim to empirically investigate whether or not there is a correlation between a user’s current
trustees/friends or distrusted friends and the ratings that user would assign to reviews issued by his
neighbors. Obviously, if there is no correlation between social context of a user and his/her ratings to
reviews written by his neighbors, then the social structure does not provide any advantage to the rating
information. On the other hand, if there exists such a correlation, then the social context could be
supplementary information to compensate for the lack of rating information to boost the accuracy of
recommendations.
The consistency of trust relations and rating information issued by users on the reviews written
by his trustees has been analyzed in [83, 23]. However, [83] also claimed that social trust (i.e., explicit
trust) and similarity between users based on their issued ratings (i.e., implicit trust) are not the same,
3 In the literature the similarity between users conveyed from the rating information issued by users and the direct relation in
the social network are usually referred to as the implicit and the explicit trust, respectively.

21

Table 7: The alignment rate of users in establishing trust/distrust relationships with future users in the
social network based on the majority vote of their current trusted/distrusted friends. The number of
trusted friends (+) and distrusted friends (-) are denoted by n + and n − , respectively. Here u denotes the
current user and w stands for a future user in the network.
Setting
Type of Relation (u
w) % of Relations Alignment Rate (%)
n+ > n−
n+ < n−
n + = n − > 0 or n + = n − = 0

+
+
+

48.80
2.54
1.15
8.02
39.49

92.09
8.15
17.88
83.42
-

and can be used complementary. According to [38], when comparing implicit social information with
explicit social information, the performance of using implicit information is slightly worse. We further
investigate the same question about the consistency of distrust relations and ratings issued by users to
their distrusted neighbors. The positive answer to this question can be interpreted as follows. Given
that user u is interested in item i , the chances that v, trusted (distrusted) by u, also likes this item i is
much higher (lower) than for user w not explicitly trusted (distrusted) by u.
To measure the similarity between users, there are several methods we can borrow in the literature.
In this paper, we adopt the most popular approach that is referred to as Pearson correlation coefficient
(PCC) P : U × U 7→ [−1, +1] [6, 47], which is defined as:
Pm
i =1 (R ui − R̄ u )(R vi − R̄ v )
, ∀u, v ∈ U ,
P (u, v) = qP
Pm
m
2
2
j =1 (R ui − R̄ u ) × j =1 (R vi − R̄ v )
where R̄ u and R̄ v are the average of ratings issued by users u and v, respectively. The PCC measures
the extent to which there is a linear relationship between the rating behaviors of the two users, the extreme values being -1 and 1. The similarity of two users becomes negative when users have completely
diverging ratings. We note that this quantity can be considered as the implicit trust between users that
is conveyed via ratings given by users.
To conduct this set of experiments, we first group all the users in the training data set based on
the number of ratings, and then measure the prediction accuracies of different user groups. Users are
grouped into five classes: "[1, 20)", "[20, 40)", "[40, 60)", "[60, 80)", and "> 81 ". In order to have a
comprehensive view of the ranking performance, we present the NDCG, recall and MAP scores of trust
and distrust alignments on the Epinions data set in Table 5 and Table 6, respectively. We note that
the data set we use in our experiments only contains bivalent trust values, i.e., -1 and +1, and it is not
possible to have an ordering on the list of friends (timestamp of relations would be an option to order
the friends but unfortunately it is not available in our data set). To compute the NDCG, we use the
ordering of trusted/distrusted friends which yields the best value.
On the positive side, we observe a clear trend of alignment between ratings assigned by a user and
the type of relation he has made in the social network. This observation coincides with our intuition.
Overall, when more ratings are observed for a user, the similarity calculation process will find more
accurate similar or dissimilar neighbors for this user since we have more information to represent or
interpret this user. Hence, by increasing the number of ratings, It is conceivable from the results in Tables 5 and 6 that the alignment between implicit and explicit neighbors becomes better. By comparing
the results in Tables 5 and 6 we can see that trust relations are slightly better aligned than the distrust
relations.
On the negative side, the results show that the NDCG on both types of relations is small. One explanation for this phenomenon is that the Epinions data set is not tightly bound to a specific application.
For example, a user may trust or distrust anther user based on his/her comments on a specific product
but they might have similar taste on other products. Furthermore, compared to other data sets such as
FilmTrusts, the Epinions data set is very sparse data set, and consequently it is relatively inaccurate to
rely on the rating information to compute the implicit trust relations. Finally, our approach to distinguish trust/distrust lists from the rating information is limited by the PCC trust metric we have utilized.
We conjecture that better trust metrics that is able to exploit other side information such as time and in22

Table 8: The accuracy of prediction of matrix factorization with three different methods measured in
terms of MAE and RMSE errors. The parameter k represents the number of latent features in factorization.
k
% of Training Measure
MF
MF+T
MF+D
MF+TD
10

60%
70%
80%
90%

20

60%
70%
80%
90%

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

0.9813 ± 0.042
1.6050 ± 0.032
0.9462 ± 0.083
1.5327 ± 0.032
0.9150± 0.022
1.3824 ± 0.032
0.8921 ± 0.025
1.2166 ± 0.017

0.8561 ± 0.032
1.4125 ± 0.022
0.8332 ± 0.092
1.2407 ± 0.063
0.8206 ± 0.041
1.1906 ± 0.042
0.8158 ± 0.016
1.1403 ± 0.027

0.9720 ± 0.038
1.5036 ± 0.040
0.9241 ± 0.012
1.4405 ± 0.023
0.8722 ± 0.034
1.3155 ± 0.026
0.8736 ± 0.053
1.1869 ± 0.049

0.8310 ± 0.016
1.2294 ± 0.086
0.8206 ± 0.023
1.1562 ± 0.043
0.8113 ± 0.032
1.1061 ± 0.021
0.8025 ± 0.014
1.0872 ± 0.020

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

0.9972 ± 0.016
1.6248 ± 0.014
0.9688 ± 0.019
1.5162 ± 0.016
0.9365 ± 0.025
1.4081 ± 0.015
0.9224 ± 0.016
1.2207 ± 0.0 18

0.8431 ± 0.018
1.3904 ± 0.042
0.8342 ± 0.062
1.2722 ± 0.027
0.8172 ± 0.011
1.1853 ± 0.023
0.8128 ± 0.021
1.1402 ± 0.026

0.9746 ± 0.060
1.5423 ± 0.046
0.9350 ± 0.022
1.4540 ± 0.075
0.8705 ± 0.016
1.3591 ± 0.073
0.8805 ± 0.032
1.1933 ± 0.028

0.8475 ± 0.012
1.1837 ± 0.023
0.8290 ± 0.034
1.1452 ± 0.016
0.8129 ± 0.025
1.1049 ± 0.082
0.8096 ± 0.010
1.0851 ± 0.011

teractional information would be helpful in distinguishing implicit trusted/distrusted friends, leading
to better alignment between implicit and explicit trust relations.
We also conduct experiments to evaluate the consistency of social network only based on the
trust/distrust relations between users. In particular, we investigate to what extent a users’ relations
are aligned with the opinion of his/her neighbors in the social network. More specifically, let u be a
user who is about to make a trust or distrust relation to another user v. We assume that n + number
of u’s neighbors trust v and n − number of u’s neighbors distrust v. We note that in the real data set
the distrust relations are hidden. To conduct this set of experiments, we randomly sample 30% of the
relations from the social network and use the remaining 70% to predict the type of sampled relations 4
by majority voting.
Table 7 shows the results on the consistency of social relations. We observe that in all cases there
is an alignment between the opinions of users’ friends and his/her own relation (92.09% and 83.42%
when the majority of friends trust and distrust the target user, respectively). This might be due to social
influence of people on social network, however, it is hard to justify the existence of such a correlation in
Epinions data set which includes reviews for diverse set of products and taste of users. One interesting
observation from the results reported in Table 7 is the case where the number of distrusted users dominates the number of trusted users (i.e., n − > n + ). While the distrust relations are private to other users,
but we can see that there is a significant alignment between users’s relation type and his distrusted
friends.

5.6 On the Power of Utilizing Social Relationships
We now turn to investigate the effect of utilizing social relationships between users on the accuracy
of recommendations in factorization-based methods. In other words, we would like to experimentally
evaluate whether incorporating distrust can indeed enhance the trust-based recommendation process.
To this end, we run four different MF (i.e., pure matrix factorization based algorithm), MF+T (i.e., matrix
factorization with only trust relationships), MF+D (i.e., matrix factorization with only distrust relationships), and MF+TD (i.e., the algorithm proposed here) algorithms on the data set. We run the algorithms
with k = 10 and k = 20 latent vector dimensions. As mentioned earlier, different amount of training data
90%, 80% , 70% and 60% has been used to create four different training sets that are increasingly sparse
but the social network remains the same in all of them. We evaluate all algorithms by both MAE and
4 A more realistic way would be to use the timestamp of relations to create the training and test sets.

23

RMSE measures.
Table 8 shows the MAE and RMSE errors for the four sampled data sets. First, as we expected, the
performance of all learning algorithms improves with an increasing number of training data. It is also
not surprising to see that the MF+T, MF+D, and MF+TD algorithms which exploit social side information perform better than the pure matrix factorization based MF algorithm. Second, the proposed
algorithm outperforms all other baseline algorithms for all the cases, indicating that it is effective to incorporate both types of social side information in recommendation. This result by itself indicates that
besides trust relationships in the social network, the distrust information is also a rich source of information and can be utilized in recommendation algorithms. We note that distrust information needs to
be incorporated carefully as its nature is totally different from trust information. Finally, it is noticeable
that the MF+T outperforms the MF+D algorithm due to huge number of trust relations to distrust relations in our data set. It is also remarkable that users are more likely to be influenced by their friends
to make trust relations than the distrust relations due to the private nature of distrust relations in Epinions data set. This might lead us to believe that distrust relations have better quality than trust relations
which requires a deeper investigation to be verified.

5.7 Comparison to Baseline Algorithms
Another question that is worthy of investigation is how state-of-the-art approaches perform compared
to the method proposed in this paper. To this end, we compare the performance of the MF-TD algorithm with the baseline algorithms introduced in Subsection 5.4. Table 9 contains the results of our
experiments with eight different algorithms on the data set with 90% of rating data. The second column
in the table represents the configuration of parameters used by each algorithm.
When we utilize trust/distrust relations in neighborhood-based algorithms, a crucial decision we
need to make is to which level the propagation must be performed (no propagation corresponds to the
single level propagation which only includes direct neighbors). Let p and q denote the level of propagation for trust and distrust relations, respectively. Let us first consider the trust propagation to decide
the value of p. We note that there is a tradeoff between accuracy and the level of trust propagation: the
longer propagation levels results in less accurate trust predictions. This is due the fact that when we use
longer propagation levels, the further away we are heading from each user, and consequently decrease
the confidence on the predictions. Obviously this affects the accuracy of the recommendations significantly. As a result, for the trust propagation we only consider single level propagation by choosing p = 1
(i.e, N+∗ = N+ ). We also note that since in the Epinions data set a user can not simultaneously trust
and distrust another user, in the neighborhood-based method with distrust relations, the debugging
only makes sense for propagated information. Therefore, we perform a three level distrust propagation
(q = 3) to constitute the set of distrusted users for each users. We note that the longer the propagation
levels, the more often distrust evidence can be found for a particular user, and hence the less neighbors will be left to participate in the recommendation process. For factorization based methods, the
value of regularization parameters, i.e., λU , λV , and λS , are determined by the procedure discussed in
Subsection 5.3.
The results of Table 9 reveal some interesting conclusions as summarized below:
• From Table 9, we can observe that for factorization-based methods, incorporating trust or distrust
information boost the performance of recommendation in terms of both accuracy measures. This
demonstrates the advantages of trust and distrust-aware recommendation algorithms. We also
can see that both MF+T and MF+D perform better than the non-social MF but the performance
of MF+T is significantly better than MF+D. As discussed before, this observation does not indicate
that the trust relations are more beneficial than the distrust relations as in our data set only 16.7%
of relations are distrust relations. The MF+TD algorithm that is able to employ both types of relations is significantly better than other algorithms that demonstrates the advantages of proposed
method to utilize trust and distrust relations.
• Looking at the results reported in Table 9, it can immediately be noticed that the incorporation
of trust and distrust information in neighborhood-based methods decreases the prediction error
but the improvement is not as significant as the factorization based methods. We note that for
the NB+T method with longer levels of propagation (p = 2, 3), our experiments revealed that the
accuracy remains almost same or gotten worse on both MAE and RMSE measures and this is
24

Table 9: Comparison with other popular methods. The reported values are the MAE and RMSE on the
data set with 90% of rating information. The values of parameters for each specific algorithm is included
in the second column.
Method
Parameter (s)
MAE
RMSE
MF
MF+T
MF+D
MF+TD

k = 10 and λU = λV = 5
k = 10, λU = λV = 5 , and α = 1
k = 10, λU = λV = 5 , and β = 10
k = 10, λU = 13, λV = 11 , and λS = 14.8

0.8921
0.8158
0.8736
0.8025

1.2166
1.1403
1.1852
1.0872

NB
NB+T
NB+TD-F
NB+TD-D

p =1
p = 1 and q = 3
p = 1 and q = 3

0.9381
0.8904
0.8692
0.8728

1.5275
1.3455
1.2455
1.2604

Table 10: The accuracy of handling cold-start users and the effect of social relations. The number of
leant features in this experiments is set to k = 10. The first column shows the number of cold-start
users sampled randomly from all users in the data set. For the cold-starts users all the ratings have
been excluded from the training data and used in the evaluation of three different algorithms.
% of Cold-start Users Measure
MF
MF+T MF+D MF+TD
30%
20%
10%
5%

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

0.9923
1.7211
0.9812
1.7088
0.9334
1.4222
0.9134
1.3852

0.8824
1.5562
0.8805
1.4339
0.8477
1.3782
0.8292
1.2921

0.9721
1.6433
0.9505
1.6250
0.9182
1.4006
0.8633
1.3255

0.8533
1.4802
0.8472
1.2630
0.8322
1.2655
0.8280
1.2888

why we only report the results only for p = 1. In contrast, for distrust propagation we found out
that q = 3 has a visible impact on the performance of both filtering and debugging methods. We
would like to emphasize that for longer levels of distrust propagation in Epinions data set, i.e.,
q > 4, we found that the size of the set of distrusted users N−∗ (·) becomes large for most of users
which degrades the prediction accuracy. We also observe another interesting result about the
performance of NB+TD method with filtering and debugging strategies. We found that although
filtering generates slightly better predictions, NB+TD-F performs almost as good as the NB+TDD method. Although this observation does not suggest any of these methods as the method of
choice in incorporating distrust, we believe that the accuracy might differ from data set to data
set and it strongly depends on the propagation/aggregation strategy.
• Considering the results for both model-based and memory-based methods in Table 9, we can
conclude few interesting observations. First, we notice that factorization-based methods with
trust/distrust information perform better than the neighborhood based methods. Second, the
incorporation of trust and distrust relations in matrix factorization has significant improvement
compared to improvement achieved by memory-based methods. Although the type of filtration
or debugging strategy could significantly affect the accuracy of incorporating distrust in memorybased methods, but the main shortcoming of these methods comes from the fact that these algorithms somehow exclude the influence of distrusted users from the rating prediction. This stands
in stark contrast to the model proposed in this paper that ranks the neighbors based on the type
of relation. This observation necessitates to devise better algorithms for propagation and aggregation of trust/distrust information in memory-based methods.

25

Table 11: The accuracy of proposed algorithm on a data set with 390257 (≈ 90%) trust relations sampled
uniformly at random from all trust relations with varied number of distrust relations. The learning is
performed based on 90% of all ratings with k = 10 as the dimension of latent features.
Method # of Trust Relations # of Distrust Relations Measure
Accuracy
MF+TD

433,619 (≈ 90%)

9,682 (≈ 10%)
19,364 (≈ 20%)
29,047 (≈ 30%)
38,729 (≈ 40%)
48,411 (≈ 50%)
58,093 (≈ 60%)
67,776 (≈ 70%)
77,458 (≈ 80%)
87,140 (≈ 90%)
96,823 (= 100%)

MF+T

481,799 (= 100%)

0

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

0.8803 ± 0.051
1.2166 ± 0.028
0.8755 ± 0.033
1.1944 ± 0.042
0.8604 ± 0.036
1.1822 ± 0.081
0.8431 ± 0.047
1.1706± 0.055
0.8165± 0.056
1.1425± 0.091
0.8130± 0.035
1.1380± 0.046
0.8122 ± 0.041
1.1306 ± 0.042
0.8095 ± 0.036
1.1290 ± 0.085
0.8061 ± 0.044
1.1176 ± 0.067
0.8050 ± 0.052
1.1092 ± 0.063

MAE
RMSE

0.8158 ± 0.016
1.1403 ± 0.027

5.8 Handling Cold-start Users by Social Side Information
In this subsection, we demonstrate the use of social network to further illustrate the potential of proposed framework and the relevance of incorporating side information. To do so, as another set of our
experiments, we intend to examine the performance of proposed algorithm on clod-start users. Addressing cold-start users (i.e., users with few ratings or new users) is a very important for the success
of recommender systems due to huge number of this type of users in many real world systems. As a
result, handling cold-start users is one the main challenges in existing systems. To evaluate different
algorithms we randomly select 30%, 20%, 10%, and 5% as the cold-start users. For cold-start users, we
do not include any rating in the training data and consider all the ratings made by cold-start users as
testing data.
Table 10 shows the performance of above mentioned algorithms. As it is clear from the Table 10,
when the number of cold-start users is low with respect to the total number of users, say 5% of total
users, the affect of distrust relationships is negligible in prediction accuracy. But, when the number of
cold-start users is high, exploiting the trust and distrust relationships significantly improve the performance of recommendation. This result is interesting as it reveals that the lack of rating information for
cold-start and new users can be alleviated by incorporating the social relations of users, and in particular both trust and distrust relationships.

5.9 Trading Trust for Distrust Relationships
We also compare the potential benefit of trust relations to distrust relations in the proposed algorithm.
More specifically, we would like to see in what extent the distrust relations can compensate for the
lack of trust relations. We run the proposed algorithm with the subset of trust and distrust relations and
compare it to the algorithm which only utilizes all of the trust relations. To setup this set of experiments,
we randomly sample a subset of trust relations and gradually increase the amount of distrust relations
to see when the effect of distrust information compensate the effect of missed trust relations.

26

We sample 433,619 (approximately 90%) trust relations from the total 481,799 trust relations and
vary the number of distrust relations and feed to the proposed algorithm. Table 11 reports the accuracy
of proposed algorithm for different number of distrust relations in the data sets. All these samplings
have been done uniformly at random. We use 90% of all ratings for training and the remaining 10% for
evaluation, and set the dimension of latent features to k = 10. As it can be concluded from Table 11,
when we feed the proposed algorithm MF+TD with 90% of trust and 50% of the distrust relations, it
reveals very similar behavior to the trust-enhanced matrix factorization based method MF+T, which
only utilizes all the trust relations in factorization. This result is interesting in the sense that the distrust
information between users is as important as the trust information (we note that in this scenario the
number trust relations excluded from the training is almost same as the number of distrust relations
included). By increasing the number of distrust relations we can observe that the accuracy of recommendations increases as expected. In summary, this set of experiments validates that incorporating
distrust relations can indeed enhance the trust-based recommendation process and could be considered as a rich source of information to be exploited.

5.10 On the Impact of Batch Size in Stochastic Optimization
As mentioned earlier in the paper, directly solving the optimization problem in (5) using full gradient
descent method requires to go through all the triplets in the constraint set ΩS which could be computationally expensive due to the huge number of triplets in ΩS . To overcome this efficiency problem, one
can turn to stochastic gradient scent method which tries to generate unbiased estimates of the gradient
at each iteration in a much cheeper way by sampling a subset of triplets from ΩS .
To accomplish this goal, we perform gradient descent and stochastic gradient descent to solve the
optimization problem in (5) to find the matrices U and V following the updating equations derived in (7)
and (8). At each iteration t , the currently learned matrices Ut and Vt are used to predict the ratings in
the test set. In particular, at each iteration, we evaluate the RMSE and MAE on the test set, and terminate
training once the RMSE and MAE starts increasing, or the maximum number of iterations is reached.
We run the algorithm with latent vectors of dimension k = 10.
We compare the computational efficiency between proposed algorithm with GD and mini-batch
SGD with different batch sizes. We note that the GD updating rule can be considered as min-batch
SGD where the batch size B is deterministically set to be B = |ΩS | and simple SGD can be considered as
mini-batch SGD with B = 1. We remark that in contrast to GD method which uses all the triplets in ΩS
for gradient computation at each iteration, for SGD method due to uniform sampling over all tuples in
ΩS , some of the tuples may be used more than once and some of the tuples might never been used for
gradient computation.
Figures 3 and 4 show the convergence rate of four different updating rules in terms of the number of
iterations t for two different measures RMSE and RME, respectively. The first algorithm denoted by GD
runs the simple full gradient descent iteratively to optimize the objective. The other three algorithms
named SGD1, SGD2, and SGD3 in the figures use the batch sizes of B = 0.1 ∗ |ΩS |, B = 0.2 ∗ |ΩS |, and
B = 0.3 ∗ |ΩS |, respectively. In our experiments, due to very slow convergence of the basic SGD method
with B = 1 in comparison to other fours methods, we simply exclude its result from the discussion.
In terms of accuracy of predictions, from both Figures 3 and 4, we can conclude that the GD has
the best convergence and SGD3 has the worst convergence in all settings. This is because, although
all of the four algorithms use an unbiased estimate of the true gradient to update the solution at each
iteration, but the variance of each stochastic gradient is proportional to the size of the batch size B .
Therefore, for larger values of B , the variance of stochastic gradients is smaller and the algorithm convergences faster, but, for smaller values of B the algorithm suffers from high variance in stochastic
gradients and convergences slowly. We emphasize that this comparison holds for iteration complexity which is different from the computational complexity (running time) of individual iterations. More
specifically, each iteration of GD requires |ΩS | gradient computations, while for SGD we only need to
perform B ¿ |ΩS | gradient computations. In summary, SGD has lightweight iteration but requires more
iterations to converge. In contrast, GD takes expensive steps in much less number of iterations. From
Figures 3 and 4, it is noticeable that although a large number of iterations is usually needed to obtain a
solution of desirable accuracy using SGD, the lightweight computation per iteration makes SGD attractive for the optimization problem in (5) for large number of users. We also not that for the GD method,
the error is a monotonically decreasing function it terms of number of iterations t , but for the SGD

27

based methods this does not hold. This is because although SGD algorithm is guaranteed to converge
to an optimal solution (at least in expectation), but there is no guarantee that the stochastic gradients
provide a descent direction for the objective at each iteration due to the noise in computing gradients.
As a result, for few iterations we can see that the objective increases but finally it convergences as expected.

6 Conclusions and Future Works
In this paper, we have made a progress towards making distrust information beneficial in social recommendation problem. In particular, we have proposed a framework based on the matrix factorization
which is able to incorporate both trust and distrust relationships between users in factorization algorithm. We experimentally investigated the potential of distrust as a side information to overcome the
data sparsity and cold-start problems in traditional recommender systems. In summary, our results
showed that more accurate recommendations can be obtained by incorporating distrust relations, indicating that distrust information can indeed be beneficial for the recommendation process.
This work leaves few directions, both theoretically and empirically, as future work. From an empirical point of view, it would be interesting to extend our model for weighted social trust and distrust
relations. One challenge in this direction is that, as far as we know, there is no publicly available data set
that includes weighted (gradual) trust and distrust information. Also, the experimental results we have
conducted on the consistency of social relations with rating information hint at a number of potential
enhancements in future work. In particular, it would be interesting to further examine the correlation between implicit and explicit distrust information. An important challenge in this direction is to
develop better metrics to measure the implicit trust between users as the simple metrics such as Pearson correlation coefficient seem to be insufficient. Furthermore, since we only consider the distrust
between users, it would be easy to generalize our model in the same way to incorporate dissimilarity
between items and investigate how it works in practice. Also, our preliminary results indicated that
hinge loss almost performs better than the exponential loss, but from the optimization viewpoint, the
exponential loss is more attractive due to its smoothness. So, an interesting direction would be to use a
smoothed version of the hinge loss to gain from both optimization efficiency and algorithmic accuracy.

References
[1] Gediminas Adomavicius and Alexander Tuzhilin. Toward the next generation of recommender
systems: A survey of the state-of-the-art and possible extensions. IEEE Transactions on Knowledge
and Data Engineering, 17(6):734–749, 2005.
[2] Deepak Agarwal and Bee-Chung Chen. flda: matrix factorization through latent dirichlet allocation. In Proceedings of the third ACM international conference on Web search and data mining,
pages 91–100. ACM, 2010.
[3] Paolo Avesani, Paolo Massa, and Roberto Tiella. A trust-enhanced recommender system application: Moleskiing. In Proceedings of the 2005 ACM symposium on Applied computing, pages 1589–
1593, 2005.
[4] Giacomo Bachi, Michele Coscia, Anna Monreale, and Fosca Giannotti. Classifying trust/distrust
relationships in online social networks. In Privacy, Security, Risk and Trust (PASSAT), 2012 International Conference on and 2012 International Confernece on Social Computing (SocialCom), pages
552–557. IEEE, 2012.
[5] Jesús Bobadilla, Fernando Ortega, Antonio Hernando, and Abraham Gutiérrez. Recommender
systems survey. Knowledge-Based Systems, 46:109–132, 2013.
[6] John S Breese, David Heckerman, and Carl Kadie. Empirical analysis of predictive algorithms for
collaborative filtering. In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence, pages 43–52. Morgan Kaufmann Publishers Inc., 1998.

28

[7] Moira Burke and Robert Kraut. Mopping up: modeling wikipedia promotion decisions. In Proceedings of the 2008 ACM conference on Computer supported cooperative work, pages 27–36. ACM,
2008.
[8] Dorwin Cartwright and Frank Harary. Structural balance: a generalization of heider’s theory. Psychological review, 63(5):277, 1956.
[9] Gang Chen, Fei Wang, and Changshui Zhang. Collaborative filtering using orthogonal nonnegative
matrix tri-factorization. Information Processing & Management, 45(3):368–379, 2009.
[10] Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan. Better mini-batch algorithms via
accelerated gradient methods. In NIPS, volume 24, pages 1647–1655, 2011.
[11] David Crandall, Dan Cosley, Daniel Huttenlocher, Jon Kleinberg, and Siddharth Suri. Feedback
effects between similarity and social influence in online communities. In Proceedings of the 14th
ACM SIGKDD international conference on Knowledge discovery and data mining, pages 160–168.
ACM, 2008.
[12] Sanjoy Dasgupta, Michael L Littman, and David McAllester. Pac generalization bounds for cotraining. Advances in neural information processing systems, 1:375–382, 2002.
[13] Mukund Deshpande and George Karypis. Item-based top-n recommendation algorithms. ACM
Transactions on Information Systems (TOIS), 22(1):143–177, 2004.
[14] Thomas DuBois, Jennifer Golbeck, and Aravind Srinivasan. Predicting trust and distrust in social
networks. In Privacy, security, risk and trust (passat), 2011 ieee third international conference on
and 2011 ieee third international conference on social computing (socialcom), pages 418–424. IEEE,
2011.
[15] Rana Forsati, Hanieh Mohammadi Doustdar, Mehrnoush Shamsfard, Andisheh Keikha, and Mohammad Reza Meybodi. A fuzzy co-clustering approach for hybrid recommender systems. International Journal of Hybrid Intelligent Systems, 10(2):71–81, 2013.
[16] Rana Forsati and Mohammad Reza Meybodi. Effective page recommendation algorithms based on
distributed learning automata and weighted association rules. Expert Systems with Applications,
37(2):1316–1330, 2010.
[17] Jennifer Golbeck. Computing and applying trust in web-based social networks. PhD thesis, 2005.
[18] Jennifer Golbeck. Generating predictive movie recommendations from trust in social networks.
Springer, 2006.
[19] Jennifer Golbeck and James Hendler. Filmtrust: Movie recommendations using trust in web-based
social networks. In Proceedings of the IEEE Consumer communications and networking conference,
volume 96. Citeseer, 2006.
[20] Nathaniel Good, J Ben Schafer, Joseph A Konstan, Al Borchers, Badrul Sarwar, Jon Herlocker, and
John Riedl. Combining collaborative filtering with personal agents for better recommendations.
In AAAI/IAAI, pages 439–446, 1999.
[21] Quanquan Gu, Jie Zhou, and Chris Ding. Collaborative filtering: Weighted nonnegative matrix
factorization incorporating user and item graphs. In SIAM SDM, pages 199–210, 2010.
[22] R. Guha, Ravi Kumar, Prabhakar Raghavan, and Andrew Tomkins. Propagation of trust and distrust. In Proceedings of the 13th International Conference on World Wide Web, pages 403–412. ACM,
2004.
[23] Guibing Guo, Jie Zhang, Daniel Thalmann, Anirban Basu, and Neil Yorke-Smith. From ratings to
trust: an empirical study of implicit trust in recommender systems. In SAC, 2014.
[24] Jonathan L Herlocker, Joseph A Konstan, Al Borchers, and John Riedl. An algorithmic framework
for performing collaborative filtering. In Proceedings of the 22nd annual international ACM SIGIR
conference on Research and development in information retrieval, pages 230–237. ACM, 1999.
29

[25] Jonathan L Herlocker, Joseph A Konstan, Loren G Terveen, and John T Riedl. Evaluating collaborative filtering recommender systems. ACM Transactions on Information Systems (TOIS), 22(1):5–53,
2004.
[26] Thomas Hofmann. Latent semantic models for collaborative filtering. ACM Transactions on Information Systems (TOIS), 22(1):89–115, 2004.
[27] Mohsen Jamali and Martin Ester. Trustwalker: a random walk model for combining trust-based
and item-based recommendation. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 397–406. ACM, 2009.
[28] Mohsen Jamali and Martin Ester. A matrix factorization technique with trust propagation for recommendation in social networks. In Proceedings of the fourth ACM conference on Recommender
systems, pages 135–142. ACM, 2010.
[29] Mohsen Jamali and Martin Ester. A transitivity aware matrix factorization model for recommendation in social networks. In Proceedings of the Twenty-Second international joint conference on
Artificial Intelligence-Volume Volume Three, pages 2644–2649. AAAI Press, 2011.
[30] Arnd Kohrs and Bernard Merialdo. Clustering for collaborative filtering applications. In In Computational Intelligence for Modelling, Control & Automation. IOS. Citeseer, 1999.
[31] Ioannis Konstas, Vassilios Stathopoulos, and Joemon M Jose. On social networks and collaborative
recommendation. In Proceedings of the 32nd international ACM SIGIR conference on Research and
development in information retrieval, pages 195–202. ACM, 2009.
[32] Yehuda Koren. Factorization meets the neighborhood: a multifaceted collaborative filtering
model. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery
and data mining, pages 426–434. ACM, 2008.
[33] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender
systems. Computer, 42(8):30–37, 2009.
[34] Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. Predicting positive and negative links in
online social networks. In Proceedings of the 19th international conference on World wide web,
pages 641–650. ACM, 2010.
[35] Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. Signed networks in social media. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 1361–1370.
ACM, 2010.
[36] Wu-Jun Li and Dit-Yan Yeung. Relation regularized matrix factorization. IJCAI-09, 2009.
[37] Juntao Liu, Caihua Wu, and Wenyu Liu. Bayesian probabilistic matrix factorization with social
relations and item contents for recommendation. Decision Support Systems, 2013.
[38] Hao Ma. An experimental study on implicit social recommendation. In Proceedings of the 36th
international ACM SIGIR conference on Research and development in information retrieval, pages
73–82. ACM, 2013.
[39] Hao Ma, Irwin King, and Michael R Lyu. Learning to recommend with social trust ensemble. In
Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 203–210. ACM, 2009.
[40] Hao Ma, Michael R Lyu, and Irwin King. Learning to recommend with trust and distrust relationships. In Proceedings of the third ACM conference on Recommender systems, pages 189–196. ACM,
2009.
[41] Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin King. Sorec: social recommendation using probabilistic matrix factorization. In Proceedings of the 17th ACM conference on Information and knowledge management, pages 931–940. ACM, 2008.

30

[42] Hao Ma, Dengyong Zhou, Chao Liu, Michael R Lyu, and Irwin King. Recommender systems with
social regularization. In Proceedings of the fourth ACM international conference on Web search and
data mining, pages 287–296. ACM, 2011.
[43] Hao Ma, Tom Chao Zhou, Michael R Lyu, and Irwin King. Improving recommender systems by
incorporating social contextual information. ACM Transactions on Information Systems (TOIS),
29(2):9, 2011.
[44] Christopher D Manning, Prabhakar Raghavan, and Hinrich Schütze. Introduction to information
retrieval, volume 1. Cambridge university press Cambridge, 2008.
[45] Paolo Massa and Paolo Avesani. Trust-aware collaborative filtering for recommender systems. In
On the Move to Meaningful Internet Systems 2004: CoopIS, DOA, and ODBASE, pages 492–508.
Springer, 2004.
[46] Paolo Massa and Paolo Avesani. Controversial users demand local trust metrics: An experimental
study on epinions. com community. In Proceedings of the National Conference on artificial Intelligence, volume 20, page 121. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999,
2005.
[47] Paolo Massa and Paolo Avesani. Trust metrics in recommender systems. In Computing with Social
Trust, pages 259–285. Springer, 2009.
[48] Prem Melville, Raymond J Mooney, and Ramadass Nagarajan. Content-boosted collaborative filtering for improved recommendations. In AAAI/IAAI, pages 187–192, 2002.
[49] Bradley N Miller, Joseph A Konstan, and John Riedl. Pocketlens: Toward a personal recommender
system. ACM Transactions on Information Systems (TOIS), 22(3):437–476, 2004.
[50] Andriy Mnih and Ruslan Salakhutdinov. Probabilistic matrix factorization. In Advances in neural
information processing systems, pages 1257–1264, 2007.
[51] Uma Nalluri. Utility of distrust in online recommender systems. Technical report, 2008.
[52] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–
1609, 2009.
[53] Akshay Patil, Golnaz Ghasemiesfeh, Roozbeh Ebrahimi, and Jie Gao. Quantifying social influence
in epinions. HUMAN, 2(2):pp–67, 2013.
[54] Dmitry Pavlov and David M Pennock. A maximum entropy approach to collaborative filtering in
dynamic, sparse, high-dimensional domains. In NIPS, volume 2, pages 1441–1448, 2002.
[55] Michael J Pazzani. A framework for collaborative, content-based and demographic filtering. Artificial Intelligence Review, 13(5-6):393–408, 1999.
[56] David M Pennock, Eric Horvitz, Steve Lawrence, and C Lee Giles. Collaborative filtering by personality diagnosis: A hybrid memory-and model-based approach. In Proceedings of the Sixteenth
conference on Uncertainty in artificial intelligence, pages 473–480. Morgan Kaufmann Publishers
Inc., 2000.
[57] Jasson DM Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative prediction. In Proceedings of the 22nd international conference on Machine learning, pages
713–719. ACM, 2005.
[58] Ruslan Salakhutdinov and Andriy Mnih. Bayesian probabilistic matrix factorization using markov
chain monte carlo. In Proceedings of the 25th international conference on Machine learning, pages
880–887. ACM, 2008.
[59] Ruslan Salakhutdinov and Andriy Mnih. Probabilistic matrix factorization. Advances in neural
information processing systems, 20:1257–1264, 2008.
31

[60] Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. Restricted boltzmann machines for
collaborative filtering. In Proceedings of the 24th international conference on Machine learning,
pages 791–798. ACM, 2007.
[61] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. Item-based collaborative filtering
recommendation algorithms. In Proceedings of the 10th international conference on World Wide
Web, pages 285–295. ACM, 2001.
[62] Wanita Sherchan, Surya Nepal, and Cecile Paris. A survey of trust in social networks. ACM Computing Surveys (CSUR), 45(4):47, 2013.
[63] Luo Si and Rong Jin. Flexible mixture model for collaborative filtering. In ICML, volume 3, pages
704–711, 2003.
[64] Ian Soboroff and Charles Nicholas. Combining content and collaboration in text filtering. In Proceedings of the IJCAI, volume 99, pages 86–91, 1999.
[65] Nathan Srebro, Tommi Jaakkola, et al. Weighted low-rank approximations. In ICML, volume 3,
pages 720–727, 2003.
[66] Nathan Srebro, Jason DM Rennie, and Tommi Jaakkola. Maximum-margin matrix factorization.
Advances in neural information processing systems, 17(5):1329–1336, 2005.
[67] Mojdeh Talabeigi, Rana Forsati, and Mohammad Reza Meybodi. A hybrid web recommender system based on cellular learning automata. In Granular Computing (GrC), 2010 IEEE International
Conference on, pages 453–458. IEEE, 2010.
[68] Nele Verbiest, Chris Cornelis, Patricia Victor, and Enrique Herrera-Viedma. Trust and distrust aggregation enhanced with path length incorporation. Fuzzy Sets and Systems, 202:61–74, 2012.
[69] Patricia Victor, Chris Cornelis, Martine De Cock, and Ankur Teredesai. Trust- and distrust-based
recommendations for controversial reviews. IEEE Intelligent Systems, 26(1):48–55, 2011.
[70] Patricia Victor, Chris Cornelis, and Martine De Cock. Trust networks for recommender systems,
volume 4. Springer, 2011.
[71] Patricia Victor, Chris Cornelis, Martine De Cock, and Enrique Herrera-Viedma. Practical aggregation operators for gradual trust and distrust. Fuzzy Sets and Systems, 184(1):126–147, 2011.
[72] Patricia Victor, Nele Verbiest, Chris Cornelis, and Martine De Cock. Enhancing the trust-based
recommendation process with explicit distrust. ACM Transactions on the Web (TWEB), 7(2):6, 2013.
[73] Fei Wang, Sheng Ma, Liuzhong Yang, and Tao Li. Recommendation on item graphs. In Data Mining, 2006. ICDM’06. Sixth International Conference on, pages 1119–1123. IEEE, 2006.
[74] Jun Wang, Arjen P De Vries, and Marcel JT Reinders. Unifying user-based and item-based collaborative filtering approaches by similarity fusion. In Proceedings of the 29th annual international
ACM SIGIR conference on Research and development in information retrieval, pages 501–508. ACM,
2006.
[75] Grzegorz Wierzowiecki and Adam Wierzbicki. Efficient and correct trust propagation using
closelook. In Web Intelligence and Intelligent Agent Technology (WI-IAT), 2010 IEEE/WIC/ACM International Conference on, volume 1, pages 676–681. IEEE, 2010.
[76] Lei Wu, Steven CH Hoi, Rong Jin, Jianke Zhu, and Nenghai Yu. Distance metric learning from
uncertain side information with application to automated photo tagging. In Proceedings of the
17th ACM international conference on Multimedia, pages 135–144. ACM, 2009.
[77] Gui-Rong Xue, Chenxi Lin, Qiang Yang, WenSi Xi, Hua-Jun Zeng, Yong Yu, and Zheng Chen. Scalable collaborative filtering using cluster-based smoothing. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages
114–121. ACM, 2005.
32

[78] Kai Yu, Anton Schwaighofer, Volker Tresp, Xiaowei Xu, and H-P Kriegel. Probabilistic memorybased collaborative filtering. Knowledge and Data Engineering, IEEE Transactions on, 16(1):56–69,
2004.
[79] Sheng Zhang, Weihong Wang, James Ford, and Fillia Makedon. Learning from incomplete ratings
using non-negative matrix factorization. SIAM, 2006.
[80] Yi Zhang and Jonathan Koren. Efficient bayesian hierarchical user modeling for recommendation
system. In Proceedings of the 30th annual international ACM SIGIR conference on Research and
development in information retrieval, pages 47–54. ACM, 2007.
[81] Jianke Zhu, Hao Ma, Chun Chen, and Jiajun Bu. Social recommendation using low-rank semidefinite program. In Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011.
[82] Cai-Nicolas Ziegler. On propagating interpersonal trust in social networks. In Computing with
Social Trust, pages 133–168. Springer, 2009.
[83] Cai-Nicolas Ziegler and Jennifer Golbeck. Investigating interactions of trust and interest similarity.
Decision Support Systems, 43(2):460–475, 2007.
[84] Cai-Nicolas Ziegler and Georg Lausen. Propagation models for trust and distrust in social networks. Information Systems Frontiers, 7(4-5):337–358, 2005.

33

2
1.9

1.8

1.8

1.7

1.7

1.6

1.6

RMSE Error

RMSE Error

2
1.9

1.5
1.4
1.3

1.5
1.4
1.3

1.2

1.2
GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

1.1
1
0

GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

1.1
1
50

100
150
It e r a tio n N u m b e r

200

250

0

50

200

250

200

250

(b) 70% of Training Data

2

2

1.9

1.9

1.8

1.8

1.7

1.7

1.6

1.6

RMSE Error

RMSE Error

(a) 60% of Training Data

100
150
It e r a tio n N u m b e r

1.5
1.4
1.3

1.5
1.4
1.3

1.2

1.2
GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

1.1
1
0

GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

1.1
1
50

100
150
It e r a tio n N u m b e r

200

250

0

(c) 80% of Training Data

50

100
150
It e r a tio n N u m b e r

(d) 90% of Training Data

Figure 3: Comparison of accuracy of prediction in terms of RMSE with GD and SGD with three varied
batch sizes.

34

1

0.95

0.95

0.9

0.9
M A E Error

M A E Error

1

0.85

0.8

0.85

0.8

0.75

0.75

GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

0.7
0

GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

0.7
50

100
150
It e r a tio n N u m b e r

200

250

0

50

200

250

200

250

(b) 70% of Training Data

1

1

0.95

0.95

0.9

0.9
RMSE Error

RMSE Error

(a) 60% of Training Data

100
150
It e r a tio n N u m b e r

0.85

0.8

0.85

0.8

0.75

0.75

GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

0.7
0

GD
S G D ⌧1
S G D ⌧2
S G D ⌧3

0.7
50

100
150
It e r a tio n N u m b e r

200

250

(c) 80% of Training Data

0

50

100
150
It e r a tio n N u m b e r

(d) 90% of Training Data

Figure 4: Comparison of accuracy of prediction in terms of MAE with GD and SGD with three varied
batch sizes.

35

Yuhan Sun

Mohamed Sarwat

CIDSE
Arizona State University
Tempe, AZ 85287-9309
Email: Yuhan.Sun.1@asu.edu

CIDSE
Arizona State University
Tempe, AZ 85287-9309
Email: msarwat@asu.edu

I. I NTRODUCTION
Graphs are widely used to model data in many application domains, including social networking, citation network
analysis, studying biological function of genes, and brain
simulation. A graph contains a set of vertices and a set of
edges that connect these vertices. Each graph vertex or edge
may possess a set of properties (aka. attributes). Thanks to the
wide spread use of GPS-enabled devices, many applications
assign a spatial attribute to a vertex (e.g., geo-tagged social
media). Figure 1 depicts an example of a social graph that
has two types of vertices: Person and Venue and two types
of edges: Follow and Like. Vertices with type Person
have two properties (i.e., attributes): name and age. Vertices
with type Venue have two properties: name and spatial
location. A spatial location attribute represents the spatial
location of the entity (i.e., Venue) represented by such vertex.
In Figure 1, vertices {e, f, g, h, i} are spatial vertices which
represent venues.

d

w

llo

Fo

Person

c

Follow

b

a
Like

Abstract—Graphs are widely used to model data in many
application domains. Thanks to the wide spread use of GPSenabled devices, many applications assign a spatial attribute
to graph vertices (e.g., geo-tagged social media). Users may
issue a Reachability Query with Spatial Range Predicate (abbr.
RangeReach). RangeReach finds whether an input vertex
can reach any spatial vertex that lies within an input spatial
range. An example of a RangeReach query is: Given a social
graph, find whether Alice can reach any of the venues located
within the geographical area of Arizona State University. The
paper proposes G EO R EACH an approach that adds spatial data
awareness to a graph database management system (GDBMS).
G EO R EACH allows efficient execution of RangeReach queries,
yet without compromising a lot on the overall system scalability
(measured in terms of storage size and initialization/maintenance
time). To achieve that, G EO R EACH is equipped with a lightweight data structure, namely SPA-Graph, that augments the
underlying graph data with spatial indexing directories. When a
RangeReach query is issued, the system employs a prunedgraph traversal approach. Experiments based on real system
implementation inside Neo4j proves that G EO R EACH exhibits
up to two orders of magnitude better query response time and
up to four times less storage than the state-of-the-art spatial and
reachability indexing approaches.

l

Like

arXiv:1603.05355v1 [cs.DB] 17 Mar 2016

GeoReach: An Efficient Approach for Evaluating
Graph Reachability Queries with Spatial Range
Predicates

j

h

k

a: {name: Alice, age: 19}
b: {name: Dan, age: 20}
c: {name: Carol, age: 35}
d: {name: Bob, age: 25}
j: {name: Kate, age: 18}
k: {name: Mat, age: 23}
l: {name: Katharine, age:21}

g
e

i
f

P
R

Venue
e: {name: Pita Jungle}
f :{name: Chipotle}
g: {name: Sushi 101}
h: {name: Subway}
i: {name: McDonald's}

Fig. 1: Location-Aware Social Graph

Graph Database Management Systems (GDBMSs) emerged
as a prominent NoSQL approach to store, query, and analyze
graph data [15], [8], [25], [24], [28]. Using a GDBMS,
users can pose reachability analysis queries like: (i) Find out
whether two vertices in the graph are reachable, e.g., Are Alice
(vertex a) and Katharine (vertex l) reachable in the social
graph given in Figure 1. (ii) Search for graph paths that match
a given regular language expression representing predicates on
graph elements, e.g., Find all venues that Alice’s Followees
and/or her Followees’ Followees also liked. Similarly, users
may issue a Reachability Query with Spatial Range Predicate
(abbr. RangeReach). A RangeReach query takes as input a
graph vertex v and a spatial range R and returns true only if v
can reach any spatial vertex (that possesses a spatial attribute)
which lies within the extent of R (formal definition is given in
Section II). An example of a RangeReach query is: Find out
whether Alice can reach any of the Venues located within the
geographical area of Arizona State University (depicted as a
dotted red rectangle R in Figure 1). As given in Figure 1, The
answer to this query is true since Alice can reach Sushi 101
(vertex g) which is located within R. Another query example
is to find out whether Katharine can reach any of the venues
located within R. The answer to this query is false due to the

fact that the only venue reachable from Katharine, Subway
(vertex h), is not located within R.
There are several straightforward approaches to execute
a RangeReach query: (1) Traversal Approach: The naive
approach traverses the graph, checks whether each visited
vertex is a spatial vertex and returns true as the answer if
the vertex’s spatial attribute lies within the input query range
R. This approach yields no storage/maintenance overhead
since no pre-computed data structure is maintained. However,
the Traversal approach may lead to high query response
time since the algorithm may traverse the whole graph to
answer the query. (2) Transitive Closure (TC) Approach: this
approach leverages the pre-computed transitive closure [27]
of the graph to retrieve all vertices that are reachable from v
and returns true if at least one spatial vertex (located in the
spatial range R) that is reachable from v. The TC approach
achieves the lowest query response time, however it needs
to pre-compute (and maintain) the graph transitive closure
which is deemed notoriously infeasible especially for largescale graphs. (3) Spatial-Reachability Indexing (SpaReach)
Approach: uses a spatial index [3], [22] to locate all spatial
vertices VR that lie within the spatial range R and then uses
a reachability index [35] to find out whether v can reach any
vertex in VR . SpaReach achieves better query response time
than the Traversal approach but it still needs to necessarily
probe the reachability index for spatial vertices that may never
be reached from the v. Moreover, SpaReach has to store and
maintain two index structures which may preclude the system
scalability.
In this paper, we propose G EO R EACH, a scalable and
time-efficient approach that answers graph reachability queries
with spatial range predicates (RangeReach). G EO R EACH is
equipped with a light-weight data structure, namely SPAGraph, that augments the underlying graph data with spatial
indexing directories. When a RangeReach query is issued,
the system employs a pruned-graph traversal approach. As
opposed to the SpaReach approach, G EO R EACH leverages
the Spa-Graph’ s auxiliary spatial indexing information to
alternate between spatial filtering and graph traversal and early
prunes those graph paths that are guaranteed: (a) not to reach
any spatial vertex or (b) to only reach spatial vertices that
outside the input spatial range query. As opposed to the TC
and SpaReach approaches, G EO R EACH decides the amount of
spatial indexing entries (attached to the graph) that strikes a
balance between query processing efficiency on one hand and
scalability (in terms of storage overhead) on the other hand. In
summary, the main contributions of this paper are as follows:
• To the best of the authors’ knowledge, the paper is the
first that formally motivates and defines RangeReach,
a novel graph query that enriches classic graph reachability analysis queries with spatial range predicates.
RangeReach finds out whether an input graph vertex can
reach any spatial vertex that lies within an input spatial
range.
• The paper proposes G EO R EACH a generic approach that
adds spatial data awareness to an existing GDBMS.

Notation
G = {V, E}
Vvout
Vvin
RF (v)
VS
RFS (v)
n
m
v1 ❀ v2
MBR(P )

Description
A graph G with a set of vertices V and set of edges E
The set of vertices that can be reached via a direct edge
from a vertex v
The set of vertices that can reach (via a direct edge) vertex
v
The set of vertices that are reachable from (via any number
of edges) vertex v
The set of spatial vertices in G such that VS ⊆ V
The set of spatial vertices that are reachable from (via any
number of edges) vertex v
The cardinality of V (n = |V |); the number of vertices in
G
The cardinality of E (m = |E|); the number of edges in G
v2 is reachable from v1 via connected path in G (such that
both v1 and v2 ∈ V )
Minimum bounding rectangle of a set of spatial polygons
P (e.g., points, rectangles)

TABLE I: Notations.

G EO R EACH allows efficient execution of RangeReach
queries issued on a GDBMS, yet without compromising
a lot on the overall system scalability (measured in terms
of storage size and initialization/maintenance time).
1
• The paper experimentally evaluates G EO R EACH
using
real graph datasets based on a system implementation
inside Neo4j (an open source graph database system).
The experiments show that G EO R EACH exhibits up to
two orders of magnitude better query response time and
occupies up to four times less storage than the state-ofthe-art spatial and reachability indexing approaches.
The rest of the paper is organized as follows: Section II lays
out the preliminary background and related work. The SPAGraph data structure, G EO R EACH query processing, initialization and maintenance algorithms are explained in Sections III
to V. Section VI experimentally evaluates the performance of
G EO R EACH. Finally, Section VII concludes the paper.
II. P RELIMINARIES

AND

BACKGROUND

This section highlights the necessary background and related research work. Table I summarizes the main notations in
the paper.
A. Preliminaries
Graph Data. G EO R EACH deals with a directed property
graph G = (V, E) where (1) V is a set of vertices such that
each vertex has a set of properties (attributes) and (2) E is a set
of edges in which every edge can be represented as a tuple of
two vertices v1 and v2 (v1 , v2 ∈ V ). The set of spatial vertices
VS ⊆ V such that each v ∈ VS has a spatial attribute (property)
v.spatial. The spatial attribute v.spatial may be a geometrical
point, rectangle, or a polygon. For ease of presentation, we
assume that a spatial attribute of spatial vertex is represented
by a point. Figure 1 depicts an example of a directed property
graph. Spatial Vertices VS are represented by black colored
circles and are located in a two-dimensional planer space while
white colored circles represent regular vertices that do not
1 https://github.com/DataSystemsLab/GeoGraphDB–Neo4j

possess a spatial attribute. Arrows indicate directions of edges
in the graph.
Graph Reachability (v1 ❀ v2 ). Given two vertices v1 and v2
in a graph G, v1 can reach v2 (v1 ❀ v2 ) or in other words v2
is reachable from v1 if and only if there is at least one graph
path from v1 to v2 . For example, in Figure 1, vertex a can
reach vertex f through the graph path a->c->i->f so it can
be represented as a ❀ f . On the other hand, c cannot reach
h.
Reachability with Spatial Range Predicate (RangeReach).
RangeReach queries find whether a graph vertex can reach a
specific spatial region (range) R. Given a vertex v ∈ V in a
Graph G and a spatial range R, RangeReach can be described
as follows:

RangeReach(v, R) =


true













f alse

if ∃ v ′ such that
(1) v ′ ∈ VS
(2) v ′ .spatial lies within R
(3) v ❀ v ′
Otherwise.
(1)

As given in Equation 1, if any spatial vertex v ′ ∈ VS that
lies within the extent of the spatial range R is reachable from
the input vertex v, then RangeReach(v, R) returns true (i.e.,
v ❀ R). For example, in Figure 1, RangeReach(a, R) = true
since a can reach at least one spatial vertex f in R. However,
RangeReach(l, R) = false since l can merely reach a spatial
vertex h which is not located in R. Vertex d cannot reach R
since it cannot reach any vertex.
B. Related Work
This section presents previous work on reachability indexes,
spatial indexes, and straightforward solutions to processing
graph reachability queries with spatial range predicates (RangeReach).
Reachability Index. Existing solutions to processing graph
reachability queries (u ❀ v) can be divided into three
categories [35]: (1) Pruned Graph Traversal [6], [30], [34]:
These approaches pre-compute some auxiliary reachability
information offline. When a query is issued, the query processing algorithm traverses the graph using a classic traversal
algorithm, e.g., Depth First Search (DFS) or Breadth First
Search (BFS), and leverages the pre-computed reachability
information to prune the search space. (2) Transitive closure
retrieval [1], [7], [17], [18], [27], [31], [32]: this approach
pre-computes the transitive closure of a graph offline and compresses it to reduce its storage footprint. When a query u ❀ v
is posed, the transitive closure of the source vertex u is fetched
and decomposed. Then the query processing algorithm checks
whether the terminal vertex v lies in the transitive closure of u.
and (3) Two-Hop label matching [5], [8], [10], [11], [12], [26]:
The two-hop label matching approach assigns each vertex v in
the graph an out-label set Lout (v) and an in-label set Lin (v).
When a reachability query is answered, the algorithm decides
that u ❀ v if and only if Lout (v) ∩ Lin (v) 6= ∅. Since the

two label sets do not contain all in and out vertices, size of
the reachability index reduces.
Spatial Index. A spatial index [21], [23], [29] is used
for efficient retrieval of either multi-dimensional objects (e.g.,
hx,yi coordinates of an object location) or objects with spatial
extents, e.g., polygon areas represented by their minimum
boundary rectangles (MBR). Spatial index structures can be
broadly classified to hierarchical (i.e., tree-based) and nonhierarchical index structures. Hierarchical tree-based spatial
index structures can be classified into another two broad
categories: (a) the class of data-partitioning trees, also known
as the class of Grow-and-Post trees [20], which refers to the
class of hierarchical data structures that basically extend the
B-tree index structure [2], [13] to support multi-dimensional
and spatial objects. The main idea is to recursively partition
the spatial data based on a spatial proximity clustering, which
means that the spatial clusters may overlap. Examples of
spatial index structures in this category include R-tree [16] and
R*-tree [3]. (b) the class of space-partitioning trees that refers
to the class of hierarchical data structures that recursively
decomposes the space into disjoint partitions. Examples of
spatial index structures in this category include the Quadtree [14] and k-d tree [4].
Spatial Data in Graphs. Some existing graph database
systems, e.g., Neo4j, allow users to define spatial properties on
graph elements. However, these systems do not provide native
support for RangeReach queries. Hence, users need to create
both a spatial index and a reachability index to efficiently
answer a RangeReach queries (drawbacks of this approach
are given in the following section). On the other hand, existing
research work [19] extends the RDF data with spatial data to
support RDF queries with spatial predicates (including range
and spatial join). However, such technique is limited to RDF
and not general graph databases. It also does not provide an
efficient solution to handle reachability queries.
C. Straightforward Solutions
There are three main straightforward approaches to process
a RangeReach query, described as follows:
Approach I: Graph Traversal. This approach executes
a spatial reachability query using a classical graph traversal
algorithm like DFS (Depth First Search) or BFS (Breadth
First Search). When RangeReach(v, R) is invoked, the system
traverses the graph from the starting vertex v. For each visited
vertex, the algorithm checks whether it is a spatial vertex and
returns true as the query answer if the vertex’s location lies
within the input query range R because the requirement of
spatial reachability is satisfied and hence v ❀ R. Otherwise,
the algorithm keeps traversing the graph. If all vertices that v
can reach do not lie in R, that means v cannot reach R.
Approach II: Transitive Closure (TC). This approach precomputes the transitive closure of the graph and stores it as an
adjacency matrix in the database. Transitive closure of a graph
stores the connectivity component of the graph which can be
used to answer reachability query in constant time. Since the
final result will be determined by spatial vertices, only spatial

vertices are stored. When RangeReach(v, R) is invoked, the
system retrieves all spatial vertices that are reachable from v
by means of the transitive closure. The system then returns
true if at least one spatial vertex that is reachable from v is
also located in the spatial range R.
Approach III: SpaReach. This approach constructs two
indexes a-priori: (1) A Spatial Index: that indexes all spatial
vertices in the graph and (2) A Reachability Index: that indexes
the reachability information of all vertices in the graph. When
a RangeReach query is issued, the system first takes advantage
of the spatial index to locate all spatial vertices VR that
lie within the spatial range R. For each vertex v ′ ∈ VR ,
a reachability query against the reachability index is issued
to test whether v can reach v ′ . For example, to answer
RangeReach(a, R) in Figure 2, spatial index is exploited first
to retrieve all spatial vertices that are located in R. From the
range query result, it can be known that g, i and f are located
in rectangle R. Then graph reachability index is accessed to
determine whether a can reach any located-in vertex. Hence, it
is obvious RangeReach(a, R) = true by using this approach.
Critique. The Graph Traversal approach yields no storage/maintenance overhead since no pre-computed data structure is maintained. However, the traversal approach may lead
to high query response time (O(m) where m is the number
of edges in the graph) since the algorithm may traverse the
whole graph to answer the query. The TC approach needs
to pre-compute (and maintain) the graph transitive closure
which is deemed notoriously infeasible especially for largescale graphs. The transitive closure computation is O(kn3 ) or
O(nm) and the TC storage overhead is O(kn2 ) where n is
total number of vertices and k is the ratio of spatial vertices
to the total number of vertices in the graph. To answer a
RangeReach query, the TC approach takes O(kn) time since it
checks whether each reachable spatial vertex in the transitive
closure is located within the query rectangle. On the other
hand, SpaReach builds a reachability index, which is a timeconsuming step, in O(n3 ) [32] time. The storage overhead
of a spatial index is O(n) and that of a reachability index
is O(nm1/2 ). To store the two indices, the overall storage
overhead is O(nm1/2 ). Storage cost of this approach is far less
than TC approach but still not small enough to accommodate
large-scale graphs. The query time complexity of a spatial
index is O(kn) while that of reachability index is m1/2 . But
for a graph reachability query, checking is demanded for each
spatial vertex in the result set generated by the range query.
Hence, cost of second step reachability query is O(knm1/2 ).
The total cost should be O(knm1/2 ). Query performance
of Spa-Reach is highly impacted by the size of the query
rectangle since the query rectangle determines how many
spatial vertices are located in the region. In Figure 1, query
rectangle R overlaps with three spatial vertices. For example,
to answer RangeReach(l, R), all three vertices {f, g, i} will
be checked against the reachability index to decide whether
any of them is reachable from l and in fact neither of them
is reachable. In a large graph, a query rectangle will possibly
contain a large number of vertices. That will definitely lead to

d
c
b

a
l

1

2
h

5

4

3

RMBR(j)

6

8

7 g

L0

e

13

Q

10

9
14

f

12

11

15

i

16

17
19

B-vertex:
a: true
d: false
f: false
h: false
k: false

k

j

18

L1

20

21

R-vertex:
j: RMBR(j)
G-vertex:
b: {2, 19}
c: {12, 14}
e: {14}
g: {12, 14}
i: {14}
l: {2}

L2

Fig. 2: SPA-Graph Overview

high unreasonable high query response time.
III. O UR A PPROACH : G EO R EACH
In this section, we give an overview of G EO R EACH an efficient and scalable approach for executing graph reachability
queries with spatial range predicates.
A. Data Structure
In this section, we explain how G EO R EACH augments a
graph structure with spatial indexing entries to form what we
call SPatially-Augmented Graph (SPA-Graph). To be generic,
G EO R EACHstores the newly added spatial indexing entries
the same way other properties are stored in a graph database
system. The structure of a SPA-Graph is similar to that of
the original graph except that each vertex v ∈ V in a SPAGraph G = {V, E} stores spatial reachability information. A
SPA-Graph has three different types of vertices, described as
follows:
•

•

B-Vertex: a B-Vertex v (v ∈ V ) stores an extra bit (i.e.,
boolean), called Spatial Reachability Bit (abbr. GeoB)
that determines whether v can reach any spatial vertex
(u ∈ VS ) in the graph. GeoB of a vertex v is set to 1
(i.e., true) in case v can reach at least one spatial vertex
in the graph and reset to 0 (i.e., false) otherwise.
R-Vertex: an R-Vertex v (v ∈ V ) stores an additional attribute, namely Reachability Minimum Bounding
Rectangle (abbr. RMBR(v)). RMBR(v) represents the
minimum bounding rectangle MBR(S) (represented by
a top-left and a lower-right corner point) that encloses all
spatial polygons which represent all spatial vertices S that
are reachable from vertex v (RMBR(v) = MBR(RFS (v)),
RFS (v) = {u|v ❀ u, u ∈ VS }).

G-Vertex: a G-Vertex v stores a list of spatial grid cells,
called the reachability grid list (abbr. ReachGrid(v)). Each
grid cell C in ReachGrid(v) belongs to a hierarchical grid
data structure that splits the total physical space into n
spatial grid cells. Each spatial vertex u ∈ VS will be
assigned a unique cell ID (k ∈ [1, n]) in case u is located
within the extents of cell k, noted as Grid(u) = k. Each
cell C ∈ ReachGrid(v) contains at least one spatial vertex
that is reachable from v (ReachGrid(v) = ∪ Grid(u),
{u|v ❀ u, u ∈ VS }).
Lemma 3.1: Let v (v ∈ V ) be a vertex in a SPA-Graph
G = {V, E} and Vvout be the set of vertices that can be
reached via a direct edge from a vertex v. The reachability
minimum bounding rectangle of v (RMBR(v)) is equivalent
to the minimum bounding rectangle that encloses all its outedge neighbors Vvout and their reachability minimum bounding
rectangles. RMBR(v) = MBRv′ ∈Vvout (RMBR(v ′ ), v ′ .spatial).
Proof: Based on the reachability definition, the set of
reachable vertices RF (v) from a vertex v is equal to the union
of the set of vertices that is reached from v via a direct edge
′
(Vvout ) and all vertices that are reached from each vertex v ∈
Vvout . Hence, the set (RFS (v)) of reachable spatial vertices
from v is given in Equation 2.
•

RFS (v) =
′

[

′

(v ′ ∪ RFS (v ))

(2)

v ∈Vvout

And since RMBR(v) = MBR(RFS (v)), then the the reachability minimum bounding rectangle of v is as follows:
RM BR(v) = M BR(
′

[

′

(v ′ ∪ RFS (v ))))
(3)

v ∈Vvout
′

′

= M BRv′ ∈Vvout (RM BR(v ), v .spatial)

That concludes the proof.
Lemma 3.2: The set of reachable spatial grid cells from a
given vertex v is equal to the union of all spatial grid cells
reached from its all its out-edge neighbors and grid cells that
contain the spatial neighbors
ReachGrid(v) =

[

(ReachGrid(v ′ ) ∪ Grid(v ′ ))

(4)

v ′ ∈Vvout

Proof: Similar to that of Lemma III-A.
Example. Figure 2 gives an example of a SPA-Graph. GeoB
of vertex b is set to 1 (true) since b can reach three spatial
vertices e, f and h. GeoB for d is 0 since d cannot reach any
spatial vertex in the graph. Figure 2 also gives an example of a
Reachability Minimum Bounding Rectangle RMBR of vertex
j (i.e., RMBR(j)). All reachable spatial vertices from j are g,
i, h and f . Figure 2 also depicts an example of ReachGrid.
There are three layers of grids, denoted as L0 , L1 , L2 from
top to bottom. The uppermost layer L0 is split into 4 × 4
grid cells; each cell is assigned a unique id from 1 to 16.
We denote grid cell with id 1 as G1 for brevity. The middle
layer gird L1 is split into four cells G17 to G20 . Each cell
in L1 covers four times larger space than each cell in L0 .
G17 in L1 covers exactly the same area of G1 , G2 , G5 , G6

Algorithm 1 Reachability Query with Spatial Range Predicate
1: Function R ANGER EACH(v, R)
2: if v is a spatial vertex and v.spatial Lie In R then return true
3: Terminate ← true
4: if v is a B-vertex then
5:
if GeoB(v) = true then Terminate ← false
6: else if v is a R-vertex then
7:
if R full contains RMBR(v) then return true
8:
if R no overlap with RMBR(v) then return false
9:
Terminate ← false
10: else if v is a G-vertex then
11:
for each grid Gi ∈ ReachGrid(v) do
12:
if R fully contains Gi then return true
13:
Gi partially overlaps with R then Terminate ← false
14: if Terminate = false then
15:
for each vertex v ′ ∈ Vvout do
16:
if R ANGER EACH(v ′ , R) = true then return true
17: return false

in L0 . The bottom layer L2 contains only a single grid cell
which covers all four grids in L1 and represents the whole
physical space. All spatial vertices reachable from vertex a
are located in G2 , G7 , G9 , G12 and G14 , respectively. Hence,
ReachGrid(a) can be {2, 7, 9, 12, 14}. Notice that vertex e
and f are both located in G9 and G14 covered by G19 in
ReachGrid(a) can be replaced by G19 . Then, ReachGrid(a)
= {2, 7, 12, 19}. In fact, there exist more options to represent
ReachGrid(a), such as {17, 18, 19, 20} or {21} by merging
into only a single grid cell in L2 . When we look into
ReachGrid of connected vertices, for instance g, ReachGrid(g)
is {12, 14} and ReachGrid(i) is {14}. It is easy to verify that
ReachGrid(g) is ReachGrid(i)∪Grid(i.spatial), which accords
with lemma 3.2.
SPA-Graph Intuition. The main idea behind the SPAGraph is to leverage the spatial reachability bit, reachability
minimum bounding rectangle and reachability grid list stored
in a B-Vertex, R-Vertex or a G-Vertex to prune graph paths
that are guaranteed (or not) to satisfy both the spatial range
predicate and the reachability condition. That way, G EO R E ACH cuts down the number of traversed graph vertices and
edges and hence significantly reduce the overall latency of a
RangeReach query.
B. Query Processing
This section explains the RangeReach query processing
algorithm. The main objective is to visit as less graph vertices
and edges as possible to reduce the overall query latency.
The query processing algorithm accelerates the SPA-Graph
traversal procedure by pruning those graph paths that are
guaranteed (or not) to satisfy the spatial reachability constraint.
Algorithm 1 gives pseudocode for query processing. The
algorithm takes as input a graph vertex v and query rectangle
R. It then starts traversing the graph starting from v. For
each visited vertex v, three cases might happen, explained as
follows:
Case I (B-vertex): In case GeoB is false, a B-vertex
cannot reach any spatial vertex and hence the algorithm stops
traversing all graph paths after this vertex. Otherwise, further
traversal from current B-vertex is required when GeoB value
is true. Line 4 to 5 in algorithm 1 is for processing such case.

RMBR

A

R

I

A
I

RMBR

Query Rectangle

RMBR
R

Query Rectangle

(a) No Overlap

(x1,y1)

(x1,y1)

B

B
e

RMBR

(b) Lie In

(x2,y2)

(x2,y2)

e
f

RMBR

Q

(a)

Query
Rectangle

(b)

A
I

RMBR

f

Q

Query Rectangle

(c) Partially Covered By

R
(x1,y1)

B

RMBR

e
(x2,y2)

Fig. 3: Relationships between RMBR and a query rectangle
f

Q

Case II (R-vertex): For a visited R-vertex u, there are three
conditions that may happen (see figure 3). They are the case
from line 6 to 9 in algorithm 1:
• Case II.A: RMBR(u) lies within the query rectangle (see
Figure 3b). In such case, the algorithm terminates and
returns true as the answer to the query since there must
exist at least a spatial vertex that is reachable from v.
• Case II.B: The spatial query region R does not overlap
with RMBR(u) (see Figure 3a). Since all reachable spatial
vertices of u must lie inside RMBR(u), there is no
reachable vertex can be located in the query rectangle.
As a result, graph paths originating at u can be pruned.
• Case III.C: RMBR(u) is partially covered by the query
rectangle (see Figure 3c). In this case, the algorithm keeps
traversing the graph by fetching the set of vertices Vvout
that can be reached via a direct edge from v.
Case III (G-vertex): For a G-vertex u, it store many
reachable grids from u. Actually, it can be regarded as many
smaller RMBRs. So three cases may also happen. Algorithm 1
line 13 to 18 is for such case. Three cases will happen are
explained as follows:
• Case III.A: The query rectangle R fully contains any
grid cell in ReachGrid(u). In such case, the algorithms
terminates and returns true as the query answer.
• Case III.B: The query rectangle have no overlap with all
grids in ReachGrid(u). This case means that v cannot
reach any grids overlapped with R. Then we never
traverse from v and this search branch is pruned.
• Case III.C: If the query rectangle fully contains none
of the reachable grid and partially overlap with any
reachable grid, it corresponds to Partially Covered By
case for RMBR. So further traversal is performed.
Figure 2 gives an example of RangeReach that finds
whether vertex a can reach query rectangle Q (the shaded one
in figure 2). At the beginning of the traversal, the algorithm
checks the category of a. In case, It is a B-vertex and its GeoB
value is true, the algorithm recursively traverses out-edge
neighbors of a and perform recursive checking. Therefore,
the algorithm retrieves vertices b, c, d and j. For vertex b,

(c)

Fig. 4: R-vertex Pruning Power
it is a G-vertex and its reachable grids are G2 and G19 . G19
cover the range of four grids in L0 . They are G9 , G10 , G13
and G14 . The spatial range is merely partially covered by
Q (Case III.C), hence it is possible for b to reach Q. We
cannot make an assured decision in this step so b is recorded
for future traversal. Another neighbor is c. ReachGrid(c) is
{12, 14} which means that G12 and G14 are reachable from
c. G14 lies in Q (Case III.A). In such case, since a ❀ c, we
can conclude that a ❀ R. The algorithm then halts the graph
traversal at this step and returns true as the query answer.
IV. SPA-G RAPH A NALYSIS
This section analyzes each SPA-Graph vertex type rom two
perspectives: (1) Storage Overhead: the amount of storage
overhead that each vertex type adds to the system (2) Pruning
Power: the probability that the query processing algorithm
terminates when a vertex of such type is visited during the
graph traversal.
B-vertex. When visiting a B-Vertex, in case GeoB is false,
the query processing algorithm prunes all subsequent graph
paths originated at such vertex. That is due to the fact that such
vertex cannot reach any spatial vertex in the graph. Otherwise,
the query processing algorithm continues traversing the graph.
As a result, pruned power of a B-vertex lies in the condition
that GeoB is false. For a given graph, number of vertices that
can reach any space is a certain value. So probability that a
vertex can reach any spatial vertex is denoted as Ptrue . This is
also the probability of a B-vertex whose GeoB value is true.
Probability of a B-vertex whose GeoB value is false, denoted
as Pf alse , will be 1 − Ptrue . To sum up, pruned power of a
B-vertex is 1 − Ptrue or Pf alse
R-vertex. When an R-vertex is visited, the condition
whether the vertex can reach any space still exists. If the
R-vertex cannot reach any space, we assign the R-vertex a
specific value to represent it(e.g. set coordinates of RMBR’s

bottom-left point bigger than that of the top-right point). In
this case, pruned power of a R-vertex will be the same with
a B-vertex, which is Pf alse . Otherwise, when the R-vertex
can reach some space, it will be more complex. Because
information of RMBR and query rectangle R have some
impact on the pruned power of this R-vertex. The algorithm
stops traversing the graph in both the No Overlap and Lie
In cases depicted in Figures 3a and 3b. Figure 4 shows the
two cases that R-vertex will stop the traversal. In Figure 4,
width and height of the total 2D space are denoted as A and
B. Assume that the query rectangle can be located anywhere
in the space with equal probability. We use (x1 , y1 ) and
(x2 , y2 ) to represent the RMBR’s top-left corner and lowerright point coordinates, respectively. Then all possible areas
where top-left vertex of query rectangle Q should be part
of the total space, denoted as I (see the shadowed area in
the figure. Its area is determined by size of query rectangle.
Denote width and height of Q are e and f , then area of I,
AI = (A − e) × (B − f ).
First, we estimate probability of No Overlap case. Figure 4a
shows one case of No Overlap. If the query rectangle Q do
not overlap with RMBR, top-left vertex of Q must lie outside
rectangle R which is forms the overlap region (drawn with
solid line in Figure 4b). Area of R (denoted as AR ) is obviously determined by the RMBR location and size of Q. It can
be easily observed that AR = (x2 −(x1 −e))×(y2 −(y1 −f )).
Another possible case is demonstrated in Figure 4b. In such
case, if we calculate R in the same way, range of R will
exceeds area of I which contains all possible locations. As a
result, AR = AI in this case. As we can see, area of overlap
region is determined by the range of R and I altogether.
Then we can have a general representation of the overlap area
AOverlap = (min(A − e, x2 ) − max(0, x1 − e)) × (min(B −
f, y2 )−max(0, x2 −f ). The No Overlap area is AI −AOverlap
and the probability of having a No Overlap case is calculated
as follows:
PN oOverlap =

AOverlap
AI − AOverlap
=1−
.
AI
AI

(5)

Figure 4c depicts the Lie In case. When top-left vertex of
Q lies in region R, then such Lie In case will happen. To
ensure that R exists, it is necessary that e > (x2 − x1 ) and
f > (y2 −y1 ). If it is not, then probability of such case must be
0. If this requirement is satisfied, then AR = (x1 − (x2 − e))×
(y1 − (y2 − f )). Recall what is met in the above-mentioned
case, R may exceed the area of I. Similarly, more general
area should be AR = (min(A − e, x1 ) − max(0, x1 − (x2 −
e))) × (min(B − f, y1 ) − max(0, y1 − (y2 − f ))). Probability
R
of such case should be A
AI . To sum up, we have
PLieIn =

(

AR
AI

0

e > (x2 − x1 ) and f > (y2 − y1 )
else

(6)

After we sum up all conditional probabilities based on
Ptrue and Pf alse , pruning power of an R-vertex is equal to

Algorithm 2 G EO R EACH Initialization Algorithm
1:
2:
3:
4:
5:
6:
7:
8:
9:

Function INITIALIZE(Graph G = {V, E})
/*PHASE I: SPA-Graph Vertex Initialization */
for each Vertex v ∈ V according their sequence in topology do
InitializeVertex(G, v, MAX_REACH_GRIDS, MAX_RMBR)
/* PHASE II: Reachable Grid Cells Merging */
for each G-vertex v do
for each layer Li from L1 to Lbottom do
for each grid cell Gi in Li do
if Number of reachable grids in corresponding region in Li−1 is larger
than MERGE_COUNT then
10:
Add Gi in Li into ReachGrid(v)
11:
Remove reachable grid cells that are covered by Gi in higher layers

(PN oOverlap +PLieIn )×Ptrue +Pf alse . Evidently, the pruning
power of an R-vertex is more powerful than a B-vertex. When
the storage overhead of an R-vertex is considered, coordinates
of RMBR’s top-left and lower-right vertices should be stored.
Thus its storage will be at least four bytes depending on the
spatial data precision. That means the storage overhead of a
G-Vertex is always higher than that of a B-Vertex.
G-vertex. For a high resolution grid, it is of no doubt that a
G-vertex possesses a high pruning power. However, this comes
at the cost of higher storage overhead because more grid cells
occupies more space. When a G-vertex is compared with an
R-vertex, the area of an R-vertex is much larger than a grid. In
this case, an R-vertex can be seen as a a simplified G-vertex
for which the grid cell size is equal to that of RMBR. One
extreme case of R-vertex is that the vertex can reach only one
spatial vertex. In such case, RMBR is location of the reachable
spatial vertex. Such R-vertex can still be counted as a G-vertex
whose grid size x → 0. According the rule, it should be with
higher storage overhead and more accuracy. Actually, storing it
as a G-vertex will cost an integer while any R-vertex requires
storage for four float or even double number.
V. I NITIALIZATION & M AINTENANCE
This section describes the SPA-Graph initialization algorithm. The G EO R EACH initialization algorithm (Pseudocode
is given in Algorithm 2) takes as input a graph Graph
G = {V, E} and runs in two main phases: (1) Phase I:
SPA-Graph Vertex Type Initialization: this phase leverages the
tradeoff between query response time and storage overhead
explained in Section IV to determine the type of each vertex.
(2) Phase II: Reachable Grid Cells Merging: This step further
reduces the storage overhead of each G-Vertex in the SPAGraph by merging a set of grid cells into a single grid cell.
Details of each phase are described in Section V-A and V-B
A. SPA-Graph Vertex Type Initialization
To determine the type of each vertex, the initialization
algorithm takes into account the following system parameters:
• MAX_RMBR: This parameter represents a threshold that
limits space area of each RMBR. If a vertex v is
an R-vertex, area of RMBR(v) cannot be larger than
MAX_RMBR. Otherwise, v will be degraded to a B-vertex.
• MAX_REACH_GRIDS: This parameter sets up the maximum number of grid cells in each ReachGrid. If a vertex
v is a G-vertex, number of grid cells in ReachGrid(v)

Algorithm 3 SPA-Graph Vertex Initialization Algorithm

Algorithm 4 Maintain R-vertex

1: Function INITIALIZE V ERTEX(Graph G = {V, E}, Vertex v)
2: Type ← InitializeType(v)
3: switch (Type)
4: case B-vertex:
5:
Set v B-vertex and GeoB(v) = true
6: case G-vertex:
7:
ReachGrid(v) ← ∅
8:
for each Vertex v ′ ∈ Vvout do
9:
Maintain-GVertex(v, v ′ )
10:
if Number of grids in ReachGrid(v) ¿ MAX_REACH_GRIDS then
11:
Set v R-vertex and break
12:
Type ← R-vertex
13:
if Number of grids in ReachGrid(v) = 0 then
14:
Set v B-vertex, GeoB(v) ← false and break
15: case R-vertex:
16:
RMBR(v) ← ∅
17:
for each Vertex v ′ ∈ Vvout do
18:
Maintain-RVertex(v, v ′ )
19:
if Area(RMBR(v)) ¿ MAX_RMBR then
20:
Set v B-vertex, GeoB(v) ← true and break
21: end switch

1: Function M AINTAIN -RV ERTEX(From-side vertex v, To-side vertex v′ )
2: switch (Type of v′ )
3: case B-vertex:
4:
if GeoB(v ′ ) = true then
5:
Set v ′ B-vertex and GeoB(v) ← true
6:
else if RMBR(v) fully contains MBR(v ′ .spatial) then
7:
return false
8:
else
9:
RMBR(v) ← MBR(RMBR(v), v ′ .spatial)
10: case R-vertex:
11:
if RMBR(v) fully contains MBR(RMBR(v ′ ), v ′ .spatial) then
12:
return false
13:
else
14:
RMBR(v) ← MBR(RMBR(v), RMBR(v ′ ), v ′ .spatial)
15: case G-vertex:
16:
if RMBR(v) fully contains MBR(RMBR(v ′ ), v ′ .spatial) then
17:
return false
18:
else
19:
RMBR(v) ← MBR(RMBR(v), RMBR(v ′ ), v ′ .spatial)
20: end switch
21: return true

cannot exceed MAX_REACH_GRIDS. Otherwise, v will
be degraded to an R-vertex.
Algorithm 3 gives the pseudocode of the vertex initialization
algorithm. Vertices are processed based on their topological
sequence in the graph. For each vertex, the algorithm first
determines the initial vertex type using the InitializeType
function (pseudocode omitted for brevity). For a vertex v,
categories of vertex v ′ ({v ′ | v ′ ∈ Vvout }) will be checked.
If there is any B-vertex v ′ with GeoB(v ′ ) = true, v is directly
initialized to a B-vertex with GeoB(v) = true. Otherwise, if
there is any R-vertex, the function will return an R-vertex type,
which means that v is initialized to R-vertex. If either of the
above happens, the function returns G-vertex type. Based on
the initial vertex type, the algorithm may encounter one of the
following three cases:
Case I (B-vertex): The algorithm directly sets v as a Bvertex and GeoB(v) = true because there must exist one outedge neighbor v ′ of v such that GeoB(v ′ ) = true.
Case III (R-vertex): For each v ′ (v ′ ∈ Vvout ), the algorithm calls the Maintain-RVertex algorithm. Algorithm 4
shows the pseudocode of the Maintain-RVertex algorithm.
Maintain-RVertex aggregates RMBR information. After each
aggregation step, area of RMBR(v) will be compared with
MAX_RMBR: In case the area of RMBR(v) is larger than
MAX_RMBR, the algorithm sets v to be a B-vertex with a true
GeoBvalue and terminates. When v ′ is either a G-vertex or
an R-vertex, the algorithm uses the new bounding rectangle
returned from MBR(RMBR(v), RMBR(v ′ ), v ′ .spatial) to
update the current RMBR(v). The algorithm calculates the
RMBR of a G-vertex in case III. In case v ′ is a B-vertex,
GeoB(v ′ ) must be reset to false. The algorithm then updates
RMBR(v) to MBR(RMBR(v), v.spatial).
Case II (G-vertex): For each vertex v ′ (v ′ ∈ Vvout ),
Maintain-GVertex (pseudocode omitted for the sake of space)
is invoked to calculate the ReachGrid of v ′ . In case v ′
is a B-vertex with GeoB(v ′ ) = false and v ′ is a spatial
vertex, the grid cell that contains the location of v ′ will be
added into ReachGrid(v). If v ′ is a G-vertex, all grid cells

in ReachGrid(v ′ ) and Grid(v ′ .spatial) will be added into
ReachGrid(v). It does not matter whether v ′ is a spatial vertex
or not. If v ′ is not a spatial vertex, Grid(v ′ .spatial) is ∅.
After accumulating information from each neighbor v ′ , the
algorithm changes the type of v to R-vertex immediately in
case the number of reachable grid cells in ReachGrid(v) is
larger than MAX_REACH_GRIDS. Therefore, the algorithm
sets the Type to R-vertex since RMBR(v) should be calculated
for possible future usage, e.g. RMBR of in-edge neighbors of
v(it will be shown in R-vertex case).
Example. Figure 2 depicts a SPA-Graph with MAX_RMBR
= 0.8A and MAX_REACH_GRIDS = 4, where A is area of the
whole space. Each vertex is attached with some information
and affiliated to one category of G EO R EACH index. Their
affiliations are listed in the figure. It is obvious that those
vertices which cannot reach any spatial vertices will be stored
as B-vertex and have a false boolean GeoB value to represent
such condition. Vertices d, f , h, i, j and k are assigned a
false value. Other vertices are G-vertex initially. ReachGrid(a)
= {2, 7, 9, 12, 14}, ReachGrid(b) = {2, 9, 14}, ReachGrid(c)
= {12, 14}, ReachGrid(e) = {14}, ReachGrid(g) = {12, 14},
ReachGrid(i) = {14}, ReachGrid(j) = {2, 7, 12, 14} and
ReachGrid(l) = {2}. Because of MAX_REACH_GRIDS, some
of them will be degraded to an R-vertex. Number of reachable grids in ReachGrid(a) and ReachGrid(j) are 4 and
5, respectively. Both of them are larger than or equal to
MERGE_COUNT. They will be degraded to R-vertex first. Then
area of their RMBR are compared with MAX_RMBR. Area
of RMBR(a) is apparently over 80% of the total space area.
According to MAX_RMBR, a is stored as a B-vertex with a true
value while j is stored as an R-vertex with an RMBR.
B. Reachable Grid Cells Merging
After the type of each vertex is decided, the initialization
algorithm performs the reachable grid cells merging phase
(lines 5 to 11 in Algorithm 2). In this phase, the algorithm
merges adjacent grid cells to reduce the overall storage overhead of each G-Vertex. To achieve that, the algorithm assumes
a system parameter, namely MERGE_COUNT. This parameter

Algorithm 5 Maintain B-vertex
1: Function M AINTAIN -BV ERTEX(From-side vertex v, To-side vertex v′ )
2: if GeoB(v) = true then
3:
return false
4: else
5:
switch (Type of v ′ )
6:
case B-vertex:
7:
if GeoB(v ′ ) = true then
8:
GeoB(v) ← true
9:
else if v ′ .spatial 6= NULL then
10:
ReachGrid(v) ← Grid(v ′ .spatial)
11:
else
12:
return false
13:
case R-vertex:
14:
RMBR(v) ← MBR(RMBR(v ′ ), v ′ .spatial)
15:
case G-vertex:
16:
ReachGrid(v) ← ReachGrid(v ′ )∪Grid(v ′ .spatial)
17:
end switch
18: return true

determines how G EO R EACH merges spatially adjacent grid
cells according to MERGE_COUNT. In each spatial region with
four grid cells, the number of reachable grid cells should not
be less than MERGE_COUNT. Otherwise, we merge the four
grid cells into a single grid cell in the lower layer.
For each G-vertex v, all grid cells in grid cell layers L1 to
Lbottom are checked. When a grid cell Gi in Li is processed,
four grid cells in Li−1 that cover the same space with Gi will
be accessed. If number of reachable grid cells is larger than or
equal to MERGE_COUNT, Gi should be added in ReachGrid(v)
first. Then all grid cells covered by Gi in layers from L0 to
Li−1 should be removed. In order to achieve that, a recursive
approach is implemented as follows. For each grid cell in Li−1
that is reachable from v, the algorithm directly remove it from
ReachGrid(v). The removal stops at this grid in this layer. No
recursive checking is required on grid cells in higher layers for
which the space is covered by the reachable grid cell. Since
all those reachable grid cells have been removed already. For
those grid cells that are not reachable from v, the algorithm
cannot assure that they do not cover some reachable grids in
a higher layer. Hence, the recursive removal is invoked until
the algorithm reaches the highest layer or other reachable grid
cells are visited.
The SPA-Graph in Figure 2 has a MERGE_COUNT set to 2.
There is no merging in e, i and l because their ReachGrids
contain only one grid. The rest are b, c and g. In ReachGrid(b),
for each grid in L1 , we make the MERGE_COUNT checking.
G17 covers four grids G1 , G2 , G5 and G6 in L0 . In such
four-grids region, only G2 is reachable from b. The merging
will not happen in G17 . It is the same case in G18 and G20 .
However, there are two grids, G9 and G14 covered by G19
in L1 . As a result, the two grids in L0 will be removed from
ReachGrid(b) with G19 being added instead. For the grid G21
in L2 , the same checking in L1 will be performed. Since, only
G19 is reachable, no merging happens. Finally, ReachGrid(b)
= {2, 19}. Similarly, we can have ReachGrid(c) = {12, 14}
and ReachGrid(g) = {12, 14} where no merging occurs.
C. SPA-Graph Maintenance
When the structure of a graph is updated, i.e., adding or
deleting edges and/or vertices, G EO R EACHneeds to maintain

the SPA-Graph structure accordingly. Moreover, when the
spatial attribute of a vertex changes, G EO R EACHmay need to
maintain the RMBRand/or ReachGridproperties of that vertex
and other connected vertices as well. As a matter of fact, all
graph updates can be simulated as a combination of adding
and/or deleting a set of edges.
Adding an edge. When an edge is added to the graph,
the directly-influenced vertices are those that are connected
to another vertex by the newly added edge. The spatial
reachability information of the to-side vertex will not be
influenced by the new edge. Based upon Lemmas III-A
and 3.2, the spatial reachability information, i.e., RMBRor
ReachGrid, of the to-side vertex should be modified based
on the the from-side vertex. On the other hand, the fromside vertex may remain the same or change. In the former
case, there is no recursive updates required for the in-edge
neighbors of the from-side vertex. Otherwise, the recursive
updates are performed in the reverse direction until no change
occurs or there is no more in-edge neighbor. A queue Q will
be exploited to track the updated vertices. When Q is not
empty, which means there are still some in-edge neighbors
waiting for updates, the algorithm retrieves the next vertex
in the queue. For such vertex, all its in-edge neighbors are
updated by using the reachability information stored on this
vertex. Updated neighbors will then be pushed into the queue.
The algorithm halts when the queue is empty. Depending on
category of the from-side vertex, corresponding maintenance
functions, including Maintain-BVertex, Maintain-RVertex and
Maintain-GVertex are used to update the newly added spatial
reachability information.
Algorithm 5 is used when the from-side vertex is a B-vertex.
In algorithm 5, if the from-side vertex v is already a B-vertex
with GeoB(v) = true. The added edge will never cause any
change on v. Hence a false value is returned. In case GeoB(v)
= false, the algorithm considers type of the to-side vertex v ′ .
•

•

•

B-vertex. If GeoB(v ′ ) = true, it is no doubt that GeoB(v)
will be set to true and a true value will be returned.
Otherwise, the algorithm checks whether v ′ is spatial.
If it is, ReachGrid(v) is updated with Grid(v ′ spatial).
Otherwise, the algorithm returns false because v is not
changed.
R-vertex. In such case, it is certain that v will be updated
to an R-vertex. The algorithm merely updates RMBR(v)
with MBR(RMBR(v ′ ), v ′ .spatial).
G-vertex. It is similar to the R-vertex case. Type of v ′ can
decide that v should be a G-vertex and the algorithm updates ReachGrid(v) with ReachGrid(v ′)∪Grid(v ′ .spatial)

Maintain-BVertex and Maintain-RVertex are what we use
in the initialization. However, there is a new condition that
should be taken into consideration. When the from-side vertex
v is an R-vertex and the to-side vertex v ′ is a G-vertex, the
algorithm needs to update the RMBR(v) with ReachGrid(v ′ ).
Under such circumstance, first a dummy RMBR(v ′ ) will be
constructed using ReachGrid(v ′ ). Although it is not the exact
RMBR of v ′ , it is still precise. Error of the width and height

will not be greater than size of a grid cell. No matter what
function is invoked to update the from-side vertex, G EO R E ACH takes into account the system parameters MAX_RMBR and
MAX_REACH_GRIDS are checked on RMBR and ReachGrid,
respectively.
Deleting an edge. When an edge is removed, the to-side
vertex will be not impacted by the deleting which is the same
with adding an edge. To maintain the correctness of spatial
reachability information stored on the from-side vertex, the
only way is to reinitialize its spatial reachability information
according to all its current out-edge neighbors. If its structure
is different from the original state due to the deleting, the
structure of all its in-edge neighbors will be rebuilt recursively.
A queue Q is used to keep track of the changed vertices. The
way G EO R EACHmaintains the queue and the operations on
each vertex in the queue are similar to the AddEdge procedure.
Maintenance cost of deleting an edge will be O(kn3 ) because
the whole G EO R EACH index may be reinitialized.
VI. E XPERIMENTAL E VALUATION
In this section, we present a comprehensive experimental evaluation of G EO R EACH performance. We compare the
following approaches: GeoMT0, GeoMT2, GeoMT3, GeoP,
GeoRMBR and SpaReach. GeoMT0, GeoMT2 and GeoMT3 are approaches that store only ReachGrid by setting MAX_REACH_GRIDS to the total number of grids in
the space and MAX_RMBR to A where A represent the
area of the whole 2D space. Their difference lies in the
value of MERGE_COUNT. GeoMT0 is an approach where
MERGE_COUNT is 0. In such approach, no higher layer
grids are merged. MERGE_COUNT is set to 2 and 3 respectively in GeoMT2 and GeoMT3. GeoP is an approach in
which MERGE_COUNT = 0, MAX_REACH_GRIDS = 200 and
MAX_RMBR = A. In such approach, reachable grids in ReachGrid will not be merged. If the number of reachable grids
of ReachGrid(v) is larger than 200 then v will be degraded
to an R-vertex. Since MAX_RMBR = A, there will be no Bvertex. In GeoRMBR, MAX_REACH_GRIDS = 0, MAX_RMBR
= A, hence only RMBR s are stored. In all ReachGrid related
approaches, the total space is split into 128 × 128 pieces in
the highest grid layer. SpaReach approach is implemented with
both spatial index and reachability index. Graph structure is
stored in Neo4j graph database. Reachability index is stored as
attributes of each graph vertex in Neo4j database. Reachability
index we use is proposed in [33]. Spatial index used SpaReach
approaches is implemented by gist index in postgresql. To
integrate Neo4j and postgresql databases, for each vertex in
the graph, we assign it an id to uniquely identify it.
Experimental Environment. The source code for evaluating query response time is implemented in Java and compiled
with java-7-openjdk-amd64. Source codes of index construction are implemented in c++ and complied using g++ 4.8.4.
Gist index is constructed automatically by using command
line in Postgresql shell. All evaluation experiments are run
on a computer with an 3.60GHz CPU, 32GB RAM running
Ubuntu 14.04 Linux OS.

TABLE II: Graph Datasets (K = 103 )
Dataset
citeseerx
go-uniprot
patent
uniprot22m
uniprot100m
uniprot150m

|V |
6540K
6968K
3775K
1595K
16087K
25038K

|E|
15011K
34770K
16519K
1595K
16087K
25038K

davg
2.30
4.99
4.38
1.00
1.00
1.00

l
59
21
32
4
9
10

Datasets. We evaluate the performance of our methods
using six real datasets [9], [33] (see Table II). Number of
vertices and edges are listed in column |V | and |E|. Column
davg and l are average degree of vertices and length of the
longest path in the graph, respectively. Citeseerx and patent
are real life citation graphs extracted from CiteSeerx2 and US
patents3 [33]. Go-uniprot is a graph generated from Gene Ontology and annotation files from Uniprot4 [33]. Uniprot22m,
uniprot100m and uniprot150m are RDF graphs from UniProt
database [33]. The aforementioned datasets represent graphs
that possess no spatial attributes. For each graph, we simulate
spatial data by assigning a spatial location to a subset of
the graph vertices. During the experiments, we change the
ratio of spatial vertices to the total number of vertices from
20% to 80%. During the experiments, we vary the spatial
distribution to be: uniform, zipf, and clustered distributions.
Unless mentioned otherwise, the number of spatial clusters is
set to 4 by default.
A. Query Response Time
In this section, we fist compare the query response time
performance of SpaReach to our GeoP approach. Afterwards,
we change tunable parameters in G EO R EACH to evaluate
influence of these thresholds. For each dataset, we change the
spatial selectivity of the input query rectangle from 0.0001 to
0.1. For each query spatial selectivity, we randomly generate
500 queries by randomly selecting 500 random vertices and
500 random spatial locations of the query rectangle. The
reported query response time is calculated as the average time
taken to answer the 500 queries.
Figure 5 depicts the query response time of GeoP and
SpaReach on four datasets. 80% of vertices in the graph
are spatial and they are randomly-distributed in space. For
brevity, we omit the results of the other two datasets, i.e.,
uniprot22m and uniprot100m, since they have almost the same
graph structure and exihibit the same performance. As it turns
out In Figure 5, GeoP outperforms SpaReach for any query
spatial selectivity in uniprot150m, go-uniprot and citeseerx.
For these datasets, SpaReach approach cost more time when
query selectivity increases. When we increasing the query
range size, the range query step tends to return a larger number
of spatial vertices. Hence, the graph reachability checking
step has to check more spatial vertices. Figure 5c and 5d
show similar experiment results. In conclusion, GeoP is much
2 http://citeseer.ist.psu.edu/
3 http://snap.stanford.edu/data/
4 http://www.uniprot.org/

query time (sec)

GeoP

SpaReach

query time (sec)

GeoP

SpaReach

query time (sec)

GeoP

SpaReach

query time (sec)

10000

100

10

100

100

10

1

10

1

1

0.1

1

0.01
0.0001

0.001

0.01

0.1

0.1
0.0001

0.001

0.01

0.1

0.01
0.0001

0.001

0.01

0.1

0.1
0.0001

0.001

GeoP

0.01

Query range

Query range

Query range

Query range

(a) uniprot150m

(b) patent

(c) go-uniprot

(d) citeseerx

SpaReach

0.1

Fig. 5: Query response time (80% spatial vertex ratio, randomly-distributed spatial data, and spatial selectivity ranging from 0.0001 to 0.1)
more query-efficient in relatively sparse graphs. Patent dataset
is the densest graph with richer reachability information.
Figure 5b indicates that even when spatial selectivity set to
0.0001, GeoP can achieve almost the same performance as
SpaReach. When spatial selectivity increases, GeoP outperforms SpaReach again. In a denser graph, the performance
difference between the two approaches is smaller than in
sparse graphs especially when the spatial selectivity is low.
Table III compares the query response time of all our approaches for the uniprot150m, patent go-uniprot and citeseerx
datasets with randomly distributed spatial vertices and spatial
ratio of 80%. In uniprot150m, all our approaches almost have
the same performance. The same pattern happens with the
uniprot22m, uniprot100m and go-uniprot datasets. So we use
uniprot150m as a representative.
For the patent graph with random-distributed spatial vertices
and spatial ratio of 20%, query efficiency difference can be
easily caught. GeoMT0 keeps information of exact reachable
grids of every vertex which brings us fast query speed, but
also the highest storage overhead. RMBR stores general spatial
boundary of reachable vertices which is the most scalable.
However, such approach spend the most time in answering the
query. Since GeoMT3 is an approach that MERGE_COUNT is
set to 3, just few grids in GeoMT3 are merged. As a result,
its query time is merely little bit longer than GeoMT0. There
are more grids getting merged in GeoMT2 than in GeoMT3.
Inaccuracy caused by more integration lowers efficiency of
GeoMT3 in query. GeoP is combination of ReachGrid and
RMBR. Its query efficiency is lower than GeoMT0 and better
than GeoRMBR. In this case, GeoMT2 outperforms GeoP. But
it is not always the case. By tuning MAX_REACH_GRIDS to
a larger number, GeoP can be more efficient in query.
In citeseerx, GeoMT0 keeps the best performance as expected. Performance of GeoP is in between GeoMT0 and
GeoRMBR as what is shown in patent. But GeoMT2 and
GeoMT3 reveal almost the same efficiency and they are worse
than GeoRMBR. Distinct polarized graph structure accounts
for the abnormal appearance. In citeseerx, all vertices can be
divided into two groups. One group consists of vertices that
cannot reach any vertex. The other group contains a what
we call center vertex. The center vertex has huge number of
out-edge neighbor vertices and is connected by huge number
of vertices as well. Because the center vertex can reach that
many vertices, it can reach nearly all grid cells in space. As

a result, vertices that can reach the center vertex can also
reach all grid cells in space. So no matter what value is
MAX_REACH_GRIDS, reachable grids in ReachGrid of these
vertices will be merged into only one grid in a lower layer
until to the bottom layer which is the whole space. Then such
ReachGrid can merely function as a GeoB which owns poorer
locality than RMBR.
B. Storage Overhead
Figure 6a gives the storage overhead of all approaches
for the uniprot150m dataset. In this experiment, the spatial
vertices are randomly distributed in space. Since uniprot22m
and uniprot100m share the same pattern with uniprot150m
(even spatial distribution of vertices varies), they are not shown
in the figure. The experiments show that G EO R EACH and
all its variants require less storage overhead than SpaReach
because of the additional overhead introduced by the spatial index. When there are less spatial vertices, SpaReach
obviously occupies less space because size of spatial index
lessens. However, SpaReach always requires more storage
than any other approaches. Storage overhead of G EO R EACH
approaches shows a two-stages pattern which means it is either
very high (ratio = 0.8, 0.6 and 0.4) or very low (ratio = 0.2).
The reason is as follows. These graphs are sparse and almost
all vertices reach the same vertex. This vertex cannot reach any
other vertex. Let us call it an end vertex. If the end vertex is
a spatial vertex, then all vertices that can reach the end vertex
will keep their spatial reachability information (no matter what
category they are) in storage. But if it is not, majority of
vertices will store nothing for spatial reachability information.
GeoMT0 and GeoP are of almost the same index size because
of sparsity and end-point phenomenon in these graphs. Such
characteristic causes that almost each vertex can just reach
only one grid which makes MAX_REACH_GRIDS invalid in
approach GeoMT0 (number of reachable grids is always less
than MAX_REACH_GRIDS) which makes GeoMT0 and GeoP
have nearly the same size. For similar reason, MERGE_COUNT
becomes invalid in these datasets which makes GeoMT2 and
GeoMT3 share the same index size with GeoMT0 and GeoP.
We also find out that index size of GeoRMBR is slightly larger
than GeoMT0 approaches. Intuitively, RMBR should be more
scalable than ReachGrid. But most of the vertices in these
three graphs can reach only one grid. In GeoRMBR, for each
vertex that have reachable spatial vertices, we assign an RMBR

TABLE III: Query Response Time in three datasets, 80% spatial vertex ratio, and spatial selectivity ranging from 0.0001 to 0.1
Selectivity
0.0001
0.001
0.01
0.1
Index size

MT0
68
65
66
69

GeoMT0
GeoP

(MB)
2400

uniprot150m
MT3 GeoP
67
66
78
66
65
65
65
75

MT2
68
77
66
65

GeoMT2
GeoRMBR

GeoMT3
SpaReach

Index size

RMBR
66
65
65
66
GeoMT0
GeoP

(MB)
6000

MT0
643
168
87
51

GeoMT2
GeoRMBR

patent
MT3
741
185
98
59

MT2
762
258
143
108

GeoMT3
SpaReach

Index size

GeoP
1570
559
217
155

GeoMT0
GeoP

(MB)
750

RMBR
2991
1965
915
348
GeoMT2
GeoRMBR

MT0
202
34
32
33

GeoMT3
SpaReach

Index size

4000

500

800

800

2000

250

400

0
0.8

0.6

0.4

0.2

0
0.8

0.6

0.4

0.2

GeoMT0
GeoP

(MB)
1200

1600

0

citeseerx
MT3 GeoP
203
210
471
207
410
189
399
160

MT2
212
460
408
399

GeoMT2
GeoRMBR

RMBR
234
215
200
183
GeoMT3
SpaReach

0
0.8

0.6

0.4

0.2

0.8

0.6

0.4

ratio

ratio

ratio

ratio

(a) uniprot150m

(b) patent

(c) go-uniprot

(d) citeseerx

0.2

Fig. 6: Storage Overhead (Randomly distributed, spatial vertex ratio from 0.8 to 0.2)
index size(MB)

Random

Clustered

Zipf

2400
1800

index size(MB)

Random

Clustered

Zipf

index size(MB)

Random

Clustered

Zipf

index size(MB)

1800

7 

6

1200



400

600

2 

200

Random

Clustered

Zipf

1200
600
0

0

0
GeoMT2

GeoP

GeoRMBR SpaReach

(a) uniprot150m

GeoMT2

GeoP

GeoRMBR SpaReach

(b) patent

0
GeoMT2

GeoP

GeoRMBR SpaReach

(c) go-uniprot

GeoMT2

GeoP

GeoRMBR SpaReach

(d) citeseerx

Fig. 7: Storage Overhead for varying spatial data distribution (randomly, cluster and zipf distributed) and 0.8 spatial vertex ratio)
which will be stored as coordinates of RMBR’s top-left and
lower-right points. It is more scalable to store one grid id than
two coordinates. So when a graph is highly sparse, index size
of GeoMT0 is possible to be less than GeoRMBR.
Figure 6c shows that in go-uniprot all G EO R EACH approaches performs better than SpaReach. When we compare
all the G EO R EACH approaches, GeoMT0, GeoMT2 and GeoMT3 lead to almost the same storage overhead. That happens
due to the fact that go-uniprot is a very sparse graph. A
vertex can only reach few grids in the whole space. Grid
cells in ReachGrid can hardly be spatially adjacent to each
other which causes no integration. The graph sparsity makes
the number of reachable grids in ReachGrid always less than
MAX_REACH_GRIDS which leads to less R-vertices and more
G-vertices. In consequence, go-uniprot, GeoMT0, GeoMT2,
GeoMT3 and GeoP lead to the same storage overhead. It
is rational that GeoRMBR requires the least storage because
RMBR occupies less storage than ReachGrid.
When graphs are denser, results become more complex.
Figure 6b shows index size of different approaches in patent
dataset with randomly-distributed spatial vertices. GeoRMBR
and GeoP, take the first and the second least storage and
are far less than other approaches because both of them
use RMBR which is more scalable. GeoMT0 takes the most
storage in all spatial ratios for that ReachGrid takes high
storage overhead. GeoMT2 and GeoMT3 require less storage
than GeoMT0 because spatially-adjacent reachable grids in

GeoMT0 are merged which brings us scalability. GeoMT3
are more scalable than GeoMT2 because MERGE_COUNT in
GeoMT2 is 2 which causes more integration. There are three
approaches, GeoMT3, GeoP and GeoRMBR, that outperform
SpaReach approach. By tuning parameters in G EO R EACH, we
are able achieve different performance in storage overhead and
can also outperform SpaReach.
Figure 6d depicts index size of all approaches in citeseerx
with randomly distributed spatial vertices. Spatial vertices ratio
ranges from 0.8 to 0.2. All G EO R EACH approaches outperform
SpaReach except for one outlier when spatial vertices ratio
is 0.2. GeoMT0 consumes huge storage. This is caused by
the center vertex which is above-mentioned. Recall that large
proportion of ReachGrid contains almost all grids in space.
After bitmap compression, it will cause low storage overhead.
This is why when spatial vertices ratio is 0.8, 0.6 and 0.4,
GeoMT0 consumes small size of index. When the ratio is 0.2,
there are less spatial vertices. Although graph structure does
not change, the center vertex reach less spatial vertices and
less grids. Then the bitmap compression brings no advantage
in storage overhead.
Figure 7 shows the impact of spatial data distribution on
the storage cost. GeoMT0, GeoMT2 and GeoMT3 are all
ReachGrid-based approaches. Spatial data distribution of vertices influences all approaches the same way. For all datasets,
SpaReach is not influenced by the spatial data distribution.
SpaReach consists of two sections: (1) The reachability index

size is determined by graph structure and (2) The spatial
index size is directly determined by number of spatial vertices. Hence, SpaReach exhibits the same storage overhead
for different spatial data distributions. When spatial vertices
distribution varies, GeoRMBR also keeps stable storage overhead. This is due to the fact that the storage overhead for
each RMBR is a constant and the number of stored RMBR
s is determined by the graph structure and spatial vertices
ratio, and not by the spatial vertices distribution. Spatial data
distribution can only influence the shape of each RMBR.
Figure 7a shows that each approach in G EO R EACH keeps
the same storage overhead under different distributions in
uniprot150m. As mentioned before, GeoMT0, GeoMT2, GeoMT3 and GeoP actually represent the same data structure
since there is only a single reachable grid in ReachGrid. When
there is only one grid reachable, varying the spatial distribution
becomes invalid for all approaches which use ReachGrid.
Figure 7b and 7c shows that the storage overhead introduced by ReachGrid-based approaches decreases when spatial
vertices become more congested. Randomly distributed spatial
data is the least congested while zipf distributed is the most.
The number of reachable spatial vertices from each vertex do
not change but these reachable spatial vertices become more
concentrated in space. This leads to less reachable grids in
ReachGrid.
Figure 7d shows that when spatial vertices are more congested, ReachGrid based approaches, i.e., GeoMT0, GeoMT2
and GeoMT3, tend to be less scalable. Recall that citeseerx
dataset is a polarized graph with a center vertex. One group
contains vertices that can reach huge number of vertices (about
200,000) due to the center vertex. When spatial vertices are
more concentrated and that will lead to more storage overhead.
C. Initialization time
In this section, we evaluate the index initialization time
for all considered approaches. For brevity, we only show the
performance results for four datasets, uniprot150m, patent,
go-uniprot and citeseerx, since uniprot22m, uniprot100m and
uniprot150m datasets exhibit the same performance. Figure 8a
shows that SpaReach requires much more construction time
than the other approaches under all spatial ratios. Although
these graphs are sparse, they contain large number of vertices. This characteristic causes huge overhead in constructing
a spatial index which dominates the initialization time in
SpaReach. Hence, SpaReach takes much more time than all
other approaches. However, the SpaReach initialization time
decreases when decreasing the number spatial vertices since
the spatial index building step deals with less spatial vertices
in such case. However, SpaReach remains the worst even when
the spatial vertex ratio is set to 20%.
Figures 8b and 8d gives the initialization time for both the
patent and citeseerx datasets, respectively. GeoRMBR takes
significantly less initialization time compared to all other
approaches. GeoP takes less time than the rest of approaches
because it is ReachGrid of partial vertices whose number of
reachable grids are less than MAX_REACH_GRIDS that are

calculated. In most cases, GeoMT0 can achieve almost equal
or better performance compared to SpaReach while GeoMT2
and GeoMT3 requires more time due to the integration of
adjacent reachable grids. To sum up, GeoRMBR and GeoP
perform much better than SpaReach in initialization even
in very dense graphs. GeoMT0 can keep almost the same
performance with SpaReach approach.
Figure 8c shows the initialization time for all six approaches on the go-uniprot dataset. Both RMBR approaches,
i.e., GeoRMBR and GeoP, still outperform SpaReach. This
is due to the fact that a spatial index constitutes a high
proportion of SpaReach initialization time. As opposed to the
uniprot150m case, the smaller performance gap between initializing GeoRMBR and SpaReach in go-uniprot.is explained
as follows. The size of go-uniprotis far less than uniprot150m
which decreases the spatial index initialization cost. As a
result, the index construction time in SpaReach is less than
that in uniprot150m. Since this graph has more reachability
information, all G EO R EACH approaches require more time
than in uniprot150m. It is conjunction of G EO R EACH and
SpaReach index size changes that causes the smaller gap.
VII. C ONCLUSION
This paper describes G EO R EACH a novel approach that
evaluates graph reachability queries and spatial range predicates side-by-side. G EO R EACH extends the functionality of a
given graph database management system with light-weight
spatial indexing entries to efficiently prune the graph traversal
based on spatial constraints. G EO R EACH allows users to
tune the system performance to achieve both efficiency and
scalability. Based on extensive experiments, we show that
G EO R EACH can be scalable and query-efficient than existing spatial and reachability indexing approaches in relatively
sparse graphs. Even in rather dense graphs, our approach
can outperform existing approaches in storage overhead and
initialization time and still achieves faster query response
time. In the future, we plan to study we plan to study
the extensibility of G EO R EACH to support different spatial
predicates. Furthermore, we aim to extend the framework
to support a distributed system environment. Last but not
least, we also plan to study the applicability of G EO R EACH
to various application domains including: Spatial Influence
Maximization, Location and Social-Aware Recommendation,
and Location-Aware Citation Network Analysis.
R EFERENCES
[1] R. Agrawal, A. Borgida, and H. V. Jagadish. Efficient Management of
Transitive Relationships in Large Data and Knowledge Bases. ACM,
1989.
[2] R. Bayer and E. M. McCreight. Organization and Maintenance of Large
Ordered Indices. Acta Informatica, 1(3):173–89, 1972.
[3] N. Beckmann, H.-P. Kriegel, R. Schneider, and B. Seeger. The R*-Tree:
An Efficient and Robust Access Method for Points and Rectangles. In
SIGMOD, pages 322–331, May 1990.
[4] J. L. Bentley. Multidimensional Binary Search Trees Used for Associative Searching. Communications of the ACM, CACM, 18(9):509–517,
1975.
[5] J. Cai and C. K. Poon. Path-hop: efficiently indexing large graphs for
reachability queries. In CIKM, pages 119–128. ACM, 2010.

Ix 	me
(sec)
480

GeoMT0
GeoP

GeoMT2
GeoRMBR

GeoMT
SpaReach

Index me

GeoMT0
GeoP

GeoMT2
GeoRMBR

GeoMT3
SpaReach

Index me

GeoMT0
GeoP

GeoMT2
GeoRMBR

GeoMT3 x me
(sec)
SpaReach
00

5


(sec)
150

320

00

100

200

1

15


50

100

0
0.8

0.

0.4

0.2

(sec)

0

08

0
0.6

0.4

0.2

08

0
0.6

0.4

0.2

08

GeoMT0
GeoP

GeoMT2
GeoRMBR

0.

0

ratio

ratio

ratio

ratio

(a) uniprot150

(b) patent

(c) go-uniprot

(d) citeseerx

GeoMT
SpaReach

02

Fig. 8: Initialization time (Randomly distributed, spatial vertex ratio from 0.8 to 0.2)

[6] L. Chen, A. Gupta, and M. E. Kurul. Stack-based algorithms for pattern
matching on dags. In VLDB, pages 493–504. VLDB Endowment, 2005.
[7] Y. Chen and Y. Chen. An efficient algorithm for answering graph
reachability queries. In ICDE, pages 893–902. IEEE, 2008.
[8] J. Cheng, S. Huang, H. Wu, and A. W.-C. Fu. Tf-label: a topologicalfolding labeling scheme for reachability querying in a large graph. In
SIGMOD, pages 193–204. ACM, 2013.
[9] J. Cheng, S. Huang, H. Wu, and A. W.-C. Fu. Tf-label: a topologicalfolding labeling scheme for reachability querying in a large graph. In
SIGMOD, pages 193–204. ACM, 2013.
[10] J. Cheng, Z. Shang, H. Cheng, H. Wang, and J. X. Yu. K-reach: who
is in your small world. PVLDB, 5(11):1292–1303, 2012.
[11] J. Cheng, J. X. Yu, X. Lin, H. Wang, and P. S. Yu. Fast computing
reachability labelings for large graphs with high compression rate. In
EDBT, pages 193–204. ACM, 2008.
[12] E. Cohen, E. Halperin, H. Kaplan, and U. Zwick. Reachability and
distance queries via 2-hop labels. SIAM Journal on Computing,
32(5):1338–1355, 2003.
[13] D. Comer. The Ubiquitous B-Tree. ACM Computing Surveys, 11(2):121–
137, 1979.
[14] R. A. Finkel and J. L. Bentley. Quad trees: A Data Structure for Retrieval
of Composite Keys. Acta Informatica, 4(1):1–9, 1974.
[15] J. E. Gonzalez, Y. Low, H. Gu, D. Bickson, and C. Guestrin. GraphChi:
Large-Scale Graph Computation on Just a PC. In OSDI, 2012.
[16] A. Guttman. R-Trees: A Dynamic Index Structure For Spatial Searching.
In SIGMOD, 1984.
[17] H. Jagadish. A compression technique to materialize transitive closure.
TODS, 15(4):558–598, 1990.
[18] R. Jin, Y. Xiang, N. Ruan, and H. Wang. Efficiently answering
reachability queries on very large directed graphs. In SIGMOD, pages
595–608. ACM, 2008.
[19] J. Liagouris, N. Mamoulis, P. Bouros, and M. Terrovitis. An effective
encoding scheme for spatial RDF data. PVLDB, 7(12):1271–1282, 2014.
[20] D. B. Lomet. Grow and Post Index Trees: Roles, Techniques and Future
Potential. In SSD, pages 183–206, Aug. 1991.
[21] P. Rigaux, M. Scholl, and A. Voisard. Spatial Databases with Application to GIS. Morgan Kaufmann, 2002.
[22] H. Samet. The Design and Analysis of Spatial Data Structures. AddisonWesley, 1990.
[23] H. Samet. Foundations of Multidimensional and Metric Data Structures.
Morgan Kaufmann, 2006.
[24] M. Sarwat, S. Elnikety, Y. He, and G. Kliot. Horton: Online Query
Execution Engine for Large Distributed Graphs. In ICDE, 2012.
[25] M. Sarwat, S. Elnikety, Y. He, and M. F. Mokbel. Horton+: A Distributed
System for Processing Declarative Reachability Queries over Partitioned
Graphs. PVLDB, 6(14):1918–1929, 2013.
[26] R. Schenkel, A. Theobald, and G. Weikum. Hopi: An efficient connection index for complex xml document collections. In EDBT, pages
237–255. Springer, 2004.
[27] S. Seufert, A. Anand, S. Bedathur, and G. Weikum. Ferrari: Flexible
and efficient reachability range assignment for graph indexing. In ICDE,
pages 1009–1020. IEEE, 2013.
[28] B. Shao, H. Wang, and Y. Li. Trinity: A Distributed Graph Engine on
a Memory Cloud. In SIGMOD, 2013.
[29] S. Shekhar and S. Chawla. Spatial Databases: A Tour. Prentice Hall,
2003.
[30] S. Trißl and U. Leser. Fast and practical indexing and querying of very
large graphs. In SIGMOD, pages 845–856. ACM, 2007.

[31] S. J. van Schaik and O. de Moor. A memory efficient reachability data
structure through bit vector compression. In SIGMOD, pages 913–924.
ACM, 2011.
[32] H. Wang, H. He, J. Yang, P. S. Yu, and J. X. Yu. Dual labeling:
Answering graph reachability queries in constant time. In ICDE, pages
75–75. IEEE, 2006.
[33] Y. Yano, T. Akiba, Y. Iwata, and Y. Yoshida. Fast and scalable
reachability queries on graphs by pruned labeling with landmarks and
paths. In CIKM, pages 1601–1606. ACM, 2013.
[34] H. Yildirim, V. Chaoji, and M. J. Zaki. Grail: Scalable reachability index
for large graphs. PVLDB, 3(1-2):276–284, 2010.
[35] A. D. Zhu, W. Lin, S. Wang, and X. Xiao. Reachability queries on large
dynamic graphs: a total order approach. In SIGMOD, pages 1323–1334.
ACM, 2014.

1384

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

LARS*: An Efficient and Scalable
Location-Aware Recommender System
Mohamed Sarwat, Justin J. Levandoski, Ahmed Eldawy, and Mohamed F. Mokbel
Abstract—This paper proposes LARS*, a location-aware recommender system that uses location-based ratings to produce
recommendations. Traditional recommender systems do not consider spatial properties of users nor items; LARS*, on the other hand,
supports a taxonomy of three novel classes of location-based ratings, namely, spatial ratings for non-spatial items, non-spatial ratings
for spatial items, and spatial ratings for spatial items. LARS* exploits user rating locations through user partitioning, a technique that
influences recommendations with ratings spatially close to querying users in a manner that maximizes system scalability while not
sacrificing recommendation quality. LARS* exploits item locations using travel penalty, a technique that favors recommendation
candidates closer in travel distance to querying users in a way that avoids exhaustive access to all spatial items. LARS* can apply
these techniques separately, or together, depending on the type of location-based rating available. Experimental evidence using
large-scale real-world data from both the Foursquare location-based social network and the MovieLens movie recommendation
system reveals that LARS* is efficient, scalable, and capable of producing recommendations twice as accurate compared to existing
recommendation approaches.
Index Terms—Recommender system, spatial, location, performance, efficiency, scalability, social

1

I NTRODUCTION

R

ECOMMENDER systems make use of community
opinions to help users identify useful items from a
considerably large search space (e.g., Amazon inventory [1],
Netflix movies1 ). The technique used by many of these systems is collaborative filtering (CF) [2], which analyzes past
community opinions to find correlations of similar users
and items to suggest k personalized items (e.g., movies)
to a querying user u. Community opinions are expressed
through explicit ratings represented by the triple (user, rating, item) that represents a user providing a numeric rating
for an item.
Currently, myriad applications can produce location-based
ratings that embed user and/or item locations. For example, location-based social networks (e.g., Foursquare2 and
Facebook Places [3]) allow users to “check-in” at spatial
destinations (e.g., restaurants) and rate their visit, thus are
capable of associating both user and item locations with ratings. Such ratings motivate an interesting new paradigm
of location-aware recommendations, whereby the recommender system exploits the spatial aspect of ratings when

1. Netflix: http://www.netflix.com.
2. Foursquare: http://foursquare.com.
• M. Sarwat, A. Eldawy, and M. F. Mokbel are with the Department
of Computer Science and Engineering, University of Minnesota,
Minneapolis, MN 55455 USA.
E-mail: {sarwat, eldawy, mokbel}@cs.umn.edu.
• J. J. Levandoski is with the Microsoft Research, Redmond, WA 98052-6399
USA. E-mail: justin.levandoski@microsoft.com.
Manuscript received 3 May 2012; revised 27 Nov. 2012; accepted
14 Jan. 2013. Date of publication 31 Jan. 2013; date of current version
29 May 2014.
Recommended for acceptance by J. Gehrke, B.C. Ooi, and E. Pitoura.
For information on obtaining reprints of this article, please send e-mail to:
reprints@ieee.org, and reference the Digital Object Identifier below.
Digital Object Identifier 10.1109/TKDE.2013.29

producing recommendations. Existing recommendation
techniques [4] assume ratings are represented by the (user,
rating, item) triple, thus are ill-equipped to produce locationaware recommendations.
In this paper, we propose LARS*, a novel locationaware recommender system built specifically to produce
high-quality location-based recommendations in an efficient manner. LARS* produces recommendations using a
taxonomy of three types of location-based ratings within a
single framework: (1) Spatial ratings for non-spatial items,
represented as a four-tuple (user, ulocation, rating, item),
where ulocation represents a user location, for example, a
user located at home rating a book; (2) non-spatial ratings
for spatial items, represented as a four-tuple (user, rating,
item, ilocation), where ilocation represents an item location,
for example, a user with unknown location rating a restaurant; (3) spatial ratings for spatial items, represented as a
five-tuple (user, ulocation, rating, item, ilocation), for example,
a user at his/her office rating a restaurant visited for lunch.
Traditional rating triples can be classified as non-spatial
ratings for non-spatial items and do not fit this taxonomy.

1.1 Motivation: A Study of Location-Based Ratings
The motivation for our work comes from analysis of two
real-world location-based rating datasets: (1) a subset of
the well-known MovieLens dataset [5] containing approximately 87K movie ratings associated with user zip codes
(i.e., spatial ratings for non-spatial items) and (2) data from
the Foursquare [6] location-based social network containing user visit data for 1M users to 643K venues across
the United States (i.e., spatial ratings for spatial items).
In our analysis we consistently observed two interesting properties that motivate the need for location-aware
recommendation techniques.

c 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
1041-4347 
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

Fig. 1. Preference locality in location-based ratings. (a) MovieLens

preference locality. (b) Foursquare preference locality.

Preference locality. Preference locality suggests users
from a spatial region (e.g., neighborhood) prefer items
(e.g., movies, destinations) that are manifestly different
than items preferred by users from other, even adjacent,
regions. Fig. 1(a) lists the top-4 movie genres using average MovieLens ratings of users from different U.S. states.
While each list is different, the top genres from Florida
differ vastly from the others. Florida’s list contains three
genres (“Fantasy", “Animation", “Musical") not in the other
lists. This difference implies movie preferences are unique
to specific spatial regions, and confirms previous work
from the New York Times [7] that analyzed Netflix user
queues across U.S. zip codes and found similar differences. Meanwhile, Fig. 1(b) summarizes our observation
of preference locality in Foursquare by depicting the visit
destinations for users from three adjacent Minnesota cities.
Each sample exhibits diverse behavior: users from Falcon
Heights, MN favor venues in St. Paul, MN (17% of visits)
Minneapolis (13%), and Roseville, MN (10%), while users
from Robbinsdale, MN prefer venues in Brooklyn Park, MN
(32%) and Robbinsdale (20%). Preference locality suggests
that recommendations should be influenced by locationbased ratings spatially close to the user. The intuition is that
localization influences recommendation using the unique
preferences found within the spatial region containing the
user.
Travel locality. Our second observation is that, when
recommended items are spatial, users tend to travel a
limited distance when visiting these venues. We refer
to this property as “travel locality.” In our analysis of
Foursquare data, we observed that 45% of users travel
10 miles or less, while 75% travel 50 miles or less. This
observation suggests that spatial items closer in travel
distance to a user should be given precedence as recommendation candidates. In other words, a recommendation
loses efficacy the further a querying user must travel
to visit the destination. Existing recommendation techniques do not consider travel locality, thus may recommend
users destinations with burdensome travel distances (e.g.,
a user in Chicago receiving restaurant recommendations
in Seattle).

1.2

Our Contribution: LARS* - A Location-Aware
Recommender System
Like traditional recommender systems, LARS* suggests k
items personalized for a querying user u. However, LARS*
is distinct in its ability to produce location-aware recommendations using each of the three types of location-based
rating within a single framework.

1385

LARS* produces recommendations using spatial ratings
for non-spatial items, i.e., the tuple (user, ulocation, rating, item), by employing a user partitioning technique that
exploits preference locality. This technique uses an adaptive
pyramid structure to partition ratings by their user location
attribute into spatial regions of varying sizes at different
hierarchies. For a querying user located in a region R, we
apply an existing collaborative filtering technique that utilizes only the ratings located in R. The challenge, however,
is to determine whether all regions in the pyramid must
be maintained in order to balance two contradicting factors: scalability and locality. Maintaining a large number of
regions increases locality (i.e., recommendations unique to
smaller spatial regions), yet adversely affects system scalability because each region requires storage and maintenance
of a collaborative filtering data structure necessary to produce recommendations (i.e., the recommender model). The
LARS* pyramid dynamically adapts to find the right pyramid shape that balances scalability and recommendation
locality.
LARS* produces recommendations using non-spatial ratings for spatial items, i.e., the tuple (user, rating, item, ilocation), by using travel penalty, a technique that exploits travel
locality. This technique penalizes recommendation candidates the further they are in travel distance to a querying
user. The challenge here is to avoid computing the travel
distance for all spatial items to produce the list of k recommendations, as this will greatly consume system resources.
LARS* addresses this challenge by employing an efficient
query processing framework capable of terminating early
once it discovers that the list of k answers cannot be altered
by processing more recommendation candidates. To produce recommendations using spatial ratings for spatial items,
i.e., the tuple (user, ulocation, rating, item, ilocation) LARS*
employs both the user partitioning and travel penalty techniques to address the user and item locations associated
with the ratings. This is a salient feature of LARS*, as
the two techniques can be used separately, or in concert,
depending on the location-based rating type available in
the system.
We experimentally evaluate LARS* using real locationbased ratings from Foursquare [6] and MovieLens [5], along
with a generated user workload of both snapshot and continuous queries. Our experiments show LARS* is scalable to
real large-scale recommendation scenarios. Since we have
access to real data, we also evaluate recommendation quality by building LARS* with 80% of the spatial ratings and
testing recommendation accuracy with the remaining 20%
of the (withheld) ratings. We find LARS* produces recommendations that are twice as accurate (i.e., able to better
predict user preferences) compared to traditional collaborative filtering. In summary, the contributions of this paper
are as follows:
•

•

We provide a novel classification of three types
of location-based ratings not supported by existing
recommender systems: spatial ratings for non-spatial
items, non-spatial ratings for spatial items, and spatial
ratings for spatial items.
We propose LARS*, a novel location-aware recommender system capable of using three classes of

1386

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

Fig. 3. Item-based similarity calculation.

(a)

(b)

Fig. 2. Item-based CF model generation. (a) Ratings matrix.

(b) Item-based CF model.

location-based ratings. Within LARS*, we propose:
(a) a user partitioning technique that exploits user
locations in a way that maximizes system scalability while not sacrificing recommendation locality and
(b) a travel penalty technique that exploits item locations and avoids exhaustively processing all spatial
recommendation candidates.
•
LARS* distinguishes itself from LARS [8] in the following points: (1) LARS* achieves higher locality
gain than LARS using a better user partitioning data
structure and algorithm. (2) LARS* exhibits a more
flexible tradeoff between locality and scalability. (3)
LARS* provides a more efficient way to maintain
the user partitioning structure, as opposed to LARS
expensive operations.
•
We provide experimental evidence that LARS* scales
to large-scale recommendation scenarios and provides better quality recommendations than traditional approaches.
This paper is organized as follows: Section 2 gives an
overview of LARS*. Sections 4, 5, and 6 cover LARS* recommendation techniques using spatial ratings for non-spatial
items, non-spatial ratings for spatial items, and spatial ratings
for spatial items, respectively. Section 7 provides experimental analysis. Section 8 covers related work, while Section 9
concludes the paper.

2

LARS* OVERVIEW

This section provides an overview of LARS* by discussing
the query model and the collaborative filtering method.

2.1 LARS* Query Model
Users (or applications) provide LARS* with a user id U,
numeric limit K, and location L; LARS* then returns K
recommended items to the user. LARS* supports both snapshot (i.e., one-time) queries and continuous queries, whereby
a user subscribes to LARS* and receives recommendation
updates as her location changes. The technique LARS*
uses to produce recommendations depends on the type of
location-based rating available in the system. Query processing support for each type of location-based rating is
discussed in Sections 4 to 6.
2.2 Item-Based Collaborative Filtering
LARS* uses item-based collaborative filtering (abbr. CF)
as its primary recommendation technique, chosen due to
its popularity and widespread adoption in commercial
systems (e.g., Amazon [1]). Collaborative filtering (CF)
assumes a set of n users U = {u1 , . . . , un } and a set of m

items I = {i1 , . . . , im }. Each user uj expresses opinions about
a set of items Iuj ⊆ I . Opinions can be a numeric rating
(e.g., the Netflix scale of one to five stars), or unary (e.g.,
Facebook “check-ins" [3]). Conceptually, ratings are represented as a matrix with users and items as dimensions, as
depicted in Fig. 2(a). Given a querying user u, CF produces
a set of k recommended items Ir ⊂ I that u is predicted to
like the most.
Phase I: Model Building. This phase computes a similarity score sim(ip ,iq ) for each pair of objects ip and iq
that have at least one common rating by the same user
(i.e., co-rated dimensions). Similarity computation is covered below. Using these scores, a model is built that stores
for each item i ∈ I , a list L of similar items ordered by a
similarity score sim(ip ,iq ), as depicted in Fig. 2(b). Building
2
this model is an O( RU ) process [1], where R and U are the
number of ratings and users, respectively. It is common to
truncate the model by storing, for each list L, only the n
most similar items with the highest similarity scores [9].
The value of n is referred to as the model size and is usually
much less than |I |.
Phase II: Recommendation Generation. Given a querying user u, recommendations are produced by computing
u’s predicted rating P(u,i) for each item i not rated by u [9]:

l∈L sim(i, l) ∗ ru,l
P(u,i) = 
(1)
l∈L |sim(i, l)|
Before this computation, we reduce each similarity list L to
contain only items rated by user u. The prediction is the sum
of ru,l , a user u’s rating for a related item l ∈ L weighted by
sim(i,l), the similarity of l to candidate item i, then normalized by the sum of similarity scores between i and l. The
user receives as recommendations the top-k items ranked
by P(u,i) .
Computing Similarity. To compute sim(ip , iq ), we represent each item as a vector in the user-rating space of
the rating matrix. For instance, Fig. 3 depicts vectors for
items ip and iq from the matrix in Fig. 2(a). Many similarity functions have been proposed (e.g., Pearson Correlation,
Cosine); we use the Cosine similarity in LARS* due to its
popularity:
sim(ip , iq ) =

ip · iq
ip iq 

(2)

This score is calculated using the vectors’ co-rated dimensions, e.g., the Cosine similarity between ip and iq in
Fig. 3 is .7 calculated using the circled co-rated dimensions.
Cosine distance is useful for numeric ratings (e.g., on a scale
[1,5]). For unary ratings, other similarity functions are used
(e.g., absolute sum [10]).
While we opt to use item-based CF in this paper, no
factors disqualify us from employing other recommendation techniques. For instance, we could easily employ

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

1387

Fig. 5. Example of Items Ratings Statistics Table.

Fig. 4. Pyramid data structure.

user-based CF [4], that uses correlations between users
(instead of items).

3

N ON -S PATIAL U SER R ATINGS FOR
N ON -S PATIAL I TEMS

The traditional item-based collaborative filtering (CF)
method is a special case of LARS*. CF takes as input the
classical rating triplet (user, rating, item) such that neither
the user location nor the item location are specified. In
such case, LARS* directly employs the traditional model
building phase (Phase-I in section 2) to calculate the similarity scores between all items. Moreover, recommendations
are produced to the users using the recommendation generation phase (Phase-II in section 2). During the rest of
the paper, we explain how LARS* incorporates either the
user spatial location or the item spatial location to serve
location-aware recommendations to the system users.

4

S PATIAL U SER R ATINGS FOR N ON -S PATIAL
I TEMS

This section describes how LARS* produces recommendations using spatial ratings for non-spatial items represented
by the tuple (user, ulocation, rating, item). The idea is to
exploit preference locality, i.e., the observation that user opinions are spatially unique (based on analysis in Section 1.1).
We identify three requirements for producing recommendations using spatial ratings for non-spatial items: (1) Locality:
recommendations should be influenced by those ratings
with user locations spatially close to the querying user
location (i.e., in a spatial neighborhood); (2) Scalability: the
recommendation procedure and data structure should scale
up to large number of users; (3) Influence: system users
should have the ability to control the size of the spatial
neighborhood (e.g., city block, zip code, or county) that
influences their recommendations.
LARS* achieves its requirements by employing a user
partitioning technique that maintains an adaptive pyramid
structure, where the shape of the adaptive pyramid is
driven by the three goals of locality, scalability, and influence.
The idea is to adaptively partition the rating tuples (user,
ulocation, rating, item) into spatial regions based on the ulocation attribute. Then, LARS* produces recommendations

using any existing collaborative filtering method (we use
item-based CF) over the remaining three attributes (user,
rating, item) of only the ratings within the spatial region
containing the querying user. We note that ratings can come
from users with varying tastes, and that our method only
forces collaborative filtering to produce personalized user
recommendations based only on ratings restricted to a specific spatial region. In this section, we describe the pyramid
structure in Section 4.1, query processing in Section 4.2, and
finally data structure maintenance in Section 4.3.

4.1 Data Structure
LARS* employs a partial in-memory pyramid structure [11]
(equivalent to a partial quad-tree [12]) as depicted in Fig. 4.
The pyramid decomposes the space into H levels (i.e., pyramid height). For a given level h, the space is partitioned
into 4h equal area grid cells. For example, at the pyramid
root (level 0), one grid cell represents the entire geographic
area, level 1 partitions space into four equi-area cells, and
so forth. We represent each cell with a unique identifier cid.
A rating may belong to up to H pyramid cells: one
per each pyramid level starting from the lowest maintained grid cell containing the embedded user location
up to the root level. To provide a tradeoff between recommendation locality and system scalability, the pyramid
data structure maintains three types of cells (see Fig. 4):
(1) Recommendation Model Cell (α-Cell), (2) Statistics Cell
(β-Cell), and (3) Empty Cell (γ -Cell), explained as follows:
Recommendation Model Cell (α-Cell). Each α-Cell
stores an item-based collaborative filtering model built
using only the spatial ratings with user locations contained
in the cell’s spatial region. Note that the root cell (level 0)
of the pyramid is an α-Cell and represents a “traditional"
(i.e., non-spatial) item-based collaborative filtering model.
Moreover, each α-Cell maintains statistics about all the ratings located within the spatial extents of the cell. Each
α-Cell Cp maintains a hash table that indexes all items (by
their IDs) that have been rated in this cell, named Items
Ratings Statistics Table. For each indexed item i in the Items
Ratings Statistics Table, we maintain four parameters; each
parameter represents the number of user ratings to item i in
each of the four children cells (i.e., C1 , C2 , C3 , and C4 ) of cell
Cp . An example of the maintained parameters is given in
Fig. 5. Assume that cell Cp contains ratings for three items
I1 , I2 , and I3 . Fig. 5 shows the maintained statistics for each
item in cell Cp . For example, for item I1 , the number of user
ratings located in child cell C1 , C2 , C3 , and C4 is equal to
109, 3200, 14, and 54, respectively. Similarly, the number of
user ratings is calculated for items I2 and I3 .
Statistics Cell (β-Cell). Like an α-Cell, a β-Cell maintains statistics (i.e., items ratings Statistics Table) about the
user/item ratings that are located within the spatial range

1388

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

of the cell. The only difference between an α-Cell and a
β-Cell is that a β-Cell does not maintain a collaborative
filtering (CF) model for the user/item ratings lying in its
boundaries. In consequence, a β-Cell is a light weight cell
such that it incurs less storage than an α-Cell. In favor of
system scalability, LARS* prefers a β-Cell over an α-Cell to
reduce the total system storage.
Empty Cell (γ -Cell). a γ -Cell is a cell that maintains
neither the statistics nor the recommendation model for the
ratings lying within its boundaries. a γ -Cell is the most
light weight cell among all cell types as it almost incurs
no storage overhead. Note that an α-Cell can have α-Cells,
β-Cells, or γ -Cells children. Also, a β-Cell can have α-Cells,
β-Cells, or γ -Cells children. However, a γ -Cell cannot have
any children.

4.1.1 Pyramid Structure Intuition
An α-Cell requires the highest storage and maintenance
overhead because it maintains a CF model as well as the
user/item ratings statistics. On the other hand, an α-Cell (as
opposed to β-Cell and γ -Cell) is the only cell that can be
leveraged to answer recommendation queries. A pyramid
structure that only contains α-Cells achieves the highest
recommendation locality, and this is why an α-Cell is considered the highly ranked cell type in LARS*. a β-Cell is
the secondly ranked cell type as it only maintains statistics
about the user/item ratings. The storage and maintenance
overhead incurred by a β-Cell is less expensive than an
α-Cell. The statistics maintained at a β-Cell determines
whether the children of that cell need to be maintained as
α-Cells to serve more localized recommendation. Finally, a
γ -Cell (lowest ranked cell type) has the least maintenance
cost, as neither a CF model nor statistics are maintained for
that cell. Moreover, a γ -Cell is a leaf cell in the pyramid.
LARS* upgrades (downgrades) a cell to a higher (lower)
cell rank, based on trade-offs between recommendation
locality and system scalability (discussed in Section 4.3). If
recommendation locality is preferred over scalability, more
α-Cells are maintained in the pyramid. On the other hand, if
scalability is favored over locality, more γ -Cells exist in the
pyramid. β-Cells comes as an intermediary stage between
α-Cells and γ -Cells to further increase the recommendation
locality whereas the system scalability is not quite affected.
We chose to employ a pyramid as it is a “spacepartitioning" structure that is guaranteed to completely
cover a given space. For our purposes, “data-partitioning"
structures (e.g., R-trees) are less ideal than a “spacepartitioning" structure for two main reasons: (1) “datapartitioning" structures index data points, and hence covers
only locations that are inserted in them. In other words,
“data-partitioning" structures are not guaranteed to completely cover a given space, which is not suitable for
queries issued in arbitrary spatial locations. (2) In contrast to “data-partitioning" structures (e.g., R-trees [13]),
“space partitioning" structures show better performance for
dynamic memory resident data [14]–[16].
4.1.2 LARS* versus LARS
Table 1 compares LARS* against LARS. Like LARS*,
LARS [8] employs a partial pyramid data structure to
support spatial user ratings for non-spatial items. LARS

TABLE 1
Comparison between LARS and LARS*

Detailed experimental evaluation results are provided in Section 7.

is different from LARS* in the following aspects: (1) As
shown in Table 1, LARS* maintains α-Cells, β-Cells, and γ Cells, whereas LARS only maintains α-Cells and γ -Cells. In
other words, LARS either merges or splits a pyramid cell
based on a tradeoff between scalability and recommendation locality. LARS* employs the same tradeoff and further
increases the recommendation locality by allowing for more
α-Cells to be maintained at lower pyramid levels. (2) As
opposed to LARS, LARS* does not perform a speculative
splitting operation to decide whether to maintain more
localized CF models. Instead, LARS* maintains extra statistics at each α-Cell and β-Cell that helps in quickly deciding
wether a CF model needs to be maintained at a child cell.
(3) As it turns out from Table 1, LARS* achieves higher recommendation locality than LARS. That is due to the fact
that LARS maintains a CF recommendation model in a cell
at pyramid level h if and only if a CF model, at its parent
cell at level h − 1, is also maintained. However, LARS* may
maintain an α-Cell at level h even though its parent cell,
at level h − 1, does not maintain a CF model, i.e., the parent cell is a β-Cell. In LARS*, the role of a β-Cell is to keep
the user/item ratings statistics that are used to quickly decide
whether the child cells needs to be γ -Cells or α-Cells. (4) As
given in Table 1, LARS* incurs more storage overhead than
LARS which is explained by the fact that LARS* maintains additional type of cell, i.e., β-Cells, whereas LARS
only maintains α-Cells and γ -Cells. In addition, LARS*
may also maintain more α-Cells than LARS does in order
to increase the recommendation locality. (5) Even though
LARS* may maintain more α-Cells than LARS besides the
extra statistics maintained at β-Cells, nonetheless LARS*
incurs less maintenance cost. That is due to the fact that
LARS* also reduces the maintenance overhead by avoiding
the expensive speculative splitting operation employed by
LARS maintenance algorithm. Instead, LARS* employs the
user/item ratings statistics maintained at either a β-Cell or an
α-Cell to quickly decide whether the cell children need to
maintain a CF model (i.e., upgraded to α-Cells), just needs
to maintain the statistics (i.e., become β-Cells), or perhaps
downgraded to γ -Cells.

4.2 Query Processing
Given a recommendation query (as described in
Section 2.1) with user location L and a limit K, LARS*
performs two query processing steps: (1) The user location

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

1389

L is used to find the lowest maintained α-Cell C in the
adaptive pyramid that contains L. This is done by hashing
the user location to retrieve the cell at the lowest level of
the pyramid. If an α-Cell is not maintained at the lowest
level, we return the nearest maintained ancestor α-Cell.
(2) The top-k recommended items are generated using the
item-based collaborative filtering technique (covered in
Section 2.2) using the model stored at C. As mentioned
earlier, the model in C is built using only the spatial ratings
associated with user locations within C.
In addition to traditional recommendation queries (i.e.,
snapshot queries), LARS* also supports continuous queries
and can account for the influence requirement as follows.
Continuous queries. LARS* evaluates a continuous
query in full once it is issued, and sends recommendations back to a user U as an initial answer. LARS* then
monitors the movement of U using her location updates.
As long as U does not cross the boundary of her current grid cell, LARS* does nothing as the initial answer is
still valid. Once U crosses a cell boundary, LARS* reevaluates the recommendation query for the new cell only if
the new cell is an α-Cell. In case the new cell is an αCell, LARS* only sends incremental updates [16] to the last
reported answer. Like snapshot queries, if a cell at level h is
not maintained, the query is temporarily transferred higher
in the pyramid to the nearest maintained ancestor α-Cell.
Note that since higher-level cells maintain larger spatial
regions, the continuous query will cross spatial boundaries less often, reducing the amount of recommendation
updates.
Influence level. LARS* addresses the influence requirement by allowing querying users to specify an optional
influence level (in addition to location L and limit K)
that controls the size of the spatial neighborhood used
to influence their recommendations. An influence level I
maps to a pyramid level and acts much like a “zoom"
level in Google or Bing maps (e.g., city block, neighborhood, entire city). The level I instructs LARS* to process the recommendation query starting from the grid
α-Cell containing the querying user location at level
I, instead of the lowest maintained grid α-Cell (the
default). An influence level of zero forces LARS* to use
the root cell of the pyramid, and thus act as a traditional (non-spatial) collaborative filtering recommender
system.

4.4 Main Idea
As time goes by, new users, ratings, and items will be added
to the system. This new data will both increase the size of
the collaborative filtering models maintained in the pyramid cells, as well as alter recommendations produced from
each cell. To account for these changes, LARS* performs
maintenance on a cell-by-cell basis. Maintenance is triggered for a cell C once it receives N% new ratings; the
percentage is computed from the number of existing ratings
in C. We do this because an appealing quality of collaborative filtering is that as a model matures (i.e., more data
is used to build the model), more updates are needed to
significantly change the top-k recommendations produced
from it [17]. Thus, maintenance is needed less often.
We note the following features of pyramid maintenance:
(1) Maintenance can be performed completely offline, i.e.,
LARS* can continue to produce recommendations using
the "old" pyramid cells while part of the pyramid is being
updated; (2) maintenance does not entail rebuilding the
whole pyramid at once, instead, only one cell is rebuilt at
a time; (3) maintenance is performed only after N% new
ratings are added to a pyramid cell, meaning maintenance
will be amortized over many operations.

4.3 Data Structure Maintenance
This section describes building and maintaining the pyramid data structure. Initially, to build the pyramid, all
location-based ratings currently in the system are used
to build a complete pyramid of height H, such that all
cells in all H levels are α-Cells and contain ratings statistics and a collaborative filtering model. The initial height
H is chosen according to the level of locality desired,
where the cells in the lowest pyramid level represent the
most localized regions. After this initial build, we invoke
a cell type maintenance step that scans all cells starting
from the lowest level h and downgrades cell types to
either (β-Cell or γ -Cell) if necessary (cell type switching is discussed in Section 4.5.2). We note that while
the original partial pyramid [11] was concerned with

4.5 Maintenance Algorithm
Algorithm 1 provides the pseudocode for the LARS* maintenance algorithm. The algorithm takes as input a pyramid cell C and level h, and includes three main steps:
Statistics Maintenance, Model Rebuild and Cell Child Quadrant
Maintenance, explained below.
Step I: Statistics Maintenance. The first step (line 4) is
to maintain the Items Ratings Statistics Table. The maintained
statistics are necessary for cell type switching decision,
especially when new location-based ratings enter the system. As the items ratings statistics table is implemented using
a hash table, then it can be queried and maintained in O(1)
time, requiring O(|IC |) space such that IC is the set of all
items rated at cell C and |IC | is the total number of items
in IC .

Algorithm 1 Pyramid maintenance algorithm
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:

/* Called after cell C receives N% new ratings */
Function PyramidMaintenance(Cell C, Level h)
/* Step I: Statistics Maintenance*/
Maintain cell C statistics
/*Step II: Model Rebuild */
if (Cell C is an α -Cell) then
Rebuild item-based collaborative filtering model for cell C
end if
/*Step III: Cell Child Quadrant Maintenance */
if (C children quadrant q cells are α -Cells) then
CheckDownGradeToSCells(q,C) /* covered in Section 4.5.2 */
else if (C children quadrant q cells are γ -Cells) then
CheckUpGradeToSCells(q,C)
else
isSwitchedToMcells ← CheckUpGradeToMCells(q,C) /* covered in
Section 4.5.3 */
if (isSwitchedToMcells is False) then
CheckDownGradeToECells(q,C)
end if
end if
return

spatial queries over static data, it did not address pyramid
maintenance.

1390

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

Step II: Model Rebuild. The second step is to rebuild the
item-based collaborative filtering (CF) model for a cell C, as
described in Section 2.2 (line 7). The model is rebuilt at cell
C only if cell C is an α-Cell, otherwise (β-Cell or γ -Cell)
no CF recommendation model is maintained, and hence
the model rebuild step does not apply. Rebuilding the CF
model is necessary to allow the model to “evolve" as new
location-based ratings enter the system (e.g., accounting for
new items, ratings, or users). Given the cost of building the
2
item-based CF model is O( RU ) (per Section 2.2), the cost
of the model rebuild for a cell C at level h is

(R/4h )2
(U/4h )

(a)

R2

= 4h U ,

assuming ratings and users are uniformly distributed.
Step III: Cell Child Quadrant Maintenance. LARS*
invokes a maintenance step that may decide whether cell
C child quadrant need to be switched to a different cell
type based on trade-offs between scalability and locality.
The algorithm first checks if cell C child quadrant q at
level h + 1 is of type α-Cell (line 10). If that case holds,
LARS* considers quadrant q cells as candidates to be downgraded to β-Cells (calling function CheckDownGradeToSCells
on line 11). We provide details of the Downgrade α-Cells to
β-Cells operation in Section 4.5.2. On the other hand, if C
have a child quadrant of type γ -Cells at level h+1 (line 12),
LARS* considers upgrading cell C four children cells at
level h + 1 to β-Cells (calling function CheckUpGradeToSCells
on line 13). The Upgrade to β-Cells operation is covered in
Section 4.5.4. However, if C has a child quadrant of type βCells at level h+1 (line 12), LARS* first considers upgrading
cell C four children cells at level h + 1 from β-Cells to αCells (calling function CheckUpGradeToMCells on line 15). If
the children cells are not switched to α-Cells, LARS* then
considers downgrading them to γ -Cells (calling function
CheckDownGradeToECells on line 17). Cell Type switching
operations are performed completely in quadrants (i.e., four
equi-area cells with the same parent). We made this decision
for simplicity in maintaining the partial pyramid.

4.5.1 Recommendation Locality
In this section, we explain the notion of locality in recommendation that is essential to understand the cell type
switching (upgrade/downgrade) operations highlighted in
the PyramidMaintenance algorithm (algorithm 1). We use
the following example to give the intuition behind recommendation locality.
Running Example. Fig. 6 depicts a two-levels pyramid
in which Cp is the root cell and its children cells are C1 , C2 ,
C3 , and C4 . In the example, we assume eight users (U1 , U2 ,
. . . , and U8 ) have rated eight different items (I1 , I2 , . . . , and
I8 ). Fig. 6(b) gives the spatial distributions of users U1 , U2 ,
U3 , U4 , U5 , U6 , U7 , and U8 as well as the items that each
user rated.
Intuition. Consider the example given in Fig. 6. In cell
Cp , users U2 and U5 that belongs to the child cell C2 have
both rated items I2 and I5 . In that case, the similarity score
between items I2 and I5 in the item-based collaborative filtering CF model built at cell C2 is exactly the same as the
one in the CF model built at cell Cp . The last phenomenon
happened because items (i.e., I2 and I5 ) have been rated
by mostly users located in the same child cell, and hence
the recommendation model at the parent cell will not be

(b)

(c)
Fig. 6. Item ratings spatial distribution example. (a) Two-levels
pyramid. (b) Ratings distribution and recommendation models.
(c) Locality loss/gain at C_p.

different from the model at the children cells. In this case,
if the CF model at C2 is not maintained, LARS* does not
lose recommendation locality at all.
The opposite case happens when an item is rated by
users located in different pyramid cells (spatially skewed).
For example, item I4 is rated by users U2 , U4 , and U7 in
three different cells (C2 , C3 , and C4 ). In this case, U2 , U4 ,
and U7 are spatially skewed. Hence, the similarity score
between item I4 and other items at the children cells is different from the similarity score calculated at the parent cell
Cp because not all users that have rated item I4 exist in the
same child cell. Based on that, we observe the following:

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

TABLE 2
Summary of Mathematical Notations

Definition 2. Locality Loss (LGc )
LGc is defined as the total locality lost by downgrading cell c
four children cells to β-Cells (0 ≤ LGc ≤ 1). It is calculated
as the sum of all items locality loss normalized by the total
number of items |Ic | in cell c.

i∈Ic LGc,i
.
(4)
LGc =
|Ic |

Observation 1. The more the user/item ratings in a parent cell
C are geographically skewed, the higher the locality gained
from building the item-based CF model at the four children
cells.
The amount of locality gained/lost by maintaining the
child cells of a given pyramid cell depends on whether
the CF models at the child cells are similar to the CF
model built at the parent cell. In other words, LARS*
loses locality if the child cells are not maintained even
though the CF model at these cells produce different recommendations than the CF model at the parent cell. LARS*
leverages Observation 1 to determine the amount of locality
gained/lost due to maintaining an item-based CF model at
the four children. LARS* calculates the locality loss/gain as
follows:
Locality Loss/Gain. Table 2 gives the main mathematical notions used in calculating the recommendation locality
loss/gain. First, the Item Ratings Pairs Set (RPc,i ) is defined
as the set of all possible pairs of users that rated item i
in cell c. For example, in Fig. 6(c) the item ratings pairs
set for item I7 in cell Cp (RPCp ,I7 ) has three elements
(i.e., RPCp ,I7 ={
U3 , U6 ,
U3 , U7 ,
U6 , U7 }) as only users U1
and U7 have rated item I1 . Similarly, RPCp ,I2 is equal to
{
U6 , U7 } (i.e., Users U2 and U5 have rated item I2 ).
For each item, we define the Skewed Item Ratings Set
(RSc,i ) as the total number of user pairs in cell c that rated
item i such that each pair of users ∈ RSc,i do not exist
in the same child cell of c. For example, in Fig. 6(c), the
skewed item ratings set for item I2 in cell Cp (RSCP ,I2 ) is ∅
as all users that rated I2 , i.e., U2 and U5 are collocated in
the same child cell C2 . For I4 , the skewed item ratings set
RSCP ,I2 ={
U2 , U7 , 
U2 , U4 , 
U4 , U7 } as all users that rated
item I2 are located in different child cells,i.e., U2 at C2 , U4
at C4 , and U7 at C3 .
Given the aforementioned parameters, we calculate Item
Locality Loss (LGc,i ) for each item, as follows:
Definition 1. Item Locality Loss (LGc,i )
LGc,i is defined as the degree of locality lost for item i from
downgrading the four children of cell c to β-Cells, such that
0 ≤ LGc,i ≤ 1.
LGc,i =

|RSc,i |
.
|RPc,i |

1391

(3)

The value of both |RSc,i | and |RPc,i | can be easily extracted
using the items ratings statistics table. Then, we use the LGc,i
values calculated for all items in cell c in order to calculate
the overall Cell Locality loss (LGc ) from downgrading the
children cells of c to α-Cells.

The cell locality loss (or gain) is harnessed by LARS*
to determine whether the cell children need to be downgraded from α-Cell to β-Cell rank, upgraded from the
γ -Cell to β-Cell rank, or downgraded from β-Cell to γ -Cell
rank. During the rest of section 4, we explain the cell rank
upgrade/downgrade operations.

4.5.2 Downgrade α-Cells to β-Cells
That operation entails downgrading an entire quadrant of
cells from α-Cells to β-Cells at level h with a common parent at level h − 1. Downgrading α-Cells to β-Cells improves
scalability (i.e., storage and computational overhead) of
LARS*, as it reduces storage by discarding the item-based
collaborative filtering (CF) models of the the four children cells. Furthermore, downgrading α-Cells to β-Cells
leads to the following performance improvements: (a) less
maintenance cost, since less CF models are periodically
rebuilt, and (b) less continuous query processing computation,
as β-Cells do not maintain a CF model and if many β-Cells
cover a large spatial region, hence, for users crossing β-Cells
boundaries, we do not need to update the recommendation
query answer. Downgrading children cells from α-Cells to
β-Cells might hurt recommendation locality, since no CF
models are maintained at the granularity of the child cells
anymore.
At cell Cp , in order to determine whether to downgrade a quadrant q cells to β-Cells (i.e., function
CheckDownGradeToSCells on line 11 in Algorithm 1), we
calculate two percentage values: (1) locality_loss (see
equation 4), the amount of locality lost by (potentially)
downgrading the children cells to β-Cells, and (2) scalability_gain, the amount of scalability gained by (potentially)
downgrading the children cells to β-Cells. Details of calculating these percentages are covered next. When deciding
to downgrade cells to β-Cells, we define a system parameter M, a real number in the range [0,1] that defines a
tradeoff between scalability gain and locality loss. LARS*
downgrades a quadrant q cells to β-Cells (i.e., discards
quadrant q) if:
(1 − M) ∗ scalability_gain > M ∗ locality_loss.

(5)

A smaller M value implies gaining scalability is important and the system is willing to lose a large amount of
locality for small gains in scalability. Conversely, a larger
M value implies scalability is not a concern, and the
amount of locality lost must be small in order to allow
for β-Cells downgrade. At the extremes, setting M=0 (i.e.,
always switch to β-Cell) implies LARS* will function as
a traditional CF recommender system, while setting M=1
causes LARS* pyramid cells to all be α-Cells, i.e., LARS*
will employ a complete pyramid structure maintaining a
recommendation model at all cells at all levels.

1392

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

Calculating Locality Loss. To calculate the locality loss
at a cell Cp , LARS* leverages the Item Ratings Statistics Table
maintained in that cell. First, LARS* calculates the item
locality loss LGCp ,i for each item i in the cell Cp . Therefore,
LARS* aggregates the item locality loss values calculated
for each item i ∈ Cp , to finally deduce the global cell locality
loss LGCp .
Calculating scalability gain. Scalability gain is measured in storage and computation savings. We measure
scalability gain by summing the recommendation model
sizes for each of the downgraded (i.e., child) cells (abbr.
sizem ), and divide this value by the sum of sizem and the recommendation model size of the parent cell. We refer to this
percentage as the storage_gain. We also quantify computation
savings using storage gain as a surrogate measurement, as
computation is considered a direct function of the amount
of data in the system.
Cost. using the Items Ratings Statistics Table maintained
at cell Cp , the locality loss at cell Cp can be calculated in
O(|ICp |) time such that |ICp | represents the total number of
items in Cp . As scalability gain can be calculated in O(1)
time, then the total time cost of the Downgrade To β-Cells
operation is O(|ICp |).
Example. For the example given in Fig. 6(c), the locality loss of downgrading cell Cp four children cells
{C1 , C2 , C3 , C4 } to β-Cells is calculated as follows: First,
we retrieve the locality loss LGCp ,i for each item i ∈
{I1 , I2 , I3 , I4 , I5 , I6 , I7 , I8 }, from the maintained statistics at
cell Cp . As given in Fig. 6(c), LGCp ,I1 , LGCp ,I2 , LGCp ,I3 ,
LGCp ,I4 , LGCp ,I5 , LGCp ,I6 , LGCp ,I7 , and LGCp ,I8 are equal to 0.0,
0.0, 1.0, 1.0, 0.0, 0.666, 0.166, and 1.0, respectively. Then, we
calculate the overall locality loss at Cp (using equation 4),
LGCp by summing all the locality loss values of all items
and dividing the sum by the total number of items. Hence,
the scalability loss is equal to ( 0.0+1.0+1.0+0.0+0.666+0.1666+1.0
)
8
= 0.48 = 48%. To calculate scalability gain, assume the sum
of the model sizes for cells C1 to C4 and CP is 4GB, and
the sum of the model sizes for cells C1 to C4 is 2GB.
Then, the scalability gain is 24 =50%. Assuming M=0.7, then
(0.3 × 50) < (0.7 × 48), meaning that LARS* will not
downgrade cells C1 , C2 , C3 , C4 to β-Cells.

4.5.3 Upgrade β-Cells to α-Cells
Upgrading β-Cells to α-Cells operation entails upgrading the
cell type of a cell child quadrant at pyramid level h under
a cell at level h − 1, to α-Cells. Upgrading β-Cells to α-Cells
operation improves locality in LARS*, as it leads to maintaining a CF model at the children cells that represent
more granular spatial regions capable of producing recommendations unique to the smaller, more “local", spatial
regions. On the other hand, upgrading cells to α-Cells hurts
scalability by requiring storage and maintenance of more
item-based collaborative filtering models. The upgrade to
α-Cells operation also negatively affects continuous query
processing, since it creates more granular α-Cells causing user locations to cross α-Cell boundaries more often,
triggering recommendation updates.
To determine whether to upgrade a cell CP (quadrant q) four children cells to α-Cells (i.e., function
CheckUpGradeToMCells on line 15 of Algorithm 1). Two percentages are calculated: locality_gain and scalability_loss.

These values are the opposite of those calculated for the
Upgrade to β-Cells operation. LARS* change cell CP child
quadrant q to α-Cells only if the following condition holds:
M ∗ locality_gain > (1 − M) ∗ scalability_loss.

(6)

This equation represents the opposite criteria of that presented for Upgrade to β-Cells operation in Equation 5.
Calculating locality gain. To calculate the locality gain,
LARS* does not need to speculatively build the CF model
at the four children cells. The locality gain is calculated the
same way the locality loss is calculated in equation 4.
Calculating scalability loss. We calculate scalability loss
by estimating the storage necessary to maintain the children cells. Recall from Section 2.2 that the maximum size
of an item-based CF model is approximately n|I|, where n
is the model size. We can multiply n|I| by the number of
bytes needed to store an item in a CF model to find an
upper-bound storage size of each potentially Upgradeded to
α-Cell cell. The sum of these four estimated sizes (abbr. sizes )
divided by the sum of the size of the existing parent cell
and sizes represents the scalability loss metric.
Cost. Similar to the CheckDownGradeToSCells operation,
scalability loss is calculated in O(1) and locality gain can
be calculated in O(|ICp |) time. Then, the total time cost of
the CheckUpGradeToMCells operation is O(|ICp |).
Example. Consider the example given in Fig. 6(c).
Assume the cell Cp is an α-Cell and its four children C1 ,
C2 , C3 , and C4 are β-Cells. The locality gain (LGCp ) is calculated using equation 4 to be 0.48 (i.e., 48%) as depicted in
the table in Fig. 6(c). Further, assume that we estimate the
extra storage overhead for upgradinging the children cells
to α-Cells (i.e., storage loss) to be 50%. Assuming M=0.7,
then (0.7 × 48) > (0.3 × 50), meaning that LARS* will
decide to upgrade CP four children cells to α-Cells as locality
gain is significantly higher than scalability loss.

4.5.4 Downgrade β-Cells to γ -Cells and Vice Versa
Downgrading β-Cells to γ -Cells operation entails downgrading the cell type of a cell child quadrant at pyramid level h
under a cell at level h − 1, to γ -Cells (i.e., empty cells).
Downgrading the child quadrant type to γ -Cells means
that the maintained statistics are no more maintained in
the children cell, which definitely reduces the overhead of
maintaining the Item Ratings Statistics Table at these cells.
Even though γ -Cells incurs no maintenance overhead, however they reduce the amount of recommendation locality
that LARS* provides.
The decision of downgrading from β-Cells to γ -Cells is
taken based on a system parameter, named MAX_SLEVELS.
It is defined as the maximum number of consecutive
pyramid levels in which descendant cells can be β-Cells.
MAX_SLEVELS can take any value between zero and the
total height of the pyramid. A high value of MAX_SLEVELS
results in maintaining more β-Cells and less γ -Cells in
the pyramid. For example, in Fig. 4, MAX_SLEVELS is
set to two, and this is why if two consecutive pyramid
levels are β-Cells, the third level β-Cells are autotmatically downgraded to γ -Cells. For each β-Cell C, a counter,
called S-Levels Counter, is maintained. The S-Levels Counter
stores of the total number of consecutive levels in the direct
ancestry of cell C such that all these levels contains β-Cells.

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

At a β-Cell C, if the cell children are β-Cells, then we
compare the S-Levels Counter at the child cells with the
MAX_SLEVELS parameter. Note that the counter counts
only the consecutive S-Levels, so if some levels in the chain
are α-Cells the counter is reset to zero at the α-Cells levels. If
S-Levels Counter is greater than or equal to MAX_SLEVELS,
then the children cells of C are downgraded to γ -Cells.
Otherwise, cell C children cells are not downgraded to
γ -Cells. Similarly, LARS* also makes use of the same
S-Levels Counter to decide whether to upgrade γ -Cells to
β-Cells.

5

N ON -S PATIAL U SER R ATINGS FOR S PATIAL
I TEMS

This section describes how LARS* produces recommendations using non-spatial ratings for spatial items represented
by the tuple (user, rating, item, ilocation). The idea is to
exploit travel locality, i.e., the observation that users limit
their choice of spatial venues based on travel distance
(based on analysis in Section 1.1). Traditional (non-spatial)
recommendation techniques may produce recommendations with burdensome travel distances (e.g., hundreds
of miles away). LARS* produces recommendations within
reasonable travel distances by using travel penalty, a technique that penalizes the recommendation rank of items the
further in travel distance they are from a querying user.
Travel penalty may incur expensive computational overhead
by calculating travel distance to each item. Thus, LARS*
employs an efficient query processing technique capable
of early termination to produce the recommendations without calculating the travel distance to all items. Section 5.1
describes the query processing framework while Section 5.2
describes travel distance computation.

5.1 Query Processing
Query processing for spatial items using the travel penalty
technique employs a single system-wide item-based collaborative filtering model to generate the top-k recommendations by ranking each spatial item i for a querying user u
based on RecScore(u, i), computed as:
RecScore(u, i) = P(u, i) − TravelPenalty(u, i).

(7)

P(u, i) is the standard item-based CF predicted rating of
item i for user u (see Section 2.2). TravelPenalty(u, i) is the
road network travel distance between u and i normalized
to the same value range as the rating scale (e.g., [0, 5]).
When processing recommendations, we aim to avoid calculating Equation 7 for all candidate items to find the top-k
recommendations, which can become quite expensive given
the need to compute travel distances. To avoid such computation, we evaluate items in monotonically increasing
order of travel penalty (i.e., travel distance), enabling us to
use early termination principles from top-k query processing [18]–[20]. We now present the main idea of our query
processing algorithm and in the next section discuss how
to compute travel penalties in an increasing order of travel
distance.
Algorithm 2 provides the pseudo code of our query
processing algorithm that takes a querying user id U, a

1393

Algorithm 2 Travel Penalty Algorithm for Spatial Items
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:

Function LARS*_SpatialItems(User U, Location L, Limit K)
/* Populate a list R with a set of K items*/
R←φ
for (K iterations) do
i ← Retrieve the item with the next lowest travel penalty (Section 5.2)
Insert i into R ordered by RecScore(U, i) computed by Equation 7
end for
LowestRecScore ← RecScore of the kth object in R
/*Retrieve items one by one in order of their penalty value */
while there are more items to process do
i ← Retrieve the next item in order of penalty score (Section 5.2)
MaxPossibleScore ← MAX_RATING - i.penalty
if MaxPossibleScore ≤ LowestRecScore then
return R /* early termination - end query processing */
end if
RecScore(U, i) ← P(U, i) - i.penalty /* Equation 7 */
if RecScore(U, i) > LowestRecScore then
Insert i into R ordered by RecScore(U, i)
LowestRecScore ← RecScore of the kth object in R
end if
end while
return R

location L, and a limit K as input, and returns the list R of
top-k recommended items. The algorithm starts by running
a k-nearest-neighbor algorithm to populate the list R with
k items with lowest travel penalty; R is sorted by the recommendation score computed using Equation 7. This initial
part is concluded by setting the lowest recommendation
score value (LowestRecScore) as the RecScore of the kth item
in R (Lines 3 to 8). Then, the algorithm starts to retrieve
items one by one in the order of their penalty score. This
can be done using an incremental k-nearest-neighbor algorithm, as will be described in the next section. For each item
i, we calculate the maximum possible recommendation score
that i can have by subtracting the travel penalty of i from
MAX_RATING, the maximum possible rating value in the
system, e.g., 5 (Line 12). If i cannot make it into the list
of top-k recommended items with this maximum possible
score, we immediately terminate the algorithm by returning R as the top-k recommendations without computing the
recommendation score (and travel distance) for more items
(Lines 13 to 15). The rationale here is that since we are
retrieving items in increasing order of their penalty and calculating the maximum score that any remaining item can
have, then there is no chance that any unprocessed item
can beat the lowest recommendation score in R. If the early
termination case does not arise, we continue to compute
the score for each item i using Equation 7, insert i into
R sorted by its score (removing the kth item if necessary),
and adjust the lowest recommendation value accordingly
(Lines 16 to 20).
Travel penalty requires very little maintenance. The only
maintenance necessary is to occasionally rebuild the single system-wide item-based collaborative filtering model
in order to account for new location-based ratings that
enter the system. Following the reasoning discussed in
Section 4.3, we rebuild the model after receiving N% new
ratings.

5.2 Incremental Travel Penalty Computation
This section gives an overview of two methods we implemented in LARS* to incrementally retrieve items one by one
ordered by their travel penalty. The two methods exhibit a

1394

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

trade-off between query processing efficiency and penalty
accuracy: (1) an online method that provides exact travel
penalties but is expensive to compute, and (2) an offline
heuristic method that is less exact but efficient in penalty
retrieval. Both methods can be employed interchangeably
in Line 11 of Algorithm 2.

5.2.1 Incremental KNN: An Exact Online Method
To calculate an exact travel penalty for a user u to item i,
we employ an incremental k-nearest-neighbor (KNN) technique [21]–[23]. Given a user location l, incremental KNN
algorithms return, on each invocation, the next item i nearest to u with regard to travel distance d. In our case, we
normalize distance d to the ratings scale to get the travel
penalty in Equation 7. Incremental KNN techniques exist
for both Euclidean distance [22] and (road) network distance [21], [23]. The advantage of using Incremental KNN
techniques is that they provide an exact travel distances
between a querying user’s location and each recommendation candidate item. The disadvantage is that distances
must be computed online at query runtime, which can be
expensive. For instance, the runtime complexity of retrieving a single item using incremental KNN in Euclidean space
is [22]: O(k + logN), where N and k are the number of total
items and items retrieved so far, respectively.
5.2.2 Penalty Grid: A Heuristic Offline Method
A more efficient, yet less accurate method to retrieve travel
penalties incrementally is to use a pre-computed penalty
grid. The idea is to partition space using an n × n grid. Each
grid cell c is of equal size and contains all items whose
location falls within the spatial region defined by c. Each
cell c contains a penalty list that stores the pre-computed
penalty values for traveling from anywhere within c to all
other n2 −1 destination cells in the grid; this means all items
within a destination grid cell share the same penalty value.
The penalty list for c is sorted by penalty value and always
stores c (itself) as the first item with a penalty of zero. To
retrieve items incrementally, all items within the cell containing the querying user are returned one-by-one (in any
order) since they have no penalty. After these items are
exhausted, items contained in the next cell in the penalty
list are returned, and so forth until Algorithm 2 terminates
early or processes all items.
To populate the penalty grid, we must calculate the
penalty value for traveling from each cell to every other cell
in the grid. We assume items and users are constrained to
a road network, however, we can also use Euclidean space
without consequence. To calculate the penalty from a single
source cell c to a destination cell d, we first find the average
distance to travel from anywhere within c to all item destinations within d. To do this, we generate an anchor point
p within c that both (1) lies on the road network segment
within c and (2) lies as close as possible to the center of
c. With these criteria, p serves as an approximate average
“starting point" for traveling from c to d. We then calculate
the shortest path distance from p to all items contained in
d on the road network (any shortest path algorithm can be
used). Finally, we average all calculated shortest path distances from c to d. As a final step, we normalize the average
distance from c to d to fall within the rating value range.

Normalization is necessary as the rating domain is usually
small (e.g., zero to five), while distance is measured in miles
or kilometers and can have large values that heavily influence Equation 7. We repeat this entire process for each cell
to all other cells to populate the entire penalty grid.
When new items are added to the system, their presence in a cell d can alter the average distance value used in
penalty calculation for each source cell c. Thus, we recalculate penalty scores in the penalty grid after N new items
enter the system. We assume spatial items are relatively
static, e.g., restaurants do not change location often. Thus,
it is unlikely existing items will change cell locations and in
turn alter penalty scores.

6

S PATIAL U SER R ATINGS FOR S PATIAL I TEMS

This section describes how LARS* produces recommendations using spatial ratings for spatial items represented by
the tuple (user, ulocation, rating, item, ilocation). A salient feature of LARS* is that both the user partitioning and travel
penalty techniques can be used together with very little
change to produce recommendations using spatial user
ratings for spatial items. The data structures and maintenance techniques remain exactly the same as discussed
in Sections 4 and 5; only the query processing framework requires a slight modification. Query processing uses
Algorithm 2 to produce recommendations. However, the
only difference is that the item-based collaborative filtering
prediction score P(u, i) used in the recommendation score
calculation (Line 16 in Algorithm 2) is generated using the
(localized) collaborative filtering model from the partial
pyramid cell that contains the querying user, instead of the
system-wide collaborative filtering model as was used in
Section 5.

7

E XPERIMENTS

This section provides experimental evaluation of LARS*
based on an actual system implementation using C++ and
STL. We compare LARS* with the standard item-based
collaborative filtering technique along with several variations of LARS*. We also compare LARS* to LARS [8].
Experiments are based on three data sets:
Foursquare: a real data set consisting of spatial user
ratings for spatial items derived from Foursquare user histories. We crawled Foursquare and collected data for
1,010,192 users and 642,990 venues across the United States.
Foursquare does not publish each “check-in" for a user,
however, we were able to collect the following pieces of
data: (1) user tips for a venue, (2) the venues for which the
user is the mayor, and (3) the completed to-do list items for
a user. In addition, we extracted each user’s friend list.
Extracting location-based ratings. To extract spatial user
ratings for spatial items from the Foursquare data (i.e.,
the five-tuple (user, ulocation, rating, item, ilocation)), we
map each user visit to a single location-based rating. The
user and item attributes are represented by the unique
Foursquare user and venue identifier, respectively. We
employ the user’s home city in Foursquare as the ulocation attribute. Meanwhile, the ilocation attribute is the item’s
inherent location. We use a numeric rating value range of
[1, 3], translated as follows: (a) 3 represents the user is the

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

“mayor” of the venue, (b) 2 represents that the user left a
“tip” at the venue, and (c) 1 represents the user visited the
venue as a completed “to-do” list item. Using this scheme,
a user may have multiple ratings for a venue, in this case
we use the highest rating value.
Data properties. Our experimental data consisted of 22,390
location-based ratings for 4K users for 2K venues all from
the state of Minnesota, USA. We used this reduced data
set in order to focus our quality experiments on a dense
rating sample. Use of dense ratings data has been shown
to be a very important factor when testing and comparing
recommendation quality [17], since use of sparse data (i.e.,
having users or items with very few ratings) tends to cause
inaccuracies in recommendation techniques.
MovieLens: a real data set consisting of spatial user ratings for non-spatial items taken from the popular MovieLens
recommender system [5]. The Foursquare and MovieLens
data are used to test recommendation quality. The
MovieLens data used in our experiments was real movie
rating data taken from the popular MovieLens recommendation system at the University of Minnesota [5]. This data
consisted of 87,025 ratings for 1,668 movies from 814 users.
Each rating was associated with the zip code of the user
who rated the movie, thus giving us a real data set of spatial
user ratings for non-spatial items.
Synthetic: a synthetically generated data set consisting
of spatial user ratings for spatial items for venues in the
state of Minnesota, USA. The synthetic data set we use
in our experiments is generated to contain 2000 users and
1000 items, and 500,000 ratings. Users and items locations
are randomly generated over the state of Minnesota, USA.
Users’ ratings to items are assigned random values between
zero and five. As this data set contains a number of ratings
that is about twenty five times and five times larger than the
foursquare data set and the Movilens data set, we use such
synthetic data set to test scalability and query efficiency.
Unless mentioned otherwise, the default value of M is
0.3, k is 10, the number of pyramid levels is 8, the influence
level is the lowest pyramid level, and MAX_SLEVELS is set
to two. The rest of this section evaluates LARS* recommendation quality (Section 7.1), trade-offs between storage and
locality (Section 7.4), scalability (Section 7.5), and query processing efficiency (Section 7.6). As the system stores its data
structures in main memory, all reported time measurements
represent the CPU time.

7.1

Recommendation Quality for Varying Pyramid
Levels
These experiments test the recommendation quality
improvement that LARS* achieves over the standard (nonspatial) item-based collaborative filtering method using
both the Foursquare and MovieLens data. To test the
effectiveness of our proposed techniques, we test the
quality improvement of LARS* with only travel penalty
enabled (abbr. LARS*-T), LARS* with only user partitioning enabled and M set to one (abbr. LARS*-U), and
LARS* with both techniques enabled and M set to one
(abbr. LARS*). Notice that LARS*-T represents the traditional item-based collaborative filtering augmented with
the travel penalty technique (section 5) to take the distance

(a)

1395

(b)

Fig. 7. Quality experiments for varying locality. (a) Foursquare data.
(b) MovieLens data.

between the querying user and the recommended items
into account. We do not plot LARS with LARS* as both give
the same result for M=1, and the quality experiments are
meant to show how locality increases the recommendation
quality.
Quality Metric. To measure quality, we build each recommendation method using 80% of the ratings from each
data set. Each rating in the withheld 20% represents a
Foursquare venue or MovieLens movie a user is known
to like (i.e., rated highly). For each rating t in this 20%, we
request a set of k ranked recommendations S by submitting
the user and ulocation associated with t. We first calculate
the quality as the weighted sum of the number of occurrences of the item associated with t (the higher the better)
in S . The weight of an item is a value between zero and one
that determines how close the rank of this item from its real
rank. The quality of each recommendation method is calculated and compared against the baseline, i.e., traditional
item-based collaborative filtering. We finally report the ratio
of improvement in quality each recommendation method
achieves over the baseline. The rationale for this metric is
that since each withheld rating represents a real visit to a
venue (or movie a user liked), the technique that produces
a large number of correctly ranked answers that contain
venues (or movies) a user is known to like is considered of
higher quality.
Fig. 7(a) compares the quality improvement of each technique (over traditional collaborative filtering) for varying
locality (i.e., different levels of the adaptive pyramid) using
the Foursquare data. LARS*-T does not use the adaptive
pyramid, thus has constant quality improvement. However,
LARS*-T shows some quality improvement over traditional
collaborative filtering. This quality boost is due to that fact
that LARS*-T uses a travel penalty technique that recommends items within a feasible distance. Meanwhile, the
quality of LARS* and LARS*-U increases as more localized pyramid cells are used to produce recommendation,
which verifies that user partitioning is indeed beneficial and
necessary for location-based ratings. Ultimately, LARS* has
superior performance due to the additional use of travel
penalty. While travel penalty produces moderate quality gain,
it also enables more efficient query processing, which we
observe later in Section 7.6.
Fig. 7(b) compares the quality improvement of LARS*U over CF (traditional collaborative filtering) for varying
locality using the MovieLens data. Notice that LARS* gives
the same quality improvement as LARS*-U because LARS*T do not apply for this dataset since movies are not spatial.
Compared to CF, the quality improvement achieved by

1396

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

(a)

(b)

Fig. 8. Quality experiments for varying answer sizes. (a) Foursquare

data. (b) MovieLens data.

LARS*-U (and LARS*) increases when it produces movie
recommendations from more localized pyramid cells. This
behavior further verifies that user partitioning is beneficial
in providing quality recommendations localized to a querying user location, even when items are not spatial. Quality
decreases (or levels off for MovieLens) for both LARS*-U
and/or LARS* for lower levels of the adaptive pyramid.
This is due to recommendation starvation, i.e., not having
enough ratings to produce meaningful recommendations.

7.2 Recommendation Quality for Varying k
These experiments test recommendation quality improvement of LARS*, LARS*-U, and LARS*-T for different values
of k (i.e., recommendation answer sizes). We do not plot
LARS with LARS* as both gives the same result for M=1,
and the quality experiments are meant to show how the
degree of locality increases the recommendation quality.
We perform experiments using both the Foursquare and
MovieLens data. Our quality metric is exactly the same as
presented previously in Section 7.1.
Fig. 8(a) depicts the effect of the recommendation list size
k on the quality of each technique using the Foursquare
data set. We report quality numbers using the pyramid
height of four (i.e., the level exhibiting the best quality from
Section 7.1 in Fig. 7(a)). For all sizes of k from one to ten,
LARS* and LARS*-U consistently exhibit better quality. In
fact, LARS* consistently achieves better quality over CF for
all k. LARS*-T exhibits similar quality to CF for smaller k
values, but does better for k values of three and larger.
Fig. 8(b) depicts the effect of the recommendation list
size k on the quality of improvement of LARS*-U (and
LARS*) over CF using the MovieLens data. Notice that
LARS* gives the same quality improvement as LARS*-U
because LARS*-T do not apply for this dataset since movies
are not spatial. This experiment was run using a pyramid
hight of seven (i.e., the level exhibiting the best quality
in Fig. 7(b)). Again, LARS*-U (and LARS*) consistently
exhibits better quality than CF for sizes of K from one to ten.
7.3 Recommendation Quality for Varying M
These experiments compares the quality improvement
achieved by both LARS and LARS* for different values of
M. We perform experiments using both the Foursquare and
MovieLens data. Our quality metric is exactly the same as
presented previously in Section 7.1.
Fig. 9(a) depicts the effect of M on the quality of both
LARS and LARS* using the Foursquare data set. Notice that
we enable both the user partitioning and travel penalty

(a)

(b)

Fig. 9. Quality experiments for varying value of M. (a) Foursquare

data. (b) MovieLens data.

techniques for both LARS and LARS*. We report quality
numbers using the pyramid height of four and the number
of recommended items of ten. When M is equal to zero,
both LARS and LARS* exhibit the same quality improvement as M = 0 represents a traditional collaborative
filtering with the travel penalty technique applied. Also,
when M is set to one, both LARS and LARS* achieve
the same quality improvement as a fully maintained pyramid is maintained in both cases. For M values between
zero and one, the quality improvement of both LARS and
LARS* increases for higher values of M due to the increase
in recommendation locality. LARS* achieves better quality
improvement over LARS because LARS* maintains α-Cells
at lower levels of the pyramid.
Fig. 9(b) depicts the effect of M on the quality of
both LARS and LARS* using the Movilens data set.
We report quality improvement over traditional collaborative filtering using the pyramid height of seven and
the number of recommended items set to ten. Similar
to Foursquare data set, the quality improvement of both
LARS and LARS* increases for higher values of M due
to the increase in recommendation locality. For M values between zero and one, LARS* consistently achieves
higher quality improvement over LARS as LARS* maintains more α-Cells at more granular levels of the pyramid
structure.

7.4 Storage Vs. Locality
Fig. 10 depicts the impact of varying M on both the storage and locality in LARS* using the synthetic data set. We
plot LARS*-M=0 and LARS*-M=1 as constants to delineate
the extreme values of M, i.e., M=0 mirrors traditional collaborative filtering, while M=1 forces LARS* to employ a
complete pyramid. Our metric for locality is locality loss
(defined in Section 4.5.2) when compared to a complete

(a)

(b)

Fig. 10. Effect of M on storage and locality (synthetic data).

(a) Storage. (b) Locality.

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

(a)

(b)

Fig. 11. Scalability of the adaptive pyramid (synthetic data).

(a) Storage. (b) Maintenance.

pyramid (i.e., M=1). LARS*-M=0 requires the lowest storage overhead, but exhibits the highest locality loss, while
LARS*-M=1 exhibits no locality loss but requires the most
storage. For LARS*, increasing M results in increased storage overhead since LARS* favors switching cells to α-Cells,
requiring the maintenance of more pyramid cells each with
its own collaborative filtering model. Each additional αCell incurs a high storage overhead over the original data
size as an additional collaborative filtering model needs to
be maintained. Meanwhile, increasing M results in smaller
locality loss as LARS* merges less and maintains more
localized cells. The most drastic drop in locality loss is
between 0 and 0.3, which is why we chose M=0.3 as a
default. LARS* leads to smaller locality loss (≈26% less)
than LARS because LARS* maintains α-Cells below β-Cells
which result in higher locality gain. On the other hand,
LARS* exhibits slightly higher storage cost (≈5% more storage) than LARS due to the fact that LARS* stores the Item
Ratings Statistics Table per each α-Cell and β-Cell.

7.5 Scalability
Fig. 11 depicts the storage and aggregate maintenance overhead required for an increasing number of ratings using the
synthetic data set. We again plot LARS*-M=0 and LARS*M=1 to indicate the extreme cases for LARS*. Fig. 11(a)
depicts the impact of increasing the number of ratings from
10K to 500K on storage overhead. LARS*-M=0 requires
the lowest amount of storage since it only maintains a
single collaborative filtering model. LARS*-M=1 requires
the highest amount of storage since it requires storage
of a collaborative filtering model for all cells (in all levels) of a complete pyramid. The storage requirement of
LARS* is in between the two extremes since it merges
cells to save storage. Fig. 11(b) depicts the cumulative
computational overhead necessary to maintain the adaptive pyramid initially populated with 100K ratings, then
updated with 200K ratings (increments of 50K reported).
The trend is similar to the storage experiment, where
LARS* exhibits better performance than LARS*-M=1 due
to switching some cells from α-Cells to β-Cells. Though
LARS*-M=0 has the best performance in terms of maintenance and storage overhead, previous experiments show
that it has unacceptable drawbacks in quality/locality.
Compared to LARS, LARS* has less maintenance overhead
(≈38% less) due to the fact that the maintenance algorithm
in LARS* avoids the expensive speculative splitting used
by LARS.

(a)
12. Query processing performance
(a) Snapshot queries. (b) Continuous queries.

Fig.

1397

(b)
(synthetic

data).

7.6 Query Processing Performance
Fig. 12 depicts snapshot and continuous query processing performance of LARS, LARS*, LARS*-U (LARS* with
only user partitioning), LARS*-T (LARS* with only travel
penalty), CF (traditional collaborative filtering), and LARS*M=1 (LARS* with a complete pyramid), using the synthetic
data set.
Snapshot queries. Fig. 12(a) gives the effect of various
number of ratings (10K to 500K) on the average snapshot query performance averaged over 500 queries posed
at random locations. LARS* and LARS*-M=1 consistently
outperform all other techniques; LARS*-M=1 is slightly better due to recommendations always being produced from
the smallest (i.e., most localized) CF models. The performance gap between LARS* and LARS*-U (and CF and
LARS*-T) shows that employing the travel penalty technique
with early termination leads to better query response time.
Similarly, the performance gap between LARS* and LARS*T shows that employing user partitioning technique with
its localized (i.e., smaller) collaborative filtering model also
benefits query processing. LARS* performance is slightly
better than LARS as LARS* sometimes maintains more
localized CF models than LARS which incurs less query
processing time.
Continuous queries. Fig. 12(b) provides the continuous
query processing performance of the LARS* variants by
reporting the aggregate response time of 500 continuous
queries. A continuous query is issued once by a user u
to get an initial answer, then the answer is continuously
updated as u moves. We report the aggregate response
time when varying the travel distance of u from 1 to 30
miles using a random walk over the spatial area covered
by the pyramid. CF has a constant query response time for
all travel distances, as it requires no updates since only
a single cell is present. However, since CF is unaware
of user location change, the consequence is poor recommendation quality (per experiments from Section 7.1).
LARS*-M=1 exhibits the worse performance, as it maintains all cells on all levels and updates the continuous
query whenever the user crosses pyramid cell boundaries.
LARS*-U has a lower response time than LARS*-M=1 due
to switching cells from α-Cells to β-Cells: when a cell is
not present on a given influence level, the query is transferred to its next highest ancestor in the pyramid. Since
cells higher in the pyramid cover larger spatial regions,
query updates occur less often. LARS*-T exhibits slightly
higher query processing overhead compared to LARS*U: even though LARS*-T employs the early termination

1398

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

algorithm, it uses a large (system-wide) collaborative filtering model to (re)generate recommendations once users
cross boundaries in the penalty grid. LARS* exhibits a
better aggregate response time since it employs the early
termination algorithm using a localized (i.e., smaller) collaborative filtering model to produce results while also
switching cells to β-Cells to reduce update frequency. LARS
has a slightly better performance than LARS* as LARS
tends to merge more cells at higher levels in the pyramid
structure.

8

R ELATED W ORK

Location-based services. Current location-based services
employ two main methods to provide interesting destinations to users. (1) KNN techniques [22] and variants (e.g.,
aggregate KNN [24]) simply retrieve the k objects nearest
to a user and are completely removed from any notion of
user personalization. (2) Preference methods such as skylines [25] (and spatial variants [26]) and location-based
top-k methods [27] require users to express explicit preference constraints. Conversely, LARS* is the first locationbased service to consider implicit preferences by using
location-based ratings to help users discover new items.
Recent research has proposed the problem of hyper-local
place ranking [28]. Given a user location and query string
(e.g., “French restaurant"), hyper-local ranking provides a
list of top-k points of interest influenced by previously
logged directional queries (e.g., map direction searches
from point A to point B). While similar in spirit to LARS*,
hyper-local ranking is fundamentally different from our
work as it does not personalize answers to the querying user,
i.e., two users issuing the same search term from the same
location will receive exactly the same ranked answer.
Traditional recommenders. A wide array of techniques
are capable of producing recommendations using nonspatial ratings for non-spatial items represented as the triple
(user, rating, item) (see [4] for a comprehensive survey).
We refer to these as “traditional" recommendation techniques. The closest these approaches come to considering
location is by incorporating contextual attributes into statistical recommendation models (e.g., weather, traffic to
a destination) [29]. However, no traditional approach has
studied explicit location-based ratings as done in LARS*.
Some existing commercial applications make cursory use
of location when proposing interesting items to users. For
instance, Netflix displays a “local favorites” list containing popular movies for a user’s given city. However, these
movies are not personalized to each user (e.g., using recommendation techniques); rather, this list is built using
aggregate rental data for a particular city [30]. LARS*, on
the other hand, produces personalized recommendations
influenced by location-based ratings and a query location.
Location-aware recommenders. The CityVoyager system [31] mines a user’s personal GPS trajectory data to
determine her preferred shopping sites, and provides recommendation based on where the system predicts the user
is likely to go in the future. LARS*, conversely, does not
attempt to predict future user movement, as it produces
recommendations influenced by user and/or item locations
embedded in community ratings.

The spatial activity recommendation system [32] mines
GPS trajectory data with embedded user-provided tags in
order to detect interesting activities located in a city (e.g., art
exhibits and dining near downtown). It uses this data to
answer two query types: (a) given an activity type, return
where in the city this activity is happening, and (b) given
an explicit spatial region, provide the activities available
in this region. This is a vastly different problem than we
study in this paper. LARS* does not mine activities from
GPS data for use as suggestions for a given spatial region.
Rather, we apply LARS* to a more traditional recommendation problem that uses community opinion histories to
produce recommendations.
Geo-measured friend-based collaborative filtering [33]
produces recommendations by using only ratings that are
from a querying user’s social-network friends that live in
the same city. This technique only addresses user location
embedded in ratings. LARS*, on the other hand, addresses
three possible types of location-based ratings. More importantly, LARS* is a complete system (not just a recommendation technique) that employs efficiency and scalability
techniques (e.g., partial pyramid structure, early query termination) necessary for deployment in actual large-scale
applications.

9

C ONCLUSION

LARS*, our proposed location-aware recommender system,
tackles a problem untouched by traditional recommender
systems by dealing with three types of location-based
ratings: spatial ratings for non-spatial items, non-spatial ratings for spatial items, and spatial ratings for spatial items.
LARS* employs user partitioning and travel penalty techniques to support spatial ratings and spatial items, respectively. Both techniques can be applied separately or in
concert to support the various types of location-based ratings. Experimental analysis using real and synthetic data
sets show that LARS* is efficient, scalable, and provides
better quality recommendations than techniques used in
traditional recommender systems.

ACKNOWLEDGMENTS
This work was supported in part by the US National
Science Foundation under Grants IIS-0811998, IIS-0811935,
CNS-0708604, IIS-0952977 and in part by a Microsoft
Research Gift.

R EFERENCES
[1] G. Linden, B. Smith, and J. York, “Amazon.com recommendations: Item-to-item collaborative filtering,” IEEE Internet Comput.,
vol. 7, no. 1, pp. 76–80, Jan./Feb. 2003.
[2] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and J. Riedl,
“GroupLens: An open architecture for collaborative filtering of
netnews,” in Proc. CSWC, Chapel Hill, NC, USA, 1994.
[3] The facebook blog. Facebook Places [Online]. Available:
http://tinyurl.com/3aetfs3
[4] G. Adomavicius and A. Tuzhilin, “Toward the next generation of
recommender systems: A survey of the state-of-the-art and possible extensions,” IEEE Trans. Knowl. Data Eng., vol. 17, no. 6,
pp. 734–749, Jun. 2005.
[5] MovieLens [Online]. Available: http://www.movielens.org/
[6] Foursquare [Online]. Available: http://foursquare.com

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

[7] New York Times - A Peek into Netflix Queues [Online]. Available:
http://www.nytimes.com/interactive/2010/01/10/nyregion/
20100110-netflix-map.html
[8] J. J. Levandoski, M. Sarwat, A. Eldawy, and M. F. Mokbel,
“LARS: A location-aware recommender system,” in Proc. ICDE,
Washington, DC, USA, 2012.
[9] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl, “Item-based collaborative filtering recommendation algorithms,” in Proc. Int. Conf.
WWW, Hong Kong, China, 2001.
[10] J. S. Breese, D. Heckerman, and C. Kadie, “Empirical analysis
of predictive algorithms for collaborative filtering,” in Proc. Conf.
UAI, San Francisco, CA, USA, 1998.
[11] W. G. Aref and H. Samet, “Efficient processing of window queries
in the pyramid data structure,” in Proc. ACM Symp. PODS, New
York, NY, USA, 1990.
[12] R. A. Finkel and J. L. Bentley, “Quad trees: A data structure for
retrieval on composite keys,” Acta Inf., vol. 4, no. 1, pp. 1–9, 1974.
[13] A. Guttman, “R-trees: A dynamic index structure for spatial
searching,” in Proc. SIGMOD, New York, NY, USA, 1984.
[14] K. Mouratidis, S. Bakiras, and D. Papadias, “Continuous monitoring of spatial queries in wireless broadcast environments,” IEEE
Trans. Mobile Comput., vol. 8, no. 10, pp. 1297–1311, Oct. 2009.
[15] K. Mouratidis and D. Papadias, “Continuous nearest neighbor
queries over sliding windows,” IEEE Trans. Knowl. Data Eng.,
vol. 19, no. 6, pp. 789–803, Jun. 2007.
[16] M. F. Mokbel, X. Xiong, and W. G. Aref, “SINA: Scalable
incremental processing of continuous queries in spatiotemporal
databases,” in Proc. SIGMOD, Paris, France, 2004.
[17] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and J. T. Riedl,
“Evaluating collaborative filtering recommender systems,” ACM
TOIS, vol. 22, no. 1, pp. 5–53, 2004.
[18] M. J. Carey and D. Kossmann, “On saying "Enough Already!" in
SQL,” in Proc. SIGMOD, New York, NY, USA, 1997.
[19] S. Chaudhuri and L. Gravano, “Evaluating top-k selection
queries,” in Proc. Int. Conf. VLDB, Edinburgh, U.K., 1999.
[20] R. Fagin, A. Lotem, and M. Naor, “Optimal aggregation algorithms for middleware,” in Proc. ACM Symp. PODS, New York,
NY, USA, 2001.
[21] J. Bao, C.-Y. Chow, M. F. Mokbel, and W.-S. Ku, “Efficient evaluation of k-range nearest neighbor queries in road networks,” in
Proc. Int. Conf. MDM, Kansas City, MO, USA, 2010.
[22] G. R. Hjaltason and H. Samet, “Distance browsing in spatial
databases,” ACM TODS, vol. 24, no. 2, pp. 265–318, 1999.
[23] K. Mouratidis, M. L. Yiu, D. Papadias, and N. Mamoulis,
“Continuous nearest neighbor monitoring in road networks,” in
Proc. Int. Conf. VLDB, Seoul, Korea, 2006.
[24] D. Papadias, Y. Tao, K. Mouratidis, and C. K. Hui, “Aggregate
nearest neighbor queries in spatial databases,” ACM TODS,
vol. 30, no. 2, pp. 529–576, 2005.
[25] S. Börzsönyi, D. Kossmann, and K. Stocker, “The skyline operator,” in Proc. ICDE, Heidelberg, Germany, 2001.
[26] M. Sharifzadeh and C. Shahabi, “The spatial skyline queries,” in
Proc. Int. Conf. VLDB, Seoul, Korea, 2006.
[27] N. Bruno, L. Gravano, and A. Marian, “Evaluating top-k queries
over web-accessible databases,” in Proc. ICDE, San Jose, CA, USA,
2002.
[28] P. Venetis, H. Gonzalez, C. S. Jensen, and A. Y. Halevy, “Hyperlocal, directions-based ranking of places,” PVLDB, vol. 4, no. 5,
pp. 290–301, 2011.
[29] M.-H. Park, J.-H. Hong, and S.-B. Cho, “Location-based recommendation system using Bayesian user’s preference model in
mobile devices,” in Proc. Int. Conf. UIC, Hong Kong, China, 2007.
[30] Netflix News and Info - Local Favorites [Online]. Available:
http://tinyurl.com/4qt8ujo
[31] Y. Takeuchi and M. Sugimoto, “An outdoor recommendation system based on user location history,” in Proc. Int. Conf. UIC, Berlin,
Germany, 2006.
[32] V. W. Zheng, Y. Zheng, X. Xie, and Q. Yang, “Collaborative location and activity recommendations with GPS history data,” in
Proc. Int. Conf. WWW, New York, NY, USA, 2010.
[33] M. Ye, P. Yin, and W.-C. Lee, “Location recommendation for
location-based social networks,” in Proc. ACM GIS, New York,
NY, USA , 2010.

1399

Mohamed Sarwat is a Doctoral candidate
in the Department of Computer Science
and Engineering, University of Minnesota,
Minneapolis, MN, USA. He received the bachelor’s degree in computer engineering from
Cairo University, Egypt, in 2007 and the master’s
degree in computer science from the University
of Minnesota in 2011. His current research interests include a broad area of data management
systems, more specifically, database systems,
database support for recommender systems,
personalized databases, database support for location-based services
and for social networking applications, distributed graph databases,
and large scale data management. He was awarded the University of
Minnesota Doctoral Dissertation Fellowship in 2012. His research was
recognized by the Best Research Paper Award at the 12th International
Symposium on Spatial and Temporal Databases, in 2011.

Justin J. Levandoski is a Researcher with
the Database Group at Microsoft Research,
Redmond, WA, USA. He received the bachelor’s
degree at Carleton College, Northfield, MN, USA
in 2003, and the master’s and Ph.D. degrees
from the University of Minnesota, Minneapolis,
MN, USA, in 2008 and 2011, respectively. His
current research interests include a broad range
of topics dealing with large-scale data management systems, such as cloud computing,
database support for new hardware paradigms,
transaction processing, query processing, and support for new dataintensive applications such as social/recommender systems.

Ahmed Eldawy is a Ph.D. student in
the Department of Computer Science
and Engineering, University of Minnesota,
Minneapolis, MN, USA. His current research
interests include spatial data management,
social networks, and cloud computing, such
as building scalable spatial data management
systems over cloud computing platforms. He
received the bachelor’s and master’s degrees
in computer science from Alexandria University,
Egypt, in 2005 and 2010, respectively.

Mohamed F. Mokbel received the B.Sc. and
M.S. degrees from Alexandria University, Egypt,
in 1996 and 1999, respectively, and the
Ph.D. degree from Purdue University, West
Lafayette, IN, USA, in 2005. Currently, he is
an Assistant Professor in the Department of
Computer Science and Engineering, University
of Minnesota, Minneapolis, MN, USA. His current
research interests include advancing the stateof-the-art in the design and implementation of
database engines to cope with the requirements
of emerging applications (e.g., location-based applications and sensor
networks). His research works has been recognized by three Best Paper
Awards at IEEE MASS 2008, MDM 2009, and SSTD 2011. He is a recipient of the US NSF CAREER Award 2010. He has actively participated
in several program committees for major conferences including ICDE,
SIGMOD, VLDB, SSTD, and ACM GIS. He is/was a program Co-Chair
for ACM SIGSPATIAL GIS 2008, 2009, and 2010. He is an ACM and an
IEEE member and a founding member of ACM SIGSPATIAL.

 For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

The first ACM SIGSPATIAL PhD symposium 2014
The ACM SIGSPATIAL Ph.D. Symposium is a forum where Ph.D. students present, discuss, and receive feedback on their research in a constructive atmosphere. The symposium will be attended by professors, researchers and practitioners in the ACM SIGSPATIAL community, who will participate actively and contribute to the discussions. The workshop is co-located with ACM SIGSPATIAL GIS 2014 The ACM SIGSPATIAL 2014 PhD Symposium provides an opportunity for doctoral students to explore and develop their research interests in the broad areas addressed by the ACM SIGSPATIAL community. We invite PhD students to submit a summary of their dissertation work to share their work with students in a similar situation as well as senior researchers in the field. We have two tracks for submission. The Junior PhD Track is for students who are in early stages of their doctoral studies. The submission should provide a clear problem definition, explain why it is important, survey related work, and summarize the new solutions that are pursued. The Senior PhD Track is for students who are close to completion (expected to graduate by 2014/2015). The submissions focused on describing the contribution they made in their doctoral dissertation. The strongest candidates are those who have a clear topic and research approach, and have made some progress, but who are not so far along that they can no longer make changes.

Mobi Social (Mobile and Social) Data Management: A Tutorial
:
The rise of the Social Internet, in the past decade, stimulated the invention of human-centered technologies that study and serve humans as individuals and in groups. For instance, social networking services provide ways for individuals to connect and interact with their friends. Also, personalized recommender systems leverage the collaborative social intelligence of all users' opinions to recommend: books, news, movies, or products in general. These social technologies have been enhancing the quality of Internet services and enriching the end-user experience. Furthermore, the Mobile Internet allows hundreds of millions of users to frequently use their mobile devices to access their healthcare information and bank accounts, interact with friends, buy stuff online, search interesting places to visit on-the-go, ask for driving directions, and more. In consequence, everything we do on the Mob Social Internet leaves breadcrumbs of digital traces that, when managed and analyzed well, could definitely be leveraged to improve life. Services that leverage Mobile and/or Social data have become killer applications in the cloud. Nonetheless, a major challenge that Cloud Service providers face is how to manage (store, index, query) Mobi Social data hosted in the cloud. Unfortunately, classic data management systems are not well adapted to handle data-intensive Mobi Social applications. The tutorial surveys state-of-the-art Mobi Social data management systems and research prototypes from the following perspectives: (1) Geo-tagged Micro blog search, location-aware and mobile social news feed queries, and GeoSocial Graph search, (2) Mobile Recommendation Services, and (3) Geo-Crowd sourcing. We finally highlight the risks and threats (e.g., privacy) that result from combining mobility and social networking. We conclude the tutorial by summarizing and presenting open research directions.

RECATHON: A Middleware for Context-Aware Recommendation in Database Systems
:
This paper presents RECATHON, a context-aware recommender system built entirely inside a database system. Unlike traditional recommender systems that are context-free where they support the general query of Recommend movies for a certain user, RECATHON users can request recommendations based on their age, location, gender, or any other contextual/ demographical/preferential user attribute. A main challenge of supporting such kind of recommenders is the difficulty of deciding what attributes to build recommenders on. RECATHON addresses this challenge as it supports building recommenders in database systems in an analogous way to building index structures. Users can decide to create recommenders on selected attributes, e.g., Age and/or gender, and then entertain efficient support of multidimensional recommenders on the selected attributes. RECATHON employs a multi-dimensional index structure for each built recommender that can be accessed using novel query execution algorithms to support efficient retrieval for recommender queries. Experimental results based on an actual prototype of RECATHON, built inside Postgre SQL, using real Movie Lens and Foursquare data show that RECATHON exhibits real time performance for large-scale multidimensional recommendation.

Interactive and Scalable Exploration of Big Spatial Data - A Data Management Perspective
:
Recently, the volume of available spatial data increased tremendously. For instance, in November 2013 NASA announced the release of hundreds of Terabytes of its earth remote sensing dataset. Such data includes but not limited to: weather maps, socioeconomic data, vegetation indices, geological maps, and more. Making sense of such spatial data will be beneficial for several applications that may transform science and society -- For example: (1) Space Science: that allows astronomers to study and probably discover new features of both the earth and the outer space, (2) Socio-Economic Analysis: that includes for example climate change analysis, study of deforestation, population migration, and variation in sea levels, (3) Urban Planning: assisting government in city planning, road network design, and transportation engineering, (4) Disaster Planning: that helps in assessing the impact of natural disasters. The main aim of this paper is to investigate novel data management techniques that enable interactive and scalable exploration of big spatial data. The paper envisions novel system architectures that provide support for interactive and spatial data exploration, as follows: (1) The paper suggests extending data analytics frameworks, e.g., Apache Spark, to support spatial data types and operations at scale. The resulting framework will serve as a scalable backbone for processing spatial data exploration tasks. (2) It also sketches novel structures and algorithms that leverage modern hardware, e.g., SSDs, and in-memory data processing techniques to efficiently store and access spatial data. Second, the paper proposes extending spatial database systems to support an exploration-aware spatial query evaluation paradigm through three novel components: (1) Spatial Query Steering: that allows the user to slightly modify the query conditions online (zooming in/out) and retrieve the new results in very low latency. (2) Recommendation-Aware Spatial Querying: that injects the recommendation functionality inside classical spatial query executors to support spatial data recommendation. It leverages recommendation algorithms to predict what spatial objects/areas the user would like based on her past interactions with the system. (3) Spatial Query Approximation: That aims at achieving interactive performance by studying the tradeoff between approximate spatial data exploration and query response time.

Context Awareness in Mobile Systems
Context represents any information that can be used to characterize the situation of an entity. An entity is a person, place, or object that is considered relevant to the interaction between a user and an application, including the user and application themselves. The ubiquity of mobile devices (e.g., smartphones, GPS devices) has in part motivated the use of contextual information in modern mobile applications. From one perspective, context in mobile systems can fall into three categories: (a) user context that includes the personal attributes of the user, e.g., spatial location and budget; (b) point-of-interest (POI) context, e.g., restaurant location, operating time, and rating; and (c) environmental context, e.g., weather and road conditions. Incorporating such context in applications provided to mobile users may significantly enhance the quality of service in terms of finding more related answers. This chapter first gives a brief overview of context and context awareness in mobile systems. It then discusses different ways of expressing the spatial location context within mobile services. The chapter later describes three main application examples that can take advantage of various mobile contexts, namely, social news feed, microblogging (e.g., Twitter) and recommendation services. The chapter finally presents a generic method that incorporates context and user preference awareness in database systems—which may serve as a backbone for context-aware mobile applications.


GeoSpark: A Cluster Computing Framework for
Processing Large-Scale Spatial Data
Jia Yu

Jinxuan Wu

Mohamed Sarwat

School of Computing, Informatics,
and Decision Systems Engineering,
Arizona State University
699 S. Mill Avenue, Tempe, AZ

School of Computing, Informatics,
and Decision Systems Engineering,
Arizona State University
699 S. Mill Avenue, Tempe, AZ

School of Computing, Informatics,
and Decision Systems Engineering,
Arizona State University
699 S. Mill Avenue, Tempe, AZ

jiayu2@asu.edu

jinxuanw@asu.edu

msarwat@asu.edu

ABSTRACT
This paper introduces GeoSpark an in-memory cluster
computing framework for processing large-scale spatial data.
GeoSpark consists of three layers: Apache Spark Layer,
Spatial RDD Layer and Spatial Query Processing Layer.
Apache Spark Layer provides basic Spark functionalities
that include loading / storing data to disk as well as regular RDD operations. Spatial RDD Layer consists of three
novel Spatial Resilient Distributed Datasets (SRDDs) which
extend regular Apache Spark RDDs to support geometrical
and spatial objects. GeoSpark provides a geometrical operations library that accesses Spatial RDDs to perform basic
geometrical operations (e.g., Overlap, Intersect). System
users can leverage the newly deﬁned SRDDs to eﬀectively
develop spatial data processing programs in Spark. The
Spatial Query Processing Layer eﬃciently executes spatial
query processing algorithms (e.g., Spatial Range, Join, KNN
query) on SRDDs. GeoSpark also allows users to create a
spatial index (e.g., R-tree, Quad-tree ) that boosts spatial
data processing performance in each SRDD partition. Preliminary experiments show that GeoSpark achieves better
run time performance than its Hadoop-based counterparts
(e.g., SpatialHadoop).

Categories and Subject Descriptors
H.2.4 [DATABASE MANAGEMENT]: Systems—Distributed databases; H.2.8 [DATABASE MANAGEMENT]: Database Applications—Spatial databases and
GIS

Keywords
Cluster computing; Large-scale data; Spatial data

1. INTRODUCTION
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from Permissions@acm.org.
SIGSPATIAL’15 November 03-06, 2015, Bellevue, WA, USA
Copyright 2015 ACM ISBN 978-1-4503-3967-4/15/11 $15.00.
http://dx.doi.org/10.1145/2820783.2820860

The volume of available spatial data increased tremendously. Such data includes but not limited to: weather
maps, socioeconomic data, vegetation indices, and more.
Moreover, novel technology allows hundreds of millions of
users to use their mobile devices to access their healthcare
information and bank accounts, interact with friends, buy
stuﬀ online, search interesting places to visit on-the-go, ask
for driving directions, and more. Making sense of such spatial data will be beneﬁcial for several applications that may
transform science and society. Challenges to building such
platform are as follows: Challenge I: System Scalability. The
underlying database system must be able to digest Petabytes
of spatial data, eﬀectively stores it, and allows applications
to eﬃciently retrieve it when necessary. Challenge II: Interactive Performance. The underlying spatial data processing
system must ﬁgure out eﬀective ways to process user’s request in a sub-second response time.
Apache Spark is an in-memory cluster computing system.
Spark provides a novel data abstraction called resilient distributed datasets (RDDs) [9] that are collections of objects
partitioned across a cluster of machines. Each RDD is built
using parallelized transformations (ﬁlter, join or groupBy)
that could be traced back to recover the RDD data. In
memory RDDs allow Spark to outperform existing models
(MapReduce). Unfortunately, Spark does not provide support for spatial data and operations. Hence, users need to
perform the tedious task of programming their own spatial
data processing jobs on top of Spark.
This paper introduces GeoSpark 1 an in-memory cluster computing system for processing large-scale spatial data.
GeoSpark extends the core of Apache Spark to support
spatial data types, indexes, and operations. In other words,
the system extends the resilient distributed datasets (RDDs)
concept to support spatial data. The key contributions of
this paper are as follows: (1) GeoSpark as a full-ﬂedged
cluster computing framework to load, process, and analyze
large-scale spatial data in Apache Spark. (2) A set of out-ofthe-box Spatial Resilient Distributed Dataset (SRDD) types
(e.g., Point RDD and Polygon RDD) that provide in house
support for geometrical and distance operations. SRDDS
provides an Application Programming Interface (API) for
Apache Spark programmers to easily develop their spatial
analysis programs. (3) Spatial data indexing strategies that
partition the input Spatial RDD using a grid structure and
assign grids to machines for parallel execution. GeoSpark
1

GeoSpark website: http://geospark.datasyslab.org













	
	












Figure 2: SRDD partitioning

	






Figure 1: GeoSpark Overview

also adaptively decides whether a spatial index needs to be
created locally on a Spatial RDD partition to strike a balance between the run time performance and memory/cpu
utilization in the cluster. Experiments show that GeoSpark
achieves better run time performance than its Hadoop-based
counterparts (e.g., SpatialHadoop).
The rest of this paper is organized as follows. Section 2
highlights the related work. GeoSpark architecture is
given in Section 3. Preliminary experiments that evaluate
GeoSpark are given in Section 4. Finally, Section 5 concludes the paper.

2. BACKGROUND AND RELATED WORK
Spatial Database Systems. Spatial database operations are vital for spatial analysis and spatial data mining.
Spatial range queries inquire about certain spatial objects
exist in a certain area (e.g., Return all parks in Phoenix).
Spatial join queries are queries that combine two datasets
or more with a spatial predicate, such as distance relations
(e.g., ﬁnd the parks that have rivers in Phoenix). Spatial
k-Nearest Neighbors queries ﬁnd the k nearest objects to a
given spatial object (e.g., show the 10 nearby restaurants).
Spatial query processing algorithms usually make use of spatial indexes to reduce the query latency. For instance, RTree [3] provides an eﬃcient data partitioning strategy to
eﬃciently index spatial data. Its key idea is that group
nearby objects and put them in the next higher level node
of the tree. Quad-Tree [8] is also a spatial index that recursively divides a two-dimensional space into four quadrants.
Parallel and Distributed Spatial Data Processing.
As the development of distributed data processing system,
more and more people in geospatial area direct their attention to deal with massive geospatial data with distributed
frameworks. Hadoop-GIS [1] utilizes global partition indexing and customizable on demand local spatial indexing
to achieve eﬃcient query processing. SpatialHadoop [2], a
comprehensive extension to Hadoop, has native support for
spatial data by modifying the underlying code of Hadoop.
MD-HBase [6] extends HBase, a non-relational database

runs on top of Hadoop, to support multidimensional indexes
which allows for eﬃcient retrieval of points using range and
kNN queries. Parallel SECONDO [4] combines Hadoop with
SECONDO, a database which can handle non-standard data
types, like spatial data, usually not supported by standard
systems. Although these systems have well-developed functions, all of them are implemented on Hadoop framework.
That means they cannot avoid the disadvantages of Hadoop,
especially a large number of reads and writes on disks.

3.

GEOSPARK ARCHITECTURE

As depicted in Figure 1, GeoSpark consists of three main
layers: (1) Apache Spark Layer: that consists of regular
operations that are natively supported by Apache Spark.
These native functions are responsible for loading / saving
data from / to persistent storage (e.g., stored on local disk or
Hadoop ﬁle system HDFS). (2) Spatial Resilient Distributed
Dataset (SRDD) Layer (Section 3.1). (3) Spatial Query Processing Layer (Section 3.2).

3.1

Spatial RDD (SRDD) Layer

This layer extends Spark with spatial RDDs (SRDDs)
that eﬃciently partition SRDD data elements across machines and introduces novel parallelized spatial transformations and actions (for SRDD) that provide a more intuitive
interface for users to write spatial data analytics programs.
The SRDD layer consists of three new RDDs: PointRDD,
RectangleRDD and PolygonRDD. One useful Geometrical
operations library is also provided for every spatial RDD.
Spatial Objects Support. GeoSpark supports various
spatial data input format (e.g., Comma Separated Value,
Tab Separated Value and Well-Known Text). Each type
of spatial objects is stored in a SRDD, PointRDD, RectangleRDD or PolygonRDD. GeoSpark provides a set of
geometrical operations which is called Geometrical Operations Library. This library natively supports geometrical
operations. For example, Overlap(): Finds all of the internal objects which are intersected with others in geometry;
MinimumBoundingRectangle(): Finds the minimum bounding rectangles for each object in a Spatial RDD or return
a large minimum bounding rectangle which contains all of
the internal objects in a Spatial RDD; Union(): Returns the
union polygon of all polygons in this RDD.
SRDD Partitioning. GeoSpark automatically partitions all loaded Spatial RDDs by creating one global grid
ﬁle for data partitioning. The main idea for assigning each
element in a Spatial RDD to the same 2-Dimensional spatial
grid space is as follows: Firstly, split the spatial space into a




	

  







Figure 3: Query execution model

number of equal geographical size grid cells which compose
a global grid ﬁle. Then traverse each element in the SRDD
and assign this element to a grid cell if the element overlaps with this grid cell. If one element intersects with two
or more grid cells, then duplicate this element and assign
diﬀerent grid IDs to the copies of this element. Figure 2 depicts tweets in the U.S. at a particular moment, tweets and
states are assigned to respective grid cells.
SRDD Indexing. Spatial indexes like Quad-Tree and
R-Tree are provided in Spatial IndexRDDs which inherit
from Spatial RDDs. Users are able to initialize a Spatial IndexRDD. Moreover, GeoSpark adaptively decides whether
a local spatial index should be created for a certain Spatial IndexRDD partition based on a tradeoﬀ between the
indexing overhead (memory and time) on one-hand and the
query selectivity as well as the number of spatial objects on
the other hand.

3.2 Spatial Query Processing Layer
This layer supports spatial queries (e.g., Range query and
Join query) for large-scale spatial datasets. After geometrical objects are stored and processed in the Spatial RDD
layer, user may invoke a spatial query provided in Spatial
Query Processing Layer. GeoSpark processes such query
and returns the ﬁnal results to the user. Figure 3 gives the
general execution model followed by GeoSpark . This execution model implements the algorithms proposed by [5]
and [10]. To accelerate a spatial query, GeoSpark leverages the grid partitioned Spatial RDDs, spatial indexing, the
fast in-memory computation and DAG scheduler of Apache
Spark to parallelize the query execution.
Spatial Range Query. GeoSpark executes the spatial
range query algorithm following the execution model: Load
target dataset, partition data, create a spatial index on each
SRDD partition if necessary, broadcast the query window
to each SRDD partition, check the spatial predicate in each
partition, and remove spatial objects duplicates that existed
due to the data partitioning phase.
Spatial Join Query. GeoSpark executes the parallel
spatial join query following the execution model. GeoSpark
ﬁrst partitions the data from the two input SRDDs as well
as creates local spatial indexes (if required) for the SRDD
which is being queried. Then it joins the two datasets by
their keys which are grid IDs. For the spatial objects (from
the two SRDDs) that have the same grid ID, GeoSpark calculates their spatial relations. If two elements from two
SRDDS are overlapped, they are kept in the ﬁnal results.
The algorithm continues to group the results for each rectangle. The grouped results are in the following format: Rect-

angle, Point, Point, ... Finally, the algorithm removes the
duplicated points and returns the result to other operations
or saves the ﬁnal result to disk.
Spatial KNN Query. To process a Spatial KNN query,
GeoSpark uses a heap based top-k algorithm[7], which contains two phases: selection and merge. It takes a partitioned
SRDD, a point P and a number k as inputs. To calculate
the k nearest objects around point P , in the selection phase,
for each SRDD partition GeoSpark calculates the distances
between each object to the given point P , then maintains a
local heap by adding or removing elements based on the distances. This heap contains the nearest k objects around the
given point P . For IndexedSRDD, the system can utilize the
local indexes to reduce the query time. After the selection
phase, GeoSpark merges results from each partition, keeps
the nearest k elements that have the shortest distances to P
and outputs the result.

4.

EXPERIMENTS

This section provides preliminary experimental evaluation that studies the run time performance of the
following large-scale spatial data processing systems:
(1) GeoSpark_NoIndex | QuadTree | RTree: GeoSpark
approach without spatial index, with spatial Quad-Tree
or R-Tree index. In these approaches, data is partitioned according grids. Required spatial indexes are created on each partition after data partitioned. (2) SpatialHadoop_NoIndex | RTree: SpatialHadoop approach without spatial index or with spatial R-Tree index.
Experimental Setup. Our cluster setting on Amazon
EC2 is as follows: (1) CPU per worker: 8 Intel Xeon Processors operating at 2.5 GHz with Turbo up to 3.3 GHz.
(2) Memory per worker: 61 GB in total and 50 GB registered memory in Spark and Hadoop. (3) Storage per worker:
Amazon general purpose SSD. We deploy Ganglia, a scalable
distributed monitoring system for high performance computing systems such as clusters, on our Amazon EC2 experimental cluster.
Datasets. We use three real spatial datasets extracted
from TIGER ﬁles in our experiments: Zcta510 1.5 GB
dataset, Areawater 6.5 GB dataset and Edges 62 GB
dataset. They contain all the cities, all the lakes and all
the meaningful boundaries in the US in rectangle format
correspondingly. All of the datasets are preprocessed by
SpatialHadoop and are open to the public on its website [2].

4.1

Impact of Data Size

This section compares GeoSpark on TIGER Areawater
6.5 GB dataset with TIGER Edges 62 GB dataset as well as
SpatialHadoop. They are tested on 16 nodes cluster. Their
performance are shown in Figure 5. As depicted in Figure 5, GeoSpark and SpatialHadoop cost more run time
on the large dataset than that on the small one. However,
GeoSpark achieves much better run time performance than
SpatialHadoop in both datasets. This superiority is more
obvious on the small dataset. The reason is that GeoSpark
can cache more percentage of the intermediate data in memory on the small scale input than that on the large one. That
accelerates the processing speed.

4.2

Performance of Spatial Iterative Analysis

Spatial co-location pattern recognition is deﬁned as two
or more species are ofter located in a neighborhood rela-

1
3
5
7

9
11
13
15
17

d o u b l e t h r e s h o l d = THRESHOLD;
double baseDistance = 1 . 0 ;
double I n t e r v a l D i s t a n c e = 0 . 5 ;
i n t c o u n t e r =0;
double CoL oc at io n C oef f i cie n t =0.0;
// I n i t i a l i z e IndexedPointRDD
IndexedPointRDD t a r g e t =
new IndexedPointRDD ( SparkContext ,
DatasetLocation ) ;
// I t e r a t i v e Adja cency Matrix C a l c u l a t i o n
w h i l e ( C o L o c a t i o n C o e f f i c i e n t >t h r e s h o l d ) {
PairRDD glbAdjMat =
t a r g e t . S p a t i a l J o i n Q u e r y ( t a r g e t , WITHIN,
baseDistance + counter ∗ IntervalDistance ) ;
C o L o c a t i o n C o e f f i c i e n t=
C a l c u l a t e C o L o c a t i o n ( glbAdjMat ) ;
c o u n t e r ++;
}
r e t u r n b a s e D i s t a n c e + ( c o u n t e r −1) ∗
IntervalDistance ;

Figure 4: Adjacency Matrix (Java code) in GeoSpark

Figure 6: Run Time Performance for Spatial Co-location
Pattern Recognition
indexing and query processing algorithms in Apache Spark
to eﬃciently analyze spatial data at scale. Experiments
on data sizes and spatial analysis show that GeoSpark
achieves better run time performance than its MapReducebased counterparts (e.g., SpatialHadoop). The proposed
ideas are packaged into an open source software artifact.
In the future, we envision GeoSpark to be used by Earth
and Space Scientists, Geographers, Politicians, Commercial
Institutions to analyze spatial data at scale. We also expect
the scientiﬁc community will contribute to GeoSpark and
add new functionalities on top-of it that serve novel spatial
data analysis applications.

6.
Figure 5: Run Time Performance for Spatial Join Over Different Spatial Datasets

tionship. It usually executes multiple times to form a 2dimension curve for observation. This calculation needs the
adjacent matrix between two type of objects which is the
result of a join query. Sample code for ﬁnding adjacent matrix is given in Figure 4. We iteratively query GeoSpark
SRDDs two times with diﬀerent distances which can be
deﬁned as neighborhood relationships in adjacent matrix.
Since SpatialHadoop doesn’t natively support iterative jobs,
we have to run SpatialHadoop_RTree two times for a reasonable comparison. We use the ﬁrst point column in both
of TIGER Zcta 1.5 GB dataset and TIGER Edges 62 GB
dataset and join them.
As shown in Figure 6, GeoSpark outperforms SpatialHadoop in spatial co-location. And their performances are
also improved when we increase the number of machines per
cluster. GeoSpark only costs the quarter time of SpatialHadoop. The main reason behind is that GeoSpark caches
these datasets in memory with SRDDs automatically after
loads from the storage system. The iterative jobs like spatial co-location can invoke these SRDDs multiple times from
memory without any data transformation and data loading. SpatialHadoop has to read and transform the original
datasets again and again.

5. CONCLUSION AND FUTURE WORK
This paper introduced GeoSpark an in-memory cluster
computing framework for processing large-scale spatial data.
GeoSpark provides an API for Apache Spark programmers to easily develop spatial analysis applications. Moreover, GeoSpark provides native support for spatial data

REFERENCES

[1] A. Aji, F. Wang, H. Vo, R. Lee, Q. Liu, X. Zhang, and
J. H. Saltz. Hadoop-GIS: A High Performance Spatial
Data Warehousing System over MapReduce. PVLDB,
6(11):1009–1020, 2013.
[2] A. Eldawy and M. F. Mokbel. A demonstration of
spatialhadoop: An eﬃcient mapreduce framework for
spatial data. PVLDB, 6(12):1230–1233, 2013.
[3] A. Guttman. R-trees: a dynamic index structure for
spatial searching. In SIGMOD, 1984.
[4] J. Lu and R. H. Guting. Parallel Secondo: Boosting
Database Engines with Hadoop. In ICPADS, pages
738 –743, 2012.
[5] G. Luo, J. F. Naughton, and C. J. Ellmann. A
non-blocking parallel spatial join algorithm. In Data
Engineering, 2002. Proceedings. 18th International
Conference on, pages 697–705. IEEE, 2002.
[6] S. Nishimura, S. Das, D. Agrawal, and A. E. Abbadi.
MD-Hbase: A Scalable Multi-dimensional Data
Infrastructure for Location Aware Services. In MDM,
pages 7–16, 2011.
[7] N. Roussopoulos, S. Kelley, and F. Vincent. Nearest
neighbor queries. In ACM SIGMOD record,
volume 24, pages 71–79. ACM, 1995.
[8] H. Samet. The quadtree and related hierarchical data
structures. ACM Computing Surveys (CSUR),
16(2):187–260, 1984.
[9] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma,
M. McCauly, M. J. Franklin, S. Shenker, and I. Stoica.
Resilient Distributed Datasets: A Fault-Tolerant
Abstraction for In-Memory Cluster Computing. In
NSDI, pages 15–28, 2012.
[10] X. Zhou, D. J. Abel, and D. Truﬀet. Data partitioning
for parallel spatial join processing. Geoinformatica,
2(2):175–204, 1998.

Matrix Factorization with Explicit Trust and Distrust Side Information
for Improved Social Recommendation
RANA FORSATI, Shahid Beheshti University and University of Minnesota
MEHRDAD MAHDAVI, Michigan State University
MEHRNOUSH SHAMSFARD, Shahid Beheshti University
MOHAMED SARWAT, University of Minnesota

With the advent of online social networks, recommender systems have became crucial for the success of
many online applications/services due to their significance role in tailoring these applications to user-specific
needs or preferences. Despite their increasing popularity, in general, recommender systems suffer from data
sparsity and cold-start problems. To alleviate these issues, in recent years, there has been an upsurge of
interest in exploiting social information such as trust relations among users along with the rating data to
improve the performance of recommender systems. The main motivation for exploiting trust information in
the recommendation process stems from the observation that the ideas we are exposed to and the choices
we make are significantly influenced by our social context. However, in large user communities, in addition
to trust relations, distrust relations also exist between users. For instance, in Epinions, the concepts of
personal “web of trust” and personal “block list” allow users to categorize their friends based on the quality
of reviews into trusted and distrusted friends, respectively. Hence, it will be interesting to incorporate this
new source of information in recommendation as well. In contrast to the incorporation of trust information
in recommendation which is thriving, the potential of explicitly incorporating distrust relations is almost
unexplored. In this article, we propose a matrix factorization-based model for recommendation in social rating
networks that properly incorporates both trust and distrust relationships aiming to improve the quality of
recommendations and mitigate the data sparsity and cold-start users issues. Through experiments on the
Epinions dataset, we show that our new algorithm outperforms its standard trust-enhanced or distrustenhanced counterparts with respect to accuracy, thereby demonstrating the positive effect that incorporation
of explicit distrust information can have on recommender systems.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and
Retrieval—Information filtering; I.2 [Computing Methodologies]: Artificial Intelligence; I.2.6 [Artificial
Intelligence]: Learning; J.4 [Computer Applications]: Social and Behavioral Sciences
General Terms: Design, Algorithms
Additional Key Words and Phrases: Matrix factorization, recommender systems, social relationships
ACM Reference Format:
Rana Forsati, Mehrdad Mahdavi, Mehrnoush Shamsfard, and Mohamed Sarwat. 2014. Matrix factorization
with explicit trust and distrust side information for improved social recommendation. ACM Trans. Inf. Syst.
32, 4, Article 17 (October 2014), 38 pages.
DOI: http://dx.doi.org/10.1145/2641564

Author’s addresses: R. Forsati (corresponding author) and M. Shamsfard, Natural Language Processing
(NLP) Research Lab, Faculty of Electrical and Computer Engineering, Shahid Beheshti University, G. C.,
Tehran, Iran; M. Mahdavi, Department of Computer Science and Engineering, Michigan State University,
East Lansing, MI; M. Sarwat, Computer Science and Engineering Department, University of Minnesota,
Minneapolis, MN; corresponding author’s email: rana.forsati@gmail.com.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by
others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to
post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
c 2014 ACM 1046-8188/2014/10-ART17 $15.00

DOI: http://dx.doi.org/10.1145/2641564
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17

17:2

R. Forsati et al.

1. INTRODUCTION

The huge amount of information available on the Web has made it increasingly challenging to cope with this information overload and find the most relevant information
one is really interested in. Recommender systems intend to provide users with recommendations of products they might appreciate, taking into account their past ratings,
purchase history, or interest. The recent proliferation of online social networks has further enhanced the need for such systems. Therefore, it is obvious why such systems are
indispensable for the success of many online applications such as Amazon, iTunes, and
Netflix to guide the search process and help users to effectively find the information or
products they are looking for [Miller et al. 2004]. Roughly speaking, the overarching
goal of recommender systems is to identify a subset of items (e.g., products, movies,
books, music, news, and webpages) that are likely to be more interesting to users based
on their interests [Deshpande and Karypis 2004; Wu et al. 2009; Forsati and Meybodi
2010; Bobadilla et al. 2013].
In general, most widely used recommender systems (RS) can be broadly classified
into content-based (CB), collaborative filtering (CF), or hybrid methods [Adomavicius
and Tuzhilin 2005]. In CB recommendation, one tries to recommend items similar
to those a given user preferred in the past. These methods usually rely on external
information, such as explicit item descriptions, user profiles, and/or the appropriate
features extracted from items to analyze item similarity or user preference to provide
recommendation. In contrast, CF recommendation, the most popular method adopted
by contemporary recommender systems, is based on the core assumption that similar
users on similar items express similar interest, and it usually relies on the rating information to build a model out of the rating information in the past without having
access to external information required in CB methods. The hybrid approaches proposed combine both CB- and CF-based recommenders to gain advantages and avoid
certain limitations of each type of systems [Good et al. 1999; Soboroff and Nicholas
1999; Pazzani 1999; Melville et al. 2002; Pavlov and Pennock 2002; Talabeigi et al.
2010; Forsati et al. 2013].
The essence of CF lies in analyzing the neighborhood information of past users and
items’ interactions in the user-item rating matrix to generate personalized recommendations based on the preferences of other users with similar behavior. CF has been
shown to be an effective approach to recommender systems. The advantage of these
types of recommender systems over content-based RS is that the CF-based methods
do not require an explicit representation of the items in terms of features, but is based
only on the judgments/ratings of the users. These CF algorithms are mainly divided
into two main categories [Gu et al. 2010]: memory-based methods (also known as
neighborhood-based methods) [Wang et al. 2006b; Chen et al. 2009] and model-based
methods [Hofmann 2004; Si and Jin 2003; Srebro and Jaakkola 2003; Zhang et al.
2006]. Recently, another direction in CF considers how to combine memory-based and
model-based approaches to take advantage of both types of methods, thereby building
a more accurate hybrid recommender system [Pennock et al. 2000; Xue et al. 2005;
Koren 2008].
The heart of memory-based CF methods is the measurement of similarity based
on ratings of items given by users: either the similarity of users (user-oriented CF)
[Herlocker et al. 1999], the similarity of items (items-oriented CF) [Sarwar et al. 2001],
or combined user-oriented and item-oriented collaborative filtering approaches to overcome the limitations specific to either of them [Wang et al. 2006a]. The user-oriented CF
computes the similarity among users, usually based on user profiles or past behavior,
and seeks consistency in the predictions among similar users [Yu et al. 2004; Hofmann
2004]. The item-oriented CF, on the other hand, allows input of additional item-wise

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:3

information and is also capable of capturing the interactions among them. If the rating
of an item by a user is unavailable, collaborative-filtering methods estimate it by computing a weighted average of known ratings of the items from the most similar users.
Memory-based collaborative filtering is most effective when users have expressed
enough ratings to have common ratings with other users, but it performs poorly for
so-called cold-start users. Cold-start users are new users who have expressed only a few
ratings. Thus, for memory-based CF methods to be effective, large amounts of userrating data are required. Unfortunately, due to the sparsity of the user-item rating
matrix, memory-based methods may fail to correctly identify the most similar users
or items, which in turn decreases the recommender accuracy. Another major issue
that memory-based methods suffer from is the scalability problem. The reason being
essentially the fact that when the number of users and items is very large, which
is common in many real-world applications, the search to identify the k most similar
neighbors of the active user is computationally burdensome. In summary, data sparsity
and non-scalability issues are two main issues current memory-based methods suffer
from.
To overcome the limitations of memory-based methods, model-based approaches have
been proposed, which establish a model using the observed ratings that can interpret
the given data and predict the unknown ratings [Adomavicius and Tuzhilin 2005]. In
contrast to memory-based algorithms, model-based algorithms try to model the users
based on their past ratings and use these models to predict the ratings on unseen items.
In model-based CF, the goal is to employ statistical and machine learning techniques
to learn models from the data and make recommendations based on the learned model.
Methods in this category include aspect model [Hofmann 2004; Si and Jin 2003], clustering methods [Kohrs and Merialdo 1999], Bayesian model [Zhang and Koren 2007],
and low-dimensional linear factor models such as matrix factorization (MF) [Srebro
et al. 2005; Srebro and Jaakkola 2003; Zhang et al. 2006; Salakhutdinov and Mnih
2008b]. Due to its efficiency in handling very huge datasets, matrix factorization-based
methods have become one of the most popular models among the model-based methods, for example, weighted low-rank matrix factorization [Srebro and Jaakkola 2003],
weighted nonnegative matrix factorization (WNMF) [Zhang et al. 2006], maximum
margin matrix factorization (MMMF) [Srebro et al. 2005], and probabilistic matrix factorization (PMF) [Salakhutdinov and Mnih 2008b]. These methods assume that user
preferences can be modeled by only a small number of latent factors [Dasgupta et al.
2002] and all focus on fitting the user-item rating matrix using low-rank approximations only based on the observed ratings. The recommender system we propose in this
article adheres to the model-based factorization paradigm.
Although latent factor models and in particular matrix factorization are able to
generate high-quality recommendations, these techniques also suffer from the data
sparsity problem in real-world scenarios and fail to address users who rated only a
few items. For instance, according to Sarwar et al. [2001], the density of non-missing
ratings in most commercial recommender systems is less than one or even much less.
Therefore, it is unsatisfactory to rely predictions on such small amounts of data, which
becomes more challenging in the presence of large number of users or items. This
observation necessitates tackling the data sparsity problem in an affirmative manner
to be able to generate more accurate recommendations.
One of the most prominent approaches to tackling the data sparsity problem is to
compensate for the lack of information in the rating matrix with other sources of
side information which are available to the recommender system. For example, social
media applications allow users to connect with each other and to interact with items
of interest such as songs, videos, pages, news, and groups. In such networks, the ideas
we are exposed to and the choices we make are significantly influenced by our social
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:4

R. Forsati et al.

context. More specifically, users generally tend to connect with other users due to some
commonalities they share, often reflected in similar interests. Moreover, in many reallife applications it may be the case that only social information about certain users
is available while interaction data between the items and those users has not yet
been observed. Therefore, the social data accumulated in social networks would be a
rich source of information for the recommender system to utilize as side information
to alleviate the data sparsity problem. To accomplish this goal, in recent years, the
trust-based recommender systems became an emerging field to provide users with
personalized item recommendations based on the historical ratings given by users and
the trust relationships among users (e.g., social friends).
Social-enhanced recommendation systems are becoming of greater significance and
practicality with the increased availability of online reviews, ratings, friendship links,
and follower relationships. Moreover, many e-commerce and consumer review websites
provide both reviews of products and a social network structure among the reviewers.
As an example, the e-commerce site Epinions [Guha et al. 2004] asks its users to indicate which reviews/users they trust and use this trust information to rank the reviews
of products. Similar patterns can be found in online communities such as Slashdot in
which millions of users post news and comment daily and are capable of tagging other
users as friends/foes or fans/freaks. Another example is the ski mountaineering site
Moleskiing [Avesani et al. 2005] which enables users to share their opinions about
the snow conditions of the different ski routes and also express how much they trust
the other users. Another well-known example is the FilmTrsut system [Golbeck and
Hendler 2006], an online social network that provides movie rating and review features
to its users. The social networking component of the website requires users to provide
a trust rating for each person they add as a friend. Also users on Wikipedia can vote
for or against the nomination of others to adminship [Burke and Kraut 2008]. These
websites have come to play an important role in guiding users’ opinions on products
and, in many cases, also influence their decisions in buying or not buying the product
or service. The results of experiments in Crandall et al. [2008] and of similar works
confirm that a social network can be exploited to improve the quality of recommendations. From this point of view, traditional recommender systems that ignore the social
structure between users may no longer be suitable.
A fundamental assumption in social-based recommender systems which has been
adopted by almost all of the relevant literature is that if two users have a friendship relation, then the recommendation from his or her friends probably has higher
trustworthiness than strangers. Therefore, the goal becomes how to combine the useritem rating matrix with the social/trust network of a user to boost the accuracy of the
recommendation system and alleviate the sparsity problem. Over the years, several
studies have addressed the issue of the transfer of trust among users in online social
networks. These studies exploit the fact that trust can be passed from one member
to another in a social network, creating trust chains, based on its propagative and
transitive nature.1 Therefore, some recommendation methods fusing social relations
by regularization [Jamali and Ester 2011; Li and Yeung 2009; Ma et al. 2011a; Zhu
et al. 2011] or factorization [Ma et al. 2008, 2011b; Salakhutdinov and Mnih 2008a,
2008b; Srebro and Jaakkola 2003; Salakhutdinov et al. 2007; Rennie and Srebro 2005]
were proposed that exploit trust relations in a social network.

1 We note that while the concept of trust has been studied in many disciplines, including sociology, psychology,
economics, and computer science from different perspectives, the issue of propagation and transitivity have
often been debated in literature, and different authors have reached different conclusions (see e.g., [Sherchan
et al. 2013] for a thorough discussion).

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:5

Also, the results of incorporating trust information in recommender systems is appealing and has been the focus of much researcher in the last few years, but in large
user communities, besides the trust relationship between users, the distrust relationships are also unavoidable. For example, Epinions provided the feature that enables
users to categorize other users in a personal web of trust list based on their quality
as a reviewer. Later on, this feature integrated with the concept of personal block list,
which reflects the members that are distrusted by a particular user. In other words, if
a user encounters a member whose reviews are consistently offensive, inaccurate, or
otherwise low quality, she can add that member to her block list. Therefore, it would be
tempting to investigate whether or not distrust information could be effectively utilized
to boost the accuracy of recommender systems as well.
In contrast to trust information for which there has been a great deal of research,
the potential advantage/disadvantage of explicitly utilizing distrust information is
almost unexplored. Recently, few attempts have been made to explicitly incorporate
the distrust relations in recommendation process [Guha et al. 2004; Ma et al. 2009b;
Victor et al. 2011b, 2013], which demonstrated that the recommender systems can
benefit from the proper incorporation of distrust relations in social networks. However,
despite these positive results, there are some unique challenges involved in distrustenhanced recommender systems. In particular, it has proven challenging to model
distrust propagation in a manner which is both logically consistent and psychologically
plausible. Furthermore, the naive modeling of distrust as negative trust raises a
number of challenges—both algorithmic and philosophical. Finally, it is an open
challenge how to incorporate trust and distrust relations in model-based methods
simultaneously. This article is concerned with these questions and gives an affirmative
solution to challenges involved with distrust-enhanced recommendation. In particular,
the proposed method makes it possible to simultaneously incorporate both trust and
distrust relationships in recommender systems to increase the prediction accuracy. To
the best of our knowledge, this is the first work that models distrust relations into the
matrix factorization problem along with trust relations at the same time.
The main intuition behind the proposed algorithm is that one can interpret the distrust relations between users as dissimilarity in their preferences. In particular, when
a user u distrusts another user v, it indicates that user u disagrees with most of the
opinions issued, or ratings made by user v. Therefore, the latent features of user u
obtained by matrix factorization must be as dissimilar as possible to v’s latent features. In other words, this intuition suggests directly incorporating the distrust into
recommendation by considering distrust as reversing the deviation of latent features.
However, when combined with the trust relations between users, due to the contradictory role of trust and distrust relations in propagating social information in the matrix
factorization process, this idea fails to effectively capture both relations simultaneously. This statement also follows from the preliminary experimental results in Victor
et al. [2011b] for memory-based CF methods that demonstrated regarding distrust as
an indication to reverse deviations in not the right way to incorporate distrust.
To remedy this problem, we settle for a less ambitious goal and propose another
method to facilitate the learning from both types of relations. In particular, we try
to learn latent features in a manner such that the latent features of users who are
distrusted by the user u have a guaranteed minimum dissimilarity gap from the worst
dissimilarity of users who are trusted by user u. By this formulation, we ensure that
when user u agrees on an item with one of his trusted friends, he will disagree on
the same item with his distrusted friends with a minimum predefined margin. We
note that this idea significantly departs from the existing works in distrust-enhanced
memory-based recommender systems [Victor et al. 2011b, 2013] that employ the distrust relations to either filter out or debug the trust relations to reduce the prediction
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:6

R. Forsati et al.

task to a trust-enhanced recommendation. In particular, the proposed method ranks
the latent features of trusted and distrusted friends of each user to reflect the effect of
relation in factorization.
Summary of Contributions. This work makes the following key contributions.
—A matrix factorization-based algorithm for simultaneous incorporation of trust and
distrust relationships in recommender systems. To the best of our knowledge, this is
the first model-based recommender algorithm that is able to leverage both types of
relationships in recommendation.
—An efficient stochastic optimization algorithm to solve the optimization problem
which makes the proposed method scalable to large social networks.
—An empirical investigation of the consistency of the social relationships with rating
information. In particular, we examine to what extent trust and distrust relations
between users are aligned with the ratings they issued on items.
—An exhaustive set of experiments on the Epinions dataset to empirically evaluate the
performance of the proposed algorithm and demonstrate its merits and advantages.
—A detailed comparison of the proposed algorithm to state-of-the-art trust/distrustenhanced memory/model-based recommender systems.
Outline. The rest of this article is organized as follows. In Section 2, we draw connections to and put our work in context of some of the most recent work on social
recommender systems. Section 3 formally introduces the matrix factorization problem,
an optimization-based framework to solve it, and its extension to incorporate the trust
relations between users. The proposed algorithm along with optimization methods are
discussed in Section 4. Section 5 includes our experimental result on the Epinions
dataset which demonstrates the merits of the proposed algorithm in alleviating the
data sparsity problem in rating matrix and generating more accurate recommendations. Finally, Section 6 concludes and discusses a few directions as future work.
2. RELATED WORK ON SOCIAL RECOMMENDATION

Earlier in the introduction, we discussed some of the main lines of research on recommender systems; here, we survey further lines of study that are most directly-related to
our work on social-enhanced recommendation. Many successful algorithms have been
developed over the past few years to incorporate social information in recommender
systems. After reviewing trust-enhanced memory-based approaches, we discuss some
model-based approaches for recommendation in social networks with trust relations.
Finally, we review major approaches in distrust modeling and distrust-enhanced recommender systems.
2.1. Trust-Enhanced Memory-Based Recommendation

Social network data has been widely investigated in the memory-based approaches.
These methods typically explore the social network and find a neighborhood of users
trusted (directly or indirectly) by a user and perform the recommendation by aggregating their ratings. These methods use the transitivity of trust and propagate trust
to indirect neighbors in the social network [Massa and Avesani 2004, 2009; Konstas
et al. 2009; Jamali and Ester 2009, 2010, 2011; Koren et al. 2009].
In Massa and Avesani [2004], a trust-aware collaborative filtering method for recommender systems is proposed. In this work, the collaborative filtering process is informed
by the reputation of users, which is computed by propagating trust. Konstas et al. [2009]
proposed a method based on the random walk algorithm to utilize social connection and
other social annotations to improve recommendation accuracy. However, this method
does not utilize the rating information and is not applicable to constructing a random
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:7

walk graph in real datasets. TidalTrust [Golbeck 2006] performs a modied breadthfirst search in the trust network to compute a prediction. To compute the trust value
between user u and v who are not directly connected, TidalTrust aggregates the trust
value between u’s direct neighbors and v weighted by the direct trust values of u and
its direct neighbors.
MoleTrust [Massa and Avesani 2004, 2005; Zhang and Koren 2007] applies the same
idea as TidalTrust, but MoleTrust considers all the raters up to a fixed maximumdepth given as an input, independent of any specific user and item. The trust metric
in MoleTrust consists of two major steps. First, cycles in trust networks are removed.
Therefore, removing trust cycles beforehand from trust networks can significantly
speed up the proposed algorithm because every user only needs to be visited once to
infer trust values. Second, trust values are calculated based on the obtained directed
acyclic graph by performing a simple graph random walk.
TrustWalker [Jamali and Ester 2009] combines trust-based and item-based recommendation to consider enough ratings without suffering from noisy data. Their experiments show that TrustWalker outperforms other existing memory-based approaches.
Each random walk on the user trust graph returns a predicted rating for user u on
target item i. The probability of stopping is directly proportional to the similarity between the target item and the most similar item j, weighted by the sigmoid function of
step size k. The more the similarity, the greater the probability of stopping and using
the rating on item j as the predicted rating for item i. As the step size increases, the
probability of stopping decreases. Thus ratings by closer friends on similar items are
considered more reliable than ratings on the target item by friends further away.
We note that all these methods are neighborhood-based methods which employ only
heuristic algorithms to generate recommendations. There are several problems with
this approach. The relationship between the trust network and the user-item matrix
has not been studied systematically. Moreover, these methods are not scalable to very
large datasets, since they may need to calculate the pairwise user similarities and
pairwise user trust scores.
2.2. Trust-Enhanced Model-Based Recommendation

Recently, researchers have exploited matrix factorization techniques to learn latent
features for users and items from the observed ratings, and fusing social relations
among users with rating data as will be detailed in Section 3. These methods can be
divided into two types: regularization-based methods and factorization-based methods.
Here we review some existing matrix factorization algorithms that incorporate trust
information in the factorization process.
2.2.1. Regularization-Based Social Recommendation. Regularization-based methods typically add a regularization term to the loss function and minimizes it. Most recently, Ma
et al. [2011a] proposed an idea based on social-regularized matrix factorization to make
recommendation based on social network information. In this approach, the social regularization term is added to the loss function, which measures the difference between
the latent feature vector of a user and those of his friends. A probability model similar
to the model in Ma et al. [2011a] is proposed by Jamali and Ester [2011]. The graph
Laplacian regularization term of social relations is added into the loss function in
Li and Yeung [2009] and minimizes the loss function by alternative projection algorithm. Zhu et al. [2011] used the same model in Li and Yeung [2009] and built
graph Laplacian of social relations using three kinds of kernel functions. In Liu et al.
[2013], the minimization problem is formulated as a low-rank semidefinite optimization
problem.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:8

R. Forsati et al.

2.2.2. Factorization-Based Social Recommendation. In factorization-based methods, social
relationships between users are represented as a social relation matrix, which is factored as well as the rating matrix. The loss function is the weighted sum of the social relation matrix factorization error and the rating matrix factorization error. For instance,
SoRec [Ma et al. 2008] incorporates the social network graph into the probabilistic matrix factorization model by simultaneously factorizing the user-item rating matrix and
the social trust networks by sharing a common latent low-dimensional user feature
matrix [Liu et al. 2013]. The experimental analysis shows that this method generates
better recommendations than the non-social filtering algorithms [Jamali and Ester
2010]. However, the disadvantage of this work is that although users’ social networks
are integrated into the recommender systems by factorizing the social trust graph,
the real-world recommendation processes are not reflected in the model. Two sets of
different feature vectors are assumed for users, which makes the interpretability of
the model very hard [Jamali and Ester 2010; Ma et al. 2009a]. This drawback not only
causes lack of interpretability in the model, but also affects the recommendation qualities. A better model named Social Trust Ensemble (STE) [Ma et al. 2009a] is proposed,
by making the latent features of a user’s direct neighbors affect the rating of the user.
Their method is a linear combination of a basic matrix factorization approach and a social network-based approach. Experiments show that their model outperforms the basic
matrix factorization-based approach and existing trust-based approaches. However, in
their model, the feature vectors of direct neighbors of u affect the ratings of u instead of
affecting the feature vector of u. This model does not handle trust propagation. Another
method for recommendation in social networks has been proposed in Ma et al. [2009b].
This method is not a generative model and defines a loss function to be minimized.
The main disadvantage of this method is that it punishes the users with lots of social
relations more than other users. Finally, SocialMF [Jamali and Ester 2010] is a matrix
factorization-based model which incorporates social influence by making the features
of every user depend on the features of his/her direct neighbors in the social network.
2.3. Distrust-Enhanced Social Recommendation

In contrast to incorporation of trust relations, unfortunately most of the literature on
social recommendation totally ignores the potential of distrust information in boosting
the accuracy of recommendations. In particular, only recently a few work have started
to investigate the rule of distrust information in the recommendation process, both
from theoretical and empirical viewpoints [Guha et al. 2004; Ziegler and Lausen 2005;
Nalluri 2008; Ziegler 2009; Ma et al. 2009b; Wierzowiecki and Wierzbicki 2010; Victor
et al. 2011b, 2011c, 2013; Verbiest et al. 2012]. Although these studies have shown
that distrust information can be plentiful, but there is a significant gap in clear understanding of distrust in recommender systems. The most important reasons for this
shortage are the lack of datasets that contain distrust information and dearth of a
unified consensus on modeling and propagation of distrust.
A formal framework of trust propagation schemes, introducing the formal and computational treatment of distrust propagation, has been developed in Guha et al. [2004].
In an extension of this work, Ziegler [2009] proposed clever adaptations in order to
handle distrust and sinks such as trust decay and normalization. In Wierzowiecki and
Wierzbicki [2010], a trust/distrust propagation algorithm called CloseLook is proposed,
which is capable of using the same kinds of trust propagation as the algorithm proposed
by Guha et al. [2004]. Leskovec et al. [2010a] extended the results of Guha et al. [2004]
using a machine-learning framework (instead of the propagation algorithms based
on an adjacency matrix) to enable the evaluation of the most informative structural
features for the prediction task of positive/negative links in online social networks. A
comprehensive framework that computes trust/distrust estimations for user pairs in
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:9

the network using trust metrics is built in Victor et al. [2011c]: given two users in the
trust network, we can search for a path between them and propagate the trust scores
along this path to obtain an estimation. When more than one path is available, we
may single out the most relevant ones (selection), and aggregation operators can then
be used to combine the propagated trust scores into one final trust score, according to
different trust score propagation operators.
Ma et al. [2009b] was the first seminal work to demonstrate that the incorporation
of distrust information could be beneficial based on a model-based recommender system. In Victor et al. [2011c, 2013], the same question is addressed in memory-based
approaches. In particular, Victor et al. [2013] embarked upon the distrust-enhanced recommendation and showed that with careful incorporation of distrust metric, distrustenhanced recommender systems are able to outperform their trust-only counterparts.
The main rational behind the algorithm proposed in Victor et al. [2013] is to employ
the distrust information to debug or filter out the users’ propagated web of trust. It is
also has been realized that the debugging methods must exhibit a moderate behavior
in order to be effective. Verbiest et al. [2012] addressed the problem of considering the
length of the paths that connect two users for computing trust-distrust between them,
according to the concept of trust decay. This work also introduced several aggregation
strategies for trust scores with variable path lengths.
Finally we note that the aforementioned works try to either model or utilize
trust/distrust information. In recent years, there has been an upsurge of interest in
predicting trust and distrust relations in a social network [Leskovec et al. 2010a;
DuBois et al. 2011; Bachi et al. 2012; Patil et al. 2013]. For instance, Leskovec et al.
[2010a] casts the problem as a sign prediction problem (i.e., +1 for friendship and −1
for opposition) and utilizes machine learning methods to predict the sign of links in the
social network. In DuBois et al. [2011] a new method is presented for computing both
trust and distrust by combining an inference algorithm that relies on a probabilistic
interpretation of trust based on random graphs with a modified spring-embedding algorithm to classify an edge. Another direction of research is to examine the consistency
of social relations with theories in social psychology [Cartwright and Harary 1956;
Leskovec et al. 2010b]. Our work significantly departs from these works on prediction
or consistency analysis of social relations, and aims to effectively incorporate distrust
information in matrix factorization for effective recommendation.
3. MATRIX FACTORIZATION-BASED RECOMMENDER SYSTEMS

This section provides a formal definition of collaborative filtering, the primary recommendation method we are concerned with in this article, followed by solution methods
for low-rank factorization that are proposed in the literature to address the problem.
(See Table I for Common notations and their meanings.)
3.1. Matrix Factorization for Recommendation

In collaborative filtering, we assume that there is a set of n users U = {u1 , . . . , un} and
a set of m items I = {i1 , . . . , im}, where each user ui expresses opinions about a set of
items. In this article, we assume opinions are expressed through an explicit numeric
rating (e.g., scale from one to five), but other rating methods such as hyperlink clicks are
possible as well. We are mainly interested in recommending a set of items for an active
user such that the user has not rated these items before. To this end, we are aimed
at learning a model from the existing ratings, that is, offline phase, and then use the
learned model to generate recommendations for active users, that is, online phase. The
rating information is summarized in an n × m matrix R ∈ Rn×m, 1 ≤ i ≤ n, 1 ≤ j ≤ m,
where the rows correspond to the users and the columns correspond to the items, and
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:10

R. Forsati et al.
Table I. Summary of Notations Consistently Used in the Article and Their Meaning
Symbol

Meaning

U = {u1 , . . . , un}, n
I = {i1 , . . . , im}, m
k
R ∈ Rn×m
R , |R |
U ∈ Rn×k
V ∈ Rm×k
S ∈ {−1, +1}n×n
S , |S |
W ∈ Rn×n
+
N (u) ⊆ [n]
N+ (u) ⊆ [n]
N− (u) ⊆ [n]
D : Rk × Rk → R+

The set of users in system and the number of users
The set of items and the number of items
The dimension of latent features in factorization
The partially observed rating matrix
The set of observed entires in rating matrix and its size
The matrix of latent features for users
The matrix of latent features for items
The social network between n users
The set of extracted triplets from the social relations and its size
The pairwise similarity matrix between users
Neighbors of user u in the social graph
The set of trusted neighbors by user u in the social graph
The set of distrusted neighbors by user u in the social graph
The measurement function used to assess the similarly of latent features

the ( p, q)th entry is the rate given by user up to the item iq . We note that the rating
matrix is partially observed, and it is sparse in most cases.
An efficient and effective approach to recommender systems is to factorize the useritem rating matrix R by a multiplicative of k-rank matrices R ≈ UV , where U ∈ Rn×k
and V ∈ Rm×k utilize the factorized user-specific and item-specific matrices, respectively,
to make further missing data prediction. The main intuition behind a low-dimensional
factor model is that there is only a small number of factors influencing the preferences,
and that a user’s preference vector is determined by how each factor applies to that
user. This low rank assumption makes it possible to effectively recover the missing
entires in the rating matrix from the observed entries. We note that the celebrated
Singular Value Decomposition (SVD) method for factorizing the rating matrix R is
not applicable here due to the fact that the rating matrix is partially available and
we are only allowed to utilize the observed entries in factorization process. There are
two basic formulations to solve this problem: optimization based (see e.g., [Rennie and
Srebro 2005; Liu et al. 2013; Ma et al. 2008; Koren et al. 2009]) and probabilistic [Mnih
and Salakhutdinov 2007]. In the following sections, we first review the optimizationbased framework for matrix factorization and then discuss how it can be extended to
incorporate trust information.
3.2. Optimization-Based Matrix Factorization

Let R be the set of observed ratings in the user-item matrix R ∈ Rn×m, that is,
R = {(i, j) ∈ [n] × [m] : Ri j has been observed},
where n is the number of users and mis the number of items to be rated. In optimizationbased matrix factorization, the goal is to learn the latent matrices U and V by solving
the following optimization problem:
⎡
⎤
 

1
λ
λ
2
U
V

min ⎣L(U, V) =
UF +
VF ⎦ ,
Ri j − Ui,:
(1)
V j,: +
U,V
2
2
2
(i, j)∈R

where  · F is the Frobenius norm of a matrix, that is, AF =

	
 

n
m
i=1

j=1

|Ai j |2 .

The optimization problem in Eq. (1) constitutes of three terms: the first term aims
to minimize the inconsistency between the observed entries and their corresponding
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:11

value obtained by the factorized matrices. The last two terms regularize the latent
matrices for users and items, respectively. The parameters λU and λV are regularization
parameters that are introduced to control the regularization of latent matrices U and
V, respectively. We would like to emphasize that the problem in Eq. (1) is non-convex
jointly in both U and V. However, despite its non-convexity, the formulation in Eq. (1)
is widely used in practical collaborative filtering applications, as the performance is
competitive, or better as compared to trace-norm minimization, while scalability is
much better. For example, as indicated in Koren et al. [2009], to address the Netflix
problem, Eq. (1) has been applied with a fair amount of success to factorize datasets
with 100 million ratings.
3.3. Matrix Factorization with Trust Side Information

Recently it has been shown that just relying on the rating matrix to build a recommender system is not as accurate as expected. The main reason for this claim is the
known cold-start users problem and the sparsity of the rating matrix. Cold-start users
are one of the most important challenges in recommender systems. Since cold-start
users are more dependent on the social network compared to users with more ratings,
the effect of using trust propagation gets more important for cold-start users. Moreover,
in many real-life systems, a very large portion of users do not express any ratings, and
they only participate in the social network. Hence, using only the observed ratings does
not allow us to learn the user features.
One of the most prominent approaches to tacking the data sparsity problem in matrix factorization is to compensate for the lack of information in rating matrix with
other sources of side information which are available to the recommender system. It
has been recently shown that social information, such as trust relationship between
users, is a rich source of side information to compensate for the sparsity. The already
mentioned traditional recommendation techniques are all based on working on the
user-item rating matrix, and ignore the abundant relationships among users. Trustbased recommendation usually involves constructing a trust network where nodes are
users and edges represent the trust placed on them. The goal of a trust-based recommendation system is to generate personalized recommendations by aggregating the
opinions of other users in the trust network. The intuition is that users tend to adopt
items recommended by trusted friends rather than strangers, and that trust is positively and strongly correlated with user preferences. Recommendation techniques that
analyze trust networks were found to provide very accurate and highly personalized
results.
To incorporate the social relations in the optimization problem formulated in Eq. (1),
a few papers [Ma et al. 2009b, 2011a; Jamali and Ester 2011; Liu et al. 2013; Zhu et al.
2011] proposed the social regularization method which aims at keeping the latent
vector of each user similar to his/her neighbors in the social network. The proposed
models force the user feature vectors to be close to those of their neighbors to be able to
learn the latent user features for users with no or very few ratings [Jamali and Ester
2011]. More specifically, the optimization problem becomes
L(U, V) =

2 λU
1  
λV

UF +
VF
V j,: +
Ri j − Ui,:
2
2
2
(i, j)∈R



n 


λS  
1

+
U j,: 
Ui,: − |N (i)|
,
2

i=1 
j∈N (i)

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

(2)

17:12

R. Forsati et al.

where λ S is the social regularization parameter and N (i) is the subset of users who has
relationship with the ith user in the social graph.
The rationale behind this social regularization idea is that every user’s taste is
relatively similar to the average taste of his friends in the social network. We note that
in using this idea, latent features of users indirectly connected in the social network
will be dependent, and hence the trust gets propagated. A more reasonable and realistic
model should treat all friends differently based on how similar they are. Let us assume
the weight of a relationship between two users i and j is captured by Wi j , where
W ∈ Rn×n denotes the social weight matrix. It is easy to extend the model in Eq. (2) to
treat friends differently based on the weight matrix W as
2 λU
1  
λV

UF +
VF
Ri j − Ui,:
L(U, V) =
V j,: +
(3)
2
2
2
(i, j)∈R




n

λS  
j∈N (i) Wi j U j,: 

+
Ui,: − 

.

2
j∈N (i) Wi j 
i=1

An alternative formulation is to regularize each user’s friends individually, resulting
in the following objective function [Ma et al. 2011a]:
2 λU
1  
λV

L(U, V) =
UF +
VF
Ri j − Ui,:
V j,: +
2
2
2
(i, j)∈R

n

2
λS 
+
Wi j Ui,: − U j,:  ,
2
i, j=1

where we simply assumed that for any j ∈
/ N (i), Wi j = 0.
As mentioned earlier, the objective function in L(U, V) is not jointly convex in both
U and V, but it is convex in each of them fixing the other one. Therefore, to find a local
solution, one can stick to the standard gradient descent method to find a solution in an
iterative manner as follows:
Ut+1 ← Ut − ηt ∇U L(U, V)|U=Ut ,V=Vt ,
Vt+1 ← Vt − ηt ∇V L(U, V)|U=Ut ,V=Vt .
4. MATRIX FACTORIZATION WITH TRUST AND DISTRUST SIDE INFORMATION

In this section, we describe the proposed algorithm for social recommendation which
is able to incorporate both trust and distrust relationships in the social network along
with the partially observed rating matrix. We then present two strategies to solve the
derived optimization problem, one based on the gradient descent optimization algorithm which generates more accurate solutions but is computationally cumbersome,
and another based on the stochastic gradient descent method which is computationally
more efficient for large rating and social matrices but suffers from slow convergence
rate.
4.1. Algorithm Description

As already discussed, the vast majority of related work in the field of matrix factorization for recommendation has primarily focused on trust propagation and has
simply ignored the distrust information between users or, intrinsically, is not capable
of exploiting it. Now, we aim at developing a matrix factorization-based model for recommendation in social rating networks to utilize both trust and distrust relationships.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:13

We incorporate the trust/distrust relationship between users in our model to improve
the quality of recommendations. While intuition and experimental evidence indicate
that trust is somewhat transitive, distrust is certainly not transitive. Thus, when we
intend to propagate distrust through a network, questions about transitivity and how
to deal with conflicting information abound.
To inject social influence in our model, the basic idea is to find appropriate latent
features for users such that each user is brought closer to the users she/he trusts and
separated from the users that she/he distrusts and who have different interests. We
note that simply incorporating this idea in matrix factorization by naively penalizing
the similarity of each user’s latent features to his distrusted friends’ latent features
fails to reach the desired goal. The main reason being that distrust is not as transitive
as trust, that is, distrust can not directly replace trust in trust propagation approaches,
and utilizing distrust requires careful consideration (trust is transitive, i.e., if user u
trusts user v and v trusts w, there is a good chance that u will trust w, but distrust is
certainly not transitive, i.e., if u distrusts v and v distrusts w, then w may be closer to
u than v or maybe even farther away). It is noticeable that this statement is consistent
with the preliminary experimental results in Victor et al. [2011b] for memory-based
CF methods that indicate regarding distrust as an indication to reverse deviations in
not the right way to incorporate distrust. Therefore, we pursue another approach to
model the distrust in the recommendation process.
The main intuition behind the proposed framework stems from the observation that
trust relations between users can be treated as agreement on items and distrust relations can be considered as disagreement on items. Then, the question becomes how can
we guarantee that when a user agrees on an item with one of his/her friends, he/she will
disagree on the same item with his/her distrusted friends with a reasonable margin.
We note that this margin should be large enough to make it possible to distinguish
between two types of friends. In terms of latent features, this observation translates to
having a margin between the similarity and dissimilarity of users’ latent features to
his/her trusted and distrusted friends.
Alternatively, one can view the proposed method from the viewpoint of connectivity of
latent features in a properly designated graph. Intuitively, certain features or groups of
features should influence how users connect in the social network, and thus it should
be possible to learn a mapping from features to connectivity in the social network
such that the mapping respects the underlying structure of the social network. In the
basic matrix factorization algorithm for recommendation, we can consider the latent
features as isolated vertices of a graph where there is no connection between nodes.
This can be generalized to the social-enhanced setting by considering the social graph
as the underlying graph between latent features with two types of edges (i.e., trust
and distrust relations correspond to positive and negative edges, respectively). Now
the problem reduces to learning the latent features for each user u such that users
trusted by u in the social network (with positive edges) are close and users which are
distrusted by u (with negative edges) are more distant. Learning latent features in this
manner respects the inherent topology of the social network.
Figure 1 shows an example to illustrate the intuition behind this idea. For ease of
exposition, we only consider the latent features for the user u1 . From the trust network
in Figure 1(a), we can see that user u1 trusts the list of users N+ = {u2 , u4 , u6 , u7 },
and from the distrust network in Figure 1(b), we see that user u1 distrusts the list
of users N− = {u3 , u5 }. The goal is to learn the latent features that obeys two goals:
(i) it minimizes the prediction error on observed entries in the rating matrix, (ii) it
respects the underlying structure of the trust and distrust networks between users.
In Figure 1(d), the latent features are depicted in the Euclidean space from the viewpoint of user u1 . As shown in Figure 1(d), for user u1 , the latent features of his/her
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:14

R. Forsati et al.

Fig. 1. A simple example with seven users {u1 , u2 , . . . , u7 } and six items {i1 , i2 , . . . , i6 } to illustrate the main
intuition behind the proposed algorithm. The inputs of the algorithm are (a) trust network, (b) distrust
network, and (c) partially observed rating matrix R, respectively. As shown in (d) for user u1 , the learned
latent features for all his trusted friends {u2 , u4 , u6 , u7 } are closer to u1 ’s latent features than his distrusted
friends {u3 , u5 } with a margin of 1.

trusted friends N+ lie inside the solid circle centered at u1 , and the latent features of
his/her distrusted friends N− lie outside the dashed circle. The gap between two circles
guarantees that there always exists a safe margin between u1 ’s agreements with his
trusted and distrusted friends. One simple way to impose these constraints on the latent features of users is to generate a set of triplets for any combination of trusted and
distrusted friends (e.g., one such triplet for user u1 can be constructed as (u1 , u2 , u5 ))
and force the margin constraint to hold for all extracted triplets. This ensures that the
minimum margin gap will definitely exist between the latent features of all the trusted
and distrusted friends as desired and makes it possible to incorporate both types of
relationships between users in the matrix factorization.
It is worth mentioning that similar to the social-enhanced recommender systems
previously discussed, the proposed algorithm is also based on hypotheses about the
existence and the correlation of trust/distrust relations and ratings in the data. The
empirical investigation of correlation between social relations and rating information
has been the focus of a bulk of recent research including [Ziegler and Golbeck 2007;
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:15

Patil et al. 2013; Ma 2013], where the results reinforce the hypothesis that ratings
from trusted people count more than those from others and in particular distrusted
neighbors. We have also conducted experiments, as will be detailed in Section 5.5, to empirically investigate the correlation/alignment between social relations and the rating
information issued by users which supports our strategy in exploiting the trust/distrust
relations in matrix factorization.
We now formalize the proposed solution. As the first ingredient, we need a measure
to evaluate the consistency between the latent features of users, that is, the matrix U,
and the trust and distrust constraints existing between users in the social network. To
this end, we introduce a monotonically-increasing convex loss function (z) to measure
the discrepancy between the latent features of different users. Let ui , u j , and uk be
three users in the model such that ui trusts u j but distrusts uk. The main intuition
behind the proposed framework is that the latent features of ui , that is, Ui,: must be
more similar to u j ’s latent features than latent features for user uk. For each such
a triplet, we penalize the objective function by (D(Ui,: , U j,: ) − D(Ui,: , Uk,: )), where the
function D : Rk ×Rk → R+ measures the similarity between two latent vectors assigned
to two different users, and  : R → R+ is a penalty function that is utilized to assess
the violation of latent vectors of trusted and distrusted users. Example loss functions
include hinge loss (z) = max(0, 1 − z) and logistic loss (z) = log(1 + e−z ), which are
widely used convex surrogates of 0-1 loss function in learning community.
Let S denote the set of extracted triplets from the social relations, that is,


S = (i, j, k) ∈ [n] × [n] × [n] : Si j = 1 & Sik = −1 .
Here, a positive relationship means friends or a trusted relationship and a negative
relationship means foes or a distrust relationship. Then, our goal becomes to find a
factorization of matrix R such that the learned latent features of users are consistent
with the constraints in S , where the consistency is reflected in the loss function. This
results in the following optimization problem:
2 λU
1  
λV

L(U, V) =
UF +
VF
Ri j − Ui,:
V j,: +
2
2
2
(i, j)∈R

+

λS
|S |



(D(Ui,: , U j,: ) − D(Ui,: , Uk,: )).

(4)

(i, j,k)∈S

Let us make this general formulation more specific by setting (·) and D(·, ·) to be the
hinge loss and the Euclidian distance, respectively. Under these two assumptions, the
objective can be formulated as
2 λU
1  
λV

L(U, V) =
VF
Ri j − Ui,:
V j,: + UF +
2
2
2
(i, j)∈R



R(U,V)

λS
+
|S |





max 0, 1 − Ui,: − U j,: 2 + Ui,: − Uk,: 2 .

(5)

(i, j,k)∈S

Here the constraints have been written in terms of hinge-losses over triplets, each
consisting of a user, his/her trusted friend, and his/her distrusted friend. Solving the
optimization problem in Eq. (5) outputs the latent features for users and items that
can be utilized to estimate the missing values in the user-item matrix. Comparing the
formulation in Eq. (5) to the existing factorization-based methods discussed earlier
reveals two main features of the proposed formulation. First, it aims to minimize
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:16

R. Forsati et al.

the error on the observed ratings and to respect the inherent structure of the social
network among the users. The trade-off between these two objectives is captured by
the regularization parameter λ S which is required to be tuned effectively.
In a similar way, applying the logistic loss to the general formulation in Eq. (4) yields
the following objective:
2 λU
1  
λV

L(U, V) =
UF +
VF
V j,: +
Ri j − Ui,:
2
2
2
(i, j)∈R

+

λS
|S |






log 1 + exp Ui,: − Uk,: 2 − Ui,: − U j,: )2 .

(6)

(i, j,k)∈S

Remark 4.1. We note that in several applications of recommender systems, besides
the observed ratings, a description of the users and/or the objects through attributes
(e.g., gender, age) or measures of similarity is available that could potentially benefit the
process of recommendation (see, e.g., [Agarwal and Chen 2010] for a few interesting
applications). In that case, it is tempting to take advantage of both known ratings
and descriptions to model the preferences of users. A natural way to incorporate the
available metadata is to kernalize the similarity measure between latent features based
on a positive definite kernel between pairs that can be deduced from the metadata.
More specifically, instead of simply using Euclidian distance as the similarity measure
between latent features in Eq. (5), we can use the kernel matrix K obtained from the
Laplacian of the graph obtained from the metadata to measure the similarity
 


D(Ui,: , U j,: ) = Ui,: − U j,: K Ui,: − U j,: ,


where K = (D − W)−1 , with D as a diagonal matrix with Di,i = nj=1 Wi j . Here, W
captures the pairwise weight between users in the similarity graph between users that
is computed based on the available metadata about users.
Remark 4.2. We would like to emphasize that it is straightforward to generalize the
proposed framework to incorporate similarity and dissimilarity information between
items. What we need is to extract the triplets from the trust/distrust links between
items and repeat the same process we did for users. This will add another term to the
objective in terms of latent features of items V, as shown in the following generalized
formulation:
2 λU
1  
λV

L(U, V) =
UF +
VF
Ri j − Ui,:
V j,: +
2
2
2
(i, j)∈R

+
+

λS
|S |
λI
|I |





max 0, 1 − Ui,: − U j,: 2 + Ui,: − Uk,: 2

(i, j,k)∈S





max 0, 1 − Vi,: − V j,: 2 + Vi,: − Vk,: 2 ,

(i, j,k)∈I

where λ I is the regularization parameter and I is the set of triplets extracted from
the similar/dissimilar links between items. The similarity/dissimilarity links between
items can be constructed according to tags issued by users or associated with items,
and categories. For example, if two items are attached with a same tag, there is a trust
link between them, and otherwise a distrust link. Alternatively, trust/distrust links
can be extracted by measuring similarity/dissimilarity based on the item properties or
profile if provided. This could further improve the accuracy of recommendations.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:17

ALGORITHM 1: GD-based Matrix Factorization with Trust and Distrust Propagation
1: Input: R: partially observed rating matrix, S
2: Output: U and V
3: for t = 1, . . . , T do
4:
Compute the gradients ∇U R(Ut , Vt ) and ∇V R(Ut , Vt ).
5:
Compute ∇U by Eq. 7
6:
Compute ∇V by Eq. 8
7:
Update:
Ut+1 = Ut − ηt ∇U |U=Ut ,V=Vt
Vt+1 = Vt − ηt ∇V |U=Ut ,V=Vt
8: end for
9: return UT +1 and VT +1 .

4.2. Batch Gradient Descent-Based Optimization

In optimization for supervised machine learning, there exist two regimes in which
popular algorithms tend to operate: the stochastic approximation regime, which
samples a small dataset per iteration, typically a single data point, and the batch or
sample average approximation regime, in which larger samples are used to compute an
approximate gradient. The choice between these two extremes outlines the well-known
trade-off between inexpensive noisy steps and expensive but more reliable steps.
Two preliminary examples of these regimes are the Gradient Descent (GD) and the
Stochastic Gradient Descent (SGD) methods, respectively. Both GD and SGD methods
start with some initial point and iteratively update the solution using the gradient
information at intermediate solutions. The main difference is that GD requires a full
gradient information at each iteration, while SGD only requires an unbiased estimate
of the full gradient which can be done by sampling.
We now discuss the application of the GD algorithm for solving the optimization
problem in Eq. (5), as detailed in Algorithm 1. Recall that the objective function is
not jointly convex in both U and V. On the other hand, the objective is convex in one
parameter by fixing the other one. Therefore, we follow an iterative method to minimize
the objective. At each iteration, first by fixing V, we take a step in the direction of the
negative gradient for U and repeat the same process for V by fixing U.
For ease of exposition, we introduce further notation. For any triplet (i, j, k) ∈ S , we
note that Ui,: − U j,: 2 − Ui,: − Uk,: 2 can be written as Tr(CU U), where Tr(·) denotes
the trace of the input matrix and C is a sparse auxiliary matrix defined for each triplet
with all entries equal to zero except: Cik = Cki = C j j = 1 and Ckk = Ci j = C ji = −1.
Having defined this notation, we can write the objective in Eq. (5) as
L(U, V) = R(U, V) +

λV
λS
λU
UF +
VF +
2
2
|S |






max 0, 1 − Tr Cikj U U .

(i, j,k)∈S

where Cikj is the C matrix previously defined which is associated with triplet (i, j, k).
To apply the GD method, we need to compute the gradient of L(U, V) with respect to
U and V, which we denote by ∇U = ∇U L(U, V) and ∇V = ∇V L(U, V), respectively. We
have
∇U = ∇U R(U, V) + λU U −

λS
|S |





k
1[Tr(Cikj U U)<1] UCik
j + UCi j ,

(i, j,k)∈S

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

(7)

17:18

R. Forsati et al.

where 1[·] is the indicator function which takes a value of one if its argument is true,
and zero otherwise. Similarly for ∇V , we have
∇V = ∇V R(U, V) + λV V.

(8)

The main shortcoming of the GD method is its high computational cost per iteration
due to the gradient computation (i.e., step (7)) which is expensive when the size of
social constraints S is large. We note that the size of S can be as large as O(n3 )
by considering all triplets in the social graph. In the next section, we provide an
alternative solution to resolving this issue using the stochastic gradient descent and
mini-batch SGD methods which are more efficient than the GD method in terms of the
computational cost per iteration but with a slow convergence rate in terms of target
approximation error.
4.3. Stochastic and Mini-Batch Optimization

As discussed, when the size of the social network is very large, the size of S may cause
computational problems in solving the optimization problem in Eq. (5) using the GD
method. The reason is essentially the fact that computing the gradient at each iteration
requires going through all the triplets in S , which is infeasible for large networks.
To alleviate this problem, we propose a stochastic gradient-based [Nemirovski et al.
2009] method for solving the optimization problem. The main idea is to choose a
fixed subset of triplets for gradient computation instead of all |S | triplets at each
iteration [Cotter et al. 2011]. More specifically, at each iteration, we sample B triplets
uniformly at random from S to compute the next solution. We note that this strategy
generates unbiased estimates of the true gradient and makes each iteration of the
algorithm computationally more efficient compared to the full gradient counterpart.
In the simplest case, the SGD algorithm, only one triplet is chosen at each iteration
to generate an unbiased estimate of the full gradient. We note that in practice, SGD
is usually implemented based on data shuffling, that is, making the sequence of the
training samples random and then training the model by going through the training
samples one by one. An intermediate solution, known as mini-batch SGD, chooses
a subset of triplets to compute the gradient. The promise is that by selecting more
triplets at each iteration, on one hand the variance of stochastic gradients decreases
promotional to the number of sampled triplets, and on the other hand the algorithm
enjoys the light computational cost of basic SGD method.
The detailed steps of the algorithm are shown in Algorithm 2. The mini-batch SGD
method improves the computational efficiency by grouping multiple constraints into a
mini-batch and only updating the U and V once for each mini-batch. For brevity, we
will refer to this algorithm as Mini-SGD. More specifically, the Mini-SGD algorithm,
instead of computing the full gradient over all triplets, samples B triplets uniformly at
random from S , where 1 ≤ B ≤ |S | is a parameter that needs to be provided to the
algorithm, and computes the stochastic gradient as


λS 
k
1[Tr(Cikj Ut Ut )<1] UCik
∇t =
j + UCi j ,
B
(i, j,k)∈ B

where  B is the set of B sampled triplets from S . We note that



λS
k
1[Tr(Cikj Ut Ut )<1] UCik
E[∇t ] =
j + UCi j ,
|S |
(i, j,k)∈S

that is, ∇t is an unbiased estimate of the full gradient in the right-hand side. When
B = |S |, each iteration handles the original objective function, and Mini-SGD reduces
to the batch GD algorithm. We note that both GD and SGD share the same convergence
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:19

ALGORITHM 2: Mini-SGD-Based Matrix Factorization with Trust and Distrust Propagation
1: Input: R: partially observed rating matrix, S , min batch size B
2: Output: U and V
3: for t = 1, . . . , T do
4:
∇t ← 0
5:
for b = 1, . . . , B do
6:
(i, j, k) ← Sample random triplet from S
7:
if (1 − Ui,: − U j,: )2 + Ui,: − Uk,: 2 > 0) then
8:
∇t ← Ut Cikj U
t
9:
end if
10:
end for
11:
Compute the gradients ∇U R(Ut , Vt ) and ∇V R(Ut , Vt ).
12:
Update:


λS
Ut+1 = Ut − ηt ∇U R(Ut , Vt ) + λU Ut +
∇t
B|S |
13:

Update:



Vt+1 = Vt − ηt ∇V R(Ut , Vt ) + λV Vt

14: end for
15: return UT +1 and VT +1 .

rate in terms of the√number of iterations in expectation for non-smooth optimization
problems (i.e., O(1/ T ) after T iterations), but the SGD method requires much less
running time to convergence compared to the GD method due to the efficiency of its
individual iterations.
5. EXPERIMENTAL RESULTS

In this section, we conduct exhaustive experiments to demonstrate the merits and
advantages of the proposed algorithm. We conduct the experiments on the wellknown Epinions2 dataset, aiming to accomplish and answer the following fundamental
questions.
(1) Prediction accuracy. How does the proposed algorithm perform in comparison to
the state-of- the-art algorithms with/without incorporating trust and distrust relationships between users. Whether or not the trust/distrust social network could
help in making more accurate recommendations?
(2) Correlation of social relations with rating information. To what extent are the
trusted and distrusted friends of a user u aligned with the ratings user u issued
for the reviews written by his friends? A positive answer to this question indicates
that two users will issue similar (dissimilar) ratings if they are connected by a trust
(distrust) relation and prefer to behave similarly.
(3) Model selection. What role do the regularization parameters λ S , λU , and λV play in
the accuracy of the proposed recommender system and what is the best strategy to
tune these parameters?
(4) Handling cold-start users. How does exploiting social relationships in the prediction
process affect the performance of recommendation for cold-start users?
(5) Trading trust for distrust. To what extent can the distrust relations compensate for
the lack of trust relations?
2 http://www.trustlet.org/wiki/Epinions

datasets.

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:20

R. Forsati et al.

(6) Efficiency of optimization. What is the trade-off between accuracy and efficiency by
moving from the gradient descent to the stochastic gradient descent with different
batch sizes?
In the following sections, we intend to answer these questions. We begin by introducing
the dataset we use in our experiments and the metrics we employ to evaluate the
results, followed by the detailed experimental results.
5.1. Dataset Description and Experimental Setup

The Epinions Dataset. We begin by discussing the dataset we have chosen for our
experiments. To evaluate the proposed algorithm on trust and distrust-aware recommendations, we use the Epinions dataset [Guha et al. 2004], a popular e-commerce site
and customer review website where users share opinions on various types of items such
as electronic products, companies, and movies, through writing reviews about them or
assigning a rating to the reviews written by other users. The rating values in Epinions
are discrete values ranging from not helpful (1/5) to most helpful (5/5). These ratings
and reviews could potentially influence future customers when they are about to decide
whether a product is worth buying or a movie is worth watching.
Epinions allows users to evaluate other users based on the quality of their reviews
and to make trust and distrust relations with other users in addition to the ratings.
Every member of Epinions can maintain a “trust” list of people he/she trusts that
is referred to as web of trust (social network with trust relationships) based on the
reviewers with consistent ratings or “distrust” list known as block list (social network
with distrust relationships) for reviewers whose reviews were consistently found to
be inaccurate or low quality. The fact that the dataset contains explicit positive and
negative relations between users makes it very appropriate for studying issues in
trust- and distrust-enhanced recommender systems. Epinions is thus an ideal source
for experiments on social recommendation. We remark that the Epinions dataset only
contains bivalent relations (i.e., contains only full trust and full distrust, and no gradual
statements).
To conduct the coming experiments, we sampled a subset of the Epinions dataset
with n = 121, 240 users and m = 685, 621 different items. The total number of observed
ratings in the sampled dataset is 12,721,437, which approximately includes 0.02% of
all entries in the rating matrix R which demonstrates the sparsity of the rating matrix.
We note that the selected items are the most frequently rated overall. The statistics
of the dataset are given in Table II. The social statistics of the this data source are
summarized in Table III. The frequencies of ratings for users are shown are Table IV.
In the user distrust network, the total number of issued distrust statements is 96,823.
As to the user trust network, the total number of issued trust statements is 481,799.
Experimental Setup. To better evaluate the effect of utilizing the social side information in recommendation accuracy, we employ different amount of training data 90%,
80% , 70%, and 60% to create four different training sets that are increasingly sparse,
but the social network remains the same in all of them. Training data 90%, for example, means we randomly select 90% of the ratings from the sampled Epinions dataset
as the training data to predict the remaining 10% of ratings. The random selection
was carried out five times independently to have a fair comparison. Also, since our
preliminary results on a smaller dataset revealed that hinge loss performs better than
exponential loss, in the rest of experiments, we stick to this loss function. However, we
note that exponential loss is slightly faster in optimizing the corresponding objective
function thanks to its smoothness, but it was negligible considering its worse accuracy
compared to hinge loss. All implementations are in Matlab, and all experiments were
performed on a 4-core 2.0GHZ of a load-free machine with a 12G of RAM.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:21

Table II. Statistics of Sample Data from Epinions Dataset
Used in Our Experiments
Statistic
Number of users
Number of items
Number of ratings
Number of trust relations
Number of distrust relations
Minimum number of ratings by users
Minimum number of ratings for items
Maximum number of ratings by users
Maximum number of ratings for items
Average number of ratings by users
Average number of ratings for items

Quantity
121,240
685,621
12,721,437
481,799
96,823
1
1
148735
945
85.08
15.26

Table III. Maximum and Average Trust and Distrust Relations
for Users in the Sampled Dataset
Statistics

Trust per user

Be Trusted per user

Max
Min
Average

1983
1
4.76

2941
0
4.76

Max
Min
Average

Distrust per user
1188
1
0.91

Be Distrusted per user
429
0
0.91

Table IV. Frequencies of User’s Rating
# of Ratings
# of Users

0–10
4,198,074
(≈33%)

11–20
3,053,144
(≈24%)

21–30
2,289,858
(≈18%)

31–40
1,526,572
(≈12%)

41–50
534,300
(≈4.2%)

51–60
267,150
(≈2.1%)

# of Ratings
# of Users

61–70
157,745
(≈1.24%)

71–80
143,752
(≈1.13%)

81–90
104,315
(≈0.82%)

91–100
43,252
(≈0.34%)

101–200
21,626
(≈0.17%)

201–300
10,686
(≈0.084%)

5.2. Metrics

5.2.1. Metrics for Rating Prediction. We employ two well-known measures, mean absolute
error (MAE) and root mean squared error (RMSE) [Herlocker et al. 2004] to measure the prediction accuracy of the proposed approach in comparison with other basic
collaborative filtering and trust/distrust-enhanced recommendation methods.
MAE is a very appropriate and useful measure for evaluating prediction accuracy in
offline tests [Herlocker et al. 2004; Massa and Avesani 2004]. To calculate MAE, the
predicted rating is compared with the real rating and the difference (in absolute value)
considered as the prediction error. Then, these individual errors are averaged over all
predictions to obtain the overall MAE value. More precisely, let T denote the set of
ratings to be predicted, that is, T = {(i, j) ∈ [n] × [m], Ri j needs to be predicted} and let
R̂ denote the prediction matrix obtained by algorithm after factorization. Then,


(i, j)∈T |Ri j − R̂i j |
,
MAE =
|T |
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:22

R. Forsati et al.

where Ri j is the real rating assigned by user i to item j, and R̂i j is the rating user i
would assign to item j that is predicted by the algorithm.
The RMSE metric is defined as


2
(i, j)∈T (Ri j − R̂i j )
RMSE =
.
|T |
The first measure (MAE) considers every error of equal value, while the second one
(RMSE) emphasizes larger errors. We would like to emphasize that even small improvements in RMSE are considered valuable in the context of recommender systems.
For example, the Netflix prize competition offered $ 1,000,000 reward for a reduction
of the RMSE by 10% [Victor et al. 2013].
5.2.2. Metrics for Evaluating the Correlation of Ratings with Trust/Distrust Relations. As part of
our experiments, we investigate how the explicit trust/distrust relations between users
in the social network are aligned with the implicit trust/distrust relations between
users conveyed from the rating information. We use recall, mean average precision
(MAP) [Manning et al. 2008], and normalized discount cumulative gain (NDCG) to
evaluate the ranking results. Recall is defined as the number of relevant friends divided
by the total number of friends in the social network. Precision is defined as the number
of relevant friends (trusted or distrusted) divided by the number of friends in the social
network. Given a user u, let ri be the relevance score of the friend ranked at position i,
where ri = 1 if the user is relevant to the u and ri = 0 otherwise. Then we can compute
the average precision (AP) as


i ri × Precision@i
.
AP =
# of relevant friends
MAP is the average of AP over all the users in the network.
NDCG is a normalization of the discounted cumulative gain (DCG) measure. DCG
is a weighted sum of the degree of relevancy of the ranked users. The weight is a
decreasing function of the rank (position) of the user, and therefore called discount.
NDCG normalizes DCG by the ideal DCG (IDCG), which is simply the DCG measure
of the best-ranking result. Thus NDCG measure is always a number in [0, 1]. NDCG
at position k is defined as
k

2ri − 1
NDCG@k = Zk
,
log(i + 1)
i=1

where k is also called the scope, which means the number of top-ranked users presented
to the user and Zk is chosen such that the perfect ranking has an NDCG value of 1.
We note that the base of the logarithm does not matter for NDCG, since constant
scaling will cancel out due to normalization. We will assume it is the natural logarithm
throughout this article.
5.3. Model Selection

Tuning of parameters (a.k.a model selection in learning community) is a critical problem in most of the learning problems. In some situations, the learning performance
may drastically vary with different choices of parameters. There are three parameters
in Eq. (5) that play very important roles in the effectivity of the proposed algorithm.
These are λU , λV , and λ S . Between these, λ S controls how much the proposed algorithm
should incorporate the information of the social network in completing the partially
observed rating matrix. In the extreme case, a very small value for λ S , the algorithm
almost forgets that social information exists between the users and only utilizes the
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:23

Fig. 2. Grid search to find the best values for λU and λC on the dataset with 90% of rating information.

observed user-item rating matrix for factorization. On the other hand, if we employ
a very large value for λ S , the social network information will dominate the learning
process, leading to poorer performance. Therefore, in order to not hurt the recommendation performance, we need to find a reasonable value for a social regularization
parameter. To this end, we analyze how the combination of these parameters affect the
recommendation performance.
We conduct a grid search on the potential values of two parameters λ S and λV to find
the combination with the best performance. Figure 2 shows the grid search results for
these parameters on a dataset with 90% of training data, where the optimal prediction
accuracy is achieved at point (14.8, 11) with the optimal RMSE = 1.12. We would
like to emphasize that we have done the cross-validation only for pairs of (λ S , λV ) and
(λ S , λU ), considering (i) the grid search for the triplet (λ S , λU , λV ) is computationally
burdensome, and (ii) our preliminary experiments showed that λV and λU behave
similarly with respect to λ S . Based on the results reported in Figure 2, in the remaining
experiments, we set λ S = 14.8, λV = 11, and λU = 13 when training is performed on
the dataset with 90% rating information. We repeat the same process to find out the
best setting of regularization parameters for other datasets with 80%, 70%, and 60%
rating data as well.
5.4. Baseline Methods

Here we briefly discuss the baseline algorithms against which we intend to compare
the proposed algorithm. The baseline algorithms are chosen from both types of
memory-based and model-based recommender systems with different types of trust
and distrust relations. In particular, we consider the following basic algorithms.
—MF (Matrix Factorization-based Recommender). This is the basic matrix
factorization-based recommender formulated in the optimization problem in Eq. (1),
which does not take social data into account.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:24

R. Forsati et al.

—MF+T (Matrix Factorization with Trust Information). To exploit the trust relations
between users in matrix factorization, Ma et al. [2009b] relied on the fact that the
distance between latent features of users who trust each other must be minimized
and can be formulated as the following objective:
n
1 
min
D(Ui,: , U j,: ),
U 2
i=1 j∈N+ (i)

where N+ (i) is the set of users the ith user trusts in the social network (i.e., Si j = +1).
By employing this intuition in the basic formulation in Eq. (1), Ma et al. [2009b]
solved the following optimization problem:
⎡
⎤
n


 

1
α
λ
λ
2
U
V

min ⎣
Ri j − Ui,:
V j,: +
D(Ui,: , U j,: ) +
UF +
VF ⎦ .
U,V
2
2
2
2
i=1 j∈N+ (i)

(i, j)∈R

—MF+D (Matrix Factorization with Distrust Information). The basic intuition behind
the algorithm proposed in Ma et al. [2009b] to exploit the distrust relations is as
follows: if user ui distrusts user u j , then we can assume that their corresponding
latent features Ui,: and U j,: would have a large distance. As a result, we aim to
maximize the following quantity for all users:
max
U

n
1 
D(Ui,: , U j,: ),
2
i=1 j∈N− (i)

where N− (i) denotes the set of users the ith users distrusts (i.e., Si j = −1). Adding this
term to the basic optimization problem in Eq. (1), we obtain the following optimization
problem:
⎡
⎤
n





1
β
λU
λV
2

min ⎣
Ri j − Ui,:
V j,: −
D(Ui,: , U j,: ) +
UF +
VF ⎦ .
U,V
2
2
2
2
(i, j)∈R

i=1 j∈N− (i)

—MF+TD (Matrix Factorization with Trust and Distrust Information). This algorithm
stands for the algorithm proposed in the present work. We note that there is no algorithm in the literature that exploits both trust and distrust relations in factorization
process simultaneously.
—NB (Neighborhood-Based Recommender). This algorithm is the basic memory-based
recommender algorithm that predicts a rating of a target item i for user u using a
combination of the ratings of neighbors of u (similar users) that already issued a
rating for item i. Formally,


u ∈N (u),Wuu >0 Wuu (Rui − R̄u)


R̂ui = R̄u +
,
(9)
u ∈N (u),Wuu Wuu
where the pairwise weight Wuu between pair of users (u, u ) is calculated by the
Pearson’s correlation coefficient [Herlocker et al. 2004]
—NB+T (Neighborhood with Trust Information) [Massa and Avesani 2004, 2009;
Golbeck 2005]. The basic idea behind the trust-based recommender systems proposed in TidalTrsut [Golbeck 2005] and MoleTrsut [Massa and Avesani 2004] is to
limit the set of neighbors in Eq. (9) to the users who are trusted by user u. The
distinguishing feature of these algorithms is the mechanism of trust propagation to
estimate the trust transitively for all the users. By adapting Eq. (9) to only consider
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

trustworthy neighbors in predicting the new ratings, we obtain


u ∈N+∗ (u),Wuu >0 Wuu (Rui − R̄u)


R̂ui = R̄u +
,
u ∈N+∗ (u),Wuu >0 Wuu

17:25

(10)

where N+∗ (u) is the set of trusted neighbors of u in the social network with propagated
trust relations (when there is no propagation, we have N+∗ (u) = N+ (u)). We note that
instead of Pearson’s correlation coefficient as the weighting schema, we can infer
the weights exploiting the social relation between the users. Since for the dataset
we consider in our experiments, the trust/distrust relations are binary values, the
social-based pairwise distance would be simply the hamming distance between the
binary vector representation of social relations of users. For implementation details,
we refer to Victor et al. [2011a, Chapter 6].
—NB+TD-F (Neighborhood with Trust Information and Distrust Information as Filtration) [Victor et al. 2011b, 2013]. A simple strategy for using distrust relations
in the recommendation is to filter out distrusted users from the list of neighbors in
predicting the ratings. As a result, we adapt Eq. (9) to exclude distrusted users from
the users’ propagated web of trust.
—NB+TD-D (Neighborhood-Based with Trust Information and Integrated Distrust Information) [Victor et al. 2011b, 2013]. In the same spirit as the filtration strategy,
we can use distrust relations to debug the trust relations. More specifically, if user u
trusts user v, v trusts w, and u distrusts w, then the latter distrust relation contradicts the propagation of the trust from u to w and can be excluded from the prediction.
In this method, distrust is used to debug the trust relations.
5.5. On the Consistency of Social Relations and Rating Information

As already mentioned, the Epinions website allows users to write reviews about products and services and to rate reviews written by other users. Epinions also allows
users to define their web of trust, that is, “reviewers whose reviews and ratings have
been consistently found to be valuable” and their block list, that is, “reviewers whose
reviews are found to be consistently inaccurate or not valuable”. Different intuitions
on interpreting these social information will result in different models. The main rationale behind incorporating trust and distrust relations in recommendation process
is to take the trust/distrust relations between users in the social network as the level
of agreement between ratings assigned to reviews by users.3 Therefore, investigating
the consistency or alignment between user ratings (implicit trust) and trust/distrust
relations in the social network (explicit trsut) become an important issue.
Here, we aim to empirically investigate whether or not there is a correlation between
a user’s current trustees/friends or distrusted friends and the ratings that user would
assign to reviews issued by his neighbors. Obviously, if there is no correlation between
the social context of a user and his/her ratings to reviews written by his neighbors,
then the social structure does not provide any advantage to the rating information.
On the other hand, if there exists such a correlation, then the social context could be
supplementary information to compensate for the lack of rating information to boost
the accuracy of recommendations.
The consistency of trust relations and rating information issued by users on the
reviews written by his trustees has been analyzed [Ziegler and Golbeck 2007; Guo et al.
2014]. However, Ziegler and Golbeck [2007] also claimed that social trust (i.e., explicit
trust) and similarity between users based on their issued ratings (i.e., implicit trust) are
3 In

the literature, the similarity between users conveyed from the rating information issued by users and
the direct relation in the social network are usually referred to as implicit and explicit trust, respectively.

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:26

R. Forsati et al.
Table V. Consistency of Implicit and Explicit Trust Relations in the Dataset for Different Ranges of
Ratings Measured in Terms of NDCG, Recall, and MAP
# of Ratings
0–20
21–40
41–60
61–80
≥81

NDCG@10

NDCG@20

Recall@10

Recall@20

Recall@40

MAP

0.083
0.108
0.117
0.120
0.135

0.078
0.103
0.112
0.117
0.126

0.054
0.080
0.083
0.088
0.091

0.092
0.125
0.128
0.132
0.151

0.156
0.198
0.225
0.230
0.253

0.140
0.190
0.208
0.230
0.244

Table VI. Consistency of Implicit and Explicit Distrust Relations in the Dataset for Different Ranges
of Ratings Measured in Terms of NDCG, Recall, and MAP
# of Ratings
0–20
21–40
41–60
61–80
≥81

NDCG@10

NDCG@20

Recall@10

Recall@20

Recall@40

MAP

0.065
0.071
0.082
0.089
0.104

0.057
0.068
0.072
0.078
0.096

0.045
0.060
0.075
0.081
0.087

0.071
0.077
0.085
0.105
0.125

0.132
0.140
0.158
0.164
0.191

0.130
0.134
0.152
0.160
0.183

not the same, and can be used complementary. According to Ma [2013], when comparing
implicit social information with explicit social information, the performance of using
implicit information is slightly worse. We further investigate the same question about
the consistency of distrust relations and ratings issued by users to their distrusted
neighbors. The positive answer to this question can be interpreted as follows. Given
that user u is interested in item i, the chances that v, trusted (distrusted) by u, also likes
this item i is much higher (lower) than for user w not explicitly trusted (distrusted)
by u.
To measure the similarity between users, there are several methods we can borrow
in the literature. In this article, we adopt the most popular approach that is referred to
as the Pearson correlation coefficient (PCC) P : U × U → [−1, +1] [Breese et al. 1998;
Massa and Avesani 2009], which is defined as

m
i=1 (Rui − R̄u)(Rvi − R̄v )
P(u, v) = 	

, ∀u, v ∈ U,

m
m
2×
2
(R
−
R̄
)
(R
−
R̄
)
ui
u
vi
v
j=1
j=1
where R̄u and R̄v are the average of ratings issued by users u and v, respectively. The
PCC measures the extent to which there is a linear relationship between the rating
behaviors of the two users, the extreme values being −1 and 1. The similarity of two
users becomes negative when users have completely diverging ratings. We note that
this quantity can be considered as the implicit trust between users that is conveyed
via ratings given by users.
To conduct this set of experiments, we first group all the users in the training data
set based on the number of ratings, and then measure the prediction accuracies of
different user groups. Users are grouped into five classes: [1, 20), [20, 40), [40, 60),
[60, 80), and >81. In order to have a comprehensive view of the ranking performance,
we present the NDCG, recall, and MAP scores of trust and distrust alignments on the
Epinions dataset in Table V and Table VI, respectively. We note that the dataset we
use in our experiments only contains bivalent trust values, that is, −1 and +1, and it
is not possible to have an ordering on the list of friends (time stamp of relations would
be an option to order the friends, but unfortunately it is not available in our dataset).
To compute the NDCG, we use the ordering of trusted/distrusted friends which yields
the best value.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:27

Table VII. Alignment Rate of Users in Establishing Trust/Distrust Relationships with Future Users in the
Social Network Based on the Majority Vote of Their Current Trusted/Distrusted Friends
Setting

Type of Relation (u ; w)

% of Relations

Alignment Rate (%)

n+ > n−

+
−
+
−
+

48.80
2.54
1.15
8.02
39.49

92.09
8.15
17.88
83.42
−

n+ < n−
n+ = n− > 0 or n+ = n− = 0

Note: The number of trusted friends (+) and distrusted friends (−) are denoted by n+ and n− ,
respectively. Here u denotes the current user and w stands for a future user in the network.

On the positive side, we observe a clear trend of alignment between ratings assigned
by a user and the type of relation he has made in the social network. This observation
coincides with our intuition. Overall, when more ratings are observed for a user, the
similarity calculation process will find more accurate similar or dissimilar neighbors
for this user, since we have more information to represent or interpret this user. Hence,
by increasing the number of ratings, it is conceivable from the results in Tables V
and VI that the alignment between implicit and explicit neighbors becomes better. By
comparing the results in Tables V and VI we can see that trust relations are slightly
better aligned than the distrust relations.
On the negative side, the results show that the NDCG on both types of relations is
small. One explanation for this phenomenon is that the Epinions dataset is not tightly
bound to a specific application. For example, a user may trust or distrust anther user
based on his/her comments on a specific product, but they might have similar taste
on other products. Furthermore, compared to other datasets such as FilmTrusts, the
Epinions dataset is a very sparse dataset, and consequently it is relatively inaccurate
to rely on the rating information to compute the implicit trust relations. Finally, our
approach to distinguishing trust/distrust lists from the rating information is limited
by the PCC trust metric we have utilized. We conjecture that better trust metrics
able to exploit other side information, such as time and interactional information,
would be helpful in distinguishing implicit trusted/distrusted friends, leading to better
alignment between implicit and explicit trust relations.
We also conduct experiments to evaluate the consistency of social network only
based on the trust/distrust relations between users. In particular, we investigate to
what extent a user’s relations are aligned with the opinion of his/her neighbors in
the social network. More specifically, let u be a user who is about to make a trust or
distrust relation to another user v. We assume that n+ number of u’s neighbors trust v
and n− number of u’s neighbors distrust v. We note that in the real dataset, the distrust
relations are hidden. To conduct this set of experiments, we randomly sample 30% of
the relations from the social network and use the remaining 70% to predict the type of
sampled relations4 by majority voting.
Table VII shows the results on the consistency of social relations. We observe that in
all cases there is an alignment between the opinions of a user’s friends and his/her own
relation (92.09% and 83.42% when the majority of friends trust and distrust the target
user, respectively). This might be due to the social influence of people on the social
network; however, it is hard to justify the existence of such a correlation in the Epinions
dataset which includes reviews for a diverse set of products and taste of users. One
interesting observation from the results reported in Table VII is the case where the
number of distrusted users dominates the number of trusted users (i.e., n− > n+ ). While
4A

more realistic way would be to use the time stamp of relations to create the training and test sets.

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:28

R. Forsati et al.

Table VIII. Accuracy of Prediction of Matrix Factorization with Three Different Methods Measured in Terms of
MAE and RMSE Errors
k

% of Training

10

60%
70%
80%
90%

20

60%
70%
80%
90%

MF

MF+T

MF+D

MF+TD

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

Measure

0.9813 ± 0.042
1.6050 ± 0.032
0.9462 ± 0.083
1.5327 ± 0.032
0.9150± 0.022
1.3824 ± 0.032
0.8921 ± 0.025
1.2166 ± 0.017

0.8561 ± 0.032
1.4125 ± 0.022
0.8332 ± 0.092
1.2407 ± 0.063
0.8206 ± 0.041
1.1906 ± 0.042
0.8158 ± 0.016
1.1403 ± 0.027

0.9720 ± 0.038
1.5036 ± 0.040
0.9241 ± 0.012
1.4405 ± 0.023
0.8722 ± 0.034
1.3155 ± 0.026
0.8736 ± 0.053
1.1869 ± 0.049

0.8310 ± 0.016
1.2294 ± 0.086
0.8206 ± 0.023
1.1562 ± 0.043
0.8113 ± 0.032
1.1061 ± 0.021
0.8025 ± 0.014
1.0872 ± 0.020

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

0.9972 ± 0.016
1.6248 ± 0.014
0.9688 ± 0.019
1.5162 ± 0.016
0.9365 ± 0.025
1.4081 ± 0.015
0.9224 ± 0.016
1.2207 ± 0.0 18

0.8431 ± 0.018
1.3904 ± 0.042
0.8342 ± 0.062
1.2722 ± 0.027
0.8172 ± 0.011
1.1853 ± 0.023
0.8128 ± 0.021
1.1402 ± 0.026

0.9746 ± 0.060
1.5423 ± 0.046
0.9350 ± 0.022
1.4540 ± 0.075
0.8705 ± 0.016
1.3591 ± 0.073
0.8805 ± 0.032
1.1933 ± 0.028

0.8475 ± 0.012
1.1837 ± 0.023
0.8290 ± 0.034
1.1452 ± 0.016
0.8129 ± 0.025
1.1049 ± 0.082
0.8096 ± 0.010
1.0851 ± 0.011

Note: The parameter k represents the number of latent features in factorization.

the distrust relations are private to other users, we can see that there is a significant
alignment between a users relation type and his distrusted friends.
5.6. On the Power of Utilizing Social Relationships

We now turn to investigate the effect of utilizing social relationships between users
on the accuracy of recommendations in factorization-based methods. In other words,
we would like to experimentally evaluate whether incorporating distrust can indeed
enhance the trust-based recommendation process. To this end, we run four different
MF (i.e., pure matrix factorization-based algorithm), MF+T (i.e., matrix factorization
with only trust relationships), MF+D (i.e., matrix factorization with only distrust relationships), and MF+TD (i.e., the algorithm proposed here) algorithms on the dataset.
We run the algorithms with k = 10 and k = 20 latent vector dimensions. As mentioned
earlier, different amounts of training data 90%, 80% , 70%, and 60% have been used to
create four different training sets that are increasingly sparse, but the social network
remains the same in all of them. We evaluate all algorithms by both MAE and RMSE
measures.
Table VIII shows the MAE and RMSE errors for the four sampled datasets. First, as
we expected, the performance of all learning algorithms improves with an increasing
number of training data. It is also not surprising to see that the MF+T, MF+D, and
MF+TD algorithms which exploit social side information perform better than the pure
matrix factorization-based MF algorithm. Second, the proposed algorithm outperforms
all other baseline algorithms for all the cases, indicating that it is effective for incorporating both types of social side information in the recommendation. This result by itself
indicates that besides trust relationships in the social network, distrust information is
also a rich source of information and can be utilized in recommendation algorithms. We
note that distrust information needs to be incorporated carefully, as its nature is totally
different from trust information. Finally, it is noticeable that MF+T outperforms the
MF+D algorithm due to a huge number of trust relations to distrust relations in our
dataset. It is also remarkable that users are more likely to be influenced by their friends
to make trust relations than the distrust relations due to the private nature of distrust
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:29

Table IX. Comparison with Other Popular Methods
Method

Parameter (s)

MAE

RMSE

MF
MF+T
MF+D
MF+TD

k = 10 and λU = λV = 5
k = 10, λU = λV = 5 , and α = 1
k = 10, λU = λV = 5 , and β = 10
k = 10, λU = 13, λV = 11 , and λ S = 14.8

0.8921
0.8158
0.8736
0.8025

1.2166
1.1403
1.1852
1.0872

NB
NB+T
NB+TD-F
NB+TD-D

p=1
p = 1 and q = 3
p = 1 and q = 3

0.9381
0.8904
0.8692
0.8728

1.5275
1.3455
1.2455
1.2604

Note: The reported values are the MAE and RMSE on the dataset with 90%
rating information. The values of parameters for each specific algorithm are
included in the second column.

relations in the Epinions dataset. This might lead us to believe that distrust relations
have better quality than trust relations, which requires a deeper investigation to be
verified.
5.7. Comparison to Baseline Algorithms

Another question that is worth investigating is how state-of-the-art approaches perform compared to the method proposed here. To this end, we compare the performance of the MF-TD algorithm with the baseline algorithms introduced in Section 5.4.
Table IX contains the results of our experiments with eight different algorithms on the
dataset with 90% rating data. The second column in the table represents the configuration of parameters used by each algorithm.
When we utilize trust/distrust relations in neighborhood-based algorithms, a crucial
decision we need to make is to which level the propagation must be performed (no propagation corresponds to single-level propagation which only includes direct neighbors).
Let p and q denote the level of propagation for trust and distrust relations, respectively.
Let us first consider the trust propagation to decide the value of p. We note that there
is a trade-off between accuracy and the level of trust propagation: longer propagation
levels results in less accurate trust predictions. This is due to the fact that when we
use longer propagation levels, the further away we are heading from each user, and
consequently decrease the confidence on the predictions. Obviously, this affects the accuracy of the recommendations significantly. As a result, for the trust propagation we
only consider single-level propagation by choosing p = 1 (i.e., N+∗ = N+ ). We also note
that since in the Epinions dataset, a user can not simultaneously trust and distrust
another user, in the neighborhood-based method with distrust relations, the debugging
only makes sense for propagated information. Therefore, we perform a three-level distrust propagation (q = 3) to constitute the set of distrusted users for each users. We
note that the longer the propagation levels, the more often distrust evidence can be
found for a particular user, and hence the less neighbors will be left to participate in
the recommendation process. For factorization-based methods, the value of regularization parameters, that is, λU , λV , and λ S , are determined by the procedure discussed in
Section 5.3.
The results of Table IX reveal some interesting conclusions as summarized here.
—From Table IX, we can observe that for factorization-based methods, incorporating
trust or distrust information boosts the performance of recommendation in terms of
both accuracy measures. This demonstrates the advantages of trust and distrustaware recommendation algorithms. We also can see that both MF+T and MF+D
perform better than the non-social MF, but the performance of MF+T is significantly
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:30

R. Forsati et al.

better than MF+D. As discussed, this observation does not indicate that trust relations are more beneficial than distrust relations, as in our dataset, only 16.7% of
relations are distrust relations. The MF+TD algorithm that is able to employ both
types of relations is significantly better than other algorithms, which demonstrates
the advantages of our proposed method in utilizing trust and distrust relations.
—Looking at the results reported in Table IX, it can immediately be noticed that
the incorporation of trust and distrust information in neighborhood-based methods
decreases the prediction error, but the improvement is not as significant as the
factorization-based methods. We note that for the NB+T method with longer levels
of propagation ( p = 2, 3), our experiments revealed that the accuracy remains almost
the same or has gotten worse on both MAE and RMSE measures, and this is why we
only report the results only for p = 1. In contrast, for distrust propagation, we found
out that q = 3 has a visible impact on the performance of both filtering and debugging
methods. We would like to emphasize that for longer levels of distrust propagation
in the Epinions dataset, that is, q > 4, we found that the size of the set of distrusted
users N−∗ (·) becomes large for most users, which degrades the prediction accuracy. We
also observe another interesting result about the performance of the NB+TD method
with filtering and debugging strategies. We found that although filtering generates
slightly better predictions, NB+TD-F performs almost as well as the NB+TD-D
method. Although this observation does not suggest any of these methods as the
method of choice in incorporating distrust, we believe that the accuracy might differ
from dataset to dataset, and it strongly depends on the propagation/aggregation
strategy.
—Considering the results for both model-based and memory-based methods in
Table IX, we can conclude a few interesting observations. First, we notice that
factorization-based methods with trust/distrust information perform better than the
neighborhood-based methods. Second, the incorporation of trust and distrust relations in matrix factorization has significant improvement compared to improvement
achieved by memory-based methods. Although the type of filtration or debugging
strategy could significantly affect the accuracy of incorporating distrust in memorybased methods, the main shortcoming of these methods comes from the fact that
these algorithms somehow exclude the influence of distrusted users from the rating prediction. This stands in stark contrast to the model proposed in this article
that ranks the neighbors based on the type of relation. This observation necessitates devising better algorithms for propagation and aggregation of trust/distrust
information in memory-based methods.
5.8. Handling Cold-Start Users by Social Side Information

In this section, we demonstrate the use of the social network to further illustrate the
potential of the proposed framework and the relevance of incorporating side information. To do so, as another set of our experiments, we intend to examine the performance
of proposed algorithm on cold-start users. Addressing cold-start users (i.e., users with
few ratings or new users) is very important for the success of recommender systems
due to the huge numbers of this type of users in many real-world systems. As a result, handling cold-start users is one of the main challenges in the existing systems.
To evaluate different algorithms, we randomly select 30%, 20%, 10%, and 5% as the
cold-start users. For cold-start users, we do not include any rating in the training data
and consider all the ratings made by cold-start users as testing data.
Table X shows the performance of the previously mentioned algorithms. As it is
clear from Table X, when the number of cold-start users is low with respect to the
total number of users, say 5% of total users, the affect of the distrust relationships
is negligible in prediction accuracy. But, when the number of cold-start users is high,
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:31

Table X. Accuracy of Handling Cold-Start Users and the Effect of Social Relations
% of Cold-start Users
30%
20%
10%
5%

Measure
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

MF

MF+T

MF+D

MF+TD

0.9923
1.7211
0.9812
1.7088
0.9334
1.4222
0.9134
1.3852

0.8824
1.5562
0.8805
1.4339
0.8477
1.3782
0.8292
1.2921

0.9721
1.6433
0.9505
1.6250
0.9182
1.4006
0.8633
1.3255

0.8533
1.4802
0.8472
1.2630
0.8322
1.2655
0.8280
1.2888

Note: The number of leant features in this experiments is set to k = 10. The first
column shows the number of cold-start users sampled randomly from all users
in the dataset. For the cold-starts users, all the ratings have been excluded from
the training data and used in the evaluation of three different algorithms.

exploiting the trust and distrust relationships significantly improves the performance
of the recommendation. This result is interesting, as it reveals that the lack of rating
information for cold-start and new users can be alleviated by incorporating the social
relations of users, and in particular, both trust and distrust relationships.
5.9. Trading Trust for Distrust Relationships

We also compare the potential benefit of trust relations to distrust relations in the
proposed algorithm. More specifically, we would like to see to what extent the distrust
relations can compensate for the lack of trust relations. We run the proposed algorithm
with the subset of trust and distrust relations and compare it to the algorithm which
only utilizes all of the trust relations. To set up this set of experiments, we randomly
sample a subset of trust relations and gradually increase the amount of distrust relations to see when the effect of distrust information compensates for the effect of missed
trust relations.
We sample 433,619 (approximately 90%) trust relations from the total 481,799 trust
relations and vary the number of distrust relations fed to the proposed algorithm.
Table XI reports the accuracy of the proposed algorithm for different numbers of distrust relations in the datasets. All these samplings have been done uniformly at random. We use 90% of all ratings for training and the remaining 10% for evaluation,
and set the dimension of the latent features to k = 10. As can be concluded from
Table XI, when we feed the proposed algorithm MF+TD with 90% of trust and 50%
of the distrust relations, it reveals very similar behavior to the trust-enhanced matrix factorization-based method MF+T, which only utilizes all the trust relations in
factorization. This result is interesting in the sense that the distrust information between users is as important as the trust information (we note that in this scenario,
the number trust relations excluded from the training is almost same as the number
of distrust relations included). By increasing the number of distrust relations, we can
observe that the accuracy of recommendations increases as expected. In summary, this
set of experiments validates that incorporating distrust relations could indeed enhance
the trust-based recommendation process and could be considered as a rich source of
information to be exploited.
5.10. On the Impact of Batch Size in Stochastic Optimization

As mentioned earlier, directly solving the optimization problem in Eq. (5) using full
gradient descent method requires going through all the triplets in the constraint set
S , which could be computationally expensive due to the huge number of triplets in
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:32

R. Forsati et al.
Table XI. Accuracy of Proposed Algorithm on a Dataset with 39,0257 (≈ 90%) Trust Relations
Sampled Uniformly at Random from All Trust Relations with Varied Number of Distrust Relations
Method

# of Trust Relations

MF+TD

433,619 (≈90%)

# of Distrust Relations
9,682 (≈10%)
19,364 (≈20%)
29,047 (≈30%)
38,729 (≈40%)
48,411 (≈ 50%)
58,093 (≈60%)
67,776 (≈70%)
77,458 (≈80%)
87,140 (≈90%)
96,823 (= 100%)

MF+T

481,799 (=100%)

0

Measure

Accuracy

MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE

0.8803 ± 0.051
1.2166 ± 0.028
0.8755 ± 0.033
1.1944 ± 0.042
0.8604 ± 0.036
1.1822 ± 0.081
0.8431 ± 0.047
1.1706 ± 0.055
0.8165 ± 0.056
1.1425 ± 0.091
0.8130 ± 0.035
1.1380 ± 0.046
0.8122 ± 0.041
1.1306 ± 0.042
0.8095 ± 0.036
1.1290 ± 0.085
0.8061 ± 0.044
1.1176 ± 0.067
0.8050 ± 0.052
1.1092 ± 0.063

MAE
RMSE

0.8158 ± 0.016
1.1403 ± 0.027

Note: The learning is performed based on 90% of all ratings with k = 10 as the dimension of
latent features.

S . To overcome this efficiency problem, one can turn to the stochastic gradient scent
method which tries to generate unbiased estimates of the gradient at each iteration in
a much cheaper way by sampling a subset of triplets from S .
To accomplish this goal, we perform gradient descent and stochastic gradient descent
to solve the optimization problem in Eq. (5) to find the matrices U and V following the
updating equations derived in Eqs. (7) and (8). At each iteration t, the currently learned
matrices Ut and Vt are used to predict the ratings in the testset. In particular, at each
iteration, we evaluate the RMSE and MAE on the testset and terminate training once
the RMSE and MAE starts increasing or once the maximum number of iterations is
reached. We run the algorithm with latent vectors of dimension k = 10.
We compare the computational efficiency between the proposed algorithm with GD
and mini-batch SGD with different batch sizes. We note that the GD updating rule
can be considered a mini-batch SGD, where the batch size B is deterministically set
to be B = |S |, and simple SGD can be considered a mini-batch SGD with B = 1. We
remark that in contrast to GD method which uses all the triplets in S for gradient
computation at each iteration, for the SGD method—due to uniform sampling over all
tuples in S —some of the tuples may be used more than once and some of the tuples
might never been used for gradient computation.
Figures 3 and 4 show the convergence rate of four different updating rules in terms
of the number of iterations t for two different measures RMSE and RME, respectively. The first algorithm denoted by GD runs the simple full gradient descent iteratively to optimize the objective. The other three algorithms SGD1, SGD2, and SGD3
in the figures use the batch sizes B = 0.1 ∗ |S |, B = 0.2 ∗ | S |, and B = 0.3 ∗ |S |,
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:33

Fig. 3. Comparison of accuracy of prediction in terms of RMSE with GD and SGD with three varied batch
sizes.

respectively. In our experiments, due to very slow convergence of the basic SGD method
with B = 1 in comparison to other fours methods, we simply exclude its result from the
discussion.
In terms of accuracy of predictions, from both Figures 3 and 4, we can conclude that
the GD has the best convergence and SGD3 has the worst convergence in all settings.
This is because, although all four of the algorithms use an unbiased estimate of the
true gradient to update the solution at each iteration, the variance of each stochastic
gradient is proportional to the size of the batch size B. Therefore, for larger values of B,
the variance of stochastic gradients is smaller, and the algorithm convergences faster,
but for smaller values of B, the algorithm suffers from high variance in stochastic
gradients and convergences slowly. We emphasize that this comparison holds for iteration complexity which is different from the computational complexity (running time)
of individual iterations. More specifically, each iteration of GD requires |S | gradient
computations, while for SGD, we only need to perform B  |S | gradient computations.
In summary, SGD has lightweight iteration but requires more iterations to converge.
In contrast, GD takes expensive steps in a much fewer number of iterations. From Figures 3 and 4, it is noticeable that although a large number of iterations is usually needed
to obtain a solution of desirable accuracy using SGD, the lightweight computation per
iteration makes SGD attractive for the optimization problem in Eq. (5) for large number
of users. We also not that for the GD method, the error is a monotonically-decreasing
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:34

R. Forsati et al.

Fig. 4. Comparison of accuracy of prediction in terms of MAE with GD and SGD with three varied batch
sizes.

function it terms of the number of iterations t, but for the SGD-based methods, this
does not hold. This is because although the SGD algorithm is guaranteed to converge to
an optimal solution (at least in expectation), there is no guarantee that the stochastic
gradients provide a descent direction for the objective at each iteration due to the noise
in computing gradients. As a result, for a few iterations, we can see that the objective
increases but finally it convergences as expected.
6. CONCLUSIONS AND FUTURE WORKS

In this article, we have made progress towards making distrust information beneficial
in the social recommendation problem. In particular, we have proposed a framework
based on matrix factorization which is able to incorporate both trust and distrust relationships between users in a factorization algorithm. We experimentally investigated
the potential of distrust as side information to overcome data sparsity and cold-start
problems in traditional recommender systems. In summary, our results showed that
more accurate recommendations can be obtained by incorporating distrust relations,
indicating that distrust information can indeed be beneficial for the recommendation
process.
This work leaves few directions, both theoretically and empirically, as future work.
From an empirical point of view, it would be interesting to extend our model for
weighted social trust and distrust relations. One challenge in this direction is that,
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:35

as far as we know, there is no publicly available dataset that includes weighted (gradual) trust and distrust information. Also, the experimental results we have conducted
on the consistency of social relations with rating information hint at a number of potential enhancements in future work. In particular, it would be interesting to further
examine the correlation between implicit and explicit distrust information. An important challenge in this direction is to develop better metrics to measure the implicit
trust between users, as the simple metrics such as the Pearson correlation coefficient
seem insufficient. Furthermore, since we only consider distrust between users, it would
be easy to generalize our model in the same way to incorporate dissimilarity between
items and investigate how it works in practice. Also, our preliminary results indicated
that hinge loss almost performs better than exponential loss, but from the optimization
viewpoint, exponential loss is more attractive due to its smoothness. So, an interesting
direction would be to use a smoothed version of hinge loss to gain from both optimization efficiency and algorithmic accuracy.
ACKNOWLEDGMENTS
The authors would like to thank the Associate Editor and three anonymous reviewers for their immensely
insightful comments and helpful suggestions on the original version of this article. R. Forsati would also
like to thank Professor Mohamed Mokbel, Department of Computer Science and Engineering, University of
Minnesota, for the opportunity to visit his research group while doing this work.

REFERENCES
Gediminas Adomavicius and Alexander Tuzhilin. 2005. Toward the next generation of recommender systems:
A survey of the state-of-the-art and possible extensions. IEEE Trans. Knowl. Data Eng. 17, 6 (2005),
734–749.
Deepak Agarwal and Bee-Chung Chen. 2010. fLDA: Matrix factorization through latent dirichlet allocation.
In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining. ACM, 91–100.
Paolo Avesani, Paolo Massa, and Roberto Tiella. 2005. A trust-enhanced recommender system application:
Moleskiing. In Proceedings of the ACM Symposium on Applied Computing. 1589–1593.
Giacomo Bachi, Michele Coscia, Anna Monreale, and Fosca Giannotti. 2012. Classifying trust/distrust relationships in online social networks. In International Conference on Privacy, Security, Risk and Trust
(PASSAT) and International Confernece on Social Computing (SocialCom). IEEE, 552–557.
Jesús Bobadilla, Fernando Ortega, Antonio Hernando, and Abraham Gutiérrez. 2013. Recommender systems
survey. Knowl. Based Syst. 46 (2013), 109–132.
John S. Breese, David Heckerman, and Carl Kadie. 1998. Empirical analysis of predictive algorithms for
collaborative filtering. In Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence.
Morgan Kaufmann Publishers Inc., 43–52.
Moira Burke and Robert Kraut. 2008. Mopping up: Modeling Wikipedia promotion decisions. In Proceedings
of the ACM Conference on Computer Supported Cooperative Work. ACM, 27–36.
Dorwin Cartwright and Frank Harary. 1956. Structural balance: A generalization of Heider’s theory. Psychol.
Rev. 63, 5 (1956), 277.
Gang Chen, Fei Wang, and Changshui Zhang. 2009. Collaborative filtering using orthogonal nonnegative
matrix tri-factorization. Inf. Process. Manag. 45, 3 (2009), 368–379.
Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan. 2011. Better mini-batch algorithms via
accelerated gradient methods. In Conference on Neural Information Processing Systems (NIPS). Vol. 24
1647–1655.
David Crandall, Dan Cosley, Daniel Huttenlocher, Jon Kleinberg, and Siddharth Suri. 2008. Feedback effects
between similarity and social influence in online communities. In Proceedings of the 14th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining. ACM, 160–168.
Sanjoy Dasgupta, Michael L. Littman, and David McAllester. 2002. PAC generalization bounds for cotraining. In Proceedings of the Conference on Neural Information Processing Systems. 375–382.
Mukund Deshpande and George Karypis. 2004. Item-based top-n recommendation algorithms. ACM Trans.
Inf. Syst. 22, 1 (2004), 143–177.
Thomas DuBois, Jennifer Golbeck, and Aravind Srinivasan. 2011. Predicting trust and distrust in social
networks. In Proceedings of the IEEE 3rd International Conference on Privacy, Security, Risk and Trust
(PASSAT), and IEEE 3rd International Conference on Social Computing (SocialCom). IEEE, 418–424.
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:36

R. Forsati et al.

Rana Forsati, Hanieh Mohammadi Doustdar, Mehrnoush Shamsfard, Andisheh Keikha, and Mohammad
Reza Meybodi. 2013. A fuzzy co-clustering approach for hybrid recommender systems. Int. J. Hybrid
Intell. Syst. 10, 2 (2013), 71–81.
Rana Forsati and Mohammad Reza Meybodi. 2010. Effective page recommendation algorithms based on
distributed learning automata and weighted association rules. Expert Syst. Appl. 37, 2 (2010), 1316–
1330.
Jennifer Golbeck. 2005. Computing and applying trust in web-based social networks. Ph.D. Dissertation,
University of Maryland at College Park.
Jennifer Golbeck. 2006. Generating predictive movie recommendations from trust in social networks. In Proceedings of the 4th International Conference on Trust Management (iTrust). Lecture Notes in Computer
Science, Vol. 3986, Springer, Berlin, 93–104.
Jennifer Golbeck and James Hendler. 2006. Filmtrust: Movie recommendations using trust in web-based
social networks. In Proceedings of the IEEE Consumer Communications and Networking Conference,
Vol. 96. Citeseer.
Nathaniel Good, J. Ben Schafer, Joseph A. Konstan, Al Borchers, Badrul Sarwar, Jon Herlocker, and John
Riedl. 1999. Combining collaborative filtering with personal agents for better recommendations. In Proceedings of the 16th National Conference on Artificial Intelligence and the 11th Innovative Applications
of Artificial Intelligence Conference (AAAI/IAAI). 439–446.
Quanquan Gu, Jie Zhou, and Chris Ding. 2010. Collaborative filtering: Weighted nonnegative matrix factorization incorporating user and item graphs. In Proceedings of the SIAM International Conference on
Data Mining (SDM). 199–210.
R. Guha, Ravi Kumar, Prabhakar Raghavan, and Andrew Tomkins. 2004. Propagation of trust and distrust.
In Proceedings of the 13th International Conference on World Wide Web. ACM, 403–412.
Guibing Guo, Jie Zhang, Daniel Thalmann, Anirban Basu, and Neil Yorke-Smith. 2014. From ratings to
trust: An empirical study of implicit trust in recommender systems. In Proceedings of the 29th ACM
Symposiam on Applied Computing (SAC).
Jonathan L. Herlocker, Joseph A. Konstan, Al Borchers, and John Riedl. 1999. An algorithmic framework
for performing collaborative filtering. In Proceedings of the 22nd Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval. ACM, 230–237.
Jonathan L. Herlocker, Joseph A. Konstan, Loren G. Terveen, and John T. Riedl. 2004. Evaluating collaborative filtering recommender systems. ACM Trans. Inf. Syst. 22, 1 (2004), 5–53.
Thomas Hofmann. 2004. Latent semantic models for collaborative filtering. ACM Trans. Inf. Syst. 22, 1
(2004), 89–115.
Mohsen Jamali and Martin Ester. 2009. TrustWalker: A random walk model for combining trust-based
and item-based recommendation. In Proceedings of the 15th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. ACM, 397–406.
Mohsen Jamali and Martin Ester. 2010. A matrix factorization technique with trust propagation for recommendation in social networks. In Proceedings of the 4th ACM Conference on Recommender Systems.
ACM, 135–142.
Mohsen Jamali and Martin Ester. 2011. A transitivity aware matrix factorization model for recommendation
in social networks. In Proceedings of the 22nd International Joint Conference on Artificial Intelligence,
Vol. 3. AAAI Press, 2644–2649.
Arnd Kohrs and Bernard Merialdo. 1999. Clustering for collaborative filtering applications. In Computational
Intelligence for Modelling, Control & Automation. IOS Press.
Ioannis Konstas, Vassilios Stathopoulos, and Joemon M. Jose. 2009. On social networks and collaborative
recommendation. In Proceedings of the 32nd International ACM SIGIR Conference on Research and
Development in Information Retrieval. ACM, 195–202.
Yehuda Koren. 2008. Factorization meets the neighborhood: A multifaceted collaborative filtering model.
In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining. ACM, 426–434.
Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender
systems. Computer 42, 8 (2009), 30–37.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. 2010a. Predicting positive and negative links in
online social networks. In Proceedings of the 19th International Conference on World Wide Web. ACM,
641–650.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. 2010b. Signed networks in social media. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 1361–1370.
Wu-Jun Li and Dit-Yan Yeung. 2009. Relation regularized matrix factorization. In Proceedings of the 21st
International Conference on Artificial Intelligence (IJCAI’09).
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

Matrix Factorization with Explicit Trust and Distrust Side Information

17:37

Juntao Liu, Caihua Wu, and Wenyu Liu. 2013. Bayesian probabilistic matrix factorization with social relations and item contents for recommendation. Decision Support Syst. 55, 3 (June 2013), 838–850.
Hao Ma. 2013. An experimental study on implicit social recommendation. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 73–82.
Hao Ma, Irwin King, and Michael R. Lyu. 2009a. Learning to recommend with social trust ensemble.
In Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in
Information Retrieval. ACM, 203–210.
Hao Ma, Michael R. Lyu, and Irwin King. 2009b. Learning to recommend with trust and distrust relationships. In Proceedings of the 3rd ACM Conference on Recommender Systems. ACM, 189–196.
Hao Ma, Haixuan Yang, Michael R. Lyu, and Irwin King. 2008. SoRec: Social recommendation using probabilistic matrix factorization. In Proceedings of the 17th ACM Conference on Information and Knowledge
Management. ACM, 931–940.
Hao Ma, Dengyong Zhou, Chao Liu, Michael R. Lyu, and Irwin King. 2011a. Recommender systems with
social regularization. In Proceedings of the 4th ACM International Conference on Web Search and Data
Mining. ACM, 287–296.
Hao Ma, Tom Chao Zhou, Michael R. Lyu, and Irwin King. 2011b. Improving recommender systems by
incorporating social contextual information. ACM Trans. Inf. Syst. 29, 2 (2011), 9.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to Information
Retrieval. Vol. 1. Cambridge University Press, Cambridge.
Paolo Massa and Paolo Avesani. 2004. Trust-aware collaborative filtering for recommender systems. In On
the Move to Meaningful Internet Systems 2004: CoopIS, DOA, and ODBASE. Lecture Notes in Computer
Science, Vol. 3290. Springer, 492–508.
Paolo Massa and Paolo Avesani. 2005. Controversial users demand local trust metrics: An experimental study
on Epinions.com community. In Proceedings of the 20th National Conference on Artificial Intelligence
(AAAI’05). 121–126.
Paolo Massa and Paolo Avesani. 2009. Trust metrics in recommender systems. In Computing with Social
Trust. Human-Computer Intercation Series, Springer, 259–285.
Prem Melville, Raymond J. Mooney, and Ramadass Nagarajan. 2002. Content-boosted collaborative filtering
for improved recommendations. In Proceedings of the 18th National Conference on Artificial Intelligence
(AAAI). 187–192.
Bradley N. Miller, Joseph A. Konstan, and John Riedl. 2004. PocketLens: Toward a personal recommender
system. ACM Trans. Inf. Syst. (TOIS) 22, 3 (2004), 437–476.
Andriy Mnih and Ruslan Salakhutdinov. 2007. Probabilistic matrix factorization. In Proceedings of the 21st
Annual Conference on Neural Information Processing Systems (NIPS). 1257–1264.
Uma Nalluri. 2008. Utility of distrust in online recommender systems. Technical Report, Coopstone.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. 2009. Robust stochastic approximation approach to stochastic programming. SIAM J. Optim. 19, 4 (2009), 1574–1609.
Akshay Patil, Golnaz Ghasemiesfeh, Roozbeh Ebrahimi, and Jie Gao. 2013. Quantifying social influence in
epinions. Human 2, 2 (2013).
Dmitry Pavlov and David M. Pennock. 2002. A maximum entropy approach to collaborative filtering in
dynamic, sparse, high-dimensional domains. In Proceedings of the Annual Conference on Neural Information Processing System (NIPS), Vol. 2. 1441–1448.
Michael J. Pazzani. 1999. A framework for collaborative, content-based and demographic filtering. Artif.
Intell. Rev. 13, 5–6 (1999), 393–408.
David M. Pennock, Eric Horvitz, Steve Lawrence, and C. Lee Giles. 2000. Collaborative filtering by personality diagnosis: A hybrid memory-and model-based approach. In Proceedings of the 16th Conference on
Uncertainty in Artificial Intelligence. Morgan Kaufmann Publishers Inc., 473–480.
Jasson D. M. Rennie and Nathan Srebro. 2005. Fast maximum margin matrix factorization for collaborative
prediction. In Proceedings of the 22nd International Conference on Machine Learning. ACM, 713–719.
Ruslan Salakhutdinov and Andriy Mnih. 2008a. Bayesian probabilistic matrix factorization using Markov
chain Monte Carlo. In Proceedings of the 25th International Conference on Machine Learning. ACM,
880–887.
Ruslan Salakhutdinov and Andriy Mnih. 2008b. Probabilistic matrix factorization. In Proceedings of the
22nd Annual Conference on Neural Information Processing Systems. 1257–1264.
Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. 2007. Restricted Boltzmann machines for collaborative filtering. In Proceedings of the 24th International Conference on Machine Learning. ACM,
791–798.

ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

17:38

R. Forsati et al.

Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item-based collaborative filtering
recommendation algorithms. In Proceedings of the 10th International Conference on World Wide Web.
ACM, 285–295.
Wanita Sherchan, Surya Nepal, and Cecile Paris. 2013. A survey of trust in social networks. ACM Comput.
Surv. 45, 4 (2013), 47.
Luo Si and Rong Jin. 2003. Flexible mixture model for collaborative filtering. In Proceedings of the 20th
International Conference on Machine Learning (ICML), Vol. 3. 704–711.
Ian Soboroff and Charles Nicholas. 1999. Combining content and collaboration in text filtering. In Proceedings
of the IJCAI Workshop on Machine Learning for Information Filtering, Vol. 99. 86–91.
Nathan Srebro and Tommi Jaakkola. 2003. Weighted low-rank approximations. In Proceedings of the 20th
International Conference on Machine Learning (ICML), Vol. 3. 720–727.
Nathan Srebro, Jason D. M. Rennie, and Tommi Jaakkola. 2005. Maximum-margin matrix factorization. In
Proceedings of the Conference on Neural Information Processing Systems (NIPS). 1329–1336.
Mojdeh Talabeigi, Rana Forsati, and Mohammad Reza Meybodi. 2010. A hybrid web recommender system
based on cellular learning automata. In Proceedings of the IEEE International Conference on Granular
Computing (GrC). IEEE, 453–458.
Nele Verbiest, Chris Cornelis, Patricia Victor, and Enrique Herrera-Viedma. 2012. Trust and distrust aggregation enhanced with path length incorporation. Fuzzy Sets Syst. 202 (2012), 61–74.
Patricia Victor, Chris Cornelis, Martine De Cock, and Ankur Teredesai. 2011b. Trust- and distrust-based
recommendations for controversial reviews. IEEE Intell. Syst. 26, 1 (2011), 48–55.
Patricia Victor, Chris Cornelis, and Martine De Cock. 2011a. Trust Networks for Recommender Systems.
Atlantis-Computational Intelligence Series, Vol. 4. Springer, Berlin.
Patricia Victor, Chris Cornelis, Martine De Cock, and Enrique Herrera-Viedma. 2011c. Practical aggregation
operators for gradual trust and distrust. Fuzzy Sets Syst. 184, 1 (2011), 126–147.
Patricia Victor, Nele Verbiest, Chris Cornelis, and Martine De Cock. 2013. Enhancing the trust-based recommendation process with explicit distrust. ACM Trans. Web 7, 2 (2013), 6.
Fei Wang, Sheng Ma, Liuzhong Yang, and Tao Li. 2006b. Recommendation on item graphs. In Proceedings
of the 6th International Conference on Data Mining (ICDM’06). IEEE, 1119–1123.
Jun Wang, Arjen P. De Vries, and Marcel J. T. Reinders. 2006a. Unifying user-based and item-based collaborative filtering approaches by similarity fusion. In Proceedings of the 29th Annual International ACM
SIGIR Conference on Research and Development in Information Retrieval. ACM, 501–508.
Grzegorz Wierzowiecki and Adam Wierzbicki. 2010. Efficient and correct trust propagation using closelook.
In Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent
Agent Technology (WI-IAT). Vol. 1. IEEE, 676–681.
Lei Wu, Steven C. H. Hoi, Rong Jin, Jianke Zhu, and Nenghai Yu. 2009. Distance metric learning from
uncertain side information with application to automated photo tagging. In Proceedings of the 17th
ACM International Conference on Multimedia. ACM, 135–144.
Gui-Rong Xue, Chenxi Lin, Qiang Yang, WenSi Xi, Hua-Jun Zeng, Yong Yu, and Zheng Chen. 2005. Scalable
collaborative filtering using cluster-based smoothing. In Proceedings of the 28th Annual International
ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 114–121.
Kai Yu, Anton Schwaighofer, Volker Tresp, Xiaowei Xu, and H.-P. Kriegel. 2004. Probabilistic memory-based
collaborative filtering. IEEE Trans. Knowl. Data Eng. 16, 1 (2004), 56–69.
Sheng Zhang, Weihong Wang, James Ford, and Fillia Makedon. 2006. Learning from incomplete ratings
using non-negative matrix factorization. In Proceedings of the 6th SIAM Conference on Data Mining
(SDM).
Yi Zhang and Jonathan Koren. 2007. Efficient bayesian hierarchical user modeling for recommendation
system. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval. ACM, 47–54.
Jianke Zhu, Hao Ma, Chun Chen, and Jiajun Bu. 2011. Social recommendation using low-rank semidefinite
program. In Proceedings of the 25th AAAI Conference on Artificial Intelligence.
Cai-Nicolas Ziegler. 2009. On propagating interpersonal trust in social networks. In Computing with Social
Trust Human-Computer Interaction Series, Springer, Berlin, 133–168.
Cai-Nicolas Ziegler and Jennifer Golbeck. 2007. Investigating interactions of trust and interest similarity.
Decision Support Syst. 43, 2 (2007), 460–475.
Cai-Nicolas Ziegler and Georg Lausen. 2005. Propagation models for trust and distrust in social networks.
Inf. Syst. Frontiers 7, 4–5 (2005), 337–358.
Received November 2013; revised April, June 2014; accepted June 2014
ACM Transactions on Information Systems, Vol. 32, No. 4, Article 17, Publication date: October 2014.

A Demonstration of MNTG A Web-based Road Network Traffic Generator
Mohamed F. Mokbel1 , Louai Alarabi2 , Jie Bao3 , Ahmed Eldawy4 , Amr Magdy5 , Mohamed Sarwat6 , Ethan Waytas7 , Steven Yackel8
of Minnesota, Minneapolis, MN 55455, USA 8 Microsoft 1 2 3 4 5 6 {mokbel ,louai ,baojie ,eldawy ,amr ,sarwat }@cs.umn.edu wayt0012@umn.edu7 spazard1@live.com8
1,2,3,4,5,6,7 University

Abstract--This demo presents Minnesota Traffic Generator (MNTG); an extensible web-based road network traffic generator. MNTG enables its users to generate traffic data at any arbitrary road networks with different traffic generators. Unlike existing traffic generators that require a lot of time/effort to install, configure, and run, MNTG is a web service with a user-friendly interface where users can specify an arbitrary spatial region, select a traffic generator, and submit their traffic generation request. Once the traffic data is generated by MNTG, users can then download and/or visualize the generated data. MNTG can be extended to support: (1) various traffic generators. It is already shipped with the two most common traffic generators, Brinkhoff and BerlinMOD, but other generators can be easily added. (2) various road network sources. It is shipped with U.S. Tiger files and OpenStreetMap, but other sources can be also added. A beta version of MNTG is launched at: http://mntg.cs.umn.edu.

generate traffic data in arbitrary spatial regions using existing traffic generators. For example, to be able to use Brinkhoff or BerlinMOD generators for a different city than the default shipped one (Oldenburg and Berlin for Brinkhoff and BerlinMOD generators, respectively), the user needs to first obtain the road network information for the city of interest, which is a tedious task by itself. For example, to get the road network information for the city of Chicago, a user may need to understand the format of OpenStreetMap [9], and then write a program that extracts the road network of Chicago from OpenStreetMap. After obtaining the new data, the user then needs to modify the obtained format to match the required one by either Brinkhoff or BerlinMOD. Such set of tedious operations made it hard for casual users to use these traffic generators for arbitrary spatial areas. As a testimony, one can observe that almost all the literature that harnessed these generators, used their default cities. This demo presents Minnesota Traffic Generator (MNTG) [7]; an extensible web-based road network traffic generator. MNTG is not a new traffic generator. Instead, it is a framework that encapsulates existing traffic generators and makes them easily accessible. MNTG overcomes the hurdles of existing traffic generators with three main features: (1) MNTG is a web service with a user-friendly map interface. Behind the scenes, MNTG carries the burden of configuring and running existing traffic generators. (2) MNTG can be used for any arbitrary spatial area, where users can just mark their area of interest on a map interface, and submit their traffic requests accordingly. (3) MNTG users do not need to worry about the processing time or computing resources, where MNTG has its own dedicated server that internally processes the request in a multi-threaded paradigm, and emails the user back when the data is generated. The notifying email includes a link to download and/or visualize the data. MNTG is extensible to support: (1) various traffic generators. Currently, MNTG is shipped with the two most common traffic generators, Brinkhoff and BerlinMOD, yet, it also has the interface that can be used to add new traffic generators. (2) various road network sources. It is currently shipped with the support for U.S. Tiger files [11] and OpenStreetMap [9], yet, it also has the interface that can be used to add other sources for road network data. A beta version of MNTG is launched as a web service for public use; a prototype can be accessed via http://mntg.cs.umn.edu. The beta version supports both Brinkhoff and BerlinMOD traffic generators on U.S Tiger files and OpenStreetMap data. The extensibility interface for adding

I.

I NTRODUCTION

Having access to road network traffic data is a major need to validate and evaluate indexing and query processing techniques in moving objects databases, spatio-temporal databases, and data streams. Unfortunately, such data is not easily available, and is usually of much smaller scale than needed. The process of extracting real traffic data requires installing and configuring many GPS-enabled devices and continuously monitoring the locations of such devices, which is a cumbersome task. For instance, GeoLife project [13] took more than four years to collect 17,621 trajectories dataset with the involvement of 182 volunteers in Beijing. As a result, researchers have been using existing traffic generators as a means of getting synthetic datasets that exhibit similar behavior to real data. The most two common traffic generators are Brinkhoff [1] and BerlinMOD [3], which have been widely adopted by large numbers of papers in the database literature, e.g., see [2], [5], [6], [8], [10], [12]. Even though existing traffic generators are quite useful, nonetheless, most of them suffer from the following: (1) It may take the user significant amount of effort to install and configure the traffic generation tool. For example, in order to run BerlinMOD, the user needs to first install a moving object database, i.e., SECONDO [4], and then get familiar with the script commands used to install it. After the installation, users still need to understand an extensive list of configuration parameters for each traffic generator. (2) It is not trivial to
This work is supported in part by the National Science Foundation, USA, under Grants IIS-0952977 and IIS-1218168.


Traffic generation request

Download/visualize traffic results

Traffic Data Users
Status Notification

System Frontend

Web Interface

Email Notifier
Status Notification

Download/ Visualization Tools

2)

System Backend

Road Network Converter
Traffic Requests

Results

Traffic Processor
Brinkhoff BerlinMOD Random New Generator

Traffic Results

as its output. This includes pruning all information outside the selected area as well as pruning the non road network information of the selected area. PrepareStandardOutput. This function takes the road network information from ExtractRoadNetwork as its input, and produces two standard text files node.txt and edge.txt, that contain the final set of nodes and edges in the selected area, respectively. The text files are in a standard format to make it portable to various traffic generators employed by MNTG traffic processor.

Road Network Data Sources Traffic Generator Developers New Data Source OpenStreetMaps US Tiger Files

B. Case Study 1: U.S. Tiger Files US Topologically Integrated Geographic Encoding and Referencing (Tiger) Files [11] are published by US census bureau on a yearly basis to provide the most recent information about US geographical data, which include city boundaries, road networks, address information, water features, and much more. A very unique feature of US Tiger Files is that the files are partitioned and organized based on US counties. In other words, all roads in a county are packed as one compressed file with a unique file identifier, e.g., tl_2010_01001_roads.zip, where tl means tiger line, 2010 indicates the publishing year of the data, 01001 is a unique identifier for the county (in this case is Autauga, Alabama), and roads represents the type of data. In this case, the function ExtractRoadNetwork identifies all counties that overlap with the rectangle area selected by the user. Then, it loads the road network files for all overlapped counties and filters out the road segments outside the selected region. Then, the PrepareStandardOutput function converts Tiger file format to the standard output format for node.txt and edge.txt. C. Case Study 2: OpenStreetMap OpenStreetMap [9] (OSM) is a free GIS service, contributed mainly by volunteers to record all spatial landmarks (including road networks) worldwide. All data in OSM is encapsulated into a large XML file Planet.osm that includes four primitive data types: (1) Node, that represents a spatial point by its latitude and longitude coordinates, (2) Way, that consists of a sequence of nodes to construct a line or a polygon, (3) Relation, that specifies the relation between ways and nodes, e.g., two ways are connected together, and (4) Tags, that provide description for other data types, i.e., node, Way and Relation, using a key-value pair. In this case, the function ExtractRoadNetwork first retrieves all geographical data with an "osm" file, based on the spatial area specified by the user, from OpenStreetMap interface. Then, it parses all XML key-value pairs in the downloaded "osm" file to extract the road network information. Then, PrepareStandardOutput converts the road network information (extracted from the "osm" file) to the standard output format for the files node.txt and edge.txt. IV. T RAFFIC P ROCESSOR

Fig. 1.

MNTG System Overview.

more generators or other road network sources is currently working internally under our support, and will be shown in the demo. Since its launch in February 2013, MNTG has received more than 500 traffic generation requests from researchers world wide. We envision that MNTG will be the de facto standard for generating road network data for researchers in spatial and spatio-temporal databases worldwide. II. S YSTEM OVERVIEW

Figure 1 gives an overview of MNTG architecture. A user interacts with MNTG through its system front-end that includes a web interface that allows users to submit traffic generation requests, an email notifier that retrieves the status updates from the back-end and keeps users posted, and download and visualization tools that allow users to download and/or visualize the generated data. Internally, MNTG system backend processes the traffic generation requests, where it includes two main components: (1) Road Network Converter, which is responsible for extracting the road network data from different data sources. It currently uses US tiger files or OpenStreetMaps. Yet, it is extensible to support other road network sources. (2) Traffic Processor, which takes the road network data and feeds it to the underlying traffic generator. It currently support both Brinkhoff and BerlinMOD data generators. Yet, it is extensible to include other data generators. III. ROAD N ETWORK C ONVERTER

MNTG employs a road network converter that (a) receives the user area of interest, (b) extracts the road network data for the area of interest, and (c) sends the extracted data to the traffic processor. We will first explain (and demonstrate) how to include a road network data source in MNTG, then, we will discuss two case studies that are already realized in MNTG; US Tiger files, and OpenStreetMap. A. Adding Road Network Data to MNTG To add a new road network data source, MNTG provides a template that consists of the following two function headers: 1) ExtractRoadNetwork. This function takes a rectangular area, defined by two <latitude, longitude> coordinates, as its input, and produces the road network information of the selected area

MNTG provides a wrapper around existing traffic generators to ensure their ease of use. The traffic processor component of MNTG is responsible on: (1) receiving the road

network data from the road network converter and feeds it to the underlying generator, (2) running the underlying selected traffic generator, and (3) producing the generating traffic to be downloaded and/or visualized on a map interface. We first explain how to include a traffic generator in MNTG. Then, we discuss two case studies that are already realized in MNTG; Brinkhoff and BerlinMOD traffic generators. A. Adding a Traffic Generator to MNTG To add a new traffic generator, MNTG provides a template that consists of the following three function headers: 1) RoadNetworkConvert. This function takes a road network in the standard format of two text files node.txt and edge.txt as its input. The output would be the same road network, but in a different format that is required by the traffic generator. TrafficGeneration. This function first pops up a dialogue interface to prompt the user to enter various parameters for the generation request, e.g., number of objects and time interval. Then, it takes these parameters along with the specified road network extracted from RoadNetworkConvert, and makes an external execution call to the traffic generator. TrafficResultConvert. This function takes its input as the output of the TrafficGeneration function. Then, it converts the input into a standard text format as < Object ID, Timestamp, type, latitude, longitude> that can be easily downloaded as a text file and/or visualized a map.

Traffic Generation Parameters

Traffic Generation Region

Fig. 2.

MNTG Web GUI: Traffic Generation (Google Maps Interface)

2)

3)

B. Case Study 1: Brinkhoff Generator Brinkhoff traffic generator is one of most widely used traffic generators [1] (cited 650+ per Google Scholar), where its general idea is to simulate the object movements from two random locations using the shortest path. To support Brinkhoff generator inside MNTG, we implement its three abstract functions as follows: The RoadNetworkConvert function converts the output of the Road Network Converter into two binary files based on the need of Brinkhoff, i.e., request_ID.node and request_ID.edge. The TrafficGeneration function prepares Brinkhoff configuration file, i.e., property.txt, and assembles the calling command based on the user's request. The TrafficResultConvert function converts the traffic data produced by Brinkhoff generator into our standard output format. C. Case Study 2: BerlinMOD Generator BerlinMOD is another very popular traffic generator [3], where it simulates human movements during the weekdays and weekends. Users can specify their work and home areas in the road networks, then the generator simulates the users movements based on two rules: (1) during the weekdays, a user leaves Home in the morning (at 8 a.m.+ T1), drives to Work, stays there until 4 p.m.+ T2 in the afternoon, and then returns back Home, (2) during the weekends, a user has an 0.4 probability to do an additional trip which may have 1-3 intermediate stops and ends at home.

To run the BerlinMOD traffic generator, a user would need to set up a SECONDO database [4], and uses a set of scripts to query it. To support BerlinMOD generator inside MNTG, we install and configure a SECONDO database in our server and implement the three abstract functions as follows: The RoadNetworkConvert function reads the standard road network files and transforms it to the format used in BerlinMOD. Ultimately, it produces a data file named street.data, where the road segments are represented by a pair of locations bounded by a set of brackets. The TrafficGeneration function prepares a customized query script for the SECONDO database, i.e., BerlinMOD_DataGenerator_RequestID.SEC. It does so by preparing a generic script in advance, and updates the parameters based on the user request. Then, the function runs the modified script to generate the traffic data. The TrafficResultConvert function converts the traffic data produced by BerlinMOD into the standard output format. V. D EMONSTRATION S CENARIO

The audience interact with MNTG through the following steps: (1) traffic request submission, where audience can submit their requests online by selecting the traffic generation area from either Google Maps or OpenStreetMap interface and the traffic generator method, (2) traffic data download and visualization, where audience can either download or visualize their requested data, (3) adding a new road network data source, where audience can add a new road network data source to MNTG, and (4) adding a new traffic generator, where audience can register a new traffic generator, and then uses it in their new traffic generation request. A. Traffic Request Submission Figure 2 gives the interface for MNTG. To generate data, a user performs the following four steps: (1) either drag/zoom the map or write an address in the search field to get the geographical area of interest, (2) draw a rectangle on the map for the traffic generation area, (3) select the traffic generator from the drop down menu, e.g., Brinkhoff or BerlinMOD, and (4) Click on the Generate button, and enter the parameters for the traffic simulation, including the user email address. Once the request is submitted, MNTG sends an email to the user acknowledging the receipt of that request. Upon

Map, which is given to the user as a verification for the new data to import. D. Traffic Processor Extension The audience may register a new traffic generator to MNTG. For demonstration purpose, we prepared a new traffic generator, termed RandomTraffic, which requires binary format of road networks, runs with a java file, and outputs the traffic result in a binary format. The RandomTraffic generator takes its input as the number of moving objects and average speed. Then, it selects random points as one per each moving object. For each random point, we select another random destination, and calculate the shortest path. Given the average speed, objects move along the shortest path till their destinations. We will show the audience the java template class file for the RandomTraffic generator, which includes the implementation of its three abstract functions, RoadNetworkConvert, RandomTraffic, and TrafficGeneration. Then, we will upload this file to MNTG, which indicates that a new traffic generator is registered. Audience will then restart MNTG to see that they can submit a new traffic generator request using the RandomTraffic generator. R EFERENCES
[1] T. Brinkhoff. A Framework for Generating Network-based Moving Objects. GeoInformatica, 6(2), 2002. [2] S. Chen, C. S. Jensen, and D. Lin. A Benchmark for Evaluating Moving Object Indexes. VLDB Journal, 1(2), 2008. [3] C. D untgen, T. Behr, and R. H. G uting. BerlinMOD: a Benchmark for Moving Object Databases. VLDB Journal, 18(6), 2009. [4] R. H. G uting, T. Behr, and C. D untgen. Secondo: A platform for moving objects database research and for publishing and integrating research implementations. IEEE Data Engineering Bulletin, 33(2), 2010. [5] H. Hu, J. Xu, and D. L. Lee. PAM: An Efficient and Privacy-Aware Monitoring Framework for Continuously Moving Objects. IEEE TKDE, 22(3), 2010. [6] H. Jeung, Q. Liu, H. T. Shen, and X. Zhou. A Hybrid Prediction Model for Moving Objects. In ICDE, 2008. [7] M. F. Mokbel, L. Alarabi, J. Bao, A. Eldawy, A. Magdy, M. Sarwat, E. Waytas, and S. Yackel. MNTG: An Extensible Web-based Traffic Generator. In SSTD, 2013. [8] M. F. Mokbel, X. Xiong, and W. G. Aref. SINA: Scalable Incremental Processing of Continuous Queries in Spatio-temporal Databases. In SIGMOD, 2004. [9] OpenStreetMaps. http://www.openstreetmap.org/. [10] H.-P. Tsai, D.-N. Yang, and M.-S. Chen. Mining Group Movement Patterns for Tracking Moving Objects Efficiently. IEEE TKDE, 23(2), 2011. [11] US TIGER LINES. http://www.census.gov/geo/maps-data/data/tigerline.html. [12] W. Wu, W. Guo, and K.-L. Tan. Distributed Processing of Moving K-Nearest-Neighbor Query on Moving Objects. In ICDE, 2007. [13] Y. Zheng, Y. Chen, X. Xie, and W.-Y. Ma. GeoLife2.0: A LocationBased Social Networking Service. In MDM, 2009.

Fig. 3.

MNTG Traffic Visualization (OpenStreetMap Interface)

(a) Extracted Nodes

(b) Extracted Edges

Fig. 4.

Adding New Road Network Data Source

completion of the traffic request, another email is sent to the user with two hyper links. One for downloading the generated data, and one for visualizing the data on MNTG web interface. The time MNTG takes to complete the request mainly depends on the request size and the underlying traffic generator, e.g., number of objects and simulation time. B. Traffic Data Download & Visualization Audience can use MNTG to download or visualize their requested data. Downloaded data will have a standard text format as follows:
OID TS Type 0 0 newpoint 1 0 newpoint Lat 44.98636 44.99894 Lng -93.29820 -93.18128

MNTG also stores the generated traffic data inside a MySQL database, which can be used for visualization. Traffic visualization is performed using Google Maps v3 API for displaying overlays in HTML. The data is loaded via ajax into the web page which then creates an overlay for each time stamp of the moving object. Figure 3 gives an example for traffic visualization, where the colored circles on the map represent the moving objects. The user can either view the animated object movements or move the handler on the bottom scroll to view the snapshot at a specific time. C. Adding a Road Network Data Source The demo attendees can add a new road network data source to MNTG. We have a prepared data set, termed HighwayRoadNetwork, which includes only the highways in US. We will show the audience how to register this new road network with MNTG using two abstract functions. Once registered, audience will see that they can submit a new request to MNTG using the new road network. Figure 4 depicts the nodes and edges of the HighwayRoadNetwork on a Google

SMILE: A Data Sharing Platform
for Mobile Apps in the Cloud
Jagan Sankaranarayanan
Hakan Hacıgümüş
NEC Labs America, Cupertino, CA

∗

Haopeng Zhang

∗

Mohamed Sarwat

University of Massachusetts Amherst

University of Minnesota

haopeng@cs.umass.edu

sarwat@cs.umn.edu

{jagan,hakan}@nec-labs.com
ABSTRACT

get a seamless experience as the three apps now behave as a single
entity.

We identify an opportunity to share data among mobile apps hosted
in the cloud, thus helping users improve their mobile experience,
while resulting in cost savings for the cloud provider. In this work,
we propose a platform for sharing data among mobile apps hosted
in the cloud. A “sharing” is specified by a triple consisting of: (a)
a set of data sources to be shared, (b) a set of specified transformations on the shared data, and (c) a staleness (freshness) requirement
on the shared data. The platform addresses the following two main
challenges: What sharings to admit into the system under a set of
specified constraints, how to implement a sharing at a low cost while
maintaining the desired level of staleness. We show that reductions
in costs are achievable by exploiting the commonalities between the
different sharings in the platform. Experimental evaluation is performed with a cloud platform containing 25 sharings among mobile apps with realistic datasets containing user, social, location and
checkin data. Our platform is able to maintain the sharings with very
few violations, even under a very high update rate. Our results show
that our method results in a cost savings of over 35% for the cloud
provider, while enabling an improved mobile experience for users.

Interestingly, with the increasing use of cloud-based resources,
many of these apps may be hosted in the same cloud infrastructure
(e.g., Amazon EC2). To enable such rich interactions, mobile apps
should make their datasets available for sharing, as a way of encouraging other apps to build complementary features. At the same time,
apps can consume several datasets from other apps in the cloud infrastructure. We identify two key considerations in sharing data that
is important to mobile app developers.
• App developers want reliable access to datasets and do not
want to deal with the complexity of creating and maintaining
mechanisms (e.g., APIs [1, 4] or web services [2]) for sharing
data. Furthermore, they desire a service that is flexible enough
to meet their needs while providing guarantees on the quality
of the service.
• App developers want timely access to datasets. Mobility of
users imposes limits on how much staleness app developers
can tolerate on the datasets. This is because many types of
user-related data get progressively less valuable with time. For
instance, the location of a mobile user that is 50 seconds stale
may be of limited use to a navigation app; however, 10 seconds stale data may be suitable.

1. INTRODUCTION
Mobile applications (apps) compete in an increasingly crowded
marketplace, with possibly thousand of apps performing similar or
identical functions. In this crowded marketplace, developers can differentiate their apps by offering features that make the user’s mobile
experience more personalized. For instance, apps like SpotiSquare
connect with Foursquare venues to determine the current location
(venue) of the user, and then choose a music playlist depending on
users’ current context (e.g., eating dinner, exercising, driving etc.).
To create such an experience requires that the app has access to additional information (i.e., datasets) about its user. The following
example shows possible interactions among three apps, showcasing
the benefits of sharing user information.

In this paper, we propose a data sharing service in the cloud called
SMILE (Sharing MIddLEware). The service provider, who manages
the cloud infrastructure, offers data sharing as service and like any
commercial business makes money by delivering services according
to agreed upon quality of service levels. Data sharing is achieved
by a mobile app developer (henceforth referred to as a consumer)
specifying a sharing. A sharing must identify datasets of interest,
the desired transformations on the data, and a staleness requirement.
The consumer and the provider enter into a Service Level Agreement
(SLA). The SLA is a contract specifying that the provider will ensure reliable access to the consumer on the shared data at the risk of
paying a penalty if it is not maintained at the agreed upon staleness.
To successfully achieve data sharing on a large scale, two practical
problems need to be solved by the provider. First, it is important to
determine if a new sharing can be admitted (i.e., accepted) into the
system and maintained at the appropriate staleness. This may not
always be possible, especially if the datasets are updated at a high
rate and the sharing needs to be maintained at a low staleness. If a
sharing is incorrectly admitted, it will result in significant losses for
the provider since the SLA may specify penalties for the provider in
case the sharing misses the staleness requirement. Second, implementing the sharings is not free in the sense that the provider has to

E XAMPLE 1. Consider the three apps — Opentable (restaurant
reservation), Plango (calendar) and Sonar (friends location monitoring). Appointments of users requiring dinner reservations are shared
by Plango with Opentable, which can then suggest restaurant options
to users. Sonar can suggest a nearby restaurant as a meeting place
by sharing their location information with Opentable. Mobile users
∗Work done while at NEC Labs
(c) 2014, Copyright is with the authors. Published in Proc. of EDBT on
OpenProceedings.org. Distribution of this paper is permitted under the
terms of the Creative Commons license CC-by-nc-nd 4.0

688

10.5441/002/edbt.2014.75

2.

pay for the resources (i.e., CPU, Disk, Network) consumed in the
cloud. Reducing provider cost is another important consideration.
The two practical problems we outlined above pose significant
technical challenges that we address in this paper.

RELATED WORK

While there has been some work on sharing in a mobile environment, they consider sharing either in an adhoc setting, such as between two mobile users [19], or among a group of mobile users [22].
A middleware for connecting mobile users and devices has been proposed [27] for providing various mobile services, such as management, security, and context awareness, but not for sharing.
Sharing using MVs adds interesting dimensions to a well studied problem domain. An MV maintenance process traditionally is
broken into a propagation step, where updates to the MV are computed and an apply step, where updates are applied to the MV. First
of all, the autonomy of the tenants means that synchronous propagation algorithms [10], where all sources are always at a consistent snapshot, are unsuitable for our purposes. Furthermore, to deal
with the autonomy of the tenants, one has to resort to a compensation algorithm [28], where the propagation is computed on asynchronous source relations [5, 24, 29]. In particular, MVs over distributed asynchronous sources have been studied in the context of
a single data warehouse [5, 29] to which all updates are sent. The
key optimization studied in [5, 29] is in terms of reducing the number of queries needed to bring the MVs to a consistent state in the
face of continuous updates on the source relations. [24] shows how
n-way asynchronous propagation queries can be computed in small
asynchronous steps, which are rolled together to bring the MVs to
any consistent state between last refresh and present. Reducing the
cost of maintenance plans of a set of materialized view S is explored
in [20], where common subexpressions [23] are created that are most
beneficial to S. Their optimization is to decide what set of common
subexpressions to create and whether to maintain views in an incremental or recomputation fashion. Staleness of MVs in a data warehouse setup is discussed in [16], where a coarse model to determine
periodicity of refresh operation is developed.
As we will see later in the paper, our setup is different from [5,29]
in the sense that multiple MVs are maintained on multiple machines
in our multitenant cloud database. Moreover, different update mechanisms with different costs and staleness properties can be generated
based on where the updates are shipped as well as where the intermediate relations are placed, making the problem harder than [5,29].
Next, [24] assumes that all the source relations are locally available on the same machine, which makes the application of their approach to our problem infeasible without an expensive distributed
query. We combine propagation queries from [24] with join ordering [11, 18], such that propagation queries involving n source relations are computed in a sequence involving two relations at a time,
requiring no distributed queries. In particular, we first ensure that
the update mechanisms can provide SLA guarantees, after which
common expressions among the various sharing arrangements are
merged to reduce cost for the provider, which is similar in spirit
to [20, 23]. Our work adds several additional dimensions to [20, 23]
in terms of placement of relations, capacity of machines, SLA guarantee, and cost.
In contrast to [16], which determines the periodicity of the refresh operation of MVs maintained in a warehouse, our work is distinguished in the following way. Our work develops refresh cycles
for multiple MVs from distributed sources with different staleness
requirements while simultaneously reducing the total maintenance
cost. This is significantly more complicated than the simple setup
in [16] where they develop a simple model for determining a single
refresh periodicity between a RDBMS and data warehouse without
considering cost.
Our work is related to traditional view selection problems [6] in
the sense that the set of sharing arrangements could have been obtained via the application of a view selection algorithm taking the

1. Maintaining the shared datasets at required staleness: The
shared datasets are maintained as materialized views (MVs)
and are always kept under the staleness specified in the SLAs.
This is challenging because the sharings involve multiple
datasets with varying staleness requirements. Note that there
are many other ways (e.g., APIs, web services) of enabling
sharing in the cloud; a discussion on the various methods and
the pros-and-cons of each is given in [25].
2. Testing for admissibility: As noted above, there is a need
for an effective method to decide whether to accept or decline a sharing agreement under multiple constraints, such as
the given staleness, SLA penalty and platform cost considerations.
3. Cost reduction for the provider: To reduce cost, the platform provider needs to identify commonalities across multiple sharings with different staleness requirements, in order to
save computational effort.
These three problems may not be specific to data sharing for mobile
apps only, but also applicable to other areas. However, we observe
that mobility makes these problems challenging and the solutions
much more relevant in real world settings.
Some of these problems have been previously considered in the
context of MV maintainance [5, 16, 23, 24] and multi-query optimization [11, 26]. However, prior work either considers the mechanics of MV maintainance [24] and refresh rates [16], or cost savings by removing commonalities [23, 26], but not staleness and cost
requirements simultaneously. While important, the feasibility and
economic value of sharing as well as infrastructure and privacy considerations are considered by [9, 13, 25], thus it is not the focus in
this work.
In this work, we focus on the practical and technical challenges
of enabling data sharing for mobile apps in the cloud. Mobility provides a perfect use-case scenario as it aligns with the three elements
of our problem setup: it is cloud-based, requires reliable access to
rich information, and has strict staleness requirements on datasets.
To our knowledge, this work represents the first systematic, cloudbased platform for enabling data sharing with staleness guarantees.
We make the following contributions in this paper.
1. A declarative sharing platform, which is fully implemented as
a part of industrial system, with staleness guarantees on the
sharings (Section 3).
2. A method of determining the admissibility of sharings ensures
that the system only admits those sharings that it can maintain
(Section 6).
3. A method for reducing the cost of maintaining the sharings by
amortizing work across multiple sharings, where each sharing
has its own constraints (Section 7).
4. Experimental evaluation is performed on a cloud platform
with 25 sharings posed on realistic user, location, social, and
checkin datasets. Our results show that the SMILE platform
can maintain a large number of sharings with very low SLA
violations, even under a high rate of updates. By amortizing
work across multiple sharings, SMILE is able to achieve a cost
savings of over 35% for the provider (Section 9).

689

consumer workload as input. Our problem shares common aspects
with the cache investment problem [14] in terms of placement (what
and where to be cached) of intermediate results and the goodness
(another notion of staleness) of cache. Cache refresh in [14] piggy
banks on queries, whereas we establish a dedicated mechanism to
keep the MVs at the desired staleness. Our work shares common
elements with [15] in the sense that merging data flow paths with
common tuple lineage is similar to the way we perform plumbing
operations on a sharing plan.
A related data sharing effort in the cloud is the F LEX S CHEME [8],
where multiple versions of shared schema are maintained in the
cloud, with the focus on enabling evolution of shared schema used
by multiple tenants. Data markets [9] for pricing data in the cloud
looks at the problem from tenant and consumer perspectives, but we
look at the problem from the provider’s perspective. A similar but
not identical problem is reducing the cost for the consumers (i.e., fair
pricing of common queries) [9] and sharing work across concurrent
running queries [26]. Although we only concern ourselves with the
staleness of the data as the only quality measure of the data being
shared, other considerations such as data completeness, accuracy are
also applicable here [7]. In our problem, the challenge is to maintain
the sharing arrangements always maintained on agreed upon terms
(i.e., SLAs), while keeping down the infrastructure costs. Satisfying
these dual goals makes the sharing problem challenging from the
provider’s perspective.

E XAMPLE 2. Plango (i.e., Calendar app) makes the base relation of User_Events of events extracted from users’ calendar
available for sharing. Opentable has its own relation User_Accts
of users that use the app. It specifies a sharing, “I want to know
about dinner events for the users who use my app within 10 seconds
of a new event being recorded so that I could offer recommendations to them.” The base relations are User_Events and User_Accts, and the transformations are specified as the following SPJ
query: EventType=“dinner" from a join of User_Events
and User_Accts. The staleness t is specified as 10 seconds and
the penalty pens is $.001 per late delivery of a tuple.
After the sharing Si is defined as per the above example, it is
given to the provider for deciding admissibility and implementation
in the platform, which is described in Section 6.

4.

SMILE ARCHITECTURE

Figure 1 shows the architecture of the system. There is a set of
machines available to implement the sharings. Each machine runs a
single database instance (Postgresql in our case). The SMILE platform consists of three main components — (a) delta capture, (b)
sharing optimizer, and (c) sharing executor — that perform the following functions, respectively: (a) capture changes (i.e., delta) on
the base relations as updates are applied on them; (b) generates plan
for moving these updates from the base relations to the MVs; and
(c) schedules the movement of these updates by taking system fluctuations into account. We briefly describe the three main system
components below.

Sharing Plan
Machine

Agent

Agent

Machine

¢R
Postgresql

Database

Gateway

R

Machine

DELTA CAPTURE

MV

Push

Workload

Machine

Heartbeat

The provider has to consider a set S of sharings {S1 , S2 · · · Sm },
for inclusion in the sharing platform. Here, we consider the case
where there are no existing sharings in the system, yet the solution
we develop is equally applicable to the case when the platform already has several prior sharings. Our solution that we later develop
will identify which of the sharings in S should be admitted into
the sharing framework, while at the same time minimizing provider
cost and meeting SLA requirements. Each Si specifies the applicable datasets, transformations, staleness requirements and penalties
as described next.
To specify a sharing Si , a consumer starts by identifying datasets
(i.e., base relations) of interest, or subsets of datasets. Next, the consumer must determine a way to combine these datasets by specifying
transformations on the data. In this work, we restrict the transformations on the base relations to include the following three operators.

Agent

Postgresql

3. PRELIMINARIES

Agent

Infrastructure

Pub/Sub
Input
Sharing
Sharing
Sharings S
Executor
Optimizer
Data Sharing Framework

1. Choose a subset of tuples using a selection predicate

Figure 1: Architecture of the sharing platform

2. Choose a subset of the attributes

4.0.1 Delta Capture and Timestamps

3. Combine base relations using a common key

As the base relations are updated, a delta capturing mechanism
(i.e., tuple creation, deletion or updates) records the modified tuples. Our mechanism uses the Streaming Replication facility [3] in
Postgresql to capture the deltas. This module in Postgres allows
the Write Ahead Log (WAL) to be streamed on to another Postgresql instance in recovery mode so that a nearly identical replica
of a database can be maintained. Our module fakes itself as a Postgresql instance and obtains a WAL stream. The modified tuples are
extracted from the stream, unpacked and written to the disk.
Every base relation R is associated with a delta relation, denoted
by ∆R that records the modified tuples as update queries are applied
on R. The tuples in ∆R are populated by the delta capturing module. The MVs in the system also contain corresponding delta tables.
If R is a MV then ∆R contains both prior updates as well as those
that have not yet been applied to R. The tuples in ∆R of a MV is
populated, moved and applied by the sharing executor.

In other words, the transformation can be specified using a SelectProject-Join (SPJ) query that is applied on the base relations. The
consumer next specifies a staleness requirement t expressed in time
units (e.g., 20 seconds) on the shared data as well as any applicable penalty pens . A sharing in SMILE is enabled by the creation
of a materialized view (MV), which describes the transformations
over the base relations. For each sharing Si , the system creates a
MV which is always maintained within a staleness of t time units as
specified by Si . This means even though the base relations are independently updated, the state of the MV is always consistent with the
state of the base relations within t seconds.
As an illustration of defining a sharing, we revisit Example 1 and
provide a more concrete example of a sharing that the Opentable
(i.e., restaurant) app may define using the SMILE platform.

690

Every relation, delta of a relation or MV in our platform records
its last modification timestamp. The timestamps are generated using
a distributed clock [17] that is periodically synchronized. Each tuple
in the delta also records an associated timestamp. Maintaining the
sharings at their appropriate level of staleness is achieved by keeping track of the last modification timestamps of the base relations
and comparing them to the timestamp of the MV. The SMILE system maintains an up-to-date timestamp information on each sharing,
hence is aware of the current staleness of all the sharings. Updates
are moved from the base relations to the MV in a way that ensures
that the sharings do not miss their SLAs.

4.0.2

that the vertices are relations or deltas of relations tied to a particular machine, and the edges apply transformational operators. The
plan is expressed using the following four edge operators, that 1)
apply updates (DeltaToRel) , 2) copy updates between machines
(CopyDelta) , 3) join updates (Join), and 4) union (i.e., merge) updates (Union).
As the plan operates on base relations that are asynchronously
updated, the input vertices to an operator may have different timestamps. An operator takes any mismatch in the timestamps into
account by rolling back all the input vertices to the minimum of
the timestamps among its inputs. This is referred to as compensations [28]. Rolling back the timestamp of a relation or a MV is
possible due to the delta relations associated it. The operators for
applying, copying and merging updates are based on their standard
interpretations, except that they additionally apply compensations to
the inputs as the first step. Our join operator performs a compensation which is an implementation of the algorithm from [28].
We will not provide the implementational details of the operators
but instead show an example of a plan that performs a relational join
on two asynchronous base relations A and B on different machines.
The plan is referred to as “in-place” as it does not involve making
the copies of the base relations. The vertices and the edge operators
in the plan periodically move the updates from the base relations to
the MV to keep it maintained incrementally.

Sharing Optimizer

Given a set S of new sharings, the sharing optimizer generates an
update mechanism for each sharing in S using a three step procedure
described below.
a. A sharing Si in S can be admitted if the system can maintain
Si at the desired level of staleness. This determination is necessary to prevent the system from entering into SLAs that it
cannot satisfy (Section 6).
b. If Si is admissible, we generate its sharing plan such that it
can move updates from the base relations to the MV within
the time specified in the staleness SLA. Moreover, the sharing
plan is also cost effective in terms of its infrastructure resource
consumption (Section 6).

A

ΔA

B

DELTA CAPTURE

ΔB

c. Once the individual sharing plans of all the sharings in S are
determined, commonalities across sharings are identified and
removed to produce a single global sharing plan D that implements all the sharings (Section 7).

4.0.3

DELTA CAPTURE

ΔA

COPY
UPDATES

JOIN

JOIN

Δ(A⋈ΔB)

Δ(ΔA⋈B) Machine m

Machine m1

Sharing Executor

2

COPY
UPDATES

COPY
UPDATES

Δ(A⋈ΔB) UNION

The sharing executor is the execution engine of the system which
maintains the sharings at or below the required staleness level. The
sharing executor is an implementation of an asynchronous view
maintenance algorithm [24].
The sharing executor computes the current staleness of a sharing
by taking the difference between the maximum of the timestamps of
all the base relations to that of the MV. The executor keeps track of
which of the sharings will soon miss their staleness SLA. It schedules the updates to be applied on the MV so that its staleness is
reduced. Each machine in the infrastructure runs an agent that communicates with the sharing executor via a pub/sub system (e.g., ActiveMQ). The agents send periodic messages to the sharing executor
with the last modification timestamps of the base relations and the
MVs.
Our implementation of the executor is lazy by design in the sense
that it does not refresh unless it is absolutely necessary or the sharing
will miss its SLA. This way, the executor bunches as much work as
possible thereby reducing redundant effort. The refresh is neither too
early nor too late, but finishes just before a sharing is about to miss
its staleness SLA. We provide more details on the sharing executor
in Section 8.

ΔB

UPDATES

Δ(A⋈B)
Machine m3

Δ(ΔA⋈B)
A⋈B

APPLY UPDATES

Figure 2: One possible plan involving an in-place join of a base
relation A on machine m1 and B on machine m2 such that the
resulting MV A 1 B is placed on machine m3

E XAMPLE 3. Figure 2 shows the plan of a sharing Si that performs a transformation A 1 B on two base relations, A and B.
The plan is a DAG consisting of 12 vertices and 10 edges. The vertices are either base relations (e.g., A, B or its copies), MVs (e.g.,
A 1 B) or delta relations (e.g., ∆(∆A 1 B)). The edges corresponds to operators that either apply, copy, merge, join updates,
to complete the transformation path from the base relations to the
MV. Note that select and project predicates can be specified in Si ’s
transformation. All the four edge types can apply select and project
predicates to their inputs if one is specified in addition to their usual
functionalities. We handle these predicates by using the pushdown
heuristic [11].

5. SHARING PLAN

Given a sharing that specifies a set of transformations on the base
relations, the plan generation algorithm enumerates all the plans that
implement the sharing. However, not all of the plans satisfy the
constraints we develop in the reminder of this section. In particular,
we concern ourselves with two key properties of a plan, namely its
critical time path and dollar costs, which are described below.

The update mechanism of a sharing is implemented as a sharing
plan, which is analogous to a query execution plan in databases. We
will henceforth refer to it simply as a plan in the rest of the paper. The plan is expressed in terms of four operators that form the
transformational path for the updates from the base relations to the
MV. This is represented using a Directed Acyclic Graph (DAG) such

691

5.1

Critical Time Path

The critical time path of the plan is the longest path in terms of
seconds that represents the most time consuming data transformation path in the plan. Note that the plan is admissible only if the
length of its critical time path is less than the required staleness of
the sharing, or else the system cannot maintain it.
The sharing optimizer estimates the critical time path of a plan
using a time cost model for each operator. The model estimates the
time taken for each operator given the size of the updates. Note that
finding the longest path between two vertices on a general graph
is an NP-hard problem, but the plans are DAGs, on which longest
path calculation is tractable. The system implements the procedure
CP(p) that takes a sharing plan p and outputs its critical time path
in seconds. For example, in the plan p shown in Figure 2, CP(p)
computes the time taken along the longest transformation path from
A or B to the MV A 1 B. Section 9 provides additional details on
how we developed the time cost model for the four operators.

5.2

C OST(p) = resCost(p) · (1 +

CP (p)
) + e(λ−µ)·s · pens
s

(1)

resCost(p) is the cost of resource usage. As discussed before, to avoid SLA violation due to multiple sharings competing for resource, we over-provision the resource by a factor of
CP (p)/s where CP (p) is the length of the critical time path of
p. e(λ·a−µ)·s · pens is the estimated penalty of missing the staleness
SLA due to higher-than-expected tuple arrival rate, where pens is
the penalty of missing the staleness SLA for a single tuple.

6.

SHARING OPTIMIZER

The goal of the sharing optimizer is to produce a low-cost admissible plan. Satisfying the dual constraints of finding an admissible
plan that is provably cheapest amongst all plans is a hard problem.
A sharing Si specifies SPJ transformations on a set of base relations. As the base relations are hosted on different machines, there
are several ways of combining them as well as where to place the
intermediate results. This results in plans with varying dollar cost
and critical time paths. For instance, performing many operations
in parallel on different machines may produce a plan with a small
critical time path. But such a plan may have a high dollar cost due to
high infrastructure costs involved in using many machines. On the
other hand, operations can also be performed sequentially to reduce
the dollar cost but at the expense of a high critical time path.
Among the generated plans those that have a critical time path
greater than the SLA of Si cannot be maintained by the system at the
desired staleness level, and hence are not admissible. The admissibility of plans forms the hard constraint of our problem in the sense
that the system should not admit a sharing that cannot be handled
by the system. At the same time, it also should not deny admitting
sharings that otherwise should have been admissible.
The sharing optimizer is based on the polynomial time heuristic
solution developed for System-R [11] and its analogous distributed
variant R∗ [18]. Our approach relies on generating, using a dynamic
programming approach, the cheapest possible plan in terms of dollar
costs, regardless of its critical time path and another plan with the
smallest critical time path, regardless of its dollar costs. We refer to
these plans as Dynamic Programming Dollar (DPD) and Dynamic
Programming Time (DPT), respectively. The DPD and DPT plans
have the following properties:

Cost Model

The cost of the plan, expressed in dollars per second, is computed by the amount of CPU, network, and disk capacity consumed
to setup the sharing and maintain it at the required staleness. The
provider periodically moves the updates to the MVs and buys CPU,
disk and network capacities from the Infrastructure as a Service
(IaaS). This cost can be further divided into two categories: resource
usage (i.e., CPU, disk capacity, network capacity) and penalty due
to possible SLA violations.
Resource Usage. There are existing analytical models that estimate the usage of various resources for maintaining a MV, based on
update rate, join selectivity, data location, etc. (e.g., [21]). This
analytical model is implemented as a resCost function that computes the cost of the resources consumed by a plan. Furthermore,
the resource usage should also vary with the staleness SLA of the
sharing. When the required staleness is much longer than the critical time path, e.g., the critical time path is 1 second and the staleness
requirement is 30 seconds, the sharing executor has much flexibility in deciding when to update the MV. Specifically, given a new
tuple to the base relations, the service provider can push it to the
MV immediately, or wait for as long as 29 second before pushing it.
On the other hand, when the staleness becomes close to the critical
time path, there is much less flexibility since other sharings in the
infrastructure may compete for resources.
In order to reduce the negative interaction at low staleness values,
the resources allocated to the plan are over-provisioned by a factor
that is inversely proportional to the required staleness. This simple
strategy ensures that the negative interactions are mostly avoided at
low staleness values.
SLA Penalty. At low staleness values the natural fluctuations in
the update rates may cause a plan to miss the SLA. This is because
the plan estimates the critical time path using the average arrival rate,
but in practice this is an over simplification as the updates frequently
vary. So, we have to estimate how much of penalty may be incurred
given the required staleness, which also has to be factored into the
cost. We estimate this by assuming a Possion arrival of updates, and
modeling the plan as a M/M/1 queuing system. Given the arrival rate
of each base relations, we can estimate the arrival rate of tuples in
the MV based on the selectivity of joins. The average service time of
the M/M/1 queue corresponds to the most time consuming operator
in the plan.
For an M/M/1 queue with arrival rate λ and service rate µ, the
percentage of items with sojourn time t larger than the staleness SLA
s is P (t > s) = e(λ−µ)·s . Thus the dynamic cost of a plan p with
staleness s is calculated as:

1. DPT is a plan with a low critical time path that is not optimized
on the operating dollar cost. If DPT is not admissible, then the
sharing can be safely rejected by the provider as there cannot
be a plan with a lower critical time path.
2. DPD is a plan with a low operating dollar cost that is not optimized on the critical time path. If DPD is admissible, then it
is also of the cheapest cost.
We provide a dynamic programming formulation to produce DPT
and DPD in Section 6.1, and provide a plan generation algorithm in
Section 6.2.

6.1 Dynamic Programming Formulation
We cast the problem of generating a plan as a bottom-up dynamic
programming formulation given by J OIN C OST in Algorithm 1. Consider a sharing that specifies a join sequence on the base relations.
For example, Figure 2 shows a join sequence of length two using the
two base relations, A and B.
Let Si be a sharing in S such that S RC(Si ) is the set of source
vertices of Si and MV(Si ) is the vertex corresponding to the MV

692

ΔA
ΔB

B

COPYDELTA
COPYDELTA

JOIN

Δ(A⋈ΔB)

m1

COPY
DELTA Δ(A ΔB)
⋈
UNION

Δ(A⋈B)

COPY
DELTA

ΔB
JOIN

Δ(ΔA⋈B)

Δ(ΔA⋈B)
DELTA
TOREL

m3

A⋈B

ΔA

m2

A

B
JO
IN

(a) (b)

B

ΔB

m2

COPYDELTA

JOIN

Δ(A⋈ΔB)
COPY
DELTA

ΔA
ΔB

Δ(ΔA⋈B)

B
A

m1

UNIO

Δ(ΔA⋈B)

A

A

ΔA
JOIN

DELTA

m1

Δ(A⋈ΔB)

Δ(ΔA⋈B)

COPYDELTA

m3

Δ(ΔA⋈B)
UNION

N

m3 Δ(A⋈B)

Δ(A⋈B) DELTATOREL A⋈B

m1

JOIN

COPYDELTA

COPYDELTA

Δ(A⋈ΔB)

ΔB

ΔA COPY

m2

ΔA

B

COPY
DELTA
DELTATOREL

ΔA

A⋈B

DELTA
TOREL

A

m3

ΔB

B
JOIN

JOIN

Δ(A⋈ΔB)
DELTATOREL

ΔB

m2

COPY
DELTA

A

(c) (d) Δ(ΔA⋈B)

Δ(A⋈ΔB)
UN

ION

Δ(A⋈B)

A⋈B

DELTATOREL

Figure 3: Four ways of joining A and B, (a) in-place (no copies of A or B), (b) copy B (c) copy A, (d) copy A and B.
Algorithm 1 sub J OIN C OST(<R, mi >)
1: for all join sequences R − a do
2: for mj ∈ set of available machines do
3:
(C AP<R−a,mj > , D <R−a,mj > ) = J OIN C OST(<R − a, mj >)
4:
for mk ∈ set of available machines do
5:
C AP0 ← C AP<R−a,mj >
6:
D 0 ← D <R−a,mj >
7:
Choose cheapest case among (a)–(d), update C AP0 and D 0
8:
Case (a): In-place join of <R − a, mj > with <a, mk >
9:
Case (b): Copy <R − a, mj > to <R − a, mk >. In-place join of

of Si . SLA(Si ) is the staleness SLA of Si . M AC(Si ) denoted the
set of dedicated machines available to host the sharing. Below, the
short-hand notation <v, m> denotes any vertex v (i.e., relation, MV
or a delta of a relation) in the plan that is placed on a machine m.
We capture the state of the problem up on creating a join sequence
R on a machine m (i.e., <R, m>) as (D<R,m> , C AP<R,m> ) such that:
1. D<R,m> is the cheapest plan among all those that produce R
on machine m. The cheapest plan is chosen by applying a
cost function C OST C ALC, which takes a plan as input and
produces a cost value. Later, we will specify two implementations of C OST C ALC that will produce the DPT and DPD
plans.

10:
11:
12:

<R,m>

13:
14:
15:
16:

2. C AP
is the remaining capacity in the infrastructure (e.g.,
CPU, disk, network capacities) after discounting the capacity
consumed by D<R,m> .
We generate the join sequence R at machine m bottom-up by
enumerating all the states corresponding to: a) R−a on any machine
in M AC(Si ), b) with any remaining capacity. R − a refers to a
join sequence R without a base relation a. D<R,m> is generated by
adding vertices and edges required to join R − a with a to the plan
from the prior state. Among the plans generated this way, we choose
the plan with the smallest value produced by C OST C ALC.
Such a formulation is used by J OIN C OST to obtain the plan of any
arbitrary join sequence R on mi ∈ M AC(Si ), which is formed by
joining R − a on mj ∈ M AC(Si ) (i.e., <R − a, mj >) with a base
relation a on machine mk ∈ M AC(Si ) (i.e., <a, mk >).
At a high level, given two relations A on machine m1 , and B on
machine m2 , we consider four ways of producing A ./ B on a machine m3 , which are illustrated in Figure 3. In particular, Figure 3a
shows the case where A ./ B is produced without copying any of
the base relations (i.e., in-place join). Figure 3b–c show the cases
when one of the base relations is copied. Figure 3d shows the case
when both A and B are copied to m3 , and then an in-place join is
performed. Note that the four cases (a)–(d) in the J OIN C OST formulation (lines 8–11) correspond to the four ways of joining two
relations A and B in Figure 3.
J OIN C OST uses an in-place join and a copy procedure to update
the plan D<R−a,mj > to produce R on machine mi . We explain below the procedure of creating a copy of a relation and joining two
relations using A and B in place of R − a and a, respectively.
To copy of <A, m1 > to form <A, m2 > requires the addition
of one vertex and two edges – vertex <A, m2 > and CopyDelta
between <∆A, m1 > and <∆A, m2 >, and DeltaToRel (apply updates) between <∆A, m2 >. An in-place join between <A, m1 > and
<B, m2 > to produce <A ./ B, m3 > can add up to 8 vertices and 8
edges, as shown in Figure 3 depending on whether m1 , m2 and m3
are all distinct machines.
After adding the necessary vertices and edges, the capacities of
machines involved are modified using the resCost function defined

<R − a, mk > and <a, mk >
Case (c): Copy <a, mk > to <a, mj >.
In-place join of
<R − a, mj > with <a, mj >.
Case (d): Copy <R − a, mj > to <R − a, mi >. Copy <a, mk >
to <a, mi >. In-place join of <R − a, mi > and <a, mi >
Update C AP<R,mi > , D <R,mi > with D 0 and C AP0 if C OST C ALC(D 0 )
is cheaper.
end for
end for
end for
return C AP<R,mi > , D <R,mi >

previously. If there is no capacity left in m1 or m2 , C OST C ALC
function would cost such a plan at ∞ indicating that the plan is infeasible.

6.2 Plan Generation Algorithm
Finally, we describe an algorithm which takes a sharing Si and
generates two sharing plans depending on the choice of the cost
function, which we had left unspecified earlier. Recall that the DPD
plan has a low dollar cost, whereas the DPT plan has a low critical
time path.
If a DPD plan is needed, C OST C ALC uses C OST function (described in Section 5.2), which computes the cost of a plan in dollars
per second. If a DPT plan is needed, C OST C ALC uses the CP function (described in Section 5.1), which computes the critical time path
of the plan.
The plan generation algorithm computes the cheapest way to build
all join sequences of length 1 < x ≤ |S RC(Si )| in a bottom-up
manner, where S RC(Si ) is set of base relations of Si . The algorithm
obtains the cost to construct longer join sequences of length x, using
the output of prior invocations for join sequences of length x − 1.
The algorithm terminates when it produces the join sequences corresponding to the set of transformations specified in the sharing. The
sharing optimizer generates both DPD and DPT plans. If the DPT
plan is not admissible, then it means that there may not exist a plan
of Si that is admissible and hence, Si is rejected by the provider. If
DPD and DPT are both admissible, the sharing optimizer uses DPD
as it has a lower cost than DPT.

7.

MULTI-SHARING OPTIMIZATION

If two different admitted sharings share similar vertices and edges,
there could be an opportunity to further reduce the cost. The
provider can take advantage of this commonality by amortizing the

693

operating costs across several sharings. The commonality here is
replacing two disjoint sets of vertices and edges belonging to different sharings that perform identical or similar transformation with a
common set for multiple sharings.
Although our idea of merging commonalities in plans is similar as
merging common subexpressions in concurrent running query execution plans [26], there are two main differences. First, our infrastructure contains multiple servers and the cost of moving the data
across the servers has to be considered. Second, as we show below,
unlike [26] we do not restrict to only merging identical subexpressions across plans.
D is a global plan obtained by merging the plan of sharings in
S and then discarding duplicate edges and vertices. Note that D
need not be a single connected component. Each vertex (or edge)
v ∈ D records the identity of all the sharings that it serves, such that
S HR(v) records the sharings to which v belongs. Given a vertex (or
a set of vertices) v, let A NC(v) be the set of vertices and edges that
are ancestors of v in D.
Commonalities in D are reduced by applying a plumbing operator
repeatedly until the D does not perform any redundant work. A
plumbing operator takes two sets of vertices v1 and v2 belonging to
plans as inputs. Then, vertices and edges that supply the vertices in
v2 (or v1 ) are retained but those supplying v1 (or v2 ) are discarded.
We now discuss the mechanics of a plumbing operator as well as an
algorithm to apply them.

7.1

Note that we do not preclude other kinds of plumbing operations
here as long as they do not compromise the correctness of the plan
and result in a cost reduction.
A plumbing operation p is implemented as follows. First, add
the necessary edges and vertices to perform p. For all vertices and
edges v ∈ A NC(D ST(p)), remove S HR(D ST(p)) from the S HR(v).
For all v ∈ A NC(S RC(p)), add S HR(D ST(p)) to S HR(v). Add
S HR(D ST(p)) to S HR(S RC(p)) as well. Remove vertices and edges
v ∈ A NC(D ST(p)) s.t., S HR(v) = ∅.

7.2 Optimization Algorithm
Given the global plan D, we want to identity a set of plumbing operations to perform, that would produce the provably least cost plan
without exceeding the staleness SLA of any of the sharings. The
hardness of this problem comes from the observation that a plumbing can affect the benefit of other plumbings. Figure 4c shows an
example of three plumbings pi , pj and pk that affect one another.
For example, pi cannot be applied if pj has already been applied to
D as applying pj would have removed the vertices in S RC(pi ). For
the same reason pj cannot be applied if pk has already been applied.
Finally, the benefit and the increase to the critical time path due to
pi is affected if pk has already been already applied.
An optimal algorithm that chooses a set of plumbings to perform
resulting in a provably cheapest plan is a hard problem. For our purposes here any plan that is cheaper than D is good enough. Our algorithm is referred to as greedy hill-climbing approach as it applies
one plumbing at a time in a greedy fashion, until no more plumbings can be applied to D. This algorithm has at least the desirable
property that it is fast to execute and intuitive to understand.
Given the global plan D, let P be the set of all possible plumbing
operations that can be performed on D. The greedy hill-climbing
algorithm at each iteration first computes P and then applies the
plumbing operation p ∈ P with the maximum benefit. The set of
plumbings P is obtained using a two step process. First, we examine
all pairs of vertices in D to determine if a copy plumbing can be performed between them. Second, we examine all applicable triples of
vertices to determine if a join plumbing can be performed between
them. The benefit and the critical time path increase due to each
plumbing are computed. If a plumbing has zero or negative benefit,
or causes a sharing to miss its SLA, it is discarded from P . The
algorithm terminates when no more plumbing operation can be applied to D. The resulting global plan forms the input to the sharing
executor. We will show later in Figure 13 that this strategy is quite
effective and results in large savings for the provider.

Plumbing Operations

A plumbing operation p is defined between a set of source vertices S RC(p) and another set of destination vertices D ST(p), such
that after the plumbing operation D ST(p) gets tuples via S RC(p).
Applying p on D results in a potential cost reduction is due to the
removal of vertices from D, although some expenditure in the way
of additional vertices and edges is made to facilitate the plumbing.
The benefit of p is defined as the dollar cost savings due to the removal of vertices and edges in the plan minus the cost of adding the
additional vertices and edges to implement p. Applying p can potentially increase the critical time path of all sharings in S HR(D ST(p)).
This is because they now obtain their tuples via S RC(p), which may
be a longer path in terms of time. We note that p is feasible only if
it has a positive benefit. Moreover, performing p should not cause
the critical time path of any of the sharings in S HR(D ST(p)) to exceed their SLA. We consider the following two kinds of plumbing
operations, which are shown in Figures 4a–b.

COPY
DELTA

ove

DST(pi)

pi

SRC(pi)

(b)

Rem

ove

Rem

(a)

DST(pi)

SRC

pi

(pi )

(c)

pi

8.

pj

SHARING EXECUTOR

The sharing executor takes a global sharing plan D and maintains
the individual sharings at the appropriate level of staleness. In its
very nature, the sharing executor must be robust to any deviations in
the update rate and the behavior of the machines in the infrastructure.
The sharing executor applies its own set of run time optimizations,
some of which are briefly described. We first describe how staleness
is computed and how the push operator reduces the staleness of a
sharing arrangement. Next, we design a model to determine the most
appropriate time to push the sharings.

JOIN

pk

Figure 4: Types of plumbings (a) CopyDelta and (b) Join and (c)
how plumbings can affect one another

8.1 Staleness and Push

1. Copy Plumbing: Takes two delta vertices on different machines and adds a CopyDelta edge between them.

Each machine runs an agent which is responsible for sending periodic timestamps updates to the sharing executor. We refer to these
messages as heartbeats. We implement the following three functions
– TS(.), M IN TS(.), M AX TS(.) – that obtain the current timestamp
of a vertex, and the minimum and maximum timestamps of a set of
vertices in the sharing plan. For instance, M IN TS(S RC(Si )) and

2. Join Plumbing: Takes two vertices – a delta and a relation
– and performs a join to obtain the destination delta vertex.
For example, ∆(A ./ ∆B) is plumbed using A and ∆B by a
Join edge and up to two CopyDelta edges.

694

M AX TS(S RC(Si )) are the minimum and maximum timestamps of
the sources of Si .
The current staleness of a sharing is defined as the difference between the maximum of the timestamps of the base relations of Si to
that of the MV of Si . Note that the staleness should always be maintained to be less than SLA(Si ). This is captured by the following
inequality.

to take into account recent system fluctuations. We record the actual
time to perform each of the operators, compare it against estimated
time and periodically recompute the time model. This feedback loop
allows our system to be reasonably robust to data and machine fluctuations.
An appropriate timestamp t to advance the MV of Si should be
greater than the current timestamp of MV but should be less than
or equal to the minimum of the timestamp of the sources of Si . In
particular,

M AX TS(S RC(Si )) − TS(MV(Si )) ≤ SLA(Si ).
To reduce the staleness of a sharing Si , the executor schedules a
sequence of P USH commands in a topological order starting with
S RC(Si ), until the timestamp of the MV has a more up-to-date
timestamp. A P USH command instructs the agent to advance the
timestamp of a vertex in the sharing plan by applying an operator
denoted by its incoming edge. The incoming edge belongs to one of
the four edge types we described in Section 5.
Suppose an agent receives a P USH command to advance a vertex
v to timestamp t. Suppose that e is an incoming edge of v. The
agent first obtains a write lock on v. It then compares the current
timestamp t0 of v with that of t. If t0 ≥ t then there is no need to
perform any work. If t0 < t, then the agent performs the operation
corresponding to e’s type so that the timestamp of v can be advanced
to t. Once the operation has been performed the agent responds
with a P USH D ONE command, and piggybacks useful statistics such
as time taken to perform the operation and current timestamps. The
executor up on receiving the P USH D ONE proceeds with the outgoing
edges of v.
When it comes to maintaining multiple sharings, the design of a
sharing executor is simplified due to the observation that any sharing in S can be pushed independently of the others in S even though
they may have common edges and vertices. Suppose a vertex is at
a timestamp t and there are two concurrent P USH commands to advance it to t1 and t2 , t ≤ t1 ≤ t2 , respectively. Regardless of the
order in which the two commands are executed, the amount of work
done by e is equal to the work done to advance the timestamp to
t2 . This is why the sharing executor does not have to coordinate
P USH commands between the various sharings that it maintains.
This makes for a simple design of the sharing executor, especially
since the sharings may have different staleness SLAs and may have
to be pushed at different rates.

8.2

TS(MV(Si )) < t ≤ M IN TS(S RC(Si )).
When the push finishes, the MV would be at the timestamp t, while
the timestamp of the sources may all be advanced by CP(Di , t −
TS(MV(Si ))). So, the staleness of the sharing at the time the push
finishes would be: M AX TS(S RC(Si )) + CP(Di , t−TS(MV(Si ))).
We stipulate that the staleness when the push finishes should be less
than the staleness SLA using the following inequality:
M AX TS(S RC(Si )) + CP(Di , t − TS(MV(Si ))) − t ≤ SLA(Si ).
On the other hand, the sharing executor does not want to push too
early as well. In other words, the sharing executor is early if the
push command could have waited a bit longer and still could have
completed before Si became stale. This can be stipulated by adding
the additional constraint that:
l ∗ SLA(Si ) ≤

M AX TS(S RC(Si ))+
CP(Di , t − TS(MV(Si ))) − t

≤ SLA(Si ),

where l > 0 (0.8 in our case) is chosen to account for run-time
deviations, such as a queuing delay if the P USH waits for the capacity on a machine to be available. An appropriate value of t is
obtained by performing a binary search between TS(MV(Si )) and
M IN TS(S RC(Si )) that satisfies the above constraint.

9.

EXPERIMENTS

In this section, we present an experimental study to validate the
SMILE sharing platform. We first describe the experimental setup
in Section 9.1. We then evaluate the performance of our system for
varying rate of updates on the base relation in Section 9.2 and varying SLA in Section 9.3. We examine the effect of varying the number of machines and the sharings in the infrastructure in Section 9.4.
Next, the efficacy of the hill-climbing algorithm applied to DPT and
DPD is shown in Section 9.5. Finally, we highlight the robustness
of the sharing executor in Section 9.6 by varying the update rates on
the base relations and varying read workload on the MV.

Design

Our sharing executor uses a simple model to determine two key
questions: a) Is it time to push Si ? b) By how much to advance
the timestamp of MV of Si ? To determine these two questions, we
develop a model to determine the most appropriate time to schedule
the push and the timestamp to push the MV to such that the sharing
will not miss its SLA.
A simpler design of a sharing executor pushes all the sharings
in S every second so that they do not violate their SLA. Given the
property that the critical time path of an admissible sharing is less
than its staleness SLA, the push will finish before the SLA is violated. However, our sharing executor does not push every second
but rather bunches up sufficient work so that the push is issued as
late as possible. Yet, it is scheduled such that the push would be
completed before Si becomes stale.
To develop the model, we modify the critical time path function
CP(Di , x) to take an additional parameter x, which corresponds to
the amount of timestamp to advance the MV of Si . In Section 5.1
when we described how we compute the critical time path of a sharing plan, x was defaulted to be one but now can take up any arbitrary
value greater than or equal to one. We also added a feedback loop
to the CP function so that it constantly recomputes the time model

9.1 Setup
Our experimental setup creates a mobile cloud ecosystem containing 25 apps where sharings are specified using user, social network, location, checkin, and user-content datasets; the datasets and
the sharings are representative of those one may find in a mobile
cloud. We collected Twitter tweets from a gardenhose stream, which
is a 10% sampling of all the tweets in Twitter, for a six month
period starting from September 2010. The tweets were unpacked
into nine base relations corresponding to the information about the
user (i.e., users relation), tweets (i.e., tweets relation), social
network (socnet relation), checkins (foursq), and user-content
(i.e., urls, hashtags, curloc, photos relations) associated
with the tweets and the location of the user (i.e., loc relation).
This creates our realistic datasets that capture rich information about
users, locations, social contacts, checkins and the various contents
the users are interest in.

695

Table 1: Twitter base relations (left) and the twenty-five sharings (right) used in the evaluations
Base Relations:

Sharings (Apps):
S1
S2
S3
S4
S5
S6
S7

users ./ socnet (twitaholic)
users ./ tweets ./ curloc (twellow)
users ./ tweets ./ urls (tweetmeme)
users ./ tweets ./ urls ./ curloc (twitdom)
users ./ tweets (tweetstats)
tweets ./ curloc (nearbytweets)
urls ./ curloc (nearbyurls)

S13
S14
S15
S16
S17
S18
S19

S8

tweets ./ photos (twitpic)

S20

S9

foursq ./ tweets (checkoutcheckins)

S21

S10
S11

hashtags ./ tweets (monitter)
foursq ./ users ./ tweets
./ curloc (arrivaltracker)
foursq ./ users ./ tweets (route)

S22
S23
S24
S25

S12

9.1.1

Total Push Time (seconds)

We specify 25 sharings by combining these 9 base relations in
different ways. A description of the base relations and the 25 sharings used in the evaluation are shown in Table 1. For each of the
25 sharings, we also mention an existing real app that may benefit from such a sharing. For instance, the application twitter-360,
which displays nearby photos may be interested in S18 which corresponds to users ./ tweets ./ photos ./ curloc. By building
an ecosystem around twitter data and choosing sharing that match
the functionalities of existing apps, we are ensuring that our evaluation is as realistic as possible. Our setup consisted of 6 machines,
such that the 25 sharings were arbitrarily assigned to the available
machines. All the machines in our setup ran identical versions of
Postgresql 9.1 database. Our system starts with 7 million tweets
prepopulated into our databases.
As we vary the rate of arrival of tweets into our system, the rate
of update on the base relations (other than tweets) depends on the
number of tweets seen so far by the system. For instance, at the
beginning any incoming tweet most likely will contain the identity
of a user not previously seen by the system, which would result in the
insertion of a tuple to the users relation. However, after the system
has ingested a sufficient number of tweets, the update rate on the
users relation will decrease as some of the users already would be
present in the users relation. The dependence between the number
of tweets ingested by the system and the chance that an incoming
tweet will result in an insertion to a base relation is expressed in
terms of an update ratio. After 7 million tweets have been ingested
by our system, the update ratio of encountering a previously unseen
user in the next tweet is around 0.3. The update ratio values for some
of the remaining relations were 0.25, 0.02, 0.1, 0.2, for socnet,
loc, curloc and urls, respectively. Using the update ratios, we
can estimate the rate of updates on all the base relations by varying
the rate of incoming tweets in the system.

foursq ./ users ./ tweets ./loc (locc.us)
tweets ./ loc (locafollow)
users ./ loc ./ tweets ./ curloc (twittervision)
foursq ./ users ./ tweets ./ socnet (yelp)
users ./ loc (twittermap)
users ./ tweets ./ photos ./ curloc (twitter-360)
users ./ tweets
./ hashtags ./ curloc (hashtags.org)
users ./ tweets ./ hashtags
./ photos ./ curloc (nearbytweets)
users ./ tweets ./ foursq
./ photos ./ curloc (nearbytweets)
foursq ./ curloc (nearbytweets)
photos ./ curloc (twitxr)
hashtags ./ curloc (nearbytweets)
hashtags ./ users ./ tweets (twistroi)

5

Total Push Time (seconds)

User info
Tweets info
Social network
User address
User current loc
Tweet links
Tweet entities
Photo links
Rest. checkins

4
3
2
1
0
0

3000

6000

0.25
0.2
0.15
0.1
0.05
0

9000

0

No. of Delta Tuples

4000

Total Push Time (seconds)

(b)

5
4
3
2
1
0
0

3000

8000

No. of Delta Tuples

(a)

Total Push Time (seconds)

users(uid, ...)
tweets(uid, uid, ...)
socnet(uid, uid2, ...)
loc(uid, place, ...)
curloc(tid, lat, lng, ...)
urls(tid, url, ...)
hashtags(tid, tags, ...)
photos(tid, urls, ...)
foursq(tid, rid, ...)

6000

9000

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

No. of Output Tuples
(c)

3000

6000

9000

No. of Output Tuples
(d)

Figure 5: Time cost model of the four edge types, namely a)
DeltaToRel, b) CopyDelta, c) Join, and d) Union

9.1.2

Dollar and Time Cost Models

The infrastructure costs for our cost model was obtained from
Amazon EC2 pricing available on the web1 . Our machines are
equivalent to large Linux instances, which cost $0.34/hour. For the
network cost, we assumed that the instances were in different availability zone but in the same region, which had a transfer cost of $0.01
per GB. For storage, we used EBS storage at $0.11 GB/month.
We developed a time model for each edge type to estimate the
time taken to process tuples, as function of the number of input tuples. Our setup to compute a time model consisted of two machines
with 15 base relations of varying sizes between 200k and 50 million tuples, number of attributes from 1 to 7 as well as different
attribute types forming tens of sharings between the base relations.
We pushed a varying number of tuples between 1 and 10k through
each edge in the setup and then measured the time taken to perform
each P USH operation, which is recorded in Figure 5. It can be seen
that the time taken to push tuples through different edge types is
linear in the number of tuples for all the edge types, although with
different slopes. These plots form the basis of our time cost model.

Snapshot Module

To determine the efficacy of our system, an independent auditing
module in our sharing executor, called snapshot, records the staleness of all the sharings once every 5 seconds. Suppose that a sharing
Sj was found to have a staleness less than the SLA staleness at snapshot i. If Sj satisfies the SLA staleness in snapshot i + 1, then the
system is assumed to have maintained Sj at the appropriate level of
staleness for all intermediate time periods between i and i + 1. The
converse is true if Sj is found to have violated the SLA at snapshot
i+1. The snapshot module also keeps track of the cost, and the number of tuples moved between snapshots. Additionally, we record the
staleness of all sharings before and after a P USH operation as well
as the cost incurred and time taken for each P USH operation.

9.2 Varying Rate
We used 6 machines and 25 sharings as shown in Table 1 with a
1

696

http://aws.amazon.com/ec2/pricing/

0

100

200

300

0

Snapshot

100 200 300
Snapshot

(S3)

0

100 200 300
Snapshot

0

100 200 300
Snapshot

(S4)

0

100 200 300
Snapshot

(S10)

0

100 200 300
Snapshot

(S5)

0

100 200 300
Snapshot

60
50
40
30
20
10
0

60
50
40
30
20
10
0

No. Tuples Moved

100000
0
0

400

300000

S16

100000
0
0

100 200 300
Snapshot

S1

200000
100000
0
0

200

400

Snapshot

300000

200
400
Snapshot

S3

200000
100000
0
0

200
400
Snapshot

1.2e+06

(S11)

0

200

200000

Snapshot

(S18)

0

S11

200000

No. Tuples Moved

100 200 300
Snapshot

60
50
40
30
20
10
0

100 200 300
Snapshot

300000

No. Tuples Moved

60
50
40
30
20
10
0

0

0

No. Tuples Moved

60
50
40
30
20
10
0

Staleness (seconds)

Staleness (seconds)

Staleness (seconds)

60
50
40
30
20
10
0

(S9)

(S17)

(S25)

ALL, SLA=45s

1.1e+06

No. Tuples Moved

100 200 300
Snapshot

100 200 300
Snapshot

60
50
40
30
20
10
0

100 200 300
Snapshot

Staleness (seconds)

0

60
50
40
30
20
10
0

0

60
50
40
30
20
10
0

0

0

60
50
40
30
20
10
0

Staleness (seconds)

10

(S2)

(S8)

(S16)

(S24)

Staleness (seconds)

20

100 200 300
Snapshot

100 200 300
Snapshot

60
50
40
30
20
10
0

100 200 300
Snapshot

Staleness (seconds)

60
50
40
30
20
10
0

0

30

60
50
40
30
20
10
0

0

0

60
50
40
30
20
10
0

Staleness (seconds)

40

(S7)

(S15)

(S23)

Staleness (seconds)

(S1), SLA=45s

50

60
50
40
30
20
10
0

100 200 300
Snapshot

60
50
40
30
20
10
0

100 200 300
Snapshot

Staleness (seconds)

Staleness (seconds)

60

0

Staleness (seconds)

Staleness (seconds)

100 200 300
Snapshot

(S14)

0

60
50
40
30
20
10
0

Staleness (seconds)

0

60
50
40
30
20
10
0

(S22)

Staleness (seconds)

100 200 300
Snapshot

(S13)

100 200 300
Snapshot

Staleness (seconds)

0

60
50
40
30
20
10
0

0

60
50
40
30
20
10
0

Staleness (seconds)

(S12)

100 200 300
Snapshot

(S21)

Staleness (seconds)

60
50
40
30
20
10
0

0

60
50
40
30
20
10
0

Staleness (seconds)

100 200 300
Snapshot

(S20)

Staleness (seconds)

0

60
50
40
30
20
10
0

Staleness (seconds)

Staleness (seconds)

(S19)

Staleness (seconds)

Staleness (seconds)
Staleness (seconds)

60
50
40
30
20
10
0

100 200 300
Snapshot

(S6)

1e+06
900000
800000
700000
600000
500000
400000
300000
200000
100000
0

0

100 200 300
Snapshot

0

100

200

300

400

Snapshot

Figure 6: Staleness of twenty-five sharings across snapshots for a rate of 6k tweets/second (left) and the number of tuples moved in
each snapshot (right). Due to space constraints, only two zoomed-in graphs are provided for readability, while the remaining figures
are rendered small to show trends.
Cost vs. Data Rate

15
10

30
20

0

S1

100

200
300
Push Iteration
(a)

400

1000

6
4
2

500
200
100

0

50 G.1k.5k1k F 2k3k5k6k

50 G.1k.5k1k F 2k3k5k6k

Tweets / second

Tweets / second

0

2000

4000

6000

Time Elapsed (seconds)

(b)

(c)

40
35

streams, and about 3 violations per sharing-hour (i.e., per hour per
sharings) for 6k tweets per second. Note that the zero violations for
both the gardenhose and firehose streams were in spite of their unpredictable arrival rate, which is shown in Figure 8c. At 6k tweets
per second, the cost was about $25 per sharing-hour, although note
that some of the sharings were much more expensive than the others.
In contrast, the average cost for the firehose (F) stream was about $6
per sharing-hour. Secondly, the number of tuples moved per snapshot (i.e., 5 seconds) across all the sharings was between 600k and
1.1 million tuples as shown in Figure 6 (right).

30

15
0

8

(a)

20

0

2000

Figure 8: (a) Cost and (b) violations per sharing-hour for gardenhose (G), and firehose (F) streams and rates from 50 to 6k.
Variations in Gardenhose (G) rate shown in (c)

25
10

5

Gardenhose Rate

10

Tweets / second

20

45

40

No.of violations per hour

25

50
Time Pushed

100

200

300

400

Push Iteration
(b)

Figure 7: a) Staleness before and after a P USH on S1 , while b)
shows how much is pushed
Figure 6 (left) shows the staleness of a sharing S1 across different
snapshots. It can be seen that the staleness of each of the sharings increases until it comes close to the SLA (i.e., 45 seconds), after which
the staleness sharply reduces to a low value due to a P USH from the
sharing executor. The staleness before and after a push operation are
shown in Figure 7a, where it can be seen that the P USH operation
reduces the staleness of S1 to less than 10 seconds, just before the
staleness of S1 was about to exceed the SLA. Figure 7b shows that
every push operation advanced the timestamp of S1 by 25 to 40 seconds, which shows the lazy behavior of the sharing executor. One
thing to note here is that there were only 31 violations for all the 25
sharings for the entire duration of the experiment lasting about 40
minutes. We summarize some of our observations below.
The number of violations with varying incoming rate of tweets as
well as the cost to maintain the sharings in sharing-hour are shown in
Figures 8a–b. The results did not show a well defined trend between
the number of violations and the rate of incoming tweets, except that
the violations were very low, even for 6k tweets/second. First of all,
there were zero violations for the firehose (F) and gardenhose (G)

% change in tuples moved

Stalenes (seconds)

55

BEFORE (S1)
AFTER (S1)

50

Violations vs. Data Rate

30

Dollars per hour

SLA of 45 seconds, while varying the rate of tweets from 50 to 6k
tweets/second. At 6k tweets/second (i.e., 3.6 billion tweets/week),
the update rate matches the current known volume of tweets in Twitter [12]. We also replayed gardenhose stream, which roughly corresponds to an average of 100 tweets/second. The rate of arrival for
tweets for a two hour window is shown in Figure 8c. As the gardenhose is a 1 out of 10 sampling of tweets from the firehose, which is
a stream containing all the tweets in Twitter, we recreated a stream
similar (although by no means equivalent) to firehose by replaying
gardenhose at 10X speed.

Tuples moved with/without sharing commonality
0
-500
-1000
-1500
-2000

Small Gap

Large Gap

-2500
-3000
S1

S3

S4 S20

S7

S8

S9 S10 S23

Sharing Arrangements

Figure 9: Number of tuples moved in the setup in Figure 6 expressed as percent reduction from the case when individual sharings are run in isolation
Next, notice in Figure 6 (left) that some sharings, such as S7, S8,
S9, S10, and S23 have a larger gap between the peak staleness value

697

9.3

7000
6000
5000
4000
3000
2000

35

8000

30

7000
Tweets/second

8000
Tweets/second

Rate vs. Sharings

Tuples/machine vs. Machines

Thousands of Tuples/second

Rate vs. Machines

and the SLA, whereas others such as S1, S3, S4, and S20 have relatively smaller gaps. The reason for this is that those sharings with
larger gaps benefit from the commonality with other sharings but not
so for those with smaller gaps. To test this hypothesis, we compared
the number of tuples moved for each sharing in the above experimental setup with the number of tuples moved when the sharings
are run in isolation. The number of tuples moved in the former case
is shown as a percentage reduction from the latter case in Figure 9.
It can be seen that sharings with small gaps only benefit modestly
from the presence of other sharings, whereas those with larger gaps
benefit immensely from the presence of other sharings.

25
20
15
10

4000

2000

0
2

No. of Machines

5000

3000

5

2 3 4 5

6000

3

4

5

20 25 30 40 50

No. of Sharings

No. of Machines

(a)

(b)

(c)

Figure 11: Maximum tweet rate for varying (a) machines, (b)
sharings on a setup with SLA = 45 seconds

Varying SLA

Table 2: Number of violations per sharing-hour (rounded-up)
for varying SLA between 10 and 60 secs
Staleness SLA
10 20
30
40
50
60
Mix
Violations
4
1
2
1
0
0
0

support without losing the stability of the system. We have built an
appropriate mechanism to monitor the stability of our system, which
is not discussed here due to lack of space. It can be seen from Figure 11a that increasing the number of machines increases the maximum rate that can be handled by our system. Moreover, adding an
extra machine increases the processing capacity of our system by at
least 25–30k tuples/sec as can be seen from Figure 11b. Next, we
varied the number of sharings from 20 to 50 keeping the number
of machines fixed at 6 and SLA of 45 seconds, as shown in Figure 11c. We increased the number of sharings beyond 25 by placing
the same sharing on more than one machine. With increasing number of sharings, the maximum rate decreases as database and other
system bottlenecks start manifesting due to the increased number of
vertices and edges that the system has to manage.

Our setup consisted of 6 machines, 25 sharings and an incoming
rate of 1000 tweets/second. We varied the SLA between 10 and 60
seconds. Table 2 shows the effect of varying the SLA in terms of
the number of violations. The number of violations is maximum
for SLA = 10 seconds at 4 violations per sharing-hour. In general,
the number of violations for staleness SLA values greater than 10
seconds is extremely low at either 1 or 2. The higher number of
violations for 30 seconds (at 2 per sharing-hour) compared to those
for 20 and 40 seconds (at 1 per sharing-hour) was due to temporary
fluctuations in system resources.

9.5 Algorithm Comparisons

Cost Change of Mix vs. Uniform SLA

-100
-200
-300
-400

SLA=40
S9 S11 S13 S15

100

S17 S19 S21 S23 S25

Sharing Arrangements

Figure 10: Cost change for mix SLA compared to uniform SLA

0.008
0.007
0.006
0.005
0.004
0.003
0.002
0.001
0

200

50

100
Snapshots

100

150

200

Snapshots

0.0025

DPT+HC

50

In the above experimental setup, we also examined a case where
we assigned non-uniform SLA (see mix in Table 2) to the 25 sharings. In particular, S1–S7 were assigned a SLA of 10 seconds, S8–
S15 a SLA of 40 seconds, and S16–S25 a SLA of 60 seconds. From
Table 2, we can see that the mix case resulted in zero violations, although having comparable dollar costs to the uniform SLA cases.
Then in Figure 10 we expressed the cost of an individual sharing in
the mix case as a percentage change to the corresponding cost from
the uniform case (i.e., compare costs of S1 from mix with uniform
when SLA was 10 seconds). It is interesting to note that although
the costs of S1–S7 have become marginally more expensive, the cost
of the other sharings (i.e., S8–S15, S16–S25) is now significantly
cheaper. Hence, we can conclude that few sharings with small SLAs
subsidize the operating cost of other (related) sharings.

9.4

150

0.0033

DPD

Snapshots

SLA=60

-500
S1 S3 S5 S7

0.008
0.007
0.006
0.005
0.004
0.003
0.002
0.001
0

0.0042

DPT

50

SLA=10

Cost (Dollars)

0.008
0.007
0.006
0.005
0.004
0.003
0.002
0.001
0

0

Cost (Dollars)

Cost (Dollars)

0.008
0.007
0.006
0.005
0.004
0.003
0.002
0.001
0

Cost (Dollars)

% change in cost

100

150

200

0.0023

DPD+HC

50

100

150

200

Snapshots

Figure 12: Cost of DPT and DPD reduced by applying hillclimbing algorithm to produce DPT+HC and DPD+HC
We examined the efficacy of the hill-climbing algorithm that we
apply to the DPD and DPT algorithms to reduce the cost for the
provider. For this experimental setup, we used 6 machines, 25 sharings and a rate of 1000 tweets/second. The cost model we considered
was same as before, except that we changed the networking pricing to be within the same availability region in EC2 (i.e., no cost).
We generated DPD and DPT sharing plans for this setup, and then
applied the hill-climbing algorithm to both these sharing plans to
produce DPD+HC, and DPT+HC, respectively. The average cost in
dollars per sharing-second for the four sharing plans in sharing-hour
were as follows — DPT 0.0042, DPD 0.0033, DPT+HC 0.0025,
and DPD+HC 0.0023 as shown in Figure 12. It can be noticed
that DPD+HC has the cheapest cost but is comparable to DPT+HC.
When we compared DPD with DPD+HC, and DPT with DPT+HC,
the difference is quite significant representing a 35% reduction in
cost, thus making a case for our hill-climbing approach.

Varying Machines and Sharings

In this experimental setup, we varied the machines from 2 to 5,
while keeping the number of sharings fixed at 25 and a SLA of 45
seconds. For every setup, the capacity of the machine was determined to be the highest rate of tweets that the set of machines can

698

Edge
Vertex

DPD

10.

DPT

240
220
200
180
160
0

2

4

6

8

10

12

14

0

2

4

6

8

10

12

14

16

Iterations

Figure 13: Reduction in vertices and edges as plumbing operations are sequentially applied to DPD and DPT

11.

Figure 13 shows the number of vertices and edges as the hillclimbing algorithm takes DPD or DPT sharing plan as input and
performs plumbing operations in a sequential fashion. As can be
seen from the figure, the sharing plan is reduced by more than 80
vertices and edges for both DPD and DPT, which represents significant savings in terms of cost.

9.6

Staleness (seconds)

8Users,
75 tweets/s

60

16Users,
75 tweets/s

32Users,
100 tweets/s

50Users,
150 tweets/s

50
40
30
20
10
0
0

50

100

150
Snapshot

200

250

REFERENCES

[1] Infochimps. http://www.infochimps.com/, 2012.
[2] Microsoft azure market. http://datamarket.azure.com/, 2012.
[3] Postgres streaming replication.
http://wiki.postgresql.org/wiki/Streaming_Replication,
2012.
[4] Xignite. http://www.xignite.com/, 2012.
[5] D. Agrawal, A. E. Abbadi, A. Singh, and T. Yurek. Efficient view maintenance at
data warehouses. In SIGMOD, 1997.
[6] S. Agrawal, S. Chaudhuri, and V. R. Narasayya. Automated selection of
materialized views and indexes in SQL databases. In VLDB, 2000.
[7] S. Al-Kiswany, H. Hacigümüs, Z. Liu, and J. Sankaranarayanan. Cost
exploration of data sharings in the cloud. In EDBT, 2013.
[8] S. Aulbach, M. Seibold, D. Jacobs, and A. Kemper. Extensibility and data
sharing in evolving multi-tenant databases. In ICDE, 2011.
[9] M. Balazinska, B. Howe, and D. Suciu. Data markets in the cloud: An
opportunity for the database community. PVLDB, 4(12):1482–1485, 2011.
[10] J. A. Blakeley, P.-A. Larson, and F. W. Tompa. Efficiently updating materialized
views. In SIGMOD, 1986.
[11] S. Chaudhuri. An overview of query optimization in relational systems. In
PODS, 1998.
[12] D. Costolo. The power of Twitter as a communication tool. http://www.
fordschool.umich.edu/video/newest/1975704207001/, 2012.
[13] V. Kantere, D. Dash, G. Gratsias, and A. Ailamaki. Predicting cost amortization
for query services. In SIGMOD, 2011.
[14] D. Kossmann, M. J. Franklin, and G. Drasch. Cache investment: integrating
query optimization and distributed data placement. TODS, 25:517–558, 2000.
[15] S. Krishnamurthy, M. J. Franklin, J. M. Hellerstein, and G. Jacobson. The case
for precision sharing. In VLDB, 2004.
[16] A. Labrinidis and N. Roussopoulos. Reduction of materialized view staleness
using online updates. CS-TR-3878, UMD CS, 1998.
[17] L. Lamport. Time, clocks, and the ordering of events in a distributed system.
CACM, 21(7):558–565, 1978.
[18] G. M. Lohman, C. Mohan, L. M. Haas, D. Daniels, B. G. Lindsay, P. G. Selinger,
and P. F. Wilms. Query processing in R∗ . In Query Processing in Database
Systems, pages 31–47. Springer, 1985.
[19] C. Mascolo, L. Capra, S. Zachariadis, and W. Emmerich. XMIDDLE: A
data-sharing middleware for mobile computing. Wireless Personal
Communications, 21(1):77–103, 2002.
[20] H. Mistry, P. Roy, S. Sudarshan, and K. Ramamritham. Materialized view
selection and maintenance using multi-query optimization. In SIGMOD, 2001.
[21] T.-V.-A. Nguyen, S. Bimonte, L. d’Orazio, and J. Darmont. Cost models for view
materialization in the cloud. In EDBT, 2012.
[22] T. Plagemann, J. Andersson, O. Drugan, V. Goebel, C. Griwodz, P. Halvorsen,
E. Munthe-Kaas, M. Puzar, N. Sanderson, and K. Skjelsvik. Middleware services
for information sharing in mobile ad-hoc networks. Broadband Sat. Comm. Sys.
and Challenges of Mob., 169:225–236, 2005.
[23] K. A. Ross, D. Srivastava, and S. Sudarshan. Materialized view maintenance and
integrity constraint checking: Trading space for time. In SIGMOD, 1996.
[24] K. Salem, K. Beyer, B. Lindsay, and R. Cochrane. How to roll a join:
Asynchronous incremental view maintenance. In SIGMOD, 2000.
[25] J. Sankaranarayanan, H. Hacigumus, and J. Tatemura. COSMOS: A platform for
seamless mobile services in the cloud. In MDM, 2011.
[26] T. Sellis. Multiple-query optimization. TODS, 13(1):23–52, 1988.
[27] M. M. Wang, J. N. Cao, J. Li, and S. K. Dasi. Middleware for wireless sensor
networks: A survey. JCST, 23(3):305–326, 2008.
[28] Y. Zhuge, H. García-Molina, J. Hammer, and J. Widom. View maintenance in a
warehousing environment. In SIGMOD, 1995.
[29] Y. Zhuge, H. García-Molina, and J. Wiener. The strobe algorithms for
multi-source warehouse consistency. In PDIS, 1997.

Robustness of Sharing Executor
70

CONCLUDING REMARKS

In this paper we presented a platform that can maintain sharings at
the appropriate level of staleness. Experimental results showed the
effectiveness of our platform in maintaining several sharings with
low violations even under a high update rate. We will examine the
following possible extensions in a future work. The platform can
be extended to support aggregate operators by developing additional
operators. Next, easy addition or removal of sharings on the fly as
the system is running can be provided. Finally, before markets for
sharing data be envisioned, issues related to the pricing of data [9]
have to be addressed.

DPT+HC

260

DPD+HC

Number of Vertices/Edges

280

300

Figure 14: Staleness of S4 across snapshots as the rate and the
workload on the MV abruptly change
This experiment shows the robustness of the sharing executor as
the machine capacities change during run-time. The setup consists
of 4 machines hosting the sharings S1 ...S4 . Figure 14 shows the
staleness of S4 with an SLA of 50 seconds (marked) as recorded
by the snapshot module. The SLA of the remaining 3 sharings was
between 20 and 70 seconds. We applied a read workload on each
of the four MVs corresponding to the four sharings by the way of a
simulated user generating a single query template in a closed loop.
Initially, the incoming rate is set at 50 tweets/second and each of the
MVs is subjected to two users.
As the system is running we abruptly vary the number of users on
each MV as well as the rate of incoming tweets. The number of users
per MV is varied in four phases from 8 to 50, while simultaneously
changing the rate of incoming tweets from 50 to 150 tweets/second.
After the first phase when the number of users was increased from 8
to 16 users, all the machines are heavily loaded. The average staleness value for each phase is also marked in Figure 14.
As the number of users and the rate of incoming tweets increase,
the machines get progressively more loaded. However, it can be seen
from the boundaries of the phase changes in the figure, the sharing
executor quickly adapts to the changing data rate and the increased
workload on the databases. The model in spite of the infrastructure
being loaded never allows the staleness of the sharing to exceed beyond 40 seconds. The sharing executor is able to do this by taking
advantage of the slack between the critical time path of the sharing
plan, which is a few seconds and that of the staleness SLA, which
is 50 seconds. Even if the time taken to push the sharing becomes
progressively slower due to the system load, the executor is able to
schedule the updates in a way that the SLAs are not violated.

699

SMILE: A Data Sharing Platform
for Mobile Apps in the Cloud
Jagan Sankaranarayanan
Hakan Hacıgümüş
NEC Labs America, Cupertino, CA

∗

Haopeng Zhang

∗

Mohamed Sarwat

University of Massachusetts Amherst

University of Minnesota

haopeng@cs.umass.edu

sarwat@cs.umn.edu

{jagan,hakan}@nec-labs.com
ABSTRACT

get a seamless experience as the three apps now behave as a single
entity.

We identify an opportunity to share data among mobile apps hosted
in the cloud, thus helping users improve their mobile experience,
while resulting in cost savings for the cloud provider. In this work,
we propose a platform for sharing data among mobile apps hosted
in the cloud. A “sharing” is specified by a triple consisting of: (a)
a set of data sources to be shared, (b) a set of specified transformations on the shared data, and (c) a staleness (freshness) requirement
on the shared data. The platform addresses the following two main
challenges: What sharings to admit into the system under a set of
specified constraints, how to implement a sharing at a low cost while
maintaining the desired level of staleness. We show that reductions
in costs are achievable by exploiting the commonalities between the
different sharings in the platform. Experimental evaluation is performed with a cloud platform containing 25 sharings among mobile apps with realistic datasets containing user, social, location and
checkin data. Our platform is able to maintain the sharings with very
few violations, even under a very high update rate. Our results show
that our method results in a cost savings of over 35% for the cloud
provider, while enabling an improved mobile experience for users.

Interestingly, with the increasing use of cloud-based resources,
many of these apps may be hosted in the same cloud infrastructure
(e.g., Amazon EC2). To enable such rich interactions, mobile apps
should make their datasets available for sharing, as a way of encouraging other apps to build complementary features. At the same time,
apps can consume several datasets from other apps in the cloud infrastructure. We identify two key considerations in sharing data that
is important to mobile app developers.
• App developers want reliable access to datasets and do not
want to deal with the complexity of creating and maintaining
mechanisms (e.g., APIs [1, 4] or web services [2]) for sharing
data. Furthermore, they desire a service that is flexible enough
to meet their needs while providing guarantees on the quality
of the service.
• App developers want timely access to datasets. Mobility of
users imposes limits on how much staleness app developers
can tolerate on the datasets. This is because many types of
user-related data get progressively less valuable with time. For
instance, the location of a mobile user that is 50 seconds stale
may be of limited use to a navigation app; however, 10 seconds stale data may be suitable.

1. INTRODUCTION
Mobile applications (apps) compete in an increasingly crowded
marketplace, with possibly thousand of apps performing similar or
identical functions. In this crowded marketplace, developers can differentiate their apps by offering features that make the user’s mobile
experience more personalized. For instance, apps like SpotiSquare
connect with Foursquare venues to determine the current location
(venue) of the user, and then choose a music playlist depending on
users’ current context (e.g., eating dinner, exercising, driving etc.).
To create such an experience requires that the app has access to additional information (i.e., datasets) about its user. The following
example shows possible interactions among three apps, showcasing
the benefits of sharing user information.

In this paper, we propose a data sharing service in the cloud called
SMILE (Sharing MIddLEware). The service provider, who manages
the cloud infrastructure, offers data sharing as service and like any
commercial business makes money by delivering services according
to agreed upon quality of service levels. Data sharing is achieved
by a mobile app developer (henceforth referred to as a consumer)
specifying a sharing. A sharing must identify datasets of interest,
the desired transformations on the data, and a staleness requirement.
The consumer and the provider enter into a Service Level Agreement
(SLA). The SLA is a contract specifying that the provider will ensure reliable access to the consumer on the shared data at the risk of
paying a penalty if it is not maintained at the agreed upon staleness.
To successfully achieve data sharing on a large scale, two practical
problems need to be solved by the provider. First, it is important to
determine if a new sharing can be admitted (i.e., accepted) into the
system and maintained at the appropriate staleness. This may not
always be possible, especially if the datasets are updated at a high
rate and the sharing needs to be maintained at a low staleness. If a
sharing is incorrectly admitted, it will result in significant losses for
the provider since the SLA may specify penalties for the provider in
case the sharing misses the staleness requirement. Second, implementing the sharings is not free in the sense that the provider has to

E XAMPLE 1. Consider the three apps — Opentable (restaurant
reservation), Plango (calendar) and Sonar (friends location monitoring). Appointments of users requiring dinner reservations are shared
by Plango with Opentable, which can then suggest restaurant options
to users. Sonar can suggest a nearby restaurant as a meeting place
by sharing their location information with Opentable. Mobile users
∗Work done while at NEC Labs
(c) 2014, Copyright is with the authors. Published in Proc. of EDBT on
OpenProceedings.org. Distribution of this paper is permitted under the
terms of the Creative Commons license CC-by-nc-nd 4.0

688

10.5441/002/edbt.2014.75

2.

pay for the resources (i.e., CPU, Disk, Network) consumed in the
cloud. Reducing provider cost is another important consideration.
The two practical problems we outlined above pose significant
technical challenges that we address in this paper.

RELATED WORK

While there has been some work on sharing in a mobile environment, they consider sharing either in an adhoc setting, such as between two mobile users [19], or among a group of mobile users [22].
A middleware for connecting mobile users and devices has been proposed [27] for providing various mobile services, such as management, security, and context awareness, but not for sharing.
Sharing using MVs adds interesting dimensions to a well studied problem domain. An MV maintenance process traditionally is
broken into a propagation step, where updates to the MV are computed and an apply step, where updates are applied to the MV. First
of all, the autonomy of the tenants means that synchronous propagation algorithms [10], where all sources are always at a consistent snapshot, are unsuitable for our purposes. Furthermore, to deal
with the autonomy of the tenants, one has to resort to a compensation algorithm [28], where the propagation is computed on asynchronous source relations [5, 24, 29]. In particular, MVs over distributed asynchronous sources have been studied in the context of
a single data warehouse [5, 29] to which all updates are sent. The
key optimization studied in [5, 29] is in terms of reducing the number of queries needed to bring the MVs to a consistent state in the
face of continuous updates on the source relations. [24] shows how
n-way asynchronous propagation queries can be computed in small
asynchronous steps, which are rolled together to bring the MVs to
any consistent state between last refresh and present. Reducing the
cost of maintenance plans of a set of materialized view S is explored
in [20], where common subexpressions [23] are created that are most
beneficial to S. Their optimization is to decide what set of common
subexpressions to create and whether to maintain views in an incremental or recomputation fashion. Staleness of MVs in a data warehouse setup is discussed in [16], where a coarse model to determine
periodicity of refresh operation is developed.
As we will see later in the paper, our setup is different from [5,29]
in the sense that multiple MVs are maintained on multiple machines
in our multitenant cloud database. Moreover, different update mechanisms with different costs and staleness properties can be generated
based on where the updates are shipped as well as where the intermediate relations are placed, making the problem harder than [5,29].
Next, [24] assumes that all the source relations are locally available on the same machine, which makes the application of their approach to our problem infeasible without an expensive distributed
query. We combine propagation queries from [24] with join ordering [11, 18], such that propagation queries involving n source relations are computed in a sequence involving two relations at a time,
requiring no distributed queries. In particular, we first ensure that
the update mechanisms can provide SLA guarantees, after which
common expressions among the various sharing arrangements are
merged to reduce cost for the provider, which is similar in spirit
to [20, 23]. Our work adds several additional dimensions to [20, 23]
in terms of placement of relations, capacity of machines, SLA guarantee, and cost.
In contrast to [16], which determines the periodicity of the refresh operation of MVs maintained in a warehouse, our work is distinguished in the following way. Our work develops refresh cycles
for multiple MVs from distributed sources with different staleness
requirements while simultaneously reducing the total maintenance
cost. This is significantly more complicated than the simple setup
in [16] where they develop a simple model for determining a single
refresh periodicity between a RDBMS and data warehouse without
considering cost.
Our work is related to traditional view selection problems [6] in
the sense that the set of sharing arrangements could have been obtained via the application of a view selection algorithm taking the

1. Maintaining the shared datasets at required staleness: The
shared datasets are maintained as materialized views (MVs)
and are always kept under the staleness specified in the SLAs.
This is challenging because the sharings involve multiple
datasets with varying staleness requirements. Note that there
are many other ways (e.g., APIs, web services) of enabling
sharing in the cloud; a discussion on the various methods and
the pros-and-cons of each is given in [25].
2. Testing for admissibility: As noted above, there is a need
for an effective method to decide whether to accept or decline a sharing agreement under multiple constraints, such as
the given staleness, SLA penalty and platform cost considerations.
3. Cost reduction for the provider: To reduce cost, the platform provider needs to identify commonalities across multiple sharings with different staleness requirements, in order to
save computational effort.
These three problems may not be specific to data sharing for mobile
apps only, but also applicable to other areas. However, we observe
that mobility makes these problems challenging and the solutions
much more relevant in real world settings.
Some of these problems have been previously considered in the
context of MV maintainance [5, 16, 23, 24] and multi-query optimization [11, 26]. However, prior work either considers the mechanics of MV maintainance [24] and refresh rates [16], or cost savings by removing commonalities [23, 26], but not staleness and cost
requirements simultaneously. While important, the feasibility and
economic value of sharing as well as infrastructure and privacy considerations are considered by [9, 13, 25], thus it is not the focus in
this work.
In this work, we focus on the practical and technical challenges
of enabling data sharing for mobile apps in the cloud. Mobility provides a perfect use-case scenario as it aligns with the three elements
of our problem setup: it is cloud-based, requires reliable access to
rich information, and has strict staleness requirements on datasets.
To our knowledge, this work represents the first systematic, cloudbased platform for enabling data sharing with staleness guarantees.
We make the following contributions in this paper.
1. A declarative sharing platform, which is fully implemented as
a part of industrial system, with staleness guarantees on the
sharings (Section 3).
2. A method of determining the admissibility of sharings ensures
that the system only admits those sharings that it can maintain
(Section 6).
3. A method for reducing the cost of maintaining the sharings by
amortizing work across multiple sharings, where each sharing
has its own constraints (Section 7).
4. Experimental evaluation is performed on a cloud platform
with 25 sharings posed on realistic user, location, social, and
checkin datasets. Our results show that the SMILE platform
can maintain a large number of sharings with very low SLA
violations, even under a high rate of updates. By amortizing
work across multiple sharings, SMILE is able to achieve a cost
savings of over 35% for the provider (Section 9).

689

consumer workload as input. Our problem shares common aspects
with the cache investment problem [14] in terms of placement (what
and where to be cached) of intermediate results and the goodness
(another notion of staleness) of cache. Cache refresh in [14] piggy
banks on queries, whereas we establish a dedicated mechanism to
keep the MVs at the desired staleness. Our work shares common
elements with [15] in the sense that merging data flow paths with
common tuple lineage is similar to the way we perform plumbing
operations on a sharing plan.
A related data sharing effort in the cloud is the F LEX S CHEME [8],
where multiple versions of shared schema are maintained in the
cloud, with the focus on enabling evolution of shared schema used
by multiple tenants. Data markets [9] for pricing data in the cloud
looks at the problem from tenant and consumer perspectives, but we
look at the problem from the provider’s perspective. A similar but
not identical problem is reducing the cost for the consumers (i.e., fair
pricing of common queries) [9] and sharing work across concurrent
running queries [26]. Although we only concern ourselves with the
staleness of the data as the only quality measure of the data being
shared, other considerations such as data completeness, accuracy are
also applicable here [7]. In our problem, the challenge is to maintain
the sharing arrangements always maintained on agreed upon terms
(i.e., SLAs), while keeping down the infrastructure costs. Satisfying
these dual goals makes the sharing problem challenging from the
provider’s perspective.

E XAMPLE 2. Plango (i.e., Calendar app) makes the base relation of User_Events of events extracted from users’ calendar
available for sharing. Opentable has its own relation User_Accts
of users that use the app. It specifies a sharing, “I want to know
about dinner events for the users who use my app within 10 seconds
of a new event being recorded so that I could offer recommendations to them.” The base relations are User_Events and User_Accts, and the transformations are specified as the following SPJ
query: EventType=“dinner" from a join of User_Events
and User_Accts. The staleness t is specified as 10 seconds and
the penalty pens is $.001 per late delivery of a tuple.
After the sharing Si is defined as per the above example, it is
given to the provider for deciding admissibility and implementation
in the platform, which is described in Section 6.

4.

SMILE ARCHITECTURE

Figure 1 shows the architecture of the system. There is a set of
machines available to implement the sharings. Each machine runs a
single database instance (Postgresql in our case). The SMILE platform consists of three main components — (a) delta capture, (b)
sharing optimizer, and (c) sharing executor — that perform the following functions, respectively: (a) capture changes (i.e., delta) on
the base relations as updates are applied on them; (b) generates plan
for moving these updates from the base relations to the MVs; and
(c) schedules the movement of these updates by taking system fluctuations into account. We briefly describe the three main system
components below.

Sharing Plan
Machine

Agent

Agent

Machine

¢R
Postgresql

Database

Gateway

R

Machine

DELTA CAPTURE

MV

Push

Workload

Machine

Heartbeat

The provider has to consider a set S of sharings {S1 , S2 · · · Sm },
for inclusion in the sharing platform. Here, we consider the case
where there are no existing sharings in the system, yet the solution
we develop is equally applicable to the case when the platform already has several prior sharings. Our solution that we later develop
will identify which of the sharings in S should be admitted into
the sharing framework, while at the same time minimizing provider
cost and meeting SLA requirements. Each Si specifies the applicable datasets, transformations, staleness requirements and penalties
as described next.
To specify a sharing Si , a consumer starts by identifying datasets
(i.e., base relations) of interest, or subsets of datasets. Next, the consumer must determine a way to combine these datasets by specifying
transformations on the data. In this work, we restrict the transformations on the base relations to include the following three operators.

Agent

Postgresql

3. PRELIMINARIES

Agent

Infrastructure

Pub/Sub
Input
Sharing
Sharing
Sharings S
Executor
Optimizer
Data Sharing Framework

1. Choose a subset of tuples using a selection predicate

Figure 1: Architecture of the sharing platform

2. Choose a subset of the attributes

4.0.1 Delta Capture and Timestamps

3. Combine base relations using a common key

As the base relations are updated, a delta capturing mechanism
(i.e., tuple creation, deletion or updates) records the modified tuples. Our mechanism uses the Streaming Replication facility [3] in
Postgresql to capture the deltas. This module in Postgres allows
the Write Ahead Log (WAL) to be streamed on to another Postgresql instance in recovery mode so that a nearly identical replica
of a database can be maintained. Our module fakes itself as a Postgresql instance and obtains a WAL stream. The modified tuples are
extracted from the stream, unpacked and written to the disk.
Every base relation R is associated with a delta relation, denoted
by ∆R that records the modified tuples as update queries are applied
on R. The tuples in ∆R are populated by the delta capturing module. The MVs in the system also contain corresponding delta tables.
If R is a MV then ∆R contains both prior updates as well as those
that have not yet been applied to R. The tuples in ∆R of a MV is
populated, moved and applied by the sharing executor.

In other words, the transformation can be specified using a SelectProject-Join (SPJ) query that is applied on the base relations. The
consumer next specifies a staleness requirement t expressed in time
units (e.g., 20 seconds) on the shared data as well as any applicable penalty pens . A sharing in SMILE is enabled by the creation
of a materialized view (MV), which describes the transformations
over the base relations. For each sharing Si , the system creates a
MV which is always maintained within a staleness of t time units as
specified by Si . This means even though the base relations are independently updated, the state of the MV is always consistent with the
state of the base relations within t seconds.
As an illustration of defining a sharing, we revisit Example 1 and
provide a more concrete example of a sharing that the Opentable
(i.e., restaurant) app may define using the SMILE platform.

690

Every relation, delta of a relation or MV in our platform records
its last modification timestamp. The timestamps are generated using
a distributed clock [17] that is periodically synchronized. Each tuple
in the delta also records an associated timestamp. Maintaining the
sharings at their appropriate level of staleness is achieved by keeping track of the last modification timestamps of the base relations
and comparing them to the timestamp of the MV. The SMILE system maintains an up-to-date timestamp information on each sharing,
hence is aware of the current staleness of all the sharings. Updates
are moved from the base relations to the MV in a way that ensures
that the sharings do not miss their SLAs.

4.0.2

that the vertices are relations or deltas of relations tied to a particular machine, and the edges apply transformational operators. The
plan is expressed using the following four edge operators, that 1)
apply updates (DeltaToRel) , 2) copy updates between machines
(CopyDelta) , 3) join updates (Join), and 4) union (i.e., merge) updates (Union).
As the plan operates on base relations that are asynchronously
updated, the input vertices to an operator may have different timestamps. An operator takes any mismatch in the timestamps into
account by rolling back all the input vertices to the minimum of
the timestamps among its inputs. This is referred to as compensations [28]. Rolling back the timestamp of a relation or a MV is
possible due to the delta relations associated it. The operators for
applying, copying and merging updates are based on their standard
interpretations, except that they additionally apply compensations to
the inputs as the first step. Our join operator performs a compensation which is an implementation of the algorithm from [28].
We will not provide the implementational details of the operators
but instead show an example of a plan that performs a relational join
on two asynchronous base relations A and B on different machines.
The plan is referred to as “in-place” as it does not involve making
the copies of the base relations. The vertices and the edge operators
in the plan periodically move the updates from the base relations to
the MV to keep it maintained incrementally.

Sharing Optimizer

Given a set S of new sharings, the sharing optimizer generates an
update mechanism for each sharing in S using a three step procedure
described below.
a. A sharing Si in S can be admitted if the system can maintain
Si at the desired level of staleness. This determination is necessary to prevent the system from entering into SLAs that it
cannot satisfy (Section 6).
b. If Si is admissible, we generate its sharing plan such that it
can move updates from the base relations to the MV within
the time specified in the staleness SLA. Moreover, the sharing
plan is also cost effective in terms of its infrastructure resource
consumption (Section 6).

A

ΔA

B

DELTA CAPTURE

ΔB

c. Once the individual sharing plans of all the sharings in S are
determined, commonalities across sharings are identified and
removed to produce a single global sharing plan D that implements all the sharings (Section 7).

4.0.3

DELTA CAPTURE

ΔA

COPY
UPDATES

JOIN

JOIN

Δ(A⋈ΔB)

Δ(ΔA⋈B) Machine m

Machine m1

Sharing Executor

2

COPY
UPDATES

COPY
UPDATES

Δ(A⋈ΔB) UNION

The sharing executor is the execution engine of the system which
maintains the sharings at or below the required staleness level. The
sharing executor is an implementation of an asynchronous view
maintenance algorithm [24].
The sharing executor computes the current staleness of a sharing
by taking the difference between the maximum of the timestamps of
all the base relations to that of the MV. The executor keeps track of
which of the sharings will soon miss their staleness SLA. It schedules the updates to be applied on the MV so that its staleness is
reduced. Each machine in the infrastructure runs an agent that communicates with the sharing executor via a pub/sub system (e.g., ActiveMQ). The agents send periodic messages to the sharing executor
with the last modification timestamps of the base relations and the
MVs.
Our implementation of the executor is lazy by design in the sense
that it does not refresh unless it is absolutely necessary or the sharing
will miss its SLA. This way, the executor bunches as much work as
possible thereby reducing redundant effort. The refresh is neither too
early nor too late, but finishes just before a sharing is about to miss
its staleness SLA. We provide more details on the sharing executor
in Section 8.

ΔB

UPDATES

Δ(A⋈B)
Machine m3

Δ(ΔA⋈B)
A⋈B

APPLY UPDATES

Figure 2: One possible plan involving an in-place join of a base
relation A on machine m1 and B on machine m2 such that the
resulting MV A 1 B is placed on machine m3

E XAMPLE 3. Figure 2 shows the plan of a sharing Si that performs a transformation A 1 B on two base relations, A and B.
The plan is a DAG consisting of 12 vertices and 10 edges. The vertices are either base relations (e.g., A, B or its copies), MVs (e.g.,
A 1 B) or delta relations (e.g., ∆(∆A 1 B)). The edges corresponds to operators that either apply, copy, merge, join updates,
to complete the transformation path from the base relations to the
MV. Note that select and project predicates can be specified in Si ’s
transformation. All the four edge types can apply select and project
predicates to their inputs if one is specified in addition to their usual
functionalities. We handle these predicates by using the pushdown
heuristic [11].

5. SHARING PLAN

Given a sharing that specifies a set of transformations on the base
relations, the plan generation algorithm enumerates all the plans that
implement the sharing. However, not all of the plans satisfy the
constraints we develop in the reminder of this section. In particular,
we concern ourselves with two key properties of a plan, namely its
critical time path and dollar costs, which are described below.

The update mechanism of a sharing is implemented as a sharing
plan, which is analogous to a query execution plan in databases. We
will henceforth refer to it simply as a plan in the rest of the paper. The plan is expressed in terms of four operators that form the
transformational path for the updates from the base relations to the
MV. This is represented using a Directed Acyclic Graph (DAG) such

691

5.1

Critical Time Path

The critical time path of the plan is the longest path in terms of
seconds that represents the most time consuming data transformation path in the plan. Note that the plan is admissible only if the
length of its critical time path is less than the required staleness of
the sharing, or else the system cannot maintain it.
The sharing optimizer estimates the critical time path of a plan
using a time cost model for each operator. The model estimates the
time taken for each operator given the size of the updates. Note that
finding the longest path between two vertices on a general graph
is an NP-hard problem, but the plans are DAGs, on which longest
path calculation is tractable. The system implements the procedure
CP(p) that takes a sharing plan p and outputs its critical time path
in seconds. For example, in the plan p shown in Figure 2, CP(p)
computes the time taken along the longest transformation path from
A or B to the MV A 1 B. Section 9 provides additional details on
how we developed the time cost model for the four operators.

5.2

C OST(p) = resCost(p) · (1 +

CP (p)
) + e(λ−µ)·s · pens
s

(1)

resCost(p) is the cost of resource usage. As discussed before, to avoid SLA violation due to multiple sharings competing for resource, we over-provision the resource by a factor of
CP (p)/s where CP (p) is the length of the critical time path of
p. e(λ·a−µ)·s · pens is the estimated penalty of missing the staleness
SLA due to higher-than-expected tuple arrival rate, where pens is
the penalty of missing the staleness SLA for a single tuple.

6.

SHARING OPTIMIZER

The goal of the sharing optimizer is to produce a low-cost admissible plan. Satisfying the dual constraints of finding an admissible
plan that is provably cheapest amongst all plans is a hard problem.
A sharing Si specifies SPJ transformations on a set of base relations. As the base relations are hosted on different machines, there
are several ways of combining them as well as where to place the
intermediate results. This results in plans with varying dollar cost
and critical time paths. For instance, performing many operations
in parallel on different machines may produce a plan with a small
critical time path. But such a plan may have a high dollar cost due to
high infrastructure costs involved in using many machines. On the
other hand, operations can also be performed sequentially to reduce
the dollar cost but at the expense of a high critical time path.
Among the generated plans those that have a critical time path
greater than the SLA of Si cannot be maintained by the system at the
desired staleness level, and hence are not admissible. The admissibility of plans forms the hard constraint of our problem in the sense
that the system should not admit a sharing that cannot be handled
by the system. At the same time, it also should not deny admitting
sharings that otherwise should have been admissible.
The sharing optimizer is based on the polynomial time heuristic
solution developed for System-R [11] and its analogous distributed
variant R∗ [18]. Our approach relies on generating, using a dynamic
programming approach, the cheapest possible plan in terms of dollar
costs, regardless of its critical time path and another plan with the
smallest critical time path, regardless of its dollar costs. We refer to
these plans as Dynamic Programming Dollar (DPD) and Dynamic
Programming Time (DPT), respectively. The DPD and DPT plans
have the following properties:

Cost Model

The cost of the plan, expressed in dollars per second, is computed by the amount of CPU, network, and disk capacity consumed
to setup the sharing and maintain it at the required staleness. The
provider periodically moves the updates to the MVs and buys CPU,
disk and network capacities from the Infrastructure as a Service
(IaaS). This cost can be further divided into two categories: resource
usage (i.e., CPU, disk capacity, network capacity) and penalty due
to possible SLA violations.
Resource Usage. There are existing analytical models that estimate the usage of various resources for maintaining a MV, based on
update rate, join selectivity, data location, etc. (e.g., [21]). This
analytical model is implemented as a resCost function that computes the cost of the resources consumed by a plan. Furthermore,
the resource usage should also vary with the staleness SLA of the
sharing. When the required staleness is much longer than the critical time path, e.g., the critical time path is 1 second and the staleness
requirement is 30 seconds, the sharing executor has much flexibility in deciding when to update the MV. Specifically, given a new
tuple to the base relations, the service provider can push it to the
MV immediately, or wait for as long as 29 second before pushing it.
On the other hand, when the staleness becomes close to the critical
time path, there is much less flexibility since other sharings in the
infrastructure may compete for resources.
In order to reduce the negative interaction at low staleness values,
the resources allocated to the plan are over-provisioned by a factor
that is inversely proportional to the required staleness. This simple
strategy ensures that the negative interactions are mostly avoided at
low staleness values.
SLA Penalty. At low staleness values the natural fluctuations in
the update rates may cause a plan to miss the SLA. This is because
the plan estimates the critical time path using the average arrival rate,
but in practice this is an over simplification as the updates frequently
vary. So, we have to estimate how much of penalty may be incurred
given the required staleness, which also has to be factored into the
cost. We estimate this by assuming a Possion arrival of updates, and
modeling the plan as a M/M/1 queuing system. Given the arrival rate
of each base relations, we can estimate the arrival rate of tuples in
the MV based on the selectivity of joins. The average service time of
the M/M/1 queue corresponds to the most time consuming operator
in the plan.
For an M/M/1 queue with arrival rate λ and service rate µ, the
percentage of items with sojourn time t larger than the staleness SLA
s is P (t > s) = e(λ−µ)·s . Thus the dynamic cost of a plan p with
staleness s is calculated as:

1. DPT is a plan with a low critical time path that is not optimized
on the operating dollar cost. If DPT is not admissible, then the
sharing can be safely rejected by the provider as there cannot
be a plan with a lower critical time path.
2. DPD is a plan with a low operating dollar cost that is not optimized on the critical time path. If DPD is admissible, then it
is also of the cheapest cost.
We provide a dynamic programming formulation to produce DPT
and DPD in Section 6.1, and provide a plan generation algorithm in
Section 6.2.

6.1 Dynamic Programming Formulation
We cast the problem of generating a plan as a bottom-up dynamic
programming formulation given by J OIN C OST in Algorithm 1. Consider a sharing that specifies a join sequence on the base relations.
For example, Figure 2 shows a join sequence of length two using the
two base relations, A and B.
Let Si be a sharing in S such that S RC(Si ) is the set of source
vertices of Si and MV(Si ) is the vertex corresponding to the MV

692

ΔA
ΔB

B

COPYDELTA
COPYDELTA

JOIN

Δ(A⋈ΔB)

m1

COPY
DELTA Δ(A ΔB)
⋈
UNION

Δ(A⋈B)

COPY
DELTA

ΔB
JOIN

Δ(ΔA⋈B)

Δ(ΔA⋈B)
DELTA
TOREL

m3

A⋈B

ΔA

m2

A

B
JO
IN

(a) (b)

B

ΔB

m2

COPYDELTA

JOIN

Δ(A⋈ΔB)
COPY
DELTA

ΔA
ΔB

Δ(ΔA⋈B)

B
A

m1

UNIO

Δ(ΔA⋈B)

A

A

ΔA
JOIN

DELTA

m1

Δ(A⋈ΔB)

Δ(ΔA⋈B)

COPYDELTA

m3

Δ(ΔA⋈B)
UNION

N

m3 Δ(A⋈B)

Δ(A⋈B) DELTATOREL A⋈B

m1

JOIN

COPYDELTA

COPYDELTA

Δ(A⋈ΔB)

ΔB

ΔA COPY

m2

ΔA

B

COPY
DELTA
DELTATOREL

ΔA

A⋈B

DELTA
TOREL

A

m3

ΔB

B
JOIN

JOIN

Δ(A⋈ΔB)
DELTATOREL

ΔB

m2

COPY
DELTA

A

(c) (d) Δ(ΔA⋈B)

Δ(A⋈ΔB)
UN

ION

Δ(A⋈B)

A⋈B

DELTATOREL

Figure 3: Four ways of joining A and B, (a) in-place (no copies of A or B), (b) copy B (c) copy A, (d) copy A and B.
Algorithm 1 sub J OIN C OST(<R, mi >)
1: for all join sequences R − a do
2: for mj ∈ set of available machines do
3:
(C AP<R−a,mj > , D <R−a,mj > ) = J OIN C OST(<R − a, mj >)
4:
for mk ∈ set of available machines do
5:
C AP0 ← C AP<R−a,mj >
6:
D 0 ← D <R−a,mj >
7:
Choose cheapest case among (a)–(d), update C AP0 and D 0
8:
Case (a): In-place join of <R − a, mj > with <a, mk >
9:
Case (b): Copy <R − a, mj > to <R − a, mk >. In-place join of

of Si . SLA(Si ) is the staleness SLA of Si . M AC(Si ) denoted the
set of dedicated machines available to host the sharing. Below, the
short-hand notation <v, m> denotes any vertex v (i.e., relation, MV
or a delta of a relation) in the plan that is placed on a machine m.
We capture the state of the problem up on creating a join sequence
R on a machine m (i.e., <R, m>) as (D<R,m> , C AP<R,m> ) such that:
1. D<R,m> is the cheapest plan among all those that produce R
on machine m. The cheapest plan is chosen by applying a
cost function C OST C ALC, which takes a plan as input and
produces a cost value. Later, we will specify two implementations of C OST C ALC that will produce the DPT and DPD
plans.

10:
11:
12:

<R,m>

13:
14:
15:
16:

2. C AP
is the remaining capacity in the infrastructure (e.g.,
CPU, disk, network capacities) after discounting the capacity
consumed by D<R,m> .
We generate the join sequence R at machine m bottom-up by
enumerating all the states corresponding to: a) R−a on any machine
in M AC(Si ), b) with any remaining capacity. R − a refers to a
join sequence R without a base relation a. D<R,m> is generated by
adding vertices and edges required to join R − a with a to the plan
from the prior state. Among the plans generated this way, we choose
the plan with the smallest value produced by C OST C ALC.
Such a formulation is used by J OIN C OST to obtain the plan of any
arbitrary join sequence R on mi ∈ M AC(Si ), which is formed by
joining R − a on mj ∈ M AC(Si ) (i.e., <R − a, mj >) with a base
relation a on machine mk ∈ M AC(Si ) (i.e., <a, mk >).
At a high level, given two relations A on machine m1 , and B on
machine m2 , we consider four ways of producing A ./ B on a machine m3 , which are illustrated in Figure 3. In particular, Figure 3a
shows the case where A ./ B is produced without copying any of
the base relations (i.e., in-place join). Figure 3b–c show the cases
when one of the base relations is copied. Figure 3d shows the case
when both A and B are copied to m3 , and then an in-place join is
performed. Note that the four cases (a)–(d) in the J OIN C OST formulation (lines 8–11) correspond to the four ways of joining two
relations A and B in Figure 3.
J OIN C OST uses an in-place join and a copy procedure to update
the plan D<R−a,mj > to produce R on machine mi . We explain below the procedure of creating a copy of a relation and joining two
relations using A and B in place of R − a and a, respectively.
To copy of <A, m1 > to form <A, m2 > requires the addition
of one vertex and two edges – vertex <A, m2 > and CopyDelta
between <∆A, m1 > and <∆A, m2 >, and DeltaToRel (apply updates) between <∆A, m2 >. An in-place join between <A, m1 > and
<B, m2 > to produce <A ./ B, m3 > can add up to 8 vertices and 8
edges, as shown in Figure 3 depending on whether m1 , m2 and m3
are all distinct machines.
After adding the necessary vertices and edges, the capacities of
machines involved are modified using the resCost function defined

<R − a, mk > and <a, mk >
Case (c): Copy <a, mk > to <a, mj >.
In-place join of
<R − a, mj > with <a, mj >.
Case (d): Copy <R − a, mj > to <R − a, mi >. Copy <a, mk >
to <a, mi >. In-place join of <R − a, mi > and <a, mi >
Update C AP<R,mi > , D <R,mi > with D 0 and C AP0 if C OST C ALC(D 0 )
is cheaper.
end for
end for
end for
return C AP<R,mi > , D <R,mi >

previously. If there is no capacity left in m1 or m2 , C OST C ALC
function would cost such a plan at ∞ indicating that the plan is infeasible.

6.2 Plan Generation Algorithm
Finally, we describe an algorithm which takes a sharing Si and
generates two sharing plans depending on the choice of the cost
function, which we had left unspecified earlier. Recall that the DPD
plan has a low dollar cost, whereas the DPT plan has a low critical
time path.
If a DPD plan is needed, C OST C ALC uses C OST function (described in Section 5.2), which computes the cost of a plan in dollars
per second. If a DPT plan is needed, C OST C ALC uses the CP function (described in Section 5.1), which computes the critical time path
of the plan.
The plan generation algorithm computes the cheapest way to build
all join sequences of length 1 < x ≤ |S RC(Si )| in a bottom-up
manner, where S RC(Si ) is set of base relations of Si . The algorithm
obtains the cost to construct longer join sequences of length x, using
the output of prior invocations for join sequences of length x − 1.
The algorithm terminates when it produces the join sequences corresponding to the set of transformations specified in the sharing. The
sharing optimizer generates both DPD and DPT plans. If the DPT
plan is not admissible, then it means that there may not exist a plan
of Si that is admissible and hence, Si is rejected by the provider. If
DPD and DPT are both admissible, the sharing optimizer uses DPD
as it has a lower cost than DPT.

7.

MULTI-SHARING OPTIMIZATION

If two different admitted sharings share similar vertices and edges,
there could be an opportunity to further reduce the cost. The
provider can take advantage of this commonality by amortizing the

693

operating costs across several sharings. The commonality here is
replacing two disjoint sets of vertices and edges belonging to different sharings that perform identical or similar transformation with a
common set for multiple sharings.
Although our idea of merging commonalities in plans is similar as
merging common subexpressions in concurrent running query execution plans [26], there are two main differences. First, our infrastructure contains multiple servers and the cost of moving the data
across the servers has to be considered. Second, as we show below,
unlike [26] we do not restrict to only merging identical subexpressions across plans.
D is a global plan obtained by merging the plan of sharings in
S and then discarding duplicate edges and vertices. Note that D
need not be a single connected component. Each vertex (or edge)
v ∈ D records the identity of all the sharings that it serves, such that
S HR(v) records the sharings to which v belongs. Given a vertex (or
a set of vertices) v, let A NC(v) be the set of vertices and edges that
are ancestors of v in D.
Commonalities in D are reduced by applying a plumbing operator
repeatedly until the D does not perform any redundant work. A
plumbing operator takes two sets of vertices v1 and v2 belonging to
plans as inputs. Then, vertices and edges that supply the vertices in
v2 (or v1 ) are retained but those supplying v1 (or v2 ) are discarded.
We now discuss the mechanics of a plumbing operator as well as an
algorithm to apply them.

7.1

Note that we do not preclude other kinds of plumbing operations
here as long as they do not compromise the correctness of the plan
and result in a cost reduction.
A plumbing operation p is implemented as follows. First, add
the necessary edges and vertices to perform p. For all vertices and
edges v ∈ A NC(D ST(p)), remove S HR(D ST(p)) from the S HR(v).
For all v ∈ A NC(S RC(p)), add S HR(D ST(p)) to S HR(v). Add
S HR(D ST(p)) to S HR(S RC(p)) as well. Remove vertices and edges
v ∈ A NC(D ST(p)) s.t., S HR(v) = ∅.

7.2 Optimization Algorithm
Given the global plan D, we want to identity a set of plumbing operations to perform, that would produce the provably least cost plan
without exceeding the staleness SLA of any of the sharings. The
hardness of this problem comes from the observation that a plumbing can affect the benefit of other plumbings. Figure 4c shows an
example of three plumbings pi , pj and pk that affect one another.
For example, pi cannot be applied if pj has already been applied to
D as applying pj would have removed the vertices in S RC(pi ). For
the same reason pj cannot be applied if pk has already been applied.
Finally, the benefit and the increase to the critical time path due to
pi is affected if pk has already been already applied.
An optimal algorithm that chooses a set of plumbings to perform
resulting in a provably cheapest plan is a hard problem. For our purposes here any plan that is cheaper than D is good enough. Our algorithm is referred to as greedy hill-climbing approach as it applies
one plumbing at a time in a greedy fashion, until no more plumbings can be applied to D. This algorithm has at least the desirable
property that it is fast to execute and intuitive to understand.
Given the global plan D, let P be the set of all possible plumbing
operations that can be performed on D. The greedy hill-climbing
algorithm at each iteration first computes P and then applies the
plumbing operation p ∈ P with the maximum benefit. The set of
plumbings P is obtained using a two step process. First, we examine
all pairs of vertices in D to determine if a copy plumbing can be performed between them. Second, we examine all applicable triples of
vertices to determine if a join plumbing can be performed between
them. The benefit and the critical time path increase due to each
plumbing are computed. If a plumbing has zero or negative benefit,
or causes a sharing to miss its SLA, it is discarded from P . The
algorithm terminates when no more plumbing operation can be applied to D. The resulting global plan forms the input to the sharing
executor. We will show later in Figure 13 that this strategy is quite
effective and results in large savings for the provider.

Plumbing Operations

A plumbing operation p is defined between a set of source vertices S RC(p) and another set of destination vertices D ST(p), such
that after the plumbing operation D ST(p) gets tuples via S RC(p).
Applying p on D results in a potential cost reduction is due to the
removal of vertices from D, although some expenditure in the way
of additional vertices and edges is made to facilitate the plumbing.
The benefit of p is defined as the dollar cost savings due to the removal of vertices and edges in the plan minus the cost of adding the
additional vertices and edges to implement p. Applying p can potentially increase the critical time path of all sharings in S HR(D ST(p)).
This is because they now obtain their tuples via S RC(p), which may
be a longer path in terms of time. We note that p is feasible only if
it has a positive benefit. Moreover, performing p should not cause
the critical time path of any of the sharings in S HR(D ST(p)) to exceed their SLA. We consider the following two kinds of plumbing
operations, which are shown in Figures 4a–b.

COPY
DELTA

ove

DST(pi)

pi

SRC(pi)

(b)

Rem

ove

Rem

(a)

DST(pi)

SRC

pi

(pi )

(c)

pi

8.

pj

SHARING EXECUTOR

The sharing executor takes a global sharing plan D and maintains
the individual sharings at the appropriate level of staleness. In its
very nature, the sharing executor must be robust to any deviations in
the update rate and the behavior of the machines in the infrastructure.
The sharing executor applies its own set of run time optimizations,
some of which are briefly described. We first describe how staleness
is computed and how the push operator reduces the staleness of a
sharing arrangement. Next, we design a model to determine the most
appropriate time to push the sharings.

JOIN

pk

Figure 4: Types of plumbings (a) CopyDelta and (b) Join and (c)
how plumbings can affect one another

8.1 Staleness and Push

1. Copy Plumbing: Takes two delta vertices on different machines and adds a CopyDelta edge between them.

Each machine runs an agent which is responsible for sending periodic timestamps updates to the sharing executor. We refer to these
messages as heartbeats. We implement the following three functions
– TS(.), M IN TS(.), M AX TS(.) – that obtain the current timestamp
of a vertex, and the minimum and maximum timestamps of a set of
vertices in the sharing plan. For instance, M IN TS(S RC(Si )) and

2. Join Plumbing: Takes two vertices – a delta and a relation
– and performs a join to obtain the destination delta vertex.
For example, ∆(A ./ ∆B) is plumbed using A and ∆B by a
Join edge and up to two CopyDelta edges.

694

M AX TS(S RC(Si )) are the minimum and maximum timestamps of
the sources of Si .
The current staleness of a sharing is defined as the difference between the maximum of the timestamps of the base relations of Si to
that of the MV of Si . Note that the staleness should always be maintained to be less than SLA(Si ). This is captured by the following
inequality.

to take into account recent system fluctuations. We record the actual
time to perform each of the operators, compare it against estimated
time and periodically recompute the time model. This feedback loop
allows our system to be reasonably robust to data and machine fluctuations.
An appropriate timestamp t to advance the MV of Si should be
greater than the current timestamp of MV but should be less than
or equal to the minimum of the timestamp of the sources of Si . In
particular,

M AX TS(S RC(Si )) − TS(MV(Si )) ≤ SLA(Si ).
To reduce the staleness of a sharing Si , the executor schedules a
sequence of P USH commands in a topological order starting with
S RC(Si ), until the timestamp of the MV has a more up-to-date
timestamp. A P USH command instructs the agent to advance the
timestamp of a vertex in the sharing plan by applying an operator
denoted by its incoming edge. The incoming edge belongs to one of
the four edge types we described in Section 5.
Suppose an agent receives a P USH command to advance a vertex
v to timestamp t. Suppose that e is an incoming edge of v. The
agent first obtains a write lock on v. It then compares the current
timestamp t0 of v with that of t. If t0 ≥ t then there is no need to
perform any work. If t0 < t, then the agent performs the operation
corresponding to e’s type so that the timestamp of v can be advanced
to t. Once the operation has been performed the agent responds
with a P USH D ONE command, and piggybacks useful statistics such
as time taken to perform the operation and current timestamps. The
executor up on receiving the P USH D ONE proceeds with the outgoing
edges of v.
When it comes to maintaining multiple sharings, the design of a
sharing executor is simplified due to the observation that any sharing in S can be pushed independently of the others in S even though
they may have common edges and vertices. Suppose a vertex is at
a timestamp t and there are two concurrent P USH commands to advance it to t1 and t2 , t ≤ t1 ≤ t2 , respectively. Regardless of the
order in which the two commands are executed, the amount of work
done by e is equal to the work done to advance the timestamp to
t2 . This is why the sharing executor does not have to coordinate
P USH commands between the various sharings that it maintains.
This makes for a simple design of the sharing executor, especially
since the sharings may have different staleness SLAs and may have
to be pushed at different rates.

8.2

TS(MV(Si )) < t ≤ M IN TS(S RC(Si )).
When the push finishes, the MV would be at the timestamp t, while
the timestamp of the sources may all be advanced by CP(Di , t −
TS(MV(Si ))). So, the staleness of the sharing at the time the push
finishes would be: M AX TS(S RC(Si )) + CP(Di , t−TS(MV(Si ))).
We stipulate that the staleness when the push finishes should be less
than the staleness SLA using the following inequality:
M AX TS(S RC(Si )) + CP(Di , t − TS(MV(Si ))) − t ≤ SLA(Si ).
On the other hand, the sharing executor does not want to push too
early as well. In other words, the sharing executor is early if the
push command could have waited a bit longer and still could have
completed before Si became stale. This can be stipulated by adding
the additional constraint that:
l ∗ SLA(Si ) ≤

M AX TS(S RC(Si ))+
CP(Di , t − TS(MV(Si ))) − t

≤ SLA(Si ),

where l > 0 (0.8 in our case) is chosen to account for run-time
deviations, such as a queuing delay if the P USH waits for the capacity on a machine to be available. An appropriate value of t is
obtained by performing a binary search between TS(MV(Si )) and
M IN TS(S RC(Si )) that satisfies the above constraint.

9.

EXPERIMENTS

In this section, we present an experimental study to validate the
SMILE sharing platform. We first describe the experimental setup
in Section 9.1. We then evaluate the performance of our system for
varying rate of updates on the base relation in Section 9.2 and varying SLA in Section 9.3. We examine the effect of varying the number of machines and the sharings in the infrastructure in Section 9.4.
Next, the efficacy of the hill-climbing algorithm applied to DPT and
DPD is shown in Section 9.5. Finally, we highlight the robustness
of the sharing executor in Section 9.6 by varying the update rates on
the base relations and varying read workload on the MV.

Design

Our sharing executor uses a simple model to determine two key
questions: a) Is it time to push Si ? b) By how much to advance
the timestamp of MV of Si ? To determine these two questions, we
develop a model to determine the most appropriate time to schedule
the push and the timestamp to push the MV to such that the sharing
will not miss its SLA.
A simpler design of a sharing executor pushes all the sharings
in S every second so that they do not violate their SLA. Given the
property that the critical time path of an admissible sharing is less
than its staleness SLA, the push will finish before the SLA is violated. However, our sharing executor does not push every second
but rather bunches up sufficient work so that the push is issued as
late as possible. Yet, it is scheduled such that the push would be
completed before Si becomes stale.
To develop the model, we modify the critical time path function
CP(Di , x) to take an additional parameter x, which corresponds to
the amount of timestamp to advance the MV of Si . In Section 5.1
when we described how we compute the critical time path of a sharing plan, x was defaulted to be one but now can take up any arbitrary
value greater than or equal to one. We also added a feedback loop
to the CP function so that it constantly recomputes the time model

9.1 Setup
Our experimental setup creates a mobile cloud ecosystem containing 25 apps where sharings are specified using user, social network, location, checkin, and user-content datasets; the datasets and
the sharings are representative of those one may find in a mobile
cloud. We collected Twitter tweets from a gardenhose stream, which
is a 10% sampling of all the tweets in Twitter, for a six month
period starting from September 2010. The tweets were unpacked
into nine base relations corresponding to the information about the
user (i.e., users relation), tweets (i.e., tweets relation), social
network (socnet relation), checkins (foursq), and user-content
(i.e., urls, hashtags, curloc, photos relations) associated
with the tweets and the location of the user (i.e., loc relation).
This creates our realistic datasets that capture rich information about
users, locations, social contacts, checkins and the various contents
the users are interest in.

695

Table 1: Twitter base relations (left) and the twenty-five sharings (right) used in the evaluations
Base Relations:

Sharings (Apps):
S1
S2
S3
S4
S5
S6
S7

users ./ socnet (twitaholic)
users ./ tweets ./ curloc (twellow)
users ./ tweets ./ urls (tweetmeme)
users ./ tweets ./ urls ./ curloc (twitdom)
users ./ tweets (tweetstats)
tweets ./ curloc (nearbytweets)
urls ./ curloc (nearbyurls)

S13
S14
S15
S16
S17
S18
S19

S8

tweets ./ photos (twitpic)

S20

S9

foursq ./ tweets (checkoutcheckins)

S21

S10
S11

hashtags ./ tweets (monitter)
foursq ./ users ./ tweets
./ curloc (arrivaltracker)
foursq ./ users ./ tweets (route)

S22
S23
S24
S25

S12

9.1.1

Total Push Time (seconds)

We specify 25 sharings by combining these 9 base relations in
different ways. A description of the base relations and the 25 sharings used in the evaluation are shown in Table 1. For each of the
25 sharings, we also mention an existing real app that may benefit from such a sharing. For instance, the application twitter-360,
which displays nearby photos may be interested in S18 which corresponds to users ./ tweets ./ photos ./ curloc. By building
an ecosystem around twitter data and choosing sharing that match
the functionalities of existing apps, we are ensuring that our evaluation is as realistic as possible. Our setup consisted of 6 machines,
such that the 25 sharings were arbitrarily assigned to the available
machines. All the machines in our setup ran identical versions of
Postgresql 9.1 database. Our system starts with 7 million tweets
prepopulated into our databases.
As we vary the rate of arrival of tweets into our system, the rate
of update on the base relations (other than tweets) depends on the
number of tweets seen so far by the system. For instance, at the
beginning any incoming tweet most likely will contain the identity
of a user not previously seen by the system, which would result in the
insertion of a tuple to the users relation. However, after the system
has ingested a sufficient number of tweets, the update rate on the
users relation will decrease as some of the users already would be
present in the users relation. The dependence between the number
of tweets ingested by the system and the chance that an incoming
tweet will result in an insertion to a base relation is expressed in
terms of an update ratio. After 7 million tweets have been ingested
by our system, the update ratio of encountering a previously unseen
user in the next tweet is around 0.3. The update ratio values for some
of the remaining relations were 0.25, 0.02, 0.1, 0.2, for socnet,
loc, curloc and urls, respectively. Using the update ratios, we
can estimate the rate of updates on all the base relations by varying
the rate of incoming tweets in the system.

foursq ./ users ./ tweets ./loc (locc.us)
tweets ./ loc (locafollow)
users ./ loc ./ tweets ./ curloc (twittervision)
foursq ./ users ./ tweets ./ socnet (yelp)
users ./ loc (twittermap)
users ./ tweets ./ photos ./ curloc (twitter-360)
users ./ tweets
./ hashtags ./ curloc (hashtags.org)
users ./ tweets ./ hashtags
./ photos ./ curloc (nearbytweets)
users ./ tweets ./ foursq
./ photos ./ curloc (nearbytweets)
foursq ./ curloc (nearbytweets)
photos ./ curloc (twitxr)
hashtags ./ curloc (nearbytweets)
hashtags ./ users ./ tweets (twistroi)

5

Total Push Time (seconds)

User info
Tweets info
Social network
User address
User current loc
Tweet links
Tweet entities
Photo links
Rest. checkins

4
3
2
1
0
0

3000

6000

0.25
0.2
0.15
0.1
0.05
0

9000

0

No. of Delta Tuples

4000

Total Push Time (seconds)

(b)

5
4
3
2
1
0
0

3000

8000

No. of Delta Tuples

(a)

Total Push Time (seconds)

users(uid, ...)
tweets(uid, uid, ...)
socnet(uid, uid2, ...)
loc(uid, place, ...)
curloc(tid, lat, lng, ...)
urls(tid, url, ...)
hashtags(tid, tags, ...)
photos(tid, urls, ...)
foursq(tid, rid, ...)

6000

9000

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

No. of Output Tuples
(c)

3000

6000

9000

No. of Output Tuples
(d)

Figure 5: Time cost model of the four edge types, namely a)
DeltaToRel, b) CopyDelta, c) Join, and d) Union

9.1.2

Dollar and Time Cost Models

The infrastructure costs for our cost model was obtained from
Amazon EC2 pricing available on the web1 . Our machines are
equivalent to large Linux instances, which cost $0.34/hour. For the
network cost, we assumed that the instances were in different availability zone but in the same region, which had a transfer cost of $0.01
per GB. For storage, we used EBS storage at $0.11 GB/month.
We developed a time model for each edge type to estimate the
time taken to process tuples, as function of the number of input tuples. Our setup to compute a time model consisted of two machines
with 15 base relations of varying sizes between 200k and 50 million tuples, number of attributes from 1 to 7 as well as different
attribute types forming tens of sharings between the base relations.
We pushed a varying number of tuples between 1 and 10k through
each edge in the setup and then measured the time taken to perform
each P USH operation, which is recorded in Figure 5. It can be seen
that the time taken to push tuples through different edge types is
linear in the number of tuples for all the edge types, although with
different slopes. These plots form the basis of our time cost model.

Snapshot Module

To determine the efficacy of our system, an independent auditing
module in our sharing executor, called snapshot, records the staleness of all the sharings once every 5 seconds. Suppose that a sharing
Sj was found to have a staleness less than the SLA staleness at snapshot i. If Sj satisfies the SLA staleness in snapshot i + 1, then the
system is assumed to have maintained Sj at the appropriate level of
staleness for all intermediate time periods between i and i + 1. The
converse is true if Sj is found to have violated the SLA at snapshot
i+1. The snapshot module also keeps track of the cost, and the number of tuples moved between snapshots. Additionally, we record the
staleness of all sharings before and after a P USH operation as well
as the cost incurred and time taken for each P USH operation.

9.2 Varying Rate
We used 6 machines and 25 sharings as shown in Table 1 with a
1

696

http://aws.amazon.com/ec2/pricing/

0

100

200

300

0

Snapshot

100 200 300
Snapshot

(S3)

0

100 200 300
Snapshot

0

100 200 300
Snapshot

(S4)

0

100 200 300
Snapshot

(S10)

0

100 200 300
Snapshot

(S5)

0

100 200 300
Snapshot

60
50
40
30
20
10
0

60
50
40
30
20
10
0

No. Tuples Moved

100000
0
0

400

300000

S16

100000
0
0

100 200 300
Snapshot

S1

200000
100000
0
0

200

400

Snapshot

300000

200
400
Snapshot

S3

200000
100000
0
0

200
400
Snapshot

1.2e+06

(S11)

0

200

200000

Snapshot

(S18)

0

S11

200000

No. Tuples Moved

100 200 300
Snapshot

60
50
40
30
20
10
0

100 200 300
Snapshot

300000

No. Tuples Moved

60
50
40
30
20
10
0

0

0

No. Tuples Moved

60
50
40
30
20
10
0

Staleness (seconds)

Staleness (seconds)

Staleness (seconds)

60
50
40
30
20
10
0

(S9)

(S17)

(S25)

ALL, SLA=45s

1.1e+06

No. Tuples Moved

100 200 300
Snapshot

100 200 300
Snapshot

60
50
40
30
20
10
0

100 200 300
Snapshot

Staleness (seconds)

0

60
50
40
30
20
10
0

0

60
50
40
30
20
10
0

0

0

60
50
40
30
20
10
0

Staleness (seconds)

10

(S2)

(S8)

(S16)

(S24)

Staleness (seconds)

20

100 200 300
Snapshot

100 200 300
Snapshot

60
50
40
30
20
10
0

100 200 300
Snapshot

Staleness (seconds)

60
50
40
30
20
10
0

0

30

60
50
40
30
20
10
0

0

0

60
50
40
30
20
10
0

Staleness (seconds)

40

(S7)

(S15)

(S23)

Staleness (seconds)

(S1), SLA=45s

50

60
50
40
30
20
10
0

100 200 300
Snapshot

60
50
40
30
20
10
0

100 200 300
Snapshot

Staleness (seconds)

Staleness (seconds)

60

0

Staleness (seconds)

Staleness (seconds)

100 200 300
Snapshot

(S14)

0

60
50
40
30
20
10
0

Staleness (seconds)

0

60
50
40
30
20
10
0

(S22)

Staleness (seconds)

100 200 300
Snapshot

(S13)

100 200 300
Snapshot

Staleness (seconds)

0

60
50
40
30
20
10
0

0

60
50
40
30
20
10
0

Staleness (seconds)

(S12)

100 200 300
Snapshot

(S21)

Staleness (seconds)

60
50
40
30
20
10
0

0

60
50
40
30
20
10
0

Staleness (seconds)

100 200 300
Snapshot

(S20)

Staleness (seconds)

0

60
50
40
30
20
10
0

Staleness (seconds)

Staleness (seconds)

(S19)

Staleness (seconds)

Staleness (seconds)
Staleness (seconds)

60
50
40
30
20
10
0

100 200 300
Snapshot

(S6)

1e+06
900000
800000
700000
600000
500000
400000
300000
200000
100000
0

0

100 200 300
Snapshot

0

100

200

300

400

Snapshot

Figure 6: Staleness of twenty-five sharings across snapshots for a rate of 6k tweets/second (left) and the number of tuples moved in
each snapshot (right). Due to space constraints, only two zoomed-in graphs are provided for readability, while the remaining figures
are rendered small to show trends.
Cost vs. Data Rate

15
10

30
20

0

S1

100

200
300
Push Iteration
(a)

400

1000

6
4
2

500
200
100

0

50 G.1k.5k1k F 2k3k5k6k

50 G.1k.5k1k F 2k3k5k6k

Tweets / second

Tweets / second

0

2000

4000

6000

Time Elapsed (seconds)

(b)

(c)

40
35

streams, and about 3 violations per sharing-hour (i.e., per hour per
sharings) for 6k tweets per second. Note that the zero violations for
both the gardenhose and firehose streams were in spite of their unpredictable arrival rate, which is shown in Figure 8c. At 6k tweets
per second, the cost was about $25 per sharing-hour, although note
that some of the sharings were much more expensive than the others.
In contrast, the average cost for the firehose (F) stream was about $6
per sharing-hour. Secondly, the number of tuples moved per snapshot (i.e., 5 seconds) across all the sharings was between 600k and
1.1 million tuples as shown in Figure 6 (right).

30

15
0

8

(a)

20

0

2000

Figure 8: (a) Cost and (b) violations per sharing-hour for gardenhose (G), and firehose (F) streams and rates from 50 to 6k.
Variations in Gardenhose (G) rate shown in (c)

25
10

5

Gardenhose Rate

10

Tweets / second

20

45

40

No.of violations per hour

25

50
Time Pushed

100

200

300

400

Push Iteration
(b)

Figure 7: a) Staleness before and after a P USH on S1 , while b)
shows how much is pushed
Figure 6 (left) shows the staleness of a sharing S1 across different
snapshots. It can be seen that the staleness of each of the sharings increases until it comes close to the SLA (i.e., 45 seconds), after which
the staleness sharply reduces to a low value due to a P USH from the
sharing executor. The staleness before and after a push operation are
shown in Figure 7a, where it can be seen that the P USH operation
reduces the staleness of S1 to less than 10 seconds, just before the
staleness of S1 was about to exceed the SLA. Figure 7b shows that
every push operation advanced the timestamp of S1 by 25 to 40 seconds, which shows the lazy behavior of the sharing executor. One
thing to note here is that there were only 31 violations for all the 25
sharings for the entire duration of the experiment lasting about 40
minutes. We summarize some of our observations below.
The number of violations with varying incoming rate of tweets as
well as the cost to maintain the sharings in sharing-hour are shown in
Figures 8a–b. The results did not show a well defined trend between
the number of violations and the rate of incoming tweets, except that
the violations were very low, even for 6k tweets/second. First of all,
there were zero violations for the firehose (F) and gardenhose (G)

% change in tuples moved

Stalenes (seconds)

55

BEFORE (S1)
AFTER (S1)

50

Violations vs. Data Rate

30

Dollars per hour

SLA of 45 seconds, while varying the rate of tweets from 50 to 6k
tweets/second. At 6k tweets/second (i.e., 3.6 billion tweets/week),
the update rate matches the current known volume of tweets in Twitter [12]. We also replayed gardenhose stream, which roughly corresponds to an average of 100 tweets/second. The rate of arrival for
tweets for a two hour window is shown in Figure 8c. As the gardenhose is a 1 out of 10 sampling of tweets from the firehose, which is
a stream containing all the tweets in Twitter, we recreated a stream
similar (although by no means equivalent) to firehose by replaying
gardenhose at 10X speed.

Tuples moved with/without sharing commonality
0
-500
-1000
-1500
-2000

Small Gap

Large Gap

-2500
-3000
S1

S3

S4 S20

S7

S8

S9 S10 S23

Sharing Arrangements

Figure 9: Number of tuples moved in the setup in Figure 6 expressed as percent reduction from the case when individual sharings are run in isolation
Next, notice in Figure 6 (left) that some sharings, such as S7, S8,
S9, S10, and S23 have a larger gap between the peak staleness value

697

9.3

7000
6000
5000
4000
3000
2000

35

8000

30

7000
Tweets/second

8000
Tweets/second

Rate vs. Sharings

Tuples/machine vs. Machines

Thousands of Tuples/second

Rate vs. Machines

and the SLA, whereas others such as S1, S3, S4, and S20 have relatively smaller gaps. The reason for this is that those sharings with
larger gaps benefit from the commonality with other sharings but not
so for those with smaller gaps. To test this hypothesis, we compared
the number of tuples moved for each sharing in the above experimental setup with the number of tuples moved when the sharings
are run in isolation. The number of tuples moved in the former case
is shown as a percentage reduction from the latter case in Figure 9.
It can be seen that sharings with small gaps only benefit modestly
from the presence of other sharings, whereas those with larger gaps
benefit immensely from the presence of other sharings.

25
20
15
10

4000

2000

0
2

No. of Machines

5000

3000

5

2 3 4 5

6000

3

4

5

20 25 30 40 50

No. of Sharings

No. of Machines

(a)

(b)

(c)

Figure 11: Maximum tweet rate for varying (a) machines, (b)
sharings on a setup with SLA = 45 seconds

Varying SLA

Table 2: Number of violations per sharing-hour (rounded-up)
for varying SLA between 10 and 60 secs
Staleness SLA
10 20
30
40
50
60
Mix
Violations
4
1
2
1
0
0
0

support without losing the stability of the system. We have built an
appropriate mechanism to monitor the stability of our system, which
is not discussed here due to lack of space. It can be seen from Figure 11a that increasing the number of machines increases the maximum rate that can be handled by our system. Moreover, adding an
extra machine increases the processing capacity of our system by at
least 25–30k tuples/sec as can be seen from Figure 11b. Next, we
varied the number of sharings from 20 to 50 keeping the number
of machines fixed at 6 and SLA of 45 seconds, as shown in Figure 11c. We increased the number of sharings beyond 25 by placing
the same sharing on more than one machine. With increasing number of sharings, the maximum rate decreases as database and other
system bottlenecks start manifesting due to the increased number of
vertices and edges that the system has to manage.

Our setup consisted of 6 machines, 25 sharings and an incoming
rate of 1000 tweets/second. We varied the SLA between 10 and 60
seconds. Table 2 shows the effect of varying the SLA in terms of
the number of violations. The number of violations is maximum
for SLA = 10 seconds at 4 violations per sharing-hour. In general,
the number of violations for staleness SLA values greater than 10
seconds is extremely low at either 1 or 2. The higher number of
violations for 30 seconds (at 2 per sharing-hour) compared to those
for 20 and 40 seconds (at 1 per sharing-hour) was due to temporary
fluctuations in system resources.

9.5 Algorithm Comparisons

Cost Change of Mix vs. Uniform SLA

-100
-200
-300
-400

SLA=40
S9 S11 S13 S15

100

S17 S19 S21 S23 S25

Sharing Arrangements

Figure 10: Cost change for mix SLA compared to uniform SLA

0.008
0.007
0.006
0.005
0.004
0.003
0.002
0.001
0

200

50

100
Snapshots

100

150

200

Snapshots

0.0025

DPT+HC

50

In the above experimental setup, we also examined a case where
we assigned non-uniform SLA (see mix in Table 2) to the 25 sharings. In particular, S1–S7 were assigned a SLA of 10 seconds, S8–
S15 a SLA of 40 seconds, and S16–S25 a SLA of 60 seconds. From
Table 2, we can see that the mix case resulted in zero violations, although having comparable dollar costs to the uniform SLA cases.
Then in Figure 10 we expressed the cost of an individual sharing in
the mix case as a percentage change to the corresponding cost from
the uniform case (i.e., compare costs of S1 from mix with uniform
when SLA was 10 seconds). It is interesting to note that although
the costs of S1–S7 have become marginally more expensive, the cost
of the other sharings (i.e., S8–S15, S16–S25) is now significantly
cheaper. Hence, we can conclude that few sharings with small SLAs
subsidize the operating cost of other (related) sharings.

9.4

150

0.0033

DPD

Snapshots

SLA=60

-500
S1 S3 S5 S7

0.008
0.007
0.006
0.005
0.004
0.003
0.002
0.001
0

0.0042

DPT

50

SLA=10

Cost (Dollars)

0.008
0.007
0.006
0.005
0.004
0.003
0.002
0.001
0

0

Cost (Dollars)

Cost (Dollars)

0.008
0.007
0.006
0.005
0.004
0.003
0.002
0.001
0

Cost (Dollars)

% change in cost

100

150

200

0.0023

DPD+HC

50

100

150

200

Snapshots

Figure 12: Cost of DPT and DPD reduced by applying hillclimbing algorithm to produce DPT+HC and DPD+HC
We examined the efficacy of the hill-climbing algorithm that we
apply to the DPD and DPT algorithms to reduce the cost for the
provider. For this experimental setup, we used 6 machines, 25 sharings and a rate of 1000 tweets/second. The cost model we considered
was same as before, except that we changed the networking pricing to be within the same availability region in EC2 (i.e., no cost).
We generated DPD and DPT sharing plans for this setup, and then
applied the hill-climbing algorithm to both these sharing plans to
produce DPD+HC, and DPT+HC, respectively. The average cost in
dollars per sharing-second for the four sharing plans in sharing-hour
were as follows — DPT 0.0042, DPD 0.0033, DPT+HC 0.0025,
and DPD+HC 0.0023 as shown in Figure 12. It can be noticed
that DPD+HC has the cheapest cost but is comparable to DPT+HC.
When we compared DPD with DPD+HC, and DPT with DPT+HC,
the difference is quite significant representing a 35% reduction in
cost, thus making a case for our hill-climbing approach.

Varying Machines and Sharings

In this experimental setup, we varied the machines from 2 to 5,
while keeping the number of sharings fixed at 25 and a SLA of 45
seconds. For every setup, the capacity of the machine was determined to be the highest rate of tweets that the set of machines can

698

Edge
Vertex

DPD

10.

DPT

240
220
200
180
160
0

2

4

6

8

10

12

14

0

2

4

6

8

10

12

14

16

Iterations

Figure 13: Reduction in vertices and edges as plumbing operations are sequentially applied to DPD and DPT

11.

Figure 13 shows the number of vertices and edges as the hillclimbing algorithm takes DPD or DPT sharing plan as input and
performs plumbing operations in a sequential fashion. As can be
seen from the figure, the sharing plan is reduced by more than 80
vertices and edges for both DPD and DPT, which represents significant savings in terms of cost.

9.6

Staleness (seconds)

8Users,
75 tweets/s

60

16Users,
75 tweets/s

32Users,
100 tweets/s

50Users,
150 tweets/s

50
40
30
20
10
0
0

50

100

150
Snapshot

200

250

REFERENCES

[1] Infochimps. http://www.infochimps.com/, 2012.
[2] Microsoft azure market. http://datamarket.azure.com/, 2012.
[3] Postgres streaming replication.
http://wiki.postgresql.org/wiki/Streaming_Replication,
2012.
[4] Xignite. http://www.xignite.com/, 2012.
[5] D. Agrawal, A. E. Abbadi, A. Singh, and T. Yurek. Efficient view maintenance at
data warehouses. In SIGMOD, 1997.
[6] S. Agrawal, S. Chaudhuri, and V. R. Narasayya. Automated selection of
materialized views and indexes in SQL databases. In VLDB, 2000.
[7] S. Al-Kiswany, H. Hacigümüs, Z. Liu, and J. Sankaranarayanan. Cost
exploration of data sharings in the cloud. In EDBT, 2013.
[8] S. Aulbach, M. Seibold, D. Jacobs, and A. Kemper. Extensibility and data
sharing in evolving multi-tenant databases. In ICDE, 2011.
[9] M. Balazinska, B. Howe, and D. Suciu. Data markets in the cloud: An
opportunity for the database community. PVLDB, 4(12):1482–1485, 2011.
[10] J. A. Blakeley, P.-A. Larson, and F. W. Tompa. Efficiently updating materialized
views. In SIGMOD, 1986.
[11] S. Chaudhuri. An overview of query optimization in relational systems. In
PODS, 1998.
[12] D. Costolo. The power of Twitter as a communication tool. http://www.
fordschool.umich.edu/video/newest/1975704207001/, 2012.
[13] V. Kantere, D. Dash, G. Gratsias, and A. Ailamaki. Predicting cost amortization
for query services. In SIGMOD, 2011.
[14] D. Kossmann, M. J. Franklin, and G. Drasch. Cache investment: integrating
query optimization and distributed data placement. TODS, 25:517–558, 2000.
[15] S. Krishnamurthy, M. J. Franklin, J. M. Hellerstein, and G. Jacobson. The case
for precision sharing. In VLDB, 2004.
[16] A. Labrinidis and N. Roussopoulos. Reduction of materialized view staleness
using online updates. CS-TR-3878, UMD CS, 1998.
[17] L. Lamport. Time, clocks, and the ordering of events in a distributed system.
CACM, 21(7):558–565, 1978.
[18] G. M. Lohman, C. Mohan, L. M. Haas, D. Daniels, B. G. Lindsay, P. G. Selinger,
and P. F. Wilms. Query processing in R∗ . In Query Processing in Database
Systems, pages 31–47. Springer, 1985.
[19] C. Mascolo, L. Capra, S. Zachariadis, and W. Emmerich. XMIDDLE: A
data-sharing middleware for mobile computing. Wireless Personal
Communications, 21(1):77–103, 2002.
[20] H. Mistry, P. Roy, S. Sudarshan, and K. Ramamritham. Materialized view
selection and maintenance using multi-query optimization. In SIGMOD, 2001.
[21] T.-V.-A. Nguyen, S. Bimonte, L. d’Orazio, and J. Darmont. Cost models for view
materialization in the cloud. In EDBT, 2012.
[22] T. Plagemann, J. Andersson, O. Drugan, V. Goebel, C. Griwodz, P. Halvorsen,
E. Munthe-Kaas, M. Puzar, N. Sanderson, and K. Skjelsvik. Middleware services
for information sharing in mobile ad-hoc networks. Broadband Sat. Comm. Sys.
and Challenges of Mob., 169:225–236, 2005.
[23] K. A. Ross, D. Srivastava, and S. Sudarshan. Materialized view maintenance and
integrity constraint checking: Trading space for time. In SIGMOD, 1996.
[24] K. Salem, K. Beyer, B. Lindsay, and R. Cochrane. How to roll a join:
Asynchronous incremental view maintenance. In SIGMOD, 2000.
[25] J. Sankaranarayanan, H. Hacigumus, and J. Tatemura. COSMOS: A platform for
seamless mobile services in the cloud. In MDM, 2011.
[26] T. Sellis. Multiple-query optimization. TODS, 13(1):23–52, 1988.
[27] M. M. Wang, J. N. Cao, J. Li, and S. K. Dasi. Middleware for wireless sensor
networks: A survey. JCST, 23(3):305–326, 2008.
[28] Y. Zhuge, H. García-Molina, J. Hammer, and J. Widom. View maintenance in a
warehousing environment. In SIGMOD, 1995.
[29] Y. Zhuge, H. García-Molina, and J. Wiener. The strobe algorithms for
multi-source warehouse consistency. In PDIS, 1997.

Robustness of Sharing Executor
70

CONCLUDING REMARKS

In this paper we presented a platform that can maintain sharings at
the appropriate level of staleness. Experimental results showed the
effectiveness of our platform in maintaining several sharings with
low violations even under a high update rate. We will examine the
following possible extensions in a future work. The platform can
be extended to support aggregate operators by developing additional
operators. Next, easy addition or removal of sharings on the fly as
the system is running can be provided. Finally, before markets for
sharing data be envisioned, issues related to the pricing of data [9]
have to be addressed.

DPT+HC

260

DPD+HC

Number of Vertices/Edges

280

300

Figure 14: Staleness of S4 across snapshots as the rate and the
workload on the MV abruptly change
This experiment shows the robustness of the sharing executor as
the machine capacities change during run-time. The setup consists
of 4 machines hosting the sharings S1 ...S4 . Figure 14 shows the
staleness of S4 with an SLA of 50 seconds (marked) as recorded
by the snapshot module. The SLA of the remaining 3 sharings was
between 20 and 70 seconds. We applied a read workload on each
of the four MVs corresponding to the four sharings by the way of a
simulated user generating a single query template in a closed loop.
Initially, the incoming rate is set at 50 tweets/second and each of the
MVs is subjected to two users.
As the system is running we abruptly vary the number of users on
each MV as well as the rate of incoming tweets. The number of users
per MV is varied in four phases from 8 to 50, while simultaneously
changing the rate of incoming tweets from 50 to 150 tweets/second.
After the first phase when the number of users was increased from 8
to 16 users, all the machines are heavily loaded. The average staleness value for each phase is also marked in Figure 14.
As the number of users and the rate of incoming tweets increase,
the machines get progressively more loaded. However, it can be seen
from the boundaries of the phase changes in the figure, the sharing
executor quickly adapts to the changing data rate and the increased
workload on the databases. The model in spite of the infrastructure
being loaded never allows the staleness of the sharing to exceed beyond 40 seconds. The sharing executor is able to do this by taking
advantage of the slack between the critical time path of the sharing
plan, which is a few seconds and that of the staleness SLA, which
is 50 seconds. Even if the time taken to push the sharing becomes
progressively slower due to the system load, the executor is able to
schedule the updates in a way that the SLAs are not violated.

699

arXiv:1604.03234v1 [cs.DB] 12 Apr 2016

Hippo: A Fast, yet Scalable, Database Indexing Approach
Jia Yu

Mohamed Sarwat

Arizona State University
699 S. Mill Avenue, Tempe, AZ

Arizona State University
699 S. Mill Avenue, Tempe, AZ

jiayu2@asu.edu

msarwat@asu.edu
TPC-H
2 GB
20 GB
200 GB

ABSTRACT
+

Even though existing database indexes (e.g., B -Tree) speed
up the query execution, they suffer from two main drawbacks: (1) A database index usually yields 5% to 15% additional storage overhead which results in non-ignorable dollar cost in big data scenarios especially when deployed on
modern storage devices like Solid State Disk (SSD) or NonVolatile Memory (NVM). (2) Maintaining a database index incurs high latency because the DBMS has to find and
update those index pages affected by the underlying table
changes. This paper proposes Hippo a fast, yet scalable,
database indexing approach. Hippo only stores the pointers
of disk pages along with light weight histogram-based summaries. The proposed structure significantly shrinks index
storage and maintenance overhead without compromising
much on query execution performance. Experiments, based
on real Hippo implementation inside PostgreSQL 9.5, using the TPC-H benchmark show that Hippo achieves up to
two orders of magnitude less storage space and up to three
orders of magnitude less maintenance overhead than traditional database indexes, i.e., B+ -Tree. Furthermore, the experiments also show that Hippo achieves comparable query
execution performance to that of the B+ -Tree for various
selectivity factors.

1.

Index size
0.25 GB
2.51 GB
25 GB

HDD
0.04 $/GB

Initialization time
30 sec
500 sec
8000 sec

(a) B+ -Tree overhead
E-HDD
SSD
0.1 $/GB 0.5 $/GB

Insertion time
10 sec
1180 sec
42000 sec
E-SSD
1.4 $/GB

(b) Storage dollar cost

Table 1: Index overhead and storage dollar cost

storage overhead. Even though the storage overhead
may not seem too high in small databases, it results in non-ignorable dollar cost in big data scenarios. Table 1a depicts the storage overhead of a B+ Tree created on the Lineitem table from the TPCH [5] benchmark (database size varies from 2, 20 and
200 GB). Moreover, the storage dollar cost is dramatically amplified when the DBMS is deployed on modern storage devices (e.g., Solid State Drives and NonVolatile Memory) because they are still more than an
order of magnitude expensive than Hard Disk Drives
(HDDs) per unit of storage. Table 1b lists the dollar
cost per storage unit collected from Amazon.com and
NewEgg.com (Enterprise is abbreviated to E). In addition, initializing an index may be a time consuming
process especially when the index is created on a large
database table. Such high initialization overhead may
delay the analysis process (see Table 1a).

INTRODUCTION

A database system (DBMS) often employs an index structure, e.g., B+ -Tree [4], to speed up query execution at the
cost of additional storage and maintenance overhead. A
DBMS user may create an index on one or more attributes
of a database table. A created index allows the DBMS to
quickly locate tuples without having to scan the whole indexed table. Even though existing database indexes significantly improve the query response time, they suffer from
the following drawbacks:

• Maintenance Overhead: A DBMS must update the
index after inserting (deleting) tuples into (from) the
underlying table. Maintaining a database index incurs high latency because the DBMS has to find and
update those index entries affected by the underlying
table changes. For instance, maintaining a B+ -Tree
searches the tree structure and perhaps performs a set
of tree nodes splitting or merging operations. That
requires plenty of disk I/O operations and hence encumbers the time performance of the entire DBMS in
big data scenarios. Table 1a shows the B+ Tree insertion overhead (insert 0.1% records) for the TPC-H
Lineitem table.

• Indexing Overhead: Indexing overhead consists of
two parts - storage and initialization time overhead.
A database index usually yields 5% to 15% additional

Existing approaches that tackle one or more of the aforementioned drawbacks are classified as follows: (1) Compressed indexes: Compressed B+ -Tree approaches [7, 8, 19]
1

reduce index storage overhead but all these methods compromise on query performance due to the additional compression and decompression time. Compressed bitmap indexes also reduce index storage overhead [9, 11, 14] but they
mainly suit low cardinality attributes which are quite rare.
For high cardinality attributes, the storage overhead of compressed bitmap indexes significantly increases [17]. (2) Approximate indexes: Approximate indexing approaches [2, 10,
12] trade query accuracy for storage to produce smaller, yet
fast, index structures. Even though approximate indexes
may shrink the storage size, users cannot rely on their unguaranteed query accuracy in many accuracy-sensitive application scenarios like banking systems or user archive systems. (3) Sparse indexes: A sparse index [3, 13, 15, 16] only
stores pointers which refer to disk pages and value ranges
(min and max values) in each page so that it can save indexing and maintenance overhead. It is generally built on
ordered attributes. For a posed query, it finds value ranges
which cover or overlap the query predicate and then rapidly
inspects the associated few parent table pages one by one
for retrieving truly qualified tuples. However, for unordered
attributes which are much more common, sparse indexes
compromise too much on query performance because they
find numerous qualified value ranges and have to inspect a
large number of pages.
This paper proposes Hippo1 a fast, yet scalable, sparse
database indexing approach. In contrast to existing tree index structures, Hippo stores disk page ranges (each works
as a pointer of one or many pages) instead of tuple pointers
in the indexed table to reduce the storage space occupied by
the index. Unlike existing approximate indexing methods,
Hippo guarantees the query result accuracy by inspecting
possible qualified pages and only emitting those tuples that
satisfy the query predicate. As opposed to existing sparse
indexes, Hippo maintains simplified histograms that represent the data distribution for pages no matter how skew it is,
as the summaries for these pages in each page range. Since
Hippo relies on histograms already created and maintained
by almost every existing DBMS (e.g., PostgreSQL), the system does not exhibit a major additional overhead to create
the index. Hippo also adopts a page grouping technique
that groups contiguous pages into page ranges based on the
similarity of their index key attribute distributions. When a
query is issued on the indexed database table, Hippo leverages the page ranges and page summaries to recognize those
pages for which the internal tuples are guaranteed not to satisfy the query predicates and inspects the remaining pages.
Thus Hippo achieves competitive performance on common
range queries without compromising the accuracy. For data
insertion and deletion, Hippo dispenses with the numerous
disk operations by rapidly locating the affected index entries. Hippo also adaptively decides whether to adopt an
eager or lazy index maintenance strategy to mitigate the
maintenance overhead while ensuring future queries are answered correctly.
We implemented a prototype of Hippo inside PostgreSQL
9.5. Experiments based on the TPC-H benchmark show
that Hippo achieves up to two orders of magnitude less storage space and up to three orders of magnitude less maintenance overhead than traditional database indexes, i.e.,
B+ -Tree. Furthermore, the experiments show that Hippo
1

Create an index

Execute a query
User

Age table
Disk Page #

Query predicate

Internal data

1,2,3,4,5,… 21,22,55,75,77,…

Compare

Bucket
2,3,4,…

(Return)

Bucket
1
2
3
4
5

Age
1 - 20
21 - 40
41 - 60
61 - 90
91 - 120

Age = 55
Compare

(Return)

Bucket
3

Complete height balanced histogram
Filter false positives

Generate partial histograms

Hippo
Bucket
2
3
4

Age
21 - 40
41 - 60
61 - 90

Partial histogram 1
Bucket
2
4
5

Age
21 - 40
61 - 90
91 - 120

Partial histogram 2

Page range Partial histogram Internal data
(Return)
1 - 10
2,3,4
21,22,55,75,77
11 – 25
2,4,5
23,24,62,91,92
26 - 30
1,2,5
11,12,25,101,110
Index Entry 1

…

Bucket
1
2
5

Age
1 - 20
21 - 40
91 - 120

Inspect
Page
1 - 10

Partial histogram 3
Legend: Index Initialization

Database
Index Search

Figure 1: Initialize and search Hippo on age table

achieves comparable query execution performance to that
of the B+ -Tree for various selectivity factors.
The remainder of the paper is structured as follows: In
Section 2, we explain the idea of Hippo and show its structure. We demonstrate how to query Hippo swiftly, build
Hippo from scratch, and maintain Hippo efficiently in Section 3, 4 and 5. In Section 6, we provide useful cost estimation for these three scenarios. Extensive experiments
and related analysis are included in Section 7. We discuss
related work then analyze the drawbacks in existing indexes
in Section 8. Finally, Section 9 concludes the paper.

2. HIPPO OVERVIEW
This section gives an overview of Hippo. A running example that describes a Hippo index built on an age table is
given in Figure 1. The figure’s right part which depicts how
to search Hippo and the left part which shows how to initialize Hippo are explained in Section 3 and 4 respectively.
The main challenges of designing an index are to reduce
the indexing overhead in terms of storage and initialization
time as well as speed up the index maintenance while still
keeping competitive query performance. To achieve that,
an index should possess the following two main properties:
(1) Less Index Entries: For better storage space utilization,
an index should determine and only store the most representative index entries that summarize the key attribute.
Keeping too many index entries inevitably results in high
storage overhead as well as high initialization time. (2) Index Entries Independence: Index entries of a created index
should be independent from each other. In other words, the
range of values that each index entry represents should have
minimal overlap with other index entries. Interdependence
among index entries, like that in a B+ -Tree, may lead to
overlapped tree nodes traverse during query processing and
several cascaded updates during index maintenance.

https://github.com/DataSystemsLab/hippo-postgresql

2

Age = 55

Data Structure. When creating an index, Hippo scans
the indexed table and generates histogram-based summaries
for disk pages based upon the index key attribute. Afterwards, these summaries are stored by Hippo along with
pointers of the pages they summarize. As shown in Figure 1, a Hippo index entry consists of the following two
components (Internal data of pages is given in the figure
only for the ease of understanding):

Bucket
1
2
3
4
5

Age > 55
Bucket
1
2
3
4
5

Age > 55 AND Age < 65

Age
1 - 20
21 - 40
41 - 60
61 - 90
91 - 120

Bucket
1
2
3
4
5

Age
1 - 20
21 - 40
41 - 60
61 - 90
91 - 120

Figure 2: Convert query predicates

• Summarized Page Range: The page range (works
as a pointer) represents the IDs (i.e., address) of the
first and last pages summarized by a certain histogram
based summary. DBMS can load particular pages into
buffer according to their customized IDs. Hippo is able
to summarize more than one contiguous (in terms of
physical storage) pages to reduce the overall index size
to a great extent (e.g., Page 1 - 10, 11 - 25, 26 - 30).
The number of summarized pages (denoted as pages
per partial histogram) in each index entry varies. For
a certain index attribute, some contiguous pages have
very similar content but some are not. Hence, Hippo
adopts a page grouping technique that groups contiguous pages into page ranges based on the similarity of
their index attribute distributions, using the partial
histogram density (explained in Section 4).

predicate. The search process leverages the index structure
to avoid worthless page inspection so that Hippo can achieve
competitive query performance.
Algorithm 1: Hippo index search
Data: A given query predicate and Hippo index
Result: Qualified tuples
1 Create Bitmap a for the given predicate;
2 foreach bucket of the complete histogram do
3
if it is hit by the query predicate then
4
Set the corresponding bit in Bitmap a to 1;
5
end
6 end
7 Create Bitmap b for recording all pages;
8 foreach partial histogram do
9
if it has joint buckets with Bitmap a then
10
Set the corresponding bits of the summarized
pages in Bitmap b to 1;
11
end
12 end
13 foreach page marked as 1 in Bitmap b do
14
Check each tuple in it against the predicate;
15 end

• Histogram-based Page Summary: The page summary in each index entry is a partial histogram
that represents a subset of the complete height balanced histogram buckets (maintained by the underlying DBMS). Each bucket if exists indicates that at
least one of the tuples of this bucket exists in the summarized pages. Each partial histogram represents the
distribution of the data in the summarized contiguous
pages. Since each bucket of a height balanced histogram roughly contains the same number of tuples,
each of them has the same probability to be hit by a
random tuple from the table. Hippo leverages this feature to handle data which has various or even skewed
distribution. To save storage space, only bucket IDs
are kept in partial histograms and partial histograms
are stored in a compressed bitmap format. For instance, the partial histogram of the first Hippo index
entry in Figure 1 is stored as 01110. Each bit, set to 1
or 0, reflects whether the corresponding bucket exists
or not.

3.1 Convert query predicates
The main idea is to check each partial histogram against
the given query predicate for filtering false positives and
so speeding up the query. However, as explained in Section 2, partial histograms are stored in bitmap formats without recording value ranges of buckets. Therefore, there has
to be an additional step to recover the missing information
for each partial histogram on-the-fly or convert the predicate to the bitmap format per query. Obviously, the later
one is more efficient.
Any query predicates for a particular attribute can be
broken down into atomic units: equality query predicate
and range query predicate. Age = 55 is a typical equality
query predicate while age > 55 is a range query predicate.
These unit predicates can be combined together by AND
operator like age > 55 AND age <65.
Each unit predicate is compared with the buckets of the
complete height balanced histogram (retrieving method is
discussed in Section 4). A bucket is hit by a predicate if
the predicate fully contains, overlaps, or is fully contained
by the bucket. Each unit predicate can hit one, at least,
or more buckets. For instance, according to the complete
histogram in Figure 1, bucket 3 whose description is 41 - 60
is hit by age = 55 while bucket 3, 4, and 5 are hit by age
> 55. This strategy is also applicable for the conjunct query
predicates. For a conjunct predicate like age > 55 and age
< 65, only buckets which are hit by all these unit predicates

Main idea. Hippo solves the aforementioned challenges as follows: (1) Each index entry summarizes many
pages and each only stores two page IDs and a compressed
bitmap.(2) Each page of the parent table is only summarized by one Hippo index entry. Hence, any updates that
occur in a certain page only affect a single independent index entry. Finally, during a query, pages whose partial histograms do not have desired buckets are guaranteed not to
satisfy certain query predicates and marked as false positives. Thus Hippo only inspects other pages that probably
satisfies the query predicate and achieves competitive query
performance.

3.

Age
1 - 20
21 - 40
41 - 60
61 - 90
91 - 120

INDEX SEARCH

The search algorithm runs in three main steps: (1) Step 1:
convert query predicates, (2) Step 2: filter false positives and
(3) Step 3: inspect possible qualified pages against the query
3

Age = 55

Partial histogram 1

(bitmap)

(bitmap)

Bucket
0
0
1
0
0

AND
0
0
1
0
0

IDs of possible qualified pages are recorded in a separate
bitmap. Each bit in this bitmap is mapped to the page at
the same position in the parent table. For instance, the bit
at position 1 in the bitmap is mapped to the page ID 1 of the
parent table. The value (1 or 0) of this bit reflects whether
the associate page is a possible qualified page or not.
Hippo has to inspect all of the possible qualified pages
recorded in the bitmap against the query predicate one by
one because every retained page from the previous step is
possible to contain qualified tuples. The only way to inspect
these possible qualified pages is to traverse them and check
each tuple in each page one by one. Qualified tuples are
returned to the DBMSs.
Algorithm 1 shows the three steps of Hippo index search.
The right part of Figure 1 describes how to search Hippo
index using a certain query predicate. Firstly, Hippo finds
query predicate age = 55 hits bucket 3. And the first one of
the three partial histograms nicely contains bucket 3. Thus
only the disk pages 1 - 10 are selected as possible qualified
pages which need further inspection. It is also worth noting
that these partial histograms summarize different number of
pages.

Bucket
0
1
1
1
0

Bitwise AND = 1
Figure 3: Bitwise AND two bitmaps to find joint buckets

simultaneously (the joint bucket 3 and 4) are kept as the final
result and others are directly discarded. Figure 2 shows the
hit buckets of three query predicates. Afterwards, the given
query predicate is converted to a bitmap. Each bit in this
bitmap reflects whether the bucket has the corresponding
ID is hit (1) or not (0). Thus the corresponding bits of all
hit buckets are set to 1.

3.2 Filter false positives
4. INDEX INITIALIZATION

Filtering false positives is the most important step of
Hippo index search. Each Hippo index entry stores a page
range and a summary of several contiguous pages but it is
very possible that none of these pages in the certain index
entry contain the qualified tuples especially for small range
queries. This kind of pages and their associated index entries are false positives. This step is to check each partial
histogram against the converted query predicate, recognize
some false positive pages utmost and finally avoid worthless
page inspection on these pages.
A given query predicate hits one ,at least, or more buckets
of the complete histogram. Pages whose partial histograms
contain the hit buckets (the corresponding bitmap bits are
1) might have qualified tuples, whereas pages whose partial histograms don’t contain these buckets (the corresponding bitmap bits are 0) are guaranteed not to contain qualified tuples. The former kind of pages are possible qualified
pages. In contrast, the later kind of pages are false positives
and excluded from the next step - inspect possible qualified
pages. The straight way to find false positive pages is to
do a nested loop between each partial histogram and the
converted query predicate to find the joint buckets.
Interestingly, because both of partial histograms and the
converted query predicate are in bitmap format, the nested
loop can be accelerated by bitwise ’AND’ing the bytes from
both sides, aka bit-level parallelism. If bitwise ’AND’ing
the two bytes from both sides returns 1, that means there
are joint buckets between the query predicate and the partial histogram. Thus the pages are possible qualified pages.
Figure 3 provides an example of how to perform a bitwise
AND using the same data in Figure 1.

Hippo performs three main steps to initialize itself:
(1) Retrieve a complete histogram, (2) Generate partial histograms, and (3) Group similar pages into page ranges, described as follows.
Algorithm 2: Hippo index initialization
Data: Pages of a parent table
Result: Hippo index
1 Create a working partial histogram (in bitmap format);
2 Set StartPage = 1 and EndPage = 1;
3 foreach page do
4
Find distinct buckets hit by its tuples;
5
Set associated bits to 1 in the partial histogram;
6
if the working partial histogram density > threshold
then
7
Store the partial histogram and the page range
(StartPage and EndPage) as an index entry;
8
Create a new working partial histogram;
9
StartPage = EndPage + 1;
10
EndPage = StartPage;
11
else
12
EndPage = EndPage + 1;
13
end
14 end

4.1 Retrieve a complete histogram
Histograms used in Hippo include a complete height balanced histogram and many partial histograms. A complete
height balanced histogram represents the distribution of all
tuples and already exists in DBMSs. Respectively, a partial
histogram, as a subsection, only contains partial buckets
from the complete histogram. Therefore, for generating any
partial histograms, a complete histogram should be retrieved
at the first priority. Full-fledged functions for retrieving a
complete histogram exist in any DBMSs. Detailed explanation for these functions is omitted in this paper since it is

3.3 Inspect possible qualified pages
The previous step recognizes many false positive pages
and excludes them from possible qualified pages. However,
one fact is that not all false positives can be detected by
the previous step. Possible qualified pages still may contain
false positives and this is why they are called ”possible”.
This step is to inspect the tuples in each possible qualified
pages and retrieve the qualified tuples directly.
4

histograms into one larger partial histogram (in other words,
summarizing more pages within one partial histogram) can
make Hippo more efficient. On the other hand, users may
want to shrink Hippo physically to a greater extent. For
example, if a partial histogram can summarize 10 pages in
one go, the new Hippo size will be much smaller. Grouping
more pages into one page range and summarizing them with
just one partial histogram are expected and practical as well.
Yet, this is not saying that all pages should be grouped
together and summarized by one merged partial histogram.
As more and more pages are summarized, this partial histogram contains more and more buckets until all buckets
from the complete histogram are included. At this moment,
this partial histogram becomes a complete histogram and
covers any possible query predicates. That means this kind
of partial histograms is unable to help Hippo to filter the
false positives and the disk pages summarized by this partial histogram will be always treated as possible qualified
pages.
One strategy is to group a fixed number of contiguous
pages per range/partial histogram. Yet, this strategy is
not suitable if some contiguous pages in a certain area have
much more similar data distribution than other areas. Lacking the awareness of data distribution cannot reduce storage overhead smartly. Under this circumstance, it is better
to let Hippo group more pages together in this area and
group less pages together in other areas dynamically. For
instance, assume original pages per partial histogram is 100.
If there are 1000 out of 10000 disk pages and the tuples in
these 1000 pages are exactly same, a better way to shrink
the index size is to set the P from 100 to 1000 for grouping/summarizing these 1000 pages into one range/partial
histogram and change it back to 100 for other 9000 pages.
A terminology - partial histogram density is introduced
here. The density of a partial histogram is the percentage
of kept buckets in the total buckets of a complete histogram.
The complete histogram has a density value of 1. The definition can be formalized as follows:

not our focus. We also assume that the complete histogram
is not changed at any time because the global distribution
of the parent table will not be affected even if some local
updates are performed.
The resolution of the complete histogram (denoted as histogram resolution) is adjustable. A complete histogram is
considered as higher resolution if it contains more buckets.
The resolution of partial histograms is consistent with their
complete histograms technically. It is apparent that a complete histogram will have larger physical size if it has higher
resolution and, accordingly, the numerous partial histograms
are also physically larger than the low resolution ones. On
the other hand, the histogram resolution also affects Hippo
query time. The cost estimation section will further discuss
this issue.

4.2 Generate partial histograms
A partial histogram only contains some buckets from the
complete histogram. It is used to represent the distribution
of parent tuples in one or many disk pages. In other words,
people can get an approximate overview from the partial
histogram of these pages: What values might lie in these
pages and what do not. These partial histograms are able
to help Hippo to recognize false positives utmost and avoid
worthless page inspection. We explain how to generate a
partial histogram for each disk page in this section.
Generating partial histograms traverses all disk pages of
the parent table from the start to end. For each page, a
nested loop passes through each tuple in this page. The
specified attribute value is extracted from each tuple and
compared with the complete histogram (using a binary
search). Buckets hit by tuples are kept for this page and
then compose a partial histogram. A partial histogram only
contains distinct buckets. For instance, there is a group of
age values like the first entry of Hippo shown in Figure 1:
21, 22, 55, 75, 77. Bucket 2 is hit by 21 and 22, bucket 3 is
hit by 55 and bucket 4 is hit by 77. Therefore, the partial
histogram for these values is just as partial histogram 1 in
Figure 1.
Shrinking the physical size of partial histograms is desirable. The basic idea is to drop all bucket value ranges and
only keep bucket IDs. Hippo in Figure 1 shows the effect.
Actually, as mentioned in Section 2, dropping value range
information does not impact much on the index search. To
further shrink the size, storing bucket IDs in integer type (4
bytes or more) is also considered as an overhead. Bitmap format storage is a better choice to bypass this overhead. Each
partial histogram is stored as a bitmap. Each bit in a bitmap
stands for a bucket at the same position in a complete histogram. Bit value 1 means the associated bucket is hit and
kept in this partial histogram while 0 means the associated
bucket is not included. Bitmap compression is introduced to
Hippo as well. The partial histogram in a bitmap format can
be compressed by any existing compression techniques. The
time of compressing and decompressing partial histograms is
ignorable in contrast to that of inspecting possible qualified
pages.

P artial histogram density =

Bucketspartial histogram
Bucketscomplete histogram

This density has an important phenomenon that, for a group
of contiguous pages, their merged partial histogram density
will be very low if these pages are very similar, vice versa.
Therefore, a partial histogram with a certain density may
summarize more pages if these contiguous pages have similar data, vice versa. Making use of this phenomenon enables Hippo to dynamically group pages and merge partial
histograms into one. In addition, it is understandable that
a lower density partial histogram (summarizes less pages)
has the high probability to be recognized as false positives
so that speed up queries.
User can easily set a same density for all partial histograms as a threshold. Each partial histogram can automatically decide how many pages it should summarize.
Algorithm 2 depicts how to initialize a Hippo and summarize more pages within one partial histogram with the help
of the partial histogram density. The basic idea is that new
pages will not be summarized into a partial histogram if
its density is larger than the threshold and a new partial
histogram will be created for the following pages.
Figure 1’s left part depicts how to initialize a Hippo on
the age table with a partial histogram density 0.6. All of

4.3 Group similar pages into page ranges
Generating a partial histogram per one disk page is as easy
as that in Section 4.2. However, for some contiguous pages
which have similar data, it is a waste of storage. Grouping
them together as many as possible and merging their partial
5

Algorithm 3: Update Hippo for data insertion
Data: A new inserted tuple belongs to Page a
Result: Updated Hippo
1 Find the bucket hit by the inserted tuple;
2 Locate a Hippo index entry which summarizes Page a;
3 if one index entry is located then
4
Retrieve the associated Hippo index entry;
5
Update the retrieved entry if necessary;
6 else
7
Retrieve the Hippo entry summarizes the last page;
8
if the partial histogram density < threshold then
9
Summarize Page a into the retrieved entry;
10
else
11
Summarize Page a into a new entry;
12
end
13 end

Updated Hippo
S	
 t
e #
L

H



Page range Partial histogram Internal data
1 - 10
2,3,4
21,22,55,75,77
Blank space
2 - 3
12
111221111
11  2

124

Mve

132324 29192

Figure 4: Hippo Index Entries Sorted List

time-consuming if every entry is retrieved from disk, deserialized and checked against the target page. Therefore,
a binary search on Hippo index entries is a good choice.
(This search actually leverages the index entries sorted list
explained in Section 5.3.)
Step 3: Update the index entry: If the new tuple
belongs to a new page not summarized by any Hippo index
entries and the density of Hippo partial histogram which
summarizes the last disk page is smaller than the density
threshold set by users, this new page will be summarized
into this partial histogram in the last index entry otherwise
a new partial histogram will be created to summarize this
page and stored in a new Hippo index entry. For a new tuple
belongs to pages already summarized by Hippo, the partial
histogram in the associated index entry will be updated if
the inserted tuple hits a new bucket.
It is worth noting that: (1) Since the compressed bitmaps
of partial histograms may have different size, the updated
index entry may not fit the space left at the old location.
Thus the updated one may be put at the end of Hippo.
(2) After some changes (replacing old or creating new index
entry) in Hippo, the corresponding position of the sorted list
needs to be updated.

the tuples are compared with the complete histogram and
IDs of distinct buckets hit by tuples are generated as partial
histograms along with page range.
So far, as Figure 1 shows, each entry in a Hippo index has the following content: a partial histogram in compressed bitmap format and two integers stand for the first
and last pages summarized by this histogram (summarized
page range). Each entry is serialized and stored on disk.

5.

inte

INDEX MAINTENANCE

Inserting (deleting) tuples into (from) the indexed table
requires maintaining the index to ensure that the DBMS
can retrieve the correct set of tuples that match the query
predicate. However, the overhead of maintaining the index
quite frequently may preclude system scalability. This section explains how Hippo handles updates.

5.2 Data deletion

5.1 Data insertion

The eager update strategy is not highly desired for data
deletion. Hippo still ensures the correctness of queries even
if it doesn’t update itself at all after deleting tuples from a
table. This benefits by inspecting possible qualified pages
in index search. Pages used to have qualified tuples might
be still marked as possible qualified pages but they are discarded after being inspected against the query predicates. A
periodic update or bulk update will be a good choice here.
For data deletion, Hippo adopts a lazy update strategy
that maintains the index after a bulk of delete operations.
In such case, Hippo traverses each index entry from the start
to end. For each index entry, Hippo inspects the header of
each summarized page for seeking notes made by DBMSs
(e.g., PostgreSQL makes notes in page headers if data is
removed from pages). Hippo re-summarizes the entire index
entry instantly within the original page range if data deletion
on one page is detected. The re-summarization follows the
same steps in Section 4. It is worth noting that this updated
Hippo index entry is not leading to the update on the sorted
list because the updated partial histogram, having same or
less buckets, can obtain same or less compress bitmap size
and the new index entry certainly fits the old space.

Hippo should instantly update or check the index at least
after inserting one record into the indexed table. Otherwise, all subsequent queries might miss the newly inserted
tuple since it is not reflected by the index. Therefore, Hippo
adopts an eager update strategy when a new tuple is inserted. Data insertion may change the physical structure
of a table. The new tuple may belong to any pages of the
indexed table. The insertion procedure (See Algorithm 3)
performs the following steps: (1) Find buckets hit by the
new tuple, (2) Locate the affected index entry, and (3) Update the index entry if necessary.
Step 1: Find buckets hit by the new tuple: Similar
with some steps of generating partial histogram in index
initialization, after retrieving the complete histogram, the
newly inserted tuple is checked against it using a binary
search and a bucket hit by this new tuple is found.
Step 2: Locate the affected index entry: The new
tuple has to belong to one page in this table. This page may
be a new one which has not been summarized by any partial
histograms before or an old one which has been summarized.
However, because the numbers of pages summarized by each
histogram are different, searching Hippo index entry to find
the one contains this target page is inevitable. From the
perspective of disk storage, in a Hippo, all partial histograms
are stored on disk in a serialized format. It will be extremely

5.3 Index Entries Sorted List
When a new tuple is inserted, Hippo executes a fast binary
search (according to the page IDs) to locate the affected
6

Term
H
D
P
T
Card
pageCard
SF

Qy  ate 1

Definition
Complete histogram resolution which means the
number of buckets in this complete histogram
Partial histogram density which is an user supplied threshold
Pages summarized by one partial histogram for a
certain attribute
Tuples summarized by one partial histogram for
a certain attribute
Cardinality (the total number of tuples) of the
indexed table
Number of tuples in each page of the indexed table
Query selectivity factor =

Query output
Query input

Miss…
Partial histogram
O
Qy  ate 2

Figure 5: Visualize how to filter false positives

The first step of Hippo index search is to traverse Hippo
index entries. Pages in each index entry are likely to be
selected for further inspection if their associated partial histogram has joint buckets with the query predicate. Determining the probability of having joint buckets contributes
to the query time cost estimation.
For the ease of presentation, Figure 5 visualizes the procedure of filtering false positives according to their partial
histograms. Partial histogram density (D) of this index is
0.2. The complete histogram constitutes of 10 buckets in
total (H = 10). Assume the indexed table’s tuples follow an
uniform distribution based upon the key attribute. Let the
query selectivity factor (SF ) be 20%. In Figure 5, buckets
hit by the query predicates and the partial histogram are
represented in a bitmap format. According to this figure,
the partial histogram misses a query predicate if the highlighted area of the predicate falls into the blank area of the
partial histogram, whereas a partial histogram is selected if
the predicate does not fall completely into the blank area of
the histogram. In other words, the probability of a partial
histogram having joint buckets with a predicate depends on
how likely a predicate doesn’t fall into the blank area of
a partial histogram. The probability is determined by the
formula given below (The terms are defined in Table 2):

* 100%

Table 2: Notations used in Cost Estimation

index entry and then updates it. Since the index entries are
not guaranteed to be sorted based on the page IDs (noted in
data insertion section), an auxiliary structure for recording
the sorted order is introduced to Hippo.
The sorted list is initialized after all steps in Section 4
with the original order of index entries and put at the first
several index pages of Hippo. During the entire Hippo life
time, the sorted list maintains a list of pointers of Hippo
index entries in the ascending order of page IDs. Actually
each pointer represents the fixed size physical address of
an index entry and these addresses can be used to retrieve
index entries directly. That way, the premise of a binary
search has been satisfied. Figure 4 depicts the Hippo index
entries sorted list. Index entry 2 in Figure 1 has a new
bucket ID 1 due to a newly inserted tuple in its internal
data and hence this entry becomes the last index entry in
Figure 4. The sorted list is still able to record the ascending
order and help Hippo to perform a binary search on the
index entries. In addition, such sorted list leads to slight
additional maintenance overhead: Some index updates need
to modify the affected pointers in the sorted list to reflect
the new physical addresses.

6.

P rob = (Buckets hit by a query predicate) ∗ D
= (SF ∗ H) ∗ D

(1)

To be precise, P rob follows a piecewise function as follows:

COST ESTIMATION

P rob =

This section gives a detailed cost estimation of Hippo.
We first provide an accurate query time cost model which
assists the DBMS query optimizer in picking an efficient
query execution plan. Estimating the storage overhead of
an index can also facilitate better disk space management
and planning. Index initialization certainly consumes a large
chunk of time. Similarly, index maintenance can present a
significant time overhead in any write-intensive application.
Both of them should be carefully estimated.
Table 2 summarizes the main notations we use to derive
the cost model. Given a database table R with Card number
of tuples (i.e., cardinality) and average number of tuples per
disk page equal to pageCard, a user may create a Hippo
index on attribute ai of R. When initializing the index,
Hippo sets the complete histogram resolution to H (it has
H buckets in total) and the partial histogram density to D.
Assume that each Hippo index entry summarizes P indexed
table pages (in terms of pages)/ T tuples (in terms of tuples).
P and T vary for each index entry. Queries executed against
the index have average selectivity factor SF .

(

(SF ∗ H) ∗ D
1

1
S∗H 6 D
1
SF ∗ H > D

SF ∗ H ∈ {1, 2, 3, 4, ...}
SF ∗ H should be no smaller than 1 no matter how small
SF is. Because the query predicate at least hits one bucket
of the complete histogram. Therefore, the probability in
Figure 5 is 20% × 10 × 0.2 = 40%. That means pages summarized by each index entry have 40% probability to be selected as possible qualified pages. Given the aforementioned
discussion, we observe the following:
Observation 1: When SF and H are fixed, the smaller
D is, the smaller P rob is.
Observation 2: When H and D are fixed, the smaller
SF is, the smaller P rob is.
Observation 3: When SF and D are fixed, the smaller
H is, the smaller P rob is.
In fact, the probability given above is equal to the percentage of inspected tuples in all tuples. In addition, considering
that Hippo index entries are much less than the inspected
tuples of the parent table, the total query time cost estimation is mainly decided by the time spent on inspecting
possible qualified pages. Thus, the query time cost estimation (in terms of disk I/O) can be concluded as follows:

6.1 Query time
7

Query time = (P rob ∗ Card)

(2)

Hippo index entries =

If we substitute P rob with its piecewise function, the
query time cost is as follows:

=

Card
T
1
H ∗ (H
+

(5)
Card
+ ... +

1
H−1

1
)
H−D∗H+1

(6)
Query time =

(

(SF ∗ H) ∗ D ∗ Card
Card

SF ∗ H 6
SF ∗ H >

1
D
1
D

D ∈ [ pageCard
, 1]
H
Some observations can be obtained from Formula 6:
Observation 1 For a certain H, the higher D there is,
the less Hippo index entries there are.
Observation 2 For a certain D, the higher H there is,
the less Hippo index entries there are. Meanwhile, the size
of each Hippo index entry is increasing with the growth of
the complete histogram resolution.
Index initialization time hinges on the number of disk I/Os
because it takes much more time than memory I/Os. In
general, the initialization time is composed of two parts:
retrieve parent tuples one by one and write index entry to
disk one by one. Accordingly, Hippo initialization time can
be deduced as follows:

SF ∗ H ∈ {1, 2, 3, 4, ...}

6.2 Indexing overhead
Indexing overhead which consists of storage overhead and
initialization time highly hinges on the number of index entries in an index. The more index entries there are, the more
disk writes and storage space an index costs. B+ -Tree and
other indexes take huge disk space and time for storing their
substantial nodes one by one.
The first problem in estimating the number of Hippo index
entries is that: how many disk pages (P ) are summarized by
one partial histogram in general? Or, how many tuples (T )
are checked against the complete histogram for generating
one partial histogram? Interestingly, this problem is very
similar with Coupon Collector’s Problem[6]. This problem
can be described like that: ”A vending machine sells H types
of coupons (a complete histogram with H buckets). Alice
is purchasing coupons from this machine. Each time (each
tuple) she can get a random type coupon (a bucket) but
she might already have a same one. Alice keeps purchasing
until she gets D ∗ H types of coupons (distinct buckets).
How many times (T ) does she need to purchase?”
Therefore, the expectations of T and P are determined by
the following formulas (The terms are defined in Table 2):
H
H
H
H
+
+
+ ... +
H
H −1
H −2
H −D∗H +1
1
1
1
+ ... +
)
=H ∗( +
H
H −1
H −D∗H +1
T
P =
pageCard

Hippo initialization time = Card + Hippo index entries
(7)
The number of Hippo index entries mentioned in the formula
above can be substituted by its mathematical expectation
in Formula 6.

6.3 Maintenance time
Data insertion. Hippo updates itself eagerly for data
insertion so that this operation is relatively time sensitive.
There are five steps cost disk I/Os in this update: retrieve
the complete histogram, locate associated Hippo index entry, retrieve the associated index entry, update the index
entry (if necessary) and update the mapped sorted list element. It is not hard to conclude that locating the associated index entry completes in log(Hippo index entries) I/O
times, whereas other four steps are able to accomplish their
assignments in constant I/O times. Thus the data insertion
time cost estimation model is summarized as follows under
different conditions:

T =

(3)
(4)

, 1]
D ∈ [ pageCard
H
The product of D ∗ H is the actual number of buckets
in each partial histogram. This value should be no smaller
than the tuples per disk page (pageCard) in case that each
tuple in a certain page hit one unique bucket.
For instance, in a Hippo, the complete histogram has 1000
buckets in total and the partial histogram density is 0.1. The
105.3
expectation of T and P will be 105.3 and pageCard
respectively. That means each partial histogram may summarize
105.3
pages under this circumstance. In another exampageCard
ple, if the total number of buckets is 10000 and the density
2230
is 0.2, T and P will be 2230 and pageCard
correspondingly.
After being aware of the expectation of the number of P ,
it is not hard to deduce the approximate number of index
entries in a Hippo. Thus the estimation of Hippo index
entries number is Formula 5. If we substitute T with their
mathematical expectations in Formula 3 and Formula 5 will
be changed to Formula 6. Hippo index size is equal to the
product of the number of index entries and the size of one
entry which roughly depends on each partial histogram size
(in compressed bitmap format).

Data insert time = log(Hippo index entries) + 4

(8)

Hippo index entries mentioned in Formula 8 can be substituted by its mathematical estimation in Formula 6.
Data deletion. Hippo updates itself lazily for data deletion so that it is hard to finalize a general estimation model.
However, it is recommended that do not update Hippo for
data deletion too frequently because Hippo will re-traverse
and re-summarize all disk pages summarized by one Hippo
index entry once it detects that one disk page has data deletion. This algorithm is more suitable for bulk deletion and
lazy update strategy.

7. EXPERIMENTS
This section provides extensive experiments of Hippo
along with reasonable analysis for supporting insights discussed before. For the ease of testing, Hippo has been implemented into PostgreSQL 9.5 kernel. All the experiments
are completed on PostgreSQL.
8

2
1
0

25x

25x

9

B+-Tree

Hippo

2.8x

6
3
2x

1.5x

0
2
20
200
TPC-H workload size (GB)

x 10000000

25x

Insertion time (ms)

Hippo

x 1000000

B+-Tree

Initialization time (ms)

x 10000

Index Size (MB)

3

5

Hippo

1200x

2
1
0
2
20
200
TPC-H workload size (GB)

2
20
200
TPC-H workload size (GB)

(a) Index size

B+-Tree

(c) Index insertion time

(b) Index initialization time

2
1
0

5

B+-Tree

Hippo

4
3
2
1
0

x 1000000

Hippo

Query time (ms)

B+-Tree

x 100000

5

Query time (ms)

x 10000

Query time (ms)

Figure 6: Index overhead on different TPC-H workload size
5

B+-Tree

Hippo

4
3
2
1
0

0.001%
Query selectivity (%)

0.01%
0.1%
1%
Query selectivity (%)

(a) 2 GB

(b) 20 GB

0.001%

0.01%
0.1%
1%
Query selectivity (%)

(c) 200 GB

Figure 7: Index query time on different TPC-H workload size
It is also worth noting that the final Hippo implementation in PostgreSQL has some slight differences from the
details above caused by some platform-dependent features
as follows:
Automatically inspect pages: Hippo only records possible qualified page IDs in a tid bitmap format and returns it
to the kernel. PostgreSQL will automatically inspect pages
and check tuples against query predicates.
Store the complete histogram on disk: Compared
with other disk operations, retrieving the complete histogram from PostgreSQL system cache is relatively slow so
that Hippo stores it on disk and executes a binary search on
it when query or update for data insertion and deletion. It
is better to rebuild Hippo index if there is a huge change of
the parent attribute’s histogram.
Vacuum tables to physically delete data: PostgreSQL DELETE command does not really remove data
from disk unless a VACUUM command is called automatically or manually. Thus Hippo will update itself for data
deletion when a VACUUM command is called.

Datasets and Workload. We use TPC-H workload in
the experiments with different scale factors (2, 20, 200). The
corresponding dataset sizes are 2 GB, 20 GB and 200 GB.
All TPC-H data follows an uniform distribution. We use
the largest table of TPC-H workload - Lineitem table in
most experiments and it has three corresponding sizes: 1.3
GB, 13.8 GB and 142 GB. We compare the query time of
Hippo with B+ -Tree through different query selectivity factors (0.001%, 0.01%, 0.1% and 1%). In addition, we also
test the two indexes using TPC-H standard queries 6, 15
and 20. We use TPC-H standard refresh operation (insert
0.1% new tuples into the DBMS) to test the maintenance
overhead of B+ -Tree and Hippo.
Experimental Setup. The test machine has 8 CPUs
(3.5 GHz per core), 32 GB memory, and 2 TB magnetic
disk with PostgreSQL 9.5 installed. Unless mentioned otherwise, Hippo sets the default partial histogram density to
20% and the default histogram resolution to 400. The impact of parameters is also discussed.

7.1 Implementation Details

7.2 Pre-tune Hippo parameters

We have implemented a prototype of Hippo inside the core
kernel of PostgreSQL 9.5 as one of the main index access
methods by leveraging the underlying interfaces which include but not limited to ”ambuild”, ”amgetbitmap”, ”aminsert” and ”amvacuumcleanup”. A database user is able to
create and query a Hippo index as follows:

Hippo is a flexible index which can be tuned by the
database user to perfectly fit his specific scenarios. There are
two parameters, partial histogram density D (Default value
is 20%) and complete histogram resolution H (Default value
is 400), discussed in this section. Referring to the estimation
before, both of them have impacts on index size, initialization time, and query time. For these experiments, we build
Hippo and B+ -tree on ”partkey” attribute in Lineitem table
of 200 GB TPC-H workload. As mentioned in Introduction,
B+ -Tree has 25 GB index size at this time.

CREATE INDEX hippo_idx ON lineitem USING hippo(partkey)
SELECT * FROM lineitem
WHERE partkey > 1000 AND partkey < 2000

7.2.1 Impact of partial histogram densities

DROP INDEX hippo_idx

9

3
2
1
0

B+-Tree

Hippo

4
3
2
1
0
400
1600
Hippo histogram resolution

Figure 8: Partial histogram density

Figure 9: Histogram resolution

Density (D)
Resolution (R)

6

B+-Tree

Hippo

4
2
0

20%
40%
Hippo partial histogram density (%)

Parameter
Default

x 1000000

4

5

Query time (ms)

Hippo

x 1000000

B+-Tree

Query time (ms)

x 1000000

Query time (ms)

5

Value
D=20% R=400

Size
1012 MB

Initial. time
2765 sec

40%

680 MB

2724 sec

80%

145 MB

2695 sec

800

822 MB

2762 sec

1600

710 MB

2760 sec

Q6
-

Figure 10: TPC-H standard queries

impact on the index size and initialization time is given in
Table 3 and the impact on query time is depicted in Figure 9.
As Table 3 illustrates, with the growth of histogram resolution, Hippo size reduces moderately. The explanation
is that Hippo which has higher histogram resolution consists of less partial histograms and each partial histogram in
this Hippo may summarize more pages but the partial histogram (in bitmap format) has larger physical size because
the bitmap has to store more bits.
As Figure 9 shows, the query time of three Hippos varies
with the growth of histogram resolution. This is because for
the large histogram resolution, the query predicate may hit
more buckets so that this Hippo is more likely to overlap
with query predicates and result in more pages are selected
as possible qualified pages. At this selectivity factor, Hippo
which has histogram resolution 400 is just a little bit worse
than B+ -Tree in terms of query time.

Table 3: Parameters affect Hippo indexing overhead

Hippo introduces a terminology ”partial histogram density” to dynamically control the number of pages summarized by one partial histogram. Based on the discussion
before, the partial histogram density may affect Hippo size,
initialization time and query time. The following experiment compares the default Hippo density (20%) with two
different densities (40% and 80%) and tests their query time
with selectivity factor 0.1%. According to the discussion in
Section 6.2, partial histograms under the three different density setting may summarize around 2 pages, 5 pages and 17
pages respectively (if one page contains 50 tuples). Thus it
can be estimated that the index size of 20% density Hippo
is around 2 times of 40% density Hippo and 8 times of 80%
density Hippo. The impact of the density on Hippo size and
initialization time is described in Table 3 and the impact on
query time is described in Figure 8.
It can be observed that as we increase the density, Hippo
indexing overhead decreases as expected (up to two orders
of magnitude smaller than B+ -Tree in terms of storage) because Hippo is able to summarize more pages per partial
histogram and write less index entries on disk. Similarly,
Hippo which has higher density costs more query time because it is more likely to overlap with query predicates and
result in more pages are selected as possible qualified pages.
At this selectivity factor, Hippo which has density 20% is
just a little bit worse than B+ -Tree in terms of query time.

7.3 Compare Hippo to B+ -Tree
This section compares Hippo with B+ -Tree in terms of
indexing overhead (index size and initialization time), index
maintenance overhead and index query time. To further illustrate the advantages of Hippo, we also compare these indexes using TPC-H standard queries. Hippo tested in this
section uses the default setting which has histogram resolution 400 and partial histogram density 20%.

7.3.1 Indexing overhead
The following experiment builds B-Tree and Hippo on
attribute ”partkey” in Lineitem table of TPC-H workload
(2 GB, 20 GB and 200 GB) and measures their indexing
overhead including index size and index initialization time.
Hippo only stores disk page pointers along with their summaries so that it may have much less index entries in contrast
with B+ -Tree. Thus it is not difficult to understand that
Hippo remains an index size which is lower than B+ -Tree.
In addition, referring to the discussion in the initialization
time estimation model, Hippo initialization time should be
far less than B+ -Tree because B+ -Tree has numerous nodes
to be written to disk.
As Figure 6a illustrates, the index size increases with the
growth of data size. The index size of Hippo is around
25 times smaller than that of B+ -Tree on all workload
sizes. Thus Hippo significantly reduces the storage overhead. Moreover, as Figure 6b shows, Hippo index initialization is at least 1.5x faster that of B+ -Tree.

7.2.2 Impact of histogram resolutions
Each partial histogram of Hippo is composed of some
buckets from the complete histogram. The number of buckets in this complete histogram represents the histogram resolution. The more buckets there are, the higher resolution
the complete histogram has. According to the discussion
before, the histogram resolution may affect index size, initialization time and query time. The following experiment
compares the default Hippo histogram resolution (400) with
two different histogram resolutions (800 and 1600) and tests
their query time with selectivity factor 0.1%. The density

7.3.2 Index maintenance overhead
Hippo updates itself eagerly after inserting a tuple into the
parent table. This eager update strategy for data insertion
10

is also adopted by B+ -Tree so that the two indexes can be
compared together. In terms of update time complexity, B+ Tree has approximate log(Card) and Hippo has (log(Hippo
index entries) + 4). Thus it can be predicted that, for inserting same percentage of tuples, while the update time of
Hippo and B+ -Tree is increasing with the growth of data
size. Hippo will take much less time to update itself than
B+ -Tree because Card is much larger than the number of
Hippo index entries. And also the difference of update
time between Hippo and B+ -Tree will be larger on larger
workload. The experiment uses TPC-H Lineitem table and
creates B+ -Tree and Hippo on attribute ”partkey”. Afterwards, TPC-H refresh transaction which inserts 0.1% new
tuples into Lineitem table is executed. The insertion time
of the indexes is compared in Figure 6c.
As Figure 6c shows, the two indexes take more time to
update on large workload. And also the difference between B+ -Tree and Hippo is more obvious (1200x) on the
largest workload as expected. This is because B+ -Tree
spends much more time on searching proper tuple insert
location (log(Card)) and its update time is increasing with
the growth of TPC-H workload.
Hippo updates itself lazily after deleting data which means
it updates itself after many data deletions occur. In contrast,
B+ -Tree takes an eager update strategy which has around
log(Card) update time cost. It may not make much sense
to compare the two indexes for data deletion.

Index
type

Fast
Query

Guaranteed
Accuracy

Low
Storage

Fast
Maintenance

B+ -Tree

✓

✓

✗

✗

Compressed

✗

✓

✓

✗

Approximate

✓

✗

✓

✗

Sparse

✗

✓

✓

✓

Hippo

✓

✓

✓

✓

Table 4: Compared Indexing Approaches

tor is 0.1%. Thus we find three TPC-H queries which have
typical range queries on ”l shipdate” attribute (Query 6, 15
and 20) and set the range query selectivity factor to 0.1%
which means one week. The query plans of the three queries
are described as follows:
Query 6 This query has a very simple plan. It firstly
performs an index search on Lineitem table using one of
the candidate indexes, then filters the returned values and
finally aggregates the values to calculate the result.
Query 15 This query builds a sub-view beforehand and
embeds it into the main query twice. The range query which
leverages the candidate indexes is a part of the sub-view.
Query 20 The candidate indexes are invoked in a subquery. Then range query results are sorted and aggregated
for calculation. The result is cached into memory and used
the upper level query.
As Figure 10 depicts, Hippo consumes similar query time
with B+ -Tree on Query 6, 15 and 20. The difference between the two indexes is more obvious on Query 15 because
this query invokes the range query twice. Therefore, we
may conclude that Hippo may achieve almost similar query
performance with B+ -Tree at the 25 times smaller storage
overhead when the query selectivity factor is 0.1%.

7.3.3 Impact of query selectivity factors
In this experiment, the query selectivity factors used for
B+ -Tree and Hippo are 0.001%, 0.01%, 0.1% and 1%. According to the query time cost estimation of Hippo, the corresponding query time costs in this experiment are 0.2Card,
0.2Card, 0.2Card and 0.8Card. Therefore, it can be predicted that there will be a great time gap between the first
three Hippo queries and the last one Hippo query. On the
other hand, B+ -Tree should be faster than Hippo at low
query selectivity factor like 0.001% but the difference between the two indexes should be narrowed with the growth
of query selectivity factors.
The result in Figure 7 perfectly matches our predication:
the last Hippo query consumes much more time than the
first three queries. Among them, query time of 0.1% selectivity factor query is a little higher than the first two because
it returns more query results which costs more to retrieve.
Both indexes cost more time on queries with the decreasing of query selectivity factors. B+ -Tree has almost similar
query time with Hippo at 0.1% query selectivity factor. It
is worth noting that B+ -Tree consumes 25 times more storage than Hippo. Therefore, we may conclude that Hippo
makes a well tradeoff between query time and index storage
overhead on medium query selectivity factors like 0.1% so
that, under this scenario, Hippo is a good substitution for
B+ -Tree if the database user is sensitive to aforementioned
index overhead.

8. RELATED WORK
Table 4 summarizes state-of-the-art database index structures in terms of query time, accuracy, storage overhead and
maintenance overhead.
Tree Index Structures: B+ -Tree is the most commonly
used type of indexes. The basic idea can be summarized as
follows: For a non-leaf node, the value of its left child node
must be smaller than that of its right child node. Each leaf
node points to the physical address of the original tuple.
With the help of this structure, searching B+ -Tree can be
completed in one binary search time scope. The excellent
query performance of B+ -Tree and other tree like indexes
is benefited by their well designed structures which consist
of many non-leaf nodes for quick searching and leaf nodes
for fast accessing parent tuples. This feature incurs two
inevitable drawbacks: (1) Storing plenty of nodes costs a
huge chunk of disk storage. As shown in Section 1, it results
in non-ignorable dollar cost and huge initialization time in
big data scenarios. (2) Index maintenance is extremely timeconsuming. For any insertions or deletions occur on parent
table, tree like indexes firstly have to traverse themselves
for finding proper update locations and then split, merge or
re-order one or more nodes which are out of date.
Compressed Index Structures: Compressed indexes
try to drop some repeated index information as much as
possible beforehand for saving space and recover it as fast
as possible upon queries from users but they all have guaranteed query accuracy. These techniques are applied to tree

7.3.4 TPC-H queries
To further explore the query performance of Hippo in the
real business decision support, we compare Hippo with B+ Tree using TPC-H standard queries. Both of the two indexes are built on ”l shipdate” attribute in Lineitem table
of 200 GB workload. As discussed before, Hippo costs similar query time with B+ -Tree when the query selectivity fac11

10. REFERENCES

indexes [8, 9] and bitmap indexes [7, 11, 14, 19] (low cardinality and read-only datasets). Though compressed indexes
are storage economy, they require additional time for compressing beforehand and decompressing on-the-fly. Compromising on the time of initialization, query and maintenance
is not desirable in many time-sensitive scenarios. Hippo on
the other hand reduces the storage overhead by dropping redundancy tuple pointers and hence still achieves competitive
query response time.
Approximate Index Structures: Approximate indexes [2, 10, 12] give up the query accuracy and only store
some representative information of parent tables for saving
indexing and maintenance overhead and improving query
performance. They propose many efficient statistics algorithms to figure out the most representative information
which is worth to be stored. In addition, some people focus on approximate query processing (AQP)[1, 18] which
relies on data sampling and error bar estimating to accelerate query speed directly. However, trading query accuracy
makes them applicable to limited scenarios. On the other
hand, Hippo, though still reduces the storage overhead, only
returns exact answer that match the query predicate.
Sparse Index Structures: Sparse index (denoted as
Zone Map Index in IBM Data Warehouse[3], Data Pack
structure in Infobright[13], Block Range Index in PostgreSQL[15], and Storage Index in Oracle Exadata[16]) is
a simple index structure implemented by many popular
DBMS in recent years. Sparse index only stores pointers
which point to disk pages of parent tables and value ranges
(min and max values) in each page so that it can save indexing and maintenance overhead. It is generally built on
ordered attributes. For a posed query, it finds value ranges
which cover or overlap the query predicate and then rapidly
inspects the associated few parent table pages one by one
for retrieving truly qualified tuples. However, for most real
life attributes which have unordered data, sparse index has
to spend lots of time on page scanning because the stored
value ranges (min and max values) may cover most query
predicates and encumber the page inspection. Therefore, an
efficient yet concise page summarizing method (i.e., Hippo)
instead of simple value ranges is highly desirable.

9.

[1] S. Agarwal, H. Milner, A. Kleiner, A. Talwalkar,
M. Jordan, S. Madden, B. Mozafari, and I. Stoica.
Knowing when you’re wrong: building fast and
reliable approximate query processing systems. In
SIGMOD, pages 481–492. ACM, 2014.
[2] M. Athanassoulis and A. Ailamaki. Bf-tree:
Approximate tree indexing. In VLDB, pages
1881–1892. VLDB Endowment, 2014.
[3] C. Bontempo and G. Zagelow. The ibm data
warehouse architecture. CACM, 41(9):38–48, 1998.
[4] D. Comer. Ubiquitous b-tree. CSUR, 11(2):121–137,
1979.
[5] T. P. P. Council. Tpc-h benchmark specification.
Published at http://www. tcp. org/hspec. html, 2008.
[6] P. Flajolet, D. Gardy, and L. Thimonier. Birthday
paradox, coupon collectors, caching algorithms and
self-organizing search. Discrete Applied Mathematics,
39(3):207–229, 1992.
[7] F. Fusco, M. P. Stoecklin, and M. Vlachos. Net-fli:
on-the-fly compression, archiving and indexing of
streaming network traffic. VLDB J., 3(1-2):1382–1393,
2010.
[8] J. Goldstein, R. Ramakrishnan, and U. Shaft.
Compressing relations and indexes. In ICDE, pages
370–379. IEEE, 1998.
[9] G. Guzun, G. Canahuate, D. Chiu, and J. Sawin. A
tunable compression framework for bitmap indices. In
ICDE, pages 484–495. IEEE, 2014.
[10] M. E. Houle and J. Sakuma. Fast approximate
similarity search in extremely high-dimensional data
sets. In ICDE, pages 619–630. IEEE, 2005.
[11] D. Lemire, O. Kaser, and K. Aouiche. Sorting
improves word-aligned bitmap indexes. Data &
Knowledge Engineering, 69(1):3–28, 2010.
[12] Y. Sakurai, M. Yoshikawa, S. Uemura, H. Kojima,
et al. The a-tree: An index structure for
high-dimensional spaces using relative approximation.
In VLDB, pages 5–16. VLDB Endowment, 2000.
[13] D. Ślezak and V. Eastwood. Data warehouse
technology by infobright. In SIGMOD, pages 841–846.
ACM, 2009.
[14] K. Stockinger and K. Wu. Bitmap indices for data
warehouses. Data Warehouses and OLAP: Concepts,
Architectures and Solutions, page 57, 2006.
[15] M. Stonebraker and L. A. Rowe. The design of
postgres. In SIGMOD, pages 340–355. ACM, 1986.
[16] R. Weiss. A technical overview of the oracle exadata
database machine and exadata storage server. Oracle
White Paper. Oracle Corporation, Redwood Shores,
2012.
[17] K. Wu, E. Otoo, and A. Shoshani. On the performance
of bitmap indices for high cardinality attributes. In
VLDB, pages 24–35. VLDB Endowment, 2004.
[18] K. Zeng, S. Gao, B. Mozafari, and C. Zaniolo. The
analytical bootstrap: a new method for fast error
estimation in approximate query processing. In
SIGMOD, pages 277–288. ACM, 2014.
[19] M. Zukowski, S. Heman, N. Nes, and P. Boncz.
Super-scalar ram-cpu cache compression. In ICDE,
pages 59–59. IEEE, 2006.

CONCLUSION

The paper introduces Hippo a sparse indexing approach
that efficiently and accurately answers database queries
while occupying up to two orders of magnitude less storage
overhead than de-facto database indexes, i.e., B+ -tree. To
achieve that, Hippo stores pointers of pages instead of tuples
in the indexed table to reduce the storage space occupied
by the index. Furthermore, Hippo maintains histograms,
which represent the data distribution for one or more pages,
as the summaries for these pages. This structure significantly shrinks index storage footprint without compromising much on performance of common analytics queries, i.e.,
TPC-H workload. Moreover, Hippo achieves about three
orders of magnitudes less maintenance overhead compared
to the B+ -tree. Such performance benefits make Hippo a
very promising alternative to index data in big data application scenarios. Furthermore, the simplicity of the proposed
structure makes it practical for database systems vendors to
adopt Hippo as an alternative indexing technique. In the
future, we plan to adapt Hippo to support more complex
data types, e.g., spatial data, unstructured data.
12

See	discussions,	stats,	and	author	profiles	for	this	publication	at:	https://www.researchgate.net/publication/283123965

Methods	for	deriving	and	calibrating	privacypreserving	heat	maps	from	mobile	sports
tracking	application	data
Article		in		Journal	of	Transport	Geography	·	September	2015
DOI:	10.1016/j.jtrangeo.2015.09.001

CITATIONS

READS

8

38

4	authors,	including:
Juha	Oksanen

Cecilia	Bergman

Finnish	Geospatial	Research	Institute	/	Natio…

Finnish	Geospatial	Research	Institute

32	PUBLICATIONS			333	CITATIONS			

3	PUBLICATIONS			10	CITATIONS			

SEE	PROFILE

SEE	PROFILE

All	content	following	this	page	was	uploaded	by	Juha	Oksanen	on	07	January	2016.

The	user	has	requested	enhancement	of	the	downloaded	file.	All	in-text	references	underlined	in	blue	are	added	to	the	original	document
and	are	linked	to	publications	on	ResearchGate,	letting	you	access	and	read	them	immediately.

Journal of Transport Geography 48 (2015) 135–144

Contents lists available at ScienceDirect

Journal of Transport Geography
journal homepage: www.elsevier.com/locate/jtrg

Methods for deriving and calibrating privacy-preserving heat maps from
mobile sports tracking application data
Juha Oksanen a,⁎, Cecilia Bergman a, Jani Sainio b, Jan Westerholm b
a
b

Department of Geoinformatics and Cartography, Finnish Geospatial Research Institute/National Land Survey of Finland, P.O. Box 84, FI-00521 Helsinki, Finland
Faculty of Science and Engineering, Åbo Akademi University, Joukahainengatan 3-5, FI-20520 Turku, Finland

a r t i c l e

i n f o

Article history:
Received 19 December 2014
Received in revised form 3 September 2015
Accepted 5 September 2015
Available online 20 September 2015
Keywords:
Cycling
Location-based services (LBSs)
Urban planning
Privacy
Big data
GIS

a b s t r a c t
Utilization of movement data from mobile sports tracking applications is affected by its inherent biases and sensitivity, which need to be understood when developing value-added services for, e.g., application users and city
planners. We have developed a method for generating a privacy-preserving heat map with user diversity
(ppDIV), in which the density of trajectories, as well as the diversity of users, is taken into account, thus
preventing the bias effects caused by participation inequality. The method is applied to public cycling workouts
and compared with privacy-preserving kernel density estimation (ppKDE) focusing only on the density of the recorded trajectories and privacy-preserving user count calculation (ppUCC), which is similar to the quadrat-count
of individual application users. An awareness of privacy was introduced to all methods as a data pre-processing
step following the principle of k-Anonymity. Calibration results for our heat maps using bicycle counting data
gathered by the city of Helsinki are good (R2 N 0.7) and raise high expectations for utilizing heat maps in a city
planning context. This is further supported by the diurnal distribution of the workouts indicating that, in addition
to sports-oriented cyclists, many utilitarian cyclists are tracking their commutes. However, sports tracking data
can only enrich ofﬁcial in-situ counts with its high spatio-temporal resolution and coverage, not replace them.
© 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license
(http://creativecommons.org/licenses/by/4.0/).

1. Introduction
Mobile sports tracking applications have become popular among the
public audience, and a large number of smartphone users are willing to
collect and compare their workouts privately, as well as to share their
data within social networks or even publicly, for all application and Internet users. Key factors in this development have been the maturity of sensor
technology, such as an accelerometer, digital compass, gyroscope, and GPS
(e.g., Lane et al., 2010), available in nearly all recent mid- and top-range
smartphones; and well-documented application programming interfaces
for third-party developers to create new applications for mobile platforms.
The aim of our work is to develop methods to enrich workout data
from a mobile sports tracking application to create privacy-preserving
information about the most popular places to do sports. While our
case study focuses on public cycling workouts collected using Sports
Tracker (http://www.sports-tracker.com/), the developed methods can
be used for any other sports recorded using any mobile sports tracking
application. The approach chosen for this study relies on visual data
mining, which utilizes human perception in data exploration, and combines human ﬂexibility, creativity, and knowledge with a computer's
storage capacity, computing power, and visualization capabilities
⁎ Corresponding author. Tel.: +358 40 831 4092.
E-mail address: juha.oksanen@nls.ﬁ (J. Oksanen).

(Keim, 2002). When integrated into a location-based service (LBS),
the result of our analysis replies to the end-user's question “Where
have most cyclists continued to from here?” In addition, we investigate
the relation of tracking data and in-situ bicycle counting information
in order to compare the quality of the derived heat maps, as well as to
calibrate the heat maps based on mobile sports tracking application
data, for example, for city planning purposes. We use the term “workout” throughout the paper to denote all recorded trajectories, be they
recreational/exercise or utilitarian by purpose.
The idea of generating heat maps from mobile sports app data to
communicate the popularity of sports is not new (e.g., Garmin, 2013;
Lin, 2012; Strava, 2014), but less attention has been paid to the methods
for making the calculation, and concerns about the appropriate understanding of the heat maps have been raised due to new application
areas of heat maps, such as city planning (Maus, 2014a) and analysis
of eye-tracking data (Bojko, 2009). When creating heat maps, an obvious surrogate for the popularity of sports is the density of workout trajectories, but other surrogates, such as the number of different people
doing sports, can also be used. According to the limited information
available on existing heat maps, the one provided by Strava uses the
number of GPS points as a pixel value (Mach, 2014), whereas in the
heat map offered by Nike+, the value at each pixel represents the number of users (Lin, 2012). As we will show in this paper, the two methods
can locally result in very different patterns of bike riding.

http://dx.doi.org/10.1016/j.jtrangeo.2015.09.001
0966-6923/© 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).

136

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135–144

The use of heat maps as a representation of the intensity of a phenomenon has its roots in spectrometry (e.g., Moon et al., 2009) and
the generation of isarithm and dot density maps (Slocum et al., 2009),
but in the context of cartography, a textbook deﬁnition of a heat map
is still missing (e.g., Trame and Keßler, 2011). Heat maps are a common
visualization technique in many ﬁelds of research where large amounts
of data are handled. For example, in human–computer interaction,
“attention heat maps” are a popular tool of visual analysis of eyetracking data (e.g., Blascheck et al., 2014; Bojko, 2009). Related to studies utilizing the increasing volumes of volunteered geographic information (VGI; Goodchild, 2007), heat maps have been used, for example, to
reveal attractive/popular places in a region using the density-based spatial clustering of geotagged images (Kisilevich et al., 2010; Kurata, 2012)
and videos (Mirković et al., 2010), or to visualize spatio-temporal patterns revealed by the distribution of tweets (e.g., Morstatter et al.,
2013; Zeile et al., 2012). The coloring of a heat map is typically selected
in such way that the interpretation of the intensity differences becomes
intuitive. This is expected to happen when warm colors, in terms of
color temperature, are used for high intensities of the represented phenomenon and cool colors for low intensities (e.g., Špakov and Miniotas,
2007).
In addition to the public audience, interest in mobile tracking applications, and enriching, especially, cycling data collected with them has
emerged among city planners (Albergotti, 2014; Charlton et al., 2011;
Hood et al., 2011). One of the biggest challenges in a city-planning context regarding non-motorized trafﬁc is the lack of documentation on the
usage and the demand ﬁgures for, for example, cyclists and pedestrians
(Lindsey et al., 2014; NBPD, 2014). Traditional approaches for monitoring cycling trafﬁc have been the use of surveys for qualitative results and
different types of manual and automatic in-situ counting for quantitative results (Grifﬁn et al., 2014; NBPD, 2014; Rantala and Luukkonen,
2014). Mobile tracking of cyclists has been seen as an attractive, inexpensive, and dynamic alternative to traditional bicycle data collection
(Hudson et al., 2012). An early approach to tracking was the development of dedicated platforms, such as CycleTracks, which was developed
at San Francisco Municipal Transportation Agency and has since been
used at a number of agencies and municipalities in the US (Charlton
et al., 2011; Masoner, 2014). In the UK, another crowdsourcing-based
application, Cycle Hackney, is expected to provide a cost-effective way
to ﬁnd out where, especially, utilitarian cycling is taking place
(CycleStreets, 2014). Recently, in the city of Oulu, Finland, there has
been a development project aiming to create a “smoothness navigator”
for cyclists, based on 1000 recorded tracks of people participating in the
pilot phase (Poikola, 2014). The problem in dedicated tracking
platforms appears to be the limited group of people interested in
using them voluntarily (SFMTA, 2013). To overcome this problem the
potential of utilizing mobile sports tracking data has been recognized,
reﬂecting the idea of utilizing humans as sensors (Goodchild, 2007)
and big data analytics (e.g., Russom, 2011). For example, Oregon's
Department of Transportation paid $20,000 to use data from the mobile
sports tracking application Strava for a year, containing 400,000 individual bicycle trips, totaling 8 million bicycle kilometers traveled (Estes,
2014; Maus, 2014b).
While mobile sports tracking data may not qualify as ‘big data’
regarding its volume – except in the sense “bigger than previously”
(Goodchild, 2013) – it shares many characteristics with other social
media data, often classiﬁed as big data. Many of the characteristics follow from the fact that big data is typically not collected with any speciﬁc
purpose in mind or not used for its original purpose (Kitchin, 2014). In
statistics, random sampling is used to guarantee the representativeness
of observations, but in big data analytics, the ‘sample’ is not randomly
chosen at all (Goodchild, 2013). Rather, the aim is to use all the data following the principle of exhaustivity in scope (Harford, 2014; Kitchin,
2014). However, considering that only a small and possibly behaviorally
biased subset of cyclists use mobile applications to track their routes, the
question is how well they represent the whole population of cyclists

(e.g., Maus, 2014a; Rantala and Luukkonen, 2014); i.e., as social media
data in general, sports tracking data is affected by self-selection bias
(Shearmur, 2015). In addition, mobile tracking applications have their
differences with respect to appearance and function, such as the available range of sports (multi-sports or single activity type), and may
therefore attract different people. As an example, the cycling and running app Strava has the reputation of being used by more competitive
or “serious” cyclists (Zahradnik, 2014) and is also targeting people
who identify themselves as “athletes” (Strava, 2014). On the other
hand, for example, Sports Tracker (ST, 2014) “want[s] to help people
train better, connect through sports, and live healthier, happier lives;”
HeiaHeia! focuses on the business-to-business sector and work welfare
(Kauppalehti, 2013); and Endomondo (2014) is, in its own words,
aiming “to motivate people to get and stay active.” It has been estimated
that 90% of the cyclists who use Strava are male (Usborne, 2013;
Vanderbilt, 2013) and in 2012, 75% of all Endomondo users were men
(Endoangela, 2012). Furthermore, participation inequality is a known
property of VGI and online communities, according to which 90% of
community members are followers and do not contribute to the community, whereas 9% contribute from time to time, and 1% account for
most contributions (Nielsen, 2006). Although sports tracking applications do not today represent shared projects where people would
track and share their workouts to promote the common good, some
typical motivations for contribution in VGI, such as social reward and
enhanced personal reputation (Coleman et al., 2009), can be identiﬁed
within their communities as well. These bias issues introduce a major
challenge in using mobile sports app data in a city-planning context.
According to Westin's tenet, privacy is an individual's right to have
full control over information about themselves, and to decide when,
how, and to what extent this information is shared with others
(Agrawal et al., 2002). Guaranteeing privacy in LBSs is extremely important, due to the unique characteristics of moving object data (Fung et al.,
2010; Montjoye et al., 2013; Verykios et al., 2008). Topics such as
anonymization of the original dataset (e.g., Monreale et al., 2010;
Pensa et al., 2008), or de-identifying a given LBS-request location (e.g.,
Bettini et al., 2005; Gedik and Liu, 2004; Gruteser and Liu, 2004), have
gained a great deal of attention in trajectory studies but are beyond
the scope of this paper. Instead, we approach privacy-preservation
from the standpoint of visualization.
The idea behind preserving privacy in visualizations is to generalize
or otherwise obfuscate data in such a manner that the disclosed data is
still useful in the particular case (Andrienko et al., 2008; Fung et al.,
2010). Various methods of geographical masking, ﬁrst introduced by
Armstrong et al. (1999), have been developed with the aim of
protecting the conﬁdentiality of individual locations by adding stochastic or deterministic noise to the geographic coordinates of the original
data points without substantially affecting analytical results or the visual characteristics of the original pattern (Kwan et al., 2004). Spatial aggregation of individual-level data for administrative areas or other
areal units that have a population greater than a chosen cutoff value is
a common procedure of preserving conﬁdentiality, for example, in
censuses where disclosure control has long been an integral part of
the process (Armstrong et al., 1999; Kwan et al., 2004; Leitner and
Curtis, 2006; Young et al., 2009). Because aggregation can hide important spatial patterns in the data, various alternative geo-masking techniques, such as random perturbation and afﬁne transformation
(translate, rotate, and scale), have been introduced to preserve the disaggregated, discrete nature of the original data (Armstrong et al., 1999;
Kwan et al., 2004). Although they have been used mainly with georeferenced, sensitive health- and crime-related point data (e.g., Leitner
and Curtis, 2006; Kounadi and Leitner, 2015), Krumm (2007) and
Seidl et al. (2015) have applied them also to GPS trajectory data. In
this study, where it was crucial to prevent re-identiﬁcation of an individual user and trajectory while providing the heat map viewer accurate
information about popular cycling paths in their actual locations on the
road network, geo-masking techniques as such were, however, not

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135–144

adequate. Instead, we followed the principle of k-Anonymity in
anonymization of the ﬁnal heat maps. K-Anonymity was originally
developed for record data by Samarati and Sweeney (1998), but has
since been extended to spatio-temporal movement data by, for example, Abul et al. (2008), and Terrovitis and Mamoulis (2008). Related to
visualization techniques, the method has been previously applied to privacy preservation for parallel coordinates (Dasgupta and Kosara, 2011).
K-Anonymity refers to the requirement according to which “each release of data must be such that every combination of values of quasiidentiﬁers can be indistinctly matched to at least k individuals”
(Samarati and Sweeney, 1998). By quasi-identiﬁers, they mean a set of
attributes whose publication needs to be somehow restricted. When
bringing the k-Anonymity requirement into the heat map context, our
goal was that all details interpretable from the ﬁnal heat map must be
such that the result has been borne out from at least a minimum
pre-deﬁned number of application users.
This paper presents a novel method for deriving privacy-preserving
heat maps from mobile sports application data which takes into account
the density of trajectories, as well as the diversity of users, attempting to
avoid the bias caused by having few very active sports application users.
As a secondary objective, the study presents a method by which heat
maps derived from a mobile sports application's cycling data can be calibrated using in-situ ﬁeld-collected bicycle counting data. With this approach, cities may use the calibrated heat maps as source information
for city planning purposes. The remainder of the paper is organized as
follows. In Section 2, we describe the data and methods used for deriving privacy-preserving heat maps. In Section 3, we compare the heat
maps and discuss the characteristics of different methods, and we also
show the relation between our heat maps and real-world in-situ bicycle
counting data. Finally, in Section 4, conclusions are drawn and some
future directions are pointed out.
2. Materials and methods
The data used for our study was obtained from Sports Tracking Technologies Ltd., the inventor of the ﬁrst mobile sports tracking application,
Sports Tracker, for mobile phones (ST, 2014). While the app turns a
smart phone into a sports computer, it is also a complete social platform
on which application users share their workouts, photos, and experiences of exercises, with their group of friends or even with everyone.
The basic unit of recorded data is a workout, which contains information
about the user, sport (28 pre-classiﬁed sports, such as cycling, walking,
and running), and a 4D (x, y, z, and t) coordinate list recorded at approximately 1-second intervals from the start to the ﬁnish of the workout.
For this study, the data contained only public workouts from the region
of Helsinki, and each user identiﬁer was changed into a pseudo-ID at
Sports Tracking Technologies Ltd. before the data delivery.
The total sample data contained 192,597 workouts, of which a
subset of 36,757 workouts collected by 2424 users represented cycling
inside our approximately 320 km2 study region. Temporally continuous
data was recorded between April 17th 2010 and November 21st 2012,
and it consisted of a total of 36,663,190 GPS points. About 82% of
the data contained valid time values accepted for further analysis. In
a pre-processing phase, the data was systematically thinned to
10-second time intervals, the time data was transformed from POSIX
time to Coordinated Universal Time (UTC + 02:00), and coordinates
were converted from WGS84 (EPSG:4326) to ETRS-TM35FIN
(EPSG:3067) (Anon, 2012). In addition, each point was supplied with
information about distance, time, and speed from the previous tracked
point. Further ﬁltering of gross errors was done based on thresholds
set for distance (b 300 m) and speed (b 50 m/s) between consecutive
GPS observations.
One characteristic of the data was that the number of people tracking and publishing a large number of workouts was small, and most of
the users had tracked less than 10 workouts (Fig. 1), which appears to
follow the principle of participation inequality (Nielsen, 2006). In the

137

Fig. 1. Number of tracked and published cycling workouts per user.

study area and within the time range, 65% of the users had published
5 or fewer workouts, and 87% had published 20 or fewer workouts.
While the most active user had published more than 600 workouts,
only 3% of the users had published more than 100 workouts. On
average, for the sample data, each user had published 15 workouts.
Further inspection of the data revealed cyclicity at a number of temporal scales. At an annual level, the popularity of cycling increased
sharply during the summer season and peaked in August (Fig. 2a and
b). In addition, the popularity of tracking cycling increased steadily
from 2010 to 2012. When focusing on the weekly pattern, tracking of
cycling was at its highest level on Tuesdays and Wednesdays, while
the minimum level was reached on Fridays (Fig. 2c). Peaks in the frequencies of tracked points between 7–8 AM and 4–5 PM (Fig. 2d) reveal
that many people use Sports Tracker to track their daily commuting
trips.
The use of pseudoIDs is not an adequate means for preserving the
privacy of the application users since location alone may also reveal sensitive personal data (e.g., Bettini et al., 2005; Samarati and Sweeney,
1998). Privacy ﬁltering of the tracked points was done in a preprocessing phase containing: 1) the generation of trajectories from all
points, 2) the generation of a user count raster (number of different
users on a 10 m grid within a 15 m search radius) from the trajectories,
3) the extraction of user count values at all points, and 4) the generation
of trajectories from the subset of points with a user count value higher
than a pre-deﬁned threshold (here 5 users). By this method, we were
able to ﬁlter privacy-preserving trajectories, which contained only the
features that were covered by an adequate number of different users.
From the ﬁltered cycling trajectories we generated heat maps by a
custom ArcGIS tool using three methods, namely privacy-preserving
user count calculation (ppUCC), privacy-preserving kernel density estimation (ppKDE), and privacy-preserving kernel density estimation
modiﬁed with the user diversity index (ppDIV). ppUCC was the simplest
of the applied methods, in which the study region was partitioned into
sub-regions of equal area and each area got its value, so-called quadrat
counts (Bailey and Gatrell, 1995), from the number of users passing the
pixel or a larger calculation window (20 m in our case). Thus, using
ppUCC, we created a 2D histogram of individual cyclists within the
study area.
Kernel density estimation (KDE) is a family of methods originally developed to obtain smooth estimates of uni- or multivariate densities
from observations (Bailey and Gatrell, 1995), but recently KDE has
also been widely used to derive heat maps from data representing
moving objects (e.g., Krisp and Peters, 2011; Willems, 2011). The
commonly used kernel function is described by Silverman (1986):
ppKDEðsÞ ¼

n
s−s 
1 X
i
;
K
nh i¼1
h

where h is the width of the calculation window (bandwidth), n is the
number of sample points, and K is the kernel function used for smoothing the estimate. Here, a quartic approximation of a Gaussian kernel
(Silverman, 1986) was used. Sample points within the radius h are

138

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135–144

Fig. 2. Frequencies of tracked points a) daily, b) monthly, c) according to weekday, and d) according to the hour of the day.

represented by si. When KDE is used for line features, such as
trajectories, the result of KDE can be thought of as the result of placing
a smooth kernel surface on top of the lines. The bandwidth was chosen
as 25 m, to generalize the positioning uncertainty of GPS devices, but

simultaneously to preserve the details of the street network along
which the cycling has occurred.
In ppDIV the result of ppKDE was scaled with the diversity of users at
each quadrat. Diversity is a descriptive statistic of a population with a

Fig. 3. Locations of the 89 bicycle counting sites in the city of Helsinki in June 2013 (Hellman, 2013) used in the study. The dashed circle represents a distance zone of 2.5 km from the city
center. The map contains data from the Topographic database by the National Land Survey of Finland, 7/2012.

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135–144

class structure (Junge, 1994). Diversity indices are used when we are interested in quantifying how many different classes there are in our data,
and simultaneously how evenly the entities are distributed among
those classes. The index used in this study is Simpson's Diversity Index
D (McDonald and Dimmick, 2003):
DðsÞ ¼ 1−

X
pðsÞ2i

ppDIVðsÞ ¼ ppKDEðsÞ  DðsÞ
where pi is the proportion of user i's trajectories among all trajectories

139

in the quadrat s. This resulted in an index that was close to 1 when
the user distribution was locally uniform, and close to 0 when skewness
of the user distribution was locally high.
Finally, the heat maps were compared with each other, as well as
with the ofﬁcial 2013 bicycle counting data gathered by the city of
Helsinki (Hellman, 2013). The challenge in a quantitative comparison
of densities represented as heat maps based on different calculation
methods is that the information varies in terms of the shape and scale
of the density zones. Therefore, point sampling was done by extracting
values of ppUCC, ppKDE, and ppDIV at the centroids of the Topographic
database's (NLS, 2014) road segments from the Helsinki region for

Fig. 4. Heat maps based on (a) privacy-preserving user count calculation (ppUCC), (b) privacy-preserving kernel density estimation (ppKDE), and (c) privacy-preserving kernel density
estimation modiﬁed with the user diversity index (ppDIV). Major differences between the methods are found in the highlighted regions A and B. The map contains data from the Topographic database by the National Land Survey of Finland, 7/2012; © OpenStreetMap contributors.

140

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135–144

regression analysis purposes. The extraction was done by a nearest
neighbor assignment for ppUCC, and by a bilinear interpolation for
ppKDE and ppDIV. Manual bicycle counting was done at 94 counting
points in early June 2013, during one weekday between 7 AM–7 PM,
and this was transformed to 24 h counts with a supply from semantically near automatic counting stations (Hellman, 2014). The precise
locations of the counting sites were undocumented, but road segments
used for the counting were published, and we adjusted the location of
the 89 unambiguous points to correspond to the local maxima of the
heat maps (Fig. 3).
3. Results and discussion
In general, the three methods for deriving heat maps produced
similar looking results, but when focusing on the details, some major
differences could be found. In the simplest heat map based on ppUCC
(Fig. 4a), patterns of the popularity of cycling in the region became
clearly visible, but the problem appeared to be a lack of knowledge
about the density of the trajectories. For example, in ppUCC, a road
segment with 100 workouts recorded by 10 users got the same heat
map score as a segment with 10 workouts recorded by 10 users. In
ppKDE (Fig. 4b), the density of the trajectories became visible and the
previously mentioned limitation of ppUCC was surpassed, but a single
very active user may result in signiﬁcant bias and overestimation of
the route section's popularity (Fig. 4a, highlights A and B). In ppDIV
(Fig. 4c), we took into account the density of the workout trajectories
and the diversity of users, which resulted in a heat map that closely
corresponded to ppKDE but that did not suffer from the bias introduced
by very active application users.
To further compare the methods, regression analysis between
ppUCC, ppKDE, and ppDIV was performed on the point sample, based
on the centroids of all road segments in the study region. In accordance
with our expectations the coefﬁcients of determination for regression
models were high (Fig. 5), R2 = 0.83–0.94 with the highest coefﬁcient
being between ppKDE and ppDIV (Fig. 5c), revealing the similarity of
the methods in general. Most often, the low number of users was associated with a small number of individual tracks, and a large number of
users also indicated a large number of tracks. However, the most
interesting features of the regression models were found from a spatial
analysis of the residuals of the models.
By residual analysis we could ﬁnd answers to questions such as why
a large number of tracks were not always related to a large number of
users (Fig. 5a), and where were the places where the impact of taking
the diversity of users into account in the calculation of densities of trajectories was the biggest (Fig. 5c). Observations below the regression
curve in Fig. 5a revealed road segments where the density of trajectories
(ppKDE) was lower than might have been expected according to the
number of individual users (ppUCC). When plotting those parts of the
observations on the map (Fig. 6a, blue dots), we could see that many
of these road segments are important through-roads and entry cycling
roads to downtown Helsinki. These also appeared to be routes not

favored by the most active cyclists, by whom we mean cyclists actively
tracking their workouts. In the opposite case (Fig. 6a, red dots), the density of trajectories (ppKDE) was higher than predicted based on the
number of individual users (ppUCC). These were found from the routes
with very active cyclists, either because of the route's popularity among
the enthusiastic sports cyclists or because of a very active user using the
Sports Tracking application to track daily commuting trips. While the
ﬁrst group is interesting in terms of ﬁnding the most popular routes
for cycling, the second group clearly displays bias, and it should be
possible to ﬁlter it out from the results.
In a similar manner, analyzing residuals from the regression model
between ppKDE and ppDIV revealed the road segments where the diversity of the users had the biggest impact on ppKDE. Again, observations below the regression curve in Fig. 5c revealed road segments
where a low diversity of users decreased the density value in the heat
map (Fig. 6b, blue dots). In other words, they were the road segments
where either a single cyclist or very few active cyclists had caused the
largest positive bias in ppKDE. In the opposite case (Fig. 6b, red dots),
a high diversity of users had resulted in a relative increase in ppKDE.
These road segments appeared to be mostly the same important
through-roads and entry cycling roads to downtown Helsinki,
highlighted in blue in Fig. 6a.
When comparing our heat maps to real-world data, all methods for
deriving heat maps performed almost equally well (Fig. 7). Any of the
heat maps can be used in predicting 24 h bicycle counting data, keeping
in mind the coefﬁcients of determination, R2 = 0.49–0.50 (p b 0.001).
In practice this means that while, in many places, heat maps and realworld counting data had a clear connection, there are places where
predictions based on heat maps fail. This may result at least partly
from a temporal mismatch of the datasets and fundamental changes
occurring in the cycling infrastructure. From the ten largest absolute
residuals in the regression model between ppDIV and 24 h counting
data (Fig. 7c, red and blue highlighting), we see that in 80% of the residuals, real-world 24 h counting data was greater compared to the predicted value based on ppDIV, and for only 20% the opposite was true.
When these ten largest residuals were plotted on a map (Fig. 8), we
noticed that the maximum (point 26) was located on Baana, a popular
cycle path opened on June 12th 2012. Our data covered the time period
from April 17th 2010 to November 21st 2012, which means that only
the last 6 months of our data contained cyclists using Baana. A similar
fundamental change in cycling infrastructure had taken place near measurement point 13 where the new cycle and pedestrian bridge Aurora
was inaugurated in late 2012 (HS, 2012). The other large positive residuals could at least partly be explained by the overall increase in cycling
(Hellman, 2013), which has been most signiﬁcant on main entry cycleways to the city (points 7, 16, 21, 28, and 30). Together with the essential cycle-way through downtown Helsinki (point 27), these might also
be locations where the difference between everyday cyclists and cyclists
using Sports Tracker to record their workouts is largest. At points 7, 16,
30, and 28, manual bicycle counting data has been collected for opposite
lanes, and the data was generalized to a single point. Using this method

Fig. 5. Regression models between (a) ppUCC and ppKDE, (b) ppUCC and ppDIV, and (c) ppKDE and ppDIV.

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135–144

141

Fig. 6. Spatial distribution of maximum absolute residuals from the regression models between (a) ppUCC and ppKDE, and (b) ppKDE and ppDIV. The lowest 5th percentile of the residuals
is represented with blue dots and the highest 5th percentile with red dots. The map contains data from the Topographic database by the National Land Survey of Finland, 7/2012.

to derive heat maps may be insensitive to the trafﬁc on different lanes
and underestimates the popularity indicating problems induced by
the size of the calculation window. On the other hand, at point 8, the

manual counting data has been collected only from one side of the
road, and a generalization of the result into a single point resulted in a
large positive bias in prediction. The other negative residual (point

Fig. 7. Regression models between (a) ppKDE, (b) ppUCC, and (c) ppDIV and 24 h bicycle counting data 2013 from the city of Helsinki. In panel (c), the ten largest positive (red, + sign) and
negative (blue, – sign) residuals from the regression model are highlighted. The same labels are used in the map in Fig. 8.

142

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135–144

Fig. 8. The ten largest positive (red) and negative (blue) residuals from the regression model between ppDIV and 24 h bicycle counting data (black) in the city of Helsinki (Fig. 7c). The
background heat map is based on ppDIV. The map contains data from the Topographic database by the National Land Survey of Finland, 7/2012.

4) might be explained by the construction site of Koivusaari metro station, which affected routes in 2013. When we compared the location of
all the above-mentioned positive residuals to the distance zones in Helsinki (Fig. 3), they were all within a 2.5 km radius from the city center.
Finally, when the observations resulting in the ten largest residuals
were removed from the data (Fig. 9a), the coefﬁcient of determination
for predicting the number of cyclists based on ppDIV rose to R2 =
0.76 (p b 0.001). When focusing on the ten largest residuals, and by removing the points where signiﬁcant changes in cycling infrastructure
have occurred (points 13, 26, and 4) and where uncertainty due to
a mismatch in in-situ counting and the heat map was the largest
(point 8), we also got a high coefﬁcient of determination, R2 =
0.72 (p = 0.15) (Fig. 9b). Furthermore, if the points for Töölönranta
(point 21) and Kaisaniemenranta (point 30) were removed, the coefﬁcient of determination rose to R2 = 0.96 (p = 0.20). This clearly indicates that, in our case study, a calibration of heat maps with the
absolute number of cyclists outside a 2.5 km radius can be done with
moderate accuracy, and calibration of the city center heat map would
be best done as a separate processing step.
To summarize the methods (Table 1), it appears that the advantages
of ppUCC are the simplicity of the calculation and the intuitive quantity
of the result, the number of different users per quadrant. In addition,
ppUCC automatically reduces the impact of very active cyclists and
therefore, for example, diminishes the bias caused by active commuters.
The disadvantage of ppUCC is that it ignores the density of trajectories

and therefore brings, for example, mass sports events with a large number of individual athletes into the heat maps. In ppKDE, the density of
trajectories is calculated using the well-established kernel density estimation, but individual active cyclists may introduce signiﬁcant bias into
the heat map. In addition, the unit of the heat map is not intuitive, even
though in a visual interpretation of the results, the unit of the quantity is
noncritical. A novel method introduced in this paper is ppDIV, which
combines the best properties of the previously mentioned methods. It
takes into account the density of the workout trajectories, as well as
the diversity of the users who have tracked the workouts. Therefore,
an individual athlete has no signiﬁcant impact on the resulting heat
map, and the computational biases of ppUCC and ppKDE are eliminated.
A disadvantage of all the methods is that the results depend on the subjective deﬁnition of the size of the calculation window. This decision
should be based, on one hand, on the positioning uncertainty of the
workout trajectories, and on the other hand, on the desired level of detail of the ﬁnal results.

4. Conclusions
In this paper, we have introduced a privacy-preserving diversity
method (ppDIV) for deriving heat maps from mobile sports tracking application data, which takes into account the density of the trajectories
and the diversity of the users. The method was applied to Sports
Tracker's public cycling workouts and compared with privacypreserving kernel density estimation (ppKDE) and privacy-preserving
user count calculation (ppUCC). In addition, we demonstrated a method
for calibrating the cycling heat map with in-situ bicycle counting data.
Table 1
Performance summary of the properties of the heat map generation methods (+ = poor,
++ = moderate, +++ = good).

Fig. 9. Regression models between ppDIV and 24 h bicycle counting data 2013 from the
city of Helsinki, when (a) observations in the city center (shown in Fig. 8) have been
removed and (b) when focusing only on the observations in the city center. In panel (b),
the two largest positive (red, + sign) and negative (blue, – sign) residuals from the regression model are highlighted.

Simplicity of calculation
Sensitivity to density of trajectories
Sensitivity to the number of individual users
Sensitivity to diversity of users
Intuitiveness of the measurement unit of the result
Sensitivity to the size of the calculation window
Ability to ﬁlter out very active users
Ability to ﬁlter out mass sports events

ppUCC

ppKDE

ppDIV

+++
+
+++
+
+++
+
+++
+

++
+++
+
+
+
+
+
+

+
+++
++
+++
+
+
+++
+

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135–144

The results show that ppUCC, ppKDE, and ppDIV reveal different
aspects of the popularity of cycling, and they all are sensitive in different
ways to bias issues in the mobile sports tracking data. The order of superiority between them depends on the requirements set for the ﬁnal result, but for general purposes, ppDIV offers the most neutral view on the
popularity of cycling, and hides the bias issues related to, for example,
very active application users. The ﬁnal added-value application that
would inspire and support visual reasoning with the aim of choosing
attractive routes for cycling could be further improved by allowing the
end-user to ﬁlter the heat map based on subjective preferences on, for
example, speed and time of day or year.
A balance between location privacy and data accuracy is an important but thus far largely uninvestigated topic in the context of heat
maps. In our study area, k-Anonymity-based privacy preservation of
the heat maps could be seen as a valid technique to guarantee the
non-disclosure of individual users without diminishing the value of
the service to the end-users. Still, an open question remains whether
the value k should be a function of the density of the population or
the underlying road network to protect the conﬁdentiality of sensitive
locations even in areas where there are small variations in the routes.
A similar issue has been discussed in many geographical masking
studies that have suggested weighting the displacement distance by
population density because in less densely populated areas the risk of
re-identiﬁcation is higher (e.g., Armstrong and Ruggles, 2005; Kwan
et al., 2004; Murad et al., 2014; Seidl et al., 2015).
When considering the usability of our heat maps from a cityplanning perspective, the chosen approach for big data analytics
appears to be promising. Despite the recognized bias issues, due to a selected group of cyclists tracking their workouts, as well as participation
inequality, calibration results for our heat maps are surprisingly good.
When doing regression modeling between in-situ bicycle counting
data and ppDIV heat map scores, by splitting the data based on a
2.5 km distance from the city center, the coefﬁcients of determination
rise to R2 N 0.7. Most likely, the coefﬁcients of determination would be
even higher when bicycle counting data and the heat map are derived
from the same period of time. This clearly shows the potential of utilizing heat maps in a city-planning context by using the in-situ bicycle
counting data to get the absolute scale and the heat map for getting a
high level of detail and a large spatial coverage of cycling activities.
The use of big data from mobile sports apps also offers a chance for
signiﬁcant savings in the investments made in light trafﬁc counting. In
the long term this could signiﬁcantly help to improve cycling
infrastructure development and planning. Still, it is worth noting that
big data analytics does not replace the need for in-situ bicycle counting
since uncalibrated heat maps suffer from the bias issues caused by a
behaviorally biased subset of cyclists using mobile applications to
track their routes, as well as participation inequality.
Acknowledgments
We thank Sports Tracking Technologies Ltd. for the possibility to use
their public workout data in our research, and Mrs. Susanne Suvanto for
assistance in data processing. The study is a part of the research project
SUPRA (Revolution of Location-Based Services: Embedded data reﬁnement in Service Processes from Massive Geospatial Datasets) funded
by Tekes, the Finnish Funding Agency for Innovation (grants 40261/12
and 40262/12). In addition, Oksanen's funding from the Academy of
Finland (grant 251987) is gratefully acknowledged.
References
Abul, O., Bonchi, F., Nanni, M., 2008. Never walk alone: uncertainty for anonymity in
moving objects databases. Proceedings of the 24th IEEE International Conference
on Data Engineering (ICDE'08), Cancun 7–12 April 2008, pp. 376–385.
Agrawal, R., Kiernan, J., Srikant, R., Xu, Y., 2002. Hippocratic databases. Proceedings of the
28th International Conference on Very Large Databases (VLDB'02), pp. 143–154.

143

Albergotti, R., 2014. Strava, popular with cyclists and runners, wants to sell its data to
urban planners. Digits - Tech News and Analysis from the Wall Street J. (http://
blogs.wsj.com/digits/2014/05/07/strava-popular-with-cyclists-and-runnerswants-to-sell-its-data-to-urban-planners/, accessed 29th July 2014).
Andrienko, G., Andrienko, N., Kopanakis, I., Ligtenberg, A., Wrobel, S., 2008. Visual analytics
methods for movement data. In: Giannotti, F., Pedreschi, D. (Eds.), Mobility, Data Mining and Privacy — Geographic Knowledge Discovery. Springer, Berlin, pp. 375–410.
Anon, 2012. ETRS89 -järjestelmään liittyvät karttaprojektiot, tasokoordinaatistot ja
karttalehtijako (map projections, projected coordinate system, and map sheet division related to ETSR89, in Finnish). Julkishallinnon suosituksia (JHS) 154 (http://
www.jhs-suositukset.ﬁ/suomi/jhs154 (accessed 15th December 2014)).
Armstrong, M.P., Ruggles, A.J., 2005. Geographic information technologies and personal
privacy. Cartographica 40 (4), 63–73. http://dx.doi.org/10.3138/RU65-81R3-0W758V21.
Armstrong, M.P., Rushton, G., Zimmerman, D.L., 1999. Geographically masking health data
to preserve conﬁdentiality. Stat. Med. 18 (5), 497–525. http://dx.doi.org/10.1002/
(SICI)1097-0258(19990315)18:5b497::AID-SIM45N3.0.CO;2-#.
Bailey, T.C., Gatrell, A.C., 1995. Interactive Spatial Data Analysis. Pearson Education,
Harlow, UK.
Bettini, C., Wang, X.S., Jajodia, S., 2005. Protecting privacy against location-based personal
identiﬁcation. In: Jonker, W., Petkovíc, M. (Eds.), Proceedings of the 2nd VDLB international conference on Secure Data Management (SDM'05)Lecture Notes in Computer Science 3674. Springer, Berlin, pp. 185–199. http://dx.doi.org/10.1007/11552338_
13.
Blascheck, T., Kurzhals, K., Raschke, M., Burch, M., Weiskopf, D., Ertl, T., 2014. State-of-theart of visualization for eye tracking data. In: Borgo, R., Maciejewski, R., Viola, I. (Eds.),
State of the Art Report, Eurographics Conference on Visualization (EuroVis'14)
(http://www.visus.uni-stuttgart.de/ﬁleadmin/vis/pdf_s_fuer_Publikationen/State-ofthe-Art_of_Visualization_for_Eye_Tracking_Data.pdf, accessed 15th December 2014).
Bojko, A., 2009. Informative or misleading? Heatmaps deconstructed. In: Jacko, J.A. (Ed.),
Human–Computer Interaction International 2009, Part I. Lecture Notes in Computer
Science 5610, pp. 30–39.
Charlton, B., Hood, J., Sall, E., Schwartz, M., 2011. Bicycle route choice data collection using
GPS-enabled smartphones. Transportation Research Board 90th Annual Meeting,
Washington DC, 23–27 Jan 2011 (10 pp.).
Coleman, D.J., Geogiadou, Y., Labonte, J., 2009. Volunteered geographic information: the
nature and motivation of produsers. Int. J. Spat. Data Infrastruct. Res 4 (http://ijsdir.
jrc.ec.europa.eu/index.php/ijsdir/article/view/140/198, accessed 25th June 2015).
CycleStreets, 2014. Cycle Hackney app created by CycleStreets. http://www.cyclestreets.
net/blog/2014/07/06/cycle-hackney-app/ (accessed 26th November 2014).
Dasgupta, A., Kosara, R., 2011. Adaptive privacy-preserving visualization using parallel
coordinates. IEEE Trans. Vis. Comput. Graph. 17 (12), 2241–2248.
Endoangela, 2012. Popular Endomondo sports tracker mobile app hits 10 million user
milestone and 320 million miles logged. http://blog.endomondo.com/2012/06/26/
popular-endomondo-sports-tracker-mobile-app-hits-10-million-user-milestoneand-320-million-miles-logged/ (accessed 7th August 2014).
Endomondo, 2014. What we do. https://www.endomondo.com/about (accessed
7th August 2014).
Estes, A.C., 2014. Why a ﬁtness-tracking app is selling its data to city planners. GIZMODO.
http://gizmodo.com/why-a-ﬁtness-tracking-app-is-selling-its-data-to-city-1572964149
(accessed 2nd September 2014).
Fung, B.C.M., Wang, K., Chen, R., Yu, P.S., 2010. Privacy-preserving data publishing: a
survey of recent developments. ACM Comput. Surv. 42 (4). http://dx.doi.org/10.
1145/1749603.1749605 (Article 14).
Garmin, 2013. Garmin connect is heating up with heat maps. http://garmin.blogs.com/
my_weblog/2013/03/garmin-connect-is-heating-up.html#.U6FbGbFABa4 (accessed
16th December 2014).
Gedik, B., Liu, L., 2004. A customizable k-anonymity model for protecting location privacy.
Technical Report GIT-CERCS-04-15. Georgia Institute of Technology (12 pp., https://
smartech.gatech.edu/xmlui/bitstream/handle/1853/100/git-cercs-04-15.pdf,
accessed 14th October 2014).
Goodchild, M.F., 2007. Citizens as sensors: the world of volunteered geography.
GeoJournal 69, 211–221. http://dx.doi.org/10.1007/s10708-007-9111-y.
Goodchild, M.F., 2013. The quality of big (geo)data. Dialogues Hum. Geogr. 3 (3),
280–284. http://dx.doi.org/10.1177/2043820613513392.
Grifﬁn, G., Nordback, K., Götschi, T., Stolz, E., Kothuri, S., 2014. Monitoring bicyclist and
pedestrian travel and behavior — current research and practice. Transportation
Research Circular E-C183 (32 pp., http://onlinepubs.trb.org/onlinepubs/circulars/
ec183.pdf, accessed 26th November 2014).
Gruteser, M., Liu, X., 2004. Protecting privacy in continuous location-tracking applications.
IEEE Secur. Priv. 2 (2), 28–34.
Harford, T., 2014. Big data: are we making a big mistake? The Financial Times. http://
www.ft.com/cms/s/2/21a6e7d8-b479-11e3-a09a-00144feabdc0.
html#axzz3GqbLWgPw (accessed 22nd October 2014).
Hellman, T., 2013. Polkupyörälaskennat Helsingissä 2013 (bicycle counting in Helsinki
2013, in Finnish). Memorandum 28th October 2013. City of Helsinki. City Planning
Department, Transportation and Trafﬁc Planning Division (http://www.hel.ﬁ/hel2/
ksv/Aineistot/Liikennesuunnittelu/Liikennetutkimus/pyoralaskennat_2013.pdf,
accessed 2nd September 2014).
Hellman, T., 2014. Personal Communication, June 13th 2014.
Hood, J., Sall, E., Charlton, B., 2011. A GPS-based bicycle route choice model for
San Francisco, California. Transp. Lett. 3, 63–75.
HS, 2012. Neljän miljoonan euron Auroransilta avautui vihdoin ulkoilijoille (four million
euro Aurora's bridge ﬁnally open for citizens, in Finnish). Helsingin Sanomat,
24th November 2012. http://www.hs.ﬁ/kaupunki/a1305621575898 (accessed
12th November 2014).

144

J. Oksanen et al. / Journal of Transport Geography 48 (2015) 135–144

Hudson, J.G., Duthie, J.C., Rathod, Y.K., Larsen, K.A., Meyer, J.L., 2012. Using smartphones to
collect bicycle travel data in Texas. Final Report of the UTCM Project #11-35-69
(http://utcm.tamu.edu/publications/ﬁnal_reports/Hudson_11-35-69.pdf, accessed
26th November 2014).
Junge, K., 1994. Diversity of ideas about diversity measurement. Scand. J. Psychol. 35 (1),
16–26. http://dx.doi.org/10.1111/j.1467-9450.1994.tb00929.x.
Kauppalehti, 2013. Suomalainen liikuntasovellus: 100 yritystä 12 maassa (Finnish sports
application: 100 companies in 12 countries, in Finnish). Kauppalehti 17th March
2013. http://www.kauppalehti.ﬁ/omayritys/suomalainen+liikuntasovellus+100+
yritysta+12+maassa/201303381093 (accessed 16th December 2014).
Keim, D.A., 2002. Information visualization and visual data mining. IEEE Trans. Vis.
Comput. Graph. 7 (1), 100–107.
Kisilevich, S., Mansmann, F., Bak, P., Keim, D., 2010. Where would you go on your next
vacation? A framework for visual exploration of attractive places. Second International Conference on Advanced Geographic Information Systems, Applications, and
Services (GEOPROCESSING), 2010, pp. 21–26. http://dx.doi.org/10.1109/GEOProcessing.
2010.11.
Kitchin, R., 2014. Big data, new epistemologies and paradigm shifts. Big Data Soc. 1 (1).
http://dx.doi.org/10.1177/2053951714528481.
Kounadi, O., Leitner, M., 2015. Deﬁning a threshold value for maximum spatial information loss of masked geo-data. ISPRS Int. J. Geo-Inf. 4 (2), 572–590. http://dx.doi.org/
10.3390/ijgi4020572.
Krisp, J.M., Peters, S., 2011. Directed kernel density estimation (DKDE) for time series
visualization. Ann. GIS 17 (3), 155–162. http://dx.doi.org/10.1080/19475683.2011.
602218.
Krumm, J., 2007. Inference attacks on location tracks. In: LaMarca, A., Langheinrich, M.,
Truong, K.N. (Eds.), Proceedings of the 5th International Conference on Pervasive
Computing (Pervasive'07)Lecture Notes in Computer Science 4480. Springer, Berlin,
pp. 127–143. http://dx.doi.org/10.1007/978-3-540-72037-9_8.
Kurata, Y., 2012. Potential-of-interest maps for mobile tourist information services. In:
Fuchs, M., Ricci, F., Cantoni, L. (Eds.), Information and Communication Technologies
in Tourism 2012. Springer, Vienna, pp. 239–248. http://dx.doi.org/10.1007/978-37091-1142-0_21.
Kwan, M.P., Casas, I., Schmitz, B., 2004. Protection of geoprivacy and accuracy of spatial
information: how effective are geographical masks? Cartographica 39 (2), 15–28.
http://dx.doi.org/10.3138/X204-4223-57MK-8273.
Lane, N.D., Miluzzo, E., Lu, H., Peebles, D., Choudhury, T., Campbell, A.T., 2010. A Survey of
mobile phone sensing. IEEE Commun. Mag. 140–150 September.
Leitner, M., Curtis, A., 2006. A ﬁrst step towards a framework for presenting the location
of conﬁdential point data on maps—results of an empirical perceptual study. Int.
J. Geogr. Inf. Sci. 20 (7), 813–822. http://dx.doi.org/10.1080/13658810600711261.
Lin, D., 2012. How is Nike+ Heat Map calculated? http://howtonike.blogspot.ﬁ/2012/06/
how-is-nike-heat-map-calculated.html (accessed 16th December 2014)
Lindsey, G., Nordback, K., Figliozzi, M.A., 2014. Institutionalizing bicycle and pedestrian
monitoring programs in three states: progress and challenges. 93rd Annual Meeting
of the Transportation Research Board, Washington, DC, January 12–16.
Mach, P., 2014. What do 220,000,000,000 GPS data points look like? http://engineering.
strava.com/global-heatmap/ (accessed 20th August 2014)
Masoner, R., 2014. Santa Cruz County: log your bike rides for transportation planning.
http://www.cyclelicio.us/2014/santa-cruz-county-log-your-bike-rides-fortransportation-planning/ (accessed 16th December 2014).
Maus, J., 2014a. A closer look at Strava's ‘heat map’ for the Portland region.
BikePortland.org (Blog Post) (http://bikeportland.org/2014/04/30/a-closer-look-atstravas-heat-map-for-the-portland-region-105280, accessed 29th July 2014).
Maus, J., 2014b. ODOT embarks on “big data” project with purchase of Strava dataset.
BikePortland.org (Blog Post) (http://bikeportland.org/2014/05/01/odot-embarkson-big-data-project-with-purchase-of-strava-dataset-105375, accessed 29th July
2014).
McDonald, D.G., Dimmick, J., 2003. The conceptualization and measurement of diversity.
Commun. Res. 30 (1), 60–79. http://dx.doi.org/10.1177/0093650202239026.
Mirković, M., Aulibrk, D., Milisavljević, S., Crnojević, V., 2010. Detecting attractive
locations using publicly available user-generated video content — central Serbia
case study. Proceedings of 18th Telecommunications Forum (TELFOR'10), Serbia,
Belgrade, November 23–25, 2010, pp. 1089–1092.
Monreale, A., Andrienko, G., Andrienko, N., Giannotti, F., Pedreschi, D., Rinzivillo, S.,
Wrobel, S., 2010. Movement data anonymity through generalization. Trans. Data
Privacy 3, 91–121 (http://www.tdp.cat/issues/tdp.a045a10.pdf, accessed 15th
December 2014).
Montjoye, Y.-A., Hidalgo, C.A., Verleysen, M., Blondel, V.D., 2013. Unique in the crowd: the
privacy bounds of human mobility. Sci. Rep. 3. http://dx.doi.org/10.1038/srep01376.
Moon, J.-Y., Jung, H.-J., Hee Moon, M., Chul Chung, B., Ho Choi, M., 2009. Heat-map visualization of gas chromatography-mass spectrometry based quantitative signatures
on steroid metabolism. J. Am. Soc. Mass Spectrom. 20 (9), 1626–1637. http://dx.doi.
org/10.1016/j.jasms.2009.04.020.
Morstatter, F., Kumar, S., Liu, H., Maciejewski, R., 2013. Understanding Twitter data with
TweetXplorer. Proceedings of the 2013 ACM SIG KDD International Conference on
Knowledge Discovery and Data Mining (KDD'13), ACM, August 11–14, 2013, Chicago,
IL, USA, pp. 1482–1485 http://dx.doi.org/10.1145/2487575.2487703.
Murad, A., Hilton, B., Horan, T., Tangenberg, J., 2014. Protecting patient geo-privacy via a
triangular displacement geo-masking method. In: Kessler, C., McKenzie, G.D., Kulik,
L. (Eds.), Proceedings of the 1st ACM SIGSPATIAL International Workshop on Privacy

View publication stats

in Geographic Information Collection and Analysis (GeoPrivacy'14), ACM, November
4–7, 2014, Dallas/Fort Worth, TX, USA http://dx.doi.org/10.1145/2675682.2676399.
NBPD, 2014. National bicycle and pedestrian documentation project. Alta Planning &
Design. Institute of Transportation Engineers (ITE) Pedestrian and Bicycle Council
(http://bikepeddocumentation.org/, accessed 12th November 2014).
Nielsen, J., 2006. The 90-9-1 Rule for Participation Inequality in Social Media and
Online Communities. Nielsen Normal Group. (http://www.nngroup.com/articles/
participation-inequality/, accessed 10th October 2014).
NLS, 2014. The Topographic database. National Land Survey of Finland. http://www.
maanmittauslaitos.ﬁ/en/digituotteet/topographic-database (accessed 14th October
2014).
Pensa, R.G., Monreale, A., Monreale, A., Pinelli, F., Pedreschi, D., 2008. Pattern-preserving
k-anonymization of sequences and its application to mobility data mining. In:
Bettini, C., Jajodia, S., Samarati, P., Wang, X.S. (Eds.), Proceedings of the 1st International Workshop on Privacy in Location-Based Applications (PiLBA '08), Malaga,
Spain, October 9, 2008 (http://ceur-ws.org/Vol-397/paper4.pdf, accessed 16th
December 2014).
Poikola, A., 2014. Sujuvuusnavigaattorin pilotointi (piloting of the smoothness navigator).
Open knowledge Finland. http://bit.ly/sujuvuusnavi_kalvot (accessed 12th November
2014).
Rantala, T., Luukkonen, T., 2014. Bicycle and Pedestrian Trafﬁc Monitoring — Guide to
Creating an Indicator Toolbox. Finnish Transport Agency, Planning Department,
Helsinki (37 pages, 2 appendices. http://www2.liikennevirasto.ﬁ/julkaisut/pdf8/lts_
2014-15_kavelyn_pyorailyn_web.pdf, accessed 30th September 2014).
Russom, P., 2011. Big data analytics. TDWI Best Practices Report, Fourth Quarter 2011
(35 pp., http://tdwi.org/research/2011/12/sas_best-practices-report-q4-big-dataanalytics.aspx?tc=page0, accessed 14th October 2014).
Samarati, P., Sweeney, L., 1998. Protecting privacy when disclosing information:
k-anonymity and its enforcement through generalization and suppression. Proceedings of the IEEE Symposium on Research in Security and Privacy (S&P). May 1998,
Oakland, CA, pp. 384–393 (http://dataprivacylab.org/dataprivacy/projects/
kanonymity/paper3.pdf, accessed 16th December 2014).
Seidl, D., Jankowski, P., Tsou, M.-H., 2015. Spatial obfuscation of GPS travel data. CSU
Geospatial Review vol. 13 (http://csugis.sfsu.edu/CSU_Geospatial_Review/2015.pdf
(accessed 25th June 2015)).
SFMTA, 2013. CycleTracks for iPhone and Android. San Francisco County Transport
Authority. http://www.sfcta.org/modeling-and-travel-forecasting/cycletracksiphone-and-android (accessed 2nd September 2014).
Shearmur, R., 2015. Dazzled by data: big data, the census and urban geography. Urban
Geogr. http://dx.doi.org/10.1080/02723638.2015.1050922.
Silverman, B.W., 1986. Density Estimation for Statistics and Data Analysis. Chapman &
Hall, London.
Slocum, T.A., McMaster, R.B., Kessler, F.C., Howard, H.H., 2009. Thematic Cartography and
Geovisualisation. Prentice Hall, New Jersey, NJ.
Špakov, O., Miniotas, D., 2007. Visualization of eye gaze data using heat maps. Electron.
Electr. Eng. 2, 55–58.
ST, 2014. Introducing Sports Tracker. Sports Tracking Technologies Ltd. (http://www.
sports-tracker.com/blog/about/, accessed 20th August 2014).
Strava, 2014. About us. Strava Inc. (http://www.strava.com/about, accessed 7th August
2014).
Terrovitis, M., Mamoulis, N., 2008. Privacy preservation in the publication of trajectories.
Proceedings of the 9th International Conference on Mobile Data Management
(MDM '08), 27–30 April 2008, pp. 65–72. http://dx.doi.org/10.1109/MDM.2008.29.
Trame, J., Keßler, C., 2011. Exploring the lineage of volunteered geographic information
with heat maps. Abstracts of GeoViz 2011, Hamburg, Germany, March 10–11, 2011.
http://www.geomatik-hamburg.de/geoviz11/abstracts/28_TrameKessler_Abstract_
GeoViz2011.pdf (accessed 12th November 2014).
Usborne, S., 2013. Can cycling app Strava change the way we ride? The Independent, 4th
July 2013. http://www.independent.co.uk/life-style/gadgets-and-tech/features/cancycling-app-strava-change-the-way-we-ride-8685996.html (accessed 7th August
2014).
Vanderbilt, T., 2013. How Strava is changing the way we ride. Outside Magazine, January
2013. http://www.outsideonline.com/ﬁtness/biking/How-Strava-Is-Changing-theWay-We-Ride.html (accessed 7th August 2014).
Verykios, V.S., Damiani, M.L., Gkoulalas-Divanis, A., 2008. Privacy and security in spatiotemporal data and trajectories. In: Giannotti, F., Pedreschi, D. (Eds.), Mobility, Data
Mining and Privacy, pp. 213–240.
Willems, C.M.E., 2011. Visualization of vessel trafﬁc (PhD Thesis) Eindhoven University of
Technology.
Young, C., Martin, D., Skinner, C., 2009. Geographically intelligent disclosure control for
ﬂexible aggregation of census data. Int. J. Geogr. Inf. Sci. 23 (4), 457–482 (http://dx.
doi.org/10.1080/13658810801949835).
Zahradnik, F., 2014. Strava Cycling app with unique social and record-keeping features.
About.com. http://gps.about.com/od/sportsandﬁtness/fr/Strava-Cycling-App-Review.
htm (accessed 7th August 2014).
Zeile, P., Memmel, M., Exner, J.-P., 2012. A new urban sensing and monitoring approach:
tagging the city with the RADAR SENSING app. Proceedings REAL CORP 2012
Tagungsband 14–16 May 2012, Schwechat (http://programm.corp.at/cdrom2012/
papers2012/CORP2012_104.pdf (26th September 2014)).

A Demonstration of GeoSpark: A Cluster Computing
Framework for Processing Big Spatial Data
Jia Yu

Jinxuan Wu

Mohamed Sarwat

School of Computing, Informatics,
and Decision Systems Engineering
Arizona State University
Tempe, Arizona 85281
Email: jiayu2@asu.edu

School of Computing, Informatics,
and Decision Systems Engineering
Arizona State University
Tempe, Arizona 85281
Email: jinxuanw@asu.edu

School of Computing, Informatics,
and Decision Systems Engineering
Arizona State University
Tempe, Arizona 85281
Email: msarwat@asu.edu

Abstract—This paper demonstrates G EO S PARK a cluster
computing framework for developing and processing large-scale
spatial data analytics programs. G EO S PARK consists of three
main layers: Apache Spark Layer, Spatial RDD Layer and Spatial
Query Processing Layer. Apache Spark Layer provides basic
Apache Spark functionalities as regular RDD operations. Spatial
RDD Layer consists of three novel Spatial Resilient Distributed
Datasets (SRDDs) which extend regular Apache Spark RDD to
support geometrical and spatial objects with data partitioning
and indexing. Spatial Query Processing Layer executes spatial
queries (e.g., Spatial Join) on SRDDs. The dynamic status of
SRDDs and spatial operations are visualized by G EO S PARK monitoring map interface. We demonstrate G EO S PARK using three
spatial analytics applications (spatial aggregation, autocorrelation
and co-location) to show how users can easily define their spatial
analytics tasks and efficiently process such tasks on large-scale
spatial data at interactive performance.

I.

I NTRODUCTION

Spatial data includes but is not limited to: weather maps,
geological maps, socioeconomic data, vegetation indices, and
more. Moreover, novel technology allows hundreds of millions
of users to use their mobile devices to access their healthcare information and bank accounts, interact with friends,
buy stuff online, search interesting places to visit on-thego, ask for driving directions, and more. In consequence,
everything we do on the mobile internet leaves breadcrumbs
of spatial digital traces, e.g., geo-tagged tweets, venue checkins. Making sense of such spatial data will be beneficial for
several applications that may transform science and society
– For example: (1) Socio-Economic Analysis: that includes
for example climate change analysis, study of deforestation,
population migration, and variation in sea levels, (2) Urban
Planning: assisting government in city/regional planning, road
network design, and transportation / traffic engineering, and
(3) Commerce and Advertisement: e.g., point-of-interest (POI)
recommendation services. The aforementioned applications
need a powerful data management platform to handle the large
volume of spatial data such applications deal with. Challenges
to building such platform are as follows:
•

Challenge I: System Scalability. The massive-scale
of available spatial data hinders making sense of it using traditional spatial database management systems.
Moreover, large-scale spatial data, besides its tremendous storage footprint, may be extremely difficult to

manage and maintain. The underlying database system
must be able to digest Petabytes of spatial data and
effectively analyze it.
•

Challenge II: Fast Analytics. In spatial data analytics
applications, users will not tolerate delays introduced
by the underlying spatial database system. Instead, the
user needs to see useful information quickly. Hence,
the underlying spatial data processing system must
figure out effective ways to execute spatial analytics
in parallel.

Existing spatial database systems extend relational database
systems with new data types, functions, operators, and index
structures to handle spatial operations based on the Open
Geospatial Consortium. Even though such systems sort of
provide full support for spatial data storage and access, they
suffer from a scalability issue. Based upon a relational database
system, such systems are not scalable enough to handle largescale analytics over big spatial data. Recent works (e.g., [1],
[2]) extend the Hadoop ecosystem to perform spatial analytics
at scale. The Hadoop-based approach indeed achieves high
scalability. However, these systems though exhibit excellent
performance in batch-processing jobs, they show poor performance handling applications that require fast data analysis.
Apache Spark [3], on the other hand, is an in-memory cluster
computing system. Spark provides a novel data abstraction
called resilient distributed datasets (RDDs) [4] that are collections of objects partitioned across a cluster of machines. Each
RDD is built using parallelized transformations (filter, join or
groupBy) that could be traced back to recover the RDD data.
In memory RDDs allow Spark to outperform existing models
(MapReduce) by up to two orders of magnitude. Unfortunately,
Spark does not provide native support for spatial data and
spatial operations. Hence, users need to perform the tedious
task of programming their own spatial data processing jobs on
top of Spark.
This paper demonstrates G EO S PARK 1 [5] an in-memory
cluster computing system for processing large-scale spatial
data. G EO S PARK extends Apache Spark to support spatial data
types and operations. In other words, the system extends the
resilient distributed datasets (RDDs) concept to support spatial
data. This problem is quite challenging due to the fact that
(1) spatial data may be quite complex, e.g., rivers’ and cities’
1 GeoSpark

Github repository: https://github.com/Sarwat/GeoSpark

Spatial Query Processing Layer
Spatial
Range

Spatial
KNN

Spatial
Join

Spatial RDD (SRDD) Layer
Indexed Spatial RDD
Point
RDD

Rectangle
RDD

Polygon
RDD

Geometrical Operations Library

Fig. 2: SRDD partitioning
…

Apache Spark Layer

Fig. 1: GeoSpark architecture

geometrical boundaries, (2) spatial (and geometric) operations
(e.g., Overlap, MinimumBoundingRectangle, Union) cannot be
easily and efficiently expressed using regular RDD transformations and actions. G EO S PARK extends RDDs to form Spatial
RDDs (SRDDs) and efficiently partitions SRDD data elements
across machines and introduces novel parallelized spatial (geometric operations that follows the Open Geosptial Consortium
(OGC) [6] standard) transformations and actions (for SRDD)
that provide a more intuitive interface for users to write spatial
data analytics programs. Moreover, G EO S PARK extends the
SRDD layer to execute spatial queries (e.g., Range query,
KNN query, and Join query) on large-scale spatial datasets.
The dynamic status of SRDDs and associated queries are visualized by G EO S PARK monitoring interface throughout each
entire spatial analytics process. We demonstrate G EO S PARK
using three applications: (1) Application 1 uses G EO S PARK
to calculate geospatial autocorrelation in a spatial dataset,
(2) Application 2 leverages the system to generate a heat map
of the San-Francisco trees population, and (3) Application 3
executes a spatial co-location pattern mining with the help of
G EO S PARK .
II.

G EO S PARK

ARCHITECTURE

As depicted in Figure 1, G EO S PARK consists of three
main layers: (1) Apache Spark Layer: that consists of regular
operations that are natively supported by Apache Spark. These
native functions are responsible for loading / saving data from
/ to persistent storage (e.g., stored on local disk or Hadoop
file system HDFS). (2) Spatial Resilient Distributed Dataset
(SRDD) Layer (Section II-A). (3) Spatial Query Processing
Layer (Section II-B).
A. Spatial RDD (SRDD) Layer
This layer extends Spark with spatial RDDs (SRDDs)
that efficiently partition SRDD data elements across machines
and introduces novel parallelized spatial transformations and
actions (for SRDD) that provide a more intuitive interface for

users to write spatial data analytics programs. The SRDD layer
consists of three new RDDs: PointRDD, RectangleRDD and
PolygonRDD. One useful Geometrical operations library is
also provided for every spatial RDD.
Spatial Objects Support. G EO S PARK supports various
spatial data input format (e.g., Comma Separated Value,
Tab Separated Value and Well-Known Text). Each type of
spatial objects is stored in a SRDD, PointRDD, RectangleRDD or PolygonRDD. G EO S PARK provides a set of geometrical operations which is called Geometrical Operations
Library. This library natively supports geometrical operations. For example, Overlap(): Finds all of the internal objects which are intersected with others in geometry;
MinimumBoundingRectangle(): Finds the minimum
bounding rectangles for each object in a Spatial RDD or return
a large minimum bounding rectangle which contains all of
the internal objects in a Spatial RDD; Union(): Returns the
union polygon of all polygons in this RDD.
SRDD Partitioning. G EO S PARK automatically partitions
all loaded Spatial RDDs by creating one global grid file for
data partitioning. The main idea for assigning each element in
a Spatial RDD to the same 2-Dimensional spatial grid space
is as follows: Firstly, split the spatial space into a number of
non-equal grid cells which compose a global grid file. This
global grid file has load balanced grids according to presampling techniques. Then traverse each element in the SRDD
and assign this element to a grid cell if the element overlaps
with this grid cell. If one element intersects with two or more
grid cells, then duplicate this element and assign different grid
IDs to the copies of this element. Figure 2 depicts tweets in
the U.S. at a particular moment, tweets and states are assigned
to respective grid cells.
SRDD Indexing. Spatial indexes like Quad-Tree and RTree are provided in Spatial IndexRDDs which inherit from
Spatial RDDs. Users are able to initialize a Spatial IndexRDD.
Moreover, G EO S PARK adaptively decides whether a local
spatial index should be created for a certain Spatial IndexRDD
partition based on a tradeoff between the indexing overhead
(memory and time) on one-hand and the query selectivity as
well as the number of spatial objects on the other hand.

B. Spatial Query Processing Layer
This layer supports spatial queries (e.g., Range query and
Join query) for large-scale spatial datasets. After geometrical
objects are stored and processed in the Spatial RDD layer, user
may invoke a spatial query provided in Spatial Query Processing Layer. G EO S PARK processes such query and returns the
final results to the user. G EO S PARK execution model implements the algorithms proposed by [7] and [8]. To accelerate a
spatial query, G EO S PARK leverages the grid partitioned Spatial
RDDs, spatial indexing, the fast in-memory computation and
DAG scheduler of Apache Spark to parallelize the query
execution.
Spatial Range Query. G EO S PARK executes the spatial
range query algorithm following the execution model: Load
target dataset, partition data, create a spatial index on each
SRDD partition if necessary, broadcast the query window
to each SRDD partition, check the spatial predicate in each
partition, and remove spatial objects duplicates that existed
due to the data partitioning phase.
Spatial Join Query. G EO S PARK executes the parallel
spatial join query following the execution model. GeoSpark
first partitions the data from the two input SRDDs as well
as creates local spatial indexes (if required) for the SRDD
which is being queried. Then it joins the two datasets by their
keys which are grid IDs. For the spatial objects (from the
two SRDDs) that have the same grid ID, GeoSpark calculates
their spatial relations. If two elements from two SRDDS are
overlapped, they are kept in the final results. The algorithm
continues to group the results for each rectangle. The grouped
results are in the following format: Rectangle, Point, Point,
... Finally, the algorithm removes the duplicated points and
returns the result to other operations or saves the final result
to disk.
Spatial KNN Query. To process a Spatial KNN query,
G EO S PARK uses a heap based top-k algorithm[9], which
contains two phases: selection and merge. It takes a partitioned
SRDD, a point P and a number k as inputs. To calculate
the k nearest objects around point P , in the selection phase,
for each SRDD partition G EO S PARK calculates the distances
between each object to the given point P , then maintains
a local heap by adding or removing elements based on the
distances. This heap contains the nearest k objects around the
given point P . For IndexedSRDD, the system can utilize the
local indexes to reduce the query time. After the selection
phase, G EO S PARK merges results from each partition, keeps
the nearest k elements that have the shortest distances to P
and outputs the result.
III.

D EMONSTRATION

SCENARIOS

We demonstrate G EO S PARK using three spatial applications which are described below. G EO S PARK provides a
monitoring map interface for system users to visualize and
monitor the spatial program dynamically. A screenshot of this
tool is provided in Figure 3. The interface allows users to
execute Scala code interactively through an integrated Scale
shell. Meanwhile, a map on-top of the shell visualizes the
SRDDs generated by Scala code. Throughout the entire spatial
analytics process, all generated SRDDs are listed on the left
side pane of the user interface. When the user clicks on any

SRDD partition (if it is still alive) on the left pane, she obtains
more detailed information from a nested menu such as the
data size in this partition, physical machine IP address, CPU
and memory utilization. Besides the description of SRDDs,
the tool also provides the status of a running spatial program
in a progress bar format. By browsing G EO S PARK Monitoring
Tool, users can interactively monitor the run time of their entire
spatial analytics program.
A. Application 1: Spatial Autocorrelation
Spatial autocorrelation studies whether neighbor spatial
data points might have correlations in some non-spatial attributes. Moran’s I and Geary’s C are two common coefficients in spatial autocorrelation. Based on them, analysts can
determine whether these objects influence each other. These
efficients are defined by two specific formulas correspondingly.
An important part of these formulas is to find the spatial
adjacent matrix. In this matrix, each tuples stands for whether
two objects, such as points, rectangles or polygons, are within
a specified distance.
An application programmer may leverage G EO S PARK
SpatialJoinQuery() to calculate the spatial adjacent
matrix. Assume one dataset is composed of millions of point
objects. The process to find the global adjacent matrix in
G EO S PARK is as as follows: (1) Call G EO S PARK PointRDD
initialization method to store the dataset in memory. Data
partitioning and indexing are also completed by G EO S PARK
at this stage. (2) Call G EO S PARK SpatialJoinQuery()
in PointRDD. The first parameter is the query point set itself
and the second one is the specified distance. (3) Use a new
instance of Spatial PairRDD to store the result of Step (2).
Step (2) will return the whole point set which has a new
column specify the neighbors of each tuple within the distance.
The expected schema is like this: Point coordinates (longitude,
latitude), neighbor 1 coordinates (longitude, latitude), neighbor
2 coordinates (longitude, latitude), ... (4) Call persistence
method in G EO S PARK to persist the resulting PointRDD.
B. Application 2: Spatial Aggregation
Assume an environmental scientist – studying the relationship between air quality and trees – would like to explore the
trees population in San Francisco. A query may leverage the
SpatialRangeQuery() provided by G EO S PARK to just
return all trees in San Francisco. Alternatively, a heat map
(spatial aggregate) that shows the distribution of trees in San
Francisco may be also helpful. This spatial aggregate query
(i.e., heat map) needs to count all trees at every single region
over the map.
In the heat map case, in terms of spatial queries, the
heat map is a spatial join in which the target set is the tree
map in San Francisco and the query area set is a set of
San Francisco regions. The region number depends on the
display resolution, or granularity, in the heat map. One proper
G EO S PARK program is as follows: (3) Use a Spatial PairRDD
to store the result of Step (2) which is the count for each
polygon. The Spatial PairRDD follows the schema like this:
(Polygon, count) such that Polygon represents the boundaries
of the spatial region. (4) Call persistence method in Spark to
persist the result PolygonRDD.

Fig. 3: G EO S PARK Monitoring Tool

C. Application 3: Spatial Co-location
Spatial co-location is defined as two or more species are often located in a neighborhood relationship. The determination
of this co-location pattern may benefit many further scientific
researches. Biologists may find symbiotic relationships, mobile
carriers can provide proper plans based users’ co-location, and
advertising agencies are able to place directed advertisements
at the center of co-located populations. For instance, one
existing co-location pattern is that one kind of tigers always
live within a certain distance from one kind of rabbits. Thus
we may infer one possible fact that these tigers feed on these
rabbits.
Some co-efficients are applied to determine the co-location
relationship. Ripley’s K function [10] is the most common one
in real life. It usually executes numerous times iteratively and
finds the ideal distance. The calculation of K function also
needs the adjacent matrix between two type of objects. As we
mentioned in spatial autocorrelation analysis, seeking adjacent
matrix may leverage G EO S PARK SpatialJoinQuery().
Programmer are able to follow the same procedure depicted in
Spatial Autocorrelation.
Furthermore, spatial co-location, different from the previous basic spatial applications, is able to maximize the in memory computation goodness of G EO S PARK . Under G EO S PARK
framework, users only need to spend time on loading data,
partitioning data, and constructing indexes in the first iteration
and then G EO S PARK automatically caches these intermediate
data in memory. In the next numerous iterations, users are

able to directly keep mining the co-location pattern using the
cache in memory instead of loading and pre-processing data
from scratch.
R EFERENCES
[1] A. Aji, F. Wang, H. Vo, R. Lee, Q. Liu, X. Zhang, and J. H. Saltz,
“Hadoop-GIS: A High Performance Spatial Data Warehousing System
over MapReduce,” Proceedings of the VLDB Endowment, PVLDB,
vol. 6, no. 11, pp. 1009–1020, 2013.
[2] A. Eldawy and M. F. Mokbel, “A demonstration of spatialhadoop: An
efficient mapreduce framework for spatial data,” Proceedings of the
VLDB Endowment, PVLDB, vol. 6, no. 12, pp. 1230–1233, 2013.
[3] “Spark,” https://spark.apache.org.
[4] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauly,
M. J. Franklin, S. Shenker, and I. Stoica, “Resilient Distributed Datasets:
A Fault-Tolerant Abstraction for In-Memory Cluster Computing,” in
Proceedings of the USENIX Symposium on Networked Systems Design
and Implementation, NSDI, 2012, pp. 15–28.
[5] J. Yu, J. Wu, and M. Sarwat, “Geosaprk: A cluster computing framework for processing large scale spatial data,” in Proceedings of ACM
SIGSPATIAL GIS, 2015.
[6] “Open Geospatial Consortium,” http://www.opengeospatial.org/.
[7] G. Luo, J. F. Naughton, and C. J. Ellmann, “A non-blocking parallel
spatial join algorithm,” in Data Engineering, 2002. Proceedings. 18th
International Conference on. IEEE, 2002, pp. 697–705.
[8] X. Zhou, D. J. Abel, and D. Truffet, “Data partitioning for parallel
spatial join processing,” Geoinformatica, vol. 2, no. 2, pp. 175–204,
1998.
[9] N. Roussopoulos, S. Kelley, and F. Vincent, “Nearest neighbor queries,”
in ACM sigmod record, vol. 24, no. 2. ACM, 1995, pp. 71–79.
[10] B. D. Ripley, Spatial statistics. John Wiley & Sons, 2005, vol. 575.

1384

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

LARS*: An Efficient and Scalable
Location-Aware Recommender System
Mohamed Sarwat, Justin J. Levandoski, Ahmed Eldawy, and Mohamed F. Mokbel
Abstract—This paper proposes LARS*, a location-aware recommender system that uses location-based ratings to produce
recommendations. Traditional recommender systems do not consider spatial properties of users nor items; LARS*, on the other hand,
supports a taxonomy of three novel classes of location-based ratings, namely, spatial ratings for non-spatial items, non-spatial ratings
for spatial items, and spatial ratings for spatial items. LARS* exploits user rating locations through user partitioning, a technique that
influences recommendations with ratings spatially close to querying users in a manner that maximizes system scalability while not
sacrificing recommendation quality. LARS* exploits item locations using travel penalty, a technique that favors recommendation
candidates closer in travel distance to querying users in a way that avoids exhaustive access to all spatial items. LARS* can apply
these techniques separately, or together, depending on the type of location-based rating available. Experimental evidence using
large-scale real-world data from both the Foursquare location-based social network and the MovieLens movie recommendation
system reveals that LARS* is efficient, scalable, and capable of producing recommendations twice as accurate compared to existing
recommendation approaches.
Index Terms—Recommender system, spatial, location, performance, efficiency, scalability, social

1

I NTRODUCTION

R

ECOMMENDER systems make use of community
opinions to help users identify useful items from a
considerably large search space (e.g., Amazon inventory [1],
Netflix movies1 ). The technique used by many of these systems is collaborative filtering (CF) [2], which analyzes past
community opinions to find correlations of similar users
and items to suggest k personalized items (e.g., movies)
to a querying user u. Community opinions are expressed
through explicit ratings represented by the triple (user, rating, item) that represents a user providing a numeric rating
for an item.
Currently, myriad applications can produce location-based
ratings that embed user and/or item locations. For example, location-based social networks (e.g., Foursquare2 and
Facebook Places [3]) allow users to “check-in” at spatial
destinations (e.g., restaurants) and rate their visit, thus are
capable of associating both user and item locations with ratings. Such ratings motivate an interesting new paradigm
of location-aware recommendations, whereby the recommender system exploits the spatial aspect of ratings when

1. Netflix: http://www.netflix.com.
2. Foursquare: http://foursquare.com.
• M. Sarwat, A. Eldawy, and M. F. Mokbel are with the Department
of Computer Science and Engineering, University of Minnesota,
Minneapolis, MN 55455 USA.
E-mail: {sarwat, eldawy, mokbel}@cs.umn.edu.
• J. J. Levandoski is with the Microsoft Research, Redmond, WA 98052-6399
USA. E-mail: justin.levandoski@microsoft.com.
Manuscript received 3 May 2012; revised 27 Nov. 2012; accepted
14 Jan. 2013. Date of publication 31 Jan. 2013; date of current version
29 May 2014.
Recommended for acceptance by J. Gehrke, B.C. Ooi, and E. Pitoura.
For information on obtaining reprints of this article, please send e-mail to:
reprints@ieee.org, and reference the Digital Object Identifier below.
Digital Object Identifier 10.1109/TKDE.2013.29

producing recommendations. Existing recommendation
techniques [4] assume ratings are represented by the (user,
rating, item) triple, thus are ill-equipped to produce locationaware recommendations.
In this paper, we propose LARS*, a novel locationaware recommender system built specifically to produce
high-quality location-based recommendations in an efficient manner. LARS* produces recommendations using a
taxonomy of three types of location-based ratings within a
single framework: (1) Spatial ratings for non-spatial items,
represented as a four-tuple (user, ulocation, rating, item),
where ulocation represents a user location, for example, a
user located at home rating a book; (2) non-spatial ratings
for spatial items, represented as a four-tuple (user, rating,
item, ilocation), where ilocation represents an item location,
for example, a user with unknown location rating a restaurant; (3) spatial ratings for spatial items, represented as a
five-tuple (user, ulocation, rating, item, ilocation), for example,
a user at his/her office rating a restaurant visited for lunch.
Traditional rating triples can be classified as non-spatial
ratings for non-spatial items and do not fit this taxonomy.

1.1 Motivation: A Study of Location-Based Ratings
The motivation for our work comes from analysis of two
real-world location-based rating datasets: (1) a subset of
the well-known MovieLens dataset [5] containing approximately 87K movie ratings associated with user zip codes
(i.e., spatial ratings for non-spatial items) and (2) data from
the Foursquare [6] location-based social network containing user visit data for 1M users to 643K venues across
the United States (i.e., spatial ratings for spatial items).
In our analysis we consistently observed two interesting properties that motivate the need for location-aware
recommendation techniques.

c 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
1041-4347 
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

Fig. 1. Preference locality in location-based ratings. (a) MovieLens

preference locality. (b) Foursquare preference locality.

Preference locality. Preference locality suggests users
from a spatial region (e.g., neighborhood) prefer items
(e.g., movies, destinations) that are manifestly different
than items preferred by users from other, even adjacent,
regions. Fig. 1(a) lists the top-4 movie genres using average MovieLens ratings of users from different U.S. states.
While each list is different, the top genres from Florida
differ vastly from the others. Florida’s list contains three
genres (“Fantasy", “Animation", “Musical") not in the other
lists. This difference implies movie preferences are unique
to specific spatial regions, and confirms previous work
from the New York Times [7] that analyzed Netflix user
queues across U.S. zip codes and found similar differences. Meanwhile, Fig. 1(b) summarizes our observation
of preference locality in Foursquare by depicting the visit
destinations for users from three adjacent Minnesota cities.
Each sample exhibits diverse behavior: users from Falcon
Heights, MN favor venues in St. Paul, MN (17% of visits)
Minneapolis (13%), and Roseville, MN (10%), while users
from Robbinsdale, MN prefer venues in Brooklyn Park, MN
(32%) and Robbinsdale (20%). Preference locality suggests
that recommendations should be influenced by locationbased ratings spatially close to the user. The intuition is that
localization influences recommendation using the unique
preferences found within the spatial region containing the
user.
Travel locality. Our second observation is that, when
recommended items are spatial, users tend to travel a
limited distance when visiting these venues. We refer
to this property as “travel locality.” In our analysis of
Foursquare data, we observed that 45% of users travel
10 miles or less, while 75% travel 50 miles or less. This
observation suggests that spatial items closer in travel
distance to a user should be given precedence as recommendation candidates. In other words, a recommendation
loses efficacy the further a querying user must travel
to visit the destination. Existing recommendation techniques do not consider travel locality, thus may recommend
users destinations with burdensome travel distances (e.g.,
a user in Chicago receiving restaurant recommendations
in Seattle).

1.2

Our Contribution: LARS* - A Location-Aware
Recommender System
Like traditional recommender systems, LARS* suggests k
items personalized for a querying user u. However, LARS*
is distinct in its ability to produce location-aware recommendations using each of the three types of location-based
rating within a single framework.

1385

LARS* produces recommendations using spatial ratings
for non-spatial items, i.e., the tuple (user, ulocation, rating, item), by employing a user partitioning technique that
exploits preference locality. This technique uses an adaptive
pyramid structure to partition ratings by their user location
attribute into spatial regions of varying sizes at different
hierarchies. For a querying user located in a region R, we
apply an existing collaborative filtering technique that utilizes only the ratings located in R. The challenge, however,
is to determine whether all regions in the pyramid must
be maintained in order to balance two contradicting factors: scalability and locality. Maintaining a large number of
regions increases locality (i.e., recommendations unique to
smaller spatial regions), yet adversely affects system scalability because each region requires storage and maintenance
of a collaborative filtering data structure necessary to produce recommendations (i.e., the recommender model). The
LARS* pyramid dynamically adapts to find the right pyramid shape that balances scalability and recommendation
locality.
LARS* produces recommendations using non-spatial ratings for spatial items, i.e., the tuple (user, rating, item, ilocation), by using travel penalty, a technique that exploits travel
locality. This technique penalizes recommendation candidates the further they are in travel distance to a querying
user. The challenge here is to avoid computing the travel
distance for all spatial items to produce the list of k recommendations, as this will greatly consume system resources.
LARS* addresses this challenge by employing an efficient
query processing framework capable of terminating early
once it discovers that the list of k answers cannot be altered
by processing more recommendation candidates. To produce recommendations using spatial ratings for spatial items,
i.e., the tuple (user, ulocation, rating, item, ilocation) LARS*
employs both the user partitioning and travel penalty techniques to address the user and item locations associated
with the ratings. This is a salient feature of LARS*, as
the two techniques can be used separately, or in concert,
depending on the location-based rating type available in
the system.
We experimentally evaluate LARS* using real locationbased ratings from Foursquare [6] and MovieLens [5], along
with a generated user workload of both snapshot and continuous queries. Our experiments show LARS* is scalable to
real large-scale recommendation scenarios. Since we have
access to real data, we also evaluate recommendation quality by building LARS* with 80% of the spatial ratings and
testing recommendation accuracy with the remaining 20%
of the (withheld) ratings. We find LARS* produces recommendations that are twice as accurate (i.e., able to better
predict user preferences) compared to traditional collaborative filtering. In summary, the contributions of this paper
are as follows:
•

•

We provide a novel classification of three types
of location-based ratings not supported by existing
recommender systems: spatial ratings for non-spatial
items, non-spatial ratings for spatial items, and spatial
ratings for spatial items.
We propose LARS*, a novel location-aware recommender system capable of using three classes of

1386

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

Fig. 3. Item-based similarity calculation.

(a)

(b)

Fig. 2. Item-based CF model generation. (a) Ratings matrix.

(b) Item-based CF model.

location-based ratings. Within LARS*, we propose:
(a) a user partitioning technique that exploits user
locations in a way that maximizes system scalability while not sacrificing recommendation locality and
(b) a travel penalty technique that exploits item locations and avoids exhaustively processing all spatial
recommendation candidates.
•
LARS* distinguishes itself from LARS [8] in the following points: (1) LARS* achieves higher locality
gain than LARS using a better user partitioning data
structure and algorithm. (2) LARS* exhibits a more
flexible tradeoff between locality and scalability. (3)
LARS* provides a more efficient way to maintain
the user partitioning structure, as opposed to LARS
expensive operations.
•
We provide experimental evidence that LARS* scales
to large-scale recommendation scenarios and provides better quality recommendations than traditional approaches.
This paper is organized as follows: Section 2 gives an
overview of LARS*. Sections 4, 5, and 6 cover LARS* recommendation techniques using spatial ratings for non-spatial
items, non-spatial ratings for spatial items, and spatial ratings
for spatial items, respectively. Section 7 provides experimental analysis. Section 8 covers related work, while Section 9
concludes the paper.

2

LARS* OVERVIEW

This section provides an overview of LARS* by discussing
the query model and the collaborative filtering method.

2.1 LARS* Query Model
Users (or applications) provide LARS* with a user id U,
numeric limit K, and location L; LARS* then returns K
recommended items to the user. LARS* supports both snapshot (i.e., one-time) queries and continuous queries, whereby
a user subscribes to LARS* and receives recommendation
updates as her location changes. The technique LARS*
uses to produce recommendations depends on the type of
location-based rating available in the system. Query processing support for each type of location-based rating is
discussed in Sections 4 to 6.
2.2 Item-Based Collaborative Filtering
LARS* uses item-based collaborative filtering (abbr. CF)
as its primary recommendation technique, chosen due to
its popularity and widespread adoption in commercial
systems (e.g., Amazon [1]). Collaborative filtering (CF)
assumes a set of n users U = {u1 , . . . , un } and a set of m

items I = {i1 , . . . , im }. Each user uj expresses opinions about
a set of items Iuj ⊆ I . Opinions can be a numeric rating
(e.g., the Netflix scale of one to five stars), or unary (e.g.,
Facebook “check-ins" [3]). Conceptually, ratings are represented as a matrix with users and items as dimensions, as
depicted in Fig. 2(a). Given a querying user u, CF produces
a set of k recommended items Ir ⊂ I that u is predicted to
like the most.
Phase I: Model Building. This phase computes a similarity score sim(ip ,iq ) for each pair of objects ip and iq
that have at least one common rating by the same user
(i.e., co-rated dimensions). Similarity computation is covered below. Using these scores, a model is built that stores
for each item i ∈ I , a list L of similar items ordered by a
similarity score sim(ip ,iq ), as depicted in Fig. 2(b). Building
2
this model is an O( RU ) process [1], where R and U are the
number of ratings and users, respectively. It is common to
truncate the model by storing, for each list L, only the n
most similar items with the highest similarity scores [9].
The value of n is referred to as the model size and is usually
much less than |I |.
Phase II: Recommendation Generation. Given a querying user u, recommendations are produced by computing
u’s predicted rating P(u,i) for each item i not rated by u [9]:

l∈L sim(i, l) ∗ ru,l
P(u,i) = 
(1)
l∈L |sim(i, l)|
Before this computation, we reduce each similarity list L to
contain only items rated by user u. The prediction is the sum
of ru,l , a user u’s rating for a related item l ∈ L weighted by
sim(i,l), the similarity of l to candidate item i, then normalized by the sum of similarity scores between i and l. The
user receives as recommendations the top-k items ranked
by P(u,i) .
Computing Similarity. To compute sim(ip , iq ), we represent each item as a vector in the user-rating space of
the rating matrix. For instance, Fig. 3 depicts vectors for
items ip and iq from the matrix in Fig. 2(a). Many similarity functions have been proposed (e.g., Pearson Correlation,
Cosine); we use the Cosine similarity in LARS* due to its
popularity:
sim(ip , iq ) =

ip · iq
ip iq 

(2)

This score is calculated using the vectors’ co-rated dimensions, e.g., the Cosine similarity between ip and iq in
Fig. 3 is .7 calculated using the circled co-rated dimensions.
Cosine distance is useful for numeric ratings (e.g., on a scale
[1,5]). For unary ratings, other similarity functions are used
(e.g., absolute sum [10]).
While we opt to use item-based CF in this paper, no
factors disqualify us from employing other recommendation techniques. For instance, we could easily employ

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

1387

Fig. 5. Example of Items Ratings Statistics Table.

Fig. 4. Pyramid data structure.

user-based CF [4], that uses correlations between users
(instead of items).

3

N ON -S PATIAL U SER R ATINGS FOR
N ON -S PATIAL I TEMS

The traditional item-based collaborative filtering (CF)
method is a special case of LARS*. CF takes as input the
classical rating triplet (user, rating, item) such that neither
the user location nor the item location are specified. In
such case, LARS* directly employs the traditional model
building phase (Phase-I in section 2) to calculate the similarity scores between all items. Moreover, recommendations
are produced to the users using the recommendation generation phase (Phase-II in section 2). During the rest of
the paper, we explain how LARS* incorporates either the
user spatial location or the item spatial location to serve
location-aware recommendations to the system users.

4

S PATIAL U SER R ATINGS FOR N ON -S PATIAL
I TEMS

This section describes how LARS* produces recommendations using spatial ratings for non-spatial items represented
by the tuple (user, ulocation, rating, item). The idea is to
exploit preference locality, i.e., the observation that user opinions are spatially unique (based on analysis in Section 1.1).
We identify three requirements for producing recommendations using spatial ratings for non-spatial items: (1) Locality:
recommendations should be influenced by those ratings
with user locations spatially close to the querying user
location (i.e., in a spatial neighborhood); (2) Scalability: the
recommendation procedure and data structure should scale
up to large number of users; (3) Influence: system users
should have the ability to control the size of the spatial
neighborhood (e.g., city block, zip code, or county) that
influences their recommendations.
LARS* achieves its requirements by employing a user
partitioning technique that maintains an adaptive pyramid
structure, where the shape of the adaptive pyramid is
driven by the three goals of locality, scalability, and influence.
The idea is to adaptively partition the rating tuples (user,
ulocation, rating, item) into spatial regions based on the ulocation attribute. Then, LARS* produces recommendations

using any existing collaborative filtering method (we use
item-based CF) over the remaining three attributes (user,
rating, item) of only the ratings within the spatial region
containing the querying user. We note that ratings can come
from users with varying tastes, and that our method only
forces collaborative filtering to produce personalized user
recommendations based only on ratings restricted to a specific spatial region. In this section, we describe the pyramid
structure in Section 4.1, query processing in Section 4.2, and
finally data structure maintenance in Section 4.3.

4.1 Data Structure
LARS* employs a partial in-memory pyramid structure [11]
(equivalent to a partial quad-tree [12]) as depicted in Fig. 4.
The pyramid decomposes the space into H levels (i.e., pyramid height). For a given level h, the space is partitioned
into 4h equal area grid cells. For example, at the pyramid
root (level 0), one grid cell represents the entire geographic
area, level 1 partitions space into four equi-area cells, and
so forth. We represent each cell with a unique identifier cid.
A rating may belong to up to H pyramid cells: one
per each pyramid level starting from the lowest maintained grid cell containing the embedded user location
up to the root level. To provide a tradeoff between recommendation locality and system scalability, the pyramid
data structure maintains three types of cells (see Fig. 4):
(1) Recommendation Model Cell (α-Cell), (2) Statistics Cell
(β-Cell), and (3) Empty Cell (γ -Cell), explained as follows:
Recommendation Model Cell (α-Cell). Each α-Cell
stores an item-based collaborative filtering model built
using only the spatial ratings with user locations contained
in the cell’s spatial region. Note that the root cell (level 0)
of the pyramid is an α-Cell and represents a “traditional"
(i.e., non-spatial) item-based collaborative filtering model.
Moreover, each α-Cell maintains statistics about all the ratings located within the spatial extents of the cell. Each
α-Cell Cp maintains a hash table that indexes all items (by
their IDs) that have been rated in this cell, named Items
Ratings Statistics Table. For each indexed item i in the Items
Ratings Statistics Table, we maintain four parameters; each
parameter represents the number of user ratings to item i in
each of the four children cells (i.e., C1 , C2 , C3 , and C4 ) of cell
Cp . An example of the maintained parameters is given in
Fig. 5. Assume that cell Cp contains ratings for three items
I1 , I2 , and I3 . Fig. 5 shows the maintained statistics for each
item in cell Cp . For example, for item I1 , the number of user
ratings located in child cell C1 , C2 , C3 , and C4 is equal to
109, 3200, 14, and 54, respectively. Similarly, the number of
user ratings is calculated for items I2 and I3 .
Statistics Cell (β-Cell). Like an α-Cell, a β-Cell maintains statistics (i.e., items ratings Statistics Table) about the
user/item ratings that are located within the spatial range

1388

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

of the cell. The only difference between an α-Cell and a
β-Cell is that a β-Cell does not maintain a collaborative
filtering (CF) model for the user/item ratings lying in its
boundaries. In consequence, a β-Cell is a light weight cell
such that it incurs less storage than an α-Cell. In favor of
system scalability, LARS* prefers a β-Cell over an α-Cell to
reduce the total system storage.
Empty Cell (γ -Cell). a γ -Cell is a cell that maintains
neither the statistics nor the recommendation model for the
ratings lying within its boundaries. a γ -Cell is the most
light weight cell among all cell types as it almost incurs
no storage overhead. Note that an α-Cell can have α-Cells,
β-Cells, or γ -Cells children. Also, a β-Cell can have α-Cells,
β-Cells, or γ -Cells children. However, a γ -Cell cannot have
any children.

4.1.1 Pyramid Structure Intuition
An α-Cell requires the highest storage and maintenance
overhead because it maintains a CF model as well as the
user/item ratings statistics. On the other hand, an α-Cell (as
opposed to β-Cell and γ -Cell) is the only cell that can be
leveraged to answer recommendation queries. A pyramid
structure that only contains α-Cells achieves the highest
recommendation locality, and this is why an α-Cell is considered the highly ranked cell type in LARS*. a β-Cell is
the secondly ranked cell type as it only maintains statistics
about the user/item ratings. The storage and maintenance
overhead incurred by a β-Cell is less expensive than an
α-Cell. The statistics maintained at a β-Cell determines
whether the children of that cell need to be maintained as
α-Cells to serve more localized recommendation. Finally, a
γ -Cell (lowest ranked cell type) has the least maintenance
cost, as neither a CF model nor statistics are maintained for
that cell. Moreover, a γ -Cell is a leaf cell in the pyramid.
LARS* upgrades (downgrades) a cell to a higher (lower)
cell rank, based on trade-offs between recommendation
locality and system scalability (discussed in Section 4.3). If
recommendation locality is preferred over scalability, more
α-Cells are maintained in the pyramid. On the other hand, if
scalability is favored over locality, more γ -Cells exist in the
pyramid. β-Cells comes as an intermediary stage between
α-Cells and γ -Cells to further increase the recommendation
locality whereas the system scalability is not quite affected.
We chose to employ a pyramid as it is a “spacepartitioning" structure that is guaranteed to completely
cover a given space. For our purposes, “data-partitioning"
structures (e.g., R-trees) are less ideal than a “spacepartitioning" structure for two main reasons: (1) “datapartitioning" structures index data points, and hence covers
only locations that are inserted in them. In other words,
“data-partitioning" structures are not guaranteed to completely cover a given space, which is not suitable for
queries issued in arbitrary spatial locations. (2) In contrast to “data-partitioning" structures (e.g., R-trees [13]),
“space partitioning" structures show better performance for
dynamic memory resident data [14]–[16].
4.1.2 LARS* versus LARS
Table 1 compares LARS* against LARS. Like LARS*,
LARS [8] employs a partial pyramid data structure to
support spatial user ratings for non-spatial items. LARS

TABLE 1
Comparison between LARS and LARS*

Detailed experimental evaluation results are provided in Section 7.

is different from LARS* in the following aspects: (1) As
shown in Table 1, LARS* maintains α-Cells, β-Cells, and γ Cells, whereas LARS only maintains α-Cells and γ -Cells. In
other words, LARS either merges or splits a pyramid cell
based on a tradeoff between scalability and recommendation locality. LARS* employs the same tradeoff and further
increases the recommendation locality by allowing for more
α-Cells to be maintained at lower pyramid levels. (2) As
opposed to LARS, LARS* does not perform a speculative
splitting operation to decide whether to maintain more
localized CF models. Instead, LARS* maintains extra statistics at each α-Cell and β-Cell that helps in quickly deciding
wether a CF model needs to be maintained at a child cell.
(3) As it turns out from Table 1, LARS* achieves higher recommendation locality than LARS. That is due to the fact
that LARS maintains a CF recommendation model in a cell
at pyramid level h if and only if a CF model, at its parent
cell at level h − 1, is also maintained. However, LARS* may
maintain an α-Cell at level h even though its parent cell,
at level h − 1, does not maintain a CF model, i.e., the parent cell is a β-Cell. In LARS*, the role of a β-Cell is to keep
the user/item ratings statistics that are used to quickly decide
whether the child cells needs to be γ -Cells or α-Cells. (4) As
given in Table 1, LARS* incurs more storage overhead than
LARS which is explained by the fact that LARS* maintains additional type of cell, i.e., β-Cells, whereas LARS
only maintains α-Cells and γ -Cells. In addition, LARS*
may also maintain more α-Cells than LARS does in order
to increase the recommendation locality. (5) Even though
LARS* may maintain more α-Cells than LARS besides the
extra statistics maintained at β-Cells, nonetheless LARS*
incurs less maintenance cost. That is due to the fact that
LARS* also reduces the maintenance overhead by avoiding
the expensive speculative splitting operation employed by
LARS maintenance algorithm. Instead, LARS* employs the
user/item ratings statistics maintained at either a β-Cell or an
α-Cell to quickly decide whether the cell children need to
maintain a CF model (i.e., upgraded to α-Cells), just needs
to maintain the statistics (i.e., become β-Cells), or perhaps
downgraded to γ -Cells.

4.2 Query Processing
Given a recommendation query (as described in
Section 2.1) with user location L and a limit K, LARS*
performs two query processing steps: (1) The user location

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

1389

L is used to find the lowest maintained α-Cell C in the
adaptive pyramid that contains L. This is done by hashing
the user location to retrieve the cell at the lowest level of
the pyramid. If an α-Cell is not maintained at the lowest
level, we return the nearest maintained ancestor α-Cell.
(2) The top-k recommended items are generated using the
item-based collaborative filtering technique (covered in
Section 2.2) using the model stored at C. As mentioned
earlier, the model in C is built using only the spatial ratings
associated with user locations within C.
In addition to traditional recommendation queries (i.e.,
snapshot queries), LARS* also supports continuous queries
and can account for the influence requirement as follows.
Continuous queries. LARS* evaluates a continuous
query in full once it is issued, and sends recommendations back to a user U as an initial answer. LARS* then
monitors the movement of U using her location updates.
As long as U does not cross the boundary of her current grid cell, LARS* does nothing as the initial answer is
still valid. Once U crosses a cell boundary, LARS* reevaluates the recommendation query for the new cell only if
the new cell is an α-Cell. In case the new cell is an αCell, LARS* only sends incremental updates [16] to the last
reported answer. Like snapshot queries, if a cell at level h is
not maintained, the query is temporarily transferred higher
in the pyramid to the nearest maintained ancestor α-Cell.
Note that since higher-level cells maintain larger spatial
regions, the continuous query will cross spatial boundaries less often, reducing the amount of recommendation
updates.
Influence level. LARS* addresses the influence requirement by allowing querying users to specify an optional
influence level (in addition to location L and limit K)
that controls the size of the spatial neighborhood used
to influence their recommendations. An influence level I
maps to a pyramid level and acts much like a “zoom"
level in Google or Bing maps (e.g., city block, neighborhood, entire city). The level I instructs LARS* to process the recommendation query starting from the grid
α-Cell containing the querying user location at level
I, instead of the lowest maintained grid α-Cell (the
default). An influence level of zero forces LARS* to use
the root cell of the pyramid, and thus act as a traditional (non-spatial) collaborative filtering recommender
system.

4.4 Main Idea
As time goes by, new users, ratings, and items will be added
to the system. This new data will both increase the size of
the collaborative filtering models maintained in the pyramid cells, as well as alter recommendations produced from
each cell. To account for these changes, LARS* performs
maintenance on a cell-by-cell basis. Maintenance is triggered for a cell C once it receives N% new ratings; the
percentage is computed from the number of existing ratings
in C. We do this because an appealing quality of collaborative filtering is that as a model matures (i.e., more data
is used to build the model), more updates are needed to
significantly change the top-k recommendations produced
from it [17]. Thus, maintenance is needed less often.
We note the following features of pyramid maintenance:
(1) Maintenance can be performed completely offline, i.e.,
LARS* can continue to produce recommendations using
the "old" pyramid cells while part of the pyramid is being
updated; (2) maintenance does not entail rebuilding the
whole pyramid at once, instead, only one cell is rebuilt at
a time; (3) maintenance is performed only after N% new
ratings are added to a pyramid cell, meaning maintenance
will be amortized over many operations.

4.3 Data Structure Maintenance
This section describes building and maintaining the pyramid data structure. Initially, to build the pyramid, all
location-based ratings currently in the system are used
to build a complete pyramid of height H, such that all
cells in all H levels are α-Cells and contain ratings statistics and a collaborative filtering model. The initial height
H is chosen according to the level of locality desired,
where the cells in the lowest pyramid level represent the
most localized regions. After this initial build, we invoke
a cell type maintenance step that scans all cells starting
from the lowest level h and downgrades cell types to
either (β-Cell or γ -Cell) if necessary (cell type switching is discussed in Section 4.5.2). We note that while
the original partial pyramid [11] was concerned with

4.5 Maintenance Algorithm
Algorithm 1 provides the pseudocode for the LARS* maintenance algorithm. The algorithm takes as input a pyramid cell C and level h, and includes three main steps:
Statistics Maintenance, Model Rebuild and Cell Child Quadrant
Maintenance, explained below.
Step I: Statistics Maintenance. The first step (line 4) is
to maintain the Items Ratings Statistics Table. The maintained
statistics are necessary for cell type switching decision,
especially when new location-based ratings enter the system. As the items ratings statistics table is implemented using
a hash table, then it can be queried and maintained in O(1)
time, requiring O(|IC |) space such that IC is the set of all
items rated at cell C and |IC | is the total number of items
in IC .

Algorithm 1 Pyramid maintenance algorithm
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:

/* Called after cell C receives N% new ratings */
Function PyramidMaintenance(Cell C, Level h)
/* Step I: Statistics Maintenance*/
Maintain cell C statistics
/*Step II: Model Rebuild */
if (Cell C is an α -Cell) then
Rebuild item-based collaborative filtering model for cell C
end if
/*Step III: Cell Child Quadrant Maintenance */
if (C children quadrant q cells are α -Cells) then
CheckDownGradeToSCells(q,C) /* covered in Section 4.5.2 */
else if (C children quadrant q cells are γ -Cells) then
CheckUpGradeToSCells(q,C)
else
isSwitchedToMcells ← CheckUpGradeToMCells(q,C) /* covered in
Section 4.5.3 */
if (isSwitchedToMcells is False) then
CheckDownGradeToECells(q,C)
end if
end if
return

spatial queries over static data, it did not address pyramid
maintenance.

1390

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

Step II: Model Rebuild. The second step is to rebuild the
item-based collaborative filtering (CF) model for a cell C, as
described in Section 2.2 (line 7). The model is rebuilt at cell
C only if cell C is an α-Cell, otherwise (β-Cell or γ -Cell)
no CF recommendation model is maintained, and hence
the model rebuild step does not apply. Rebuilding the CF
model is necessary to allow the model to “evolve" as new
location-based ratings enter the system (e.g., accounting for
new items, ratings, or users). Given the cost of building the
2
item-based CF model is O( RU ) (per Section 2.2), the cost
of the model rebuild for a cell C at level h is

(R/4h )2
(U/4h )

(a)

R2

= 4h U ,

assuming ratings and users are uniformly distributed.
Step III: Cell Child Quadrant Maintenance. LARS*
invokes a maintenance step that may decide whether cell
C child quadrant need to be switched to a different cell
type based on trade-offs between scalability and locality.
The algorithm first checks if cell C child quadrant q at
level h + 1 is of type α-Cell (line 10). If that case holds,
LARS* considers quadrant q cells as candidates to be downgraded to β-Cells (calling function CheckDownGradeToSCells
on line 11). We provide details of the Downgrade α-Cells to
β-Cells operation in Section 4.5.2. On the other hand, if C
have a child quadrant of type γ -Cells at level h+1 (line 12),
LARS* considers upgrading cell C four children cells at
level h + 1 to β-Cells (calling function CheckUpGradeToSCells
on line 13). The Upgrade to β-Cells operation is covered in
Section 4.5.4. However, if C has a child quadrant of type βCells at level h+1 (line 12), LARS* first considers upgrading
cell C four children cells at level h + 1 from β-Cells to αCells (calling function CheckUpGradeToMCells on line 15). If
the children cells are not switched to α-Cells, LARS* then
considers downgrading them to γ -Cells (calling function
CheckDownGradeToECells on line 17). Cell Type switching
operations are performed completely in quadrants (i.e., four
equi-area cells with the same parent). We made this decision
for simplicity in maintaining the partial pyramid.

4.5.1 Recommendation Locality
In this section, we explain the notion of locality in recommendation that is essential to understand the cell type
switching (upgrade/downgrade) operations highlighted in
the PyramidMaintenance algorithm (algorithm 1). We use
the following example to give the intuition behind recommendation locality.
Running Example. Fig. 6 depicts a two-levels pyramid
in which Cp is the root cell and its children cells are C1 , C2 ,
C3 , and C4 . In the example, we assume eight users (U1 , U2 ,
. . . , and U8 ) have rated eight different items (I1 , I2 , . . . , and
I8 ). Fig. 6(b) gives the spatial distributions of users U1 , U2 ,
U3 , U4 , U5 , U6 , U7 , and U8 as well as the items that each
user rated.
Intuition. Consider the example given in Fig. 6. In cell
Cp , users U2 and U5 that belongs to the child cell C2 have
both rated items I2 and I5 . In that case, the similarity score
between items I2 and I5 in the item-based collaborative filtering CF model built at cell C2 is exactly the same as the
one in the CF model built at cell Cp . The last phenomenon
happened because items (i.e., I2 and I5 ) have been rated
by mostly users located in the same child cell, and hence
the recommendation model at the parent cell will not be

(b)

(c)
Fig. 6. Item ratings spatial distribution example. (a) Two-levels
pyramid. (b) Ratings distribution and recommendation models.
(c) Locality loss/gain at C_p.

different from the model at the children cells. In this case,
if the CF model at C2 is not maintained, LARS* does not
lose recommendation locality at all.
The opposite case happens when an item is rated by
users located in different pyramid cells (spatially skewed).
For example, item I4 is rated by users U2 , U4 , and U7 in
three different cells (C2 , C3 , and C4 ). In this case, U2 , U4 ,
and U7 are spatially skewed. Hence, the similarity score
between item I4 and other items at the children cells is different from the similarity score calculated at the parent cell
Cp because not all users that have rated item I4 exist in the
same child cell. Based on that, we observe the following:

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

TABLE 2
Summary of Mathematical Notations

Definition 2. Locality Loss (LGc )
LGc is defined as the total locality lost by downgrading cell c
four children cells to β-Cells (0 ≤ LGc ≤ 1). It is calculated
as the sum of all items locality loss normalized by the total
number of items |Ic | in cell c.

i∈Ic LGc,i
.
(4)
LGc =
|Ic |

Observation 1. The more the user/item ratings in a parent cell
C are geographically skewed, the higher the locality gained
from building the item-based CF model at the four children
cells.
The amount of locality gained/lost by maintaining the
child cells of a given pyramid cell depends on whether
the CF models at the child cells are similar to the CF
model built at the parent cell. In other words, LARS*
loses locality if the child cells are not maintained even
though the CF model at these cells produce different recommendations than the CF model at the parent cell. LARS*
leverages Observation 1 to determine the amount of locality
gained/lost due to maintaining an item-based CF model at
the four children. LARS* calculates the locality loss/gain as
follows:
Locality Loss/Gain. Table 2 gives the main mathematical notions used in calculating the recommendation locality
loss/gain. First, the Item Ratings Pairs Set (RPc,i ) is defined
as the set of all possible pairs of users that rated item i
in cell c. For example, in Fig. 6(c) the item ratings pairs
set for item I7 in cell Cp (RPCp ,I7 ) has three elements
(i.e., RPCp ,I7 ={
U3 , U6 ,
U3 , U7 ,
U6 , U7 }) as only users U1
and U7 have rated item I1 . Similarly, RPCp ,I2 is equal to
{
U6 , U7 } (i.e., Users U2 and U5 have rated item I2 ).
For each item, we define the Skewed Item Ratings Set
(RSc,i ) as the total number of user pairs in cell c that rated
item i such that each pair of users ∈ RSc,i do not exist
in the same child cell of c. For example, in Fig. 6(c), the
skewed item ratings set for item I2 in cell Cp (RSCP ,I2 ) is ∅
as all users that rated I2 , i.e., U2 and U5 are collocated in
the same child cell C2 . For I4 , the skewed item ratings set
RSCP ,I2 ={
U2 , U7 , 
U2 , U4 , 
U4 , U7 } as all users that rated
item I2 are located in different child cells,i.e., U2 at C2 , U4
at C4 , and U7 at C3 .
Given the aforementioned parameters, we calculate Item
Locality Loss (LGc,i ) for each item, as follows:
Definition 1. Item Locality Loss (LGc,i )
LGc,i is defined as the degree of locality lost for item i from
downgrading the four children of cell c to β-Cells, such that
0 ≤ LGc,i ≤ 1.
LGc,i =

|RSc,i |
.
|RPc,i |

1391

(3)

The value of both |RSc,i | and |RPc,i | can be easily extracted
using the items ratings statistics table. Then, we use the LGc,i
values calculated for all items in cell c in order to calculate
the overall Cell Locality loss (LGc ) from downgrading the
children cells of c to α-Cells.

The cell locality loss (or gain) is harnessed by LARS*
to determine whether the cell children need to be downgraded from α-Cell to β-Cell rank, upgraded from the
γ -Cell to β-Cell rank, or downgraded from β-Cell to γ -Cell
rank. During the rest of section 4, we explain the cell rank
upgrade/downgrade operations.

4.5.2 Downgrade α-Cells to β-Cells
That operation entails downgrading an entire quadrant of
cells from α-Cells to β-Cells at level h with a common parent at level h − 1. Downgrading α-Cells to β-Cells improves
scalability (i.e., storage and computational overhead) of
LARS*, as it reduces storage by discarding the item-based
collaborative filtering (CF) models of the the four children cells. Furthermore, downgrading α-Cells to β-Cells
leads to the following performance improvements: (a) less
maintenance cost, since less CF models are periodically
rebuilt, and (b) less continuous query processing computation,
as β-Cells do not maintain a CF model and if many β-Cells
cover a large spatial region, hence, for users crossing β-Cells
boundaries, we do not need to update the recommendation
query answer. Downgrading children cells from α-Cells to
β-Cells might hurt recommendation locality, since no CF
models are maintained at the granularity of the child cells
anymore.
At cell Cp , in order to determine whether to downgrade a quadrant q cells to β-Cells (i.e., function
CheckDownGradeToSCells on line 11 in Algorithm 1), we
calculate two percentage values: (1) locality_loss (see
equation 4), the amount of locality lost by (potentially)
downgrading the children cells to β-Cells, and (2) scalability_gain, the amount of scalability gained by (potentially)
downgrading the children cells to β-Cells. Details of calculating these percentages are covered next. When deciding
to downgrade cells to β-Cells, we define a system parameter M, a real number in the range [0,1] that defines a
tradeoff between scalability gain and locality loss. LARS*
downgrades a quadrant q cells to β-Cells (i.e., discards
quadrant q) if:
(1 − M) ∗ scalability_gain > M ∗ locality_loss.

(5)

A smaller M value implies gaining scalability is important and the system is willing to lose a large amount of
locality for small gains in scalability. Conversely, a larger
M value implies scalability is not a concern, and the
amount of locality lost must be small in order to allow
for β-Cells downgrade. At the extremes, setting M=0 (i.e.,
always switch to β-Cell) implies LARS* will function as
a traditional CF recommender system, while setting M=1
causes LARS* pyramid cells to all be α-Cells, i.e., LARS*
will employ a complete pyramid structure maintaining a
recommendation model at all cells at all levels.

1392

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

Calculating Locality Loss. To calculate the locality loss
at a cell Cp , LARS* leverages the Item Ratings Statistics Table
maintained in that cell. First, LARS* calculates the item
locality loss LGCp ,i for each item i in the cell Cp . Therefore,
LARS* aggregates the item locality loss values calculated
for each item i ∈ Cp , to finally deduce the global cell locality
loss LGCp .
Calculating scalability gain. Scalability gain is measured in storage and computation savings. We measure
scalability gain by summing the recommendation model
sizes for each of the downgraded (i.e., child) cells (abbr.
sizem ), and divide this value by the sum of sizem and the recommendation model size of the parent cell. We refer to this
percentage as the storage_gain. We also quantify computation
savings using storage gain as a surrogate measurement, as
computation is considered a direct function of the amount
of data in the system.
Cost. using the Items Ratings Statistics Table maintained
at cell Cp , the locality loss at cell Cp can be calculated in
O(|ICp |) time such that |ICp | represents the total number of
items in Cp . As scalability gain can be calculated in O(1)
time, then the total time cost of the Downgrade To β-Cells
operation is O(|ICp |).
Example. For the example given in Fig. 6(c), the locality loss of downgrading cell Cp four children cells
{C1 , C2 , C3 , C4 } to β-Cells is calculated as follows: First,
we retrieve the locality loss LGCp ,i for each item i ∈
{I1 , I2 , I3 , I4 , I5 , I6 , I7 , I8 }, from the maintained statistics at
cell Cp . As given in Fig. 6(c), LGCp ,I1 , LGCp ,I2 , LGCp ,I3 ,
LGCp ,I4 , LGCp ,I5 , LGCp ,I6 , LGCp ,I7 , and LGCp ,I8 are equal to 0.0,
0.0, 1.0, 1.0, 0.0, 0.666, 0.166, and 1.0, respectively. Then, we
calculate the overall locality loss at Cp (using equation 4),
LGCp by summing all the locality loss values of all items
and dividing the sum by the total number of items. Hence,
the scalability loss is equal to ( 0.0+1.0+1.0+0.0+0.666+0.1666+1.0
)
8
= 0.48 = 48%. To calculate scalability gain, assume the sum
of the model sizes for cells C1 to C4 and CP is 4GB, and
the sum of the model sizes for cells C1 to C4 is 2GB.
Then, the scalability gain is 24 =50%. Assuming M=0.7, then
(0.3 × 50) < (0.7 × 48), meaning that LARS* will not
downgrade cells C1 , C2 , C3 , C4 to β-Cells.

4.5.3 Upgrade β-Cells to α-Cells
Upgrading β-Cells to α-Cells operation entails upgrading the
cell type of a cell child quadrant at pyramid level h under
a cell at level h − 1, to α-Cells. Upgrading β-Cells to α-Cells
operation improves locality in LARS*, as it leads to maintaining a CF model at the children cells that represent
more granular spatial regions capable of producing recommendations unique to the smaller, more “local", spatial
regions. On the other hand, upgrading cells to α-Cells hurts
scalability by requiring storage and maintenance of more
item-based collaborative filtering models. The upgrade to
α-Cells operation also negatively affects continuous query
processing, since it creates more granular α-Cells causing user locations to cross α-Cell boundaries more often,
triggering recommendation updates.
To determine whether to upgrade a cell CP (quadrant q) four children cells to α-Cells (i.e., function
CheckUpGradeToMCells on line 15 of Algorithm 1). Two percentages are calculated: locality_gain and scalability_loss.

These values are the opposite of those calculated for the
Upgrade to β-Cells operation. LARS* change cell CP child
quadrant q to α-Cells only if the following condition holds:
M ∗ locality_gain > (1 − M) ∗ scalability_loss.

(6)

This equation represents the opposite criteria of that presented for Upgrade to β-Cells operation in Equation 5.
Calculating locality gain. To calculate the locality gain,
LARS* does not need to speculatively build the CF model
at the four children cells. The locality gain is calculated the
same way the locality loss is calculated in equation 4.
Calculating scalability loss. We calculate scalability loss
by estimating the storage necessary to maintain the children cells. Recall from Section 2.2 that the maximum size
of an item-based CF model is approximately n|I|, where n
is the model size. We can multiply n|I| by the number of
bytes needed to store an item in a CF model to find an
upper-bound storage size of each potentially Upgradeded to
α-Cell cell. The sum of these four estimated sizes (abbr. sizes )
divided by the sum of the size of the existing parent cell
and sizes represents the scalability loss metric.
Cost. Similar to the CheckDownGradeToSCells operation,
scalability loss is calculated in O(1) and locality gain can
be calculated in O(|ICp |) time. Then, the total time cost of
the CheckUpGradeToMCells operation is O(|ICp |).
Example. Consider the example given in Fig. 6(c).
Assume the cell Cp is an α-Cell and its four children C1 ,
C2 , C3 , and C4 are β-Cells. The locality gain (LGCp ) is calculated using equation 4 to be 0.48 (i.e., 48%) as depicted in
the table in Fig. 6(c). Further, assume that we estimate the
extra storage overhead for upgradinging the children cells
to α-Cells (i.e., storage loss) to be 50%. Assuming M=0.7,
then (0.7 × 48) > (0.3 × 50), meaning that LARS* will
decide to upgrade CP four children cells to α-Cells as locality
gain is significantly higher than scalability loss.

4.5.4 Downgrade β-Cells to γ -Cells and Vice Versa
Downgrading β-Cells to γ -Cells operation entails downgrading the cell type of a cell child quadrant at pyramid level h
under a cell at level h − 1, to γ -Cells (i.e., empty cells).
Downgrading the child quadrant type to γ -Cells means
that the maintained statistics are no more maintained in
the children cell, which definitely reduces the overhead of
maintaining the Item Ratings Statistics Table at these cells.
Even though γ -Cells incurs no maintenance overhead, however they reduce the amount of recommendation locality
that LARS* provides.
The decision of downgrading from β-Cells to γ -Cells is
taken based on a system parameter, named MAX_SLEVELS.
It is defined as the maximum number of consecutive
pyramid levels in which descendant cells can be β-Cells.
MAX_SLEVELS can take any value between zero and the
total height of the pyramid. A high value of MAX_SLEVELS
results in maintaining more β-Cells and less γ -Cells in
the pyramid. For example, in Fig. 4, MAX_SLEVELS is
set to two, and this is why if two consecutive pyramid
levels are β-Cells, the third level β-Cells are autotmatically downgraded to γ -Cells. For each β-Cell C, a counter,
called S-Levels Counter, is maintained. The S-Levels Counter
stores of the total number of consecutive levels in the direct
ancestry of cell C such that all these levels contains β-Cells.

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

At a β-Cell C, if the cell children are β-Cells, then we
compare the S-Levels Counter at the child cells with the
MAX_SLEVELS parameter. Note that the counter counts
only the consecutive S-Levels, so if some levels in the chain
are α-Cells the counter is reset to zero at the α-Cells levels. If
S-Levels Counter is greater than or equal to MAX_SLEVELS,
then the children cells of C are downgraded to γ -Cells.
Otherwise, cell C children cells are not downgraded to
γ -Cells. Similarly, LARS* also makes use of the same
S-Levels Counter to decide whether to upgrade γ -Cells to
β-Cells.

5

N ON -S PATIAL U SER R ATINGS FOR S PATIAL
I TEMS

This section describes how LARS* produces recommendations using non-spatial ratings for spatial items represented
by the tuple (user, rating, item, ilocation). The idea is to
exploit travel locality, i.e., the observation that users limit
their choice of spatial venues based on travel distance
(based on analysis in Section 1.1). Traditional (non-spatial)
recommendation techniques may produce recommendations with burdensome travel distances (e.g., hundreds
of miles away). LARS* produces recommendations within
reasonable travel distances by using travel penalty, a technique that penalizes the recommendation rank of items the
further in travel distance they are from a querying user.
Travel penalty may incur expensive computational overhead
by calculating travel distance to each item. Thus, LARS*
employs an efficient query processing technique capable
of early termination to produce the recommendations without calculating the travel distance to all items. Section 5.1
describes the query processing framework while Section 5.2
describes travel distance computation.

5.1 Query Processing
Query processing for spatial items using the travel penalty
technique employs a single system-wide item-based collaborative filtering model to generate the top-k recommendations by ranking each spatial item i for a querying user u
based on RecScore(u, i), computed as:
RecScore(u, i) = P(u, i) − TravelPenalty(u, i).

(7)

P(u, i) is the standard item-based CF predicted rating of
item i for user u (see Section 2.2). TravelPenalty(u, i) is the
road network travel distance between u and i normalized
to the same value range as the rating scale (e.g., [0, 5]).
When processing recommendations, we aim to avoid calculating Equation 7 for all candidate items to find the top-k
recommendations, which can become quite expensive given
the need to compute travel distances. To avoid such computation, we evaluate items in monotonically increasing
order of travel penalty (i.e., travel distance), enabling us to
use early termination principles from top-k query processing [18]–[20]. We now present the main idea of our query
processing algorithm and in the next section discuss how
to compute travel penalties in an increasing order of travel
distance.
Algorithm 2 provides the pseudo code of our query
processing algorithm that takes a querying user id U, a

1393

Algorithm 2 Travel Penalty Algorithm for Spatial Items
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:

Function LARS*_SpatialItems(User U, Location L, Limit K)
/* Populate a list R with a set of K items*/
R←φ
for (K iterations) do
i ← Retrieve the item with the next lowest travel penalty (Section 5.2)
Insert i into R ordered by RecScore(U, i) computed by Equation 7
end for
LowestRecScore ← RecScore of the kth object in R
/*Retrieve items one by one in order of their penalty value */
while there are more items to process do
i ← Retrieve the next item in order of penalty score (Section 5.2)
MaxPossibleScore ← MAX_RATING - i.penalty
if MaxPossibleScore ≤ LowestRecScore then
return R /* early termination - end query processing */
end if
RecScore(U, i) ← P(U, i) - i.penalty /* Equation 7 */
if RecScore(U, i) > LowestRecScore then
Insert i into R ordered by RecScore(U, i)
LowestRecScore ← RecScore of the kth object in R
end if
end while
return R

location L, and a limit K as input, and returns the list R of
top-k recommended items. The algorithm starts by running
a k-nearest-neighbor algorithm to populate the list R with
k items with lowest travel penalty; R is sorted by the recommendation score computed using Equation 7. This initial
part is concluded by setting the lowest recommendation
score value (LowestRecScore) as the RecScore of the kth item
in R (Lines 3 to 8). Then, the algorithm starts to retrieve
items one by one in the order of their penalty score. This
can be done using an incremental k-nearest-neighbor algorithm, as will be described in the next section. For each item
i, we calculate the maximum possible recommendation score
that i can have by subtracting the travel penalty of i from
MAX_RATING, the maximum possible rating value in the
system, e.g., 5 (Line 12). If i cannot make it into the list
of top-k recommended items with this maximum possible
score, we immediately terminate the algorithm by returning R as the top-k recommendations without computing the
recommendation score (and travel distance) for more items
(Lines 13 to 15). The rationale here is that since we are
retrieving items in increasing order of their penalty and calculating the maximum score that any remaining item can
have, then there is no chance that any unprocessed item
can beat the lowest recommendation score in R. If the early
termination case does not arise, we continue to compute
the score for each item i using Equation 7, insert i into
R sorted by its score (removing the kth item if necessary),
and adjust the lowest recommendation value accordingly
(Lines 16 to 20).
Travel penalty requires very little maintenance. The only
maintenance necessary is to occasionally rebuild the single system-wide item-based collaborative filtering model
in order to account for new location-based ratings that
enter the system. Following the reasoning discussed in
Section 4.3, we rebuild the model after receiving N% new
ratings.

5.2 Incremental Travel Penalty Computation
This section gives an overview of two methods we implemented in LARS* to incrementally retrieve items one by one
ordered by their travel penalty. The two methods exhibit a

1394

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

trade-off between query processing efficiency and penalty
accuracy: (1) an online method that provides exact travel
penalties but is expensive to compute, and (2) an offline
heuristic method that is less exact but efficient in penalty
retrieval. Both methods can be employed interchangeably
in Line 11 of Algorithm 2.

5.2.1 Incremental KNN: An Exact Online Method
To calculate an exact travel penalty for a user u to item i,
we employ an incremental k-nearest-neighbor (KNN) technique [21]–[23]. Given a user location l, incremental KNN
algorithms return, on each invocation, the next item i nearest to u with regard to travel distance d. In our case, we
normalize distance d to the ratings scale to get the travel
penalty in Equation 7. Incremental KNN techniques exist
for both Euclidean distance [22] and (road) network distance [21], [23]. The advantage of using Incremental KNN
techniques is that they provide an exact travel distances
between a querying user’s location and each recommendation candidate item. The disadvantage is that distances
must be computed online at query runtime, which can be
expensive. For instance, the runtime complexity of retrieving a single item using incremental KNN in Euclidean space
is [22]: O(k + logN), where N and k are the number of total
items and items retrieved so far, respectively.
5.2.2 Penalty Grid: A Heuristic Offline Method
A more efficient, yet less accurate method to retrieve travel
penalties incrementally is to use a pre-computed penalty
grid. The idea is to partition space using an n × n grid. Each
grid cell c is of equal size and contains all items whose
location falls within the spatial region defined by c. Each
cell c contains a penalty list that stores the pre-computed
penalty values for traveling from anywhere within c to all
other n2 −1 destination cells in the grid; this means all items
within a destination grid cell share the same penalty value.
The penalty list for c is sorted by penalty value and always
stores c (itself) as the first item with a penalty of zero. To
retrieve items incrementally, all items within the cell containing the querying user are returned one-by-one (in any
order) since they have no penalty. After these items are
exhausted, items contained in the next cell in the penalty
list are returned, and so forth until Algorithm 2 terminates
early or processes all items.
To populate the penalty grid, we must calculate the
penalty value for traveling from each cell to every other cell
in the grid. We assume items and users are constrained to
a road network, however, we can also use Euclidean space
without consequence. To calculate the penalty from a single
source cell c to a destination cell d, we first find the average
distance to travel from anywhere within c to all item destinations within d. To do this, we generate an anchor point
p within c that both (1) lies on the road network segment
within c and (2) lies as close as possible to the center of
c. With these criteria, p serves as an approximate average
“starting point" for traveling from c to d. We then calculate
the shortest path distance from p to all items contained in
d on the road network (any shortest path algorithm can be
used). Finally, we average all calculated shortest path distances from c to d. As a final step, we normalize the average
distance from c to d to fall within the rating value range.

Normalization is necessary as the rating domain is usually
small (e.g., zero to five), while distance is measured in miles
or kilometers and can have large values that heavily influence Equation 7. We repeat this entire process for each cell
to all other cells to populate the entire penalty grid.
When new items are added to the system, their presence in a cell d can alter the average distance value used in
penalty calculation for each source cell c. Thus, we recalculate penalty scores in the penalty grid after N new items
enter the system. We assume spatial items are relatively
static, e.g., restaurants do not change location often. Thus,
it is unlikely existing items will change cell locations and in
turn alter penalty scores.

6

S PATIAL U SER R ATINGS FOR S PATIAL I TEMS

This section describes how LARS* produces recommendations using spatial ratings for spatial items represented by
the tuple (user, ulocation, rating, item, ilocation). A salient feature of LARS* is that both the user partitioning and travel
penalty techniques can be used together with very little
change to produce recommendations using spatial user
ratings for spatial items. The data structures and maintenance techniques remain exactly the same as discussed
in Sections 4 and 5; only the query processing framework requires a slight modification. Query processing uses
Algorithm 2 to produce recommendations. However, the
only difference is that the item-based collaborative filtering
prediction score P(u, i) used in the recommendation score
calculation (Line 16 in Algorithm 2) is generated using the
(localized) collaborative filtering model from the partial
pyramid cell that contains the querying user, instead of the
system-wide collaborative filtering model as was used in
Section 5.

7

E XPERIMENTS

This section provides experimental evaluation of LARS*
based on an actual system implementation using C++ and
STL. We compare LARS* with the standard item-based
collaborative filtering technique along with several variations of LARS*. We also compare LARS* to LARS [8].
Experiments are based on three data sets:
Foursquare: a real data set consisting of spatial user
ratings for spatial items derived from Foursquare user histories. We crawled Foursquare and collected data for
1,010,192 users and 642,990 venues across the United States.
Foursquare does not publish each “check-in" for a user,
however, we were able to collect the following pieces of
data: (1) user tips for a venue, (2) the venues for which the
user is the mayor, and (3) the completed to-do list items for
a user. In addition, we extracted each user’s friend list.
Extracting location-based ratings. To extract spatial user
ratings for spatial items from the Foursquare data (i.e.,
the five-tuple (user, ulocation, rating, item, ilocation)), we
map each user visit to a single location-based rating. The
user and item attributes are represented by the unique
Foursquare user and venue identifier, respectively. We
employ the user’s home city in Foursquare as the ulocation attribute. Meanwhile, the ilocation attribute is the item’s
inherent location. We use a numeric rating value range of
[1, 3], translated as follows: (a) 3 represents the user is the

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

“mayor” of the venue, (b) 2 represents that the user left a
“tip” at the venue, and (c) 1 represents the user visited the
venue as a completed “to-do” list item. Using this scheme,
a user may have multiple ratings for a venue, in this case
we use the highest rating value.
Data properties. Our experimental data consisted of 22,390
location-based ratings for 4K users for 2K venues all from
the state of Minnesota, USA. We used this reduced data
set in order to focus our quality experiments on a dense
rating sample. Use of dense ratings data has been shown
to be a very important factor when testing and comparing
recommendation quality [17], since use of sparse data (i.e.,
having users or items with very few ratings) tends to cause
inaccuracies in recommendation techniques.
MovieLens: a real data set consisting of spatial user ratings for non-spatial items taken from the popular MovieLens
recommender system [5]. The Foursquare and MovieLens
data are used to test recommendation quality. The
MovieLens data used in our experiments was real movie
rating data taken from the popular MovieLens recommendation system at the University of Minnesota [5]. This data
consisted of 87,025 ratings for 1,668 movies from 814 users.
Each rating was associated with the zip code of the user
who rated the movie, thus giving us a real data set of spatial
user ratings for non-spatial items.
Synthetic: a synthetically generated data set consisting
of spatial user ratings for spatial items for venues in the
state of Minnesota, USA. The synthetic data set we use
in our experiments is generated to contain 2000 users and
1000 items, and 500,000 ratings. Users and items locations
are randomly generated over the state of Minnesota, USA.
Users’ ratings to items are assigned random values between
zero and five. As this data set contains a number of ratings
that is about twenty five times and five times larger than the
foursquare data set and the Movilens data set, we use such
synthetic data set to test scalability and query efficiency.
Unless mentioned otherwise, the default value of M is
0.3, k is 10, the number of pyramid levels is 8, the influence
level is the lowest pyramid level, and MAX_SLEVELS is set
to two. The rest of this section evaluates LARS* recommendation quality (Section 7.1), trade-offs between storage and
locality (Section 7.4), scalability (Section 7.5), and query processing efficiency (Section 7.6). As the system stores its data
structures in main memory, all reported time measurements
represent the CPU time.

7.1

Recommendation Quality for Varying Pyramid
Levels
These experiments test the recommendation quality
improvement that LARS* achieves over the standard (nonspatial) item-based collaborative filtering method using
both the Foursquare and MovieLens data. To test the
effectiveness of our proposed techniques, we test the
quality improvement of LARS* with only travel penalty
enabled (abbr. LARS*-T), LARS* with only user partitioning enabled and M set to one (abbr. LARS*-U), and
LARS* with both techniques enabled and M set to one
(abbr. LARS*). Notice that LARS*-T represents the traditional item-based collaborative filtering augmented with
the travel penalty technique (section 5) to take the distance

(a)

1395

(b)

Fig. 7. Quality experiments for varying locality. (a) Foursquare data.
(b) MovieLens data.

between the querying user and the recommended items
into account. We do not plot LARS with LARS* as both give
the same result for M=1, and the quality experiments are
meant to show how locality increases the recommendation
quality.
Quality Metric. To measure quality, we build each recommendation method using 80% of the ratings from each
data set. Each rating in the withheld 20% represents a
Foursquare venue or MovieLens movie a user is known
to like (i.e., rated highly). For each rating t in this 20%, we
request a set of k ranked recommendations S by submitting
the user and ulocation associated with t. We first calculate
the quality as the weighted sum of the number of occurrences of the item associated with t (the higher the better)
in S . The weight of an item is a value between zero and one
that determines how close the rank of this item from its real
rank. The quality of each recommendation method is calculated and compared against the baseline, i.e., traditional
item-based collaborative filtering. We finally report the ratio
of improvement in quality each recommendation method
achieves over the baseline. The rationale for this metric is
that since each withheld rating represents a real visit to a
venue (or movie a user liked), the technique that produces
a large number of correctly ranked answers that contain
venues (or movies) a user is known to like is considered of
higher quality.
Fig. 7(a) compares the quality improvement of each technique (over traditional collaborative filtering) for varying
locality (i.e., different levels of the adaptive pyramid) using
the Foursquare data. LARS*-T does not use the adaptive
pyramid, thus has constant quality improvement. However,
LARS*-T shows some quality improvement over traditional
collaborative filtering. This quality boost is due to that fact
that LARS*-T uses a travel penalty technique that recommends items within a feasible distance. Meanwhile, the
quality of LARS* and LARS*-U increases as more localized pyramid cells are used to produce recommendation,
which verifies that user partitioning is indeed beneficial and
necessary for location-based ratings. Ultimately, LARS* has
superior performance due to the additional use of travel
penalty. While travel penalty produces moderate quality gain,
it also enables more efficient query processing, which we
observe later in Section 7.6.
Fig. 7(b) compares the quality improvement of LARS*U over CF (traditional collaborative filtering) for varying
locality using the MovieLens data. Notice that LARS* gives
the same quality improvement as LARS*-U because LARS*T do not apply for this dataset since movies are not spatial.
Compared to CF, the quality improvement achieved by

1396

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

(a)

(b)

Fig. 8. Quality experiments for varying answer sizes. (a) Foursquare

data. (b) MovieLens data.

LARS*-U (and LARS*) increases when it produces movie
recommendations from more localized pyramid cells. This
behavior further verifies that user partitioning is beneficial
in providing quality recommendations localized to a querying user location, even when items are not spatial. Quality
decreases (or levels off for MovieLens) for both LARS*-U
and/or LARS* for lower levels of the adaptive pyramid.
This is due to recommendation starvation, i.e., not having
enough ratings to produce meaningful recommendations.

7.2 Recommendation Quality for Varying k
These experiments test recommendation quality improvement of LARS*, LARS*-U, and LARS*-T for different values
of k (i.e., recommendation answer sizes). We do not plot
LARS with LARS* as both gives the same result for M=1,
and the quality experiments are meant to show how the
degree of locality increases the recommendation quality.
We perform experiments using both the Foursquare and
MovieLens data. Our quality metric is exactly the same as
presented previously in Section 7.1.
Fig. 8(a) depicts the effect of the recommendation list size
k on the quality of each technique using the Foursquare
data set. We report quality numbers using the pyramid
height of four (i.e., the level exhibiting the best quality from
Section 7.1 in Fig. 7(a)). For all sizes of k from one to ten,
LARS* and LARS*-U consistently exhibit better quality. In
fact, LARS* consistently achieves better quality over CF for
all k. LARS*-T exhibits similar quality to CF for smaller k
values, but does better for k values of three and larger.
Fig. 8(b) depicts the effect of the recommendation list
size k on the quality of improvement of LARS*-U (and
LARS*) over CF using the MovieLens data. Notice that
LARS* gives the same quality improvement as LARS*-U
because LARS*-T do not apply for this dataset since movies
are not spatial. This experiment was run using a pyramid
hight of seven (i.e., the level exhibiting the best quality
in Fig. 7(b)). Again, LARS*-U (and LARS*) consistently
exhibits better quality than CF for sizes of K from one to ten.
7.3 Recommendation Quality for Varying M
These experiments compares the quality improvement
achieved by both LARS and LARS* for different values of
M. We perform experiments using both the Foursquare and
MovieLens data. Our quality metric is exactly the same as
presented previously in Section 7.1.
Fig. 9(a) depicts the effect of M on the quality of both
LARS and LARS* using the Foursquare data set. Notice that
we enable both the user partitioning and travel penalty

(a)

(b)

Fig. 9. Quality experiments for varying value of M. (a) Foursquare

data. (b) MovieLens data.

techniques for both LARS and LARS*. We report quality
numbers using the pyramid height of four and the number
of recommended items of ten. When M is equal to zero,
both LARS and LARS* exhibit the same quality improvement as M = 0 represents a traditional collaborative
filtering with the travel penalty technique applied. Also,
when M is set to one, both LARS and LARS* achieve
the same quality improvement as a fully maintained pyramid is maintained in both cases. For M values between
zero and one, the quality improvement of both LARS and
LARS* increases for higher values of M due to the increase
in recommendation locality. LARS* achieves better quality
improvement over LARS because LARS* maintains α-Cells
at lower levels of the pyramid.
Fig. 9(b) depicts the effect of M on the quality of
both LARS and LARS* using the Movilens data set.
We report quality improvement over traditional collaborative filtering using the pyramid height of seven and
the number of recommended items set to ten. Similar
to Foursquare data set, the quality improvement of both
LARS and LARS* increases for higher values of M due
to the increase in recommendation locality. For M values between zero and one, LARS* consistently achieves
higher quality improvement over LARS as LARS* maintains more α-Cells at more granular levels of the pyramid
structure.

7.4 Storage Vs. Locality
Fig. 10 depicts the impact of varying M on both the storage and locality in LARS* using the synthetic data set. We
plot LARS*-M=0 and LARS*-M=1 as constants to delineate
the extreme values of M, i.e., M=0 mirrors traditional collaborative filtering, while M=1 forces LARS* to employ a
complete pyramid. Our metric for locality is locality loss
(defined in Section 4.5.2) when compared to a complete

(a)

(b)

Fig. 10. Effect of M on storage and locality (synthetic data).

(a) Storage. (b) Locality.

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

(a)

(b)

Fig. 11. Scalability of the adaptive pyramid (synthetic data).

(a) Storage. (b) Maintenance.

pyramid (i.e., M=1). LARS*-M=0 requires the lowest storage overhead, but exhibits the highest locality loss, while
LARS*-M=1 exhibits no locality loss but requires the most
storage. For LARS*, increasing M results in increased storage overhead since LARS* favors switching cells to α-Cells,
requiring the maintenance of more pyramid cells each with
its own collaborative filtering model. Each additional αCell incurs a high storage overhead over the original data
size as an additional collaborative filtering model needs to
be maintained. Meanwhile, increasing M results in smaller
locality loss as LARS* merges less and maintains more
localized cells. The most drastic drop in locality loss is
between 0 and 0.3, which is why we chose M=0.3 as a
default. LARS* leads to smaller locality loss (≈26% less)
than LARS because LARS* maintains α-Cells below β-Cells
which result in higher locality gain. On the other hand,
LARS* exhibits slightly higher storage cost (≈5% more storage) than LARS due to the fact that LARS* stores the Item
Ratings Statistics Table per each α-Cell and β-Cell.

7.5 Scalability
Fig. 11 depicts the storage and aggregate maintenance overhead required for an increasing number of ratings using the
synthetic data set. We again plot LARS*-M=0 and LARS*M=1 to indicate the extreme cases for LARS*. Fig. 11(a)
depicts the impact of increasing the number of ratings from
10K to 500K on storage overhead. LARS*-M=0 requires
the lowest amount of storage since it only maintains a
single collaborative filtering model. LARS*-M=1 requires
the highest amount of storage since it requires storage
of a collaborative filtering model for all cells (in all levels) of a complete pyramid. The storage requirement of
LARS* is in between the two extremes since it merges
cells to save storage. Fig. 11(b) depicts the cumulative
computational overhead necessary to maintain the adaptive pyramid initially populated with 100K ratings, then
updated with 200K ratings (increments of 50K reported).
The trend is similar to the storage experiment, where
LARS* exhibits better performance than LARS*-M=1 due
to switching some cells from α-Cells to β-Cells. Though
LARS*-M=0 has the best performance in terms of maintenance and storage overhead, previous experiments show
that it has unacceptable drawbacks in quality/locality.
Compared to LARS, LARS* has less maintenance overhead
(≈38% less) due to the fact that the maintenance algorithm
in LARS* avoids the expensive speculative splitting used
by LARS.

(a)
12. Query processing performance
(a) Snapshot queries. (b) Continuous queries.

Fig.

1397

(b)
(synthetic

data).

7.6 Query Processing Performance
Fig. 12 depicts snapshot and continuous query processing performance of LARS, LARS*, LARS*-U (LARS* with
only user partitioning), LARS*-T (LARS* with only travel
penalty), CF (traditional collaborative filtering), and LARS*M=1 (LARS* with a complete pyramid), using the synthetic
data set.
Snapshot queries. Fig. 12(a) gives the effect of various
number of ratings (10K to 500K) on the average snapshot query performance averaged over 500 queries posed
at random locations. LARS* and LARS*-M=1 consistently
outperform all other techniques; LARS*-M=1 is slightly better due to recommendations always being produced from
the smallest (i.e., most localized) CF models. The performance gap between LARS* and LARS*-U (and CF and
LARS*-T) shows that employing the travel penalty technique
with early termination leads to better query response time.
Similarly, the performance gap between LARS* and LARS*T shows that employing user partitioning technique with
its localized (i.e., smaller) collaborative filtering model also
benefits query processing. LARS* performance is slightly
better than LARS as LARS* sometimes maintains more
localized CF models than LARS which incurs less query
processing time.
Continuous queries. Fig. 12(b) provides the continuous
query processing performance of the LARS* variants by
reporting the aggregate response time of 500 continuous
queries. A continuous query is issued once by a user u
to get an initial answer, then the answer is continuously
updated as u moves. We report the aggregate response
time when varying the travel distance of u from 1 to 30
miles using a random walk over the spatial area covered
by the pyramid. CF has a constant query response time for
all travel distances, as it requires no updates since only
a single cell is present. However, since CF is unaware
of user location change, the consequence is poor recommendation quality (per experiments from Section 7.1).
LARS*-M=1 exhibits the worse performance, as it maintains all cells on all levels and updates the continuous
query whenever the user crosses pyramid cell boundaries.
LARS*-U has a lower response time than LARS*-M=1 due
to switching cells from α-Cells to β-Cells: when a cell is
not present on a given influence level, the query is transferred to its next highest ancestor in the pyramid. Since
cells higher in the pyramid cover larger spatial regions,
query updates occur less often. LARS*-T exhibits slightly
higher query processing overhead compared to LARS*U: even though LARS*-T employs the early termination

1398

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 26, NO. 6, JUNE 2014

algorithm, it uses a large (system-wide) collaborative filtering model to (re)generate recommendations once users
cross boundaries in the penalty grid. LARS* exhibits a
better aggregate response time since it employs the early
termination algorithm using a localized (i.e., smaller) collaborative filtering model to produce results while also
switching cells to β-Cells to reduce update frequency. LARS
has a slightly better performance than LARS* as LARS
tends to merge more cells at higher levels in the pyramid
structure.

8

R ELATED W ORK

Location-based services. Current location-based services
employ two main methods to provide interesting destinations to users. (1) KNN techniques [22] and variants (e.g.,
aggregate KNN [24]) simply retrieve the k objects nearest
to a user and are completely removed from any notion of
user personalization. (2) Preference methods such as skylines [25] (and spatial variants [26]) and location-based
top-k methods [27] require users to express explicit preference constraints. Conversely, LARS* is the first locationbased service to consider implicit preferences by using
location-based ratings to help users discover new items.
Recent research has proposed the problem of hyper-local
place ranking [28]. Given a user location and query string
(e.g., “French restaurant"), hyper-local ranking provides a
list of top-k points of interest influenced by previously
logged directional queries (e.g., map direction searches
from point A to point B). While similar in spirit to LARS*,
hyper-local ranking is fundamentally different from our
work as it does not personalize answers to the querying user,
i.e., two users issuing the same search term from the same
location will receive exactly the same ranked answer.
Traditional recommenders. A wide array of techniques
are capable of producing recommendations using nonspatial ratings for non-spatial items represented as the triple
(user, rating, item) (see [4] for a comprehensive survey).
We refer to these as “traditional" recommendation techniques. The closest these approaches come to considering
location is by incorporating contextual attributes into statistical recommendation models (e.g., weather, traffic to
a destination) [29]. However, no traditional approach has
studied explicit location-based ratings as done in LARS*.
Some existing commercial applications make cursory use
of location when proposing interesting items to users. For
instance, Netflix displays a “local favorites” list containing popular movies for a user’s given city. However, these
movies are not personalized to each user (e.g., using recommendation techniques); rather, this list is built using
aggregate rental data for a particular city [30]. LARS*, on
the other hand, produces personalized recommendations
influenced by location-based ratings and a query location.
Location-aware recommenders. The CityVoyager system [31] mines a user’s personal GPS trajectory data to
determine her preferred shopping sites, and provides recommendation based on where the system predicts the user
is likely to go in the future. LARS*, conversely, does not
attempt to predict future user movement, as it produces
recommendations influenced by user and/or item locations
embedded in community ratings.

The spatial activity recommendation system [32] mines
GPS trajectory data with embedded user-provided tags in
order to detect interesting activities located in a city (e.g., art
exhibits and dining near downtown). It uses this data to
answer two query types: (a) given an activity type, return
where in the city this activity is happening, and (b) given
an explicit spatial region, provide the activities available
in this region. This is a vastly different problem than we
study in this paper. LARS* does not mine activities from
GPS data for use as suggestions for a given spatial region.
Rather, we apply LARS* to a more traditional recommendation problem that uses community opinion histories to
produce recommendations.
Geo-measured friend-based collaborative filtering [33]
produces recommendations by using only ratings that are
from a querying user’s social-network friends that live in
the same city. This technique only addresses user location
embedded in ratings. LARS*, on the other hand, addresses
three possible types of location-based ratings. More importantly, LARS* is a complete system (not just a recommendation technique) that employs efficiency and scalability
techniques (e.g., partial pyramid structure, early query termination) necessary for deployment in actual large-scale
applications.

9

C ONCLUSION

LARS*, our proposed location-aware recommender system,
tackles a problem untouched by traditional recommender
systems by dealing with three types of location-based
ratings: spatial ratings for non-spatial items, non-spatial ratings for spatial items, and spatial ratings for spatial items.
LARS* employs user partitioning and travel penalty techniques to support spatial ratings and spatial items, respectively. Both techniques can be applied separately or in
concert to support the various types of location-based ratings. Experimental analysis using real and synthetic data
sets show that LARS* is efficient, scalable, and provides
better quality recommendations than techniques used in
traditional recommender systems.

ACKNOWLEDGMENTS
This work was supported in part by the US National
Science Foundation under Grants IIS-0811998, IIS-0811935,
CNS-0708604, IIS-0952977 and in part by a Microsoft
Research Gift.

R EFERENCES
[1] G. Linden, B. Smith, and J. York, “Amazon.com recommendations: Item-to-item collaborative filtering,” IEEE Internet Comput.,
vol. 7, no. 1, pp. 76–80, Jan./Feb. 2003.
[2] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and J. Riedl,
“GroupLens: An open architecture for collaborative filtering of
netnews,” in Proc. CSWC, Chapel Hill, NC, USA, 1994.
[3] The facebook blog. Facebook Places [Online]. Available:
http://tinyurl.com/3aetfs3
[4] G. Adomavicius and A. Tuzhilin, “Toward the next generation of
recommender systems: A survey of the state-of-the-art and possible extensions,” IEEE Trans. Knowl. Data Eng., vol. 17, no. 6,
pp. 734–749, Jun. 2005.
[5] MovieLens [Online]. Available: http://www.movielens.org/
[6] Foursquare [Online]. Available: http://foursquare.com

SARWAT ET AL.: LARS*: AN EFFICIENT AND SCALABLE LOCATION-AWARE RECOMMENDER SYSTEM

[7] New York Times - A Peek into Netflix Queues [Online]. Available:
http://www.nytimes.com/interactive/2010/01/10/nyregion/
20100110-netflix-map.html
[8] J. J. Levandoski, M. Sarwat, A. Eldawy, and M. F. Mokbel,
“LARS: A location-aware recommender system,” in Proc. ICDE,
Washington, DC, USA, 2012.
[9] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl, “Item-based collaborative filtering recommendation algorithms,” in Proc. Int. Conf.
WWW, Hong Kong, China, 2001.
[10] J. S. Breese, D. Heckerman, and C. Kadie, “Empirical analysis
of predictive algorithms for collaborative filtering,” in Proc. Conf.
UAI, San Francisco, CA, USA, 1998.
[11] W. G. Aref and H. Samet, “Efficient processing of window queries
in the pyramid data structure,” in Proc. ACM Symp. PODS, New
York, NY, USA, 1990.
[12] R. A. Finkel and J. L. Bentley, “Quad trees: A data structure for
retrieval on composite keys,” Acta Inf., vol. 4, no. 1, pp. 1–9, 1974.
[13] A. Guttman, “R-trees: A dynamic index structure for spatial
searching,” in Proc. SIGMOD, New York, NY, USA, 1984.
[14] K. Mouratidis, S. Bakiras, and D. Papadias, “Continuous monitoring of spatial queries in wireless broadcast environments,” IEEE
Trans. Mobile Comput., vol. 8, no. 10, pp. 1297–1311, Oct. 2009.
[15] K. Mouratidis and D. Papadias, “Continuous nearest neighbor
queries over sliding windows,” IEEE Trans. Knowl. Data Eng.,
vol. 19, no. 6, pp. 789–803, Jun. 2007.
[16] M. F. Mokbel, X. Xiong, and W. G. Aref, “SINA: Scalable
incremental processing of continuous queries in spatiotemporal
databases,” in Proc. SIGMOD, Paris, France, 2004.
[17] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and J. T. Riedl,
“Evaluating collaborative filtering recommender systems,” ACM
TOIS, vol. 22, no. 1, pp. 5–53, 2004.
[18] M. J. Carey and D. Kossmann, “On saying "Enough Already!" in
SQL,” in Proc. SIGMOD, New York, NY, USA, 1997.
[19] S. Chaudhuri and L. Gravano, “Evaluating top-k selection
queries,” in Proc. Int. Conf. VLDB, Edinburgh, U.K., 1999.
[20] R. Fagin, A. Lotem, and M. Naor, “Optimal aggregation algorithms for middleware,” in Proc. ACM Symp. PODS, New York,
NY, USA, 2001.
[21] J. Bao, C.-Y. Chow, M. F. Mokbel, and W.-S. Ku, “Efficient evaluation of k-range nearest neighbor queries in road networks,” in
Proc. Int. Conf. MDM, Kansas City, MO, USA, 2010.
[22] G. R. Hjaltason and H. Samet, “Distance browsing in spatial
databases,” ACM TODS, vol. 24, no. 2, pp. 265–318, 1999.
[23] K. Mouratidis, M. L. Yiu, D. Papadias, and N. Mamoulis,
“Continuous nearest neighbor monitoring in road networks,” in
Proc. Int. Conf. VLDB, Seoul, Korea, 2006.
[24] D. Papadias, Y. Tao, K. Mouratidis, and C. K. Hui, “Aggregate
nearest neighbor queries in spatial databases,” ACM TODS,
vol. 30, no. 2, pp. 529–576, 2005.
[25] S. Börzsönyi, D. Kossmann, and K. Stocker, “The skyline operator,” in Proc. ICDE, Heidelberg, Germany, 2001.
[26] M. Sharifzadeh and C. Shahabi, “The spatial skyline queries,” in
Proc. Int. Conf. VLDB, Seoul, Korea, 2006.
[27] N. Bruno, L. Gravano, and A. Marian, “Evaluating top-k queries
over web-accessible databases,” in Proc. ICDE, San Jose, CA, USA,
2002.
[28] P. Venetis, H. Gonzalez, C. S. Jensen, and A. Y. Halevy, “Hyperlocal, directions-based ranking of places,” PVLDB, vol. 4, no. 5,
pp. 290–301, 2011.
[29] M.-H. Park, J.-H. Hong, and S.-B. Cho, “Location-based recommendation system using Bayesian user’s preference model in
mobile devices,” in Proc. Int. Conf. UIC, Hong Kong, China, 2007.
[30] Netflix News and Info - Local Favorites [Online]. Available:
http://tinyurl.com/4qt8ujo
[31] Y. Takeuchi and M. Sugimoto, “An outdoor recommendation system based on user location history,” in Proc. Int. Conf. UIC, Berlin,
Germany, 2006.
[32] V. W. Zheng, Y. Zheng, X. Xie, and Q. Yang, “Collaborative location and activity recommendations with GPS history data,” in
Proc. Int. Conf. WWW, New York, NY, USA, 2010.
[33] M. Ye, P. Yin, and W.-C. Lee, “Location recommendation for
location-based social networks,” in Proc. ACM GIS, New York,
NY, USA , 2010.

1399

Mohamed Sarwat is a Doctoral candidate
in the Department of Computer Science
and Engineering, University of Minnesota,
Minneapolis, MN, USA. He received the bachelor’s degree in computer engineering from
Cairo University, Egypt, in 2007 and the master’s
degree in computer science from the University
of Minnesota in 2011. His current research interests include a broad area of data management
systems, more specifically, database systems,
database support for recommender systems,
personalized databases, database support for location-based services
and for social networking applications, distributed graph databases,
and large scale data management. He was awarded the University of
Minnesota Doctoral Dissertation Fellowship in 2012. His research was
recognized by the Best Research Paper Award at the 12th International
Symposium on Spatial and Temporal Databases, in 2011.

Justin J. Levandoski is a Researcher with
the Database Group at Microsoft Research,
Redmond, WA, USA. He received the bachelor’s
degree at Carleton College, Northfield, MN, USA
in 2003, and the master’s and Ph.D. degrees
from the University of Minnesota, Minneapolis,
MN, USA, in 2008 and 2011, respectively. His
current research interests include a broad range
of topics dealing with large-scale data management systems, such as cloud computing,
database support for new hardware paradigms,
transaction processing, query processing, and support for new dataintensive applications such as social/recommender systems.

Ahmed Eldawy is a Ph.D. student in
the Department of Computer Science
and Engineering, University of Minnesota,
Minneapolis, MN, USA. His current research
interests include spatial data management,
social networks, and cloud computing, such
as building scalable spatial data management
systems over cloud computing platforms. He
received the bachelor’s and master’s degrees
in computer science from Alexandria University,
Egypt, in 2005 and 2010, respectively.

Mohamed F. Mokbel received the B.Sc. and
M.S. degrees from Alexandria University, Egypt,
in 1996 and 1999, respectively, and the
Ph.D. degree from Purdue University, West
Lafayette, IN, USA, in 2005. Currently, he is
an Assistant Professor in the Department of
Computer Science and Engineering, University
of Minnesota, Minneapolis, MN, USA. His current
research interests include advancing the stateof-the-art in the design and implementation of
database engines to cope with the requirements
of emerging applications (e.g., location-based applications and sensor
networks). His research works has been recognized by three Best Paper
Awards at IEEE MASS 2008, MDM 2009, and SSTD 2011. He is a recipient of the US NSF CAREER Award 2010. He has actively participated
in several program committees for major conferences including ICDE,
SIGMOD, VLDB, SSTD, and ACM GIS. He is/was a program Co-Chair
for ACM SIGSPATIAL GIS 2008, 2009, and 2010. He is an ACM and an
IEEE member and a founding member of ACM SIGSPATIAL.

 For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

A Demonstration of MNTG A Web-based Road Network Traffic Generator
Mohamed F. Mokbel1 , Louai Alarabi2 , Jie Bao3 , Ahmed Eldawy4 , Amr Magdy5 , Mohamed Sarwat6 , Ethan Waytas7 , Steven Yackel8
1,2,3,4,5,6,7 University

of Minnesota, Minneapolis, MN 55455, USA
8 Microsoft
1
2
3
4
5
6
{mokbel ,louai ,baojie ,eldawy ,amr ,sarwat }@cs.umn.edu
wayt0012@umn.edu7
spazard1@live.com8

Abstract—This demo presents Minnesota Traffic Generator
(MNTG); an extensible web-based road network traffic generator.
MNTG enables its users to generate traffic data at any arbitrary
road networks with different traffic generators. Unlike existing
traffic generators that require a lot of time/effort to install,
configure, and run, MNTG is a web service with a user-friendly
interface where users can specify an arbitrary spatial region,
select a traffic generator, and submit their traffic generation
request. Once the traffic data is generated by MNTG, users can
then download and/or visualize the generated data. MNTG can be
extended to support: (1) various traffic generators. It is already
shipped with the two most common traffic generators, Brinkhoff
and BerlinMOD, but other generators can be easily added.
(2) various road network sources. It is shipped with U.S. Tiger
files and OpenStreetMap, but other sources can be also added.
A beta version of MNTG is launched at: http://mntg.cs.umn.edu.

I.

I NTRODUCTION

Having access to road network traffic data is a major
need to validate and evaluate indexing and query processing techniques in moving objects databases, spatio-temporal
databases, and data streams. Unfortunately, such data is not
easily available, and is usually of much smaller scale than
needed. The process of extracting real traffic data requires
installing and configuring many GPS-enabled devices and
continuously monitoring the locations of such devices, which
is a cumbersome task. For instance, GeoLife project [13] took
more than four years to collect 17,621 trajectories dataset with
the involvement of 182 volunteers in Beijing. As a result,
researchers have been using existing traffic generators as a
means of getting synthetic datasets that exhibit similar behavior
to real data. The most two common traffic generators are
Brinkhoff [1] and BerlinMOD [3], which have been widely
adopted by large numbers of papers in the database literature,
e.g., see [2], [5], [6], [8], [10], [12].
Even though existing traffic generators are quite useful,
nonetheless, most of them suffer from the following: (1) It
may take the user significant amount of effort to install and
configure the traffic generation tool. For example, in order
to run BerlinMOD, the user needs to first install a moving
object database, i.e., SECONDO [4], and then get familiar with
the script commands used to install it. After the installation,
users still need to understand an extensive list of configuration
parameters for each traffic generator. (2) It is not trivial to
∗

This work is supported in part by the National Science Foundation,
USA, under Grants IIS-0952977 and IIS-1218168.

generate traffic data in arbitrary spatial regions using existing
traffic generators. For example, to be able to use Brinkhoff
or BerlinMOD generators for a different city than the default shipped one (Oldenburg and Berlin for Brinkhoff and
BerlinMOD generators, respectively), the user needs to first
obtain the road network information for the city of interest,
which is a tedious task by itself. For example, to get the road
network information for the city of Chicago, a user may need
to understand the format of OpenStreetMap [9], and then write
a program that extracts the road network of Chicago from
OpenStreetMap. After obtaining the new data, the user then
needs to modify the obtained format to match the required
one by either Brinkhoff or BerlinMOD. Such set of tedious
operations made it hard for casual users to use these traffic
generators for arbitrary spatial areas. As a testimony, one
can observe that almost all the literature that harnessed these
generators, used their default cities.
This demo presents Minnesota Traffic Generator
(MNTG) [7]; an extensible web-based road network traffic
generator. MNTG is not a new traffic generator. Instead, it
is a framework that encapsulates existing traffic generators
and makes them easily accessible. MNTG overcomes the
hurdles of existing traffic generators with three main features:
(1) MNTG is a web service with a user-friendly map interface.
Behind the scenes, MNTG carries the burden of configuring
and running existing traffic generators. (2) MNTG can be used
for any arbitrary spatial area, where users can just mark their
area of interest on a map interface, and submit their traffic
requests accordingly. (3) MNTG users do not need to worry
about the processing time or computing resources, where
MNTG has its own dedicated server that internally processes
the request in a multi-threaded paradigm, and emails the user
back when the data is generated. The notifying email includes
a link to download and/or visualize the data.
MNTG is extensible to support: (1) various traffic generators. Currently, MNTG is shipped with the two most common
traffic generators, Brinkhoff and BerlinMOD, yet, it also has
the interface that can be used to add new traffic generators.
(2) various road network sources. It is currently shipped with
the support for U.S. Tiger files [11] and OpenStreetMap [9],
yet, it also has the interface that can be used to add other
sources for road network data.
A beta version of MNTG is launched as a web service for public use; a prototype can be accessed via
http://mntg.cs.umn.edu. The beta version supports both
Brinkhoff and BerlinMOD traffic generators on U.S Tiger files
and OpenStreetMap data. The extensibility interface for adding

Traffic generation request

Download/visualize
traffic results

Traffic Data Users

System
Frontend

Status

Web Interface

Notification

Status

System
Backend

Download/
Visualization
Tools

Email Notifier

Road Network
Converter

Notification

Results

Traffic Processor

Traffic Requests

Brinkhoff
BerlinMOD

Traffic Results

Random
New Generator

Road Network Data Sources
Traffic Generator
Developers
New Data Source OpenStreetMaps US Tiger Files

Fig. 1.

MNTG System Overview.

more generators or other road network sources is currently
working internally under our support, and will be shown in the
demo. Since its launch in February 2013, MNTG has received
more than 500 traffic generation requests from researchers
world wide. We envision that MNTG will be the de facto
standard for generating road network data for researchers in
spatial and spatio-temporal databases worldwide.
II.

S YSTEM OVERVIEW

Figure 1 gives an overview of MNTG architecture. A user
interacts with MNTG through its system front-end that includes
a web interface that allows users to submit traffic generation
requests, an email notifier that retrieves the status updates
from the back-end and keeps users posted, and download
and visualization tools that allow users to download and/or
visualize the generated data. Internally, MNTG system backend processes the traffic generation requests, where it includes
two main components: (1) Road Network Converter, which
is responsible for extracting the road network data from
different data sources. It currently uses US tiger files or OpenStreetMaps. Yet, it is extensible to support other road network
sources. (2) Traffic Processor, which takes the road network
data and feeds it to the underlying traffic generator. It currently
support both Brinkhoff and BerlinMOD data generators. Yet,
it is extensible to include other data generators.
III.

ROAD N ETWORK C ONVERTER

MNTG employs a road network converter that (a) receives
the user area of interest, (b) extracts the road network data
for the area of interest, and (c) sends the extracted data to the
traffic processor. We will first explain (and demonstrate) how
to include a road network data source in MNTG, then, we will
discuss two case studies that are already realized in MNTG;
US Tiger files, and OpenStreetMap.
A. Adding Road Network Data to MNTG
To add a new road network data source, MNTG provides
a template that consists of the following two function headers:
1)

ExtractRoadNetwork. This function takes
a rectangular area, defined by two <latitude,
longitude> coordinates, as its input, and produces
the road network information of the selected area

2)

as its output. This includes pruning all information
outside the selected area as well as pruning the non
road network information of the selected area.
PrepareStandardOutput.
This
function
takes the road network information from
ExtractRoadNetwork as its input, and produces
two standard text files node.txt and edge.txt,
that contain the final set of nodes and edges in the
selected area, respectively. The text files are in a
standard format to make it portable to various traffic
generators employed by MNTG traffic processor.

B. Case Study 1: U.S. Tiger Files
US Topologically Integrated Geographic Encoding and
Referencing (Tiger) Files [11] are published by US census
bureau on a yearly basis to provide the most recent information about US geographical data, which include city
boundaries, road networks, address information, water features,
and much more. A very unique feature of US Tiger Files
is that the files are partitioned and organized based on US
counties. In other words, all roads in a county are packed
as one compressed file with a unique file identifier, e.g.,
tl_2010_01001_roads.zip, where tl means tiger line,
2010 indicates the publishing year of the data, 01001 is
a unique identifier for the county (in this case is Autauga,
Alabama), and roads represents the type of data.
In this case, the function ExtractRoadNetwork identifies all counties that overlap with the rectangle area selected
by the user. Then, it loads the road network files for all
overlapped counties and filters out the road segments outside
the selected region. Then, the PrepareStandardOutput
function converts Tiger file format to the standard output
format for node.txt and edge.txt.
C. Case Study 2: OpenStreetMap
OpenStreetMap [9] (OSM) is a free GIS service, contributed mainly by volunteers to record all spatial landmarks
(including road networks) worldwide. All data in OSM is
encapsulated into a large XML file Planet.osm that includes four primitive data types: (1) Node, that represents a
spatial point by its latitude and longitude coordinates, (2) Way,
that consists of a sequence of nodes to construct a line or a
polygon, (3) Relation, that specifies the relation between ways
and nodes, e.g., two ways are connected together, and (4) Tags,
that provide description for other data types, i.e., node, Way
and Relation, using a key-value pair.
In this case, the function ExtractRoadNetwork first
retrieves all geographical data with an “osm” file, based on
the spatial area specified by the user, from OpenStreetMap
interface. Then, it parses all XML key-value pairs in the
downloaded “osm” file to extract the road network information.
Then, PrepareStandardOutput converts the road network information (extracted from the “osm” file) to the standard output format for the files node.txt and edge.txt.
IV.

T RAFFIC P ROCESSOR

MNTG provides a wrapper around existing traffic generators to ensure their ease of use. The traffic processor
component of MNTG is responsible on: (1) receiving the road

network data from the road network converter and feeds it to
the underlying generator, (2) running the underlying selected
traffic generator, and (3) producing the generating traffic to
be downloaded and/or visualized on a map interface. We first
explain how to include a traffic generator in MNTG. Then, we
discuss two case studies that are already realized in MNTG;
Brinkhoff and BerlinMOD traffic generators.

Traffic Generation
Parameters

A. Adding a Traffic Generator to MNTG
To add a new traffic generator, MNTG provides a template
that consists of the following three function headers:
1)

2)

3)

RoadNetworkConvert. This function takes a road
network in the standard format of two text files
node.txt and edge.txt as its input. The output
would be the same road network, but in a different
format that is required by the traffic generator.
TrafficGeneration. This function first pops up
a dialogue interface to prompt the user to enter various parameters for the generation request, e.g., number of objects and time interval. Then, it takes these
parameters along with the specified road network
extracted from RoadNetworkConvert, and makes
an external execution call to the traffic generator.
TrafficResultConvert. This function takes its
input as the output of the TrafficGeneration
function. Then, it converts the input into a standard
text format as < Object ID, Timestamp, type, latitude,
longitude> that can be easily downloaded as a text
file and/or visualized a map.

B. Case Study 1: Brinkhoff Generator

Traffic Generation
Region

Fig. 2.

MNTG Web GUI: Traffic Generation (Google Maps Interface)

To run the BerlinMOD traffic generator, a user would
need to set up a SECONDO database [4], and uses a
set of scripts to query it. To support BerlinMOD generator inside MNTG, we install and configure a SECONDO
database in our server and implement the three abstract
functions as follows: The RoadNetworkConvert function
reads the standard road network files and transforms it to
the format used in BerlinMOD. Ultimately, it produces a
data file named street.data, where the road segments
are represented by a pair of locations bounded by a set
of brackets. The TrafficGeneration function prepares
a customized query script for the SECONDO database,
i.e., BerlinMOD_DataGenerator_RequestID.SEC. It
does so by preparing a generic script in advance, and updates
the parameters based on the user request. Then, the function
runs the modified script to generate the traffic data. The
TrafficResultConvert function converts the traffic data
produced by BerlinMOD into the standard output format.

Brinkhoff traffic generator is one of most widely used
traffic generators [1] (cited 650+ per Google Scholar),
where its general idea is to simulate the object movements from two random locations using the shortest
path. To support Brinkhoff generator inside MNTG, we
implement its three abstract functions as follows: The
RoadNetworkConvert function converts the output of
the Road Network Converter into two binary files based
on the need of Brinkhoff, i.e., request_ID.node and
request_ID.edge. The TrafficGeneration function
prepares Brinkhoff configuration file, i.e., property.txt,
and assembles the calling command based on the user’s request. The TrafficResultConvert function converts the
traffic data produced by Brinkhoff generator into our standard
output format.

The audience interact with MNTG through the following
steps: (1) traffic request submission, where audience can submit their requests online by selecting the traffic generation
area from either Google Maps or OpenStreetMap interface
and the traffic generator method, (2) traffic data download and
visualization, where audience can either download or visualize
their requested data, (3) adding a new road network data
source, where audience can add a new road network data
source to MNTG, and (4) adding a new traffic generator,
where audience can register a new traffic generator, and then
uses it in their new traffic generation request.

C. Case Study 2: BerlinMOD Generator

A. Traffic Request Submission

BerlinMOD is another very popular traffic generator [3],
where it simulates human movements during the weekdays
and weekends. Users can specify their work and home areas
in the road networks, then the generator simulates the users
movements based on two rules: (1) during the weekdays, a
user leaves Home in the morning (at 8 a.m.+ T1), drives to
Work, stays there until 4 p.m.+ T2 in the afternoon, and then
returns back Home, (2) during the weekends, a user has an
0.4 probability to do an additional trip which may have 1-3
intermediate stops and ends at home.

V.

D EMONSTRATION S CENARIO

Figure 2 gives the interface for MNTG. To generate data,
a user performs the following four steps: (1) either drag/zoom
the map or write an address in the search field to get the
geographical area of interest, (2) draw a rectangle on the map
for the traffic generation area, (3) select the traffic generator
from the drop down menu, e.g., Brinkhoff or BerlinMOD, and
(4) Click on the Generate button, and enter the parameters for
the traffic simulation, including the user email address.
Once the request is submitted, MNTG sends an email
to the user acknowledging the receipt of that request. Upon

Map, which is given to the user as a verification for the new
data to import.
D. Traffic Processor Extension

Fig. 3.

MNTG Traffic Visualization (OpenStreetMap Interface)

(a) Extracted Nodes

Fig. 4.

(b) Extracted Edges

Adding New Road Network Data Source

completion of the traffic request, another email is sent to the
user with two hyper links. One for downloading the generated
data, and one for visualizing the data on MNTG web interface.
The time MNTG takes to complete the request mainly depends
on the request size and the underlying traffic generator, e.g.,
number of objects and simulation time.
B. Traffic Data Download & Visualization
Audience can use MNTG to download or visualize their
requested data. Downloaded data will have a standard text
format as follows:
OID TS Type
0
0 newpoint
1
0 newpoint

Lat
44.98636
44.99894

Lng
-93.29820
-93.18128

MNTG also stores the generated traffic data inside a
MySQL database, which can be used for visualization. Traffic
visualization is performed using Google Maps v3 API for
displaying overlays in HTML. The data is loaded via ajax
into the web page which then creates an overlay for each
time stamp of the moving object. Figure 3 gives an example
for traffic visualization, where the colored circles on the map
represent the moving objects. The user can either view the
animated object movements or move the handler on the bottom
scroll to view the snapshot at a specific time.
C. Adding a Road Network Data Source
The demo attendees can add a new road network data
source to MNTG. We have a prepared data set, termed
HighwayRoadNetwork, which includes only the highways in
US. We will show the audience how to register this new
road network with MNTG using two abstract functions. Once
registered, audience will see that they can submit a new request
to MNTG using the new road network. Figure 4 depicts the
nodes and edges of the HighwayRoadNetwork on a Google

The audience may register a new traffic generator to
MNTG. For demonstration purpose, we prepared a new traffic
generator, termed RandomTraffic, which requires binary format
of road networks, runs with a java file, and outputs the traffic
result in a binary format. The RandomTraffic generator takes
its input as the number of moving objects and average speed.
Then, it selects random points as one per each moving object.
For each random point, we select another random destination,
and calculate the shortest path. Given the average speed,
objects move along the shortest path till their destinations.
We will show the audience the java template class file for
the RandomTraffic generator, which includes the implementation of its three abstract functions, RoadNetworkConvert,
RandomTraffic, and TrafficGeneration. Then, we will
upload this file to MNTG, which indicates that a new traffic
generator is registered. Audience will then restart MNTG to
see that they can submit a new traffic generator request using
the RandomTraffic generator.
R EFERENCES
[1] T. Brinkhoff. A Framework for Generating Network-based Moving
Objects. GeoInformatica, 6(2), 2002.
[2] S. Chen, C. S. Jensen, and D. Lin. A Benchmark for Evaluating Moving
Object Indexes. VLDB Journal, 1(2), 2008.
[3] C. Düntgen, T. Behr, and R. H. Güting. BerlinMOD: a Benchmark for
Moving Object Databases. VLDB Journal, 18(6), 2009.
[4] R. H. Güting, T. Behr, and C. Düntgen. Secondo: A platform for moving
objects database research and for publishing and integrating research
implementations. IEEE Data Engineering Bulletin, 33(2), 2010.
[5] H. Hu, J. Xu, and D. L. Lee. PAM: An Efficient and Privacy-Aware
Monitoring Framework for Continuously Moving Objects. IEEE TKDE,
22(3), 2010.
[6] H. Jeung, Q. Liu, H. T. Shen, and X. Zhou. A Hybrid Prediction Model
for Moving Objects. In ICDE, 2008.
[7] M. F. Mokbel, L. Alarabi, J. Bao, A. Eldawy, A. Magdy, M. Sarwat,
E. Waytas, and S. Yackel. MNTG: An Extensible Web-based Traffic
Generator. In SSTD, 2013.
[8] M. F. Mokbel, X. Xiong, and W. G. Aref. SINA: Scalable Incremental
Processing of Continuous Queries in Spatio-temporal Databases. In
SIGMOD, 2004.
[9] OpenStreetMaps. http://www.openstreetmap.org/.
[10] H.-P. Tsai, D.-N. Yang, and M.-S. Chen. Mining Group Movement
Patterns for Tracking Moving Objects Efficiently. IEEE TKDE, 23(2),
2011.
[11] US TIGER LINES. http://www.census.gov/geo/maps-data/data/tigerline.html.
[12] W. Wu, W. Guo, and K.-L. Tan. Distributed Processing of Moving
K-Nearest-Neighbor Query on Moving Objects. In ICDE, 2007.
[13] Y. Zheng, Y. Chen, X. Xie, and W.-Y. Ma. GeoLife2.0: A LocationBased Social Networking Service. In MDM, 2009.

