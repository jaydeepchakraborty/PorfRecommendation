Sparse Latent Space Policy Search
Kevin Sebastian Luck
Arizona State University Interactive Robotics Lab AZ 85281 Tempe, USA mail@kevin-luck.net

Joni Pajarinen
Aalto University Intelligent Robotics Group 02150 Espoo, Finland Joni.Pajarinen@aalto.fi

Erik Berger
Technical University Bergakademie Freiberg Institute of Computer Science 09599 Freiberg, Germany erik.berger@informatik.tu-freiberg.de

Ville Kyrki
Aalto University Intelligent Robotics Group 02150 Espoo, Finland ville.kyrki@aalto.fi Abstract
Computational agents often need to learn policies that involve many control variables, e.g., a robot needs to control several joints simultaneously. Learning a policy with a high number of parameters, however, usually requires a large number of training samples. We introduce a reinforcement learning method for sampleefficient policy search that exploits correlations between control variables. Such correlations are particularly frequent in motor skill learning tasks. The introduced method uses Variational Inference to estimate policy parameters, while at the same time uncovering a lowdimensional latent space of controls. Prior knowledge about the task and the structure of the learning agent can be provided by specifying groups of potentially correlated parameters. This information is then used to impose sparsity constraints on the mapping between the high-dimensional space of controls and a lowerdimensional latent space. In experiments with a simulated bi-manual manipulator, the new approach effectively identifies synergies between joints, performs efficient low-dimensional policy search, and outperforms state-of-the-art policy search methods.

Heni Ben Amor
Arizona State University Interactive Robotics Lab AZ 85281 Tempe, USA hbenamor@asu.edu in a lower-dimensional latent space for control which, in turn, reduces cognitive effort and training time during skill acquisition. The existence of synergies has been reported in a variety of human motor tasks, e.g., grasping (Santello, Flanders, and Soechting 1998), walking (Wang, O'Dwyer, and Halaki 2013), or balancing (Torres-Oviedo and Ting 2010). Recently, various synergy-inspired strategies have been put forward to improve the efficiency of RL for motor skill acquisition (Bitzer, Howard, and Vijayakumar 2010; Kolter and Ng 2007). Typically, these approaches use dimensionality reduction as a pre-processing step in order to extract a lower-dimensional latent space of control variables. However, extracting the latent space using standard dimensionality reduction techniques requires a significantly large training set of (approximate) solutions, prior simulations, or human demonstrations. Even if such data exists, it may drastically bias the search by limiting it to the subspace of initially provided solutions. In our previous work, we introduced an alternative approach called latent space policy search that tightly integrates RL and dimensionality reduction (Luck et al. 2014). Using an expectation-maximization (EM) framework (Dempster, Laird, and Rubin 1977) we presented a latent space policy search algorithm that iteratively refines both the estimates of the low-dimensional latent space, as well as the policy parameters. Only samples produced during the search process were used. In this paper, we propose a different kind of latent space policy search approach, which similarly to our previous work combines RL and dimensionality reduction, but which also allows for prior structural knowledge to be included. Our method is based on the Variational Bayes (VB) (Neumann 2011; van de Meent et al. 2015) framework. Variational Bayes is a Bayesian generalization of the expectationmaximization algorithm, which returns a distribution over optimal parameters instead of a single point estimate. It is a powerful framework for approximating integrals that would otherwise be intractable. Our RL algorithm exploits these properties in order to (1) perform efficient policy search, (2) infer the low-dimensional latent space of the task, and

Introduction
Reinforcement learning (RL) is a promising approach to automated motor skill acquisition (Peters et al. 2011). Instead of a human hand-coding specific controllers, an agent autonomously explores the task at hand through trial-and-error and learns necessary movements. Yet, reinforcement learning of motor skills is also considered to be a challenging problem, since it requires sample-efficient learning in highdimensional state and action spaces. A possible strategy to address this challenge can be found in the human motor control literature (Bernstein 1967). Research on human motor control provides evidence for motor synergies; joint coactivations of a set of muscles from a smaller number of neural commands. The reduction in involved parameters results
Copyright c 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.

Time Group 1 Group 2 Group 3 Group 4 Time

Samples

Figure 1: The main idea of Group Factor Policy Search: A number of variables, for example the joints of an arm or leg of a NAO robot, form one group. Given several of such groups for the action vector (left matrix) the transformation matrix W can be divided in several submatrices corresponding to those groups. Subsequently each factor, given by a column in W, encodes information for all groups, e.g. four in the example given above. Factors may be non-zero for all groups, for a subset of groups, for exactly one group or zero for all groups. In the figure, grey areas correspond to non-zero values and white areas to zero values in the sparse transformation matrix. The transformation matrix is multiplied by the latent variables given by ~ = (~ ~t , · · · , z ~T ) distributed by z ~t  N (0, trace((st , t)(st , t)T )I). Z z1 , · · · , z (3) incorporate prior structural information. Prior knowledge about locality of synergies can be included by specifying distinct groups of correlated sub-components. Often such prior knowledge about groups of variables, e.g. coactivated joints and limbs, is readily available from the mechanical structure of a system. Structural prior knowledge is also common in other application domains. For example, in a wireless network the network topology defines receiver groups (Sagduyu and Ephremides 2004). Our approach draws inspiration and incorporates ideas from Factor Analysis, in particular Group Factor Analysis (Klami et al. 2015), as can be seen in Fig. 1. Groups of variables, e.g., robot joints grouped into arms and legs, are provided as prior structural knowledge by a user. A factorized control policy is then learned through RL, which includes a transformation matrix W. The transformation matrix holds factors that describe dependencies between either all of the groups or a subset of them. The individual factors can be regarded as synergies among the joints of the robot. We will show that the resulting algorithm effectively ties together prior structural knowledge, latent space identification, and policy search in a coherent way.

E [r = 1] =

p(, )p(r = 1|)dd,

(1)

where the probability of the trajectory p(, ) contains the (stochastic) policy, r is a binary variable indicating maximum reward, and p(r = 1|)  exp {-c ()} (Toussaint 2009) is the conditional probability of receiving maximum expected reward given a cost function. Assuming the Markov property and the independence of actions, the probability of a trajectory can be written as
T

p(, ) = p()p(s1 )
t=1

p(st+1 |st , at ) (at |st , ).

(2)

Policy Search
Policy search methods try to find an optimal policy for an agent which acts in an uncertain world with an unknown world model. At each time step t the agent executes an action at in state st and moves to the next state st+1 with probability p(st+1 |st , at ). After executing a certain number of actions, the agent receives a reward feedback given by an unknown reward function based on the performed execution trace (or trajectory/history)  = (s1 , a1 , . . . , sT , aT , sT +1 ). The overall objective in policy search is to maximize the expected reward over trajectories and policy parameters . For bounded rewards, maximizing expected reward is equivalent to maximizing the probability of a binary reward r (Toussaint and Storkey 2006):

The stochastic policy  (at |st , ) depends on the parameters  for which we additionally introduce prior distributions p(). This formulation will subsequently be used for structuring the policy model. The prior distributions may also depend on hyperparameters ­ for reasons of clarity, however, we will omit any such parameters below. Furthermore, we assume that the initial state distribution p(s1 ) and transition dynamics p(st+1 |st , at ) are unknown but fixed. Thus, they will cancel out as constant values.

Group Factor Policy Search
We will now introduce a new policy search method, called Group Factor Policy Search (GrouPS ), that uncovers the latent space on-the-fly based on prior structural information. In this section, we discuss how to incrementally improve the policy and the actual form of the new policy model. We parameterize the policy using Group Factor Analysis (Klami et al. 2015) in order to utilize prior information about the parameters and their correlations. Since our policy is a linear stochastic model with prior distributions, we first present a novel general Variational Inference framework for policy search that takes priors into account. Subsequently, we discuss how the policy is parameterized, and finally show

the policy model update equations for Group Factor Policy Search which we derive using the introduced Variational Inference method.

Variational Inference for Policy Search
In each iteration our new policy search method samples a distribution over trajectories pold () using the current policy, and based on pold () finds a new policy which maximizes a lower bound on the expected reward. This is repeated until convergence. In order to find a new policy based on samples from the old one, we introduce the sampling distribution pold () and the approximated parameter distribution q () (defined later) into Equation 1. By applying the log-function and using Jensen's inequality (Kober and Peters 2009; Bishop 2006, Eq. (1.115)) we derive the lower bound log  p(, ) p(r = 1|)dd pold ()q () p(, ) p(r = 1|)dd. pold ()q () log pold ()q () (3) pold ()q ()

Input: Reward function R (·) and initializations of parameters. Choose number of latent dimension n. Set fixed hyper-parameters ~  a , b ~ , a , b ,  2 and define groupings.
1 2 3 4 5 6

7 8 9 10 11 12 13 14 15 16 17 18

while reward not converged do for h=1:H do # Sample H rollouts for t=1:T do at = Wi Z + M + E with Z  N (0, I) and E  N (0,  ~), (m) where  ~ = ~m I Execute action at Observe and store reward R () Initialization of q-distribution while not converged do Update q (M) with Eq. (16) Update q (W) with Eq. (19) ~ with Eq. (22) Update q Z Update q () with Eq. (12) Update q ( ~) with Eq. (25) M = Eq(M) [M] W = Eq(W) [W]  = Eq() []  ~ = Eq( ~] ~ ) [ Result: Linear weights M for the feature vector , representing the final policy. The columns of W represents the factors of the latent space.

Since pold () was generated using the old policy it does not depend on  and we can simplify the lower bound to pold ()q () log = const +   · log   p ()
t=1

p(, ) pold ()q ()

p(r = 1|)dd

19 20

pold ()q ()
T

 (at |, st ) q ()

   p(r = 1|)dd, 

(4)

Algorithm 1: Outline of the Group Factor Policy Search (GrouPS) algorithm. et al. 2015). The main idea of GFA is to introduce prior distributions for the parameters, in particular a prior for a structured transformation matrix W. The transformation matrix, responsible for mapping between a low-dimensional subspace and the original high-dimensional space, is forced to be sparse and constructed using prior knowledge about grouping of the dimensions, that is, W is a concatenation of transform matrices W(m) for each group m. For example, if the dimensions of a vector represent the joints of a legged robot, we can group joints belonging to the same leg into the same group (see e.g. Fig. 1). Applying the idea of Group Factor Analysis for directed sampling leads to a linear model, i.e. a stochastic policy at
(m)

where the constant term can be ignored for the maximization of this term. By assuming the factorization q () = qi (i ) for the parameters and applying the Variational Bayes approach, we get the approximated distributions of the parameters: log qj (j ) = const +
- j T i=j

qi (i ) (5) p(r = 1|) R dd-j ,

pold () log
t=1

 (at , |st )

where the parameter vector -j contains all parameters except j . The normalization constant R is given by the integral R= pold ()p(r = 1|)d. (6)

= W(m) Zt + M(m) + Et
(m)

(m)

 (st , t) ,

(7)

Formulation of Group Factor Policy Search
In order to identify sets of correlated variables during policy search, we use a linear stochastic policy of a form similar to the model used in Group Factor Analysis (GFA) (Klami

where, for group m, the action at  RDm ×1 is a linear projection of a feature vector  (st , t)  Rp×1 . Each dimension of the feature vector is given by a basis function, which may depend on the current state and/or time. In the remainder of the paper, we will write  instead of  (s, t) for simplicity, even though there is an implicit dependency

of  on the current state of a trajectory. W(m)  RDm ×l is a transformation matrix mapping from the l-dimensional subspace to the original space. Each entry of the latent matrix Zt  Rl×p is distributed according to a standard normal distribution where N (0, 1), M(m)  RDm ×p is the mean (m) matrix, and the entries of the noise matrix Et  RDm ×p -1 are distributed by N (0,  ~m ). We can derive a stochastic policy from the model defined in Equation 7. Since Zt   N (0, trace(T )I) (8) Figure 2: Graphical model in Plate notation for Group Factor Policy Search. The basis functions (st , t) as well as the (m) action vector at are observed. Equation 9 shows the de(m) ~t depend on the pendencies for at . The latent variables z feature vector as stated in Equation (8) . The parameter m might either be given by a Gamma distribution as stated in Equation (12) or by a log-linear model with dependencies on parameters U and V. The hyper-parameters a and b are fixed and set to a small positive value. The prior distributions given above will lead to three kind of factors: (1) factors which are nonzero for only one group, (2) factors which are nonzero for several groups or (3) factors which are zero. In addition to the standard GFA prior distributions above, we introduce further ~ such that all prior distribuprior distributions for M and z tions are given with M  N Mold ,  2 I , m,k  G (a , b ) , ~  N 0, Tr T I , z
~   ~m  G a , b~ .

holds (see e.g. (Luck et al. 2014)), we can substitute Zt  by ~t  Rl×1 resulting in the policy the random variable z  (at |, st ) = 
M

N
m=1

m) a( t

W

(m)

~t + M z

(m)

Tr T ,  ~m

 I .

(9)

~t If we take a closer look at the latent space given by Wz we first find that the length of each factor is determined by (st , t) 2 2 . Secondly, a factor may be non-zero only for one or a subset of groups as can be seen in Fig. 1. This leads to a sparse transformation matrix and specialized factors and dimensions. As mentioned before, the form of our linear model in Equation 7 above is based on Group Factor Analysis. While GFA typically maps a vector from the latent space to the high-dimensional space, we map here a matrix from the latent space to the original space and then use this matrix as a linear policy on the feature vectors. GFA does not apply factor analysis (see e.g. (Harman 1976)) on each group of variables separately, but instead introduces a sparsity prior on the transformation matrix W thereby forcing correlations between groups:
M K Dm -1 , N wd,k 0, m,k m=1 k=1 d=1 (m)

Fig. 2 shows a graphical model of Group Factor Policy Search, given by the distributions stated above. Instead of ~t is used, which depends on (st , t) Z the latent variable z given a state and a point in time.

p ( W |) =

(10)

Derivation of Update Equations
~ ~ We assume fixed hyper-parameters a , b , a and b for the distributions which we determine using the Variational Inference method presented earlier, assuming a factorization of the q-distributions

with M being number of groups, Dm the number of dimensions of the m-th group and K the number of factors, i.e. number of columns of W. The precision  is given by a log-linear model with log  = UVT + µu 1T + 1µT v, (11)

~ )q (W) q ( q () = q (Z ~) q (M) q ()
T

(15)

where U  RM ×R , V  RK ×R and µu  RM as well as µv  RK model the mean profile. R defines the rank of the linear model and is chosen R min (M, K ). However, for the special case of R = min (M, K ) the precision is given by a simple gamma distribution (Klami et al. 2015)
 q (m,k ) = G a m , bm,k

(12)

with parameters
 a m =a +

Dm , 2

(13) (14)

~) = and additionally the assumption q (Z q (~ zt ) with ~ :,t = z ~t . Z By using the factorization given above and the Variational Inference rule for deriving the parameter distribution in Equation (5), we can derive the approximated parameter distributions, which maximize the expected reward. The approximated distribution for the mean matrix is given by a multiplicative normal distribution
M Dm

b m,k

1 (m) T (m) = b + Eq(W) wk wk . 2


qM (M) =
m=1 j =1

N

mj,:

(m) T

M µM mj , j

(16)

where the mean and covariance parameters in dependency of the group and dimension are given by M j =  -2 I +   p ( r = 1 |  )   R -1 E m ] ~ [~ (17) and µM mj =  M j moldj,: T + M · j · 2
T

and
Z µZ t = t ·   T (m) (m) (m) M E a - M  W W t   .  -1 T Tr  E m ] m=1 ~ [~ ~ ~

T

(24)

Ep()

T Tr T

t=1

Unlike the other distributions, the precision is given by a multiplicative gamma distribution q ) = ~ (~ 1 1 ~ ~ ~ G  ~m |a + Dm T, b + b 2 2 m m=1
M

(25)

Ep()

 p(r = 1|) R

 at,j - Ew wj,:

(m)

(m)

Ez zt ] ~ [~
-1

 

t=1

Tr T E m ] ~ [~

with one fixed parameter and one variable parameter. Esti~ mation of the parameter b m is the most complex and computationally expensive operation given by
~ b m = Ep()

(18) with mj,: given by the j -th row of M. The q-distribution for the transformation matrix is similarly given by
M Dm

p(r = 1|) R M
(m)

T

Tr T
t=1

-1

at

(m) T (m) at

-

(m) T 2at EM T


T

qW (W) =
m=1 j =1

N

(m) T W wj,: |µW mj , m

(19)

+ 2Ez zt ] EW W(m) ~ [~ - 2at
(m) T

EM M(m) 

with the mean and covariance parameters W m =  
t=1 T

EW W(m) Ez zt ] ~ [~
T

Ep()

p(r = 1|) R -1
-1

+ T EM M(m) M(m)  (20) , ¯ m,K  + + Tr EW W(m) W(m) Covz zt ] ~ [~ zt ] EW W(m) W(m) Ez +E z zt ] ~ [~ ~ [~
T T T

~t z ~T Ez ~ z t Tr T E m ] ~ [~

. (26)

and
W µW mj = m · Ep() T (m)

p(r = 1|) R
(m) T

Algorithm
(21) . Algorithm 1 summarizes all update steps for performing Group Factor Policy Search. The reward function R (·), number n of latent dimensions, and a set of hyperparameters need to be provided by the user.

at,j - EM mj,:

zt ]  Ez ~ [~
-1

t=1

Tr T E m ] ~ [~

¯ m,K ) = The diagonal matrix  ¯ m,K is given by diag ( (m,1 , m,2 , · · · , m,K ). The distribution for the latent ~ depends on the trajectory and time. Hence the variables Z ~ of a multireward can be seen as a probabilistic weight R plicative normal distribution. However, since we assume in~h dependent latent variables z t we can ignore the reward and get
H T

Evaluation
For evaluations and experiments the expectation Ep() [·] used above in Eq.(16-20,25) was approximated by a sample mean, H 1 f (i ) (27) Ep() [f ()]  H i=1 as proposed in (Kober and Peters 2009), where i is the ith of the H realized trajectories and f () a function value, vector or matrix for i and will be replaced by the parameter approximations given above.

~ qZ ~ Z =

~ R
t=1

N

~ ~ Z Z ~h z t |µt , t

,

(22)

with time-dependent parameters Z t =
M ~

Tr T
T

-1

I+ -1  , (23)

Setup of the Evaluation
For the comparison between the above presented GrouPS algorithm and previous policy search algorithms, a simulated task of a bi-manual robot operating in a planar task space was used. Each of the two arms (see Fig. 3) has six degreesof-freedom and the same base for the first joint. The initial

EW W(m) W(m)
-1 Tr T E ~m ~m 

m=1

Figure 3: Two simulated arms with six degrees-of-freedom and the same base in their initial position. Each end effector has a desired position for each time step, s shown by the green and red dots. The final position at time step 25 is given by the coordinate (0, 4). The numbers represent the joints with l for left and r for right.
180 160 140
1 Group POWER NAC 2 Groups PePPEr 4 Groups

Sum. Distances

120 100 80 60 40 20 0

physical robotic experiments (Kober and Peters 2011). PePPEr is also based on EM and incorporates policy search and dimensionality reduction, but without priors and thus without a structured transformation matrix. For comparison with PePPEr and PoWER the GrouPS algorithm was evaluated in three different configurations: (1) One group which contains all joints of both arms, (2) two groups, where each group contains the joints of one arm and (3) four groups with two groups per arm and joints 1-4 in one and joints 5-6 in the second group. The hyper-parameters of GrouPS were set to ~ ~ a = b = 1000, a = b = 1 and  2 = 100. No optimizations of the hyper-parameters were performed. Furthermore, to prevent early convergence and collapsing of the distributions due to small sample sizes the parameter W and  ~ are resized after each iteration by a factor of 1.5. The same is done after  each iteration for PePPEr. However, the factor was set to 1.09 since higher numbers lead to divergence in the parameters of the algorithm with unstable and divergent results. PePPEr was implemented as presented in (Luck et al. 2014) and in each iteration 20 inner iterations for the optimizations of the parameters were used. The same setup was used for GrouPS and for both algorithms the number of latent dimensions were set to six. The static variance parameter for PoWER as presented in (Kober and Peters 2009) and the initial variance of the other algorithms were all set to 101.5 , also for NAC with learning parameter set to 0.5. In each iteration, we sampled 30 trajectories and evaluated the trajectories based on the reward function
25

R() =
t=1
0 200 400 600 800 1000

exp (- · exp (-

effl (at ) - posl (t) effr (at ) - posr (t)

2) 2) ,

(28)

Iterations

Figure 4: Comparison between PePPEr, PoWER, Natural Actor-Critic and three instances of the GrouPS algorithm on the presented simulated task. Values correspond to the summarized distances between each end effector and its desired position given the current policy for the iteration. The mean value as well as the standard deviations are shown. configuration of the arms is presented in Fig. 3 as well as the desired positions for each end effector (tip of an arm). At each of the 25 time steps we give a different goal position for each arm's end effector, starting from the left for the left arm and starting from the right for the right arm, with the same final position at (0, 4) for both arms. In this task, the 12 dimensions of the action vector a represent the joint angles for each arm. For the basis functions eleven isotropic Gaussian distributions were used with i (t) = N (t|µ i , 3) for t  {1, 2, . . . , 24, 25}. In total, 132 parameters have to be estimated given M  R12×11 . As reference algorithms PoWER (Kober and Peters 2009), Natural Actor-Critic (NAC) (Peters and Schaal 2008) and PePPEr (Luck et al. 2014) were chosen: NAC is a policy gradient method while PoWER is an efficient policy search method based on expectation maximization (EM). PoWER has been experimentally validated in both simulated and

where the function effl (at ) returns the position of the left end effector given the action vector and posl (t) the corresponding desired goal position for time point t. effr (at ) and posr (t) return the actual and desired positions, respectively, for the right end effector. Then the 15 best trajectories are chosen for the computation of the parameters for each algorithm as described in (Kober and Peters 2009).

Results
Fig. 4 depicts the results of the explained experiment. For each algorithm ten different runs were executed and both mean and standard deviation computed. As can be seen in the figure, PePPEr outperforms both PoWER and NAC, as well as our method in case only one group spanning all variables is used. However, using two groups (one for each arm) already leads to comparable performance. Finally, the GrouPS algorithm with 4 different groups significantly outperforms the comparison methods.

Importance of the Choice of Groups
In order to investigate the effect of choosing joint groups we conducted an additional experiment. Our working hypothesis throughout the paper is that structural information about inherent groups of correlated variables will improve the search. Conversely, if we provide wrong in-

180 160 140
Swap1 Swap2 Swap3 4 Groups

Sum. Distances

120 100 80 60 40 20 0

0

200

400

600

800

1000

Iterations

Figure 7: Final policy found by the GrouPS algorithm after 100 iterations. A high reward is given if the head as well as the left foot of the robot are high above the ground. sult corroborates our assumption that a proper selection of groups can ameliorate the performance of the policy search algorithm.

Figure 5: Comparison between the original chosen four groups and three permutations of the Groups. Values correspond to the summarized distance between each end effector and its desired position for each time step given the current policy for the iteration.
180 160 140
Swap4 Swap5 4 Groups

Experiment: Lifting a Leg
To test the GrouPS algorithm in experiments following the real world closely, we reproduced the experiment stated in (Luck et al. 2014): We simulate a NAO robot (Gouaillier et al. 2008) using the V-REP framework (Rohmer, Singh, and Freese 2013) in the task of lifting its left leg without falling. The same reward function was used as presented in (Luck et al. 2014, Eq. (22)) with parameters  = 5,  = 10,  = 10 and max = 6. The V-REP framework (Rohmer, Singh, and Freese 2013) allows for simulations with high physical accuracy by utilizing the bullet physics library. In this experiment, the actions represent the 26 joint velocities for each of the 15 points in time. Again, for feature functions Gaussian distributions were used and the same parameters for GrouPS were chosen like given in the evaluation above. We ran GrouPS for 100 iterations. In each iteration, we used a set of 20 samples, of which ten were randomly selected from the set of 20 in the previous iteration and ten generated by the current policy. We used ten best samples out of this set of 20 for computing the new policy parameters. The groups were created in such a manner that the joints of each arm or leg form a single group as well as the joints of the head. The results are given in Fig. 7, where we find that the GrouPS algorithm is able to find a satisfactory solution even with a relatively small number of samples: the head and left leg of the NAO robot are at high positions corresponding to a high reward.

Sum. Distances

120 100 80 60 40 20 0

0

200

400

600

800

1000

Iterations

Figure 6: Comparison between the original grouping and two other variants with a different splitting point. Again, the values represent the summarized distances and shaded ares corresponds to the standard deviation given ten executions. formation about groupings the performance of the algorithm should deteriorate. To evaluate this hypothesis, we took the original partitioning of the joints into four groups and swapped two, later three pairs of joints randomly. As described above, the original group partitioning is {(1l, 2l, 3l, 4l), (5l, 6l), (1r, 2r, 3r, 4r), (5r, 6r)}. Performing two random swaps between the left and right side results in {(1l, 2l, 2r, 4l), (5l, 5r), (1r, 3l, 3r, 4r) , (6l, 6r)} (Fig. 6, Swap4). For three swaps the resulting partition is {(1l, 6r, 2r, 4l), (3r, 6l), (1r, 3l, 5l, 4r), (5r, 2l)} (Fig. 6, Swap5). Furthermore, three other groupings with different splitting points were evaluated: {(1l, 2l), (3l, 4l, 5l, 6l), (1r, 2r), (3r, 4r, 5r, 6r)} (Fig. 5, Swap1), {(1l, 2l), (3l, 4l), (5l, 6l), (1r, 2r), (3r, 4r), (5r, 6r)} (Fig. 5, Swap2) and {(1l, 2l, 3l), (4l, 5l, 6l), (1r, 2r, 3r), (4r, 5r, 6r)} (Fig. 5, Swap3). The result of executing GrouPS with these groupings can be seen in Fig. 5 and Fig. 6. All new groupings (resulting from above swaps) are clearly outperformed by the original partition. This re-

Conclusion and Future Work
In this paper, we introduced a novel algorithm for reinforcement learning in low-dimensional latent spaces. To this end, we derived a Variational Inference framework for policy search that takes prior structural information into account. The resulting policy search algorithm can efficiently learn new policy parameters, while also uncovering the underlying latent space of solutions, and incorporating prior knowledge about groups of correlated parameters. In experiments using motor skill learning tasks, we showed that the introduced GrouPS algorithm efficiently learns new motor skills. It significantly outperformed state-of-the-art policy

search methods, whenever prior information about structural groups was provided. So far, the dimensionality of the latent space needs to be provided as a parameter to the reinforcement learning algorithm. We plan to investigate automatic adjustments of the dimensionality using current rewards. In this paper, we focused on intra-group correlations. In future work, we plan to investigate correlations among extracted group factors, e.g., correlations between arms and legs.

Acknowledgments
J.Pajarinen and V.Kyrki were supported by the Academy of Finland, decision 271394.

References
Bernstein, N. A. 1967. The co-ordination and regulation of movements. Pergamon Press. Bishop, C. M. 2006. Pattern recognition and machine learning. Springer. Bitzer, S.; Howard, M.; and Vijayakumar, S. 2010. Using dimensionality reduction to exploit constraints in reinforcement learning. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 3219­3225. IEEE. Dempster, A. P.; Laird, N. M.; and Rubin, D. B. 1977. Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society. Series B (Methodological) 39(1):1­38. Gouaillier, D.; Hugel, V.; Blazevic, P.; Kilner, C.; Monceaux, J.; Lafourcade, P.; Marnier, B.; Serre, J.; and Maisonnier, B. 2008. The nao humanoid: a combination of performance and affordability. arXiv preprint arXiv:0807.3223. Harman, H. H. 1976. Modern factor analysis. University of Chicago Press. Klami, A.; Virtanen, S.; Leppaaho, E.; and Kaski, S. 2015. Group factor analysis. IEEE Transactions on Neural Networks and Learning Systems 26(9):2136­2147. Kober, J., and Peters, J. 2009. Policy search for motor primitives in robotics. In Advances in Neural Information Processing Systems (NIPS), 849­856. Kober, J., and Peters, J. 2011. Policy search for motor primitives in robotics. Machine Learning 84(1):171­203. Kolter, J. Z., and Ng, A. Y. 2007. Learning omnidirectional path following using dimensionality reduction. In Proceedings of the Robotics: Science and Systems (R:SS) conference. The MIT Press. Luck, K. S.; Neumann, G.; Berger, E.; Peters, J.; and Ben Amor, H. 2014. Latent space policy search for robotics. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 1434­ 1440. IEEE. Neumann, G. 2011. Variational inference for policy search in changing situations. In Proceedings of the 28th International Conference on Machine Learning (ICML), 817­824. Peters, J., and Schaal, S. 2008. Natural actor-critic. Neurocomputing 71(7):1180­1190.

Peters, J.; M¨ ulling, K.; Kober, J.; Nguyen-Tuong, D.; and Kr¨ omer, O. 2011. Towards motor skill learning for robotics. In Robotics Research. Springer. 469­482. Rohmer, E.; Singh, S. P.; and Freese, M. 2013. V-REP: A versatile and scalable robot simulation framework. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 1321­1326. IEEE. Sagduyu, Y. E., and Ephremides, A. 2004. The problem of medium access control in wireless sensor networks. IEEE Wireless Communications 11(6):44­53. Santello, M.; Flanders, M.; and Soechting, J. 1998. Postural hand synergies for tool use. The Journal of Neuroscience 18(23). Torres-Oviedo, G., and Ting, L. H. 2010. Subject-specific muscle synergies in human balance control are consistent across different biomechanical contexts. Journal of Neurophysiology 103(6):3084­3098. Toussaint, M., and Storkey, A. 2006. Probabilistic inference for solving discrete and continuous state Markov Decision Processes. In Proceedings of the 23rd International Conference on Machine Learning (ICML), 945­952. Toussaint, M. 2009. Robot trajectory optimization using approximate inference. In Proceedings of the 26th annual International Conference on Machine Learning (ICML), 1049­1056. ACM. van de Meent, J.-W.; Tolpin, D.; Paige, B.; and Wood, F. 2015. Black-box policy search with probabilistic programs. arXiv preprint arXiv:1507.04635. Wang, X.; O'Dwyer, N.; and Halaki, M. 2013. A review on the coordinative structure of human walking and the application of principal component analysis. Neural Regeneration Research 8(7):662­670.

All in-text references underlined in blue are linked to publications on ResearchGate, letting you access and read them immediately.

2015 IEEE International Conference on Robotics and Automation (ICRA) Washington State Convention Center Seattle, Washington, May 26-30, 2015

Learning Multiple Collaborative Tasks with a Mixture of Interaction Primitives
Marco Ewerton1 , Gerhard Neumann1 , Rudolf Lioutikov1 , Heni Ben Amor2 , Jan Peters1,3 and Guilherme Maeda1

Abstract-- Robots that interact with humans must learn to not only adapt to different human partners but also to new interactions. Such a form of learning can be achieved by demonstrations and imitation. A recently introduced method to learn interactions from demonstrations is the framework of Interaction Primitives. While this framework is limited to represent and generalize a single interaction pattern, in practice, interactions between a human and a robot can consist of many different patterns. To overcome this limitation this paper proposes a Mixture of Interaction Primitives to learn multiple interaction patterns from unlabeled demonstrations. Specifically the proposed method uses Gaussian Mixture Models of Interaction Primitives to model nonlinear correlations between the movements of the different agents. We validate our algorithm with two experiments involving interactive tasks between a human and a lightweight robotic arm. In the first, we compare our proposed method with conventional Interaction Primitives in a toy problem scenario where the robot and the human are not linearly correlated. In the second, we present a proof-of-concept experiment where the robot assists a human in assembling a box.

Holding tool Plate handover Human trajectories

Screw handover Robot trajectories

Plate handover Holding tool Screw handover Plate handover Holding tool Screw handover

Human trajectories

Robot trajectories

I. I NTRODUCTION Robots that can assist us in the industry, in the household, in hospitals, etc. can be of great benefit to the society. The variety of tasks in which a human may need assistance is, however, practically unlimited. Thus, it is very hard (if not impossible) to program a robot in the traditional way to assist humans in scenarios that have not been exactly prespecified. Learning from demonstrations is therefore a promising idea. Based on this idea, Interaction Primitive (IP) is a framework that has been recently proposed to alleviate the problem of programming a robot for physical collaboration and assistive tasks [1], [2]. At the core, IPs are primitives that capture the correlation between the movements of two agents--usually a human and a robot. Then, by observing one of the agents, say the human, it is possible to infer the controls for the robot such that collaboration can be achieved. A main limitation of IPs is the assumption that the movements of the human and the movements of the robot assistant are linearly correlated. This assumption is reflected in the underlying Gaussian distribution that is used to model
1 Intelligent Autonomous Systems Lab, Department of Computer Science, Technische Universität Darmstadt, Hochschulstr. 10, 64289 Darmstadt, Germany {ewerton, neumann, lioutikov, peters,

Fig. 1. Illustration of a task consisting of multiple interaction patterns, where each can be represented as an Interaction Primitive. In this work, we want to learn multiple interaction patterns from an unlabeled data set of interaction trajectories.

maeda}@ias.tu-darmstadt.de
2 Institute of Robotics and Intelligent Machines, Georgia Institute of Technology, 801 Atlantic Drive, Atlanta, GA 30332-0280, USA

hbenamor@cc.gatech.edu
3 Max Planck Institute for Intelligent Systems, Spemannstr. 38, 72076 Tuebingen, Germany jan.peters@tuebingen.mpg.de

the demonstrations. While this assumption holds for tasks that cover a small region of the workspace (a high-five task in [1] or handover of objects in [2]), it limits the use of IPs in two aspects. First, as illustrated in Fig. 1, a task such as the assembly of a toolbox consists of several interaction patterns that differ significantly from each other and therefore can not be captured by a single Gaussian. Moreover, even within a single interaction pattern, the correlation between the two agents may be nonlinear, for example, if the movements of the human are measured in the Cartesian space, while the movements of the robot are measured in joint space. Manually labeling each subtask (e.g. "plate handover", "screw handover", "holding screw driver") is a way to model interactions with multiple subtasks. Ideally, however, robots should be able to identify different subtasks by themselves. Moreover, it may not be clear to a human how to separate a number of demonstrated interactions in different, linearly correlated groups. Thus, a method to learn multiple interaction patterns from unlabeled demonstrations is necessary. The main contribution of this paper is the development of such a method. In particular, this work uses Gaussian Mixture Models (GMMs) to create a Mixture of Interaction

978-1-4799-6922-7/15/$31.00 ©2015 IEEE

1535

Probabilistic Movement Primitives [2]. The remainder of this paper is organized as follows. Section II presents related work. In Section III, Probabilistic Movement Primitives (ProMPs) and Interaction ProMPs are briefly introduced, followed by the proposition of the main contribution of this paper: a Mixture of Interaction ProMPs based on Gaussian Mixture Models (GMMs). Section IV evaluates the proposed method first on a toy problem that is useful to clarify the characteristics of the method and then on a practical application of a collaborative toolbox assembly. Section V presents conclusions and ideas for future work. II. R ELATED W ORK Physical human-robot interaction poses the problem of both action recognition and movement control. Interaction dynamics need to be specified in a way that allows for robust reproduction of the collaborative task under different external disturbances, and a common approach is based on direct force sensing or emulation. Rozo et al. [3] propose a framework for haptic collaboration between a human and a robot manipulator. Given a set of kinesthetic demonstrations, their method learns a mapping between measured forces and the impedance parameters used for actuating the robot, e.g., the stiffness of virtual springs governing the collaborative task. In another force-based approach, Lawitzky et al. [4] propose learning physical assistance in a collaborative transportation task. In the early learning phase, the robot uses the measured force values to follow the human guidance during the task. Recorded force and motion patterns are then used to learn a Hidden Markov Model (HMM) which can predict the human's next action and over time the robot learns to take over a more active role in the interaction. Kulvicius et al. [5] also address a transportation task where the two agents are modeled as two point particles coupled by a spring. The forces applied by the other agent tell the robot how to adapt its own trajectory. Our work differs significantly from the cited works in the sense that our method does not use nor emulate force signals, but instead learns the correlation between the trajectories of two agents. Correlating trajectories not only simplifies the problem in terms of hardware and planning/control but also allows us to correlate multi-agent movements that do not generate force during the interaction, for example, the simple gesture of asking and receiving an object. Graphical models have also been used to describe interaction dynamics. In the computer vision community, HMMs have been widely adopted to model interaction dynamics from input video streams [6], [7]. As a result, graphical models have also gained considerable attention in the field of human-robot interaction. In [8], Hawkins and colleagues use a Bayes network to improve the fluency in a joint assembly task. The Bayes network learns to infer the current state of the interaction, as well as task constraints and the anticipated timing of human actions. Tanaka et al. [9] use a Markov model to predict the positions of a worker in an assembly line. Wang et al. [10] propose the Intention-Driven Dynamics Model (IDDM) as a probabilistic graphical model

with observations, latent states and intentions where the transitions between latent states and the mapping from latent states to observations are modeled as Gaussian Processes. Koppula et al. [11] use a conditional random field with sub-activities, human poses, object affordances and object locations over time. Inference on the graphical model allows a robot to anticipate human activity and choose a corresponding, preprogrammed robot response. Lee et al. [12] learn a hierarchical HMM which triggers action primitives in response to observed behaviors of a human partner. While very successful for classifying actions, graphical models, however, may not be the best option when it comes to generating motions. In [13], for example, the use of a HMM with discrete states, although very successful in action classification, introduces artifacts into the motion generation part that hinders motion generalization. Therefore, a clear problem in physical human-robot interaction is that while graphical models may be suitable in the action recognition domain, motion generation at the continuous level must also be taken into account. Llorens et al. [14] present a hybrid design for a robot to be used on the shoulder. In their work, Petri Nets accounts for discrete control transitions while, at the motion level, Partial Least Squares Regression has been used to find the best action of the robot at future time steps. Perhaps the principal distinction of our method is the use of Interaction Primitives (IPs), introduced by Ben Amor et al. [1], initially based on dynamical movement primitives [15] and later extended to Probabilistic Movement Primitives [16] with action recognition in the work of Maeda et al. [2]. As shown in [2], Interaction Primitives can be used to not only recognize the action of an agent, but also to coordinate the actions of a collaborator at the movement level; thus overcoming in a single framework both layers of discrete action recognition and continuous movement control. Differently from [2], where different interaction patterns must be hand-labeled, our contribution is the unsupervised learning of a Mixture of Interaction Primitives. III. M IXTURE OF I NTERACTION P RIMITIVES In this section, we will briefly discuss the Interaction Primitive framework based on Probabilistic Movement Primitives [2], [16], followed by the presentation of the proposed method, based on Gaussian Mixture Models. A. Probabilistic Movement Primitives A Probabilistic Movement Primitive (ProMP) [16] is a movement representation based on a distribution over trajectories. The probabilistic formulation of a movement primitive allows operations from probability theory to seamlessly combine primitives, specify via points, and correlate joints via conditioning. Given a number of demonstrations, ProMPs are designed to capture the variance of the positions q and velocities q  as well as the covariance between different joints. For simplicity, let us first consider only the positions q for one degree of freedom (DOF). The position qt at time step t can be approximated by a linear combination of basis

1536

functions,
T qt = t w+ ,

(1)

where is Gaussian noise. The vector t contains the N basis functions i , i  {1, 2, 3, ..., N }, evaluated at time step t where we will use the standard normalized Gaussian basis functions. The weight vector w is a compact representation of a trajectory1 . Having recorded a number of trajectories of q , we can infer a probability distribution over the weights w. Typically, a single Gaussian distibution is used to represent p(w). While a single w represents a single trajectory, we can obtain a distribution p(q1:T ) over trajectories q1:T by integrating w out, p(q1:T ) = p(q1:T |w)p(w)dw. (2)

T T where K = w Ht (D + Ht w Ht )-1 , D is the observation noise, and   o (t )(1,1) 0 0 0   o  0 ( t )(P,P ) 0 0    (5) Ht =   0 0 0 0c   (1,1)   0 0 0 0c (Q,Q)

is the observation matrix where the unobserved states of the robot are filled with zero bases. Here, the human and the robot are assumed to have P and Q DOFs, respectively. Now, by combining (1), (3) and (4), we can compute the probability distribution over the trajectories q1:T given the observation D. For a detailed implementation the interested reader is referred to [2]. C. Mixture of Interaction ProMPs

If p(w) is a Gaussian, p(q1:T ) is also Gaussian. The distribution p(q1:T ) is called a Probabilistic Movement Primitive (ProMP). B. Interaction ProMP An Interaction ProMP builds upon the ProMP formulation, with the fundamental difference that we will use a distribution over the trajectories of all agents involved in the interaction. Hence, q is multidimensional and contains the positions in joint angles or Cartesian coordinates of all agents. In this paper, we are interested in the interaction between two agents, here defined as the observed agent (human) and the controlled agent (robot). Thus, the vector q is now given as q = [(q o )T , (q c )T ]T , where (·)o and (·)c refer to the observed and controlled agent, respectively. Let us suppose we have observed a sequence of positions o qt at m specific time steps t, m  T . We will denote this sequence by D. Given those observations, we want to infer the most likely remaining trajectory of both the human and the robot. T T T Defining w ¯ = [wo , wc ] as an augmented vector that contains the weights of the human and of the robot for one demonstration, we write the conditional probability over trajectories q1:T given the observations D of the human as p(q1:T |D) = p(q1:T |w ¯ )p(w ¯ |D)dw ¯. (3)

The goal of our method is to learn several interaction patterns given the weight vectors that parameterize our unlabeled training trajectories. For this purpose, we learn a GMM in the weight space, using the Expectation-Maximization algorithm (EM) [17]. Assume a training set with n vectors w ¯ representing the concatenated vectors of human-robot weights as defined in section III-B. In order to implement EM for a GMM with a number K of Gaussian mixture components, we need to implement the Expectation step and the Maximization step and iterate over those steps until convergence of the probability distribution over the weights, p(w ¯ ; 1:K , µ1:K , 1:K ), where 1:K = {1 , 2 , · · · , K }, µ1:K = {µ1 , µ2 , · · · , µK } and 1:K = {1 , 2 , · · · , K }. Here, k = p(k ), µk and k are the prior probability, the mean and the covariance matrix of mixture component k , respectively. We initialize the parameters 1:K , µ1:K and 1:K using k-means clustering before starting the Expectation-Maximization loop. The number K of Gaussian mixture components is found by leave-one-out cross-validation. The mixture model can be formalized as
K K

p(w ¯) =
k=1

p(k )p(w ¯ |k ) =
k=1

k N (w ¯ ; µk , k ).

(6)

Expectation step: Compute the responsibilities rik , where rik is the probability of cluster k given weight vector w ¯i , rik = p(k |w ¯i ) = N (w ¯i ; µk , k )k
K l=1

We compute a normal distribution from n demonstrations by stacking several weight vectors [w ¯1 , ..., w ¯n ]T , one for each demonstration, such that w ¯  N (µw , w ). A posterior distribution can be obtained after observing D with
T µnew = µw + K (D - Ht µw ), w T new = w - K (Ht w ), w

l N (w ¯i ; µl , l )

.

(7)

Maximization step: Update the parameters k , µk and k of each cluster k , using
n

(4) nk =

rik , k =
i=1 n ¯i i=1 rik w

nk , n ,

(8)

1 In order to cope with the different speeds of execution during demonstration, the trajectories must be time-aligned before parameterization. The interested reader is referred to [2] for details.

µk =

nk

(9)

1537

k =

1 nk

n

rik (w ¯i - µk )(w ¯i - µk )T
i=1

.

(10)

Finally, we want to use our model to infer the trajectories of the controlled agent given observations from the observed agents. We need to find the posterior probability distribution over trajectories q1:T given the observations D, as in Section III-B. In order to compute this posterior using our GMM prior, first we find the most probable cluster k  given the observation D, using the Bayes' theorem. The posterior over the clusters k given the observation D is given by p(k |D)  p(D|k )p(k ), where p(D|k ) = and p(w ¯ |k ) = N (w ¯ ; µk , k ). Thus the most probable cluster k given the observation D is k  = arg max p(k |D). (12)
k 

Algorithm 1 Training 1) Parameterize demonstrated trajectories: Find vector of weights w ¯ for each trajectory, such that qt  T t w ¯. 2) Find GMM in parameter space, using EM: Initialize GMM parameters 1:K , µ1:K and 1:K with kmeans clustering. repeat E step rik = p(k |w ¯i ) = M step
n

N (w ¯i ; µk , k )k
K l=1

l N (w ¯i ; µl , l )

(11)

p(D|w ¯ )p(w ¯ |k )dw ¯

nk =
i=1

rik , k =
n ¯i i=1 rik w

nk n

µk = k = 1 nk
n

nk

rik (w ¯i - µk )(w ¯i - µk )T
i=1

The output of the proposed algorithm is the posterior probability distribution over trajectories q1:T , conditioning cluster k  to the observation D, p (q1:T |D) = p (q1:T |w ¯ ) p (w ¯ |k  , D) dw ¯. (13)

until p(w ¯ ; 1:K , µ1:K , 1:K ) converges

Algorithm 2 Inference 1) Find most probable cluster given observation: p(k |D)  p(D|k )p(k )

Algorithms 1 and 2 provide a compact description of the proposed methods for training and inference, respectively. IV. E XPERIMENTS This section presents experimental results in two different scenarios using a 7-DOF KUKA lightweight arm with a 5finger hand2 . The goal of the first scenario is to expose the issue of the original Interaction Primitives [1], [2] when dealing with trajectories that have a clear multimodal distribution. In the second scenario we propose a real application of our method where the robot assistant acts as a third hand of a worker assembling a toolbox (please, refer to the accompanying video3 ). A. Nonlinear Correlations between the Human and the Robot on a Single Task To expose the capability of our method of dealing with multimodal distributions, we propose a toy problem where a human specifies a position on a table and the robot must point at the same position. The robot is not provided any form of exteroceptive sensors; the only way it is capable to generate the appropriate pointing trajectory is by correlating
2 Regarding the control of the robot, the design of a stochastic controller capable of reproducing the distribution of trajectories is also part of ProMPs and the interested reader is referred to [16] for details. Here we use a compliant, human-safe standard inverse-dynamics based feedback controller. 3 Also available at http://youtu.be/9XwqW_V0bDw

k  = arg max p(k |D)
k

2) Condition on observation, using cluster k  : p(q1:T |D) = p(q1:T |w ¯ )p(w ¯ |k  , D)dw ¯

its movement with the trajectories of the human. As shown in Fig. 2, however, we placed a pole in front of the robot such that the robot can only achieve the position specified by the human by moving either to the right or to the left of the pole. This scenario forces the robot to assume quite different configurations, depending on which side of the pole its arm is moving around. During demonstrations the robot was moved by kinesthetic teaching to point at the same positions indicated by the human (tracked by motion capture) without touching the pole. For certain positions, as the one indicated by the arrow in Fig. 2(a), only one demonstration was possible. For other positions, both right and left demonstrations could be provided as shown in Fig. 2(a) and 2(b). The demonstrations, totaling 28 pairs of human-robot trajectories, resulted in a multimodal distribution of right and left trajectory patterns moving around the pole. In this scenario, modeling the whole distribution over

1538

0.1 0.08 RMS error (m) 0.06 0.04 0.02 0

2

4

6

8

10

12

14

16

Number of clusters

Fig. 4.
(a) (b)

Root Mean Square Error with models using up to 17 Gaussians.

Fig. 2. Experimental setup of a toy problem used to illustrate the properties of the Mixture of Interaction Primitives. The robot is driven by kinesthetic teaching to point at the positions specified by the human (pointed with the wand). Certain pointed positions can be achieved by either moving the arm to the right (a) or to left (b) of the pole placed on the table. Other positions, such as the one indicated by the arrow, can only be achieved by one interaction pattern.
ground truth prediction

clusters it is observed that the prediction error fluctuates around 4 cm. The experiments previously shown in Fig. 3(b) were done with eight clusters. B. Assembling a Box with a Robot Assistant In this experiment, we recorded a number of demonstrations of different interaction patterns between a human and the robot cooperating to assemble a box. We used the same robot described in the previous experiment. During demonstrations, the human wore a bracelet with markers whose trajectories in Cartesian coordinates were recorded by motion capture. Similarly to the first scenario, the robot was moved in gravity compensation mode by another human during the training phase and the trajectories of the robot in joint space were recorded. There are three interaction patterns. Each interaction pattern was demonstrated several times to reveal the variance of the movements. In one of them, the human extends his/her hand to receive a plate. The robot fetches a plate from a stand and gives it to the human. In a second interaction, the human fetches the screwdriver, the robot grasps and gives a screw to the human as a pre-emptive collaborator would do. The third type of interaction consists of giving/receiving a screwdriver. Each interaction of plate handover, screw handover and holding the screwdriver was demonstrated 15, 20, and 13 times, respectively. The pairs of trajectories of each interaction are shown in Fig. 54 . As described in section III, all training data are fed to the algorithm resulting in 48 human-robot pairs of unlabeled demonstrations as shown in the upper row of Fig. 7. The presented method parameterizes the trajectories and performs clustering in the parameter space in order to encode the mixture of primitives. In the upper row of Fig. 7, each mixture is represented by a different color. The human is represented by the (x, y, z ) Cartesian coordinates while the robot is represented by the seven joints of the arm. The figure shows the first four joints of the robot (starting from the base).
4 Due to the experimental setup, for the sub-tasks of plate and screw handover we added an initial hand-coded trajectory that runs before the kinesthetic teaching effectively starts. These trajectories are used to make the robot grasp and remove the plate or screw from their respective stands. This is reflected in the figure as the deterministic part at the beginning of the trajectory of the robot. This initial trajectory, however, has no effect on the proposed method itself.

(a)

(b)

Fig. 3. Results of the predictions of the robot trajectories in Cartesian space. Both subplots show the same ground truth trajectories generated by driving the robot in kinesthetic teaching. The predictions are generated by leave-one-out cross-validation on the whole data set comprised of 28 demonstrations. (a) Prediction using the conventional Interaction ProMPs with a single Gaussian. (b) Prediction using the proposed method with a mixture of Gaussians.

the parameters of the trajectories with one single Gaussian (as in the original Interaction Primitive formulation) is not capable of generalizing the movements of the robot to other positions in a way that resembles the training, as the original framework is limited by assuming a single pattern. This limitation is clearly shown in Fig. 3(a), where several trajectories generated by a single cluster GMM (as in the original Interaction Primitive) cross over the middle of the demonstrated trajectories, which, in fact, represents the mean of the single Gaussian distribution. Fig. 3(b) shows the predictions using the proposed method with a mixture of Gaussians. By modeling the distribution over the parameters of the trajectories using GMMs as described in section III-C, a much better performance could be achieved. The GMM assumption that the parameters are only locally linear correlated seemed to represent the data much more accurately. As shown in Fig. 4, this improvement is quantified in terms of the Root Mean Square (RMS) Error of the prediction of the trajectory in relation to the ground truth using leave-one-out cross-validation over the whole data set. The same figure also shows that there is a sharp decrease in the RMS error up to six clusters, especially when taking into account the variance among the 28 tests. Beyond seven

1539

human human robot robot

human

robot

(a) Handing over a plate

(b) Handing over a screw

(c) Holding the screw driver

Fig. 5. Demonstrations of the three different interactions and their respective trajectories. For the case of plate and screw handover the beginning of the robot trajectory shows a deterministic part that accounts for the fact that the robot has to remove objects from their respective stands, which is not part of the kinesthetic teaching and does not affect the algorithm in any sense.

Joint RMS prediction error (deg)

40

30

20

10

0

1

2

3

4

5

6

7

8

Number of clusters

Fig. 6. Root Mean Square Error of the joint trajectories (averaged over all tests) using a leave-one-out cross-validation as a function of the number of clusters (mixture components). The plateau after three clusters seems to be consistent with the training data since it consists of three distinct interaction patterns.

Figure 6 shows the RMS prediction error averaged over all tests as the number of mixture components increase. The prediction is obtained by leave-one-out cross-validation over the whole set of 48 demonstrations. As one would expect, since the unlabeled data contains three distinct interaction patterns, the improvement is clearly visible up to three mixture components. No significant improvement is obtained afterwards, thus the GMM with three mixture components was selected for experiments. In the inference/execution phase, the algorithm first computes the most probable Interaction Primitive mixture component based on the observation of the position of the wrist of the human with (12). Using the same observation, we then condition the most probable Interaction Primitive, which allows computing a posterior distribution over trajectories for all seven joints of the robot arm as in (13). Finally, the mean of each joint posterior distribution is fed to a standard inverse

dynamics feedback tracking controller. The lower row of Fig. 7 depicts the posterior distribution for one test example where a three-cluster GMM was trained with the other 47 trajectories. The GMM prior is shown in gray where the patches of different clusters overlap. The observation consists only of the final position of the wrist, shown as asterisks in the figure. The black lines are the ground truth trajectories of each degree of freedom. The posterior, in red, is represented by its mean and by the region inside ± two standard deviations. The mean of this posterior is the most probable trajectory for each degree of freedom given the observed end position of the wrist of the human. We assembled the toolbox, consisting of seven parts and 12 screws, two times. The experiments demanded more than 40 executions of the Interaction Primitives. The selection of the right mixture component was 100% correct. (Please refer to the accompanying video). We evaluated the precision of the interactions by computing the final position of the hand of the robot with forward kinematics. The forward kinematics was fed with the conditioned robot trajectories predicted by leave-oneout cross validation. The interactions of plate handover and holding screwdriver resulted in mean error with two standard deviations (mean error ±2 ) of 3.2 ± 2.6 cm and 2.1 ± 2.3 cm, respectively. We did not evaluate the precision of the handover of the screw, as the position at which the robot hands the screw is not correlated to the human (please refer to the accompanying video). As an example, Fig. 8 shows the robot executing the plate handover at three different positions based on the location of the wrist marker. Note that the postures of the arm are very different, although they are all captured by the same Interaction Primitive.

1540

Fig. 7. Upper row: Mixture components represented by their mean trajectories and the region inside two standard deviations (µ ± 2 ). Each mixture component is represented by a different color and corresponds to a different interaction pattern. The light gray trajectories are the training trajectories. Obs.: The plots show only the part of the trajectories generated by kinesthetic teaching. Lower row: Posterior probability distribution (red) given observation depicted by the blue asterisks. The GMM prior is shown in gray.

addressing the estimation of the phase of the execution of the primitive for switching tasks in real time. Also, we are addressing the use of the stochastic feedback controller provided by the original ProMP work in [16]. Although this work focused on human-robot trajectories, we are currently considering extensions of our work where the human is replaced by other variables of interest. For example, the same framework can be used to correlate joint and endeffector trajectories of the same robot to learn nonlinear forward/inverse kinematic models. Similarly the Mixture of Interaction Primitives can be used to correlate the interaction between motor commands and joint trajectories to learn inverse dynamics models. VI. ACKNOWLEDGMENTS The research leading to these results has received funding from the project BIMROB of the "Forum für interdisziplinäre Forschung" (FiF) of the TU Darmstadt, from the European Community's Seventh Framework Programme (FP7ICT-2013-10) under grant agreement 610878 (3rdHand) and from the European Community's Seventh Framework Programme (FP7-ICT-2009-6) under grant agreement 270327 (ComPLACS). R EFERENCES
[1] H. Ben Amor, G. Neumann, S. Kamthe, O. Kroemer, and J. Peters, "Interaction primitives for human-robot cooperation tasks," in Proceedings of 2014 IEEE International Conference on Robotics and Automation (ICRA), 2014. [2] G. Maeda, M. Ewerton, R. Lioutikov, H. Ben Amor, J. Peters, and G. Neumann, "Learning interaction for collaborative tasks with probabilistic movement primitives," in Proceedings of the International Conference on Humanoid Robots (HUMANOIDS), 2014. [3] L. Rozo, S. Calinon, D. G. Caldwell, P. Jimenez, and C. Torras, "Learning collaborative impedance-based robot behaviors," in AAAI Conference on Artificial Intelligence, Bellevue, Washington, USA, 2013. [4] M. Lawitzky, J. Medina, D. Lee, and S. Hirche, "Feedback motion planning and learning from demonstration in physical robotic assistance: differences and synergies," in Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, Oct 2012, pp. 3646­3652.

Fig. 8. Handover of a plate. Conditioning on three different positions of the wrist (using motion capture) of a human coworker.

V. C ONCLUSIONS In this paper we presented a Mixture of Interaction Primitives where Gaussian Mixture Models are used to model multiple interaction patterns from unlabeled data. The multimodal prior probability distribution is obtained over parameterized demonstration trajectories of two agents working in collaboration. During the execution, the algorithm selects the mixture component with the highest probability given the observation of the human, which is then conditioned to infer the appropriate robot reaction. The proposed method is able to learn and recognize multiple human-robot collaboration tasks from an arbitrary number of demonstrations consisting of unlabeled interaction patterns, what was not possible with the previous Interaction Primitive framework. In the context of human-robot interaction we are currently

1541

[5] T. Kulvicius, M. Biehl, M. J. Aein, M. Tamosiunaite, and F. Wörgötter, "Interaction learning for dynamic movement primitives used in cooperative robotic tasks," Robotics and Autonomous Systems, vol. 61, no. 12, pp. 1450­1459, 2013. [6] M. Brand, N. Oliver, and A. Pentland, "Coupled hidden markov models for complex action recognition," in Proceedings of the 1997 Conference on Computer Vision and Pattern Recognition (CVPR '97), ser. CVPR '97. Washington, DC, USA: IEEE Computer Society, 1997, pp. 994­. [7] N. Oliver, B. Rosario, and A. Pentland, "A bayesian computer vision system for modeling human interactions," Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 831­843, Aug 2000. [8] K. Hawkins, N. Vo, S. Bansal, and A. F. Bobic, "Probabilistic human action prediction and wait-sensitive planning for responsive humanrobot collaboration," in Proceedings of the International Conference on Humanoid Robots (HUMANOIDS), 2013. [9] Y. Tanaka, J. Kinugawa, Y. Sugahara, and K. Kosuge, "Motion planning with worker's trajectory prediction for assembly task partner robot," in Proceedings of the 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2012, pp. 1525­ 1532. [10] Z. Wang, K. Mülling, M. P. Deisenroth, H. Ben Amor, D. Vogt, B. Schölkopf, and J. Peters, "Probabilistic movement modeling for intention inference in human­robot interaction," The International Journal of Robotics Research, vol. 32, no. 7, pp. 841­858, 2013. [11] H. S. Koppula and A. Saxena, "Anticipating human activities using object affordances for reactive robotic response." in Robotics: Science and Systems, 2013. [12] D. Lee, C. Ott, Y. Nakamura, and G. Hirzinger, "Physical human robot interaction in imitation learning," in Robotics and Automation (ICRA), 2011 IEEE International Conference on. IEEE, 2011, pp. 3439­3440. [13] H. Ben Amor, D. Vogt, M. Ewerton, E. Berger, B. Jung, and J. Peters, "Learning responsive robot behavior by imitation," in Proceedings of the 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2013, pp. 3257­3264. [14] B. Llorens-Bonilla and H. H. Asada, "A robot on the shoulder: Coordinated human-wearable robot control using coloured petri nets and partial least squares predictions," in Proceedings of the 2014 IEEE International Conference on Robotics and Automation, 2014. [15] A. J. Ijspeert, J. Nakanishi, H. Hoffmann, P. Pastor, and S. Schaal, "Dynamical movement primitives: learning attractor models for motor behaviors," Neural computation, vol. 25, no. 2, pp. 328­373, 2013. [16] A. Paraschos, C. Daniel, J. Peters, and G. Neumann, "Probabilistic movement primitives," in Advances in Neural Information Processing Systems (NIPS), 2013, pp. 2616­2624. [17] C. M. Bishop et al., Pattern recognition and machine learning. springer New York, 2006, vol. 1.

1542

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/273134654

ExploitingSymmetriesandExtrusionsfor GraspingHouseholdObjects
ConferencePaperinProceedings-IEEEInternationalConferenceonRoboticsandAutomation·May2015
DOI:10.1109/ICRA.2015.7139713

CITATIONS

READS

2
7authors,including: AnaHuamanQuispe GeorgiaInstituteofTechnology
7PUBLICATIONS4CITATIONS
SEEPROFILE

265

MarcoA.Gutierrez UniversidaddeExtremadura
11PUBLICATIONS23CITATIONS
SEEPROFILE

HenrikIskovChristensen UniversityofCalifornia,SanDiego
478PUBLICATIONS6,419CITATIONS
SEEPROFILE

HeniBenAmor ArizonaStateUniversity
56PUBLICATIONS434CITATIONS
SEEPROFILE

AllcontentfollowingthispagewasuploadedbyHeniBenAmoron05March2015.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Exploiting Symmetries and Extrusions for Grasping Household Objects
Ana Huam´ an Quispe1 Beno^ it Milville1 Marco A. Guti´ errez2 Can Erdogan1 Henrik Christensen1 Heni Ben Amor1 Mike Stilman

Abstract--In this paper we present an approach for creating complete shape representations from a single depth image for robot grasping. We introduce algorithms for completing partial point clouds based on the analysis of symmetry and extrusion patterns in observed shapes. Identified patterns are used to generate a complete mesh of the object, which is, in turn, used for grasp planning. The approach allows robots to predict the shape of objects and include invisible regions into the grasp planning step. We show that the identification of shape patterns, such as extrusions, can be used for fast generation and optimization of grasps. Finally, we present experiments performed with our humanoid robot executing pick-up tasks based on single depth images and discuss the applications and shortcomings of our approach.

I. I NTRODUCTION The ability to grasp and manipulate objects is an important skill for autonomous robots. Many important tasks, e.g., assisting humans in household environments, require robots to reliably plan and execute grasps on surrounding objects. To generate plans for manipulation tasks, information about the shape of the object is required. A frequent approach to grasp planning is to use a database of polygonal meshes representing the different objects that the robot can manipulate [8]. Such information about object geometry can be used by grasp planners to synthesize an appropriate hand shape and orientation for physical interaction. While this approach is valid for structured domains with a small set of different objects, it does not scale to unstructured environments in which many objects may have never been seen before. Other approaches to grasp planning employ depth cameras to acquire 3D point clouds of new objects, which in turn are used to generate grasps. Since the point clouds are acquired from a specific perspective, they only hold partial shape information about the visible frontal part. Using only partial point clouds to plan manipulation tasks can be very limiting, since many grasps involve placing fingers on opposite sides of an object. To fill any gaps and produce a complete point cloud, multiple images can be acquired by either iteratively moving the camera or the object. This process is time-consuming and
1 Institute

Fig. 1: Extracted information of rotational symmetries in the object is used to create a complete shape from a partial point cloud. The generated mesh is used by a grasp planner to generate a continuous set of grasps around the symmetry axis.

for of

gia

Institute

Robotics and Intelligent Machines, Technology, Atlanta, GA 30332,

GeorUSA.

ahuaman3@gatech.edu, cerdogan@cc.gatech.edu, benoit.milville@gadz.org, hic@cc.gatech.edu, hbenamor@cc.gatech.edu
2 Robotics and Artificial Laboratory, Univesity of Extremadura, C´ aceres, 10003, Spain. marcog@unex.es

introduces new challenges such as the precise matching of the individual point clouds of each view. Alternatively, the robot can use geometric cues to predict the shape of the object in unseen regions. Through the analysis of inherent shape properties such as mirror symmetries and rotational extrusions, estimates of the complete point cloud can be generated from a single image. The extracted symmetry parameters can be used to extend observed shape patterns, e.g., the profile curve of an object, to occluded regions. In this paper, we show how compact object representations for manipulation tasks can be generated from a partial point cloud. Given a single RGB-D image, we generate a complete mesh model of the observed object as well as additional shape information, e.g., axis of symmetry or superquadric approximations. We show that these compact representations can be later exploited for the fast synthesis of a continuous set of grasps. In turn, the set is used to plan robot manipulation tasks. Our approach builds both upon recent developments in symmetry-based [3, 18], as well as extrusion-based object representations [16]. Symmetry-based representations mirror observed object parts into occluded regions. Extrusion-based approaches, on the other hand, try

to identify a two-dimensional profile which can be linearly or rotationally extruded to complete an object. In this work we show how symmetries and extrusions can be used to extract two different types of object representations, namely superquadric approximations and 2D shape profiles. We also show how these representations can used to generate grasps on the object. The rest of this paper is organized as follows: Section II summarizes relevant literature. Section III introduces two compact object representations that are based on detecting symmetries and extrusions. Section IV shows how compact object representations based on extrusion patterns can be exploited for fast grasp planning with a small number of parameters. Section V presents experimental results of the object completion, as well as its application to robot grasping tasks. Finally in Section VI we discuss our approach and its advantages and shortcomings. II. R ELATED W ORK For a robot to physically interact with its environment, algorithms for both grasp planning and perception are required. Traditional approaches for grasp generation are often based on fitting 3D CAD models to the observed scene [14, 15]. Such an approach, however, cannot be used to grasp novel objects since it requires accurate, prior knowledge about the shape. With the advent of depth cameras, various researchers have turned towards point cloud representations for perception and grasp planning. Huebner et al. [11] showed that bounding boxes computed from point clouds can be used to grasp novel objects. In a similar vein, Jiang et al. [12] proposed a socalled grasping-rectangle representation which can be used to infer the best grasp parameters given an RGB-D image of a novel object (given an offline training step). Przybylski et al. [21] showed simulation results in which a medial axis representation of objects can be used to find successful grasps without compromising on the approximation quality. Other than boxes and spheres [17], superquadrics [9] have also been considered for grasping applications given their compactness and ability to represent many diverse shapes with a limited number of parameters. Recently, Duncan et al. presented a fast hierarchical approach to fit superquadrics online [5]. On the side of grasp generation, a popular metric used to predict grasp robustness is the  metric proposed by Ferrari and Canny [6]. While many popular grasp generators, such as GraspIt! use this metric to evaluate and refine the grasp search, it has been noted [4] that a grasp with a good metric does not translate to a robust grasp in a real-world execution. Researchers such as Hsiao [10] and Balasubramanian [1] have shown that grasps obtained using simple human heuristics can produce comparable or even better results when evaluated in a real, non-simulated environment. A real world scenario - contrary to a simulated one presents its own set of challenges: errors in perception, control and modeling must be considered and might render an optimal simulated grasp into an infeasible one. Regarding incomplete perceptual information, such as one-view point clouds for a

given object, Bohg et al. [3] proposed a simple approach that exploits the symmetry of most common household objects to predict the full shape of an object on a tabletop scenario. Following Bohg's observation that most common household objects present similar characteristics (such as symmetry, extrusion-like geometry and primitive shapes), we use them to approximate the shape of objects. This is also useful in the event of occlusion, in which a complete point cloud is not available. III. G ENERATING C OMPACT O BJECT R EPRESENTATIONS FROM S INGLE RGB-D I MAGES In this section, we present two compact representations of objects that can be generated from partial point clouds. These representations can be used to plan grasps on objects involving regions of the point cloud that are currently invisible. As a result, a wider range of grasps can be planned, including, for example, side grasps which are based on an opposition of fingers placed at the front (seen) and the back (unseen) of the object. We will first present a superquadric representation which is based on determining symmetries in point clouds. After that, we will turn towards a more detailed representation which makes use of rotational symmetries and linear extrusions to characterize an object. A. Superquadric Representation Superquadrics are a family of geometric shapes that can represent a wide range of diverse objects. The equation describing superquadrics in their canonical form can be written as x a
2 2

F (x) =

y + b

2 2

2 1

+

z c

2 1

= 1.

(1)

where a,b,c are the scaling factors along the principal axes, 1 is the shape factor of the superquadric cross section in a plane orthogonal to XY containing the axis Z, and 2 is the shape factor of the superquadric cross section in a plane parallel to XY. If a general transformation is considered, then the total number of parameters required to define a superquadric is 11 (the 6 additional being the rotational and translational degrees-of-freedom (DoFs) {x, y, z, , , }). By minimizing the error between each point and the general superquadric equation, a shape that best fits the point cloud can be obtained:
n

min
k k=0



abcF 1 (x; ) - 1

2

(2)

As mentioned in Section II, superquadrics have previously been used to generate grasp configurations for simple objects [2, 22]. Most of these approaches assume that the complete shape of the object is given or that the parameters can be learned beforehand. However, when working with depth cameras this is not a reasonable assumption to make. In recent work, Duncan et al. [5] presented a superquadric fitting

Hypotheses

Initial Estimation

Optimization

Fig. 3: The three steps used for optimizing the axis of extrusion. First, we generate hypotheses by analysing pairs of points. The resulting estimates are used to produce an initial estimate of the axis of extrusion. Finally, optimization is used to improve the extrusion axis.

choose suitable candidates for task execution. For example, detecting the axis of symmetry in a rotationally symmetric object allows us to rotate any feasible grasp around this axis. In this paper, extrusion detection is performed using a threestep approach, see Fig. 3 for an overview of the approach using rotational extrusions. In the first step, we use points from the partial point clouds to generate hypotheses for the extrusion axis. In the case of rotational extrusions, we randomly sample pairs of points and use the normal of each point to create a line. Each pair of lines is intersected and the resulting point is used as a hypothesis for the axis of extrusion. Fig. 3 shows an example for points sampled from a cylindrical object. To account for noise, we use the midpoint of the line connecting the closest points, in case the two lines do not intersect. The collected hypotheses points are then used to create an initial estimate of the axis of extrusion. To this end, we fit a line into the set of hypotheses using linear least-squares. The RANSAC [7] algorithm is further used to reduce the influence of outliers. Given this initial estimate, we perform optimization to produce a more accurate axis of extrusion. Specifically, we use the dynamic hill climbing algorithm [23] to search for an axis of extrusion which reduces the dispersion of points along the profile of the object. In every iteration, the axis of extrusion is used to rotate all points of the partial point cloud back onto a plane. We then estimate the density of the points using a kernel density estimator [20]. By maximizing the density using the hill climbing algorithm, we can reduce the dispersion of the projected points, thereby recreating the profile of the object. However, performing a kernel density estimation in each step of the optimization process is computationally expensive and does not scale to large point clouds. The following method is, therefore, a discrete approximation of the kernel density, which produced accurate results in practice while at the same time being fast. We create an approximation of the kernel density estimator by creating a grid over the projected point cloud. The number of cells used in our experiments varied between 5 and 30 cells in each dimension. For each cell i  {1, .., M } we count the number of points ci that lie within. We then calculate the average of the differences to neighbouring cells j  {1, .., N }. The overall objective function of the optimization can be

Fig. 2: An example for the superquadric fitting with symmetry analysis (middle) and without it (bottom). approach which uses a voxel representation to reduce the computational complexity of the task. We found that this approach worked well when the segmented point cloud of the object had a good viewing point (i.e. the front, side and top of the object were seen). For point clouds in which only one side of the object was seen (i.e. only front), the performance quickly deteriorated, producing fitting parameters that in many cases exceeded greatly the original dimensions of the objects. While this could be partially alleviated by hardcoding limits in the dimension of the axes, this is not practical when dealing with novel objects, for which we might not know the dimensions beforehand. Inspired by work presented by Bohg et al. [3], we added an additional pre-processing step to the superquadric minimization process. Instead of using the original point cloud as input, we generated a mirrored version (see Fig. 2) by finding an optimal symmetry plane perpendicular to the table where the object resides (for more details of this process, please refer to the original paper [3]). B. Object Completion from Extrusions Planning task-specific grasps requires information about the complete shape of the object to be manipulated. Many household objects are based on extrusions. Indeed many modelling and manufacturing systems use linear and rotational extrusions in a hierarchy to generate the models used for manufacturing. Uncovering extrusions in partial point clouds can therefore help to generate a complete point cloud from a partial observation. In addition, this knowledge can be used to create a large set of feasible grasps from which a planner can

Iteration 2
25 20 15 10

Iteration 10

Iteration 50

20

15

10

5 5 0 0 1 2 3 4 5 6 7 0

Fig. 4: Density estimation at different stages of optimization. At the beginning of the optimization, the projected points are highly dispersed. The axis of extrusion is then changed to minimize the dispersion, such that the outer profile of the object emerges as can be seen in iteration 50. On the right side we can see the object to which the profile belongs. written as E= 1 1 MN Fig. 6: Extracted object profile for the linearly extruded objects. The extracted profiles are used to create a complete point cloud. ||ci - cj || (3) and evaluate grasp quality using existing metrics. In contrast, traditional grasp quality metrics cannot be directly applied to partial point clouds. Similarly, having a complete mesh allows a grasp planner to evaluate a large variety of grasps, which can then be pruned based on task constraints. However, generating many grasps often involves repeated applications of grasp optimization methods which can be computationally demanding, in particular in the presence of many degrees-of-freedom in the robot arm and hand. Extracted shape information from extrusions can be used to improve the efficiency of this process by significantly reducing the number of degrees-of-freedom of the problem. The main insight of this section is that hand shapes during object grasping are invariant to movements along the axis of extrusion. As long as the robot hand moves along the axis of extrusion, no expensive replanning of the hand shape is necessary. In the case of linear extrusions, the robot hand can move up and down the axis of extrusion without having to change the hand shape. Similarly, in the case of rotational extrusions, the hand can be rotated around the axis of extrusion. This knowledge can be exploited during grasp generation in order to turn each single detected grasp into a continuous set of grasps. Subsequently, we present a specific example how information about extrusions can be used to reduce the dimensionality and complexity of a grasp re-planning task. Fig. 7a shows a scenario, in which a grasp is executed on a rotationally symmetric object. The grasp has a low manipulability index which is not sufficient to achieve the task constraints. Typically, this means that a new grasp and arm pose needs to be planned, which involves (sampling-based) optimization in the high-dimensional space of joint angles. Given that the grasp is performed on a rotationally symmetric object, the grasp generation can be modeled as an inverse kinematics problem where the goal is to determine an arm configuration q that is collision free. The output is constrained by the end-effector position on the object and the corresponding inverse kinematics solution. The end-effector pose x can be parametrized by (1) the rotation around the axis of extrusion  and (2) the distance along the axis of extrusion

M

N

i

j

where E is the energy to be minimized. Fig. 4 shows three iterations during the optimization of the axis of extrusion. Dark areas correspond to regions of high density of points, while lighter areas correspond to low density regions. In early iterations, the estimate of the axis does not produce a clear profile when points are projected (rotationally) onto a plane. In iteration ten, we can see that high density regions start forming. After fifty iterations, an approximate profile of the object starts to emerge. After optimization is finished, we regard the projected points as the profile of the object and rotationally extrude them around the axis of extrusion to generate a complete point cloud. Fig. 5 shows a set of household objects, the recorded depth images, as well as the reconstructed complete meshes. Given the completed point cloud, we reconstructed the meshes using Poisson surface reconstruction [13]. For the case of linear extrusions along an axis, a different method for the estimation of the initial axis of extrusion needs to be used. For linear extrusions, we compare the normal vectors of pairs of points and generate a hypothesis if the difference between the normals is below a threshold. The resulting set of hypothesis can then be clustered, such that each cluster represents a possible axis of extrusions. For example, for a box, up to six clusters can be found. Note, that in our approach we use a point cloud to represent the profile of an extrusion. For revolute objects, the profile defines the outer curve of the object, which can be rotated around the axis of extrusion to generate the complete shape. For linear extrusions, the cloud represents the basic 2D shape which can be extruded to form the object. Fig. 6 shows the extracted object profiles for objects with linear extrusions. IV. U SING C OMPACT O BJECT R EPRESENTATIONS FOR G RASP P LANNING Grasp planning greatly benefits from the completed point clouds. A complete point cloud can be triangulated and used as an input to existing grasp generation and planning algorithms. In contrast to the partial point cloud, the completed and triangulated mesh can be used to perform collision checks

Fig. 5: Reconstruction of rotationally symmetric household objects. The top row shows a photo of the object. The middle row shows the corresponding depth image recorded using a Microsoft Kinect. The bottom row shows the completed mesh. Reconstruction was performed from a single image through the analysis of extrusions. , x = pose(, ). The inverse kinematics solution q with a 7-DoF arm for an end-effector pose x can be parametrized by an additional variable  which represents the angle between the wrist-elbow-shoulder plane and the ground, q = IK (x, ). At each iteration i, the new arm position is computed using an updated grasp position from the parameter space {i-1 ±  , i-1 ±  } and the corresponding inverse kinematics parametrized by {i-1 -  , i-1 , i-1 +  }. Let P represent the full space of the variables , , and . The algorithm iteratively updates these parameters by determining which tuple leads to the maximum manipulability [19]. This is realized by solving for the following objective q i = argmax det(J (q )J T (q )) (4) set of experiments focuses on the complexity and accuracy of point cloud completion when generating compact object representations. The second set of experiments shows the application of the approach to grasp planning on a humanoid robot. The used humanoid robot is based on Schunk LWA3 arms with 7 DoF. A Schunk gripper with a maximum aperture of 7cm was used. Partial point clouds were recorded using a Microsoft Kinect camera. A. Accuracy of Fit We first analyzed the accuracy of fit of the two presented compact object representations. For extrusions, we collected a set of rotationally symmetric meshes from internet databases from which we generated partial point clouds. We then cut out a partial point cloud representing 30% of the data and simulated Kinect-like noise by adding holes and noise to the dataset. The partial cloud was then completed using the extrusion detection methods from Sec. III-B. To measure the accuracy, we compared the completed clouds to the original mesh of the object. On average, the approach produced an error (distance of points to mesh) of 2mm, where objects had a diameter between 10 - 20cm. Analysis of the extrusions required on average 200ms. For superquadric fitting we conducted a similar experiment. However, in this case we noticed larger variations in the reconstructed shapes depending on the perspective of the camera to the object. We therefore placed each object at one of five different locations in front of the camera and measured the run time of the algorithm including symmetry analysis and without it. As depicted in Tab. I, the fitting time is shorter when additional points are added via symmetry analysis. While this

{,, }P

where q = IK (pose(, ), ). The sequence in Fig. 7 shows several snapshots during this optimization. In this scenario, the robot grasps a rotationally symmetric bottle. The initial random grasp sample in Fig. 7a yields a manipulability of 0.268 which is then improved in Fig. 7d leading to a value of 0.540. To optimize the manipulability, the planner iteratively changes the grasp position on the robot with the  and  parameters, and the inverse kinematics parameter . This optimization can be performed efficiently since, the highdimensional configuration space of the hand does not need to be represented thanks to the extracted symmetries. Instead, a three-dimensional space of parameters {, , }  P is used. V. E XPERIMENTAL R ESULTS In this section, we present a set of experiments which we conducted to evaluate the proposed approach. The first

Fig. 7: Grasp manipulability optimization along the axis of extrusion. Since the object is symmetric, the same hand configuration can be rotated around the object (A-B, C-D). At the same time, the extra DOF in the inverse kinematics solution is also utilized to maximize manipulability (B-C). may seem unintuitive, we found that the superquadric shape has more constraints when considering mirrored points. As a result, the optimization process required for fitting quickly settles on a good solution. TABLE I: Comparison of fitting times
Object Apple Milk Jam Raisins Creamer Input Symmetry Plain Symmetry Plain Symmetry Plain Symmetry Plain Symmetry Plain P1 0.02 0.14 0.20 0.42 0.06 0.08 0.29 0.36 0.15 0.65 P2 0.13 0.17 0.07 0.56 0.11 0.29 0.25 0.40 0.15 0.09 P3 0.01 0.06 0.03 0.27 0.13 0.10 0.31 0.43 0.22 0.39 P4 0.06 0.06 0.05 0.53 0.08 0.08 0.14 0.43 0.13 0.26 P5 0.07 0.06 0.04 0.06 0.21 0.11 0.27 0.32 0.14 0.29 Avg. Time 0.05s 0.098s 0.078s 0.368s 0.118s 0.132s 0.252s 0.388s 0.158s 0.336s Extrusion Success SQ

to the invariance along the axis of extrusion. Images of the executed grasp and the experimental setup can be found in Fig. 8. TABLE II: Experimental results, 3 trials per object per location
Location B4 C3 C4 B4 C3 C4 B4 C3 C4 B4 C3 C4 Creamer 100% 0% 100% 100% 100% 100% 1040 800 1270 11 7 10 Dove 100% 0% 100% 100% 100% 100% 900 400 320 11 3 5 Roll 0% 0% 100% 0% 66% 0% 1200 2200 800 7 1 5 Micro 100% 0% 100% 100% 100% 100% 640 800 1020 11 7 13

Extrusion Grasps SQ

B. Robot Grasping Experiments Next, we conducted an experiment in which a humanoid robot was used to grasp household objects located in front of it. We also placed several other objects as clutter on the table. Given the depth image all objects were reconstructed using compact object representations. After that, the robot planned and executed grasps using the normal at a point as an approach direction and the method described in Sec. IV for ensuring manipulability and obstacle avoidance. We conducted trials with 4 objects which were placed at 4 different locations on the table. Each trial was repeated three times. A grasp was regarded successful if the robot was able to lift the object. Tab. II summarizes the results of the experiment. We can see that the approach using superquadrics performs well on most objects with the exeption of the roll. In contrast, the extrusionbased approach seems to have difficulties with a specific location (C3). Analyzing the robot executions, we found that superquadric approach typically leads to approximate shapes which are slightly larger than the original object. Hence, the executed grasp includes a "buffer" zone that allows it to succeed in the presence of sensor and calibration noise. Grasps planned for the shapes generated by the symmetry detection, however, are tighly fit to the object. This often lead to premature contact with the object during grasp execution. In Tab. II we also see the number of different grasps found using the two approaches. We can see that the symmetry based approach leads to a larger number of different grasps, due VI. D ISCUSSION AND C ONCLUSION In this paper we introduced methods for generating compact and complete object representations that are particularly useful for robot grasping applications. The approach exploits natural patterns found in many shapes, e.g., symmetries, linear extrusions, and rotational extrusions to generate a complete mesh from a single depth image. We also showed that the extraction of this information can be used to improve the efficiency and quality of the grasp planning step. The work presented in this paper can be seen as a first step towards shape priors that can be used by a robot to generate hypotheses about the shape of an object in invisible regions. Other cues, such as curvature and texture may also be helpful in predicting the complete shape from partial observations. At the moment the introduced approach is limited to household objects, which are often based on linear and rotational extrusions. However, it can also be extended to work in a hierarchy to complete more complex objects. In future work, we hope to investigate this aspect in more detail. The performed robot experiments showed that the approach can be used to create a variety of grasps. In particular, we can generate grasps that extend to parts of the object that are not seen. This is in contrast to other methods which limit the approach direction of the robot to the visible part of the object. We have shown in the experiments that the method can be used to reconstruct objects in a cluttered scene without prior

Fig. 8: Grasps on household objects generated via grasp planning on compact object representations. All objects on the table were reconstructed. Objects that were not grasps were regarded as obstacles to be avoided during the manipulation task. information. Yet, the additional information gained by creating complete meshes also imposes additional requirements on the accuracy of the robot controller. Planning grasps with more accurate reconstructions of the observed object means that the robot needs to be very precise in the task execution. So far, we do not have a model of the inherent sensor and actuation noise. We hope to investigate Bayesian approaches to object fitting, which would allow us to use information about the uncertainty during task execution. ACKNOWLEDGMENTS This work is dedicated to the memory of Mike Stilman, whose enthusiasm for making robots do cool things will always be remembered. R EFERENCES
[1] R. Balasubramanian, L. Xu, P. D. Brook, J.R. Smith, and Y. Matsuoka. Human-guided grasp measures improve grasp robustness on physical robot. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2010. [2] G. Biegelbauer and M. Vincze. Efficient "3d" object detection by fitting superquadrics to range image data for robot's object manipulation. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2007. [3] J. Bohg, M. Johnson-Roberson, B. Le´ on, J. Felip, X. Gratal, N. Bergstrom, D. Kragic, and A. Morales. Mind the gap: Robotic grasping under incomplete observation. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2011. [4] R. Diankov. Automated Construction of Robotic Manipulation Programs. PhD thesis, Robotics Institute, Carnegie Mellon University, 2010. [5] K. Duncan, S. Sarkar, R. Alqasemi, and R. Dubey. Multi-scale superquadric fitting for efficient shape and pose recovery of unknown objects. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2013. [6] C. Ferrari and J. Canny. Planning optimal grasps. In IEEE Int. Conf. on Robotics and Automation (ICRA), 1992. [7] M. A. Fischler and R. C. Bolles. Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 1981. [8] C. Goldfeder and P. Allen. Data-driven grasping. Autonomous Robots, 2011. [9] C. Goldfeder, P. Allen, C. Lackner, and R. Pelossof. Grasp planning via decomposition trees. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2007. [10] K. Hsiao, S. Chitta, M. Ciocarlie, and E. Jones. Contact-reactive grasping of objects with partial shape information. In IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2010. [11] K. Huebner and D. Kragic. Selection of robot pre-grasps using boxbased shape approximation. In IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2008. [12] Y. Jiang, S. Moseson, and A. Saxena. Efficient grasping from rgbd images: Learning using a new rectangle representation. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2011. [13] M. Kazhdan, M. Bolitho, and H. Hoppe. Poisson surface reconstruction. In Proc. of the Fourth Eurographics Symposium on Geometry Processing, 2006. [14] U. Klank, D. Pangercic, R.B. Rusu, and M. Beetz. Real-time cad model matching for mobile manipulation and grasping. In 9th IEEE-RAS Int. Conf. on Humanoid Robots (Humanoids), 2009. [15] D. Kragic, A. Miller, and P. Allen. Real-time tracking meets online grasp planning. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2001. [16] O. Kroemer, H. Ben Amor, M. Ewerton, and J. Peters. Point cloud completion using extrusions. In Int. Conf. on Humanoid Robots(Humanoids), 2012. [17] A. Miller, S. Knoop, H. I. Christensen, and P. Allen. Automatic grasp planning using shape primitives. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2003. [18] Niloy J. Mitra, Leonidas J. Guibas, and Mark Pauly. Partial and approximate symmetry detection for 3d geometry. In ACM SIGGRAPH 2006 Papers, SIGGRAPH '06, pages 560­568, New York, NY, USA, 2006. ACM. [19] Y. Nakamura and H. Hanafusa. Inverse kinematic solutions with singularity robustness for robot manipulator control. Journal of dynamic systems, measurement and control, 1986. [20] E. Parzen. On estimation of a probability density function and mode. The Annals of Mathematical Statistics, 1962. [21] M. Przybylski, T. Asfour, and R. Dillmann. Unions of balls for shape approximation in robot grasping. In IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2010. [22] F. Solina and R. Bajcsy. Recovery of parametric models from range images: The case for superquadrics with global deformations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1990. [23] D. Yuret and M. de la Maza. Dynamic hill climbing: Overcoming the limitations of optimization techniques. In Second Turkish Symposium on Artificial Intelligence and Neural Networks, 1993.

View publication stats

Auton Robot (2014) 36:1­3 DOI 10.1007/s10514-013-9379-3

Special issue on autonomous grasping and manipulation
Heni Ben Amor · Ashutosh Saxena · Nicolas Hudson · Jan Peters

Published online: 27 November 2013 © Springer Science+Business Media New York 2013

Grasping and manipulation of objects are essential motor skills for robots to interact with their environment and perform meaningful, physical tasks. Since the dawn of robotics, grasping and manipulation have formed a core research field with a large number of dedicated publications. The field has reached an important milestone in recent years as various robots can now reliably perform basic grasps on unknown objects. However, these robots are still far from being capable of human-level manipulation skills including in-hand or bimanual manipulation of objects, interactions with nonrigid objects, and multi-object tasks such as stacking and tool-usage. Progress on such advanced manipulation skills is slowed down by requiring a successful combination of a multitude of different methods and technologies, e.g., robust vision, tactile feedback, grasp stability analysis, modeling of uncertainty, learning, long-term planning, and much more. In order to address these difficult issues, there have been an increasing number of governmental research programs such as the European projects DEXMART, GeRT and GRASP, and
H. Ben Amor (B) Robotics and Intelligent Machines, Georgia Institute of Technology, 801 Atlantic Drive, Atlanta, GA 30332-0280, USA e-mail: hbenamor@cc.gatech.edu A. Saxena Cornell University, 4159 Upson Hall, Ithaca, NY 14853, USA e-mail: asaxena@cs.cornell.edu N. Hudson Jet Propulsion Laboratory, M/S 82-105 4800 Oak Grove Drive, Pasadena, CA 91109, USA e-mail: nicolas.h.hudson@jpl.nasa.gov J. Peters Technical University Darmstadt, Hochschulstr. 10 , 64289 Darmstadt, Germany e-mail: mail@jan-peters.net

the American DARPA Autonomous Robotic Manipulation (ARM) project. This increased interest has become apparent in several international workshops at important robotics conferences, such as the well-attended workshop "Beyond Robot Grasping" at IROS 2012 in Portugal. Hence, this special issue of the Autonomous Robots journal aims at presenting important recent success stories in the development of advanced robot grasping and manipulation abilities. The issue covers a wide range of different papers that are representative of the current state-of-the-art within the field. Papers were solicited with an open call that was circulated in the 4 months preceding the deadline. As a result, we have received 37 submissions to the special issue which were rigorously reviewed by up to four reviewers as well as by at least one of the guest editors. Altogether twelve papers were selected for publication in this special issue. We are in particular happy to include four papers which detail the approach and goal of the DARPA ARM project as well as detailed descriptions of the developed methods. We start with the paper "The DARPA Autonomous Robotic Arm (ARM) Program - Synopsis" by Hacket et al. which provides a general overview of the recent ARM project. The paper motivates the decisions made during the ARM project and gives insights into its history, structure and current state. In the paper "An Autonomous Manipulation System based on Force Control and Optimization", Righetti et al. introduce a manipulation architecture that uses force-torque control, variable compliance and optimization methods to realize robust grasping. The architecture includes components for calibration, perception, motion planning, motion primitive execution, inverse kinematics and force control, and, therefore, addresses a number of vital sub-tasks of grasping and manipulation. The efficiency of this approach was demonstrated during competitions in the DARPA ARM project,

123

2

Auton Robot (2014) 36:1­3

where Righetti and colleagues were consistently ranked among the top two teams. The system of another top team at the DARPA ARM project is described in the paper "Model-Based Autonomous System for Performing Dextrous Human-Level Manipulation Tasks" by Hudson et al. from NASA JPL. The paper discusses the challenges faced during the project and the related competition and presents system architecture for autonomous bimanual manipulation. The required perceptual capabilities including mapping, object detection and object tracking are described in detail in the first part of the paper. The second part of the paper focuses on planning, control and action execution. The paper provides important insights into the design choices that were made during the development of the system, as well as their advantages and drawbacks. In the last paper on ARM entitled "Learning of Grasp Selection based on Shape-Templates", Herzog et al. present an algorithm for selecting good grasp poses for unknown objects from point cloud data. The approach uses a local grasp shape descriptor to encode suitable grasp locations on an object. This descriptor is used together with kinesthetic teaching methods in order to create a grasp library of stable grasps. The approach also includes a learning component that allows feedback on success or failure of a grasp to be used for adapting the library. The robustness of the approach is demonstrated in an extensive experiment with a variety of household objects. The next three papers of the special issue highlight recent advances in the design of robotic actuators for grasping. The paper "Design and Control of a Three-Fingered TendonDriven Robotic Hand with Active and Passive Tendons" by Ozawa et al. presents a new three-fingered robotic hand and a set of corresponding controllers. The paper discusses important aspects in the design of tendon-driven robotic fingers and shows how active and passive tendons can be used to realize variable stiffness control. The approach was validated by demonstrating five different types of stable grasps on a variety of objects. Another type of robot grippers is presented in the paper "A Compliant Self-Adaptive Gripper with Proprioceptive Haptic Feedback" by Belizle et al. The presented gripper features compliant joints, under actuation and a haptic interface. In addition to the technical description of this actuator, Belizle and colleagues also present a theoretical model using a quasistatic analysis. Finally, they also demonstrate the advantageous features of the new gripper in extensive simulated and real experimental results. A more bio-inspired approach to the design grippers is introduced in the paper "A Variable Compliance Soft Gripper" by Giannaccini et al. The paper presents a novel tentaclelike gripper that has a large degree of variability in its shape. In addition to the shape the authors also show how the compli-

ance of the gripper can be changed according to the requirements of the task. The remaining five papers focus on the algorithmic and theoretical modelling of grasping and manipulation. In "An Active Sensing Strategy for Contact Location without Tactile Sensors Using Robot Geometry and Kinematics" Lee et al. describe new methods for locating contacts without relying on specialized sensors. A geometric estimation of the contact point between the robot actuator and the environment is used in conjunction with a control strategy in order to improve estimation accuracy. Experimental results show that the estimated forces are more accurate that those achieved using force-torque controllers. The paper "Teaching Robots to Cooperate with Humans in Dynamic Manipulation Tasks Based on Multi-Modal Human-in-the-Loop Approach" by Peternel et al. focuses on compliant robotic manipulation in the presence of a human interaction partner. In this approach, a human demonstrator first tele-operates a robot arm using a motion capture setup in order to provide training data for a subsequent imitation learning step. However, in contrast to previous work on imitation learning, not only the position of the human's hand is recorded, but also the human's muscle activation. This information is, in turn, used to modulate the stiffness of the robot. The approach is verified in a cooperative wood sawing task where a human and a robot have to collaborate. In "Autonomously Learning to Visually Detect Where Manipulation Will Succeed", Nguyen et al. turn towards active learning of visual classifiers. They introduce a methodology for predicting successful locations for manipulation based on visual features. A robot autonomously generates training data by acting in his environment. The resulting data set is then processed via dimensionality reduction and Support Vector Machines. A set of experiments with a PR2 robot show how this methodology can be used by a robot to autonomously improve the success rates during daily tasks, e.g., turning a switch. The paper "Object Search by Manipulation" by Dogar et al. addresses the question of how to search for an object in a cluttered environment. In such a situation a robot needs to push away occluding objects in order to find what it is looking for. Dogar and colleagues show that even a greedy approach to pushing objects away can be optimal under certain conditions. They also present a second algorithm which approaches polynomial time complexity and produces optimal plans under all situations. Both algorithms are evaluated on a real-world mobile robot platform. Additionally, the authors provide a Markov Decision Problem formulation of the problem and present a partial proof of optimality.` Finally, the paper "Analyzing Dexterous Hands using a Parallel Robots Framework" by Borras et al. adapts and extends an existing mathematical framework from the par-

123

Auton Robot (2014) 36:1­3

3 Nicolas Hudson is currently a member of technical staff in the Robotic Manipulation and Sampling group at the Jet Propulsion Laboratory, California Institute of Technology. He received the BE(Hons) degree from the University of Canterbury in 2002, the M.Eng degree from the California Institute of Technology in 2004, and a Ph.D. in Mechanical Engineering from the California Institute of Technology in 2009. Dr. Hudson received the 2012 NASA Early Career Achievement Medal for technical achievement in autonomous manipulation, and a 2012 NASA Group Achievement Award as part of the Autonomous Robotic Manipulation software team. He is the Task Manager of the JPL-Caltech DARPA ARM-S task.

allel robotics literature to analyze underactuacted robotic hands. The authors apply the framework to analyze a simple example hand. In this analysis they show how the underactuation design parameters such as the transmission ratio and the stiffness constants of the finger joints can modify the size of the feasible workspace. All 12 papers present significant developments in robot grasping and manipulation and we hope that you will enjoy reading them as much as we did.

Heni Ben Amor is a Research Scientist at the Robotics and Intelligent Machines Institute at GeorgiaTech in Atlanta. He received his M.Sc. from the University of Koblenz-Landau, and his Ph.D. from the Technische Universitaet Bergakademie Freiberg. Heni has been a visiting researcher with the Intelligent Robotics Group of the University of Osaka, Japan and a postdoctoral scholar with the Technical University Darmstadt, Germany. His research has won several awards such as the Bernhard-v.-Cotta Award 2011 and the CoTeSys Best Paper Award at IEEE RO-MAN 2009. Since 2011, he is also a recipient of the Daimler-Benz Fellowship. His research interests include robotics, virtual reality, machine learning, motor skill learning and human­robot interaction.

Ashutosh Saxena is an assistant professor in the Computer Science department at Cornell University. His research interests include machine learning, robotics and computer vision. He received his MS in 2006 and Ph.D. in 2009 from Stanford University, and his B.Tech. in 2004 from Indian Institute of Technology (IIT) Kanpur. He has also won best paper awards in 3DRR, RSS and IEEE ACE. He was named a co-chair of IEEE technical committee on robot learning. He was a recipient of National Talent Scholar award in India and Google Faculty award in 2011. He was named Alred P. Sloan Fellow in 2011, a Microsoft Faculty Fellow in 2012, and received a NSF Career award in 2013.

Jan Peters is a full professor (W3) at the Technische Universitaet Darmstadt and senior research scientist at the MaxPlanck Institute for Intelligent Systems. Jan Peters has received the Dick Volz Best 2007 US PhD Thesis Runner Up Award, the 2012 Robotics: Science & Systems - Early Career Spotlight, the Young Investigator Award of the International Neural Network Society, and the IEEE Robotics & Automation Society's Early Career Award. Jan Peters has received four Master's degrees in Computer Science, Electrical, Mechanical and Aerospace Engineering at TU Munich and FernUni Hagen in Germany, at the National University of Singapore (NUS) and the University of Southern California (USC) as well as a Computer Science Ph.D. from USC. Jan Peters has also been a researcher in Germany at DLR, TU Munich and the Max Planck Institute for Biological Cybernetics, in Japan at ATR, at USC and at both NUS and Siemens Advanced Engineering in Singapore.

123

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1

Learning Two-Person Interaction Models for Responsive Synthetic Humanoids
David Vogt, Heni Ben Amor, Erik Berger, Bernhard Jung Institute for Informatics TU Bergakademie Freiberg Bernhard-von-Cotta-Straße 2 09599 Freiberg {david.vogt|erik.berger|jung}@informatik.tu-freiberg.de http://humanoids.tu-freiberg.de FB Informatik TU Darmstadt Hochschulstraße 10 64289 Darmstadt amor@ias.tu-darmstadt.de www: http://www.ias.tu-darmstadt.de/Member/HeniBenAmor
 

Abstract
Imitation learning is a promising approach for generating life-like behaviors of virtual humans and humanoid robots. So far, however, imitation learning has been mostly restricted to single agent settings where observed motions are adapted to new environment conditions but not to the dynamic behavior of interaction partners. In this paper, we introduce a new imitation learning approach that is based on the simultaneous motion capture of two human interaction partners. From the observed interactions, low-dimensional motion models are extracted and a mapping between these motion models is learned. This interaction model allows the real-time generation of agent behaviors that are responsive to the body movements of an interaction partner. The interaction model can be applied both

to the animation of virtual characters as well as to the behavior generation for humanoid robots.. Keywords: Imitation learning, motor learning, motion adaptation, interaction learning, virtual characters, humanoid robots

1

Introduction

Digital Peer Publishing Licence Any party may pass on this Work by electronic means and make it available for download under the terms and conditions of the current version of the Digital Peer Publishing Licence (DPPL). The text of the licence may be accessed and retrieved via Internet at http://www.dipp.nrw.de/.
First presented at Virtuelle und Erweiterte Realit¨ at 9. Workshop der GI-Fachgruppe VR/AR 2012, extended and revised for JVRB

Generating natural behavior for synthetic humanoids such as virtual characters and humanoid robots in interactive settings is a difficult task. Many degrees of freedom have to be controlled and coordinated so as to realize smooth movements and convincing reactions to the environment and interaction partners. A common approach for the generation of human-like behaviors is based on motion capture data. Movements recorded from human demonstrators are replayed and possibly slightly altered to fit the current situation. So far, however, approaches based on real-time adaptation of motion capture data have been mostly restricted to settings in which the behavior of only one agent is recorded during the data acquisition phase. As a result, the mutual dependencies inherent to the interaction between two agents cannot be represented and, thus, reproduced. During a live interaction, a synthetic humanoid may not be able to appropriately respond to the behavior of a human interaction partner.

urn:nbn:de:0009-6-38565, ISSN 1860-2037

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1 Moreover, due to recent developments in cheap motion capture technologies, e.g. the Kinect camera, there is an increasing need for algorithms that can generate responses to perceived body movements of a human interaction partner. E.g. in gaming, a virtual character should recognize the behavior of the human player and respond with an appropriate reaction. In this paper, we present a novel approach for realizing responsive synthetic humanoids, that can learn to react to the body movements of a human interaction partner. The approach extends traditional imitation learning [BCDS08] to settings involving two persons. Using motion capture technology, the movements of a pair of persons are first recorded and then processed using machine learning algorithms. The resulting interaction model encodes which postures of the passive interaction partner have been obtained depending on postures of the active interaction partner. Once a model is learned, it can be used by a synthetic humanoid (passive interaction partner) to engage in a similar interaction with a human counter part. The presented interaction learning approach addresses not only the question of what to imitate, but also when to imitate (c.f. [DN02]). This paper is organized as follows: In Section 2 we discuss related work. In Section 3 we introduce twoperson interaction models and describe how to learn them from the simultaneous motion capture of two interaction partners. Section 4 shows examples how two-person interaction models can be applied to the behavior generation of virtual characters. In Section 5, we discuss two alternative machine learning algorithms for training our two-person interaction models. In section 6 we discuss the applicability of two-person interaction models to humanoid robots briefly before we finally conclude in Section 7. the virtual environment. The approach formulates motion tracking as an optimal control problem whereby optimization methods derive parameters of a controller for real-time generation of full-body motions. Multon and colleagues [MKHK09] present a framework for animating virtual characters, where motion capture data is adapted in real-time based on a morphology-independent representation of motion and space-time constraints. Ishigaki et al. [IWZL09] introduced a control mechanism that utilizes example motions as well as live user movements to drive a kinematic controller. Here, a physics model generates a character's reactive motions. Other work aims to adapt motion capture data through the inclusion of inverse kinematics solvers, e.g. [KHL05, MM05]. Generating natural looking motions is also a focal point in the field of humanoid robotics. Researchers have extended the concept of motion capture by introducing imitation learning techniques, which can learn a compact representation of the observed behavior [BCDS08]. Once a motion representation is learned, it can be used to synthesize new movements similar to the shown behavior while at the same time adapting them to the current environmental conditions. Imitation learning, thus, aims to combine the advantages of model-driven approaches, such as adaptability to unknown execution contexts, with benefits of data-driven approaches, such as synthesis of more natural-looking motions. In an motion generation approach presented by Calinon et al. [CDS+ 10] Gaussian Mixture Regression models and Hidden Markov models (HMMs) are used to learn new gestures from multiple human demonstrations. In doing so time-independent models are built to reproduce the dynamics of observed movements. In [YT08] the authors utilize recurrent neural networks to learn abstract task representations, e.g. pick and place. During offline training a robots' arms are guided to desired positions and the teachers' demonstrations are captured. In recent years, various attempts have been undertaken for using machine learning in human-robotinteraction scenarios. In [WDA+ 12], an extension of the Gaussian Process Dynamics Model was used to infer the intention of a human player during a table-tennis game. Through the analysis of the human player's movement, a robot player was able to

2

Related Work

Motion capture has been widely used for animating virtual characters. However, adapting the acquired motion data to new situations is a difficult task that is usually performed offline. In contrast, interactive settings as considered in our approach require the realtime adaptation of motion capture data. Some of the previous approaches for this are discussed in the following. In [YL10], for example, feedback controllers are constructed that can be used to adapt motion capture data sequences to physical pertubations and changes in urn:nbn:de:0009-6-38565, ISSN 1860-2037

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1

Figure 1: Use of two-person interaction models: Two humans' live-motions are captured, e.g. using the Kinect depth camera, and projected to a low-dimensional posture space. This space is used to learn a model of the shown interaction which is then utilized for motion generation for a synthetic humanoid in real-time. determine the position to which the ball will be returned. This predictive ability allowed the robot to initiate its movements even before the human hit the ball. In [IAM+ 12], Gaussian mixture models were used to adapt the timing of a humanoid robot to that of a human partner in close-contact interaction scenarios. The parameters of the interaction model were updated using binary evaluation information obtained from the human. While the approach allowed for human-in-theloop learning and adaptation, it did not include any imitation of observed interactions. In a similar vein, the work in [LN10] showed how a robot can be actively involved in learning how to interact with a human partner. The robot performed a previously learned motion pattern and observed the partner's reaction to it. Learning was realized by recognizing the observed reaction and by encoding the action-reaction patterns in a HMM. The HMM was then used to synthesize similar interactions. However, in all of the above approaches, the synthetic humanoid's motion is generated from motion demonstrations by only one actor. In contrast, the approach presented below draws on the simultaneous motion capture of two actors that demonstrate example human-human interactions. man actors. The interaction model is built in three steps: (1) Interactions between two persons are recorded via motion capture; (2) the dimensionality of the recorded motions is reduced; and, (3) a mapping between the two low-dimensional motions is learned.

3.1

Motion capture of two-person interactions

3

Learning Two-Person Interaction Models

In this section, a novel interaction learning method is introduced that allows virtual characters as well as humanoid robots to responsively react to the on-going movements of a human interaction partner (see Figure 1). At the core of our approach is an interaction model which is learned from example interactions of two huurn:nbn:de:0009-6-38565, ISSN 1860-2037

In the first step, the movements of two people interacting with each other are captured. In general, the presented interaction learning approach is independent of particular motion capture systems. For development and testing, we used the Kinect sensor to record joint angles. The precision of the calculated joint angles depends on the sampling rate. In general, low sampling rates lead to small datasets lacking accuracy whereas high sampling rates result in larger datasets with increased precision. Also, higher sampling rates may lead to redundant joint angle values for slow motions. For the behaviors used in this paper a sampling rate of 30fps is used. Recorded motions are very high dimensional as 48 joint angles are captured per actor. A problem that arises when recording such motions is that not all measured variables are important in order to understand the underlying phenomena [EL04]. Hence, dimensionality reduction should be applied to remove the redundant information, producing a more economic representation of the recorded motion. To this end, we use Principal Component Analysis (PCA) for construction of a low-dimensional posture space [Amo10] which will be explained further in the following section.

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1 important that every point in the input posture space can be mapped to the output posture space and not just a subset of points, e.g. points lying on the lowdimensional trajectory of the demonstrated movement. In this way, the interaction model can also map postures that are similar to but not exactly like postures seen during training. In the following, two machine learning algorithms with suitable generalization capabilities for learning such mapping functions are described. 3.3.1 Feedforward Neural Net

Figure 2: A low-dimensional posture space for a kick behavior reduced to two dimensions with principal component analysis. Every point in this space corresponds to a posture which can be reprojected to its original dimensionality and adopted by a virtual human or humanoid robot.

3.2

Dimensionality Reduction

Principal Component Analysis (PCA) is applied to the high-dimensional motion capture data to yield a lowdimensional posture space [Amo10]. PCA reduces the dimensionality of a dataset based on the covariance matrix of modeled variables. Dimensionality reduction is achieved by finding a small set of orthogonal linear combinations (the principal components) of the original variables depending on the largest variance. Previous results indicate that for most skills 90% of the original information can be retained with two to four principal components [ABVJ09]. Each point in the posture space corresponds to a pose of the synthetic humanoid as illustrated in figure 2. Accordingly, a trajectory in posture space corresponds to a continuous movement. Hence, new behaviors can be created by generating trajectories in the posture space and projecting these back to the original high-dimensional space of joint angle values [Amo10].

3.3

Two-Person Interaction Models

A two-person interaction model is the combination of two low-dimensional posture spaces with a mapping function from one posture space to the other. It is urn:nbn:de:0009-6-38565, ISSN 1860-2037

Learning input-output relationships from recorded motion data can be considered as the problem of approximating an unknown function. A Feedforward Neural Net (FNN) is known to be well suited for this task [MC01]. In our experiments, we use a simple FNN consisting of three layers, i.e. an input, a hidden and an output layer. How many neurons the hidden layer consists of and which connectivity value has to be used, depends on the recorded motion data. In general, the net should just have enough neurons to fit the data adequately while providing enough generalization capabilities for complex functions. We use 10 neurons in the hidden layer since we found that this fits the data adequately while retaining generalization capabilities for unseen low dimensional points. More neurons could increase generalization but also increase the risk of overfitting the data. For the training of a FNN Levenberg-Marquardt backpropagation is utilized and all points from the low-dimensional trajectory of the active interaction partner are used as training data. Overfitting is avoided by using early stopping. For that low dimensional data is divided into two subsets: for training and testing. The training data is used to adapt the nets weights and biases. The test data is utilized to monitor the validation error. This error normally decreases for a number of iterations. When it starts increasing again, this indicates overfitting and the training is stopped. Generally, not only the current posture but also its history is necessary to determine an appropriate response. As FNNs have no short-term memory, the input to the FNN cannot be limited to the current posture. Instead, the FNN takes as input a sequence of low-dimensional postures, i.e. the current posture and a number of history postures. The labels (output) of the net are all low-

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1

Figure 3: Multiple human postures are mapped on single robot/virtual human posture. Input to both nets (Left: FNN; Right: RNN) are the low-dimensional postures from a sliding window over time, i.e. the current posture with a history of preceeding postures. The output is the low-dimensional posture that represents the robot's/virtual human's response to the currently observed posture. dimensional points on the low-dimensional trajectory of the second, reactive interaction partner. Figure 3 shows how a sequence of postures is mapped onto a single output posture using an FNN. den neuron feeds back to all hidden neurons. This embodies the desired short term memory. The hidden layer is updated not only with the current external input but also with activations from the previous timestep. This allows the usage of smaller sliding windows and, therefore, less input neurons. In our experiments 3.3.2 Layered Recurrent Neural Network the amount of neurons in the hidden layer is the same Strict feedforward neural nets have no short term as for feedforward neural nets. memory and cannot store history postures. Desired memory effects are only created due to the way how past inputs are represented in the net. Hence, a sliding window has to be used. And, thus, the amount of inFor training, we use Bayesian regulation backpropaput neurons increases with each additional pose by the gation. Training examples are low-dimensional points number of dimensions of the low-dimensional posture of the active partner's movements and their mapping space. onto low-dimensional points on the trajectory of the To circumvent this problem we utilize layered re- reactive interaction partner (see figure 3). The trained current neural nets (RNN) [Elm90] which have proven RNN defines a continuous mapping from the input to be well suited for modeling various temporal se- posture space to the output posture space while prequences [FS02, YT08, NNT08]. RNNs have a feed- serving the temporal context of the recorded interacback activation in the hidden layer where each hid- tion. urn:nbn:de:0009-6-38565, ISSN 1860-2037

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1

Figure 4: Two different punches and defenses have been used to learn an interaction model. Left: A low punch is defended with both arms pointing downwards. Right: A high punch is defended by pulling both arms up for defense.

4

Examples

In the following, we describe two examples where two persons are recorded while performing martial arts. The first interaction is a combination of punches varying in height with proper defenses. The second scenario consists of different kicks also with suitable defenses. For both behaviors, a virtual human learns to block the attacks with the trained defenses. The motions of both the attacker and the defender start and end in an upright position with both arms stretched. The Kinect sensor was used as motion capture device in both examples.

4.1

Punching

ries are utilized to train both mapping algorithms. In our experiments a 10 point pose history for FNNs, i.e. a 0.66 second window, has proven to be well suited. For RNNs good learning results have been achieved with only two points in the history. For live-interaction with a virtual character, the user's current posture is captured at 30fps and projected into the previously created low-dimensional posture space. In combination with previous points a sequence of history poses is used as input for the mapping algorithm which predicts a low-dimensional point in the virtual character's posture space. The resulting point is then projected back to its original dimension and used as target pose for the virtual character. Figure 5 shows how the learned two person inter-

In the first example a two person interaction model is used to generalize various punches and continuously compute motions for a defending virtual character. For that, two behaviors have been recorded. The first is a sequence of low punches at stomach level where the defender stretches his arms forward to block the attack. The second behavior consists of high punches at face level where the defender had to pull up both arms for a proper defense. In the center of figure 4 both punches with their defense motion are shown. The training of the mapping algorithms is based on these two recordings. For each behavior, three repetitions of the respective punching style and their defense are captured. Then, the motions are projected into low dimensional space which can be seen in figure 4 for both interaction partners. All low dimensional points of the behavior trajectourn:nbn:de:0009-6-38565, ISSN 1860-2037

Figure 5: The image shows three punches for a user driven avatar (red skeleton) and the calculated virtual characters defense motion (blue skeleton). The user's motion on the left and right were similar to the ones used for model learning. However, the punch height in the middle has not been trained explicitly. Still, the learned model can calculate plausible motions.

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1

Figure 6: Two different kicks and defenses were used to learn an interaction model. For low kicks the defender needs to crouch down in order to reach the attackers leg. action model calculates the postures of a virtual character (blue skeleton) depending on postures obtained by a user driven avatar (red skeleton). The left and right of the figure display a newly recorded user motion similar to the ones used to train the mapping algorithm. However, the motion displayed in the center of the figure exhibits a punch height that has not been trained explicitly. Nevertheless, the interaction model can calculate suitable motions for the defending virtual character. With the learned mappings, the virtual character can defend itself against the trained attacks. A clear differentiation between punching heights has been learned. Even for varying punch heights, the virtual human can still respond with correct defense motions. of a hidden layer with 10 neurons. Addtionally, the size of the pose history has been set to 5 for FNNs and 2 for RNNs respectively. For a live interaction with the character, the user's current posture is once again fed into the mapping algorithm and an appropriate pose for the defending character is predicted. Figure 7 shows a user controlled character repeating various kicks which where similar to the ones used to drive the nets training. Additionally, a kick height (figure 7 center) not present in the recordings was performed. Since, varying kick heights of the attacker result in different low dimensional points, the virtual characters trajectories are located in the range of the previously recorded motion.

4.2

Kicking

In the second example a two-person interaction model is learned from various kicks allowing a virtual character to defend itself with suitable defenses. For that two different kicks have been recorded. The first is a low kick where the defender needed to bend its knees in order to reach the attackers' foot. The second is a higher kick which was defended with an upright position (see figure 6). For model learning both attack styles were performed three times and combined in a single animation. Since each motion started and ended in an upright position, an enclosed trajectory is created in lowdimensional space, as can be seen in figure 6. This 4-dimensional posture space contains enough information to reconstruct animations with a negligible error. The neural nets were configured to consist urn:nbn:de:0009-6-38565, ISSN 1860-2037

Figure 7: A learned interaction model is used to calculate a virtual character's defend motions (blue character) based on a user controlled avatar (red character). The kick heights on the left and right are similar to the ones obtained during training. However, the kick height in the center was not present in any recording. Nevertheless, the two-person interaction model can generate suitable motions for the defending virtual character.

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1

Figure 8: Euclidean distances are used to validate both mapping algorithm's qualities. Here, the graph shows distances for a novel punch data set. As it can be seen RNNs exhibit a smaller overall error. For all three kick heights the virtual character learned when to defend itself using a crouched defense and when to block in an upright posture. In both cases the arms need to block the attacker's leg at the right time. For similar kicks, yet with different heights, proper defenses are generated by both FNN and RNN. The interaction model that has been learned during this example allows the control of a virtual character based on demonstrations of only two variants of the behavior. The character is controlled continuously in a low-dimensional space and the model is robust to unseen user postures. Since the executed behaviors are based on human motion data, a life-like appeal of virtual characters can been produced. posture histories unnecessary. However, we found that by the inclusion of a small number history poses, i.e. two additional postures, smoother responses are generated by the RNN. Figure 9 shows the prediction for training data with various history sizes. As can be seen, the overall response time of the synthetic humanoid increases if additional postures are used to predict the character's pose. We found that 2-point histories, i.e. the original point and two previous points, represent a good compromise between fast response times and high smoothness of the generated movements. This corresponds to the works of Ziemke et al. [Zie96] which came to a similar conclusion. The combination of recurrence and a small sliding window tends to work best.

5

Comparison of Mapping Algorithms 6 Applicability to Humanoid Robots

The interaction models for the examples were learned with both mapping algorithms described above, i.e. FNN and RNN. In order to evaluate the quality of the mappings, the Euclidean distances between desired and trained low-dimensional points are measured, e.g. the training error. For that we recorded another punching example that is similar to the original human motion. A comparison of the resulting error is shown in figure 8 for the punching example. Overall, we found that both mapping algorithms produce similar results. However, FNNs tend to exhibit weaker generalization capabilities. With varying interaction learning scenarios, differing net sizes were used in order to minimize the training error. Since the required size is hard to determine beforehand a cumbersome, trial and error process is inevitable. In contrast, RNNs are more tolerant towards untrained inputs and no architecture changes have been required throughout the examples. Furthermore, RNNs maintain a short term memory, which renders urn:nbn:de:0009-6-38565, ISSN 1860-2037

Two person interaction models can also be used to control humanoid robots, although an additional processing step to adapt the resulting motion is necessary. It is generally known that human motion cannot be transfered to robots without further optimization and is generally referred to as the correspondence problem [ANDA03]. To overcome this the low-dimensional trajectory of the defender has to be altered to fit the robot's kinematic chain and stability constraints. For that we utilize an inverse kinematics solver. The two-person interaction model thus maps the original low-dimensional movement of the attacker to the adopted low-dimensional movement of the defender (robot). The learned interaction model is then used to predict robot postures depending on observed user postures. As can be seen in figure 10 a Nao robot successfully mimics the demonstrated defense behavior.

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1

Figure 9: A pose history improves the overall mapping quality. The figure illustrates how various sizes influence smoothness and response time for the punch behavior of a RNN. With increasing history sizes the net's prediction is shifted to the left, leading to larger response times but at the same time smoother motions are generated. We found that three points, i.e. the original point and two previous points in the posture history produced the best results.

Figure 10: Utilizing a two-person interaction model, a humanoid robot can react to the on-going behavior of a human interaction partner. The robot's motion is controlled by a low-dimensional mapping which preserves temporal coherences of the trained behavior.

7

Conclusion

In this paper we presented a new approach for teaching synthetic humanoids, such as virtual characters and humanoid robots, how to react to human behavior. A recorded example of the interaction between two persons is used to learn an interaction model specifying how to move in a particular situation. Two machine learning algorithms have been implemented to establish a mapping between (low-dimensional) movements of the two persons: Simple feedforward neural networks (FNN) and layered recurrent neural networks (RNN). While the FNN requires a large sliding window of current and recent body postures as input, the RNN can store previous poses in its internal state and thus can produce appropriate responses with a small sliding window of history poses. In our experiments, the RNN performed slightly better than the FNN.

ory, no explicit segmentation of the seen behavior into separate parts is required. The responses of the synthetic humanoid are calculated continuously, based on learned mapping between the body postures of the observed humans. As a result, the interaction model can generalize to different situations while at the same time producing, smooth continuous movements. Our method can be used to learn how to control synthetic humanoids from observation of similar situations between two humans. This can help to significantly increase the realism in the interaction between a robot and a human, or a virtual character and a human.

So far, however, our approach is based on joint angle data and, hence, response generation is based on similarities in joint-space. However, in order to more accurately reflect critical spatial relationships between the body parts of the interaction partners, motion synthesis could also be based on optimizations in taskIn contrast to previous approaches to interaction space. By doing so, we could also better account for modeling found in computer animation and game the- varying body heights of both interactants.

urn:nbn:de:0009-6-38565, ISSN 1860-2037

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1 An interesting addition to the current approach [DN02] would be a higher-level component for planning and strategic decision making. This could, for example, be realized through a combination of our approach and the approach of Wampler et al. [WAH+ 10]. We also hope to model multiple possible responses per stimulus. In our current setup each movement of the human [EL04] can trigger only one possible response of the virtual character. A mixture-of-experts approach [RP06], in which several interaction models are combined, can help to overcome this problem. Kerstin Dautenhahn and Chrystopher L. Nehaniv (eds.), Imitation in animals and artifacts, ch. The agent-based perspective on imitation, MIT Press, Cambridge, MA, USA, 2002. Ahmed Elgammal and Chan-Su Lee, Inferring 3D Body Pose from Silhouettes Using Activity Manifold Learning, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Los Alamitos, CA, USA), vol. 2, IEEE Computer Society, 2004, pp. 681­688, ISBN 0-7695-2158-4. Jeffrey L. Elman, Finding structure in time, Cognitive Science 14 (1990), no. 2, 179­211, ISSN 0364-0213. Eberhart E. Fetz and Larry E. Shupe, The hand book of brain theory and neural network, ch. Recurrent network: neurophysiological modeling, pp. 860­863, MIT Press, Cambridge, 2002. Shuhei Ikemoto, Heni Ben Amor, Takashi Minato, Bernhard Jung, and Hiroshi Ishiguro, Physical Human-Robot Interaction: Mutual Learning and Adaptation, Robotics Automation Magazine, IEEE 19 (2012), no. 4, 24­35, ISSN 1070-9932.

References
[ABVJ09] Heni Ben Amor, Erik Berger, David Vogt, [Elm90] and Bernhard Jung, Kinesthetic bootstrapping: teaching motor skills to humanoid robots through physical interaction, Proceedings of the 32nd annual [FS02] German conference on advances in artificial intelligence (Berlin, Heidelberg), KI'09, Springer-Verlag, 2009, pp. 492­ 499, ISBN 978-3-642-04616-2. [Amo10] Heni Ben Amor, Imitation Learning of [IAM+ 12] Motor Skills for Synthetic Humanoids, Ph.D. thesis, Technische Universit¨ at Bergakademie Freiberg, 2010.

[ANDA03] Aris Alissandrakis, Chrystopher L. Nehaniv, Kerstin Dautenhahn, and Hatfield Herts Al Ab, Solving the Correspondence Problem between Dissimilarly Embodied [IWZL09] Satoru Ishigaki, Timothy White, VicRobotic Arms Using the ALICE Imitation tor B. Zordan, and C. Karen Liu, Mechanism, Proceedings of the second Performance-based control interface for international symposium on imitation in character animation, ACM SIGGRAPH animals and artifacts, 2003, pp. 79­92. 2009 papers (New York, NY, USA), ACM, 2009, Article no. 61, ISBN 978-1[BCDS08] Aude Billard, Sylvain Calinon, Ruediger 60558-726-4. Dillmann, and Stefan Schaal, Handbook of Robotics, ch. 59: Robot Programming by Demonstration, pp. 1371­1389, MIT [KHL05] Taku Komura, Edmond S. L. Ho, and Rynson W. H. Lau, Animating reactive Press, 2008. motion using momentum-based inverse [CDS+ 10] Sylvain Calinon, Florent D'halluin, kinematics: Motion Capture and ReEric L. Sauser, Darwin G. Caldwell, trieval, Comput. Animat. Virtual Worlds and Aude G. Billard, Evaluation of a 16 (2005), no. 3-4, 213­223, ISSN 1546probabilistic approach to learn and re4261. produce gestures by imitation, Robotics Dongheui Lee and Yoshihiko Nakamura, and Automation (ICRA), 2010 IEEE [LN10] Mimesis Model from Partial ObservaInternational Conference on, 2010, tions for a Humanoid Robot, Int. J. Rob. pp. 2671­2676. urn:nbn:de:0009-6-38565, ISSN 1860-2037

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1 Res. 29 (2010), no. 1, 60­80, ISSN 0278- [YL10] 3649. [MC01] Danilo Mandic and Jonathon Chambers, Recurrent neural networks for prediction: learning algorithms, architectures and stability, Adaptive and learning sys- [YT08] tems for signal processing, communications, and control, John Wiley and Sons, Inc. New York, NY, USA, 2001, ISBN 0471495174. Yuting Ye and C. Karen Liu, Optimal feedback control for character animation using an abstract model, ACM Trans. Graph. 29 (2010), no. 4, 74:1­74:9, ISSN 0730-0301. Yuichi Yamashita and Jun Tani, Emergence of Functional Hierarchy in a Multiple Timescale Neural Network Model: A Humanoid Robot Experiment, PLoS Comput Biol 4 (2008), no. 3, e1000220, ISSN 0730-0301. Tom Ziemke, Radar Image Segmentation using Recurrent Artificial Neural Networks, Pattern Recognition Letters 17 (1996), no. 4, 319­334, ISSN 0167-8655.

[MKHK09] Franck Multon, Richard Kulpa, Ludovic Hoyet, and Taku Komura, Interactive an- [Zie96] imation of virtual humans based on motion capture data, Comput. Animat. Virtual Worlds 20 (2009), no. 5-6, 491­500, ISSN 1546-4261. [MM05] Michael Meredith and Steve Maddock, Adapting motion capture data using weighted real-time inverse kinematics, Comput. Entertain. 3 (2005), no. 1, 5­5, ISSN 1544-3574. Ryunosuke Nishimoto, Jun Namikawa, and Jun Tani, Learning Multiple Goal-Directed Actions Through SelfOrganization of a Dynamic Neural Network Model: A Humanoid Robot Experiment, Adaptive Behavior 16 (2008), no. 2-3, 166­181, ISSN 1059-7123. Romesh Ranawana and Vasile Palade, Multi-Classifier Systems: Review and a roadmap for developers, Int. J. Hybrid Intell. Syst. 3 (2006), no. 1, 35­61, ISSN 1448-5869.

[NNT08]

Citation David Vogt, Heni Ben Amor, Erik Berger, and Bernhard Jung, Learning Two-Person Interaction Models for Responsive Synthetic Humanoids, Journal of Virtual Reality and Broadcasting, 11(2014), no. 1, January 2014, urn:nbn:de: urn:nbn:de:0009-6-38565, ISSN 1860-2037.

[RP06]

[WAH+ 10] Kevin Wampler, Erik Andersen, Evan Herbst, Yongjoon Lee, and Zoran Popovi´ c, Character animation in twoplayer adversarial games, ACM Trans. Graph. 29 (2010), no. 3, ISSN 0730-0301, Article no. 26. [WDA+ 12] Zhikun Wang, Marc Deisenroth, Heni Ben Amor, David Vogt, Bernhard Schoelkopf, and Jan Peters, Probabilistic Modeling of Human Movements for Intention Inference, Proceedings of Robotics: Science and Systems (R:SS), 2012. urn:nbn:de:0009-6-38565, ISSN 1860-2037

2014 14th IEEE-RAS International Conference on Humanoid Robots (Humanoids) November 18-20, 2014. Madrid, Spain

Handover planning for every occasion
Ana Huam´ an Quispe Heni Ben Amor Mike Stilman

Abstract-- In this paper we explore shared manipulation tasks involving collaboration between two agents: a bimanual manipulator and a movable humanoid. We propose a deterministic planner that outputs a handover pose for the object manipulated, the grasps to be used for both agents, and arm trajectories that allow the agents to reach, grasp, interchange and place the object at its final goal pose. Most existing approaches to shared manipulation assume that both participating agents are (or can be) positioned face-to-face. In this paper, we consider a more general set of scenarios in which the relative pose between agents is not tightly constrained to facing each other. To accomplish a successful interaction between the agents, careful planning must be done in order to select the best possible handover pose such that the movable agent should not need to move its torso, only its arm, to interact with the object. We present 3 simulation experiments with manipulation tasks performed by a bimanual fixed manipulator and a humanoid robot.

Reaching a cup from front Reaching a jar from above

Reaching a hammer from below Fig. 1: Shared manipulation tasks with non face-to-face interaction between agents between examples (i.e. the humanoid does not face the fixed manipulator in neither of these examples, a fact that is commonly assumed for shared manipulation tasks). In this paper, we intend to address situations as the ones depicted, in which the movable receiver can vary its pose with respect to the fixed robot. Among some questions that must be answered to solve this problem are: A) Which arm should the fixed robot use to hand the object? B) How should the object be grasped such that the movable receiver can grasp it easily? and C) Which handover pose is good enough as to reduce the overall effort of the movable receiver?. The rest of this paper is organized as follows: Section II summarizes relevant literature on shared manipulation tasks. Section III and IV explains our algorithm, the assumptions considered and our current limitations. Section V presents simulation results in 3 different arbitrary environments and finally in Section VI we discuss our approach, its advantages and shortcomings as well as future work. II. P REVIOUS W ORK In [5] Edsinger et al. argue that fluent handover between a robot and a human allows for a range of cooperative manipulation tasks. They present a set of perception and control modules which can be used to realize a simple object exchange between interaction partners. Kemp and colleagues argue that the complimentary skills of humans and robots can be exploited in such a scenario. More specifically, the challenging task of robot grasp planning can be simplified by having the human place the object in the robot's end effector. Mainprice and colleagues [8] investigated how the spatial preferences of a human partner can be incorporated into the robot decision making during handover tasks. They
431

I. I NTRODUCTION Useful home robots should be capable to perform simple tasks that make human lives easier. In particular, we consider collaborative manipulation tasks in which a robot acts as an assistant, handing objects to a user. While inherently simple, these types of tasks are more likely to happen in a household in a regular basis, so we consider it a key robot capability. Shared manipulation tasks can be studied from different perspectives. From the side of human-robot interaction, there is a vast amount of work that analyses the information exchanged between robot and user during the handing over of the manipulated object [2]. On the other hand, the manipulation planning community frames the problem mostly from the point of view of the robot in order to select a robust grasp on the object and then find a feasible arm trajectory that allows the robot to perform its share of the task successfully. In this paper, our goal is to enable a fixed robot to effectively perform a handover task while keeping the movable receiver agent in mind. We acknowledge the movable receiver agent by considering the receiver's dexterity while placing the object at its goal pose. Furthermore, we address the fact that for different tasks, the movable receiver might not be ideally located with respect to the fixed robot (i.e. facing it), so it is the fixed robot's task to select an arm trajectory that places the manipulated object in a pose such that the receiver can easily grasp it. Depending on the specific situation, this could be naively simple or notoriously difficult. As an illustration, consider the examples shown in Figure 1. All of them involve a bimanual fixed manipulator handing over an object to a movable humanoid whose relative pose with respect to the fixed manipulator varies drastically
Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA 30332, USA.ahuaman3@gatech.edu,

hbenamor@cc.gatech.edu
978-1-4799-7173-2/14/$31.00 ©2014 IEEE

introduced a planning algorithm which can be parametrized by specifying a mobility parameter. The mobility parameter reflects the ability or willingness of the human partner to move to a new position to obtain an object. In case of a high mobility parameter, the robot can plan handover actions that require some effort from the human partner. In contrast, a low mobility parameter means that the robot should reduce the human's effort by moving to the exchange site location. In [9], they also investigated how gaze direction and postural comfort can be used to identify a transfer point In [4] Cakmak and colleagues investigated human preferences in robot handover configurations. In particular, they analyzed how humans responded to planned robot joint configurations during handover tasks. In their study they also allowed human subjects to provide positive and negative feedback to the robot on which configurations are more appropriate. This feedback was, then, used to improve the naturalness and legibility of the robot handover motion. Their experiments showed that planned handover motion can provide higher reachability of the object, while learned handover behavior can improve the naturalness and appropriateness of robot joint configurations. Koene et al. [7] showed that temporal precision plays an important role during object exchange. For fluent interaction, a robot needs to synchronize the timing of the arm movement with the arm movement of the human partner. While we acknowledge that fluency and naturalness are important characteristics for robot interactions with humans, this paper focuses on planning interactions that allow two robot agents to keep dexterous arm configurations during the handover, placing less priority to the factors mentioned above (although a high dexterity is usually correlated to smooth, humanlooking arm configurations). III. P RELIMINARIES In this section, we formally define the scenario and the shared manipulation task to be addressed through this paper. We also provide an intuitive overview of our solution approach, to be expanded in the following section. A. Problem definition Our shared manipulation task can be defined with the following elements: · A fully known 3D environment · A target object O to be manipulated. · Two agents: a fixed robot giver (Rg ) and a movable robot receiver (Rr ). In this and the following section, we will use as an example our butler scenario, whose problem definition is shown in Figure 2. B. Task definition The shared manipulation task consists in transporting the target object O from its start pose Ts to a goal pose Tg . Furthermore, we assume Ts to be only reachable by Rg and Tg reachable by Rr , hence the object must be picked up by Rg , handed off to Rr and finally placed in its final pose Tg by Rr . An example of Ts and Tg for our butler test scenario is shown in Figure 3.
432

Known scenario

Object (O)

Giver (Rg )

Receiver (Rr )

Fig. 2: Problem definition for butler scenario

Start pose Ts

Goal pose Tg

Fig. 3: Task definition for butler scenario C. Algorithm Overview At high level, our algorithm can be summarized as follows: 1) Decide which arm Rg will use to grasp the object. 2) Calculate a set of feasible grasps for Rr (Gr ) that allows it to place O at Tg 3) Calculate a set of feasible grasps for Rg (Gg ) that allows it to reach O at its start pose Ts 4) Calculate a set of feasible handover solutions H using the information provided by the set of feasible grasps Gg and Gr and the pose of the two robotic agents Rg and Rr . 5) Select the handover solution from H that maximizes the dexterity of the arms in the shared manipulation task and plan the corresponding arm trajectories. In section IV we will expand and explain each point above in more detail. Any solution from H should have information such as Figure 4: The handover pose Th and the grasps to be executed by Rg and Rr . IV. A LGORITHM In this section we will go in detail over the points briefly addressed in III-C: A. Arm selection for Rg Our planner must first determine if O is located within the reach of Rg . For this, the reachability space [6] of Rg (Rg ) is generated offline. We fill Rg by sampling a large number of random joint configurations for each arm of Rg and storing the average manipulability [11] of each sample in the voxel corresponding to the Tool Center Point (TCP) position of the end effectors (an entry of 0 means that the

Initial setup

Reach O at Ts Grasp 0/28 Grasp 11/28

Handover

Place O at Tg

Fig. 4: Expected solution for butler scenario arm cannot reach that location) . Since Rg is a (fixed) dual manipulator, RG will provide us with a manipulability metric for both left and right arms at each voxel. The object O is considered within reach of Rg if the voxel corresponding to Ts in RG has a non-zero entry for either the left arm, right arm or both. If both entries are non-zero, then the arm selected will be the one with the highest entry value, since this suggests that the corresponding arm will have more dexterity. If only one entry is non-zero, then the corresponding arm is selected. If both entries are zero then the task is considered infeasible and no further action is required. B. Generating Grasp candidates for Rr (Gr ) The next step consists on calculating a set of grasps and arm configurations that allow Rr to place O at its goal location Tg . For this step we use a set of precomputed grasps for O. After placing the object O at Tg , we test each of the grasps in Gr and prune the candidates that either not kinematically feasible or that present collision with the environment. This procedure can be seen in Algorithm 1. For our butler scenario, some of the non-pruned grasps ( Gr ) are depicted in Figure 5. Algorithm 1: PruneGrasps(R , O, G ) Input: Robot R , Object O and its corresponding grasp set G Output: A feasible subset of G (G  ) and its arm configurations (Q )
1 2 3 4 5 6

Grasp 17/28

Grasp 26/28

Fig. 5: Candidate grasps at Tg executed by Rr of these feasible grasps for our butler scenario example can be seen in Figure 6.

Grasp 0/30

Grasp 3/30

Grasp 8/30

Grasp 18/30

Fig. 6: Candidate grasps at Ts executed by Rg D. Calculation of feasible handover poses There exists an infinite number of possible poses for the object O to be handed from Rg to Rr . To perform an exhaustive search in this space would demand significant computation time. Instead, we approach the problem by reducing the possible handover poses space by considering 2 simple observations from human grasping: · When transporting an object, humans tend to reduce orientation changes in the object being grasped, unless necessary. · Whenever possible, we keep our wrist in a resting pose while manipulating an object. Given the points above, we generate a set of handover tuples (stored in H ) using Algorithm 2. Each tuple contains a handover pose for the object O, the grasps for Rr and Rg with an arm configuration that allows Rr and Rg to execute the grasp.
433

foreach g  G do q  R .executeGrasp(g , O) if checkCollision() is false then G  G  g Q  Q  q return G  , Q

C. Generating Grasp candidates for Rg (Gg ) Similar to the step above but now the object O is placed in Ts and the generated grasps are stored in Gg . An example

The specific procedure to calculate a handover grasp pose Th is shown in Algorithm 3. The translation component of Th is obtained from the set of near neighbours of the middle point between the shoulders of Rr and Rg . The voxel that can be reached from both arms and that has the highest average manipulability is chosen as the position for Th . Regarding the rotation, we take advantage of the information provided by the grasp set Gr : We calculate the pose by applying a minimum rotation between the Rr 's hand approach vector at Tg and the direction between the shoulders of Rr and Rg . A couple of example poses for our butler scenario can be seen in Figure 7. Grasp 4/29 Algorithm 2: Get set of candidate handover poses
1 2 3 4 5 6 7 8 9

Grasp 21/29

Fig. 7: Candidate handover poses during transfer phase and generate arm trajectories between them in order to reach, pick, handover and place the object. For this we use an standard IKBiRRT [3] approach. V. S IMULATION R ESULTS We set 3 simulation environments, as shown in Figure 1. All three scenarios involve 2 robots, which will be briefly described in the following subsection: A. Generalities 1) Crichton: Crichton is a bimanual fixed manipulator consisting on 2 Schunk LWA4 arms with a Schunk Dexterous Hand (SDH) attached to each last arm link. Each arm has 7 DOF, whereas the SDH possess 3 fingers and 8 DOF. A pseudo-analytical IK solver for the LWA4 arm was used for IK queries. 2) Hubo: Hubo is a humanoid robot with two 7-DOF arms. Each arm has a hand at their last link. The left hand has 3 fingers and 6 DOF whereas the right hand has 4 fingers and 8 DOF. A inverse-Jacobian IK solver for the Hubo arms was used for IK queries. 3) Simulation environment: For our simulation experiments we used DART [1]. The environment used, as well as the object and robot models, are replicas of their real hardware counterparts in order to make the transition to hardware experiments easier. 4) Grasp generation: The grasp datasets for each object were generated offline and loaded online for all the experiments shown. The numbers of grasps generated per each object are of a few hundreds, as it can be seen in Table I, hence, the selection of a handover pose and the corresponding grasps to use per each robot is not trivial. Since the grasp set generation is independent of the planning algorithm presented here, any method to generate grasps can be used. In particular, we used the common approach of evenly sampling the object's surface and set the hand's palm just above each sample, with the approach direction parallel to the normal of the object at each of these points. Other methods could be used to generate the set of precomputed grasps, such as GraspIt! [10] or other similar open source tools. Finally, all experiments were ran in an Intel Core i7 machine (1.6GHz).
434

foreach gr  Gr do Th , qr  GetHandoverPose(Rg , Rr ,O,gr ,Tg ) if Th = NULL then O.set(Th ) Rr .set(qr , gr )  Gg  PruneGrasps(Rg , Gg )  if Gr =  then  foreach gg  Gg do H  (Th ,gr , gg , qr , qg )

Algorithm 3: GetHandoverPose(Rg , Rr , O, gr , Tg ) Input: Robots, object, goal pose and grasp for Rr Output: Handover pose Th and arm conf. for Rr (qr )
1

zshoulder getVec(Rr .shoulder(), Rg .shoulder())
/* Get translation for Th */

2 3 4 5 6 7 8 9 10 11 12 13 14 15

p  Rr .shoulder() + 0.5 · zshoulder Sr  GetClosestVoxels(Rr , p) Sg  GetClosestVoxels(Rg , p) S  Sr  Sg
/* Sort according to manipulability */

S   Sort(S ) Th .trans()  S  .front()
/* Get rotation for Th */

O.setPose(Tg ) qr  Rr .executeGrasp(gr ,O) Thand  Rr .getHandPose() rot  minRot(Thand .z(), zshoulder ) Th .rotation()  rot × Thand × gr .Tho () O.setPose(Th ) qr  Rr .executeGrasp(gr , O) return Th ,qr

E. Selection of dexterous handover pose and arm trajectory generation After the step described before, our planner has a set H of handover poses with the corresponding grasps for Rr and Rg . The only remaining task is to select one of these tuples

TABLE I: Number of grasps per object
Object Cup - Butler scenario Hammer - Stool scenario Food jar - Kneeling scenario SDH Hand 316 445 448 Hubo left hand 368 590 448 Hubo right hand 368 644 448

TABLE II: Results for 100 randomized butler scenarios
Parameter measured Average Planning time Successful plans Average number of grasps for Rg Average number of grasps for Rr Average number of handover poses Values 2.31 seconds 100/100 32.36 39.77 16.64

B. Experiment 1: Butler scenario Our first test problem is depicted in Figure 8: The target object O (a green cup) is initially resting - upside down - on a cart located to the left of the fixed manipulator (Crichton). The goal is to place O upright on the middle of the tray carried by Hubo's right hand. We performed 100 tests for randomized scenarios similar to the one depicted in Figure 8. The variables randomized were: · Hubo's translation along an axes parallel to the table's rim (total range of movement 1.6 m). · Hubo's rotation around its Z axis (pointing up) (total range: 30 degrees with respect to orientation directly facing Crichton). · Tray's height: The tray held by Hubo's right hand was randomly moved up and down in a total range of 6 cm. Some statistics (such as planning time and number of average handover poses generated) are shown in Table II and 2 handover poses for different randomized examples are shown in Figure 9

C. Experiment 2: Stool scenario Our second test scenario consists on Hubo standing on top of a 3-step stool while Crichton hands it a hammer from its start pose on the table in order for Hubo to lift it in front of it with its head pointing to the right (see Figure 10).

Ts : Hammer on table Tg : Hammer pointing up (+Y) Fig. 10: Start and goal configurations for stool test case In a similar way as the butler case, here we also performed 100 randomly varied runs. The varied parameters were the location of Hubo (first, second or third step on the stool) and the pose of the stool (some translation along an axis parallel to the table longest side and a small rotation around the up axis, no larger than 20 degrees). Results regarding number of possible handover solutions are shown in Table III and some solutions for the random cases are shown in Figure 11

Ts : Inverted cup on cart

Tg : Cup upright on tray

Fig. 8: Start and goal configurations for butler test case

Random test case 1 (first step)

Butler random case 1

Random test case 2 (third step) Fig. 11: Sample solutions for random stool test cases Butler random case 2 Fig. 9: Sample solutions for random butler test cases Notice that the planning time increases for this example, this probably due to the fact that the size of the grasp sets for the hammer object is bigger than for the cup, hence the search space increases in size.
435

TABLE III: Results for randomized stool scenarios
Parameter measured Average Planning time Successful plans Average number of grasps for Rg Average number of grasps for Rr Average number of handover poses Values 4.62 seconds 100/100 56.87 143.28 89.49

TABLE IV: Results for 100 randomized kneeling scenarios
Parameter measured Average Planning time Successful plans Average number of grasps for Rg Average number of grasps for Rr Average number of handover poses Values 3.1845 seconds 100/100 30.45 72.91 39.89

D. Experiment 3: Kneeling scenario Our final test scenario is depicted in Figure 12: Hubo is kneeling and needs to be handed an object from the table, in this example O is a dog food jar whose goal pose is above the dog's plate and slightly tilted (rotated with respect to the X axis going from table towards Hubo).

Ts : Dog jar on table

Tg : Dog jar in pouring pose

Fig. 12: Start and goal configurations for kneeling test case As in the cases mentioned before, the average results of running our algorithm 100 times on this test case are shown in Table IV. The randomized parameters for these cases were the goal location of the dog food jar (position change of no more than 3 cm and roll rotation changes of up to 20 degrees).

that maximizes the dexterity of Rr while performing the final object placing. Our approach reduces its computation time by considering only a subset of all the possible handover poses. For this, it takes advantage of the grasps calculated for Rr . By using them and the direction between the shoulders of both robots, our handover pose search space is greatly reduced. We presented 3 simulated test cases in which our approach works across diverse randomized situations. While our method shows early indications of efficiency, it should be noted that it is not complete; hence, the possibility exists that our algorithm might not find a handover pose even if one exists. As future work, we are transitioning our simulation setup to our real hardware in order to validate our preliminary simulation results with experimental tests. R EFERENCES
[1] Dynamic Animation and Robotics Toolkit. http://dartsim. github.io/dart/. Accessed: 2014-07-21. [2] H. Admoni, A. Dragan, S. Srinivasa, and B. Scassellati. Deliberate delays during robot-to-human handovers improve compliance with gaze communication. In Proceedings of the 2014 ACM/IEEE international conference on Human-robot interaction, pages 49­56. ACM, 2014. [3] D. Berenson, S. Srinivasa, D. Ferguson, A. Collet, and J. Kuffner. Manipulation planning with workspace goal regions. In Robotics and Automation, 2009. ICRA'09. IEEE International Conference on, pages 618­624. IEEE, 2009. [4] M. Cakmak, S. Srinivasa, M.K. Lee, J. Forlizzi, and S. Kiesler. Human preferences for robot-human hand-over configurations. In Intelligent Robots and Systems (IROS), 2011 IEEE/RSJ International Conference on, pages 1986­1993. IEEE, 2011. [5] A. Edsinger and C. Kemp. Human-robot interaction for cooperative manipulation: Handing objects to one another. In Robot and Human interactive Communication, 2007. RO-MAN 2007. The 16th IEEE International Symposium on, pages 1167­1172. IEEE, 2007. [6] D. Kee and W. Karwowski. Analytically derived three-dimensional reach volumes based on multijoint movements. Human factors: the journal of the human factors and ergonomics society, 44(4):530­544, 2002. [7] A. Koene and M. Remazeilles. Relative importance of spatial and temporal precision for user satisfaction in human-robot object handover interactions. [8] J. Mainprice, M. Gharbi, T. Sim´ eon, and R. Alami. Sharing effort in planning human-robot handover tasks. In RO-MAN, 2012 IEEE, pages 764­770. IEEE, 2012. [9] J. Mainprice, E. Sisbot, T. Sim´ eon, and R. Alami. Planning safe and legible hand-over motions for human-robot interaction. In IARP Workshop on Technical Challenges for Dependable Robots in Human Environments, volume 2, page 7, 2010. [10] A. Miller and P.K. Allen. Graspit! a versatile simulator for robotic grasping. Robotics & Automation Magazine, IEEE, 11(4):110­122, 2004. [11] Y. Nakamura and H. Hanafusa. Inverse kinematic solutions with singularity robustness for robot manipulator control. Journal of dynamic systems, measurement, and control, 108(3):163­171, 1986.

Kneeling random test case 1

Kneeling random test case 2 Fig. 13: Sample solutions for random kneeling test cases VI. D ISCUSSION AND C ONCLUSION In this paper we have presented a planner to solve shared manipulation tasks. Given an object that must be passed on from one robot Rg to another Rr , our planner calculates a set of possible handover solutions H and selects the one
436

2014 14th IEEE-RAS International Conference on Humanoid Robots (Humanoids) November 18-20, 2014. Madrid, Spain

Learning Interaction for Collaborative Tasks with Probabilistic Movement Primitives
Guilherme Maeda1 , Marco Ewerton1 , Rudolf Lioutikov1 , Heni Ben Amor2 , Jan Peters1,3 , Gerhard Neumann1

Abstract-- This paper proposes a probabilistic framework based on movement primitives for robots that work in collaboration with a human coworker. Since the human coworker can execute a variety of unforeseen tasks a requirement of our system is that the robot assistant must be able to adapt and learn new skills on-demand, without the need of an expert programmer. Thus, this paper leverages on the framework of imitation learning and its application to human-robot interaction using the concept of Interaction Primitives (IPs). We introduce the use of Probabilistic Movement Primitives (ProMPs) to devise an interaction method that both recognizes the action of a human and generates the appropriate movement primitive of the robot assistant. We evaluate our method on experiments using a lightweight arm interacting with a human partner and also using motion capture trajectories of two humans assembling a box. The advantages of ProMPs in relation to the original formulation for interaction are exposed and compared.

Fig. 1. Illustration of two collaborative tasks where a semi-autonomous robot helps a worker assembling a box. The robot must predict what is the action to execute, to hand over the screw driver or to hold the box. Its movement must also be coordinated relative to the location at which the human worker executes the task.

I. I NTRODUCTION While the traditional use of robots is to replace humans in dangerous and repetitive tasks we motivate this paper by semi-autonomous robots that assist humans. Semiautonomous robots have the ability to physically interact with the human in order to achieve a task in a collaborative manner. The assembly of products in factories, the aiding of the elderly at home, the control of actuated prosthetics, and the shared control in tele-operated repetitive processes are just a few examples of application. Only recently, physical human-robot interaction became possible due advances in robot design and safe, compliant control. As a consequence, algorithms for collaborative robots are still in the early stages of development. Assistance poses a variety of challenges related to the human presence. For example, Fig. 1 illustrates a robot assistant that helps a human to assemble a box. The robot must not only predict what is the most probable action to be executed based on the observations of the worker (to hand over a screw driver or to hold the box) but also the robot movement must be coordinated with the worker movement. Pre-programming a robot for all possible tasks that a worker may eventually need assistance with is unfeasible. A robot assistant must be able
1 Intelligent Autonomous Systems Lab, Technische Universitaet Darmstadt, 64289 Darmstadt Germany. Correspondence should be addressed to

maeda@ias.tu-darmstadt.de
2 Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, 801 Atlantic Drive, Atlanta, GA 30332-0280, USA. 3 Max Planck Institute for Intelligent Systems Spemannstr. 38, 72076 Tuebingen, Germany

to learn the interaction and to adapt to a variety of unforeseen tasks without the need of an expert programmer. Motivated by the described scenario, this work proposes the use of imitation learning [1] in the context of collaboration. Imitation learning has been widely used as a method to overcome the expensive programming of autonomous robots. Only recently, however, its application for physical interaction has been introduced under the concept of Interaction Primitives (IP) by Lee at al. in [2], defined as skills that allow robots to engage in collaborative activities with a human partner by Ben Amor et al. in [3]. Leveraging on the framework of [3], our approach is based on probabilistically modeling the interaction using a distribution of observed trajectories. We propose using Probabilistic Movement Primitives (ProMPs) [4] for modeling such a distribution. In a manufacturing scenario such a distribution of trajectories can be obtained by observing how two coworkers assemble a product, several times throughout the day, providing a rich data set for imitation learning. Such a collection of trajectories is used to create a prior model of the interaction in a lower dimensional weight space. The model is then used to recognize the intention of the observed agent and to generate the movement primitive of the unobserved agent given the same observations. The movement primitive of the unobserved agent can then be used to control a robot assistant. The main contribution of this paper is the introduction of the Probabilistic Movement Primitives [4] in the context of imitation learning for human-robot interaction and action
527

978-1-4799-7173-2/14/$31.00 ©2014 IEEE

recognition. We will show how Interaction ProMPs can be applied to address the three main problems previously illustrated in Fig. 1, that is: (a) learning a collaborative model by imitation learning and thus avoiding expert programming, (b) the ability to recognize a task by observing the worker, and (c) the coordination of the assistant movement in relation to the worker movement. We also show the advantages of ProMPs over the original DMP-based framework [3], and present an algorithm for aligning data using local optimization in order to avoid the issue of slope constraints typical of dynamic time warping. II. R ELATED W ORK The data-driven analysis and classification of interactions between multiple persons has long been addressed within the computer vision community. In particular visual surveillance tasks, e.g., tracking of multiple pedestrians, require methods for identifying the occurrence and type of person-to-person interactions. In a seminal paper, Oliver et al. [5] show that hidden Markov models (HMMs), and more generally graphical models, are suited for representing the mutual dependencies of the behaviors between interacting agents. Graphical models have gained popularity in the field of human-robot interaction as they naturally include temporal information into the inference process and the Bayesian semantics provides a simple way to encode prior knowledge. In [6], Lee et al. use a hierarchical HMM to learn and represent responsive robot behaviors. In their approach, a high-level HMM identifies the current state of the interaction and triggers low-level HMMs which correspond to the robot's motor primitives. In order to ensure that the robot adapts to the movement of the human partner, virtual springs are attached between markers on the human body and corresponding positions on the robot. In a similar vein, Ben Amor et al. [7] use a path-map HMM to model interactions in cooperative tasks. In their approach, a backbone of shared hidden states correlates the actions of the interacting agents. Tanaka et al. [8] use a Markov model to predict the positions of a worker in an assembly line. The space in which the worker moves is discretized into different regions. A Gaussian mixture model relates positions to procedures. Using this information a robot, then, delivers tools and parts to a human worker along the assembly line. Besides HMMs, other probabilistic graphical models have also been used to address interaction tasks. Koppula et al. [9] use a conditional random field with sub-activities, human poses, object affordances and object locations over time. Inference on the graphical model, allows a robot to anticipate human activity and choose a corresponding, preprogrammed robot response. Wang et al. [10] propose the intention-driven dynamics model, which models human intentions as latent states in graphical model. Intentions can be modeled as discrete variables, e.g., action labels, or continuous variables, e.g., an object's final position. The transitions between latent states and the mapping from latent states to observations are modeled via Gaussian Processes. As evidenced by these works, graphical models can be very powerful tools
528

in classifying interactions. However, this often requires a substantial set of training data. In particular for humanoid motion generation with many degrees-of-freedom, it is often challenging to acquire sufficiently large and general data sets. For more efficient learning and generalization, various authors investigated the projection of the original trajectories into a new, low-dimensional space where correlations between the agents are easier to unravel. Llorens et al. [11] show how such a low-dimensional interaction space can be used to implement an assistive robot arm. Similarly in [7], probabilistic principal component analysis is used to find a shared latent space. Dynamic Movement Primitives (DMPs) [12] allows for a low-dimensional, adaptive representation of a trajectory. The general idea is to encode a recorded trajectory as dynamical systems, which can be used to generate different variations of the original movement. In the context of interaction, Prada et al. [13] present a modified version of DMPs, that adapts the trajectory of one agent to a time-varying goal. By setting the goal to the wrist of another agent, the method can be used to generate handover motions. Although graphical models and HMMs have been successfully used for action and intention recognition in a discretized symbolic level, the generation of trajectories for the continuous dynamic control of the robot is usually addressed by a different level of representation (e.g. a lowerlevel HMM [6] or DMPs). In relation to the previously cited works, here, we propose a framework based solely on a continuous movement representation that is used to both recognize actions and generate trajectories in the form of movement primitives; mainly leveraging on DMP-based Interaction Primitives [3] and Probabilistic Movement Primitives (ProMPs) [4]. By using ProMPs rather than DMPs our prosed method naturally correlates different agents directly in the same space in which observations are made, since observations of a task are usually given by their trajectories. This is an advantage in relation to the original framework of [3] since the representation of collaboration in the space of accelerations/forces due to the use of DMPs obfuscates the algorithm and increases its sensitivity to noise in the observations. III. P ROPOSED M ETHOD This section briefly introduces Probabilistic Movement Primitives for a single degree of freedom as presented in [4] and proposes its extension for interaction and collaboration. Although not covered in this work, in its original proposition, the design of a feedback controller that tracks the distribution of trajectories is also part of ProMPs and the interested reader is referred to [4] for details; here we assume the existence of a human-safe standard feedback controller such as a low-gain PD controller. This section also exposes the main characteristics of the interaction framework based on DMPs in [3] and its relation to the approach of this paper. Finally, a simple local optimization algorithm is proposed for aligning several demonstrations provided by a human.

A. ProMPs for a Single DOF For the purposes of the following derivations we generically refer to each joint or Cartesian coordinates of a human or robot simply as a degree of freedom (DOF) with position q and velocity q . Starting with the case of a single DOF, we denote y (t) = [q (t) q (t)]T and a trajectory as a sequence  = {y (t)}t=0,...T . We adopt linear regression with n Gaussian basis functions  . The state vector y (t) can then be represented by a n-dimensional column vector of weights w as  (t) q (t) =  y (t) = w + y, (1) q (t)  (t)  (t)]T is a 2×n dimensional timewhere t = [ (t),  dependent basis matrix and y N (0, y ) is zero-mean i.i.d. Gaussian noise. The probability of observing the whole trajectory is then
T

where w ¯d is the augmented weight vector corresponding to the d-th demonstration, wp is the n-dimensional column vector of weights of the p-th DOF of the observed agent, and wq is the vector of weights of the q -th DOF of the controlled agent. The mean and covariance are then computed by stacking all demonstration weights µw = mean([w ¯1 , ..., w ¯d , , ..., w ¯D ]T ), w = Cov([w ¯1 , ..., w ¯d , , ..., w ¯D ]T ), (5)

where D is the number of demonstrations. Gaussian conditioning can then be applied on-line as each new observation is made using recursive updates in the form
-  T - µ+ w = µw + K (y (t) - Ht µw ) - T - + w = w - K (Ht w ) T  T + -1 K = - , w Ht (y + Ht w Ht )

(6)

p( |w) =
0

N (y (t)|t w, y ).

(2)

Similar to DMPs the speed of the execution of the movement is decoupled from the speed of the original trajectory by using a phase variable z (t). The phase variable replaces the time in order to control the location of the basis functions with  (z (t)). For simplicity we will use z (t) = t such that  (t) =  (z (t)) while remembering that any monotonically increasing function can be used [4]. Each trajectory is now represented by a low-dimensional vector w since the number of basis is usually much smaller than the number of time steps. Trajectory variations obtained by different demonstrations are captured by defining the distribution over the weights p(w| ), where  is the learning parameter. The probability of the trajectory becomes p( | ) = p( |w)p(w| )dw. (3)

where K is the Kalman gain matrix, y  (t) is the observed value at time t,  y is the measurement noise, and the upperscripts - and + the values before and after the update. The observation matrix Ht is block diagonal and each diagonal  (t)]T for each observed entry contains the 2×n basis [ (t),  joint   t . . . 0   . . .. . (7) Ht =  . . . .  0 ... t In the collaboration case only measurements of the observed agent are provided. By maintaining consistency with definition (4) where the entries of the observed agent come before the controlled agent, the mean is then µw = T [µo µc w w ] and the observation matrix Ht is partitioned as   (o 0 0 0 t )(1,1)    0 (o 0 0  t )(P,P )   Ht =   (8) 0 0 0 0c   (1,1)   0 0 0 0c (Q,Q) where each zero entry is of 2×n dimension. Note that if only positions of the observed agent are provided (o t )(p,p) = [ (t), 0(t)]T . In general, since (6) is a full state linear estimator, any partial combination of observations (for example when y  only contains positions, or velocities, or a mixture of both) provides the optimal estimate of states µ+ w and their uncertainty + . w C. Action Recognition for Primitive Activation Here we use the ProMP framework in a multi-task scenario where each task is one encoded by one interaction primitive. Consider a specific task s  {1, .., K } and assume that for each task an Interaction ProMP has been generated as it was proposed in section III-B. Using the recursive notation of (6), the upper script (·)- refers to the parameters of the Interaction ProMP updated up to the previous observation, - - that is s = {µ- w , w }s . The probability of the observation
529

So far  captures the correlation among the weights within the trajectory and between demonstrations of the same DOF. B. ProMPs for Collaboration The key aspect for the realization of the interaction primitives is the introduction of a parameter  that captures the correlation of all DOFs of multiple agents. Assuming that the distribution of trajectories of different agents is normal, then p(w;  ) = N (w|µw , w ). Under this assumption we redefine the vector of weights w to account for all degrees of freedom of multiple-agents. Following the definitions in [3] we will refer to the assisted human as the observed agent, and assume that he/she provides the observed DOFs of the model (e.g by motion capture). The robot will be referred to as the controlled agent. For an observed agent with P DOFs and a controlled agent with Q DOFs, we construct a row weight vector by concatenating the trajectory weights
T T T o T T T c w ¯d = {[w1 , ...wp , ..., wP ] , [w1 , ...wq , ..., wQ ] } (4)

- at a subsequent time t given the parameters s of one of the tasks is - p(y  (t); s )= - p(y  (t)|t w,  y )p(w |s )dw

E. Time Warping with Local Optimization One issue of imitation learning for trajectories is that multiple demonstrations provided by humans are usually, sometimes severely, warped in time. Demonstrations must be unwarped or time-aligned before the distribution of the weights can be computed. Here we propose aligning trajectories by taking one of the demonstrations as a reference yr and using local optimization of the time warping function with j +1 + g (v j )tj tj (15) = v0 w, w where tj w represents a vector containing the warped time of demonstration yw at the j -th iteration of the optimization. We propose g as a smooth, linear Gaussian-basis-model j j , ..., vP ] as the parameters to be with P weights v j = [v1 j is used to shift the time optimized. The extra parameter v0 which is useful when the reference and warped trajectories are, in fact, identical but start at different instants. The j = 0 and tj optimization is initialized with v0 w = tr for j j = 1. The parameters v are optimized with gradient descent to decrease the absolute cumulative distance between the reference and warped trajectories
K

(9)

-  = N (y  (t)|t µ- w , t w t + y ). (10)

The task s can now be recognized by applying Bayes rule p(s|y  (t)) =
- p(y  (t)|s )p(s) K k=1 - p(y  (t)|k )p(k )

,

(11)

where p(s) is the initial probability of the task (e.g. p(s) = 1/K for uniform distribution). We will evaluate Eqs. (9)-(11) using real collaboration data in the experimental section of this paper. D. Relation to Interaction DMPs It is now straightforward to relate our proposed method with the previous interaction primitives based on DMPs [3]. The principal difference is that in the framework of interaction DMPs the weights are mapped from the forcing function f (t) as opposed to the positions q (t). Using the linear basis-function model f (t) =  (t)T w, (12)

v = arg min
v k=0

j |yr (tr (k )) - yw (v0 + g (v j )tj w )|.

(16)

where  (t) are the normalized Gaussian basis functions. Similarly to the ProMP case a distribution of weights p(w) is learned based on several demonstrations of a task. For each DOF, the forcing function adds a nonlinear behavior on the movement which complements a linear and stable spring-damper system q ¨ = [y (y (g - q ) - q/  ) + f (t)] 2 , (13)

While Dynamic Time Warping (DTW) [14] is widely used for such problems, our local method forces alignment without "jumping" the indexes of the warped time vector which is an usual outcome of DTW and renders unrealistic and non-smooth trajectories. While this problem is usually minimized by imposing a slope constraint [14], the use of a smooth function g not only avoids the tunning of this parameter but also preserves the overall shape of the trajectory. IV. E XPERIMENTS This section presents results on a simple simulated scenario to compare the differences between the original work of Interaction DMPs with Interaction ProMPs. Next, we evaluate the accuracy of Interaction ProMPs for generating reference trajectories for an anthropomorphic robot arm conditioned on the movement of a human. Finally, we will show experimental results with Interaction ProMPs used in a collaborative scenario of a box assembly to both recognize and predict the action of two collaborators. A. Comparison with Interaction DMPs In a typical interaction task the observations of a coworker might arrive asynchronously, at irregular periods of time, for example, when the measurement signal is prone to interruption (a typical case is occlusion in motion capture systems). Fig. 2 (a) illustrates a simple case where both observed and controlled agents have a single joint each. The training data was created by sketching two sets of trajectories on a PC screen using a computer mouse. We than use these two sets as proxies of the observed and controlled agents resulting on the initial distribution of trajectories (in blue). The upper
530

where g is the goal attractor, y , y are user-defined parameters that characterize the spring-damper behavior and  controls the speed of execution. For details on DMPs please refer to [12] and references therein. When using imitation learning a demonstration is executed and measurements are usually given in the form of positions, which must be differentiated twice such that the forcing function can be computed f (t) = q ¨/ 2 - y (y (g - q ) - q/  ). (14)

Referring back to (6) the Gaussian conditioning is now based on the observation of forces or accelerations, that is y  (t) = f (¨ q , (·), t) . As our evaluations will show, the fact that forces are usually computed using second derivatives of the position can be restrictive for applications with asynchronous or sparse measurements as the observed accelerations needed for conditioning are hard to obtain in this case. In contrast, in the ProMP framework, it is possible to directly condition on the observed quantities, i.e., the position of the agent.

Initial distribution
Agent A (Observed) 0.8 X amplitude X amplitude 0.6 0.4 0.2 0 0.8 0.6 0.4 0.2 0

Current prediction
Agent A (Observed)

Training

-0.2 0 0.5

Sparse observations
1.5 2 2.5 Time (s) Agent B (Controlled) 1 3

-0.2 0 0.5 1

Noisy observation
1.5 2 2.5 Time (s) Agent B (Controlled) 3

Test

0.8 X amplitude X amplitude 0.6 0.4 0.2 0 0 0.5 1 1.5 2 Time (s) 2.5 3

0.8 0.6 0.4 0.2 0 0 0.5 1 1.5 2 Time (s) 2.5 3

(a) Z

-0.2

-0.2

(a)

(b)

Y
Fig. 2. Two scenarios where the Interaction ProMPs are advantageous over Interaction DMPs. (a) Sparse and asynchronous observations. (b) Noisy stream of observed data ( 2 = 0.04). The patches represent the ± 2 deviations from the mean.
Interaction DMP 0.16 0.14 0.12 RMS prediction error 0.1 0.08 0.06 0.04 0.02 0 RMS prediction error 0.16 0.14 0.12 0.1 0.08 0.06 0.04 0.02 0 Interaction ProMP var:0 var:0.01 var:0.02 var:0.03 var:0.04

(b)
Fig. 4. An interactive task where the robot has to point at the same position previously pointed by the human. The robot, however, has no exteroceptive sensors and its predicted trajectory is based solely on the correlation with the observed human movement. (a) The nine positions used to create the Interaction ProMP (dot markers) and the extra nine positions used to test the method (cross markers). (b) An example where the human points at the test position #1 and the robot points to the same position.

20

40 60 80 100 Observed trajectory (%)

20

40 60 80 100 Observed trajectory (%)

Fig. 3. Root-mean-square prediction error of the movement of collaborator B as a function of the number of observed samples of the trajectory of collaborator A. The different bars indicate the amount of noise added to the observation of the position of collaborator A.

plot shows the prediction (in green) of the observed agent after observing four measurements. Note that following (4) the predicted mean µ+ w has the dimension of the augmented weight vector, that is, if each single-DOF agent trajectory is encoded by n basis functions µ+ w is a column vector of size 2n. The bottom figure depicts the predicted distribution of the controlled agent. Note that the same experiment can not be reproduced with Interaction DMPs as the second derivative on such sparse measurement is hard to compute and introduce innacuracies on the representation of the true force. In Fig. 2 (b) the ProMP is being conditioned on a constant synchronous stream of noisy position measurements. The plot shows the case where the true trajectory is corrupted with a Gaussian noise with variance  2 = 0.04. Interaction DMPs suffer from noisy position measurements as the observation must be differentiated twice to compute the forcing function. While low-pass filters alleviate this problem, the introduction of phase lag is an issue that can be potentially
531

avoided with ProMPs. Fig. 3 compares the prediction error over the whole trajectory of Interaction DMPs and ProMPs given the same noisy observed data. With DMPs the error is greatly influenced by the amount of noise while ProMPs show much less sensitivity. For the case where the full trajectory of collaborator A is observed (indicated by the arrow) the prediction error increased by a factor of five times using the Interaction DMPs when noise ranged from a clean signal to a signal of noise variance 0.04. In contrast, the error deteriorates by a factor of two with Interaction ProMPs. B. Robot Control with Interaction ProMPs We evaluated the ability of Interaction ProMPs in generating the appropriate movement primitive for controlling a robot based on observations of a human partner. The experiment consisted in measuring the [x, y, z ] trajectory coordinates of the wrist of an observed agent via motion capture1 while pointing at a certain position on a table placed in front of our robot (a 7-DOF anthropomorphic arm with a 5-finger hand). Then, the robot was moved in gravity compensation mode to point with its index finger at the same position on the table while its joint positions were being recorded (kinesthetic teaching). This pair of
1 All human positions were measured in relation to the world reference frame located at the torso of the robot (as shown in Fig. 4(b)).

Initial distribution
1 0.95 0.9 x (m) y (m) 0.85 0.8 0.75 0.7 0.65 0 1 2 Time (s) 3 0.5 0.4 0.3

Prediction

Observed
-0.15 -0.2

Predicted robot trajectory
1 0.95 0.9 x (m) 0.85 0.8 0.75 0.7 0.65 0 1 2 Time (s)
0 -20 q4(deg) -40 -60 -80

Initial distribution
0.5 0.4 0.3 y (m)

Prediction

Observed
-0.15 -0.2 z (m) -0.25 -0.3 -0.35

Predicted robot trajectory

Human

z (m)

0.2 0.1 0 -0.1 -0.2 0 1 2 Time (s)
120 100 q3(deg) 80 60 40 2 4 Time (s) 6 20 0 2

-0.25 -0.3 -0.35

0.2 0.1 0 -0.1

3

-0.4

3

0

1

2 Time (s)

3

-0.2

0

1

2 Time (s)
120 56.99 100 56.985 q3(deg)

3

-0.4

0

1

2 Time (s)

3

60 40

80 70 q2(deg) 60 50 40 30 2 4 Time (s) 6 20 0

120 60 110 40 q5(deg) q1(deg) q6(deg) 100 20 90 0 -20 80 0 0

-22 80 -24 70 q2(deg) q7(deg) -26 60 -28 50 -30 40 2 2 4 4 6 6 Time Time (s) (s) -32 30 0 0 2 2 4 4 6 6 Time Time (s) (s)

50 0 -20 45 x(m) q4(deg) y(m) q5(deg) 2 2 4 4 6 6 Time Time (s) (s) -40 40 -60 35 -80 30 2 2 4 4 6 6 Time Time (s) (s) -100 25 0 0

Robot

q1(deg)

56.98 80 56.975 60 56.97 40 56.965 20 0 0

20 0 -20 0

-10 -15 -20 -25

4 Time (s)

6

-100 0

2

4 Time (s)

6

(a)

(b)

Fig. 5. The upper row shows the human Cartesian coordinates of the wrist. The bottom row shows the first four joints of the 7-DOF robotic arm. (a) Conditioned results of the test position #6. (b) Conditioned results of the test position #8.

trajectories formed by the Cartesian positions of the wrist and the joint positions of the arm where then mapped into the weight space and concatenated as in Eq. (4). In total, nine different positions were pointed to collect training data, sparsely covering an approximate circular area of diameter 30 cm. The pointed positions are shown in Fig. 4(a) by the dots. After creating the Interaction ProMPs, as described in Section III-B, we defined extra nine marked positions shown in Fig. 4(a) by the crosses. The human then pointed at one of the crosses while motion capture was used to measure the trajectory of the wrist. These observations were then used to condition the Interaction ProMP to predict trajectories for each joint of the robot, whose mean values where used as reference for a standard trajectory tracking inverse dynamics feedback controller with low gains. Fig. 4(b) shows one example of the interactive task where the human pointed to the position marked by the cross #1, which was not part of the training; the robot was capable of pointing to the same position. Note that the robot was not provided with any exteroceptive feedback, such as cameras, to reveal the location of the pointed position. Although the robot was not directly "aware" of the position of the pointed cross, the interaction primitive provides the robot the capability to predict what movement to make based solely on the observed trajectories of the partner. Figure 5 shows two examples on the conditioned interaction primitives when the human pointed at positions #6 in (a) and #8 in (b) (refer back to Fig. 4(a) for the physical position of the crosses). The first row in each subplot shows the [x, y, z ] coordinates of the wrist. The second row shows the first four joints of the robot, starting from the shoulder joint. Since we are only interested in the final pointing position, the interaction primitive was conditioned on the final measurements of the wrist position. As positions #6 and #8 were physically distant from each other, the difference between their predicted trajectories were quite large in relation to each other, roughly covering the whole span of the prior
532

XY pointing error (cm)

3 2.5 2 1.5 1 0.5 0 0 1 2 3 4 5 6 Test number (#) (Cross markers) 7 8 9 10

Fig. 6. Accuracy of the pointed positions by the robot when using the test positions given by the cross markers. The error was computed by taking the actual pointed position and the true position of the markers on the table.

distribution (in blue) for some certain DOFs of the arm. Figure 6 shows the distance error on the plane of the table between the position pointed by the robot and its true position. The robot was able to reasonably point even at locations at the limits of the training data such as position #1, #7, and #8 (see Fig. 4). The maximum error was of 3 cm, or 10% in relation to the total area covered by the training points (approximately a circle of diameter 30 cm). The experiments show that the physical movement of the robot is clearly conditioned by the position indicated by the human (see the accompanying video2 ). Note that this not a precision positioning experiment; the markers on the wrist were not fixed in a rigid, repeatable manner, neither the finger of the robot could be positioned with milimetric precision during the kinesthetic teaching phase. The framework of Interaction ProMPs allows, however, to seamlessly integrate additional sensing to increase accuracy in precision tasks. This is naturally achieved adjusting the observation vector y  in (6) and the zero entries in (8) to include new sensory information such as the reference position of a hole in which the robot must insert a peg. C. Action Recognition for Primitive Activation While in the previous experiments interaction primitives were evaluated for the case of a single task, here we show
2 also

available from http://youtu.be/2Ok6KQQQDNQ

Agent A Hand over

Agent B

Agent A

Agent B Screwing Start Start Holdiing box Reaching box

Agent A

Agent B

Start

Pick screw driver

Grasping plate

Start

Pick screw

Box flipped Start Start

(a) Hand over

(b) Fastening

(c) Plate insertion

Fig. 7. Three collaborative tasks involved when assembling a box by two co-workers. From left to right, the photos show the hand over of a screw, the fastening of the screw where one agent grasps the screw driver while the other holds the box steadily, and the insertion of a plate, which requires one agent to flip the box such that the slot becomes accessible to the other agent. The distribution of aligned demonstrations for each task are shown under their respective photos. The plot shows the covariance in the x-y plane at each corresponding z height.

how Interaction ProMPs can be used for recognizing the action of the observed agent and to select the appropriate desired movement primitive of the controlled agent. This capability allows the robot to maintain a library of several tasks encoded as Interaction ProMPs and to activate the appropriate primitive based on the observation of the current task. As shown in the photos of Fig. 7, we collected collaborative data in the form of the Cartesian coordinate positions of the wrists of two humans assembling a box. As in the previous experiment of section IV-B, all measurements were taken in relation to the torso of the robot. The collaborator on the right plays the role of the observed agent while the collaborator at the left plays the role of the controlled agent. The controlled agent will be referred to as the predicted agent since he/she can not be controlled. In the "hand-over" task shown in Fig. 7(a), the observed agent stretches his hand as a gesture to request a screw. The predicted agent then grasps a screw sitting on the table and hand it over to the collaborator. In the "fastening" task shown in Fig. 7(b), the observed agent grasps an electrical screwdriver. The predicted agent reacts by holding the box firmly while the observed agent fastens the screw. In the "plate insertion" task shown in Fig. 7(c), the observed agent grasps the bottom plate of the box. The predicted agent then flips the box such that the slots to which the plate slides in are directed towards the observed agent. Each task is repeated 40 times. All trajectories are aligned using the method described in Section III-E. The aligned trajectories are shown in Fig. 7 under their respective photos as three-dimensional plots for each of the tasks where the covariance in x-y directions are shown at the corresponding
533

Initial distribution
70 60 50 40 30 20 0 60 55 50 x (cm) y (cm)

Current prediction
10 0 -10 -20 -30 -40 0

Observed

40 35 30

2

4 Time (s)

6

25 0

2

4 Time (s)

6

z (cm)

45

2

4 Time (s)

6

(a) Interaction model: fastening task
80 70 60 x (cm) y (cm) 50 40 30 20 0 2 Time (s) 4 70 60 50 40 30 20 10 0 2 Time (s) 4 z (cm) 10 0 -10 -20 -30 -40 0

2 Time (s)

4

(b) Interaction model: hand over task

Fig. 8. Action recognition based on conditioning the movement primitives of the observed agent. In this example the observations of the fastening task also overlaps with the primitives of the hand over task.

heights (Z direction) of the movement. Interaction ProMPs are created for each task using the distribution of aligned trajectories. We evaluated action recognition using Eqs. (9)-(11) on the three presented tasks for box assembly. Fig. 8 shows one evaluation as an example. Note from the figure that the majority of observations indicate that the fastening task is taking place. The last five observations (surrounded by the ellipse), however, fits both tasks and could be a potential source of ambiguity in task recognition. Even in those

Likelihood of the task

0.8 0.6 0.4 0.2 0
Probability of the task 1 0.8 0.6

0.8 hand_over 0.6 fastening 0.4 plate_insertion 0.2

hand_over fastening plate_insertion

fastening plate_insertion

0 0 20 40 60 80 0.4 0 20 40 60 80 100 Number of observations 0.2 60 80Number 100 of observations 0 Number of observations
0 2 4

100

(a)

6 8 10 Number of observations

12

14

16

automatically generate different Interaction ProMPs without a priori hand labeling of multiple tasks. We are also investigating tasks in which some of the involved DOFs do not correlate linearly and also when certain tasks do not induce correlation. The later is especially true for tasks where the movement of the agents are not related by causality. VI. ACKNOWLEDGMENTS The research leading to these results has received funding from the European Community's Seventh Framework Programmes (FP7-ICT-2013-10) under grant agreement 610878 (3rdHand) and (FP7-ICT-2009-6) under grant agreement 270327 (ComPLACS). The authors would like to acknowledge Filipe Veiga, Tucker Hermans and Serena Ivaldi for their assistance during the preparation of this manuscript. R EFERENCES
[1] S. Schaal, "Is imitation learning the route to humanoid robots?" Trends in cognitive sciences, vol. 3, no. 6, pp. 233­242, 1999. [2] D. Lee, C. Ott, and Y. Nakamura, "Mimetic communication model with compliant physical contact in humanhumanoid interaction," The International Journal of Robotics Research, vol. 29, no. 13, pp. 1684­ 1704, 2010. [3] H. Ben Amor, G. Neumann, S. Kamthe, O. Kroemer, and J. Peters, "Interaction primitives for human-robot cooperation tasks," in Proceedings of 2014 IEEE International Conference on Robotics and Automation (ICRA), 2014. [4] A. Paraschos, C. Daniel, J. Peters, and G. Neumann, "Probabilistic movement primitives," in Advances in Neural Information Processing Systems (NIPS), 2013, pp. 2616­2624. [5] N. Oliver, B. Rosario, and A. Pentland, "A bayesian computer vision system for modeling human interactions," Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 831­843, Aug 2000. [6] D. Lee, C. Ott, and Y. Nakamura, "Mimetic communication model with compliant physical contact in human-humanoid interaction," Int. Journal of Robotics Research., vol. 29, no. 13, pp. 1684­1704, Nov. 2010. [7] H. Ben Amor, D. Vogt, M. Ewerton, E. Berger, B. Jung, and J. Peters, "Learning responsive robot behavior by imitation," in Proceedings of the 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2013, pp. 3257­3264. [8] Y. Tanaka, J. Kinugawa, Y. Sugahara, and K. Kosuge, "Motion planning with worker's trajectory prediction for assembly task partner robot," in Proceedings of the 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2012, pp. 1525­ 1532. [9] H. S. Koppula and A. Saxena, "Anticipating human activities using object affordances for reactive robotic response." in Robotics: Science and Systems, 2013. [10] Z. Wang, K. M¨ ulling, M. P. Deisenroth, H. B. Amor, D. Vogt, B. Sch¨ olkopf, and J. Peters, "Probabilistic movement modeling for intention inference in human­robot interaction," The International Journal of Robotics Research, vol. 32, no. 7, pp. 841­858, 2013. [11] B. Llorens-Bonilla and H. H. Asada, "A robot on the shoulder: Coordinated human-wearable robot control using coloured petri nets and partial least squares predictions," in Proceedings of the 2014 IEEE International Conference on Robotics and Automation, 2014. [12] A. J. Ijspeert, J. Nakanishi, H. Hoffmann, P. Pastor, and S. Schaal, "Dynamical movement primitives: learning attractor models for motor behaviors," Neural computation, vol. 25, no. 2, pp. 328­373, 2013. [13] M. Prada, A. Remazeilles, A. Koene, and S. Endo, "Dynamic movement primitives for human-robot interaction: comparison with human behavioral observation," in Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2013, pp. 1168­ 1175. [14] H. Sakoe and S. Chiba, "Dynamic programming algorithm optimization for spoken word recognition," Acoustics, Speech and Signal Processing, IEEE Transactions on, vol. 26, no. 1, pp. 43­49, 1978.

Probability of the task

1 0.8 0.6 0.4 0.2 0 0 2 4 6 8 10 Number of observations 12 14 16

(b)

Probability of the task

1 0.8 0.6 0.4 0.2 0 0 2 4 6 8 10 Number of observations 12 14 16

(c)

Fig. 9. Action recognition given three different Interaction ProMPs, one for each task involved in assembling the box. The three Interaction ProMPs are conditioned on the same observations of the observed agent. Probabilities of tasks are shown as a function of the number of observations along the trajectory of the observed agent. (a) Recognition of the hand over task. (b) Recognition of the fastening task. (c) Recognition of the plate insertion task.

cases, ProMPs can clearly distinguish among tasks as shown by the plots in Fig. 9 where the probabilities of the task are given as a function of the number of observations. Subplots (a), (b) and (c) show the task recognition for the fastening, hand over and plant insertion tasks, respectively. In general, we observed that 3-5 observations are required to achieve a 100% certainty for each task. (The last part of the accompanying video shows our method controlling the robot assistant to assembly a box with recognition of two different handover tasks). V. C ONCLUSION This paper introduced a method for collaboration suited for new applications using semi-autonomous robots whose movements must be coordinated with the movements of a human partner. By leveraging on the original framework of Interaction Primitives [3] we proposed the use of ProMPs for the realization of primitives that capture the correlation between trajectories of multiple agents. This work compared the main differences between DMPs and ProMPs for interaction and advocates the later for applications where measurements are noisy and/or prone to interruption. Using a 7-DOF lightweight arm we evaluated the capability of Interaction ProMPs in generating the appropriate primitive for controlling the robot in an interactive task involving a human partner. We also proposed a method for task recognition that naturally fits the ProMP framework. Our current work addresses the use of mixture-models to
534

International Conference on Humanoid Robots (HUMANOIDS), 2014

Online Multi-Camera Registration for Bimanual Workspace Trajectories
Neil T. Dantam Heni Ben Amor Henrik I. Christensen Mike Stilman

Abstract-- We demonstrate that millimeter-level bimanual manipulation accuracy can be achieved without the static camera registration typically required for visual servoing. We register multiple cameras online, converging in seconds, by visually tracking features on the robot hands and filtering the result. Then, we compute and track continuous-velocity relative workspace trajectories for the end-effector. We demonstrate the approach using Schunk LWA4 and SDH manipulators and Logitech C920 cameras, showing accurate relative positioning for pen-capping and object hand-off tasks. Our filtering software is available under a permissive license.1

I. I NTRODUCTION Visual feedback of hand movements provides rich information that can be used to correct for errors and improve manipulation accuracy. Recent evidence suggests that humans use visual feedback of the hand to guide reach and grasp tasks [15]. Continuously tracking and monitoring the state of the hand allows us to dynamically accommodate to internal and external perturbations, e.g., muscle impairments, thereby achieving a high degree of robustness during manipulation tasks. In robotics, using visual feedback depends on a kinematic registration between the camera and the manipulators. Typically, this is viewed as a static task: registration is computed offline and assumed to be constant. In reality, camera registration changes during operation due to external perturbations, wear and tear, or even human repositioning. For example, during the recent DARPA Robotics Challenge trials, impacts from falls resulted in camera issues which significantly affected the robot behavior [10]. The pose registration process should be treated as a dynamic task in which the involved parameters are continuously updated. Such an online approach to pose registration is challenging, since it requires the constant visibility of a calibration reference and sufficient accuracy to perform manipulation tasks. Bimanual manipulation requires accurate coordination of both end-effectors. To perform smooth and accurate bimanual manipulation, we propose an online estimation and control approach that combines (1) visual tracking of the manipulators, (2) co-estimation of poses for cameras and end-effectors using a special Euclidean group median and extended Kalman filter, and (3) continuous geometric interpolation on the special Euclidean group. Our key insight is to combine perception and control online, using the robot

Fig. 1. Bimanual Schunk LWA4/SDH capping a pen using visual feedback from online camera registration and end-effector tracking.

body frame as a reference. This work extends the singlecamera and manipulator registration presented in [5] to multicamera and multi-manipulator estimation, and it integrates the spherical blending approach of [6] to enable continuous motion of the manipulator in the workspace. II. R ELATED W ORK Typical camera registration methods collect a set of calibration data using an external reference object, compute the calibration, then proceed assuming the calibration is static. OpenCV determines camera registration from point correspondences, typically using a chessboard [12]. Pradeep, et. al, develop a camera and arm calibration approach based on bundle adjustment and demonstrate it on the PR2 robot [13]. This approach requires approximately 20 minutes to collect data and another 20 minutes for computation, a challenge for handling changing pose online. Visual servo control incorporates camera feedback into robot motion control [1], [2]. The two main types of visual servoing are image-based visual servo control (IBVS), which operates on features in the 2D image, and position-based visual servo control, which operates on 3D parameters. Both of these methods assume a given camera registration. While IBVS is locally stable with regard to pose errors, under PBVS, even small pose errors can result in large tracking error [1]. Compared to both IBVS and PVBS, our method requires no initial camera registration, instead estimating the registration online. Additionally, compared to IBVS, we estimate the full kinematics of the camera and robot,

The authors are with the Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA 30332, USA. ntd@gatech.edu, hbenamor@cc.gatech.edu,

hic@cc.gatech.edu
1 software

available at http://github.com/golems/reflex

and thus can directly follow workspace reference poses and trajectories, such as [6], rather than being limited to imagespace reference points and trajectories. Other recent work has explored online visual parameter identification. A single-camera and single-arm version of the approach in this paper was presented in [5]. [11] tracks a robot arm to identify encoder offsets. This method assumes a given camera registration, but is also tolerant to some registration error. In contrast, our work identifies the camera registration online. Though we do not explicitly consider encoder offsets, our method is empirically robust to offsets  of even 30 (see Sect. V). [9] considers bimanual arm and object tracking with vision and tactile feedback. Though the hardware and implementation differ from work presented in this paper, we obtain similar accuracy using inexpensive webcams and without tactile sensing. [16] uses maps generated from a Simultaneous Localization and Mapping (SLAM) algorithm to calibrate a depth sensor. In our approach, unlike typical environments for SLAM, the object to which we are trying to register our camera ­ the manipulator ­ will necessarily be in motion. III. E STIMATION : P OSES OF C AMERAS AND H ANDS We estimate poses of multiple cameras and manipulators by visually detecting the 3D pose of each manipulator. First, we detect texture features on the end-effector and fit a transform, providing an instantaneous estimate of camera and hand pose. To obtain sufficient accuracy for manipulation we then combine median and extended Kalman filtering of these poses. To use the robot body as a reference for camera registration, we track the 3D pose of features on the end-effector. These 3D poses can be estimated with marker-based [14] and model-based approaches [3]. Marker-based approaches require attaching fiducials to known locations on the robot, such as the fingers. Model-based tracking, on the other hand, requires accurate polygon meshes of the tracked object. In our implementation, we use the ALVAR library [14] for marker-based tracking. For computational reasons, we used the dual quaternion representation for the special Euclidean group SE (3). Compared to matrices, the dual quaternion has lower dimensionality and is more easily normalized, both advantages for our filtering implementation. Compared to the unit quaternion plus translation vector representation, dual quaternions are more convenient for algebraic manipulation because they are chained through multiplication. For Euclidean transformations, we use the conventional coordinate notation where the leading superscript denotes the parent frame and the following subscript denotes the child frame, i.e., aSb gives the origin of b relative to a. The transformation aSb followed by bSc is given as the dual quaternion multiplication aSb  b Sc = aSc . We represent an orientation quaternion as aqb , a translation vector as axb , a rotational velocity as ab , and the  b. combined translational and rotational velocity as a

A. Asynchronous Pose Co-Estimation Each camera image provides pose measurements for visible end-effector features. To reduce estimation latency, we process and filter the measurements from each camera asynchronously as they arrive rather than collecting images from all cameras at a fixed timestep. The kinematic chain through the manipulator, feature, and camera is defined as:
b

Swi  wiSwi  wiSfp = bScj  cjSfp

(1)

where bSwi is the encoder-measured pose of wrist i in body frame, wiSwi is the estimated offset pose of wrist i, wiSfp is the encoder-measured transform from wrist i to feature p on the hand, bScj is the estimated pose registration of camera j , and cjSfp is the visually-measured pose feature p in camera j . For a depiction of the setup see Fig. 3. Based on (1), we produce measurements for wrist offset wi Swi and camera registration bScj :
wiS wi cj

= (bSwi )-1  bScj  cjSfp  (wiSfp )-1 = Swi 
b wiS wi

(2) (3)

bS



wi

Sfp  ( Sfp )

cj

-1

where wiSwi is the wrist offset measurement from this image and feature, bScj is the camera registration measurement, b wiS wi is the currently estimated wrist offset, and Scj is the currently estimated camera pose. B. SE (3) Median and Extended Kalman Filter We apply median and extended Kalman filtering in the special Euclidean group SE (3) to the measurements for wrist offset wiSwi and camera registration bScj , similar to the approach in [5]. First, to reject outliers, we compute the median measurement over a sliding time window. Then, we use an extended Kalman Filter over time to compute optimal pose estimates under a Gaussian noise assumption. To compute the median of orientation q over the sliding window, the structure of rotations in SO(3) offers a convenient distance metric between two orientations: the angle between them. Using this geometric interpretation, the median orientation q is the orientation with minimum angular distance to all other orientations.
n

q = arg min
qi Q j =0

 | ln(qi  qj )|

(4)

The median translation v is the conventional geometric median, the translation with minimum Euclidean distance to all other translations:
n

v = arg min
vi  V j =0

| vi - vj |

(5)

In the extended Kalman filter, we consider state x composed of a quaternion q , a translation vector v , and the translational and rotational velocities, v  and  : x = (q , v ) = [qx , qy , qz , qw , vx , vy , vz , x , y , z , v x, v y , v z ]

er

er S

e

,

erS e

Workspace Control
e b

Se (t1 )

Relative Trajectory

Sw

= = =
wiS b b

e

Sw  w Sw

Sw bS  w

Ser  erSe  e Sw  e  e Sw Ser  erS

r Sr , S

J+

x r x - xr - kx  r ln (q  qr )

- k (J + J - I )

wiS

w , i

bS

cj wiS w i w i

 r (joint velocity) 
wiS w i bS cj

EKF

median

wiS

w i

...
0 bS

(bSwi )-1  bScj  cjSf  (wiSf )-1
b

ROBOT image Feature Detector

EKF

bS

cj

median

bS

cj

cj

...
0

Swi  wiSw  wiSf  (cjSf )-1
i cj

Sf0 . . . cjSfn

Fig. 2.

Block diagram of the control system. 3D feature poses

cj

Sfp are detected from visual data. Instantaneous wrist offsets

wiS wi

and camera

registrations bScj are computed. Then the median of these poses is taken over a sliding window and subsequently Kalman-filtered. The filtered poses are r. used to track a relative left-right workspace trajectory, and the Jacobian damped-least squares gives the reference joint velocities 

b b

Swr

Sc 0

wr S

wr

wr Ser

c0

Sf3

wr Sf

3

Frame Source: Encoders Visions Filter

Fig. 3.

Setup for a dual-arm and dual-camera system. The kinematic frames are shown for one of the arms and cameras.

The measurement z is the median pose from the sliding window: z = (q , v ) = [qx , qy , qz , qw , vx , vy , vz ] The general EKF prediction step for time k is: x ^k|k-1 = f (^ xk-1 ) f Fk - 1 = x x ^k-1|k-1
T Pk|k-1 = Fk-1 Pk-1|k-1 Fk -1 + Qk-1

Now, we find the process Jacobian F . The translation portion is a diagonal matrix of the translational velocity. For the orientation portion, we find the quaternion derivative q  from the rotational velocity: = q 1 q 2 (10)

(6) (7) (8)

This quaternion multiplication can be converted into the following matrix multiplication: 1 1   q = Mr (q )  2 2  qw qz -qz qw Mr (q ) =   qy -qx -qx -qy

where x ^ is the estimated state, f (x) is the process model, F is the Jacobian of f , P is the state covariance matrix, and Q is the process noise model. The process model integrates the translational and rotational velocity, staying in the SE (3) manifold using the dual quaternion exponential of the twist :   (, v,  v ) = , v ×  + v  f (x) = exp t   (q, v ) 2 (9)

 -qy qx   qw  -qz

(11)

Note that we omit the w column of the typical quaternion multiplication matrix because the w element of rotational velocity  is zero.

This gives the following process 13 × 13 Jacobian F :   1 I4×4 0 0 2 tMr q  0 I3×3 0 tI3×3   F = (12)  0 0 I3×3 0  0 0 0 I3×3 Now we consider the EKF correction step. The general form is: z ^k = h(^ xk|k-1 ) h Hk = x x ^ k |k - 1 yk = v (zk , z ^) Sk = Hk Pk|k-1 =
T Hk Pk|k-1 Hk T Sk Kk

IV. C ONTROL : C ONTINUOUS W ORKSPACE T RAJECTORIES To perform smooth, bimanual motion, we compute a relative workspace trajectory between the two manipulators, transform the relative pose and velocity of the trajectory to the body frame, then compute joint velocities using the Jacobian damped least squares pseudoinverse. We compute a relative trajectory for the two endeffectors using the spherical parabolic blends described in [6]. This provides a straight-line, constant-axis, and continuous-velocity workspace path for the end-effector by blending subsequent spherical linear interpolation segments. Given a list of relative left-right waypoint poses and times, er Se (t0 ), . . . , erSe (tn ), we compute the reference left-right  e (t). pose and velocity as a function of time: erSe (t), er er  el From the relative reference pose Se and velocity er between the left and right end-effectors, we control the left arm in workspace, by first converting the relative pose and velocity to the body frame, then computing the Jacobian damped-least-squares inverse kinematics solution. The left-arm wrist pose bSw follows directly from the kinematic chain through the right arm:
b e

(13) (14) (15) + Rk (16) (17) (18) (19)

x ^k|k = p(^ xk|k-1 , Kk yk ) Pk|k = (I - Kk Hk )Pk|k-1

where z is the measurement, h is the measurement model, H is the Jacobian of h, z ^ is the estimated measurement, R is the measurement noise model, and K is the Kalman gain, v is a function to compute measurement residual, and p is a function to compute the state update. We compute the EKF residuals and state updates using relative quaternions to remain in SE (3) without needing additional normalization. The observation h(x) is a pose estimate: h(x) = (q, v ) H = I7×7 (20) We compute the measurement residual based on the relative rotation between the measured and estimated pose: v (z, z ^) = (yq , yv )
 yq = ln zq  z ^q q

Sw = bSer  erSe  e Sw (25)

Sw = e Sw  w Sw

Next, we compute the body-frame feedforward reference  b . Since there is only one changing frame, erSe , velocity, a we could find the corresponding body frame motion by rotating the velocity. However, the typical computation is notationally cumbersome [4, p140].2 Instead, we find an elegant and more general solution by merely taking the derivative of the pose: Sw = bSer  erSe  e Sw 0 d er      b b ( Se  e Sw )  Sw =   Ser  (erSe  e Sw ) + bSer  dt B0 e  ¨ e  = bSe  erSe  e S  bSw w + rS e  Sw r ¨¨  = bSe  erS  e  e Sw  bSw r
0 b

yv = zv - z ^v

(21)

where yq is the orientation part of the residual and yv the  translation part. Note that ln zq  z ^q corresponds to a velocity in the direction of the relative transform between the actual and expected pose measurement and that we can consider yq as a quaternion derivative. Then, the update function will integrate the pose portion of y , again using the exponential of the twist. First, we find the twist corresponding to the product of the Kalman gain K and the measurement residual y : (Ky ) = (Ky )q  q  (Ky, v ) = ((Ky ) , v × (Ky ) + (Ky )v ) (22) Then, we integrate estimated pose using the exponential of this twist: t   (q, v ) (23) (x(q,v) )k|k = exp 2 Finally, the velocity component of innovation y is scaled and added: (x,v  )k|k = x,v  + (Ky ),v  (24)

(26)

U indicates that S cancels to zero, and we assume the  where  S e  ). right arm and left fingers are stationary (0 = bS er = S w Relative motion with both arms moving could be computed by including the nonzero derivative bS er in the computation. We can use the product rule for this derivation because dual quaternion poses are chained through multiplication. Using the quaternion plus translation vector representation,
2 The complexity of the velocity transformation notation in [4, p140] stems from its representation using Gibbs's vector calculus which decouples the quaternion multiplication into separate dot and cross products. Hamilton's and Study's classical quaternion and dual quaternion notation is simpler and more elegant for this kinematic computation. A similar computation is also possible using transformation matrices and their derivatives, but these matrices are more difficult to normalize than quaternions, increasing numerical error.

Fig. 4. Manipulation error using only encoders for position feedback. Without using visual feedback, there is a 15mm relative positioning error between the two end-effectors.

chaining is not a multiplication, so an equivalent derivation would be more complex. Velocity and the dual quaternion derivative are related as follows: dR(S ) 1 =   R( S ) dt 2 1 dR(S ) dD(S ) = x   R(S ) + x  dt 2 dt

Fig. 5. Testing relative positioning accuracy by aligning the end-effectors. Incorporating visual feedback and online registration reduces manipulation error from 15mm to  2mm. 1

(27)
quaternion, q

where R(S ) is the real part of S , D(S ) is the dual part of S ,  is rotational velocity, and x is translation. Finally, we compute reference joint velocities using the Jacobian damped least squares with a nullspace projection to keep joints near the zero position: r = J+  x r x - xr - kx  r ) ln (q  qr -k (J + J -I ) (28)

x y z w

0.5

0

-0.5

0

1

2

3

4 5 6 time (s)

7

8

9 10

where x is the actual translation, q is the actual orientation quaternion, xr is the reference translation, qr is the reference orientation quaternion,  is the actual rotational velocity, r is the reference rotational velocity, kx is the workspace position error gain, k is the null-space projection gain, and  is the configuration. We then use joint-level velocity control  r . A block diagram to track the reference joint velocities  depicting the components of the control system and their interplay can be found in Fig. 2. V. E XPERIMENTS We implement this approach on a pair of Schunk LWA4 manipulators with SDH end-effectors, and use a pair of Logitech C920 webcams to track the robot and objects. Our estimation and control software is implemented as a distributed system using the Ach real-time communication library [7]. The Schunk LWA4 has seven degrees of freedom and uses harmonic drives, which enable repeatable positioning precision of ±0.15mm [8]. However, absolute positioning accuracy is subject to encoder offset calibration and link rigidity. In practice, we achieve ±15mm accuracy when using only the joint encoders for feedback, as can be seen in Fig. 4. The Logitech C920 provides a resolution of 1920x1080 at 15 frames per second. To measure ground-truth distances, we use a ruler and meter-stick. To test the relative positioning accuracy of our implementation, we servo the end-effectors to a reference zero relative alignment, Fig. 5, and then measure the actual relative error between the two end-effectors. We conduct this test using

0.5

x y z

translation, x

0

-0.5

0

1

2

3

4 5 6 time (s)

7

8

9 10

Fig. 6. Relative trajectory of erSe between left and right end-effectors for pen-capping. The trajectory has constant acceleration, constant velocity, and constant deceleration segments.

only encoder feedback, then with visual feedback. We also  repeat the test injecting encoder error of 15 at the initial   shoulder joint, 30 at the shoulder, and 15 at both the shoulder and elbow. The results of this test are summarized in Table I. In addition, we use this method to perform the pen-capping task shown in Fig. 3 and the object hand off task shown in Fig. 7. The relative trajectory of erSe for the pen-capping task is plotted in Fig. 6 VI. D ISCUSSION The results of Sect. V show that this method achieves bimanual positioning accuracy of a few millimeters without  static camera registration and even with significant (30 ) error in the joint encoders.

TABLE I P OSITIONING T EST R ESULTS (mm) Mean encoder visual 16.5 2.2 155 2.8 280 1.3 240 0.95 Std. Dev. encoder visual 0.5 0.94 0.6 0.78 0 0.95 0 1.1

VII. C ONCLUSION We have presented an online method to identify multiple camera and manipulator poses and track continuous relative trajectories for bimanual manipulation tasks. This is useful for the typical case where camera registration is not static but changes due to model error, disturbances, or wear and tear. The key point is to track both manipulators, and follow a trajectory based on the visually estimated relative 3D pose between the end-effectors. By combining median and Kalman filtering, we are able to achieve millimeter-level manipulation accuracy. We have shown in our experiments that online registration can be used to improve positioning accuracy during bimanual manipulation tasks where successful operation depends on relative end-effector pose. This method uses feedback only from joint encoders and visual tracking of the robot hand. Further improvements could be made by including force and tactile sensing and by visually tracking in-hand objects. R EFERENCES
[1] Franc ¸ ois Chaumette and Seth Hutchinson. Visual servo control, part I: Basic approaches. Robotics and Automation Magazine, 13(4):82­90, 2006. [2] Franc ¸ ois Chaumette and Seth Hutchinson. Visual servo control, part II: Advanced approaches. Robotics and Automation Magazine, 14(1):109­118, 2007. [3] Changhyun Choi and Henrik I Christensen. Robust 3d visual tracking using particle filtering on the special euclidean group: A combined approach of keypoint and edge features. The International Journal of Robotics Research, 31(4):498­519, 2012. [4] J. Craig. Introduction to Robotics: Mechanics and Control. Pearson, 3rd edition, 2005. [5] N. Dantam, H. Ben Amor, H. Christensen, and M. Stilman. Online camera registration for robot manipulation (presented). In International Symposium on Experimental Robotics, 2014. [6] N. Dantam and M. Stilman. Spherical parabolic blends for robot workspace trajectories (accepted). In International Conference on Intelligent Robots and Systems, 2014. [7] Neil Dantam, Daniel Lofaro, Ayonga Hereid, Paul Oh, Aaron Ames, and Mike Stilman. Multiprocess communication and control software for humanoid robots (accepted). Robotics and Automation Magazine, 2014. [8] Schunk GmbH. Dextrous lightweight arm LWA 4D, technical data. http://mobile.schunk-microsite.com/en/produkte/ produkte/dextrous-lightweight-arm-lwa-4d.html. [9] Paul Hebert, Nicolas Hudson, Jeremy Ma, and Joel W Burdick. Dual arm estimation for coordinated bimanual manipulation. In Intl. Conf. on Robotics and Automation, pages 120­125. IEEE, 2013. [10] Sungmoon Joo and Michael Grey. DRC-Hubo retrospective, January 2014. Personal Communication. [11] Matthew Klingensmith, Thomas Galluzzo, Christopher Dellin, Moslem Kazemi, J. Andrew (Drew) Bagnell, and Nancy Pollard. Closed-loop servoing using real-time markerless arm tracking. In Intl. Conf. on Robotics and Automation (Humanoids Workshop), May 2013. [12] OpenCV API Reference. http://docs.opencv.org/master/ modules/refman.html. [13] Vijay Pradeep, Kurt Konolige, and Eric Berger. Calibrating a multiarm multi-sensor robot: A bundle adjustment approach. In Experimental Robotics, pages 211­225. Springer, 2014. [14] Kari Rainio and Alain Boyer. ALVAR ­ A Library for Virtual and Augmented Reality User's Manual. VTT Augmented Reality Team, December 2013. [15] Jeffrey A. Saunders and David C. Knill. Humans use continuous visual feedback from the hand to control both the direction and distance of pointing movements. Experimental Brain Research, 162(4):458­473, 2005. [16] A. Teichman, S. Miller, and S. Thrun. Unsupervised intrinsic calibration of depth sensors via slam. In Robotics: Science and Systems (RSS), 2013.

No Offset  shoulder: 15  shoulder: 30  shoulder & elbow: 15

Fig. 7.

An object hand-off task.

There are a variety of error sources that we address in this system. For the kinematics, error from encoder offsets in the arm, imprecise link lengths, and flexing of links all contribute inaccurate kinematic pose estimates. For perception, error from inaccurate camera intrinsics, imprecise fiducial sizes, offsets in object models, and noise in the image all contribute to error in visual pose estimates. To achieve accurate manipulation, we must account for these potential sources of error. The position of the tracked features on the robot has an important effect on error correction. Kinematic errors between the robot body origin and the tracked features, e.g., due to flex or encoder offsets, are incorporated into the camera registration and handled through the servo loop. Error between the observed features and the end-effector cannot be corrected. Thus, it is better to track features as close to the end-effector as possible. Consequently, we placed the fiducial markers on the fingers of the SDH end-effector. One source of error for manipulation that we do not address is error in grasping. Because we track only the robot hand, any error in the relative pose between the hand and grasped object is not corrected. In reality, when grasping an object, the object itself becomes the robot's end-effector. Thus, to accurately manipulate in-hand objects, it would be better to track the objects themselves. Since a grasped object is likely to be partially occluded, model-based tracking such as [3], which is robust to occlusions, is a potential approach. A crucial additional consideration in manipulation is force and tactile sensing. Using visual feedback without force and tactile sensing already reduces the error to a few millimeters and allows the robot to perform tasks such as pen capping and object hand-off. However, considering the generated contact forces during the manipulation would further improve performance and allow even more accurate operation, in particular during the post-contact phase. This is a key area for improvement in this approach.

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/268521702

TransferEntropyforFeatureExtractionin PhysicalHuman-RobotInteraction:Detecting Perturbationsfrom...
ConferencePaper·October2014
DOI:10.1109/HUMANOIDS.2014.7041459

CITATION

READS

1
5authors,including: ErikBerger TechnischeUniversitätBergakademieFreiberg
18PUBLICATIONS52CITATIONS
SEEPROFILE

90

DavidVogt TechnischeUniversitätBergakademieFreiberg
20PUBLICATIONS86CITATIONS
SEEPROFILE

HeniBenAmor ArizonaStateUniversity
56PUBLICATIONS434CITATIONS
SEEPROFILE

Someoftheauthorsofthispublicationarealsoworkingontheserelatedprojects:

Mining-RoX:AutonomousRobotsinUndergroundMiningViewproject

AllcontentfollowingthispagewasuploadedbyHeniBenAmoron21November2014.

Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Transfer Entropy for Feature Extraction in Physical Human-Robot Interaction: Detecting Perturbations from Low-Cost Sensors
Erik Berger1 , David M¨ uller1 , David Vogt1 , Bernhard Jung1 , Heni Ben Amor2
Abstract-- In physical human-robot interaction, robot behavior must be made robust against forces applied by the human interaction partner. For measuring such forces, special-purpose sensors may be used, e.g. force-torque sensors, that are however often heavy, expensive and prone to noise. In contrast, we propose a machine learning approach for measuring external perturbations of robot behavior that uses commonly available, low-cost sensors only. During the training phase, behaviorspecific statistical models of sensor measurements, so-called perturbation filters, are constructed using Principal Component Analysis, Transfer Entropy and Dynamic Mode Decomposition. During behavior execution, perturbation filters compare measured and predicted sensor values for estimating the amount and direction of forces applied by the human interaction partner. Such perturbation filters can therefore be regarded as virtual force sensors that produce continuous estimates of external forces.

I. I NTRODUCTION Autonomous robots require accurate sensing capabilities in order to act in an intelligent and meaningful way within their environment. In particular human-robot interaction tasks require sensors for measuring physical contact with a human partner. Recorded measurements can be used by a robot to ensure safety during interactions and to react to physical perturbations. To this end, it is important that both the occurrence as well the magnitude of an external perturbation, e.g., a push, are reliably detected. Existing sensing technologies, such as force-torque sensors, are often heavy, expensive, and noise-prone. However, there are numerous affordable lowcost sensors available which, while not directly measuring perturbation forces, can be used to generate estimates of external perturbations. In this paper, we present an approach for perturbation detection which is based on a combination of low-cost sensors and machine learning techniques. During a training phase, we extract a compact representation, called a perturbation filter, which specifies the evolution of sensor readings during regular execution of a motor skill. The extraction is guided by information-theoretic measures such as Transfer Entropy, that determine the relevance of a specific sensor w.r.t. the executed robot behavior. In contrast to our previous work [1], we will not use any higher level stability parameters, such as the center-of-mass, center-of-pressure, or zero-moment-point for learning. Instead, we will learn the perturbation filter from low-level sensor data, solely. As a result, no knowledge about the robot kinematics or dynamics is required.
1 Institute of Computer Science, Technical University Bergakademie Freiberg, Bernhard-von-Cotta-Str. 2, 09599 Freiberg, Germany 2 Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, 801 Atlantic Drive, Atlanta, GA 30332-0280, USA

Fig. 1: A NAO robot estimates the influence of external perturbations applied by a human interaction partner to its current behavior execution.

After a perturbation filter is learned, it is used to generate a continuous estimate of the amount of external human perturbations. During physical interaction between a robot and a human, the estimated perturbations can be used to compensate for the external forces or infer the intended guidance of a human interaction partner. The presented perturbation filter can be regarded as a virtual force sensor that produces a continuous estimate of external forces. II. R ELATED W ORK In recent years, natural and intuitive approaches to HRI have gained popularity. Various researchers have proposed the so-called soft robotics paradigm: compliant robots that "can cooperate in a safe manner with humans" [2]. An important robot control method for realizing such a compliance is impedance control [3]. Impedance control can be used to allow for touch based interaction and human guidance. To this end, impedance controllers require accurate sensing capabilities, in the form of force-torque sensors. However, such sensors are typically heavy, expensive and suffer from significant noise. Other sensors, such as torque sensors are

Data Acquisition Offline
SensorData Data Sensor
Training Data

Feature Extraction
PCA + Transfer Entropy Feature

Interpolation
Dynamic Mode Decomposition

Training Space Data

Data Model

Target

Online

DataData Sensor

Live

Feature Space Projection

Distance Measurement using DTW

Perturbation

Live Interaction

Target Estimation

Perturbation Estimation

Fig. 2: An overview of the presented machine learning approach. Training data, together with a labeling target vector will be processed using Principal Component Analysis, Transfer Entropy and Dynamic Mode Decomposition algorithms, providing a training data model of vectors comprising the Feature Space. During live interaction, the recorded data is being projected into this space and mapped to the nearest data model vector and its target vector using Dynamic Time Warping.

even more prone to issues related to noise and drift. Still, the ability to sense physical influences is at the core of recent advances made in the field of HRI. For example, Lee et al. [4] use impedance control and force-torque sensors in order to realize human-robot interaction during programming by demonstration tasks. Wang et al. [5] present a robot adapting its dancing steps based on the external forces exerted by a human dance partner. Ben Amor et al. [6] use touch information to teach new motor skills to a humanoid robot. Touch information is only used to collect data for subsequent learning of a robotic motor skill. Robot learning approaches based on such kinesthetic teach-in have gained considerable attention in the literature, with similar results reported in [7] and [8]. A different approach aiming at joint physical activities between humans and robots has been reported in [9]. Ikemoto et al. use Gaussian mixture models to adapt the timing of a humanoid robot to that of a human partner in close-contact interaction scenarios. This approach significantly improves physical interactions, but is limited to learning timing information. St¨ uckler et al. [10] present a cooperative transportation task where a robot follows the human guidance using arm compliance. In doing so, the robot recognizes the desired walking direction through visual observation of the object being transported. A similar setting has been investigated by Yokoyama et al. [11]. They use a HRP-2P humanoid robot with a biped locomotion controller and an aural human interface to carry a large panel together with a human. Forces measured with sensors on the wrists are utilized to derive the walking direction. The main disadvantage of the above approaches is that they require special aural and visual input devices or force

sensors, which are not present on many robot platforms. Additionally, none of the approaches using force-torque sensors addresses the problem of uncertainty in the measurements. As a result, all of these approaches assume high-quality sensing capabilities and low-speed execution of the joint motor task. We propose a new filtering algorithm that can learn the natural variation in sensor values as a motor skill is executed. III. A PPROACH The objective of the presented method is to estimate the strength and direction of external perturbations caused by a human interaction partner. To infer these estimates from low-cost sensor readings, we condition behavior-specific perturbation filters. An overview of the approach can be seen in Figure 2. First, we record training data for a behavior with different parameter configurations, e.g., varying step lengths during walking. In this data acquisition phase, no external perturbations from humans are applied. Thereafter, the training data is used to create a Feature Space data model during feature extraction. Linear combinations of different sensors are weighted by their relevance to the observed parameter and projected into the low-dimensional Feature Space. In the following, the configuration parameter will be referred to as the target vector. The relevance of a specific sensor to the target vector is extracted using Transfer Entropy [12] (TE). In this context, TE is used as a measure of predictability and information flow between the target vector and the conduct of sensors. Sensors that have a high TE w.r.t. the robot's behavior are deemed more influential and relevant. During behavior execution, an external perturbation is de-

tected by comparing the recorded training data to the current sensor data within the low-dimensional Feature Space. Dynamic Time Warping (DTW) [13] is used as a distance function in order to include the temporal pattern for the comparison. The estimation of a perturbation value is performed by comparing the current sensor readings to the sensor readings acquired during training. The perturbation value is then inferred from the difference between the currently configured behavior parameter, e.g., the currently employed step length, and the estimated behavior parameter which produced similar sensor readings during training. In the following section, we will depict each step of our approach in more detail. We will describe how to perform feature space extraction and how to use the resulting embedding to estimate a continuous perturbation value. A. Data Acquisition The first step in our approach is to record training data that reflects the evolution of sensor values during the regular execution of a motor skill. To this end, we perform the investigated motor skill with varying parameter values, e.g. varying step lengths during walking. For generalization purposes, it is important to record the motor skill under large a set of possible target parameter configurations. However, since the parameter space may have a dynamic range, this can lead to a time-consuming recording phase, which in consequence leads to wear and tear of the robot hardware. To avoid a lengthy training session, Dynamic Mode Decomposition can be used to learn a model of the sensor data using few training samples. This process is not being detailed in this publication, the interested reader is referred to our previous work [14]. The training data is sampled equidistantly with 100Hz . Please note, that we only record low-level sensor data. Preprocessed variables, such as center-of-mass or the zero moment point are not included in this process. In contrast to our previous work, we will automatically identify and combine relevant low-level sensor data. To prevent a comparison between sensors of different units (i.e. comparing angles with pressure values), a sensor group is assigned to each sensor, enabling to deduce conclusions from their individual relations. B. Feature Extraction The next step in our approach is to extract relevant features from the stream of sensor data. While it is often possible to acquire a large number of different sensor values, we are typically faced with significant redundancy and noise. Additionally, it is often unclear which of these readings we should pay attention to. Feature extraction can help to single out important parts of a sensor stream. For feature extraction, we first compute a low-dimensional embedding of the sensor data by performing a PCA-like procedure. We extract the principal components of the feature space using an eigenvector decomposition of the sensor data matrix. In traditional PCA the eigenvalues define how much information each eigenvector carries. The eigenvector with

highest eigenvalue is the direction with highest variance. Hence, in traditional PCA the relevance of a feature is defined by the observed variance along that dimension. Instead of using the variance to infer the relevance of a feature, we will focus on the relationship between the feature values and the future state of the robot. Features that have a strong statistical coherence with the future state of the robot are more likely to be relevant. In other words, a feature is deemed relevant, if its past activity is a good predictor of the robot's next state. From an information theoretic point of view, this type of relationship can be estimated using Transfer Entropy. [12] We employ TE in order to measure the directed information transfer between the target and each PC vector separately. TE is a recently introduced information-theoretic measure, which has been used extensively in diverse fields of science [15][16][17]. T EJ I = p(it+1 , it , jt )log2 p(it+1 |it , jt ) p(it+1 |it ) (1)

TE quantifies the incorrectness of the assumption, that in the absence of information flow from system J to system I , the state of J has no influence on the transition probabilities on system I [12]. To compute the TE, the conditional probability as well as joint probability of co-occurrences in J and I is required. To estimate these probabilities without resorting to density estimation we quantize the sensor values and use a frequentist approach. The optimal parameters for the quantization are empirically determined. We quantize each PC vector into 200 value levels and use a quantile based transformation, which has the advantages of stability and independence of input value transformations. Variations of the robot's target vector could possibly have a time-delayed impact on sensory data. However, the standard TE algorithm only allows to draw conclusions based on transitions of a one-step delay between samples of J and I . A more general approach can be implemented using Delayed Transfer Entropy [18] which introduces multiple possible time delays. p(it+1 |it , jt+1-d ) p(it+1 |it ) (2)  We calculate the Transfer Entropy peak values T ET Pi = argmaxj T ET P (dj ) between the target vector T and each principal component Pi over a number of time delays dj within a preset time delay window D, j dj = D. See Figure 3 for further details. These T E  peak values are then used to scale the previously acquired principal components, such that components of higher T E  values are stretched and those of lower T E  values are shrunk. The resulting scaled PCA space is the so-called Feature Space. As mentioned earlier, within the feature space, the relevance of variables depends on their influence on the target vector. The feature space projection of the acquired training data set is now stored as the feature space data model. T EJ I (d) = p(it+1 , it , jt+1-d )log2

Target PC1 ... PCn ...

0.8 ... 0.1

... ... ...

0.6 ... 0.3

Fig. 3: The Transfer Entropy measures the target vector's (red graph) influence on each PC vector for each time delay (black arrow for d=1, orange arrow for d=20). The TE peak value of each PC component (marked by red squares) is the largest of each delayed TE value.

4. PC

3. PC

2. PC

1. PC

d=1

TEJ I(d) ... d=20


PCA Space

Feature Space

0

5

10

15

20

25

Time [s]

C. Target Estimation The next step is to use this data model to continuously estimate current target behavior values and to detect and qualify possible perturbations upon live human-robot interaction. To do so, a time window of raw sensor data is projected into the previously specified low dimensional Feature Space. To identify the most similar segment of the projected training data, we use the subsequence dynamic time warping technique (SDTW) [19]. v = argmin (X, Y ). (3)

Fig. 4: The principle components (blue) and the principle components scaled with TE (red) of a walking gait's training data. The third principle component has the highest TE value and is stretched while the others are dampened. The scaled principle components comprise the low dimensional Feature Space.

robot should react to a perturbation depends on the specific use-case and is left open for further research at this point. IV. E XPERIMENTS In the following section, we evaluated our approach using a NAO robot from Aldebaran Robotics. To do so, we recorded a total of 52 seconds of training data from the robot's walking gait with step lengths between 3 cm and -3 cm. Each sample contains readings of the angle and pressure sensors. Next, we interpolated these samples with a resolution of 0.01 cm utilizing Dynamic Mode Decomposition. Retaining 95 % of information we applied Principal Component Analysis on the robots 24 angle sensors and its 8 foot pressure sensors separately resulting in a 4d-angle space and a 6d-pressure space. Finally, each principal component is scaled by its delayed Transfer Entropy whereas the target value equals the gaits step-length. The resulting dimensions of the low dimensional Feature Space are shown in Figure 4. While the first, second and fourth dimensions are dampened, the third dimension is stretched. This is due to the fact that PCA extracts the most characteristic properties of a behavior and not its dependency on the adjusted parameter. Figure 5 shows the vector length of P C · T E  , the resulting Feature Space vectors compared to the simultaneous longitudinal center-of-mass which was used extensively in our previous research [1] [14]. Obviously, they are very similar even though our new approach has no knowledge about the kinematic chain or the mass distribution of the robot. A. Estimation Quality We utilize the learned low dimensional Feature Space data model during runtime while the robot frequently reduces its step-length. Figure 6 shows the resulting mean absolute error (M AE ) for the angle and pressure sensor groups. The angles are especially influenced by spurious relationships, which are strongly dampened by PCA, even without TE.

For this, the SDTW algorithm  is measuring the distance between two temporal sequences X = (x1 , . . . , xN ) and Y = (y1 , . . . , yM ) of length N  N and M  N. In our specific case, the goal is to find the training data Y with minimal distance to the projection of the currently observed sequence X . As a result, the target behavior value v of the corresponding training data can be used as an estimation for the current one. Since we captured the behavior for a discrete set of target behavior values we can only make estimations for these. An efficient way to expand our data model to cover continuous behavior parameters can be achieved using interpolation schemes. Therefore, we use a novel interpolation method from fluid dynamics, the so called Dynamic Mode Decomposition (DMD) [20][21]. A detailed explanation of DMD and how it can be used for the interpolation of robot sensor data can be found in our previous publication [14]. D. Reaction Accordingly, we can generate an estimate for possible ^ by calculating the difference interfering external forces E between the configured behavior parameter P used to control ^ identified the robot and the estimated behavior parameter P by the learned model for each sensor group. ^=P ^-P E (4)

Thus, our approach can be used in scenarios where a robot has to detect and react to external perturbations in order to fulfill a specified task. Certain sensor groups will be suitable to qualify certain perturbations, allowing conclusions about perturbation characteristics. This and details about how the

|P C · T E  |
3

Prediction Configuration

Step Length [cm]
39 52

2.5

2

Longitudinal Center of Mass

1.5

1

0

13

26

Time [s]

0.5 0

5

10

15

Time [s]

Fig. 5: The vector length of P C · T E  compared to the simultaneous longitudinal center-of-mass. In contrast to the center-of-mass our approach does not need any knowledge about the robots kinematics or mass distribution to generate a comparable result.
¬PCA ¬TE ¬PCA TE PCA ¬TE

Fig. 7: The configured behavior parameter (red) decreases over time. The estimated behavior parameter (blue) recognized the robot's reactions with a time delay, since the robot needs to finish the current step before adapting to the new parameter.

3.5 3 2.5

PCA TE

C. Perturbation Estimation In this experiment, the human perturbs the robot during execution of a walking gait. To verify the correctness and universality of our approach, the perturbations are applied to different parts of the robot. Figure 8 shows the resulting parameter estimations for the angle and pressure groups as well as the configured parameter value. Perturbation (a) is recognized by both sensor groups, because the angles as well as the force sensors are affected. If no external perturbation is recognized during (b), the parameter estimations of both sensor groups measure almost the configured parameter value. However, in (c) the perturbation does not affect angles and in consequence can't be measured by the angle but by the pressure group. Finally, perturbation (d) leads to a measurement of flat zeros for each pressure sensor, which is not part of the training data set and, consequently, can only be recognized by the angle group. This confirms our assumption, that redundant sensor groups can help to recognize and qualify a variety of perturbations. V. C ONCLUSION In this paper, we presented an approach for estimating external perturbations during physical human-robot interaction tasks. Instead of using expensive force-torque sensors, we leverage available information from low-cost sensors. To this end, we introduced a machine learning approach that can learn behavior-specific perturbation filters in software. In turn, these filters can be used to generate a continuous estimate of the inflicted external perturbations. An informationtheoretic measure, in particular Transfer Entropy, is used to guide the feature extraction process. Given a set of low-level sensor data, our approach allows for the automatic identification of relevant sensor values by calculating the information flow from sensors to future robot states. We have shown, that this approach automatically leads to the emergence of features that are remarkably similar to the center-of-mass,

M AE

2

1.5 1 0.5 0

Angle Group

Pressure Group

Fig. 6: The mean absolute error prediction results in cm. Left: The angle group error with all permutations of PCA and TE. Right: The pressure group error with all permutations of PCA and TE. Obviously, using a combination of PCA and TE increases the accuracy of the estimation.

Furthermore, as shown by the pressure group, PCA fails to identify the relations between the sensors and the behavior parameter. However, our Feature Space, combining PCA and TE, increases the accuracy of the estimation for both sensor groups.

B. Parameter Estimation In this experiment, we measure the robot's hardware delay using the 52 seconds of angle training data without interpolation. For this, the step length is reduced from three to one centimeters over a period of 15 seconds with a sliding window of a 0.25 seconds. Figure 7 shows that the robot needs about one second to react to parameter reconfigurations. This indicates, that the robot has an average hardware delay of 0.75 seconds.

2.5

(a) Pulling

(b) No Perturbation

(c) Pushing

(d) Lifting

2

Step Length [cm]

1.5

1

0.5

0 0

Angle Estimation
5

Pressure Estimation
10 15

Configured Value
20

Time [s]

25

30

35

40

45

50

^ for each of the external perturbations is the difference between the estimated Fig. 8: The estimated perturbation value E ^ parameter P and the configured parameter value P , as defined in Formula 4. (a) can be detected by both the angle and the pressure sensors while the perturbations in (c) and (d) can only be detected by one of the sensor groups.

without actually having to provide prior knowledge about the robot kinematics or mass distributions. The automatic determination of these features is important, since manufacturersupplied mass distributions are effectively invalidated in tasks where the robot is carrying weights. Further characterization of causing effects and details about how to react to certain perturbations should be investigated in future work. Using spatial groupings of sensors on the robot could be used to localize the external influences. R EFERENCES
[1] E. Berger, D. Vogt, N. Haji-Ghassemi, B. Jung, and H. Ben Amor, "Inferring guidance information in cooperative human-robot tasks," in Humanoids'13, 2013. [2] A. Albu-Sch¨ affer, O. Eiberger, M. Fuchs, M. Grebenstein, S. Haddadin, C. Ott, A. Stemmer, T. Wimb¨ ock, S. Wolf, C. Borst, and G. Hirzinger, "Anthropomorphic soft robotics from torque control to variable intrinsic compliance," in Robotics Research, ser. Springer Tracts in Advanced Robotics, C. Pradalier, R. Siegwart, and G. Hirzinger, Eds. Springer Berlin Heidelberg, 2011, vol. 70, pp. 185­207. [3] S. Haddadin, Towards Safe Robots - Approaching Asimov's 1st Law. Springer, 2014. [4] D. Lee and C. Ott, "Incremental kinesthetic teaching of motion primitives using the motion refinement tube," Autonomous Robots, vol. 31, no. 2-3, pp. 115­131, 2011. [5] H. Wang and K. Kosuge, "Control of a robot dancer for enhancing haptic human-robot interaction in waltz," IEEE Trans. Haptics, vol. 5, no. 3, pp. 264­273, Jan. 2012. [6] H. Ben Amor, E. Berger, D. Vogt, and B. Jung, "Kinesthetic bootstrapping: Teaching motor skills to humanoid robots through physical interaction," in KI 2009: Advances in Artificial Intelligence. Springer Berlin Heidelberg, 2009, pp. 492­499. [7] S. Calinon, Robot programming by demonstration: A probabilistic approach. EPFL Press, 2009. [8] J. Kober and J. Peters, "Policy search for motor primitives in robotics," Machine Learning, vol. 84, no. 1, pp. 171­203, 2011.

[9] S. Ikemoto, H. Ben Amor, T. Minato, B. Jung, and H. Ishiguro, "Physical human-robot interaction: Mutual learning and adaptation," IEEE Robotics & Automation Magazine, vol. 19, no. 4, pp. 24­35, 2012. [10] J. St¨ uckler and S. Behnke, "Following human guidance to cooperatively carry a large object," in Humanoids'11, 2011, pp. 218­223. [11] K. Yokoyama, H. Handa, T. Isozumi, Y. Fukase, K. Kaneko, F. Kanehiro, Y. Kawai, F. Tomita, and H. Hirukawa, "Cooperative works by a human and a humanoid robot," in Proceedings. ICRA '03. IEEE International Conference on Robotics and Automation 2003., vol. 3, 2003, pp. 2985­2991 vol.3. [12] T. Schreiber, "Measuring information transfer," Physical Review Letters, vol. 85, no. 2, pp. 461­464, 2000. [13] H. Sakoe, "Dynamic programming algorithm optimization for spoken word recognition," IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 26, pp. 43­49, 1978. [14] E. Berger, M. Sastuba, D. Vogt, B. Jung, and H. Ben Amor, "Dynamic mode decomposition for perturbation estimation in human robot interaction," in RO-MAN, 2014 IEEE, to appear 2014. [15] S. Ito, M. E. Hansen, R. Heiland, A. Lumsdaine, A. M. Litke, and J. M. Beggs, "Extending transfer entropy improves identification of effective connectivity in a spiking cortical network model," PloS one, vol. 6, no. 11, p. e27431, 2011. [16] M. P. Machado, C. W. Kulp, and D. Schlingman, "Composition and analysis of music using mathematica," Mathematica in Education and Research, p. 1, 2007. [17] S. K. Baek, W.-S. Jung, O. Kwon, and H.-T. Moon, "Transfer entropy analysis of the stock market," arXiv preprint physics/0509014, 2005. [18] S. Ito, M. E. Hansen, R. Heiland, A. Lumsdaine, A. M. Litke, and J. M. Beggs, "Extending Transfer Entropy Improves Identification of Effective Connectivity in a Spiking Cortical Network Model," PLoS ONE, vol. 6, no. 11, pp. e27 431+, Nov. 2011. [19] M. M¨ uller, Information Retrieval for Music and Motion. Secaucus, NJ, USA: Springer-Verlag New York, Inc., 2007. [20] P. J. Schmid, "Dynamic mode decomposition of numerical and experimental data," Journal of Fluid Mechanics, vol. 656, pp. 5­28, 8 2010. [21] C. W. Rowley, I. Igor Mez, I. Shervin Bagher, R. Philipp Schlatte, and D. S. Henningson, "Spectral analysis of nonlinear flows," Journal of Fluid Mechanics, vol. 641, no. -1, pp. 115­127, 2009.

View publication stats

Latent Space Policy Search for Robotics
Kevin Sebastian Luck1 , Gerhard Neumann1 , Erik Berger2 , Jan Peters1,4 and Heni Ben Amor3
Abstract-- Learning motor skills for robots is a hard task. In particular, a high number of degrees-of-freedom in the robot can pose serious challenges to existing reinforcement learning methods, since it leads to a highdimensional search space. However, complex robots are often intrinsically redundant systems and, therefore, can be controlled using a latent manifold of much smaller dimensionality. In this paper, we present a novel policy search method that performs efficient reinforcement learning by uncovering the low-dimensional latent space of actuator redundancies. In contrast to previous attempts at combining reinforcement learning and dimensionality reduction, our approach does not perform dimensionality reduction as a preprocessing step but naturally combines it with policy search. Our evaluations show that the new approach outperforms existing algorithms for learning motor skills with high-dimensional robots.

I. INTRODUCTION Creating autonomous robots that can adapt to the current task by interacting with their environment is an important vision of artificial intelligence. In recent years, many successful applications of reinforcement learning (RL) to complex robot tasks have been reported, including autonomous helicopter flight [1], robot table-tennis [2], or quadruped locomotion [3]. One of the most successful methods for learning such motor tasks is policy search [4]. Policy search tries to directly uncover the parameters of a given policy representation that yield high rewards. In this paper we focus on policy search for robots with a high number of degrees-of-freedom (DOF). Typically, the number of parameters of our control policy heavily depends on the number of DOFs of the robot. Hence, we generally need a large number of evaluations to learn acceptable policies. However, evaluating hundred thousands of different policies on a real robot is often infeasible due to wear and tear, the required logistics, or space and time constraints. At the same time, many
1 Kevin S. Luck, Gerhard Neuman and Jan Peters are with the Department of Computer Science, Technische Universit¨ at Darmstadt, 64289 Darmstadt, Germany

Fig. 1: A NAO robot learns to lift up one leg and stay balanced using a novel latent space policy search method. The co-articulation of the joints, needed for successful execution of the motor skill, is represented in the low-dimensional latent space.

robot control tasks, such as motor skills, are highly redundant in the controlled DOFs. Typically, the intrinsic dimensionality of such movements is much smaller than the actual controlled number of DOFs. Hence, robot learning can be performed much more efficiently if we can determine the lower-dimensional latent space of the movement we want to learn. In this paper we present an efficient policy search algorithm for learning policies in low-dimensional latent spaces. The learning algorithm produces control signals for high-dimensional robot systems by estimating policies in a latent space with a significantly lower number of dimensions. The latent space encodes correlations between the controlled DOFs of the robot. The parameters of the policy as well as the projection parameters of the latent space are efficiently estimated from samples during the policy search iterations. The key insight to our algorithm is that policy search as well as dimensionality reduction can be integrated in an expectation-maximization (EM) framework. As a result,

{luck, geri, peters}@ias.tu-darmstadt.de
2 Erik Berger is with the Department of Mathematics and Computer Science, Technische Universit¨ at Bergakademie Freiberg, 09599 Freiberg, Germany

Erik.Berger@informatik.tu-freiberg.de
3 Heni Ben Amor is with the Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, GA 30332, USA

hbenamor@cc.gatech.edu
4 Jan Peters is with the Max Planck Institute for Intelligent Systems, 72076 T¨ ubingen, Germany

we can formulate a coherent algorithmic approach that naturally combines policy search and dimensionality reduction. In contrast to previous attempts for combining reinforcement learning and dimensionality reduction for robotic applications, our approach does not perform dimensionality reduction as a preprocessing step. Instead, the parameters of the latent space are adapted based on the reward signal from the environment. II. Related Work Policy search has attracted considerable attention in the robot learning community. An excellent overview of the topic and detailed descriptions of various state-of-the-art algorithms can be found in [5] and [4]. Previous combinations of dimensionality reduction and policy search, typically use a clear separation between the reinforcement learning algorithm and the dimensionality reduction step. In [6], data from a simulator was used in a preprocessing step to identify a possible low-dimensional latent space of policies using Reduced Rank Regression. Learning on the real robot was then restricted to the extracted latent space. Similarly, Bitzer et al. [7] used user-provided training data to learn a low-dimensional subspace using linear and non-linear dimensionality reduction for robot learning. Using dimensionality reduction as a preprocessing step, or as an independent process that can be executed after several iterations of reinforcement learning, may lead to serious limitations. First, extracting the latent space as a preprocess requires a significantly large training set of (approximate) solutions, prior simulations, or human demonstrations. Even if such data is available, it can be counterproductive to use it, since the reinforcement learning algorithm cannot change the parameters of the latent space in these approaches. For example, when using human demonstrations, e.g., recorded joint configurations, to identify the latent space, the extracted latent space might not be appropriate for controlling the robot as we neglect the correspondence problem [8], i.e., there is no one-to-one mapping of the human joints to the robot joints. Hence, we need to adapt the projection of the latent space during the reinforcement learning process. Using dimensionality reduction as an independent process also leads to a decreased learning efficiency, since it neglects reward information when identifying subspaces. III. Policy Search In the following section, we will introduce the general problem statement for reinforcement learning and dimensionality reduction and introduce the notation that will be used throughout the paper. For a more detailed description of theses topics, the reader is referred to [9] and [10].

A. Problem Statement Reinforcement learning methods can be used to autonomously learn robot control strategies through the interaction with an environment. Given the current state st  S a robot executes an action at  A, transitions into the state st+1 and receives a reward rt (st , at ). The action selection process is governed by the control policy (at |st , t), which is specified as conditional probability distribution over the actions given the current state st . Generally, RL algorithms try to determine an optimal policy which maximizes the expected reward. In this paper, we will focus on policy search methods. Policy search approaches typically use a parametrized stochastic policy represented by a  (at |st , t) with parameters . A typical representation of the policy in robotics is to use a Gaussian distribution as policy where the mean depends linearly on an observed feature vector  of the task, e.g., the location of an object to grasp. The goal of learning is to optimize the expected return of the policy with parameters  with J ( ) =
T

p () R () d,

(1)

where the expectation integrates over all possible trajectories  in the set T. Each trajectory  = [s1:T +1 , a1:T ] is specified by a sequence of length T of states and actions. The return R () of a trajectory is defined as the accumulated immediate rewards rt , i.e.,
T

R () =
t=1

rt (st , at ) + rT +1 ( sT +1 ),

(2)

where rT +1 denotes the final reward for reaching state sT +1 . Note that in many robot applications, the reward function and the policy are explicitly modelled to be time dependent. Due to the Markov property, the trajectory distribution p () can be written as
T

p () = p(s1 )
t=1

p (st+1 |st , at )  (at |st , t) .

(3)

Reinforcement learning algorithms try to determine policy parameters  that maximize Equation 1. B. Expectation Maximization Approaches to Policy Search In contrast to traditional approaches to reinforcement learning, EM-based methods formalize the policy search problem as inference problem with latent variables. They transform the rewards into an improper probability distribution such that the reward can be interpreted as (unnormalized) probability of a binary reward event. In our discussion, we will assume that the rewards have already been transformed to such a improper probability distribution, i.e., the rewards are non-negative. As in

the standard EM-algorithm, we can now optimize a lower bound, that is in this case a lower bound on the expected return, instead of optimizing the original objective. According to Kober and Peters [11], the lower bound of the expected return (1) is given by Lold () = pold () R () log p () d  (4)  T         = IE p ()  Q (st , at , t) log  (at |st , t)  , 
T
old

t=1

where Q is defined as the expected reward to come for time step t, when the robot is in state st and execute action at ,   T          Q (s, a, t) = IE     . (5)  rt~ (st~, at~) |st = s, at = a
~=t t



the exploration of the policy in a lower dimensional latent space. This low-dimensional exploration z is then projected in to the high-dimensional original space by a projection matrix. In order to infer such a model with latent variables, we can again use the expectation maximization algorithm. This time we infer a structured policy from the weighted data points. More specifically we use the marginalization rule [15] to introduce a hidden variable z to our policy by specifying that p (at |st , t) = Z p (at , z|st , t) dz. This step leads to a new lower bound given by   T         IE p ()  Q (st , at , t) log p (at , z|st , t) dz   
old

IE pold

     ()  

t=1 T

Z

t =1

 (7)    Q (st , at , t) IEq(z|at ,st ) log p (at , z|st , t)   ,

In practice, Q (s, a, t) is estimated by a single rollout,
i] [i] i.e, Q s[ t , at , t  T ~=t t i] rt[ ~ , where i denotes the index of



the episode. An important advantage of this approach is that the policy update is formulated as a weighted maximum likelihood (ML) estimate for the parameters , where the reward to come Q (s, a, t) is used as weight for the samples. Due to the weighted ML update, there is no need for a user-specified learning rate which is often a critical factor for achieving good performance in policy gradient algorithms [12]. The policy is typically modelled as linear policy with Gaussian noise. In the PoWER [11] algorithm, this Gaussian noise is added to the parameter vector of the policy, i.e., a = (M + E) . (6)

old where the distribution q(z|at , st ) = p (a |s ,t) is given old t t by the posterior of the latent variables given the old policy parameters old . In this lower bound, the EMalgorithm is applied twice. First, to derive the policy update by weighted maximum likelihood estimates. Second, we use EM to update the joint distribution p (at , z|st , t) instead of the marginal. While this lower bound can be used for any latent variable model, we will discuss our specific case of estimating projection parameters in more detail in the following section.

p

(at ,z|st ,t)

IV. The PePP C Er Algorithm In this section, we will describe the "Policy Search with Probabilistic Principle Component Exploration" Algorithm (PePP C Er) for policy search in low-dimensional latent spaces. We will first start with a short recap of Probabilistic PCA, explain the relevant probability distributions for the PePP C Er algorithm and derive the EM update equations. A. Revisiting Probabilistic PCA Probabilistic Principal Component Analysis (PPCA) is the probabilistic formulation of the PCA algorithm for performing linear dimensionality reduction. PPCA relates a d-dimensional data point x  Rd to a lowdimensional latent variable z  Rn through a linear Gaussian model x = Wz + µ + (8)

M is the mean of the policy and E denotes a Gaussian noise term that is either isotropic or anisotropically distributed. In our experiments, we will use the more commonly used isotropic version of the noise. In contrast to the standard formulation of PoWER [11], we use matrix-variate normal distributions [13] for the exploration noise E  Nd, p 0, 2 I , where 0 has d rows and p columns. We will use the notation Nd, p (·, ·) for such matrix-variate normal distributions and N (·, ·) for multi-variate normal distributions. In the remainder of this paper, we will write the stochastic policy  (at |st , t) as p (at |st , t) to ensure consistent notation. C. Using Structured Policies with Latent Variables Another important advantage of weighted ML updates, is that we can use structured policy representations that again include latent variables z. For example, mixture models [14] or low-dimensional factor models can be used. In our specific case, the latent variable defines

where the latent variable z  Rn is Gaussian distributed according to p (z) = N (0, I). The transformation matrix W  Rd×n maps each low-dimensional vector z to the high dimensional space. The matrix W spans a lowdimensional subspace and µ  Rd is the mean of

the high-dimensional distribution. A high dimensional isotropic noise  Rd with zero mean and 2 I variance is added to this projection. The parameters of this model are given by µ, 2 and W and can efficiently be estimated using an EM algorithm (see [10] for details). However, PPCA is a unsupervised learning method while policy search is supervised. B. Deriving the Update Equations for PePP C Er Building on the insights from PPCA, we can decompose a stochastic policy into a low-dimensional distribution and projection parameters for generating the required high-dimensional action. More specifically, we can write a = W Z  + M + E,
T

and p a|ZT  = N W ZT  + M, 2 tr T I , the posterior distribution can be written as pold ZT |a = N CWT (a - M) , C2 tr T , (15) -1 T 2 where C =  I + W W . Given this posterior distribution, we can now determine the equations of the expectation step IE p (ZT |a) ZT  = CWT (a - M) , old IE p (ZT |a) ZT  ZT  old
T

(14)

(16) (17)

= C2 tr T
T

(9)

where W is a projection matrix. The terms M and E are again the mean and the Gaussian noise term. The term ZT  with Z  N p,n (0, I) generates an exploration noise in a low-dimensional latent space, which is then projected into the high-dimensional space of actions via W. Due to the projection from the latent space to the original high dimensional state, the uncorrelated explorative action from the latent space becomes a correlated action in the high dimensional space. Hence, the projection matrix W can be understood as a matrix that defines synergies in the action space that are used for correlated exploration. Both, the mean M of the policy and the projection matrix W are learned by the policy search algorithm. Given the model in Equation 9, we can derive the expectation of our probability distribution p (a) in a straight-forward fashion IE [a] = IE W ZT  +M + IE E = M.
0 0

+IE p (ZT |a) ZT  IE p (ZT |a) ZT  . old old 1. Maximization Step for M We use a maximum likelihood estimate to identify the value of M in each iteration. To this end, we calculate the derivative of the log-likelihood function w.r.t. M,  ln p (a) = D-1 aT - MT , M (18)

where D = tr T WWT + 2 I = DT . After inserting this result into the EM policy search framework and set the derivative to zero, we get  T     ln p (at ) Q   t     0 = IE pold ()  (19)     M t =1  T   T -1           at T Q T Q       t  t              M = IE pold ()  IE ( )        p    old T  T  tr  tr  t=1 t=1 such that M maximizes the log-likelihood function ln p (a). 2. Maximization Step for W For optimizing W we have to use the new lower bound given in Equation 7 and set the derivative of this term w.r.t W to zero. Accordingly the derivative can be written as  ln p a, ZT  W -a ZT 
T

(10)

Similarly, we can also use the properties of matrixvariate normal distributions [13] to get the covariance cov (a) = IE (a - IE [a]) (a - IE [a])T = IE WZT T ZWT + IE ET ET = tr T WWT + 2 I , where tr (·) denotes the trace of a matrix. From Equation 10 and Equation 11 it follows that the prior distribution over actions is p (a) = N M, tr T WWT + 2 I . (12) (11)

= - 2 tr T + WZT  ZT 
T

-1 T

+ M ZT 

(20)

Now, in order to apply EM, we have to determine the posterior distribution p (Z|a) over matrices Z. The posterior distribution can be simplified by treating ZT  as a latent variable. Since the result of this product is a vector, we can use Bayes theorem for Gaussian variables [15, p.93] to derive the posterior distribution p ZT |a . Given both distributions p Z  = N 0, tr 
T T

from which follows that the optimal value of W that maximizes the log-likelihood is given by   T T   T (a - M) IE  t   pold (ZT |a) Z  Qt       W = IE pold ()      T   tr  t =1   -1 T        T IE p T ZT  ZT  Q     t  old (Z |at )                 IE .    pold ()      T         tr 
t =1

I

(13)

(21)

3. Maximization Step for 2 Similarly to the estimation of W, we can also derivate the log-likelihood of ln p a, ZT  with respect to 2 in order to identify a new estimate of 2 with  ln p a, ZT 
-1 d 2 2 T + 2  tr  2 22 T T a - WZ  - M a - WZT  - M . (22)

Input: Initialized parameters 2 0 , W0 and M0 and the dimensionality n of the low dimensional manifold. The function  (st , t) represents the feature vector for the policy. repeat

=-

Setting the above derivative to zero leads to the following maximum-likelihood estimate of the variance:  T  -1 1  2  tr T  = IE pold ()    d t=1 (at - M)T (at - M) -2 (at - M)T WIE p (ZT |at ) ZT  old +tr IE p (ZT |at ) ZT T Z WT W Q t old   T -1               Qt      IE pold ()     . (23)
t=1

Sampling: for h=1:H do # Sample the H rollouts for t=1:T do T ah t = Wi Z  + Mi  + E with Z  N p,n (0, I) and E  Nd, p 0, 2 iI Execute action ah t h Observe and store reward rt sh t , at Calculate weights: Q (s, a, t) = IE
T ~=t t

rt~ (st~, at~) |st = s, at = a

Expectation: foreach ah t do Compute IE p (ZT |ah ZT  with (16). t) old Compute IE p (ZT |ah ZT  ZT  t) old Maximization: Compute Mi+1 with (19). Compute Wi+1 with (21). Compute 2 i+1 with (23). until Mi  Mi+1 Output: Linear weights M for the feature vector .
T

with (17).

C. Complete Algorithm The resulting algorithm that implements all of the above steps can be found in Alg. 1. The initial values for the parameters 2 , W and M can either be randomly chosen or initialized using a PPCA on a set of demonstrations. Additionally, the algorithm requires the number of latent dimensions n as input. After convergence, a policy is given by a weight matrix M which is multiplied by the feature vector  (s, t) to receive an action for a given state and time. V. Experiments The PePP C Er Algorithm has been evaluated on a simulated and a real-world robot task. In this section, we will describe the experimental setup of these evaluations and present the achieved results in comparison to PoWER and the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) [16] algorithm. A. Learning Inverse Kinematics In our first experiment, we will focus on learning inverse kinematics. A range of methods exist for analytically or numerically solving the inverse kinematics problem. However, various researchers have also looked at inverse kinematics from a machine learning point of view [17]. In our experiment, we use a simulated robot with d hinge-joints and d + 1 segments. The goal of the simulated robot is to track the position of a sphere that is moving on a circle. Setting d to values higher than two results in a redundant system with more DOF

Algorithm 1: Policy Search with Probabilistic Principle Component Exploration in the Action Space (PePP C Er)

than required to accomplish the task. To learn inverse kinematics, we set the reward function to rt (st , at ) = e-D , (24)

where D is the distance of the end-effector to the target, when action at is executed. Then, we use PePP C Er to determine a suitable policy for the task. During the optimization process, PePP C Er uncovers the redundancies of the system by determining the low-dimensional latent space of joint angle configurations that lead to touching the target. The latent space models the co-articulation of different links. An example result of a learned policy can be found in Fig. 2. As can be seen in the figure, a 20 linked robot arm successfully tracks the target along a circular path. We ran the explained setup with different specifications of policy search algorithms resulting in the graph depicted in Fig. 3. The graph depicts the sum

60

50

Sum. Distances

40

30

Fig. 2: A tentacle-like robot with 20 links tracks a target along a circular path. of distances of the end-effector to the target positions. For a balance evaluation, we compared to two different implementations of the PoWER algorithm. In one implementation the 2 was static, while in the other implementation an automatic adaptation of a diagonal covariance matrix was performed. This feature was also implemented in the PePP C Er algorithm, which results in a slightly different update equation for 2 . In each iteration 30 samples were drawn and executed on a simulated 20-linked robot. As features we used 19 time-dependent Gaussians, so we had to estimate 380 parameters for 50 time steps. We repeated each experiment 10 times and calculated the mean (bold lines) and standard deviation of the results (light colors around the mean). The figure shows that PePP C Er outperforms CMA-ES and PoWER. In particular in the early iterations both policy search methods perform comparatively well. At the same time, we can see that both PoWER implementations start to stagnate at around 50 iterations. PePP C Er continues to reduce the distance to the targets.

20

10

0

1

2

3

4 5 6 7 8 9 Number of Latent Dimensions

10

11

12

Fig. 4: The mean sum of distances and the standard deviation between the 450th and 500th iteration for an 12-linked robot. Five solutions were learned by PePP C Er for different values of the dimensionality n of the latent space. latent space was set to n = 5. In order to evaluate the effect of this parameter on the results, we repeated the evaluation of PePP C Er with varying values for n in an inverse kinematics task for a 12-linked robot, as can be seen in Fig. 4. In the depicted graph, we can see a bump in the average distance at around 5 and 9 dimensions. This is an interesting phenomenon of latent space policy search: too small a value for n restricts the search space, too high a value for n diminishes the effect of dimensionality reduction. In our specific example, the best value for n seems to be 4 or 5. B. Learning to Stand on One Leg We also performed a learning task on a real robot. More specifically, we used PePP C Er to learn policies for standing on one leg. The task of standing on one leg is a synergistic motor skill that requires the co-articulation of different body parts for successful execution. It is often used in biomechanical studies on synergies and low-dimensional control in humans, such as in [18]. In our experiment, we set the episodic reward of the robot proportional to the height of the right leg after execution of the policy. Furthermore, we have to consider in our reward function, that the head and the right foot of the robot should not move a lot. Hence, the reward function can be written as R(h, rf , lf ) = exp { · h +  · rf -  · lf - MAX } , (25) where , ,   R+ , h is the height of the head, rf the height of the right foot and lf the height of the left foot in the final position. The constant MAX is the maximal possible value of the first part of the sum. The height of the head is responsible for a low reward if the robot falls

Fig. 3: Comparison between PePP C Er, PoWER and CMA-ES on the inverse kinematics task with a 20linked robot. In each iteration we executed 30 different joint configurations on the simulated robot. For the static PoWER we set  = 15. For the dynamic PoWER and PePP C Er we computed the diagonal covariance matrix. In the above experiment, the dimensionality of the

Acknowledgment The research leading to these results has received funding from the European Union under grant agreement #270327 (CompLACS). References
[1] A. Y. Ng, A. Coates, M. Diel, V. Ganapathi, J. Schulte, B. Tse, E. Berger, and E. Liang, "Autonomous inverted helicopter flight via reinforcement learning," in Proceedings of the International Symposium on Experimental Robotics, 2004, pp. 363­372. [2] K. Muelling, J. Kober, O. Kroemer, and J. Peters, "Learning to select and generalize striking movements in robot table tennis," International Journal of Robotics Research, no. 3, pp. 263­279, 2013. [3] J. Zico Kolter and A. Y. Ng, "The stanford littledog: A learning and rapid replanning approach to quadruped locomotion," Int. J. Rob. Res., vol. 30, no. 2, pp. 150­174, Feb. 2011. [4] J. Kober and J. Peters, "Reinforcement learning in robotics: a survey," in Reinforcement Learning. Springer Berlin Heidelberg, 2012, pp. 579­610. [5] M. P. Deisenroth, G. Neumann, and J. Peters, "A survey on policy search for robotics," Foundations and Trends in Robotics, vol. 2, no. 12, pp. 1­142, 2013. [6] J. Z. Kolter and A. Y. Ng, "Learning omnidirectional path following using dimensionality reduction," in in Proceedings of Robotics: Science and Systems, 2007. [7] S. Bitzer, M. Howard, and S. Vijayakumar, "Using dimensionality reduction to exploit constraints in reinforcement learning," in Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ International Conference on, Oct 2010, pp. 3219­3225. [8] C. L. Nehaniv and K. Dautenhahn, "Imitation in animals and artifacts," K. Dautenhahn and C. L. Nehaniv, Eds. Cambridge, MA, USA: MIT Press, 2002, ch. The Correspondence Problem, pp. 41­61. [9] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction. MIT Press, 1998. [10] M. E. Tipping and C. M. Bishop, "Probabilistic principal component analysis," Journal of the Royal Statistical Society, Series B, vol. 61, pp. 611­622, 1999. [11] J. Kober and J. Peters, "Policy search for motor primitives in robotics," Machine Learning, vol. 84, no. 1-2, pp. 171­203, 2011. [12] J. Peters and S. Schaal, "Reinforcement learning of motor skills with policy gradients," Neural networks, vol. 21, no. 4, pp. 682­ 697, 2008. [13] A. K. Gupta and D. K. Nagar, Matrix variate distributions. CRC Press, 2000, vol. 104. [14] C. Daniel, G. Neumann, and J. Peters, "Learning concurrent motor skills in versatile solution spaces," in Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, Oct 2012, pp. 3591­3597. [15] C. M. Bishop and N. M. Nasrabadi, Pattern recognition and machine learning. Springer New York, 2006, vol. 1. [16] N. Hansen, S. Muller, and P. Koumoutsakos, "Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES)." Evolutionary Computation, vol. 11, no. 1, pp. 1­18, 2003. [17] K. Grochow, S. L. Martin, A. Hertzmann, and Z. Popovi´ c, "Stylebased inverse kinematics," ACM Trans. Graph., vol. 23, no. 3, pp. 522­531, Aug. 2004. [18] G. Torres-Oviedo and L. H. Ting, "Subject-specific muscle synergies in human balance control are consistent across different biomechanical contexts," Journal of neurophysiology, vol. 103, no. 6, pp. 3084­3098, 2010.
Policy 2 Policy 1

Fig. 5: Two different policies for standing on one leg learned using latent space policy search. Only 100 samples were needed to learn policy 1.

during learning. As features, time-dependent Gaussians were used in this experiment.Actions were represented by the change in the 25 robot joint angles between two consecutive time steps. The goal in robot learning is to learn from few trials. We therefore restricted the maximum number of samples (executions on the robot) to 600 samples. For automation and repeatability purposes, learning was performed in a physics-based simulator. However, we want to stress that, given the relatively small number of trials needed by PePP C Er to learn a policy, we can also perform learning directly on the real robot. Fig. 5 shows two learned policies acquired using PePP C Er. Learning started from random initializations and did not require any demonstrations. Policy 1 was learned using a sample size of 20 samples and 5 iterations, i.e., 100 execution on the robot in total. We can see, that it results in a smooth and stable motor skill. Policy 2 required 600 evaluations in total and allows the robot to lift the leg even higher. VI. CONCLUSIONS In this paper we presented a novel policy search algorithm for robotics applications. The PePP C Er algorithm determines the correlations between different joints of the robot and uses the information for fast and efficient reinforcement learning. The presented method combines policy search and dimensionality reduction in a natural way and has been derived from basic principles. Applications on a simulated and a real robot indicate that the approach can be employed to learn new motor skills for complex, redundant robots using a relatively small number of trials on the robot. In our future work we want to combine the introduced approach with imitation learning, in order to start in a good region of the search space. Additionally, we want to investigate methods for identifying the dimensionality of the current task.

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/263458522

OnlineCameraRegistrationforRobot Manipulation
ConferencePaper·June2014
DOI:10.1007/978-3-319-23778-7_13

CITATIONS

READS

2
4authors,including: NeilDantam RiceUniversity
23PUBLICATIONS101CITATIONS
SEEPROFILE

65

HeniBenAmor ArizonaStateUniversity
56PUBLICATIONS434CITATIONS
SEEPROFILE

HenrikIskovChristensen UniversityofCalifornia,SanDiego
478PUBLICATIONS6,419CITATIONS
SEEPROFILE

AllcontentfollowingthispagewasuploadedbyHeniBenAmoron27June2014.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Online Camera Registration for Robot Manipulation
Neil Dantam, Heni Ben Amor, Henrik Christensen, and Mike Stilman
Institute for Robotics and Intelligent Machines, Georgia Institute of Technology Atlanta, GA 30332, USA. ntd@gatech.edu, hbenamor@cc.gatech.edu, hic@cc.gatech.edu, mstilman@cc.gatech.edu

Abstract. We demonstrate that millimeter-level manipulation accuracy can be achieved without the static camera registration typically required for visual servoing. We register the camera online, converging in seconds, by visually tracking features on the robot and filtering the result. This online registration handles cases such as perturbed camera positions, wear and tear on camera mounts, and even a camera held by a human. We implement the approach on a Schunk LWA4 manipulator and Logitech C920 camera, servoing to target and pre-grasp configurations. Our filtering software is available under a permissive license.1

1

Introduction

Using visual feedback for robot manipulation requires registration between the camera and the manipulator. Typically, this is viewed as a static task: registration is computed offline and assumed to be constant. In reality, camera registration changes during operation due to external perturbations, wear and tear,

kinematics

kinematics

B

Sf

registration vision registration vision
C

Sf

B

SC

Camera Perturbation

Controlled Motion

Uncontrolled Motion

Fig. 1. Use cases for online camera registration. We combine the visual and kinematic pose estimates of end effector and filter the result to estimate the camera pose in robot body frame.
 1

This work supported by a grant from Peugeot S.A. software available at http://github.com/golems/reflex

2

Dantam, Ben Amor, Christensen, Stilman

or even human repositioning. For example, during the recent DARPA Robotics Challenge trials, impacts from falls resulted in camera issues which significantly affected the robot behavior for some teams [10]. Fig. 1 shows additional use cases which may change the camera pose. The pose registration process should be treated as a dynamic task in which the involved parameters are continuously updated. Such an online approach to pose registration is challenging, since it requires the constant visibility of a calibration reference and sufficient accuracy to perform manipulation tasks. To address changes in camera pose during operation, we propose an online camera registration method that combines (1) visual tracking of features on the manipulator, (2) a novel expectation-maximization inspired algorithm for pose filtering and tracking, and (3) an special Euclidean group constrained extended Kalman filter. Our key insight is to use the robot body as a reference for the registration process. By tracking known patterns or objects on the robot, we can continuously collect evidence for the current camera pose. However, na¨ ive filtering of these pose estimates can lead to large variances in the calculated poses. The challenge is obtaining sufficient accuracy for manipulation through the online registration. To address this challenge, we combine pose filtering and manipulator control, incorporating camera registration into our manipulation feedback loop. This paper presents a method for online registration and manipulation that combines object tracking, pose filtering, and visual servoing. First, we use perceptual information to identify the pose of specific features on the end-effector of the controlled robot (see Sect. 3.1). Then, we perform an initial fit to find offsets of the features on the robot, (see Sect. 3.2). A special Kalman filter is, then, used in conjunction with median filtering in order to perform online registration of the camera (Sect. 3.3). In our evaluation (see Sect. 4), we investigate the accuracy of the proposed method by applying it to robot grasping and manipulation tasks.

2

Related Work

Typical camera registration methods collect a set of calibration data using an external reference object, compute the calibration, then proceed assuming the calibration is static. OpenCV determines camera registration from point correspondes, typically using a chessboard [15]. Pradeep, et. al, develop a camera and arm calibration approach based on bundle adjustment and demonstrate it on the PR2 robot [16]. This approach requires approximately 20 minutes to collect data and another 20 minutes for computation, a challenge for handling changing pose online. Visual servo control incorporates camera feedback into robot motion control [2,3]. The two main types of visual servoing are image-based visual servo control (IBVS), which operates on features in the 2D image, and position-based visual servo control, which operates on 3D parameters. Both of these methods assume a given camera registration. While IBVS is locally stable with regard to pose

Online Camera Registration for Robot Manipulation

3

errors, under PBVS, even small pose errors can result in large tracking error [2]. Our proposed method addresses these challenges by correcting the camera registration online. In our experiments we show the importance of treating the registration process as a dynamic task. Furthermore, we show that our online registration achieves millimeter positioning accuracy of the manipulator. This is particularly important for grasping tasks performed using multi-fingered robot hands [1]. During such grasping tasks, inaccuracies in perception and forward kinematics often lead to premature contact between one finger and the object. As a result of the ensuing object movement, the intended grasp might not be satisfactorily executed or may fail altogether. Other recent work has explored online visual parameter identification. [11] tracks a robot arm to identify encoder offsets. This method assumes a given camera registration, but is also tolerant of some registration error. In contrast, our work identifies the camera registration online, but does not explicitly consider encoder offsets. [19] uses maps generated from a Simultaneous Localization and Mapping (SLAM) algorithm to calibrate a depth sensor. In our approach, unlike typical environments for SLAM, the object to which we are trying to register our camera ­ the manipulator ­ will necessarily be in motion.

3

Technical Approach

We determine the pose registration between the camera and the manipulator by visually tracking the 3D pose of the arm. We identify the pose of texture or shape features on the arm and fit a transformation based on the corresponding kinematic pose estimates of those features. To obtain sufficient accuracy for manipulation, we combine several methods to fit and filter the visual pose esti-

Position Servo
B

Workspace Control
-1

e,r = ln

B

Se,a 

B

SC  CSo



r = J+ 

-kx

x  

- k (J + J - I )  (joint velocity) 

B

SC
B

C

EKF

So

SC

med
B

B

SC

-1 S0  CSf ...

Registration Filter
C

 (angles) Median filter So
C

0

ROBOT image

EKF

C

So

Sf0 . . . CSfn Feature Estimator

(target pose)

(feature poses)

Object Filter

Fig. 2. Block Diagram of Control System. 3D poses for features are estimated from visual data. The median camera transform is computed over all features and then Kalman filtered. With this registration, the robot servos in workspace to a target object location.

4

Dantam, Ben Amor, Christensen, Stilman

Fig. 3. Marker-based tracking (left) and model-based tracking (right).

mates before servoing to the target object. This estimation and control loop is summarized in Fig. 2. For computational reasons, we used the dual quaternion representation for the special Euclidean group SE (3). Compared to matrices, the dual quaternion has lower dimensionality and is more easily normalized, both advantages for our filtering implementation. The relevant dual quaternion equations are summarized in appendix A. We represent the dual quaternion S for a transformation implicitly as a tuple of a rotation quaternion q and translation vector v : S = (q , v ). This requires only seven elements. For Euclidean transformations, we use the typical coordinate notation where leading superscript denotes the parent frame and following subscript denotes the child frame, i.e., xSy gives the origin of y relative to x. The transformation aSb followed by bSc is given as the dual quaternion multiplication aSb  bSc = aSc .

3.1

Feature Estimation

To use the robot body as a reference for camera registration, it is important to identify and track body parts, e.g., the end-effector, in 3D. These 3D poses can be estimated with marker-based [17] and model-based approaches [4], see Fig. 3. Marker-based approaches require binary fiducials to known locations on the robot, such as the fingers. Model-based tracking, on the other hand, requires accurate polygon meshes of the tracked object. In our implementation, we use the ALVAR library [17] for marker-based tracking. For model-based tracking, we use the approach from [4]. In each frame the 3D pose of the object is computed by projecting a 3D CAD model into the 2D image. After projection, we identify salient edges in the model and align them with edges in the 2D image. A particle filter is then used to filter the pose estimates over time. Both marker-based and model-based tracking provide 3D pose estimates of tracked features, but with frequent outliers and noise. Markers have the advantage of being easy to deploy, while model-based tracking can deal with partial occlusions of the scene.

Online Camera Registration for Robot Manipulation

5

3.2

Offset Identification

To improve the accuracy of kinematic pose estimates for features, we initially perform a static expectation-maximization-like [6] procedure, based on the following model: B Sk  kSf = BSC  CSf (1) where BSk is the measured nominal feature pose in the body frame determined from encoder positions and forward kinematics, kSf is the unknown static pose offset of the feature due to inaccuracy of manual placement, BSC is the unknown camera registration in the body frame, and CSf is the visually measured feature pose in the camera frame. These transforms are summarized in Fig. 1, with B Sk  kSf combined as BSf . As an initialization step, we iteratively fix either kSf or BSC in (2) and solve for the other using Umeyama's algorithm [20]. This gives us the relative transforms for the features kSf which we assume are static. 3.3 Filtering

To compute the online registration, where BSC is changing, we combine median and Kalman filtering. The median filter is applied independently at each time step to reject major outliers in the estimated feature poses. Compared to weighted least squares methods, the median requires no parameter tuning and is especially resistant, tolerating outliers in up to 50% of the data [8]. Given the median at each step, the Kalman filter is applied over time to generate an optimal registration estimate under a Gaussian noise assumption. Based on (2), each observed feature on the robot gives on estimate for the camera registration BSC :
B

Sk  kSf  (CSf )-1 = BSC

(2)

Median Filtering At each time step, we find the median registration over all observed features. Each observed feature gives a candiate registration BSC . First, we collect a set Q of the orientation candidates:
1 Q = (BqC )i | (BSk )i  kSf  (CSf )- i

(3)

Then, we compute the median of the candidate orientation registrations Q. To find this median, the structure of rotations in SO(3)offers a convenient distance metric between two orientations: the angle between them. Using this geometric interpretation, the median orientation q ^ is the orientation with minimum angular distance to all other orientations.
n Bq C

= arg min
qi Q j =0

 | ln(qi  qj )|

(4)

The median translation x ^ is the conventional geometric median, the translation with minimum Euclidean distance to all other translations. First, we find

6

Dantam, Ben Amor, Christensen, Stilman

the set of candidate translations Z by rotating the feature translation in camera frame Cvf and subtracting from the body frame translation Bvf : Z = zi | zi = Bvf,i - BqC  Cvf,i  BqC


(5)

Then, we compute the geometric median of the candidate translations by finding the element with minimum distance to all other elements:
n Bv C

= arg min
zi Z j =0

|zi - zj |

(6)

Then, the median transform is the combination of the orientation and translation parts:
BS C

=

Bq , Bv C C

(7)

Kalman Filtering Next, we use an Extended Kalman filter (EKF) to attenuate noise over time, taking care to remain in the SE (3) manifold. Similar Kalman filters are discussed in [13,5]. The quasi-linearity of quaternions means the EKF is suitable for orientation estimation in this application [12]. To filter SE (3) poses, we consider state x composed of a quaternion q , a translation vector v , and the translational and rotational velocities, v  and  : x = (q , v ) = [qx , qy , qz , qw , vx , vy , vz , v x, v y , v  z , x , y , z ] The measurement z is the pose: z = (q , v ) = [qx , qy , qz , qw , vx , vy , vz ] The general EKF prediction step for time k is: x ^k|k-1 = f (xk ^ -1 ) Fk - 1 = f x
x ^k-1|k-1

(8) (9) (10)

T Pk|k-1 = Fk-1 Pk-1|k-1 Fk -1 + Qk-1

where x ^ is the estimated state, f (x) is the process model, F is the Jacobian of f , P is the state covariance matrix, and Q is the process noise model. The process model then integrates the translational and rotational velocity, staying in the SE (3) manifold using the exponential of the twist  :  (, v,  v ) = (, v ×  + v ) f (x) = exp t  2  (q, v ) (11)

Online Camera Registration for Robot Manipulation

7

Now, we find the process Jacobian F . The translation portion is a diagonal matrix of the translational velocity. For the orientation portion, we find the quaternion derivative q  from the rotational velocity: = q 1 q 2 (12)

This quaternion multiplication can be converted into the following matrix multiplication: 1 1   q = Mr (q )  2 2  qw qz -qz qw Mr (q ) =   qy -qx -qx -qy

 -qy qx   qw  -qz

(13)

Note that we omit the w column of the typical quaternion multiplication matrix because the w element of rotational velocity  is zero. This gives the following process 13 × 13 Jacobian F :   1 I4×4 0 2 tMr q 0  0 I3×3 0 tI3×3   F = (14)  0 0 I3×3 0  0 0 0 I3×3 Now we consider the EKF correction step. The general form is: z ^k = h(^ xk|k-1 ) h Hk = x
x ^ k |k - 1

(15) (16) (17) + Rk (18) (19) (20) (21)

yk = v (zk , z ^) Sk = Hk Pk|k-1 =
T Hk Pk|k-1 Hk T Sk Kk

x ^k|k = p(^ xk|k-1 , Kk yk ) Pk|k = (I - Kk Hk )Pk|k-1

where z is the measurement, h is the measurement model, H is the Jacobian of h, z ^ is the estimated measurement, R is the measurement noise model, and K is the Kalman gain, v is a function to compute measurement residual, and p is a function to compute the state update. We compute the EKF residuals and state updates using relative quaternions to remain in SE (3) without needing additional normalization. The observation h(x) is a pose estimate: h(x) = (q, v ) H = I7×7 (22)

8

Dantam, Ben Amor, Christensen, Stilman

We compute the measurement residual based on the relative rotation between the measured and estimated pose: v (z, z ^) = (yq , yv )
 yq = ln zq  z ^q q

yv = zv - z ^v

(23)

where yq is the orientation part of the residual and yv the translation part. Note  corresponds to a velocity in the direction of the relative that that ln zq  z ^q transform between the actual and expected pose measurement and that we can consider yq as a quaternion derivative. Then, the update function will integrate the pose portion of y , again using the exponential of the twist. First, we find the twist corresponding to the product of the Kalman gain K and the measurement residual y : (Ky ) = (Ky )q  q   (Ky, v ) = ((Ky ) , v × (Ky ) + (Ky )v ) (24) Then, we integrate estimated pose using the exponential of this twist: (x(q,v) )k|k = exp t  2  (q, v ) (25)

Finally, the velocity component of innovation y is scaled and added: (x,v  )k|k = x,v  + (Ky ),v  3.4 Registered Visual Servoing (26)

We use the computed camera registration BSC to servo to a target object according to the control loop in Fig. 2. This is position-based visual servoing, incorporating the dynamically updated registration. First, we compute a reference twist Be,ref from the position error using camera pose BSC and object pose CSo :
B B

Se,ref = BSC  CSobj
B -1 Se,act  BSe,ref

(27) (28)

e,ref = ln

Then, we find the reference velocity for twist Be,ref : x  D(Be,ref ) - (2D(BSe )  R(BSe )-1 ) × R(Be,ref ) =  R(Be,ref ) where R(X ) is the real part of X and D(X ) is the dual part of X . (29)

Online Camera Registration for Robot Manipulation

9

Finally, we compute joint velocities using the Jacobian damped least squares, also using a nullspace projection to keep joints near the zero position:   r = J + -kx x   - k (J + J - I ) (30)

where J is the manipulator Jacobian matrix, J + is its damped pseudoinverse, kx is a gain for the position error, and k is a gain for the joint error.

4

Experiments

We implement this approach on a Schunk LWA4 manipulator with SDH endeffector, see Fig. 1, and use a Logitech C920 webcam to track the robot and objects. The Schunk LWA4 has seven degrees of freedom and uses harmonic drives, which enable repeatable positioning precision of ±0.15mm [7]. However, absolute positioning accuracy is subject to encoder offset calibration and link rigidity. In practice, we achieve ±1cm accuracy when using only the joint encoders for feedback. The Logitech C920 provides a resolution of 1920x1080 at 15 frames per second. To measure ground-truth distances, we used a Bosch DLR165 laser rangefinder and a Craftsman 40181 vernier caliper. We initially test the convergence and resistance of our approach while moving the camera. With the camera mounted on a tripod, we compute the filtered registration while the camera is perturbed, rotated, and translated. The resulting registrations under moving camera are plotted in Fig. 4. The visual pose estimates contain frequent outliers in addition to a small amount of noise. The filtered registration removes the outliers and converges within 5s. To demonstrate the suitability of this approach for manipulation tasks, we test the positioning accuracy attainable with this online registration. As shown in Fig. 5, we place a marker on a table, measure linear distance to the marker with a laser ranger, servo the end-effector to the visually estimated marker position using the control loop in Fig. 2, and measure the distance to the end-effector which should be directly over the marker. The resulting position accuracy achievable with online registration is summarized in Table 1. For an ideal camera placement with close, direct view of the end-effector (i.e. the angle  between the camera and the markers is 45 or less), positioning accuracy is in the submillimiter range. Larger camera distances and angles, resulted in positioning error of 1 - 2 millimeters. Finally, we test the pre-grasp positioning accuracy of this method as shown in Fig. 6. We place an object, in particular, a cup, at a variety of locations on the table, servo the end-effector to the visually detected object position using the control loop in Fig. 2, and then measure the distance of each finger to the object using a vernier caliper. The results of the pre-grasp positioning are summarized in Table 2. A small number of trials resulted in centimeter-level error for objects placed near the edge of the image frame. Ommitting these outliers, the average positioning error of the pre-grasp configuration was 3.3mm.

10

Dantam, Ben Amor, Christensen, Stilman

0.5 quaternion 0 -0.5 -1 0 10

translation (m)

x y z w

1.5 1 0.5 0 -0.5 0 10

x y z

20 30 time (s) (a) x y z w

40

20 30 time (s) (b) x y z

40

0.5 quaternion 0 -0.5 -1 0 10

1.5 translation (m) 40 1 0.5 0 -0.5 0 10

20 30 time (s) (c)

20 30 time (s) (d)

40

Fig. 4. Registration while camera is bumped (8s), rotates (15s) and translated (24s). camera is bumped. (a)-(b) registration from raw visual pose estimates of one feature. Contains many outliers. (c)-(d) filtered registration. Outliers and noise eliminated.

5

Experimental Insights

There are a number of error sources we must handle in this system. For the kinematics, error from encoder offsets in the arm, imprecise link lengths, and flexing of links all contribute inaccurate kinematic pose estimates. For perception, error from inaccurate camera intrinsics, imprecise fiducial sizes, offsets in object models, and noise in the image all contribute to error in visual pose estimates. To achieve accurate manipulation, we must account for these potential sources of error. The key point of the servo loop in Fig. 2 is that we depend not on minimizing absolute error, but on minimizing relative error. We are minimizing error between end-effector pose Se and target pose So . Because we continually update the camera registration, we effectively minimize this error in the image. As long as there is distance between camera frame poses CSe and CSo , we will move the end-effector towards the target, and as long as the visual distance estimate is zero when we reach the target, the arm will stop at the target. Thus, even if there is absolute registration error due to, e.g., unmodeled lens distortion, it is only necessary that relative error between visual estimates of the end-effector and target be small and converge to zero. The relative error between end-effector and

Online Camera Registration for Robot Manipulation

11

Laser

Fig. 5. Experimental setup for evaluating the positioning accuracy during camera registration. A cube was placed on a marker and the distance to a laser ranger was captured. Subsequently, the cube was placed in the hand of the robot, which, then, servoed to the position of the marker. Again, the distance was measured using the laser ranger.

target is crucial in manipulation, and our technique is well suited to minimizing this error. The position of the tracked features on the robot has an important effect on error correction. Kinematic errors between the robot body origin and the tracked features, e.g., due to flex or encoder offsets, are incorporated into the camera registration and handled through the servo loop. Error between the observed features and the end-effector cannot be corrected. Thus, it is better to track features as close to the end-effector as possible. Consequently, we placed the fiducual markers on the fingers of the SDH end-effector. The principal challenge in the implementation stems from observing the robot pose using small,  3cm, markers. While marker translation is reliably detected, outliers in orientation are frequent. Ample lighting improves detection but does not eliminate outliers. The median pose, (4)-(6), was effective at eliminating outliers from visual estimates. Alternative methods for combining orientations estimates include Davenport's q-method [14] and the Huber loss function [9]. In contract to these other methods, the median has no parameters such as thresholds which require adjustment. Thus, it is especially suited to this online registration application where outlier frequency may vary depending on camera placement, lighting, etc. A potential challenge is that the direct computation of
Setup Average Stdev   45 0.5mm 0.52mm  > 45 1.5mm 1.26mm Table 1. Positioning experiment results. Average and standard deviation [mm] of measured difference between commanded position and object location. Data Average Stdev All 5.8mm 8.5mm Inliers 3.3mm 2.3mm Table 2. Pre-grasp experiment results. Average and standard deviation [mm] of measured difference between object and end-effector position

560.10mm

12

Dantam, Ben Amor, Christensen, Stilman

Fig. 6. Pre-grasp experiment: using the introduced camera registration, the open robot hand is servoed to the position of a glass. The distances between the fingers and the glass are then measured. Since the glass is rotationally symmetric, the distances of both used robot fingers should be identical in the ideal case.

(4) leads to an O(n2 ) algorithm in the number of orientations. However, for the small number of poses we consider at each step here, the computation time is negligable. On a Xeon E5-1620 CPU, computing the median of 32 orientations requires 30µs.

6

Conclusion

We have presented an online method to identify the camera poses for robot manipulation tasks. This is useful for the typical case where camera registration is not static but changes due to model error, disturbances, or wear and tear. The key point is to track both the object and the robot in the image, and servo based on the visually estimated relative pose between the object and robot. By combining median and Kalman filtering of the registration pose, we are able to achieve millimeter-level manipulation accuracy. We have shown in our experiments that online registration can be used to improve positioning accuracy during grasping and manipulation tasks, thereby avoiding typical challenges such as premature contact between fingers and objects. A useful extension to this work would be to handle online registration with multiple cameras. This could provide additional data to improve accuracy or permit greater field of view, e.g., observing both hands in bimanual tasks. We anticipate that considering median deviation and applying a similar extended Kalman filter to multiple simultaneous poses will extend this online approach to multi-camera setups.

Online Camera Registration for Robot Manipulation

13

Acknowledgements
This work would not have been possible without Mike Stilman's tireless guidance and support.

A

Dual Quaternion Computation

Dual quaternions are a numerically convenient representation for Euclidean transformations, SE (3). Compared to ordinary quaternions which can represent rotation, dual quaternions can represent both rotation and translation. Mathematically, they are the extension of quaternions to the dual numbers [18]. Dual numbers are of the form r + d, where r is real part, d is the dual part, and  is the dual element such that  = 0 and 2 = 0. A dual quaternion S can be represented as  a pair of quaternions, S = sr + sd , which we represent with the  tuple sr , sd . We represent the vector and scalar components of the ordinary quaternion parts of a dual quaternion as:   S = r , d  =    rx i + ry j + rz k , rw , dx i + dy j + dz k , dw  =   (rv , rw ) , (dv , dw ) (31) where rv and dv are the vector parts and rw and dw are the scalar parts. The dual quaternion representing orientation q and translation v is:     1  q , v  q  S = sr , sd  =  2 (32)

Dual quaternion Euclidean transforms are normalized by dividing by the real magnitude:   sr sd     S = , (33)  |sr | |sr | Operations on the dual quaternions can be derived from those of ordinary quaternions and the properties of dual numbers. However, this requires care to handle singularities. Generally, the values at these singularities can be computed by identifying singular factors with convergent Taylor series. Given suitable singular factors, a computer algebra system, e.g., Maxima, Mathematica, can be used to compute the Taylor series. We summarize the relevant functions and suitable Taylor series below. Dual quaternion multiplication is:   A  B = ar  br , ar  bd + ad  br  (34)

14

Dantam, Ben Amor, Christensen, Stilman

The dual quaternion exponential is:  = |rv | k = rv · dv  s S w ~ e =e  rv , c ,   c- s dv +  2
s 

(35) (36)  s  krv , - k    (37)

where s = sin , c = cos , w ~ = rw + dw , and rv · dv is the dot product of rv and dv . Then, to handle the singularity at  = 0, we use the following Taylor expansions: 2 4 6 sin  =1- + - + ... (38)  6 120 5040 cos  - 2
sin  

1 2 4 6 =- + - + + ... 3 30 840 45360

(39)

The dual quaternion logarithm is:  = atan2 (|rv | , rw ) k = rv · dv = (ln S )r = (ln S )d =
 rw - |r v| 2 |rv |

(40) (41)

|r|

2

= ln |r| rv +

1 |r|

cos   - 2 3 sin () sin ()

(42) (43)

 rv , |rv | k - dw |r|
2

 dv , |rv |

k+

rw dw |r|
2

(44)

where (ln S )r is the real part of the logarithm and (ln S )d is the dual part of the logarithm. Note that  represents the angle between the real and imaginary parts of unit quaternion r. To handle the singularity at |rv | = 0 and knowing |r| = 1:    = |r | = v |rv | sin 
|r |

(45) (46)

 2 74 316 =1+ + + + ... sin  6 360 15120 Then, for  in (42): c  2 1 17 4 29 6 - 3 = - - 2 -  -  + ... s2 s 3 5 420 4200

(47)

Online Camera Registration for Robot Manipulation

15

References
1. H. Ben Amor, O. Kroemer, U. Hillenbrand, G. Neumann, and J. Peters. Generalization of human grasping for multi-fingered robot hands. In Proceedings of the International Conference on Robot Systems (IROS), 2012. 2. Fran¸ cois Chaumette and Seth Hutchinson. Visual servo control, part I: Basic approaches. Robotics and Automation Magazine, 13(4):82­90, 2006. 3. Fran¸ cois Chaumette and Seth Hutchinson. Visual servo control, part II: Advanced approaches. Robotics and Automation Magazine, 14(1):109­118, 2007. 4. Changhyun Choi and Henrik I Christensen. Robust 3d visual tracking using particle filtering on the special euclidean group: A combined approach of keypoint and edge features. The International Journal of Robotics Research, 31(4):498­519, 2012. 5. Daniel Choukroun, Itzhack Y. Bar-Itzhack, and Yaakov Oshman. Novel quaternion Kalman filter. Trans. on Aerospace and Electronic Systems, 42(1):174­190, 2006. 6. A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1­38, 1977. 7. Schunk GmbH. Dextrous lightweight arm LWA 4D, technical data. http://mobile.schunk-microsite.com/en/produkte/produkte/ dextrous-lightweight-arm-lwa-4d.html. 8. Frank R Hampel, Elvezio M Ronchetti, Peter J Rousseeuw, and Werner A Stahel. Robust statistics: the approach based on influence functions, volume 114. John Wiley & Sons, 2011. 9. Peter J Huber et al. Robust estimation of a location parameter. The Annals of Mathematical Statistics, 35(1):73­101, 1964. 10. Sungmoon Joo and Michael Grey. DRC-Hubo retrospective, January 2014. Personal Communication. 11. Matthew Klingensmith, Thomas Galluzzo, Christopher Dellin, Moslem Kazemi, J. Andrew (Drew) Bagnell, and Nancy Pollard. Closed-loop servoing using realtime markerless arm tracking. In International Conference on Robotics And Automation (Humanoids Workshop), May 2013. 12. Joseph J. Laviola. A comparison of unscented and extended Kalman filtering for estimating quaternion motion. In American Control Conference, volume 3, pages 2435­2440. IEEE, 2003. 13. Ern J. Lefferts, F. Landis Markley, and Malcolm D. Shuster. Kalman filtering for spacecraft attitude estimation. Journal of Guidance, Control, and Dynamics, 5(5):417­429, 1982. 14. F Landis Markley, Yang Cheng, John Lucas Crassidis, and Yaakov Oshman. Averaging quaternions. Journal of Guidance, Control, and Dynamics, 30(4):1193­1197, 2007. 15. OpenCV API Reference. http://docs.opencv.org/master/modules/refman. html. 16. Vijay Pradeep, Kurt Konolige, and Eric Berger. Calibrating a multi-arm multisensor robot: A bundle adjustment approach. In Experimental Robotics, pages 211­225. Springer, 2014. 17. Kari Rainio and Alain Boyer. ALVAR ­ A Library for Virtual and Augmented Reality User's Manual. VTT Augmented Reality Team, December 2013. 18. Eduard Study. Geometrie der dynamen, 1903. 19. A. Teichman, S. Miller, and S. Thrun. Unsupervised intrinsic calibration of depth sensors via slam. In Robotics: Science and Systems (RSS), 2013.

16

Dantam, Ben Amor, Christensen, Stilman

20. Shinji Umeyama. Least-squares estimation of transformation parameters between two point patterns. Pattern Analysis and Machine Intelligence, 13(4):376­380, 1991.

View publication stats

Directing Policy Search with Interactively Taught Via-Points
College of Computing Georgia Tech Atlanta, GA 30332, USA

Yannick Schroecker

yschroecker3@gatech.edu

hbenamor@asu.edu

Interactive Robotics Lab Arizona State University Tempe, AZ 85282, USA

Heni Ben Amor

athomaz@ece.utexas.edu

Department of ECE University of Texas at Austin Austin, TX 78701, USA

Andrea Thomaz

ABSTRACT
Policy search has been successfully applied to robot motor learning problems. However, for moderately complex tasks the necessity of good heuristics or initialization still arises. One method that has been used to alleviate this problem is to utilize demonstrations obtained by a human teacher as a starting point for policy search in the space of trajectories. In this paper we describe an alternative way of giving demonstrations as soft via-points and show how they can be used for initialization as well as for active corrections during the learning process. With this approach, we restrict the search space to trajectories that will be close to the taught via-points at the taught time and thereby significantly reduce the number of samples necessary to learn a good policy. We show with a simulated robot arm that our method can efficiently learn to insert an object in a hole with just a minimal demonstration and evaluate our method further on a synthetic letter reproduction task.

Keywords
Reinforcement Learning, Learning from Demonstration, Reinforcement Learning for Motor Skills, Dynamic Movement Primitives, Keyframe Demonstrations

1.

INTRODUCTION

Robotics research in recent years has been working towards employing robots in unknown and often unstructured environments. However, controlling a robot in these environments poses major difficulties as the robot has to adapt its actions to the environment and needs to perform tasks with incomplete knowledge of the domain. Reinforcement learning and policy search methods in particular have shown great promise for autonomous learning of motor skills as trajectories. However, due to the high dimensionality and size of the state-action space, the required amount of samples can be prohibitive. A prominent approach to overcome this challenge is to use Learning from Demonstration to obtain an initial trajectory and to ensure that the learning process will quickly converge to the right optimum, see e.g. [15, 18, 4]. Unfortunately, this approach suffers from three major issues: First, providing trajectories as demonstrations can Appears in: Proceedings of the 15th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2016), J. Thangarajah, K. Tuyls, C. Jonker, S. Marsella (eds.), May 9­13, 2016, Singapore. Copyright c 2016, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.

be difficult due to the available input modalities. Recording the desired trajectories using teleoperation or kinesthetic demonstration can be difficult for the user and thus often leads to undesirable pauses, sprints or imperfections. Second, providing full trajectories as demonstrations does not allow the user to put focus on critical segments and limits exploration for segments that the user cannot demonstrate as well as other parts of the trajectory. Third, demonstrations are typically only provided as an initialization of the policy search process. Refining a learned policy and removing undesirable effects usually requires a repeated recording of the entire demonstration and cannot be limited to specific aspects of the task. This problem is amplified if the policy search process has already been started as the learned policy can usually not be combined with new demonstrations. To address these issues, we propose a method that uses soft via-points to initialize and interactively shape the policy search process. Recent research [1] has shown that via-point based representations can be efficiently obtained by a human teacher in the form of demonstrations and provide the teacher with a more natural way of teaching the desired trajectory. This kind of demonstration can achieve smoother trajectories by separating the act of moving the robot as a teacher from the intended trajectory that the robot should follow. Furthermore, it allows the teacher to focus on the most important aspects of the trajectory. In this paper, we introduce a method to use demonstrations in this form with policy search methods based on Dynamic Movement Primitives. More specifically, we look at policy search methods that can optimize DMP parameters and thereby improve the policy by evaluating parameters sampled around the current best estimate of the optimal policy. This class of policy search algorithms has shown great promise and includes methods such as PoWER [14], REPS [20], policy search based on CMA-ES [8, 22] and PI2 [24]. By combining these two approaches we allow the teacher to demonstrate the salient points of a trajectory in a natural way while autonomously learning and improving the shape of the trajectory between those via-points. Our method achieves this goal by learning a single and smooth trajectory that adheres to the demonstrated via-points. Furthermore, we introduce a method to modify existing policy distributions by selecting the most likely trajectories based on the provided demonstration. Modifying the trajectory distribution in this way allows the user to provide corrections to existing policies and thereby shape the learning process. These corrections can be applied to policies learned by demonstration, either continuous demonstrations or via-point based demonstrations,

1052

as well as to policies learned during the autonomous learning process. Finally, we propose to use models of via-points to handle variations of tasks that differ in parameters such as the position and orientation of key-objects in a manipulation task. We show that this facilitates efficient contextual policy search [13, 16] and allows to learn classes of tasks.

2.

RELATED WORK

The work presented in this paper is focusing on policy search for robotics. For a survey of this topic, see Deisenroth et al. [6]. Specifically, we are looking at utilizing policy search methods for learning Dynamic Movement Primitives [10] that share the characteristic that they are sampling directly from the current estimate of the optimal policy. One example for this is the Covariance Matrix Adaption Evolution Strategy (CMA-ES) which has been used in [22] in order to learn dynamic movement primitives based on a reward signal. In [14], Kober et al. propose Policy Learning by Weighting Exploration with the Returns (PoWER) which uses regression weighted by rewards in order to obtain a new policy. Another method in this class is PI2 [24] which uses path integrals to improve on the current policy. In our evaluation, we use Episodic Relative Entropy Policy Search (REPS) [20] as the underlying policy search method. Peters et al. derive a sample based approximation of the optimal distribution of DMP parameters given trajectory- and reward samples as well as a bound on the KL-divergence between the optimized distribution and the distribution from which the trajectories were sampled. Using this approximation, the mean can iteratively be updated using weighted linear regression over the sampled DMP parameters and the variance can be updated by the weighted sample variance. The proper weights can be calculated by solving a convex optimization problem based on the bound on the KL-divergence and the sampled parameters and rewards. We refer to [20] for details. In this paper, we consider an approach based on directing the policy search with demonstrations obtained by a human teacher. The field of Learning from Demonstration(LfD) has been extensively covered in [4] and LfD methods have successfully been combined with policy search for Dynamic Movement Primitives [15, 18]. In particular, we are looking at recording partial demonstrations in the form of via-points which have a long history in trajectory generation. Teaching via-points by demonstrations is also known as keyframe demonstration and has been shown to constitute a user-friendly and efficient way to obtain demonstrations [1]. Wada et al. extract via-points from a continuous demonstration and show that these can be used to create a trajectory that minimizes torque change [25]. Miyamoto et al. extend this approach and apply it to learning robot motor skills [17]. Bitzer et al. introduce an approach that combines keyframe demonstrations with reinforcement learning by learning a lower-dimensional manifold to simplify the state-space for a non-episodic reinforcement learner [3]. This method differs from our approach in that it only learns a simpler state-space representation and does not learn a heuristic for specific trajectories. Utilizing corrections in order to change a policy learned from demonstration has been introduced by Argall et al. who propose to use tactile corrections [2]. While this approach is interesting, it cannot be straight-forwardly integrated into autonomous policy search algorithms such as the ones utilized in our approach.

Utilizing feedback from human teachers has been investigated in the field of interactive reinforcement learning. Knox and Stone [12] introduce TAMER+RL, a reinforcement learning framework that utilizes a reward signal obtained by a human teacher in order to learn a regression model that can fulfill a role similar to a Q-function. Another example is given by Griffith et al. [7] who have introduced Policy Shaping. Policy shaping is utilizing rewards obtained by a human teacher in order to learn a separate policy. This policy can then be combined with the policy learned by standard reinforcement learning methods. Judah et al. [11] propose an integrated approach which optimizes a modified objective function based on the reward as well as on human feedback in the form of binary labels. All three methods utilize feedback provided by a human teacher but are different from our approach in that the feedback takes the shape of a reward-like signal instead of demonstrations. One can see both, the learning from demonstration based approach as well as the interactive reinforcement learning approach as belonging to a generalized class of algorithms that utilize insight obtained by a human teacher in order to improve the policy. In this view, the interactive reinforcement learning is online and less structured whereas the classical learning from demonstration based approach is offline and utilizes structured feedback. Our approach is structured as well but can be used in an offline manner as well as online. Another representation of distributions over trajectories that can be restricted to go through specified via points is called Probabilistic Movement Primitives and has been proposed in [19]. Probabilistic Movement Primitives define feed-forward trajectories directly as combination of basisfunctions and define operations on Gaussian distributions over such trajectories. The conditioning operation defined in this approach is similar to the operation for distributions over DMPs introduced in section 3.2. However, while it is likely that Probabilistic Movement Primitives could also be used with our approach, we are focusing on Dynamic Movement Primitives as they are more popular and better understood.

3.

APPROACH

In this paper, we want to utilize a human teacher in order to provide corrections and suggestions before and during the learning process. Our goal is to use this information to help the learning algorithm converge to a better solution after seeing fewer samples. Specifically, we propose a setup where the teacher is observing the reinforcement learning process and can, before starting the learning process or in-between iterations, inspect the current estimate of the best policy and provide suggestions by physically or remotely moving the robot. Suggestions are recorded as soft via-points y  that the trajectories have to pass through at a specified time t . In the case of corrections, the time t can be naturally recorded by having the user stop the robot during an execution in order to provide the correction. This process is illustrated in Algorithm 1. In the case of initial demonstrations, the time t can be estimated manually based on domain knowledge. While choosing the right t in this case may require some thought, we have found that simple heuristics such as distributing via-points equally in time or choosing t to be proportional to the spatial distance of the via-point are usually sufficient. We base our method on episodic policy search in the space

1053

of trajectories represented as Dynamic Movement Primitives (DMPs) [10, 21] which we describe in detail in section 3.1. These algorithms optimize the parameters of the DMP w.r.t. a given reward function, often by a weighted average with weights obtained based on a transformation of the rewards. The parameters of the DMP uniquely define a policy and thus, we loosely refer to the parameters  as policy and to the distribution over parameters  ( ) as policy distribution. To incorporate demonstrations and corrections into this framework, we use the given via-points as a heuristic to guide the learning process to the solution without directly modifying the given objective function. To this end, we obtain a modified policy distribution and sample only trajectories that are close to the demonstrated via-points. By modifying the policy directly, the effects of demonstrations and corrections are immediate and the learning process never samples trajectories that are far from the demonstrated viapoints. This leads to faster convergence and safer samples. As we modify the policy directly, we require policy search methods that improve upon the given policy in independent iterations based on samples obtained directly from the policy such as PoWER [14], REPS [20] and CMA-ES [22] which obtain a new policy based on a weighted average of the samples obtained from the old policy distribution as well as PI2 [24]. In many cases it can be desirable to consider parameterized tasks as this allows us to learn variations of a task instead of optimizing for a single trajectory. For example, the optimal trajectory in a manipulation task may depend on the location of key objects. One way to solve tasks such as these is to learn a linear model of the parameters µ = A  for some features  to serve as the mean of the policy distribution [16]. To handle parameterized tasks, we generalize our notion of via-points to models that are linear in , i.e. y  = B  where B is either derived from domain knowledge or learned with linear regression. Algorithm 1 Policy search with interactive demonstrations 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: Initialize  (0) ( )  N ( ; µ(0) , (0) ) Obtain initial via-points y , t from demonstration for y  , t  y , t do  (0) ( )  p( |y  , t , µ(0) , (0) ) (see Eqs. 18-21) end for for k  1 to N iterations do  (k) ( )  policy search iteration( (k) ) Execute trajectory defined by mean parameters µ(k) while teacher stops execution do Record stop time t Let teacher move the robot, obtain correction y   (k) ( )  p( |y  , t , µ(k) , (k) ) Execute mean trajectory defined by µ(k) end while end for

jectory separately from the shape of the trajectory. Furthermore, DMPs have the concept of a phase which is a function of time which can be adapted by changing the time-scale. This is meant to reduce the dependency on the time. DMPs are defined as y ¨ =  2   (g - y ) - z  = - z z. y   +  2 fw (z ), (1) (2)

where  is the time scaling parameter and z is a parameter that shapes the phase function.  and  are parameters that are analogous to the gains of a PD-controller and define how the system is drawn to the goal of the trajectory which is defined by g . fw (z ) is the forcing function which determines the shape of the trajectory and is defined as a mixture of radial basis functions with parameters K  N, c, h  RK . fw (z ) =
K i=1 i (z )wi z, K i=1 i (z ) 2

(3) . (4)

1 (z - ci ) . i (z ) = exp - 2 hi

Commonly, the weights wi ; 0  i < K are taken as the parameters of the DMP that are learned by demonstration or autonomously whereas the other parameters are given as hyper-parameters. However, in some cases the goals are not known. We therefore include the goal position in the . g parameters  = ( w ). K then determines the dimensionality of the parameter-space. In this paper, we are utilizing Gaussian distributions over DMP parameters as policy distributions and optimize this distribution with respect to the reward. The mean of this distribution  will be given as a linear function of the features of the task parameters:  ( ) = N ( , A ,  ).

3.2

Obtaining a Sample Distribution

Assuming that we have via-points obtained from demonstration as described above, we derive a modified policy distribution for the underlying policy search that passes close to this via-point at the specified time t : . H ( ) = p( |t , y  )  p(y  |t , y I ,  )p( |t ). (5) The latter distribution over parameters  denotes the prior of where we assume that the human teacher would want the samples to lie. A possible prior is the current policy N ( |µ(k) , (k) ) where µ(k) = A(k) . This prior is reasonable as we want our samples to still follow the current policy where no via points are given. The modified sampling distribution then depends on the previous policy distribution and is given by H ( ) = p( |t , y  , A(k) , (k) )  p (y  |t , y I ,  ) ( |A(k) , (k) ) . (6)

3.1

Dynamic Movement Primitives

Policy search relies on optimizing the parameters of a parametric policy representation. One such policy representation are Dynamic Movement Primitives which have been introduced by Ijspeert et al. in [10]. DMPs are given as dynamical systems that are attracted by a goal position while following a superimposed trajectory. As such they have the property that they can adjust the goal position of the tra-

The former distribution p(y  |t , y I ,  ) denotes the probability of a given DMP going through the specified via point with a specified variance y . This distribution is dependent on the trajectory that the DMP is following. As DMPs are defining accelerations as a linear differential equation, they can be solved for y given a starting position and velocity in order to obtain an estimate of the position at any given time. Note that the solution will not be exact as a real robotic system always has noise and the DMP will react to that noise

1054

102 101

Reward

0 -101 -102
With via-point Without via-point

(a)

(b)

-103 -104 0 900 1800 2700 Samples

Figure 1: a) Rest position of the robot. The robot has to find a trajectory that puts the object in the box. b) Viapoint with the key placed in front of the hole, as it was given to the robot. All trajectories have to be close to this viapoint which gives significant aid for finding the opening in the box.

3600

4500

as well as inaccuracies in the underlying controller. Only the goal position is being tracked exactly. However, using this estimate is reasonable as long as the noise of the estimate is significantly smaller than the inaccuracies introduced by the human teacher when recording a demonstration. Assuming that we start from a rest position where the position y0 and velocity y  0 are zero (we make this assumption for the sake of simplicity. See the appendix for a derivation of the sampling distribution for arbitrary y0 and y  0 ), the dynamical system can be solved for the position y using Duhamel's principle [23]:
y  y t

Figure 2: Rewards obtained over 20 trials during the learning process with and without via-points, plotted on a scale that is linear from -10 to 10 and logarithmic outside of that span. In 18 out of 20 trials, the learner with the via-point finds the opening in the box and obtains a reward close to zero. This results in a low standard deviation (error bars). Without the via-point the learning process converges to a local optimum.

Note that this equation is for a single dimension but can straightforwardly be extended to multiple dimensions by extending M diagonally and adding rows to  such that y=
Mt 0 ··· 0 Mt ··· 0 1

. . .

. . . . ..

. . .

,

=
0

ht (s)

0  2 (g +(z (s))T w)

ds,

(7)

where . (t-s) - 2  ht (s) = e . z (t) = e- z t , . i (z ) = i (z )z . K j =0 j (z )
0 1 - 

where j denotes the parameters of the DMP for dimension j . Therefore for DMPs, we can write the likelihood distribution p(y  |t , y I ,  ) as p(y  |t , y I ,  ) = N (y  |Mt  , y I ). (17)

,

(8) (9) (10)

This equation is linear w.r.t the parameters  and can therefore be written as:
t 0  2 (g +(z (s))T w)

Now we can calculate the sampling distribution as the posterior distribution of the likelihood p(y  |t , y I ,  ), encoding information about the via-point, and the prior distribution  which consists of the previously learned policy. For Gaussian distributions, the sampling distribution is therefore given as H ( ) = N ( |AH , H ), . T -1 1  y IMt + Mt H = -  (k) = (k) -
T  (k) Mt

(18) (19) (20)

y = (1 0)
0

ht (s)
t

ds

(11) (12)

(y I + Mt (k) ,

= (1 0)

g
0 N

ht (s)
t

0 2

ds
0 2

T  Mt (k) Mt

-1

. T -1 -1  y I B +  (k) A (k) AH = H Mt .  

(21)

wi
i=0 0

i (z (s))ht (s)

ds (13)

= Mt  , where Mt = ( m0
t m1 ... mN +1 0 2

Where the application of the Woodbury matrix identity in Eq. 20 allows for numerically stable computation of H . This sampling distribution can then be used in place of the policy in order to obtain the samples used in the next iteration of the policy search as described in Algorithm 1.

), ds,
0 2

(14)

4.
(15) ds. (16)

EXPERIMENTS

m0 =
0 t

ht (s)

mi;0<i<N +1 =
0

i-1 (z (s))ht (s)

In section 3, we showed how to derive a modified sampling distribution based on via-points with timing information that are provided by a human demonstration. In this section, we first utilize a simulated robot arm to show that

1055

-3 -4 -5

Reward

-6 -7 -8 -10 -9 0 500 Continuous demonstration Via-point demonstration 1000 Samples 1500 2000

(a)

(b)

-11

Figure 3: a) Mean trajectory of initial policy (bottom) derived from via-points (center). The trajectory is smooth and barely deviates from the desired trajectory (top) even in-between via-points. b) Mean trajectory (bottom) of initial policy derived from a continuous demonstration (center). Especially the last letter shows how the trajectory mimics imperfections of the demonstration.

Figure 4: Comparison of reward obtained within 2000 samples, averaged over 30 trials. Error bars are showing the standard deviation. The reward function is defined as the minimum squared distance to the goal position. Learning initialized with via-points consistently outperforms the learning process initialized with a continuous demonstration.

such a modified sampling distribution can drastically improve the outcome of a reinforcement learner by having it learn how to insert an object in a hole while requiring only minimal information from the user. We then utilize a word reproduction task in order to provide a comparison to continuous demonstrations and to exhaustively analyze the keyproperties of our algorithm.

provided via-point is further away from the box than the local optima.

4.2

Letter Writing

4.1

Object Insertion with a Robot Arm

In our first experiment, we are evaluating the influence of a small number of demonstrated via-points on the reinforcement learning process and show that it can drastically improve the convergence of the policy search. To this end, we are utilizing a simulated 7 DoF robot arm and have it learn how to insert an object into a hole without any knowledge about the environment. We provide a single via-point (see Figure 1b) and show that this is sufficient to lead the learning process to the right solution which cannot reliably be found without a demonstration. To learn this task, we are utilizing our method with REPS as the underlying policy search algorithm to optimize the distribution over trajectories. These trajectories are represented by 7 Dynamic Movement Primitives with 6-dimensional weights leading to a 49-dimensional action vector  . For evaluation, the outcome of the learning process is averaged over 20 trials with 30 policy search iterations per trial and 150 samples for each iterations. As can be seen in Figure 2, the reward curve observed by using the via-point is converging to a value close to 0 and therefore the distance of the trajectory to the goal position is converging to 0 as well. The learning process initialized with a zero mean policy, on the other hand, is converging to different values. This can easily be explained by the local optima that arise around the box, i.e. the robot is converging to solutions where the end-effector is pressing against the middle of other the sides of the box in order to get closer to the goal position. As a consequence, it stops exploring the box and does not find the opening. Note that the learning process always exceeds the initial reward obtained by our approach as the reinforcement learner always finds at least the closest points outside of the box which cannot be derived by the provided via-point alone, i.e. the

To further investigate the properties of this algorithm we are evaluating our approach on a letter reproduction task as well. Variations of this task have been used in the past to evaluate different properties of Dynamic Movement Primitives as it has many similarities with learning trajectories for robot arms while allowing for intuitive visualization, easy recording of demonstrations and thus good conditions for a comparison to conventional demonstrations as well as fast execution [10, 9]. The objective of the letter reproduction task is to learn trajectories for both dimensions which, when followed by a simulated pen, can accurately reproduce a sequence of letters. We are representing those trajectories as DMPs with a 60 dimensional weight-vector for each dimension and initialize the policy by a continuous demonstration or via-points before optimizing it using REPS. For the policy search, we defined the reward function as the number of 104 . Note that overlapping black pixels: - #IntersectingP ixels+1 solving this task requires the learning algorithm to learn trajectories that are far more complex than in the previous task while utilizing a sparser reward signal. First, we will compare our algorithm to learning trajectories from continuous demonstrations as a baseline, then we will evaluate the use of corrections after the learning process has started and finally we will show that linear models of via-points can be used to learn policies that can reproduce trajectories with different rotation and scaling factors.

4.2.1

Comparison to Continuous Demonstrations

The first instance of the letter reproduction task compares learning initialized with a normal distribution around a continuous demonstration to learning initialized on a fixed set of via-points at equi-distant points in time. In this experiment we show that despite the lack of information between via-points, our approach will converge to a more accurate solution in fewer iterations. To obtain an initial policy from a continuous demonstration, we use standard least squares

1056

(a)

(b)

- 3. 5 - 4. 0 - 4. 5 - 5. 0 - 5. 5 - 6. 0 - 6. 5 - 7. 0 - 7. 5 - 8. 0

Reward

100 200 300

500 800 1300

0

500

Figure 5: a) Mean trajectory of the initial policy (bottom) derived by using only every second via-point (center). This trajectory omits whole letters when compared to the desired trajectory (top). b) Trajectory after 3 iterations (bottom), when the second half of the via-points have been added (center). The policy now shows the desired word and can be improved in future iterations. The "r" is not fully formed immediately after the correction due to learned behavior. to learn the mean and then add an initial variance of 103 for the weights and 5 for the goals of the DMPs. We have found these values to yield the best result in the continuous case. For policy search initialized with via-points, we start with a multivariate normal distribution with mean 0 and a variance of 105 for the weights and 500 for the goal positions. Note that the higher initial variance is necessary as conditioning on the via-points will otherwise lead to an overly narrow distribution with each added via-point decreasing the variance of the policy. This initial distribution is then used as the prior distribution to obtain a modified initial policy based on the via-points that can be seen in Figure 3a. In Figures 3a and b, it can be seen that the initial policy derived from the via-points is very smooth whereas the initial policy derived from a continuous demonstration mirrors the imperfections of that demonstration. Note that these imperfections are often much larger in practice as policy search is unnecessary in domains where given demonstrations already solve the task perfectly. Furthermore, the figures show that the errors introduced by missing information between the via-points is of the same order as the errors that can be introduced by linear regression and that the mean of the initial policy is already describing a good trajectory which leads to a fast learning process. Finally, Figure 4 shows that our approach yields both, higher initial reward as well as higher final reward and therefore better trajectories before and after the learning process, when compared to the baseline. Note that the higher initial reward is tied to the amount of exploration that is necessary in the beginning and can, in many cases, be a desirable property when it comes to safe exploration. While our approach only explores the areas in-between the via-points, a reinforcement learner that has been initialized with a continuous demonstration has to explore around the full trajectory. It is possible to reduce the exploration around the continuous demonstrations; however this would also reduce the final reward that can be obtained.

1000 Samples

1500

2000

Figure 6: Comparison of rewards obtained with the rest of the via-points added after different numbers of samples. Early demonstrations are clearly better than late demonstrations but late demonstrations can still be effective.

be used to modify an already trained policy. This is especially useful in situations where the optimal trajectory is not immediately apparent to the user. To investigate the impact of providing via-points at later iterations we are looking at a variation of the letter writing task where only every second of the initial via-points are provided from the start. Figure 5a shows that this constitutes a far worse initial policy and that we would expect large gains by providing the second half of the via-points. As can be seen in Figure 5b as well as in the reward curve in Figure 6, via-points provided at a later point can indeed improve the policy significantly. However, while giving the via-points after some number of iterations can still cause a significant jump in reward, the size of this jump decreases for later iterations. After some time, the modified sample distribution will even decrease the performance of the learning process. This effect can be attributed to a mismatch of the chosen prior distribution, i.e. the current policy, with the optimal prior distribution which is the unknown policy according to which the human teacher is sampling his via-points. As the learning process converges to a sub optimal policy, it is impossible to sample trajectories that adhere to both, the learned policy as well as the specified via-points. Depending on the value of the variance parameter, the learner can then either sam-

(a)

(b)

4.2.2

Active Corrections

One important aspect of the approach presented in this paper is that via-points can be provided at any time and can

Figure 7: Example of giving via-points interactively. a) Original policy in comparison to the desired trajectory (top) and the via-point that has been provided as a correction (bottom). b) Mean of the modified policy. The trajectory goes through the via-point (top) and thereby matches the desired trajectory more closely (bottom).

1057

- 2. 8 - 3. 0 - 3. 2 - 3. 4 - 3. 6 - 3. 8 - 4. 0 - 4. 2 0 2000
Parameterized task Non-parametric task

5.

CONCLUSION

4000 6000 Samples

8000

10000

Figure 8: Reward obtained by contextual REPS shows that via-point models are an effective initialization. Rewards on the non-parametric task are displayed for comparison. The initial reward is identical due to the linear model and is improved upon significantly in both cases.

ple degenerate trajectories from low-probability areas of the current policy or ignore the given suggestion. The variance parameter is thus a measure of safety and specified how far the new policy can stray from the learned policy in order to adhere to the correction. To avoid this effect, via-points should always be given while the uncertainty in the current policy is relatively high in comparison to the deviation of the via-points from this policy. Note that in this experiment, the via-points are already known from the start even if the agent doesn't utilize them. This allowed us to analyze the effect of giving late demonstrations without having to account for the human factor. However, in practice, late demonstrations should be given depending on trajectories sampled from the current policy. This way, the user can actively shape the trajectory and guide the learning process to the right solution. We illustrate the process of providing via-points interactively and show the impact of a correction in Figure 7.

In this paper we introduced an approach to utilize partial demonstrations to interactively guide the policy search process. We have shown that our approach of using soft viapoints significantly outperforms continuous demonstrations when used to initialize the learning process. Furthermore, our results show that our approach can be used to guide the learning process in an interactive way, utilizing the knowledge gained from observing the robot to change specific aspects of the policy. Finally, we have shown that we can use linear models of via-points to generalize over variations of a task. One key insight is that when applying our method during the learning process, the results are largely dependent on the covariance of the already learned policy. While sampling completely new trajectories ensures that the robot continues exploration and does not return to the original trajectory after reaching the via-point, it also requires a prior distribution that specifies sensible trajectories. In our approach, this distribution is given by the policy search. In the future, we plan to investigate other prior distributions based on different models of how humans give via-points to allow providing corrections to already converged policies. Furthermore, for each via-point the user is required to specify an exact point in time. We plan to relax this assumption and allow the user to specify distributions in time as the importance of preserving the timing is heavily dependent on the task. Finally, the method presented in this paper is based on the assumption that the policy is a Gaussian distribution. While this can be a reasonable assumption, there are cases where other distributions would be preferable. Multi-modal policies, for example, can be used to learn tasks with multiple solutions [5]. In the future, we would like to explore this avenue and extend our approach to different types of policies.

Reward

Acknowledgments
This work was conducted as a part of the OpenLabs project 1436618 sponsored by PSA Peugeot and partially funded under ONR grant number N000141410003.

4.2.3

Evaluation of Learning with Linear Models

For the third experiment, we investigated the properties of learning a parametric generalization of the above task with a linear via-point model. We are looking at learning a model that can recreate words with respect to rotation and scaling of the target image. Note that similar kinds of parametric tasks can be found in practice, where the parameters often denote the location and orientation of an object. We are assuming that good features are known as this would be a necessity for obtaining a via-point model in practice. In this case we are using the feature vector (sx cos(), sy cos(), sx sin(), sy sin(), 1) with sx and sy representing the scaling and lie between 0.6 and 1.4 whereas  represents the rotation of the target image which lies between -45 and 45 degrees. The linear via-point models are then given w.r.t. these features and are used for learning a linear Gaussian policy using locally linear weighted regression and contextual REPS. Figure 8 shows that the initial policy adapts to the task parameters and that it can be used in conjunction with contextual REPS to obtain a similar reward curve for arbitrary rotations and scaling as for a single instance of the task.

APPENDIX A. DERIVING H () FOR ARBITRARY INITIAL POSITIONS AND VELOCITIES

In section 3, we derived a sampling distribution under the assumption that y0 = 0 and y  0 = 0. However, while y0 = 0 can be assumed w.l.o.g., y  0 = 0 is only true for trajectories that start from a rest position. Here, we derive an equivalent sampling distribution for the general case. In the general case, the closed form for dynamic movement primitives is given by
y  y t

=
0

e
t

(t-s)

0 1 - 2  - 

0  2 (g +(z (s))T w)

ds (22)

+e

0 1 - 2  - 

y0 y 0

.

And therefore y = Mt  + c (23)

1058

where
t . c = (1 0)e 1 - 2  -    0 y0 y 0

.

(24)

The sampling distribution p( |t , y ) = N ( |AH , H ) is then computed with the modified likelihood distribution p(y  |t ,  ) = N (y  |Mt  + c, y I ). (25)

The mean of this distribution differs slightly from the distribution derived in section 3 so that
T -1 -1  y I (B  - c) +  (k) A (k)  . AH  = H Mt  

(26)

Note that we can assume w.l.o.g. that  is of the form  = ( 1 2 ··· 1 )T . The sampling distribution is then defined by . T  (27) H = (k) - (k) Mt
T  y I + Mt (k) Mt -1

Mt (k) ,
0 c )) 1 + - A (k) .  (k) 

. T -1  y I (B - ( 0 ... AH = H Mt

(28)

REFERENCES
[1] B. Akgun, M. Cakmak, J. W. Yoo, and A. L. Thomaz. Trajectories and keyframes for kinesthetic teaching: a human-robot interaction perspective. In International Conference on Human-Robot Interaction, pages 391­398, 2012. [2] B. Argall, E. Sauser, and A. Billard. Policy adaptation through tactile correction. In Annual Convention of the Society for the Study of Artificial Intelligence and Simulation of Behaviour (AISB), 2010. [3] S. Bitzer, M. Howard, and S. Vijayakumar. Using dimensionality reduction to exploit constraints in reinforcement learning. In International Conference on Intelligent Robots and Systems (IROS), pages 3219­3225, 2010. [4] S. Chernova and A. L. Thomaz. Robot learning from human teachers. Synthesis Lectures on Artificial Intelligence and Machine Learning, 8(3):1­121, 2014. [5] C. Daniel, G. Neumann, and J. Peters. Hierarchical relative entropy policy search. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2012. [6] M. P. Deisenroth, G. Neumann, and J. Peters. A survey on policy search for robotics. Foundations and Trends in Robotics, 2(1-2):1­142, 2013. [7] S. Griffith, K. Subramanian, J. Scholz, C. Isbell, and A. L. Thomaz. Policy shaping: integrating human feedback with reinforcement learning. In Advances in Neural Information Processing Systems (NIPS), pages 2625­2633, 2013. [8] N. Hansen and A. Ostermeier. Adapting arbitrary normal mutation distributions in evolution strategies: the covariance matrix adaptation. In International Conference on Evolutionary Computation (ICEC), pages 312­317, 1996. [9] A. J. Ijspeert, J. Nakanishi, H. Hoffmann, P. Pastor, and S. Schaal. Dynamical movement primitives: learning attractor models for motor behaviors. Neural Computation, 25(2):328­373, 2013.

[10] A. J. A. Ijspeert, J. Nakanishi, and S. Schaal. Learning attractor landscapes for learning motor primitives. In Advances in Neural Information Processing Systems (NIPS), pages 1547­1554, 2002. [11] K. Judah, S. Roy, A. Fern, and T. G. Dietterich. Reinforcement Learning Via Practice and Critique Advice. AAAI, 2010. [12] W. B. Knox and P. Stone. Reinforcement learning from simultaneous human and MDP reward categories and subject descriptors. In Autonomous Agents and Multiagent Systems (AAMAS), pages 475­482, 2012. [13] J. Kober, E. Oztop, and J. Peters. Reinforcement learning to adjust robot movements to new situations. In Robotics: Science and Systems (RSS), 2010. [14] J. Kober and J. Peters. Policy search for motor primitives in robotics. In Advances in Neural Information Processing Systems (NIPS)., pages 849­856, 2008. [15] P. Kormushev, S. Calinon, and D. G. Caldwell. Robot motor skill coordination with EM-based reinforcement learning. In Intelligent Robots and Systems (IROS), pages 3232­3237, 2010. [16] A. G. Kupcsik, M. P. Deisenroth, J. Peters, and G. Neumann. Data-Efficient Generalization of Robot Skills with Contextual Policy Search. In AAAI Conference on Artificial Intelligence (AAAI), 2013. [17] H. Miyamoto, S. Schaal, F. Gandolfo, H. Gomi, Y. Koike, R. Osu, E. Nakano, Y. Wada, and M. Kawato. A Kendama learning robot based on bi-directional theory. Neural Networks, 9(8):1281­1302, 1996. [18] K. M¨ ulling, J. Kober, O. Kroemer, and J. Peters. Learning to select and generalize striking movements in robot table tennis. International Journal of Robotics Research, 32(3):263­279, 2013. [19] A. Paraschos, C. Daniel, J. Peters, and G. Neumann. Probabilistic movement primitives. In Advances in Neural Information Processing Systems (NIPS), pages 2616­2624, 2013. [20] J. Peters, K. M¨ ulling, and Y. Altun. Relative Entropy Policy Search. In AAAI Conference on Artificial Intelligence (AAAI), pages 1607­1612, 2010. [21] S. Schaal. Dynamic movement primitives - a framework for motor control in humans and humanoid robotics. In Adaptive Motion of Animals and Machines, pages 261­280. Springer Tokyo, 2003. [22] F. Stulp and O. Sigaud. Path integral policy improvement with covariance matrix adaptation. In International Conference on Machine Learning (ICML), pages 281­288, 2012. [23] G. Teschl. Ordinary differential equations and dynamical systems, volume 140. American Mathematical Soc., 2012. [24] E. Theodorou, J. Buchli, and S. Schaal. Learning policy improvements with path integrals. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 828­835, 2010. [25] Y. Wada, Y. Koike, E. Vatikiotis-Bateson, and M. Kawato. A computational model for cursive handwriting based on the minimization principle. In Advances in Neural Information Processing Systems (NIPS), pages 727­734, 1993.

1059

Interaction Primitives for Human-Robot Cooperation Tasks
Heni Ben Amor1 , Gerhard Neumann2 , Sanket Kamthe2 , Oliver Kroemer2 , Jan Peters2,3

Abstract-- To engage in cooperative activities with human partners, robots have to possess basic interactive abilities and skills. However, programming such interactive skills is a challenging task, as each interaction partner can have different timing or an alternative way of executing movements. In this paper, we propose to learn interaction skills by observing how two humans engage in a similar task. To this end, we introduce a new representation called Interaction Primitives. Interaction primitives build on the framework of dynamic motor primitives (DMPs) by maintaining a distribution over the parameters of the DMP. With this distribution, we can learn the inherent correlations of cooperative activities which allow us to infer the behavior of the partner and to participate in the cooperation. We will provide algorithms for synchronizing and adapting the behavior of humans and robots during joint physical activities.

I. INTRODUCTION Creating autonomous robots that assist humans in situations of daily life has always been among the most important visions in robotics research. Such human-friendly assistive robotics requires robots with dexterous manipulation abilities and safe compliant control as well as algorithms for humanrobot interaction during skill acquisition. Today, however, most robots have limited interaction capabilities and are not prepared to appropriately respond to the movements and behaviors of their human partners. The main reason for this limitation is the fact that programming robots for such interaction scenarios is notoriously hard, as it is difficult to foresee many possible actions and responses of the human counterpart. Over the last ten years, the field of imitation learning [12] has made tremendous progress. In imitation learning, a user does not specify the robot's movements using traditional programming languages. Instead, he only provides one or more demonstrations of the desired behavior. Based on these demonstrations, the robot autonomously generates a control program that allows it to generalize the skill to different situations. Imitation learning has been successfully used to learn a wide range of tasks in robotics [2], such as basic robot walking [4], [5], driving robot cars [10], object manipulation [9], and helicopter manoeuvring [1]. A particularly successful approach to imitation learning is based on Dynamic Motor Primitives (DMPs)[6]. DMPs use dynamical systems as a way of representing control policies,
1 Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, 801 Atlantic Drive, Atlanta, GA 30332-0280, USA 2 Intelligent Autonomous Systems, Department of Computer Science, Technical University Darmstadt, Hochschulstr. 10, 64289 Darmstadt, Germany 3 Max Planck Institute for Intelligent Systems Spemannstr. 38, 72076 Tuebingen, Germany

Fig. 1. A human-robot interaction scenario as investigated in this paper. A robot needs to learn when and how to interact with a human partner. Programming such a behavior manually is a time-consuming and errorprone process, as it hard to foresee how the interaction partner will behave.

which can be generalized to new situations. Several motor primitives can be chained together to realize more complex movement sequences. In this paper, we generalize the concept of imitation learning to human-robot interaction scenarios. In particular, we learn interactive motor skills, which allow anthropomorphic robots to engage in joint physical activities with a human partner. To this end, the movements of two humans are recorded using motion capture and subsequently used to learn a compact model of the observed interaction. In the remainder of this paper, we will call such a model an Interaction Primitive (IP). A learned IP is used by a robot to engage in a similar interaction with a human partner. The main contribution of this paper is to provide the theoretical foundations of interaction primitives and their algorithmic realization. We will discuss the general setup and introduce three core components, namely methods for (1) phase estimation, (2) learning of predictive DMP distributions, and (3) correlation the movements of two agents. Using examples from handwriting synthesis and human-robot interaction tasks, we will clarify how these components relate to each other. Finally, we will apply our approach to real-world interaction scenarios using motion capture systems. II. RELATED WORK Finding simple and natural ways of specifying robot control programs is a focal point in robotics. Imitation learning, also known as programming by demonstration, has been proposed as a possible solution to this problem [12]. Most ap-

proaches to imitation learning obtain a control policy which encodes the behavior demonstrated by the user. The policy can subsequently be used to generate a similar behavior that is adapted to the current situation. Another way of encoding policies is to use statistical modeling methods. For example, in the mimesis model [8] a continuous hidden Markov model is used for encoding the teacher's demonstrations. A similar approach to motion generation is presented by Calinon et al. [3] who used Gaussian mixture regression to learn gestures. The methods discussed so far are limited to single agent imitation learning scenarios. Recently, various attempts have been undertaken for using machine learning in human-robotinteraction scenarios. In [13], an extension of the Gaussian process dynamics model was used to infer the intention of a human player during a table-tennis game. Through the analysis of the human player's movement, a robot player was able to determine the position to which the ball will be returned. This predictive ability allowed the robot to initiate its movements even before the human hit the ball. In [7], Gaussian mixture models were used to adapt the timing of a humanoid robot to a human partner in close-contact interaction scenarios. The parameters of the interaction model were updated using binary evaluation information obtained from the human. While the approach allowed for humanin-the-loop learning and adaptation, it did not include any imitation of observed interactions. In a similar vein, the work in [8] showed how a robot can be actively involved in learning how to interact with a human partner. The robot performed a previously learned motion pattern and observed the partner's reaction to it. Learning was realized by recognizing the observed reaction and by encoding the action-reaction patterns in a HMM. The HMM was then used to synthesize similar interactions. In our approach, learning of motion and interaction are not split into two separate parts. Instead, we learn one integrated interaction primitive which can directly synthesize an appropriate movement in response to an observed movement of the human partner. Furthermore, instead of modelling symbolic action-reaction pairs, our approach models the joint movement of a continuous level. This continuous control is realized through the use of DMPs as the underlying representation. By introducing probabilistic distributions and bayesian inference in the context of DMPs, we obtain a new set of tools for predicting and reacting to human movements. III. INTERACTION PRIMITIVES The goal of learning an Interaction Primitive is to obtain a compact representation of a joint physical activity between two persons and use it in human robot interaction. An interaction primitive specifies how a person adapts his movements to the movement of the interaction partner, and vice versa. For example, in a handing-over task, the receiving person adapts his arm movements to the reaching motion of the person performing the handing-over. In this paper, we propose an imitation learning approach for learning such interaction primitives. First, one or several demonstrations of the

interaction task are performed by two human demonstrators in a motion capture environment. Using the motion capture data, we extract an interaction primitive which specifies the reciprocal dependencies during the execution of the task. Finally, the learned model is used by a robot to engage in a similar interaction with a human partner. An IP should also be applicable to a wide variety of related interactions, for example, handing over an object at different locations. An example of an interaction of a humanoid robot with a human partner performing a high-five movement is given in Fig. 2. At the core of our approach is a new representation for interaction tasks. An IP can formally be regarded as a special type of DMP which represents a joint activity of two interaction partners. After an IP is learned from the demonstration data, it is used to control a robot in a similar task. For the sake of notational clarity, we will refer to the first interaction partner, i.e., the human, as the observed agent, while the second interaction partner, i.e., the robot, will be called controlled agent. The IP performs three steps to infer an appropriate way of reacting to the movement of the observed agent. 1) Phase Estimation: The actions of the interaction partners are executed in synchrony. In particular, the robot adapt its timing such that it matches the timing of the human partner. For this synchronization, we need to identify the current phase of the interaction 2) Predictive DMP distributions: As a next step, we compute predictions over the behavior of an agent given a partial trajectory  o of the observed agent. To do so, we use a probabilistic approach and model a distribution p( ) over the parameters of the DMPs. This distribution can be conditioned on a new, partial observation, i.e., to obtain an updated parameter distribution p( | o ). We use samples of this distribution to predict the future behavior of the agent. 3) Correlating both Agents: In order to perform a successful cooperation, the movement of the robot needs to be correlated with the movement of the human. Such operation is a straightforward extension to the predictive DMP distributions. Instead of conditioning on the observation of all DoFs, we only condition on the DoFs of the observed agent, and, hence, we also obtain a distribution over the DMP parameters of the controlled agent, that can be used to control the robot. In the following, we will address each of the steps above in detail. First, we will recapitulate the basic properties and components of DMPs. Subsequently, we will describe how phase estimation, adaptation, and correlation can be realized within the DMP framework in order to produce an interactive motor primitive. A. Dynamic Motor Primitives A DMP is an adaptive representation of a trajectory representing a human or robot movement [6]. In this section, we will give a brief recap of DMPs. The general idea is to encode a recorded trajectory as dynamical systems, which can be used to generate different variations of the original

Motion Capture

Interaction Primitive

Human-Robot Interaction

Fig. 2. An overview of the presented machine learning approach. Left: Using motion capture we first record the movements of two persons during an interaction task. Center: Given the recorded motion capture data, we learn an interaction primitive specifying each persons' movement as well as the dependencies between them. Right: During human-robot interaction, the learned interaction primitive is used by the robot to adapt its behavior to that of his human interaction partner.

movement. As a result, a robot can generalize a demonstrated movement to new situations that may arise. Formally, a DMP can be written as a dynamical system y ¨ = (y (y (g - y ) - ((y  )/ )) + f (x))  2 (1)

B. Phase Estimation by Dynamic Time Warping For a joint activity to succeed, the movement of the interaction partners needs to be temporally aligned. During the execution of human-robot interaction, the robot observes a partial movement of the human counterpart. Given this partial movement sequence, we need to determine the current state of the interaction. This is achieved by determining the current value of the phase variable x. To this end, we will use the dynamic time warping (DTW) algorithm [11]. DTW is a method for the alignment of time series data. Given two time series u = (u1 , · · · , uN ) and v = (v1 , · · · , vM ) of size N and M , DTW finds optimal correspondences between data points, such that a given distance function D is minimized. This task can be formulated as finding an alignment between a reference trajectory u and an observed subsequence v. In our specific case, the reference trajectory is the movement of the observed agent during the original demonstration of the task, and the query trajectory is the currently seen partial movement sequence of the human interaction partner. We define an alignment  as a set of tuples (1 , 2 ) specifying a correspondence between point 1 of the first time series and point 2 of the second time series. To find such an alignment, we first calculate the accumulated cost matrix, which is defined as D(1, m) D(n, 1) D(n, m) = = = m k=1 c(u1 , vk ), m  [1 : M ] n k=1 c(uk , v1 ), n  [1 : N ] (5) (6) (7)

where y is a state variable such as the joint angle to be controlled, g is the corresponding goal state, and  is a time scaling factor. The first set of terms represents a criticallydamped linear system with constant coefficients y and y . The last term is called the forcing function f ( x) =
m i=1 i (x)wi x m j =1 j (x)

= (x)T w

(2)

where i (x) are Gaussian basis functions and w the corresponding weight vectors. The basis functions only depend on the phase variable x, which is the state of a canonical system shared by all degrees of feedom (DoFs). The canonical system acts as a timer to synchronize the different movement components. It has the form x  = -x x , where x0 = 1 at the beginning of the motion and, thereafter, it decays towards zero. The elements of the weight vector w are denoted as shape-parameters, as they determine the acceleration profile of the movement, and, hence, indirectly also the shape of the movement. Typically, we learn a separate set of shape parameters w as well as the goal attractor g for each DoF. The goal attractor g can be used to change the target position of the movement while the time scaling parameter  can be used to change the execution speed of the movement. The weight parameters w of the DMP can be straightforwardly obtained from observed trajectories {y1:T , y  1:T , y ¨1:T } by first computing the forcing function that would reproduce the given trajectory, i.e., Fi = 1 y ¨i - y (y (g - yi ) - y  i / ). 2 (3)

min{D(n - 1, m - 1), D(n, m - 1), . . . D(n - 1, m)} + c(un , vm )

Subsequently, we can solve the system w = F in a least squares sense, i.e., w = (T )-1 T F , (4)

where  is a matrix containing of basis vectors for all time steps, i.e., t = T t = (xt ).

where c is a local distance measure, which is often set to the squared Euclidean distance, i.e., c = ||u - v ||2 . In the original DTW formulation, finding the optimal alignment is cast as the problem of finding a path from (1, 1) to (N, M ) producing minimal costs according to the accumulated cost matrix. This optimization is achieved using a dynamic programming recursion. The DTW approach above assumes that both time series have approximately the same length. However, in our case we want to match a partial movement to the reference movement. To this end, we modify the DTW algorithm and determine the path with minimal distance starting at (1, 1)

with µ =
Training Partial Completion

S j =1

 [j ]

S

,

 =

S [j ] j =1 (

- µ)( [j ] - µ)T S

.

(10)

Fig. 3. Phase estimation and pattern completion using DTW and DMPs. Given the partial observation (black), we estimate the current phase, and use it to generate the unseen part (red) of the letter. The goal does not have to be specified and is estimated alongside the other parameters.

In order to obtain a predictive distribution, we observe a partial trajectory  o = y 1:t up to time point t and our goal is to estimate the distribution p( | o ) over the parameters  of the DMP. These parameters can be used to predict the remaining movement y t :T of the observed agent. The updated distribution p( | o ) can simply be computed by applying Bayes rule, i.e., p( | o )  p( o | )p( ). (11)

and ending at (n , M ), where n is given by n = argmin D(n, M ).
n

(8)

The index n reflects the frame in the reference movement which produces minimal costs with respect to the observed query movement. As a result it can be used to estimate the current value of the phase variable x of the canonical system. More specifically, calculating (n /N ) yields an estimated of the relative time that has passed, assuming a constant sampling frequency. Scaling this term nonlinearly yields an estimate of the phase x of the canonical system x = exp -x n N


In order to compute this operation, we first have to discuss how the likelihood p( o | ) can be implemented. The parameters of a DMP directly specify the forcing function, however, we also include the goal positions gi in the forcing function fi for the ith DoF. Therefore, we reformulate the forcing function, i.e., for a single degree of freedom i, we will write the forcing function fi (t) as fi (xt ) = T t w i + y y gi = [T t , y y ] wi gi . (12) (13)



.

(9)

A simple example for the explained phase estimation algorithm can be see Fig 3. The grey trajectories show the demonstrated handwriting samples for which DMPs have been learned. The black trajectories show a new, partial handwriting sample. Using DTW, we can identify the phase of the DMP and then use it to automatically complete the observed letter. To this end, we can set the starting position of the DMP to the last point in the partial trajectory, and set the phase according to the estimated x. By specifying any goal position g , we can generate the missing part of the observed movement. C. Predictive Distributions over DMPs In this section we will introduce predictive distributions over DMP parameters that can be used to predict the behavior of an agent given a partial observed trajectory. We will first describe how to generate such predictive distribution for a single agent and later show in the next section how this model can be easily extended to infer a control policy for the controlled agent in an interaction scenario. In our probabilistic approach, we model a distribution p( ) over the parameters of a DMP. In order to also estimate the target position of the movement, we include the shape parameters wi as well as the goal attractors gi for all DoFs i in the parameter vector  of the DMP, i.e., T  = [wT 1 , g1 , . . . , w N , gN ], where N is the number of DoFs for the agent. Given the parameter vector samples  [j ] of multiple demonstrations j = 1 . . . S , we estimate a Gaussian distribution over the parameter  , i.e., p( ) = N ( |µ ,  ),

The forcing function can be written in matrix form for all DoFs of the observed agent and for all time steps 1  t  t , i.e.,    ~  w1  0 .....   ~ 0 . .   g1   0     . =  , (14) F = .  . . . . .  .   . . . . . .  .  wN  ~ 0 .....  gN


~ t,· = [T , y y ] of  ~ contains the basis where the t-th row  t functions for time step t and a constant as basis for the goal attractor gi . The matrix  contains the basis functions for all DoFs on its block-diagonal. The vector F contains the value of the forcing function for all time steps and all degrees T T of freedom, i.e., F = [f T 1 , . . . , f N ] , where f i contains the values of the forcing function for all time steps for the ith DoF. By concatenating the forcing vectors f i for the single DoFs and by writing  as a block-diagonal matrix, we achieve that the parameters of all DoF can be concatenated in the vector  . Given an observed trajectory y 1:t , we can also compute the forcing function vector F  from the observation, i.e., the elements of the F  vector are given by 1 ¨i (t) - y (-y oi (t) - o i (t)/ ). (15) fi (xt ) = 2 o  Now, we can use a Gaussian observation model for the likelihood p( o | ) = N (F  | ,  2 I ), (16) where  2 is the observation noise variance which will act as regularizer in the subsequent conditioning.

0.5 0

attraction-point

goal

attraction-point

y-axis [m]
Training Partial Completion
Fig. 4. Given a set of training trajectories (in gray) we learn a predictive distribution over the DMP weights. The distribution can then be used to sample new trajectories with a similar shape. In this example, DTW is used to determine the current phase of a partially observed trajectory (black). The completions of this trajectory are performed by estimating the most likely distribution of DMP weights.

-0.5 -1 -1.5 -2

-2

-1.5

-1

-0.5

0

0.5

1

1.5

2

Observed Agent

x-axis [m]

Controlled Agent

Fig. 5. A simple simulation setup for studying Interaction Primitives. Two opposing (robot) arms of different kinematic structure (2 links vs. 3 links) execute a high-five movement to a specific goal. Using optimal control methods we can calculate optimal trajectories that reach the goal, while at the same time being attracted by "attraction points". The resulting data set contains strong correlations between the movements of the two agents.

In order to perform the conditioning, we first write p( o ,  ) = p( o | )p( ) as a joint Gaussian distribution and, subsequently, condition on  o . The joint distribution is given by p( o ,  ) = N F  A  , µ  T   (17)

with A =  2 I +  T . The conditional distribution p( | o ) is now also Gaussian with mean and variance µ| o  |  o = µ +  T A-1 (F  - µ ), =  -  T A-1  . (18)

~  , with  ~ = [, 0]. The conditioning equations given F = in (18) can now be applied straightforwardly by replacing  ~ . The estimated parameter vector  can, thus, be used with  to to predict the remaining movement of the observed agent (using  o ) but also to control the robot in the current situation (using  c ). Hence, its behavior is always related the behavior of the human interaction partner. IV. E XPERIMENTS To evaluate our approach under controlled and varying conditions, we designed a simple simulation environment that can be used to study interaction scenarios. The simulation consists of two opposing robot arms with different kinematic structures as can be seen in Fig. 5. The robot on the left side (the observed agent) consists of two links which are connected through a hinge joint, while the robot on the right side (the controlled agent) has three links and two joints. The task of the robots is to execute a high-five movement. In order to generate training data for this task we first synthesize a set of training trajectories for both agents. Using optimal control we determine for each agent a joint angle trajectory which brings it from its start position to a specified goal position. Attractor points are added in order to generate realistic-looking high-five movements. During the synthesis of training data, both the goal position and the attractor positions are varied. Given this training data, we learn an IP capturing the mutual dependencies of the agents during the high-five movement. After learning, we the use the IP to control the three linked robot arm. Given partial joint angle trajectories of the observed agent, we use the IP and conditioning to (1) determine the most likely current goal, (2) the ideal joint angle configurations of the controlled agent. Fig. 6 depicts task space trajectories for the controlled agent after conditioning. On the left side we see the results of conditioning when 40% of the movement of the observed agent is seen. On the right side are the results after conditioning with 60% of the data. Each trajectory is a possible

Using the above distribution we can compute the most likely weights µ| o of the DMPs for any observed partial movement. In Fig. 4, we see an example of using the above procedure for estimating weights from partial observations. On the left side, we see the demonstrations that have been used to train the DMPs. The different demonstrations reflect different versions of the same alphabet letter. On the right side, we see the partial observation (black) of a new handwritten sample as well as the automatic reconstruction using a DMP with estimated weights. D. Correlating two Agents with Predictive Distributions Correlating the controlled agent with its interaction partner is now a straightforward extension of the predictive DMP framework. We now assume that we have two agents, the observed and the controlled agent. In order to capture the correlation between the agents, we use a combined parameter T T vector  = [ T o ,  c ] which contains the DMP parameters  o of the observed and the parameters  c of the controlled agent. We can now use the approach for obtaining a predictive distribution p( | o ), however, in contrast to the previous section, the observed trajectory only contains the DoFs of the observed agent. Hence, in order to write the forcing vector F =  o in terms of the complete parameter vector  , we need to append a zero-matrix to the feature matrix , i.e.,

0.0
2

-0.1
y-axis [m]
1

y -a x is [m ]
25 50 75 100 0 25 50 75 100

-0.2 -0.3 -0.4 -0.5 -0.6 1.5 1.6 1.7 1.8 1.9

0

0

x-axis [m]

Fig. 6. The uncertainty over the goal position shrinks with increasing amount of data points. Left: distribution after observing 40% of the partners movements. Right: distribution after observing 60% of the partners movements
16 14 12 10 6 4 2 0 0 10 20 30 40 50 60 70 80 90 100

x - ax i s[ m ]

Controlled Observed

Fig. 8. Difference between ground truth and predicted task space goals. Blue circles show the true positions of the goals. Red circles depict the predicted goals after observing 60% of the interaction.

MSE

Fig. 7. Mean squared error based on different percentage of partially observed trajectories of the interaction partner. The red curve shows the accuracy of predicting the movements of the observed agent from partial trajectories. The black curve shows the accuracy in inferring the right reaction in response to the observed movement.

Middle

Percentage of trajectory [%]

Side

8

(a)

(b)

(c)

prediction of the task space movement of the agent. The figure shows that the uncertainty significantly shrinks when we transition fom 40% to 60% in this example. To further analyze the accuracy of prediction, we generated a new test data set consisting of interactions to different goal positions that were not part of the original training data. We then calculated the mean squared error (MSE) between the predicted joint angle trajectories generated from the IP and the ground-truth data. Fig. 7 shows the evolution of the MSE for observed partial trajectories of increasing size. We can see that the MSE in the prediction of the observed agent significantly drops at around 20% and again at 60%. A similar trend, albeit with higher variance, can also be seen in the predictions for the ideal response of the controlled agent. The plot suggests that after seeing 60% of the movement of the interaction partner we can already roughly infer the goal he is aiming at. We also analyzed the difference in task space between the inferred and the ground-truth data for the controlled agent. More specifically, we evaluated how far the predicted goal is from the true goal of the trajectory. Fig. 8 shows the true goals and the inferred goals in task space after seeing 60% of the movements of the observed agent. We can see that the predicted goals are in close proximity to the true goal locations. Please note, that the depicted positions are in task space while the inferred values are in joint space.

Fig. 9. Two reactions synthesized from an Interaction Primitive for a handing-over task. The humanoid on the left side (controlled agent) is controlled through the IP and has to receive an object from the humanoid on the right side. Depending on the location where the object is handed over, the reaction of the controlled agent is changed. Training of the IP was performed using motion capture data of two humans.

The depicted position is calculated by performing forward kinematics using the inferred joint angle data. As a result, even small errors in the joint angle of the first link will propagate and produce larger errors at the end-effector. Still, the results indicate that the approach can be efficiently used for intention inference in human-robot tasks. By predicting the most likely goal of the human interaction partner, the robot can proactively initiate his own response. We also conducted experiments using real motion capture data collected from the interactions of two human subjects. In a first experiment we asked two humans to demonstrate a "handing-over" in which the first subject hands a cup to the second subject. The demonstrations were then used to learn an IP. However, in order to cope with the high dimensionality of the dataset, we applied Principal Component Analysis (PCA) as a pre-processing step, in order to project the data onto a five-dimensional space. After training, we tested the learned IP on a set of data points that have not been used during training. Fig. 9 shows two example situations, where the controlled agent (left humanoid) automatically infers the optimal reaction to the behavior of his interaction partner

Fig. 10. A frame sequence from a high-five interaction between a human and a humanoid. The robot automatically reacts to the movement of the human and estimates the appripriate location of the executed high-five. The human interaction partner is tracked using an OptiTrack motion capture system.

(right humanoid). In the first sequence the controlled agent correctly turns to the left side to receive an object. In contrast to that, in the second sequence, the agent reaches for the middle in order to properly react to the observed movement. Finally, we performed a set of interaction experiments on a real humanoid robot. The humanoid has two arms with 7 DoF each. During the experiment we used one arm with four DoFs. More specifically, we trained an Interaction Primitive for the high-five. Again, we collected motion capture data from two humans for training this IP. After training, the robot used the IP to predict the joint configuration at the goal position as well as the weight parameters of the DMP. Fig. 10 shows an example interaction realized via the presented approach. Using prediction in this task is important, as it helps the robot to match the timing of the human interaction partner. Notice that the starting location of the robot is quite far from the rest poses in the learning database. V. C ONCLUSION In this paper, we proposed the novel Interaction Primitive framework based on DMPs and introduced a set of basic algorithmic tools for synchronizing, adapting, and correlating motor primitives between cooperating agents. The research introduced here lays the foundation for imitation learning methods that are geared towards multi-agent scenarios. We showed how demonstrations recorded from two interacting persons can be used to learn an interaction primitive, which specifies both the executed movements, as well as the correlations in the executed movements. The introduced phase estimation method based on dynamic time warp proved to very important for applying a learned interaction primitive in new situations. Timing is a highly variable parameter, which varies among different persons, but can also vary depending on the current mood or fatigue. In future work, we plan to concentrate on more complex interaction scenarios, which are composes of several interaction primitives. These primitives could executed in sequence or in a hierarchy in order to produce complex interactions with a human partner. We are also already working on using the interaction primitive framework for predicting the most likely future movements of a human interaction partner. The underlying idea is that the same representation which is

used for movement synthesis can also be used for movement prediction. The predicted actions of the human could then be integrated into the action selection process of the robot, in order to avoid any dangerous situations. VI. ACKNOWLEDGEMENTS The work presented in this paper is funded through the Daimler-and-Benz Foundation and the European Communitys Seventh Framework Programme under the grant agreement n ICT-600716 (CoDyCo). R EFERENCES
[1] P. Abbeel, A. Coates, and A. Ng. Autonomous Helicopter Aerobatics through Apprenticeship Learning. International Journal of Robotic Research, 29:1608­1639, 2010. [2] A. Billard, S. Calinon, R. Dillmann, and S. Schaal. Survey: Robot Programming by Demonstration. In Handbook of Robotics, volume chapter 59. MIT Press, 2008. [3] S. Calinon, E.L. Sauser, A.G. Billard, and D.G. Caldwell. Evaluation of a probabilistic approach to learn and reproduce gestures by imitation. In Proc. IEEE Intl Conf. on Robotics and Automation (ICRA), pages 2381­2388, Anchorage, Alaska, USA, May 2010. [4] R. Chalodhorn, D. Grimes, K. Grochow, and R. Rao. Learning to walk through imitation. In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI'07, pages 2084­2090, San Francisco, CA, USA, 2007. Morgan Kaufmann Publishers Inc. [5] D. Grimes, R. Chalodhorn, and R. Rao. Dynamic imitation in a humanoid robot through nonparametric probabilistic inference. In In Proceedings of Robotics: Science and Systems (RSS). MIT Press, 2006. [6] A. Ijspeert, J. Nakanishi, H. Hoffmann, P. Pastor, and S. Schaal. Dynamical movement primitives: Learning attractor models for motor behaviors. Neural Comput., 25(2):328­373, February 2013. [7] S. Ikemoto, H Ben Amor, T. Minato, B. Jung, and H. Ishiguro. Physical human-robot interaction: Mutual learning and adaptation. IEEE Robotics and Automation Magazine, 19(4):24­35, Dec. [8] D. Lee, C. Ott, and Y. Nakamura. Mimetic communication model with compliant physical contact in human-humanoid interaction. Int. Journal of Robotics Research., 29(13):1684­1704, November 2010. [9] M. M¨ uhlig, M. Gienger, and J. Steil. Interactive imitation learning of object movement skills. Autonomous Robots, 32:97­114, 2012. [10] D. Pomerleau. ALVINN: an autonomous land vehicle in a neural network. In David S. Touretzky, editor, Advances in Neural Information Processing Systems 1, pages 305­313. San Francisco, CA: Morgan Kaufmann, 1989. [11] H. Sakoe and S. Chiba. Dynamic programming algorithm optimization for spoken word recognition. Acoustics, Speech and Signal Processing, IEEE Transactions on, 26(1):43­49, 1978. [12] S. Schaal. Is imitation learning the route to humanoid robots? Trends in Cognitive Sciences, 3:233­242, 1999. [13] Z. Wang, M. Deisenroth, H. Ben Amor, D. Vogt, B. Schoelkopf, and J Peters. Probabilistic modeling of human dynamics for intention inference. In Proceedings of Robotics: Science and Systems (R:SS), 2012.

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/265605707

DynamicModeDecompositionforPerturbation EstimationinHumanRobotInteraction
ConferencePaper·August2014
DOI:10.13140/2.1.2957.7601

CITATIONS

READS

4
5authors,including: ErikBerger TechnischeUniversitätBergakademieFreiberg
18PUBLICATIONS52CITATIONS
SEEPROFILE

103

DavidVogt TechnischeUniversitätBergakademieFreiberg
20PUBLICATIONS86CITATIONS
SEEPROFILE

HeniBenAmor ArizonaStateUniversity
56PUBLICATIONS434CITATIONS
SEEPROFILE

Someoftheauthorsofthispublicationarealsoworkingontheserelatedprojects:

Mining-RoX:AutonomousRobotsinUndergroundMiningViewproject

AllcontentfollowingthispagewasuploadedbyErikBergeron15September2014.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Dynamic Mode Decomposition for Perturbation Estimation in Human Robot Interaction
Erik Berger1 , Mark Sastuba2 , David Vogt1 , Bernhard Jung1 , Heni Ben Amor3
Abstract-- In many settings, e.g. physical human-robot interaction, robotic behavior must be made robust against more or less spontaneous application of external forces. Typically, this problem is tackled by means of special purpose force sensors which are, however, not available on many robotic platforms. In contrast, we propose a machine learning approach suitable for more common, although often noisy sensors. This machine learning approach makes use of Dynamic Mode Decomposition (DMD) which is able to extract the dynamics of a nonlinear system. It is therefore well suited to separate noise from regular oscillations in sensor readings during cyclic robot movements under different behavior configurations. We demonstrate the feasibility of our approach with an example where physical forces are exerted on a humanoid robot during walking. In a training phase, a snapshot based DMD model for behavior specific parameter configurations is learned. During task execution the robot must detect and estimate the external forces exerted by a human interaction partner. We compare the DMDbased approach to other interpolation schemes and show that the former outperforms the latter particularly in the presence of sensor noise. We conclude that DMD which has so far been mostly used in other fields of science, particularly fluid mechanics, is also a highly promising method for robotics.

I. I NTRODUCTION Robots need accurate and efficient sensing capabilities in order to react to influences from the environment. This is particularly true for robots that are engaging in joint physical activities with a human partner. In such scenarios, forces and torques applied by the human can severely perturb the execution of a motor skill and need to be accounted for in the decision making process. In order to appropriately respond to a perturbation, a robot needs to detect both the occurrence of such an event as well as the degree by which it occurred. One way of implementing such detection is to use readings from a special purpose sensor, e.g., force-torque sensor, along with a thresholding method. However, such sensors are often heavy, expensive and prone to error. In practice many sensors return non-zero readings even when the robot merely moves. Distinguishing between external, human perturbations and natural variation in the sensor values can therefore become a challenging task. In this paper, we present a machine learning approach to robot sensing that is well suited for identifying external influences caused by a human partner as shown in Figure 1.
1 Institute of Computer Science, Technical University Bergakademie Freiberg, Bernhard-von-Cotta-Str. 2, 09599 Freiberg, Germany 2 Institute of Mechanics and Fluid Dynamics, Technical University Bergakademie Freiberg, Lampadiusstr. 4, 09599 Freiberg, Germany 3 Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, 801 Atlantic Drive, Atlanta, GA 30332-0280, USA

Fig. 1: A NAO robot detects the existence and amount of external perturbations applied by a human interaction partner. The direction and amount of the perturbation is used to infer human guidance, e.g., walk backwards.

The approach focuses on learning probabilistic, behaviorspecific models of regular oscillations in sensor readings during motor skill execution. These models are used to (1) identify perturbations by detecting irregularities in sensor readings that cannot be explained by the inherent noise, and (2) to generate a continuous estimate of the amount of external perturbation. Due to the data-driven nature of the approach, no detection threshold needs to be provided by the user. The presented perturbation filter can be regarded as a virtual force sensor that produces a continuous estimate of external forces. To this end, we use Sparse Dynamic Mode Decomposition to learn a model of the system dynamics during the robot execution of a specific motor skill. During human-robot interaction, the model is then used to determine the existence and amount of irregularities in the sensor readings. By modeling the correlations as well as the timedependent variation in the original sensor values, our filter can robustly deal with uncertainties in estimating the human physical influence on the robot. During task execution, the estimated perturbation value can be used to compensate for the external forces or infer the intended guidance of a human interaction partner. Experiments on a real robot show that

External Perturbation

Perturbation Filter
Perturbation Detection Perturbation Estimation
Prediction

Reaction

Sensor Data

Behavior Behavior Parameter Parameter

Costs

Perturbation Value

Evaluation

Fig. 2: An overview of the presented machine learning approach. An external perturbation is filtered by using a previously learned predictive model of behavior parameters. After detecting a perturbation the strength and direction is estimated in the behavior parameter space. The resulting perturbation value can be used for an adequate reaction.

learned models can be used to accurately determine even small disturbances. II. R ELATED W ORK A popular approach to the design of human-robot interaction (HRI) is the use of mediating artefacts, such as pendants, joysticks or mobile phones [1]. The approach allows a programmer to pre-specify a set of tasks, commands and corresponding robot reactions. Since communication is mediated through the artefact, no filtering or interpretation of the human commands is required. In recent years, more natural and intuitive approaches to HRI have gained popularity. Various researchers have proposed the so-called soft robotics paradigm: compliant robots that "can cooperate in a safe manner with humans" [2]. An important robot control method for realizing such a compliance is impedance control [3]. Impedance control can be used to allow for touch based interaction and human guidance. To this end, impedance controllers require accurate sensing capabilities, in the form of force-torque sensors. However, such sensors are typically heavy, expensive and suffer from noise. Other sensors, such as current based torque sensors are even more prone to issues related to noise and drift. Still, the ability to sense physical influences is at the core of recent advances made in the field of HRI. For example, Lee et al. [4] use impedance control and force-torque sensors in order to realize human-robot interaction during programming by demonstration tasks. Wang et al. [5] present a robot that can adapt its dancing steps based on the external forces exerted by a human dance partner. Ben Amor et al. [6] uses touch information to teach new motor skills to a humanoid robot. Touch information is therefore only used to collect data for subsequent learning of a robotic motor skill. Robot learning approaches that are based on such kinesthetic teachin have gained considerable attention in the literature, with similar results reported in [7] and [8]. A different approach aiming at joint physical activities between humans and robots has been reported in [9]. Ikemoto et al. use Gaussian mixture models to adapt the timing of a humanoid robot to that

of a human partner in close-contact interaction scenarios. The parameters of the interaction model are updated using binary evaluation information obtained from the human. The approach significantly improves physical interactions, but is limited to learning timing information. St¨ uckler et al. [10] present a cooperative transportation task where a robot follows the human guidance using arm compliance. In doing so, the robot recognizes the desired walking direction through visual observation of the object being transported. A similar setting has been investigated by Yokoyama et al. [11]. They use a HRP-2P humanoid robot equipped with a biped locomotion controller and an aural human interface to carry a large panel together with a human. Forces measured with sensors on the wrists are utilized to derive the walking direction. Similarly, Bussy et al. [12] also use force-torque sensors on the wrists to adapt the robot behavior during object transportation tasks. Lawitzky et al. [13] also shows how load sharing and role allocation can be used to balance the contribution of each interaction partner depending on the current situation. The main drawback of the above approaches is that they require special aural and visual input devices or force sensors which are not present on many robot platforms. Additionally, none of the approaches using force-torque sensors addresses the problem of uncertainty in the provided measurements. As a result, all of these approaches assume high-quality sensing capabilities and low-speed execution of the joint motor task. In contrast to the above approaches, we propose a new filtering algorithm that can learn the natural variation in sensor values during a motor skill. Using predictive models learned by Dynamic Mode Decomposition, the filtering algorithm also estimates the perturbation which best explains an observed set of new sensor values. III. APPROACH In our approach the robot recognizes and automatically differs between strength and direction of external perturbations which may be caused by a human interaction partner. An overview of the approach can be seen in Figure 2. First,

we record training data for a behavior with different parameter configurations, e.g. walking with varying step lengths, in a controlled environment without external perturbations. The training data is used to learn a dynamic model utilizing a state of the art interpolation method from fluid dynamics (DMD). During behavior execution an external perturbation is detected by comparing the recorded training data with the current sensor data. For estimating the perturbation value the current sensor readings are compared to new sensor values generated from the learned model. The perturbation value is then calculated from the difference between the current behavior parameter and the behavior parameter of the sensor characteristic with the highest compliance to the current sensor characteristic. In the following, we will address each step of our approach in more detail. Subsequently, we will describe how perturbation detection, model learning and perturbation estimation are realized in order to allow a whole variety of HRI scenarios. A. Recording Training Data The first step in our approach is to record training data that reflects the evolution of sensor values during regular execution of a motor skill. It is important to record several executions of the behavior, since motor skills can often be executed with different parameters, e.g., varying step lengths during walking. However, since we use machine learning methods, we will later see that the number of required training data can be limited to about five examples. Each recorded example contains training data sampled with 100Hz for one repetition of the modelled robot behavior. In our specific case of training a perturbation filter for walking, we record both the center of mass (CoM) and the proper acceleration of the robot for four seconds. Acquiring training data requires less than one minute in total. B. Phase Estimation Since we are dealing with time-varying data, it is important to estimate the phase of the robot during the execution of a motor skill. Depending on the phase, e.g., the left leg is lifted, the variance in the sensor readings can change drastically. To determine the current phase, a time window of sensor values is captured and temporally aligned to the training data. To this end, we use the dynamic time warping technique (DTW) [14]. DTW is a time series alignment algorithm for measuring the similarity between two temporal sequences X = (x1 , . . . , xN ) and Y = (y1 , . . . , yM ) of length N  N and M  N. In our specific case, the goal is to find the optimal correspondence between the sensor data Y recorded during the training phase and the currently observed sequence X, where M is much larger then N . Due to this significant difference in length of X and Y, we formulate our task as finding a subsequence Y(a : b ) = (ya , ya +1 , . . . , yb ) (1)

a*

p* b* Y X

Fig. 3: Given the recorded data (black) and the partial observation (red), we calculate the optimal warping path p between a and b .

subsequence X. This technique is also known as subsequence dynamic time warping (SDTW) [15]. To find the optimal subsequence we first have to calculate the accumulated cost matrix D, which for SDTW is defined as
n

D(n, 1) =
k=1

c(xk , y1 ), n  [1 : N ],

D(1, m) = c(x1 , ym ), m  [2 : M ], D(n, m) = min{D(n - 1, m - 1), D(n - 1, m), D(n, m - 1)} + c(xn , ym ) where c is a local distance measure, which in our case is defined as c = |x - y |. The goal of the SDTW algorithm is to determine the path with minimal overall costs C ending at (b , M ), where b is given by b = argmin D(N, b).
b[1:M ]

(2)

To determine the warping path p = (p1 , . . . , pL ) starting at p1 = (a , 1) and ending at pL = (b , M ) a dynamic programming recursion is used. As illustrated in Figure 3 the resulting path p represents the optimal subsequence of X in Y. As a result SDTW can be used to estimate the current state of a behavior using a subset of temporally measured sensor values which are mapped to the recorded data. In more detail, we use the subsequence p as prediction of sensor values at the current state. C. Perturbation Detection Due to uncertainties in the real world, a motor skill is never twice executed in exactly the same way. To accommodate for such natural noise in the behavior, we use learned, behaviorspecific information about the temporal evolution of sensor variances. Different approaches can be used to learn such a probabilistic model. One solution is to use Gaussian Process Regression (GPR) [16]. An important advantage of GPR is the ability to learn a probabilistic model from a small set of training

with 1  a  b  M , where a is the starting index and b is the end index that optimally fits to the corresponding

Standard Deviation Sensor Value

Prediction

Measurement

0

10

20

30

40

50

0

10

20

30

40

50

Time Step [1/100s]

Fig. 4: After estimating the current phase of the behavior the deviation between the measured and predicted sensor values can be used to detect external influences. Left: There is no external perturbation. Right: An external perturbation is detected.

knowledge of A is not required for the following variant: xN = a0 x0 + a1 x1 + · · · + aN -1 xN -1 + r. The final snapshot xN can be expressed as a linear combination of the previous ones [x0 , . . . , xN -1 ] by computing the weighting factors [a0 , . . . , aN -1 ], considering the residual r is minimized in a least squares sense, to form the companion matrix   0 a0 1 0 a1      . .. .. .  CN × N . (3) S= . . .      1 0 aN -2 1 aN -1 In [17] the author describes a more robust solution, which is achieved by applying a singular value decomposition on K1 ~  CN × N such that K1 = U W  . The full-rank matrix S is determined on the subspace spanned by the orthogonal ~ = U  K2 W -1 . basis vectors U of K1 , described by S ~ Solving the eigenvalue problem Sµ = µ leads to a subset of complex eigenvectors µ. The DMD modes are defined by  = U µ, which implies a mapping of the eigenvectors µ  CN ×N from a lower dimensional space to a higher dimensional space CM ×N . The complex eigenvalues  contain growth/decay rates  = [log()]/t and frequencies f = [log()]/(2 t) of the corresponding DMD modes . The temporal behavior of the DMD modes is contained in the Vandermonde matrix Vand , which is formed by  -1  1 1 · · · N 1 1 2 · · · N -1  2   (4) Vand =  . . . . . . . . . . . . .  1 N ···
-1 N N

data. The main drawback of this approach is the large computational effort. Another, computationally less expensive solution is to compute the standard deviation  for each time step of the recorded data separately. Given a probabilistic model as described above, we can detect a perturbation by calculating the likelihood of the current sensor readings. In our implementation, we trigger a detection when the sensor values are outside of the computed standard deviation  . Figure 4 shows an example for a regular and a disturbed execution of a behavior. D. Modelling Robot Dynamics using Dynamic Mode Decomposition In this section we use Dynamic Mode Decomposition (DMD) to learn a predictive model describing the change in sensor values under different behavior parameters. DMD is a novel data processing technique from fluid dynamics and was introduced in [17] and [18]. Once a DMD is learned, it can be applied to simulated sensor values under different parameter conditions. DMD presents a modal decomposition for nonlinear flows and features the extraction of coherent structures with a single frequency and growth/decay rate. It computes a linear model which approximates the underlying nonlinear dynamics. Given is an equidistant snapshot sequence N + 1 of an observable x = (u1 , . . . , uM )  CM ×1 which is stacked into two matrices K1 = [x0 . . . , xN -1 ]  CM ×N and K2 = [x1 . . . , xN ]  CM ×N . The matrices K1 and K2 are shifted by one time step t and can be linked via the mapping matrix (system matrix) A  CM ×M such that K2 = AK1 = K1 S . Since the data stem from experiments, the system matrix A is unknown and for a very large system it is computationally impossible to solve the eigenvalue problem directly as well as to fulfill the storage demand [19]. The idea is to solve an approximate eigenvalue problem by projecting A onto an N -dimensional Krylow subspace and to compute the eigenvalues and eigenvectors of the resulting low-rank operator as described in [20]. One type of Krylow methods is the Arnoldi algorithm and the

The DMD modes  must be scaled in order to perform a data recalculation of the first snapshot sequence K1 = D Vand . Therefore, having a look into Vand shows that the first snapshot x0 is independent from temporal behavior since  = [1 , . . . , N ] = 1. The scaling factors  = [1 . . . N ] are calculated by D = x0 , where D = diag {a}. A new solution to find the scaling vectors  was introduced in [21]. Here,  is obtained by considering the temporal growth/decay rates of the DMD modes in order to approximate the entire data sequence K1 optimally. Therefore the problem can be brought into the following form min J () = |W  - µD Vand |F
 2

(5)

which is a convex optimization problem. Its solution leads to
 ))-1 diag (V   = ((µ µ)  (Vand Vand and W  µ)

(6)

where the over line denotes the complex-conjugate of a vector/matrix. However, the key challenge is to identify a subset of DMD modes that captures the most important dynamic structures in order to achieve a good quality approximation. To solve that problem, the sparsity-promoting dynamic mode decomposition (SDMD) [21] was developed. The sparsity structure of the vector of amplitudes  is fixed in order

Standard Deviation Sensor Value

Prediction

Measurement

4

6

8

10

12
-3

400

Trained CoM

x 10

Time Step [1 / 100 s]

300

200

100

C

-4
100 200

-2

0

2

4

Time Step [1/100s]

300

400

500

600

400

Interpolated CoM

Time Step [1 / 100 s]

Fig. 5: External perturbations which differ in strength and direction are increasing the overall warping costs C during behavior execution.

300

200

to determine the optimal values of the non-zero amplitudes. Therefore the objective function J () is extended with an additional term such that
N

100

min J () + 
 i=1

|i | ,

(7)
-4 -2 0 2 4

Step Length [ cm ]

where  denotes a regularization parameter that focuses on the sparsity of the vector . As a result, instead of considering only the modes with largest amplitudes, the sparsity-promoting DMD aims to identify the modes that have the strongest influence on the entire time sequence. The lower the number of non-zero amplitudes, the more the sparse-promoting DMD concentrates on the low-frequency modes. As already mentioned, the data presented here stems from low cost sensors which may be affected by disturbance. Hence, forcing a low number of non-zero amplitudes in  can reduce the influence of noise in the approximation. For our implementation of DMD in a human robot interaction scenario, the snapshot data N +1 is represented by the sensor data recorded during training data acquisition. Each column of the snapshot matrices K1 and K2 contains a fixed number of sensor values, i.e., the longitudinal CoM. E. Calculating a Continuous Measure of Perturbation If the deviation between measured and predicted sensor value is larger than the allowed variance  we assume that an external perturbation is influencing the execution of the behavior. However, the question remains: how strong is the external perturbation? To estimate the strength of the perturbation, we simulate different behavior parameters using the learned DMD model and select the one that produces sensor values similar to our

Fig. 6: DMD is used to generate new sensor values for unknown parameter settings. Top: The training data which consists of five equidistant samples of the longitudinal CoM during walking. Bottom: The longitudinal CoM is interpolated with an interval of 0.01cm resulting in predictions for 800 possible parameter configurations.

current readings. For this task, we make use of the previously described SDTW method. As mentioned the SDTW finds the optimal warping path p for a currently measured subsequence X to a previous recorded dataset Y. Whenever a perturbation is detected, we perform iterative optimization by generating predictions using a DMD model and calculating the warping costs using SDTW. The goal of this optimization process is to identify the behavior parameter that would best explain the currently observed sensor values. Optimization is performed using a stochastic optimization technique, i.e, Covariance Matrix Adaptation Evolution Strategy (CMAES). The warping costs C generated by SDTW are used as objective function. Figure 5 shows the warping costs C calculated during a walking task. The behavior parameter which produces least costs C is regarded as the true behavior parameter if human forces are taken into account. Accordingly, we can generate an estimate

20 DMD SDMD LWR Spline Cubic

Standard Deviation a

Prediction

Measurement b

15 M RE

10

longitudinal CoM

c

d

5

0

CoM

Proper Acceleration

Time Step

Fig. 7: The DMD techniques are compared with a set of classical interpolation schemes. Left: The DMD shows the highest accuracy for the CoM. Right: In presence of high noise, which is the case for proper acceleration estimates, SDMD produces higher accuracy than DMD or classical interpolation schemes.

Fig. 9: Perturbation detection during walking using the robot's longitudinal CoM. Top Left: Slight push from the front. Top Right: Strong push from the front. Bottom Left: Slight push from the back. Bottom Right: Strong push from the back.

for the human forces by calculating the difference between the behavior parameter used to control the robot and the behavior parameter identified by the learned model. IV. E XPERIMENTS In the following experiments we use DMD, SDMD and classical interpolation schemes to learn a model of a robot's walking gait. Furthermore, we evaluate and compare the quality of each of these models. The best model is then used to detect and estimate external perturbations during a human robot interaction task. A. Prediction Quality For the evaluation of DMD and SDMD we make use of a walking dataset recorded on a Nao robot. The longitudinal CoM was recorded for a walking gait with five different equidistant step lengths between -4cm and +4cm. The data is recorded with 100Hz for four seconds. Both the DMD and SDMD algorithms were applied on this dataset, resulting in four DMD modes. Given the learned models, the goal is to generate new sensor values for step lengths that were not recorded during training. Figure 6 shows the five training samples of the longitudinal CoM and the generated model which was interpolated with an interval of 0.01cm. To evaluate the precision of the generated data we additionally recorded test samples with step lengths in an interval of 1cm and measured their mean relative error MRE w.r.t. the corresponding generated data. We also compared the results with a set of classical interpolation schemes. For the CoM, Figure 7 shows that DMD results in the highest accuracy among all methods. SDMD reduces the number of used modes to three and results in a slightly less accurate model. Since, we want to work with low cost sensors which may have significant noise, we additionally recorded the robot's longitudinal acceleration and applied

the same data generation techniques as above. Again, the original DMD uses all extracted modes to predict new sensor values. However, these predictions are corrupted by the fact that some of these extracted modes mainly contain noise. As a result, the prediction performance of DMD deteriorates to about the same level as classical interpolation schemes. In contrast, SDMD concentrates on the three DMD modes that best approximate the sensor data. In this case, one mode was set to zero which obviously contained strong noise. However, because of its smaller MRE, we use the DMD model in conjunction with the CoM for the following experiments. B. Perturbation Detection In the following experiments we detect external perturbations while the robot performs a walking gait with a step length of 0.5cm for 35 seconds. During slowly walking the human perturbs the robot by touching and pushing it as shown in Figure 8. While Figure 8a, 8c show slight pushes, which just marginally disturb the walking gait. We also applied strong pushes as shown in Figure 8b, 8d. Especially, the strong push from the back as shown in Figure 8d significantly affected the robot`s stability during walking. We use the DMD model to generate the predicted sensor values for the current step length. During behavior execution the longitudinal CoM is measured with 100Hz and saved in a sliding window with 10 measurements. To estimate the current walking phase, we calculate the optimal warping path from this subsequence in the predicted data using SDTW. The resulting path is used as time dependent prediction of the longitudinal CoM for the currently measured values. Figure 9 shows the measured and predicted longitudinal CoM for the external perturbations a-d as shown in Figure 8. A perturbation is detected when the measured longitudinal CoM is outside the variance of the predicted one.

(a) Slight Push Front

(b) Strong Push Front

(c) Slight Push Back

(d) Strong Push Back

Fig. 8: The human touches and pushes the robot during the execution of a walking behavior. The estimated perturbation values differ in strength and direction and reflect the amount of force applied on the robot.

Perturbation Value [cm]

2 1 0 -1 -2 0

a

b

c

d

500

1000

Time Step [1 / 100s ]

1500

2000

2500

3000

3500

Fig. 11: The perturbation value for the external perturbation a-d is the difference between the predicted parameter with minimal costs and the current behavior parameter. Perturbation d produces a large oscillation which is dampened over time.
a
0.2

C. Perturbation Estimation
0.15 0.1 0.05 0 0.4 0.3

b

If a perturbation is detected, we have to find another behavior parameter and its corresponding sensor evolution, which has minimal mapping costs C for the SDTW. As mentioned before, there are several different approaches for this minimization problem. However, to prove the correctness of our approach we compute C for each step length of the learned DMD model. Figure 10 shows the overall costs C for all possible step lengths of our DMD model during the peeks of the external perturbations as shown in Figure 8ad. Obviously, pushes from the front produces minimal costs for negative step lengths while pushes from the back lead to positive step lengths. As a result, the parameters with minimal costs can be seen as behavior parameters which counteract the external perturbation. Finally, the perturbation value is calculated from the difference of the current step length of 0.5cm and the predicted step length. Since the behavior parameter is specified in cm the measuring unit for the perturbation value is also in cm. The perturbation value for the complete behavior execution is shown in Figure 11.

C

c

d

C

0.2 0.1 0 -4 -2 0 2 4 -4 -2 0 2 4

Step Length [cm]

Fig. 10: The overall costs C for all possible parameters during the peaks of the external perturbations a-d. The step length which produces the minimal costs (black crosses) is the predicted step length which is used to calculate the perturbation value.

of this approach to industry-grade robots and collaborative assembly tasks. R EFERENCES
[1] P. Rouanet, P.-Y. Oudeyer, F. Danieau, and D. Filliat, "The Impact of Human-Robot Interfaces on the Learning of Visual Objects," IEEE Transactions on Robotics, vol. 29, no. 2, pp. 525­541, Apr. 2013. [2] A. Albu-Sch¨ affer, O. Eiberger, M. Fuchs, M. Grebenstein, S. Haddadin, C. Ott, A. Stemmer, T. Wimb¨ ock, S. Wolf, C. Borst, and G. Hirzinger, "Anthropomorphic soft robotics from torque control to variable intrinsic compliance," in Robotics Research, ser. Springer Tracts in Advanced Robotics, C. Pradalier, R. Siegwart, and G. Hirzinger, Eds. Springer Berlin Heidelberg, 2011, vol. 70, pp. 185­207. [3] S. Haddadin, Towards Safe Robots - Approaching Asimov's 1st Law. Springer, 2014. [4] D. Lee and C. Ott, "Incremental kinesthetic teaching of motion primitives using the motion refinement tube," Autonomous Robots, vol. 31, no. 2-3, pp. 115­131, 2011. [5] H. Wang and K. Kosuge, "Control of a robot dancer for enhancing haptic human-robot interaction in waltz," IEEE Trans. Haptics, vol. 5, no. 3, pp. 264­273, Jan. 2012. [6] H. Ben Amor, E. Berger, D. Vogt, and B. Jung, "Kinesthetic bootstrapping: Teaching motor skills to humanoid robots through physical interaction," in KI 2009: Advances in Artificial Intelligence. Springer Berlin Heidelberg, 2009, pp. 492­499. [7] S. Calinon, Robot programming by demonstration: A probabilistic approach. EPFL Press, 2009. [8] J. Kober and J. Peters, "Policy search for motor primitives in robotics," Machine Learning, vol. 84, no. 1, pp. 171­203, 2011. [9] S. Ikemoto, H. Ben Amor, T. Minato, B. Jung, and H. Ishiguro, "Physical human-robot interaction: Mutual learning and adaptation," IEEE Robotics & Automation Magazine, vol. 19, no. 4, pp. 24­35, 2012. [10] J. St¨ uckler and S. Behnke, "Following human guidance to cooperatively carry a large object," in Humanoids'11, 2011, pp. 218­223. [11] K. Yokoyama, H. Handa, T. Isozumi, Y. Fukase, K. Kaneko, F. Kanehiro, Y. Kawai, F. Tomita, and H. Hirukawa, "Cooperative works by a human and a humanoid robot," in Proceedings. ICRA '03. IEEE International Conference on Robotics and Automation 2003., vol. 3, 2003, pp. 2985­2991 vol.3. [12] A. Bussy, P. Gergondet, A. Kheddar, F. Keith, and A. Crosnier, "Proactive behavior of a humanoid robot in a haptic transportation task with a human partner," in RO-MAN, 2012 IEEE. IEEE, 2012, pp. 962­967. [13] M. Lawitzky, A. Mortl, and S. Hirche, "Load sharing in human-robot cooperative manipulation," in RO-MAN, 2010 IEEE, 2010, pp. 185­ 191. [14] H. Sakoe, "Dynamic programming algorithm optimization for spoken word recognition," IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 26, pp. 43­49, 1978. [15] M. M¨ uller, Information Retrieval for Music and Motion. Secaucus, NJ, USA: Springer-Verlag New York, Inc., 2007. [16] C. E. Rasmussen and C. K. I. Williams, Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning). The MIT Press, 2005. [17] P. J. Schmid, "Dynamic mode decomposition of numerical and experimental data," Journal of Fluid Mechanics, vol. 656, pp. 5­28, 8 2010. [18] C. W. Rowley, I. Igor Mez, I. Shervin Bagher, R. Philipp Schlatte, and D. S. Henningson, "Spectral analysis of nonlinear flows," Journal of Fluid Mechanics, vol. 641, no. -1, pp. 115­127, 2009. [19] S. Bagheri, "Analysis and control of transitional shear flows using global modes," Ph.D. dissertation, KTH, Mechanics, 2010. [20] K. Chen, J. Tu, and C. Rowley, "Variants of dynamic mode decomposition: Boundary condition, koopman, and fourier analyses," Journal of Nonlinear Science, vol. 22, no. 6, pp. 887­915, 2012. [21] M. R. Jovanovi, P. J. Schmid, and J. W. Nichols, "Sparsity-promoting dynamic mode decomposition," Physics of Fluids (1994-present), vol. 26, no. 2, 2014. [22] E. Berger, D. Vogt, N. Haji-Ghassemi, B. Jung, and H. Ben Amor, "Inferring guidance information in cooperative human-robot tasks," in Proceedings of the International Conference on Humanoid Robots (HUMANOIDS), 2013.

Fig. 12: During a cooperative transportation task, a humanoid robot continuously estimates the amount and direction of external perturbations in order to react to human guidance.

D. Reaction Our approach can be used in scenarios where a robot has to detect and react to external perturbations in order to fulfill a specified task. As shown in Figure 12 it can be used to follow the human guidance in a cooperative transportation task as investigated in our previous publication [22]. A video of the functionality can be found under this link1 . Furthermore, our approach can be used to implement collision detection and safety constrains. In addition, the method can also be used to measure the weight of a carried object during a manipulation task. In general, behavior specific filtering allows for a variety of close contact interactions with the environment. V. C ONCLUSION In this paper, we presented a new approach for learning behavior-specific filters that can be used to accurately identify human physical influences on a robot. The approach uses DTW and DMD/SDMD in order to (1) detect an external perturbation, and (2) to quantify the amount of external perturbations. The generated perturbation value can then be used by a robot to adapt its movements to the applied forces or interpret a human command such as "walk backwards". In our experiments we showed that the learned perturbation filter can be used to accurately estimate touch information from noisy, low-cost sensors. Our approach produces a continuous perturbation value that can be used to detect even subtle physical interactions with a human partner. Since we are using a data-driven approach, no thresholds need to be defined by the user. At the core of our approach lies Dynamic Mode Decomposition, which so far has mostly been used in other fields of science, particularly fluid mechanics. We conclude that DMD is also a highly promising method for robotics. For future work, we hope to hierarchically combine several filters in a mixture-of-experts approach, in order to generalize perturbation estimation to new, untrained behaviors. We are currently also investigating the application
1 http://youtu.be/48y0hEix2fY

View publication stats

Probabilistic Movement Modeling for Intention Inference in Human-Robot Interaction
Zhikun Wang1,2 , Katharina Mülling1,2 , Marc Peter Deisenroth2 , Heni Ben Amor2 , David Vogt3 , Bernhard Schölkopf1 , and Jan Peters1,2 Max Planck Institute for Intelligent Systems Spemannstr. 38, 72076 Tübingen, Germany. 2 Technische Universität Darmstadt Hochschulstr. 10, 64289 Darmstadt, Germany. 3 Technical University Bergakademie Freiberg Bernhard-von-Cotta-Str. 2, 09596 Freiberg, Germany.
1

Abstract

Intention inference can be an essential step toward efficient humanrobot interaction. For this purpose, we propose the Intention-Driven Dynamics Model (IDDM) to probabilistically model the generative process of movements that are directed by the intention. The IDDM allows to infer the intention from observed movements using Bayes' theorem. The IDDM simultaneously finds a latent state representation of noisy and highdimensional observations, and models the intention-driven dynamics in the latent states. As most robotics applications are subject to real-time constraints, we develop an efficient online algorithm that allows for real-time intention inference. Two human-robot interaction scenarios, i.e., target prediction for robot table tennis and action recognition for interactive humanoid robots, are used to evaluate the performance of our inference algorithm. In both intention inference tasks, the proposed algorithm achieves substantial improvements over support vector machines and Gaussian processes.

1 Introduction
Recent advances in sensors and algorithms allow for robots with improved perception abilities. For example, robots can now recognize human poses in real time using depth cameras (Shotton et al., 2011), which can enhance the robot's ability to interact with humans. However, effective perception alone may not be sufficient for Human-Robot 1

g x1 z1 x2 z2
(a) GPDM

x3 z3

··· x1 z1 x2 z2
(b) IDDM

x3 z3

···

Figure 1: Graphical models of the Gaussian process dynamical model (GPDM) and the proposed intention-driven dynamics model (IDDM), where we denote the intention by g , state by xt , and observation by zt . The proposed model explicitly incorporates the intention as an input to the transition function (Wang et al., 2012b). Interaction (HRI), since the robot's reactions ideally depend on the underlying intention of the human's action, including the others' goal, target, desire, and plan (Simon, 1982). Human beings rely heavily on the skill of intention inference (for example, in sports, games, and social interaction) and can improve the ability of intent prediction by training. For example, skilled tennis players are usually trained to possess substantially better anticipation than amateurs (Williams et al., 2002). This observation raises the question of how robots can learn to infer the human's underlying intention from movements. In this article, we focus on intention inference from a movement based on modeling how the dynamics of a movement are governed by the intention. This idea is inspired by the hypothesis that a human movement usually follows a goal-directed policy (Baker et al., 2009; Friesen and Rao, 2011). The resulting dynamics model allows to estimate the probability distribution over intentions from observations using Bayes' theorem and to update the belief as additional observation is obtained. The human movement considered here is represented by a time series of observations, which makes discrete-time dynamics models a straightforward choice for movement modeling and intention inference. In a robotics scenario, we often rely on noisy and high-dimensional sensor data. However, the intrinsic states are typically not observable, and may have lower dimensions. Therefore, we seek a latent state representation of the relevant information in the data, and then model how the intention governs the dynamics in this latent state space, as shown in Fig. 1b. The resulting model jointly learns both the latent state representation and the dynamics in the state space. Designing a parametric dynamics model is difficult due to the complexity of human movement, e.g., its unknown nonlinear and stochastic nature. To address this issue, Gaussian processes (GPs), see (Rasmussen and Williams, 2006), have been successfully applied to modeling human dynamics. For example, the Gaussian Process Dynamical Model (GPDM) proposed in (Wang et al., 2008) uses GPs for modeling the

2

(a) Robot table tennis.

(b) Interactive humanoid robot.

Figure 2: Two examples of HRI scenarios where intention inference plays an important role: (a) target prediction in robot table tennis games, and (b) action recognition for human-robot interaction. generative process of human motion with a nonlinear dynamical system, as shown in Fig. 1a. Since the GP is a probabilistic nonparametric model, the unknown structure of the human moment can be inferred from data, while maintaining posterior uncertainty about the learned model itself. As an extension to the GPDM, we propose the Intention-Driven Dynamics Model (IDDM), which models the generative process of intention-driven movements. The dynamics in the latent states are driven by the intention of the human action/behavior, as shown in Fig.1b. The IDDM can simultaneously find a good latent state representation of noisy and high-dimensional observations and describe the dynamics in the latent state space. The dynamics in latent state and the mapping from latent state to observations are described by GP models. Using the learned generative model, the human intention can be inferred from an ongoing movement using Bayesian inference. However, exact intention inference is not tractable due to the nonlinear and nonparametric GP transition model. Therefore, we propose an efficient approximate inference algorithm to infer the intention of a human partner. The remainder of the article is organized as follows. First, in this section, we illustrate the considered scenarios (Section 1.1) and discuss the related work (Section 1.2). Subsequently, we present the Intention- Driven Dynamics Model (IDDM) and address the problem of its training in Section 2. In Section 3, we study approximate algorithms for intention inference and extend them to online inference in Section 4. We evaluate the performance of the proposed methods in the two scenarios, i.e., target prediction in robot table tennis and action recognition, in Section 5 and 6. Finally, we summarize our contributions and discuss properties of the IDDM in Section 7.

1.1 Considered Scenarios
To verify the feasibility of the proposed methods, we discuss two representative scenarios where intention inference plays an important role in human-robot interactions: (1) Target inference in robot table tennis. We consider human-robot table tennis games (Mülling et al., 2011), where the robot plays against a human opponent as shown

3

in Fig. 2a. The robot's hardware constraints often impose strong limitations on its flexibility in such a high-speed scenario; for example, the Barrett WAM robot arm often cannot reach incomining balls due to a lack of time caused by acceleration and torque limits for the biomimetic robot table tennis player described in (Mülling et al., 2011). The robot is kinematically capable of reaching a large hitting plane with pre-defined hitting movements such as forehand, middle, and backhand stroke movements that are capable in returning the ball shot into their corresponding hitting regions. However, movement initiation requires an early decision on the type of movement. In practice, it appears that to achieve the required velocity for returning the ball for the whole kinematically reachable hitting plane, this decision needs to be taken at least 80 ms before the opponent returns the ball (Wang et al., 2011b). Hence, it is necessary to choose the hitting movement before the opponent's racket has even touched the ball. This choice can be made based on inference of the target location where the opponent intends to return the ball from his incomplete stroke movement. We show that the IDDM can improve the prediction of the human player's intended target over a baseline method based on Gaussian process regression, and can thus expand the robot's hitting region substantially by utilizing multiple hitting movements. (2) Action recognition for interactive humanoid robots. In this setting, we use our IDDM to recognize the actions of the human, as shown in Fig. 2b, which can improve the interaction capabilities of a robot (Jenkins et al., 2007). In order to realize natural and compelling interactions, the robot needs to correctly recognize the actions of its human partner. In turn, this ability allows the robot to react in a proactive manner. We show that the IDDM has the potential to identify the action from movements in a simplified scenario. In most robotics applications, including the scenarios discussed above, the decision making systems are subject to real-time constraints and need to deal with a stream of data. Moreover, the human's intention may vary over time. To address these issues, we propose an algorithm for online intention inference. The online algorithm can process the stream data and fulfill the real-time requirements. In the experiments, the proposed online intention inference algorithm achieved over four times acceleration over our previous method in (Wang et al., 2012b).

1.2 Related Work
We review methods for intention inference and for modeling human movements that are related to the proposed IDDM and inference methods. 1.2.1 Intention Inference Inference of intentions has been investigated in different settings. Most of previous work relies on probabilistic reasoning. Intention inference with discrete states and actions has been extensively studied, using Hidden Markov Models (HMMs) to model and predict human behavior where different dynamics models were adopted to the corresponding behaviors (Pentland and Liu, 1999). Online learning of intentional motion patterns and prediction of intentions based on HMMs was proposed in (Vasquez et al., 2008), which allows efficient inference in real time. The HMM can be learned incrementally to cope with new motion

4

patterns in parallel with prediction (Vasquez et al., 2009). Probabilistic approaches to plan recognition in artificial intelligence (Liao et al., 2007) typically represent plans as policies in terms of state-action pairs. When the intention is to maximize an unknown utility function, inverse reinforcement learning (IRL) infers the underlying utility function from an agent's behavior (Abbeel and Ng, 2004). IRL has also been applied to model intention-driven behavior. For instance, maximum entropy IRL (Ziebart et al., 2008) has been used to model goal-directed trajectories of pedestrians (Ziebart et al., 2009) and target-driven pointing trajectories (Ziebart et al., 2012). In cognitive science, Bayesian models were used for inferring goals from behavior in (Rao et al., 2004), where a policy conditional on the agent's goal is learned to represent the behavior. Bayesian models can be used to interpret the agent's behavior and predict its behavior in a similar environment with the learned model (Baker et al., 2006). In a recent work (Friesen and Rao, 2011), a computational framework was proposed to model gaze following, where GPs are used to model the dynamics with actions driven by a goal. These methods assume that the states can be observed. However, in practice the states are often not well-defined or not observable for complex human movement. One can also consider the intention inference jointly with decision making, such as autonomous driving (Bandyopadhyay et al., 2012), control (Hauser, 2012), or navigation in human crowds (Kuderer et al., 2012). For example, when the state space is finite, the problem can be formulated as a Partially Observable Markov Decision Process (Kurniawati et al., 2011) and solved efficiently (Wang et al., 2012a). In contrast, our method assumes that the robot's decision does not influence the intention of the human and considers intention inference and decision making separately, which allows us to efficiently deal with high-dimensional data stream and fulfill the real-time constraints. 1.2.2 Gaussian Process Dynamical Model and Extensions Observations of human movements often consist of high-dimensional features. Determining a low-dimensional latent state space is an important issue for understanding observed actions. The Gaussian Process Latent Variable Model (GPLVM) (Lawrence, 2004) finds the most likely latent variables while marginalizing out the function mapping from latent to observed space. The resulting latent variable representation allows to model the dynamics in a low-dimensional space. For example, the Gaussian Process Dynamical Model (Wang et al., 2008) uses an additional GP transition model for the dynamics of human motion on the latent state space. In robotics applications, the GPLVM can also be used for learning dynamical system motor primitives (Ijspeert et al., 2002) in a low-dimensional latent space, to achieve robust dynamics and fast learning (Bitzer and Vijayakumar, 2009). Nonparametric dynamics models are also applied for tracking a small robotic blimp with two cameras (Ko and Fox, 2009), where GP-Bayes filters were proposed for efficient filtering. In a follow-up work (Ko and Fox, 2011), the model is learned based on the GPLVM, so that the latent states need not be provided for learning. The use of a GP transition model renders exact inference in the GPDM and, hence, 5

in the IDDM, analytically intractable. Nevertheless, approximate inference methods have been successfully applied based on filtering and smoothing in nonlinear dynamical systems. For the GPDM and its extensions, approximate inference can be achieved using Particle Filters (GP-PF), Extended Kalman Filters (GP-EKF), and Unscented Kalman Filters (GP-UKF) as proposed by (Ko and Fox, 2009). GP Assumed Density Filters (GP-ADF) for efficient GP filtering, and general smoothing in GPDMs were proposed in (Deisenroth et al., 2009) and (Deisenroth et al., 2012), respectively. These filtering and smoothing techniques allow the use of Expectation-Maximization (EM) framework for approximate inference (Ghahramani and Roweis, 1999; Turner et al., 2010; Wang et al., 2012b).

2 Intention-Driven Dynamics Model
We propose the Intention-Driven Dynamics Model (IDDM), which is an extension of the GPDM (Wang et al., 2008). The GPDM is a nonparametric approach to learning the transition function in the latent state space and the measurement mapping from states to observations simultaneously. As shown in Fig. 1a, the transition function in the GPDM is only determined by the latent state. However, in the applications considered in this paper, the underlying intention, as an important drive of human movements, can hardly be discovered directly from the observations. Considering that the dynamics can be substantially different when the actions are based on different intentions, we propose the Intention-Driven Dynamics Model. As shown in Fig. 1b, the IDDM explicitly incorporates the intention into the transition function in the latent state space. This dynamics model was inspired by the hypothesis that the human action is directed by the goal (Baker et al., 2009; Friesen and Rao, 2011). For example, in table tennis, the player swings the racket in order to return the ball to an intended target. The target is, hence, a driving factor in the dynamics of the racket. We present the proposed model and address the problem of its training in this section. Later, in Section 3, we study approximate algorithms for intention inference, and extend it for online inference in Section 4.

2.1 Measurement and Transition Models
In the proposed IDDM, one set of GPs models the transition function in the latent space conditioned on the intention g . A second set of GPs models the measurement mapping from the latent states x and the observations z. For notational simplicity, we assume the intention variable g is discrete or a scalar. The model and method can easily generalize to multi-variate intention variables. We detail both the measurement and transition models in the following. This article extensively uses properties of the Gaussian processes, e.g., predictive distribution and marginal likelihood. We refer to (Rasmussen and Williams, 2006) for a comprehensive introduction to GPs.

6

2.1.1 Measurement model The observations of a movement are a time series z1:T [z1 , . . . , zT ], where zt  RDz . In the proposed generative model, we assume that an observation zt  RDz is generated by a latent state variable xt  RDx according to zt = Wh(xt ) + Wnz,t , nz,t  N (0, Sz ) , (1)

where the diagonal matrix W = diag(w1 , . . . , wDz ) scales the outputs of h(xt ). The scaling parameters W allow for dealing with raw features that are measured in different units, such as positions and velocities. We place a GP prior distribution on each dimension of the unknown function h, which is marginalized out during learning and inference. The GP prior GP (mz (·), kz (·, ·)) is fully specified by a mean function mz (·) and a positive semidefinite covariance (kernel) function kz (·, ·). Without specific prior knowledge on the latent state space, we use the same mean and covariance function for the GP prior on every dimension of the unknown measurement function h, and use the noise (co)variance Sz = s2 z I. The predictive probability of the observations zt is given by a Gaussian distribution zt  N (mz (xt ), z (xt )) , where the predictive mean and covariance are computed based on training inputs Xz and outputs Yz , given by
1 mz (xt ) = Yz K- z kz (xt ),

(2) (3)
T 1 K- z kz (xt ) ,

z (xt ) =
2 z (xt )

2 z (xt )I,

= kz (xt , xt ) - kz (xt )

(4)

where, we use the shorthand notation kz (xt ) to represent the cross-covariance vector between h(Xz ) and h(xt ), and use Kz to represent the kernel matrix of Xz . 2.1.2 Transition model We consider first-order Markov transition model, see Fig. 1b, with a latent transition function f , such that xt+1 = f (xt , g ) + nx,t , nx,t  N (0, Sx ) . (5)

The state xt+1 at time t + 1 depends on the latent state xt at time t as well as on the intention g . We place a GP prior GP (mx (·), kx (·, ·)) on every dimension of f with shared mean and covariance functions. Subsequently, the predictive distribution of the latent state xt+1 conditioned on the current state xt and intention g is a Gaussian distribution given by xt+1  N (mx ([xt , g ]), x ([xt , g ])) based on training inputs Xx and outputs Yx , with
1 mx ([xt , g ]) =Yx K- x kx ([xt , g ]),

(6) (7)
T 1 K- x kx ([xt , g ]) ,

x ([xt , g ])
2 x ([xt , g ])

2 = x ([xt , g ])I,

=kx ([xt , g ], [xt , g ]) - kx ([xt , g ])

(8)

where Kx is the kernel matrix of training data Xx = [x1 , g1 ], . . . , [xn , gn ] . The transition function f may also depend on environment inputs u, e.g., controls or motor commands. We assume that environment inputs are observable and omit them in the description of model for notational simplicity. 7

2.2 Covariance Functions
By convention, we use GP prior mean functions that are zero everywhere for notational simplicity, i.e., mz (·)  0 and mx (·)  0. Hence, the model is determined by the covariance functions kz (·, ·) and kx (·, ·), which will be motivated in the following. The underlying dynamics of human motion are usually nonlinear. To account for nonlinearities, we use a flexible Gaussian tensor-product covariance function for the dynamics, i.e., kx ([xi , gi ], [xj , gj ]; ) = kx (xi , xj ; )kx (gi , gj ; ) + knoise = 1 exp
2 - 2

(9)

xi - xj

2

-

3 2 (gi

- gj )

2

+ 4 ij ,

where  = [1 , 2 , 3 , 4 ] is the set of all hyperparameters, and  is the Kronecker delta function. When the intention g is a discrete variable, we set the hyperparameter 3 =  such that kx (gi , gj ; )  ij . The covariance function for the measurement mapping from the state space to observation space is chosen depending on the task. For example, the GPDM in (Wang et al., 2008) uses an isotropic Gaussian covariance function
1 kz (x, x ;  ) = exp -  2 x-x

2

+ 2 x,x ,

(10)

parameterized by the hyperparameters  , as, intuitively, the latent states that generate human poses lie on a nonlinear manifold. Note that the hyperparameters  do not contain the signal variance, which is parameterized by the scaling factors W in Eq. (1). In the context of target prediction in table tennis games, we use the linear kernel kz (x, x ;  ) = xT x + 1 x,x , as the observations are already low-dimensional, but subject to substantial noise. (11)

2.3 Learning the IDDM
The proposed IDDM can be learned from a training data set D = {Z, g } of J movements and corresponding intentions. Each movement Zj consists of a time series of j T observations given by Zj = [zj 1 , . . . , zT ] . We construct the overall observation matrix Z by vertically concatenating the observation matrices Z1 , . . . , ZJ , and the overall intention matrix g from g 1 , . . . , g J . In the robot table tennis example, one movement corresponds to a stroke of the opponent, represented by a time series of observed racket and ball configurations. We assume the intention g can be obtained for training, for example by post-processing the data. In the robot table tennis example, the observed intention corresponds to the target where the opponent returns the ball to (see Fig. 5 for an illustration). In the table-tennis training data, we can obtained the target's coordinates by post-processing. In the action recognition, the label of action is provided directly in the training data. Similar to the GPDM (Wang et al., 2008), we find maximum a posteriori (MAP) estimates of the latent states X. Alternative learning methods and an empirical comparison can be found in (Turner et al., 2010; Damianou et al., 2011). Given the model 8

hyperparameters, the posterior distribution of latent states X can be decomposed into the probability of the observations given the states and the probability of the states given the intention, i.e., p(X|Z, g, ,  , W)  p(Z|X,  , W)p(X|g, ), (12)

both obtained by the GP marginal likelihood (Rasmussen and Williams, 2006). The GP marginal probability of the observations Z given the latent states X is given by a Gaussian distribution p(Z|X,  , W) = 
|W|M (2 )M Dz |Kz |Dz -1 T exp - 1 2 tr Kz ZWW Z

,

(13)

where M JT is the length of observations Z, and Kz is the kernel matrix computed by the kernel function kz (·, ·). Given the intention g , the sequence of latent states X has a Gaussian probability p(X|g, ) = p(X1 )p(X2:T |X1:T -1 , g, ) =
p(X1 ) (2 )mDx |Kx |Dx -1 T exp - 1 2 tr Kx X2:T X2:T

,

(14)

where Xt , t  {1, . . . , T } is constructed by vertically concatenating state matrices J x1 J (T - 1) is the length of X2:T , and Kx is the kernel matrix of X2:T , t , . . . , xt , m computed by the kernel function kx (·, ·). We use a Gaussian prior distribution on the initial states X1 . Based on Eqs. (13)­(14), the MAP estimates of the states are obtained by maximizing the posterior in Eq. (12). In practice, we minimize the negative log-posterior
T -1 1 z - M log |W| L(X) = D 2 log |Kz | + 2 tr Kz ZWW Z

+

Dx 2

-1 T T 1 log |Kx | + 1 2 tr Kx X2:T X2:T + 2 tr X1 X1 + const

(15)

with respect to the states X, using the Scaled Conjugate Gradient (SCG) method (Møller, 1993).

2.4 Learning Hyperparameters
A reliable approach to learning the hyperparameters  = {,  , W} is to maximize the marginal likelihood p(Z|g, ) = p(Z, X|g, )dX, (16)

which can be achieved approximately by using the Expectation-Maximization (EM) algorithm (Bishop, 2006). The EM algorithm computes the posterior distribution of states q (X) = p(X|Z, g, ), given in Eq. (12), in the Expectation (E) step and updates the hyperparameters by maximizing the expected data likelihood Eq [p(Z, X|g, )] in the Maximization (M) step. However, the posterior distribution q (X) is difficult to compute in the IDDM. Following (Wang et al., 2008), we draw samples of the states 9

Algorithm 1: Learning the model hyperparameters ,  , and W by maximizing the marginal likelihood, using the Monte Carlo EM algorithm. Input : Data: D = {Z, g } Input : Number of EM iterations: L Output: Model hyperparameters:  = {,  , W} 1 for l  1 to L do 2 for i  1 to I do 3 Initialize X by its MAP estimate ; 4 Draw sample X(i) from p(X|Z, g, ) using HMC;
5

Maximize

1 I

I i=1

log p(Z, X(i) |g, ) w.r.t.  using SCG;

X(1) , . . . , X(I ) from the posterior distribution using hybrid Monte Carlo (Andrieu et al., 2003), and, hence, the data likelihood is estimated via Monte Carlo integration according to I 1 Eq [p(Z, X|g, )]  p(Z, X(i) |g, ). (17) I i=1 In the M step, we use SCG to update the hyperparameters. In practice, we choose the number of samples I = 50 and the number of EM iterations L = 10. Although this procedure, as described in Algorithm 1, is time-demanding, in practice, we can learn the hyperparameters off-line. In practice, the maximum likelihood estimate of the hyperparameters may lead to over-fitting. For the IDDM, we found that the noise variance 4 in Eq. (9) of the transition model is occasionally underestimated, e.g., 4 < e-6 , as Algorithm 1 estimates it based on only a few samples. The underestimated noise variance may prevent the learned model from generalizing to test data that have significant deviation from the training data. This phenomenon of over-confidence has been discussed in (Lawrence, 2005; Wang et al., 2008). To alleviate this problem, we add a small constant e-3 to the learned noise variance 4 . The model also depends on the hyperparameter Dx , i.e., the dimensionality of the latent state space. Choosing an appropriate Dx is important. If the dimensionality is too small, the latent states cannot recover the observations, which leads to significant prediction errors. On the other hand, a high-dimensional state space results in redundancy and can cause a drop in performance and computational efficiency. Nevertheless, model selection, based on cross-validation for example, is conducted before learning and applying the model. To summarize, the model M = {X, } can be learned from a data set D. Subsequently, we use the model to infer the unobserved intention of a new ongoing movement, as described in the following section.

10

3 Approximate Intention Inference
After learning the model M from the training data set D, the intention g can be inferred from a sequence of new observations z1:T . For notational simplicity, we do not explicitly condition on the model M and the data set D. The measurement model defined in Eq. (1) scales the observations by a diagonal matrix W. Therefore, we pre-process every received observation with the scaling matrix W and omit W hereafter as well. The IDDM models the generative process of movements, represented by observations z1:T , given an intention g . Using Bayes' rule, we estimate the posterior probability (belief) on an intention g from observations z1:T . The posterior is given by p(g |z1:T ) = p(z1:T |g )p(g ) p(z1:T ) p(z1:T , x1:T |g )dx1:T , (18) (19)

 p(g )

where computing the marginal likelihood p(z1:T |g ) requires to integrate out the latent states x1:T . Exactly computing the posterior in Eq. (19) is not tractable due to the use of nonlinear GP transition model. Hence, we resort to approximate inference. In (Wang et al., 2012b), we introduced an EM algorithm for finding the maximum likelihood estimate of intention. However, this point estimate may not suffice for the reactive policies of the robot that also take into account the uncertainty in the intention inference (Wang et al., 2011a,b; Bandyopadhyay et al., 2012). For example, in the table tennis task, the robot may need to choose the optimal time to initiate its hitting movement, and such a choice is ideally made based on how certain the prediction of target is (Wang et al., 2011b). In this article, we extend our previous inference method introduced in (Wang et al., 2012b), such that the uncertainty about the intention is explicitly modeled and taken into account when making decisions. The key challenge in estimating the belief in Eq. (19) is integrating out the latent states x1:T . A common approximation to the log marginal posterior is to compute a lower bound B (g )  log p(g |z1:T ) based on Jensen's inequality (Bishop, 2006). The bound is given by B (g ) Eq [log p(z1:T , x1:T , g )] + H(q ) = log p(g |z1:T ) - KL (q ||p(x1:T |z1:T , g ))  log p(g |z1:T ) , which holds for any distribution q (x1:T ) on the latent states. Here, the KullbackLeibler (KL) divergence KL (q ||p(x1:T |z1:T , g )) determines how well B (g ) can approximate the belief. Based on this approximation, the inference problem consists of two steps, namely, (a) finding an approximation q (x1:T )  p(x1:T |z1:T , g ), and (b) computing the approximate belief B(g ). When using the EM algorithm for the maximum likelihood estimate of the intention g , as in (Wang et al., 2012b), the E-step and M-step correspond to these two steps, respectively. For step (a), we approximate the posterior of latent states p(x1:T |z1:T , g ) by a Gaussian distribution q (x1:T ). For this purpose, we use the forward-backward smoothing method proposed in (Deisenroth et al., 2009, 2012), which is based on moment 11 (20)

matching. Typically, Gaussian moment-matching provides credible error bars, i.e., it is robust to incoherent estimates. The resulting approximate distribution q that we use in the lower bound B in Eq. (20) is given by q (x1:T ) = N (µq , q )  p(x1:T |z1:T , g ), with the mean and block-tri-diagonal covariance matrix  x 1|T  µx  x 1| T 2,1|T  .   µq =  . .  , q =   µx T |T 0  x 1, 2| T .. . .. . 0 .. .. . . x T -1,T |T x T |T    ,   (22) (21)

x T,T -1|T

where we only need to consider the cross-covariance between consecutive states1 . For step (b), based on the approximation q (x1:T ), the posterior belief p(g |z1:T ) can then be approximated by the lower bound B(g ) in Eq. (20). In the following, we first detail step (a), i.e., the computation of q for our IDDM, in Section 3.1. Subsequently, we discuss step (b), i.e., efficient belief estimation, in Section 3.2.

3.1 Filtering and Smoothing in the IDDM
To obtain the posterior distribution p(x1:T |z1:T , g ), approximate filtering and smoothing with GPs are crucial in our proposed IDDM. We place a Gaussian prior on the initial state x1 . Subsequently, Gaussian approximations q (xt-1 , xt ) of p(xt-1 , xt |z1:T , g ) for t = 2, . . . , T are computed. We explicitly determine the marginals p(xt |z1:T , g ) for t = 1, . . . , T , and the cross-covariance terms cov[xt-1 , xt |z1:T , g ], t = 2, . . . , T . These steps yield a Gaussian approximation with a block-tri-diagonal covariance matrix, see Eq. (22). These computations are based on forward-backward smoothing (GPRTSS) as proposed in (Deisenroth et al., 2012). As a first step, we compute the posterior distributions p(xt |z1:T , g ) with t = 1, . . . , T . To compute these posteriors using Bayesian forward-backward smoothing in the IDDM, it suffices to compute both joint distributions p(xt-1 , xt |z1:t-1 , g ) and p(xt , zt |z1:t-1 , g ). The Gaussian filtering and smoothing updates can be expressed solely in terms of means and (cross-)covariances of these joint distributions, see (Deisenroth and Ohlsson, 2011; Deisenroth et al., 2012). Hence, we have
x xz z -1 µx (zt - µz t|t = µt|t-1 + t|t-1 (t|t-1 ) t|t-1 ) ,

(23) (24) (25)

x t|t µx t-1|T x t|T

= = =

xz z -1 zx x t|t-1 t|t-1 - t|t-1 (t|t-1 ) x x x µt-1|t-1 + Jt-1 (µt|T - µt|t-1 ) ,

,

x t-1|t-1

+

Jt-1 (x t|T

-

x t|t-1 )Jt-1

,

(26)

1 We use the short-hand notation ad where a = µ denotes the mean µ and a =  denotes the b|c covariance, b denotes the time step of interest, c denotes the time step up to which we consider measurements, and d  {x, z } denotes either the latent space (x) or the observed space (z ).

12

where we define
x -1 Jt-1 = x . t-1,t|t-1 (t|t-1 )

(27)

In the following, we first detail the computations required for a Gaussian approximation of the joint distribution p(xt-1 , xt |z1:t-1 , g ) using moment matching. Here, we approximate the joint distribution p(xt-1 , xt |z1:t-1 , g ) by the Gaussian N x µx t-1|t-1 t-1|t-1 , x x µt|t-1 t,t-1|t-1 x t-1,t|t-1 x t|t-1 . (28)

x Without loss of generality, the marginal distribution N (xt-1 | µx t-1|t-1 , t-1|t-1 ), which corresponds to the filter distribution at time step t - 1, is assumed known. We compute the remaining elements of the mean and covariance in Eq. (28) in the following paragraphs. We will derive our results for the more general case where we have a joint Gaussian distribution p(xt-1 , xt , g |z1:t-1 ). The known mean and covariance ~ t-1|t-1 = [(µx of distribution p(xt-1 , g |z1:t-1 ) are given by µ t-1|t-1 ) , µg ] and ~ t-1|t-1 , respectively, where the covariance matrix  ~ t-1|t-1 is block-diagonal with  blocks x t-1|t-1 and g . By setting the mean µg = g and g = 0, we obtain the ~ = [x , g ] . results from (Wang et al., 2012b). For convenience, we define x Using the law of iterated expectations, the a-th dimension of the predictive mean of the marginal p(xt |z1:t-1 ) is given as

~ t-1 ]|z1:t-1 (µx xt-1 )|x t|t-1 )a = Ext-1 Efa [fa (~ = ~ t-1 , xt-1 )p(~ xt-1 |z1:t-1 )dx ma x (~

(29)

where we substituted the posterior GP mean function for the inner expectation. Note that if g is given then g = 0. Writing out the posterior mean function and defining 1  a := K- x ya , with yai , i = 1, . . . , M , being the training targets of the GP with target dimension a, we obtain (µx t|t-1 )a = q  a , where we define q = ~ t-1 . kx ([xt-1 , g ], Xx )p(~ xt-1 |z1:t-1 )dx (31) (30)

~ i = [xi , gi ] of the transition Here, Xx denotes the set of the M GP training inputs x GP. Since kx is a Gaussian kernel, we can solve the integral in Eq. (31) analytically and obtain the vector q with entries qi with i = 1, . . . , M as
-1 i , qi = 1 ||- 2 exp - 1 2  i () 1

(32) + I, (33)

~i - µ ~ t-1|t-1 , i = x

 = t-1|t-1 

-1

13

where  is a diagonal matrix of concatenated length-scales 2 I and 3 I. By applying x the law of total variances, the entries ab of the marginal predictive covariance matrix x t|t-1 in Eq. (28) are given by
x ab =

 a (Qx - qq ) b  a (Q - qq ) b + 1 - tr (Kx + 4 I)
x -1

if a = b , Q
x

+ 4

if a = b .

(34)

We define the entries of Qx  RM ×M as Qx ij = with ~ t-1|t-1 (-1 + -1 ) + I , R :=  a b
-1 1 ~ -1 T = - a + b + t-1|t-1 , 1 1 ~ t-1|t-1 ) + - ~ t-1|t-1 ) . zij := - xi - µ xj - µ a (~ b (~ a b ~ t-1|t-1 ])kx ~ t-1|t-1 ]) kx ([xi , gi ], [µ ([xj , gj ], [µ

|R |

exp

-1 1 zij 2 zij T

For a detailed derivation, we refer to (Deisenroth, 2010; Deisenroth et al., 2012). To fully determine the joint Gaussian distribution in Eq. (28), the cross-covariance x t-1,t|t-1 = cov[xt-1 , xt |z1:t-1 , g ] is given as the upper part of the cross-covariance
M

cov[xt-1 , xt , g |z1:t-1 ] =
i=1

~ t-1|t-1 -1 (~ ~ t-1|t-1 ) , ai qai  xi - µ

when we set µg = g and g = 0. Note that q and  are defined in Eq. (32) and (33), respectively. Up to now, we have computed a Gaussian approximation to the joint probability distribution p(xt-1 , xt |z1:t-1 , g ). Let us now have closer look at the second joint distribution p(xt , zt |z1:t-1 , g ), which is the missing contribution for Gaussian smoothing (Deisenroth and Ohlsson, 2011), see Eq. (23)­(26). To determine a Gaussian approximation N x µx t|t-1 t|t-1 , z µt|t-1 zx t|t-1 xz t|t-1 z t|t-1 (35)

to p(xt , zt |z1:t-1 , g ) it remains to compute the mean and the covariance of the marginal distribution p(zt |z1:t-1 , g ) and the cross-covariance terms cov[xt , zt |z1:t-1 , g ]. We omit these computations for the nonlinear Gaussian kernel as they are very similar to the computations to determine the joint distribution p(xt-1 , xt |z1:t-1 , g ). For the linear measurement kernel in Eq. (11), we compute the marginal mean µz t|t-1 in Eq. (35) for observation dimension a = 1, . . . , Dz according to Eh,xt-1 [ha (xt )|z1:t-1 , g ] = = m(xt )p(xt |z1:t-1 , g ) dxt xt Xz p(xt |z1:t-1 , g ) dxt  a = q  a , (36)

q = Xz µx t|t-1 . 14

1 Here, Xz comprises the training inputs for the measurement model and  a = K- z Yza , where Yza are the training targets of the ath dimension, a = 1, . . . , Dz . The elements z ab of the marginal covariance matrix z t|t-1 in Eq. (35) are given as z ab =

a (Qz - qq )a
x x x t|t-1 + µt|t-1 (µt|t-1 )

if a = b , - tr
1 z K- z Q

+  a (Q - qq ) a
z

if a = b , (37)

a, b = 1, . . . , Dz , where we define Qz =
x x Xz xt xt Xz p(xt |z1:t-1 , g ) dxt = Xz (x t|t-1 + µt|t-1 (µt|t-1 ) )Xz .

The cross-covariance xz t|t-1 = cov[xt , zt |z1:t-1 , g ] in Eq. (35) is given as
x xz t|t-1 = t|t-1 Xz  a

(38)

for all observed dimensions a = 1, . . . , Dz . The mean µz t|t-1 in Eq. (36), the covariand the cross-covariance in Eq. (38) fully determine the ance matrix z in Eq. (37), t|t-1 Gaussian distribution in Eq. (35). Hence, following (Deisenroth and Ohlsson, 2011), we can now compute the latent state posteriors (filter and smoothing distributions) according to Eq. (23)­(26). These smoothing updates in Eq. (23)­(26) yield the marginals of our Gaussian approximation to p(x1:T |z1:T , g ), see Eq. (22). The missing cross-covariances x t-1,t|T of p(x1:T |z1:T , g ) that finally fully determine the block-tri-diagonal covariance matrix in Eq. (22) are given by
x x t-1,t|T = Jt-1 t|T ,

(39)

where Jt-1 is given in Eq. (27). For detailed derivations, we refer to (Deisenroth, 2010). These computations conclude step (1) on lower-bounding the posterior distribution on the intention, see Eq. (20), i.e., the computation of the approximate distribution q in Eq. (21). It remains to compute the bound B itself, which is described in the following.

3.2 Estimating the Belief on Intention
For a given intention g , we compute a Gaussian approximation q (x1:T ) to the posterior p(x1:T |z1:T , g ), given by q (xt , xt+1 ) = N x µx t|T t|T , x µx t+1,t|T t+1|T x t,t+1|T x t+1|T (40)

for t = 1, . . . , T - 1. The belief p(g |z1:T )  exp(B (g )) is estimated using Eq. (20), where the computation can be decomposed according to
T -1

B (g ) =
t=1

Eq [log p(xt+1 |xt , g )] +p(g ) + H(q ) + const.
Qt ( g )

(41)

15

Here the smoothing distribution q (x1:T |g )  p(x1:T |z1:T , g ) is computed given the intention g . As we only need to estimate the unnormalized belief, the constant term needs not to be computed. The entropy H(q ) of the Gaussian distribution q can be computed analytically, and is given by H (q ) = We define Qt (g ) = = Eq [log p(xt+1 |xt , g )] q (xt , xt+1 ) log p(xt+1 |xt , g )dxt+1 dxt q (xt , xt+1 ) log (p(xt+1 |xt , g )q (xt )) dxt+1 dxt -
q ~(xt ,xt+1 )

1 (T Dx + T Dx log(2 ) + log |q |) . 2

(42)

(43)

q (xt ) log q (xt )dxt ,

where p(xt+1 |xt , g )q (xt ) can be approximated by a Gaussian distribution q ~(xt , xt+1 ) = N (µq ,  ) based on moment matching (Quiñonero-Candela et al., 2003). Here, we ~ q ~ only compute the diagonal elements in the covariance matrix of q . As a result, ~ Eq. (43) is approximated as Qt (g )  KL q (xt , xt+1 )||q ~(xt , xt+1 ) + H q (xt , xt+1 ) + H q (xt ) , (44)

where H(q ) is the entropy of the distribution q and KL(q ||q ~) is the Kullback-Leibler (KL) divergence between q and q ~, both of which are Gaussians. The KL divergence also has a closed-form expression, given by KL(q ||q ~) = 1 2
1 T -1 tr(- ~) q ~) - log q ~ q ) + (µq - µq ~ (µq - µq

|q | |q ~|

+ const.

As a result, we can compute the unnormalized belief B (g ) for a given intention g approximately according to Eq. (41). We aim to determine the posterior distribution p(g |z1:T ) of the intention g . Using the posterior distribution instead of point estimates allows us to express uncertainty about the inferred intention g . Computing Gaussian approximations of the posterior distributions can be done using the unscented transformation (Deisenroth et al., 2012), for instance. However, when the posterior is not unimodal, a Gaussian approximation may lose important information. Particle filtering can preserve all the modes (Ko and Fox, 2009), but will not be sufficiently efficient due to the real-time constraints. As we focus on one-dimensional intentions in this article, we advocate the discretization of intention. For example, in the table tennis task, the intention (opponent's target position) is a bounded scalar variable g  [gmin , gmax ], where the bounds are given by physical constraints such as the table width and the length of robot arm. We uniformly choose {v1 , . . . , vK } from [gmin , gmax ] and represent intention by the index, i.e., g  {1, . . . , K }.

16

Algorithm 2: Inference of the discretized intentions by computing the posterior probabilities for every value of the intention. Input : Observations x1:T Output: Posterior probabilities for every intention value g  {1, . . . , K } 1 foreach g  {1, . . . , K } do 2 Compute smoothing distribution q (x1:T )  p(x1:T |z1:T , g ) ; 3 Compute the value of B(g ) = Eq [log p(x1:T |g )] + log p(g ) using the approximation in Eq. (44) ;
4

Estimate the posterior p(g |x1:T )  exp B(g )/(

K g =1

exp B(g )).

3.3 Discussion of the Approximate Inference Method
To summarize, the algorithm for computing the posterior distribution over discrete or discretized intentions g is given in Algorithm 2. The smoothing distribution q defined in Eq. (40) depends on the current estimate of intention g . However, it is often time-demanding to enumerate the intention g and compute the smoothing distribution q for each g individually. The computational complexity of the 3 2 3 smoothing step in Algorithm 2 is O(T K (Dz + Dx Dz + N 2 Dx )) when using the linear 2 2 3 ))) + Dz + N 2 Dx (Dx kernel function for the measurement mapping, and O(T K (Dz when using the Gaussian kernel function, where T is the number of observations obtained, K the number of (discretized) intentions, N the number of training data, and Dx and Dz the dimensionality of state and observation. The complexity of com2 ). The computational efficiency can be improved puting the belief is O(T KN 2 Dx to meet the tight time constraints in robotics applications by introducing further approximations, such as adopting GP pseudo inputs to reduce the size of training data N (Snelson and Ghahramani, 2006; Quiñonero-Candela and Rasmussen, 2005), using dimensionality reduction or feature selection techniques to obtain a small number of features Dz (van der Maaten et al., 2009; Ding and Peng, 2005), and reducing the sample size K of intention g . However, the dependence of complexity on the number of observations T still prevents the algorithm from being applied to online scenarios. For these, T keeps growing as new observations come, whereas observations obtained a long time ago do not provide as much information as recent ones. To address this issue, we will introduce an approximation in the online inference method in Section 4.

4 Online Intention Inference
The introduced inference algorithm can be seen as a batch algorithm that relies on the segmentation of human movements. However, in online human-robot interaction, the intention inference algorithm faces new challenges to deal with the stream of observations. The complexity of Algorithm 2 grows with the number of existing observations, which does not fulfill the real-time requirements of an online method. In addition, the intention can vary over time in an online inference scenario. For example, the intended targets in table tennis games vary between strokes. Hence, the online method should model and track the change of intention. To address these issues, we generalize the inference method to an online scenario. 17

g

···

xt - 1

xt

xt + 1

···

zt - 1

zt

zt + 1

Figure 3: The graphical model of the IDDM in an online manner, which can handle a stream of observations. That is, the observations are obtained constantly, and the belief on the intention is reestimated after receiving a new observation. A computational bottleneck in the batch method is that the smoothing distribution q is computed for every value of intention. For efficient inference, we compute a marginal smoothing distribution q according to current belief on intention p(g ), i.e., we integrate out the intention, q (x1:t )
g

p(g )qg (x1:t ).

(45)

The online inference algorithm then estimates the belief Bt (g ) on the intention based on the marginal smoothing distribution q after receiving an observation, which can be sufficiently efficient for real-time intention inference with a small sacrifice in accuracy. Based on the marginal smoothing distribution, we update the belief on intention using dynamic programming. which will be discussed as follows.

4.1 Online Inference using Dynamic Programming
Assuming the marginal smoothing distribution q is given, we develop an online inference method using dynamic programming (see Fig. 3). The method maintains the belief (i.e., log of the unnormalized posterior) of the intention g based on the obtained observations z1:t-1 according to Eq. (41), given by Bt-1 (g )  Eq [log p(g, x1:t-1 )] + const. (46) Here, we consider discretized intentions g  {1, . . . , K }, and write the belief Bt-1 as a vector of length K . For a new observation zt , we decompose p(g, x1:t ) according to p(g, x1:t ) = p(xt |xt-1 , g )p(g, x1:t-1 ). As a result, the belief Bt becomes Bt (g ) = Eq [log p(g, x1:t )] + const = Eq [log p(xt |xt-1 , g )] + Eq [log p(g, x1:t-1 )] + const = Eq [log p(xt |xt-1 , g )] + Bt-1 (g ) + const, 18 (48) (49) (50) (47)

which is in a recursive form and can be computed efficiently using dynamic programming. Given a new observation zt , the belief is updated based on Eq [log p(xt |xt-1 , g )], which is computed according to Eqs. (43)-(44). The belief Bt is then normalized, i.e., g exp(B t (g )) = 1. In addition, the intention can vary over time in an online inference scenario. As the new observation zt can be more informative than the previous observations z1:t-1 , we introduce a forgetting factor to shrink the belief Bt-1 . The recursive formula of the belief is subsequently given by Bt (g ) = Eq [log p(xt |xt-1 , g )] + (1 - )Bt-1 (g ), where the shrinking factor observations. (51)

determines how fast the algorithm forgets the previous

4.2 Marginal Smoothing Distribution
The inference method relies on the smoothing distribution q at time t, which in turn depends on the intention belief Bt-1 . In analogy to the EM algorithm, we iteratively update the belief on intention B and the smoothing distribution q . However, full forward-backward smoothing on x1:t is impractical as the computational complexity grows when we obtain more observations. Full smoothing is also not unnecessary since we do not update the previous belief B1:t-1 on the intention. Hence, given a new observation zt , we only need to compute q (xt-1:t ), which requires a single-step forward filtering and a single-step backward smoothing, based on the current belief Bt-1 . The filtering and smoothing need to integrate out the uncertainty in the intention. For discrete intentions, we can simply compute the smoothing distributions qg for every value of intention gt-1 , and average over them q (xt-1:t ) 
g

qg (xt-1:t )pt-1 (g ),

(52)

where the belief pt-1 (g )  exp(Bt-1 (g )). The resulting distribution q will still be a Gaussian distribution. For continuous intentions, enumerating the discretized intention may be inefficient. To address this problem, we use the moment matching to approximate the distribution on intention by a Gaussian distribution, which is also adopted in the filtering and 2 smoothing method. Specifically, we compute the mean µg and variance g according to the belief Bt-1 . As a result, the marginal smoothing distribution is given by q (xt-1:t ) 
2 qg (xt-1:t )N (g |µg , g )dg,

(53)

which is computed using moment matching.

4.3 Discussion of the Online Inference Method
The online inference algorithm described in Algorithm 3 iteratively updates the belief of intention and latent states. 19

Algorithm 3: The online algorithm for the inference of discrete intention g  {1, . . . , K }.
1 2 3 4 5 6

7

8 9 10 11

Obtain the initial observation z1 ; Initialize the approximate distribution q (x1 ) ; Initialize B1 (g ) = log p(g ) according to the prior ; for t = 2, 3, . . . do Obtain the observation zt ; Compute marginal filtering distribution q (xt ) according to current belief Bt-1 ; Update marginal smoothing distribution q (xt-1 ) according to current belief Bt-1 ; foreach gt = {1, . . . , K } do Compute B0 (g ) = Qt-1 (g ) using the approximation in Eq. (44) ; Update the belief Bt = B0 + (1 - )Bt-1 ; Normalize the belief Bt  Bt - log
g

exp(Bt (g )) ;

3 The computational complexity of the smoothing step in Algorithm 3 is O(Dz + 2 2 3 Dx Dz + N Dx ) when using the linear kernel function for the measurement mapping, 3 2 3 and O(Dz + N 2 ( Dx Dx + Dx )) when using the Gaussian kernel function, which no longer depends on the number of observations T and the number of intentions K . The 2 complexity of computing the belief is O(KN 2 Dx ). Comparing to the batch algorithm, the efficiency is improved by a factor of T . To summarize, we proposed an efficient online method for intention inference from a new movement. The online method updates the belief of the intention by taking into account both the current belief and the new evidence (i.e., new observation). We list the employed approximations in both the batch and online inference methods in Table 1.

5 Target Prediction for Robot Table Tennis
Playing table tennis is a challenging task for robots, and, hence, has been used by many researchers as a benchmark task in robotics (Anderson, 1988; Billingsley, 1984; Fässler et al., 1990; Matsushima et al., 2005; Mülling et al., 2011). Up to now, none of the groups that have been working on robot table tennis ever reached levels of a young child, despite having robots with better perception, processing power, and accuracy Table 1: Important approximations employed in the batch and online inference. belief p(g |z1:T ) approx. belief B(g ) distr. p(x1:T |z1:T , g ) stream of observations batch online Jensen's lower bound B(g ); cf. Eq. (20) moment matching; cf. Eq. (44) q (x1:T |g ) for each g q (x1:T ) for all g ; cf. Eq. (53) sliding window recursive update; cf. Eq. (51)

20

than humans (Mülling et al., 2011). Likely explanations for this performance gap are (i) the human ability to predict hitting points from opponent movements and (ii) the robustness of human hitting movements (Mülling et al., 2011). In this article, we focus on the first issue: anticipation of the hitting region from opponent movements. Using the proposed method, we can predict the where the ball is likely to be shot before the opponent hits the ball, which gives the robot a head start of more than 200 ms additional time to initiate its movement2 . This additional time can be crucial due to robot's hardware constraints, for example, acceleration and torque limits in the considered setting (Mülling et al., 2011). Note that the predicted intention is only used to choose a hitting type, e.g., forehand, middle, or backhand. Fine-tuning of the robot's movement can be done when the robot is adjusted to the forehand/middle/backhand preparation pose and once the returned ball can be reliably predicted from the ball's trajectory alone. Hence, a certain amount of intention prediction error is tolerable since the robot can apply small changes to its basic hitting plan based on the ball's trajectory. However, the robot cannot return the ball outside the corresponding hitting region once it is adjusted to a preparation pose, see the video3 . Therefore, prediction accuracy directly influences the performance of the robot player (Wang et al., 2011b).

5.1 Experimental Setting
Our anticipation system has been evaluated in conjunction with the biomimetic robot table tennis player (Mülling et al., 2011), as this setup allowed exhibiting how much of an advantage such a system may offer. We expect that the system will help similarly or more when deployed within our skill learning framework (Mülling et al., 2013) as well as many of the recent table tennis learning systems (Huang et al., 2013; Yang et al., 2010; Matsushima et al., 2005). We used a Barrett WAM robot arm to play table tennis against human players. The robot's hardware constraints impose strong limitations on its acceleration, which severely restricts its movement abilities. This limitation can best be illustrated using typical table tennis stroke movements as shown in Fig. 4, see (Ramanantsoa and Durey, 1994; Mülling et al., 2011), which consist of four stages, namely awaiting stage, preparation stage, hitting stage, and finishing stage. In the awaiting stage, the ball moves toward the opponent and is returned by the opponent. The robot player moves to the awaiting pose and stays there during this stage. The preparation stage starts when the hitting movement is chosen according to the predicted opponent's target. The arm swings backward to a preparation pose. The robot requires sufficient time to execute a ball-hitting plan in the hitting stage. To achieve the required velocity for returning the ball in the hitting stage, movement initiation to an appropriate preparation pose in the preparation stage is needed, which is often before the opponent hits the ball. The robot player uses different preparation poses for different hitting plans. Hence, it is necessary to choose among them based on modeling the opponent's preference (Wang et al.,
2 Our methods allows the robot to initiate its movement at least 80 ms before the opponent hits the ball. As the ball can usually be reliably predicted more than 120 ms after the opponent returns, the robot could gain more than 200 ms additional execution time by using our prediction method. 3 http://robot-learning.de/Research/ProbabilisticMovementModeling

21

(a) Awaiting Stage

(b) Preparation Stage

(c) Hitting Stage

(d) Finishing Stage

Figure 4: The four stages of a typical table tennis ball rally are shown with the red curve representing the ball trajectories. Blue trajectories depict the typical racket movements of players. The racket of human player is to the left of the table in the pictures. Figures are adapted from (Mülling et al., 2011). 2011a) and inference of the opponent's target location for the ball (Wang et al., 2011b). The robot perceives the ball and the opponent's racket in real-time, using seven Prosilica GE640C cameras. These cameras were synchronized and calibrated to the coordinate system of the robot. The ball tracking system uses four cameras to capture the ball on both courts of the table (Lampert and Peters, 2012). The racket tracking system provides the information of the opponent's racket, i.e., the position and orientation (Wang et al., 2011b). As a result, the observation zt includes the ball's position and velocity as well as the opponent's racket position, velocity, and orientation before the human plays the ball. For the anticipation system described here, we process the observations every 80 ms. Here, the position and velocity of the ball were processed online with an extended Kalman filter, based on a known physical model (Mülling et al., 2011). However, the same smoothing method cannot be applied to the racket's trajectory, as its dynamics are directed by the unknown intended target. Therefore, the obtained states of the racket were subject to substantial noise and the model has to be robust to this noise. The proposed inference method can jointly smooth on the racket's trajectory, given by the smoothing distribution q , and infer the intended target, given by the belief B . In our setting, the robot always chooses its hitting point on a virtual hitting plane, which is 80 cm behind the table, as shown in Fig. 5. We define the human's intended target g as the intersection of the returned ball's trajectory with the robot's virtual hitting plane. As the x-coordinate (see Fig. 5) is most important for choosing among forehand/middle/backhand hitting plans (Wang et al., 2011b), the intention g considered here is the x-coordinate of the hitting point. Physical limitations of the robot restrict the x-coordinate to the range to ±1.2 m from the robot's base (table is 1.52 m

22

Figure 5: The robot's hitting point is the intersection of the coming ball's trajectory and the virtual hitting plane 80 cm behind the table. Figure is adapted from (Mülling et al., 2011). wide). To evaluate the performance of the target prediction, we collected a data set with recorded stroke movements from different human players. The true targets were obtained from the ball tracking system. The data set was divided into a training set of 100 plays and a test set of 126 plays. The standard deviation of the target coordinate in the test set is 102.2 cm. A straightforward approach to prediction is to learn a mapping from the features zt (including the position, orientation, and velocity of the racket and the position and velocity of the ball) to the target g . We compared our method to this baseline Gaussian Process Regression (GPR) using a Gaussian kernel with automatic relevance determination (Rasmussen and Williams, 2006). We considered using a sliding window on the sequence of observations, and conducted model selection to choose the optimal window size. The best accuracy of GPR was achieved when using a sliding window of size two, i.e., the input features consist of zt-1 and zt . The hyperparameters were learned by maximizing the marginal likelihood of training data, following the standard routine (Rasmussen and Williams, 2006). For every recorded play, we compared the performance of the proposed IDDM intention inference and the GPR prediction at 80 ms, 160 ms, 240 ms, and 320 ms before the opponent hits the ball. Note that this time step was only used such that the algorithms could be compared, and that the algorithms were not aware of the hitting time of the opponent in advance. We evaluated both the batch algorithm and online algorithm.

5.2 Results
As demonstrated in Fig. 6, the proposed IDDM model outperformed the GPR baseline. At 80 ms before the opponent hit the ball, the batch algorithm resulted in the mean absolute error of 31.5 cm, which achieved a 11.3% improvement over the GPR, whose average error was 35.6 cm. The online algorithm had a mean absolute error of 32.5 cm,

23

Online 40 Mean absolute error in cm

Batch

GPR

35

30

25

20

320

240 160 time in ms before stroke

80

Figure 6: Mean absolute error of the ball's target with standard error of the mean. The algorithms use the observations obtained before the opponent has hit the ball. which also outperformed GPR by an 8.5% improvement in the accuracy. One modelfree naive intention prediction is to always predict the median of the intentions in the training set. This naive prediction model caused an error of 78.8 cm. Hence, both the GPR and IDDM substantially outperformed naive goal prediction. The online algorithm, with a shrinking factor = 0.2 given in Eq. (51), took on average 70 ms to process every observation, which can potentially fulfill the real-time requirements of 80 ms. The batch algorithm used a sliding window of size 4, and took on average 300 ms to process every observation. The online algorithm was significantly faster than the batch algorithm, with a small loss in accuracy4 . Nevertheless, a certain amount of error is tolerable since the robot can apply small changes to its basic hitting plan based on the ball's trajectory. Therefore, we advocate the use of the online algorithm for applications with tight real-time constraints. We performed model selection to determine the covariance function kz , which can be either an isotropic Gaussian kernel, see Eq. (10), or a linear kernel, see Eq. (11). Furthermore, we performed model selection to find the dimension Dx of the latent states. In the experiments, the model was selected by cross-validation on the training set. The best model under consideration was with a linear kernel and a four dimensional latent state space. Experiments on the test set verified the model selection result, as shown in Table 2.
4 The reason is that the online algorithm only updates the smoothing distribution q (x t-1:t ) instead of the entire smoothing distribution q (x1:T ), and, hence, reduces the time complexity by a factor of T , see Section 3.3 and 4.3

24

Table 2: The mean absolute errors (in cm) with standard error of the mean of the goal inference made 80 ms before the opponent hits the ball, where Dx denotes the dimensionality of the state space. kernel linear Gaussian Dx = 3 41.5 ± 3.0 38.5 ± 2.7 Dx = 4 31.5 ± 2.2 34.2 ± 2.5 Dx = 5 35.4 ± 2.4 34.4 ± 2.7 Dx = 6 37.0 ± 2.6 37.3 ± 2.7

(a) Forehand pose.

(b) Middle pose.

(c) Backhand pose.

Figure 7: Preparation poses of the three pre-defined hitting movements in the prototype system, i.e., (a) forehand, (b) middle, and (c) backhand. The shadowed areas represent the corresponding hitting regions. Our results demonstrated that the IDDM can improve the target prediction in robot table tennis and choose the correct hitting plan. We have developed a proof-of-concept prototype system in which the robot is equipped with three pre-defined hitting movements, i.e., forehand, middle, and backhand movements, with their hitting regions shown in Fig. 7. As exhibited in Fig. 8, our method allows the robot to choose the responding hitting movement before the opponent has hit the ball himself, which is often necessary for the robot to have sufficient time to execute the hitting movement, and substantially expands the robot's overall hitting region to cover almost the entire accessible workspace, see the video. Furthermore, we expect that the method can further enhance the robot's capability when equipped with more and self-improving hitting primitives (Mülling et al., 2013).

6 Action Recognition in Human-Robot Interaction
To realize safe and meaningful human-robot interaction, it is important that robots can recognize the human's action. The advent of robust, marker-less motion capture techniques (Shotton et al., 2011) has provided us with the technology to record the full skeletal configuration of the human during HRI. Nevertheless, recognition of the human's action from this high-dimensional data set poses serious challenges. In this paper, we show that the IDDM has the potential to infer the intention of actions from movements in a simplified scenario. Using a Kinect camera, we recorded the 32-dimensional skeletal configuration of a human during the execution of a set of

25

approx. 320ms 0.5 0.4 posterior 0.3 0.2 0.1 0 backhand middle forehand

approx. 160ms

approx. 80ms (before hit)

-0.2

0.4

X

-0.2

0.4

X

-0.2

0.4

X

Figure 8: Bar plots show the distribution of the target (X coordinate) at approximately 320ms, 160ms, and 80ms before the player hits the ball. The prediction becomes more and more certain as the player finishes the stroke, and the robot later chose the middle hitting movement accordingly.

26

Table 3: Comparison of the accuracy and efficiency using different algorithms for the action recognition task. Here, n denotes the size of sliding windows and is the shrinking factor of the online method. algorithm SVM(n=5) GPC(n=5) batch(n=4) batch(n=5) batch(n=6) online( =0.3) online( =0.2) online( =0.1) accuracy 77.5% 79.4% 79.0% 83.8% 83.0% 83.0% 83.0% 82.6% time(s) <0.01 >1 0.27 0.32 0.39 0.07 0.07 0.07

actions namely: crouching (C), jumping (J), kick-high (KH), kick-low (KL), defense (D), punch-high (PH), punch-low (PL), and turn-kick (TK). For each type of action we collected a training set consisting of ten repetitions and a test set of three repetitions. The system down-sampled the output of Kinect and processes three skeletal configurations per second. In this task, the intention g is a discrete variable and corresponds to the type of action. Action recognition can be regarded as a classification problem. We compared our proposed algorithms to Support Vector Machines (SVMs), see (Schölkopf and Smola, 2001), and multi-class Gaussian Process Classification (GPC), see (Khan et al., 2012). We used off-the-shelf toolboxes, i.e., LIBSVM (Chang and Lin, 2011) and catLGM5 , and followed their standard routines for prediction. The algorithms made a prediction after observing a new skeletal configuration. The batch algorithm used a sliding window of length n = 5, i.e., it recognized actions based on the recent n observations. We chose the IDDM with a linear covariance function for the covariance function kz of the measurement GP and a two-dimensional latent state space. The batch algorithm achieved the precision of 83.8%, which outperformed SVM (77.5%) and GPC (79.4%) using the same sliding windows. The online algorithm achieved the precision of 83.0% with significantly reduced computational time. We observed that both the SVM and GPC confused crouching with jumping, as they were similar in the early and late stages. In contrast, the IDDM could distinguish between crouching (C) and jumping (J) from their different dynamics, which became clearly separable while the human performed the actions. The batch algorithm needs to choose the size of sliding windows, which influences both the accuracy and efficiency. As shown in Table 3, the batch algorithm could yield real-time action recognition in 3 Hz with a sliding window of size 5. The online algorithm, as shown in Table 3, achieved a speedup of over four times compared to the batch algorithm with a sliding window. The online algorithm relies on the shrinking factor in Eq. (51), which describes how likely the type of actions is expected to change. We also found that the performance of online algorithm is not sensitive to this
5 http://www.cs.ubc.ca/~emtiyaz/software/catLGM.html

27

parameter.

7 Discussion
In this article, we have proposed the intention-driven dynamics model (IDDM), a latent-variable model for inferring intentions from observed human movements. We have introduced efficient approximate inference algorithms that allow for real-time inference. Our contributions include: (1) suggesting the IDDM, which simultaneously finds a latent state representation of noisy and high-dimensional observations and models the dynamics that are driven by the intention; (2) introducing an online algorithm to efficiently infer the human's intention from an ongoing movement; (3) verifying the proposed model in two human-robot interaction scenarios. In particular, we have considered target inference in robot table tennis and action recognition for interactive robots. In these two scenarios, we show that modeling the intention-driven dynamics can achieve better predictions than algorithms without modeling the dynamics. The proposed method outperformed the GPR in the robot table tennis scenario and SVM and GPC in the action recognition scenario. Nevertheless, we would not draw the overstated conclusion that IDDM is a better model than SVM or GP based on these empirical results, as this discussion would be a comparison of generative and discriminative models. The performance of IDDM and SVM/GP should be studied on a case-by-case basis. However, two important properties of these approaches should be noticed: (1) computational efficiency and (2) robustness to measurement noise. Firstly, the IDDM is often more computationally demanding than GP and SVM. Nevertheless, the proposed online inference method, and described possible approximations, make the IDDM applicable to real-time scenarios. As demonstrated in the prototype robot table tennis system, the IDDM was successfully used in a real system with tight time constraints. Secondly, the IDDM is generally less prone to measurement noise than SVM/GP, as it models the noise in the generative process of observations. In conclusion, the IDDM takes into account the generative process of movements in which the intention is the driving factor. Hence, we advocate the use of IDDM when the movement is indeed driven by the intention (or target to predict), as the IDDM captures the causal relationship of the intention and the observed movements.

Acknowledgments
Part of the research leading to these results has received funding from the European Community's Seventh Framework Programme under grant agreements no. ICT-270327 (CompLACS) and no. ICT-248273 (GeRT). We thank Abdeslam Boularias for valuable discussions on this work.

References
Abbeel, P. and Ng, A. (2004). Apprenticeship learning via inverse reinforcement learning. In International Conference on Machine Learning. 28

Anderson, R. (1988). A Robot Ping-Pong Player: Experiment in Real-time Intelligent Control. MIT Press. Andrieu, C., De Freitas, N., Doucet, A., and Jordan, M. (2003). An introduction to MCMC for machine learning. Machine Learning, 50(1):5­43. Baker, C., Saxe, R., and Tenenbaum, J. (2009). Action understanding as inverse planning. Cognition, 113(3). Baker, C., Tenenbaum, J., and Saxe, R. (2006). Bayesian models of human action understanding. In Advances in Neural Information Processing Systems. Bandyopadhyay, T., Won, K. S., Frazzoli, E., Hsu, D., Lee, W. S., and Rus, D. (2012). Intention-aware motion planning. In International Workshop on the Algorithmic Foundations of Robotics. Billingsley, J. (1984). Machineroe joins new title fight. Practical Robotics, pages 14­16. Bishop, C. (2006). Pattern Recognition and Machine Learning. Springer New York. Bitzer, S. and Vijayakumar, S. (2009). Latent spaces for dynamic movement primitives. In IEEE-RAS International Conference on Humanoid Robots, pages 574­581. IEEE. Chang, C.-C. and Lin, C.-J. (2011). LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1­27:27. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm. Damianou, A., Titsias, M., and Lawrence, N. (2011). Variational Gaussian process dynamical systems. In Advances in Neural Information Processing Systems. Deisenroth, M. (2010). Efficient Reinforcement Learning using Gaussian Processes. KIT Scientific Publ. Deisenroth, M., Huber, M., and Hanebeck, U. (2009). Analytic moment-based Gaussian process filtering. In International Conference on Machine Learning. Deisenroth, M. and Ohlsson, H. (2011). A general perspective on Gaussian filtering and smoothing. In American Control Conference. Deisenroth, M., Turner, R., Huber, M., Hanebeck, U., and Rasmussen, C. (2012). Robust filtering and smoothing with Gaussian processes. Trans. on Automatic Control. Ding, C. and Peng, H. (2005). Minimum redundancy feature selection from microarray gene expression data. Journal of bioinformatics and computational biology, 3(02):185­205. Fässler, H., Beyer, H., and Wen, J. (1990). A robot ping pong player: optimized mechanics, high perfromance 3d vision, and intelligent sensor control. Robotersysteme, 6(3):161­170.

29

Friesen, A. and Rao, R. (2011). Gaze following as goal inference: A Bayesian model. In Annual Conference of the Cognitive Science Society. Ghahramani, Z. and Roweis, S. (1999). Learning nonlinear dynamical systems using an em algorithm. In Advances in Neural Information Processing Systems. Hauser, K. (2012). Recognition, prediction, and planning for assisted teleoperation of freeform tasks. In Proceedings of Robotics: Science & Systems. Huang, Y., Xu, D., Tan, M., and Su, H. (2013). Adding active learning to LWR for ping-pong playing robot. IEEE Trans. on Control Systems Technology, accepted for publication. Ijspeert, A., Nakanishi, J., and Schaal, S. (2002). Movement imitation with nonlinear dynamical systems in humanoid robots. In IEEE International Conference on Robotics and Automation. Jenkins, O., Serrano, G., and Loper, M. (2007). Interactive human pose and action recognition using dynamical motion primitives. International Journal of Humanoid Robotics, 4(2):365­386. Khan, M., Mohamed, S., Marlin, B., and Murphy, K. (2012). A stick-breaking likelihood for categorical data analysis with latent Gaussian models. In International Conference on Artificial Intelligence and Statistics. Ko, J. and Fox, D. (2009). GP-BayesFilters: Bayesian filtering using Gaussian process prediction and observation models. Autonomous Robots, 27(1):75­90. Ko, J. and Fox, D. (2011). Learning GP-BayesFilters via Gaussian process latent variable models. Autonomous Robots, 30(1):3­23. Kuderer, M., Kretzschmar, H., Sprunk, C., and Burgard, W. (2012). Feature-based prediction of trajectories for socially compliant navigation. In Proceedings of Robotics: Science & Systems. Kurniawati, H., Du, Y., Hsu, D., and Lee, W. (2011). Motion planning under uncertainty for robotic tasks with long time horizons. The International Journal of Robotics Research, 30(3):308­323. Lampert, C. H. and Peters, J. (2012). Real-time detection of colored objects in multiple camera streams with off-the-shelf hardware components. Journal of Real-Time Image Processing. Lawrence, N. (2004). Gaussian process latent variable models for visualization of high dimensional data. In Advances in Neural Information Processing Systems. Lawrence, N. (2005). Probabilistic non-linear principal component analysis with Gaussian process latent variable models. The Journal of Machine Learning Research, 6:1783­1816.

30

Liao, L., Patterson, D., Fox, D., and Kautz, H. (2007). Learning and inferring transportation routines. Artificial Intelligence, 171(5-6). Matsushima, M., Hashimoto, T., Takeuchi, M., and Miyazaki, F. (2005). A learning approach to robotic table tennis. IEEE Trans. on Robotics, 21(4). Møller, M. (1993). A scaled conjugate gradient algorithm for fast supervised learning. Neural networks, 6(4):525­533. Mülling, K., Kober, J., Kroemer, O., and Peters, J. (2013). Learning to select and generalize striking movements in robot table tennis. International Journal of Robotics Research, accepted for publication. Mülling, K., Kober, J., and Peters, J. (2011). A biomimetic approach to robot table tennis. Adaptive Behavior, 19(5):359­376. Pentland, A. and Liu, A. (1999). Modeling and prediction of human behavior. Neural Computation, 11(1). Quiñonero-Candela, J., Girard, A., Larsen, J., and Rasmussen, C. (2003). Propagation of uncertainty in Bayesian kernel models-application to multiple-step ahead forecasting. In IEEE International Conference on Acoustics, Speech, and Signal Processing. Quiñonero-Candela, J. and Rasmussen, C. (2005). A unifying view of sparse approximate Gaussian process regression. The Journal of Machine Learning Research, 6:1939­1959. Ramanantsoa, M. and Durey, A. (1994). Towards a stroke construction model. International Journal of Table Tennis Science, 2:97­114. Rao, R., Shon, A., and Meltzoff, A. (2004). A Bayesian model of imitation in infants and robots. Imitation and Social Learning in Robots, Humans, and Animals. Rasmussen, C. and Williams, C. (2006). Gaussian Processes for Machine Learning. MIT Press. Schölkopf, B. and Smola, A. (2001). Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press. Shotton, J., Fitzgibbon, A., Cook, M., Sharp, T., Finocchio, M., Moore, R., Kipman, A., and Blake, A. (2011). Real-time human pose recognition in parts from single depth images. In IEEE Conference on Computer Vision and Pattern Recognition. Simon, M. (1982). Understanding Human Action: Social Explanation and the Vision of Social Science. State University of New York Press. Snelson, E. and Ghahramani, Z. (2006). Sparse Gaussian processes using pseudoinputs. In Advances in Neural Information Processing Systems.

31

Turner, R., Deisenroth, M., and Rasmussen, C. (2010). State-space inference and learning with Gaussian processes. In International Conference on Artificial Intelligence and Statistics. van der Maaten, L., Postma, E., and van den Herik, J. (2009). Dimensionality reduction: A comparative review. Journal of Machine Learning Research, 10:1­41. Vasquez, D., Fraichard, T., Aycard, O., and Laugier, C. (2008). Intentional motion on-line learning and prediction. Machine Vision and Applications, 19(5):411­425. Vasquez, D., Fraichard, T., and Laugier, C. (2009). Growing hidden Markov models: An incremental tool for learning and predicting human and vehicle motion. The International Journal of Robotics Research, 28(11-12):1486­1506. Wang, J., Fleet, D., and Hertzmann, A. (2008). Gaussian process dynamical models for human motion. IEEE Trans. on Pattern Analysis and Machine Intelligence, 30(2):283­298. Wang, Y., Won, K., Hsu, D., and Lee, W. (2012a). Monte Carlo Bayesian reinforcement learning. In International Conference on Machine Learning. Wang, Z., Boularias, A., Mülling, K., and Peters, J. (2011a). Balancing safety and exploitability in opponent modeling. In AAAI Conference on Artificial Intelligence. Wang, Z., Deisenroth, M., Amor, H., Vogt, D., Schölkopf, B., and Peters, J. (2012b). Probabilistic modeling of human movements for intention inference. In Proceedings of Robotics: Science and Systems. Wang, Z., Lampert, C., Mülling, K., Schölkopf, B., and Peters, J. (2011b). Learning anticipation policies for robot table tennis. In IEEE/RSJ International Conference on Intelligent Robots and Systems. Williams, A., Ward, P., Knowles, J., and Smeeton, N. (2002). Anticipation skill in a real-world task: Measurement, training, and transfer in tennis. Journal of Experimental Psychology, 8(4):259. Yang, P., Xu, D., Wang, H., and Zhang, Z. (2010). Control system design for a 5-DOF table tennis robot. In International Conference on Control Automation Robotics & Vision, pages 1731­1735. Ziebart, B., Dey, A., and Bagnell, J. (2012). Probabilistic pointing target prediction via inverse optimal control. In ACM International Conference on Intelligent User Interfaces, pages 1­10. Ziebart, B., Maas, A., Bagnell, J., and Dey, A. (2008). Maximum entropy inverse reinforcement learning. In AAAI Conference on Artificial Intelligence, pages 1433­ 1438. Ziebart, B., Ratliff, N., Gallagher, G., Mertz, C., Peterson, K., Bagnell, J., Hebert, M., Dey, A., and Srinivasa, S. (2009). Planning-based prediction for pedestrians. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 3931­ 3936. 32

2013 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) November 3-7, 2013. Tokyo, Japan

Learning Responsive Robot Behavior by Imitation
Heni Ben Amor1 , David Vogt2 , Marco Ewerton1 , Erik Berger2 , Bernhard Jung2 , Jan Peters1

Abstract-- In this paper we present a new approach for learning responsive robot behavior by imitation of human interaction partners. Extending previous work on robot imitation learning, that has so far mostly concentrated on learning from demonstrations by a single actor, we simultaneously record the movements of two humans engaged in on-going interaction tasks and learn compact models of the interaction. Extracted interaction models can thereafter be used by a robot to engage in a similar interaction with a human partner. We present two algorithms for deriving interaction models from motion capture data as well as experimental results on a humanoid robot.

I. INTRODUCTION While robots are becoming increasingly better at performing a wide range of motor skills, they are still limited in their human interaction capabilities. To date, most robots are not prepared to appropriately respond to the movements or the behavior of a human partner. However, with application domains of robots coming closer to our everyday life, there is a need for adaptive algorithms that ensure responsive robot behavior for human-robot interaction. We present a new approach to robot learning that allows anthropomorphic robots to learn a library of interaction skills from demonstration. Traditional approaches to modelling interactions assume a pre-specified symbolic representation of the available actions. For example, they model interactions in terms of commands such as wait, pick-up, and place. Instead of such a top-down approach, we want to focus on learning responsive behavior in a bottom-up fashion using a trajectory based approach. The key idea behind our approach is that the observation of human-human collaborations can provide rich information specifying how and when to interact in a particular situation. For example, by observing how two human workmen collaborate on lifting a heavy box, a robot could use machine learning algorithms to extract an interaction model that specifies the states, movements, and situational responses of the involved parties. In turn, such a model can be used by the robot to assist in a similar lifting task. Our approach is as an extension of imitation learning [3] to multi-agent scenarios, in which the behavior and the mutual interplay between two agents is imitated. In this paper, we describe the general multi-agent imitation learning setup for learning interaction models from motion capture data. We also provide two first algorithms that enable a robot to learn such interaction models between interacting
1 Heni Ben Amor, Marco Ewerton and Jan Peters are with the Technische Universitaet Darmstadt, Intelligent Autonomous Systems, Darmstadt, Germany. {amor,ewerton,peters}@ias.tu-darmstadt.de 2 David Vogt, Erik Berger and Bernhard Jung are with the Technische Universitaet Bergakademie Freiberg, Virtual Reality and Multimedia Group, Freiberg, Germany. {david.vogt,bergere,jungb}@tu-freiberg.de

Fig. 1. A humanoid robot receives a book that is handed over by a human interaction partner. The robot learned what to do in this situation by observing a similar situation between two humans.

agents. The first algorithm PPCA-IM (Probabilistic Principal Component Analysis-Interaction Model) frames the task as a missing value estimation problem. The second algorithm called PM-IM (Path Map-Interaction Model) uses a Hidden Markov Model (HMM) [20] to represent the mutual dependencies of the interacting agents. A set of shared latent states is used to map the behavior of one agent to the behavior of the interaction partner. The principal difference between the two algorithms presented in this paper is the representation of the temporal dynamics of interaction. The PPCA-IM uses an implicit representation of time via a temporal embedding of the training data. In contrast, the PM-IM uses an explicit representation of time via a discrete set of hidden nodes. Through a series of experiments, we will show how the two algorithms can be used to create a responsive robot that learns to react to the movements and gestures of humans. We will also provide a comparison of PPCA-IM and PM-IM and discuss the advantages and drawbacks of each approach. II. R ELATED W ORK Finding simple and natural ways of specifying robot control programs is a focal point in robotics. Imitation learning, also known as Programming by Demonstration, has been proposed as a possible solution to this problem [22]. Based on human-provided demonstrations of a specific skill, a robot autonomously generates a control program that allows it to generalize the skill to different situations. Most approaches to imitation learning obtain a control policy which encodes the behavior demonstrated by the user. The policy can subsequently be used to generate a similar behavior that is

978-1-4673-6358-7/13/$31.00 ©2013 IEEE

3257

Fig. 2. Overview of the interaction learning approach presented in this paper. The interaction behavior of two humans is observed, analyzed and imitated in order in human-robot interaction scenarios. Left: The movements of two persons are recorded using motion capture technology. Middle: A compact interaction model specifying the mutual influences and responses is learned. Right: The interaction model enables a robot to compute the best response to the current behavior of a human interaction partner.

adapted to the current situation. For example, the Dynamical Motor Primitive (DMP) [13] approach uses dynamical systems to represent control policies. The DMP approach has been widely accepted in the imitation learning community and has been used to learn various motor skills such as locomotion [16], or drumming [13]. Another way of encoding policies is to use statistical modelling methods. For example, in the Mimesis Model [17] a continuous hidden Markov model is used for encoding the teacher's demonstrations. A similar approach to motion generation is presented by Calinon et al. [7] who used Gaussian Mixture Regression to learn gestures. The advantage of statistical and probabilistic approaches, is the ability to naturally model the spatial and temporal variability of human motion. The methods discussed so far are limited to single agent imitation learning scenarios. Once the behavior is learned, it is executed without taking into account the reaction of an interaction partner. In recent years, various attempts have been undertaken for using machine learning in human-robot interaction scenarios. In [15], a recurrent neural network was used to learn a simple interaction game between a human and a robot. More recently, Wang et al. [24] presented an extension of the Gaussian Process Dynamics Model that was used to infer the intention of a human player during a tabletennis game. Through the analysis of the human player's movement, a robot player was able to determine the position to which the ball will be returned. This predictive ability allowed the robot to initiate its movements even before the human hit the ball. In [14], Gaussian mixture models were used to adapt the timing of a humanoid robot to that of a human partner in close-contact interaction scenarios. The parameters of the interaction model were updated using binary evaluation information obtained from the human. While the approach allowed for human-in-the-loop learning and adaptation, it did not include any imitation of observed interactions. In a similar vein, the work in [17] showed how a robot can be actively involved in learning how to interact with a

human partner. The robot performed a previously learned motion pattern and observed the partner's reaction to it. Learning was realized by recognizing the observed reaction and by encoding the action-reaction patterns in a HMM. The HMM was then used to synthesize similar interactions. In contrast, in our approach, learning of motion and interaction is not split into two parts. Instead, we learn one integrated interaction model which can directly synthesize an appropriate movement in response to an observed movement of the human partner. Further, instead of modelling symbolic action-reaction pairs, our approach is based on modelling the joint dynamics during the execution of a movement. In general, while the learning approaches discussed above are placed within human-robot interaction settings, they only learn from demonstrations by a single actor at a time. In contrast, the work presented here focuses on imitation learning from simultaneously recorded movements by two human interaction partners in order to learn integrated models of the joint interaction. III. L EARNING I NTERACTION M ODELS The goal of learning an interaction model is to derive a compact representation of how two agents behave and, in particular, how they react to each other when they perform a cooperative or competitive task together. The approach followed in this paper derives such a representation from observations of human-human interactions. In Figure 2 we see an overview of this approach. First, the movements of a pair of persons performing a competitive (or cooperative) task are recorded using motion capture technology. Subsequently, an interaction model is learned from the recorded data. The interaction model captures the reciprocal influences during the execution of the task. In turn, the interaction model enables us to predict the state (skeletal configuration) of one human based on the observed states of the second human. Finally, the learned model is used by a robot to engage in a similar interaction with a human partner. In the example depicted in Figure 2, the humanoid robot learns to perfom defensive movements in response to a human performing punching movements.

3258

An interaction model can be regarded as a mapping from the current state of one agent to the state of a second agent. In our particular application, we want to learn a mapping from the state of the opponent agent (i.e. the human) to the state of the controlled agent (i.e. the robot)1 . Input to the learning algorithms are the two data sets A (controlled agent) and B (opponent agent) consisting of joint angle configurations of the two agents. Each point in A contains information about the skeletal configuration of the controlled agent at a particular time step, while B contains a joint angle configurations for the opponent agent. Once a mapping from B to A is learned, it can be used to compute the most appropriate response of the controlled agent, given the observed movements of the opponent agent. In this section, we will present two algorithms that can learn such a mapping. A. Algorithm 1: PPCA-IM The first algorithm that we present is the Probabilistic Principal Component Analysis - Interaction Model. The method exploits the low-dimensional nature of human movement in order to create a compact model of the interaction. It is well known from human motor control, that motor tasks, e.g., grasping [21], walking [9], and also interactions between humans [4] lie on low-dimensional manifolds. Such a manifold typically has a much smaller dimensionality than the total number of joints involved in the motor task. Therefore, instead of finding a mapping in the high-dimensional space involving all joints, we can find a low-dimensional space in which the relationship between the postures and movements can be learned in a more efficient way. After learning is finished, the joint values of the controlled agent are treated as missing values that are estimated by maximizing the likelihood in the low-dimensional latent space. The first step to PPCA-IM is the temporal embedding of the opponent agent's data. Temporal embedding allows us to disambiguate between similar movements by including information from prior time steps. In the second step, namely dimensionality reduction, we then compute a lowdimensional projection of the data set and use this space to estimate the most likely response for the controlled agent. 1) Temporal Embedding: One possible approach to learning an interaction model is to learn a mapping between individual pairs of samples in A and B directly. However, such an approach does not take the temporal offset between action and reaction into account, and is therefore prone to fail for many behaviors. For example, a stretched out arm can either mean that the opponent wants to shake hands or perform a Karate movement. To disambiguate the behavior in such scenarios it is important to take the temporal development of the movement into account. When using PPCA-IM we perform a temporal embedding of the data in
the sake of clarity we will henceforth use the terms controlled agent and opponent agent to refer to the agents involved in an interaction. Note that this naming convention does not restrict the application to competitive tasks only.
1 For

B yielding a new data set B . To each point in B we add joint angles of the  last time steps:   bt ,  .  b (1)   bt  B, t > . t = . . bt- This embedding is comparable to modelling the interaction as a Markov chain of order  , rather than a traditional firstorder Markov chain. A similar preprocessing of the data was proposed in [2]. 2) Dimensionality Reduction: The next step in PPCA-IM is to create a shared latent space that models the interaction dynamics of the two agents. As the name of the algorithm suggests, we use Probabilistic Principal Component Analysis (PPCA) to learn a shared low-dimensional representation of the movements. To this end, we create a combined data set Z which is a concatenation of A and B :
  z = [at , b t ]  at , bt  A, B

(2)

On the new data set Z we can then perform PPCA. PPCA is an iterative version of the original PCA algorithm which uses Expectation-Maximization (EM) [11] to determine the optimal projection matrix C that maps the data set Z onto a lower-dimensional principal component space. The PPCA algorithm used here is based on [19]. An advantage of PPCA over PCA is that it provides a probabilistic framework for performing PCA on data sets that have missing values. In our case, we treat the current joint angles of the controlled agent as missing values that need to be estimated. The EMalgorithm can be used to estimate the missing values of our data. This estimation is done by adding an additional entry z to Z, which consists of an observed part zo and a hidden part zh . The observed part contains (temporally embedded) joint values zo of the opponent agent. The hidden part zh will be estimated during the EM-algorithm and is initially filled with zeros. Before starting the EM algorithm, we initialize the projection matrix C and the variance  2 with random values between zero and one. In the E-step, we first calculate new estimates for the covariance matrix  and matrix Y of projected points: E-Step:  Y  I +  -2 CCT   -2 CZ,
-1

,

Based on these estimates, we can update in the M-step the projection matrix C and the variance  2 : M-Step: C 
2

ZYT (S  + YYT )-1 , 1 S Tr{CT C} + ||zi - CT yi ||2 + Dh  2 ,  SD i=1
S

where S is the number of samples, D is the dimensionality of the samples, and Dh is the total number of missing

3259

200

PC 1

0

-200

-200

0

200

-200

=0

 = 10

PC 2

0

200

-200

0

200

 = 20

Fig. 4. PPCA projections of a defense interaction for different values of  . During the interaction the opponent agent attacks and retracts back to the rest pose while the controlled agent goes to defense stance and then retracts to the rest pose. When  = 0 the temporal context of a posture is not taken into account.

Fig. 3. The projection of high-five motions into a low-dimensional space using Probabilistic Principal Component Analysis. Each point in this space corresponds to specific interaction situation and defines the postures of both agents. Even if we observe the postures of one agent only, we can still infer the most likely posture of the interaction partner using PPCA and missing value estimation.

values in Z. The missing values zh of the matrix Z are re-estimated before performing the next E-Step by first calculating Zestim : Zestim = CT Y, (3)

and then replacing the missing values zh with the newly estimated values from Zestim . The above EM-steps can be iterated until the change in the error of the following objective function is below a given threshold (in our experiments the threshold is 10-5 ):
2 (C,  ) = SD log  2 +  -2 Dh old + S

pose. Figure 4 shows the projected movement for different values of parameter  . When  = 0 (Figure 4 left), the postures for going towards the defensive stance (green) and the postures for retracting back from the defensive stance (red) are mapped onto the same points in the low-dimensional space. As a result a robot cannot distinguish between the two different modes of this particular movement and would produce the same reaction in both situations. We can also see in Figure 4, that with increased value for  the points are more and more disentangled. We can see that  = 20 produces a clear separation between the two modes, i.e. going to and pulling back from the defense stance, of the movement. The trajectory starts at the rest posture in (-250, 0)T , moves to the defense stance at (250, 0)T , and then moves back to a position close to the rest posture. Note, that in this example we used a two-dimensional projection for visualization purposes only. To find a suitable value for the dimensionality of the low-dimensional space, we can use intrinsic dimensionality estimation methods [5]. A simpler approach, which was used in this paper, is to use the number of principal components that insures that 95% of the information in our training data is retained after PPCA. B. Algorithm 2: PM-IM

 -2
i=1 2 old

||zi - CT yi ||2 + Tr CT C

,

where is the previous value for the variance. Once the EM-algorithm is finished, we can use the missing values zh as the new desired joint angles for the controlled agent. Figure 3 shows the low-dimensional projection of a highfive interaction calculated with PPCA. Each point in the low-dimensional space encodes the reaction of the controlled agent with respect to the previous movements of the opponent agent. We can see that the interaction forms a smooth trajectory in the low-dimensional space. To better understand the role of temporal embedding in our learning algorithm, we computed several PPCA projections with different values for the parameter  (from Equation (1)). For this purpose, we recorded a defensive movement in which a human starts in a rest pose, then moves both arms in a defensive stance, and finally goes back to the rest

The second algorithm for learning interaction models is called Path Map-Interaction Model (PM-IM). The algorithm uses a HMM to represent the mutual dependency of the interaction partners at different time steps. A HMM is an efficient tool for modelling probability distributions over time-series'. It assumes that a set of observations was generated by a process with hidden internal states. Given the Markov assumption, any state st only depends on the predecessor state st-1 . Following the notation in [20] and [6], a HMM can be defined as the tuple  = (S , , Pj i , pi (o|si )) where
· ·

S = {s1 , ..., sN } is the set of states si of the HMM  = {1 , ..., N } is a probability distribution specifying the probability i of starting in state i Pj i is the state transition matrix defining the probability p(sj |si )of transitioning from state si to sj

·

3260

a1

a2

a3

S1

S2

S3

...

every situation. An interesting feature of HMMs is the ability to use several HMM models in parallel. For example, assume that we learn two HMMs for different interaction tasks, e.g., punching and handing-over. Given a new observed movement of the attacking agent, we can calculate the likelihood of this movement with respect to each learned HMM and select the model with highest likelihood according to:  = arg max p(bt |).


b1

b2

b3

(4)

Fig. 5. The graphical model of a path map Hidden Markov Model. Each hidden node (white) is connected to two observed nodes (colored) corresponding to each of the interacting agents. Each observed node contains the joint angle configuration of the respective agent which is depicted by a small skeleton. The diagram shows a path map for an punch/defense interaction.

Once the HMM with highest likelihood is selected, we can calculate the emissions for the current situation and use the resulting joint angle values for controlling the robot. The above feature allows us to use the knowledge of several HMMs in order to recognize an interaction scenario and also to respond to the behavior of the human partner. The set of interaction skills can, therefore, be gradually expanded. IV. E XPERIMENTS

·

p(o|si ) is the emission probability distribution which defines the probability of observing output o while in state si . The emission probability distribution is modelled using a Gaussian distribution.

As already mentioned, the nodes of an HMM can be divided into observable nodes and hidden nodes. Typically, an HMM is defined in such a way that each hidden node is connected to one observable node only. In the following, however, we will use an extension of HMM, sometimes also referred to as a path map[6], which has a different graph structure. A path map relates the time-series behavior of a cue system to the behavior a target system. This is achieved by connecting each hidden node to two observables nodes: one observable for the cue system and one observable for the target system. A path map for the task of interaction modelling can be seen in Figure 5. The colored nodes correspond to the observables of the controlled agent (blue) and the opponent agent (red) respectively. Each observable state holds the full joint angle configuration of the respective agent in the current situation. The white nodes depict the hidden states of the interaction task. Each hidden state models a specific context or situation during the interaction. In contrast to the standard HMM, a path map contains two emission probability distributions pA (at |st ) and pB (bt |st ); one for each of the two agents. The training of the path map, however, can be performed using the same approach as for a standard HMM. First, a K-Means[18] clustering algorithm is used to initialize the hidden states of the HMM. Using the EM [11] algorithm, we can then estimate all missing parameters of the HMM. A detailed description of HMM training can be found in [20]. Once it is learned, the path map in Figure 5 allows us to estimate the behavior of one agent by observing the movements of the other. We first calculate the most likely sequence of states given the observed behavior of the opponent agent. Using the emission probability distribution pB (bt |st ) of each state, we can then generate an appropriate response for the controlled agent in

To evaluate the algorithms proposed in this paper, we conducted a set of interaction experiments and analyzed the results. In the following sections, we will report the results achieved by applying the algorithms in simulation as well as on a real robot performing human-robot interaction with a human partner. A. Interaction Data Before training any specific model, we first collected a set of training data representing different competitive and cooperative interaction tasks. Specifically, we collected motion capture data of two interacting humans. The data set consisted of various interaction tasks acquired from the CMU Motion Capture Library2 , as well as additional data gathered using two Kinect cameras and two human subjects. In order to be independent of a specific tracking device, we transformed all motion capture data into the BioVision file format and used the resulting joint angle information as input to the learning algorithms. In total, we used 18 joints, with each joint being parametrized by three joint angles. The final data set consisted of four different interaction tasks: · Boxing: One agent attacks with punches at different heights and from different directions while the other agent defends. · Martial Arts: One agent attacks with punches and kicks while the other defends. · High Five: Both agents perform a high-five movement. · Handing over: One agent hands a book over to the other agent. B. Runtimes In interactive scenarios, a robot needs to quickly respond to the behavior of the human partner. In the following we will, therefore, analyze the computational demands of the proposed algorithms and the runtimes for predicting the appropriate response in the current situation. Figure 6 depicts
2 http://mocap.cs.cmu.edu/

3261

0.03

z-position [m ]

T ime [sec]

0.02

PPCA-IM 20 PPCA-IM 10 PPCA-IM 5

0.4 0.3 0.2

Human
PM-IM 20 cl. PM-IM 40 cl. PM-IM 60 cl.
2.0 1.8 1.6 1.4 1.2 1.0 0 20 40 60 80 100 120

Top Center Bottom

0.01

0.1 0.0 1

0.0 1

T ime step
2 3 4 5
2.0

2

3

4

5

Num. of Behaviors

Num. of Behaviors
z-position [m ]

PPCA-IM
2.0 1.8 1.6 1.4 1.2 1.0 0 20 40 60 80 100 120 0 20 40 1.8 1.6 1.4 1.2 1.0

PM-IM

Fig. 6. The runtime of the PPCA-IM and the PM-IM algorithms. The values indicate the measured time needed for predicting the optimal response of the robot given the human's action at a particular time step. With increasing number of learned behaviors, the time needed to predict the optimal response increases, too. Additionally, the plots also show how the size of the temporal embedding window (20,10,5) affects the runtime of the PPCA. The right plot shows how the number of states/clusters affects the runtime of the PM-IM.

60

80

100

120

T ime step

T ime step

the runtimes of the PPCA-IM and the PM-IM algorithms. For training we used an increasingly complex data set with one to five different behaviors. Each behavior consisted of approx. 120 data samples. The plots show how the number of interactive behaviors affects the response time of the robot when applying an interaction model after learning. As can be seen in Figure 6, PPCA-IM has a significantly faster response time. Especially, with increasing number of clusters/states in the PM-IM, the response times quickly deteriorate. With 60 hidden states, the PM-IM requires about 0.4 seconds to compute a prediction. A smaller number of states can be used to speed up the algorithm. However, this comes at the price of a significantly lower quality of the learned model. In the above example, the PM-IM was only able to produce accurate responses when 55 or more states were used. The PPCA-IM was used with a 7 dimensional latent space. C. Generalization Another important feature of interaction models is the ability to generalize learned behaviors to new situations. To analyze the generalization ability, we conducted a set of experiments in which we trained interaction models for a boxing/defending behavior. The models were trained with high- and low-punches, and were later tested with several other punches that aimed at a position inbetween the trained punches. Figure 7 shows the z-position of the wrist of the controlled agent while trying to defend several punches. The gound truth data gathered from the human clearly shows that the hand needs to be lifted to different levels, in order to defend from the upcoming punch. The trajectories generated with the PPCA-IM model have similar characteristics to the human trajectories. The blue trajectory in Figure 7 corresponds to a movement that was not seen in the training data. Despite that it was not trained, the PPCA-IM was still able to generalize the learned movement to this new situation. Both the shape and height of the trajectory are close to the ground truth of the human demonstrator. In contrast to that, the PM-IM does not exhibit a similar generalization ability. In the depicted case, the PM-IM repeatedly switches between

Fig. 7. The wrist position of the controlled agent for different defenses. The human raises his hand to different heights depending on the type of punch he receives. The defense movement for a center punch was not learned. The PPCA-IM algorithm is still able to generalize to this situation. The PM-IM algorithm does not generalize well in this situation.

states for high-punches and low-punches leading, over time, to oscillations with an increasing amplitude. An interesting property of PPCA-IM is the fact that it automatically produces continuous outputs in every time step. The controlled agent reacts even to small changes in the behavior of the opponent agent. By nature the PM-IM is a discrete model and does not produce different outputs in every time step. Still, a continuous output can be generated by using interpolation (as done in the above examples) or by incorporating velocity information into the model. D. Robot Experiments In order to validate our results on a real robot, we conducted an experiment in which a NAO robot learned a set of interaction skills that can be performed cooperatively or competitively with a human partner. However, in order to replay any of the synthesized movements with the used NAO robot, we first have to find a mapping between the body parts of the human demonstrator and the body parts of the robot. This problem is commonly referred to as the correspondence problem [8] and is a fundamental problem of imitation learning. In this paper, we performed the mapping by using inverse kinematics (IK) on the extremities of the robot. The human skeleton was scaled to the size of the robot and IK was used to ensure that the positions of the feet, hands, pelvis and head of the robot matched the positions of the human extremities. More specifically, we used the iTaSC [10] IK algorithm for fitting the human skeleton to the robot. We have released the software package for IK-based correspondence matching of the NAO robot as an open-source tool for the general public3 . To test our algorithms, we trained interaction models for the martial arts data set. The robot learns to recognize and
3 The software can be downloaded as a Blender -extension from https://bitbucket.org/JuvenileFlippancy/naoblender

3262

Robot

Human

Punch right high Punch right low Kick right low Punch left high Punch left low

Log-likelihood

200

-500

Time

Fig. 8. A martial arts scenario trained and executed with the PM-IM algorithm. Top: The captured movement of the human. Second row: The joint angle configurations generated by the PM-IM after observing the human movement. Third row: The log-likelihoods for the different behaviors. Below: Pictures of the interaction between the human and the robot. The human movement was recorded with a Kinect camera.

defend different types of attacks, e.g., punchrighthigh, punchleftlow, kickrightlow. A set of 12 behaviors was used for training. Depending on the type of attack a different defense behavior is executed. For learning the PM-IM we used 60 to 70 hidden states. The number of hidden states for each behavior was estimated using cross-validation on the training data. For PPCA-IM we used a sampling rate of 10Hz and  = 20.

E. Discussion The results suggest that both PPCA-IM and PM-IM can be used encode and reproduce the joint dynamics of the interaction partners in a shared task. However, the results also show various advantages and shortcomings of these algorithms. The PPCA-IM approach is particularly well suited for modelling continuous reponses and correlations in the movements of the interaction partners. It has limited computational demands and generalizes to some extent to new situations. This shows that dimensionality reduction can be an effective measure for extracting the hidden structure in interaction data. Without dimensionality reduction, the use of the temporal embedding for motion capture data would be computationally prohibitive and the learning would require a significantly larger amount of data. At the same time, PPCA-IM does not provide information about the state or the development of the interaction. In this regard, the HMM-based PM-IM algorithm provides a richer set of tools for recognizing and estimating of the current state of the interaction. Yet, this comes at the price of significantly higher computational demands, as well as limited generalization abilities. Consequently, it would be interesting to combine the approaches presented in this paper by using a HMM with PPCA-IM models as emissions. In such a case, the HMM can be used to realize a spacetime linearization of the training data, while the PPCAIM can take on the role of modelling the correlations in the movements of the interaction partners in a particular temporal context. Recent advances in HMM training [23]

Figure 8 shows the movements of the human and the responses of the NAO robot. Note that the defense posture for an attack with the right hand and attacks with the left hand are different: robot lifts only one arm or both arms for defense. Similarly, the defense stance for a low-kick requires the robot to kneel down and block with one arm. The Figure also shows the log-likelihoods for the different behaviors that are generated by the HMMs. Interestingly the difference in the log-likelihood is very high when the opponent agent executes a kick-right-low movement. The robot can easily disambiguate this case as it is only when of two trained behaviors which use the leg. Both the PM-IM as well as the PPCA-IM model can solve the above task and produce appropriate responses for the NAO robot. The PPCA-IM model was again trained with 7 latent dimensions. Apart from martial arts examples we have also trained interaction models for the other data sets. Figure 1 shows the behavior of the robot after training a handing-over interaction task.

3263

also suggest a closer relationship between dimensionality reduction and temporal models such as HMMs. V. CONCLUSIONS In this paper we presented a new approach for teaching robots how to respond to the movements of a human partner. Using motion capture technology, the movements of a pair of persons are first recorded, and then processed using machine learning algorithms. The result is a model of how each person adapted its behavior to the movements of the respective other. Once an interaction model is learned, it can be used by a robot to engage in a similar task with a human counter part. We have also provided two algorithms called PPCA-IM and PM-IM, that are extensions to known methods, which can be used for learning interaction models from motion capture data. The algorithms allow a robot to learn when and how to respond to the behavior of a human partner. All methods were implemented on a NAO humanoid robot and were evaluated in cooperative and competitive tasks. After learning an interaction model, the NAO robot was able to generate appropriate defense responses in a challenging martial arts scenario. The discussion of the advantages and shortcomings of each of the two algorithms suggests that a combination of temporal models and dimensionality reduction can be an interesting path for developing more sophisticated models of interactions. While the results in this paper are encouraging, there are various aspects of imitation learning in multi-agent scenarios that need further investigation. In particular, it is interesting to investigate how learned models can be used to predict the future behavior of an interaction partner given the actions of the controlled agents. This can be helpful in avoiding decisions that potentially lead to dangerous situations or injuries. In this paper, we did not investigate the aspect of force transfer between a human and a robot. Even small forces that are exchanged between interaction partners can have a significant impact on the execution and success of a joint task. First research results on incorporating force transfer in interaction models can be found in [1]. Another aspect that needs further investigation is task space control. For some interaction tasks it is important that constraints are fulfilled within the task space. We are currently investigating the use of Interaction Meshes [12] for this purpose. Finally, it is also important to include tertiary objects, e.g., a jointly lifted box, into the interaction model. VI. ACKNOWLEDGMENT The work presented in this paper is funded through the CoDyCo project of the European Community's Seventh Framework Programme under the grant agreement n ICT600716 (CoDyCo). R EFERENCES
[1] E. Berger, H. Ben Amor, N. Haji-Ghassemi, D. Vogt, and B. Jung. Inferring guidance information in cooperative human-robot tasks. In IEEE-RAS International Conference on Humanoid Robots (HUMANOIDS). IEEE, 2013 (submitted).

[2] F. Biessmann, F. C. Meinecke, A. Gretton, A. Rauch, G. Rainer, N. Logothetis, and K.-R. M¨ uller. Temporal kernel canonical correlation analysis and its application in multimodal neuronal data analysis. Machine Learning, 79(1-2), 2009. [3] A. Billard, S. Calinon, R. Dillmann, and S. Schaal. Survey: Robot Programming by Demonstration. In Handbook of Robotics, volume chapter 59. MIT Press, 2008. [4] D.P. Black, M.A. Riley, and C.K. McCord. Synergies in intraand interpersonal interlimb rhythmic coordination. Motor Control, 11(4):348­73, 2007. [5] C. Bouveyron, G. Celeux, and G. Stphane. Intrinsic dimension estimation by maximum likelihood in isotropic probabilistic {PCA}. Pattern Recognition Letters, 32(14):1706 ­ 1713, 2011. [6] M. Brand and A. Hertzmann. Style machines. In Proceedings of the 27th annual conference on Computer graphics and interactive techniques, SIGGRAPH '00, pages 183­192, New York, NY, USA, 2000. ACM Press/Addison-Wesley Publishing Co. [7] S. Calinon, E.L. Sauser, A.G. Billard, and D.G. Caldwell. Evaluation of a probabilistic approach to learn and reproduce gestures by imitation. In Proc. IEEE Intl Conf. on Robotics and Automation (ICRA), pages 2381­2388, Anchorage, Alaska, USA, May 2010. [8] K. Dautenhahn and C. L. Nehaniv. Imitation in Animals and Artifacts. MIT Press, Campridge, 2002. [9] A. d'Avella, P. Saltiel, and E. Bizzi. Combinations of muscle synergies in the construction of a natural motor behavior. Nat Neurosci, 6(3):300­8, 2003. [10] J. De Schutter, T. De Laet, J. Rutgeerts, W. Decr´ e, R. Smits, E. Aertbeli¨ en, K. Claes, and H. Bruyninckx. Constraint-based task specification and estimation for sensor-based robot systems in the presence of geometric uncertainty. Int. J. Rob. Res., 26(5):433­455, May 2007. [11] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1­38, 1977. [12] E. Ho, T. Komura, and C. Tai. Spatial relationship preserving character motion adaptation. ACM Transactions on Graphics, 29(4):1­8, 2010. [13] A. Ijspeert, J. Nakanishi, and S. Schaal. Learning attractor landscapes for learning motor primitives. In Suzanna Becker, Sebastian Thrun, and Klaus Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 1523­1530. MIT Press, 2002. [14] S. Ikemoto, H Ben Amor, T. Minato, B. Jung, and H. Ishiguro. Physical human-robot interaction: Mutual learning and adaptation. IEEE Robotics and Automation Magazine, 19(4):24­35, Dec. [15] M. Ito and J. Tani. On-line imitative interaction with a humanoid robot using a dynamic neural network model of a mirror system. Adaptive Behavior, 12(2):93­115, 2004. [16] Z. Kolter, P. Abbeel, and A. Ng. Hierarchical Apprenticeship Learning with Application to Quadruped Locomotion. In John C. Platt, Daphne Koller, Yoram Singer, and Sam T. Roweis, editors, Advances in Neural Information Processing Systems (NIPS). MIT Press, 2007. [17] D. Lee, C. Ott, and Y. Nakamura. Mimetic communication model with compliant physical contact in human-humanoid interaction. Int. Journal of Robotics Research., 29(13):1684­1704, November 2010. [18] J. B. MacQueen. Some methods for classification and analysis of multivariate observations. In L. M. Le Cam and J. Neyman, editors, Proc. of the fifth Berkeley Symposium on Mathematical Statistics and Probability, volume 1, pages 281­297. University of California Press, 1967. [19] J. Porta, J. Verbeek, and B. Krose. Active appearance-based robot localization using stereo vision. Autonomous Robots, 18(1):59­80, 2005. [20] L. Rabiner. A tutorial on HMM and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257­286, February 1989. [21] M. Santello, M. Flanders, and J. F. Soechting. Postural Hand Synergies for Tool Use. The Journal of Neuroscience, 18(23):10105­10115, December 1998. [22] S. Schaal. Is imitation learning the route to humanoid robots? Trends in Cognitive Sciences, 3:233­242, 1999. [23] L. Song, B. Boots, S. M. Siddiqi, G. J. Gordon, and A. J. Smola. Hilbert space embeddings of hidden Markov models. In Proc. 27th Intl. Conf. on Machine Learning (ICML), 2010. [24] Z. Wang, M. Deisenroth, H. Ben Amor, D. Vogt, B. Schoelkopf, and J Peters. Probabilistic modeling of human dynamics for intention inference. In Proceedings of Robotics: Science and Systems (R:SS), 2012.

3264

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/234883726

PhysicalHuman-RobotInteraction:Mutual LearningandAdaptation
ArticleinIEEERobotics&AutomationMagazine·February2012
DOI:10.1109/MRA.2011.2181676

CITATIONS

READS

26
5authors,including: ShuheiIkemoto OsakaUniversity
38PUBLICATIONS171CITATIONS
SEEPROFILE

278

HiroshiIshiguro OsakaUniversity
354PUBLICATIONS5,174CITATIONS
SEEPROFILE

Someoftheauthorsofthispublicationarealsoworkingontheserelatedprojects:

AndroidprojectViewproject

ERATOIshiguroSymbioticHuman-RobotInteractionProjectViewproject

AllcontentfollowingthispagewasuploadedbyHeniBenAmoron13April2014.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Physical Human­Robot Interaction
Mutual Learning and Adaptation
By Shuhei Ikemoto, Heni Ben Amor, Takashi Minato, Bernhard Jung, and Hiroshi Ishiguro

·

lose physical interaction between robots and humans is a particularly challenging aspect of robot development. For successful interaction and cooperation, the robot must have the ability to adapt its behavior to the human counterpart. Based on our earlier work, we present and evaluate a computationally efficient machine learning algorithm that is well suited for such close-contact interaction scenarios. We show that this algorithm helps to improve the quality of the interaction between a robot and a human
Digital Object Identifier 10.1109/MRA.2011.2181676 Date of publication: 29 February 2012

C

caregiver. To this end, we present two human-in-the-loop learning scenarios that are inspired by human parenting behavior, namely, an assisted standing-up task and an assisted walking task. Human­Robot Interaction and Cooperation Until recently, robotic systems mostly remained in the realm of industrial applications and academic research. However, in recent years, robotics technology has significantly matured and produced highly realistic android robots. As a result of this ongoing process, the application domains of robots have slowly expanded into domestic environments, offices, and other human-inhabited locations. In turn, interaction and
MARCH 2012

1070-9932/12/$31.00ª2012 IEEE

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

1

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

cooperation between humans and robots has become an increasingly important and, at the same time, challenging aspect of robot development. Particularly challenging is the physical interaction and cooperation between humans and robots. For such interaction to be successful and meaningful, the following technical difficulties need to be addressed: l guaranteeing safety at all times l ensuring that the robot reacts appropriately to the force applied by the human interaction partner l improving the behavior of the robot using a machinelearning algorithm in a physical human­robot interaction (PHRI). In our previous research [1], we presented a PHRI scenario in which we addressed the above topics. Inspired by the parenting behavior observed in humans, a test subject was asked to physically assist a state-of-the-art robot in a standing-up task. In such a situation, both the human and the robot are required to adapt their behaviors to cooperatively complete the task. However, most machine learning scenarios to date do not address the question of how learning can be achieved for tightly coupled, physical interactions between a learning agent and a human partner. Building on the results in [2], we present an extended evaluation and discussion of such human-in-the-loop learning scenarios. To realize learning and adaptation on the robot's side, we employ a computationally efficient learning algorithm based on a dimensional reduction technique. In particular, after each trial, the human can judge whether the interaction was successful, then the judgment is used in a machine learning algorithm to apply a dimensional reduction technique and update the behavior of the robot. As learning progresses, the robot creates a behavioral model, which implicitly includes the actions of the human counterpart. At the same time, refining the motions of the robot during a physical interaction requires the motions of the human to be improved, because the two motions influence each other. Hence, the human counterpart is part of the learning system and overall dynamics. To analyze the efficiency of the proposed learning algorithm and the effect of human habituation to the robot during such close-contact interactions, we perform a set of PHRI experiments. In addition to the assisted standing-up interaction scenario presented in [2], we also present and discuss the first results based on a novel interaction scenario. More specifically, we present an assisted walking task in which a human caregiver must assist a humanoid robot while walking. We believe that human-in-the-loop learning scenarios, such as that presented herein, will be particularly interesting in the future because they can help to strengthen the mutual relationship between humans and robots. Ideally, this will lead to a higher acceptance of robotic agents in society. Related Research Important aspects of PHRIs have been investigated in a perspective research project conducted by the European
2

Network of Excellence (EURON) [3]. The objective of the project was to present and discuss important requirements for safe and dependable robots involved in PHRIs. Initial approaches for achieving these requirements are currently being addressed in a follow-up research project called PHRIENDS (a PHRI that is dependable and safe). To reduce risks and fatalities in industrial manufacturing workplaces, the primary goal of the PHRIENDS project is to design robots that are intrinsically safe. This requires the development of new actuator concepts, safety measures, and control algorithms, which take the presence of human subjects into account. The results of this project are also relevant to applications outside the manufacturing industry. However, learning and adaptation between humans and robots is not the focus of the PHRIENDS project. Khatib et al. [4] discussed the basic capabilities needed to enable robots to operate in human-populated environments. In particular, they discussed how mobile robots can calculate collision-free paths and manipulate surrounding objects. In their approach, they characterized free space using an elastic strip approach. However, the described robots were not expected to come into direct (physical) contact with the surrounding human subjects. The importance of direct physical interaction was highlighted in the haptic creature project [5], which investigated the role of affective touch in fostering the companionship between humans and robots. In an attempt to improve human­robot interaction, Kosuge et al. presented a robot that can dance with a human by adaptively changing the dance steps according to the force/moment applied to the robot [6]. Amor et al. [7] used kinesthetic interactions to teach new behaviors to a small humanoid robot. Furthermore, the behavior of the robot may be optimized with respect to a given criterion in simulation. In this learning scheme, the robot is a purely passive interaction partner and acts only after the learning process is complete. Similar approaches to teaching new skills have also been reported in [8] and [9] using different learning methods, i.e., continuous time-recurrent neural networks and Gaussian mixture models (GMMs), respectively. Odashima et al. [10] developed a robot that can come into direct physical contact with humans. This robot is intended for caregiving tasks such as carrying injured persons to a nearby physician. The robot can also learn new behaviors and assistive tasks by observing human experts as they perform these tasks. However, this learning does not take place during interactions but rather in offline sessions using immersive virtual environments. In [11], Evrard et al. present a humanoid robot with the ability to perform a collaborative manipulation task together with a human operator. In a teaching phase, the robot is first teleoperated using a force-feedback device. The recorded forces and positions are then used to learn a controller for the collaborative task. The main hypothesis underlying this approach is that the intentions of the human interaction partner can

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

MARCH 2012

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

be guessed from haptic cues. In [12], physical interactions between a robot's hand and the hand of a human are modeled by recording their distances. The distances are then encoded in a hidden Markov model (HMM), which in turn is used to synthesize similar hand contacts. A recent survey on modern approaches to physical and tactile human­robot interaction can be found in [13]. In this article, we present experiments with a flexiblejoint robot that is involved in close physical interaction with a human caregiver. In contrast to the above research, both human and robot play an active role in the interaction to learn and adapt their behaviors to their partner so as to achieve a common goal. This tight coupling of robot and human learning and coadaptation is a unique feature and is the primary contribution of the present study. We assume that it is important to focus on the active role in the interaction because the forces generated during the active behavior of the robot influence the behavior of the human, which in turn influences the passive behavior of the robot. In addition, these active and passive roles cannot be clearly separated because the robot and the human influence each other when they are in physical contact. Physical Interaction Learning Approach The goal of interaction learning is to improve the cooperation of humans and robots while they are working to achieve a common goal. Figure 1 shows an overview of the learning scheme used in this article. After an initial physical interaction between a human and a robot, the human is given the chance to evaluate the behavior of the robot. More precisely, the human can judge whether the interaction was a success or failure (binary evaluation). The feedback can be provided in various ways, such as through touch or through a simple graphical user interface. Once the evaluation information is collected by the robot system, it is stored in a database in the memory. The memory collects information about recent successful interactions and manages the data for the subsequent learning step. This allows us to optimize the set of training examples used for learning to improve learning quality. Figure 1 shows the human-in-the-loop learning system considered in this article, where the behavior of the human influences the behavior of the robot and, simultaneously, the behavior of the robot influences the behavior of the human. Furthermore, the behavior of the robot changes as learning progresses, which in turn influences the behavior of the human and its physical support. This system demonstrates one of the applications of a tightly coupled physical interaction. After a number of interactions, the learning system queries the memory for a new set of training data. The data are then projected onto a low-dimensional manifold using dimensional reduction techniques. There are three justifications for this step. First, dimensional reduction allows a reduction of the space in which learning takes place. Thus, the learning can be much faster and more efficient. In addition, dimensional reduction generally helps to detect

Learning

Memory

DR

Critique

GMM

Physical Interaction (a)

(b)
Figure 1. (a) Overview of the physical interaction learning approach. After physical interaction, the human judges whether the interaction was successful. This information is stored in the robot's memory and used for later learning. (b) Flexible-joint humanoid robot used in the experiments in this study. (Photos courtesy of ERATO Asada Project.)

meaningful low-dimensional structures in high-dimensional inputs. Second, dimensional reduction allows us to visualize and understand the adaptation taking place during interaction. This is particularly helpful for later review and analysis purposes. Finally, dimensional reduction reduces the negative influence of outliers on learning. The inputs to the dimensional reduction step are high-dimensional state vectors describing the postures of the robot during the interaction. The output is a low-dimensional posture space. Once the state vectors are projected onto a low-dimensional manifold, we group the resulting points into sets according to the action performed in that state. Thus, we obtain for each possible action a set of states in which the corresponding action should be triggered. For each action, a GMM is learned. The model encodes a probability density function of the learned state vectors. The ideal number of Gaussian mixtures is estimated using the Bayesian information criterion (BIC) [14]. By computing the likelihood of a given state vector p in a GMM of action A, we can estimate how likely it is
MARCH 2012

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

3

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

that the robot should perform action A when in posture p. The learned models are then used during the next physical interaction trial to determine the actions of the robot. Here, each new posture is projected into the lowdimensional posture space. Then, the likelihood of the projected point for each GMM is computed. Following a maximum-likelihood rationale, the action corresponding to the GMM with the highest likelihood is executed by the robot. With each iteration of the above learning loop, the robot adapts its model more and more toward successful interactions. The result is a smoother and easier cooperative behavior between the human and the robot. The CB2 Robot The robot used in this study is called the child­robot with biomimetic body, or CB2 [15]. The robot has the following features. l Its height is 130 cm, and its mass is approximately 33 kg. l The degree of freedom (DOF) is 56. l The supplied air pressure is 0.6 MPa. l The efficient torque of the knee is theoretically 28.6 NÆm. l All joints, apart from the joints used to move the eyes and eyelids, are driven by pneumatic actuators. l All joints, apart from the joints used to move the fingers, are equipped with potentiometers. The joints have low mechanical impedance because of the compressibility of air. The joints can also be made to be completely passive if the system discontinues air

Behavior System of CB2 Switching Mechanism x *(i ) (i = 1, 2, . . . , n) Desired Posture
  x 2, . x  {x 1,

. ., x  k}

Control System of CB2 d dt ­ d dt ­ P . . . + ­ d dt P D + Joint k + P D D + Joint 1 + + Joint 2 +

x1



+

 x2

+

compression during robot motion. This helps the robot to perform passive motions during physical interaction and helps to ensure the safety of the human partner. This is in contrast to most other robots, in which the joints are driven by electric motors with decelerators. The flexible actuators enable the joints to produce seemingly smooth motions, even when the input signal changes drastically. This feature of the CB2 robot is used to realize complex motions using the simple control architecture [1] depicted in Figure 2. More specifically, full body motions of the robot are realized by switching between a set of successive desired postures. Furthermore, the flexible actuators enable motions generated by this simple control architecture to be adaptively changed in response to an applied force from the human partner. Each posture is described by a posture vector x, with each entry of the vector denoting the angular value of a particular joint. A low-level controller is implemented by the proportional-integral-differential (PID) control of angular values. Each time the desired posture is switched drastically, large drive torques are generated, resulting in an active force being applied to the human caregiver. As the posture of the robot approaches the desired posture, the passive motion gradually becomes the dominant motion of the robot because the amount of error in the angular control gradually becomes smaller. Figure 3 shows how the examined standing-up task is realized using the proposed control architecture. The behavior is realized by switching between three desired postures. At first glance, the specifications of the robot motion appear to be extremely simple. However, the switching times are highly dependent on the human interaction. More specifically, the switching times depend on the anatomy and Current Posture skills of the human. This means that x {x 1, x 2, . . ., x k} the robot has to adapt the switching times to the characteristics of its partner during the period of interacx1 tion. In addition, it must be noted that this motion cannot be performed by the robot if a human does not assist in its execution.
x2

 xk

xk

Figure 2. Control architecture of the CB2 robot. The desired posture is encoded as a vector xÃ of angular values. Using a PID controller, drive torques are generated to attain the desired posture. The switching mechanism changes between a set of different desired postures to achieve complex robot motions.

Learning Method In the standing-up task, the goal of learning is to determine the ideal timing for switching actions between different xÃ 2 X Ã  X desired postures. Here, xÃ is a desired posture, X Ã is a set of desired postures prepared for control, and X is a posture space that is constructed from all joint angles. This is achieved by learning three different probabilistic lowdimensional posture models: 1) for

4

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

MARCH 2012

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Switching 1 When the Hands Are Pulled up by the Human

Switching 2 When the Legs Are Bent More Than in Posture 2 Posture 2 Posture 3

Posture 1

Figure 3. The three desired postures used in the standing-up task of the experiment. The learning task is to determine the ideal switching conditions between the desired postures. (Photo courtesy of ERATO Asada Project.)

the case in which no switching occurs, 2) for the first switching action, and 3) for the second switching action. At each time step of an interaction between the human and the robot, the realized posture and the current desired posture of the robot are recorded. The robot posture r is a 52-dimensional vector that codes the current angular value of each joint. After the interaction is complete, the postures are stored in a database in the memory. The database holds the information for the last ten interactions. Although there are several possible ways to integrate this new data into the database, the general policy used here is this new data overwrite old data, and successful interactions overwrite failed interactions. After ten interactions, the training data from the memory are used for learning. The goal of the learning is to construct a model that indicates when the robot should switch actions by changing the current desired posture. This rule is described by a mapping from the current posture of the robot to the desired posture that the robot should use. To realize this map, we use a GMM that can construct a probabilistic model. Therefore, the objective model of the learning is a probabilistic model that indicates the likelihood of desired postures in the current state. First, dimensional reduction is applied to the data because a 52-dimensional vector has too many dimensions to learn the model. Although a number of methods can be applied for this task, in this article, we used a principal component analysis (PCA). To perform the PCA, the mean rm is subtracted from all recorded posture vectors, and the covariance matrix M of the resulting points is computed. A singular value decomposition (SVD) on M yields matrices U , V , and W , such that M ¼ UWV T : (1)

(PCs), of matrix M . The matrix W is a diagonal matrix containing singular values. Each PC has a corresponding singular value that indicates how much information of the data set is covered by a specific PC. The first few PCs are then used as the axes of the lower-dimensional PCA space. Given a new data point, we can compute its coordinates in PCA space by subtracting the mean and calculating the dot product for each of the PCs. Next, we compute a GMM for each of the three switching classes. Here, we divide the projected data points into distinct sets. If no switching occurred, then the corresponding point is assigned to the first data set. Otherwise, the corresponding point is assigned to one of the other two sets. For each set of projected points, we learn a probability density function by a weighted sum of K Gaussian distributions: p(x) ¼
K X k¼1

pk p(xjk),

(2)

with pk being the weight of the kth Gaussian and p(xjk) being the conditional density function. The conditional density function is a d-dimensional Gaussian distribution:
T À1 1 1 p(xjk) ¼ pffiffiffiffiffid pffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi eÀ2ð(xÀlk ) Ck (xÀlk )Þ , 2p det(Ck )

(3)

The columns of matrix V contain orthonormal vectors, also known as the eigenvectors or principal components

with mean lk and covariance matrix Ck . The above p(xjk) can also be written as N (xjlk , Ck ). The expectation-maximization (EM) [16] algorithm is used to estimate the parameters {lk , Ck , pk } for each of the Gaussian kernels. Fortunately, performing the EM algorithm in low-dimensional spaces improves the convergence of the algorithm. After the learning process, we end up with three GMMs coding three probability density functions,
MARCH 2012

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

5

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

namely, p1 (x), p2 (x), and p3 (x). In our experiments, each GMM had between five and ten Gaussians. Each probability density function can be used to determine the probability of a point in a low-dimensional posture space with respect to a particular switching action. For example, computing p2 (r ) for a given projected robot posture, r , returns the likelihood of the robot having to switch from the second to the third desired posture when the robot is in state r . When the next interaction with the human starts, the robot can use the newly learned model to decide its current state and the desired posture. Here, the current joint values are projected onto the learned low-dimensional posture space. The result is a d -dimensional point. The optimal desired subsequent switching action can be computed in a maximum-likelihood fashion as follows:
Ã ¼ argmax ps (x): xnext xÃ 2X Ã

(4)

In each step of the control loop, the robot calculates snext and sends the angular values of the corresponding desired posture to a low-level controller. The controller then computes the needed joint torques to take on this posture. After the interaction is complete, the human evaluation information is collected and used to update the memory. The learning loop is then repeated. The above algorithm is closely related to HMMs [17]. At the same time, however, our algorithm deviates in various ways from HMM. More specifically, we do not learn the sequencing of states in our system. As a result, no explicit transition probabilities between the states are modeled. Figure 4 shows an example of a set of interactions projected onto a low-dimensional space. Each point in the plot represents one posture of the CB2 robot during an interaction. The points were colored according to the desired posture that was active during that particular time step.

Initial Desired Posture Desired Posture 1 Desired Posture 2

First Principal Component
Figure 4. Interaction data for the standing-up task projected into a low-dimensional posture space. Each point corresponds to one posture of the robot.

Experiment and Results To investigate tightly coupled adaptation and the learning scheme proposed in this article, we conducted a PHRI experiment using the interaction for the standing-up task introduced earlier. In particular, we considered the following question: "Does the learning algorithm lead to a symmetric learning process, in which both human and robot adapt their behaviors?" Furthermore, we wanted to measure the contribution of the learning algorithm to any improvement in the interaction. This required a careful experiment design that would allow us to distinguish between learning-based adaptation and adaptation due to human habituation to the robot. The experiment was split into three independent parts. Throughout the experiment, five subjects were asked to repeatedly assist the robot in standing up. In the first part, after every ten trials, the accumulated data in the memory were used for learning a new model, according to the learning scheme described in the "Physical Interaction Learning Approach: Learning Method" section. In total, 30 interactions with two intermediate learning steps were performed. In the second part of the experiment, learning by the robot was disabled and fixed time steps were used for switching between the postures. In this baseline scenario, the only type of adaptation that was possible was the adaptation of the human to the robot. In the third and final part, learning was once again enabled (the results of the first part were not included; hence, learning started from the beginning again). The experimental design ensures that we have baseline data, allowing us to compare the results of the interactions with and without learning. In addition, by performing the baseline experiment between the learning experiments, we ensure that the user is already familiar with the robot. Thus, we rule out any distortion of the baseline result because of unfamiliarity. To determine the ideal number of PCs on which to project the 52-dimensional posture vector of the robot, intrinsic dimensionality estimation techniques can be used [18] as a criterion. A simple estimation technique is based on the analysis of eigenvalues, which store the amount of information that is captured by each of the PCs. Hence, the eigenvalues determine how many PCs are needed to retain a specific percentage of information found in the data set. In our implementation, we automatically determine the number of PCs that capture more than 85% of the information in the data set. For our standing-up data set, this resulted in a projection onto two PCs. Figure 5 shows sequential photographs of the interactions of two test subjects. Figure 5(a) shows the initial interaction, whereas Figure 5(b) shows the interaction after learning. The white dashed line indicates the height of the hips in each snapshot. In the figures, we can observe a smoother transition of the hip height after the learning interaction, when compared with that before the learning interaction. In particular, the center photographs reveal strong contact between the feet and the ground and an

6

·

Second Principal Component

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

MARCH 2012

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

increased hip height after learning, in contrast to the poor contact with the ungainly leg posture beforehand. Since the degree to which the human helped the robot in the task and the evaluation of the robot performance are somewhat subjective, in our evaluation, we focus only on whether the robot motion is refined to the degree that inefficient and jerky motions are avoided. Figure 6 shows the interaction trajectories for two users before and after learning. Each trajectory was computed by projecting the robot postures into the low-dimensional posture space. Before learning, the trajectories contain loops and are partially linear. These linear pieces of the trajectories are due to jerky movements and large changes in the robot postures. In particular, for the first user, the variance in the trajectory decreases after learning. The trajectories become more similar and take on a V-shaped form. This can be explained by the fact that the interaction consists of three desired postures. Therefore, in successful trials, the interaction leads the robot from a starting posture to an intermediate posture and then to a final posture, as shown in Figure 3. In a low-dimensional space, the result is a V-shaped or triangular-shaped trajectory. This allows us to qualitatively evaluate the efficiency and naturalness of the interaction by analyzing the smoothness and shape of the low-dimensional trajectories. For example, in the case of the second subject, the trajectories before learning contain large loops at the point (1:7, À 1:5)T , which is the lowdimensional coordinate of the second desired posture. This phenomenon can easily be explained if we take into account our previous analysis. In the initial trials, the robot has poor contact with the floor and the legs are often not symmetrically arranged when reaching the second desired posture. As a result, lifting the robot becomes more difficult for the human and involves slight modifications of the

robot posture to make the feet more stable. This interrupts the flow of the standing-up task and increases the interaction burden for the human caregiver. To confirm the above discussion, we quantified the robot motion using the posture change norm. The posture change norm a of the robot motion was calculated using the Euclidean distance between the data of t and t À 1 in the posture space X defined using each joint angle as a base: a(t) ¼ jjx(t ) À x(t À1) jj2 , x 2 X : (5)

Computing the posture change norm at each time step of the interaction results in the time series depicted in Figure 7. The solid line shows the posture change norm during the initial interaction phase. We can see a sudden peak indicating a large change in the robot posture and, consequently, a nonsmooth motion. This is undesirable because large changes in the robot posture result from strong forces acting on the robot. The other lines show the evolution of the norm after each learning step. With each learning step, the number of peaks in the time series is reduced. In other words, the fluctuations in the posture change norm decrease, leading to a smoother and more efficient motion. A statistical analysis of the data further underlines the above hypothesis. Here, we computed the mean and standard deviation of the summation of the posture change norm during the interactions. Figure 8 shows the evolution of these values with each learning step. For all subjects, we see that the mean and standard deviation of the posture change norm decreased as the experiment progressed. In the baseline experiment, only one subject was able to significantly improve the interactions, where statistical significance is computed using a t test. None of the other subjects

(a)

(b)
Figure 5. Sequential photographs of the (a) first and (b) last interactions of the test subjects with the robot. The white curve depicts the change in position of the robot's hips. The center photograph of each sequence shows how the robot learns to maintain firm contact between its feet and the ground for both subjects. (Photos courtesy of ERATO Asada Project.)

MARCH 2012

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

7

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Before Training 2.0 Principal Component 2 1.5 1.0 0.5 0.0 ­0.5 ­1.0 ­0.5 0.0 0.5 1.0 1.5 2.0 Principal Component 1 (a) Before Training 1.5 Principal Component 2 Principal Component 2 1.0 0.5 0.0 ­0.5 ­1.0 ­1.5 ­2.0 ­1.0 ­0.5 0.0 0.5 1.0 1.5 2.0 Principal Component 1 (c) 2.5 3.0 1.5 1.0 0.5 0.0 ­0.5 ­1.0 ­1.5 2.5 3.0 Principal Component 2 2.0 1.5 1.0 0.5 0.0 ­0.5

After Training

­1.0 ­0.5 0.0 0.5 1.0 1.5 2.0 Principal Component 1 (b) After Training

2.5

3.0

­2.0 ­1.0 ­0.5 0.0 0.5 1.0 1.5 2.0 Principal Component 1 (d)

2.5

3.0

Figure 6. Projected interactions in the low-dimensional posture space: (a) and (b) the interaction trajectories for the first subject before and after learning and (c) and (d) the interaction trajectories for the second subject. In both the cases, the trajectories become smoother after learning and sudden jumps and knots are reduced. Furthermore, the trajectories become V-shaped, clearly indicating a smooth transition between the three desired postures.

were able to improve their interactions. In the first experiment, in which the proposed learning system is used, three subjects show significant improvement. Finally, in the second learning experiment, all of the subjects showed significant improvement in their interactions. This indicates that while a human can adapt to a robot and thus improve their interactions (as in the baseline experiment), this adaptation can be significantly improved by empowering the robot with learning capabilities (first and second learning experiments). We also analyzed the maximum values of the posture change norm during the interaction. Figure 9 shows the change in the maximum posture change norm during each learning phase of the baseline experiment and the first learning experiment. No significant difference in the maximum posture change norm is observed in the baseline experiment. On the other hand, in the learning experiment, there are large changes in the maximum posture change norm. For all subjects, the values drastically decrease after learning. Still, one possible implication from above results cannot be ruled out by the experiments performed so far.
8

Specifically, it remains unclear how much the learning system contributes to the improvement of interaction. A possible argument would be that the observed improvements are due to the long-term habituation and experience with the robot. If this argument is true, then we should see a similar improvement of interactions as above, even if we simply repeat the baseline experiment (where learning is disabled) three times in a row. To investigate this question, we performed the aforementioned experiment (three times baseline) with all subjects. For the subjects, the experiment looked exactly the same as the other experiments: the difference was not transparent. Figure 10 compares the summation of posture change norm between the first and the third baseline experiment. In each of the experiments, only one subject made significant improvement during the intermediate learning steps. On the whole, although for some subjects slight improvement was visible (notably Subject 5), the results are not as comprehensive as when learning is enabled. This means that, while long-term habituation and experience aids the learning process, it is not sufficient for a general improvement in PHRI.

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

MARCH 2012

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

2.5 Posture Change Norm 2.0 1.5 1.0 0.5 0.0 Initial After Learning 1 After Learning 2 Summation of Posture Change Norms

20 0.0325 15

10

0

10 20 30 40 50 60 70 80 90 100 Time (a) Initial After Learning 1 After Learning 2

5 1 2 3 4 Subject Number (a) 2.32e­4 1.18e­4 5

2.0 Posture Change Norm 20

Summation of Posture Change Norms

1.5

0.0873

1.0

5.19e­4 4.96e­4 15

0.5

0.0

0

10 20 30 40 50 60 70 80 90 100 Time (b)

10

Summation of Posture Change Norms

Figure 7. Evolution of the posture change norm during one learning experiment. The solid, dotted, and dashed lines show the evolution of the value when the robot has not yet learned, after the first intermediate learning step, and after the second intermediate learning step, respectively. (a) Subject 1 and (b) Subject 2.

5 1 20 2 3 4 Subject Number (b) 5

0.0049 0.022 0.033 0.0073 0.0028 0.0021

Discussion The following observations are based on the results of the above experiments. First, when learning and adaptation were only possible on the side of the human caregiver, generally, little or no improvement could be measured. However, even in this asymmetric learning situation, at least one subject was able to adapt to the robot so as to significantly improve the interaction quality. This shows the human ability to quickly adapt to new situations and motor tasks. The second observation is that the interaction quality significantly improved in the first learning experiment, and the improvement was even more remarkable during the second learning experiment. These results support our working hypothesis that the proposed learning system facilitates PHRI. Another interesting observation is that the human adaptation to the robot occurred in stages throughout the experiment. At the beginning of the experiment, the users were intimidated by the robot and the experimental setup. However, during the course of the experiment, the test subjects became more and more comfortable with the situation and the robot dynamics. As a result, the test subjects found it easier to interact with the robot. This suggests that algorithms for improving PHRI

3.05e­7 1.36e­5 4.63e­4 8.27e­8

15

10

5 1 2 3 4 Subject Number (c) 5

Figure 8. Mean and standard deviation of the summation of the posture change norm of test subjects in the (a) baseline, (b) first learning, and (c) final training experiments. The dark gray, white, and light gray bars indicate the mean and standard deviation values during each of the intermediate learning steps (after every ten trials). In (a), the baseline experiment, only Subject 2 shows a significant improvement after all trials. In (b) the first learning experiment, Subjects 2, 4, and 5 show significant improvements. In (c) the final experiment, the interaction with the robot improved for all subjects. With each learning trial, the indicated values decrease, and the movement of the robot becomes smoother and more synchronized with that of the subject.

MARCH 2012

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

9

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

2.5 2 1.5 1 0.5 0

Summation of Posture Change Norm

3 Maximum Posture Change Norm

20

15

0.0029

10

1

2

3 4 Subject Number (a)

5

0

1

2

3 4 Subject Number (a)

5

2.5 2 1.5 1 0.5 0

Summation of Posture Change Norm

3 Maximum Posture Change Norm

20

0.0313 15

10

0

1

2

3 4 Subject Number (b)

5

1

2

3 4 Subject Number (b)

5

Figure 9. Change in the maximum posture change norm in each phase during (a) the baseline and (b) the first learning experiments. No significant difference in the maximum posture change norm is observed in the baseline experiment. On the other hand, there are large changes in the maximum posture change norm in the learning experiment. For all subjects, the values decrease drastically after learning.

Figure 10. Baseline experiment repeated three times in a row to investigate whether improvement can be made without the robot's learning system enabled. In each of the experiments only one subject made a significant improvement. On the whole, although for some subjects slight improvement is visible (notably Subject 5), the results are not as comprehensive as when learning is enabled. (a) First baseline and (b) third baseline experiments.

can be made more efficient if the familiarization of the human with the robot is taken into account. A special familiarization phase, in which the human caregiver becomes accustomed to the robot before any cooperative tasks, might be one approach. Another method by which to familiarize the human with the robot might be a welldesigned interaction protocol that involves tasks that are intended only to familiarize the human with the robot. An interesting feature of the proposed algorithm is the ability to monitor the progress of learning as trajectories in a lowdimensional space. The results of this study indicate that the trajectories converge toward a V-shaped pattern for the standing-up task. Furthermore, the trajectories, after learning, appear to have particular points or bottlenecks through which they pass. This is reminiscent of the study by Kuniyoshi et al. [19] in which it was shown that the dynamic motions for a particular task often have a
10

bottleneck in the state space. This bottleneck is the result of the interaction of the human body and the environment. Kuniyoshi et al. referred to this property as knack and showed that the knack can be exploited to efficiently control a humanoid robot. In the proposed PHRI scenario, the dynamics of the robot strongly depends on the dynamics of the human caregiver. A knack may be said to appear in PHRI because of the strong coupling between the human and the robot and the resulting joint dynamics. In other words, the human can be regarded as a changing environment that constraints the robot dynamics. Note that, although only the posture of the robot was used to create the trajectories, we can still discern a knack that is based on joint dynamics. However, it can be argued that posture information is not sufficient enough to draw final conclusions about the joint dynamics. To address this question, we are currently investigating a different cooperative

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

MARCH 2012

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

PHRI task, namely that of assisted walking as can be seen in Figure 11. In this scenario, the human caregiver must assist the robot while the latter is trying to walk. Similar to the standing-up task, the assisted walking is realized using three desired postures: left leg up, standing, and right leg up. These postures are repeated in a predetermined order (standing ! left leg up ! Figure 11. The assisted walking task where a human caregiver assists the robot in his or standing ! right leg up) to create a her attempt to perform several walking steps. (Photo courtesy of ERATO Asada Project.) cyclic walking motion. During an interaction session, the human assists the robot in performing four cycles of the latter sequence. For a fast assessment of the applicability of our approach to different scenarios, we 90 performed an experiment using the same setup and para85 meters as for the standing-up task. However, in this case, we had only one test subject performing 30 interactions 80 with learning enabled and 30 interactions as baseline. Figure 12 shows the comparison of posture change norms 75 between each phase (a phase consists of ten trials) in each experiment (one baseline and one learning experiment). 70 As opposed to the baseline experiment, we can see 65 that the posture change norms decrease when learning is enabled. Note that the baseline experiment was per60 formed after the learning experiment to account for the Baseline Learning human's habituation. These early results show that the proposed human-in- Figure 12. Summation of posture change norm for baseline the-loop learning system is not limited to the uprising and learning experiments in the assisted walking task. Each bar interaction and that other types of interactions can be real- corresponds to a phase of ten interactions with the robot. ized. At the same time, in our experiments, we found that the robot often failed to keep up when the human demon- human interaction partner. This method has a low compustrator drastically increased or reduced the speed of his or tational load and can be run online during the interaction her walking gaits. This is due to the reactive nature of esti- with the robot and requires relatively few training data. In mating the joint dynamics from the postures only. To keep contrast to previous research in this field, the robot considup with a human interaction partner in this scenario, the ered in this study is in close physical contact with the robot must be more predictive in its estimation of the joint human partner and plays an active role during the dynamics. One possible approach to overcome this prob- performance of the cooperative task. The CB2 robot, lem is to include sensor information into the probabilistic through its flexible-joint design and soft silicone skin, is low-dimensional posture models. That is, the state of the particularly well suited to such tasks because physical robot would be based on the current joint angles as well as interactions become more natural and lifelike. In an the information gathered from the sensors under the skin. experiment inspired by parenting behavior in humans, we In this case, switching between one posture and another were able to show that the proposed learning method would also be influenced by the amount of pressure results in measurable improvements of interaction. Quanexerted by the human caregiver on the robot's body, e.g., titative evaluations based on the posture change norm conthe arms during assisted walking. Further studies are firm the significance of these improvements. Thus far, the control system used herein has three underway to obtain a conclusive answer to these questions. parameters: the set of desired postures, the feedback gains, and the switching rule. In this article, we focused on learnConclusions In this article, we presented a PHRI scenario in which suc- ing the switching rule only. However, for more complex cessful task completion can only be achieved through coor- interaction scenarios it might be important to adapt all of dinated actions involving physical contact. We introduced these parameters. Another limitation of the proposed a simple machine learning algorithm for adapting the learning algorithm is the use of binary evaluation informabehavior of the robot according to an evaluation by a tion. As a result, optimization of the parameters in a
Summation of Posture Change Norm
MARCH 2012

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

11

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

gradient descent manner is not possible. Another drawback of binary evaluation information is that only positive feedback examples are retained for use in the learning set while negative feedback examples are removed from the learning set. With respect to the first limitation, the desired postures and feedback gains can be regarded as attractors and velocities in a low-dimensional space. Amor et al. [7] have shown that such attractors can be efficiently learned in a low-dimensional space while incorporating kinesthetic assistance provided by the user. In the future, we therefore hope to integrate such a method into the proposed PHRI algorithm. As for the second limitation, we are considering the use of pressure sensors on the body of the robot. The amount of pressure issued by the caregiver can then be used as an approximate evaluation information. This allows for a finer grained reward value and, consequently, the use of modern optimization algorithms. Pressure sensors are also helpful to distinguish whether the human is currently in contact with the robot. In summary, this study provided interesting insights into the dynamics of PHRIs. The combination of a softbody robot and an efficient learning scheme is an important step toward responsive robots that share a common living space with humans. References
[1] S. Ikemoto, T. Minato, and H. Ishiguro, "Analysis of physical human-robot interaction for motor learning with physical help," Appl. Bionics Biomech. (Special Issue on Humanoid Robots), vol. 5, no. 4, pp. 213­223, 2008. [2] S. Ikemoto, H. B. Amor, T. Minato, H. Ishiguro, and B. Jung, "Physical interaction learning: Behavior adaptation in cooperative human-robot tasks involving physical contact," in Proc. IEEE Int. Symp. Robot and Human Interactive Communication (Ro-Man), Sept. 2009, pp. 504­509. [3] A. De Santis, B. Siciliano, A. De Luca, and A. Bicchi, "An atlas of physical human-robot interaction," Mechanism Mach. Theory, vol. 43, no. 3, pp. 253­270, Mar. 2008. [4] O. Khatib, K. Yokoi, O. Brock, K. Chang, and A. Casal, "Robots in human environments," in Proc. 1st Workshop Robot Motion and Control, 1999, pp. 213­221. [5] S. Yohanan and K. E. MacLean, "The haptic creature project: Social human-robot interaction through affective touch," in Proc. AISB Symp. Reign of Catz & Dogs: The 2nd AISB Symp. Role of Virtual Creatures in a Computerised Society, 2008, vol. 1, pp. 7­11. [6] K. Kosuge, T. Hayashi, Y. Hirata, and R. Tobiyama, "Dance partner robot--MS-danSer," in Proc. IEEE/RSJ Int. Conf. Intelligent Robots and Systems, 2003, vol. 3, pp. 3459­3464. [7] H. B. Amor, E. Berger, D. Vogt, and B. Jung, "Kinesthetic bootstrapping: Teaching motor skills to humanoid robots through physical interaction," KI 2009: Advances in Artificial Intelligence (ser. Lecture Notes in Artificial Intelligence), B. Mertsching, Ed. Berlin, Germany: SpringerVerlag, 2009, pp. 492­499. [8] J. Tani, R. Nishimoto, J. Namikawa, and M. Ito, "Codevelopmental learning between human and humanoid robot using dynamic neural

network model," IEEE Trans. Syst., Man, Cybern., vol. 38, no. 1, pp. 43­ 59, 2008. [9] S. Calinon and A. Billard, "What is the teacher's role in robot programming by demonstration?--Toward benchmarks for improved learning" Interaction Studies (Special Issue on Psychological Benchmarks in Human-Robot Interaction), vol. 8, no. 3, pp. 441­464, 2007. [10] T. Odashima, M. Onishi, K. Tahara, K. Takagi, F. Asano, Y. Kato, H. Nakashima, Y. Kobayashi, T. Mukai, Z. W. Luo, and S. Hosoe, "A soft human-interactive robot ri-man," in Proc. IEEE/RSJ Int. Conf. Intelligent Robotics and Systems (IROS, v018), 2006, p. 1­1 (video session). [11] P. Evrard, E. Gribovskaya, S. Calinon, A. Billard, and A. Kheddar, "Teaching physical collaborative tasks: Object-lifting case study with a humanoid," in Proc. IEEE-RAS Int. Conf. Humanoid Robots (Humanoids), Dec. 2009, pp. 399­404. [12] D. Lee, C. Ott, and Y. Nakamura, "Mimetic communication with impedance control for physical human-robot interaction," in Proc. IEEE Int. Conf. Robotics and Automation (ICRA) , 2009, pp. 1535 ­ 1542. [13] B. D. Argall and A. G. Billard, "A survey of tactile human-robot interactions," Robot. Autonom. Syst., vol. 58, no. 10, pp. 1159­1176, 2010. [14] G. Schwarz, "Estimating the dimension of a model," Ann. Statist, vol. 6, no. 2, pp. 461­464, 1978. [15] T. Minato, Y. Yoshikawa, T. Noda, S. Ikemoto, H. Ishiguro, and M. Asada, "Cb2: A child robot with biomimetic body for cognitive developmental robotics," in Proc. IEEE-RAS/RSJ Int. Conf. Humanoid Robots (Humanoids), 2007, pp. 557­562. [16] A. P. Dempster, N. M. Laird, and D. B. Rubin, "Maximum likelihood from incomplete data via the EM algorithm," J. R. Statist. Soc. Series B (Methodological), vol. 39, no. 1, pp. 1­38, 1977. [17] L. R. Rabiner, "A tutorial on hidden Markov models and selected applications in speech recognition," Proc. IEEE, vol. 77, no. 2, pp. 257­ 285, Feb. 1989. [18] K. Fukunaga and D. Olsen, "An algorithm for finding intrinsic dimensionality of data," IEEE Trans. Comput., vol. 20, no. 2, pp. 176­183, 1971. [19] Y. Kuniyoshi, Y. Ohmura, K. Terada, A. Nagakubo, S. Eitoku, and T. Yamamoto, "Embodied basis of invariant features in execution and perception of whole body dynamic actions--Knacks and focuses of rolland-rise motion," Robot. Autonom. Syst., vol. 48, no. 4, pp. 189­201, 2004.

Shuhei Ikemoto, Department of Multimedia Engineering, Osaka University, Japan. E-mail: ikemoto@ist.osaka-u.ac.jp. Heni Ben Amor, Intelligent Autonomous Systems Group, Technische Universitaet Darmstadt, Germany. E-mail: amor@ias.tu-darmstadt.de. Takashi Minato, ATR Hiroshi Ishiguro Laboratory, Kyoto, Japan. E-mail: minato@atr.jp. Bernhard Jung, Virtual Reality and Multimedia Group, Technische Universit at Bergakademie Freiberg, Germany. E-mail: jung@informatik.tu-freiberg.de.

12

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

MARCH 2012

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Hiroshi Ishiguro, Department of Systems Innovation, Osaka University, Japan. E-mail: ishiguro@is.sys.es. osaka-u.ac.jp.

·
Close physical interaction between robots and humans is a particularly challenging aspect of robot development.

·
Until recently, robotic systems mostly remained in the realm of industrial applications and academic research.

·
In recent years, robotics technology significantly matured and produced highly realistic android robots.

·

·

·

·
The human counterpart is part of the learning system and overall dynamics.

·
The joints have low mechanical impedance because of the compressibility of air.

·
The joints have low mechanical impedance because of the compressibility of air.

· ·

·

MARCH 2012
View publication stats

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

13

Point Cloud Completion Using Extrusions
Oliver Kroemer, Heni Ben Amor, Marco Ewerton, and Jan Peters
Intelligent Autonomous Systems Technische Universitaet Darmstadt Email: {oli, amor, peters}@ias.tu-darmstadt.de

Abstract--In this paper, we propose modelling objects using extrusion-based representations, which can be used to complete partial point clouds. These extrusion-based representations are particularly well-suited for modelling basic household objects that robots will often need to manipulate. In order to efficiently complete a partial point cloud, we first detect planar reflection symmetries. These symmetries are then used to determine initial candidates for extruded shapes in the point clouds. These candidate solutions are then used to locally search for a suitable set of parameters to complete the point cloud. The proposed method was tested on real data of household objects and it successfully detected the extruded shapes of the objects. By using the extrusion-based representation, the system could accurately capture various details of the objects' shapes.

20 40 60 80 100 120 140 160 0 50 100 150 200

I. I NTRODUCTION In the future, service robots working in everyday environments will need to grasp and manipulate a wide range of different objects. Given these unstructured environments, many of the encountered objects will be novel to the robot, and their complete shapes will initially be unknown. This shape information is however vital for successfully and efficiently manipulating the objects. Hence, the robot will need to autonomously determine the shapes of novel objects. One approach to acquiring a suitable 3D model is to scan the object from different perspectives by either shifting the object or moving around the object [1]. The information from these multiple perspectives can then be accumulated to form a 3D model. Although this approach can acquire accurate object models, the process of acquiring multiple images is a time-consuming and non-trivial task. Alternatively, the robot can attempt to predict the shape of an object from only a single perspective. The partial model acquired from one perspective can be completed by detecting patterns in the observed shape and extending these patterns into the occluded regions [2]. For example, planes of symmetry can be detected and, subsequently, used to complete the point cloud accordingly [3]. In this paper, we show how the partial point clouds of objects can be completed by detecting extruded shapes. Extruded 3D shapes are 2D shapes that have been extended into the third dimension along a particular path, such as a line segment (linear extrusion) or circle (rotational extrusion). For example, a cube is a linearly extruded square, and a sphere is a rotationally extruded circle. Our overall approach to detecting extruded shapes is similar to the shape from symmetry framework proposed by Thrun

Figure 1. The top left image shows a heart-shaped box. The top right image shows the depth data collected from the heart-shaped box. The bottom images show the object models obtained using the extrusion-based approach to point cloud completion.

and Wegbreit [4]. The robot first searches for planes of symmetry in the partial point cloud, which are entailed by both linear and rotational symmetries. The detected symmetries are then used to initialize local searches for suitable extrusion parameters. Finally, the detected extrusions are evaluated according to a scoring system, and used to complete the point cloud when applicable. The main contribution of this paper is therefore the use of extrusion-based, rather than symmetrybased, representations. Extrusion-based representations are well-suited for robots working in everyday environments, wherein many objects are manufactured. Not only are linear and rotational extrusions often used to design objects, but they are also common in the manufacturing process. As a result, many everyday objects have basic extruded shapes. In this paper, we will be focusing on completing the point clouds of basic objects that can be described by single extrusions. Although the proposed approach is similar to the shape from symmetry method, extrusion-based representations can complete some point clouds that the symmetry-based representations cannot. For example, the extrusion-based approach can complete surfaces by extruding edges, as illustrated in Fig. 1. The heart-shaped box has two curved surfaces that were not visible in the original point cloud. However, by

extruding the curved edges observed along the top of the box, the extrusion-based approach could complete these parts of the point cloud. The symmetry-based approach relies on projecting observed surfaces into occluded regions. Given that all of the observed surfaces are flat, this approach cannot complete the curved surfaces. A similar problem can occur when completing the point cloud of a rectangular box when only two of the six sides are observed. The extrusion-based approach can extrude one surface according to the other in order to complete the entire box. The symmetry-based approach would, however, have problems completing the third pair of opposing surfaces. There are obviously also situations in which a symmetrical object cannot be modeled by an extrusion-based representation. Thus, these two representations complement each other and could be used together. The proposed method is explained in Section II, which also includes an overview of related work in completing point clouds for robot applications. The applicability of the extrusion-based approach is demonstrated in Section III, wherein the robot successfully completes the shapes of various household objects from a single perspective. II. E XTRUSION - BASED P OINT-C LOUD C OMPLETION In this section, we explain how the observed point clouds can be completed by detecting extruded shapes in the partial point cloud. We begin by giving a brief overview of methods used in robotics applications to complete partial point clouds. In Section II-B, we detail the extrusion-based representation, which is flexible enough to model a wide range of objects shapes. Sections II-C to II-F explain how one can search for extrusions in a partial point cloud. The proposed method assumes that the robot's vision system acquires structured point clouds; i.e., each point cloud corresponds to a pixel in a 2D grid. This form of data can be acquired from dense stereo, time-of-flight camera, Kinect, or other active stereo cameras. A. Point Cloud Completion in Robotics Determining the shape of an object from a single view is a common problem in robotics. One approach to solving this problem is to provide the robot with a library of previously scanned models, which it can then fit into the observed scene [5, 6, 7, 8]. This approach allows the robot to accurately reconstruct the scene. However, it also relies on the robot having a model of the object, and requires searching through a large library of known objects. In the field of computer vision, Pauly et al. [9] presented a method for completing the shape of objects using the models of objects with similar shapes. Hence, a smaller library of objects could be used, as the objects generalize to novel objects. Another approach is to fit primitive shapes, such as cubes and cylinders or superquadrics, to the partial view [10]. Primitive shapes cannot only be used to represent simple objects, but also parts of more complex objects [11]. The primitive shapes are generally parameterized such that they

3D

Symmetry

Split

ICP

(A)

(B)

(C)

(D)

Figure 2. The figure illustrates how the initial extrusion hypotheses are generated. The top row corresponds to rotational extrusions, and the bottom row demonstrates linear extrusions. (A) The 3D objects of a tube and a box are shown. (B) A top view of the objects, as well as the detected plane of symmetry indicated by the dashed line. (C) The point clouds are divided into two regions, indicated by red and green. The arrows in the top image indicate the observed surface normals for these regions, which are used to divide the points. (D) The ICP algorithm is used to slide one region into the other. The rigid body transformation found by the ICP algorithm is then used to compute the extrusion parameters.

can be adapted to model a range of similar object parts. The ability to adapt the primitives in this manner is important, as the additional flexibility allows the model to capture more details of the object. Point cloud completion can also be performed by predicting the full shape of an object from symmetry [4, 3]. As already mentioned, this approach is the most similar to the one presented in this paper. Thrun and Wegbreit proposed a hierarchy of symmetries that can be detected from a partial view and, subsequently, used to complete the point cloud [4]. They begin by performing a grid search over the entire object for valid local symmetries, followed by a local optimization of the symmetry parameters using the hill climbing algorithm. The resulting candidate symmetries are evaluated using a scoring system based on the probability of observing the completed point cloud. The symmetry-based approach to point cloud completion was extended to robot manipulation by Bohg et al. [3]. B. Extrusion-based Object Representations The goal of the work presented in this paper is to detect extruded shapes of objects from a single perspective. In this manner, the robot can attempt to complete the shapes of the objects in occluded region, such that the model can be used for manipulating the object. An extruded shape consists of two components: the profile and the path. The profile is the basic 2D shape that the 3D extruded shape is based on. The path is the line indicating how the profile is extended into the third dimension. In this paper, we focus on paths defined by straight line segments (linear extrusions) and circles (rotational extrusions). This family of shapes allows us to represent a wide range of different primitive shapes, including spheres, rectangular prisms, cylinders, and cones.

However, the robot will also encounter more complex extruded shapes. Hence, we need the representation to be flexible enough to capture these shapes in detail. We achieve this goal by representing the profile of the object as a 2D point cloud. This low-dimensional point cloud can be used to represent a wide range of shapes. The profile can, thus, also be directly obtained from the observed 3D point cloud. The path of a linear extrusion is defined by a 3D coordinate system and the length of the extrusion. The direction of the extrusion is always in the coordinate frame's z-direction, and the 2D profile defines the shape in the x-y plane. For a rotational extrusion, we define the axis of rotation as a 3D line. The 2D profile points define the location along the axis of rotation, as well as the radial distance from this axis. C. Detecting Planes of Symmetry One important characteristic of both linear and rotational extrusions is that they result in symmetric shapes; i.e. the extrusions entail a mirror symmetry. Therefore, to find extruded shapes in the point cloud, we begin by first searching for planar reflection symmetries. Instead of using a grid search to detect these symmetries [4, 3], we adopt the fast votingbased approach proposed by Mitra et al. [12]. First, the normal vector and curvature are computed for each point in the point cloud. The normal vector can be obtained by computing the eigenvectors for a local neighborhood of points. The normal direction is given by the eigenvector with the smallest eigenvalue, which points towards the camera. The eigenvectors can then be computed for the local neighborhood of normal directions. The resulting two largest eigenvalues are used to approximate the local curvature. Subsequently, each point is compared with every other point in the cloud in order to find pair-wise symmetries. The plane of symmetry for two points xa and xb is located at the midpoint 0.5(xa + xb ), with a normal aligned with the direction xb - xa . However, this pair-wise symmetry is only considered valid if the points' normals are also reflected in this plane, and the difference in curvature values between the two points is below a given threshold. The parameters of each valid plane of symmetry are treated as one vote. The goal is therefore to find parameter settings with many votes, which correspond to large symmetric regions in the point clouds. In order to find these planes of symmetries, the distribution of symmetry plane parameters is modelled as a kernel density estimate [12]. All of the modes of this distribution can then be found using mean-shift clustering [13]. The corresponding symmetry parameters form the basis for local searches for extruded shapes. D. Computing Initial Extrusion Parameters using ICP Given a plane of symmetry, a set of extrusion parameters needs to be computed. The steps used to perform this computation are outlined in Fig. 2. We begin by dividing the point cloud into two parts according to the plane of symmetry. To detect extrusions, we divide the points according to which side of the plane of

symmetry the point lies on. To detect rotations, we separate the points according to their normals n and the symmetry plane's normal p. If the inner product pT n is greater than zero, the point is assigned to the first region and otherwise it is assigned to the second region. The dividing of the point cloud into two regions is illustrated in Fig. 2C. Once the point cloud has been divided into two parts, we need to find a rigid-body transformation that shifts one of the parts to the pose of the other. We use the iterative closest point (ICP) algorithm to compute this transformation [14]. The computed transformation should have the same effect as sliding the part along the extrusion's path. Hence, a linear extrusion should result in a translation, and a rotational extrusion should result in a rotation about an axis (see Fig. 2D). Given the transformation computed using ICP, we can compute the direction of the path for the linear extrusions, and the axis of rotation for rotational extrusions. It should be noted that the symmetry-detection algorithm proposed by Mitra et al. [12] also uses ICP to detect symmetries more accurately. However, their approach searches for symmetries, and then uses ICP to find the same type of symmetry more accurately. Instead, our approach first detects planar reflection symmetries, and then uses ICP to find extrusions, which are a different type of pattern. In this manner, we exploit the self-similarity property of extruded shapes. E. Local Search Given an initial hypothesis for an extrusion, the parameter and profile can usually be further improved using a local search. The 2D point cloud for representing the extrusion's profile also needs to be determined at this stage. For linear extrusions, the profile will be a plane of points that is orthogonal to the path of the extrusion. Hence, we must align the direction of the path d with the normal of the profile plane. We perform this local alignment using an iterative procedure. We first define the set of profile points as those points that have a normal n such that -dT n >  , where 1 >  > 0 is a threshold value. Given this set of profile points, the direction of the path is updated as the negative mean of the profile points' normals. The set of profile points can then again be updated according to the update path direction. In order to improve the robustness of this process, we begin with a low threshold value, e.g.,  = 0.5, and increase the value in each iteration up to a maximum value, e.g.  = 0.95. After the path direction has been aligned with the normal of one of the object's sides, we need to fit a plane to this side. We first find the location of this plane along the path direction by computing the position of the current profile points in this direction and selecting the mode. The points that are near to this plane are then used to define the final set of profile points. The direction of the extrusion path is given by the normal of this plane. The length of the extrusion path is set according to the length of the surfaces that are orthogonal to the direction of the extrusion.

Figure 3. The columns correspond to the results for a cup, a funnel, a pot, a heart-shaped box, a roll of toilet paper, and a box respectively. The first row shows a picture of each object. The second row shows the depth image taken of the object, which corresponds to the partial point cloud. Darker red regions are further away than yellow regions, and blue regions have a depth of zero. The bottom two rows show the reconstructed object shape from different angles. These reconstructions were made from one perspective of the object and the 3D meshes were not post-processed.

For rotational extrusions, we want to set the rotational axis such that many of the profile points are mapped onto each other. We begin by projecting the data into the 2D profile space according to the initial hypothesis, and marking points in dense regions as profile points. For each of the m profile points, located at 3D points x1:m , we compute the position along the rotation axis a1:n and the distance from the rotation axis r1:m to the point. We also compute the normalized direction d1:m from each point to the closest point on the axis. For the ith data point, we now compute the m m locally weighted mean radius r ^i = j =1 wij rj / k=1 wik where the weight wij is given by wij = exp(-(ai - aj )2 /v 2 ) and v is a length scale parameter. This radius value r ^i is an approximation of the desired radius at this point along the axis. Using this desired radius, we create a new 3D point y i = xi + r ^i di . Once all y 1:n have been computed, we fit a line to these points, which then becomes the new axis of rotation. Note that the point y i will be closer to xi than the axis of rotation if ri > r ^i and, thus, will draw the axis of rotation closer to xi . Similar to the linear extrusion case, we iterate over these steps multiple times to acquire a suitable axis of rotation. The final profile for the rotational extrusion can be smoothed by using local averaging over the radii component. F. Visibility Score In the final stage of the extrusion detection process, we assign each candidate extrusion a score in order to select the one that best represents the robot's observations. Similar to the scoring systems used in symmetry-based approaches [4, 3], the score is defined according to how well the

completed point cloud matches the observed scene. Our basic scoring system is based on the depth images obtained from the camera, such as the one shown in Fig. 3. This data structure is similar to a z-buffer in computer graphics, and indicates which regions in space are occluded. Using the 3D positions of the observed points, and their pixel location in the z-buffer, we can compute a projection matrix P between the 3D camera space and the 2D z-buffer pixel space. The partial point cloud is first completed according to the extrusion parameters currently being evaluated. In order to compare different extrusions in a fair manner, each point in the profile should be used to generate the same number of extruded points. The completed point cloud is then projected into the z-buffer space using the projection matrix P . Each projected point is assigned to the nearest pixel in the z-buffer. A projected point is assigned a score according to its z value in the camera coordinate frame zc and the depth value of the assigned z-buffer pixel zp . We also define a length scale parameter h. If the depth values are close together zp - zc < 2h, then the point provides evidence for the extrusion and, hence, is assigned a positive score of exp(-0.5(zp - zc )2 /h2 ). If the point has a depth greater than the z-buffer zp - zc > 2h, its location corresponds to an occluded region and, hence, it is assigned a score of zero. Finally, if the point has a depth that is less than the z-buffer zp - zc < -2h, then it contradicts the observed scene and, hence, it is assigned a negative score, e.g. -3. The score for the entire point cloud is given by the sum of the scores obtained by the individual points, divided by the number of points in the profile. The extrusion with the largest score is used to complete

the partial point cloud, which can then be used to create a 3D model for manipulating the object. III. E XPERIMENT
Probability Density

2 1.8 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2 0 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2

The proposed method was implemented and applied to a set of common household objects. The results show that the method can detect the extruded shapes in the point clouds, and even capture details of the objects' shapes. A. Setup and Results In this experiment, we evaluated the accuracy of the extrusion paths found by the proposed method. In particular, we measured the errors in the radii of rotational extrusions and the path lengths of linear extrusions. Using a standard Kinect camera, we collected 30 partial point clouds of common household objects. The objects were placed individually on a table, and the table was segmented out of the point cloud. The segmented point cloud was then subsampled to obtain around 3000 points. The approach presented in Section II was then used to complete each of the partial point clouds. The desired type of extrusion was pre-specified for each object. The actual lengths and radii of the extrusions were also measured manually using the point cloud. By measuring these distances directly from the point clouds, cameraspecific calibration errors do not affect our results. The measured distances were then compared to those found by the point cloud completion method. The robot found suitable extrusions for 28 of the 30 images (93% success rate). In one of the failed trials, the robot extruded an incorrect surface. In the other failed trial, the robot did not find a suitable axis of rotation for describing the shape of the object. In the successful trials, the error in the computed distances could be measured. The distribution over these errors is shown in Fig. 4. The mode and mean of the distribution are at -1.36 mm and -1.19 mm respectively. The standard deviation of the distribution is 2.23 mm. Detecting the planar symmetries for each object in the experiments took on average 19.9 seconds. Fig. 3 shows a set of models obtained using the proposed method. For the rotationally extruded objects, a stochastic optimization was used to improve the alignment of the axis of rotation and increase the visibility score. The 3D models were generated from the completed point clouds using the ball-pivot algorthim [15]. Some of the objects may seem shorter than the actual object, which is a result of the table segmentation. Standard post-processing methods, such as smoothing, were not applied to the point clouds, in order to display the quality of the profiles more clearly. For a real application, we recommend post-processing the 3D model. B. Discussion The results show that the extrusion-based approach could accurately complete the objects' shapes in most of the trials. In practice, the accuracy of the model would decrease due to other sources of error, such as the camera calibration.

Extrusion Error (cm)
Figure 4. The distribution over errors in extrusion lengths and radii. The distribution was modelled using a kernel density estimate with a Gaussian kernel with a width of  = 0.1cm. A negative value indicates that the extruded shape found by the proposed method was smaller than the actual size.

However, the computed models should still be suffficiently accurate for performing coarse manipulations with the objects. The results also show that the voting-based symmetry detection method performs well even when applied to noisy data. The detected planes of symmetry allowed the robot to find valid extrusions in most of the objects used in this experiment. The computed extrusions are slightly biased towards being too small. This may be a result of the relatively large penalization for overestimating the size of the extrusion. However, a slight bias is also to be expected for some objects, such as the cup. The point cloud of the cup contains points from both the outside and the inside of the cup. Hence, a rotational extrusion that maps the front-outer points onto the back-inner points would achieve a higher visibility score, but would also result in a smaller cup radius than the actual radius. The models in Fig. 3 show the importance of using a flexible profile representation. The extrusions are capable of modelling details, such as the lip of the pot and the hole in the middle of the toilet paper roll. The quality of the profiles could be improved by reincorporating more points from the original point cloud, once a valid set of extrusion parameters has been found. Fine details, such as the texture of the cup, are obviously lost due to noise in the data. One shortcoming of the current method is that it can sometimes detect degenerate extrusions when a linear extrusion is applied to a rotationally extruded object, or vice versa. For example a rotational extrusion applied to a box may detect a cylinder that fits into the shape of the box. We plan to address this problem in the future in order to render the method more robust. The method is also able to cope with additional parts of

R EFERENCES [1] M. Krainin, P. Henry, X. Ren, and D. Fox, "Manipulator and object tracking for in-hand 3d object modeling.," I. J. Robotic Res., vol. 30, no. 11, pp. 1311­1327, 2011. [2] T. P. Breckon and R. B. Fisher, "Amodal volume completion: 3d visual completion," Comput. Vis. Image Underst., vol. 99, pp. 499­526, Sept. 2005. [3] J. Bohg, M. Johnson-Roberson, B. León, J. Felip, X. Gratal, N. Bergström, D. Kragic, and A. Morales, "Mind the Gap - Robotic Grasping under Incomplete Observation," in ICRA 2011, May 2011. [4] S. Thrun and B. Wegbreit, "Shape from symmetry," in ICCV 2005, pp. 1824­1831, IEEE, 2005. [5] S. Savarese and F.-F. Li, "3d generic object categorization, localization and pose estimation," in ICCV 2007, pp. 1­8, 2007. [6] R. Detry, N. Pugeault, and J. Piater, "A probabilistic framework for 3D visual object representation," IEEE Trans. Pattern Anal. Mach. Intell., vol. 31, no. 10, pp. 1790­1803, 2009. [7] A. Aldoma, N. Blodow, D. Gossow, S. Gedikli, R. Rusu, M. Vincze, and G. Bradski, "Cad-model recognition and 6 dof pose," in ICCV 2011, 3D Representation and Recognition (3dRR11), 11/2011 2011. [8] G. Biegelbauer and M. Vincze, "Efficient 3d object detection by fitting superquadrics to range image data for robot's object manipulation," in ICRA 2007, pp. 1086 ­1091, 2007. [9] M. Pauly, N. J. Mitra, J. Giesen, M. Gross, and L. Guibas, "Example-based 3d scan completion," in Symposium on Geometry Processing, pp. 23­32, 2005. [10] Z. C. Marton, L. C. Goron, R. B. Rusu, and M. Beetz, "Reconstruction and Verification of 3D Object Models for Grasping," in ISRR 2009, (Lucerne, Switzerland), 2009. [11] C. Goldfeder, P. K. Allen, C. Lackner, and R. Pelossof, "Grasp planning via decomposition trees," in ICRA'07, pp. 4679­4684, 2007. [12] N. J. Mitra, L. Guibas, and M. Pauly, "Partial and approximate symmetry detection for 3d geometry," ACM Transactions on Graphics (SIGGRAPH), vol. 25, no. 3, pp. 560­568, 2006. [13] D. Comaniciu and P. Meer, "Mean shift: a robust approach toward feature space analysis," IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 24, pp. 603 ­619, may 2002. [14] Y. Chen and G. Medioni, "Object modeling by registration of multiple range images," in ICRA 1991, pp. 2724­2729, 1991. [15] F. Bernardini, J. Mittleman, H. Rushmeier, C. Silva, G. Taubin, and S. Member, "The ball-pivoting algorithm for surface reconstruction," IEEE Trans. Visualization and Computer Graphics, vol. 5, pp. 349­359, 1999.

Figure 5. The top left image show the watering can. The top right image shows the depth image taken of the watering can. Darker red regions are further away than yellow regions, and blue regions have a depth of zero. The bottom images show the results of applying the proposed point cloud completion approach to the watering can's partial point cloud. The watering can is an example of an object that consists of multiple extruded parts.

the object that are not extruded, as shown by the handle of the cup. In the future, we will investigate how a robot can robustly decompose more complex objects into multiple extruded parts, using the method proposed by Mitra et al. [12]. An early result of this approach applied to a watering can is shown in Fig. 5. Although the system failed to complete the top handle, and incorrectly completed the side handle, these initial results are promising. Overall, the experiment has demonstrated that the proposed method could detect most of the extruded shapes and, thus, accurately complete the point clouds. IV. C ONCLUSION In this paper, we investigated how point clouds of basic objects can be represented and completed by linear and rotational extrusions. These extrusions are represented in a flexible manner, which allows them to accurately model a wide range of shapes. By detecting local symmetries in partial point clouds, we can search for extrusions in an efficient manner, and use these extrusions to complete the point cloud. In the experiment, the proposed method was applied to point clouds obtained from real household objects, and successfully completed most of the partial point clouds. In the future, we plan to use the proposed method in order to plan grasps on novel objects. In particular, the proposed method allows us to compute contact points for occluded regions of the object. ACKNOWLEDGEMENTS The project receives funding from the European Community's Seventh Framework Programme under grant agreement n° ICT- 248273 GeRT.

IEEE/RSJ International Conference on Intelligent Robots and Systems -- IROS 2012

Generalization of Human Grasping for Multi-Fingered Robot Hands
Heni Ben Amor, Oliver Kroemer, Ulrich Hillenbrand, Gerhard Neumann, and Jan Peters

Abstract-- Multi-fingered robot grasping is a challenging problem that is difficult to tackle using hand-coded programs. In this paper we present an imitation learning approach for learning and generalizing grasping skills based on human demonstrations. To this end, we split the task of synthesizing a grasping motion into three parts: (1) learning efficient grasp representations from human demonstrations, (2) warping contact points onto new objects, and (3) optimizing and executing the reach-and-grasp movements. We learn low-dimensional latent grasp spaces for different grasp types, which form the basis for a novel extension to dynamic motor primitives. These latent-space dynamic motor primitives are used to synthesize entire reach-and-grasp movements. We evaluated our method on a real humanoid robot. The results of the experiment demonstrate the robustness and versatility of our approach.

I. I NTRODUCTION The ability to grasp is a fundamental motor skill for humans and a prerequisite for performing a wide range of object manipulations. Therefore, grasping is also a fundamental requirement for robot assistants, if they are to perform meaningful tasks in human environments. Although there have been many advances in robot grasping, determining how to perform grasps on novel objects using multi-fingered hands still remains an open and challenging problem. A lot of research has been conducted on robot grippers with few degrees of freedom (DoF) which may not be particularly versatile. However, the number of robot hands developed with multiple fingers has been steadily increasing in recent years. This progress comes at the cost of a much higher dimensionality of the control problem and, therefore, more challenges for movement generation. Hard coded grasping strategies will typically result in unreliable robot controllers that can not sufficiently adapt to changes in the environment, such as the object's shape or pose. Such hard coded strategies will also often lead to unnatural `robotic looking' grasps, that do not account for the increased sophistication of the hardware. Alternative approaches, such as the optimization of grasps using stochastic optimization techniques, are computationally expensive and require the specification of a grasp quality metric [27]. Defining an adequate grasp metric is often hard to do, as it requires specifying intuitive concepts in a mathematical form. Additionally, such approaches typically do not consider the whole reachand-grasp movement but exclusively concentrate on the hand
Heni Ben Amor, Oliver Kroemer, Gerhard Neumann and Jan Peters are with the Technische Universitaet Darmstadt, Intelligent Autonomous Systems, Darmstadt, Germany. {amor, kroemer, neumann, Fig. 1. The Justin robot learns to grasp and lift-up a mug by imitation. The reach-and-grasp movement is learned from human demonstrations. Latentspace dynamic motor primitives generalize the learned movement to new situations.

configuration at the goal. In this paper, we present an imitation learning approach for grasp synthesis. Imitation learning allows a human to easily program a humanoid robot [3], and also to transfer implicit knowledge to the robot. Instead of programming elaborate grasping strategies, we use machine learning techniques to successfully synthesize new grasps from human demonstration. The benefits of this approach are threefold. First, the computational complexity of the task is significantly reduced by using the human demonstrations along with compact lowdimensional representations thereof. Second, the approach allows us to imitate human behavior throughout the entire reach-and-grasp movement, resulting in seamless, naturallooking motions. Typical transitions between a discrete set of hand shapes, as can be found in traditional approaches, are thus avoided. Finally, this approach also allows the user to have control over the type of grasp that is executed. By providing demonstrations of only one particular grasp type, the synthesis algorithm can be used to generate distinct grasps e.g., only lateral, surrounding, or tripod grasp. The use of assorted grasps can considerably improve the robustness of the grasping strategy as the robot can choose a grasp type which is appropriate for the current task. A. Related Work In order to generalize human grasping movements, we need to understand how humans perform grasps. Human grasping motions consist of two components: the reaching motion of the arm for transporting the hand, and the motions

peters}@ias.tu-darmstadt.de Ulrich.Hillenbrand@dlr.de

Ulrich Hillenbrand is with the German Aerospace Center - DLR, Institute of Robotics and Mechatronics, Oberpfaffenhofen, Germany.

Grasp Type Learning

DB
Contact Warping
Human Demonstration Grasp Spaces

Grasp Spaces Grasp Optimizer

Grasp Configuration

LS-DMP

Fig. 2. An overview of the proposed approach. The contact points of a known object are warped on the current object. Using the resulting positions, an optimizer finds the ideal configuration of the hand during the grasp. The optimizer uses low-dimensional grasp spaces learned from human demonstrations. Finally, a latent space dynamic motor primitive robustly executes the optimized reach-and-grasp motion. The approach is data-driven and can be used to train and execute different types grasps.

of the fingers for shaping the hand [16], [17]. These two components are synchronized during the grasping movement [7]. For example, at around 75% of the movement duration, the hand reaches its preshape posture and the fingers begin to close [15]. At this point in time, the reaching motion of the hand shifts into a low velocity movement phase. Early studies into human hand control assumed muscles and joints as being controlled individually by the central nervous system [26], [19]. However, more recent studies have found evidence suggesting that the fingers are controlled using hand synergies [23], [2] -- i.e., the controlled movements of the fingers are synchronized. According to this view, fingers are moved "synergistically" thereby reducing the number of DoF needed for controlling the hand. Such hand synergies can be modeled as projections of the hand configuration space into lower-dimensional subspaces [20] such as the principal components. Movements along the first principal component of this subspace result in a basic opening and closing behavior of the hand. The second and higher-order principal components refine this motion and allow for more precise shaping of the hand [20], [24], see Fig. 3. Although the majority of the variation in the finger configurations is within the first two principal components, higher-order principal components also contain important information for accurately executing grasps [23]. The gain in grasp accuracy does, however, plateau at around five dimensions [22], [20]. Therefore, the space of human hand synergies during grasping can be well represented by a five-dimensional subspace. Following this idea, various researchers have used dimensionality reduction techniques to find finger synergies in recorded human grasps [4], [8]. Once a low-dimensional representation of finger synergies is found, it can be used to synthesize new grasps in a generate-and-test fashion. For example, the authors of [8] use Simulated Annealing to find

an optimal grasp on a new object while taking into account the finger synergies. Common to such approaches is the use of a grasp metric [27] that estimates the quality of a potential solution candidate. However, such metrics can be computationally demanding and rely on having an accurate model of the objects. In general, it is difficult to define a grasp metric that includes both, physical aspects of the grasps (such as the stability) as well as functional aspects that depend upon the following manipulations. Alternative approaches to grasp synthesis predict the success probability of grasps for different parts of the object. For example, good grasping regions are estimated from recorded 2D images of the object in [25]. A labeled training set of objects including the grasping region is subsequently produced by using a ray-tracing algorithm. The resulting dataset is then used to train a probabilistic model of the ideal grasping region. The learned model, in turn, allows a robot to automatically identify suitable grasping regions based on visual features. In a similar vein, Boularias et al. [6] use a combination of local features and Markov Random Fields to infer good grasping regions from recorded point clouds. Given an inferred grasping region, the reach-andgrasp motion still needs to be generated using a set of heuristics. Additionally, this approach does not address the problems of how to shape the hand and where to place the finger contacts. Tegin et. al. [28] also used imitation learning from human demonstration to extract different grasp types. However, they do not model the whole reach-and-grasp movement and circumvent the high-dimensionality problem by using simpler manipulators. II. O UR A PPROACH In our approach, we address the challenges of robot grasping by decomposing the task into three different stages: (1)

learning efficient grasp representations from human demonstrations, (2) warping contact points onto new objects, and (3) optimizing and executing the synchronized reach-andgrasp movements. An overview of the proposed approach can be seen in Fig. 2. The contact points of a known object are first warped onto the current object using the techniques in Sec. II-B. The warped contact points are then used by the optimizer to identify all parameters needed for executing the grasp, i.e., the configuration of the fingers and the position and orientation of the hand. The optimization is performed in low-dimensional grasp spaces which are learned from human demonstrations. Finally, the reach-and-grasp movement is executed using a novel extension to dynamic motor primitive [14] called latent-space dynamical systems motor primitive (LS-DMP). A. Learning Grasp Types from Human Demonstration Using human demonstrations as reference when synthesizing robot grasps can help to narrow down the set of solutions and increase the visual appeal of the generated grasp. At the same time, a discrete set of example grasps can also heavily limit the power of such an approach. To overcome this problem, we use dimensionality reduction techniques on the set of human demonstrations in order to infer the low-dimensional grasp space. To this end, we recorded the movements of nine test-subjects, where each test subject was asked to perform reach-and-grasp actions on a set of provided objects. We subsequently performed Principal Component Analysis (PCA) on the dataset, projecting it onto five principal components. This choice of dimensionality is based on research on the physiology of the human hand [22], [20] which suggested that five principle components are sufficient for accurately modeling the movements of the human hand. The resulting grasp space is a compact representation of the recorded grasps as it models the synergies between the different fingers and finger segments. The first principal component, for example, encodes the opening and closing of the hand. Fig. 3 shows grasps from the space spanned by the first two principal components. The above approach yields general grasp spaces that do not give the user control over the grasp type to be executed by the robot. However, for many tasks it is important to favor a particular grasp type over another when synthesizing the robot movements. For example, for carrying a pen one can use a tip grasp, while for writing with the pen an extension grasp is better suited. Hence, in a second experiment with the same test subjects we learned grasp spaces for specific grasp types, such as lateral grasps or tripod grasps. To determine the grasp space, we devised a grasp taxonomy [10] consisting of twelve grasp types and recorded specific datasets for each of these types. The datasets were subsequently used to learn grasp spaces for the specific grasp type. Due to the differences in kinematics of the human and robot hand, there are multiple ways to map the hand state to the robot state, also known as the correspondence problem

2. Principal Component

1. Principal Component
Fig. 3. The space spanned by the first two principal components of human recorded grasps applied to the robot hand. The first component describes the opening and closing of the hand. The second principal component modulates the shape of the grasp.

in the robotics literature [9]. In this paper, we solve the correspondence problem by dividing the generalization of grasps into two parts, i.e., the reproduction of the hand shape and the adaptation of Cartesian contact points. The reproduction of the hand shape is realized by directly mapping the human joint angles to the robot hand. For the index, middle and ring fingers this results in an accurate mapping with robot hand configurations similar to the demonstrated human hand shapes. In order to map the thumb, an additional offset needed to be added to the carpometacarpal joint. Using this type of mapping, the reproduced hand shapes will be similar to those of the human. The generalization of the Cartesian contact points is achieved by the contact warping algorithm described in Sec. II-B. The two generalizations in Cartesian space and in joint space are then reconciled through the optimization process explained in Sec. II-D. B. Generalizing Grasps through Contact Warping In this section, we introduce the contact warping algorithm. This algorithm allows the robot to adapt given contact points from a known object to a novel object. As a result, we can generalize demonstrated contact points to new situations. Assume that we are given two 3D shapes from the same semantic/functional category through dense sets of range data points. In our approach, the process of shape warping, that is, computing a mapping from the source shape to the target shape, has been broken down into three steps. 1) Rigid alignment of source and target shapes, such that semantically/functionally corresponding points get close to each other.

2) Assignment of correspondences between points from the source shape and points on the target shape. 3) Interpolation of correspondences to a continuous (but possibly non-smooth) mapping. The alignment step involves sampling and aligning many surflet pairs, i.e., pairs of surface points and their local normals, from source and target shapes. The estimation of relative clusters of the pose parameters is obtained from the surflet-pair alignments [11], [12]. Since the alignment of source and target shapes has brought corresponding parts close to each other, we can again rely on the local surface description by surflets to find correspondences, based on proximity of points and alignment of normal vectors. The correspondence assignment that we have used here is an improved version of the method described in [11]. In this approach, correspondences were assigned for each source surflet independently into the set of target surflets. For strong shape variations or unfavorable alignment between source and target, such an approach could result in a confusion of similar parts. In order to cope with larger shape variation, some interaction between assignments of neighboring points has to be introduced. We have, therefore, formulated correspondence search as an optimal assignment problem. In this formulation, interaction between assignments of different points is enforced through uniqueness constraints. Let {x1 , . . . , xN } be points from the source shape, transformed to align with the target shape; let {y1 , . . . , yN } be points from the target shape.1 Assignment of source point i to target point j is expressed as an assignment matrix, aij = 1 0 if i is assigned to j , otherwise. (1)

Fig. 4. Mug warping example. A dense set of surface points from the source mug (top row) and their mappings to the target mug (bottom row) are colored to code their three Cartesian source coordinates (three columns).

Furthermore, let dij = xi - yj be the Euclidean distances between source and target points and cij = ni · mj be the angle cosines between the unit normal vectors ni and mj at source point i and target point j , respectively. The objective is to minimize the sum of distances between correspondences, i.e., mutually assigned points,
N N

i.e., to assign only between points with inter-normal angle of  90 degrees. The two equality constraints (3) and (4) mediate the desired interaction between assignments of different points. The inequality constraint (5) can exclude points from being assigned and, therefore, the problem may become infeasible. Thus, we have to add imaginary source and target points x0 and y0 which have no position and no normal direction. They can be accommodated by appending large entries d0j and di0 to the distance matrix, which larger than all real distances in the data set, as well as zero entries c0j = ci0 = 0 to the angle cosine matrix. These imaginary points can be assigned to all real points with a penalty, which is chosen such that only points without a compatible partner will receive this imaginary assignment. We subsequently minimize the cost function C (a01 , . . . , a0N , a10 , . . . , aN N ) = D(a11 , . . . , a1N , a21 , . . . , aN N )
N N

(6)

+
i=1

di0 ai0 +
j =1

d 0j a 0j .

D(a11 , . . . , a1N , a21 , . . . , aN N ) =
i=1 j =1

dij aij ,

(2)

subject to the constraints
N

aij = 1 j  {1, . . . , N } ,
i=1

(3)

i.e., to assign every target point to exactly one source point,
N

aij = 1 i  {1, . . . , N } ,
j =1

(4)

i.e., to assign every source point to exactly one target point, and cij aij  0 i, j  {1, . . . , N } , (5)
equal number N of points from source and target shapes can always be re-sampled from the original data sets.
1 An

For solving this constrained optimization problem, we use the interior-point algorithm, which is guaranteed to find an optimal solution in polynomial time [30]. Finally, point correspondences are interpolated to obtain a continuous (but possibly non-smooth) mapping of points from the source domain to the target domain. More theory and systematic evaluations of the procedure are given in [13]. Fig. 4 shows an example of a dense set of surface points warped between two mugs. A warp of the contact points of an actual grasp from the source to the target mug is shown on the left of Fig. 2. C. Latent Space Dynamic Motor Primitives In order to execute different grasps, the robot requires a suitable representation of the grasping actions. Ideally, the grasping action should be straightforward to learn from

a couple of human demonstrations and easily adapted to various objects and changes in the object locations. The action representation should also ensure that the components of the grasping movement are synchronized. The dynamical systems motor primitives (DMPs) representation fulfills all of the above requirements [14]. DMPs have been widely adopted in the robotics community, and are well-known for their use in imitation learning [21], [18]. The DMP framework represents the movements of the robot as a set of dynamical systems y ¨ =  z (z  - 2 (g - y ) -  - 1 y  ) + a -2 f (x, 1:N ) where y is a state variable, g is the corresponding goal state, and  is a time scale. The first set of terms represents a critically-damped linear system with constant coefficients z and z . The last term, with amplitude coefficient a = g - y0 , incorporates a shaping function f (x, 1:N ) =
N i=1 i (x)i x , N j =1 j (x)

where i (x) are Gaussian basis functions, and the weight parameters 1:N define the general shape of the movements. The weight parameters 1:N are straightforward to learn from a single human demonstration of a goal directed movement. The variable x is the state of a canonical system shared by all DoFs. The canonical system acts as a timer to synchronize the different movement components. It has the form x  = - x, where x0 = 1 at the beginning of the motion and thereafter decays towards zero. The metaparameters g , a, and  can be used to generalize the learned DMP to new situations. For example, the goal state g of the reaching movement is defined by the position of the object and the desired grasp. We explain how the DMP goal meta-parameters are computed for new objects in Sec. II-D. However, we need first to define how the finger trajectories can be encoded as DMPs, such that they generalize to new situations in a human-like manner. Representing and generalizing the motions of the fingers is a challenging task due to the high dimensionality of the finger-configuration space. A naive solution would be to assign one DMP to each joint [19]. However, as previously discussed in Sec. I-A, humans seem to generalize their movement trajectories within lower-dimensional spaces of the finger configurations, and not at the level of each joint independently [23], [20]. If the robot's generalization of the grasping action does not resemble the human's execution, implicit information contained within the human demonstrations is lost. Therefore, in order to facilitate behavioral cloning of human movements, the DMPs for multi-fingered hands should be realized in a lower dimensional space. In addition, overfitting is avoided by representing the movement in a lower-dimensional space. In particular, the DMPs can be defined in the latent spaces learned in Sec. II-A. As such spaces are learned from complete trajectories of the grasping movements, they also include the finger configurations needed for representing

the hand during the approach and preshaping phases of the action, as well as the final grasps [20]. We use a DMP for each of the latent space dimensions as well as DMPs for the wrist position and orientation. The weight parameters for these DMPs can be learned from human demonstrations by first projecting the tracked motions into the latent space and subsequently learning the weights from the resulting trajectory. Thus, the same data that is used to learn the latent space can be reused for learning the weight parameters. The resulting latent-space DMPs, as well as the reaching movement's DMPs, are linked to the same canonical system, thus, ensuring that they remain synchronized. The output of the latent-space DMPs is afterwards mapped back into the high-dimensional joint space by the PCA projection. In this manner, the grasping action can be executed seamlessly, and the robot can begin closing its fingers before the hand has reached its final position. Thus, we have defined a human-like representation of the grasping movements that can be acquired by imitation learning. Given this DMP representation, the robot still needs to determine the meta-parameters for new situations. This process is described in the next section. D. Estimating the Goal Parameters In order to generalize the latent-space DMPs to new objects, we need to estimate the goal state g for each latentspace dimension, as well as the orientation of the hand for a new set of contact points which we have acquired from contact warping as discussed in Sec. II-B. We use one contact-point per finger, where the contact point is always located at the finger tip. Each point is specified in Cartesian coordinates. As we have four fingers, this results into a 12-dimensional task space vector xC . Additionally, we also want to estimate the position and orientation of the hand in the world coordinate frame. We therefore add six virtual joints v, i.e., three translational and three rotational joints. We will denote the transformation matrix, which is defined by these six virtual joints, as T(v). We define the finger tip position vector x1:4 as the concatenation of all four finger tip positions. This vector is a function of the transformation matrix T(v) and the joint configurations of the fingers q = m + Kg, i.e., W (y) = T(v)H (m + Kg). The vector m represents the mean of the PCA transformation and K is given by the first five eigenvectors. The function H (q) calculates the finger tip-positions in the local hand coordinate frame. This setup is an inverse kinematics problem with the difference that we want to optimize the joint positions of the fingers in the latent space instead of directly optimizing the joint positions q. Thus, the inverse kinematics problem is over-constrained as we have twelve task variables and only eleven degrees of freedom. Therefore, instead of the standard Jacobian pseudo-inverse solution, we need to employ a different approach. Our task is to estimate the optimal configuration y =  [v , g ] of the hand, which consists of the orientation and

the latent space coordinates, such that the squared distance between finger-tip positions x1:4 and the contact points xC is minimal, i.e., y L( y ) = = [v , g ] = argmaxy L(y), -(W (y) - xC )T C-1 (W (y) - xC ) (7) +yT Wy.
Fig. 6. The Justin robot executes a reach-and-grasp movement in simulation. Using the trained LS-DMP a new trajectory (red) to the target object is synthesized. The optimal hand position and orientation (shown as a coordinate system) is estimated along with the optimal hand shape in latent-space.

The matrix W = diag(w) defines a damping or regularization term for the step-size of y, and C = diag(c) defines the inverse precision for each task variable. The Jacobian J =  W / y for this problem can be obtained straightforwardly, i.e., for the derivation w.r.t v it is given by the standard geometric Jacobian and for the derivation w.r.t the latent variable g it is given by Jl = Jq K, where Jq denotes the geometric Jacobian. We will solve the optimization problem given in Equation (7) by iteratively applying a least squares solution. Given the current hand configuration yk and the desired finger-tip positions xC , the update step for the hand configuration is therefore given by yk = (JT CJ + W )-1 JT C (xC - W (yk )) . (8)

As we have to solve an overconstrained inverse kinematics setting, in contrast to the more common underconstrained inverse kinematics setting, we use the left-pseudo inverse in Equation (8). This update equation also corresponds to a Bayesian view on inverse kinematics [29]. We repeat the update until convergence in order to get the optimal hand configuration y . We always start our optimization from an initial posture where the hand is pointing downwards. III. S ETUP AND E VALUATIONS To evaluate the proposed approach, we conducted a set of experiments using the Rollin' Justin robot platform [5]. Justin is a mobile humanoid robot system with an upperbody including 43 actuated degrees-of-freedom (DoF). In our experiments we controlled 22 DoF pertaining to the Torso (3 DoF), the right arm (7 DoF), and the four-fingered right hand (12 DoF). The experiments were performed both in simulation and on the physical robot. A. Simulation Results In the first experiment, we evaluated the performance and the results of our approach in a simulated environment for the Justin robot. As explained in Sec. II, we trained individual LS-DMPs for each of the principal components of the demonstrated reach-and-grasp movement. Fig. 5 shows the latent-space trajectories for three out of the five principal components of the hand shape. The example trajectories are depicted in blue, while the trajectory learned by the LSDMP is depicted in red. This figure reveals an interesting insight into the nature of the recorded human reach-and-grasp movements: many of the example trajectories have a distinct sigmoid shape that has a bell-shaped velocity profile. This insight corresponds to the results in [1] , which showed that

humans perform point-to-point reaching movements such that the velocity profile along the path can be characterized by a symmetric bell-shape. Our results indicate that a similar property holds for the latent space trajectories of the hand shape during a reach and grasp. After learning, we first executed the LS-DMP in simulation. Fig. 6 shows the start and end configuration during one run of the algorithm. The red curve depicts the trajectory of the hand as generated by the LS-DMP, while the displayed coordinate system shows the estimated hand orientation of the robot. To evaluate the accuracy of the produced grasping motions, we repeatedly changed the position and orientation of the target object and measured the distance between the warped contact points on the object and the fingertip positions. Ideally, the fingertips should always coincide with the contact points. Tab. I shows the average distance of the fingers to the warped contact points after executing a reachand-grasp movement. We also varied the grasp spaces in order to evaluate the effect of the grasp type on the the resulting hand shape. The grasp space indicated by Multi in Tab. I was learned using all available human demonstrations. This grasp space encompasses a wide range of variations of the human hand. As can be seen in the table, we achieved the most accurate results by using this grasp space. In this case, the average error is about 7mm. It should be noted that the fingers of the robot are much larger than human fingers and have a width of about 3cm. Given the size of the robot's fingers, the produced error only corresponds to about a quarter of the finger width. The table clearly shows that changing the grasp type results in higher average error. This increased error is to be expected, as we constrained the space of possible solutions to a specific grasp type. At the same time, visual inspection of the resulting grasps shows that this error does not deteriorate the quality of the resulting grasps, as will be seen in the next section. B. Real Robot Experiments We also conducted experiments with the real Rollin' Justin robot. Three different types of mugs were used during the experiments. After placing a mug on a table in front of the robot, all information about the pose of the mug

2

2

2

1.5

1.5

1.5

Eigenvector 1

Eigenvector 3

1

1

Eigenvector 5
0 0.2 0.4 0.6 0.8 1

1

0.5

0.5

0.5

0

0

0

-0.5

0

0.2

0.4

0.6

0.8

1

-0.5

-0.5

0

0.2

0.4

0.6

0.8

1

Time

Time

Time

Fig. 5. The plots show example trajectories (blue), and the mean trajectory (red), for three (out of five) latent space dimensions during the closing of the hand. The trajectories have been shifted and scaled to start at zero and end at one, in order to allow for easier comparison of their shapes. As the scaled trajectories have similar shapes, they can be represented by individual DMPs and easily learned from human demonstrations.

TABLE I AVERAGE DISTANCE BETWEEN WARPED CONTACT POINTS AND FINGERTIPS AFTER GRASPING . Grasp Type Avg. Error (m) Multi 0.007 Tripod 0.013 Surrounding 0.0157 Lateral 0.014
Human

Tripod

Surrounding

Lateral

was estimated using a Kinect camera and the techniques explained in [12]. Subsequently, using the contact warping techniques from Sec. II-B the contact points from a known mug were warped onto the currently seen mug. The resulting contact points were subsequently fed into the optimizer to estimate all parameters that are needed to execute the reachand-grasp movement. The estimation of all parameters using the algorithm in Sec. II-D takes about one to five seconds. We performed about 20 repetitions of this experiment with the different mugs placed at various positions and heights. Additionally, we included a lifting-up motion to our movement, in order to evaluate whether the resulting grasp was stable or not. In all of the repetitions the robot was able to successfully grasp and lift-up the observed object. Furthermore, we also executed the reach-and-grasp movements using grasp spaces belonging to different grasp types. No change was made to the structure or other parameters of the algorithm. The only difference between each execution run was the grasp space to be loaded. Fig. 7 shows three of the grasp types used in our taxonomy along with the result of applying them to the Justin robot. The figure clearly shows that changing the grasp type can have a significant effect on the appearance of the executed grasp. For example, we can see that the use of the tripod-grasp results in delicate grasps with little finger opposition, while surrounding grasps lead to more caging grasps with various finger oppositions. Our approach exploits the redundancy in hand configurations and allows desired grasp types to be set according to the requirements of the manipulation task that is going to be executed. Fig. 8 shows a sequence of pictures captured from one of the reach-and-grasp movements executed on the real robot. Reach-and-grasp movements for different grasp types and situations are shown in the video submitted as supplemental material.

Fig. 7. The three grasp types lateral, surrounding, and tripod from our taxonomy are demonstrated by a human and later reproduced by the Justin robot. All parameters of the reach-and-grasp movement, such as the shape of the hand, its position, and orientation are automatically determined using latent space dynamic motor primitives.

Robot

IV. C ONCLUSION In this paper, we presented a new approach for imitation and generalization of human grasping skills for multifingered robots. The approach is fully data-driven and learns from human demonstrations. As a result, it can be used to easily program new grasp types into a robot ­ the user only needs to perform a set of example grasps. In addition to stable grasps on the object, this approach also leads to visually appealing hand configurations of the robot. Contact points from a known object are processed by a contact warping technique in order to estimate good contact points on a new object. We, furthermore, presented latent-space dynamic motor primitives as an extension to dynamic motor primitives that explicitly models synergies between different body parts. This significantly reduces the number of parameters needed to control systems with many DoF such as the human hand. Additionally, we have presented a principled optimization scheme that exploits the low-dimensional grasp spaces to estimate all parameters of the reach and grasp movement.

Fig. 8. A sequence of images showing the execution of a reach-and-grasp movement by the Justin humanoid robot. The executed latent-space dynamic motor primitive was learned by imitation. The type of the grasp to be executed can be varied according to the requirements of the task to subsequently executed. New grasp types can be trained within minutes by recording a new set of human demonstrations.

The proposed methods were evaluated both in simulation and on the real Justin robot. The experiments exhibited the robustness of the approach with respect to changes in the environment. In all of the experiments on the real, physical robot, the method successfully generated reach-and-grasp movements for lifting up the seen object. ACKNOWLEDGMENT We thank Florian Schmidt and Christoph Borst from the DLR - German Aerospace Center for their help with the Justin robot and for their valuable comments and suggestions. H. Ben Amor was supported by a grant from the Daimlerund-Benz Foundation. The project receives funding from the European Community's Seventh Framework Programme under grant agreement n ICT- 248273 GeRT. R EFERENCES
[1] W. Abend, E. Bizzi, and P. Morasso. Human arm trajectory formation. Brain : a journal of neurology, 105(Pt 2):331­348, jun 1982. [2] M. Arbib, T. Iberall, and D. Lyons. Coordinated control programs for movements of the hand. Experimental brain research, pages 111­129, 1985. [3] H. Ben Amor. Imitation learning of motor skills for synthetic humanoids. PhD Thesis, Technische Universitaet Bergakademie Freiberg, Freiberg, Germany, 2011. [4] H. Ben Amor, G. Heumer, B. Jung, and A. Vitzthum. Grasp synthesis from low-dimensional probabilistic grasp models. Comput. Animat. Virtual Worlds, 19(3-4):445­454, sep 2008. [5] C. Borst, T. Wimbock, F. Schmidt, M. Fuchs, B. Brunner, F. Zacharias, P. R. Giordano, R. Konietschke, W. Sepp, S. Fuchs, C. Rink, A. AlbuSchaffer, and G. Hirzinger. Rollin' justin - mobile platform with variable base. In Robotics and Automation, 2009. ICRA '09. IEEE International Conference on, pages 1597 ­1598, may 2009. [6] A. Boularias, O. Kroemer, and J. Peters. Learning robot grasping from 3-d images with markov random fields. In 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2011, pages 1548­1553, 2011. [7] S Chieffi and M Gentilucci. Coordination between the transport and the grasp components during prehension movements. Experimental Brain Research, pages 471­477, 1993. [8] M. T. Ciocarlie and P. K. Allen. Hand posture subspaces for dexterous robotic grasping. Int. J. Rob. Res., 28(7):851­867, July 2009. [9] K. Dautenhahn and C. L. Nehaniv. Imitation in Animals and Artifacts. MIT Press, Campridge, 2002. [10] G. Heumer. Simulation, Erfassung und Analyse direkter Objektmanipulationen in virtuellen Umgebungen. PhD Thesis, Technische Universitaet Bergakademie Freiberg, Freiberg, Germany, 2011. [11] U. Hillenbrand. Non-parametric 3d shape warping. In Pattern Recognition (ICPR), 2010 20th International Conference on, pages 2656 ­2659, 2010.

[12] U. Hillenbrand and A. Fuchs. An experimental study of four variants of pose clustering from dense range data. Computer Vision and Image Understanding, 115(10):1427 ­ 1448, 2011. [13] U. Hillenbrand and M. A. Roa. Transferring functional grasps through contact warping and local replanning. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS, 2012. [14] A.J. Ijspeert, J. Nakanishi, and S. Schaal. Movement imitation with nonlinear dynamical systems in humanoid robots. In Robotics and Automation, 2002. Proceedings. ICRA '02. IEEE International Conference on, volume 2, pages 1398 ­1403, 2002. [15] M Jeannerod. The timing of natural prehension movements. Journal of Motor Behavior, 16(3):235­254, 1984. [16] M. Jeannerod. Perspectives of Motor Behaviour and Its Neural Basis, chapter Grasping Objects: The Hand as a Pattern Recognition Device. 1997. [17] M. Jeannerod. Sensorimotor Control of Grasping: Physiology and Pathophysiology, chapter The study of hand movements during grasping. A historical perspective. Cambridge University Press, 2009. [18] J. Kober, B. Mohler, and J. Peters. Learning perceptual coupling for motor primitives. In Intelligent Robots and Systems, 2008. IROS 2008. IEEE/RSJ International Conference on, pages 834 ­839, sept. 2008. [19] R. N. Lemon. Neural control of dexterity: what has been achieved? Exp Brain Res, 128:6­12+, 1999. [20] C. R. Mason, J. E. Gomez, and T. J. Ebner. Hand Synergies During Reach-to-Grasp. Journal of Neurophysiology, 86(6):2896­ 2910, December 2001. [21] J. Nakanishi, J. Morimoto, G. Endo, G. Cheng, S. Schaal, and M. Kawato. Learning from demonstration and adaptation of biped locomotion. Robotics and Autonomous Systems, 47:79?­91, 2004. [22] M. Saleh, K. Takahashi, and N.G. Hatsopoulos. Encoding of coordinated reach and grasp trajectories in primary motor cortex. J Neurosci, 32(4):1220­32, 2012. [23] M. Santello, M. Flanders, and J. F. Soechting. Postural Hand Synergies for Tool Use. The Journal of Neuroscience, 18(23):10105­10115, December 1998. [24] M. Santello and J. F. Soechting. Gradual molding of the hand to object contours. Journal of neurophysiology, 79(3):1307­1320, March 1998. [25] A. Saxena, J. Driemeyer, and A. Y. Ng. Robotic grasping of novel objects using vision. Int. J. Rob. Res., 27(2):157­173, feb 2008. [26] M H Schieber. How might the motor cortex individuate movements? Trends Neurosci, 13(11):440­5, 1990. [27] R. Su´ arez, M. Roa, and J. Cornella. Grasp quality measures. Technical report, Technical University of Catalonia, 2006. [28] Johan Tegin, Staffan Ekvall, Danica Kragic, Jan Wikander, and Boyko Iliev. Demonstration-based learning and control for automatic grasping. Intelligent Service Robotics, 2009. [29] M. Toussaint and C. Goerick. A bayesian view on motor control and planning. In From Motor Learning to Interaction Learning in Robots, pages 227­252. 2010. [30] R. J. Vanderbei. Linear Programming: Foundations and Extensions. Springer, 2001.

In this paper we propose an approach to robot grasp prioritization based on a combined arm-and-hand metric. Most traditional approaches evaluate grasps based on hand-centric metrics such as force-closure, finger spread, contact surface area and similar measures. While these are certainly important factors to predict the robustness of a grasp, they do not carry information on the feasibility of the reaching action needed to execute the grasp. Based on our observations of physical pick-up experiments, we suggest that the execution success of a pick-up task is partially dependant on the easiness of the reaching movement. We present our metric, which combines 2 measures involving arm-kinematics and an existing hand heuristic metric. Results of simulated as well as physical experiments in our robot, Crichton, are presented.Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/303802736

Experience-basedTorqueEstimationforan IndustrialRobot
ConferencePaper·May2016
DOI:10.1109/ICRA.2016.7487127

CITATIONS

READS

0
5authors,including: ErikBerger TechnischeUniversitätBergakademieFreiberg
18PUBLICATIONS52CITATIONS
SEEPROFILE

62

DavidVogt TechnischeUniversitätBergakademieFreiberg
20PUBLICATIONS86CITATIONS
SEEPROFILE

HeniBenAmor ArizonaStateUniversity
56PUBLICATIONS434CITATIONS
SEEPROFILE

Someoftheauthorsofthispublicationarealsoworkingontheserelatedprojects:

Mining-RoX:AutonomousRobotsinUndergroundMiningViewproject

AllcontentfollowingthispagewasuploadedbyErikBergeron04June2016.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Experience-based Torque Estimation for an Industrial Robot
Erik Berger1 , Steve Grehl1 , David Vogt1 , Bernhard Jung1 , Heni Ben Amor2

Abstract-- Robotic manipulation tasks often require the control of forces and torques exerted on external objects. This paper presents a machine learning approach for estimating forces when no force sensors are present on the robot platform. In the training phase, the robot executes the desired manipulation tasks under controlled conditions with systematically varied parameter sets. All internal sensor data, in the presented case from more than 100 sensors, as well as the force exerted by the robot are recorded. Using Transfer Entropy, a statistical model is learned that identifies the subset of sensors relevant for torque estimation in the given task. At runtime, the model is used to accurately estimate the torques exerted during manipulations of the demonstrated kind. The feasibility of the approach is shown in a setting where a robotic manipulator operates a torque wrench to fasten a screw nut. Torque estimates with an accuracy of well below ±1 N m are achieved. A strength of the presented model is that no prior knowledge of the robot's kinematics, mass distribution or sensor instrumentation is required.

I. I NTRODUCTION Physical interaction between a robot and its environment requires accurate approaches for measuring and assessing forces applied by the robot to external objects and vice versa. For example, in human-robot interaction scenarios, exchanged forces may indicate unwanted collisions or result from intended human guidance. Accurate measurement of external forces is also important in manipulation tasks involving tool-usage. Humans often rely on sensory stimuli in order to estimate the state of a manipulation process, e.g., how hard a screw has been tightened. This paper addresses the problem of measuring forces during a manipulation task. Force-torque (FT) sensors are often the first choice for measuring the forces exerted in interactions with external objects. However, these sensors are limited to a specific location. The spatial distribution and cost of these sensors therefore need to be carefully balanced. In dynamic tasks, such as manipulation tasks, it may also become challenging to distinguish between different kinds of forces such as, for instance, forces caused by the task, sensor noise, or external perturbations of the current behavior. Furthermore, modern robots, such as the UR5 robotic arm, come with built-in, purely software-based capabilities for force sensing utilizing a mass-acceleration model. However, such methods require detailed knowledge about the kinematics and mass distribution of the robot. A drawback of such a method is that it quickly deteriorates in estimation accuracy when the model is imprecise, e.g. due to additionally attached equipment. In particluar, when the
1 Institute of Computer Science, Technical University Bergakademie Freiberg, Bernhard-von-Cotta-Str. 2, 09599 Freiberg, Germany 2 School of Computing, Informatics and, Decision Systems Engineering, Arizona State University, 699 S Mill Ave, Tempe, AZ 85281, USA

Fig. 1. A 3-finger gripper mounted on an UR5 robotic arm is utilizing a usual torque wrench.

robot is extended by a gripper or a tool, the mass distribution needs to be re-calibrated accurately. In case of the UR5 robot, the manufacturer specifies a force accuracy of ±25 N for the robot's tool center point (TCP). Also, the manufacturer warns that such a kind of force estimation provides no protection against momentum. In contrast to the approaches mentioned above, and motivated by the ability of a trained mechanic to estimate a screw nut's tightening torque from prior experience, this paper proposes an experience-based approach that does not require prior information about the robot platform. Task models are learned during a training phase based on the available sensor data, without relying on any further knowledge, such as mass distribution or robot kinematics. During runtime, expected and measured sensor values are compared and detected discrepancies are turned into force estimates. A specific manipulation scenario is investigated, in which a UR5 learns to adjust a screw nut using a torque wrench. Figure 1 shows the principle setup which is elaborated below.

II. R ELATED W ORK Robots that engage in physical interactions with humans and objects need to regulate the forces exchanged with their

Data Acquisition
Time Torque Sensor Data

Model Generation
Phase Feature Space Torque Feature Space

Feature Space Projection
Phase Estimation

Realtime Sensor Data

Torque Estimation

Fig. 2. An overview of the presented machine learning approach. During an offline training phase, sensor data together with information about the actual time and torque is recorded (section III-A) to generate two low-dimensional Feature Spaces (section III-B). In order to estimate the actual torque, realtime sensor data is projected into this spaces and compared with the training data (section III-C). The most similar matching is used as estimation of the actual torque.

environment. This requires methods for estimating the occurring forces as well as methods for adapting the robot behavior accordingly. Recent developments in compliant control have lead to the emergence of robots with joint torque sensing and feedback control [1]. For measuring external forces and perturbations, however, typically additional force sensors, e.g. force-torque sensors are used. Such sensors are often expensive, add weight to the robot, and are limited in their spatial resolution. Hence, various authors have suggested using algorithmic approaches for inferring applied forces. In [2], a depth camera is used in order to estimate applied forces. Using a depth camera allows for generic contact locations on the robot. In [3] machine learning methods are used to extract an inverse dynamics model for a cabledriven robot manipulator. Measuring the difference between predicted controls from the inverse dynamics model and the executed controls provides an estimate for external forces applied on the robot. A major challenge for such approaches however, is training an inverse dynamics model that is general enough to be applied to different situations. Using machine learning for force-based robot control has also been suggested a number of other publications. In [4] a robot learns to adapt its motion by anticipating human intentions from force measurements only. In [5], a method for learning force-based manipulation skills from demonstrations was presented. The approach generated variable-impedance control strategies thereby producing the necessary compliance for handling deformable objects. The work presented in the remainder of our paper focuses on different aspect: how can a robot learn to estimate forces from experience? In contrast to earlier discussed papers, we learn behavior-specific models for force estimation, that are narrower in scope, but accurate in results. III. A PPROACH An overview of the presented approach is shown in Figure 2, while the setup is further explained in Figure 3. The physical interaction considered is the tightening of a screw nut by using a wrench. Hence, the torque results in a preload force which is countered by a pressure spring. To secure a

Fig. 3. A 3-finger gripper mounted on a UR5 robotic arm turns a usual torque wrench about 45 . The applied torque (black) results in a preload force (blue) which is countered by the force conducted exerted by a pressure spring (green).

given tightening torque, a 3-fingered gripper mounted on a robotic arm is used to adjust the torque utilizing a custom torque wrench. The goal is to generate a behavior-specific model, which is able to estimate the torque from previous experience. The first step of the presented approach is to record example data representing the evolution of sensor values for different torque values. In the training phase, the values of all sensors available on the robot together with the actually exerted torque and time stamps are recorded during a 45 tightening movement. This is performed multiple times for different preconfigured torques. The recorded data is used to generate a model consisting of two components, the PhaseFeature Space (P-FS), and the Torque-Feature Space (TFS). These components are low-dimensional embeddings of the original high-dimensional data, that are extracted using Transfer Entropy [6] (TE) and Principal Component Analysis (PCA). During task execution, the P-FS is used to identify the phase of the behavior while the T-FS estimates the

applied torque based on the selected phase. To this end, the real-time sensor data is compared to sensor readings acquired during the training phase. Finally, the tightening torque is estimated, by determining the training data point with the highest similarity to the actual sensor readings. In the following, each step of the presented approach will be explained in more detail. A. Data Acquisition Similar to a skilled mechanic who is able to estimate a screw nuts torque from previous experience, the robot needs training data of the behavior. The realtime interface of the UR5 provides overall 105 different sensors, containing less useful information, e.g., the actual mainboard voltage and redundant data as control, target and actual joint values. However, the goal is to identify the sensors which are most significant for estimating the behavior phase and the exerted torque. For this, the m = 105 sensor values r = (r1 , . . . , rm ), are recorded during behavior execution. Additionally, the relative time w and the preconfigured torque v are recorded and stored in a m + 2 dimensional vector s = (r, w, v ). In contrast to the torque v , the relative time w increases during behavior execution and is reset after. Consequently, as the time for each training phase remains the same, w contains always the same data for each single recording. However, one behavior execution is recorded for two seconds with 125 Hz. The resulting training data S = (s1 ; . . . ; sn ) represents the training data for one possible torque v consisting of n = 250 equidistant samples. To estimate the tightening torque, multiple behavior executions for varying configurations are recorded. First, the screw nut was tightened with 6.0 N m and equidistantly increased by 1.0 N m up to 15.0 N m. In the following, the recorded training data D = (S1 ; . . . ; Sk ) for k = 10 different torque configurations is used to estimate the torque. To achieve a higher level of accuracy without resulting in a time consuming training phase, virtual training sets are interpolated. For this, Dynamic Mode Decomposition (DMD) [7] is utilized . A detailed explanation of DMD and how it is used in the context of sensor data interpolation can be found in [8]. B. Model Generation The recorded data D is investigated in order to extract relevant features. For this, two relevance values o = (o1 , . . . , om )T and p = (p1 , . . . , pm )T are computed for each sensor. On the one hand o describes how strong the sensor is influenced by the preconfigured torque v = D·,m+2 and on the other hand p describes how strong a sensor is influenced by the relative time w = D·,m+1 . The main idea is that sensor with a high TE are beneficial for estimating the corresponding value. In previous work [9], TE has been used to solve related tasks for perturbation detection during human-robot interaction. In the present work, TE is used as a measure of predictability and information flow between the relative time or torque and the evolution of sensor readings.

o[33...50] p[33...50]

Angle Sensors

Velocity Sensors

Current Sensors

Fig. 4. The Transfer Entropy for the actual angle, velocity and current sensors is calculated for the torque applied to the screw nut (blue) and the relative time (orange).

6Nm

10Nm

15Nm

Ampere Radian

4 3 2 1

-1.3 -1.4 -1.5 -1.6 0 125 250

Timestep
Fig. 5. The peak TE sensor streams. Top: Different torques result in varying sensor readings for the current sensor. Bottom: The angle sensor is not affected by the torque but increases over time.

The main idea is that sensors with a high TE w.r.t. the robot's behavior are deemed more influential and relevant. The formula for the TE between j and i is defined as
|i|-1

T E (j, i) =
t=1

p(it+1 , it , jt )log2

p(it+1 |it , jt ) , p(it+1 |it )

(1)

where the function p describes the conditional probability. Utilizing Formula 1 the TE between the relative time and the sensors p(q) and for the torque and the sensor o(q) is calculated by p(q ) = T E (w, D·,q ) o(q ) = T E (v, D·,q ), (2)

where q  [1 . . . m]. Figure 4 shows the resulting TE for the actual angle, velocity and current sensors contained in p33...50 and o33...50 . For o the highest TE values was found for the current sensors. The sensor with the highest TE o49 is visualized in the top of Figure 5. Obviously, the sensor stream differs for varying torques. The angle and velocity sensor share likewise less TE. Though, for p the TE is almost completely contained in two angle sensors. The sensor with the peak TE o35 is visualized in the bottom of Figure 5. As can be seen, the sensor stream is nearly the same for

different training data. Hence, independent from the applied torque, the sensor values are increasing over time. Therefore, the sensor is suitable for estimating the actual phase of the motor skill. Next, the sensors with at least 90% of the overall TE are used to select a subset of sensors from D in order to build a feature space with
^  : R m  Rm

(3)

where m ^  m. The threshold is determined empirically but can be changed in order to adapt the computational effort. However, sensors which are not influenced by the relative time are ignored during phase estimation while sensor not related to the torque are ignored during torque estimation. Finally, the dimensionality of the feature spaces is reduced even further by applying well-known PCA. During the further procedure, the low dimensional embedding of  (o) is used for the phase estimation and therefore is called P-FS. In contrast to that, the dimensional reduced  (p) is used to predict the torque and is called T-FS. C. Phase Estimation Due to small time shifts, occurring between multiple executions of a motor skill, the relative time is not sufficient to estimate the current phase of a behavior. Because of that, in contrast to the training data, the realtime data does not contain information about the relative time. Instead, the current phase is estimated from the P-FS which among other sensors is strongly affected from joint positions. For the estimation of the current phase of the behavior a time window with size t of the actual sensor stream is projected into the P-FS resulting in X = (x1 , . . . , xt ). In previous work [8], the Subsequence Dynamic Time Warping technique (SDTW) [10] was used for measuring the similarity between two sequences. In the case presented here, the low dimensional sequence X is compared with the low dimensional training data in the rows of the P-FS   y1,1 · · · y1,n  .  . .. . Y= . (4) . . .  yk,1 ··· yk,n where n = 250, k = 10, n  t and y is the low dimensional projection of a single time step. Since the UR5 provides equidistant sensor readings, the optimization problem of the SDTW is reduced to finding the optimal path p = (b , . . . , b + t - 1), where b is given by
t

are almost the same. This effect is utilized in order to further decrease the computational effort. For this, the training data in the first row k = 1 of Y is compared with X by applying Formula 5. The resulting path p 1 represents the current phase of the behavior and the last element in p 1 the estimated actual state of the robot. Due to the similarity of the phases, the search space for all other rows in Y is reduced by applying a hill climbing approach. The starting point of the search space for the remaining training sets is at b  p 1 . By applying Formula 5, the hill climbing method is searching the neighbors of b and stops when no reduced costs can be found. Since p 1 contains the global minimum it is assumed  that, due to their similarity, p 2 , . . . , pk only contain global  minima. Finally, all phases are stored in P = (p 1 , . . . , pk ). D. Torque Estimation In the following, the torque is estimated based on the previous phase estimation. By projecting the current time ^ = (x window into the T-FS resulting in X ^1 , . . . , x ^t ) the data can be compared to the low dimensional training data   y ^1,1 · · · y ^1,n  . . .. ^ = . (6) Y  . . . .  y ^k,1 ··· y ^k,n where n = 250, k = 10 and n  t and y ^ is the low dimensional projection of a single timestep. Comparing the ^ is time consuming. time window with the complete matrix Y In order to reduce the computational effort the comparison is shrunk to the actual phases P calculated in the previous section. This reduces the number of computations from (n - t) · k to k . In order to estimate the torque, the training ^ data l with the highest correspondence to the actual data X is calculated by
t

l = argmin
l[1:k] i=1

c(^ xi , y ^l,P (k,i) )

(7)

b = argmin
b[0:(n-t)] i=1

c(xi , yb+i )

(5)

and c is a local distance measure, which in our case is the Euclidean distance c = |x - y|. In contrast to SDTW no accumulated cost matrix or warping need to be computed what results in significant less computational effort. As illustrated in the bottom of Figure 5, an advantage of the presented phase estimation method is that the phases between the k recordings are just slightly shifted and therefore

where l  [1 . . . k ]. The preconfigured torque recorded during execution of the training data vl is used as estimation of the actual one. Since, the robot performs the movement during the estimation phase, the torque continuous increases. This makes a realtime approach necessary. As mentioned above, the training data is interpolated which effectivley increases the number of datasets k . In order to further decrease the computational demands, the previous mentioned hill climbing method is utilized. Instead of computing k possible matches the algorithm has a maximal computation time in which it searches l from random start positions in l. This time is set to a value less than the data rate provided by the robot, which in the case of the UR5 is 125 Hz. This ensures that the robot always know its actual state and is able to stop when a certain torque is reached. Obviously, the computational demands depend to the size of the time window t. However, in the following experiments 8 ms are sufficient for estimating the torque utilizing the introduced method.

2

Relative Time in [s]

P-FS ¬P-FS P-FS Actual Phase

TABLE I T HE MEAN ABSOLUTE ERRORS IN N m RESULTING FROM DIFFERENT
FEATURE SPACES FOR DIFFERENT SIGNAL - TO - NOISE RATIOS .

T HE

ERROR SIGNAL WAS GENERATED UTILIZING WHITE NOISE .

SN R
1

T-FS MAE 0.0 0.0 0.0 0.01 0.07

¬T-FS MAE 1.05 1.34 1.42 1.74 2.51

T-FS MAE 0.53 0.89 1.03 1.18 1.75

20 10 5 2.5 1.0
11

25

125

250

Timestep
Fig. 6. Phase estimation during behavior execution utilizing different sensor selection schemes. P-FS is generated from the sensors with 90% of the overall Transfer Entropy (orange) while ¬P-FS is generated from the remaining sensors (blue) and P-FS from all sensors (green). As can be seen, only P-FS is able to predict the actual phase (black) accurately.

Estimated Torque Configured Torque

Torque in [Nm]

10

IV. E XPERIMENTS To validate the proposed method different experiments are conducted. Therefore ten training data sets are recorded. For a more accurate torque estimation, the training data sets are further interpolated after feature extraction. The resulting data set contains 901 equidistant samples for torques in between [6.0 N m . . . 15.0 N m]. The sensors selection process is evaluated and the reliability is proven by investigating the absolute error. Consequently, the accuracy of the presented results is 0.01 N m. A. Feature Selection Evaluation The evaluation of the P-FS is done by comparing it to all sensors P-FS and the negation of it ¬P-FS. The torque is randomly chosen and low-dimensional projections of the last 25 recordings is provided. Figure 6 shows the resulting phase estimation for the three different groups. As presumed, the original P-FS is able to estimate the current phase accurately while both other groups fail at certain time steps. Although, there is nearly no difference between the result of ¬P-FS and P-FS. This is due to the fact that suitable sensors form a small subsets, identified by the introduced feature selection. This shows that the resulting feature space P-FS is suitable, to estimate the current phase of the behavior. Next, the quality of the selected sensors for the torque estimation is evaluated. Therefore, the data set for a torque of 8.0 N m is selected from the training data. In order to prove the robustness of the T-FS it is disturbed with white noise of the following signal-to-noise ratio: SN R =
2 Y ^
k,n

9

0

Timestep

125

250

Fig. 7. The torque estimation during realtime behavior execution. The screw nut is preconfigured with 9.6 N m (green). The torque estimation (blue) fails at the beginning because of the small size of the captured realtime data. After 0.2 s (highlighted area) the estimation gets close to the correct value. The accuracy of the estimation directly depends to the size of the recorded realtime data and gets close to the real torque after 2 s.

mations. That proofs that the selected sensors are a suitable choice for torque estimation. B. Realtime Torque Estimation In the final experiment a torque of 9.6 N m was set. The goal is to identify this torque as fast as possible in order to avoid an additional tightening. As mentioned the realtime interface of the UR5 provides 105 different sensors 125 times per second. Each sample is first projected into the P-FS and appended to the previous ones. For the P-FS the size of this low-dimensional segment is set to a size of 25 values what, as illustrated in Figure 6, results in accurate phase estimation results. However, as illustrated in Figure 6 before the segment contains 25 elements, no phase estimation is solved. Instead, the relative time is used in order to allow at least a approximate torque estimation. Utilizing the actual phase, the T-FS is used to estimate the actual torque by comparing the realtime projection with the training data. As can be seen in Figure 7 the torque estimation fails at the beginning, due to the small size of the projected segment. In contrast to the estimation of the phase, the torque estimation accuracy is increased by using larger segments. In more detail, the size of the low-dimensional projection window is not restricted. As can be seen in Figure 7 after 0.2 s the estimated torque matches the preconfigured one. In order to

2 noise

,

(8)

where  is the standard deviation. As done previously T-FS and ¬T-FS are generated in order to evaluate the selected sensors in the T-FS. Table I shows the resulting mean absolute errors for different levels of noise. Similar to the phase estimation, the T-FS produces the best results while ¬T-FS and T-FS result in incorrect esti-

avoid an additional tightening of the screw nut, the robot may stop the behavior execution or perform a reverse motion to adjust the screw nut to the initial value. However, after 2 s the torque was estimated with sufficient accuracy. This confirms the assumption, that the generated feature spaces can be applied in order to estimate the actual torque from previous experience. In the following section, the experimental results are discussed. C. Discussion As can be seen in Figure 6 the accuracy of the torque estimation is time dependent. This comes from the size and the actual values within the recorded time window. From time step 5 to about 60 the estimation overshoots the correct torque and converges slowly afterwards. This is due to the quality of the data recorded during the training phase. Taking into account Figure 5 top, one can see that the data model up to time step 40 contains overlapping sensor data. This leads to a less accurate model for this subsequence of the behavior. Past time step 40 the sensor data is clearly separated and therefore has a better estimation quality. Consequently, as the behavior continues, the phase shifts to a region with higher accuracy and improves the overall estimation results. The overall estimation converges slowly, since the time window still contains the less accurate data from the beginning of the estimation. Taking a closer look at Figure 6, the offset after 0.5 s is 0.3 N m and after 2.0 s still about 0.1 N m. The length of the torque wrench from the original tool center point is exactly .3 N m 0.23 m. This results in an accuracy of 1.3 N = 0 0.23 m after 0.1 N m 0.5 s and 0.43 N = 0.23 m after 2.0 s. Thus, the proposed model learning approach is more accurate than the mentioned force accuracy of ±25 N specified by the manufacturer. A limitation of the approach is that the robot needs to be configured exactly the same way between offline training and realtime estimation. However, in industrial settings this usually is the case. V. C ONCLUSION In order to provide a robot with a sense of force, an approach for torque estimation from prior experience was presented. Instead of using dedicated force sensors, data from all sensors available on the robot is recorded, including e.g. angle, velocity and current sensors. Using Transfer Entropy, a model consisting of two feature spaces is learned from the recorded sensor data. During runtime, sensor data is captured and projected into these feature spaces. The first

feature space is used to estimate the actual phase of the motor skill by comparing realtime and training data. The second feature space estimates the actual torque, again by comparing realtime with training data. By taking into account the previously calculated phase estimate, the computational effort for torque estimation is reduced. As explained in the discussion, the quality of the extracted models varies over time. In some cases, this could lead to TE values that vary for different phases of the behavior. Thus, a phase-dependent selection of the optimal subset of samples could be a beneficial extension of the proposed approach in order to increase the overall accuracy of the torque estimate. Nonetheless, with the present implementation torques were accurately estimated, achieving a quality that compares to force estimation capabilities achieved by humans. R EFERENCES
[1] R. Bischoff, J. Kurth, G. Schreiber, R. Koeppe, A. Albu-Sch¨ affer, A. Beyer, O. Eiberger, S. Haddadin, A. Stemmer, G. Grunwald, and G. Hirzinger, "The KUKA-DLR lightweight robot arm - a new reference platform for robotics research and manufacturing," in ISR/ROBOTIK 2010, Proceedings for the joint conference of ISR. VDE Verlag, 2010, pp. 1­8. [2] E. Magrini, F. Flacco, and A. De Luca, "Control of generalized contact motion and force in physical human-robot interaction," in Robotics and Automation (ICRA), 2015 IEEE International Conference on, May 2015, pp. 2298­2304. [3] A. Colome, D. Pardo, G. Alenya, and C. Torras, "External force estimation during compliant robot manipulation," in Robotics and Automation (ICRA), 2013 IEEE International Conference on, May 2013, pp. 3535­3540. [4] E. Gribovskaya, A. Kheddar, and A. Billard, "Motion learning and adaptive impedance for robot control during physical interaction with humans." in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2011, pp. 4326­4332. [5] A. Lee, H. Lu, A. Gupta, S. Levine, and P. Abbeel, "Learning force-based manipulation of deformable objects from multiple demonstrations," in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2015. [6] T. Schreiber, "Measuring information transfer," Physical Review Letters, vol. 85, no. 2, pp. 461­464, 2000. [7] M. R. Jovanovi, P. J. Schmid, and J. W. Nichols, "Sparsity-promoting dynamic mode decomposition," Physics of Fluids (1994-present), vol. 26, no. 2, 2014. [8] E. Berger, M. Sastuba, D. Vogt, B. Jung, and H. B. Amor, "Estimation of perturbations in robotic behavior using dynamic mode decomposition," Advanced Robotics, vol. 29, no. 5, pp. 331­343, 2015. [9] E. Berger, D. M¨ uller, D. Vogt, B. Jung, and H. Ben Amor, "Transfer entropy for feature extraction in physical human-robot interaction: Detecting perturbations from low-cost sensors," in Humanoids'14, 2014. [10] H. Sakoe, "Dynamic programming algorithm optimization for spoken word recognition," IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 26, pp. 43­49, 1978.

View publication stats

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/305775316

EstimatingPerturbationsfromExperienceusing NeuralNetworksandInformationTransfer
ConferencePaper·October2016

CITATIONS

READS

0
5authors,including: ErikBerger TechnischeUniversitätBergakademieFreiberg
18PUBLICATIONS52CITATIONS
SEEPROFILE

51

DavidVogt TechnischeUniversitätBergakademieFreiberg
20PUBLICATIONS86CITATIONS
SEEPROFILE

HeniBenAmor ArizonaStateUniversity
56PUBLICATIONS434CITATIONS
SEEPROFILE

Someoftheauthorsofthispublicationarealsoworkingontheserelatedprojects:

Mining-RoX:AutonomousRobotsinUndergroundMiningViewproject

AllcontentfollowingthispagewasuploadedbyHeniBenAmoron02August2016.
Theuserhasrequestedenhancementofthedownloadedfile.

Estimating Perturbations from Experience using Neural Networks and Information Transfer
Erik Berger1 , David Vogt1 , Steve Grehl1 , Bernhard Jung1 , Heni Ben Amor2
Abstract-- In order to ensure safe operation, robots must be able to reliably detect behavior perturbations that result from unexpected physical interactions with their environment and human co-workers. While some robots provide firmware force sensors that generate rough force estimates, more accurate force measurements are usually achieved with dedicated force-torque sensors. However, such sensors are often heavy, expensive and require an additional power supply. In the case of lightweight manipulators, the already limited payload capabilities may be reduced in a significant way. This paper presents an experience-based approach for accurately estimating external forces being applied to a robot without the need for a forcetorque sensor. Using Information Transfer, a subset of sensors relevant to the executed behavior are identified from a larger set of internal sensors. Models mapping robot sensor data to force-torque measurements are learned using a neural network. These models can be used to predict the magnitude and direction of perturbations from affordable, proprioceptive sensors only. Experiments with a UR5 robot show that our method yields force estimates with accuracy comparable to a dedicated force-torque sensor. Moreover, our method yields a substantial improvement in accuracy over force-torque values provided by the robot firmware.

Fig. 1. For safe behavior execution, robots must be able to reliably detect perturbations resulting from unexpected physical interactions with their environment and human co-workers.

I. I NTRODUCTION The ability to sense the environment is a vital requirement for intelligent and safe robotics. Modern sensors, such as force-torque sensors (FT) can be used to measure external influences on a robot and, in turn, generate adequate responses. However, it is often difficult to distinguish between natural, behavior-related fluctuations in the sensor readings and external perturbations that are caused by forces or collisions applied by the outside world. Dynamic tasks in particular can cause significant variation in sensor readings that could potentially be mistaken for external influence. In recent years, various methods have been proposed for behavior-specific estimation of external perturbations [1][2]. In prior work, we have introduced a new methodology for estimating forces from experience [3]. We have shown that nonlinear state prediction and machine learning can be used to generate accurate estimates of external perturbations, even in the absence of force measuring sensors. First, a model of the expected sensory feedback during a physical task is learned. During runtime, expected sensations are compared to measured sensor values and the difference is transformed into a perturbation estimate. Our results showed that feature extraction is a key component to the above methodology.
1 Institute of Computer Science, Technical University Bergakademie Freiberg, Bernhard-von-Cotta-Str. 2, 09599 Freiberg, Germany 2 School of Computing, Informatics and, Decision Systems Engineering, Arizona State University, 699 S Mill Ave, Tempe, AZ 85281, USA

Recent advances in deep learning research have produced powerful neural network models for feature extraction and nonlinear regression [4]. In this paper, we investigate such deep learning techniques as the core component of our methodology. To this end, we will contrast feature extraction using autoencoders to our previous approach using Transfer Entropy [5]. For modeling the evolution of sensor values in time, we will employ different neural network architectures, including feedforward and recurrent neural networks. One potential advantage of neural networks over previous approaches, is their scalability to large numbers of input dimensions and large numbers of data points, as well as their ability to represent both feature extraction and regression within the same framework. In the remainder of this paper, we will introduce our methodology for perturbation estimation and show how neural networks can be used to model the evolution of sensor values in time. We will present a number of experiments to identify performance of different neural network architectures in this application domain and evaluate the accuracy of these results. II. R ELATED W ORK Robots that engage in physical interactions with humans and objects need to regulate the forces exchanged with their environment. This requires methods for estimating the occurring intrinsic and external forces to allow for appropriate robot responses. Recent developments in compliant

Data Acquisition
Time FT Data Robot Data

Model Generation
Intrinsic FT- Model Total FT-Model

Perturbation Estimation
Intrinsic FT-Prediction Total FT-Prediction

Realtime Robot Data

Perturbation Value

Fig. 2. An overview of the presented machine learning approach. During an offline training phase, the robot sensor data together with information about the time, force, and torque are recorded (Section III-A). The recorded data is analyzed for features and used to learn two models (Section III-B). In order to estimate external influences, the robot's realtime sensor data is used to predict intrinsic (Section III-C) and total force-torque measurements(Section III-D). The difference between both is used as estimation of the actual external perturbation.

control have lead to the emergence of robots with joint torque sensing and feedback control [6]. For measuring external forces and perturbations, however, typically additional force-torque (FT) sensors are used. Such sensors are often expensive, add weight to the robot, and require additional power supply. Hence, various authors have suggested using algorithmic approaches for inferring applied forces. In [7], a depth camera is used in order to estimate applied forces. Using a depth camera allows for generic contact locations on the robot. In [1] machine learning methods are used to extract an inverse dynamics model for a cable-driven robot manipulator. Measuring the difference between predicted controls from the inverse dynamics model and the executed controls provides an estimate for external forces applied on the robot. Using machine learning for force-based robot control has also been suggested a number of other publications. In [8] a robot learns to adapt its motion by anticipating human intentions from force measurements only. In [9], a method for learning force-based manipulation skills from demonstrations was presented. The approach generated variable-impedance control strategies thereby producing the necessary compliance for handling deformable objects. The work presented in the remainder of our paper focuses on different aspect: how can a robot learn to estimate forces from experience? In contrast to earlier discussed papers, we learn behavior-specific models for intrinsic and total force estimation. To increase the accuracy, an additional FT-sensor is used during training these models. During runtime the FTsensor is not available anymore and therefore is predicted by the learned models from the robot's sensor data only. Finally, the difference between intrinsic and total forces is used to predict the magnitude and direction of external perturbations. III. A PPROACH The goal of the presented approach is to learn a behaviorspecific perturbation model which is able to predict the readings of a FT-sensor from previous experience. Specifically, a FT-sensor with six degrees of freedom (Robotiq FT150) is mounted close to the tool center point (TCP) of a robotic arm (Universal Robots UR5). Figure 1 shows the principal setup including a gripper for pick-and-place operations.

The depicted robot provides 104 different measurements from a multitude of internal sensors, e.g., the applied current or the joint encoder state. In our approach, sensor readings generated from these sensors of the UR5 are used to learn a model that predicts FT-values measured by the FT150. The force-torque sensor is used in this context to provide ground truth data about all measured forces acting on the robot. The basic rationale underlying our approach is that accurate estimates of forces applied on the robot can be generated by fusing information and evidence from a large number of low-cost sensors. Note that these sensors do not have to be related to force estimation. A crucial component in our methodology is the identification of relevant features that are used for learning the predictive models. Figure 2 show an overview of the approach. The first step of the presented approach is to record training data representing the evolution of sensor values for the particular behavior. In contrast to our previous work [3], [2] no labeled data is recorded. Instead, the values of the FT150 are used as ground truth data for the actual force and torque. Next, the recorded data is analyzed for relevant features. As presented in [10], classical dimensionality reduction methods such as Principal Component Analysis fail to identify behavior-specific features. Instead, Transfer Entropy [5] (TE) is utilized to extract the most relevant features. In turn, these features are used to train two neural network models. The first model is used to predict the natural, behavior-related dynamics and is therefore called intrinsic FT-model. The second network is trained to predict total FT-values acting on the robot (intrinsic dynamics + external perturbations) and is called total FT-model. The difference between total and intrinsic prediction is used to estimate the magnitude and direction of external perturbations applied by a human or through a collision. Since both models require only robot sensor data to predict the actual FT-values, no expensive and heavy FT-sensor is required during runtime. In the following, each step of the presented approach will be explained in more detail. A. Data Acquisition The realtime interface of the UR5 provides 104 different sensors, containing a variety of different information

Force [N]

sources, e.g., the mainboard voltage, control, target and actual joint values. Some of these sources are relevant to the FT prediction task while others are redundant or noninformative. Hence we first identify a smaller set of relevant, informative sensors. To this end, the robot sensor values s = (s1 , . . . , s104 ) and the FT-values f = (f1 , . . . , f6 ) are recorded during the execution of a behavior. Since the FT150 data rate is less than the UR5's 125Hz , FT-values need to be interpolated to generate intermediate values. We use Dynamic Mode Decomposition (DMD) [11], a nonlinear interpolation method as described in [2]. Additionally, the time index r is recorded. During behavior execution the time index increases and is reset after each repetition. This time index will be relevant for the later feature selection step of the intrinsic FT-model. For all following experiments a pick and place behavior was executed on the robot for 60 seconds. During this process, the behavior was repeated about 16 times. The recorded data R = ((s, f , r)1 ; . . . ; (s, f , r)n ) represents training data consisting of n = 7500 equidistant samples with no external perturbations. In addition, two other data sets T1 and T2 with a length of 60 seconds were recorded. In contrast to the training data, T1 was perturbed by a human during the last half of its execution, while T2 was continuously perturbed. B. Model Learning

Measured
All sensors
50

FF

TD

NARX

REC

25

Force [N]

0

-25

-50

Autoencoder features
50

25

Force [N]

0

-25

-50

TE features
50

25

0

-25

-50

In the following section, different neural network architectures for dynamical systems modeling are used to model the recorded robot dynamics. More specifically, a feedforward (FF), a time-delay (TD), a recurrent (REC), and a nonlinear autoregressive network with exogenous inputs (NARX) are employed. For the sake of reproducibility, each network was generated with one hidden layer containing six neurons. Training was performed with the LevenbergMarquardt method. In order to allow for a dynamic response, all but the FF network require setting a temporal delay parameter d. The TD network has a delay on the input weights while the REC network layer has a delayed recurrent connection to itself. In addition to delayed input weights, the NARX network makes use of previous predictions. For a more detailed description of the different neural network architectures, the reader is referred to [12]. All network architectures have been trained to map input data s to output data f . C. Intrinsic FT-Prediction In the following section, all neural networks are trained with R and a delay of d = 2. The goal is to predict the intrinsic state for the semi-perturbed data set T1. First, the neural networks is trained using all sensors. The resulting predictions for f2 can be seen in Figure 3. As long as no perturbation occurs, all networks are approximately predicting the correct value. However, during the second half of T1 all networks fail to predict the correct intrinsic state. This is due to the fact that the 104 input sensors contain non-relevant information that may obfuscate important features. Hence,

0

10

20

30

40

50

60

Time [s]

Fig. 3. The different predictions of f2  T1 for different neural network architectures trained with the data set R. Using all sensors (top), autoencoders (middle), or TE feature selection (bottom) influences the accuracy of the predictions. Only the sensors selected by utilizing TE was able to suppress the external perturbations in the second half of the data set.
Discarded Selected

0.12 0.1

TEr

0.08 0.06 0.04 0.02 0 0 10 20 30 40 50 60 70 80 90 100

Sensors

Fig. 4. 10 sensors (highlighted green) with 50% of the overall TE are selected as features for training. The remaining 94 sensors (highlighted blue) are classified as less important for predicting the actual phase and therefore being discarded.

feature extraction methods are needed to identify relevant features. A feature extraction approach that is gaining popularity, is the use of autoencoders [4]. An autoencoder consists of an encoder and a decoder component, both of which are neural networks. Broadly speaking, the encoder maps the input data to a smaller set of hidden neurons while the decoder tries to

reconstruct the original input. This process can be stacked to reduce the dimensionality of the input data through a stepwise layering. Using autoencoders, we reduce 104 sensors to 50 values using the first encoder, and then to 10 values by stacking a second encoder. For the encoding process, a logistic sigmoid function was used while a linear transfer function was utilized for the decoders. As a result, the 104 dimensional input data s is reduced to a 10 dimensional feature data set. Next, this feature data was used to train the neural networks. The resulting predictions for f2  T1 can be seen in Figure 3 middle. Especially during the perturbation phase, the overall accuracy increases since irrelevant information is discarded. However, a problem of autoencoders is that the temporal influence and correlation of variables is not taken into account. To predict the time-dependent intrinsic values of the FTsensor the actual phase of the behavior is taken into account. TE is used as a measure of predictability and information flow between the robots sensor values and the relative time by TEr = T E (s, r). The TE from J to I is defined as T E (I, J) =
iI,j J

0.08

Discarded Selected

0.06

TEf

0.04

0.02

0 0 10 20 30 40 50 60 70 80 90 100

Sensors

Fig. 5. 9 sensors (highlighted green) with 50% of the overall TE are selected as features for training. The remaining 95 sensors (highlighted blue) are classified as less important for predicting the actual total FT-values and therefore being discarded.

40

20

Force [N]

0

-20

-40

p(i + 1|i, j ) , p(i + 1, i, j )log2 p(i + 1|i)

-60

Measured All Sensors Autoencoders TE features
0 10 20 30 40 50 60

where (i1 , . . . , iq )  I and (j1 , . . . , jq )  J are the possible states of quantized time-series data and the function p(·|·) describes the conditional probability. For a more detailed derivation the interested reader is referred to [5]. The resulting TEr describes how strong each sensor of the robot is influenced by the relative time. Sensors with a high TE are assumed to be beneficial for predicting the relative time and therefore the actual phase of the behavior. Figure 4 shows the normalized and ordered TE values of TEr. As can be seen, only 10 sensors with at least 50% overall TE are selected for training the neural networks. These sensors are, in particular, target and control values of the robot's joint states (e.g. position and velocity), which are independent of external influences and therefore are good predictors for the actual phase of the behavior. The resulting predictions for this subset of sensors can be seen in Figure 3 bottom. In contrast to the previous results, the selected sensors are able to accurately predict the intrinsic fluctuations. Also during external perturbations the predicted intrinsic forces are not influenced. The difference between the actual measured total FT-values and the intrinsic ones can be used to estimate the magnitude and direction of an external perturbation. D. Total FT-Prediction In contrast to the prediction of the intrinsic FT-values explained in the previous section the total FT-values are not exclusively dependent on the state of the robot. Consequently, it is important that the neural networks are additionally trained with information about how external perturbations influence the sensors of the robot. To this end, the neural networks are trained with R and the perturbed data set T2. Furthermore, feature selection is not calculate with respect to relative time anymore. Instead, the TE is calculated between the robots sensor values and each FT-value by TEf = |T E (s, f )|.

Time [s]

Fig. 6. The different predictions of f2  T1 for a recurrent neural network which was trained with the data sets R and T2. Using all sensors (red), autoencoders (yellow), or TE feature selection (purple) influences the accuracy of the predictions.

Figure 5 shows the normalized and ordered TE values of TEf . As can be seen in the figure, a small set of 9 sensors contains more than 50% of the overall TE information. These are especially measured values (e.g. current values close to the TCP) and no more target and control values. This is due to the fact that external perturbations does not perturb target/control values but the measured actual ones instead. The resulting mean squared errors for the different network predictions of f  T1 are shown in Table I. As can be seen, utilizing TE for feature selection outperforms the usage of autoencoders or all sensors. Furthermore, for NARX and REC networks using autoencoders further deteriorates the results. A possible explanation for this effect is that the objective function of autoencoders only focuses on the amount of information retained by using the generated features. This may be detrimental in various physical tasks in which some
TABLE I T HE MEAN SQUARED ERRORS RESULTING FROM DIFFERENT INPUTS AND NEURAL NETWORK ARCHITECTURES . All Sensors FF TD NARX REC 155.24 95.48 22.43 13.74 Autoencoder 72.62 81.45 62.34 42.48 TE Features 25.14 14.82 7.33 3.41

sensors have limited variability but strong influence on the task. The best result is obtained by using TE features and the REC network architecture. Predictions of f2  T1 generated by the REC network are compared to using all sensors and autoencoders in Figure 6. Given these results, the following experiment utilizes the proposed TE feature selection method combined with recurrent neural networks. IV. E XPERIMENTS Different experiments have been conducted to validate the proposed method. To this end, the data set T1 introduced in Section III-A was used.

Firmware

Measurement

Prediction

50

Force [N] Force [N] Force [N]

0

-50

50

A. Accuracy of Estimates First, the RECo model is evaluated by investigating the mean absolute error (MAE) in comparison to ground truth data of the FT150 and an intrinsic FT-sensor included in the firmware of the UR5. This sensor makes use of joint torques and a kinematic model of the robot to predict the FT-values at the TCP. In order to get comparable results, the firmware was calibrated to the mass and size of the FT150. For the FT-sensor, the manufacturer specifies a force accuracy of 25 N at the TCP and a detection time of 250 ms. The different force estimates for each dimension can be seen in Figure 7. The estimates provided by the firmware sensor follow the general trend but exhibit significant noise. By contrast, the estimates of the RECo model are close to the ground truth data of the FT150. The accuracy of the firmware sensor resulted in a MAE of 26.7401 N which is slightly below the 25 N specified by the manufacturer. In addition, considering the mentioned detection time delay of 250 ms did not decrease the MAE score. In comparison, the MAE of the RECo model is 3.8336 N. Furthermore, the detection time is less than 10 ms (on a dual core with 3.2 GHz) and scales with the performance of the computer system. Increasing the network delay from d = 2 to d = 125 further reduces the MAE to 1.9417 N. Consequently, the model requires one second of continuous sensor data to start the prediction while the detection time slightly increases to 15 ms. However, in order to keep the robot reactive from the beginning, a minimal delay of d = 2 was used for all experiments. Additionally, the RECo model also provides better torque estimates (1.0761 N m) when compared to the firmware sensor (5.6735 N m). B. Perturbation Estimation The intrinsic forces need to be predicted in order to dissect external perturbations from the predicted total forces. For this task, the RECi model is used. Figure 8 shows the resulting intrinsic force predictions. As can be seen in Figure 8, the intrinsic forces are not affected by perturbations in the second half of the recording. Finally, the perturbation value shown in Figure 9 is defined as the difference between total and intrinsic FT-values. The length and direction of the perturbation value is used to estimate the magnitude and direction of external perturbations. Generated estimates only represent the external perturbations applied by humans,

0

-50

-100

60

40

20

0

-20

0

10

20

30

40

50

60

Time [s]

Fig. 7. The x (top), y (middle) and z (bottom) force values of the firmware FT-sensor (green), the FT150 sensor (blue), and the prediction of the learned model (purple). The predicted values provide a tighter approximation of the measured ground truth data.
x-direction
25 20 15

y-direction

z-direction

Force [N]

10 5 0 -5 -10

0

10

20

30

40

50

60

Time [s]

Fig. 8. The intrinsic force predictions are not influenced by the external perturbations.

collisions or other external factors. As a result, the proposed method can be applied during runtime without making use of a FT-sensor in order to estimate total, intrinsic and in consequence external FT-values from previous experience. A video of some further experiments can be found here 1 .
1 https://youtu.be/60ue0X25S6k

40

x-direction y-direction z-direction

20

0

-20

-40

-60

0

10

20

30

40

50

60

Time [s]

Fig. 9. The difference between the total and intrinsic forces represent the perturbation value which is the FT-value applied to the robot from its environment.
Perturbation
50

light-weight manipulators with a limited payload.A further strength of the presented approach is that no prior knowledge of the robot kinematics, dynamics, or sensor characteristics is required. As a result, the approach generalizes to arbitrary robot platforms. We have shown that, without further adjustment, usual neural network architectures produce adequate estimates of the intrinsic, external, and total FT-values. Adapting the network properties, for instance by increasing the network delay, could further increase the estimation accuracy. A limitation of the approach is that the robot needs to perform the same behavior during training and runtime estimation. However, early results on the generalization capability of this approach show that it generalizes to mild variations of the behavior. A more in-depth evaluation of the generalization ability will be conducted in future work.

Perturbation [N]

No Perturbation

40

R EFERENCES
[1] A. Colome, D. Pardo, G. Alenya, and C. Torras, "External force estimation during compliant robot manipulation," in Robotics and Automation (ICRA), 2013 IEEE International Conference on, May 2013, pp. 3535­3540. [2] E. Berger, M. Sastuba, D. Vogt, B. Jung, and H. B. Amor, "Estimation of perturbations in robotic behavior using dynamic mode decomposition," Advanced Robotics, vol. 29, no. 5, pp. 331­343, 2015. [3] E. Berger, S. Grehl, D. Vogt, B. Jung, and H. Ben Amor, "Experiencebased torque estimation for an industrial robot," in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2016. [4] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol, "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion," J. Mach. Learn. Res., vol. 11, pp. 3371­3408, Dec. 2010. [5] T. Schreiber, "Measuring information transfer," Physical Review Letters, vol. 85, no. 2, pp. 461­464, 2000. [6] R. Bischoff, J. Kurth, G. Schreiber, R. Koeppe, A. Albu-Sch¨ affer, A. Beyer, O. Eiberger, S. Haddadin, A. Stemmer, G. Grunwald, and G. Hirzinger, "The KUKA-DLR lightweight robot arm - a new reference platform for robotics research and manufacturing," in ISR/ROBOTIK 2010, Proceedings for the joint conference of ISR. VDE Verlag, 2010, pp. 1­8. [7] E. Magrini, F. Flacco, and A. De Luca, "Control of generalized contact motion and force in physical human-robot interaction," in Robotics and Automation (ICRA), 2015 IEEE International Conference on, May 2015, pp. 2298­2304. [8] E. Gribovskaya, A. Kheddar, and A. Billard, "Motion learning and adaptive impedance for robot control during physical interaction with humans." in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2011, pp. 4326­4332. [9] A. Lee, H. Lu, A. Gupta, S. Levine, and P. Abbeel, "Learning force-based manipulation of deformable objects from multiple demonstrations," in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2015. [10] E. Berger, D. M¨ uller, D. Vogt, B. Jung, and H. Ben Amor, "Transfer entropy for feature extraction in physical human-robot interaction: Detecting perturbations from low-cost sensors," in Humanoids'14, 2014. [11] M. R. Jovanovi, P. J. Schmid, and J. W. Nichols, "Sparsity-promoting dynamic mode decomposition," Physics of Fluids (1994-present), vol. 26, no. 2, 2014. [12] M. Nrgaard, O. E. Ravn, N. K. Poulsen, and L. K. Hansen, Neural Networks for Modelling and Control of Dynamic Systems: A Practitioner's Handbook, 1st ed. Secaucus, NJ, USA: Springer-Verlag New York, Inc., 2000.

Force [N]

30

20

10

0

0

5

10

15

20

Time [s]

Fig. 10. behavior.

Non-perturbed and perturbed execution of a pick and place

C. Discussion As can be seen in Figure 7 (bottom), the force applied on the robot is estimated to be about 23 N. This is due to the weight of the gripper mounted on top of the FT-sensor ­ the manufacturer specifies a weight of 2.3 kg. A strength of the presented approach is that, no prior knowledge of the robots kinematics, dynamics, sensor instrumentation, or parameters is required. As a result, our approach can quickly be applied to any kind of robot platform. In addition, no hard thresholds for detecting external perturbations need to be set. Figure 10 illustrates this point. The green trajectory shows the force readings during a normal execution of a pick and place behavior. As can be seen, forces of 22 ± 3N are generated. In contrast to that, the blue trajectory shows an execution with different degrees of perturbations. V. C ONCLUSION In this paper, we have described a methodology for estimating forces from experience and showed how to use neural network models in order to generate accurate estimates of external perturbations. The main advantage of the presented approach is that the learned models are able to map low-cost robot sensor data to accurate FT measurements. Hence, no FT-sensor is required during runtime and consequently the robot's weight is reduced. This is particularly beneficial for

View publication stats

Grasping for a Purpose: Using Task Goals for Efficient Manipulation Planning
Ana Huam´ an Quispe Heni Ben Amor Henrik I. Christensen M. Stilman
Abstract--In this paper we propose an approach for efficient grasp selection for manipulation tasks of unknown objects. Even for simple tasks such as pick-and-place, a unique solution is rare to occur. Rather, multiple candidate grasps must be considered and (potentially) tested till a successful, kinematically feasible path is found. To make this process efficient, the grasps should be ordered such that those more likely to succeed are tested first. We propose to use grasp manipulability as a metric to prioritize grasps. We present results of simulation experiments which demonstrate the usefulness of our metric. Additionally, we present experiments with our physical robot performing simple manipulation tasks with a small set of different household objects.

arXiv:1603.04338v1 [cs.RO] 14 Mar 2016

I. I NTRODUCTION The ability to grasp objects in order to accomplish a task is one of the hallmarks of human intelligence. Numerous psychological studies show that humans grasp selection depends on the goal to be accomplished [14]. Decision making during grasping is therefore not only based on stability during manipulation, but also based on task requirements. If a specific grasp does not facilitate the execution of the upcoming subtasks, it is omitted from the reasoning process. In contrast to that, research on robot grasp synthesis has been tilted towards optimizing stability metrics only. A prominent approach is to generate a set of physically stable grasps, one of which is then selected by the high-level planner. If a high-level task planner cannot achieve the goals of the task, it has to back track and try a different grasp. Since no information is flowing between high-level planning and lowerlevel grasp generation, a large number of grasps may have to evaluated. If the required grasp is not within the optimized set of candidates, the entire task will fail. In this paper, we introduce a method for manipulation planning which uses foresight to identify tasks constraints. Constraints extracted from subsequent sub-tasks are used to synthesize grasps that facilitate overall task completion. Our goal is to derive a fast planning algorithm that can efficiently generate manipulation sequences for previously unseen objects. These latter properties, hence, allow a robot to perform manipulation tasks in new environments without resorting to prior 3D models of the object or pre-calculated grasp sets. This ability to generalize is realized by using a super-quadric representation of objects. We show how super-quadrics can be extracted from a single depth image and how they can be used to generate a large set grasp candidates.
Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA 30332, USA. ahuaman3@gatech.edu,

Fig. 1: Example of grasp selection based on goal constraints: The goal location of the chips box is surrounded by nearby objects, hence an overhead grasp must be selected for manipulation. If not for the obstacles present, a different grasp (overlayed in red) from the side would be easier to execute. Short planning times are realized by introducing a heuristics that efficiently guides the search by incorporating arm kinematics. Inspired by the end-comfort effect [20] in humans, grasps are preferred which lead to a comfortable arm configuration at the end of a task. We borrow ideas from this work and propose to use a metric based on manipulability as a measurement of end-comfort. The contributions of this paper are threefold, namely (1) a framework for online grasp planning that incorporates future task constraints into the grasp synthesis process, (2) an efficient grasp generation approach based on super-quadrics that works with previously unseen objects, (3) the end-comfort heuristics for efficient search during manipulation planning. The rest of this paper is organized as follows: Section II present relevant work in the area of grasp synthesis and grasp selection. Section IV presents our grasp generation method using a primitive-based approach and in Section V we introduce our manipulability-based strategies to prioritize the generated grasps. Section VI shows the results of the comparisons in simulation and the metrics we used to compare their performance. Finally, we present the application of our approach in our physical robot. We conclude this paper with section VII, where we provide some discussion regarding future work, and the advantages and shortcomings of our approach.

hbenamor@cc.gatech.edu, hic@cc.gatech.edu

Superquadric Fitting

Grasp Generation at Goal Pose

Validation at Start Pose

Prioritization

End Yes

Found Solution?

Plan Pick and Place

Pop First Grasp

No

Fig. 2: Manipulation planning pipeline: a partial point cloud of the object is first analyzed for symmetries and then turned into a superquadric representation. Grasps at the goal and the start position are generated and then prioritized according to end-comfort. Potential grasps are then analyzed within the plan and then executed. II. R ELATED W ORK In this section we review work concerning grasp synthesis and grasp selection. For a more detailed review of previous research in the area, we suggest the interested reader to consult the excellent reviews from Bohg [3] and Sahbani [21]. Pioneering work on grasp selection was developed by Cutkosky [5], who observed that humans select grasps in order to satisfy 3 main types of constraints: Hand, object and taskbased constraints. As pointed out by Bohg et al. in [3], there is little work on task-dependent grasping when compared to work focused on the first two constraints. Hence, the main goal for a planner is to find a grasp such that the robot can approach the object and execute the said grasp, without further regard of what the robot will do once the object is picked. Grasp generation methods vary widely depending on the assumptions considered. In the case of grasp planning for known objects, Ciocarlie et al. [4] presented the concept of eigengrasps, which was exploited to generate candidate grasps searching in a low dimensional hand posture space using their GraspIt! simulator. Diankov generated grasps by sampling the surfaces of object meshes and using the normals at the sample points to guide the approach direction of the hand [6]. Approaches using primitive representations were also proposed such that the grasp generation depends on the particular primitive characterization: Miller et al. [15] proposed to use a set of primitive shapes (cylinder, box, ball) to decompose complex objects. Huebner and Kragic [12] used bounding boxes, Przybylski et al. proposed the Medial Axis representation [18], Goldfeder et al. [9] used superquadrics due to their versatility to express different geometry types with only 5 parameters. In all the cases mentioned, the grasps are generated offline and stored in a database for future use. These grasps are usually ranked based on their force-closure properties, which theoretically express the robustness and stability of a grasp. One of the most popular metrics ( ) was proposed by Ferrari and Canny [7]. However, it has been noted by different authors that analytical metrics do not guaranteee a stable grasp when executed in a real robot. This can be explained by the fact that these classical metrics consider assumptions that don't always hold true in real scenarios (dynamics, perceptual and modelling inaccuracies, friction conditions). On the other hand, studies that consider human heuristics to guide grasp search have shown remarkable results, outperforming classical approaches. In [1], Balasubramanian observed that when humans kinestetically teach a robot how to grasp objects, they strongly tend to align the robotic hand along one of the object's principal axis, which later results in more robust grasps. The author termed skewness to the metric measuring the axis deviation. In [19], Przybylski et al. combine the latter metric with and use it to rank grasps produced with GraspIt!. Berenson et al. [2] proposed a score combining 3 measures: , object clearance and the robot relative position to the object. In this work we are interested in manipulation of unknown objects. Multiple approaches of this kind have flourished during the last few years, particularly due to the advent of affordable RGB-Depth sensors. Since the 3D information is partial and noisy, classical approaches to grasp generation cannot be directly used. Rather, most of the current work uses heuristics to guide grasp generation based on local representation of the object geometry features (or global features if the object shape is approximated). In [10], Hsiao et al. use the bounding box of the object segmented pointcloud to calculate grasp approach directions using a set of heuristics. We should notice that for most of these approaches, their effectiveness can only be verified empirically. As we mentioned at the beginning of this section, the metrics we discussed above do not consider the task to be executed after the grasp is achieved. Some authors, however, have investigated this issue at some level. In pioneering work, [13] Li and Sastry proposed the concept of the task ellipsoid, which maximizes the forces to be applied in the direction of the task. More recently, Pandey et al. [16] proposed a framework to select a grasp such that the object grasped can be manipulated in a human-robot interaction scenario in which the goal pose of the object is not entirely constrained. Finally, although one of our main concerns is to select a grasp that is suitable for the task to be executed, we also consider important to use a grasp that allows for a simple, easy

Fig. 3: System setup and problem description.

a complete point cloud, multiple images can be acquired by either iteratively moving the camera or the object. This process is time-consuming and introduces new challenges such as the precise matching of the individual point clouds of each view. To solve this problem, we use a super-quadric representation of objects and reason about symmetry in order to infer the shape of any invisible part. Superquadrics are a family of geometric shapes that can represent a wide range of diverse objects using a limited set of parameters. Superquadrics can be expressed with their implicit equation: x a
2 2

arm execution. Interestingly, the problem of grasp planning is usually considered isolated from arm planning. In some recent work, Vahrenkamp et al. proposed Grasp-RRT [22] in order to perform both grasp and arm planning combined. In a similar vein, Roa et al. also proposed an approach that solve both problems simultaneously [8]. Both approaches focus on reaching tasks. Our approach tackles pick-and-place tasks in which reaching is only the first half of the solution (object placing being the second). We make use of our proposed heuristics to solve the complete pick-and-place problem in a manner as efficient as possible by means of grasp prioritization. III. P ROBLEM D EFINITION AND A SSUMPTIONS Our problem description can be explained as follows: Given a bimanual manipulator R and a simple object O, the manipulation task consists on transporting O from a given start pose w Ts to a final pose w Tg . Figure 3 depicts the problem described. The following constraints are considered: · A 3D model of O is not available beforehand. · A one-view pointcloud of the scenario is available from the Kinect sensor mounted on top of the robot shoulders. · Each limb of R consists of a 7-DOF arm (AL , AR ) and a 3-fingered hand (HL , HR ). A semi-analytical IK solver is available for AL and AR In the following sections we will describe our basic approach for problems in which only the use of one arm is required to solve the manipulation task described. IV. G RASP G ENERATION FOR U NKNOWN O BJECTS As our problem description stated, our approach must find a plan such that O can be grasped at w Ts , transported and finally repositioned at w Tg . Our approach consists on 4 main steps, shown in Figure 2. The first 3 steps (object fitting, grasp generation and grasp validation) will be explained in the rest of this section. A. Object Representation using Superquadrics Requiring complete 3D models of objects before grasp synthesis severely limits the application domains of robot manipulation. Modern depth cameras partly solve the problem, since they allow the robot to estimate the surface of an object. Yet, since the point clouds are acquired from a specific perspective, they only hold partial shape information about the visible frontal part. To fill any gaps and produce

y + b

2 2

2 1

+

z c

2 1

=1

(1)

In our approach, we generate a super-quadric representation using a single depth image. Fitting of the parameters can be performed online by minimizing the difference between the model and the partial point cloud [11]. However, since only one side of the object is visible, a standard approach to fitting will result in erroneous approximations of the object. To reproduce the entire shape from a partial point cloud, we added an additional pre-processing step to the superquadric fitting process. Instead of using the original point cloud as input, we generate a mirrored version by finding an optimal symmetry plane [11]. The goals of this step is to exploit symmetries to infer invisible parts of an object. The output of this process for a given object O consists on a transformation w To in world coordinates and the parameters, psq = { a, b, c,
1, 2

}

defining its approximated geometry. A good number of household objects can be easily described with generic shapes such as boxes, cylinders and ovoids, for which we can further bound the shape parameters considered:
1, 2

 [0.1, 1.9]

Figure 4 shows different geometric shapes corresponding to superquadrics with different values for 1 and 2 . The superquadric approach turns the pointcloud-based representation into a parametric representation, which can be much more efficiently used during grasp synthesis. Calculations of principal-axes, normals and other features are much faster and less susceptible to noise. B. Generating Valid Candidates Once the shape of O is approximated, we can proceed to generate candidate grasps gi using a simulation of the robot, and the object O, whose mesh is reconstructed by using the superquadric parameters found in the previous section. The candidates grasps must be kinematically feasible to execute with O located at both start and goal conditions (w Ts and w Tg ). Our approach accomplish this with Algorithm 1. First, we set O at its goal pose w Tg and generate a set of kinematically feasible grasps for it (G ). Next, we set O at its start pose w Ts . Finally, we test each of the grasps gi from G in this scenario, discarding the grasps for which there exist

1.2

superquadrics equation. We use the method proposed by Pilu and Fischer [17] to obtain an evenly-spaced set of points and normals. To define a grasp we calculate the transformation of the hand H w.r.t. O(o Th ). We use the samples to generate this transformation (lines 3 to 7 of Algorithm 2). After positioning the TCP of the H at w Tp , we close the fingers. If there are not collisions with the environment, we proceed to evaluate if there exists at least an arm configuration that allows the hand to execute the grasp. If so, then a corresponding grasp is stored.
1.0

Fig. 4: Examples of superquadrics with different shapes. A variety of shapes can be represented using a small number of parameters.

Algorithm 2: GenerateGrasps(H, A, w To ,O,psq ) Input: H, A , w To , O, psq Output: A feasible set of grasps G
/* psq = { 1 , 2 ,a,b,c}
1

*/

S = sample_SQ(psq ) foreach (pi ,ni )  S do
/* p: TCP point in the hand H */ */ */
o

not a single IK solution. The surviving grasps in G are then grasps that can be executed for the object O at both w Ts and w Tg . Algorithm 1: get Valid Candidates Input: H, A , w Ts , w Tg , O, psq Output: Set of Candidate grasps G
1

2

3

Tp .trans = pi Tp .z = -ni

/* z: Approach direction of H
4 o

/* x: Fingers closing direction
5 6 o

set_Pose(O,w Tg )
/* Generate grasps with O at
wT g

7

*/
8

Tp .x = smallest_Axis(a,b,c) o Tp .y = o Th .z × o Th .x w Tp = w To ·o Tp
/* h: Origin of hand H
w w p

2 3

G  generate_Grasps(H, A ,w Tg ,O, psq ) set_Pose(O,w Ts )
/* Discard grasps invalid with O at
wT s

*/

*/

9 10 11 12 13

4 5 6 7 8

foreach gi  G do if exist_IK_sol(gi , H, A ) is false then G .erase(gi ) return G

Th = Tp · Th setHand_Tcp(H, w Tp ) close_Hand(H ) if check_collision(H ) is false then if exist_IK_conf(H, A ,w Th ) is true then G  Grasp(H, o Tp ·p Th )

14 15

return G

1) Grasp Generation at Goal Pose: The function generate_Grasps, which we use to produce grasps exploiting the shape parameters of O is shown in Algorithm 2. First, we uniformly sample the surface of O. This is easily done by using the explicit equation defining the points in a superquadric and their corresponding normals:     x a cos 1  cos 2    y  =  b cos 1  sin 2   with 2 <  < 2 (2) << z c sin 1    1 2- 1 2- 2    cos   a cos nx 1  2 - ny  =  cos2- 1  sin 2   (3) b    nz 1 sin2- 1  c Sampling uniformly  and  does not produce a uniform sampling of surface points due to the high nonlinearity of the

Algorithm 2 generates at most one grasp per each sampled point. Optionally, we generated 2 additional possible grasps per each point by rotating the hand an angle ± around the x axis of o Tp . We added this since we noticed that, when executing the grasps on the physical robot, a slight inclination usually made the grasp much easier to reach. In this paper we used  = 30o . An example of the variated grasps generated using  is shown in Figure 5. 2) Validation at Start Pose: Once a set of grasps feasible to execute on O at w Tg is obtained, our algorithm discards the grasps that cannot be executed with O at w Ts (lines 4 to 6). V. G RASP P RIORITIZATION Once a set of feasible grasps G is generated, paths for reaching and placing the object must be produced. A brute-force approach would be to exhaustively try each grasp in a random

Fig. 5: Generating grasps varying the approach direction by rotating the hand around the local x axis(pointing out the page): Left: Original. Middle,right: After rotating by ± order until a solution is found. However, arm planning can be a time-consuming process, particularly when using samplingbased methods. It is therefore desirable to first evaluate grasps that are more likely to produce a solution. Since there is likely more than one solution in G , it is preferable to choose grasps such that the solution is quickly found. We propose to use situated grasp manipulability as a metric to prioritize the grasps and, hence, as a heuristic for guiding the search process. Manipulability(m) measures how dexterous the end-effector of a robotic arm is at a given joint configuration q . Initially proposed by Yoshikawa [23], m(q ) is defined as: m(q ) = J (q )J T (q ) . (4)

From the shown example, it becomes evident that for a pick-and-place manipulation problem, there are at least two possible metrics to use per grasp gi : mg measured either at w Ts or w Tg . Choosing the first option means that we prioritize grasps in which the pick phase is executed comfortably (w Ts ), whereas by choosing the latter, we favor grasps in which the arm configuration used at placing the object (w Tg ) is more relaxed. In section VI, we present the results of experiments comparing these two metrics and an additional control measure to analyze their performance and choose which one is best suited for our problem. VI. E XPERIMENTS AND R ESULTS In this section, we perform a set of experiments in simulation and on the real robot in order to evaluate the introduced manipulation planning algorithm. The simulation experiments are used to analyze the situated grasp manipulability using a large number of trials. Experiments on the real robots are performed to show the generation of manipulations based on task goals and previously unknown objects. Generation of superquadric object models was performed on the spot within 1 second. A. Simulation Experiments In this experiment, we consider three alternatives: w · Measure mg for grasp situated at Ts w · Measure mg for grasp situated at Tg · Average of both measures above We use 3 measures to compare the performance of the 3 evaluated metrics. · First success: The main goal of the metrics evaluated is to prioritize the grasps such that the first one tried is the most likely to succeed. This metric indicates the number of times that a solution is found by evaluating only the grasp with the biggest value for the evaluated metric. · Planning time: It measures the average planning time of the successing grasps. The planning time is the total time to plan a reach and transport path for the given grasp. · Path length: It indicates the number of steps required for the pick-and-place solution. The step length is a normalized value in joint space, so this metric compare the paths in configuration space. The scenario we used is depicted in Figure 7. We fix the w Tg to the middle of the table (red marker) and vary the start pose w Ts to 35 positions, each separated 0.1 m (green markers). We devised 2 kind of experiments: In the first, w Ts and w Tg have the same orientation, with only the position being changed (35 scenarios). In the second case, w Ts presents a rotation around  steps, the Z axis w.r.t. w Tg in the interval [0, 2 ] at each 4 so in total 35 × 8 = 280 scenarios are tested. We used an standard IK-BiRRT to perform the arm planning. To account for its randomness, the results presented in Table I and Table II are an average of 5 runs per each experiment. From the tables, we can observe that using mg evaluated at w Tg produces the best results in terms of success at the first

Manipulability is typically defined for a single joint configuration. In our scenario, we describe a situated grasp gi for which multiple q might exist, due to redundancy. This naturally leads to the definition of situated grasp manipulability(mg ). Given a target object O located at w To , and its corresponding grasp gi , we define mg as the average manipulability of a uniform set of collision-free arm configurations qi that allow executing gi : mg = 1 N
N

m(qi )
i=1

(5)

Please note that mg depends on both gi , w To and the environment (for collisions) since only collision-free grasps that reach the object are considered. Figure 6 shows an example of a pick-and-place task wherein the green and red markers indicate the w Ts and w Tg . In this case, mg at w Tg is bigger than w Ts (where Ns = 76 and Ng = 108 are the number of IK solutions for both situations). When the object is at w Tg , the arm movement requires less effort.

Fig. 6: Examples of mg measured at w Ts and w Tg

TABLE II: Evaluation with rotation change
Metric Type mg at mg at
wT s wT g

Path Steps 100.7 99.6 100.4

Planning Time 4.70 4.49 3.83

Success 255/278 275/278 260/278

Avg. mg

Fig. 7: Setup for unimanual evaluation experiments TABLE I: Evaluation with no rotation change
Metric Type mg at mg at
wT s wT g

Path Steps 82.92 89.28 92.92

Planning Time 2.17 2.218 2.29

Success 21.8/35 33/35 31.4/35

Avg. mg

trial, whereas w Ts present the worst results. The average path length for the general case of Table II is rather similar for the 3 cases. Regarding planning times, the average mg gives better results. Given the results presented, we chose to use the mg at w Tg . Its next best competitor (the avg. mg ) was not considered since in order to calculate it, the mg at both w Ts and w Tg must be calculated, which increases the computation time (for the examples presented, the computation time of mg was 2 seconds). Given that the advantage of planning time is not significant, we chose mg at w Tg . B. Robot Experiments Next, we perform a set of experiments on the real robot. All performed experiments are pick-and-place tasks. However, in some tasks we add environmental constraints at the goal location which limit the range of applicable grasps. Figure 8 depcits two trials without any environmental constraint. The robot has to pick an object at the starting location (green) and move it successfully to the goal location (orange). The robot has no prior knowledge of the object and needs to extract shape information from a single depth image produced by a depth sensor mounted in the head. As can be seen in the figure, the grasp direction and the hand shape is adapted to suit the object. A different set of experiments can be seen in Figure 9. Here, environmental constraints at the goal are introduced. In the top row, the object has to be placed in a box. Accordingly, the robot has to choose a grasp that allows it to place the object in the box without colliding with it. Hence, the selected grasps are mostly from above. The middle row show a different scenario, in which the object has to be placed on a box which is farther away. Choosing the wrong approach direction, e.g. from above, would prevent the robot from successfully finishing the manipulation process, due to workspace limitations. The bottom row shows normal runs without any environmental constraints.

Fig. 8: Two examples of a pick-and-place without constraints. The robot can identify suitable grasp for novel objects using the superquadric representation. VII. C ONCLUSION In this paper, we introduced a new method for manipulation planning with task constraints. Given a previously unknown object and goals of the task, the method synthesizes online a grasp that facilitates task completion. Planning and grasp synthesis are effectively merged to efficiently produce manipulation sequences. Object acquisition, representation, grasp synthesis and planning can be performed within a couple of seconds, i.e., 2-5 seconds, for the presented examples. We showed how superquadrics and a new heuristic, i.e., the situated grasp manipulability can be used towards this end. These properties, hence, allow a robot to perform successful, goal-driven manipulation tasks in new environments without resorting to prior 3D models of the object or pre-calculated grasps. While superquadrics can be efficiently calculated, they lack accuracy when representing complex shapes and objects. In this paper, we showed that many objects, in particular household objects can be represented by superquadrics. In our future work, we want to explore extensions of this representation that can model a larger set of objects. In particular, we want to build upon our previous research on the identification of rotational and linear extrusions [11] to represent more complex shapes. In addition, we want to verify the introduced planning approach on longer manipulation sequences.

Fig. 9: Final grasp configuration during manipulation with goal constraints (top and middle) and without goal constraints (bottom).

R EFERENCES
[1] R. Balasubramanian, L. Xu, P. D Brook, J.R. Smith, and Y. Matsuoka. Physical human interactive guidance: Identifying grasping principles from human-planned grasps. In The Human Hand as an Inspiration for Robot Hand Development. Springer, 2014. [2] D. Berenson, R. Diankov, K. Nishiwaki, S. Kagami, and J. Kuffner. Grasp planning in complex scenes. In 7th IEEE-RAS Int. Conf. on Humanoid Robots, 2007. [3] J. Bohg, A. Morales, T. Asfour, and D. Kragic. Data-driven grasp synthesis: A survey. IEEE Transactions on Robotics, 2014. [4] M. Ciocarlie, C. Goldfeder, and P. Allen. Dimensionality reduction for hand-independent dexterous robotic grasping. In IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), pages 3270­3275, 2007. [5] M.R. Cutkosky. On grasp choice, grasp models, and the design of hands for manufacturing tasks. IEEE Transactions on Robotics and Automation, 1989. [6] R. Diankov and J. Kuffner. Openrave: A planning architecture for autonomous robotics. Robotics Institute, Pittsburgh, PA, CMU-RI-TR08-34, 79, 2008. [7] C. Ferrari and J. Canny. Planning optimal grasps. In IEEE Int. Conf. on Robotics and Automation (ICRA), 1992. [8] J. Fontanals, B.A. Dang-Vu, O. Porges, J. Rosell, and M. Roa. Integrated grasp and motion planning using independent contact regions. 14th IEEE-RAS Int. Conf. on Humanoid Robots (Humanoids), 2014.

[9] C. Goldfeder, P.K. Allen, C. Lackner, and R. Pelossof. Grasp planning via decomposition trees. In IEEE Int. Conf. on Robotics and Automation (ICRA), pages 4679­4684, 2007. [10] K. Hsiao, S. Chitta, M. T. Ciocarlie, and E. G. Jones. Contact-reactive grasping of objects with partial shape information. In IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), volume 99, page 124, 2010. [11] A. Huam´ an Quispe, B. Milville, MA. Guti´ errez, C. Erdogan, M. Stilman, HI. Christensen, and H. Ben Amor. Exploiting symmetries and extrusions for grasping household objects. IEEE Int. Conf. on Robotics and Automation (ICRA)(to appear), 2015. [12] K. Huebner and D. Kragic. Selection of robot pre-grasps using boxbased shape approximation. In IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2008. [13] Z. Li and S.S. Sastry. Task-oriented optimal grasping by multifingered robot hands. IEEE Journal of Robotics and Automation, 1988. [14] J. Loucks and J. A. Sommerville. The role of motor experience in understanding action function: The case of the precision grasp. Child development, 83(3):801­809, 2012. [15] A.T. Miller, S. Knoop, H.I. Christensen, and P.K. Allen. Automatic grasp planning using shape primitives. In IEEE Int. Conf. on Robotics and Automation,(ICRA), 2003. [16] A.K. Pandey, J-P Saut, D. Sidobre, and R. Alami. Towards planning human-robot interactive manipulation tasks: Task dependent and human

[17] [18] [19] [20] [21] [22] [23]

oriented autonomous selection of grasp and placement. In 4th IEEE RAS & EMBS Int. Conf. on Biomedical Robotics and Biomechatronics (BioRob), 2012. M. Pilu and R.B. Fisher. Equal-distance sampling of superellipse models. University of Edinburgh, Department of Artificial Intelligence, 1995. M. Przybylski, T. Asfour, and R. Dillmann. Unions of balls for shape approximation in robot grasping. In IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2010. M. Przybylski, T. Asfour, R. Dillmann, R. Gilster, and H. Deubel. Human-inspired selection of grasp hypotheses for execution on a humanoid robot. In 11th IEEE-RAS Int. Conf. on Humanoid Robots, 2011. D. A. Rosenbaum, C. M. van Heugten, and G. E. Caldwell. From cognition to biomechanics and back: The end-state comfort effect and the middle-is-faster effect. Acta psychologica, 94(1):59­85, 1996. A. Sahbani, S. El-Khoury, and P. Bidaud. An overview of 3d object grasp synthesis algorithms. Robotics and Autonomous Systems, 60(3):326­336, 2012. N. Vahrenkamp, T. Asfour, and R. Dillmann. Simultaneous grasp and motion planning: Humanoid robot armar-iii. Robotics & Automation Magazine, 2012. T. Yoshikawa. Manipulability of robotic mechanisms. The International Journal of Robotics Research, 4(2):3­9, 1985.

Physical humanârobot interaction tasks require robots that can detect and react to external perturbations caused by the human partner. In this contribution, we present a machine learning approach for detecting, estimating, and compensating for such external perturbations using only input from standard sensors. This machine learning approach makes use of Dynamic Mode Decomposition (DMD), a data processing technique developed in the field of fluid dynamics, which is applied to robotics for the first time. DMD is able to isolate the dynamics of a nonlinear system and is therefore well suited for separating noise from regular oscillations in sensor readings during cyclic robot movements. In a training phase, a DMD model for behavior-specific parameter configurations is learned. During task execution, the robot must estimate the external forces exerted by a human interaction partner. We compare the DMD-based approach to other interpolation schemes. A variant, sparsity promoting DMD, is particularly well suited for high-noise sensors. Results of a user study show that our DMD-based machine learning approach can be used to design physical humanârobot interaction techniques that not only result in robust robot behavior but also enjoy a high usability.BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

1

arXiv:1507.07882v1 [cs.CV] 27 Jul 2015

Occlusion-Aware Object Localization, Segmentation and Pose Estimation
Samarth Brahmbhatt
http://www.cc.gatech.edu/~sbrahmbh

Heni Ben Amor
http://henibenamor.weebly.com

School of Interactive Computing Georgia Institute of Technology Atlanta, GA USA

Henrik Christensen
http://www.cc.gatech.edu/~hic

Abstract We present a learning approach for localization and segmentation of objects in an image in a manner that is robust to partial occlusion. Our algorithm produces a bounding box around the full extent of the object and labels pixels in the interior that belong to the object. Like existing segmentation aware detection approaches, we learn an appearance model of the object and consider regions that do not fit this model as potential occlusions. However, in addition to the established use of pairwise potentials for encouraging local consistency, we use higher order potentials which capture information at the level of image segments. We also propose an efficient loss function that targets both localization and segmentation performance. Our algorithm achieves 13.52% segmentation error and 0.81 area under the false-positive per image vs. recall curve on average over the challenging CMU Kitchen Occlusion Dataset. This is a 42.44% decrease in segmentation error and a 16.13% increase in localization performance compared to the state-of-the-art. Finally, we show that the visibility labelling produced by our algorithm can make full 3D pose estimation from a single image robust to occlusion.

1

Introduction and Related Work

In this paper we address the problem of localizing and segmenting partially occluded objects. We do this by generating a bounding box around the full extent of the objects, while also segmenting the visible parts inside the box. This is different from semantic segmentation, which typically does not provide information about the spatial position of labelled pixels inside the object. While a lot of progress has been made in object detection [9, 13, 21], occlusion by other objects still remains a challenge. A common theme is to model occlusion geometrically or appearance-wise, thereby allowing it to contribute to the detection process. Wang et al. [19] use a holistic Histogram of Oriented Gradients (HOG) template [6] to scan through the image and use specially trained part templates for instances where some cells of the holistic template respond poorly. Girshick et al. [11] force the Deformable Part Model detector to place a trained `occluder' part in regions where the original parts respond weakly. The object masks produced by both of these algorithms are only accurate up to the parts and hence not usable for many applications e.g. edge-based 3D pose estimation. Xiang and Savarese [20]
c 2015. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.

2

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

approximate object structure in 3D using planar parts. A Conditional Random Field (CRF) is then used to reason about visibility of the parts when the 3D planes are projected to the image. However, such methods work well only for large objects that can be approximated with planar parts. Our approach is entitled Segmentation and Detection using Higher-Order Potentials (SDHOP). It is based on discriminatively learned HOG templates for objects and occlusion. Whereas the object templates model the objects of interest, the occlusion templates provide discriminative support and do not model a specific occluder. Segmentation is done by considering not only the response of patches to these templates, but also the segmentation of neighbouring patches through a CRF with higher-order connections that encompass image regions. We will compare our approach to two existing approaches that have been designed to handle partial occlusion. Hsiao and Hebert [14] approximate all occluders by boxes and generate occlusion hypotheses by finding locations of mismatch between image gradient and object model gradient. These hypotheses are then validated by the visibility of other points of the object and by an occlusion prior which assumes all objects rest on the same planar surface. Our algorithm does not need such assumptions which reduce the segmentation accuracy. Gao et al. [10] learn discriminative appearance models of the object and occlusion seen during training. Segmentation is achieved by defining a CRF to assign binary labels to patches based on their response to these two filters. We build on their work but add several important modifications that lead to better localization and segmentation performance. Firstly, we replace the edge-based pairwise terms with 4-connected pairwise terms that are better able to propagate visibility relations. Secondly, we introduce the use of higher-order potentials defined over groups of patches, allowing us to reason at the level of image segments which contain much more information than pairs of patches. We also introduce a new loss function for structured learning that targets both localization and segmentation performance but is still decomposable over the energy terms. Lastly, we introduce a simple procedure to convert the granular patch-level object mask produced by the algorithm to a fine pixel-level mask that can be used to make 3D pose estimation of detected objects robust to partial occlusion. Our algorithm outperforms these approaches (Hsiao and Hebert [14], Gao et al. [10]) at both object localization and segmentation on the CMU Kitchen Occlusion dataset as shown in Section 3. The rest of the paper is organized as follows. Section 2 describes our proposed approach. We present evaluations on standard datasets and our own laboratory dataset in Section 3 and summarize in Section 4.

2

Method

The training phase for SD-HOP requires a set of images with different occlusions of the object(s) of interest. Each training sample is (1) over-segmented and (2) annotated with a bounding box around the full extent of the object and a binary segmentation of the area inside the box into object vs. non-object pixels. Given these training images and labels, we train a structured Support Vector Machine (SVM) that produces the HOG templates and CRF weights. Figure 1 shows an overview of our approach. Object segmentation is done by assigning binary labels to all HOG cells within the bounding box, 1 for visible and 0 for occluded. Instead of making independent decisions for every cell, we allow neighbouring cells to influence each other. Neighbour influence can

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION
Training Object 1 Object 2 Feature Extraction Feature Vector Find Most Violated Constraint Segmentation Pyramid Testing Model Learning Solve Constrained QP Labels

3

HOG Feature Pyramid

Inference

Figure 1: Overview of our approach. Top: During training, images are segmented and features are extracted from pyramids of segmentations and HOG features. An SVM model is learned by max-margin learning. Bottom: After training, the model can be used to infer a bounding box and visible segments of the object. take two forms: (1) pairwise terms (Rother et al. [17]) that impose a cost for 4-connected neighbours to have different labels and (2) higher-order potentials (Kohli et al. [16]) that impose a cost for cells to have a different label than the dominant label in their segment of the image. These segments are produced separately by an unsupervised segmentation algorithm.

2.1

Notation

The label for an object in an image x is represented as y = (p, v, a), where p is the bounding box, v is a vector of binary variables indicating the visibility of HOG cells within p and a  [1, A] indexes the discrete viewpoint. p = ( px , py , p ) indicates the position of the top left corner and the level in a scale-space pyramid. The width and height of the box are fixed per viewpoint as wa and ha HOG cells respectively. Hence v has wa · ha elements. All training images are also over-segmented to collect statistics for higher-order potentials. Any unsupervised algorithm can be used for this, e.g. Felzenszwalb and Huttenlocher [8] and Arbelaez et al. [2].

2.2

Feature Extraction

Given an image x and a labelling y, a sparse joint feature vector (x, y) is formed by stacking A vectors. Each of these vectors has features for a different discretized viewpoint. All vectors except for the one corresponding to viewpoint a are zeroed out. Below, we describe the components of this vector. 1. 31-dimensional HOG features are extracted for all cells of 8x8 pixels in p as described in Felzenszwalb et al. [9]. The feature vector is is constructed by stacking two groups which are formed by zeroing out different parts, similarly to Vedaldi and Zisserman [18]. The visible group v (x, p) has the HOG features zeroed out for cells labelled 0 and the occlusion group nv (x, p) has them zeroed out for cells labelled 1. 2. Complemented visibility labels, to learn a prior for a cell to be labelled 0: [1wh - v].

4

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

3. Count c(p) of cells in bounding box p lying outside the image boundaries, to learn a cost for truncation by the image boundary, similarly to Vedaldi and Zisserman [18]. 4. Number of 4-connected neighbouring cells in the bounding box that have different labels, to learn a pairwise cost. 5. Each segment in the bounding box obtained from unsupervised segmentation defines a clique of cells. To learn higher-order potentials, we need a vector HOP that captures the distribution of 0/1 label agreement within cliques. A vector c  RK +1 is constructed for each clique c as (c )k = 1 if ic vi = k. The sum of all c within p gives HOP . In practice, since cliques do not have the same size we employ the normalization strategy described in Gould [12] and transform statistics of all cliques to a standard clique size K (K = 4 in our experiments). 6. The constant 1, used to learn a bias term for different viewpoints.

2.3

Learning

Suppose w is a vector of weights for elements of the joint feature vector. We define wT (x, y) as the `energy' of the labelling y. The aim of learning is to find w such that the energy of the correct label is minimum. Hence we define the label predicted by the algorithm as f (x; w) = y = argmin wT (x, y)
y

(1)

We use a labelled dataset (xi , yi )N i=1 and learn w by solving the following constrained Quadratic Program (QP) N 1 min w 2 + C  i (2) w, 2 i=1 s.t. ^ i ) - (xi , yi )) + i  (yi , y ^ i ) i, y ^  Yi wT ((xi , y i  0 i D2 w  0 Intuitively this formulation requires that the score wT (xi , yi ) of any ground truth labelled ^ i ) of any other labelling y ^ i by the distance image xi must be smaller than the score wT (xi , y ^ i ) minus the slack variable i , where w 2 and i are between the two labellings (yi , y minimized. The regularization constant C adjusts the importance of minimizing the slack variables. The above formulation has exponential constraints for each training image. For tractability, training is performed by using the cutting plane training algorithm of Joachims et al. [15] which maintains a working set Yi of most violated constraints (MVCs) for each image. Gould [12] adapts this algorithm for training higher-order potentials. It uses D2 as a second order curvature constraint on the K + 1 weights for the higher-order potentials, which forces them to make a concave lower envelope. This encourages most cells in the image segments to agree in visibility labelling. D2 is an appropriately 0-padded (to the left and right) version of   -1 2 1 0 ...  .  . . .. .  . . . . . . . 0 . . . -1 2 -1

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

5

^ is called the loss function. It depends on the amount The distance between two labels y and y of overlap between the two bounding boxes and the Hamming distance between the visibility labellings ^) ^) area(p p area(p p ^) = 1 - ^) + · H (v, v (3) (y, y ^) ^) area(p p area(p p ^ ) between two labellings v and v ^ (potentially having difThe mean Hamming distance H (v, v ferent sizes as they might belong to different viewpoints) is calculated after projecting them to the lowest level of the pyramid. By construction of the loss function, the difference in segmentation starts contributing to the loss only after the two bounding boxes start overlapping each other. It also has the nice property of decomposing over the energy terms, as described in Section 2.4.1.

2.4

Inference

To perform the inference as described in Eq. 1 we have to search through Y = A × P × V where A is the set of viewpoints, P is the set of all pyramid locations and V is the exponential set of all combinations of visibility variables. We enumerate over A and P and use an s - t mincut to search over V at every location. By construction, the feature vector w can be decomposed into weight vectors for the different viewpoints i.e. w = [w1 , w2 , . . . , wA ]. In the following description, we will consider one viewpoint and omit the superscript for brevity of notation. w can also be decomposed as [wv , wnv , w pr , wtrunc , W , wHOP , wbias ] into the six components described in Section 2.2. We define the following terms that are used to construct the graph shown in Figure 2(b). i (x, p) are the vectorized HOG features extracted at cell i in bounding box p. Unary terms Fi (p) = wv,i T i (x, p) and Bi (p) = wnv,i T i (x, p) are the responses at cell i for object and occlusion filters respectively. Ri = w pr,i is the prior for cell i to be labelled 0. Constant term C(y) = wtrunc · c(p) + wbias is the sum of image boundary truncation cost and bias. E is the set of 4-connected neighbouring cells in p and W is the pairwise weight. C (p) is the set of all cliques in p and c (vc ) is the higher-order potential for clique c having nodes with visibility labels vc . Combining these terms, the energy for a particular labelling is formulated as
wh

E (x, y) = wT (x, y) =  Fi (p)vi + Bi (p)(1 - vi ) + Ri (1 - vi )
i=1

(4) W |vi - v j | +
cC (p)

+

(i, j)E





c (vc ) + C(y)

c (vc ), the higher-order potential for clique c is defined as mink=1...K (sk ic vi + bk ), following Gould [12]. Intuitively, it is the lower envelope of a set of lines whose slope is defined as sk = M K ((wHOP )k - (wHOP )k-1 ) and intercept as bk = (wHOP )k - sk k (recall that wHOP is a K + 1 dimensional weight vector). M is the size of the clique. The normalization in sk makes the potential invariant to the size of the clique (refer to Gould [12] for details). Figure 2(a) shows a sample higher-order potential curve for a clique of K cells. Given an image, a location, and a viewpoint we use s - t mincut on the graph construction shown in Figure 2(b) to find the labelling v that minimizes the energy in Eq. 4. Each variable vi , i  {1, 2, . . . , wh} defines a node and each clique has K - 1 auxiliary nodes in the graph, z1 . . . zK -1 . For a detailed derivation of this graph structure please see Boykov and Kolmogorov [3] and Gould [12]. After the maxflow algorithm finishes, the nodes vi still connected to s are labelled 1 and others are labelled 0.

6

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

(a)

(b)

Figure 2: (a): Concave higher-order potentials encouraging cells in a clique to have the same binary label, (b): Construction of graph to compute the energy minimizing binary labelling of cells by s - t mincut. Algorithm 1 Response-transfer between object detectors in overlapping regions for all o  [1, L] do {L is the number of objects,  denotes the Hadamard product} for all p  P do B(p) = C(p)  1[V (p) = 0] {Transfer equation for all cells in p} end for y = argminy wT (x, y) C(y (p)) = F (y (p))  y (v) {Update equations for all cells in p} V (y (p)) = o · y (v) end for

2.4.1

Loss-augmented Inference

Loss-augmented inference is an important part of the cutting plane training algorithm (`separation oracle' in Joachims et al. [15]) and is used to find the most violated constraints. It T ^ ) - (y, y ^ ), where y is the ground-truth labelling. is defined as yMVC = argminy ^ w (x, y Our formulation of the loss function makes it solvable with the same complexity as normal inference (Eq. 1) by decomposing the loss over the terms in Eq 4. The first term of Eq. 3 is added to C(y), while the second term is distributed across Fi (p) and Bi (p) in Eq. 4.

2.5

Detection of Multiple Objects

Multiple objects of interest might overlap. Running the individual object detectors separately leaves regions of ambiguity in overlapping areas if multiple detectors mark the same location as visible. We find that running one iteration of  -expansion (see Boykov et al. [4]) in overlapping areas resolves ambiguities coherently. The detectors are run sequentially. We maintain a label map V that stores for each cell the label of the object that last marked it visible, and a collected response map C that stores for each cell the object filter response (Fi (p)) from the object that last marked it visible. While running the location search for object o, we transfer object filter responses from C to the occlusion filter response map (B(p)) for the current object as described in Algorithm 1. This is effectively one iteration

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

7

of  -expansion (see supplementary material for details). It causes decisions in overlapping regions to be made between responses of well-defined object filters rather than between responses of an object filter and a generic occlusion filter. Such response-transfer requires the object models to be compatible with each other. We achieve this by training the object models together as if they were different viewpoint components of the same object. The bias term in the feature vector makes the filter responses of different components comparable.

2.6

3D Pose Estimation

The basic principle of many model based 3D pose estimation algorithms is to fit a given 3D model of the object to its corresponding edges in the image e.g. in Choi and Christensen [5], the 3D CAD model is projected into the image and correspondences between the projected model edges and image edges are set up. The pose is estimated by solving an Iterative Re-weighted Least Squares (IRLS) problem. However, partial occlusion causes these approaches to fail by introducing new edges. We make the algorithm robust to partial occlusion by first identifying visible pixels of the object using SD-HOP and discarding correspondences outside the visibility mask. We call our extension of the algorithm Occlusion Reasoning-IRLS (OR-IRLS).

3

Evaluation

We implemented SD-HOP in Matlab, with MVC search and inference implemented in CUDA since they are massively parallel problems. Inference on a 640x480 image with 11 scales takes 3s for a single object with a single viewpoint on our 3.4 GHz CPU and NVIDIA GT730 GPU.

3.1

Localization and Segmentation

We evaluated our approach on the CMU Kitchen Occlusion Dataset from Hsiao and Hebert [14]. This dataset was chosen because (1) it provides extensive labelled training data in the form of images with bounding boxes and object masks, and (2) the dataset is challenging and offers the opportunity to compare against an algorithm designed specifically to handle occlusion. For the localization task we generated false positives per image (FPPI) vs. recall curves, while for the segmentation task we measured the mean segmentation error against ground truth as defined by the Pascal VOC segmentation challenge in Everingham et al. [7]. C = 25 (see eq. 2) was chosen by 5-fold cross-validation. While both results are presented for the single pose part of the dataset, multiple poses are easily handled in our algorithm as different components of the feature vector. Figure 3 shows FPPI vs. recall curves compared with those reported by the rLINE2d+OCLP algorithm of Hsiao and Hebert [14] and those generated from our implementation of Gao et al. [10]. Table 1 presents segmentation errors compared with Gao et al. [10]. Hsiao and Hebert [14] do not report a segmentation of the object. Figure 3 shows that while both SD-HOP and Gao et al. [10] have similar recall at 1.0 FPPI, SD-HOP consistently preforms better in terms of area under the curve (AUC). Averaged over the 8 objects, SD-HOP achieves 16.13% more AUC than Gao et al. [10]. Table 1

8
Bakingpan 1 0.8

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION
Colander 1 1 Thermos 1 Pitcher

0.8

0.8

0.8

Recall

Recall

Recall

0.4

0.4

0.4

Recall rLINE2D+OCLP SD-HOP Gao et al. 0 0.2 0.4 FPPI 0.6 0.8 1

0.6

0.6

0.6

0.6

0.4

0.2 0 0 0.2

rLINE2D+OCLP SD-HOP Gao et al. 0.4 FPPI 0.6 0.8 1

0.2

rLINE2D+OCLP SD-HOP Gao et al. 0 0.2 0.4 FPPI 0.6 0.8 1

0.2

0.2

rLINE2D+OCLP SD-HOP Gao et al. 0 0.2 0.4 FPPI 0.6 0.8 1

0

0

0

Saucepan 1 0.8 1 0.8

Cup 1

Scissors 1

Shaker

0.8

0.8

Recall

Recall

Recall

0.4

0.4

0.4

Recall rLINE2D+OCLP SD-HOP Gao et al. 0 0.2 0.4 FPPI 0.6 0.8 1

0.6

0.6

0.6

0.6

0.4

0.2 0 0 0.2

rLINE2D+OCLP SD-HOP Gao et al. 0.4 FPPI 0.6 0.8 1

0.2 0 0 0.2

rLINE2D+OCLP SD-HOP Gao et al. 0.4 FPPI 0.6 0.8 1

0.2

0.2

rLINE2D+OCLP SD-HOP Gao et al. 0 0.2 0.4 FPPI 0.6 0.8 1

0

0

Figure 3: Object localization results on the CMU Kitchen Occlusion dataset Table 1: Mean object segmentation error Object Gao et al. [10] SD-HOP Bakingpan Colander Cup Pitcher Saucepan Scissors Shaker Thermos 0.2904 0.2095 0.2144 0.2499 0.1956 0.2391 0.2654 0.2271 0.1516 0.1249 0.1430 0.1131 0.1103 0.1649 0.1453 0.1285

Table 2: Mean 3D pose estimation error Pose parameter IRLS OR-IRLS X (cm) Y (cm) Z (cm) Roll (degrees) Pitch (degrees) Yaw (degrees) 1.6874 1.4953 8.228 1.1711 7.9100 5.7712 0.5774 0.6516 2.1506 0.7152 2.3191 2.6055

shows that SD-HOP consistently outperforms Gao et al. [10] in terms of segmentation error, achieving 42.44% less segmentation error averaged over the 8 objects. Figure 5 shows examples of the algorithm's output on various images from the CMU Kitchen Occlusion dataset.

3.2

Ablation Study

We conducted an ablation study on the `pitcher' object of the CMU Kitchen Occlusion dataset to determine the individual effect of our contributions. Using the loss function from Gao et al. [10] caused the segmentation error to increase from 0.1131 to 0.1547 and area under curve (AUC) of FPPI vs. recall to drop from 0.7877 to 0.7071. To discern the effect of 4-connected pairwise terms we removed the higher order terms from the model too. Using the pairwise terms as described in Gao et al. [10] caused the segmentation error to increase from 0.1547 to 0.2499 and AUC to decrease from 0.7071 to 0.6414. Lastly, to quantify the effect of higher order potentials, we compared the full SD-HOP model against one with higher order potentials removed. Removing higher order potentials

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

9

Figure 4: 3D pose estimation. Left to right: Pose estimation with IRLS, SD-HOP raw segmentation mask, SD-HOP refined segmentation mask, Pose estimation with OR-IRLS. Best viewed in colour. caused the segmentation error to increase from 0.1131 to 0.1430 and AUC to drop from 0.7877 to 0.7544. We hypothesize that for small objects like the ones in the CMU Kitchen Occlusion dataset, 4-connected pairwise terms are almost as informative as higher order terms. To check this hypothesis we tested the effect of removing higher order potentials on a close-up dataset of 41 images of a pasta-box occluded by various amounts through various household objects. Removing the higher order potentials caused the segmentation error to increase from 0.1308 to 0.1516 and area under curve AUC to drop from 0.9546 to 0.9008. This indicates that higher order terms are more useful for objects with larger and hence more informative segments.

3.3

3D Pose Estimation

We collected 3D pose estimation results produced by IRLS and OR-IRLS on a dataset which has 17 images of a car-door in an indoor environment. The ground truth pose for the cardoor was obtained by an ALVAR marker alv [1]. Table 2 shows the mean errors in the six pose parameters. To discern the effect of errors inherent in the pose estimation process from the effect of occlusion reasoning, the pose of the cardoor was constant throughout the dataset, with various partial occlusions being introduced. The granular HOG cell-level mask produced by SD-HOP caused some important silhouette edges to be missed for pose estimation. To solve this problem we utilized the unsupervised segmentation done earlier for defining higher order terms. If more than 80% of the area within a segment was marked 1, we marked the whole segment with 1. Since segments follow object boundaries, this produced much cleaner masks for pose estimation. Figure 4 shows the masks and pose estimation results for an example image from the dataset, with more such examples presented in the supplementary material. Note that the segmentation errors mentioned in Table 1 use the raw masks.

4

Conclusion

We presented an algorithm (SD-HOP) that localizes partially occluded objects robustly and segments their visible regions accurately. In contrast to previous approaches that model occlusion, our algorithm uses higher order potentials to reason at the level of image segments and employs a loss function that targets both localization and segmentation performance. We demonstrated that our algorithm outperforms existing approaches on both tasks, when evaluated on a challenging dataset. Finally, we have shown that the segmentation output from

10

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

Figure 5: Object localization and segmentation results on the CMU Kitchen Occlusion dataset. Left: Image, Center: Raw mask from SD-HOP, Right: Refined mask from SD-HOP SD-HOP can be used to improve pose estimation performance in the presence of occlusion. Avenues of future research include (1) training from weakly labelled data i.e. without segmentations, (2) a post-training algorithm to make object models comparable without having to train them together, and (3) using the occlusion information to reason about interactions between objects in scene understanding applications. We would like to acknowledge Ana Huamán Quispe's help with implementing this system on a bimanual robot. The system was used to enable the robot to pick up partially visible objects lying on a table.

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

11

References
[1] ALVAR tracking library. http://virtual.vtt.fi/virtual/proj2/ multimedia/alvar/index.html. Acessed: 2015-05-03. [2] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. Contour detection and hierarchical image segmentation. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 33(5):898­916, 2011. URL http://ieeexplore.ieee. org/xpl/login.jsp?tp=&arnumber=5557884. [3] Yuri Boykov and Vladimir Kolmogorov. An experimental comparison of mincut/max-flow algorithms for energy minimization in vision. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 26(9):1124­1137, 2004. URL http: //ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1316848. [4] Yuri Boykov, Olga Veksler, and Ramin Zabih. Fast approximate energy minimization via graph cuts. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 23 (11):1222­1239, 2001. URL http://ieeexplore.ieee.org/xpl/login. jsp?tp=&arnumber=969114. [5] Changhyun Choi and Henrik I Christensen. Robust 3D visual tracking using particle filtering on the special Euclidean group: A combined approach of keypoint and edge features. The International Journal of Robotics Research, 31(4):498­519, 2012. URL http://ijr.sagepub.com/content/early/2012/03/01/ 0278364912437213. [6] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886­893. IEEE, 2005. URL http: //ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=1467360. [7] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The Pascal Visual Object Classes (VOC) Challenge. International Journal of Computer Vision, 88 (2):303­338, June 2010. URL http://link.springer.com/article/10. 1007%2Fs11263-009-0275-4. [8] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficient graph-based image segmentation. International Journal of Computer Vision, 59(2):167­ 181, 2004. URL http://link.springer.com/article/10.1023%2FB% 3AVISI.0000022288.19776.77. [9] Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object detection with discriminatively trained part-based models. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(9):1627­1645, 2010. URL http: //ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5255236. [10] Tianshi Gao, Benjamin Packer, and Daphne Koller. A segmentation-aware object detection model with occlusion handling. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1361­1368. IEEE, 2011. URL http: //ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5995623.

12

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

[11] Ross B Girshick, Pedro F Felzenszwalb, and David A Mcallester. Object detection with grammar models. In Advances in Neural Information Processing Systems, pages 442­ 450, 2011. URL http://citeseerx.ist.psu.edu/viewdoc/summary? doi=10.1.1.231.2429. [12] Stephen Gould. Max-margin learning for lower linear envelope potentials in binary markov random fields. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 193­200, 2011. URL http://ieeexplore.ieee. org/xpl/articleDetails.jsp?arnumber=6945904. [13] Stefan Hinterstoisser, Cedric Cagniart, Slobodan Ilic, Peter Sturm, Nassir Navab, Pascal Fua, and Vincent Lepetit. Gradient response maps for real-time detection of textureless objects. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34 (5):876­888, 2012. URL http://ieeexplore.ieee.org/xpls/abs_all. jsp?arnumber=6042881. [14] Edward Hsiao and Martial Hebert. Occlusion reasoning for object detection under arbitrary viewpoint. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 3146­3153. IEEE, 2012. URL http://ieeexplore.ieee. org/xpl/login.jsp?tp=&arnumber=6248048. [15] Thorsten Joachims, Thomas Finley, and Chun-Nam John Yu. Cutting-plane training of structural SVMs. Machine Learning, 77(1):27­59, 2009. URL http://link. springer.com/article/10.1007%2Fs10994-009-5108-8. [16] Pushmeet Kohli, Philip HS Torr, et al. Robust higher order potentials for enforcing label consistency. International Journal of Computer Vision, 82(3):302­ 324, 2009. URL http://ieeexplore.ieee.org/xpl/login.jsp?tp= &arnumber=4587417. [17] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. Grabcut: Interactive foreground extraction using iterated graph cuts. In ACM Transactions on Graphics (TOG), volume 23, pages 309­314. ACM, 2004. URL http://dl.acm.org/ citation.cfm?id=1015720. [18] Andrea Vedaldi and Andrew Zisserman. Structured output regression for detection with partial truncation. In Advances in neural information processing systems, pages 1928­1936, 2009. [19] Xiaoyu Wang, Tony X Han, and Shuicheng Yan. An HOG-LBP human detector with partial occlusion handling. In Computer Vision, 2009 IEEE 12th International Conference on, pages 32­39. IEEE, 2009. URL http://ieeexplore.ieee.org/ xpl/login.jsp?tp=&arnumber=5459207. [20] Yu Xiang and Silvio Savarese. Object Detection by 3D Aspectlets and Occlusion Reasoning. In Computer Vision Workshops (ICCVW), 2013 IEEE International Conference on, pages 530­537. IEEE, 2013. URL http://ieeexplore.ieee.org/xpl/ articleDetails.jsp?reload=true&arnumber=6755942. [21] Menglong Zhu, Konstantinos G Derpanis, Yinfei Yang, Samarth Brahmbhatt, Mabel Zhang, Cody Phillips, Matthieu Lecce, and Kostas Daniilidis. Single image 3D object detection and pose estimation for grasping. In Robotics and

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

13

Automation (ICRA), 2014 IEEE International Conference on, pages 3936­3943. IEEE, 2014. URL http://www.cis.upenn.edu/~menglong/papers/ icra2014_object_grasping.pdf.

Downloaded from orbit.dtu.dk on: Dec 21, 2016

Measuring and Modelling Delays in Robot Manipulators for Temporally Precise Control using Machine Learning.

Andersen, Thomas Timm; Amor, Heni Ben; Andersen, Nils Axel; Ravn, Ole Published in: Proceedings of IEEE ICMLA'15

Publication date: 2015 Document Version Publisher's PDF, also known as Version of record Link to publication

Citation (APA): Andersen, T. T., Amor, H. B., Andersen, N. A., & Ravn, O. (2015). Measuring and Modelling Delays in Robot Manipulators for Temporally Precise Control using Machine Learning. In Proceedings of IEEE ICMLA'15. IEEE.

General rights Copyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright owners and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights. · Users may download and print one copy of any publication from the public portal for the purpose of private study or research. · You may not further distribute the material or use it for any profit-making activity or commercial gain · You may freely distribute the URL identifying the publication in the public portal ? If you believe that this document breaches copyright please contact us providing details, and we will remove access to the work immediately and investigate your claim.

Measuring and Modelling Delays in Robot Manipulators for Temporally Precise Control using Machine Learning
Thomas Timm Andersen , Heni Ben Amor , Nils Axel Andersen and Ole Ravn  Department of Automation and Control, DTU Electrical Engineering Technical University of Denmark, DK-2800 Kgs. Lyngby, Denmark. {ttan, naa, or}@elektro.dtu.dk  Institute for Robotics and Intelligent Machines, College of Computing Georgia Tech, Atlanta, GA 30332, USA. hbenamor@cc.gatech.edu

Abstract--Latencies and delays play an important role in temporally precise robot control. During dynamic tasks in particular, a robot has to account for inherent delays to reach manipulated objects in time. The different types of occurring delays are typically convoluted and thereby hard to measure and separate. In this paper, we present a data-driven methodology for separating and modelling inherent delays during robot control. We show how both actuation and response delays can be modelled using modern machine learning methods. The resulting models can be used to predict the delays as well as the uncertainty of the prediction. Experiments on two widely used robot platforms show significant actuation and response delays in standard control loops. Predictive models can, therefore, be used to reason about expected delays and improve temporal accuracy during control. The approach can easily be used on different robot platforms. Keywords-Robot control; Automation; Machine learning algorithms;
Figure 1. Temporally precise control of an industrial robot is realized by modelling the inherent delay in the system. The picture depicts a fast robot movement during data acquisition. Recorded data is processed using machine learning algorithms to generate predictive models for system and response delay.

I. I NTRODUCTION For robots to engage in complex physical interactions with their environment, efficient and precise action generation and execution methods are needed. Manipulation of small objects such as screws and bolts, for example, requires spatially precise movements. However, in dynamically changing environments, spatial precision alone is often insufficient to achieve the goals of the task. In order to intercept a rolling ball on the table, for instance, a robot has to perform temporally precise control--the right action command has to be executed at the right time. Yet, by their very nature, actuation commands are never instantaneously executed. Delays and latencies, therefore, play an important role in temporally precise control and can occur at different locations in the robot control loop. Actuation delay is the delay type that most roboticists are aware of. When an action command is sent to the robot's controller, it takes a short while to process the command and calculate the required joint motor input. Imagine a welding robot with an uncompensated actuation delay of 50 ms, working an object on a fairly slow-moving conveyor belt with a speed of 0.5 m/s. The incurred delay would result in a tracking error of 2.5 cm, which could easily destroy a product, or at the very least result in a suboptimal result.

A different type of delay is the response delay which measures the amount of time until a real-world event is sensed, processed and updated in memory. Response delay is usually assumed zero, as one would naturally assume that this is sampled and transmitted instantaneously whenever a motion occurs. However, since there is a sampling clock and since the controller also needs some time to pack the data for transmission, the response delay can be a non-negligible amount of time. An important implication of the response delay is the discrepancy between the robot's belief of its own state and the true value of that state. When data is received from the robot, indicating that the robot is at a certain position moving with some velocity, the data is in reality describing a state in the past. In order to effectively act in dynamic environments and reason about timing, a robot has to be aware of both the actuation delay as well as the response delay. Sadly, such information is not readily accessible form the robotics company, and no method is currently available for identifying it. This has lead many researches to develop their own

controllers, but this is rarely an opportunity for industrial users. Safety during operation is the most crucial issue for robot controllers, but each robotic company may has different strategies which affect the architecture of the robot controller. It is therefore necessary to consider the controller as a black box from which we must learn the controllerdependent delay characteristics. Direct measurement of these delays is typically difficult, since the different delay types are convoluted and hard to separate. An important challenge is therefore the question of how to separate these two delays as, depending on the executed task, a robot has to compensate for a different type of delay. In this paper, we present a methodology for measuring and modelling the inherent delays during robot control. We introduce an experimental setup which allows us to collect evidence for both the actuation delay, as well as the response delay. The collected data is then used to learn controllerdependent predictive models of each type of delay. The learned predictive models can be used by a robot to reason about timing and perform temporally precise control. The contributions of this publication are three-fold. First, we provide a generic method for measuring the actuation and response delay of a robot manipulator. Due to its datadriven nature, the method can be used on a variety of actuators. Second, we show how existing machine learning methods can be used to model and predict the inherent delay. Finally, we show modelling results for two widely used robot platforms, namely the Kuka KR 5 Sixx and the Universal Robots' UR10 robot. The acquired data is made publicly available to the robotics community [1]. II. R ELATED WORK Modelling time delays is a vital research topic in computer network engineering. In order to ensure fast communication over large computer networks, various models have been put forward to model the mean delay experienced by a message while it is moving through a network [15]. These analytic models typically require the introduction of assumptions, e.g. Kleinrock's independence assumption [11], to make them tractable. Yet, since the network communication is based on a limited number of communication protocols, it is reasonable to use and constantly refine such analytic approaches. Another domain in which latencies and delays play a vital role is virtual reality (VR). As noted in [6], latencies lead to a sensory mismatch between ocular and vestibular information, can reduce the subjective sense of presence, and most importantly, can change the pattern of behavior such that users make more errors during speeded reaching, grasping, or object tracking. In VR applications, measuring and modelling delays can be very challenging, since the delay can heavily vary based on the involved software components, e.g., rendering engine, as well as highly heterogeneous hardware components, e.g., data gloves, wands, tracking

devices etc. In [6] a methodology for estimating delays is presented, which focuses on VR application domains. In robotics, the delay inherent to control loops can have a detrimental impact on system performance. This is particularly true for sensor-based control used in autonomous robots. Visual servoing of a robot, for example, can be sensitive to the delays introduced through image acquisition and processing [9]. Similarly, delays in proprioception can produce instabilities during dynamic motion generation. In [2], a dynamically smooth controller has been proposed that can deal with delay in proprioceptive readings. However, the approach assumes constant and known time-delay. A major milestone in robot control with time-delay was the ROTEX experiment [8]. Here, extended Kalman filters and graphical representation were used to estimate the state of objects in space, thereby enabling sensor-based long-range teleoperation. How to effectively deal with such communication delays has been a central research question in robotic tele-operation. Delays in robot control loops are not limited to sensor measurements only. A prominent approach for dealing with actuation delays is the Smith Predictor [17]. The Smith Predictor assumes a model of the plant, e.g. robot system, and can become unstable in the presence of model inaccuracies. A different approach has been proposed in [3]. A neural network was first trained to predict the state of mobile robots based on positions, orientations, and the previously issued action commands. The decision making process was, then, based on predicted states instead of perceived states, e.g. sensor readings. The approach presented in our paper follows a similar line of thought. However, instead of predicting specific states of the robot, we are interested in predicting the delay occurring at different parts of the control loop. III. M ETHODOLOGY In this section, we describe a data-driven methodology for modelling delays in robotic manipulators. We show how to acquire evidence for different types of delays and how this information can be used in conjunction with machine learning methods to produce predictive models for control. A. Measuring the delay The purpose of the presented method is to establish the actuation and response delay that a high-level control program can expect when issuing commands to a robotic controller. To measure these delays, we need to synchronize the issuing of commands with the control loop of the robot controller. To this end, we use the published current state of the robot, which most controllers send out in each control cycle. The overall system setup which will be used in the remainder of the paper is depicted in Figure 2 (left). A highlevel control program is running on a computer, which sends the commands to the robot control box. The control box,

Transmission Delay

Actuation Delay

Computer
Transmission Delay

Control Box
Response Delay

Robot

Gyro/IMU

Processing Delay

Figure 2. Left: Delays during the control of a robot manipulator. Transmission delay affects information flow between main control computer and the robot control box. Actuation delay and response delay are introduced in the communication between the control box and the physical robot. Right: For delay modelling an external sensor is mounted, e.g. a gyroscope, to measure discrepancies between command times and execution times.

in turn, calculates and issues the low-level commands that drive the robot. The delay between the high-level controller and the control box will be referred to as the transmission delay. The transmission delay has already been extensively studied in computer networking [15] and will thus not be treated in this paper. It is particularly crucial in tele-operation scenarios, in which the high-level controller and the robot control box may be separated by thousands of kilometers. In this paper, however, we focus on the delays incurred between the control box and the robot manipulator. A command that is received by the control box from the high-level program at time t = 0 is typically only executed after a delay of 1 . This is the actuation delay. Similarly, once a command is executed by the robot at time t = 1 , it takes another delay of 2 until the motion is reflected in the controllers memory and transmitted to the high-level program running on the central computer. This is the response delay. The fundamental idea of our approach is to compare time stamps at the moment a command is issued, the moment the command is executed, and the moment the command gets reflected in the published state of a robot. To this end, it is important to know the ground truth about the true timing of the robot movement. This is realized using an external apparatus in our setup, e.g., a gyroscope or accelerometer, see Figure 2 (right). 1) Determining ground truth: Since we want to measure the delay of the robot, we need a reliable and accurate method of measuring robot motion. The method needs to measure the current motion without adding a significant delay of its own. This can be achieved by imposing a significantly higher sampling rate than the robot controller. We use microelectromechanical (MEMS) gyroscopes, or angular rate sensors, for the revolving joints, and MEMS accelerometers for prismatic joints. They offer very high sampling rates of several orders of magnitude higher than many robot controllers publish (e.g. several kHz for affordable sensors), and practically no delay from motion to available measurement. Such sensors cannot readily be

used to infer where in the kinematical chain a motion has occurred, hence measurements have to be performed a single joint at a time. Gyroscope measurements often come with significant noise, while accelerometer measurements suffer from drift. However, both of these issues can be compensated for using simple offline filtering in-between measurement and training the model. 2) Acquiring measurements: As mentioned before, our approach is based on comparing time stamps throughout the robot control loop. To this end, we use the published state from the robot as the main sample clock and reference. An experimental trial starts at t = 0 upon reception of a first package from the controller. The system time stamp is recorded as soon as data is read, and the byte-encoded package is stored for later parsing to extract the current joint state. Upon reception, a command is sent instructing the robot to start moving a single joint, which we monitor with our angular rate sensor or accelerometer. The commanded movement consists of a rapid acceleration in one direction, followed by a fast deceleration before returning to return to the starting pose. The entire motion trial takes about a second, and all packages received until the robot stands still are stored. Sensor readings from the external sensor are stored by the central computer in order to identify the ground truth time stamp of the moment in which the robot moved. There are several perturbations that can lead to variations in the incurred delays, in particular physical perturbations. For instance, the force resulting from the gravitational pull on the robot varies with the joint configuration of the robot, just as the direction of motion effects whether the motor needs to work against or along gravity. The different size of motors and gearing in the robot also yields varying results. These perturbations lead to varying static and kinetic friction in the moving parts of a robot. This variation in turn leads to a varying actuation delay. As the magnitude of the static friction is usually larger than that of the kinetic friction, we assume that the delay is

mostly affected by the robot's joint configuration when the motion starts. We assume that the effect by the other joints during a motion after the static friction has been overcome can be neglected. A similar assumption of joint independence if often employed on the joint position controller when using Independent joint control [14]. To acquire a representative data set for modelling delays, we therefore need to map out the delay of each joint for all the different joint combinations, moving in both positive and negative direction. To capture variance in the delay, each combination of joint configuration and direction should be measured several times. 3) Filtering data and computing delays: When extracting the delays, we evaluate the difference between the recorded data. Before doing that, thought, we use a high order lowpass FIR filter (Figure 3) on the data from the angular rate sensor and correct for any drifting of the accelerometer, based on data recorded while the sensor was held stationary on the robot.
Frequency (Hz)
0 5
Speed unfiltered Speed filtered PSD Unfiltered PSD Filtered

B. Learning Predictive Models of Delay Next, we want to use the recorded data in order to learn predictive models of robot latencies. Once a predictive model is learned, it can be used by a robot to infer the most likely delay in a given situation. A common approach in robot control is to use a path planner running on the central computer to generate a starting joint configuration and an execution time of the trajectory. To find the actual real time that the robot will use to get to the goal state, we can query the learned predictive models for each moving joint. The individual delay is then added to the execution time of each joint to identify the real execution time. As input features for the model we use the starting joint configuration of the robot. As mentioned before, forces acting on the robot vary depending on the joint configuration and impact in particular the actuation delay. The output of the model is the expected delay. We learn individual models for the actuation delay and the response delay, since these two delays are unrelated. In line with the assumption of independence between the joints, a separate model is learned for each joint. Introducing the above structured approach, allows for accurate predictions of the delay. To evaluate how a unified model, predicting the delay of all joints, performs, such a model is also trained. The goal of learning is to generate predictive models that can generalize to new situations and lead to accurate predictions of the expected latencies. To this end, we use three different machine learning methods, namely neural networks (NN) [4], regression trees (RT) [5], and Gaussian processes (GPR) [12]. We use these methods as they can all effectively recover nonlinear relationships between input and output data. In our specific implementation, we used a feed-forward neural network with 30 neurons in a single hidden layer. Learning was performed using the Levenberg-Marquardt [10] algorithm. In contrast, the regression tree method hierarchically partitions the training data into a set of partitions each of which is modelled through a simple linear model. Both NNs and the RTs produce a single result and do not provide information about the uncertainty in the predicted value. In contrast to that, GPR can learn probabilistic, nonlinear mappings between two data sets. Due to the inherent noise and related phenomena, uncertainty handling is a crucial issue when dealing with delays. By providing the mean and the variance of any prediction, the GPR approach allows us to reason about uncertainty of our prediction. Together, mean and variance form Gaussian probability distribution indicating the expected range of predictions. This information can potentially be exploited to generate upper- and lower-bounds for the expected delays, which is in contrast to both NN and RT. As both NN, RT, and GPR are well known machine learning methods and we do not add anything to these

500

1000

1500

2000

2500

3000

3500 30 20

0 0 -10 -20 -30 -5 0 10 20 30 40 50 -40 60

Time (sec)

Figure 3. Gyroscope readings are filtered using a FIR filter. A 60 second datastream (green), recorded without moving the robot, is passed through the filter to remove noise (blue). The frequency component of the data before and after filtering is shown in red and black, calculated using Welch's Power Spectral Density (PSD) estimate [18]

To calculate the delay, we evaluate our two data series generated in each trial; the speed output from the robot controller and the filtered sensor data. The actuation delay is the difference between the moment a command is sent to the robot and the moment a sensor registers the motion, while the response delay is the difference between the moment a sensor registers motion and the moment it is reflected in the robot's current speed data. Both are calculated while taking into account the transmission delay from Figure 2 (left). Even when filtering out the noise, it can be challenging to establish the exact moment in time when the sensor determines that a motion has started as the measured speed is hardly ever zero. Instead we identify extrema of our recorded data to detect the time difference between the set target speed, the measured speed, and the reported current speed.

Magnitude (dB)

Speed (deg/sec)

10

Figure 4. The Universal Robot UR10 with mounted measuring equipment. The enclosure keeps the sensor at a stable temperature thus avoiding temperature-related drift in measurements.

methods, the theory behind them will not be covered further in this paper. IV. R ESULTS A. Experimental setup In our experiments, we model the performance of both a Kuka KR 5 sixx (Figure 1) and a Universal Robot UR10 (Figure 4). To generate the training data, we mounted a MPU6000 combined angular rate sensor and accelerometer to the end-effector. To avoid temperature-related drifts, we
100
Commanded speed Measured speed Reported speed Actuation delay Response Delay

moving 10 times in both positive and negative direction in 1,920 different joint configurations. A total of 33,500 trials were performed on each robot in order to generate a comprehensive dataset, to be released to the public [1]. For purposes of machine learning, only a subset of the data was later used. To be able to compare the performance of the two robots, we used the same 1,920 physical joint configuration (i.e. all links vertical) for both robots rather than using the same joint values. This is a necessity since the DenavitHartenberg parameters of the robots are not identical and the home position varies, thus positive joint rotation on one robot might lead to negative joint rotation on the other. Sampling only a subspace of the robots' total workspace does not introduce bias in the data, but rather limits the model to predict delays within that subspace. By sampling more poses, the model can routinely be extended to cover the entire workspace if needed. B. Delay output As explained in Section III-A3, delays are determined by evaluation of the extrema of the recorded motions. An example of how the delays vary for the two robots can be seen on Figure 6. The distribution of the actuation delays can be seen on Figure 7, while a boxplot showing the individual delays per joint is shown on Figure 8. The same plots for the reaction delays can be seen on Figure 9 and Figure 10, respectively. C. Model comparison

Speed (deg/sec)

50

0

-50

-100 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7

Time (sec)

Figure 5. Typical plot of logged data from a single trial. 33,500 trials were completed on each robot.

mount the sensor on the robot in an enclosure with low heat conductivity and then let the sensor warm up before measurements. Since these robots only have revolute joints, only the angular rate sensor is used, which outputs data at a rate of 8 kHz. To collect the data used for training the model, we perform a series of short trials, wherein the robot is commanded to perform a fast acceleration and deceleration motion. For controlling the Kuka robot, the Kuka RSI [7] protocol is used. It operates with a sample rate of 83.3 Hz (12 ms). As argued in [13], the UR10 is controlled using URScript SpeedJ commands. It operates with a sample rate of 125 Hz (8 ms). An example trajectory, along with typical outcome of a trial, can be seen on Figure 5. The plots clearly show a significant time difference in the commanded speed, the reported speed and the measured speed. To capture the variations of the delays, we performed trials on 4 different joints,

The extracted delays were used to train and validate models based on different machine learning algorithms, namely NN, RT, and GPR. For the NN and RT algorithms, we used the standard MatLab implementation, while we used GPstuff [16] for the GPR implementation. The starting joint configuration, the actuated joint, and the rotational direction were used as input. The delays that were measured at each input combination were used for training and testing, using k-fold cross validation with 10 folds to limit overfitting the data and to give an insight on how the model will generalize to an independent dataset. The mean squared error (MSE) from each fold were averaged together and used as a measure of how well the model predicts delays. Models for predicting both the delay of individual joints, as well as a combined model that can predict the delay of all joints were trained. The mean error of each model is derived by taking the square root of the MSE and is shown in Table I and II. The tables also shows the resulting mean error if the delay was assumed that of the median of the corresponding boxplots. This gives an indication of the performance of the trained models. Lower values indicate better generalization capabilities, while larger mean error values indicate poor prediction performance.

Joint 2 Joint 3 angle (deg) angle (deg)

-20 -50 -80 -110 65 25 -15 -55 0.12 0.10 0.08 0.06 0.03 0.02 0.01 0

Joint 2 Joint 3 angle (deg) angle (deg) Actuation Delay (s) Response Delay (s) Number of trials in bin
0.08 0.085 0.09 0.095 0.1 0.105

-70 -100 -130 -160 45 -5 -35 -75 0.065 0.045 0.025 0.005 0.03 0.02 0.01 0

Figure 6. Actuation and response delay for joint 3 moving in positive direction as a function of varying joint 2 and 3. The red graph is the mean and the gray area is ±2 standard deviations, corresponding to a 95% confidence interval. Note the different y axis interval. Left: Kuka. Right: Universal Robot.
6000 6000 5000 4000 3000 2000 1000 0 0 0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04

Number of trials in bin

Response Delay (s)

Actuation Delay (s)

5000 4000 3000 2000 1000 0 0.075

Delay (s)

Delay (s)

Figure 7.

Combined distribution of the actuation delay of all joints. Note the different x axis interval. Left: Kuka. Right: Universal Robot.
0.05

0.115 0.04 0.105

Delay (s)

Delay (s)

0.03

0.095

0.085

0.02

0.075 Joint 1 Joint 2 Joint 3 Joint 5 All joints

0.01 Joint 1 Joint 2 Joint 3 Joint 5 All joints

Figure 8.
6000

Boxplot of individual joint's actuation delay. Note the different y axis interval. Left: Kuka. Right: Universal Robot.

Number of trials in bin

Number of trials in bin
0 0.005 0.01 0.015 0.02 0.025 0.03 0.035

8000 6000 4000 2000 0 0 0.005 0.01 0.015 0.02 0.025 0.03 0.035

5000 4000 3000 2000 1000 0

Delay (s)

Delay (s)

Figure 9.

Combined distribution of the response delay of all joints. Left: Kuka. Right: Universal Robot.

0.03

0.03

Delay (s)

0.02

Delay (s)
Joint 1 Joint 2 Joint 3 Joint 5 All joints

0.02

0.01

0.01

0.00

0.00 Joint 1 Joint 2 Joint 3 Joint 5 All joints

Figure 10.

Boxplot of individual joint's response delay. Left: Kuka. Right: Universal Robot.

Table I M EAN ERROR IN MILLISECONDS OF MODEL FIT FOR ACTUATION DELAY. Kuka Median NN RT GPR UR Median NN RT GPR Joint 1 2.27 1.79 1.99 1.85 Joint 1 6.18 4.08 4.63 5.01 Joint 2 2.68 2.55 2.48 2.27 Joint 2 4.64 5.32 5.41 4.89 Joint 3 4.61 4.74 3.74 4.30 Joint 3 6.08 3.86 4.28 3.68 Joint 5 3.51 3.37 3.76 3.39 Joint 5 2.59 2.49 2.72 2.47 Combined 3.67 3.21 3.33 3.48 Combined 5.33 3.12 3.42 3.36 Average 3.35 3.13 3.06 3.06 Average 4.96 3.77 4.09 3.88

the delay and assuming the delay constant at the median of each boxplot would thus decrease the error to within 0.3 cm for a delay within ±6 ms. If we include the whiskers of the boxplot, corresponding to  ±2.7 or 99.3% of the data, the worst case error would within 0.85 cm for a delay within ±17 ms. Figure 8 also shows that on both robots, it is joint 2 that has the highest delay. This is the shoulder joint, and the one that lifts the most. This supports our theory that gravity influences the actuation delay. Figure 10 suggests that the response delay on the other hand is not varying between the joints. This is not surprising, as the response delay, as mentioned previously, is largely incurred by the sampling clock, packing of data and transmission. This most likely happens simultaneously for each joint. The seemingly correlation between actuation and response delay on Figure 6 is a consequence of the relatively low temporal resolution of the robot controller data. This is also why it is more dominant on the Kuka robot. As the sum of the actuation and response delay will always be a multiple of the sample period, an actuation delay a few ms below the mean at a specific pose will result in a response delay a few ms above the mean at that pose. A surprising finding on Figure 9 is that the response delay for the Kuka robot is more than one sample period, which suggests that sampling and transmission of data takes place in separate sample clock cycles. B. Evaluating the models' performance All of the models are able to predict the delays very accurately to within a mean error of 5ms and it is thus difficult to say anything conclusive about which model is best. Though all of the models would have a mean error less than 0.35 cm if used for a typical task like welding, which is an improvement of more than a factor 12 for the Kuka robot and almost a factor 3 for the Universal Robot, compared to using the controllers and not assuming any delay. Comparing the learned models with measuring the delay and assuming it to be static shows an improvement

Table II M EAN E RROR IN MILLISECONDS OF MODEL FIT FOR REACTION DELAY. Kuka Median NN RT GPR UR Median NN RT GPR Joint 1 2.13 1.70 1.86 1.63 Joint 1 4.82 4.11 4.66 3.88 Joint 2 2.37 2.32 2.21 2.17 Joint 2 2.00 2.48 2.44 2.44 Joint 3 4.30 4.68 3.62 4.23 Joint 3 4.08 4.24 4.47 3.75 Joint 5 3.09 2.22 2.31 3.11 Joint 4 4.90 5.22 5.84 5.20 Combined 3.33 2.36 2.37 2.63 Combined 5.09 4.84 5.33 5.05 Average 3.05 2.65 2.47 2.75 Average 4.18 4.01 4.35 3.82

V. D ISCUSSION A. Evaluating the two robots' delays As it can be seen on Figure 7, the actuation delay of the Kuka is significantly higher than on the Universal Robot, even factoring in the higher sample period; the average delay for the Kuka is 7.5 sample periods vs. 2.5 sample periods for the UR. If we relate the figure to the example from the introduction, where a welding robot need to weld an object on a conveyor belt moving at 0.5 m/s, our claim that it is important to compensate for the delay is clearly justified. The Kuka robot would, without compensation, make a welding seam displaced 4.5cm ± 0.5cm from the target, while the Universal Robot would miss with 0.75cm - 1.25cm. A deeper look into the actuation delays, which is on Figure 8, shows that the delays in general only vary with a few ms for each joint. Using our method for measuring

between 6 and 24%. The response delay for the Universal Robot shows the least benefit from modeling. This is most likely due to the fact that the spread of the delays are so small. The missing improvement with machine learning is thus a result of the median delay yield a very good guess, and not a result of the models being poor at learning those delays. It is worth noticing that the mean error in some cases are significantly higher for the Universal Robot models than those of the Kuka robot. This correspond with Figure 6, where the confidence interval is much broader for the Universal Robot than for the Kuka. It should also be noted that GPR does not only supply a prediction of the delay, but also outputs a measure of uncertainty, which is not reflected in the tables. For the Universal Robot's large variance, this is certainly an added bonus. VI. C ONCLUSION In this paper we presented a methodology for measuring and separating actuation and response delays in robot control loops. In addition, we introduced a data-driven approach for modelling inherent delays using machine learning algorithms. We showed that the introduced models can be efficiently used to predict occurring delays during temporally precise control. Real world experiments were used to identify latencies in two widely used robot platforms. The measured delay showed a large potential for improving temporal precision, with more than a factor 12 improvement for one of the robots. All the employed machine learning algorithms showed similar abilities to further improve the accuracy, with no algorithm showing significantly better accuracy than the others. Still, Gaussian processes seem to be better suited for this task, since they provide a probability distribution over the expected delay. In turn, such a distribution can be used to reason about upper- and lower-bounds in temporal precision. In our future work we will investigate how inverse models of time delay can be learned. Given a specific time constraint during a control task, an inverse model can be queried for the most appropriate action which will meet the goals of the task while ensuring time constraints. R EFERENCES
[1] http://aut.elektro.dtu.dk/staff/ttan/delay.html. [2] S. Bahrami and M. Namvar. Motion tracking in robotic manipulators in presence of delay in measurements. In Robotics and Automation (ICRA), 2010 IEEE International Conference on, pages 3884­3889, May 2010.

[3] S. Behnke, A. Egorova, A. Gloye, R. Rojas, and M. Simon. Predicting away robot control latency. In RoboCup 2003: Robot Soccer World Cup VII, Lecture Notes in Computer Science, pages 712­719. Springer Berlin Heidelberg, 2004. [4] C. M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, Inc., New York, NY, USA, 1995. [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth and Brooks, Monterey, CA, 1984. [6] M. Di Luca. New method to measure end-to-end delay of virtual reality. Presence: Teleoper. Virtual Environ., 19(6):569­ 584, December 2010. [7] KUKA Robot Group. KUKA.Ethernet RSI XML 1.1, kst ethernet rsi xml 1.1 v1 en edition, 12 2007. [8] G. Hirzinger, K. Landzettel, and Ch. Fagerer. Telerobotics with large time delays-the rotex experience. In Intelligent Robots and Systems '94. 'Advanced Robotic Systems and the Real World', IROS '94. Proceedings of the IEEE/RSJ/GI International Conference on, volume 1, pages 571­578 vol.1, Sep 1994. [9] A.J. Koivo and N. Houshangi. Real-time vision feedback for servoing robotic manipulator with self-tuning controller. Systems, Man and Cybernetics, IEEE Transactions on, 21(1):134­142, Jan 1991. [10] D. W. Marquardt. An algorithm for least-squares estimation of nonlinear parameters. SIAM Journal on Applied Mathematics, 11(2):431­441, 1963. [11] A. Popescu and D. Constantinescu. On kleinrocks independence assumption. In DemetresD. Kouvatsos, editor, Network Performance Engineering, volume 5233 of Lecture Notes in Computer Science, pages 1­13. Springer Berlin Heidelberg, 2011. [12] C. E. Rasmussen and Ch. K. I. Williams. Gaussian Processes for Machine Learning. The MIT Press, 2005. [13] O. Ravn, N. A. Andersen, and T. T. Andersen. Ur10 performance analysis. Technical report, Technical University of Denmark, Department of Electrical Engineering, 2014. [14] M. W. Spong, S. Hutchinson, and M. Vidyasagar. Robot modeling and control. John Wiley & Sons New York, 2006. [15] A. S. Tanenbaum and D. J. Wetherall. Computer Networks. Prentice Hall, 5th edition, 2011. [16] Jarno Vanhatalo, Jaakko Riihim¨ aki, Jouni Hartikainen, Pasi Jyl¨ anki, Ville Tolvanen, and Aki Vehtari. Gpstuff: Bayesian modeling with gaussian processes. The Journal of Machine Learning Research, 14(1):1175­1179, 2013. [17] P.D. Welch. A controller to overcome dead time. ISA Journal, 6(2):28­33, 1959. [18] P.D. Welch. A direct digital method of power spectrum estimation. IBM Journal of Research and Development, 5(2):141­156, 1961.

Sparse Latent Space Policy Search
Kevin Sebastian Luck
Arizona State University Interactive Robotics Lab AZ 85281 Tempe, USA mail@kevin-luck.net

Joni Pajarinen
Aalto University Intelligent Robotics Group 02150 Espoo, Finland Joni.Pajarinen@aalto.fi

Erik Berger
Technical University Bergakademie Freiberg Institute of Computer Science 09599 Freiberg, Germany erik.berger@informatik.tu-freiberg.de

Ville Kyrki
Aalto University Intelligent Robotics Group 02150 Espoo, Finland ville.kyrki@aalto.fi Abstract
Computational agents often need to learn policies that involve many control variables, e.g., a robot needs to control several joints simultaneously. Learning a policy with a high number of parameters, however, usually requires a large number of training samples. We introduce a reinforcement learning method for sampleefficient policy search that exploits correlations between control variables. Such correlations are particularly frequent in motor skill learning tasks. The introduced method uses Variational Inference to estimate policy parameters, while at the same time uncovering a lowdimensional latent space of controls. Prior knowledge about the task and the structure of the learning agent can be provided by specifying groups of potentially correlated parameters. This information is then used to impose sparsity constraints on the mapping between the high-dimensional space of controls and a lowerdimensional latent space. In experiments with a simulated bi-manual manipulator, the new approach effectively identifies synergies between joints, performs efficient low-dimensional policy search, and outperforms state-of-the-art policy search methods.

Heni Ben Amor
Arizona State University Interactive Robotics Lab AZ 85281 Tempe, USA hbenamor@asu.edu in a lower-dimensional latent space for control which, in turn, reduces cognitive effort and training time during skill acquisition. The existence of synergies has been reported in a variety of human motor tasks, e.g., grasping (Santello, Flanders, and Soechting 1998), walking (Wang, O'Dwyer, and Halaki 2013), or balancing (Torres-Oviedo and Ting 2010). Recently, various synergy-inspired strategies have been put forward to improve the efficiency of RL for motor skill acquisition (Bitzer, Howard, and Vijayakumar 2010; Kolter and Ng 2007). Typically, these approaches use dimensionality reduction as a pre-processing step in order to extract a lower-dimensional latent space of control variables. However, extracting the latent space using standard dimensionality reduction techniques requires a significantly large training set of (approximate) solutions, prior simulations, or human demonstrations. Even if such data exists, it may drastically bias the search by limiting it to the subspace of initially provided solutions. In our previous work, we introduced an alternative approach called latent space policy search that tightly integrates RL and dimensionality reduction (Luck et al. 2014). Using an expectation-maximization (EM) framework (Dempster, Laird, and Rubin 1977) we presented a latent space policy search algorithm that iteratively refines both the estimates of the low-dimensional latent space, as well as the policy parameters. Only samples produced during the search process were used. In this paper, we propose a different kind of latent space policy search approach, which similarly to our previous work combines RL and dimensionality reduction, but which also allows for prior structural knowledge to be included. Our method is based on the Variational Bayes (VB) (Neumann 2011; van de Meent et al. 2015) framework. Variational Bayes is a Bayesian generalization of the expectationmaximization algorithm, which returns a distribution over optimal parameters instead of a single point estimate. It is a powerful framework for approximating integrals that would otherwise be intractable. Our RL algorithm exploits these properties in order to (1) perform efficient policy search, (2) infer the low-dimensional latent space of the task, and

Introduction
Reinforcement learning (RL) is a promising approach to automated motor skill acquisition (Peters et al. 2011). Instead of a human hand-coding specific controllers, an agent autonomously explores the task at hand through trial-and-error and learns necessary movements. Yet, reinforcement learning of motor skills is also considered to be a challenging problem, since it requires sample-efficient learning in highdimensional state and action spaces. A possible strategy to address this challenge can be found in the human motor control literature (Bernstein 1967). Research on human motor control provides evidence for motor synergies; joint coactivations of a set of muscles from a smaller number of neural commands. The reduction in involved parameters results
Copyright c 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.

Time Group 1 Group 2 Group 3 Group 4 Time

Samples

Figure 1: The main idea of Group Factor Policy Search: A number of variables, for example the joints of an arm or leg of a NAO robot, form one group. Given several of such groups for the action vector (left matrix) the transformation matrix W can be divided in several submatrices corresponding to those groups. Subsequently each factor, given by a column in W, encodes information for all groups, e.g. four in the example given above. Factors may be non-zero for all groups, for a subset of groups, for exactly one group or zero for all groups. In the figure, grey areas correspond to non-zero values and white areas to zero values in the sparse transformation matrix. The transformation matrix is multiplied by the latent variables given by ~ = (~ ~t , · · · , z ~T ) distributed by z ~t  N (0, trace((st , t)(st , t)T )I). Z z1 , · · · , z (3) incorporate prior structural information. Prior knowledge about locality of synergies can be included by specifying distinct groups of correlated sub-components. Often such prior knowledge about groups of variables, e.g. coactivated joints and limbs, is readily available from the mechanical structure of a system. Structural prior knowledge is also common in other application domains. For example, in a wireless network the network topology defines receiver groups (Sagduyu and Ephremides 2004). Our approach draws inspiration and incorporates ideas from Factor Analysis, in particular Group Factor Analysis (Klami et al. 2015), as can be seen in Fig. 1. Groups of variables, e.g., robot joints grouped into arms and legs, are provided as prior structural knowledge by a user. A factorized control policy is then learned through RL, which includes a transformation matrix W. The transformation matrix holds factors that describe dependencies between either all of the groups or a subset of them. The individual factors can be regarded as synergies among the joints of the robot. We will show that the resulting algorithm effectively ties together prior structural knowledge, latent space identification, and policy search in a coherent way.

E [r = 1] =

p(, )p(r = 1|)dd,

(1)

where the probability of the trajectory p(, ) contains the (stochastic) policy, r is a binary variable indicating maximum reward, and p(r = 1|)  exp {-c ()} (Toussaint 2009) is the conditional probability of receiving maximum expected reward given a cost function. Assuming the Markov property and the independence of actions, the probability of a trajectory can be written as
T

p(, ) = p()p(s1 )
t=1

p(st+1 |st , at ) (at |st , ).

(2)

Policy Search
Policy search methods try to find an optimal policy for an agent which acts in an uncertain world with an unknown world model. At each time step t the agent executes an action at in state st and moves to the next state st+1 with probability p(st+1 |st , at ). After executing a certain number of actions, the agent receives a reward feedback given by an unknown reward function based on the performed execution trace (or trajectory/history)  = (s1 , a1 , . . . , sT , aT , sT +1 ). The overall objective in policy search is to maximize the expected reward over trajectories and policy parameters . For bounded rewards, maximizing expected reward is equivalent to maximizing the probability of a binary reward r (Toussaint and Storkey 2006):

The stochastic policy  (at |st , ) depends on the parameters  for which we additionally introduce prior distributions p(). This formulation will subsequently be used for structuring the policy model. The prior distributions may also depend on hyperparameters ­ for reasons of clarity, however, we will omit any such parameters below. Furthermore, we assume that the initial state distribution p(s1 ) and transition dynamics p(st+1 |st , at ) are unknown but fixed. Thus, they will cancel out as constant values.

Group Factor Policy Search
We will now introduce a new policy search method, called Group Factor Policy Search (GrouPS ), that uncovers the latent space on-the-fly based on prior structural information. In this section, we discuss how to incrementally improve the policy and the actual form of the new policy model. We parameterize the policy using Group Factor Analysis (Klami et al. 2015) in order to utilize prior information about the parameters and their correlations. Since our policy is a linear stochastic model with prior distributions, we first present a novel general Variational Inference framework for policy search that takes priors into account. Subsequently, we discuss how the policy is parameterized, and finally show

the policy model update equations for Group Factor Policy Search which we derive using the introduced Variational Inference method.

Variational Inference for Policy Search
In each iteration our new policy search method samples a distribution over trajectories pold () using the current policy, and based on pold () finds a new policy which maximizes a lower bound on the expected reward. This is repeated until convergence. In order to find a new policy based on samples from the old one, we introduce the sampling distribution pold () and the approximated parameter distribution q () (defined later) into Equation 1. By applying the log-function and using Jensen's inequality (Kober and Peters 2009; Bishop 2006, Eq. (1.115)) we derive the lower bound log  p(, ) p(r = 1|)dd pold ()q () p(, ) p(r = 1|)dd. pold ()q () log pold ()q () (3) pold ()q ()

Input: Reward function R (·) and initializations of parameters. Choose number of latent dimension n. Set fixed hyper-parameters ~  a , b ~ , a , b ,  2 and define groupings.
1 2 3 4 5 6

7 8 9 10 11 12 13 14 15 16 17 18

while reward not converged do for h=1:H do # Sample H rollouts for t=1:T do at = Wi Z + M + E with Z  N (0, I) and E  N (0,  ~), (m) where  ~ = ~m I Execute action at Observe and store reward R () Initialization of q-distribution while not converged do Update q (M) with Eq. (16) Update q (W) with Eq. (19) ~ with Eq. (22) Update q Z Update q () with Eq. (12) Update q ( ~) with Eq. (25) M = Eq(M) [M] W = Eq(W) [W]  = Eq() []  ~ = Eq( ~] ~ ) [ Result: Linear weights M for the feature vector , representing the final policy. The columns of W represents the factors of the latent space.

Since pold () was generated using the old policy it does not depend on  and we can simplify the lower bound to pold ()q () log = const +   · log   p ()
t=1

p(, ) pold ()q ()

p(r = 1|)dd

19 20

pold ()q ()
T

 (at |, st ) q ()

   p(r = 1|)dd, 

(4)

Algorithm 1: Outline of the Group Factor Policy Search (GrouPS) algorithm. et al. 2015). The main idea of GFA is to introduce prior distributions for the parameters, in particular a prior for a structured transformation matrix W. The transformation matrix, responsible for mapping between a low-dimensional subspace and the original high-dimensional space, is forced to be sparse and constructed using prior knowledge about grouping of the dimensions, that is, W is a concatenation of transform matrices W(m) for each group m. For example, if the dimensions of a vector represent the joints of a legged robot, we can group joints belonging to the same leg into the same group (see e.g. Fig. 1). Applying the idea of Group Factor Analysis for directed sampling leads to a linear model, i.e. a stochastic policy at
(m)

where the constant term can be ignored for the maximization of this term. By assuming the factorization q () = qi (i ) for the parameters and applying the Variational Bayes approach, we get the approximated distributions of the parameters: log qj (j ) = const +
- j T i=j

qi (i ) (5) p(r = 1|) R dd-j ,

pold () log
t=1

 (at , |st )

where the parameter vector -j contains all parameters except j . The normalization constant R is given by the integral R= pold ()p(r = 1|)d. (6)

= W(m) Zt + M(m) + Et
(m)

(m)

 (st , t) ,

(7)

Formulation of Group Factor Policy Search
In order to identify sets of correlated variables during policy search, we use a linear stochastic policy of a form similar to the model used in Group Factor Analysis (GFA) (Klami

where, for group m, the action at  RDm ×1 is a linear projection of a feature vector  (st , t)  Rp×1 . Each dimension of the feature vector is given by a basis function, which may depend on the current state and/or time. In the remainder of the paper, we will write  instead of  (s, t) for simplicity, even though there is an implicit dependency

of  on the current state of a trajectory. W(m)  RDm ×l is a transformation matrix mapping from the l-dimensional subspace to the original space. Each entry of the latent matrix Zt  Rl×p is distributed according to a standard normal distribution where N (0, 1), M(m)  RDm ×p is the mean (m) matrix, and the entries of the noise matrix Et  RDm ×p -1 are distributed by N (0,  ~m ). We can derive a stochastic policy from the model defined in Equation 7. Since Zt   N (0, trace(T )I) (8) Figure 2: Graphical model in Plate notation for Group Factor Policy Search. The basis functions (st , t) as well as the (m) action vector at are observed. Equation 9 shows the de(m) ~t depend on the pendencies for at . The latent variables z feature vector as stated in Equation (8) . The parameter m might either be given by a Gamma distribution as stated in Equation (12) or by a log-linear model with dependencies on parameters U and V. The hyper-parameters a and b are fixed and set to a small positive value. The prior distributions given above will lead to three kind of factors: (1) factors which are nonzero for only one group, (2) factors which are nonzero for several groups or (3) factors which are zero. In addition to the standard GFA prior distributions above, we introduce further ~ such that all prior distribuprior distributions for M and z tions are given with M  N Mold ,  2 I , m,k  G (a , b ) , ~  N 0, Tr T I , z
~   ~m  G a , b~ .

holds (see e.g. (Luck et al. 2014)), we can substitute Zt  by ~t  Rl×1 resulting in the policy the random variable z  (at |, st ) = 
M

N
m=1

m) a( t

W

(m)

~t + M z

(m)

Tr T ,  ~m

 I .

(9)

~t If we take a closer look at the latent space given by Wz we first find that the length of each factor is determined by (st , t) 2 2 . Secondly, a factor may be non-zero only for one or a subset of groups as can be seen in Fig. 1. This leads to a sparse transformation matrix and specialized factors and dimensions. As mentioned before, the form of our linear model in Equation 7 above is based on Group Factor Analysis. While GFA typically maps a vector from the latent space to the high-dimensional space, we map here a matrix from the latent space to the original space and then use this matrix as a linear policy on the feature vectors. GFA does not apply factor analysis (see e.g. (Harman 1976)) on each group of variables separately, but instead introduces a sparsity prior on the transformation matrix W thereby forcing correlations between groups:
M K Dm -1 , N wd,k 0, m,k m=1 k=1 d=1 (m)

Fig. 2 shows a graphical model of Group Factor Policy Search, given by the distributions stated above. Instead of ~t is used, which depends on (st , t) Z the latent variable z given a state and a point in time.

p ( W |) =

(10)

Derivation of Update Equations
~ ~ We assume fixed hyper-parameters a , b , a and b for the distributions which we determine using the Variational Inference method presented earlier, assuming a factorization of the q-distributions

with M being number of groups, Dm the number of dimensions of the m-th group and K the number of factors, i.e. number of columns of W. The precision  is given by a log-linear model with log  = UVT + µu 1T + 1µT v, (11)

~ )q (W) q ( q () = q (Z ~) q (M) q ()
T

(15)

where U  RM ×R , V  RK ×R and µu  RM as well as µv  RK model the mean profile. R defines the rank of the linear model and is chosen R min (M, K ). However, for the special case of R = min (M, K ) the precision is given by a simple gamma distribution (Klami et al. 2015)
 q (m,k ) = G a m , bm,k

(12)

with parameters
 a m =a +

Dm , 2

(13) (14)

~) = and additionally the assumption q (Z q (~ zt ) with ~ :,t = z ~t . Z By using the factorization given above and the Variational Inference rule for deriving the parameter distribution in Equation (5), we can derive the approximated parameter distributions, which maximize the expected reward. The approximated distribution for the mean matrix is given by a multiplicative normal distribution
M Dm

b m,k

1 (m) T (m) = b + Eq(W) wk wk . 2


qM (M) =
m=1 j =1

N

mj,:

(m) T

M µM mj , j

(16)

where the mean and covariance parameters in dependency of the group and dimension are given by M j =  -2 I +   p ( r = 1 |  )   R -1 E m ] ~ [~ (17) and µM mj =  M j moldj,: T + M · j · 2
T

and
Z µZ t = t ·   T (m) (m) (m) M E a - M  W W t   .  -1 T Tr  E m ] m=1 ~ [~ ~ ~

T

(24)

Ep()

T Tr T

t=1

Unlike the other distributions, the precision is given by a multiplicative gamma distribution q ) = ~ (~ 1 1 ~ ~ ~ G  ~m |a + Dm T, b + b 2 2 m m=1
M

(25)

Ep()

 p(r = 1|) R

 at,j - Ew wj,:

(m)

(m)

Ez zt ] ~ [~
-1

 

t=1

Tr T E m ] ~ [~

with one fixed parameter and one variable parameter. Esti~ mation of the parameter b m is the most complex and computationally expensive operation given by
~ b m = Ep()

(18) with mj,: given by the j -th row of M. The q-distribution for the transformation matrix is similarly given by
M Dm

p(r = 1|) R M
(m)

T

Tr T
t=1

-1

at

(m) T (m) at

-

(m) T 2at EM T


T

qW (W) =
m=1 j =1

N

(m) T W wj,: |µW mj , m

(19)

+ 2Ez zt ] EW W(m) ~ [~ - 2at
(m) T

EM M(m) 

with the mean and covariance parameters W m =  
t=1 T

EW W(m) Ez zt ] ~ [~
T

Ep()

p(r = 1|) R -1
-1

+ T EM M(m) M(m)  (20) , ¯ m,K  + + Tr EW W(m) W(m) Covz zt ] ~ [~ zt ] EW W(m) W(m) Ez +E z zt ] ~ [~ ~ [~
T T T

~t z ~T Ez ~ z t Tr T E m ] ~ [~

. (26)

and
W µW mj = m · Ep() T (m)

p(r = 1|) R
(m) T

Algorithm
(21) . Algorithm 1 summarizes all update steps for performing Group Factor Policy Search. The reward function R (·), number n of latent dimensions, and a set of hyperparameters need to be provided by the user.

at,j - EM mj,:

zt ]  Ez ~ [~
-1

t=1

Tr T E m ] ~ [~

¯ m,K ) = The diagonal matrix  ¯ m,K is given by diag ( (m,1 , m,2 , · · · , m,K ). The distribution for the latent ~ depends on the trajectory and time. Hence the variables Z ~ of a multireward can be seen as a probabilistic weight R plicative normal distribution. However, since we assume in~h dependent latent variables z t we can ignore the reward and get
H T

Evaluation
For evaluations and experiments the expectation Ep() [·] used above in Eq.(16-20,25) was approximated by a sample mean, H 1 f (i ) (27) Ep() [f ()]  H i=1 as proposed in (Kober and Peters 2009), where i is the ith of the H realized trajectories and f () a function value, vector or matrix for i and will be replaced by the parameter approximations given above.

~ qZ ~ Z =

~ R
t=1

N

~ ~ Z Z ~h z t |µt , t

,

(22)

with time-dependent parameters Z t =
M ~

Tr T
T

-1

I+ -1  , (23)

Setup of the Evaluation
For the comparison between the above presented GrouPS algorithm and previous policy search algorithms, a simulated task of a bi-manual robot operating in a planar task space was used. Each of the two arms (see Fig. 3) has six degreesof-freedom and the same base for the first joint. The initial

EW W(m) W(m)
-1 Tr T E ~m ~m 

m=1

Figure 3: Two simulated arms with six degrees-of-freedom and the same base in their initial position. Each end effector has a desired position for each time step, s shown by the green and red dots. The final position at time step 25 is given by the coordinate (0, 4). The numbers represent the joints with l for left and r for right.
180 160 140
1 Group POWER NAC 2 Groups PePPEr 4 Groups

Sum. Distances

120 100 80 60 40 20 0

physical robotic experiments (Kober and Peters 2011). PePPEr is also based on EM and incorporates policy search and dimensionality reduction, but without priors and thus without a structured transformation matrix. For comparison with PePPEr and PoWER the GrouPS algorithm was evaluated in three different configurations: (1) One group which contains all joints of both arms, (2) two groups, where each group contains the joints of one arm and (3) four groups with two groups per arm and joints 1-4 in one and joints 5-6 in the second group. The hyper-parameters of GrouPS were set to ~ ~ a = b = 1000, a = b = 1 and  2 = 100. No optimizations of the hyper-parameters were performed. Furthermore, to prevent early convergence and collapsing of the distributions due to small sample sizes the parameter W and  ~ are resized after each iteration by a factor of 1.5. The same is done after  each iteration for PePPEr. However, the factor was set to 1.09 since higher numbers lead to divergence in the parameters of the algorithm with unstable and divergent results. PePPEr was implemented as presented in (Luck et al. 2014) and in each iteration 20 inner iterations for the optimizations of the parameters were used. The same setup was used for GrouPS and for both algorithms the number of latent dimensions were set to six. The static variance parameter for PoWER as presented in (Kober and Peters 2009) and the initial variance of the other algorithms were all set to 101.5 , also for NAC with learning parameter set to 0.5. In each iteration, we sampled 30 trajectories and evaluated the trajectories based on the reward function
25

R() =
t=1
0 200 400 600 800 1000

exp (- · exp (-

effl (at ) - posl (t) effr (at ) - posr (t)

2) 2) ,

(28)

Iterations

Figure 4: Comparison between PePPEr, PoWER, Natural Actor-Critic and three instances of the GrouPS algorithm on the presented simulated task. Values correspond to the summarized distances between each end effector and its desired position given the current policy for the iteration. The mean value as well as the standard deviations are shown. configuration of the arms is presented in Fig. 3 as well as the desired positions for each end effector (tip of an arm). At each of the 25 time steps we give a different goal position for each arm's end effector, starting from the left for the left arm and starting from the right for the right arm, with the same final position at (0, 4) for both arms. In this task, the 12 dimensions of the action vector a represent the joint angles for each arm. For the basis functions eleven isotropic Gaussian distributions were used with i (t) = N (t|µ i , 3) for t  {1, 2, . . . , 24, 25}. In total, 132 parameters have to be estimated given M  R12×11 . As reference algorithms PoWER (Kober and Peters 2009), Natural Actor-Critic (NAC) (Peters and Schaal 2008) and PePPEr (Luck et al. 2014) were chosen: NAC is a policy gradient method while PoWER is an efficient policy search method based on expectation maximization (EM). PoWER has been experimentally validated in both simulated and

where the function effl (at ) returns the position of the left end effector given the action vector and posl (t) the corresponding desired goal position for time point t. effr (at ) and posr (t) return the actual and desired positions, respectively, for the right end effector. Then the 15 best trajectories are chosen for the computation of the parameters for each algorithm as described in (Kober and Peters 2009).

Results
Fig. 4 depicts the results of the explained experiment. For each algorithm ten different runs were executed and both mean and standard deviation computed. As can be seen in the figure, PePPEr outperforms both PoWER and NAC, as well as our method in case only one group spanning all variables is used. However, using two groups (one for each arm) already leads to comparable performance. Finally, the GrouPS algorithm with 4 different groups significantly outperforms the comparison methods.

Importance of the Choice of Groups
In order to investigate the effect of choosing joint groups we conducted an additional experiment. Our working hypothesis throughout the paper is that structural information about inherent groups of correlated variables will improve the search. Conversely, if we provide wrong in-

180 160 140
Swap1 Swap2 Swap3 4 Groups

Sum. Distances

120 100 80 60 40 20 0

0

200

400

600

800

1000

Iterations

Figure 7: Final policy found by the GrouPS algorithm after 100 iterations. A high reward is given if the head as well as the left foot of the robot are high above the ground. sult corroborates our assumption that a proper selection of groups can ameliorate the performance of the policy search algorithm.

Figure 5: Comparison between the original chosen four groups and three permutations of the Groups. Values correspond to the summarized distance between each end effector and its desired position for each time step given the current policy for the iteration.
180 160 140
Swap4 Swap5 4 Groups

Experiment: Lifting a Leg
To test the GrouPS algorithm in experiments following the real world closely, we reproduced the experiment stated in (Luck et al. 2014): We simulate a NAO robot (Gouaillier et al. 2008) using the V-REP framework (Rohmer, Singh, and Freese 2013) in the task of lifting its left leg without falling. The same reward function was used as presented in (Luck et al. 2014, Eq. (22)) with parameters  = 5,  = 10,  = 10 and max = 6. The V-REP framework (Rohmer, Singh, and Freese 2013) allows for simulations with high physical accuracy by utilizing the bullet physics library. In this experiment, the actions represent the 26 joint velocities for each of the 15 points in time. Again, for feature functions Gaussian distributions were used and the same parameters for GrouPS were chosen like given in the evaluation above. We ran GrouPS for 100 iterations. In each iteration, we used a set of 20 samples, of which ten were randomly selected from the set of 20 in the previous iteration and ten generated by the current policy. We used ten best samples out of this set of 20 for computing the new policy parameters. The groups were created in such a manner that the joints of each arm or leg form a single group as well as the joints of the head. The results are given in Fig. 7, where we find that the GrouPS algorithm is able to find a satisfactory solution even with a relatively small number of samples: the head and left leg of the NAO robot are at high positions corresponding to a high reward.

Sum. Distances

120 100 80 60 40 20 0

0

200

400

600

800

1000

Iterations

Figure 6: Comparison between the original grouping and two other variants with a different splitting point. Again, the values represent the summarized distances and shaded ares corresponds to the standard deviation given ten executions. formation about groupings the performance of the algorithm should deteriorate. To evaluate this hypothesis, we took the original partitioning of the joints into four groups and swapped two, later three pairs of joints randomly. As described above, the original group partitioning is {(1l, 2l, 3l, 4l), (5l, 6l), (1r, 2r, 3r, 4r), (5r, 6r)}. Performing two random swaps between the left and right side results in {(1l, 2l, 2r, 4l), (5l, 5r), (1r, 3l, 3r, 4r) , (6l, 6r)} (Fig. 6, Swap4). For three swaps the resulting partition is {(1l, 6r, 2r, 4l), (3r, 6l), (1r, 3l, 5l, 4r), (5r, 2l)} (Fig. 6, Swap5). Furthermore, three other groupings with different splitting points were evaluated: {(1l, 2l), (3l, 4l, 5l, 6l), (1r, 2r), (3r, 4r, 5r, 6r)} (Fig. 5, Swap1), {(1l, 2l), (3l, 4l), (5l, 6l), (1r, 2r), (3r, 4r), (5r, 6r)} (Fig. 5, Swap2) and {(1l, 2l, 3l), (4l, 5l, 6l), (1r, 2r, 3r), (4r, 5r, 6r)} (Fig. 5, Swap3). The result of executing GrouPS with these groupings can be seen in Fig. 5 and Fig. 6. All new groupings (resulting from above swaps) are clearly outperformed by the original partition. This re-

Conclusion and Future Work
In this paper, we introduced a novel algorithm for reinforcement learning in low-dimensional latent spaces. To this end, we derived a Variational Inference framework for policy search that takes prior structural information into account. The resulting policy search algorithm can efficiently learn new policy parameters, while also uncovering the underlying latent space of solutions, and incorporating prior knowledge about groups of correlated parameters. In experiments using motor skill learning tasks, we showed that the introduced GrouPS algorithm efficiently learns new motor skills. It significantly outperformed state-of-the-art policy

search methods, whenever prior information about structural groups was provided. So far, the dimensionality of the latent space needs to be provided as a parameter to the reinforcement learning algorithm. We plan to investigate automatic adjustments of the dimensionality using current rewards. In this paper, we focused on intra-group correlations. In future work, we plan to investigate correlations among extracted group factors, e.g., correlations between arms and legs.

Acknowledgments
J.Pajarinen and V.Kyrki were supported by the Academy of Finland, decision 271394.

References
Bernstein, N. A. 1967. The co-ordination and regulation of movements. Pergamon Press. Bishop, C. M. 2006. Pattern recognition and machine learning. Springer. Bitzer, S.; Howard, M.; and Vijayakumar, S. 2010. Using dimensionality reduction to exploit constraints in reinforcement learning. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 3219­3225. IEEE. Dempster, A. P.; Laird, N. M.; and Rubin, D. B. 1977. Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society. Series B (Methodological) 39(1):1­38. Gouaillier, D.; Hugel, V.; Blazevic, P.; Kilner, C.; Monceaux, J.; Lafourcade, P.; Marnier, B.; Serre, J.; and Maisonnier, B. 2008. The nao humanoid: a combination of performance and affordability. arXiv preprint arXiv:0807.3223. Harman, H. H. 1976. Modern factor analysis. University of Chicago Press. Klami, A.; Virtanen, S.; Leppaaho, E.; and Kaski, S. 2015. Group factor analysis. IEEE Transactions on Neural Networks and Learning Systems 26(9):2136­2147. Kober, J., and Peters, J. 2009. Policy search for motor primitives in robotics. In Advances in Neural Information Processing Systems (NIPS), 849­856. Kober, J., and Peters, J. 2011. Policy search for motor primitives in robotics. Machine Learning 84(1):171­203. Kolter, J. Z., and Ng, A. Y. 2007. Learning omnidirectional path following using dimensionality reduction. In Proceedings of the Robotics: Science and Systems (R:SS) conference. The MIT Press. Luck, K. S.; Neumann, G.; Berger, E.; Peters, J.; and Ben Amor, H. 2014. Latent space policy search for robotics. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 1434­ 1440. IEEE. Neumann, G. 2011. Variational inference for policy search in changing situations. In Proceedings of the 28th International Conference on Machine Learning (ICML), 817­824. Peters, J., and Schaal, S. 2008. Natural actor-critic. Neurocomputing 71(7):1180­1190.

Peters, J.; M¨ ulling, K.; Kober, J.; Nguyen-Tuong, D.; and Kr¨ omer, O. 2011. Towards motor skill learning for robotics. In Robotics Research. Springer. 469­482. Rohmer, E.; Singh, S. P.; and Freese, M. 2013. V-REP: A versatile and scalable robot simulation framework. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 1321­1326. IEEE. Sagduyu, Y. E., and Ephremides, A. 2004. The problem of medium access control in wireless sensor networks. IEEE Wireless Communications 11(6):44­53. Santello, M.; Flanders, M.; and Soechting, J. 1998. Postural hand synergies for tool use. The Journal of Neuroscience 18(23). Torres-Oviedo, G., and Ting, L. H. 2010. Subject-specific muscle synergies in human balance control are consistent across different biomechanical contexts. Journal of Neurophysiology 103(6):3084­3098. Toussaint, M., and Storkey, A. 2006. Probabilistic inference for solving discrete and continuous state Markov Decision Processes. In Proceedings of the 23rd International Conference on Machine Learning (ICML), 945­952. Toussaint, M. 2009. Robot trajectory optimization using approximate inference. In Proceedings of the 26th annual International Conference on Machine Learning (ICML), 1049­1056. ACM. van de Meent, J.-W.; Tolpin, D.; Paige, B.; and Wood, F. 2015. Black-box policy search with probabilistic programs. arXiv preprint arXiv:1507.04635. Wang, X.; O'Dwyer, N.; and Halaki, M. 2013. A review on the coordinative structure of human walking and the application of principal component analysis. Neural Regeneration Research 8(7):662­670.

All in-text references underlined in blue are linked to publications on ResearchGate, letting you access and read them immediately.

2015 IEEE International Conference on Robotics and Automation (ICRA) Washington State Convention Center Seattle, Washington, May 26-30, 2015

Learning Multiple Collaborative Tasks with a Mixture of Interaction Primitives
Marco Ewerton1 , Gerhard Neumann1 , Rudolf Lioutikov1 , Heni Ben Amor2 , Jan Peters1,3 and Guilherme Maeda1

Abstract-- Robots that interact with humans must learn to not only adapt to different human partners but also to new interactions. Such a form of learning can be achieved by demonstrations and imitation. A recently introduced method to learn interactions from demonstrations is the framework of Interaction Primitives. While this framework is limited to represent and generalize a single interaction pattern, in practice, interactions between a human and a robot can consist of many different patterns. To overcome this limitation this paper proposes a Mixture of Interaction Primitives to learn multiple interaction patterns from unlabeled demonstrations. Specifically the proposed method uses Gaussian Mixture Models of Interaction Primitives to model nonlinear correlations between the movements of the different agents. We validate our algorithm with two experiments involving interactive tasks between a human and a lightweight robotic arm. In the first, we compare our proposed method with conventional Interaction Primitives in a toy problem scenario where the robot and the human are not linearly correlated. In the second, we present a proof-of-concept experiment where the robot assists a human in assembling a box.

Holding tool Plate handover Human trajectories

Screw handover Robot trajectories

Plate handover Holding tool Screw handover Plate handover Holding tool Screw handover

Human trajectories

Robot trajectories

I. I NTRODUCTION Robots that can assist us in the industry, in the household, in hospitals, etc. can be of great benefit to the society. The variety of tasks in which a human may need assistance is, however, practically unlimited. Thus, it is very hard (if not impossible) to program a robot in the traditional way to assist humans in scenarios that have not been exactly prespecified. Learning from demonstrations is therefore a promising idea. Based on this idea, Interaction Primitive (IP) is a framework that has been recently proposed to alleviate the problem of programming a robot for physical collaboration and assistive tasks [1], [2]. At the core, IPs are primitives that capture the correlation between the movements of two agents--usually a human and a robot. Then, by observing one of the agents, say the human, it is possible to infer the controls for the robot such that collaboration can be achieved. A main limitation of IPs is the assumption that the movements of the human and the movements of the robot assistant are linearly correlated. This assumption is reflected in the underlying Gaussian distribution that is used to model
1 Intelligent Autonomous Systems Lab, Department of Computer Science, Technische Universität Darmstadt, Hochschulstr. 10, 64289 Darmstadt, Germany {ewerton, neumann, lioutikov, peters,

Fig. 1. Illustration of a task consisting of multiple interaction patterns, where each can be represented as an Interaction Primitive. In this work, we want to learn multiple interaction patterns from an unlabeled data set of interaction trajectories.

maeda}@ias.tu-darmstadt.de
2 Institute of Robotics and Intelligent Machines, Georgia Institute of Technology, 801 Atlantic Drive, Atlanta, GA 30332-0280, USA

hbenamor@cc.gatech.edu
3 Max Planck Institute for Intelligent Systems, Spemannstr. 38, 72076 Tuebingen, Germany jan.peters@tuebingen.mpg.de

the demonstrations. While this assumption holds for tasks that cover a small region of the workspace (a high-five task in [1] or handover of objects in [2]), it limits the use of IPs in two aspects. First, as illustrated in Fig. 1, a task such as the assembly of a toolbox consists of several interaction patterns that differ significantly from each other and therefore can not be captured by a single Gaussian. Moreover, even within a single interaction pattern, the correlation between the two agents may be nonlinear, for example, if the movements of the human are measured in the Cartesian space, while the movements of the robot are measured in joint space. Manually labeling each subtask (e.g. "plate handover", "screw handover", "holding screw driver") is a way to model interactions with multiple subtasks. Ideally, however, robots should be able to identify different subtasks by themselves. Moreover, it may not be clear to a human how to separate a number of demonstrated interactions in different, linearly correlated groups. Thus, a method to learn multiple interaction patterns from unlabeled demonstrations is necessary. The main contribution of this paper is the development of such a method. In particular, this work uses Gaussian Mixture Models (GMMs) to create a Mixture of Interaction

978-1-4799-6922-7/15/$31.00 ©2015 IEEE

1535

Probabilistic Movement Primitives [2]. The remainder of this paper is organized as follows. Section II presents related work. In Section III, Probabilistic Movement Primitives (ProMPs) and Interaction ProMPs are briefly introduced, followed by the proposition of the main contribution of this paper: a Mixture of Interaction ProMPs based on Gaussian Mixture Models (GMMs). Section IV evaluates the proposed method first on a toy problem that is useful to clarify the characteristics of the method and then on a practical application of a collaborative toolbox assembly. Section V presents conclusions and ideas for future work. II. R ELATED W ORK Physical human-robot interaction poses the problem of both action recognition and movement control. Interaction dynamics need to be specified in a way that allows for robust reproduction of the collaborative task under different external disturbances, and a common approach is based on direct force sensing or emulation. Rozo et al. [3] propose a framework for haptic collaboration between a human and a robot manipulator. Given a set of kinesthetic demonstrations, their method learns a mapping between measured forces and the impedance parameters used for actuating the robot, e.g., the stiffness of virtual springs governing the collaborative task. In another force-based approach, Lawitzky et al. [4] propose learning physical assistance in a collaborative transportation task. In the early learning phase, the robot uses the measured force values to follow the human guidance during the task. Recorded force and motion patterns are then used to learn a Hidden Markov Model (HMM) which can predict the human's next action and over time the robot learns to take over a more active role in the interaction. Kulvicius et al. [5] also address a transportation task where the two agents are modeled as two point particles coupled by a spring. The forces applied by the other agent tell the robot how to adapt its own trajectory. Our work differs significantly from the cited works in the sense that our method does not use nor emulate force signals, but instead learns the correlation between the trajectories of two agents. Correlating trajectories not only simplifies the problem in terms of hardware and planning/control but also allows us to correlate multi-agent movements that do not generate force during the interaction, for example, the simple gesture of asking and receiving an object. Graphical models have also been used to describe interaction dynamics. In the computer vision community, HMMs have been widely adopted to model interaction dynamics from input video streams [6], [7]. As a result, graphical models have also gained considerable attention in the field of human-robot interaction. In [8], Hawkins and colleagues use a Bayes network to improve the fluency in a joint assembly task. The Bayes network learns to infer the current state of the interaction, as well as task constraints and the anticipated timing of human actions. Tanaka et al. [9] use a Markov model to predict the positions of a worker in an assembly line. Wang et al. [10] propose the Intention-Driven Dynamics Model (IDDM) as a probabilistic graphical model

with observations, latent states and intentions where the transitions between latent states and the mapping from latent states to observations are modeled as Gaussian Processes. Koppula et al. [11] use a conditional random field with sub-activities, human poses, object affordances and object locations over time. Inference on the graphical model allows a robot to anticipate human activity and choose a corresponding, preprogrammed robot response. Lee et al. [12] learn a hierarchical HMM which triggers action primitives in response to observed behaviors of a human partner. While very successful for classifying actions, graphical models, however, may not be the best option when it comes to generating motions. In [13], for example, the use of a HMM with discrete states, although very successful in action classification, introduces artifacts into the motion generation part that hinders motion generalization. Therefore, a clear problem in physical human-robot interaction is that while graphical models may be suitable in the action recognition domain, motion generation at the continuous level must also be taken into account. Llorens et al. [14] present a hybrid design for a robot to be used on the shoulder. In their work, Petri Nets accounts for discrete control transitions while, at the motion level, Partial Least Squares Regression has been used to find the best action of the robot at future time steps. Perhaps the principal distinction of our method is the use of Interaction Primitives (IPs), introduced by Ben Amor et al. [1], initially based on dynamical movement primitives [15] and later extended to Probabilistic Movement Primitives [16] with action recognition in the work of Maeda et al. [2]. As shown in [2], Interaction Primitives can be used to not only recognize the action of an agent, but also to coordinate the actions of a collaborator at the movement level; thus overcoming in a single framework both layers of discrete action recognition and continuous movement control. Differently from [2], where different interaction patterns must be hand-labeled, our contribution is the unsupervised learning of a Mixture of Interaction Primitives. III. M IXTURE OF I NTERACTION P RIMITIVES In this section, we will briefly discuss the Interaction Primitive framework based on Probabilistic Movement Primitives [2], [16], followed by the presentation of the proposed method, based on Gaussian Mixture Models. A. Probabilistic Movement Primitives A Probabilistic Movement Primitive (ProMP) [16] is a movement representation based on a distribution over trajectories. The probabilistic formulation of a movement primitive allows operations from probability theory to seamlessly combine primitives, specify via points, and correlate joints via conditioning. Given a number of demonstrations, ProMPs are designed to capture the variance of the positions q and velocities q  as well as the covariance between different joints. For simplicity, let us first consider only the positions q for one degree of freedom (DOF). The position qt at time step t can be approximated by a linear combination of basis

1536

functions,
T qt = t w+ ,

(1)

where is Gaussian noise. The vector t contains the N basis functions i , i  {1, 2, 3, ..., N }, evaluated at time step t where we will use the standard normalized Gaussian basis functions. The weight vector w is a compact representation of a trajectory1 . Having recorded a number of trajectories of q , we can infer a probability distribution over the weights w. Typically, a single Gaussian distibution is used to represent p(w). While a single w represents a single trajectory, we can obtain a distribution p(q1:T ) over trajectories q1:T by integrating w out, p(q1:T ) = p(q1:T |w)p(w)dw. (2)

T T where K = w Ht (D + Ht w Ht )-1 , D is the observation noise, and   o (t )(1,1) 0 0 0   o  0 ( t )(P,P ) 0 0    (5) Ht =   0 0 0 0c   (1,1)   0 0 0 0c (Q,Q)

is the observation matrix where the unobserved states of the robot are filled with zero bases. Here, the human and the robot are assumed to have P and Q DOFs, respectively. Now, by combining (1), (3) and (4), we can compute the probability distribution over the trajectories q1:T given the observation D. For a detailed implementation the interested reader is referred to [2]. C. Mixture of Interaction ProMPs

If p(w) is a Gaussian, p(q1:T ) is also Gaussian. The distribution p(q1:T ) is called a Probabilistic Movement Primitive (ProMP). B. Interaction ProMP An Interaction ProMP builds upon the ProMP formulation, with the fundamental difference that we will use a distribution over the trajectories of all agents involved in the interaction. Hence, q is multidimensional and contains the positions in joint angles or Cartesian coordinates of all agents. In this paper, we are interested in the interaction between two agents, here defined as the observed agent (human) and the controlled agent (robot). Thus, the vector q is now given as q = [(q o )T , (q c )T ]T , where (·)o and (·)c refer to the observed and controlled agent, respectively. Let us suppose we have observed a sequence of positions o qt at m specific time steps t, m  T . We will denote this sequence by D. Given those observations, we want to infer the most likely remaining trajectory of both the human and the robot. T T T Defining w ¯ = [wo , wc ] as an augmented vector that contains the weights of the human and of the robot for one demonstration, we write the conditional probability over trajectories q1:T given the observations D of the human as p(q1:T |D) = p(q1:T |w ¯ )p(w ¯ |D)dw ¯. (3)

The goal of our method is to learn several interaction patterns given the weight vectors that parameterize our unlabeled training trajectories. For this purpose, we learn a GMM in the weight space, using the Expectation-Maximization algorithm (EM) [17]. Assume a training set with n vectors w ¯ representing the concatenated vectors of human-robot weights as defined in section III-B. In order to implement EM for a GMM with a number K of Gaussian mixture components, we need to implement the Expectation step and the Maximization step and iterate over those steps until convergence of the probability distribution over the weights, p(w ¯ ; 1:K , µ1:K , 1:K ), where 1:K = {1 , 2 , · · · , K }, µ1:K = {µ1 , µ2 , · · · , µK } and 1:K = {1 , 2 , · · · , K }. Here, k = p(k ), µk and k are the prior probability, the mean and the covariance matrix of mixture component k , respectively. We initialize the parameters 1:K , µ1:K and 1:K using k-means clustering before starting the Expectation-Maximization loop. The number K of Gaussian mixture components is found by leave-one-out cross-validation. The mixture model can be formalized as
K K

p(w ¯) =
k=1

p(k )p(w ¯ |k ) =
k=1

k N (w ¯ ; µk , k ).

(6)

Expectation step: Compute the responsibilities rik , where rik is the probability of cluster k given weight vector w ¯i , rik = p(k |w ¯i ) = N (w ¯i ; µk , k )k
K l=1

We compute a normal distribution from n demonstrations by stacking several weight vectors [w ¯1 , ..., w ¯n ]T , one for each demonstration, such that w ¯  N (µw , w ). A posterior distribution can be obtained after observing D with
T µnew = µw + K (D - Ht µw ), w T new = w - K (Ht w ), w

l N (w ¯i ; µl , l )

.

(7)

Maximization step: Update the parameters k , µk and k of each cluster k , using
n

(4) nk =

rik , k =
i=1 n ¯i i=1 rik w

nk , n ,

(8)

1 In order to cope with the different speeds of execution during demonstration, the trajectories must be time-aligned before parameterization. The interested reader is referred to [2] for details.

µk =

nk

(9)

1537

k =

1 nk

n

rik (w ¯i - µk )(w ¯i - µk )T
i=1

.

(10)

Finally, we want to use our model to infer the trajectories of the controlled agent given observations from the observed agents. We need to find the posterior probability distribution over trajectories q1:T given the observations D, as in Section III-B. In order to compute this posterior using our GMM prior, first we find the most probable cluster k  given the observation D, using the Bayes' theorem. The posterior over the clusters k given the observation D is given by p(k |D)  p(D|k )p(k ), where p(D|k ) = and p(w ¯ |k ) = N (w ¯ ; µk , k ). Thus the most probable cluster k given the observation D is k  = arg max p(k |D). (12)
k 

Algorithm 1 Training 1) Parameterize demonstrated trajectories: Find vector of weights w ¯ for each trajectory, such that qt  T t w ¯. 2) Find GMM in parameter space, using EM: Initialize GMM parameters 1:K , µ1:K and 1:K with kmeans clustering. repeat E step rik = p(k |w ¯i ) = M step
n

N (w ¯i ; µk , k )k
K l=1

l N (w ¯i ; µl , l )

(11)

p(D|w ¯ )p(w ¯ |k )dw ¯

nk =
i=1

rik , k =
n ¯i i=1 rik w

nk n

µk = k = 1 nk
n

nk

rik (w ¯i - µk )(w ¯i - µk )T
i=1

The output of the proposed algorithm is the posterior probability distribution over trajectories q1:T , conditioning cluster k  to the observation D, p (q1:T |D) = p (q1:T |w ¯ ) p (w ¯ |k  , D) dw ¯. (13)

until p(w ¯ ; 1:K , µ1:K , 1:K ) converges

Algorithm 2 Inference 1) Find most probable cluster given observation: p(k |D)  p(D|k )p(k )

Algorithms 1 and 2 provide a compact description of the proposed methods for training and inference, respectively. IV. E XPERIMENTS This section presents experimental results in two different scenarios using a 7-DOF KUKA lightweight arm with a 5finger hand2 . The goal of the first scenario is to expose the issue of the original Interaction Primitives [1], [2] when dealing with trajectories that have a clear multimodal distribution. In the second scenario we propose a real application of our method where the robot assistant acts as a third hand of a worker assembling a toolbox (please, refer to the accompanying video3 ). A. Nonlinear Correlations between the Human and the Robot on a Single Task To expose the capability of our method of dealing with multimodal distributions, we propose a toy problem where a human specifies a position on a table and the robot must point at the same position. The robot is not provided any form of exteroceptive sensors; the only way it is capable to generate the appropriate pointing trajectory is by correlating
2 Regarding the control of the robot, the design of a stochastic controller capable of reproducing the distribution of trajectories is also part of ProMPs and the interested reader is referred to [16] for details. Here we use a compliant, human-safe standard inverse-dynamics based feedback controller. 3 Also available at http://youtu.be/9XwqW_V0bDw

k  = arg max p(k |D)
k

2) Condition on observation, using cluster k  : p(q1:T |D) = p(q1:T |w ¯ )p(w ¯ |k  , D)dw ¯

its movement with the trajectories of the human. As shown in Fig. 2, however, we placed a pole in front of the robot such that the robot can only achieve the position specified by the human by moving either to the right or to the left of the pole. This scenario forces the robot to assume quite different configurations, depending on which side of the pole its arm is moving around. During demonstrations the robot was moved by kinesthetic teaching to point at the same positions indicated by the human (tracked by motion capture) without touching the pole. For certain positions, as the one indicated by the arrow in Fig. 2(a), only one demonstration was possible. For other positions, both right and left demonstrations could be provided as shown in Fig. 2(a) and 2(b). The demonstrations, totaling 28 pairs of human-robot trajectories, resulted in a multimodal distribution of right and left trajectory patterns moving around the pole. In this scenario, modeling the whole distribution over

1538

0.1 0.08 RMS error (m) 0.06 0.04 0.02 0

2

4

6

8

10

12

14

16

Number of clusters

Fig. 4.
(a) (b)

Root Mean Square Error with models using up to 17 Gaussians.

Fig. 2. Experimental setup of a toy problem used to illustrate the properties of the Mixture of Interaction Primitives. The robot is driven by kinesthetic teaching to point at the positions specified by the human (pointed with the wand). Certain pointed positions can be achieved by either moving the arm to the right (a) or to left (b) of the pole placed on the table. Other positions, such as the one indicated by the arrow, can only be achieved by one interaction pattern.
ground truth prediction

clusters it is observed that the prediction error fluctuates around 4 cm. The experiments previously shown in Fig. 3(b) were done with eight clusters. B. Assembling a Box with a Robot Assistant In this experiment, we recorded a number of demonstrations of different interaction patterns between a human and the robot cooperating to assemble a box. We used the same robot described in the previous experiment. During demonstrations, the human wore a bracelet with markers whose trajectories in Cartesian coordinates were recorded by motion capture. Similarly to the first scenario, the robot was moved in gravity compensation mode by another human during the training phase and the trajectories of the robot in joint space were recorded. There are three interaction patterns. Each interaction pattern was demonstrated several times to reveal the variance of the movements. In one of them, the human extends his/her hand to receive a plate. The robot fetches a plate from a stand and gives it to the human. In a second interaction, the human fetches the screwdriver, the robot grasps and gives a screw to the human as a pre-emptive collaborator would do. The third type of interaction consists of giving/receiving a screwdriver. Each interaction of plate handover, screw handover and holding the screwdriver was demonstrated 15, 20, and 13 times, respectively. The pairs of trajectories of each interaction are shown in Fig. 54 . As described in section III, all training data are fed to the algorithm resulting in 48 human-robot pairs of unlabeled demonstrations as shown in the upper row of Fig. 7. The presented method parameterizes the trajectories and performs clustering in the parameter space in order to encode the mixture of primitives. In the upper row of Fig. 7, each mixture is represented by a different color. The human is represented by the (x, y, z ) Cartesian coordinates while the robot is represented by the seven joints of the arm. The figure shows the first four joints of the robot (starting from the base).
4 Due to the experimental setup, for the sub-tasks of plate and screw handover we added an initial hand-coded trajectory that runs before the kinesthetic teaching effectively starts. These trajectories are used to make the robot grasp and remove the plate or screw from their respective stands. This is reflected in the figure as the deterministic part at the beginning of the trajectory of the robot. This initial trajectory, however, has no effect on the proposed method itself.

(a)

(b)

Fig. 3. Results of the predictions of the robot trajectories in Cartesian space. Both subplots show the same ground truth trajectories generated by driving the robot in kinesthetic teaching. The predictions are generated by leave-one-out cross-validation on the whole data set comprised of 28 demonstrations. (a) Prediction using the conventional Interaction ProMPs with a single Gaussian. (b) Prediction using the proposed method with a mixture of Gaussians.

the parameters of the trajectories with one single Gaussian (as in the original Interaction Primitive formulation) is not capable of generalizing the movements of the robot to other positions in a way that resembles the training, as the original framework is limited by assuming a single pattern. This limitation is clearly shown in Fig. 3(a), where several trajectories generated by a single cluster GMM (as in the original Interaction Primitive) cross over the middle of the demonstrated trajectories, which, in fact, represents the mean of the single Gaussian distribution. Fig. 3(b) shows the predictions using the proposed method with a mixture of Gaussians. By modeling the distribution over the parameters of the trajectories using GMMs as described in section III-C, a much better performance could be achieved. The GMM assumption that the parameters are only locally linear correlated seemed to represent the data much more accurately. As shown in Fig. 4, this improvement is quantified in terms of the Root Mean Square (RMS) Error of the prediction of the trajectory in relation to the ground truth using leave-one-out cross-validation over the whole data set. The same figure also shows that there is a sharp decrease in the RMS error up to six clusters, especially when taking into account the variance among the 28 tests. Beyond seven

1539

human human robot robot

human

robot

(a) Handing over a plate

(b) Handing over a screw

(c) Holding the screw driver

Fig. 5. Demonstrations of the three different interactions and their respective trajectories. For the case of plate and screw handover the beginning of the robot trajectory shows a deterministic part that accounts for the fact that the robot has to remove objects from their respective stands, which is not part of the kinesthetic teaching and does not affect the algorithm in any sense.

Joint RMS prediction error (deg)

40

30

20

10

0

1

2

3

4

5

6

7

8

Number of clusters

Fig. 6. Root Mean Square Error of the joint trajectories (averaged over all tests) using a leave-one-out cross-validation as a function of the number of clusters (mixture components). The plateau after three clusters seems to be consistent with the training data since it consists of three distinct interaction patterns.

Figure 6 shows the RMS prediction error averaged over all tests as the number of mixture components increase. The prediction is obtained by leave-one-out cross-validation over the whole set of 48 demonstrations. As one would expect, since the unlabeled data contains three distinct interaction patterns, the improvement is clearly visible up to three mixture components. No significant improvement is obtained afterwards, thus the GMM with three mixture components was selected for experiments. In the inference/execution phase, the algorithm first computes the most probable Interaction Primitive mixture component based on the observation of the position of the wrist of the human with (12). Using the same observation, we then condition the most probable Interaction Primitive, which allows computing a posterior distribution over trajectories for all seven joints of the robot arm as in (13). Finally, the mean of each joint posterior distribution is fed to a standard inverse

dynamics feedback tracking controller. The lower row of Fig. 7 depicts the posterior distribution for one test example where a three-cluster GMM was trained with the other 47 trajectories. The GMM prior is shown in gray where the patches of different clusters overlap. The observation consists only of the final position of the wrist, shown as asterisks in the figure. The black lines are the ground truth trajectories of each degree of freedom. The posterior, in red, is represented by its mean and by the region inside ± two standard deviations. The mean of this posterior is the most probable trajectory for each degree of freedom given the observed end position of the wrist of the human. We assembled the toolbox, consisting of seven parts and 12 screws, two times. The experiments demanded more than 40 executions of the Interaction Primitives. The selection of the right mixture component was 100% correct. (Please refer to the accompanying video). We evaluated the precision of the interactions by computing the final position of the hand of the robot with forward kinematics. The forward kinematics was fed with the conditioned robot trajectories predicted by leave-oneout cross validation. The interactions of plate handover and holding screwdriver resulted in mean error with two standard deviations (mean error ±2 ) of 3.2 ± 2.6 cm and 2.1 ± 2.3 cm, respectively. We did not evaluate the precision of the handover of the screw, as the position at which the robot hands the screw is not correlated to the human (please refer to the accompanying video). As an example, Fig. 8 shows the robot executing the plate handover at three different positions based on the location of the wrist marker. Note that the postures of the arm are very different, although they are all captured by the same Interaction Primitive.

1540

Fig. 7. Upper row: Mixture components represented by their mean trajectories and the region inside two standard deviations (µ ± 2 ). Each mixture component is represented by a different color and corresponds to a different interaction pattern. The light gray trajectories are the training trajectories. Obs.: The plots show only the part of the trajectories generated by kinesthetic teaching. Lower row: Posterior probability distribution (red) given observation depicted by the blue asterisks. The GMM prior is shown in gray.

addressing the estimation of the phase of the execution of the primitive for switching tasks in real time. Also, we are addressing the use of the stochastic feedback controller provided by the original ProMP work in [16]. Although this work focused on human-robot trajectories, we are currently considering extensions of our work where the human is replaced by other variables of interest. For example, the same framework can be used to correlate joint and endeffector trajectories of the same robot to learn nonlinear forward/inverse kinematic models. Similarly the Mixture of Interaction Primitives can be used to correlate the interaction between motor commands and joint trajectories to learn inverse dynamics models. VI. ACKNOWLEDGMENTS The research leading to these results has received funding from the project BIMROB of the "Forum für interdisziplinäre Forschung" (FiF) of the TU Darmstadt, from the European Community's Seventh Framework Programme (FP7ICT-2013-10) under grant agreement 610878 (3rdHand) and from the European Community's Seventh Framework Programme (FP7-ICT-2009-6) under grant agreement 270327 (ComPLACS). R EFERENCES
[1] H. Ben Amor, G. Neumann, S. Kamthe, O. Kroemer, and J. Peters, "Interaction primitives for human-robot cooperation tasks," in Proceedings of 2014 IEEE International Conference on Robotics and Automation (ICRA), 2014. [2] G. Maeda, M. Ewerton, R. Lioutikov, H. Ben Amor, J. Peters, and G. Neumann, "Learning interaction for collaborative tasks with probabilistic movement primitives," in Proceedings of the International Conference on Humanoid Robots (HUMANOIDS), 2014. [3] L. Rozo, S. Calinon, D. G. Caldwell, P. Jimenez, and C. Torras, "Learning collaborative impedance-based robot behaviors," in AAAI Conference on Artificial Intelligence, Bellevue, Washington, USA, 2013. [4] M. Lawitzky, J. Medina, D. Lee, and S. Hirche, "Feedback motion planning and learning from demonstration in physical robotic assistance: differences and synergies," in Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, Oct 2012, pp. 3646­3652.

Fig. 8. Handover of a plate. Conditioning on three different positions of the wrist (using motion capture) of a human coworker.

V. C ONCLUSIONS In this paper we presented a Mixture of Interaction Primitives where Gaussian Mixture Models are used to model multiple interaction patterns from unlabeled data. The multimodal prior probability distribution is obtained over parameterized demonstration trajectories of two agents working in collaboration. During the execution, the algorithm selects the mixture component with the highest probability given the observation of the human, which is then conditioned to infer the appropriate robot reaction. The proposed method is able to learn and recognize multiple human-robot collaboration tasks from an arbitrary number of demonstrations consisting of unlabeled interaction patterns, what was not possible with the previous Interaction Primitive framework. In the context of human-robot interaction we are currently

1541

[5] T. Kulvicius, M. Biehl, M. J. Aein, M. Tamosiunaite, and F. Wörgötter, "Interaction learning for dynamic movement primitives used in cooperative robotic tasks," Robotics and Autonomous Systems, vol. 61, no. 12, pp. 1450­1459, 2013. [6] M. Brand, N. Oliver, and A. Pentland, "Coupled hidden markov models for complex action recognition," in Proceedings of the 1997 Conference on Computer Vision and Pattern Recognition (CVPR '97), ser. CVPR '97. Washington, DC, USA: IEEE Computer Society, 1997, pp. 994­. [7] N. Oliver, B. Rosario, and A. Pentland, "A bayesian computer vision system for modeling human interactions," Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 831­843, Aug 2000. [8] K. Hawkins, N. Vo, S. Bansal, and A. F. Bobic, "Probabilistic human action prediction and wait-sensitive planning for responsive humanrobot collaboration," in Proceedings of the International Conference on Humanoid Robots (HUMANOIDS), 2013. [9] Y. Tanaka, J. Kinugawa, Y. Sugahara, and K. Kosuge, "Motion planning with worker's trajectory prediction for assembly task partner robot," in Proceedings of the 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2012, pp. 1525­ 1532. [10] Z. Wang, K. Mülling, M. P. Deisenroth, H. Ben Amor, D. Vogt, B. Schölkopf, and J. Peters, "Probabilistic movement modeling for intention inference in human­robot interaction," The International Journal of Robotics Research, vol. 32, no. 7, pp. 841­858, 2013. [11] H. S. Koppula and A. Saxena, "Anticipating human activities using object affordances for reactive robotic response." in Robotics: Science and Systems, 2013. [12] D. Lee, C. Ott, Y. Nakamura, and G. Hirzinger, "Physical human robot interaction in imitation learning," in Robotics and Automation (ICRA), 2011 IEEE International Conference on. IEEE, 2011, pp. 3439­3440. [13] H. Ben Amor, D. Vogt, M. Ewerton, E. Berger, B. Jung, and J. Peters, "Learning responsive robot behavior by imitation," in Proceedings of the 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2013, pp. 3257­3264. [14] B. Llorens-Bonilla and H. H. Asada, "A robot on the shoulder: Coordinated human-wearable robot control using coloured petri nets and partial least squares predictions," in Proceedings of the 2014 IEEE International Conference on Robotics and Automation, 2014. [15] A. J. Ijspeert, J. Nakanishi, H. Hoffmann, P. Pastor, and S. Schaal, "Dynamical movement primitives: learning attractor models for motor behaviors," Neural computation, vol. 25, no. 2, pp. 328­373, 2013. [16] A. Paraschos, C. Daniel, J. Peters, and G. Neumann, "Probabilistic movement primitives," in Advances in Neural Information Processing Systems (NIPS), 2013, pp. 2616­2624. [17] C. M. Bishop et al., Pattern recognition and machine learning. springer New York, 2006, vol. 1.

1542

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/273134654

ExploitingSymmetriesandExtrusionsfor GraspingHouseholdObjects
ConferencePaperinProceedings-IEEEInternationalConferenceonRoboticsandAutomation·May2015
DOI:10.1109/ICRA.2015.7139713

CITATIONS

READS

2
7authors,including: AnaHuamanQuispe GeorgiaInstituteofTechnology
7PUBLICATIONS4CITATIONS
SEEPROFILE

265

MarcoA.Gutierrez UniversidaddeExtremadura
11PUBLICATIONS23CITATIONS
SEEPROFILE

HenrikIskovChristensen UniversityofCalifornia,SanDiego
478PUBLICATIONS6,419CITATIONS
SEEPROFILE

HeniBenAmor ArizonaStateUniversity
56PUBLICATIONS434CITATIONS
SEEPROFILE

AllcontentfollowingthispagewasuploadedbyHeniBenAmoron05March2015.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Exploiting Symmetries and Extrusions for Grasping Household Objects
Ana Huam´ an Quispe1 Beno^ it Milville1 Marco A. Guti´ errez2 Can Erdogan1 Henrik Christensen1 Heni Ben Amor1 Mike Stilman

Abstract--In this paper we present an approach for creating complete shape representations from a single depth image for robot grasping. We introduce algorithms for completing partial point clouds based on the analysis of symmetry and extrusion patterns in observed shapes. Identified patterns are used to generate a complete mesh of the object, which is, in turn, used for grasp planning. The approach allows robots to predict the shape of objects and include invisible regions into the grasp planning step. We show that the identification of shape patterns, such as extrusions, can be used for fast generation and optimization of grasps. Finally, we present experiments performed with our humanoid robot executing pick-up tasks based on single depth images and discuss the applications and shortcomings of our approach.

I. I NTRODUCTION The ability to grasp and manipulate objects is an important skill for autonomous robots. Many important tasks, e.g., assisting humans in household environments, require robots to reliably plan and execute grasps on surrounding objects. To generate plans for manipulation tasks, information about the shape of the object is required. A frequent approach to grasp planning is to use a database of polygonal meshes representing the different objects that the robot can manipulate [8]. Such information about object geometry can be used by grasp planners to synthesize an appropriate hand shape and orientation for physical interaction. While this approach is valid for structured domains with a small set of different objects, it does not scale to unstructured environments in which many objects may have never been seen before. Other approaches to grasp planning employ depth cameras to acquire 3D point clouds of new objects, which in turn are used to generate grasps. Since the point clouds are acquired from a specific perspective, they only hold partial shape information about the visible frontal part. Using only partial point clouds to plan manipulation tasks can be very limiting, since many grasps involve placing fingers on opposite sides of an object. To fill any gaps and produce a complete point cloud, multiple images can be acquired by either iteratively moving the camera or the object. This process is time-consuming and
1 Institute

Fig. 1: Extracted information of rotational symmetries in the object is used to create a complete shape from a partial point cloud. The generated mesh is used by a grasp planner to generate a continuous set of grasps around the symmetry axis.

for of

gia

Institute

Robotics and Intelligent Machines, Technology, Atlanta, GA 30332,

GeorUSA.

ahuaman3@gatech.edu, cerdogan@cc.gatech.edu, benoit.milville@gadz.org, hic@cc.gatech.edu, hbenamor@cc.gatech.edu
2 Robotics and Artificial Laboratory, Univesity of Extremadura, C´ aceres, 10003, Spain. marcog@unex.es

introduces new challenges such as the precise matching of the individual point clouds of each view. Alternatively, the robot can use geometric cues to predict the shape of the object in unseen regions. Through the analysis of inherent shape properties such as mirror symmetries and rotational extrusions, estimates of the complete point cloud can be generated from a single image. The extracted symmetry parameters can be used to extend observed shape patterns, e.g., the profile curve of an object, to occluded regions. In this paper, we show how compact object representations for manipulation tasks can be generated from a partial point cloud. Given a single RGB-D image, we generate a complete mesh model of the observed object as well as additional shape information, e.g., axis of symmetry or superquadric approximations. We show that these compact representations can be later exploited for the fast synthesis of a continuous set of grasps. In turn, the set is used to plan robot manipulation tasks. Our approach builds both upon recent developments in symmetry-based [3, 18], as well as extrusion-based object representations [16]. Symmetry-based representations mirror observed object parts into occluded regions. Extrusion-based approaches, on the other hand, try

to identify a two-dimensional profile which can be linearly or rotationally extruded to complete an object. In this work we show how symmetries and extrusions can be used to extract two different types of object representations, namely superquadric approximations and 2D shape profiles. We also show how these representations can used to generate grasps on the object. The rest of this paper is organized as follows: Section II summarizes relevant literature. Section III introduces two compact object representations that are based on detecting symmetries and extrusions. Section IV shows how compact object representations based on extrusion patterns can be exploited for fast grasp planning with a small number of parameters. Section V presents experimental results of the object completion, as well as its application to robot grasping tasks. Finally in Section VI we discuss our approach and its advantages and shortcomings. II. R ELATED W ORK For a robot to physically interact with its environment, algorithms for both grasp planning and perception are required. Traditional approaches for grasp generation are often based on fitting 3D CAD models to the observed scene [14, 15]. Such an approach, however, cannot be used to grasp novel objects since it requires accurate, prior knowledge about the shape. With the advent of depth cameras, various researchers have turned towards point cloud representations for perception and grasp planning. Huebner et al. [11] showed that bounding boxes computed from point clouds can be used to grasp novel objects. In a similar vein, Jiang et al. [12] proposed a socalled grasping-rectangle representation which can be used to infer the best grasp parameters given an RGB-D image of a novel object (given an offline training step). Przybylski et al. [21] showed simulation results in which a medial axis representation of objects can be used to find successful grasps without compromising on the approximation quality. Other than boxes and spheres [17], superquadrics [9] have also been considered for grasping applications given their compactness and ability to represent many diverse shapes with a limited number of parameters. Recently, Duncan et al. presented a fast hierarchical approach to fit superquadrics online [5]. On the side of grasp generation, a popular metric used to predict grasp robustness is the  metric proposed by Ferrari and Canny [6]. While many popular grasp generators, such as GraspIt! use this metric to evaluate and refine the grasp search, it has been noted [4] that a grasp with a good metric does not translate to a robust grasp in a real-world execution. Researchers such as Hsiao [10] and Balasubramanian [1] have shown that grasps obtained using simple human heuristics can produce comparable or even better results when evaluated in a real, non-simulated environment. A real world scenario - contrary to a simulated one presents its own set of challenges: errors in perception, control and modeling must be considered and might render an optimal simulated grasp into an infeasible one. Regarding incomplete perceptual information, such as one-view point clouds for a

given object, Bohg et al. [3] proposed a simple approach that exploits the symmetry of most common household objects to predict the full shape of an object on a tabletop scenario. Following Bohg's observation that most common household objects present similar characteristics (such as symmetry, extrusion-like geometry and primitive shapes), we use them to approximate the shape of objects. This is also useful in the event of occlusion, in which a complete point cloud is not available. III. G ENERATING C OMPACT O BJECT R EPRESENTATIONS FROM S INGLE RGB-D I MAGES In this section, we present two compact representations of objects that can be generated from partial point clouds. These representations can be used to plan grasps on objects involving regions of the point cloud that are currently invisible. As a result, a wider range of grasps can be planned, including, for example, side grasps which are based on an opposition of fingers placed at the front (seen) and the back (unseen) of the object. We will first present a superquadric representation which is based on determining symmetries in point clouds. After that, we will turn towards a more detailed representation which makes use of rotational symmetries and linear extrusions to characterize an object. A. Superquadric Representation Superquadrics are a family of geometric shapes that can represent a wide range of diverse objects. The equation describing superquadrics in their canonical form can be written as x a
2 2

F (x) =

y + b

2 2

2 1

+

z c

2 1

= 1.

(1)

where a,b,c are the scaling factors along the principal axes, 1 is the shape factor of the superquadric cross section in a plane orthogonal to XY containing the axis Z, and 2 is the shape factor of the superquadric cross section in a plane parallel to XY. If a general transformation is considered, then the total number of parameters required to define a superquadric is 11 (the 6 additional being the rotational and translational degrees-of-freedom (DoFs) {x, y, z, , , }). By minimizing the error between each point and the general superquadric equation, a shape that best fits the point cloud can be obtained:
n

min
k k=0



abcF 1 (x; ) - 1

2

(2)

As mentioned in Section II, superquadrics have previously been used to generate grasp configurations for simple objects [2, 22]. Most of these approaches assume that the complete shape of the object is given or that the parameters can be learned beforehand. However, when working with depth cameras this is not a reasonable assumption to make. In recent work, Duncan et al. [5] presented a superquadric fitting

Hypotheses

Initial Estimation

Optimization

Fig. 3: The three steps used for optimizing the axis of extrusion. First, we generate hypotheses by analysing pairs of points. The resulting estimates are used to produce an initial estimate of the axis of extrusion. Finally, optimization is used to improve the extrusion axis.

choose suitable candidates for task execution. For example, detecting the axis of symmetry in a rotationally symmetric object allows us to rotate any feasible grasp around this axis. In this paper, extrusion detection is performed using a threestep approach, see Fig. 3 for an overview of the approach using rotational extrusions. In the first step, we use points from the partial point clouds to generate hypotheses for the extrusion axis. In the case of rotational extrusions, we randomly sample pairs of points and use the normal of each point to create a line. Each pair of lines is intersected and the resulting point is used as a hypothesis for the axis of extrusion. Fig. 3 shows an example for points sampled from a cylindrical object. To account for noise, we use the midpoint of the line connecting the closest points, in case the two lines do not intersect. The collected hypotheses points are then used to create an initial estimate of the axis of extrusion. To this end, we fit a line into the set of hypotheses using linear least-squares. The RANSAC [7] algorithm is further used to reduce the influence of outliers. Given this initial estimate, we perform optimization to produce a more accurate axis of extrusion. Specifically, we use the dynamic hill climbing algorithm [23] to search for an axis of extrusion which reduces the dispersion of points along the profile of the object. In every iteration, the axis of extrusion is used to rotate all points of the partial point cloud back onto a plane. We then estimate the density of the points using a kernel density estimator [20]. By maximizing the density using the hill climbing algorithm, we can reduce the dispersion of the projected points, thereby recreating the profile of the object. However, performing a kernel density estimation in each step of the optimization process is computationally expensive and does not scale to large point clouds. The following method is, therefore, a discrete approximation of the kernel density, which produced accurate results in practice while at the same time being fast. We create an approximation of the kernel density estimator by creating a grid over the projected point cloud. The number of cells used in our experiments varied between 5 and 30 cells in each dimension. For each cell i  {1, .., M } we count the number of points ci that lie within. We then calculate the average of the differences to neighbouring cells j  {1, .., N }. The overall objective function of the optimization can be

Fig. 2: An example for the superquadric fitting with symmetry analysis (middle) and without it (bottom). approach which uses a voxel representation to reduce the computational complexity of the task. We found that this approach worked well when the segmented point cloud of the object had a good viewing point (i.e. the front, side and top of the object were seen). For point clouds in which only one side of the object was seen (i.e. only front), the performance quickly deteriorated, producing fitting parameters that in many cases exceeded greatly the original dimensions of the objects. While this could be partially alleviated by hardcoding limits in the dimension of the axes, this is not practical when dealing with novel objects, for which we might not know the dimensions beforehand. Inspired by work presented by Bohg et al. [3], we added an additional pre-processing step to the superquadric minimization process. Instead of using the original point cloud as input, we generated a mirrored version (see Fig. 2) by finding an optimal symmetry plane perpendicular to the table where the object resides (for more details of this process, please refer to the original paper [3]). B. Object Completion from Extrusions Planning task-specific grasps requires information about the complete shape of the object to be manipulated. Many household objects are based on extrusions. Indeed many modelling and manufacturing systems use linear and rotational extrusions in a hierarchy to generate the models used for manufacturing. Uncovering extrusions in partial point clouds can therefore help to generate a complete point cloud from a partial observation. In addition, this knowledge can be used to create a large set of feasible grasps from which a planner can

Iteration 2
25 20 15 10

Iteration 10

Iteration 50

20

15

10

5 5 0 0 1 2 3 4 5 6 7 0

Fig. 4: Density estimation at different stages of optimization. At the beginning of the optimization, the projected points are highly dispersed. The axis of extrusion is then changed to minimize the dispersion, such that the outer profile of the object emerges as can be seen in iteration 50. On the right side we can see the object to which the profile belongs. written as E= 1 1 MN Fig. 6: Extracted object profile for the linearly extruded objects. The extracted profiles are used to create a complete point cloud. ||ci - cj || (3) and evaluate grasp quality using existing metrics. In contrast, traditional grasp quality metrics cannot be directly applied to partial point clouds. Similarly, having a complete mesh allows a grasp planner to evaluate a large variety of grasps, which can then be pruned based on task constraints. However, generating many grasps often involves repeated applications of grasp optimization methods which can be computationally demanding, in particular in the presence of many degrees-of-freedom in the robot arm and hand. Extracted shape information from extrusions can be used to improve the efficiency of this process by significantly reducing the number of degrees-of-freedom of the problem. The main insight of this section is that hand shapes during object grasping are invariant to movements along the axis of extrusion. As long as the robot hand moves along the axis of extrusion, no expensive replanning of the hand shape is necessary. In the case of linear extrusions, the robot hand can move up and down the axis of extrusion without having to change the hand shape. Similarly, in the case of rotational extrusions, the hand can be rotated around the axis of extrusion. This knowledge can be exploited during grasp generation in order to turn each single detected grasp into a continuous set of grasps. Subsequently, we present a specific example how information about extrusions can be used to reduce the dimensionality and complexity of a grasp re-planning task. Fig. 7a shows a scenario, in which a grasp is executed on a rotationally symmetric object. The grasp has a low manipulability index which is not sufficient to achieve the task constraints. Typically, this means that a new grasp and arm pose needs to be planned, which involves (sampling-based) optimization in the high-dimensional space of joint angles. Given that the grasp is performed on a rotationally symmetric object, the grasp generation can be modeled as an inverse kinematics problem where the goal is to determine an arm configuration q that is collision free. The output is constrained by the end-effector position on the object and the corresponding inverse kinematics solution. The end-effector pose x can be parametrized by (1) the rotation around the axis of extrusion  and (2) the distance along the axis of extrusion

M

N

i

j

where E is the energy to be minimized. Fig. 4 shows three iterations during the optimization of the axis of extrusion. Dark areas correspond to regions of high density of points, while lighter areas correspond to low density regions. In early iterations, the estimate of the axis does not produce a clear profile when points are projected (rotationally) onto a plane. In iteration ten, we can see that high density regions start forming. After fifty iterations, an approximate profile of the object starts to emerge. After optimization is finished, we regard the projected points as the profile of the object and rotationally extrude them around the axis of extrusion to generate a complete point cloud. Fig. 5 shows a set of household objects, the recorded depth images, as well as the reconstructed complete meshes. Given the completed point cloud, we reconstructed the meshes using Poisson surface reconstruction [13]. For the case of linear extrusions along an axis, a different method for the estimation of the initial axis of extrusion needs to be used. For linear extrusions, we compare the normal vectors of pairs of points and generate a hypothesis if the difference between the normals is below a threshold. The resulting set of hypothesis can then be clustered, such that each cluster represents a possible axis of extrusions. For example, for a box, up to six clusters can be found. Note, that in our approach we use a point cloud to represent the profile of an extrusion. For revolute objects, the profile defines the outer curve of the object, which can be rotated around the axis of extrusion to generate the complete shape. For linear extrusions, the cloud represents the basic 2D shape which can be extruded to form the object. Fig. 6 shows the extracted object profiles for objects with linear extrusions. IV. U SING C OMPACT O BJECT R EPRESENTATIONS FOR G RASP P LANNING Grasp planning greatly benefits from the completed point clouds. A complete point cloud can be triangulated and used as an input to existing grasp generation and planning algorithms. In contrast to the partial point cloud, the completed and triangulated mesh can be used to perform collision checks

Fig. 5: Reconstruction of rotationally symmetric household objects. The top row shows a photo of the object. The middle row shows the corresponding depth image recorded using a Microsoft Kinect. The bottom row shows the completed mesh. Reconstruction was performed from a single image through the analysis of extrusions. , x = pose(, ). The inverse kinematics solution q with a 7-DoF arm for an end-effector pose x can be parametrized by an additional variable  which represents the angle between the wrist-elbow-shoulder plane and the ground, q = IK (x, ). At each iteration i, the new arm position is computed using an updated grasp position from the parameter space {i-1 ±  , i-1 ±  } and the corresponding inverse kinematics parametrized by {i-1 -  , i-1 , i-1 +  }. Let P represent the full space of the variables , , and . The algorithm iteratively updates these parameters by determining which tuple leads to the maximum manipulability [19]. This is realized by solving for the following objective q i = argmax det(J (q )J T (q )) (4) set of experiments focuses on the complexity and accuracy of point cloud completion when generating compact object representations. The second set of experiments shows the application of the approach to grasp planning on a humanoid robot. The used humanoid robot is based on Schunk LWA3 arms with 7 DoF. A Schunk gripper with a maximum aperture of 7cm was used. Partial point clouds were recorded using a Microsoft Kinect camera. A. Accuracy of Fit We first analyzed the accuracy of fit of the two presented compact object representations. For extrusions, we collected a set of rotationally symmetric meshes from internet databases from which we generated partial point clouds. We then cut out a partial point cloud representing 30% of the data and simulated Kinect-like noise by adding holes and noise to the dataset. The partial cloud was then completed using the extrusion detection methods from Sec. III-B. To measure the accuracy, we compared the completed clouds to the original mesh of the object. On average, the approach produced an error (distance of points to mesh) of 2mm, where objects had a diameter between 10 - 20cm. Analysis of the extrusions required on average 200ms. For superquadric fitting we conducted a similar experiment. However, in this case we noticed larger variations in the reconstructed shapes depending on the perspective of the camera to the object. We therefore placed each object at one of five different locations in front of the camera and measured the run time of the algorithm including symmetry analysis and without it. As depicted in Tab. I, the fitting time is shorter when additional points are added via symmetry analysis. While this

{,, }P

where q = IK (pose(, ), ). The sequence in Fig. 7 shows several snapshots during this optimization. In this scenario, the robot grasps a rotationally symmetric bottle. The initial random grasp sample in Fig. 7a yields a manipulability of 0.268 which is then improved in Fig. 7d leading to a value of 0.540. To optimize the manipulability, the planner iteratively changes the grasp position on the robot with the  and  parameters, and the inverse kinematics parameter . This optimization can be performed efficiently since, the highdimensional configuration space of the hand does not need to be represented thanks to the extracted symmetries. Instead, a three-dimensional space of parameters {, , }  P is used. V. E XPERIMENTAL R ESULTS In this section, we present a set of experiments which we conducted to evaluate the proposed approach. The first

Fig. 7: Grasp manipulability optimization along the axis of extrusion. Since the object is symmetric, the same hand configuration can be rotated around the object (A-B, C-D). At the same time, the extra DOF in the inverse kinematics solution is also utilized to maximize manipulability (B-C). may seem unintuitive, we found that the superquadric shape has more constraints when considering mirrored points. As a result, the optimization process required for fitting quickly settles on a good solution. TABLE I: Comparison of fitting times
Object Apple Milk Jam Raisins Creamer Input Symmetry Plain Symmetry Plain Symmetry Plain Symmetry Plain Symmetry Plain P1 0.02 0.14 0.20 0.42 0.06 0.08 0.29 0.36 0.15 0.65 P2 0.13 0.17 0.07 0.56 0.11 0.29 0.25 0.40 0.15 0.09 P3 0.01 0.06 0.03 0.27 0.13 0.10 0.31 0.43 0.22 0.39 P4 0.06 0.06 0.05 0.53 0.08 0.08 0.14 0.43 0.13 0.26 P5 0.07 0.06 0.04 0.06 0.21 0.11 0.27 0.32 0.14 0.29 Avg. Time 0.05s 0.098s 0.078s 0.368s 0.118s 0.132s 0.252s 0.388s 0.158s 0.336s Extrusion Success SQ

to the invariance along the axis of extrusion. Images of the executed grasp and the experimental setup can be found in Fig. 8. TABLE II: Experimental results, 3 trials per object per location
Location B4 C3 C4 B4 C3 C4 B4 C3 C4 B4 C3 C4 Creamer 100% 0% 100% 100% 100% 100% 1040 800 1270 11 7 10 Dove 100% 0% 100% 100% 100% 100% 900 400 320 11 3 5 Roll 0% 0% 100% 0% 66% 0% 1200 2200 800 7 1 5 Micro 100% 0% 100% 100% 100% 100% 640 800 1020 11 7 13

Extrusion Grasps SQ

B. Robot Grasping Experiments Next, we conducted an experiment in which a humanoid robot was used to grasp household objects located in front of it. We also placed several other objects as clutter on the table. Given the depth image all objects were reconstructed using compact object representations. After that, the robot planned and executed grasps using the normal at a point as an approach direction and the method described in Sec. IV for ensuring manipulability and obstacle avoidance. We conducted trials with 4 objects which were placed at 4 different locations on the table. Each trial was repeated three times. A grasp was regarded successful if the robot was able to lift the object. Tab. II summarizes the results of the experiment. We can see that the approach using superquadrics performs well on most objects with the exeption of the roll. In contrast, the extrusionbased approach seems to have difficulties with a specific location (C3). Analyzing the robot executions, we found that superquadric approach typically leads to approximate shapes which are slightly larger than the original object. Hence, the executed grasp includes a "buffer" zone that allows it to succeed in the presence of sensor and calibration noise. Grasps planned for the shapes generated by the symmetry detection, however, are tighly fit to the object. This often lead to premature contact with the object during grasp execution. In Tab. II we also see the number of different grasps found using the two approaches. We can see that the symmetry based approach leads to a larger number of different grasps, due VI. D ISCUSSION AND C ONCLUSION In this paper we introduced methods for generating compact and complete object representations that are particularly useful for robot grasping applications. The approach exploits natural patterns found in many shapes, e.g., symmetries, linear extrusions, and rotational extrusions to generate a complete mesh from a single depth image. We also showed that the extraction of this information can be used to improve the efficiency and quality of the grasp planning step. The work presented in this paper can be seen as a first step towards shape priors that can be used by a robot to generate hypotheses about the shape of an object in invisible regions. Other cues, such as curvature and texture may also be helpful in predicting the complete shape from partial observations. At the moment the introduced approach is limited to household objects, which are often based on linear and rotational extrusions. However, it can also be extended to work in a hierarchy to complete more complex objects. In future work, we hope to investigate this aspect in more detail. The performed robot experiments showed that the approach can be used to create a variety of grasps. In particular, we can generate grasps that extend to parts of the object that are not seen. This is in contrast to other methods which limit the approach direction of the robot to the visible part of the object. We have shown in the experiments that the method can be used to reconstruct objects in a cluttered scene without prior

Fig. 8: Grasps on household objects generated via grasp planning on compact object representations. All objects on the table were reconstructed. Objects that were not grasps were regarded as obstacles to be avoided during the manipulation task. information. Yet, the additional information gained by creating complete meshes also imposes additional requirements on the accuracy of the robot controller. Planning grasps with more accurate reconstructions of the observed object means that the robot needs to be very precise in the task execution. So far, we do not have a model of the inherent sensor and actuation noise. We hope to investigate Bayesian approaches to object fitting, which would allow us to use information about the uncertainty during task execution. ACKNOWLEDGMENTS This work is dedicated to the memory of Mike Stilman, whose enthusiasm for making robots do cool things will always be remembered. R EFERENCES
[1] R. Balasubramanian, L. Xu, P. D. Brook, J.R. Smith, and Y. Matsuoka. Human-guided grasp measures improve grasp robustness on physical robot. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2010. [2] G. Biegelbauer and M. Vincze. Efficient "3d" object detection by fitting superquadrics to range image data for robot's object manipulation. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2007. [3] J. Bohg, M. Johnson-Roberson, B. Le´ on, J. Felip, X. Gratal, N. Bergstrom, D. Kragic, and A. Morales. Mind the gap: Robotic grasping under incomplete observation. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2011. [4] R. Diankov. Automated Construction of Robotic Manipulation Programs. PhD thesis, Robotics Institute, Carnegie Mellon University, 2010. [5] K. Duncan, S. Sarkar, R. Alqasemi, and R. Dubey. Multi-scale superquadric fitting for efficient shape and pose recovery of unknown objects. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2013. [6] C. Ferrari and J. Canny. Planning optimal grasps. In IEEE Int. Conf. on Robotics and Automation (ICRA), 1992. [7] M. A. Fischler and R. C. Bolles. Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 1981. [8] C. Goldfeder and P. Allen. Data-driven grasping. Autonomous Robots, 2011. [9] C. Goldfeder, P. Allen, C. Lackner, and R. Pelossof. Grasp planning via decomposition trees. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2007. [10] K. Hsiao, S. Chitta, M. Ciocarlie, and E. Jones. Contact-reactive grasping of objects with partial shape information. In IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2010. [11] K. Huebner and D. Kragic. Selection of robot pre-grasps using boxbased shape approximation. In IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2008. [12] Y. Jiang, S. Moseson, and A. Saxena. Efficient grasping from rgbd images: Learning using a new rectangle representation. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2011. [13] M. Kazhdan, M. Bolitho, and H. Hoppe. Poisson surface reconstruction. In Proc. of the Fourth Eurographics Symposium on Geometry Processing, 2006. [14] U. Klank, D. Pangercic, R.B. Rusu, and M. Beetz. Real-time cad model matching for mobile manipulation and grasping. In 9th IEEE-RAS Int. Conf. on Humanoid Robots (Humanoids), 2009. [15] D. Kragic, A. Miller, and P. Allen. Real-time tracking meets online grasp planning. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2001. [16] O. Kroemer, H. Ben Amor, M. Ewerton, and J. Peters. Point cloud completion using extrusions. In Int. Conf. on Humanoid Robots(Humanoids), 2012. [17] A. Miller, S. Knoop, H. I. Christensen, and P. Allen. Automatic grasp planning using shape primitives. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2003. [18] Niloy J. Mitra, Leonidas J. Guibas, and Mark Pauly. Partial and approximate symmetry detection for 3d geometry. In ACM SIGGRAPH 2006 Papers, SIGGRAPH '06, pages 560­568, New York, NY, USA, 2006. ACM. [19] Y. Nakamura and H. Hanafusa. Inverse kinematic solutions with singularity robustness for robot manipulator control. Journal of dynamic systems, measurement and control, 1986. [20] E. Parzen. On estimation of a probability density function and mode. The Annals of Mathematical Statistics, 1962. [21] M. Przybylski, T. Asfour, and R. Dillmann. Unions of balls for shape approximation in robot grasping. In IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2010. [22] F. Solina and R. Bajcsy. Recovery of parametric models from range images: The case for superquadrics with global deformations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1990. [23] D. Yuret and M. de la Maza. Dynamic hill climbing: Overcoming the limitations of optimization techniques. In Second Turkish Symposium on Artificial Intelligence and Neural Networks, 1993.

View publication stats

Auton Robot (2014) 36:1­3 DOI 10.1007/s10514-013-9379-3

Special issue on autonomous grasping and manipulation
Heni Ben Amor · Ashutosh Saxena · Nicolas Hudson · Jan Peters

Published online: 27 November 2013 © Springer Science+Business Media New York 2013

Grasping and manipulation of objects are essential motor skills for robots to interact with their environment and perform meaningful, physical tasks. Since the dawn of robotics, grasping and manipulation have formed a core research field with a large number of dedicated publications. The field has reached an important milestone in recent years as various robots can now reliably perform basic grasps on unknown objects. However, these robots are still far from being capable of human-level manipulation skills including in-hand or bimanual manipulation of objects, interactions with nonrigid objects, and multi-object tasks such as stacking and tool-usage. Progress on such advanced manipulation skills is slowed down by requiring a successful combination of a multitude of different methods and technologies, e.g., robust vision, tactile feedback, grasp stability analysis, modeling of uncertainty, learning, long-term planning, and much more. In order to address these difficult issues, there have been an increasing number of governmental research programs such as the European projects DEXMART, GeRT and GRASP, and
H. Ben Amor (B) Robotics and Intelligent Machines, Georgia Institute of Technology, 801 Atlantic Drive, Atlanta, GA 30332-0280, USA e-mail: hbenamor@cc.gatech.edu A. Saxena Cornell University, 4159 Upson Hall, Ithaca, NY 14853, USA e-mail: asaxena@cs.cornell.edu N. Hudson Jet Propulsion Laboratory, M/S 82-105 4800 Oak Grove Drive, Pasadena, CA 91109, USA e-mail: nicolas.h.hudson@jpl.nasa.gov J. Peters Technical University Darmstadt, Hochschulstr. 10 , 64289 Darmstadt, Germany e-mail: mail@jan-peters.net

the American DARPA Autonomous Robotic Manipulation (ARM) project. This increased interest has become apparent in several international workshops at important robotics conferences, such as the well-attended workshop "Beyond Robot Grasping" at IROS 2012 in Portugal. Hence, this special issue of the Autonomous Robots journal aims at presenting important recent success stories in the development of advanced robot grasping and manipulation abilities. The issue covers a wide range of different papers that are representative of the current state-of-the-art within the field. Papers were solicited with an open call that was circulated in the 4 months preceding the deadline. As a result, we have received 37 submissions to the special issue which were rigorously reviewed by up to four reviewers as well as by at least one of the guest editors. Altogether twelve papers were selected for publication in this special issue. We are in particular happy to include four papers which detail the approach and goal of the DARPA ARM project as well as detailed descriptions of the developed methods. We start with the paper "The DARPA Autonomous Robotic Arm (ARM) Program - Synopsis" by Hacket et al. which provides a general overview of the recent ARM project. The paper motivates the decisions made during the ARM project and gives insights into its history, structure and current state. In the paper "An Autonomous Manipulation System based on Force Control and Optimization", Righetti et al. introduce a manipulation architecture that uses force-torque control, variable compliance and optimization methods to realize robust grasping. The architecture includes components for calibration, perception, motion planning, motion primitive execution, inverse kinematics and force control, and, therefore, addresses a number of vital sub-tasks of grasping and manipulation. The efficiency of this approach was demonstrated during competitions in the DARPA ARM project,

123

2

Auton Robot (2014) 36:1­3

where Righetti and colleagues were consistently ranked among the top two teams. The system of another top team at the DARPA ARM project is described in the paper "Model-Based Autonomous System for Performing Dextrous Human-Level Manipulation Tasks" by Hudson et al. from NASA JPL. The paper discusses the challenges faced during the project and the related competition and presents system architecture for autonomous bimanual manipulation. The required perceptual capabilities including mapping, object detection and object tracking are described in detail in the first part of the paper. The second part of the paper focuses on planning, control and action execution. The paper provides important insights into the design choices that were made during the development of the system, as well as their advantages and drawbacks. In the last paper on ARM entitled "Learning of Grasp Selection based on Shape-Templates", Herzog et al. present an algorithm for selecting good grasp poses for unknown objects from point cloud data. The approach uses a local grasp shape descriptor to encode suitable grasp locations on an object. This descriptor is used together with kinesthetic teaching methods in order to create a grasp library of stable grasps. The approach also includes a learning component that allows feedback on success or failure of a grasp to be used for adapting the library. The robustness of the approach is demonstrated in an extensive experiment with a variety of household objects. The next three papers of the special issue highlight recent advances in the design of robotic actuators for grasping. The paper "Design and Control of a Three-Fingered TendonDriven Robotic Hand with Active and Passive Tendons" by Ozawa et al. presents a new three-fingered robotic hand and a set of corresponding controllers. The paper discusses important aspects in the design of tendon-driven robotic fingers and shows how active and passive tendons can be used to realize variable stiffness control. The approach was validated by demonstrating five different types of stable grasps on a variety of objects. Another type of robot grippers is presented in the paper "A Compliant Self-Adaptive Gripper with Proprioceptive Haptic Feedback" by Belizle et al. The presented gripper features compliant joints, under actuation and a haptic interface. In addition to the technical description of this actuator, Belizle and colleagues also present a theoretical model using a quasistatic analysis. Finally, they also demonstrate the advantageous features of the new gripper in extensive simulated and real experimental results. A more bio-inspired approach to the design grippers is introduced in the paper "A Variable Compliance Soft Gripper" by Giannaccini et al. The paper presents a novel tentaclelike gripper that has a large degree of variability in its shape. In addition to the shape the authors also show how the compli-

ance of the gripper can be changed according to the requirements of the task. The remaining five papers focus on the algorithmic and theoretical modelling of grasping and manipulation. In "An Active Sensing Strategy for Contact Location without Tactile Sensors Using Robot Geometry and Kinematics" Lee et al. describe new methods for locating contacts without relying on specialized sensors. A geometric estimation of the contact point between the robot actuator and the environment is used in conjunction with a control strategy in order to improve estimation accuracy. Experimental results show that the estimated forces are more accurate that those achieved using force-torque controllers. The paper "Teaching Robots to Cooperate with Humans in Dynamic Manipulation Tasks Based on Multi-Modal Human-in-the-Loop Approach" by Peternel et al. focuses on compliant robotic manipulation in the presence of a human interaction partner. In this approach, a human demonstrator first tele-operates a robot arm using a motion capture setup in order to provide training data for a subsequent imitation learning step. However, in contrast to previous work on imitation learning, not only the position of the human's hand is recorded, but also the human's muscle activation. This information is, in turn, used to modulate the stiffness of the robot. The approach is verified in a cooperative wood sawing task where a human and a robot have to collaborate. In "Autonomously Learning to Visually Detect Where Manipulation Will Succeed", Nguyen et al. turn towards active learning of visual classifiers. They introduce a methodology for predicting successful locations for manipulation based on visual features. A robot autonomously generates training data by acting in his environment. The resulting data set is then processed via dimensionality reduction and Support Vector Machines. A set of experiments with a PR2 robot show how this methodology can be used by a robot to autonomously improve the success rates during daily tasks, e.g., turning a switch. The paper "Object Search by Manipulation" by Dogar et al. addresses the question of how to search for an object in a cluttered environment. In such a situation a robot needs to push away occluding objects in order to find what it is looking for. Dogar and colleagues show that even a greedy approach to pushing objects away can be optimal under certain conditions. They also present a second algorithm which approaches polynomial time complexity and produces optimal plans under all situations. Both algorithms are evaluated on a real-world mobile robot platform. Additionally, the authors provide a Markov Decision Problem formulation of the problem and present a partial proof of optimality.` Finally, the paper "Analyzing Dexterous Hands using a Parallel Robots Framework" by Borras et al. adapts and extends an existing mathematical framework from the par-

123

Auton Robot (2014) 36:1­3

3 Nicolas Hudson is currently a member of technical staff in the Robotic Manipulation and Sampling group at the Jet Propulsion Laboratory, California Institute of Technology. He received the BE(Hons) degree from the University of Canterbury in 2002, the M.Eng degree from the California Institute of Technology in 2004, and a Ph.D. in Mechanical Engineering from the California Institute of Technology in 2009. Dr. Hudson received the 2012 NASA Early Career Achievement Medal for technical achievement in autonomous manipulation, and a 2012 NASA Group Achievement Award as part of the Autonomous Robotic Manipulation software team. He is the Task Manager of the JPL-Caltech DARPA ARM-S task.

allel robotics literature to analyze underactuacted robotic hands. The authors apply the framework to analyze a simple example hand. In this analysis they show how the underactuation design parameters such as the transmission ratio and the stiffness constants of the finger joints can modify the size of the feasible workspace. All 12 papers present significant developments in robot grasping and manipulation and we hope that you will enjoy reading them as much as we did.

Heni Ben Amor is a Research Scientist at the Robotics and Intelligent Machines Institute at GeorgiaTech in Atlanta. He received his M.Sc. from the University of Koblenz-Landau, and his Ph.D. from the Technische Universitaet Bergakademie Freiberg. Heni has been a visiting researcher with the Intelligent Robotics Group of the University of Osaka, Japan and a postdoctoral scholar with the Technical University Darmstadt, Germany. His research has won several awards such as the Bernhard-v.-Cotta Award 2011 and the CoTeSys Best Paper Award at IEEE RO-MAN 2009. Since 2011, he is also a recipient of the Daimler-Benz Fellowship. His research interests include robotics, virtual reality, machine learning, motor skill learning and human­robot interaction.

Ashutosh Saxena is an assistant professor in the Computer Science department at Cornell University. His research interests include machine learning, robotics and computer vision. He received his MS in 2006 and Ph.D. in 2009 from Stanford University, and his B.Tech. in 2004 from Indian Institute of Technology (IIT) Kanpur. He has also won best paper awards in 3DRR, RSS and IEEE ACE. He was named a co-chair of IEEE technical committee on robot learning. He was a recipient of National Talent Scholar award in India and Google Faculty award in 2011. He was named Alred P. Sloan Fellow in 2011, a Microsoft Faculty Fellow in 2012, and received a NSF Career award in 2013.

Jan Peters is a full professor (W3) at the Technische Universitaet Darmstadt and senior research scientist at the MaxPlanck Institute for Intelligent Systems. Jan Peters has received the Dick Volz Best 2007 US PhD Thesis Runner Up Award, the 2012 Robotics: Science & Systems - Early Career Spotlight, the Young Investigator Award of the International Neural Network Society, and the IEEE Robotics & Automation Society's Early Career Award. Jan Peters has received four Master's degrees in Computer Science, Electrical, Mechanical and Aerospace Engineering at TU Munich and FernUni Hagen in Germany, at the National University of Singapore (NUS) and the University of Southern California (USC) as well as a Computer Science Ph.D. from USC. Jan Peters has also been a researcher in Germany at DLR, TU Munich and the Max Planck Institute for Biological Cybernetics, in Japan at ATR, at USC and at both NUS and Siemens Advanced Engineering in Singapore.

123

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1

Learning Two-Person Interaction Models for Responsive Synthetic Humanoids
David Vogt, Heni Ben Amor, Erik Berger, Bernhard Jung Institute for Informatics TU Bergakademie Freiberg Bernhard-von-Cotta-Straße 2 09599 Freiberg {david.vogt|erik.berger|jung}@informatik.tu-freiberg.de http://humanoids.tu-freiberg.de FB Informatik TU Darmstadt Hochschulstraße 10 64289 Darmstadt amor@ias.tu-darmstadt.de www: http://www.ias.tu-darmstadt.de/Member/HeniBenAmor
 

Abstract
Imitation learning is a promising approach for generating life-like behaviors of virtual humans and humanoid robots. So far, however, imitation learning has been mostly restricted to single agent settings where observed motions are adapted to new environment conditions but not to the dynamic behavior of interaction partners. In this paper, we introduce a new imitation learning approach that is based on the simultaneous motion capture of two human interaction partners. From the observed interactions, low-dimensional motion models are extracted and a mapping between these motion models is learned. This interaction model allows the real-time generation of agent behaviors that are responsive to the body movements of an interaction partner. The interaction model can be applied both

to the animation of virtual characters as well as to the behavior generation for humanoid robots.. Keywords: Imitation learning, motor learning, motion adaptation, interaction learning, virtual characters, humanoid robots

1

Introduction

Digital Peer Publishing Licence Any party may pass on this Work by electronic means and make it available for download under the terms and conditions of the current version of the Digital Peer Publishing Licence (DPPL). The text of the licence may be accessed and retrieved via Internet at http://www.dipp.nrw.de/.
First presented at Virtuelle und Erweiterte Realit¨ at 9. Workshop der GI-Fachgruppe VR/AR 2012, extended and revised for JVRB

Generating natural behavior for synthetic humanoids such as virtual characters and humanoid robots in interactive settings is a difficult task. Many degrees of freedom have to be controlled and coordinated so as to realize smooth movements and convincing reactions to the environment and interaction partners. A common approach for the generation of human-like behaviors is based on motion capture data. Movements recorded from human demonstrators are replayed and possibly slightly altered to fit the current situation. So far, however, approaches based on real-time adaptation of motion capture data have been mostly restricted to settings in which the behavior of only one agent is recorded during the data acquisition phase. As a result, the mutual dependencies inherent to the interaction between two agents cannot be represented and, thus, reproduced. During a live interaction, a synthetic humanoid may not be able to appropriately respond to the behavior of a human interaction partner.

urn:nbn:de:0009-6-38565, ISSN 1860-2037

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1 Moreover, due to recent developments in cheap motion capture technologies, e.g. the Kinect camera, there is an increasing need for algorithms that can generate responses to perceived body movements of a human interaction partner. E.g. in gaming, a virtual character should recognize the behavior of the human player and respond with an appropriate reaction. In this paper, we present a novel approach for realizing responsive synthetic humanoids, that can learn to react to the body movements of a human interaction partner. The approach extends traditional imitation learning [BCDS08] to settings involving two persons. Using motion capture technology, the movements of a pair of persons are first recorded and then processed using machine learning algorithms. The resulting interaction model encodes which postures of the passive interaction partner have been obtained depending on postures of the active interaction partner. Once a model is learned, it can be used by a synthetic humanoid (passive interaction partner) to engage in a similar interaction with a human counter part. The presented interaction learning approach addresses not only the question of what to imitate, but also when to imitate (c.f. [DN02]). This paper is organized as follows: In Section 2 we discuss related work. In Section 3 we introduce twoperson interaction models and describe how to learn them from the simultaneous motion capture of two interaction partners. Section 4 shows examples how two-person interaction models can be applied to the behavior generation of virtual characters. In Section 5, we discuss two alternative machine learning algorithms for training our two-person interaction models. In section 6 we discuss the applicability of two-person interaction models to humanoid robots briefly before we finally conclude in Section 7. the virtual environment. The approach formulates motion tracking as an optimal control problem whereby optimization methods derive parameters of a controller for real-time generation of full-body motions. Multon and colleagues [MKHK09] present a framework for animating virtual characters, where motion capture data is adapted in real-time based on a morphology-independent representation of motion and space-time constraints. Ishigaki et al. [IWZL09] introduced a control mechanism that utilizes example motions as well as live user movements to drive a kinematic controller. Here, a physics model generates a character's reactive motions. Other work aims to adapt motion capture data through the inclusion of inverse kinematics solvers, e.g. [KHL05, MM05]. Generating natural looking motions is also a focal point in the field of humanoid robotics. Researchers have extended the concept of motion capture by introducing imitation learning techniques, which can learn a compact representation of the observed behavior [BCDS08]. Once a motion representation is learned, it can be used to synthesize new movements similar to the shown behavior while at the same time adapting them to the current environmental conditions. Imitation learning, thus, aims to combine the advantages of model-driven approaches, such as adaptability to unknown execution contexts, with benefits of data-driven approaches, such as synthesis of more natural-looking motions. In an motion generation approach presented by Calinon et al. [CDS+ 10] Gaussian Mixture Regression models and Hidden Markov models (HMMs) are used to learn new gestures from multiple human demonstrations. In doing so time-independent models are built to reproduce the dynamics of observed movements. In [YT08] the authors utilize recurrent neural networks to learn abstract task representations, e.g. pick and place. During offline training a robots' arms are guided to desired positions and the teachers' demonstrations are captured. In recent years, various attempts have been undertaken for using machine learning in human-robotinteraction scenarios. In [WDA+ 12], an extension of the Gaussian Process Dynamics Model was used to infer the intention of a human player during a table-tennis game. Through the analysis of the human player's movement, a robot player was able to

2

Related Work

Motion capture has been widely used for animating virtual characters. However, adapting the acquired motion data to new situations is a difficult task that is usually performed offline. In contrast, interactive settings as considered in our approach require the realtime adaptation of motion capture data. Some of the previous approaches for this are discussed in the following. In [YL10], for example, feedback controllers are constructed that can be used to adapt motion capture data sequences to physical pertubations and changes in urn:nbn:de:0009-6-38565, ISSN 1860-2037

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1

Figure 1: Use of two-person interaction models: Two humans' live-motions are captured, e.g. using the Kinect depth camera, and projected to a low-dimensional posture space. This space is used to learn a model of the shown interaction which is then utilized for motion generation for a synthetic humanoid in real-time. determine the position to which the ball will be returned. This predictive ability allowed the robot to initiate its movements even before the human hit the ball. In [IAM+ 12], Gaussian mixture models were used to adapt the timing of a humanoid robot to that of a human partner in close-contact interaction scenarios. The parameters of the interaction model were updated using binary evaluation information obtained from the human. While the approach allowed for human-in-theloop learning and adaptation, it did not include any imitation of observed interactions. In a similar vein, the work in [LN10] showed how a robot can be actively involved in learning how to interact with a human partner. The robot performed a previously learned motion pattern and observed the partner's reaction to it. Learning was realized by recognizing the observed reaction and by encoding the action-reaction patterns in a HMM. The HMM was then used to synthesize similar interactions. However, in all of the above approaches, the synthetic humanoid's motion is generated from motion demonstrations by only one actor. In contrast, the approach presented below draws on the simultaneous motion capture of two actors that demonstrate example human-human interactions. man actors. The interaction model is built in three steps: (1) Interactions between two persons are recorded via motion capture; (2) the dimensionality of the recorded motions is reduced; and, (3) a mapping between the two low-dimensional motions is learned.

3.1

Motion capture of two-person interactions

3

Learning Two-Person Interaction Models

In this section, a novel interaction learning method is introduced that allows virtual characters as well as humanoid robots to responsively react to the on-going movements of a human interaction partner (see Figure 1). At the core of our approach is an interaction model which is learned from example interactions of two huurn:nbn:de:0009-6-38565, ISSN 1860-2037

In the first step, the movements of two people interacting with each other are captured. In general, the presented interaction learning approach is independent of particular motion capture systems. For development and testing, we used the Kinect sensor to record joint angles. The precision of the calculated joint angles depends on the sampling rate. In general, low sampling rates lead to small datasets lacking accuracy whereas high sampling rates result in larger datasets with increased precision. Also, higher sampling rates may lead to redundant joint angle values for slow motions. For the behaviors used in this paper a sampling rate of 30fps is used. Recorded motions are very high dimensional as 48 joint angles are captured per actor. A problem that arises when recording such motions is that not all measured variables are important in order to understand the underlying phenomena [EL04]. Hence, dimensionality reduction should be applied to remove the redundant information, producing a more economic representation of the recorded motion. To this end, we use Principal Component Analysis (PCA) for construction of a low-dimensional posture space [Amo10] which will be explained further in the following section.

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1 important that every point in the input posture space can be mapped to the output posture space and not just a subset of points, e.g. points lying on the lowdimensional trajectory of the demonstrated movement. In this way, the interaction model can also map postures that are similar to but not exactly like postures seen during training. In the following, two machine learning algorithms with suitable generalization capabilities for learning such mapping functions are described. 3.3.1 Feedforward Neural Net

Figure 2: A low-dimensional posture space for a kick behavior reduced to two dimensions with principal component analysis. Every point in this space corresponds to a posture which can be reprojected to its original dimensionality and adopted by a virtual human or humanoid robot.

3.2

Dimensionality Reduction

Principal Component Analysis (PCA) is applied to the high-dimensional motion capture data to yield a lowdimensional posture space [Amo10]. PCA reduces the dimensionality of a dataset based on the covariance matrix of modeled variables. Dimensionality reduction is achieved by finding a small set of orthogonal linear combinations (the principal components) of the original variables depending on the largest variance. Previous results indicate that for most skills 90% of the original information can be retained with two to four principal components [ABVJ09]. Each point in the posture space corresponds to a pose of the synthetic humanoid as illustrated in figure 2. Accordingly, a trajectory in posture space corresponds to a continuous movement. Hence, new behaviors can be created by generating trajectories in the posture space and projecting these back to the original high-dimensional space of joint angle values [Amo10].

3.3

Two-Person Interaction Models

A two-person interaction model is the combination of two low-dimensional posture spaces with a mapping function from one posture space to the other. It is urn:nbn:de:0009-6-38565, ISSN 1860-2037

Learning input-output relationships from recorded motion data can be considered as the problem of approximating an unknown function. A Feedforward Neural Net (FNN) is known to be well suited for this task [MC01]. In our experiments, we use a simple FNN consisting of three layers, i.e. an input, a hidden and an output layer. How many neurons the hidden layer consists of and which connectivity value has to be used, depends on the recorded motion data. In general, the net should just have enough neurons to fit the data adequately while providing enough generalization capabilities for complex functions. We use 10 neurons in the hidden layer since we found that this fits the data adequately while retaining generalization capabilities for unseen low dimensional points. More neurons could increase generalization but also increase the risk of overfitting the data. For the training of a FNN Levenberg-Marquardt backpropagation is utilized and all points from the low-dimensional trajectory of the active interaction partner are used as training data. Overfitting is avoided by using early stopping. For that low dimensional data is divided into two subsets: for training and testing. The training data is used to adapt the nets weights and biases. The test data is utilized to monitor the validation error. This error normally decreases for a number of iterations. When it starts increasing again, this indicates overfitting and the training is stopped. Generally, not only the current posture but also its history is necessary to determine an appropriate response. As FNNs have no short-term memory, the input to the FNN cannot be limited to the current posture. Instead, the FNN takes as input a sequence of low-dimensional postures, i.e. the current posture and a number of history postures. The labels (output) of the net are all low-

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1

Figure 3: Multiple human postures are mapped on single robot/virtual human posture. Input to both nets (Left: FNN; Right: RNN) are the low-dimensional postures from a sliding window over time, i.e. the current posture with a history of preceeding postures. The output is the low-dimensional posture that represents the robot's/virtual human's response to the currently observed posture. dimensional points on the low-dimensional trajectory of the second, reactive interaction partner. Figure 3 shows how a sequence of postures is mapped onto a single output posture using an FNN. den neuron feeds back to all hidden neurons. This embodies the desired short term memory. The hidden layer is updated not only with the current external input but also with activations from the previous timestep. This allows the usage of smaller sliding windows and, therefore, less input neurons. In our experiments 3.3.2 Layered Recurrent Neural Network the amount of neurons in the hidden layer is the same Strict feedforward neural nets have no short term as for feedforward neural nets. memory and cannot store history postures. Desired memory effects are only created due to the way how past inputs are represented in the net. Hence, a sliding window has to be used. And, thus, the amount of inFor training, we use Bayesian regulation backpropaput neurons increases with each additional pose by the gation. Training examples are low-dimensional points number of dimensions of the low-dimensional posture of the active partner's movements and their mapping space. onto low-dimensional points on the trajectory of the To circumvent this problem we utilize layered re- reactive interaction partner (see figure 3). The trained current neural nets (RNN) [Elm90] which have proven RNN defines a continuous mapping from the input to be well suited for modeling various temporal se- posture space to the output posture space while prequences [FS02, YT08, NNT08]. RNNs have a feed- serving the temporal context of the recorded interacback activation in the hidden layer where each hid- tion. urn:nbn:de:0009-6-38565, ISSN 1860-2037

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1

Figure 4: Two different punches and defenses have been used to learn an interaction model. Left: A low punch is defended with both arms pointing downwards. Right: A high punch is defended by pulling both arms up for defense.

4

Examples

In the following, we describe two examples where two persons are recorded while performing martial arts. The first interaction is a combination of punches varying in height with proper defenses. The second scenario consists of different kicks also with suitable defenses. For both behaviors, a virtual human learns to block the attacks with the trained defenses. The motions of both the attacker and the defender start and end in an upright position with both arms stretched. The Kinect sensor was used as motion capture device in both examples.

4.1

Punching

ries are utilized to train both mapping algorithms. In our experiments a 10 point pose history for FNNs, i.e. a 0.66 second window, has proven to be well suited. For RNNs good learning results have been achieved with only two points in the history. For live-interaction with a virtual character, the user's current posture is captured at 30fps and projected into the previously created low-dimensional posture space. In combination with previous points a sequence of history poses is used as input for the mapping algorithm which predicts a low-dimensional point in the virtual character's posture space. The resulting point is then projected back to its original dimension and used as target pose for the virtual character. Figure 5 shows how the learned two person inter-

In the first example a two person interaction model is used to generalize various punches and continuously compute motions for a defending virtual character. For that, two behaviors have been recorded. The first is a sequence of low punches at stomach level where the defender stretches his arms forward to block the attack. The second behavior consists of high punches at face level where the defender had to pull up both arms for a proper defense. In the center of figure 4 both punches with their defense motion are shown. The training of the mapping algorithms is based on these two recordings. For each behavior, three repetitions of the respective punching style and their defense are captured. Then, the motions are projected into low dimensional space which can be seen in figure 4 for both interaction partners. All low dimensional points of the behavior trajectourn:nbn:de:0009-6-38565, ISSN 1860-2037

Figure 5: The image shows three punches for a user driven avatar (red skeleton) and the calculated virtual characters defense motion (blue skeleton). The user's motion on the left and right were similar to the ones used for model learning. However, the punch height in the middle has not been trained explicitly. Still, the learned model can calculate plausible motions.

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1

Figure 6: Two different kicks and defenses were used to learn an interaction model. For low kicks the defender needs to crouch down in order to reach the attackers leg. action model calculates the postures of a virtual character (blue skeleton) depending on postures obtained by a user driven avatar (red skeleton). The left and right of the figure display a newly recorded user motion similar to the ones used to train the mapping algorithm. However, the motion displayed in the center of the figure exhibits a punch height that has not been trained explicitly. Nevertheless, the interaction model can calculate suitable motions for the defending virtual character. With the learned mappings, the virtual character can defend itself against the trained attacks. A clear differentiation between punching heights has been learned. Even for varying punch heights, the virtual human can still respond with correct defense motions. of a hidden layer with 10 neurons. Addtionally, the size of the pose history has been set to 5 for FNNs and 2 for RNNs respectively. For a live interaction with the character, the user's current posture is once again fed into the mapping algorithm and an appropriate pose for the defending character is predicted. Figure 7 shows a user controlled character repeating various kicks which where similar to the ones used to drive the nets training. Additionally, a kick height (figure 7 center) not present in the recordings was performed. Since, varying kick heights of the attacker result in different low dimensional points, the virtual characters trajectories are located in the range of the previously recorded motion.

4.2

Kicking

In the second example a two-person interaction model is learned from various kicks allowing a virtual character to defend itself with suitable defenses. For that two different kicks have been recorded. The first is a low kick where the defender needed to bend its knees in order to reach the attackers' foot. The second is a higher kick which was defended with an upright position (see figure 6). For model learning both attack styles were performed three times and combined in a single animation. Since each motion started and ended in an upright position, an enclosed trajectory is created in lowdimensional space, as can be seen in figure 6. This 4-dimensional posture space contains enough information to reconstruct animations with a negligible error. The neural nets were configured to consist urn:nbn:de:0009-6-38565, ISSN 1860-2037

Figure 7: A learned interaction model is used to calculate a virtual character's defend motions (blue character) based on a user controlled avatar (red character). The kick heights on the left and right are similar to the ones obtained during training. However, the kick height in the center was not present in any recording. Nevertheless, the two-person interaction model can generate suitable motions for the defending virtual character.

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1

Figure 8: Euclidean distances are used to validate both mapping algorithm's qualities. Here, the graph shows distances for a novel punch data set. As it can be seen RNNs exhibit a smaller overall error. For all three kick heights the virtual character learned when to defend itself using a crouched defense and when to block in an upright posture. In both cases the arms need to block the attacker's leg at the right time. For similar kicks, yet with different heights, proper defenses are generated by both FNN and RNN. The interaction model that has been learned during this example allows the control of a virtual character based on demonstrations of only two variants of the behavior. The character is controlled continuously in a low-dimensional space and the model is robust to unseen user postures. Since the executed behaviors are based on human motion data, a life-like appeal of virtual characters can been produced. posture histories unnecessary. However, we found that by the inclusion of a small number history poses, i.e. two additional postures, smoother responses are generated by the RNN. Figure 9 shows the prediction for training data with various history sizes. As can be seen, the overall response time of the synthetic humanoid increases if additional postures are used to predict the character's pose. We found that 2-point histories, i.e. the original point and two previous points, represent a good compromise between fast response times and high smoothness of the generated movements. This corresponds to the works of Ziemke et al. [Zie96] which came to a similar conclusion. The combination of recurrence and a small sliding window tends to work best.

5

Comparison of Mapping Algorithms 6 Applicability to Humanoid Robots

The interaction models for the examples were learned with both mapping algorithms described above, i.e. FNN and RNN. In order to evaluate the quality of the mappings, the Euclidean distances between desired and trained low-dimensional points are measured, e.g. the training error. For that we recorded another punching example that is similar to the original human motion. A comparison of the resulting error is shown in figure 8 for the punching example. Overall, we found that both mapping algorithms produce similar results. However, FNNs tend to exhibit weaker generalization capabilities. With varying interaction learning scenarios, differing net sizes were used in order to minimize the training error. Since the required size is hard to determine beforehand a cumbersome, trial and error process is inevitable. In contrast, RNNs are more tolerant towards untrained inputs and no architecture changes have been required throughout the examples. Furthermore, RNNs maintain a short term memory, which renders urn:nbn:de:0009-6-38565, ISSN 1860-2037

Two person interaction models can also be used to control humanoid robots, although an additional processing step to adapt the resulting motion is necessary. It is generally known that human motion cannot be transfered to robots without further optimization and is generally referred to as the correspondence problem [ANDA03]. To overcome this the low-dimensional trajectory of the defender has to be altered to fit the robot's kinematic chain and stability constraints. For that we utilize an inverse kinematics solver. The two-person interaction model thus maps the original low-dimensional movement of the attacker to the adopted low-dimensional movement of the defender (robot). The learned interaction model is then used to predict robot postures depending on observed user postures. As can be seen in figure 10 a Nao robot successfully mimics the demonstrated defense behavior.

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1

Figure 9: A pose history improves the overall mapping quality. The figure illustrates how various sizes influence smoothness and response time for the punch behavior of a RNN. With increasing history sizes the net's prediction is shifted to the left, leading to larger response times but at the same time smoother motions are generated. We found that three points, i.e. the original point and two previous points in the posture history produced the best results.

Figure 10: Utilizing a two-person interaction model, a humanoid robot can react to the on-going behavior of a human interaction partner. The robot's motion is controlled by a low-dimensional mapping which preserves temporal coherences of the trained behavior.

7

Conclusion

In this paper we presented a new approach for teaching synthetic humanoids, such as virtual characters and humanoid robots, how to react to human behavior. A recorded example of the interaction between two persons is used to learn an interaction model specifying how to move in a particular situation. Two machine learning algorithms have been implemented to establish a mapping between (low-dimensional) movements of the two persons: Simple feedforward neural networks (FNN) and layered recurrent neural networks (RNN). While the FNN requires a large sliding window of current and recent body postures as input, the RNN can store previous poses in its internal state and thus can produce appropriate responses with a small sliding window of history poses. In our experiments, the RNN performed slightly better than the FNN.

ory, no explicit segmentation of the seen behavior into separate parts is required. The responses of the synthetic humanoid are calculated continuously, based on learned mapping between the body postures of the observed humans. As a result, the interaction model can generalize to different situations while at the same time producing, smooth continuous movements. Our method can be used to learn how to control synthetic humanoids from observation of similar situations between two humans. This can help to significantly increase the realism in the interaction between a robot and a human, or a virtual character and a human.

So far, however, our approach is based on joint angle data and, hence, response generation is based on similarities in joint-space. However, in order to more accurately reflect critical spatial relationships between the body parts of the interaction partners, motion synthesis could also be based on optimizations in taskIn contrast to previous approaches to interaction space. By doing so, we could also better account for modeling found in computer animation and game the- varying body heights of both interactants.

urn:nbn:de:0009-6-38565, ISSN 1860-2037

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1 An interesting addition to the current approach [DN02] would be a higher-level component for planning and strategic decision making. This could, for example, be realized through a combination of our approach and the approach of Wampler et al. [WAH+ 10]. We also hope to model multiple possible responses per stimulus. In our current setup each movement of the human [EL04] can trigger only one possible response of the virtual character. A mixture-of-experts approach [RP06], in which several interaction models are combined, can help to overcome this problem. Kerstin Dautenhahn and Chrystopher L. Nehaniv (eds.), Imitation in animals and artifacts, ch. The agent-based perspective on imitation, MIT Press, Cambridge, MA, USA, 2002. Ahmed Elgammal and Chan-Su Lee, Inferring 3D Body Pose from Silhouettes Using Activity Manifold Learning, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Los Alamitos, CA, USA), vol. 2, IEEE Computer Society, 2004, pp. 681­688, ISBN 0-7695-2158-4. Jeffrey L. Elman, Finding structure in time, Cognitive Science 14 (1990), no. 2, 179­211, ISSN 0364-0213. Eberhart E. Fetz and Larry E. Shupe, The hand book of brain theory and neural network, ch. Recurrent network: neurophysiological modeling, pp. 860­863, MIT Press, Cambridge, 2002. Shuhei Ikemoto, Heni Ben Amor, Takashi Minato, Bernhard Jung, and Hiroshi Ishiguro, Physical Human-Robot Interaction: Mutual Learning and Adaptation, Robotics Automation Magazine, IEEE 19 (2012), no. 4, 24­35, ISSN 1070-9932.

References
[ABVJ09] Heni Ben Amor, Erik Berger, David Vogt, [Elm90] and Bernhard Jung, Kinesthetic bootstrapping: teaching motor skills to humanoid robots through physical interaction, Proceedings of the 32nd annual [FS02] German conference on advances in artificial intelligence (Berlin, Heidelberg), KI'09, Springer-Verlag, 2009, pp. 492­ 499, ISBN 978-3-642-04616-2. [Amo10] Heni Ben Amor, Imitation Learning of [IAM+ 12] Motor Skills for Synthetic Humanoids, Ph.D. thesis, Technische Universit¨ at Bergakademie Freiberg, 2010.

[ANDA03] Aris Alissandrakis, Chrystopher L. Nehaniv, Kerstin Dautenhahn, and Hatfield Herts Al Ab, Solving the Correspondence Problem between Dissimilarly Embodied [IWZL09] Satoru Ishigaki, Timothy White, VicRobotic Arms Using the ALICE Imitation tor B. Zordan, and C. Karen Liu, Mechanism, Proceedings of the second Performance-based control interface for international symposium on imitation in character animation, ACM SIGGRAPH animals and artifacts, 2003, pp. 79­92. 2009 papers (New York, NY, USA), ACM, 2009, Article no. 61, ISBN 978-1[BCDS08] Aude Billard, Sylvain Calinon, Ruediger 60558-726-4. Dillmann, and Stefan Schaal, Handbook of Robotics, ch. 59: Robot Programming by Demonstration, pp. 1371­1389, MIT [KHL05] Taku Komura, Edmond S. L. Ho, and Rynson W. H. Lau, Animating reactive Press, 2008. motion using momentum-based inverse [CDS+ 10] Sylvain Calinon, Florent D'halluin, kinematics: Motion Capture and ReEric L. Sauser, Darwin G. Caldwell, trieval, Comput. Animat. Virtual Worlds and Aude G. Billard, Evaluation of a 16 (2005), no. 3-4, 213­223, ISSN 1546probabilistic approach to learn and re4261. produce gestures by imitation, Robotics Dongheui Lee and Yoshihiko Nakamura, and Automation (ICRA), 2010 IEEE [LN10] Mimesis Model from Partial ObservaInternational Conference on, 2010, tions for a Humanoid Robot, Int. J. Rob. pp. 2671­2676. urn:nbn:de:0009-6-38565, ISSN 1860-2037

Journal of Virtual Reality and Broadcasting, Volume 11(2014), no. 1 Res. 29 (2010), no. 1, 60­80, ISSN 0278- [YL10] 3649. [MC01] Danilo Mandic and Jonathon Chambers, Recurrent neural networks for prediction: learning algorithms, architectures and stability, Adaptive and learning sys- [YT08] tems for signal processing, communications, and control, John Wiley and Sons, Inc. New York, NY, USA, 2001, ISBN 0471495174. Yuting Ye and C. Karen Liu, Optimal feedback control for character animation using an abstract model, ACM Trans. Graph. 29 (2010), no. 4, 74:1­74:9, ISSN 0730-0301. Yuichi Yamashita and Jun Tani, Emergence of Functional Hierarchy in a Multiple Timescale Neural Network Model: A Humanoid Robot Experiment, PLoS Comput Biol 4 (2008), no. 3, e1000220, ISSN 0730-0301. Tom Ziemke, Radar Image Segmentation using Recurrent Artificial Neural Networks, Pattern Recognition Letters 17 (1996), no. 4, 319­334, ISSN 0167-8655.

[MKHK09] Franck Multon, Richard Kulpa, Ludovic Hoyet, and Taku Komura, Interactive an- [Zie96] imation of virtual humans based on motion capture data, Comput. Animat. Virtual Worlds 20 (2009), no. 5-6, 491­500, ISSN 1546-4261. [MM05] Michael Meredith and Steve Maddock, Adapting motion capture data using weighted real-time inverse kinematics, Comput. Entertain. 3 (2005), no. 1, 5­5, ISSN 1544-3574. Ryunosuke Nishimoto, Jun Namikawa, and Jun Tani, Learning Multiple Goal-Directed Actions Through SelfOrganization of a Dynamic Neural Network Model: A Humanoid Robot Experiment, Adaptive Behavior 16 (2008), no. 2-3, 166­181, ISSN 1059-7123. Romesh Ranawana and Vasile Palade, Multi-Classifier Systems: Review and a roadmap for developers, Int. J. Hybrid Intell. Syst. 3 (2006), no. 1, 35­61, ISSN 1448-5869.

[NNT08]

Citation David Vogt, Heni Ben Amor, Erik Berger, and Bernhard Jung, Learning Two-Person Interaction Models for Responsive Synthetic Humanoids, Journal of Virtual Reality and Broadcasting, 11(2014), no. 1, January 2014, urn:nbn:de: urn:nbn:de:0009-6-38565, ISSN 1860-2037.

[RP06]

[WAH+ 10] Kevin Wampler, Erik Andersen, Evan Herbst, Yongjoon Lee, and Zoran Popovi´ c, Character animation in twoplayer adversarial games, ACM Trans. Graph. 29 (2010), no. 3, ISSN 0730-0301, Article no. 26. [WDA+ 12] Zhikun Wang, Marc Deisenroth, Heni Ben Amor, David Vogt, Bernhard Schoelkopf, and Jan Peters, Probabilistic Modeling of Human Movements for Intention Inference, Proceedings of Robotics: Science and Systems (R:SS), 2012. urn:nbn:de:0009-6-38565, ISSN 1860-2037

2014 14th IEEE-RAS International Conference on Humanoid Robots (Humanoids) November 18-20, 2014. Madrid, Spain

Handover planning for every occasion
Ana Huam´ an Quispe Heni Ben Amor Mike Stilman

Abstract-- In this paper we explore shared manipulation tasks involving collaboration between two agents: a bimanual manipulator and a movable humanoid. We propose a deterministic planner that outputs a handover pose for the object manipulated, the grasps to be used for both agents, and arm trajectories that allow the agents to reach, grasp, interchange and place the object at its final goal pose. Most existing approaches to shared manipulation assume that both participating agents are (or can be) positioned face-to-face. In this paper, we consider a more general set of scenarios in which the relative pose between agents is not tightly constrained to facing each other. To accomplish a successful interaction between the agents, careful planning must be done in order to select the best possible handover pose such that the movable agent should not need to move its torso, only its arm, to interact with the object. We present 3 simulation experiments with manipulation tasks performed by a bimanual fixed manipulator and a humanoid robot.

Reaching a cup from front Reaching a jar from above

Reaching a hammer from below Fig. 1: Shared manipulation tasks with non face-to-face interaction between agents between examples (i.e. the humanoid does not face the fixed manipulator in neither of these examples, a fact that is commonly assumed for shared manipulation tasks). In this paper, we intend to address situations as the ones depicted, in which the movable receiver can vary its pose with respect to the fixed robot. Among some questions that must be answered to solve this problem are: A) Which arm should the fixed robot use to hand the object? B) How should the object be grasped such that the movable receiver can grasp it easily? and C) Which handover pose is good enough as to reduce the overall effort of the movable receiver?. The rest of this paper is organized as follows: Section II summarizes relevant literature on shared manipulation tasks. Section III and IV explains our algorithm, the assumptions considered and our current limitations. Section V presents simulation results in 3 different arbitrary environments and finally in Section VI we discuss our approach, its advantages and shortcomings as well as future work. II. P REVIOUS W ORK In [5] Edsinger et al. argue that fluent handover between a robot and a human allows for a range of cooperative manipulation tasks. They present a set of perception and control modules which can be used to realize a simple object exchange between interaction partners. Kemp and colleagues argue that the complimentary skills of humans and robots can be exploited in such a scenario. More specifically, the challenging task of robot grasp planning can be simplified by having the human place the object in the robot's end effector. Mainprice and colleagues [8] investigated how the spatial preferences of a human partner can be incorporated into the robot decision making during handover tasks. They
431

I. I NTRODUCTION Useful home robots should be capable to perform simple tasks that make human lives easier. In particular, we consider collaborative manipulation tasks in which a robot acts as an assistant, handing objects to a user. While inherently simple, these types of tasks are more likely to happen in a household in a regular basis, so we consider it a key robot capability. Shared manipulation tasks can be studied from different perspectives. From the side of human-robot interaction, there is a vast amount of work that analyses the information exchanged between robot and user during the handing over of the manipulated object [2]. On the other hand, the manipulation planning community frames the problem mostly from the point of view of the robot in order to select a robust grasp on the object and then find a feasible arm trajectory that allows the robot to perform its share of the task successfully. In this paper, our goal is to enable a fixed robot to effectively perform a handover task while keeping the movable receiver agent in mind. We acknowledge the movable receiver agent by considering the receiver's dexterity while placing the object at its goal pose. Furthermore, we address the fact that for different tasks, the movable receiver might not be ideally located with respect to the fixed robot (i.e. facing it), so it is the fixed robot's task to select an arm trajectory that places the manipulated object in a pose such that the receiver can easily grasp it. Depending on the specific situation, this could be naively simple or notoriously difficult. As an illustration, consider the examples shown in Figure 1. All of them involve a bimanual fixed manipulator handing over an object to a movable humanoid whose relative pose with respect to the fixed manipulator varies drastically
Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA 30332, USA.ahuaman3@gatech.edu,

hbenamor@cc.gatech.edu
978-1-4799-7173-2/14/$31.00 ©2014 IEEE

introduced a planning algorithm which can be parametrized by specifying a mobility parameter. The mobility parameter reflects the ability or willingness of the human partner to move to a new position to obtain an object. In case of a high mobility parameter, the robot can plan handover actions that require some effort from the human partner. In contrast, a low mobility parameter means that the robot should reduce the human's effort by moving to the exchange site location. In [9], they also investigated how gaze direction and postural comfort can be used to identify a transfer point In [4] Cakmak and colleagues investigated human preferences in robot handover configurations. In particular, they analyzed how humans responded to planned robot joint configurations during handover tasks. In their study they also allowed human subjects to provide positive and negative feedback to the robot on which configurations are more appropriate. This feedback was, then, used to improve the naturalness and legibility of the robot handover motion. Their experiments showed that planned handover motion can provide higher reachability of the object, while learned handover behavior can improve the naturalness and appropriateness of robot joint configurations. Koene et al. [7] showed that temporal precision plays an important role during object exchange. For fluent interaction, a robot needs to synchronize the timing of the arm movement with the arm movement of the human partner. While we acknowledge that fluency and naturalness are important characteristics for robot interactions with humans, this paper focuses on planning interactions that allow two robot agents to keep dexterous arm configurations during the handover, placing less priority to the factors mentioned above (although a high dexterity is usually correlated to smooth, humanlooking arm configurations). III. P RELIMINARIES In this section, we formally define the scenario and the shared manipulation task to be addressed through this paper. We also provide an intuitive overview of our solution approach, to be expanded in the following section. A. Problem definition Our shared manipulation task can be defined with the following elements: · A fully known 3D environment · A target object O to be manipulated. · Two agents: a fixed robot giver (Rg ) and a movable robot receiver (Rr ). In this and the following section, we will use as an example our butler scenario, whose problem definition is shown in Figure 2. B. Task definition The shared manipulation task consists in transporting the target object O from its start pose Ts to a goal pose Tg . Furthermore, we assume Ts to be only reachable by Rg and Tg reachable by Rr , hence the object must be picked up by Rg , handed off to Rr and finally placed in its final pose Tg by Rr . An example of Ts and Tg for our butler test scenario is shown in Figure 3.
432

Known scenario

Object (O)

Giver (Rg )

Receiver (Rr )

Fig. 2: Problem definition for butler scenario

Start pose Ts

Goal pose Tg

Fig. 3: Task definition for butler scenario C. Algorithm Overview At high level, our algorithm can be summarized as follows: 1) Decide which arm Rg will use to grasp the object. 2) Calculate a set of feasible grasps for Rr (Gr ) that allows it to place O at Tg 3) Calculate a set of feasible grasps for Rg (Gg ) that allows it to reach O at its start pose Ts 4) Calculate a set of feasible handover solutions H using the information provided by the set of feasible grasps Gg and Gr and the pose of the two robotic agents Rg and Rr . 5) Select the handover solution from H that maximizes the dexterity of the arms in the shared manipulation task and plan the corresponding arm trajectories. In section IV we will expand and explain each point above in more detail. Any solution from H should have information such as Figure 4: The handover pose Th and the grasps to be executed by Rg and Rr . IV. A LGORITHM In this section we will go in detail over the points briefly addressed in III-C: A. Arm selection for Rg Our planner must first determine if O is located within the reach of Rg . For this, the reachability space [6] of Rg (Rg ) is generated offline. We fill Rg by sampling a large number of random joint configurations for each arm of Rg and storing the average manipulability [11] of each sample in the voxel corresponding to the Tool Center Point (TCP) position of the end effectors (an entry of 0 means that the

Initial setup

Reach O at Ts Grasp 0/28 Grasp 11/28

Handover

Place O at Tg

Fig. 4: Expected solution for butler scenario arm cannot reach that location) . Since Rg is a (fixed) dual manipulator, RG will provide us with a manipulability metric for both left and right arms at each voxel. The object O is considered within reach of Rg if the voxel corresponding to Ts in RG has a non-zero entry for either the left arm, right arm or both. If both entries are non-zero, then the arm selected will be the one with the highest entry value, since this suggests that the corresponding arm will have more dexterity. If only one entry is non-zero, then the corresponding arm is selected. If both entries are zero then the task is considered infeasible and no further action is required. B. Generating Grasp candidates for Rr (Gr ) The next step consists on calculating a set of grasps and arm configurations that allow Rr to place O at its goal location Tg . For this step we use a set of precomputed grasps for O. After placing the object O at Tg , we test each of the grasps in Gr and prune the candidates that either not kinematically feasible or that present collision with the environment. This procedure can be seen in Algorithm 1. For our butler scenario, some of the non-pruned grasps ( Gr ) are depicted in Figure 5. Algorithm 1: PruneGrasps(R , O, G ) Input: Robot R , Object O and its corresponding grasp set G Output: A feasible subset of G (G  ) and its arm configurations (Q )
1 2 3 4 5 6

Grasp 17/28

Grasp 26/28

Fig. 5: Candidate grasps at Tg executed by Rr of these feasible grasps for our butler scenario example can be seen in Figure 6.

Grasp 0/30

Grasp 3/30

Grasp 8/30

Grasp 18/30

Fig. 6: Candidate grasps at Ts executed by Rg D. Calculation of feasible handover poses There exists an infinite number of possible poses for the object O to be handed from Rg to Rr . To perform an exhaustive search in this space would demand significant computation time. Instead, we approach the problem by reducing the possible handover poses space by considering 2 simple observations from human grasping: · When transporting an object, humans tend to reduce orientation changes in the object being grasped, unless necessary. · Whenever possible, we keep our wrist in a resting pose while manipulating an object. Given the points above, we generate a set of handover tuples (stored in H ) using Algorithm 2. Each tuple contains a handover pose for the object O, the grasps for Rr and Rg with an arm configuration that allows Rr and Rg to execute the grasp.
433

foreach g  G do q  R .executeGrasp(g , O) if checkCollision() is false then G  G  g Q  Q  q return G  , Q

C. Generating Grasp candidates for Rg (Gg ) Similar to the step above but now the object O is placed in Ts and the generated grasps are stored in Gg . An example

The specific procedure to calculate a handover grasp pose Th is shown in Algorithm 3. The translation component of Th is obtained from the set of near neighbours of the middle point between the shoulders of Rr and Rg . The voxel that can be reached from both arms and that has the highest average manipulability is chosen as the position for Th . Regarding the rotation, we take advantage of the information provided by the grasp set Gr : We calculate the pose by applying a minimum rotation between the Rr 's hand approach vector at Tg and the direction between the shoulders of Rr and Rg . A couple of example poses for our butler scenario can be seen in Figure 7. Grasp 4/29 Algorithm 2: Get set of candidate handover poses
1 2 3 4 5 6 7 8 9

Grasp 21/29

Fig. 7: Candidate handover poses during transfer phase and generate arm trajectories between them in order to reach, pick, handover and place the object. For this we use an standard IKBiRRT [3] approach. V. S IMULATION R ESULTS We set 3 simulation environments, as shown in Figure 1. All three scenarios involve 2 robots, which will be briefly described in the following subsection: A. Generalities 1) Crichton: Crichton is a bimanual fixed manipulator consisting on 2 Schunk LWA4 arms with a Schunk Dexterous Hand (SDH) attached to each last arm link. Each arm has 7 DOF, whereas the SDH possess 3 fingers and 8 DOF. A pseudo-analytical IK solver for the LWA4 arm was used for IK queries. 2) Hubo: Hubo is a humanoid robot with two 7-DOF arms. Each arm has a hand at their last link. The left hand has 3 fingers and 6 DOF whereas the right hand has 4 fingers and 8 DOF. A inverse-Jacobian IK solver for the Hubo arms was used for IK queries. 3) Simulation environment: For our simulation experiments we used DART [1]. The environment used, as well as the object and robot models, are replicas of their real hardware counterparts in order to make the transition to hardware experiments easier. 4) Grasp generation: The grasp datasets for each object were generated offline and loaded online for all the experiments shown. The numbers of grasps generated per each object are of a few hundreds, as it can be seen in Table I, hence, the selection of a handover pose and the corresponding grasps to use per each robot is not trivial. Since the grasp set generation is independent of the planning algorithm presented here, any method to generate grasps can be used. In particular, we used the common approach of evenly sampling the object's surface and set the hand's palm just above each sample, with the approach direction parallel to the normal of the object at each of these points. Other methods could be used to generate the set of precomputed grasps, such as GraspIt! [10] or other similar open source tools. Finally, all experiments were ran in an Intel Core i7 machine (1.6GHz).
434

foreach gr  Gr do Th , qr  GetHandoverPose(Rg , Rr ,O,gr ,Tg ) if Th = NULL then O.set(Th ) Rr .set(qr , gr )  Gg  PruneGrasps(Rg , Gg )  if Gr =  then  foreach gg  Gg do H  (Th ,gr , gg , qr , qg )

Algorithm 3: GetHandoverPose(Rg , Rr , O, gr , Tg ) Input: Robots, object, goal pose and grasp for Rr Output: Handover pose Th and arm conf. for Rr (qr )
1

zshoulder getVec(Rr .shoulder(), Rg .shoulder())
/* Get translation for Th */

2 3 4 5 6 7 8 9 10 11 12 13 14 15

p  Rr .shoulder() + 0.5 · zshoulder Sr  GetClosestVoxels(Rr , p) Sg  GetClosestVoxels(Rg , p) S  Sr  Sg
/* Sort according to manipulability */

S   Sort(S ) Th .trans()  S  .front()
/* Get rotation for Th */

O.setPose(Tg ) qr  Rr .executeGrasp(gr ,O) Thand  Rr .getHandPose() rot  minRot(Thand .z(), zshoulder ) Th .rotation()  rot × Thand × gr .Tho () O.setPose(Th ) qr  Rr .executeGrasp(gr , O) return Th ,qr

E. Selection of dexterous handover pose and arm trajectory generation After the step described before, our planner has a set H of handover poses with the corresponding grasps for Rr and Rg . The only remaining task is to select one of these tuples

TABLE I: Number of grasps per object
Object Cup - Butler scenario Hammer - Stool scenario Food jar - Kneeling scenario SDH Hand 316 445 448 Hubo left hand 368 590 448 Hubo right hand 368 644 448

TABLE II: Results for 100 randomized butler scenarios
Parameter measured Average Planning time Successful plans Average number of grasps for Rg Average number of grasps for Rr Average number of handover poses Values 2.31 seconds 100/100 32.36 39.77 16.64

B. Experiment 1: Butler scenario Our first test problem is depicted in Figure 8: The target object O (a green cup) is initially resting - upside down - on a cart located to the left of the fixed manipulator (Crichton). The goal is to place O upright on the middle of the tray carried by Hubo's right hand. We performed 100 tests for randomized scenarios similar to the one depicted in Figure 8. The variables randomized were: · Hubo's translation along an axes parallel to the table's rim (total range of movement 1.6 m). · Hubo's rotation around its Z axis (pointing up) (total range: 30 degrees with respect to orientation directly facing Crichton). · Tray's height: The tray held by Hubo's right hand was randomly moved up and down in a total range of 6 cm. Some statistics (such as planning time and number of average handover poses generated) are shown in Table II and 2 handover poses for different randomized examples are shown in Figure 9

C. Experiment 2: Stool scenario Our second test scenario consists on Hubo standing on top of a 3-step stool while Crichton hands it a hammer from its start pose on the table in order for Hubo to lift it in front of it with its head pointing to the right (see Figure 10).

Ts : Hammer on table Tg : Hammer pointing up (+Y) Fig. 10: Start and goal configurations for stool test case In a similar way as the butler case, here we also performed 100 randomly varied runs. The varied parameters were the location of Hubo (first, second or third step on the stool) and the pose of the stool (some translation along an axis parallel to the table longest side and a small rotation around the up axis, no larger than 20 degrees). Results regarding number of possible handover solutions are shown in Table III and some solutions for the random cases are shown in Figure 11

Ts : Inverted cup on cart

Tg : Cup upright on tray

Fig. 8: Start and goal configurations for butler test case

Random test case 1 (first step)

Butler random case 1

Random test case 2 (third step) Fig. 11: Sample solutions for random stool test cases Butler random case 2 Fig. 9: Sample solutions for random butler test cases Notice that the planning time increases for this example, this probably due to the fact that the size of the grasp sets for the hammer object is bigger than for the cup, hence the search space increases in size.
435

TABLE III: Results for randomized stool scenarios
Parameter measured Average Planning time Successful plans Average number of grasps for Rg Average number of grasps for Rr Average number of handover poses Values 4.62 seconds 100/100 56.87 143.28 89.49

TABLE IV: Results for 100 randomized kneeling scenarios
Parameter measured Average Planning time Successful plans Average number of grasps for Rg Average number of grasps for Rr Average number of handover poses Values 3.1845 seconds 100/100 30.45 72.91 39.89

D. Experiment 3: Kneeling scenario Our final test scenario is depicted in Figure 12: Hubo is kneeling and needs to be handed an object from the table, in this example O is a dog food jar whose goal pose is above the dog's plate and slightly tilted (rotated with respect to the X axis going from table towards Hubo).

Ts : Dog jar on table

Tg : Dog jar in pouring pose

Fig. 12: Start and goal configurations for kneeling test case As in the cases mentioned before, the average results of running our algorithm 100 times on this test case are shown in Table IV. The randomized parameters for these cases were the goal location of the dog food jar (position change of no more than 3 cm and roll rotation changes of up to 20 degrees).

that maximizes the dexterity of Rr while performing the final object placing. Our approach reduces its computation time by considering only a subset of all the possible handover poses. For this, it takes advantage of the grasps calculated for Rr . By using them and the direction between the shoulders of both robots, our handover pose search space is greatly reduced. We presented 3 simulated test cases in which our approach works across diverse randomized situations. While our method shows early indications of efficiency, it should be noted that it is not complete; hence, the possibility exists that our algorithm might not find a handover pose even if one exists. As future work, we are transitioning our simulation setup to our real hardware in order to validate our preliminary simulation results with experimental tests. R EFERENCES
[1] Dynamic Animation and Robotics Toolkit. http://dartsim. github.io/dart/. Accessed: 2014-07-21. [2] H. Admoni, A. Dragan, S. Srinivasa, and B. Scassellati. Deliberate delays during robot-to-human handovers improve compliance with gaze communication. In Proceedings of the 2014 ACM/IEEE international conference on Human-robot interaction, pages 49­56. ACM, 2014. [3] D. Berenson, S. Srinivasa, D. Ferguson, A. Collet, and J. Kuffner. Manipulation planning with workspace goal regions. In Robotics and Automation, 2009. ICRA'09. IEEE International Conference on, pages 618­624. IEEE, 2009. [4] M. Cakmak, S. Srinivasa, M.K. Lee, J. Forlizzi, and S. Kiesler. Human preferences for robot-human hand-over configurations. In Intelligent Robots and Systems (IROS), 2011 IEEE/RSJ International Conference on, pages 1986­1993. IEEE, 2011. [5] A. Edsinger and C. Kemp. Human-robot interaction for cooperative manipulation: Handing objects to one another. In Robot and Human interactive Communication, 2007. RO-MAN 2007. The 16th IEEE International Symposium on, pages 1167­1172. IEEE, 2007. [6] D. Kee and W. Karwowski. Analytically derived three-dimensional reach volumes based on multijoint movements. Human factors: the journal of the human factors and ergonomics society, 44(4):530­544, 2002. [7] A. Koene and M. Remazeilles. Relative importance of spatial and temporal precision for user satisfaction in human-robot object handover interactions. [8] J. Mainprice, M. Gharbi, T. Sim´ eon, and R. Alami. Sharing effort in planning human-robot handover tasks. In RO-MAN, 2012 IEEE, pages 764­770. IEEE, 2012. [9] J. Mainprice, E. Sisbot, T. Sim´ eon, and R. Alami. Planning safe and legible hand-over motions for human-robot interaction. In IARP Workshop on Technical Challenges for Dependable Robots in Human Environments, volume 2, page 7, 2010. [10] A. Miller and P.K. Allen. Graspit! a versatile simulator for robotic grasping. Robotics & Automation Magazine, IEEE, 11(4):110­122, 2004. [11] Y. Nakamura and H. Hanafusa. Inverse kinematic solutions with singularity robustness for robot manipulator control. Journal of dynamic systems, measurement, and control, 108(3):163­171, 1986.

Kneeling random test case 1

Kneeling random test case 2 Fig. 13: Sample solutions for random kneeling test cases VI. D ISCUSSION AND C ONCLUSION In this paper we have presented a planner to solve shared manipulation tasks. Given an object that must be passed on from one robot Rg to another Rr , our planner calculates a set of possible handover solutions H and selects the one
436

2014 14th IEEE-RAS International Conference on Humanoid Robots (Humanoids) November 18-20, 2014. Madrid, Spain

Learning Interaction for Collaborative Tasks with Probabilistic Movement Primitives
Guilherme Maeda1 , Marco Ewerton1 , Rudolf Lioutikov1 , Heni Ben Amor2 , Jan Peters1,3 , Gerhard Neumann1

Abstract-- This paper proposes a probabilistic framework based on movement primitives for robots that work in collaboration with a human coworker. Since the human coworker can execute a variety of unforeseen tasks a requirement of our system is that the robot assistant must be able to adapt and learn new skills on-demand, without the need of an expert programmer. Thus, this paper leverages on the framework of imitation learning and its application to human-robot interaction using the concept of Interaction Primitives (IPs). We introduce the use of Probabilistic Movement Primitives (ProMPs) to devise an interaction method that both recognizes the action of a human and generates the appropriate movement primitive of the robot assistant. We evaluate our method on experiments using a lightweight arm interacting with a human partner and also using motion capture trajectories of two humans assembling a box. The advantages of ProMPs in relation to the original formulation for interaction are exposed and compared.

Fig. 1. Illustration of two collaborative tasks where a semi-autonomous robot helps a worker assembling a box. The robot must predict what is the action to execute, to hand over the screw driver or to hold the box. Its movement must also be coordinated relative to the location at which the human worker executes the task.

I. I NTRODUCTION While the traditional use of robots is to replace humans in dangerous and repetitive tasks we motivate this paper by semi-autonomous robots that assist humans. Semiautonomous robots have the ability to physically interact with the human in order to achieve a task in a collaborative manner. The assembly of products in factories, the aiding of the elderly at home, the control of actuated prosthetics, and the shared control in tele-operated repetitive processes are just a few examples of application. Only recently, physical human-robot interaction became possible due advances in robot design and safe, compliant control. As a consequence, algorithms for collaborative robots are still in the early stages of development. Assistance poses a variety of challenges related to the human presence. For example, Fig. 1 illustrates a robot assistant that helps a human to assemble a box. The robot must not only predict what is the most probable action to be executed based on the observations of the worker (to hand over a screw driver or to hold the box) but also the robot movement must be coordinated with the worker movement. Pre-programming a robot for all possible tasks that a worker may eventually need assistance with is unfeasible. A robot assistant must be able
1 Intelligent Autonomous Systems Lab, Technische Universitaet Darmstadt, 64289 Darmstadt Germany. Correspondence should be addressed to

maeda@ias.tu-darmstadt.de
2 Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, 801 Atlantic Drive, Atlanta, GA 30332-0280, USA. 3 Max Planck Institute for Intelligent Systems Spemannstr. 38, 72076 Tuebingen, Germany

to learn the interaction and to adapt to a variety of unforeseen tasks without the need of an expert programmer. Motivated by the described scenario, this work proposes the use of imitation learning [1] in the context of collaboration. Imitation learning has been widely used as a method to overcome the expensive programming of autonomous robots. Only recently, however, its application for physical interaction has been introduced under the concept of Interaction Primitives (IP) by Lee at al. in [2], defined as skills that allow robots to engage in collaborative activities with a human partner by Ben Amor et al. in [3]. Leveraging on the framework of [3], our approach is based on probabilistically modeling the interaction using a distribution of observed trajectories. We propose using Probabilistic Movement Primitives (ProMPs) [4] for modeling such a distribution. In a manufacturing scenario such a distribution of trajectories can be obtained by observing how two coworkers assemble a product, several times throughout the day, providing a rich data set for imitation learning. Such a collection of trajectories is used to create a prior model of the interaction in a lower dimensional weight space. The model is then used to recognize the intention of the observed agent and to generate the movement primitive of the unobserved agent given the same observations. The movement primitive of the unobserved agent can then be used to control a robot assistant. The main contribution of this paper is the introduction of the Probabilistic Movement Primitives [4] in the context of imitation learning for human-robot interaction and action
527

978-1-4799-7173-2/14/$31.00 ©2014 IEEE

recognition. We will show how Interaction ProMPs can be applied to address the three main problems previously illustrated in Fig. 1, that is: (a) learning a collaborative model by imitation learning and thus avoiding expert programming, (b) the ability to recognize a task by observing the worker, and (c) the coordination of the assistant movement in relation to the worker movement. We also show the advantages of ProMPs over the original DMP-based framework [3], and present an algorithm for aligning data using local optimization in order to avoid the issue of slope constraints typical of dynamic time warping. II. R ELATED W ORK The data-driven analysis and classification of interactions between multiple persons has long been addressed within the computer vision community. In particular visual surveillance tasks, e.g., tracking of multiple pedestrians, require methods for identifying the occurrence and type of person-to-person interactions. In a seminal paper, Oliver et al. [5] show that hidden Markov models (HMMs), and more generally graphical models, are suited for representing the mutual dependencies of the behaviors between interacting agents. Graphical models have gained popularity in the field of human-robot interaction as they naturally include temporal information into the inference process and the Bayesian semantics provides a simple way to encode prior knowledge. In [6], Lee et al. use a hierarchical HMM to learn and represent responsive robot behaviors. In their approach, a high-level HMM identifies the current state of the interaction and triggers low-level HMMs which correspond to the robot's motor primitives. In order to ensure that the robot adapts to the movement of the human partner, virtual springs are attached between markers on the human body and corresponding positions on the robot. In a similar vein, Ben Amor et al. [7] use a path-map HMM to model interactions in cooperative tasks. In their approach, a backbone of shared hidden states correlates the actions of the interacting agents. Tanaka et al. [8] use a Markov model to predict the positions of a worker in an assembly line. The space in which the worker moves is discretized into different regions. A Gaussian mixture model relates positions to procedures. Using this information a robot, then, delivers tools and parts to a human worker along the assembly line. Besides HMMs, other probabilistic graphical models have also been used to address interaction tasks. Koppula et al. [9] use a conditional random field with sub-activities, human poses, object affordances and object locations over time. Inference on the graphical model, allows a robot to anticipate human activity and choose a corresponding, preprogrammed robot response. Wang et al. [10] propose the intention-driven dynamics model, which models human intentions as latent states in graphical model. Intentions can be modeled as discrete variables, e.g., action labels, or continuous variables, e.g., an object's final position. The transitions between latent states and the mapping from latent states to observations are modeled via Gaussian Processes. As evidenced by these works, graphical models can be very powerful tools
528

in classifying interactions. However, this often requires a substantial set of training data. In particular for humanoid motion generation with many degrees-of-freedom, it is often challenging to acquire sufficiently large and general data sets. For more efficient learning and generalization, various authors investigated the projection of the original trajectories into a new, low-dimensional space where correlations between the agents are easier to unravel. Llorens et al. [11] show how such a low-dimensional interaction space can be used to implement an assistive robot arm. Similarly in [7], probabilistic principal component analysis is used to find a shared latent space. Dynamic Movement Primitives (DMPs) [12] allows for a low-dimensional, adaptive representation of a trajectory. The general idea is to encode a recorded trajectory as dynamical systems, which can be used to generate different variations of the original movement. In the context of interaction, Prada et al. [13] present a modified version of DMPs, that adapts the trajectory of one agent to a time-varying goal. By setting the goal to the wrist of another agent, the method can be used to generate handover motions. Although graphical models and HMMs have been successfully used for action and intention recognition in a discretized symbolic level, the generation of trajectories for the continuous dynamic control of the robot is usually addressed by a different level of representation (e.g. a lowerlevel HMM [6] or DMPs). In relation to the previously cited works, here, we propose a framework based solely on a continuous movement representation that is used to both recognize actions and generate trajectories in the form of movement primitives; mainly leveraging on DMP-based Interaction Primitives [3] and Probabilistic Movement Primitives (ProMPs) [4]. By using ProMPs rather than DMPs our prosed method naturally correlates different agents directly in the same space in which observations are made, since observations of a task are usually given by their trajectories. This is an advantage in relation to the original framework of [3] since the representation of collaboration in the space of accelerations/forces due to the use of DMPs obfuscates the algorithm and increases its sensitivity to noise in the observations. III. P ROPOSED M ETHOD This section briefly introduces Probabilistic Movement Primitives for a single degree of freedom as presented in [4] and proposes its extension for interaction and collaboration. Although not covered in this work, in its original proposition, the design of a feedback controller that tracks the distribution of trajectories is also part of ProMPs and the interested reader is referred to [4] for details; here we assume the existence of a human-safe standard feedback controller such as a low-gain PD controller. This section also exposes the main characteristics of the interaction framework based on DMPs in [3] and its relation to the approach of this paper. Finally, a simple local optimization algorithm is proposed for aligning several demonstrations provided by a human.

A. ProMPs for a Single DOF For the purposes of the following derivations we generically refer to each joint or Cartesian coordinates of a human or robot simply as a degree of freedom (DOF) with position q and velocity q . Starting with the case of a single DOF, we denote y (t) = [q (t) q (t)]T and a trajectory as a sequence  = {y (t)}t=0,...T . We adopt linear regression with n Gaussian basis functions  . The state vector y (t) can then be represented by a n-dimensional column vector of weights w as  (t) q (t) =  y (t) = w + y, (1) q (t)  (t)  (t)]T is a 2×n dimensional timewhere t = [ (t),  dependent basis matrix and y N (0, y ) is zero-mean i.i.d. Gaussian noise. The probability of observing the whole trajectory is then
T

where w ¯d is the augmented weight vector corresponding to the d-th demonstration, wp is the n-dimensional column vector of weights of the p-th DOF of the observed agent, and wq is the vector of weights of the q -th DOF of the controlled agent. The mean and covariance are then computed by stacking all demonstration weights µw = mean([w ¯1 , ..., w ¯d , , ..., w ¯D ]T ), w = Cov([w ¯1 , ..., w ¯d , , ..., w ¯D ]T ), (5)

where D is the number of demonstrations. Gaussian conditioning can then be applied on-line as each new observation is made using recursive updates in the form
-  T - µ+ w = µw + K (y (t) - Ht µw ) - T - + w = w - K (Ht w ) T  T + -1 K = - , w Ht (y + Ht w Ht )

(6)

p( |w) =
0

N (y (t)|t w, y ).

(2)

Similar to DMPs the speed of the execution of the movement is decoupled from the speed of the original trajectory by using a phase variable z (t). The phase variable replaces the time in order to control the location of the basis functions with  (z (t)). For simplicity we will use z (t) = t such that  (t) =  (z (t)) while remembering that any monotonically increasing function can be used [4]. Each trajectory is now represented by a low-dimensional vector w since the number of basis is usually much smaller than the number of time steps. Trajectory variations obtained by different demonstrations are captured by defining the distribution over the weights p(w| ), where  is the learning parameter. The probability of the trajectory becomes p( | ) = p( |w)p(w| )dw. (3)

where K is the Kalman gain matrix, y  (t) is the observed value at time t,  y is the measurement noise, and the upperscripts - and + the values before and after the update. The observation matrix Ht is block diagonal and each diagonal  (t)]T for each observed entry contains the 2×n basis [ (t),  joint   t . . . 0   . . .. . (7) Ht =  . . . .  0 ... t In the collaboration case only measurements of the observed agent are provided. By maintaining consistency with definition (4) where the entries of the observed agent come before the controlled agent, the mean is then µw = T [µo µc w w ] and the observation matrix Ht is partitioned as   (o 0 0 0 t )(1,1)    0 (o 0 0  t )(P,P )   Ht =   (8) 0 0 0 0c   (1,1)   0 0 0 0c (Q,Q) where each zero entry is of 2×n dimension. Note that if only positions of the observed agent are provided (o t )(p,p) = [ (t), 0(t)]T . In general, since (6) is a full state linear estimator, any partial combination of observations (for example when y  only contains positions, or velocities, or a mixture of both) provides the optimal estimate of states µ+ w and their uncertainty + . w C. Action Recognition for Primitive Activation Here we use the ProMP framework in a multi-task scenario where each task is one encoded by one interaction primitive. Consider a specific task s  {1, .., K } and assume that for each task an Interaction ProMP has been generated as it was proposed in section III-B. Using the recursive notation of (6), the upper script (·)- refers to the parameters of the Interaction ProMP updated up to the previous observation, - - that is s = {µ- w , w }s . The probability of the observation
529

So far  captures the correlation among the weights within the trajectory and between demonstrations of the same DOF. B. ProMPs for Collaboration The key aspect for the realization of the interaction primitives is the introduction of a parameter  that captures the correlation of all DOFs of multiple agents. Assuming that the distribution of trajectories of different agents is normal, then p(w;  ) = N (w|µw , w ). Under this assumption we redefine the vector of weights w to account for all degrees of freedom of multiple-agents. Following the definitions in [3] we will refer to the assisted human as the observed agent, and assume that he/she provides the observed DOFs of the model (e.g by motion capture). The robot will be referred to as the controlled agent. For an observed agent with P DOFs and a controlled agent with Q DOFs, we construct a row weight vector by concatenating the trajectory weights
T T T o T T T c w ¯d = {[w1 , ...wp , ..., wP ] , [w1 , ...wq , ..., wQ ] } (4)

- at a subsequent time t given the parameters s of one of the tasks is - p(y  (t); s )= - p(y  (t)|t w,  y )p(w |s )dw

E. Time Warping with Local Optimization One issue of imitation learning for trajectories is that multiple demonstrations provided by humans are usually, sometimes severely, warped in time. Demonstrations must be unwarped or time-aligned before the distribution of the weights can be computed. Here we propose aligning trajectories by taking one of the demonstrations as a reference yr and using local optimization of the time warping function with j +1 + g (v j )tj tj (15) = v0 w, w where tj w represents a vector containing the warped time of demonstration yw at the j -th iteration of the optimization. We propose g as a smooth, linear Gaussian-basis-model j j , ..., vP ] as the parameters to be with P weights v j = [v1 j is used to shift the time optimized. The extra parameter v0 which is useful when the reference and warped trajectories are, in fact, identical but start at different instants. The j = 0 and tj optimization is initialized with v0 w = tr for j j = 1. The parameters v are optimized with gradient descent to decrease the absolute cumulative distance between the reference and warped trajectories
K

(9)

-  = N (y  (t)|t µ- w , t w t + y ). (10)

The task s can now be recognized by applying Bayes rule p(s|y  (t)) =
- p(y  (t)|s )p(s) K k=1 - p(y  (t)|k )p(k )

,

(11)

where p(s) is the initial probability of the task (e.g. p(s) = 1/K for uniform distribution). We will evaluate Eqs. (9)-(11) using real collaboration data in the experimental section of this paper. D. Relation to Interaction DMPs It is now straightforward to relate our proposed method with the previous interaction primitives based on DMPs [3]. The principal difference is that in the framework of interaction DMPs the weights are mapped from the forcing function f (t) as opposed to the positions q (t). Using the linear basis-function model f (t) =  (t)T w, (12)

v = arg min
v k=0

j |yr (tr (k )) - yw (v0 + g (v j )tj w )|.

(16)

where  (t) are the normalized Gaussian basis functions. Similarly to the ProMP case a distribution of weights p(w) is learned based on several demonstrations of a task. For each DOF, the forcing function adds a nonlinear behavior on the movement which complements a linear and stable spring-damper system q ¨ = [y (y (g - q ) - q/  ) + f (t)] 2 , (13)

While Dynamic Time Warping (DTW) [14] is widely used for such problems, our local method forces alignment without "jumping" the indexes of the warped time vector which is an usual outcome of DTW and renders unrealistic and non-smooth trajectories. While this problem is usually minimized by imposing a slope constraint [14], the use of a smooth function g not only avoids the tunning of this parameter but also preserves the overall shape of the trajectory. IV. E XPERIMENTS This section presents results on a simple simulated scenario to compare the differences between the original work of Interaction DMPs with Interaction ProMPs. Next, we evaluate the accuracy of Interaction ProMPs for generating reference trajectories for an anthropomorphic robot arm conditioned on the movement of a human. Finally, we will show experimental results with Interaction ProMPs used in a collaborative scenario of a box assembly to both recognize and predict the action of two collaborators. A. Comparison with Interaction DMPs In a typical interaction task the observations of a coworker might arrive asynchronously, at irregular periods of time, for example, when the measurement signal is prone to interruption (a typical case is occlusion in motion capture systems). Fig. 2 (a) illustrates a simple case where both observed and controlled agents have a single joint each. The training data was created by sketching two sets of trajectories on a PC screen using a computer mouse. We than use these two sets as proxies of the observed and controlled agents resulting on the initial distribution of trajectories (in blue). The upper
530

where g is the goal attractor, y , y are user-defined parameters that characterize the spring-damper behavior and  controls the speed of execution. For details on DMPs please refer to [12] and references therein. When using imitation learning a demonstration is executed and measurements are usually given in the form of positions, which must be differentiated twice such that the forcing function can be computed f (t) = q ¨/ 2 - y (y (g - q ) - q/  ). (14)

Referring back to (6) the Gaussian conditioning is now based on the observation of forces or accelerations, that is y  (t) = f (¨ q , (·), t) . As our evaluations will show, the fact that forces are usually computed using second derivatives of the position can be restrictive for applications with asynchronous or sparse measurements as the observed accelerations needed for conditioning are hard to obtain in this case. In contrast, in the ProMP framework, it is possible to directly condition on the observed quantities, i.e., the position of the agent.

Initial distribution
Agent A (Observed) 0.8 X amplitude X amplitude 0.6 0.4 0.2 0 0.8 0.6 0.4 0.2 0

Current prediction
Agent A (Observed)

Training

-0.2 0 0.5

Sparse observations
1.5 2 2.5 Time (s) Agent B (Controlled) 1 3

-0.2 0 0.5 1

Noisy observation
1.5 2 2.5 Time (s) Agent B (Controlled) 3

Test

0.8 X amplitude X amplitude 0.6 0.4 0.2 0 0 0.5 1 1.5 2 Time (s) 2.5 3

0.8 0.6 0.4 0.2 0 0 0.5 1 1.5 2 Time (s) 2.5 3

(a) Z

-0.2

-0.2

(a)

(b)

Y
Fig. 2. Two scenarios where the Interaction ProMPs are advantageous over Interaction DMPs. (a) Sparse and asynchronous observations. (b) Noisy stream of observed data ( 2 = 0.04). The patches represent the ± 2 deviations from the mean.
Interaction DMP 0.16 0.14 0.12 RMS prediction error 0.1 0.08 0.06 0.04 0.02 0 RMS prediction error 0.16 0.14 0.12 0.1 0.08 0.06 0.04 0.02 0 Interaction ProMP var:0 var:0.01 var:0.02 var:0.03 var:0.04

(b)
Fig. 4. An interactive task where the robot has to point at the same position previously pointed by the human. The robot, however, has no exteroceptive sensors and its predicted trajectory is based solely on the correlation with the observed human movement. (a) The nine positions used to create the Interaction ProMP (dot markers) and the extra nine positions used to test the method (cross markers). (b) An example where the human points at the test position #1 and the robot points to the same position.

20

40 60 80 100 Observed trajectory (%)

20

40 60 80 100 Observed trajectory (%)

Fig. 3. Root-mean-square prediction error of the movement of collaborator B as a function of the number of observed samples of the trajectory of collaborator A. The different bars indicate the amount of noise added to the observation of the position of collaborator A.

plot shows the prediction (in green) of the observed agent after observing four measurements. Note that following (4) the predicted mean µ+ w has the dimension of the augmented weight vector, that is, if each single-DOF agent trajectory is encoded by n basis functions µ+ w is a column vector of size 2n. The bottom figure depicts the predicted distribution of the controlled agent. Note that the same experiment can not be reproduced with Interaction DMPs as the second derivative on such sparse measurement is hard to compute and introduce innacuracies on the representation of the true force. In Fig. 2 (b) the ProMP is being conditioned on a constant synchronous stream of noisy position measurements. The plot shows the case where the true trajectory is corrupted with a Gaussian noise with variance  2 = 0.04. Interaction DMPs suffer from noisy position measurements as the observation must be differentiated twice to compute the forcing function. While low-pass filters alleviate this problem, the introduction of phase lag is an issue that can be potentially
531

avoided with ProMPs. Fig. 3 compares the prediction error over the whole trajectory of Interaction DMPs and ProMPs given the same noisy observed data. With DMPs the error is greatly influenced by the amount of noise while ProMPs show much less sensitivity. For the case where the full trajectory of collaborator A is observed (indicated by the arrow) the prediction error increased by a factor of five times using the Interaction DMPs when noise ranged from a clean signal to a signal of noise variance 0.04. In contrast, the error deteriorates by a factor of two with Interaction ProMPs. B. Robot Control with Interaction ProMPs We evaluated the ability of Interaction ProMPs in generating the appropriate movement primitive for controlling a robot based on observations of a human partner. The experiment consisted in measuring the [x, y, z ] trajectory coordinates of the wrist of an observed agent via motion capture1 while pointing at a certain position on a table placed in front of our robot (a 7-DOF anthropomorphic arm with a 5-finger hand). Then, the robot was moved in gravity compensation mode to point with its index finger at the same position on the table while its joint positions were being recorded (kinesthetic teaching). This pair of
1 All human positions were measured in relation to the world reference frame located at the torso of the robot (as shown in Fig. 4(b)).

Initial distribution
1 0.95 0.9 x (m) y (m) 0.85 0.8 0.75 0.7 0.65 0 1 2 Time (s) 3 0.5 0.4 0.3

Prediction

Observed
-0.15 -0.2

Predicted robot trajectory
1 0.95 0.9 x (m) 0.85 0.8 0.75 0.7 0.65 0 1 2 Time (s)
0 -20 q4(deg) -40 -60 -80

Initial distribution
0.5 0.4 0.3 y (m)

Prediction

Observed
-0.15 -0.2 z (m) -0.25 -0.3 -0.35

Predicted robot trajectory

Human

z (m)

0.2 0.1 0 -0.1 -0.2 0 1 2 Time (s)
120 100 q3(deg) 80 60 40 2 4 Time (s) 6 20 0 2

-0.25 -0.3 -0.35

0.2 0.1 0 -0.1

3

-0.4

3

0

1

2 Time (s)

3

-0.2

0

1

2 Time (s)
120 56.99 100 56.985 q3(deg)

3

-0.4

0

1

2 Time (s)

3

60 40

80 70 q2(deg) 60 50 40 30 2 4 Time (s) 6 20 0

120 60 110 40 q5(deg) q1(deg) q6(deg) 100 20 90 0 -20 80 0 0

-22 80 -24 70 q2(deg) q7(deg) -26 60 -28 50 -30 40 2 2 4 4 6 6 Time Time (s) (s) -32 30 0 0 2 2 4 4 6 6 Time Time (s) (s)

50 0 -20 45 x(m) q4(deg) y(m) q5(deg) 2 2 4 4 6 6 Time Time (s) (s) -40 40 -60 35 -80 30 2 2 4 4 6 6 Time Time (s) (s) -100 25 0 0

Robot

q1(deg)

56.98 80 56.975 60 56.97 40 56.965 20 0 0

20 0 -20 0

-10 -15 -20 -25

4 Time (s)

6

-100 0

2

4 Time (s)

6

(a)

(b)

Fig. 5. The upper row shows the human Cartesian coordinates of the wrist. The bottom row shows the first four joints of the 7-DOF robotic arm. (a) Conditioned results of the test position #6. (b) Conditioned results of the test position #8.

trajectories formed by the Cartesian positions of the wrist and the joint positions of the arm where then mapped into the weight space and concatenated as in Eq. (4). In total, nine different positions were pointed to collect training data, sparsely covering an approximate circular area of diameter 30 cm. The pointed positions are shown in Fig. 4(a) by the dots. After creating the Interaction ProMPs, as described in Section III-B, we defined extra nine marked positions shown in Fig. 4(a) by the crosses. The human then pointed at one of the crosses while motion capture was used to measure the trajectory of the wrist. These observations were then used to condition the Interaction ProMP to predict trajectories for each joint of the robot, whose mean values where used as reference for a standard trajectory tracking inverse dynamics feedback controller with low gains. Fig. 4(b) shows one example of the interactive task where the human pointed to the position marked by the cross #1, which was not part of the training; the robot was capable of pointing to the same position. Note that the robot was not provided with any exteroceptive feedback, such as cameras, to reveal the location of the pointed position. Although the robot was not directly "aware" of the position of the pointed cross, the interaction primitive provides the robot the capability to predict what movement to make based solely on the observed trajectories of the partner. Figure 5 shows two examples on the conditioned interaction primitives when the human pointed at positions #6 in (a) and #8 in (b) (refer back to Fig. 4(a) for the physical position of the crosses). The first row in each subplot shows the [x, y, z ] coordinates of the wrist. The second row shows the first four joints of the robot, starting from the shoulder joint. Since we are only interested in the final pointing position, the interaction primitive was conditioned on the final measurements of the wrist position. As positions #6 and #8 were physically distant from each other, the difference between their predicted trajectories were quite large in relation to each other, roughly covering the whole span of the prior
532

XY pointing error (cm)

3 2.5 2 1.5 1 0.5 0 0 1 2 3 4 5 6 Test number (#) (Cross markers) 7 8 9 10

Fig. 6. Accuracy of the pointed positions by the robot when using the test positions given by the cross markers. The error was computed by taking the actual pointed position and the true position of the markers on the table.

distribution (in blue) for some certain DOFs of the arm. Figure 6 shows the distance error on the plane of the table between the position pointed by the robot and its true position. The robot was able to reasonably point even at locations at the limits of the training data such as position #1, #7, and #8 (see Fig. 4). The maximum error was of 3 cm, or 10% in relation to the total area covered by the training points (approximately a circle of diameter 30 cm). The experiments show that the physical movement of the robot is clearly conditioned by the position indicated by the human (see the accompanying video2 ). Note that this not a precision positioning experiment; the markers on the wrist were not fixed in a rigid, repeatable manner, neither the finger of the robot could be positioned with milimetric precision during the kinesthetic teaching phase. The framework of Interaction ProMPs allows, however, to seamlessly integrate additional sensing to increase accuracy in precision tasks. This is naturally achieved adjusting the observation vector y  in (6) and the zero entries in (8) to include new sensory information such as the reference position of a hole in which the robot must insert a peg. C. Action Recognition for Primitive Activation While in the previous experiments interaction primitives were evaluated for the case of a single task, here we show
2 also

available from http://youtu.be/2Ok6KQQQDNQ

Agent A Hand over

Agent B

Agent A

Agent B Screwing Start Start Holdiing box Reaching box

Agent A

Agent B

Start

Pick screw driver

Grasping plate

Start

Pick screw

Box flipped Start Start

(a) Hand over

(b) Fastening

(c) Plate insertion

Fig. 7. Three collaborative tasks involved when assembling a box by two co-workers. From left to right, the photos show the hand over of a screw, the fastening of the screw where one agent grasps the screw driver while the other holds the box steadily, and the insertion of a plate, which requires one agent to flip the box such that the slot becomes accessible to the other agent. The distribution of aligned demonstrations for each task are shown under their respective photos. The plot shows the covariance in the x-y plane at each corresponding z height.

how Interaction ProMPs can be used for recognizing the action of the observed agent and to select the appropriate desired movement primitive of the controlled agent. This capability allows the robot to maintain a library of several tasks encoded as Interaction ProMPs and to activate the appropriate primitive based on the observation of the current task. As shown in the photos of Fig. 7, we collected collaborative data in the form of the Cartesian coordinate positions of the wrists of two humans assembling a box. As in the previous experiment of section IV-B, all measurements were taken in relation to the torso of the robot. The collaborator on the right plays the role of the observed agent while the collaborator at the left plays the role of the controlled agent. The controlled agent will be referred to as the predicted agent since he/she can not be controlled. In the "hand-over" task shown in Fig. 7(a), the observed agent stretches his hand as a gesture to request a screw. The predicted agent then grasps a screw sitting on the table and hand it over to the collaborator. In the "fastening" task shown in Fig. 7(b), the observed agent grasps an electrical screwdriver. The predicted agent reacts by holding the box firmly while the observed agent fastens the screw. In the "plate insertion" task shown in Fig. 7(c), the observed agent grasps the bottom plate of the box. The predicted agent then flips the box such that the slots to which the plate slides in are directed towards the observed agent. Each task is repeated 40 times. All trajectories are aligned using the method described in Section III-E. The aligned trajectories are shown in Fig. 7 under their respective photos as three-dimensional plots for each of the tasks where the covariance in x-y directions are shown at the corresponding
533

Initial distribution
70 60 50 40 30 20 0 60 55 50 x (cm) y (cm)

Current prediction
10 0 -10 -20 -30 -40 0

Observed

40 35 30

2

4 Time (s)

6

25 0

2

4 Time (s)

6

z (cm)

45

2

4 Time (s)

6

(a) Interaction model: fastening task
80 70 60 x (cm) y (cm) 50 40 30 20 0 2 Time (s) 4 70 60 50 40 30 20 10 0 2 Time (s) 4 z (cm) 10 0 -10 -20 -30 -40 0

2 Time (s)

4

(b) Interaction model: hand over task

Fig. 8. Action recognition based on conditioning the movement primitives of the observed agent. In this example the observations of the fastening task also overlaps with the primitives of the hand over task.

heights (Z direction) of the movement. Interaction ProMPs are created for each task using the distribution of aligned trajectories. We evaluated action recognition using Eqs. (9)-(11) on the three presented tasks for box assembly. Fig. 8 shows one evaluation as an example. Note from the figure that the majority of observations indicate that the fastening task is taking place. The last five observations (surrounded by the ellipse), however, fits both tasks and could be a potential source of ambiguity in task recognition. Even in those

Likelihood of the task

0.8 0.6 0.4 0.2 0
Probability of the task 1 0.8 0.6

0.8 hand_over 0.6 fastening 0.4 plate_insertion 0.2

hand_over fastening plate_insertion

fastening plate_insertion

0 0 20 40 60 80 0.4 0 20 40 60 80 100 Number of observations 0.2 60 80Number 100 of observations 0 Number of observations
0 2 4

100

(a)

6 8 10 Number of observations

12

14

16

automatically generate different Interaction ProMPs without a priori hand labeling of multiple tasks. We are also investigating tasks in which some of the involved DOFs do not correlate linearly and also when certain tasks do not induce correlation. The later is especially true for tasks where the movement of the agents are not related by causality. VI. ACKNOWLEDGMENTS The research leading to these results has received funding from the European Community's Seventh Framework Programmes (FP7-ICT-2013-10) under grant agreement 610878 (3rdHand) and (FP7-ICT-2009-6) under grant agreement 270327 (ComPLACS). The authors would like to acknowledge Filipe Veiga, Tucker Hermans and Serena Ivaldi for their assistance during the preparation of this manuscript. R EFERENCES
[1] S. Schaal, "Is imitation learning the route to humanoid robots?" Trends in cognitive sciences, vol. 3, no. 6, pp. 233­242, 1999. [2] D. Lee, C. Ott, and Y. Nakamura, "Mimetic communication model with compliant physical contact in humanhumanoid interaction," The International Journal of Robotics Research, vol. 29, no. 13, pp. 1684­ 1704, 2010. [3] H. Ben Amor, G. Neumann, S. Kamthe, O. Kroemer, and J. Peters, "Interaction primitives for human-robot cooperation tasks," in Proceedings of 2014 IEEE International Conference on Robotics and Automation (ICRA), 2014. [4] A. Paraschos, C. Daniel, J. Peters, and G. Neumann, "Probabilistic movement primitives," in Advances in Neural Information Processing Systems (NIPS), 2013, pp. 2616­2624. [5] N. Oliver, B. Rosario, and A. Pentland, "A bayesian computer vision system for modeling human interactions," Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 831­843, Aug 2000. [6] D. Lee, C. Ott, and Y. Nakamura, "Mimetic communication model with compliant physical contact in human-humanoid interaction," Int. Journal of Robotics Research., vol. 29, no. 13, pp. 1684­1704, Nov. 2010. [7] H. Ben Amor, D. Vogt, M. Ewerton, E. Berger, B. Jung, and J. Peters, "Learning responsive robot behavior by imitation," in Proceedings of the 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2013, pp. 3257­3264. [8] Y. Tanaka, J. Kinugawa, Y. Sugahara, and K. Kosuge, "Motion planning with worker's trajectory prediction for assembly task partner robot," in Proceedings of the 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2012, pp. 1525­ 1532. [9] H. S. Koppula and A. Saxena, "Anticipating human activities using object affordances for reactive robotic response." in Robotics: Science and Systems, 2013. [10] Z. Wang, K. M¨ ulling, M. P. Deisenroth, H. B. Amor, D. Vogt, B. Sch¨ olkopf, and J. Peters, "Probabilistic movement modeling for intention inference in human­robot interaction," The International Journal of Robotics Research, vol. 32, no. 7, pp. 841­858, 2013. [11] B. Llorens-Bonilla and H. H. Asada, "A robot on the shoulder: Coordinated human-wearable robot control using coloured petri nets and partial least squares predictions," in Proceedings of the 2014 IEEE International Conference on Robotics and Automation, 2014. [12] A. J. Ijspeert, J. Nakanishi, H. Hoffmann, P. Pastor, and S. Schaal, "Dynamical movement primitives: learning attractor models for motor behaviors," Neural computation, vol. 25, no. 2, pp. 328­373, 2013. [13] M. Prada, A. Remazeilles, A. Koene, and S. Endo, "Dynamic movement primitives for human-robot interaction: comparison with human behavioral observation," in Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2013, pp. 1168­ 1175. [14] H. Sakoe and S. Chiba, "Dynamic programming algorithm optimization for spoken word recognition," Acoustics, Speech and Signal Processing, IEEE Transactions on, vol. 26, no. 1, pp. 43­49, 1978.

Probability of the task

1 0.8 0.6 0.4 0.2 0 0 2 4 6 8 10 Number of observations 12 14 16

(b)

Probability of the task

1 0.8 0.6 0.4 0.2 0 0 2 4 6 8 10 Number of observations 12 14 16

(c)

Fig. 9. Action recognition given three different Interaction ProMPs, one for each task involved in assembling the box. The three Interaction ProMPs are conditioned on the same observations of the observed agent. Probabilities of tasks are shown as a function of the number of observations along the trajectory of the observed agent. (a) Recognition of the hand over task. (b) Recognition of the fastening task. (c) Recognition of the plate insertion task.

cases, ProMPs can clearly distinguish among tasks as shown by the plots in Fig. 9 where the probabilities of the task are given as a function of the number of observations. Subplots (a), (b) and (c) show the task recognition for the fastening, hand over and plant insertion tasks, respectively. In general, we observed that 3-5 observations are required to achieve a 100% certainty for each task. (The last part of the accompanying video shows our method controlling the robot assistant to assembly a box with recognition of two different handover tasks). V. C ONCLUSION This paper introduced a method for collaboration suited for new applications using semi-autonomous robots whose movements must be coordinated with the movements of a human partner. By leveraging on the original framework of Interaction Primitives [3] we proposed the use of ProMPs for the realization of primitives that capture the correlation between trajectories of multiple agents. This work compared the main differences between DMPs and ProMPs for interaction and advocates the later for applications where measurements are noisy and/or prone to interruption. Using a 7-DOF lightweight arm we evaluated the capability of Interaction ProMPs in generating the appropriate primitive for controlling the robot in an interactive task involving a human partner. We also proposed a method for task recognition that naturally fits the ProMP framework. Our current work addresses the use of mixture-models to
534

International Conference on Humanoid Robots (HUMANOIDS), 2014

Online Multi-Camera Registration for Bimanual Workspace Trajectories
Neil T. Dantam Heni Ben Amor Henrik I. Christensen Mike Stilman

Abstract-- We demonstrate that millimeter-level bimanual manipulation accuracy can be achieved without the static camera registration typically required for visual servoing. We register multiple cameras online, converging in seconds, by visually tracking features on the robot hands and filtering the result. Then, we compute and track continuous-velocity relative workspace trajectories for the end-effector. We demonstrate the approach using Schunk LWA4 and SDH manipulators and Logitech C920 cameras, showing accurate relative positioning for pen-capping and object hand-off tasks. Our filtering software is available under a permissive license.1

I. I NTRODUCTION Visual feedback of hand movements provides rich information that can be used to correct for errors and improve manipulation accuracy. Recent evidence suggests that humans use visual feedback of the hand to guide reach and grasp tasks [15]. Continuously tracking and monitoring the state of the hand allows us to dynamically accommodate to internal and external perturbations, e.g., muscle impairments, thereby achieving a high degree of robustness during manipulation tasks. In robotics, using visual feedback depends on a kinematic registration between the camera and the manipulators. Typically, this is viewed as a static task: registration is computed offline and assumed to be constant. In reality, camera registration changes during operation due to external perturbations, wear and tear, or even human repositioning. For example, during the recent DARPA Robotics Challenge trials, impacts from falls resulted in camera issues which significantly affected the robot behavior [10]. The pose registration process should be treated as a dynamic task in which the involved parameters are continuously updated. Such an online approach to pose registration is challenging, since it requires the constant visibility of a calibration reference and sufficient accuracy to perform manipulation tasks. Bimanual manipulation requires accurate coordination of both end-effectors. To perform smooth and accurate bimanual manipulation, we propose an online estimation and control approach that combines (1) visual tracking of the manipulators, (2) co-estimation of poses for cameras and end-effectors using a special Euclidean group median and extended Kalman filter, and (3) continuous geometric interpolation on the special Euclidean group. Our key insight is to combine perception and control online, using the robot

Fig. 1. Bimanual Schunk LWA4/SDH capping a pen using visual feedback from online camera registration and end-effector tracking.

body frame as a reference. This work extends the singlecamera and manipulator registration presented in [5] to multicamera and multi-manipulator estimation, and it integrates the spherical blending approach of [6] to enable continuous motion of the manipulator in the workspace. II. R ELATED W ORK Typical camera registration methods collect a set of calibration data using an external reference object, compute the calibration, then proceed assuming the calibration is static. OpenCV determines camera registration from point correspondences, typically using a chessboard [12]. Pradeep, et. al, develop a camera and arm calibration approach based on bundle adjustment and demonstrate it on the PR2 robot [13]. This approach requires approximately 20 minutes to collect data and another 20 minutes for computation, a challenge for handling changing pose online. Visual servo control incorporates camera feedback into robot motion control [1], [2]. The two main types of visual servoing are image-based visual servo control (IBVS), which operates on features in the 2D image, and position-based visual servo control, which operates on 3D parameters. Both of these methods assume a given camera registration. While IBVS is locally stable with regard to pose errors, under PBVS, even small pose errors can result in large tracking error [1]. Compared to both IBVS and PVBS, our method requires no initial camera registration, instead estimating the registration online. Additionally, compared to IBVS, we estimate the full kinematics of the camera and robot,

The authors are with the Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA 30332, USA. ntd@gatech.edu, hbenamor@cc.gatech.edu,

hic@cc.gatech.edu
1 software

available at http://github.com/golems/reflex

and thus can directly follow workspace reference poses and trajectories, such as [6], rather than being limited to imagespace reference points and trajectories. Other recent work has explored online visual parameter identification. A single-camera and single-arm version of the approach in this paper was presented in [5]. [11] tracks a robot arm to identify encoder offsets. This method assumes a given camera registration, but is also tolerant to some registration error. In contrast, our work identifies the camera registration online. Though we do not explicitly consider encoder offsets, our method is empirically robust to offsets  of even 30 (see Sect. V). [9] considers bimanual arm and object tracking with vision and tactile feedback. Though the hardware and implementation differ from work presented in this paper, we obtain similar accuracy using inexpensive webcams and without tactile sensing. [16] uses maps generated from a Simultaneous Localization and Mapping (SLAM) algorithm to calibrate a depth sensor. In our approach, unlike typical environments for SLAM, the object to which we are trying to register our camera ­ the manipulator ­ will necessarily be in motion. III. E STIMATION : P OSES OF C AMERAS AND H ANDS We estimate poses of multiple cameras and manipulators by visually detecting the 3D pose of each manipulator. First, we detect texture features on the end-effector and fit a transform, providing an instantaneous estimate of camera and hand pose. To obtain sufficient accuracy for manipulation we then combine median and extended Kalman filtering of these poses. To use the robot body as a reference for camera registration, we track the 3D pose of features on the end-effector. These 3D poses can be estimated with marker-based [14] and model-based approaches [3]. Marker-based approaches require attaching fiducials to known locations on the robot, such as the fingers. Model-based tracking, on the other hand, requires accurate polygon meshes of the tracked object. In our implementation, we use the ALVAR library [14] for marker-based tracking. For computational reasons, we used the dual quaternion representation for the special Euclidean group SE (3). Compared to matrices, the dual quaternion has lower dimensionality and is more easily normalized, both advantages for our filtering implementation. Compared to the unit quaternion plus translation vector representation, dual quaternions are more convenient for algebraic manipulation because they are chained through multiplication. For Euclidean transformations, we use the conventional coordinate notation where the leading superscript denotes the parent frame and the following subscript denotes the child frame, i.e., aSb gives the origin of b relative to a. The transformation aSb followed by bSc is given as the dual quaternion multiplication aSb  b Sc = aSc . We represent an orientation quaternion as aqb , a translation vector as axb , a rotational velocity as ab , and the  b. combined translational and rotational velocity as a

A. Asynchronous Pose Co-Estimation Each camera image provides pose measurements for visible end-effector features. To reduce estimation latency, we process and filter the measurements from each camera asynchronously as they arrive rather than collecting images from all cameras at a fixed timestep. The kinematic chain through the manipulator, feature, and camera is defined as:
b

Swi  wiSwi  wiSfp = bScj  cjSfp

(1)

where bSwi is the encoder-measured pose of wrist i in body frame, wiSwi is the estimated offset pose of wrist i, wiSfp is the encoder-measured transform from wrist i to feature p on the hand, bScj is the estimated pose registration of camera j , and cjSfp is the visually-measured pose feature p in camera j . For a depiction of the setup see Fig. 3. Based on (1), we produce measurements for wrist offset wi Swi and camera registration bScj :
wiS wi cj

= (bSwi )-1  bScj  cjSfp  (wiSfp )-1 = Swi 
b wiS wi

(2) (3)

bS



wi

Sfp  ( Sfp )

cj

-1

where wiSwi is the wrist offset measurement from this image and feature, bScj is the camera registration measurement, b wiS wi is the currently estimated wrist offset, and Scj is the currently estimated camera pose. B. SE (3) Median and Extended Kalman Filter We apply median and extended Kalman filtering in the special Euclidean group SE (3) to the measurements for wrist offset wiSwi and camera registration bScj , similar to the approach in [5]. First, to reject outliers, we compute the median measurement over a sliding time window. Then, we use an extended Kalman Filter over time to compute optimal pose estimates under a Gaussian noise assumption. To compute the median of orientation q over the sliding window, the structure of rotations in SO(3) offers a convenient distance metric between two orientations: the angle between them. Using this geometric interpretation, the median orientation q is the orientation with minimum angular distance to all other orientations.
n

q = arg min
qi Q j =0

 | ln(qi  qj )|

(4)

The median translation v is the conventional geometric median, the translation with minimum Euclidean distance to all other translations:
n

v = arg min
vi  V j =0

| vi - vj |

(5)

In the extended Kalman filter, we consider state x composed of a quaternion q , a translation vector v , and the translational and rotational velocities, v  and  : x = (q , v ) = [qx , qy , qz , qw , vx , vy , vz , x , y , z , v x, v y , v z ]

er

er S

e

,

erS e

Workspace Control
e b

Se (t1 )

Relative Trajectory

Sw

= = =
wiS b b

e

Sw  w Sw

Sw bS  w

Ser  erSe  e Sw  e  e Sw Ser  erS

r Sr , S

J+

x r x - xr - kx  r ln (q  qr )

- k (J + J - I )

wiS

w , i

bS

cj wiS w i w i

 r (joint velocity) 
wiS w i bS cj

EKF

median

wiS

w i

...
0 bS

(bSwi )-1  bScj  cjSf  (wiSf )-1
b

ROBOT image Feature Detector

EKF

bS

cj

median

bS

cj

cj

...
0

Swi  wiSw  wiSf  (cjSf )-1
i cj

Sf0 . . . cjSfn

Fig. 2.

Block diagram of the control system. 3D feature poses

cj

Sfp are detected from visual data. Instantaneous wrist offsets

wiS wi

and camera

registrations bScj are computed. Then the median of these poses is taken over a sliding window and subsequently Kalman-filtered. The filtered poses are r. used to track a relative left-right workspace trajectory, and the Jacobian damped-least squares gives the reference joint velocities 

b b

Swr

Sc 0

wr S

wr

wr Ser

c0

Sf3

wr Sf

3

Frame Source: Encoders Visions Filter

Fig. 3.

Setup for a dual-arm and dual-camera system. The kinematic frames are shown for one of the arms and cameras.

The measurement z is the median pose from the sliding window: z = (q , v ) = [qx , qy , qz , qw , vx , vy , vz ] The general EKF prediction step for time k is: x ^k|k-1 = f (^ xk-1 ) f Fk - 1 = x x ^k-1|k-1
T Pk|k-1 = Fk-1 Pk-1|k-1 Fk -1 + Qk-1

Now, we find the process Jacobian F . The translation portion is a diagonal matrix of the translational velocity. For the orientation portion, we find the quaternion derivative q  from the rotational velocity: = q 1 q 2 (10)

(6) (7) (8)

This quaternion multiplication can be converted into the following matrix multiplication: 1 1   q = Mr (q )  2 2  qw qz -qz qw Mr (q ) =   qy -qx -qx -qy

where x ^ is the estimated state, f (x) is the process model, F is the Jacobian of f , P is the state covariance matrix, and Q is the process noise model. The process model integrates the translational and rotational velocity, staying in the SE (3) manifold using the dual quaternion exponential of the twist :   (, v,  v ) = , v ×  + v  f (x) = exp t   (q, v ) 2 (9)

 -qy qx   qw  -qz

(11)

Note that we omit the w column of the typical quaternion multiplication matrix because the w element of rotational velocity  is zero.

This gives the following process 13 × 13 Jacobian F :   1 I4×4 0 0 2 tMr q  0 I3×3 0 tI3×3   F = (12)  0 0 I3×3 0  0 0 0 I3×3 Now we consider the EKF correction step. The general form is: z ^k = h(^ xk|k-1 ) h Hk = x x ^ k |k - 1 yk = v (zk , z ^) Sk = Hk Pk|k-1 =
T Hk Pk|k-1 Hk T Sk Kk

IV. C ONTROL : C ONTINUOUS W ORKSPACE T RAJECTORIES To perform smooth, bimanual motion, we compute a relative workspace trajectory between the two manipulators, transform the relative pose and velocity of the trajectory to the body frame, then compute joint velocities using the Jacobian damped least squares pseudoinverse. We compute a relative trajectory for the two endeffectors using the spherical parabolic blends described in [6]. This provides a straight-line, constant-axis, and continuous-velocity workspace path for the end-effector by blending subsequent spherical linear interpolation segments. Given a list of relative left-right waypoint poses and times, er Se (t0 ), . . . , erSe (tn ), we compute the reference left-right  e (t). pose and velocity as a function of time: erSe (t), er er  el From the relative reference pose Se and velocity er between the left and right end-effectors, we control the left arm in workspace, by first converting the relative pose and velocity to the body frame, then computing the Jacobian damped-least-squares inverse kinematics solution. The left-arm wrist pose bSw follows directly from the kinematic chain through the right arm:
b e

(13) (14) (15) + Rk (16) (17) (18) (19)

x ^k|k = p(^ xk|k-1 , Kk yk ) Pk|k = (I - Kk Hk )Pk|k-1

where z is the measurement, h is the measurement model, H is the Jacobian of h, z ^ is the estimated measurement, R is the measurement noise model, and K is the Kalman gain, v is a function to compute measurement residual, and p is a function to compute the state update. We compute the EKF residuals and state updates using relative quaternions to remain in SE (3) without needing additional normalization. The observation h(x) is a pose estimate: h(x) = (q, v ) H = I7×7 (20) We compute the measurement residual based on the relative rotation between the measured and estimated pose: v (z, z ^) = (yq , yv )
 yq = ln zq  z ^q q

Sw = bSer  erSe  e Sw (25)

Sw = e Sw  w Sw

Next, we compute the body-frame feedforward reference  b . Since there is only one changing frame, erSe , velocity, a we could find the corresponding body frame motion by rotating the velocity. However, the typical computation is notationally cumbersome [4, p140].2 Instead, we find an elegant and more general solution by merely taking the derivative of the pose: Sw = bSer  erSe  e Sw 0 d er      b b ( Se  e Sw )  Sw =   Ser  (erSe  e Sw ) + bSer  dt B0 e  ¨ e  = bSe  erSe  e S  bSw w + rS e  Sw r ¨¨  = bSe  erS  e  e Sw  bSw r
0 b

yv = zv - z ^v

(21)

where yq is the orientation part of the residual and yv the  translation part. Note that ln zq  z ^q corresponds to a velocity in the direction of the relative transform between the actual and expected pose measurement and that we can consider yq as a quaternion derivative. Then, the update function will integrate the pose portion of y , again using the exponential of the twist. First, we find the twist corresponding to the product of the Kalman gain K and the measurement residual y : (Ky ) = (Ky )q  q  (Ky, v ) = ((Ky ) , v × (Ky ) + (Ky )v ) (22) Then, we integrate estimated pose using the exponential of this twist: t   (q, v ) (23) (x(q,v) )k|k = exp 2 Finally, the velocity component of innovation y is scaled and added: (x,v  )k|k = x,v  + (Ky ),v  (24)

(26)

U indicates that S cancels to zero, and we assume the  where  S e  ). right arm and left fingers are stationary (0 = bS er = S w Relative motion with both arms moving could be computed by including the nonzero derivative bS er in the computation. We can use the product rule for this derivation because dual quaternion poses are chained through multiplication. Using the quaternion plus translation vector representation,
2 The complexity of the velocity transformation notation in [4, p140] stems from its representation using Gibbs's vector calculus which decouples the quaternion multiplication into separate dot and cross products. Hamilton's and Study's classical quaternion and dual quaternion notation is simpler and more elegant for this kinematic computation. A similar computation is also possible using transformation matrices and their derivatives, but these matrices are more difficult to normalize than quaternions, increasing numerical error.

Fig. 4. Manipulation error using only encoders for position feedback. Without using visual feedback, there is a 15mm relative positioning error between the two end-effectors.

chaining is not a multiplication, so an equivalent derivation would be more complex. Velocity and the dual quaternion derivative are related as follows: dR(S ) 1 =   R( S ) dt 2 1 dR(S ) dD(S ) = x   R(S ) + x  dt 2 dt

Fig. 5. Testing relative positioning accuracy by aligning the end-effectors. Incorporating visual feedback and online registration reduces manipulation error from 15mm to  2mm. 1

(27)
quaternion, q

where R(S ) is the real part of S , D(S ) is the dual part of S ,  is rotational velocity, and x is translation. Finally, we compute reference joint velocities using the Jacobian damped least squares with a nullspace projection to keep joints near the zero position: r = J+  x r x - xr - kx  r ) ln (q  qr -k (J + J -I ) (28)

x y z w

0.5

0

-0.5

0

1

2

3

4 5 6 time (s)

7

8

9 10

where x is the actual translation, q is the actual orientation quaternion, xr is the reference translation, qr is the reference orientation quaternion,  is the actual rotational velocity, r is the reference rotational velocity, kx is the workspace position error gain, k is the null-space projection gain, and  is the configuration. We then use joint-level velocity control  r . A block diagram to track the reference joint velocities  depicting the components of the control system and their interplay can be found in Fig. 2. V. E XPERIMENTS We implement this approach on a pair of Schunk LWA4 manipulators with SDH end-effectors, and use a pair of Logitech C920 webcams to track the robot and objects. Our estimation and control software is implemented as a distributed system using the Ach real-time communication library [7]. The Schunk LWA4 has seven degrees of freedom and uses harmonic drives, which enable repeatable positioning precision of ±0.15mm [8]. However, absolute positioning accuracy is subject to encoder offset calibration and link rigidity. In practice, we achieve ±15mm accuracy when using only the joint encoders for feedback, as can be seen in Fig. 4. The Logitech C920 provides a resolution of 1920x1080 at 15 frames per second. To measure ground-truth distances, we use a ruler and meter-stick. To test the relative positioning accuracy of our implementation, we servo the end-effectors to a reference zero relative alignment, Fig. 5, and then measure the actual relative error between the two end-effectors. We conduct this test using

0.5

x y z

translation, x

0

-0.5

0

1

2

3

4 5 6 time (s)

7

8

9 10

Fig. 6. Relative trajectory of erSe between left and right end-effectors for pen-capping. The trajectory has constant acceleration, constant velocity, and constant deceleration segments.

only encoder feedback, then with visual feedback. We also  repeat the test injecting encoder error of 15 at the initial   shoulder joint, 30 at the shoulder, and 15 at both the shoulder and elbow. The results of this test are summarized in Table I. In addition, we use this method to perform the pen-capping task shown in Fig. 3 and the object hand off task shown in Fig. 7. The relative trajectory of erSe for the pen-capping task is plotted in Fig. 6 VI. D ISCUSSION The results of Sect. V show that this method achieves bimanual positioning accuracy of a few millimeters without  static camera registration and even with significant (30 ) error in the joint encoders.

TABLE I P OSITIONING T EST R ESULTS (mm) Mean encoder visual 16.5 2.2 155 2.8 280 1.3 240 0.95 Std. Dev. encoder visual 0.5 0.94 0.6 0.78 0 0.95 0 1.1

VII. C ONCLUSION We have presented an online method to identify multiple camera and manipulator poses and track continuous relative trajectories for bimanual manipulation tasks. This is useful for the typical case where camera registration is not static but changes due to model error, disturbances, or wear and tear. The key point is to track both manipulators, and follow a trajectory based on the visually estimated relative 3D pose between the end-effectors. By combining median and Kalman filtering, we are able to achieve millimeter-level manipulation accuracy. We have shown in our experiments that online registration can be used to improve positioning accuracy during bimanual manipulation tasks where successful operation depends on relative end-effector pose. This method uses feedback only from joint encoders and visual tracking of the robot hand. Further improvements could be made by including force and tactile sensing and by visually tracking in-hand objects. R EFERENCES
[1] Franc ¸ ois Chaumette and Seth Hutchinson. Visual servo control, part I: Basic approaches. Robotics and Automation Magazine, 13(4):82­90, 2006. [2] Franc ¸ ois Chaumette and Seth Hutchinson. Visual servo control, part II: Advanced approaches. Robotics and Automation Magazine, 14(1):109­118, 2007. [3] Changhyun Choi and Henrik I Christensen. Robust 3d visual tracking using particle filtering on the special euclidean group: A combined approach of keypoint and edge features. The International Journal of Robotics Research, 31(4):498­519, 2012. [4] J. Craig. Introduction to Robotics: Mechanics and Control. Pearson, 3rd edition, 2005. [5] N. Dantam, H. Ben Amor, H. Christensen, and M. Stilman. Online camera registration for robot manipulation (presented). In International Symposium on Experimental Robotics, 2014. [6] N. Dantam and M. Stilman. Spherical parabolic blends for robot workspace trajectories (accepted). In International Conference on Intelligent Robots and Systems, 2014. [7] Neil Dantam, Daniel Lofaro, Ayonga Hereid, Paul Oh, Aaron Ames, and Mike Stilman. Multiprocess communication and control software for humanoid robots (accepted). Robotics and Automation Magazine, 2014. [8] Schunk GmbH. Dextrous lightweight arm LWA 4D, technical data. http://mobile.schunk-microsite.com/en/produkte/ produkte/dextrous-lightweight-arm-lwa-4d.html. [9] Paul Hebert, Nicolas Hudson, Jeremy Ma, and Joel W Burdick. Dual arm estimation for coordinated bimanual manipulation. In Intl. Conf. on Robotics and Automation, pages 120­125. IEEE, 2013. [10] Sungmoon Joo and Michael Grey. DRC-Hubo retrospective, January 2014. Personal Communication. [11] Matthew Klingensmith, Thomas Galluzzo, Christopher Dellin, Moslem Kazemi, J. Andrew (Drew) Bagnell, and Nancy Pollard. Closed-loop servoing using real-time markerless arm tracking. In Intl. Conf. on Robotics and Automation (Humanoids Workshop), May 2013. [12] OpenCV API Reference. http://docs.opencv.org/master/ modules/refman.html. [13] Vijay Pradeep, Kurt Konolige, and Eric Berger. Calibrating a multiarm multi-sensor robot: A bundle adjustment approach. In Experimental Robotics, pages 211­225. Springer, 2014. [14] Kari Rainio and Alain Boyer. ALVAR ­ A Library for Virtual and Augmented Reality User's Manual. VTT Augmented Reality Team, December 2013. [15] Jeffrey A. Saunders and David C. Knill. Humans use continuous visual feedback from the hand to control both the direction and distance of pointing movements. Experimental Brain Research, 162(4):458­473, 2005. [16] A. Teichman, S. Miller, and S. Thrun. Unsupervised intrinsic calibration of depth sensors via slam. In Robotics: Science and Systems (RSS), 2013.

No Offset  shoulder: 15  shoulder: 30  shoulder & elbow: 15

Fig. 7.

An object hand-off task.

There are a variety of error sources that we address in this system. For the kinematics, error from encoder offsets in the arm, imprecise link lengths, and flexing of links all contribute inaccurate kinematic pose estimates. For perception, error from inaccurate camera intrinsics, imprecise fiducial sizes, offsets in object models, and noise in the image all contribute to error in visual pose estimates. To achieve accurate manipulation, we must account for these potential sources of error. The position of the tracked features on the robot has an important effect on error correction. Kinematic errors between the robot body origin and the tracked features, e.g., due to flex or encoder offsets, are incorporated into the camera registration and handled through the servo loop. Error between the observed features and the end-effector cannot be corrected. Thus, it is better to track features as close to the end-effector as possible. Consequently, we placed the fiducial markers on the fingers of the SDH end-effector. One source of error for manipulation that we do not address is error in grasping. Because we track only the robot hand, any error in the relative pose between the hand and grasped object is not corrected. In reality, when grasping an object, the object itself becomes the robot's end-effector. Thus, to accurately manipulate in-hand objects, it would be better to track the objects themselves. Since a grasped object is likely to be partially occluded, model-based tracking such as [3], which is robust to occlusions, is a potential approach. A crucial additional consideration in manipulation is force and tactile sensing. Using visual feedback without force and tactile sensing already reduces the error to a few millimeters and allows the robot to perform tasks such as pen capping and object hand-off. However, considering the generated contact forces during the manipulation would further improve performance and allow even more accurate operation, in particular during the post-contact phase. This is a key area for improvement in this approach.

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/268521702

TransferEntropyforFeatureExtractionin PhysicalHuman-RobotInteraction:Detecting Perturbationsfrom...
ConferencePaper·October2014
DOI:10.1109/HUMANOIDS.2014.7041459

CITATION

READS

1
5authors,including: ErikBerger TechnischeUniversitätBergakademieFreiberg
18PUBLICATIONS52CITATIONS
SEEPROFILE

90

DavidVogt TechnischeUniversitätBergakademieFreiberg
20PUBLICATIONS86CITATIONS
SEEPROFILE

HeniBenAmor ArizonaStateUniversity
56PUBLICATIONS434CITATIONS
SEEPROFILE

Someoftheauthorsofthispublicationarealsoworkingontheserelatedprojects:

Mining-RoX:AutonomousRobotsinUndergroundMiningViewproject

AllcontentfollowingthispagewasuploadedbyHeniBenAmoron21November2014.

Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Transfer Entropy for Feature Extraction in Physical Human-Robot Interaction: Detecting Perturbations from Low-Cost Sensors
Erik Berger1 , David M¨ uller1 , David Vogt1 , Bernhard Jung1 , Heni Ben Amor2
Abstract-- In physical human-robot interaction, robot behavior must be made robust against forces applied by the human interaction partner. For measuring such forces, special-purpose sensors may be used, e.g. force-torque sensors, that are however often heavy, expensive and prone to noise. In contrast, we propose a machine learning approach for measuring external perturbations of robot behavior that uses commonly available, low-cost sensors only. During the training phase, behaviorspecific statistical models of sensor measurements, so-called perturbation filters, are constructed using Principal Component Analysis, Transfer Entropy and Dynamic Mode Decomposition. During behavior execution, perturbation filters compare measured and predicted sensor values for estimating the amount and direction of forces applied by the human interaction partner. Such perturbation filters can therefore be regarded as virtual force sensors that produce continuous estimates of external forces.

I. I NTRODUCTION Autonomous robots require accurate sensing capabilities in order to act in an intelligent and meaningful way within their environment. In particular human-robot interaction tasks require sensors for measuring physical contact with a human partner. Recorded measurements can be used by a robot to ensure safety during interactions and to react to physical perturbations. To this end, it is important that both the occurrence as well the magnitude of an external perturbation, e.g., a push, are reliably detected. Existing sensing technologies, such as force-torque sensors, are often heavy, expensive, and noise-prone. However, there are numerous affordable lowcost sensors available which, while not directly measuring perturbation forces, can be used to generate estimates of external perturbations. In this paper, we present an approach for perturbation detection which is based on a combination of low-cost sensors and machine learning techniques. During a training phase, we extract a compact representation, called a perturbation filter, which specifies the evolution of sensor readings during regular execution of a motor skill. The extraction is guided by information-theoretic measures such as Transfer Entropy, that determine the relevance of a specific sensor w.r.t. the executed robot behavior. In contrast to our previous work [1], we will not use any higher level stability parameters, such as the center-of-mass, center-of-pressure, or zero-moment-point for learning. Instead, we will learn the perturbation filter from low-level sensor data, solely. As a result, no knowledge about the robot kinematics or dynamics is required.
1 Institute of Computer Science, Technical University Bergakademie Freiberg, Bernhard-von-Cotta-Str. 2, 09599 Freiberg, Germany 2 Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, 801 Atlantic Drive, Atlanta, GA 30332-0280, USA

Fig. 1: A NAO robot estimates the influence of external perturbations applied by a human interaction partner to its current behavior execution.

After a perturbation filter is learned, it is used to generate a continuous estimate of the amount of external human perturbations. During physical interaction between a robot and a human, the estimated perturbations can be used to compensate for the external forces or infer the intended guidance of a human interaction partner. The presented perturbation filter can be regarded as a virtual force sensor that produces a continuous estimate of external forces. II. R ELATED W ORK In recent years, natural and intuitive approaches to HRI have gained popularity. Various researchers have proposed the so-called soft robotics paradigm: compliant robots that "can cooperate in a safe manner with humans" [2]. An important robot control method for realizing such a compliance is impedance control [3]. Impedance control can be used to allow for touch based interaction and human guidance. To this end, impedance controllers require accurate sensing capabilities, in the form of force-torque sensors. However, such sensors are typically heavy, expensive and suffer from significant noise. Other sensors, such as torque sensors are

Data Acquisition Offline
SensorData Data Sensor
Training Data

Feature Extraction
PCA + Transfer Entropy Feature

Interpolation
Dynamic Mode Decomposition

Training Space Data

Data Model

Target

Online

DataData Sensor

Live

Feature Space Projection

Distance Measurement using DTW

Perturbation

Live Interaction

Target Estimation

Perturbation Estimation

Fig. 2: An overview of the presented machine learning approach. Training data, together with a labeling target vector will be processed using Principal Component Analysis, Transfer Entropy and Dynamic Mode Decomposition algorithms, providing a training data model of vectors comprising the Feature Space. During live interaction, the recorded data is being projected into this space and mapped to the nearest data model vector and its target vector using Dynamic Time Warping.

even more prone to issues related to noise and drift. Still, the ability to sense physical influences is at the core of recent advances made in the field of HRI. For example, Lee et al. [4] use impedance control and force-torque sensors in order to realize human-robot interaction during programming by demonstration tasks. Wang et al. [5] present a robot adapting its dancing steps based on the external forces exerted by a human dance partner. Ben Amor et al. [6] use touch information to teach new motor skills to a humanoid robot. Touch information is only used to collect data for subsequent learning of a robotic motor skill. Robot learning approaches based on such kinesthetic teach-in have gained considerable attention in the literature, with similar results reported in [7] and [8]. A different approach aiming at joint physical activities between humans and robots has been reported in [9]. Ikemoto et al. use Gaussian mixture models to adapt the timing of a humanoid robot to that of a human partner in close-contact interaction scenarios. This approach significantly improves physical interactions, but is limited to learning timing information. St¨ uckler et al. [10] present a cooperative transportation task where a robot follows the human guidance using arm compliance. In doing so, the robot recognizes the desired walking direction through visual observation of the object being transported. A similar setting has been investigated by Yokoyama et al. [11]. They use a HRP-2P humanoid robot with a biped locomotion controller and an aural human interface to carry a large panel together with a human. Forces measured with sensors on the wrists are utilized to derive the walking direction. The main disadvantage of the above approaches is that they require special aural and visual input devices or force

sensors, which are not present on many robot platforms. Additionally, none of the approaches using force-torque sensors addresses the problem of uncertainty in the measurements. As a result, all of these approaches assume high-quality sensing capabilities and low-speed execution of the joint motor task. We propose a new filtering algorithm that can learn the natural variation in sensor values as a motor skill is executed. III. A PPROACH The objective of the presented method is to estimate the strength and direction of external perturbations caused by a human interaction partner. To infer these estimates from low-cost sensor readings, we condition behavior-specific perturbation filters. An overview of the approach can be seen in Figure 2. First, we record training data for a behavior with different parameter configurations, e.g., varying step lengths during walking. In this data acquisition phase, no external perturbations from humans are applied. Thereafter, the training data is used to create a Feature Space data model during feature extraction. Linear combinations of different sensors are weighted by their relevance to the observed parameter and projected into the low-dimensional Feature Space. In the following, the configuration parameter will be referred to as the target vector. The relevance of a specific sensor to the target vector is extracted using Transfer Entropy [12] (TE). In this context, TE is used as a measure of predictability and information flow between the target vector and the conduct of sensors. Sensors that have a high TE w.r.t. the robot's behavior are deemed more influential and relevant. During behavior execution, an external perturbation is de-

tected by comparing the recorded training data to the current sensor data within the low-dimensional Feature Space. Dynamic Time Warping (DTW) [13] is used as a distance function in order to include the temporal pattern for the comparison. The estimation of a perturbation value is performed by comparing the current sensor readings to the sensor readings acquired during training. The perturbation value is then inferred from the difference between the currently configured behavior parameter, e.g., the currently employed step length, and the estimated behavior parameter which produced similar sensor readings during training. In the following section, we will depict each step of our approach in more detail. We will describe how to perform feature space extraction and how to use the resulting embedding to estimate a continuous perturbation value. A. Data Acquisition The first step in our approach is to record training data that reflects the evolution of sensor values during the regular execution of a motor skill. To this end, we perform the investigated motor skill with varying parameter values, e.g. varying step lengths during walking. For generalization purposes, it is important to record the motor skill under large a set of possible target parameter configurations. However, since the parameter space may have a dynamic range, this can lead to a time-consuming recording phase, which in consequence leads to wear and tear of the robot hardware. To avoid a lengthy training session, Dynamic Mode Decomposition can be used to learn a model of the sensor data using few training samples. This process is not being detailed in this publication, the interested reader is referred to our previous work [14]. The training data is sampled equidistantly with 100Hz . Please note, that we only record low-level sensor data. Preprocessed variables, such as center-of-mass or the zero moment point are not included in this process. In contrast to our previous work, we will automatically identify and combine relevant low-level sensor data. To prevent a comparison between sensors of different units (i.e. comparing angles with pressure values), a sensor group is assigned to each sensor, enabling to deduce conclusions from their individual relations. B. Feature Extraction The next step in our approach is to extract relevant features from the stream of sensor data. While it is often possible to acquire a large number of different sensor values, we are typically faced with significant redundancy and noise. Additionally, it is often unclear which of these readings we should pay attention to. Feature extraction can help to single out important parts of a sensor stream. For feature extraction, we first compute a low-dimensional embedding of the sensor data by performing a PCA-like procedure. We extract the principal components of the feature space using an eigenvector decomposition of the sensor data matrix. In traditional PCA the eigenvalues define how much information each eigenvector carries. The eigenvector with

highest eigenvalue is the direction with highest variance. Hence, in traditional PCA the relevance of a feature is defined by the observed variance along that dimension. Instead of using the variance to infer the relevance of a feature, we will focus on the relationship between the feature values and the future state of the robot. Features that have a strong statistical coherence with the future state of the robot are more likely to be relevant. In other words, a feature is deemed relevant, if its past activity is a good predictor of the robot's next state. From an information theoretic point of view, this type of relationship can be estimated using Transfer Entropy. [12] We employ TE in order to measure the directed information transfer between the target and each PC vector separately. TE is a recently introduced information-theoretic measure, which has been used extensively in diverse fields of science [15][16][17]. T EJ I = p(it+1 , it , jt )log2 p(it+1 |it , jt ) p(it+1 |it ) (1)

TE quantifies the incorrectness of the assumption, that in the absence of information flow from system J to system I , the state of J has no influence on the transition probabilities on system I [12]. To compute the TE, the conditional probability as well as joint probability of co-occurrences in J and I is required. To estimate these probabilities without resorting to density estimation we quantize the sensor values and use a frequentist approach. The optimal parameters for the quantization are empirically determined. We quantize each PC vector into 200 value levels and use a quantile based transformation, which has the advantages of stability and independence of input value transformations. Variations of the robot's target vector could possibly have a time-delayed impact on sensory data. However, the standard TE algorithm only allows to draw conclusions based on transitions of a one-step delay between samples of J and I . A more general approach can be implemented using Delayed Transfer Entropy [18] which introduces multiple possible time delays. p(it+1 |it , jt+1-d ) p(it+1 |it ) (2)  We calculate the Transfer Entropy peak values T ET Pi = argmaxj T ET P (dj ) between the target vector T and each principal component Pi over a number of time delays dj within a preset time delay window D, j dj = D. See Figure 3 for further details. These T E  peak values are then used to scale the previously acquired principal components, such that components of higher T E  values are stretched and those of lower T E  values are shrunk. The resulting scaled PCA space is the so-called Feature Space. As mentioned earlier, within the feature space, the relevance of variables depends on their influence on the target vector. The feature space projection of the acquired training data set is now stored as the feature space data model. T EJ I (d) = p(it+1 , it , jt+1-d )log2

Target PC1 ... PCn ...

0.8 ... 0.1

... ... ...

0.6 ... 0.3

Fig. 3: The Transfer Entropy measures the target vector's (red graph) influence on each PC vector for each time delay (black arrow for d=1, orange arrow for d=20). The TE peak value of each PC component (marked by red squares) is the largest of each delayed TE value.

4. PC

3. PC

2. PC

1. PC

d=1

TEJ I(d) ... d=20


PCA Space

Feature Space

0

5

10

15

20

25

Time [s]

C. Target Estimation The next step is to use this data model to continuously estimate current target behavior values and to detect and qualify possible perturbations upon live human-robot interaction. To do so, a time window of raw sensor data is projected into the previously specified low dimensional Feature Space. To identify the most similar segment of the projected training data, we use the subsequence dynamic time warping technique (SDTW) [19]. v = argmin (X, Y ). (3)

Fig. 4: The principle components (blue) and the principle components scaled with TE (red) of a walking gait's training data. The third principle component has the highest TE value and is stretched while the others are dampened. The scaled principle components comprise the low dimensional Feature Space.

robot should react to a perturbation depends on the specific use-case and is left open for further research at this point. IV. E XPERIMENTS In the following section, we evaluated our approach using a NAO robot from Aldebaran Robotics. To do so, we recorded a total of 52 seconds of training data from the robot's walking gait with step lengths between 3 cm and -3 cm. Each sample contains readings of the angle and pressure sensors. Next, we interpolated these samples with a resolution of 0.01 cm utilizing Dynamic Mode Decomposition. Retaining 95 % of information we applied Principal Component Analysis on the robots 24 angle sensors and its 8 foot pressure sensors separately resulting in a 4d-angle space and a 6d-pressure space. Finally, each principal component is scaled by its delayed Transfer Entropy whereas the target value equals the gaits step-length. The resulting dimensions of the low dimensional Feature Space are shown in Figure 4. While the first, second and fourth dimensions are dampened, the third dimension is stretched. This is due to the fact that PCA extracts the most characteristic properties of a behavior and not its dependency on the adjusted parameter. Figure 5 shows the vector length of P C · T E  , the resulting Feature Space vectors compared to the simultaneous longitudinal center-of-mass which was used extensively in our previous research [1] [14]. Obviously, they are very similar even though our new approach has no knowledge about the kinematic chain or the mass distribution of the robot. A. Estimation Quality We utilize the learned low dimensional Feature Space data model during runtime while the robot frequently reduces its step-length. Figure 6 shows the resulting mean absolute error (M AE ) for the angle and pressure sensor groups. The angles are especially influenced by spurious relationships, which are strongly dampened by PCA, even without TE.

For this, the SDTW algorithm  is measuring the distance between two temporal sequences X = (x1 , . . . , xN ) and Y = (y1 , . . . , yM ) of length N  N and M  N. In our specific case, the goal is to find the training data Y with minimal distance to the projection of the currently observed sequence X . As a result, the target behavior value v of the corresponding training data can be used as an estimation for the current one. Since we captured the behavior for a discrete set of target behavior values we can only make estimations for these. An efficient way to expand our data model to cover continuous behavior parameters can be achieved using interpolation schemes. Therefore, we use a novel interpolation method from fluid dynamics, the so called Dynamic Mode Decomposition (DMD) [20][21]. A detailed explanation of DMD and how it can be used for the interpolation of robot sensor data can be found in our previous publication [14]. D. Reaction Accordingly, we can generate an estimate for possible ^ by calculating the difference interfering external forces E between the configured behavior parameter P used to control ^ identified the robot and the estimated behavior parameter P by the learned model for each sensor group. ^=P ^-P E (4)

Thus, our approach can be used in scenarios where a robot has to detect and react to external perturbations in order to fulfill a specified task. Certain sensor groups will be suitable to qualify certain perturbations, allowing conclusions about perturbation characteristics. This and details about how the

|P C · T E  |
3

Prediction Configuration

Step Length [cm]
39 52

2.5

2

Longitudinal Center of Mass

1.5

1

0

13

26

Time [s]

0.5 0

5

10

15

Time [s]

Fig. 5: The vector length of P C · T E  compared to the simultaneous longitudinal center-of-mass. In contrast to the center-of-mass our approach does not need any knowledge about the robots kinematics or mass distribution to generate a comparable result.
¬PCA ¬TE ¬PCA TE PCA ¬TE

Fig. 7: The configured behavior parameter (red) decreases over time. The estimated behavior parameter (blue) recognized the robot's reactions with a time delay, since the robot needs to finish the current step before adapting to the new parameter.

3.5 3 2.5

PCA TE

C. Perturbation Estimation In this experiment, the human perturbs the robot during execution of a walking gait. To verify the correctness and universality of our approach, the perturbations are applied to different parts of the robot. Figure 8 shows the resulting parameter estimations for the angle and pressure groups as well as the configured parameter value. Perturbation (a) is recognized by both sensor groups, because the angles as well as the force sensors are affected. If no external perturbation is recognized during (b), the parameter estimations of both sensor groups measure almost the configured parameter value. However, in (c) the perturbation does not affect angles and in consequence can't be measured by the angle but by the pressure group. Finally, perturbation (d) leads to a measurement of flat zeros for each pressure sensor, which is not part of the training data set and, consequently, can only be recognized by the angle group. This confirms our assumption, that redundant sensor groups can help to recognize and qualify a variety of perturbations. V. C ONCLUSION In this paper, we presented an approach for estimating external perturbations during physical human-robot interaction tasks. Instead of using expensive force-torque sensors, we leverage available information from low-cost sensors. To this end, we introduced a machine learning approach that can learn behavior-specific perturbation filters in software. In turn, these filters can be used to generate a continuous estimate of the inflicted external perturbations. An informationtheoretic measure, in particular Transfer Entropy, is used to guide the feature extraction process. Given a set of low-level sensor data, our approach allows for the automatic identification of relevant sensor values by calculating the information flow from sensors to future robot states. We have shown, that this approach automatically leads to the emergence of features that are remarkably similar to the center-of-mass,

M AE

2

1.5 1 0.5 0

Angle Group

Pressure Group

Fig. 6: The mean absolute error prediction results in cm. Left: The angle group error with all permutations of PCA and TE. Right: The pressure group error with all permutations of PCA and TE. Obviously, using a combination of PCA and TE increases the accuracy of the estimation.

Furthermore, as shown by the pressure group, PCA fails to identify the relations between the sensors and the behavior parameter. However, our Feature Space, combining PCA and TE, increases the accuracy of the estimation for both sensor groups.

B. Parameter Estimation In this experiment, we measure the robot's hardware delay using the 52 seconds of angle training data without interpolation. For this, the step length is reduced from three to one centimeters over a period of 15 seconds with a sliding window of a 0.25 seconds. Figure 7 shows that the robot needs about one second to react to parameter reconfigurations. This indicates, that the robot has an average hardware delay of 0.75 seconds.

2.5

(a) Pulling

(b) No Perturbation

(c) Pushing

(d) Lifting

2

Step Length [cm]

1.5

1

0.5

0 0

Angle Estimation
5

Pressure Estimation
10 15

Configured Value
20

Time [s]

25

30

35

40

45

50

^ for each of the external perturbations is the difference between the estimated Fig. 8: The estimated perturbation value E ^ parameter P and the configured parameter value P , as defined in Formula 4. (a) can be detected by both the angle and the pressure sensors while the perturbations in (c) and (d) can only be detected by one of the sensor groups.

without actually having to provide prior knowledge about the robot kinematics or mass distributions. The automatic determination of these features is important, since manufacturersupplied mass distributions are effectively invalidated in tasks where the robot is carrying weights. Further characterization of causing effects and details about how to react to certain perturbations should be investigated in future work. Using spatial groupings of sensors on the robot could be used to localize the external influences. R EFERENCES
[1] E. Berger, D. Vogt, N. Haji-Ghassemi, B. Jung, and H. Ben Amor, "Inferring guidance information in cooperative human-robot tasks," in Humanoids'13, 2013. [2] A. Albu-Sch¨ affer, O. Eiberger, M. Fuchs, M. Grebenstein, S. Haddadin, C. Ott, A. Stemmer, T. Wimb¨ ock, S. Wolf, C. Borst, and G. Hirzinger, "Anthropomorphic soft robotics from torque control to variable intrinsic compliance," in Robotics Research, ser. Springer Tracts in Advanced Robotics, C. Pradalier, R. Siegwart, and G. Hirzinger, Eds. Springer Berlin Heidelberg, 2011, vol. 70, pp. 185­207. [3] S. Haddadin, Towards Safe Robots - Approaching Asimov's 1st Law. Springer, 2014. [4] D. Lee and C. Ott, "Incremental kinesthetic teaching of motion primitives using the motion refinement tube," Autonomous Robots, vol. 31, no. 2-3, pp. 115­131, 2011. [5] H. Wang and K. Kosuge, "Control of a robot dancer for enhancing haptic human-robot interaction in waltz," IEEE Trans. Haptics, vol. 5, no. 3, pp. 264­273, Jan. 2012. [6] H. Ben Amor, E. Berger, D. Vogt, and B. Jung, "Kinesthetic bootstrapping: Teaching motor skills to humanoid robots through physical interaction," in KI 2009: Advances in Artificial Intelligence. Springer Berlin Heidelberg, 2009, pp. 492­499. [7] S. Calinon, Robot programming by demonstration: A probabilistic approach. EPFL Press, 2009. [8] J. Kober and J. Peters, "Policy search for motor primitives in robotics," Machine Learning, vol. 84, no. 1, pp. 171­203, 2011.

[9] S. Ikemoto, H. Ben Amor, T. Minato, B. Jung, and H. Ishiguro, "Physical human-robot interaction: Mutual learning and adaptation," IEEE Robotics & Automation Magazine, vol. 19, no. 4, pp. 24­35, 2012. [10] J. St¨ uckler and S. Behnke, "Following human guidance to cooperatively carry a large object," in Humanoids'11, 2011, pp. 218­223. [11] K. Yokoyama, H. Handa, T. Isozumi, Y. Fukase, K. Kaneko, F. Kanehiro, Y. Kawai, F. Tomita, and H. Hirukawa, "Cooperative works by a human and a humanoid robot," in Proceedings. ICRA '03. IEEE International Conference on Robotics and Automation 2003., vol. 3, 2003, pp. 2985­2991 vol.3. [12] T. Schreiber, "Measuring information transfer," Physical Review Letters, vol. 85, no. 2, pp. 461­464, 2000. [13] H. Sakoe, "Dynamic programming algorithm optimization for spoken word recognition," IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 26, pp. 43­49, 1978. [14] E. Berger, M. Sastuba, D. Vogt, B. Jung, and H. Ben Amor, "Dynamic mode decomposition for perturbation estimation in human robot interaction," in RO-MAN, 2014 IEEE, to appear 2014. [15] S. Ito, M. E. Hansen, R. Heiland, A. Lumsdaine, A. M. Litke, and J. M. Beggs, "Extending transfer entropy improves identification of effective connectivity in a spiking cortical network model," PloS one, vol. 6, no. 11, p. e27431, 2011. [16] M. P. Machado, C. W. Kulp, and D. Schlingman, "Composition and analysis of music using mathematica," Mathematica in Education and Research, p. 1, 2007. [17] S. K. Baek, W.-S. Jung, O. Kwon, and H.-T. Moon, "Transfer entropy analysis of the stock market," arXiv preprint physics/0509014, 2005. [18] S. Ito, M. E. Hansen, R. Heiland, A. Lumsdaine, A. M. Litke, and J. M. Beggs, "Extending Transfer Entropy Improves Identification of Effective Connectivity in a Spiking Cortical Network Model," PLoS ONE, vol. 6, no. 11, pp. e27 431+, Nov. 2011. [19] M. M¨ uller, Information Retrieval for Music and Motion. Secaucus, NJ, USA: Springer-Verlag New York, Inc., 2007. [20] P. J. Schmid, "Dynamic mode decomposition of numerical and experimental data," Journal of Fluid Mechanics, vol. 656, pp. 5­28, 8 2010. [21] C. W. Rowley, I. Igor Mez, I. Shervin Bagher, R. Philipp Schlatte, and D. S. Henningson, "Spectral analysis of nonlinear flows," Journal of Fluid Mechanics, vol. 641, no. -1, pp. 115­127, 2009.

View publication stats

Latent Space Policy Search for Robotics
Kevin Sebastian Luck1 , Gerhard Neumann1 , Erik Berger2 , Jan Peters1,4 and Heni Ben Amor3
Abstract-- Learning motor skills for robots is a hard task. In particular, a high number of degrees-of-freedom in the robot can pose serious challenges to existing reinforcement learning methods, since it leads to a highdimensional search space. However, complex robots are often intrinsically redundant systems and, therefore, can be controlled using a latent manifold of much smaller dimensionality. In this paper, we present a novel policy search method that performs efficient reinforcement learning by uncovering the low-dimensional latent space of actuator redundancies. In contrast to previous attempts at combining reinforcement learning and dimensionality reduction, our approach does not perform dimensionality reduction as a preprocessing step but naturally combines it with policy search. Our evaluations show that the new approach outperforms existing algorithms for learning motor skills with high-dimensional robots.

I. INTRODUCTION Creating autonomous robots that can adapt to the current task by interacting with their environment is an important vision of artificial intelligence. In recent years, many successful applications of reinforcement learning (RL) to complex robot tasks have been reported, including autonomous helicopter flight [1], robot table-tennis [2], or quadruped locomotion [3]. One of the most successful methods for learning such motor tasks is policy search [4]. Policy search tries to directly uncover the parameters of a given policy representation that yield high rewards. In this paper we focus on policy search for robots with a high number of degrees-of-freedom (DOF). Typically, the number of parameters of our control policy heavily depends on the number of DOFs of the robot. Hence, we generally need a large number of evaluations to learn acceptable policies. However, evaluating hundred thousands of different policies on a real robot is often infeasible due to wear and tear, the required logistics, or space and time constraints. At the same time, many
1 Kevin S. Luck, Gerhard Neuman and Jan Peters are with the Department of Computer Science, Technische Universit¨ at Darmstadt, 64289 Darmstadt, Germany

Fig. 1: A NAO robot learns to lift up one leg and stay balanced using a novel latent space policy search method. The co-articulation of the joints, needed for successful execution of the motor skill, is represented in the low-dimensional latent space.

robot control tasks, such as motor skills, are highly redundant in the controlled DOFs. Typically, the intrinsic dimensionality of such movements is much smaller than the actual controlled number of DOFs. Hence, robot learning can be performed much more efficiently if we can determine the lower-dimensional latent space of the movement we want to learn. In this paper we present an efficient policy search algorithm for learning policies in low-dimensional latent spaces. The learning algorithm produces control signals for high-dimensional robot systems by estimating policies in a latent space with a significantly lower number of dimensions. The latent space encodes correlations between the controlled DOFs of the robot. The parameters of the policy as well as the projection parameters of the latent space are efficiently estimated from samples during the policy search iterations. The key insight to our algorithm is that policy search as well as dimensionality reduction can be integrated in an expectation-maximization (EM) framework. As a result,

{luck, geri, peters}@ias.tu-darmstadt.de
2 Erik Berger is with the Department of Mathematics and Computer Science, Technische Universit¨ at Bergakademie Freiberg, 09599 Freiberg, Germany

Erik.Berger@informatik.tu-freiberg.de
3 Heni Ben Amor is with the Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, GA 30332, USA

hbenamor@cc.gatech.edu
4 Jan Peters is with the Max Planck Institute for Intelligent Systems, 72076 T¨ ubingen, Germany

we can formulate a coherent algorithmic approach that naturally combines policy search and dimensionality reduction. In contrast to previous attempts for combining reinforcement learning and dimensionality reduction for robotic applications, our approach does not perform dimensionality reduction as a preprocessing step. Instead, the parameters of the latent space are adapted based on the reward signal from the environment. II. Related Work Policy search has attracted considerable attention in the robot learning community. An excellent overview of the topic and detailed descriptions of various state-of-the-art algorithms can be found in [5] and [4]. Previous combinations of dimensionality reduction and policy search, typically use a clear separation between the reinforcement learning algorithm and the dimensionality reduction step. In [6], data from a simulator was used in a preprocessing step to identify a possible low-dimensional latent space of policies using Reduced Rank Regression. Learning on the real robot was then restricted to the extracted latent space. Similarly, Bitzer et al. [7] used user-provided training data to learn a low-dimensional subspace using linear and non-linear dimensionality reduction for robot learning. Using dimensionality reduction as a preprocessing step, or as an independent process that can be executed after several iterations of reinforcement learning, may lead to serious limitations. First, extracting the latent space as a preprocess requires a significantly large training set of (approximate) solutions, prior simulations, or human demonstrations. Even if such data is available, it can be counterproductive to use it, since the reinforcement learning algorithm cannot change the parameters of the latent space in these approaches. For example, when using human demonstrations, e.g., recorded joint configurations, to identify the latent space, the extracted latent space might not be appropriate for controlling the robot as we neglect the correspondence problem [8], i.e., there is no one-to-one mapping of the human joints to the robot joints. Hence, we need to adapt the projection of the latent space during the reinforcement learning process. Using dimensionality reduction as an independent process also leads to a decreased learning efficiency, since it neglects reward information when identifying subspaces. III. Policy Search In the following section, we will introduce the general problem statement for reinforcement learning and dimensionality reduction and introduce the notation that will be used throughout the paper. For a more detailed description of theses topics, the reader is referred to [9] and [10].

A. Problem Statement Reinforcement learning methods can be used to autonomously learn robot control strategies through the interaction with an environment. Given the current state st  S a robot executes an action at  A, transitions into the state st+1 and receives a reward rt (st , at ). The action selection process is governed by the control policy (at |st , t), which is specified as conditional probability distribution over the actions given the current state st . Generally, RL algorithms try to determine an optimal policy which maximizes the expected reward. In this paper, we will focus on policy search methods. Policy search approaches typically use a parametrized stochastic policy represented by a  (at |st , t) with parameters . A typical representation of the policy in robotics is to use a Gaussian distribution as policy where the mean depends linearly on an observed feature vector  of the task, e.g., the location of an object to grasp. The goal of learning is to optimize the expected return of the policy with parameters  with J ( ) =
T

p () R () d,

(1)

where the expectation integrates over all possible trajectories  in the set T. Each trajectory  = [s1:T +1 , a1:T ] is specified by a sequence of length T of states and actions. The return R () of a trajectory is defined as the accumulated immediate rewards rt , i.e.,
T

R () =
t=1

rt (st , at ) + rT +1 ( sT +1 ),

(2)

where rT +1 denotes the final reward for reaching state sT +1 . Note that in many robot applications, the reward function and the policy are explicitly modelled to be time dependent. Due to the Markov property, the trajectory distribution p () can be written as
T

p () = p(s1 )
t=1

p (st+1 |st , at )  (at |st , t) .

(3)

Reinforcement learning algorithms try to determine policy parameters  that maximize Equation 1. B. Expectation Maximization Approaches to Policy Search In contrast to traditional approaches to reinforcement learning, EM-based methods formalize the policy search problem as inference problem with latent variables. They transform the rewards into an improper probability distribution such that the reward can be interpreted as (unnormalized) probability of a binary reward event. In our discussion, we will assume that the rewards have already been transformed to such a improper probability distribution, i.e., the rewards are non-negative. As in

the standard EM-algorithm, we can now optimize a lower bound, that is in this case a lower bound on the expected return, instead of optimizing the original objective. According to Kober and Peters [11], the lower bound of the expected return (1) is given by Lold () = pold () R () log p () d  (4)  T         = IE p ()  Q (st , at , t) log  (at |st , t)  , 
T
old

t=1

where Q is defined as the expected reward to come for time step t, when the robot is in state st and execute action at ,   T          Q (s, a, t) = IE     . (5)  rt~ (st~, at~) |st = s, at = a
~=t t



the exploration of the policy in a lower dimensional latent space. This low-dimensional exploration z is then projected in to the high-dimensional original space by a projection matrix. In order to infer such a model with latent variables, we can again use the expectation maximization algorithm. This time we infer a structured policy from the weighted data points. More specifically we use the marginalization rule [15] to introduce a hidden variable z to our policy by specifying that p (at |st , t) = Z p (at , z|st , t) dz. This step leads to a new lower bound given by   T         IE p ()  Q (st , at , t) log p (at , z|st , t) dz   
old

IE pold

     ()  

t=1 T

Z

t =1

 (7)    Q (st , at , t) IEq(z|at ,st ) log p (at , z|st , t)   ,

In practice, Q (s, a, t) is estimated by a single rollout,
i] [i] i.e, Q s[ t , at , t  T ~=t t i] rt[ ~ , where i denotes the index of



the episode. An important advantage of this approach is that the policy update is formulated as a weighted maximum likelihood (ML) estimate for the parameters , where the reward to come Q (s, a, t) is used as weight for the samples. Due to the weighted ML update, there is no need for a user-specified learning rate which is often a critical factor for achieving good performance in policy gradient algorithms [12]. The policy is typically modelled as linear policy with Gaussian noise. In the PoWER [11] algorithm, this Gaussian noise is added to the parameter vector of the policy, i.e., a = (M + E) . (6)

old where the distribution q(z|at , st ) = p (a |s ,t) is given old t t by the posterior of the latent variables given the old policy parameters old . In this lower bound, the EMalgorithm is applied twice. First, to derive the policy update by weighted maximum likelihood estimates. Second, we use EM to update the joint distribution p (at , z|st , t) instead of the marginal. While this lower bound can be used for any latent variable model, we will discuss our specific case of estimating projection parameters in more detail in the following section.

p

(at ,z|st ,t)

IV. The PePP C Er Algorithm In this section, we will describe the "Policy Search with Probabilistic Principle Component Exploration" Algorithm (PePP C Er) for policy search in low-dimensional latent spaces. We will first start with a short recap of Probabilistic PCA, explain the relevant probability distributions for the PePP C Er algorithm and derive the EM update equations. A. Revisiting Probabilistic PCA Probabilistic Principal Component Analysis (PPCA) is the probabilistic formulation of the PCA algorithm for performing linear dimensionality reduction. PPCA relates a d-dimensional data point x  Rd to a lowdimensional latent variable z  Rn through a linear Gaussian model x = Wz + µ + (8)

M is the mean of the policy and E denotes a Gaussian noise term that is either isotropic or anisotropically distributed. In our experiments, we will use the more commonly used isotropic version of the noise. In contrast to the standard formulation of PoWER [11], we use matrix-variate normal distributions [13] for the exploration noise E  Nd, p 0, 2 I , where 0 has d rows and p columns. We will use the notation Nd, p (·, ·) for such matrix-variate normal distributions and N (·, ·) for multi-variate normal distributions. In the remainder of this paper, we will write the stochastic policy  (at |st , t) as p (at |st , t) to ensure consistent notation. C. Using Structured Policies with Latent Variables Another important advantage of weighted ML updates, is that we can use structured policy representations that again include latent variables z. For example, mixture models [14] or low-dimensional factor models can be used. In our specific case, the latent variable defines

where the latent variable z  Rn is Gaussian distributed according to p (z) = N (0, I). The transformation matrix W  Rd×n maps each low-dimensional vector z to the high dimensional space. The matrix W spans a lowdimensional subspace and µ  Rd is the mean of

the high-dimensional distribution. A high dimensional isotropic noise  Rd with zero mean and 2 I variance is added to this projection. The parameters of this model are given by µ, 2 and W and can efficiently be estimated using an EM algorithm (see [10] for details). However, PPCA is a unsupervised learning method while policy search is supervised. B. Deriving the Update Equations for PePP C Er Building on the insights from PPCA, we can decompose a stochastic policy into a low-dimensional distribution and projection parameters for generating the required high-dimensional action. More specifically, we can write a = W Z  + M + E,
T

and p a|ZT  = N W ZT  + M, 2 tr T I , the posterior distribution can be written as pold ZT |a = N CWT (a - M) , C2 tr T , (15) -1 T 2 where C =  I + W W . Given this posterior distribution, we can now determine the equations of the expectation step IE p (ZT |a) ZT  = CWT (a - M) , old IE p (ZT |a) ZT  ZT  old
T

(14)

(16) (17)

= C2 tr T
T

(9)

where W is a projection matrix. The terms M and E are again the mean and the Gaussian noise term. The term ZT  with Z  N p,n (0, I) generates an exploration noise in a low-dimensional latent space, which is then projected into the high-dimensional space of actions via W. Due to the projection from the latent space to the original high dimensional state, the uncorrelated explorative action from the latent space becomes a correlated action in the high dimensional space. Hence, the projection matrix W can be understood as a matrix that defines synergies in the action space that are used for correlated exploration. Both, the mean M of the policy and the projection matrix W are learned by the policy search algorithm. Given the model in Equation 9, we can derive the expectation of our probability distribution p (a) in a straight-forward fashion IE [a] = IE W ZT  +M + IE E = M.
0 0

+IE p (ZT |a) ZT  IE p (ZT |a) ZT  . old old 1. Maximization Step for M We use a maximum likelihood estimate to identify the value of M in each iteration. To this end, we calculate the derivative of the log-likelihood function w.r.t. M,  ln p (a) = D-1 aT - MT , M (18)

where D = tr T WWT + 2 I = DT . After inserting this result into the EM policy search framework and set the derivative to zero, we get  T     ln p (at ) Q   t     0 = IE pold ()  (19)     M t =1  T   T -1           at T Q T Q       t  t              M = IE pold ()  IE ( )        p    old T  T  tr  tr  t=1 t=1 such that M maximizes the log-likelihood function ln p (a). 2. Maximization Step for W For optimizing W we have to use the new lower bound given in Equation 7 and set the derivative of this term w.r.t W to zero. Accordingly the derivative can be written as  ln p a, ZT  W -a ZT 
T

(10)

Similarly, we can also use the properties of matrixvariate normal distributions [13] to get the covariance cov (a) = IE (a - IE [a]) (a - IE [a])T = IE WZT T ZWT + IE ET ET = tr T WWT + 2 I , where tr (·) denotes the trace of a matrix. From Equation 10 and Equation 11 it follows that the prior distribution over actions is p (a) = N M, tr T WWT + 2 I . (12) (11)

= - 2 tr T + WZT  ZT 
T

-1 T

+ M ZT 

(20)

Now, in order to apply EM, we have to determine the posterior distribution p (Z|a) over matrices Z. The posterior distribution can be simplified by treating ZT  as a latent variable. Since the result of this product is a vector, we can use Bayes theorem for Gaussian variables [15, p.93] to derive the posterior distribution p ZT |a . Given both distributions p Z  = N 0, tr 
T T

from which follows that the optimal value of W that maximizes the log-likelihood is given by   T T   T (a - M) IE  t   pold (ZT |a) Z  Qt       W = IE pold ()      T   tr  t =1   -1 T        T IE p T ZT  ZT  Q     t  old (Z |at )                 IE .    pold ()      T         tr 
t =1

I

(13)

(21)

3. Maximization Step for 2 Similarly to the estimation of W, we can also derivate the log-likelihood of ln p a, ZT  with respect to 2 in order to identify a new estimate of 2 with  ln p a, ZT 
-1 d 2 2 T + 2  tr  2 22 T T a - WZ  - M a - WZT  - M . (22)

Input: Initialized parameters 2 0 , W0 and M0 and the dimensionality n of the low dimensional manifold. The function  (st , t) represents the feature vector for the policy. repeat

=-

Setting the above derivative to zero leads to the following maximum-likelihood estimate of the variance:  T  -1 1  2  tr T  = IE pold ()    d t=1 (at - M)T (at - M) -2 (at - M)T WIE p (ZT |at ) ZT  old +tr IE p (ZT |at ) ZT T Z WT W Q t old   T -1               Qt      IE pold ()     . (23)
t=1

Sampling: for h=1:H do # Sample the H rollouts for t=1:T do T ah t = Wi Z  + Mi  + E with Z  N p,n (0, I) and E  Nd, p 0, 2 iI Execute action ah t h Observe and store reward rt sh t , at Calculate weights: Q (s, a, t) = IE
T ~=t t

rt~ (st~, at~) |st = s, at = a

Expectation: foreach ah t do Compute IE p (ZT |ah ZT  with (16). t) old Compute IE p (ZT |ah ZT  ZT  t) old Maximization: Compute Mi+1 with (19). Compute Wi+1 with (21). Compute 2 i+1 with (23). until Mi  Mi+1 Output: Linear weights M for the feature vector .
T

with (17).

C. Complete Algorithm The resulting algorithm that implements all of the above steps can be found in Alg. 1. The initial values for the parameters 2 , W and M can either be randomly chosen or initialized using a PPCA on a set of demonstrations. Additionally, the algorithm requires the number of latent dimensions n as input. After convergence, a policy is given by a weight matrix M which is multiplied by the feature vector  (s, t) to receive an action for a given state and time. V. Experiments The PePP C Er Algorithm has been evaluated on a simulated and a real-world robot task. In this section, we will describe the experimental setup of these evaluations and present the achieved results in comparison to PoWER and the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) [16] algorithm. A. Learning Inverse Kinematics In our first experiment, we will focus on learning inverse kinematics. A range of methods exist for analytically or numerically solving the inverse kinematics problem. However, various researchers have also looked at inverse kinematics from a machine learning point of view [17]. In our experiment, we use a simulated robot with d hinge-joints and d + 1 segments. The goal of the simulated robot is to track the position of a sphere that is moving on a circle. Setting d to values higher than two results in a redundant system with more DOF

Algorithm 1: Policy Search with Probabilistic Principle Component Exploration in the Action Space (PePP C Er)

than required to accomplish the task. To learn inverse kinematics, we set the reward function to rt (st , at ) = e-D , (24)

where D is the distance of the end-effector to the target, when action at is executed. Then, we use PePP C Er to determine a suitable policy for the task. During the optimization process, PePP C Er uncovers the redundancies of the system by determining the low-dimensional latent space of joint angle configurations that lead to touching the target. The latent space models the co-articulation of different links. An example result of a learned policy can be found in Fig. 2. As can be seen in the figure, a 20 linked robot arm successfully tracks the target along a circular path. We ran the explained setup with different specifications of policy search algorithms resulting in the graph depicted in Fig. 3. The graph depicts the sum

60

50

Sum. Distances

40

30

Fig. 2: A tentacle-like robot with 20 links tracks a target along a circular path. of distances of the end-effector to the target positions. For a balance evaluation, we compared to two different implementations of the PoWER algorithm. In one implementation the 2 was static, while in the other implementation an automatic adaptation of a diagonal covariance matrix was performed. This feature was also implemented in the PePP C Er algorithm, which results in a slightly different update equation for 2 . In each iteration 30 samples were drawn and executed on a simulated 20-linked robot. As features we used 19 time-dependent Gaussians, so we had to estimate 380 parameters for 50 time steps. We repeated each experiment 10 times and calculated the mean (bold lines) and standard deviation of the results (light colors around the mean). The figure shows that PePP C Er outperforms CMA-ES and PoWER. In particular in the early iterations both policy search methods perform comparatively well. At the same time, we can see that both PoWER implementations start to stagnate at around 50 iterations. PePP C Er continues to reduce the distance to the targets.

20

10

0

1

2

3

4 5 6 7 8 9 Number of Latent Dimensions

10

11

12

Fig. 4: The mean sum of distances and the standard deviation between the 450th and 500th iteration for an 12-linked robot. Five solutions were learned by PePP C Er for different values of the dimensionality n of the latent space. latent space was set to n = 5. In order to evaluate the effect of this parameter on the results, we repeated the evaluation of PePP C Er with varying values for n in an inverse kinematics task for a 12-linked robot, as can be seen in Fig. 4. In the depicted graph, we can see a bump in the average distance at around 5 and 9 dimensions. This is an interesting phenomenon of latent space policy search: too small a value for n restricts the search space, too high a value for n diminishes the effect of dimensionality reduction. In our specific example, the best value for n seems to be 4 or 5. B. Learning to Stand on One Leg We also performed a learning task on a real robot. More specifically, we used PePP C Er to learn policies for standing on one leg. The task of standing on one leg is a synergistic motor skill that requires the co-articulation of different body parts for successful execution. It is often used in biomechanical studies on synergies and low-dimensional control in humans, such as in [18]. In our experiment, we set the episodic reward of the robot proportional to the height of the right leg after execution of the policy. Furthermore, we have to consider in our reward function, that the head and the right foot of the robot should not move a lot. Hence, the reward function can be written as R(h, rf , lf ) = exp { · h +  · rf -  · lf - MAX } , (25) where , ,   R+ , h is the height of the head, rf the height of the right foot and lf the height of the left foot in the final position. The constant MAX is the maximal possible value of the first part of the sum. The height of the head is responsible for a low reward if the robot falls

Fig. 3: Comparison between PePP C Er, PoWER and CMA-ES on the inverse kinematics task with a 20linked robot. In each iteration we executed 30 different joint configurations on the simulated robot. For the static PoWER we set  = 15. For the dynamic PoWER and PePP C Er we computed the diagonal covariance matrix. In the above experiment, the dimensionality of the

Acknowledgment The research leading to these results has received funding from the European Union under grant agreement #270327 (CompLACS). References
[1] A. Y. Ng, A. Coates, M. Diel, V. Ganapathi, J. Schulte, B. Tse, E. Berger, and E. Liang, "Autonomous inverted helicopter flight via reinforcement learning," in Proceedings of the International Symposium on Experimental Robotics, 2004, pp. 363­372. [2] K. Muelling, J. Kober, O. Kroemer, and J. Peters, "Learning to select and generalize striking movements in robot table tennis," International Journal of Robotics Research, no. 3, pp. 263­279, 2013. [3] J. Zico Kolter and A. Y. Ng, "The stanford littledog: A learning and rapid replanning approach to quadruped locomotion," Int. J. Rob. Res., vol. 30, no. 2, pp. 150­174, Feb. 2011. [4] J. Kober and J. Peters, "Reinforcement learning in robotics: a survey," in Reinforcement Learning. Springer Berlin Heidelberg, 2012, pp. 579­610. [5] M. P. Deisenroth, G. Neumann, and J. Peters, "A survey on policy search for robotics," Foundations and Trends in Robotics, vol. 2, no. 12, pp. 1­142, 2013. [6] J. Z. Kolter and A. Y. Ng, "Learning omnidirectional path following using dimensionality reduction," in in Proceedings of Robotics: Science and Systems, 2007. [7] S. Bitzer, M. Howard, and S. Vijayakumar, "Using dimensionality reduction to exploit constraints in reinforcement learning," in Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ International Conference on, Oct 2010, pp. 3219­3225. [8] C. L. Nehaniv and K. Dautenhahn, "Imitation in animals and artifacts," K. Dautenhahn and C. L. Nehaniv, Eds. Cambridge, MA, USA: MIT Press, 2002, ch. The Correspondence Problem, pp. 41­61. [9] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction. MIT Press, 1998. [10] M. E. Tipping and C. M. Bishop, "Probabilistic principal component analysis," Journal of the Royal Statistical Society, Series B, vol. 61, pp. 611­622, 1999. [11] J. Kober and J. Peters, "Policy search for motor primitives in robotics," Machine Learning, vol. 84, no. 1-2, pp. 171­203, 2011. [12] J. Peters and S. Schaal, "Reinforcement learning of motor skills with policy gradients," Neural networks, vol. 21, no. 4, pp. 682­ 697, 2008. [13] A. K. Gupta and D. K. Nagar, Matrix variate distributions. CRC Press, 2000, vol. 104. [14] C. Daniel, G. Neumann, and J. Peters, "Learning concurrent motor skills in versatile solution spaces," in Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, Oct 2012, pp. 3591­3597. [15] C. M. Bishop and N. M. Nasrabadi, Pattern recognition and machine learning. Springer New York, 2006, vol. 1. [16] N. Hansen, S. Muller, and P. Koumoutsakos, "Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES)." Evolutionary Computation, vol. 11, no. 1, pp. 1­18, 2003. [17] K. Grochow, S. L. Martin, A. Hertzmann, and Z. Popovi´ c, "Stylebased inverse kinematics," ACM Trans. Graph., vol. 23, no. 3, pp. 522­531, Aug. 2004. [18] G. Torres-Oviedo and L. H. Ting, "Subject-specific muscle synergies in human balance control are consistent across different biomechanical contexts," Journal of neurophysiology, vol. 103, no. 6, pp. 3084­3098, 2010.
Policy 2 Policy 1

Fig. 5: Two different policies for standing on one leg learned using latent space policy search. Only 100 samples were needed to learn policy 1.

during learning. As features, time-dependent Gaussians were used in this experiment.Actions were represented by the change in the 25 robot joint angles between two consecutive time steps. The goal in robot learning is to learn from few trials. We therefore restricted the maximum number of samples (executions on the robot) to 600 samples. For automation and repeatability purposes, learning was performed in a physics-based simulator. However, we want to stress that, given the relatively small number of trials needed by PePP C Er to learn a policy, we can also perform learning directly on the real robot. Fig. 5 shows two learned policies acquired using PePP C Er. Learning started from random initializations and did not require any demonstrations. Policy 1 was learned using a sample size of 20 samples and 5 iterations, i.e., 100 execution on the robot in total. We can see, that it results in a smooth and stable motor skill. Policy 2 required 600 evaluations in total and allows the robot to lift the leg even higher. VI. CONCLUSIONS In this paper we presented a novel policy search algorithm for robotics applications. The PePP C Er algorithm determines the correlations between different joints of the robot and uses the information for fast and efficient reinforcement learning. The presented method combines policy search and dimensionality reduction in a natural way and has been derived from basic principles. Applications on a simulated and a real robot indicate that the approach can be employed to learn new motor skills for complex, redundant robots using a relatively small number of trials on the robot. In our future work we want to combine the introduced approach with imitation learning, in order to start in a good region of the search space. Additionally, we want to investigate methods for identifying the dimensionality of the current task.

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/263458522

OnlineCameraRegistrationforRobot Manipulation
ConferencePaper·June2014
DOI:10.1007/978-3-319-23778-7_13

CITATIONS

READS

2
4authors,including: NeilDantam RiceUniversity
23PUBLICATIONS101CITATIONS
SEEPROFILE

65

HeniBenAmor ArizonaStateUniversity
56PUBLICATIONS434CITATIONS
SEEPROFILE

HenrikIskovChristensen UniversityofCalifornia,SanDiego
478PUBLICATIONS6,419CITATIONS
SEEPROFILE

AllcontentfollowingthispagewasuploadedbyHeniBenAmoron27June2014.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Online Camera Registration for Robot Manipulation
Neil Dantam, Heni Ben Amor, Henrik Christensen, and Mike Stilman
Institute for Robotics and Intelligent Machines, Georgia Institute of Technology Atlanta, GA 30332, USA. ntd@gatech.edu, hbenamor@cc.gatech.edu, hic@cc.gatech.edu, mstilman@cc.gatech.edu

Abstract. We demonstrate that millimeter-level manipulation accuracy can be achieved without the static camera registration typically required for visual servoing. We register the camera online, converging in seconds, by visually tracking features on the robot and filtering the result. This online registration handles cases such as perturbed camera positions, wear and tear on camera mounts, and even a camera held by a human. We implement the approach on a Schunk LWA4 manipulator and Logitech C920 camera, servoing to target and pre-grasp configurations. Our filtering software is available under a permissive license.1

1

Introduction

Using visual feedback for robot manipulation requires registration between the camera and the manipulator. Typically, this is viewed as a static task: registration is computed offline and assumed to be constant. In reality, camera registration changes during operation due to external perturbations, wear and tear,

kinematics

kinematics

B

Sf

registration vision registration vision
C

Sf

B

SC

Camera Perturbation

Controlled Motion

Uncontrolled Motion

Fig. 1. Use cases for online camera registration. We combine the visual and kinematic pose estimates of end effector and filter the result to estimate the camera pose in robot body frame.
 1

This work supported by a grant from Peugeot S.A. software available at http://github.com/golems/reflex

2

Dantam, Ben Amor, Christensen, Stilman

or even human repositioning. For example, during the recent DARPA Robotics Challenge trials, impacts from falls resulted in camera issues which significantly affected the robot behavior for some teams [10]. Fig. 1 shows additional use cases which may change the camera pose. The pose registration process should be treated as a dynamic task in which the involved parameters are continuously updated. Such an online approach to pose registration is challenging, since it requires the constant visibility of a calibration reference and sufficient accuracy to perform manipulation tasks. To address changes in camera pose during operation, we propose an online camera registration method that combines (1) visual tracking of features on the manipulator, (2) a novel expectation-maximization inspired algorithm for pose filtering and tracking, and (3) an special Euclidean group constrained extended Kalman filter. Our key insight is to use the robot body as a reference for the registration process. By tracking known patterns or objects on the robot, we can continuously collect evidence for the current camera pose. However, na¨ ive filtering of these pose estimates can lead to large variances in the calculated poses. The challenge is obtaining sufficient accuracy for manipulation through the online registration. To address this challenge, we combine pose filtering and manipulator control, incorporating camera registration into our manipulation feedback loop. This paper presents a method for online registration and manipulation that combines object tracking, pose filtering, and visual servoing. First, we use perceptual information to identify the pose of specific features on the end-effector of the controlled robot (see Sect. 3.1). Then, we perform an initial fit to find offsets of the features on the robot, (see Sect. 3.2). A special Kalman filter is, then, used in conjunction with median filtering in order to perform online registration of the camera (Sect. 3.3). In our evaluation (see Sect. 4), we investigate the accuracy of the proposed method by applying it to robot grasping and manipulation tasks.

2

Related Work

Typical camera registration methods collect a set of calibration data using an external reference object, compute the calibration, then proceed assuming the calibration is static. OpenCV determines camera registration from point correspondes, typically using a chessboard [15]. Pradeep, et. al, develop a camera and arm calibration approach based on bundle adjustment and demonstrate it on the PR2 robot [16]. This approach requires approximately 20 minutes to collect data and another 20 minutes for computation, a challenge for handling changing pose online. Visual servo control incorporates camera feedback into robot motion control [2,3]. The two main types of visual servoing are image-based visual servo control (IBVS), which operates on features in the 2D image, and position-based visual servo control, which operates on 3D parameters. Both of these methods assume a given camera registration. While IBVS is locally stable with regard to pose

Online Camera Registration for Robot Manipulation

3

errors, under PBVS, even small pose errors can result in large tracking error [2]. Our proposed method addresses these challenges by correcting the camera registration online. In our experiments we show the importance of treating the registration process as a dynamic task. Furthermore, we show that our online registration achieves millimeter positioning accuracy of the manipulator. This is particularly important for grasping tasks performed using multi-fingered robot hands [1]. During such grasping tasks, inaccuracies in perception and forward kinematics often lead to premature contact between one finger and the object. As a result of the ensuing object movement, the intended grasp might not be satisfactorily executed or may fail altogether. Other recent work has explored online visual parameter identification. [11] tracks a robot arm to identify encoder offsets. This method assumes a given camera registration, but is also tolerant of some registration error. In contrast, our work identifies the camera registration online, but does not explicitly consider encoder offsets. [19] uses maps generated from a Simultaneous Localization and Mapping (SLAM) algorithm to calibrate a depth sensor. In our approach, unlike typical environments for SLAM, the object to which we are trying to register our camera ­ the manipulator ­ will necessarily be in motion.

3

Technical Approach

We determine the pose registration between the camera and the manipulator by visually tracking the 3D pose of the arm. We identify the pose of texture or shape features on the arm and fit a transformation based on the corresponding kinematic pose estimates of those features. To obtain sufficient accuracy for manipulation, we combine several methods to fit and filter the visual pose esti-

Position Servo
B

Workspace Control
-1

e,r = ln

B

Se,a 

B

SC  CSo



r = J+ 

-kx

x  

- k (J + J - I )  (joint velocity) 

B

SC
B

C

EKF

So

SC

med
B

B

SC

-1 S0  CSf ...

Registration Filter
C

 (angles) Median filter So
C

0

ROBOT image

EKF

C

So

Sf0 . . . CSfn Feature Estimator

(target pose)

(feature poses)

Object Filter

Fig. 2. Block Diagram of Control System. 3D poses for features are estimated from visual data. The median camera transform is computed over all features and then Kalman filtered. With this registration, the robot servos in workspace to a target object location.

4

Dantam, Ben Amor, Christensen, Stilman

Fig. 3. Marker-based tracking (left) and model-based tracking (right).

mates before servoing to the target object. This estimation and control loop is summarized in Fig. 2. For computational reasons, we used the dual quaternion representation for the special Euclidean group SE (3). Compared to matrices, the dual quaternion has lower dimensionality and is more easily normalized, both advantages for our filtering implementation. The relevant dual quaternion equations are summarized in appendix A. We represent the dual quaternion S for a transformation implicitly as a tuple of a rotation quaternion q and translation vector v : S = (q , v ). This requires only seven elements. For Euclidean transformations, we use the typical coordinate notation where leading superscript denotes the parent frame and following subscript denotes the child frame, i.e., xSy gives the origin of y relative to x. The transformation aSb followed by bSc is given as the dual quaternion multiplication aSb  bSc = aSc .

3.1

Feature Estimation

To use the robot body as a reference for camera registration, it is important to identify and track body parts, e.g., the end-effector, in 3D. These 3D poses can be estimated with marker-based [17] and model-based approaches [4], see Fig. 3. Marker-based approaches require binary fiducials to known locations on the robot, such as the fingers. Model-based tracking, on the other hand, requires accurate polygon meshes of the tracked object. In our implementation, we use the ALVAR library [17] for marker-based tracking. For model-based tracking, we use the approach from [4]. In each frame the 3D pose of the object is computed by projecting a 3D CAD model into the 2D image. After projection, we identify salient edges in the model and align them with edges in the 2D image. A particle filter is then used to filter the pose estimates over time. Both marker-based and model-based tracking provide 3D pose estimates of tracked features, but with frequent outliers and noise. Markers have the advantage of being easy to deploy, while model-based tracking can deal with partial occlusions of the scene.

Online Camera Registration for Robot Manipulation

5

3.2

Offset Identification

To improve the accuracy of kinematic pose estimates for features, we initially perform a static expectation-maximization-like [6] procedure, based on the following model: B Sk  kSf = BSC  CSf (1) where BSk is the measured nominal feature pose in the body frame determined from encoder positions and forward kinematics, kSf is the unknown static pose offset of the feature due to inaccuracy of manual placement, BSC is the unknown camera registration in the body frame, and CSf is the visually measured feature pose in the camera frame. These transforms are summarized in Fig. 1, with B Sk  kSf combined as BSf . As an initialization step, we iteratively fix either kSf or BSC in (2) and solve for the other using Umeyama's algorithm [20]. This gives us the relative transforms for the features kSf which we assume are static. 3.3 Filtering

To compute the online registration, where BSC is changing, we combine median and Kalman filtering. The median filter is applied independently at each time step to reject major outliers in the estimated feature poses. Compared to weighted least squares methods, the median requires no parameter tuning and is especially resistant, tolerating outliers in up to 50% of the data [8]. Given the median at each step, the Kalman filter is applied over time to generate an optimal registration estimate under a Gaussian noise assumption. Based on (2), each observed feature on the robot gives on estimate for the camera registration BSC :
B

Sk  kSf  (CSf )-1 = BSC

(2)

Median Filtering At each time step, we find the median registration over all observed features. Each observed feature gives a candiate registration BSC . First, we collect a set Q of the orientation candidates:
1 Q = (BqC )i | (BSk )i  kSf  (CSf )- i

(3)

Then, we compute the median of the candidate orientation registrations Q. To find this median, the structure of rotations in SO(3)offers a convenient distance metric between two orientations: the angle between them. Using this geometric interpretation, the median orientation q ^ is the orientation with minimum angular distance to all other orientations.
n Bq C

= arg min
qi Q j =0

 | ln(qi  qj )|

(4)

The median translation x ^ is the conventional geometric median, the translation with minimum Euclidean distance to all other translations. First, we find

6

Dantam, Ben Amor, Christensen, Stilman

the set of candidate translations Z by rotating the feature translation in camera frame Cvf and subtracting from the body frame translation Bvf : Z = zi | zi = Bvf,i - BqC  Cvf,i  BqC


(5)

Then, we compute the geometric median of the candidate translations by finding the element with minimum distance to all other elements:
n Bv C

= arg min
zi Z j =0

|zi - zj |

(6)

Then, the median transform is the combination of the orientation and translation parts:
BS C

=

Bq , Bv C C

(7)

Kalman Filtering Next, we use an Extended Kalman filter (EKF) to attenuate noise over time, taking care to remain in the SE (3) manifold. Similar Kalman filters are discussed in [13,5]. The quasi-linearity of quaternions means the EKF is suitable for orientation estimation in this application [12]. To filter SE (3) poses, we consider state x composed of a quaternion q , a translation vector v , and the translational and rotational velocities, v  and  : x = (q , v ) = [qx , qy , qz , qw , vx , vy , vz , v x, v y , v  z , x , y , z ] The measurement z is the pose: z = (q , v ) = [qx , qy , qz , qw , vx , vy , vz ] The general EKF prediction step for time k is: x ^k|k-1 = f (xk ^ -1 ) Fk - 1 = f x
x ^k-1|k-1

(8) (9) (10)

T Pk|k-1 = Fk-1 Pk-1|k-1 Fk -1 + Qk-1

where x ^ is the estimated state, f (x) is the process model, F is the Jacobian of f , P is the state covariance matrix, and Q is the process noise model. The process model then integrates the translational and rotational velocity, staying in the SE (3) manifold using the exponential of the twist  :  (, v,  v ) = (, v ×  + v ) f (x) = exp t  2  (q, v ) (11)

Online Camera Registration for Robot Manipulation

7

Now, we find the process Jacobian F . The translation portion is a diagonal matrix of the translational velocity. For the orientation portion, we find the quaternion derivative q  from the rotational velocity: = q 1 q 2 (12)

This quaternion multiplication can be converted into the following matrix multiplication: 1 1   q = Mr (q )  2 2  qw qz -qz qw Mr (q ) =   qy -qx -qx -qy

 -qy qx   qw  -qz

(13)

Note that we omit the w column of the typical quaternion multiplication matrix because the w element of rotational velocity  is zero. This gives the following process 13 × 13 Jacobian F :   1 I4×4 0 2 tMr q 0  0 I3×3 0 tI3×3   F = (14)  0 0 I3×3 0  0 0 0 I3×3 Now we consider the EKF correction step. The general form is: z ^k = h(^ xk|k-1 ) h Hk = x
x ^ k |k - 1

(15) (16) (17) + Rk (18) (19) (20) (21)

yk = v (zk , z ^) Sk = Hk Pk|k-1 =
T Hk Pk|k-1 Hk T Sk Kk

x ^k|k = p(^ xk|k-1 , Kk yk ) Pk|k = (I - Kk Hk )Pk|k-1

where z is the measurement, h is the measurement model, H is the Jacobian of h, z ^ is the estimated measurement, R is the measurement noise model, and K is the Kalman gain, v is a function to compute measurement residual, and p is a function to compute the state update. We compute the EKF residuals and state updates using relative quaternions to remain in SE (3) without needing additional normalization. The observation h(x) is a pose estimate: h(x) = (q, v ) H = I7×7 (22)

8

Dantam, Ben Amor, Christensen, Stilman

We compute the measurement residual based on the relative rotation between the measured and estimated pose: v (z, z ^) = (yq , yv )
 yq = ln zq  z ^q q

yv = zv - z ^v

(23)

where yq is the orientation part of the residual and yv the translation part. Note  corresponds to a velocity in the direction of the relative that that ln zq  z ^q transform between the actual and expected pose measurement and that we can consider yq as a quaternion derivative. Then, the update function will integrate the pose portion of y , again using the exponential of the twist. First, we find the twist corresponding to the product of the Kalman gain K and the measurement residual y : (Ky ) = (Ky )q  q   (Ky, v ) = ((Ky ) , v × (Ky ) + (Ky )v ) (24) Then, we integrate estimated pose using the exponential of this twist: (x(q,v) )k|k = exp t  2  (q, v ) (25)

Finally, the velocity component of innovation y is scaled and added: (x,v  )k|k = x,v  + (Ky ),v  3.4 Registered Visual Servoing (26)

We use the computed camera registration BSC to servo to a target object according to the control loop in Fig. 2. This is position-based visual servoing, incorporating the dynamically updated registration. First, we compute a reference twist Be,ref from the position error using camera pose BSC and object pose CSo :
B B

Se,ref = BSC  CSobj
B -1 Se,act  BSe,ref

(27) (28)

e,ref = ln

Then, we find the reference velocity for twist Be,ref : x  D(Be,ref ) - (2D(BSe )  R(BSe )-1 ) × R(Be,ref ) =  R(Be,ref ) where R(X ) is the real part of X and D(X ) is the dual part of X . (29)

Online Camera Registration for Robot Manipulation

9

Finally, we compute joint velocities using the Jacobian damped least squares, also using a nullspace projection to keep joints near the zero position:   r = J + -kx x   - k (J + J - I ) (30)

where J is the manipulator Jacobian matrix, J + is its damped pseudoinverse, kx is a gain for the position error, and k is a gain for the joint error.

4

Experiments

We implement this approach on a Schunk LWA4 manipulator with SDH endeffector, see Fig. 1, and use a Logitech C920 webcam to track the robot and objects. The Schunk LWA4 has seven degrees of freedom and uses harmonic drives, which enable repeatable positioning precision of ±0.15mm [7]. However, absolute positioning accuracy is subject to encoder offset calibration and link rigidity. In practice, we achieve ±1cm accuracy when using only the joint encoders for feedback. The Logitech C920 provides a resolution of 1920x1080 at 15 frames per second. To measure ground-truth distances, we used a Bosch DLR165 laser rangefinder and a Craftsman 40181 vernier caliper. We initially test the convergence and resistance of our approach while moving the camera. With the camera mounted on a tripod, we compute the filtered registration while the camera is perturbed, rotated, and translated. The resulting registrations under moving camera are plotted in Fig. 4. The visual pose estimates contain frequent outliers in addition to a small amount of noise. The filtered registration removes the outliers and converges within 5s. To demonstrate the suitability of this approach for manipulation tasks, we test the positioning accuracy attainable with this online registration. As shown in Fig. 5, we place a marker on a table, measure linear distance to the marker with a laser ranger, servo the end-effector to the visually estimated marker position using the control loop in Fig. 2, and measure the distance to the end-effector which should be directly over the marker. The resulting position accuracy achievable with online registration is summarized in Table 1. For an ideal camera placement with close, direct view of the end-effector (i.e. the angle  between the camera and the markers is 45 or less), positioning accuracy is in the submillimiter range. Larger camera distances and angles, resulted in positioning error of 1 - 2 millimeters. Finally, we test the pre-grasp positioning accuracy of this method as shown in Fig. 6. We place an object, in particular, a cup, at a variety of locations on the table, servo the end-effector to the visually detected object position using the control loop in Fig. 2, and then measure the distance of each finger to the object using a vernier caliper. The results of the pre-grasp positioning are summarized in Table 2. A small number of trials resulted in centimeter-level error for objects placed near the edge of the image frame. Ommitting these outliers, the average positioning error of the pre-grasp configuration was 3.3mm.

10

Dantam, Ben Amor, Christensen, Stilman

0.5 quaternion 0 -0.5 -1 0 10

translation (m)

x y z w

1.5 1 0.5 0 -0.5 0 10

x y z

20 30 time (s) (a) x y z w

40

20 30 time (s) (b) x y z

40

0.5 quaternion 0 -0.5 -1 0 10

1.5 translation (m) 40 1 0.5 0 -0.5 0 10

20 30 time (s) (c)

20 30 time (s) (d)

40

Fig. 4. Registration while camera is bumped (8s), rotates (15s) and translated (24s). camera is bumped. (a)-(b) registration from raw visual pose estimates of one feature. Contains many outliers. (c)-(d) filtered registration. Outliers and noise eliminated.

5

Experimental Insights

There are a number of error sources we must handle in this system. For the kinematics, error from encoder offsets in the arm, imprecise link lengths, and flexing of links all contribute inaccurate kinematic pose estimates. For perception, error from inaccurate camera intrinsics, imprecise fiducial sizes, offsets in object models, and noise in the image all contribute to error in visual pose estimates. To achieve accurate manipulation, we must account for these potential sources of error. The key point of the servo loop in Fig. 2 is that we depend not on minimizing absolute error, but on minimizing relative error. We are minimizing error between end-effector pose Se and target pose So . Because we continually update the camera registration, we effectively minimize this error in the image. As long as there is distance between camera frame poses CSe and CSo , we will move the end-effector towards the target, and as long as the visual distance estimate is zero when we reach the target, the arm will stop at the target. Thus, even if there is absolute registration error due to, e.g., unmodeled lens distortion, it is only necessary that relative error between visual estimates of the end-effector and target be small and converge to zero. The relative error between end-effector and

Online Camera Registration for Robot Manipulation

11

Laser

Fig. 5. Experimental setup for evaluating the positioning accuracy during camera registration. A cube was placed on a marker and the distance to a laser ranger was captured. Subsequently, the cube was placed in the hand of the robot, which, then, servoed to the position of the marker. Again, the distance was measured using the laser ranger.

target is crucial in manipulation, and our technique is well suited to minimizing this error. The position of the tracked features on the robot has an important effect on error correction. Kinematic errors between the robot body origin and the tracked features, e.g., due to flex or encoder offsets, are incorporated into the camera registration and handled through the servo loop. Error between the observed features and the end-effector cannot be corrected. Thus, it is better to track features as close to the end-effector as possible. Consequently, we placed the fiducual markers on the fingers of the SDH end-effector. The principal challenge in the implementation stems from observing the robot pose using small,  3cm, markers. While marker translation is reliably detected, outliers in orientation are frequent. Ample lighting improves detection but does not eliminate outliers. The median pose, (4)-(6), was effective at eliminating outliers from visual estimates. Alternative methods for combining orientations estimates include Davenport's q-method [14] and the Huber loss function [9]. In contract to these other methods, the median has no parameters such as thresholds which require adjustment. Thus, it is especially suited to this online registration application where outlier frequency may vary depending on camera placement, lighting, etc. A potential challenge is that the direct computation of
Setup Average Stdev   45 0.5mm 0.52mm  > 45 1.5mm 1.26mm Table 1. Positioning experiment results. Average and standard deviation [mm] of measured difference between commanded position and object location. Data Average Stdev All 5.8mm 8.5mm Inliers 3.3mm 2.3mm Table 2. Pre-grasp experiment results. Average and standard deviation [mm] of measured difference between object and end-effector position

560.10mm

12

Dantam, Ben Amor, Christensen, Stilman

Fig. 6. Pre-grasp experiment: using the introduced camera registration, the open robot hand is servoed to the position of a glass. The distances between the fingers and the glass are then measured. Since the glass is rotationally symmetric, the distances of both used robot fingers should be identical in the ideal case.

(4) leads to an O(n2 ) algorithm in the number of orientations. However, for the small number of poses we consider at each step here, the computation time is negligable. On a Xeon E5-1620 CPU, computing the median of 32 orientations requires 30µs.

6

Conclusion

We have presented an online method to identify the camera poses for robot manipulation tasks. This is useful for the typical case where camera registration is not static but changes due to model error, disturbances, or wear and tear. The key point is to track both the object and the robot in the image, and servo based on the visually estimated relative pose between the object and robot. By combining median and Kalman filtering of the registration pose, we are able to achieve millimeter-level manipulation accuracy. We have shown in our experiments that online registration can be used to improve positioning accuracy during grasping and manipulation tasks, thereby avoiding typical challenges such as premature contact between fingers and objects. A useful extension to this work would be to handle online registration with multiple cameras. This could provide additional data to improve accuracy or permit greater field of view, e.g., observing both hands in bimanual tasks. We anticipate that considering median deviation and applying a similar extended Kalman filter to multiple simultaneous poses will extend this online approach to multi-camera setups.

Online Camera Registration for Robot Manipulation

13

Acknowledgements
This work would not have been possible without Mike Stilman's tireless guidance and support.

A

Dual Quaternion Computation

Dual quaternions are a numerically convenient representation for Euclidean transformations, SE (3). Compared to ordinary quaternions which can represent rotation, dual quaternions can represent both rotation and translation. Mathematically, they are the extension of quaternions to the dual numbers [18]. Dual numbers are of the form r + d, where r is real part, d is the dual part, and  is the dual element such that  = 0 and 2 = 0. A dual quaternion S can be represented as  a pair of quaternions, S = sr + sd , which we represent with the  tuple sr , sd . We represent the vector and scalar components of the ordinary quaternion parts of a dual quaternion as:   S = r , d  =    rx i + ry j + rz k , rw , dx i + dy j + dz k , dw  =   (rv , rw ) , (dv , dw ) (31) where rv and dv are the vector parts and rw and dw are the scalar parts. The dual quaternion representing orientation q and translation v is:     1  q , v  q  S = sr , sd  =  2 (32)

Dual quaternion Euclidean transforms are normalized by dividing by the real magnitude:   sr sd     S = , (33)  |sr | |sr | Operations on the dual quaternions can be derived from those of ordinary quaternions and the properties of dual numbers. However, this requires care to handle singularities. Generally, the values at these singularities can be computed by identifying singular factors with convergent Taylor series. Given suitable singular factors, a computer algebra system, e.g., Maxima, Mathematica, can be used to compute the Taylor series. We summarize the relevant functions and suitable Taylor series below. Dual quaternion multiplication is:   A  B = ar  br , ar  bd + ad  br  (34)

14

Dantam, Ben Amor, Christensen, Stilman

The dual quaternion exponential is:  = |rv | k = rv · dv  s S w ~ e =e  rv , c ,   c- s dv +  2
s 

(35) (36)  s  krv , - k    (37)

where s = sin , c = cos , w ~ = rw + dw , and rv · dv is the dot product of rv and dv . Then, to handle the singularity at  = 0, we use the following Taylor expansions: 2 4 6 sin  =1- + - + ... (38)  6 120 5040 cos  - 2
sin  

1 2 4 6 =- + - + + ... 3 30 840 45360

(39)

The dual quaternion logarithm is:  = atan2 (|rv | , rw ) k = rv · dv = (ln S )r = (ln S )d =
 rw - |r v| 2 |rv |

(40) (41)

|r|

2

= ln |r| rv +

1 |r|

cos   - 2 3 sin () sin ()

(42) (43)

 rv , |rv | k - dw |r|
2

 dv , |rv |

k+

rw dw |r|
2

(44)

where (ln S )r is the real part of the logarithm and (ln S )d is the dual part of the logarithm. Note that  represents the angle between the real and imaginary parts of unit quaternion r. To handle the singularity at |rv | = 0 and knowing |r| = 1:    = |r | = v |rv | sin 
|r |

(45) (46)

 2 74 316 =1+ + + + ... sin  6 360 15120 Then, for  in (42): c  2 1 17 4 29 6 - 3 = - - 2 -  -  + ... s2 s 3 5 420 4200

(47)

Online Camera Registration for Robot Manipulation

15

References
1. H. Ben Amor, O. Kroemer, U. Hillenbrand, G. Neumann, and J. Peters. Generalization of human grasping for multi-fingered robot hands. In Proceedings of the International Conference on Robot Systems (IROS), 2012. 2. Fran¸ cois Chaumette and Seth Hutchinson. Visual servo control, part I: Basic approaches. Robotics and Automation Magazine, 13(4):82­90, 2006. 3. Fran¸ cois Chaumette and Seth Hutchinson. Visual servo control, part II: Advanced approaches. Robotics and Automation Magazine, 14(1):109­118, 2007. 4. Changhyun Choi and Henrik I Christensen. Robust 3d visual tracking using particle filtering on the special euclidean group: A combined approach of keypoint and edge features. The International Journal of Robotics Research, 31(4):498­519, 2012. 5. Daniel Choukroun, Itzhack Y. Bar-Itzhack, and Yaakov Oshman. Novel quaternion Kalman filter. Trans. on Aerospace and Electronic Systems, 42(1):174­190, 2006. 6. A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1­38, 1977. 7. Schunk GmbH. Dextrous lightweight arm LWA 4D, technical data. http://mobile.schunk-microsite.com/en/produkte/produkte/ dextrous-lightweight-arm-lwa-4d.html. 8. Frank R Hampel, Elvezio M Ronchetti, Peter J Rousseeuw, and Werner A Stahel. Robust statistics: the approach based on influence functions, volume 114. John Wiley & Sons, 2011. 9. Peter J Huber et al. Robust estimation of a location parameter. The Annals of Mathematical Statistics, 35(1):73­101, 1964. 10. Sungmoon Joo and Michael Grey. DRC-Hubo retrospective, January 2014. Personal Communication. 11. Matthew Klingensmith, Thomas Galluzzo, Christopher Dellin, Moslem Kazemi, J. Andrew (Drew) Bagnell, and Nancy Pollard. Closed-loop servoing using realtime markerless arm tracking. In International Conference on Robotics And Automation (Humanoids Workshop), May 2013. 12. Joseph J. Laviola. A comparison of unscented and extended Kalman filtering for estimating quaternion motion. In American Control Conference, volume 3, pages 2435­2440. IEEE, 2003. 13. Ern J. Lefferts, F. Landis Markley, and Malcolm D. Shuster. Kalman filtering for spacecraft attitude estimation. Journal of Guidance, Control, and Dynamics, 5(5):417­429, 1982. 14. F Landis Markley, Yang Cheng, John Lucas Crassidis, and Yaakov Oshman. Averaging quaternions. Journal of Guidance, Control, and Dynamics, 30(4):1193­1197, 2007. 15. OpenCV API Reference. http://docs.opencv.org/master/modules/refman. html. 16. Vijay Pradeep, Kurt Konolige, and Eric Berger. Calibrating a multi-arm multisensor robot: A bundle adjustment approach. In Experimental Robotics, pages 211­225. Springer, 2014. 17. Kari Rainio and Alain Boyer. ALVAR ­ A Library for Virtual and Augmented Reality User's Manual. VTT Augmented Reality Team, December 2013. 18. Eduard Study. Geometrie der dynamen, 1903. 19. A. Teichman, S. Miller, and S. Thrun. Unsupervised intrinsic calibration of depth sensors via slam. In Robotics: Science and Systems (RSS), 2013.

16

Dantam, Ben Amor, Christensen, Stilman

20. Shinji Umeyama. Least-squares estimation of transformation parameters between two point patterns. Pattern Analysis and Machine Intelligence, 13(4):376­380, 1991.

View publication stats

Directing Policy Search with Interactively Taught Via-Points
College of Computing Georgia Tech Atlanta, GA 30332, USA

Yannick Schroecker

yschroecker3@gatech.edu

hbenamor@asu.edu

Interactive Robotics Lab Arizona State University Tempe, AZ 85282, USA

Heni Ben Amor

athomaz@ece.utexas.edu

Department of ECE University of Texas at Austin Austin, TX 78701, USA

Andrea Thomaz

ABSTRACT
Policy search has been successfully applied to robot motor learning problems. However, for moderately complex tasks the necessity of good heuristics or initialization still arises. One method that has been used to alleviate this problem is to utilize demonstrations obtained by a human teacher as a starting point for policy search in the space of trajectories. In this paper we describe an alternative way of giving demonstrations as soft via-points and show how they can be used for initialization as well as for active corrections during the learning process. With this approach, we restrict the search space to trajectories that will be close to the taught via-points at the taught time and thereby significantly reduce the number of samples necessary to learn a good policy. We show with a simulated robot arm that our method can efficiently learn to insert an object in a hole with just a minimal demonstration and evaluate our method further on a synthetic letter reproduction task.

Keywords
Reinforcement Learning, Learning from Demonstration, Reinforcement Learning for Motor Skills, Dynamic Movement Primitives, Keyframe Demonstrations

1.

INTRODUCTION

Robotics research in recent years has been working towards employing robots in unknown and often unstructured environments. However, controlling a robot in these environments poses major difficulties as the robot has to adapt its actions to the environment and needs to perform tasks with incomplete knowledge of the domain. Reinforcement learning and policy search methods in particular have shown great promise for autonomous learning of motor skills as trajectories. However, due to the high dimensionality and size of the state-action space, the required amount of samples can be prohibitive. A prominent approach to overcome this challenge is to use Learning from Demonstration to obtain an initial trajectory and to ensure that the learning process will quickly converge to the right optimum, see e.g. [15, 18, 4]. Unfortunately, this approach suffers from three major issues: First, providing trajectories as demonstrations can Appears in: Proceedings of the 15th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2016), J. Thangarajah, K. Tuyls, C. Jonker, S. Marsella (eds.), May 9­13, 2016, Singapore. Copyright c 2016, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.

be difficult due to the available input modalities. Recording the desired trajectories using teleoperation or kinesthetic demonstration can be difficult for the user and thus often leads to undesirable pauses, sprints or imperfections. Second, providing full trajectories as demonstrations does not allow the user to put focus on critical segments and limits exploration for segments that the user cannot demonstrate as well as other parts of the trajectory. Third, demonstrations are typically only provided as an initialization of the policy search process. Refining a learned policy and removing undesirable effects usually requires a repeated recording of the entire demonstration and cannot be limited to specific aspects of the task. This problem is amplified if the policy search process has already been started as the learned policy can usually not be combined with new demonstrations. To address these issues, we propose a method that uses soft via-points to initialize and interactively shape the policy search process. Recent research [1] has shown that via-point based representations can be efficiently obtained by a human teacher in the form of demonstrations and provide the teacher with a more natural way of teaching the desired trajectory. This kind of demonstration can achieve smoother trajectories by separating the act of moving the robot as a teacher from the intended trajectory that the robot should follow. Furthermore, it allows the teacher to focus on the most important aspects of the trajectory. In this paper, we introduce a method to use demonstrations in this form with policy search methods based on Dynamic Movement Primitives. More specifically, we look at policy search methods that can optimize DMP parameters and thereby improve the policy by evaluating parameters sampled around the current best estimate of the optimal policy. This class of policy search algorithms has shown great promise and includes methods such as PoWER [14], REPS [20], policy search based on CMA-ES [8, 22] and PI2 [24]. By combining these two approaches we allow the teacher to demonstrate the salient points of a trajectory in a natural way while autonomously learning and improving the shape of the trajectory between those via-points. Our method achieves this goal by learning a single and smooth trajectory that adheres to the demonstrated via-points. Furthermore, we introduce a method to modify existing policy distributions by selecting the most likely trajectories based on the provided demonstration. Modifying the trajectory distribution in this way allows the user to provide corrections to existing policies and thereby shape the learning process. These corrections can be applied to policies learned by demonstration, either continuous demonstrations or via-point based demonstrations,

1052

as well as to policies learned during the autonomous learning process. Finally, we propose to use models of via-points to handle variations of tasks that differ in parameters such as the position and orientation of key-objects in a manipulation task. We show that this facilitates efficient contextual policy search [13, 16] and allows to learn classes of tasks.

2.

RELATED WORK

The work presented in this paper is focusing on policy search for robotics. For a survey of this topic, see Deisenroth et al. [6]. Specifically, we are looking at utilizing policy search methods for learning Dynamic Movement Primitives [10] that share the characteristic that they are sampling directly from the current estimate of the optimal policy. One example for this is the Covariance Matrix Adaption Evolution Strategy (CMA-ES) which has been used in [22] in order to learn dynamic movement primitives based on a reward signal. In [14], Kober et al. propose Policy Learning by Weighting Exploration with the Returns (PoWER) which uses regression weighted by rewards in order to obtain a new policy. Another method in this class is PI2 [24] which uses path integrals to improve on the current policy. In our evaluation, we use Episodic Relative Entropy Policy Search (REPS) [20] as the underlying policy search method. Peters et al. derive a sample based approximation of the optimal distribution of DMP parameters given trajectory- and reward samples as well as a bound on the KL-divergence between the optimized distribution and the distribution from which the trajectories were sampled. Using this approximation, the mean can iteratively be updated using weighted linear regression over the sampled DMP parameters and the variance can be updated by the weighted sample variance. The proper weights can be calculated by solving a convex optimization problem based on the bound on the KL-divergence and the sampled parameters and rewards. We refer to [20] for details. In this paper, we consider an approach based on directing the policy search with demonstrations obtained by a human teacher. The field of Learning from Demonstration(LfD) has been extensively covered in [4] and LfD methods have successfully been combined with policy search for Dynamic Movement Primitives [15, 18]. In particular, we are looking at recording partial demonstrations in the form of via-points which have a long history in trajectory generation. Teaching via-points by demonstrations is also known as keyframe demonstration and has been shown to constitute a user-friendly and efficient way to obtain demonstrations [1]. Wada et al. extract via-points from a continuous demonstration and show that these can be used to create a trajectory that minimizes torque change [25]. Miyamoto et al. extend this approach and apply it to learning robot motor skills [17]. Bitzer et al. introduce an approach that combines keyframe demonstrations with reinforcement learning by learning a lower-dimensional manifold to simplify the state-space for a non-episodic reinforcement learner [3]. This method differs from our approach in that it only learns a simpler state-space representation and does not learn a heuristic for specific trajectories. Utilizing corrections in order to change a policy learned from demonstration has been introduced by Argall et al. who propose to use tactile corrections [2]. While this approach is interesting, it cannot be straight-forwardly integrated into autonomous policy search algorithms such as the ones utilized in our approach.

Utilizing feedback from human teachers has been investigated in the field of interactive reinforcement learning. Knox and Stone [12] introduce TAMER+RL, a reinforcement learning framework that utilizes a reward signal obtained by a human teacher in order to learn a regression model that can fulfill a role similar to a Q-function. Another example is given by Griffith et al. [7] who have introduced Policy Shaping. Policy shaping is utilizing rewards obtained by a human teacher in order to learn a separate policy. This policy can then be combined with the policy learned by standard reinforcement learning methods. Judah et al. [11] propose an integrated approach which optimizes a modified objective function based on the reward as well as on human feedback in the form of binary labels. All three methods utilize feedback provided by a human teacher but are different from our approach in that the feedback takes the shape of a reward-like signal instead of demonstrations. One can see both, the learning from demonstration based approach as well as the interactive reinforcement learning approach as belonging to a generalized class of algorithms that utilize insight obtained by a human teacher in order to improve the policy. In this view, the interactive reinforcement learning is online and less structured whereas the classical learning from demonstration based approach is offline and utilizes structured feedback. Our approach is structured as well but can be used in an offline manner as well as online. Another representation of distributions over trajectories that can be restricted to go through specified via points is called Probabilistic Movement Primitives and has been proposed in [19]. Probabilistic Movement Primitives define feed-forward trajectories directly as combination of basisfunctions and define operations on Gaussian distributions over such trajectories. The conditioning operation defined in this approach is similar to the operation for distributions over DMPs introduced in section 3.2. However, while it is likely that Probabilistic Movement Primitives could also be used with our approach, we are focusing on Dynamic Movement Primitives as they are more popular and better understood.

3.

APPROACH

In this paper, we want to utilize a human teacher in order to provide corrections and suggestions before and during the learning process. Our goal is to use this information to help the learning algorithm converge to a better solution after seeing fewer samples. Specifically, we propose a setup where the teacher is observing the reinforcement learning process and can, before starting the learning process or in-between iterations, inspect the current estimate of the best policy and provide suggestions by physically or remotely moving the robot. Suggestions are recorded as soft via-points y  that the trajectories have to pass through at a specified time t . In the case of corrections, the time t can be naturally recorded by having the user stop the robot during an execution in order to provide the correction. This process is illustrated in Algorithm 1. In the case of initial demonstrations, the time t can be estimated manually based on domain knowledge. While choosing the right t in this case may require some thought, we have found that simple heuristics such as distributing via-points equally in time or choosing t to be proportional to the spatial distance of the via-point are usually sufficient. We base our method on episodic policy search in the space

1053

of trajectories represented as Dynamic Movement Primitives (DMPs) [10, 21] which we describe in detail in section 3.1. These algorithms optimize the parameters of the DMP w.r.t. a given reward function, often by a weighted average with weights obtained based on a transformation of the rewards. The parameters of the DMP uniquely define a policy and thus, we loosely refer to the parameters  as policy and to the distribution over parameters  ( ) as policy distribution. To incorporate demonstrations and corrections into this framework, we use the given via-points as a heuristic to guide the learning process to the solution without directly modifying the given objective function. To this end, we obtain a modified policy distribution and sample only trajectories that are close to the demonstrated via-points. By modifying the policy directly, the effects of demonstrations and corrections are immediate and the learning process never samples trajectories that are far from the demonstrated viapoints. This leads to faster convergence and safer samples. As we modify the policy directly, we require policy search methods that improve upon the given policy in independent iterations based on samples obtained directly from the policy such as PoWER [14], REPS [20] and CMA-ES [22] which obtain a new policy based on a weighted average of the samples obtained from the old policy distribution as well as PI2 [24]. In many cases it can be desirable to consider parameterized tasks as this allows us to learn variations of a task instead of optimizing for a single trajectory. For example, the optimal trajectory in a manipulation task may depend on the location of key objects. One way to solve tasks such as these is to learn a linear model of the parameters µ = A  for some features  to serve as the mean of the policy distribution [16]. To handle parameterized tasks, we generalize our notion of via-points to models that are linear in , i.e. y  = B  where B is either derived from domain knowledge or learned with linear regression. Algorithm 1 Policy search with interactive demonstrations 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: Initialize  (0) ( )  N ( ; µ(0) , (0) ) Obtain initial via-points y , t from demonstration for y  , t  y , t do  (0) ( )  p( |y  , t , µ(0) , (0) ) (see Eqs. 18-21) end for for k  1 to N iterations do  (k) ( )  policy search iteration( (k) ) Execute trajectory defined by mean parameters µ(k) while teacher stops execution do Record stop time t Let teacher move the robot, obtain correction y   (k) ( )  p( |y  , t , µ(k) , (k) ) Execute mean trajectory defined by µ(k) end while end for

jectory separately from the shape of the trajectory. Furthermore, DMPs have the concept of a phase which is a function of time which can be adapted by changing the time-scale. This is meant to reduce the dependency on the time. DMPs are defined as y ¨ =  2   (g - y ) - z  = - z z. y   +  2 fw (z ), (1) (2)

where  is the time scaling parameter and z is a parameter that shapes the phase function.  and  are parameters that are analogous to the gains of a PD-controller and define how the system is drawn to the goal of the trajectory which is defined by g . fw (z ) is the forcing function which determines the shape of the trajectory and is defined as a mixture of radial basis functions with parameters K  N, c, h  RK . fw (z ) =
K i=1 i (z )wi z, K i=1 i (z ) 2

(3) . (4)

1 (z - ci ) . i (z ) = exp - 2 hi

Commonly, the weights wi ; 0  i < K are taken as the parameters of the DMP that are learned by demonstration or autonomously whereas the other parameters are given as hyper-parameters. However, in some cases the goals are not known. We therefore include the goal position in the . g parameters  = ( w ). K then determines the dimensionality of the parameter-space. In this paper, we are utilizing Gaussian distributions over DMP parameters as policy distributions and optimize this distribution with respect to the reward. The mean of this distribution  will be given as a linear function of the features of the task parameters:  ( ) = N ( , A ,  ).

3.2

Obtaining a Sample Distribution

Assuming that we have via-points obtained from demonstration as described above, we derive a modified policy distribution for the underlying policy search that passes close to this via-point at the specified time t : . H ( ) = p( |t , y  )  p(y  |t , y I ,  )p( |t ). (5) The latter distribution over parameters  denotes the prior of where we assume that the human teacher would want the samples to lie. A possible prior is the current policy N ( |µ(k) , (k) ) where µ(k) = A(k) . This prior is reasonable as we want our samples to still follow the current policy where no via points are given. The modified sampling distribution then depends on the previous policy distribution and is given by H ( ) = p( |t , y  , A(k) , (k) )  p (y  |t , y I ,  ) ( |A(k) , (k) ) . (6)

3.1

Dynamic Movement Primitives

Policy search relies on optimizing the parameters of a parametric policy representation. One such policy representation are Dynamic Movement Primitives which have been introduced by Ijspeert et al. in [10]. DMPs are given as dynamical systems that are attracted by a goal position while following a superimposed trajectory. As such they have the property that they can adjust the goal position of the tra-

The former distribution p(y  |t , y I ,  ) denotes the probability of a given DMP going through the specified via point with a specified variance y . This distribution is dependent on the trajectory that the DMP is following. As DMPs are defining accelerations as a linear differential equation, they can be solved for y given a starting position and velocity in order to obtain an estimate of the position at any given time. Note that the solution will not be exact as a real robotic system always has noise and the DMP will react to that noise

1054

102 101

Reward

0 -101 -102
With via-point Without via-point

(a)

(b)

-103 -104 0 900 1800 2700 Samples

Figure 1: a) Rest position of the robot. The robot has to find a trajectory that puts the object in the box. b) Viapoint with the key placed in front of the hole, as it was given to the robot. All trajectories have to be close to this viapoint which gives significant aid for finding the opening in the box.

3600

4500

as well as inaccuracies in the underlying controller. Only the goal position is being tracked exactly. However, using this estimate is reasonable as long as the noise of the estimate is significantly smaller than the inaccuracies introduced by the human teacher when recording a demonstration. Assuming that we start from a rest position where the position y0 and velocity y  0 are zero (we make this assumption for the sake of simplicity. See the appendix for a derivation of the sampling distribution for arbitrary y0 and y  0 ), the dynamical system can be solved for the position y using Duhamel's principle [23]:
y  y t

Figure 2: Rewards obtained over 20 trials during the learning process with and without via-points, plotted on a scale that is linear from -10 to 10 and logarithmic outside of that span. In 18 out of 20 trials, the learner with the via-point finds the opening in the box and obtains a reward close to zero. This results in a low standard deviation (error bars). Without the via-point the learning process converges to a local optimum.

Note that this equation is for a single dimension but can straightforwardly be extended to multiple dimensions by extending M diagonally and adding rows to  such that y=
Mt 0 ··· 0 Mt ··· 0 1

. . .

. . . . ..

. . .

,

=
0

ht (s)

0  2 (g +(z (s))T w)

ds,

(7)

where . (t-s) - 2  ht (s) = e . z (t) = e- z t , . i (z ) = i (z )z . K j =0 j (z )
0 1 - 

where j denotes the parameters of the DMP for dimension j . Therefore for DMPs, we can write the likelihood distribution p(y  |t , y I ,  ) as p(y  |t , y I ,  ) = N (y  |Mt  , y I ). (17)

,

(8) (9) (10)

This equation is linear w.r.t the parameters  and can therefore be written as:
t 0  2 (g +(z (s))T w)

Now we can calculate the sampling distribution as the posterior distribution of the likelihood p(y  |t , y I ,  ), encoding information about the via-point, and the prior distribution  which consists of the previously learned policy. For Gaussian distributions, the sampling distribution is therefore given as H ( ) = N ( |AH , H ), . T -1 1  y IMt + Mt H = -  (k) = (k) -
T  (k) Mt

(18) (19) (20)

y = (1 0)
0

ht (s)
t

ds

(11) (12)

(y I + Mt (k) ,

= (1 0)

g
0 N

ht (s)
t

0 2

ds
0 2

T  Mt (k) Mt

-1

. T -1 -1  y I B +  (k) A (k) AH = H Mt .  

(21)

wi
i=0 0

i (z (s))ht (s)

ds (13)

= Mt  , where Mt = ( m0
t m1 ... mN +1 0 2

Where the application of the Woodbury matrix identity in Eq. 20 allows for numerically stable computation of H . This sampling distribution can then be used in place of the policy in order to obtain the samples used in the next iteration of the policy search as described in Algorithm 1.

), ds,
0 2

(14)

4.
(15) ds. (16)

EXPERIMENTS

m0 =
0 t

ht (s)

mi;0<i<N +1 =
0

i-1 (z (s))ht (s)

In section 3, we showed how to derive a modified sampling distribution based on via-points with timing information that are provided by a human demonstration. In this section, we first utilize a simulated robot arm to show that

1055

-3 -4 -5

Reward

-6 -7 -8 -10 -9 0 500 Continuous demonstration Via-point demonstration 1000 Samples 1500 2000

(a)

(b)

-11

Figure 3: a) Mean trajectory of initial policy (bottom) derived from via-points (center). The trajectory is smooth and barely deviates from the desired trajectory (top) even in-between via-points. b) Mean trajectory (bottom) of initial policy derived from a continuous demonstration (center). Especially the last letter shows how the trajectory mimics imperfections of the demonstration.

Figure 4: Comparison of reward obtained within 2000 samples, averaged over 30 trials. Error bars are showing the standard deviation. The reward function is defined as the minimum squared distance to the goal position. Learning initialized with via-points consistently outperforms the learning process initialized with a continuous demonstration.

such a modified sampling distribution can drastically improve the outcome of a reinforcement learner by having it learn how to insert an object in a hole while requiring only minimal information from the user. We then utilize a word reproduction task in order to provide a comparison to continuous demonstrations and to exhaustively analyze the keyproperties of our algorithm.

provided via-point is further away from the box than the local optima.

4.2

Letter Writing

4.1

Object Insertion with a Robot Arm

In our first experiment, we are evaluating the influence of a small number of demonstrated via-points on the reinforcement learning process and show that it can drastically improve the convergence of the policy search. To this end, we are utilizing a simulated 7 DoF robot arm and have it learn how to insert an object into a hole without any knowledge about the environment. We provide a single via-point (see Figure 1b) and show that this is sufficient to lead the learning process to the right solution which cannot reliably be found without a demonstration. To learn this task, we are utilizing our method with REPS as the underlying policy search algorithm to optimize the distribution over trajectories. These trajectories are represented by 7 Dynamic Movement Primitives with 6-dimensional weights leading to a 49-dimensional action vector  . For evaluation, the outcome of the learning process is averaged over 20 trials with 30 policy search iterations per trial and 150 samples for each iterations. As can be seen in Figure 2, the reward curve observed by using the via-point is converging to a value close to 0 and therefore the distance of the trajectory to the goal position is converging to 0 as well. The learning process initialized with a zero mean policy, on the other hand, is converging to different values. This can easily be explained by the local optima that arise around the box, i.e. the robot is converging to solutions where the end-effector is pressing against the middle of other the sides of the box in order to get closer to the goal position. As a consequence, it stops exploring the box and does not find the opening. Note that the learning process always exceeds the initial reward obtained by our approach as the reinforcement learner always finds at least the closest points outside of the box which cannot be derived by the provided via-point alone, i.e. the

To further investigate the properties of this algorithm we are evaluating our approach on a letter reproduction task as well. Variations of this task have been used in the past to evaluate different properties of Dynamic Movement Primitives as it has many similarities with learning trajectories for robot arms while allowing for intuitive visualization, easy recording of demonstrations and thus good conditions for a comparison to conventional demonstrations as well as fast execution [10, 9]. The objective of the letter reproduction task is to learn trajectories for both dimensions which, when followed by a simulated pen, can accurately reproduce a sequence of letters. We are representing those trajectories as DMPs with a 60 dimensional weight-vector for each dimension and initialize the policy by a continuous demonstration or via-points before optimizing it using REPS. For the policy search, we defined the reward function as the number of 104 . Note that overlapping black pixels: - #IntersectingP ixels+1 solving this task requires the learning algorithm to learn trajectories that are far more complex than in the previous task while utilizing a sparser reward signal. First, we will compare our algorithm to learning trajectories from continuous demonstrations as a baseline, then we will evaluate the use of corrections after the learning process has started and finally we will show that linear models of via-points can be used to learn policies that can reproduce trajectories with different rotation and scaling factors.

4.2.1

Comparison to Continuous Demonstrations

The first instance of the letter reproduction task compares learning initialized with a normal distribution around a continuous demonstration to learning initialized on a fixed set of via-points at equi-distant points in time. In this experiment we show that despite the lack of information between via-points, our approach will converge to a more accurate solution in fewer iterations. To obtain an initial policy from a continuous demonstration, we use standard least squares

1056

(a)

(b)

- 3. 5 - 4. 0 - 4. 5 - 5. 0 - 5. 5 - 6. 0 - 6. 5 - 7. 0 - 7. 5 - 8. 0

Reward

100 200 300

500 800 1300

0

500

Figure 5: a) Mean trajectory of the initial policy (bottom) derived by using only every second via-point (center). This trajectory omits whole letters when compared to the desired trajectory (top). b) Trajectory after 3 iterations (bottom), when the second half of the via-points have been added (center). The policy now shows the desired word and can be improved in future iterations. The "r" is not fully formed immediately after the correction due to learned behavior. to learn the mean and then add an initial variance of 103 for the weights and 5 for the goals of the DMPs. We have found these values to yield the best result in the continuous case. For policy search initialized with via-points, we start with a multivariate normal distribution with mean 0 and a variance of 105 for the weights and 500 for the goal positions. Note that the higher initial variance is necessary as conditioning on the via-points will otherwise lead to an overly narrow distribution with each added via-point decreasing the variance of the policy. This initial distribution is then used as the prior distribution to obtain a modified initial policy based on the via-points that can be seen in Figure 3a. In Figures 3a and b, it can be seen that the initial policy derived from the via-points is very smooth whereas the initial policy derived from a continuous demonstration mirrors the imperfections of that demonstration. Note that these imperfections are often much larger in practice as policy search is unnecessary in domains where given demonstrations already solve the task perfectly. Furthermore, the figures show that the errors introduced by missing information between the via-points is of the same order as the errors that can be introduced by linear regression and that the mean of the initial policy is already describing a good trajectory which leads to a fast learning process. Finally, Figure 4 shows that our approach yields both, higher initial reward as well as higher final reward and therefore better trajectories before and after the learning process, when compared to the baseline. Note that the higher initial reward is tied to the amount of exploration that is necessary in the beginning and can, in many cases, be a desirable property when it comes to safe exploration. While our approach only explores the areas in-between the via-points, a reinforcement learner that has been initialized with a continuous demonstration has to explore around the full trajectory. It is possible to reduce the exploration around the continuous demonstrations; however this would also reduce the final reward that can be obtained.

1000 Samples

1500

2000

Figure 6: Comparison of rewards obtained with the rest of the via-points added after different numbers of samples. Early demonstrations are clearly better than late demonstrations but late demonstrations can still be effective.

be used to modify an already trained policy. This is especially useful in situations where the optimal trajectory is not immediately apparent to the user. To investigate the impact of providing via-points at later iterations we are looking at a variation of the letter writing task where only every second of the initial via-points are provided from the start. Figure 5a shows that this constitutes a far worse initial policy and that we would expect large gains by providing the second half of the via-points. As can be seen in Figure 5b as well as in the reward curve in Figure 6, via-points provided at a later point can indeed improve the policy significantly. However, while giving the via-points after some number of iterations can still cause a significant jump in reward, the size of this jump decreases for later iterations. After some time, the modified sample distribution will even decrease the performance of the learning process. This effect can be attributed to a mismatch of the chosen prior distribution, i.e. the current policy, with the optimal prior distribution which is the unknown policy according to which the human teacher is sampling his via-points. As the learning process converges to a sub optimal policy, it is impossible to sample trajectories that adhere to both, the learned policy as well as the specified via-points. Depending on the value of the variance parameter, the learner can then either sam-

(a)

(b)

4.2.2

Active Corrections

One important aspect of the approach presented in this paper is that via-points can be provided at any time and can

Figure 7: Example of giving via-points interactively. a) Original policy in comparison to the desired trajectory (top) and the via-point that has been provided as a correction (bottom). b) Mean of the modified policy. The trajectory goes through the via-point (top) and thereby matches the desired trajectory more closely (bottom).

1057

- 2. 8 - 3. 0 - 3. 2 - 3. 4 - 3. 6 - 3. 8 - 4. 0 - 4. 2 0 2000
Parameterized task Non-parametric task

5.

CONCLUSION

4000 6000 Samples

8000

10000

Figure 8: Reward obtained by contextual REPS shows that via-point models are an effective initialization. Rewards on the non-parametric task are displayed for comparison. The initial reward is identical due to the linear model and is improved upon significantly in both cases.

ple degenerate trajectories from low-probability areas of the current policy or ignore the given suggestion. The variance parameter is thus a measure of safety and specified how far the new policy can stray from the learned policy in order to adhere to the correction. To avoid this effect, via-points should always be given while the uncertainty in the current policy is relatively high in comparison to the deviation of the via-points from this policy. Note that in this experiment, the via-points are already known from the start even if the agent doesn't utilize them. This allowed us to analyze the effect of giving late demonstrations without having to account for the human factor. However, in practice, late demonstrations should be given depending on trajectories sampled from the current policy. This way, the user can actively shape the trajectory and guide the learning process to the right solution. We illustrate the process of providing via-points interactively and show the impact of a correction in Figure 7.

In this paper we introduced an approach to utilize partial demonstrations to interactively guide the policy search process. We have shown that our approach of using soft viapoints significantly outperforms continuous demonstrations when used to initialize the learning process. Furthermore, our results show that our approach can be used to guide the learning process in an interactive way, utilizing the knowledge gained from observing the robot to change specific aspects of the policy. Finally, we have shown that we can use linear models of via-points to generalize over variations of a task. One key insight is that when applying our method during the learning process, the results are largely dependent on the covariance of the already learned policy. While sampling completely new trajectories ensures that the robot continues exploration and does not return to the original trajectory after reaching the via-point, it also requires a prior distribution that specifies sensible trajectories. In our approach, this distribution is given by the policy search. In the future, we plan to investigate other prior distributions based on different models of how humans give via-points to allow providing corrections to already converged policies. Furthermore, for each via-point the user is required to specify an exact point in time. We plan to relax this assumption and allow the user to specify distributions in time as the importance of preserving the timing is heavily dependent on the task. Finally, the method presented in this paper is based on the assumption that the policy is a Gaussian distribution. While this can be a reasonable assumption, there are cases where other distributions would be preferable. Multi-modal policies, for example, can be used to learn tasks with multiple solutions [5]. In the future, we would like to explore this avenue and extend our approach to different types of policies.

Reward

Acknowledgments
This work was conducted as a part of the OpenLabs project 1436618 sponsored by PSA Peugeot and partially funded under ONR grant number N000141410003.

4.2.3

Evaluation of Learning with Linear Models

For the third experiment, we investigated the properties of learning a parametric generalization of the above task with a linear via-point model. We are looking at learning a model that can recreate words with respect to rotation and scaling of the target image. Note that similar kinds of parametric tasks can be found in practice, where the parameters often denote the location and orientation of an object. We are assuming that good features are known as this would be a necessity for obtaining a via-point model in practice. In this case we are using the feature vector (sx cos(), sy cos(), sx sin(), sy sin(), 1) with sx and sy representing the scaling and lie between 0.6 and 1.4 whereas  represents the rotation of the target image which lies between -45 and 45 degrees. The linear via-point models are then given w.r.t. these features and are used for learning a linear Gaussian policy using locally linear weighted regression and contextual REPS. Figure 8 shows that the initial policy adapts to the task parameters and that it can be used in conjunction with contextual REPS to obtain a similar reward curve for arbitrary rotations and scaling as for a single instance of the task.

APPENDIX A. DERIVING H () FOR ARBITRARY INITIAL POSITIONS AND VELOCITIES

In section 3, we derived a sampling distribution under the assumption that y0 = 0 and y  0 = 0. However, while y0 = 0 can be assumed w.l.o.g., y  0 = 0 is only true for trajectories that start from a rest position. Here, we derive an equivalent sampling distribution for the general case. In the general case, the closed form for dynamic movement primitives is given by
y  y t

=
0

e
t

(t-s)

0 1 - 2  - 

0  2 (g +(z (s))T w)

ds (22)

+e

0 1 - 2  - 

y0 y 0

.

And therefore y = Mt  + c (23)

1058

where
t . c = (1 0)e 1 - 2  -    0 y0 y 0

.

(24)

The sampling distribution p( |t , y ) = N ( |AH , H ) is then computed with the modified likelihood distribution p(y  |t ,  ) = N (y  |Mt  + c, y I ). (25)

The mean of this distribution differs slightly from the distribution derived in section 3 so that
T -1 -1  y I (B  - c) +  (k) A (k)  . AH  = H Mt  

(26)

Note that we can assume w.l.o.g. that  is of the form  = ( 1 2 ··· 1 )T . The sampling distribution is then defined by . T  (27) H = (k) - (k) Mt
T  y I + Mt (k) Mt -1

Mt (k) ,
0 c )) 1 + - A (k) .  (k) 

. T -1  y I (B - ( 0 ... AH = H Mt

(28)

REFERENCES
[1] B. Akgun, M. Cakmak, J. W. Yoo, and A. L. Thomaz. Trajectories and keyframes for kinesthetic teaching: a human-robot interaction perspective. In International Conference on Human-Robot Interaction, pages 391­398, 2012. [2] B. Argall, E. Sauser, and A. Billard. Policy adaptation through tactile correction. In Annual Convention of the Society for the Study of Artificial Intelligence and Simulation of Behaviour (AISB), 2010. [3] S. Bitzer, M. Howard, and S. Vijayakumar. Using dimensionality reduction to exploit constraints in reinforcement learning. In International Conference on Intelligent Robots and Systems (IROS), pages 3219­3225, 2010. [4] S. Chernova and A. L. Thomaz. Robot learning from human teachers. Synthesis Lectures on Artificial Intelligence and Machine Learning, 8(3):1­121, 2014. [5] C. Daniel, G. Neumann, and J. Peters. Hierarchical relative entropy policy search. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2012. [6] M. P. Deisenroth, G. Neumann, and J. Peters. A survey on policy search for robotics. Foundations and Trends in Robotics, 2(1-2):1­142, 2013. [7] S. Griffith, K. Subramanian, J. Scholz, C. Isbell, and A. L. Thomaz. Policy shaping: integrating human feedback with reinforcement learning. In Advances in Neural Information Processing Systems (NIPS), pages 2625­2633, 2013. [8] N. Hansen and A. Ostermeier. Adapting arbitrary normal mutation distributions in evolution strategies: the covariance matrix adaptation. In International Conference on Evolutionary Computation (ICEC), pages 312­317, 1996. [9] A. J. Ijspeert, J. Nakanishi, H. Hoffmann, P. Pastor, and S. Schaal. Dynamical movement primitives: learning attractor models for motor behaviors. Neural Computation, 25(2):328­373, 2013.

[10] A. J. A. Ijspeert, J. Nakanishi, and S. Schaal. Learning attractor landscapes for learning motor primitives. In Advances in Neural Information Processing Systems (NIPS), pages 1547­1554, 2002. [11] K. Judah, S. Roy, A. Fern, and T. G. Dietterich. Reinforcement Learning Via Practice and Critique Advice. AAAI, 2010. [12] W. B. Knox and P. Stone. Reinforcement learning from simultaneous human and MDP reward categories and subject descriptors. In Autonomous Agents and Multiagent Systems (AAMAS), pages 475­482, 2012. [13] J. Kober, E. Oztop, and J. Peters. Reinforcement learning to adjust robot movements to new situations. In Robotics: Science and Systems (RSS), 2010. [14] J. Kober and J. Peters. Policy search for motor primitives in robotics. In Advances in Neural Information Processing Systems (NIPS)., pages 849­856, 2008. [15] P. Kormushev, S. Calinon, and D. G. Caldwell. Robot motor skill coordination with EM-based reinforcement learning. In Intelligent Robots and Systems (IROS), pages 3232­3237, 2010. [16] A. G. Kupcsik, M. P. Deisenroth, J. Peters, and G. Neumann. Data-Efficient Generalization of Robot Skills with Contextual Policy Search. In AAAI Conference on Artificial Intelligence (AAAI), 2013. [17] H. Miyamoto, S. Schaal, F. Gandolfo, H. Gomi, Y. Koike, R. Osu, E. Nakano, Y. Wada, and M. Kawato. A Kendama learning robot based on bi-directional theory. Neural Networks, 9(8):1281­1302, 1996. [18] K. M¨ ulling, J. Kober, O. Kroemer, and J. Peters. Learning to select and generalize striking movements in robot table tennis. International Journal of Robotics Research, 32(3):263­279, 2013. [19] A. Paraschos, C. Daniel, J. Peters, and G. Neumann. Probabilistic movement primitives. In Advances in Neural Information Processing Systems (NIPS), pages 2616­2624, 2013. [20] J. Peters, K. M¨ ulling, and Y. Altun. Relative Entropy Policy Search. In AAAI Conference on Artificial Intelligence (AAAI), pages 1607­1612, 2010. [21] S. Schaal. Dynamic movement primitives - a framework for motor control in humans and humanoid robotics. In Adaptive Motion of Animals and Machines, pages 261­280. Springer Tokyo, 2003. [22] F. Stulp and O. Sigaud. Path integral policy improvement with covariance matrix adaptation. In International Conference on Machine Learning (ICML), pages 281­288, 2012. [23] G. Teschl. Ordinary differential equations and dynamical systems, volume 140. American Mathematical Soc., 2012. [24] E. Theodorou, J. Buchli, and S. Schaal. Learning policy improvements with path integrals. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 828­835, 2010. [25] Y. Wada, Y. Koike, E. Vatikiotis-Bateson, and M. Kawato. A computational model for cursive handwriting based on the minimization principle. In Advances in Neural Information Processing Systems (NIPS), pages 727­734, 1993.

1059

Interaction Primitives for Human-Robot Cooperation Tasks
Heni Ben Amor1 , Gerhard Neumann2 , Sanket Kamthe2 , Oliver Kroemer2 , Jan Peters2,3

Abstract-- To engage in cooperative activities with human partners, robots have to possess basic interactive abilities and skills. However, programming such interactive skills is a challenging task, as each interaction partner can have different timing or an alternative way of executing movements. In this paper, we propose to learn interaction skills by observing how two humans engage in a similar task. To this end, we introduce a new representation called Interaction Primitives. Interaction primitives build on the framework of dynamic motor primitives (DMPs) by maintaining a distribution over the parameters of the DMP. With this distribution, we can learn the inherent correlations of cooperative activities which allow us to infer the behavior of the partner and to participate in the cooperation. We will provide algorithms for synchronizing and adapting the behavior of humans and robots during joint physical activities.

I. INTRODUCTION Creating autonomous robots that assist humans in situations of daily life has always been among the most important visions in robotics research. Such human-friendly assistive robotics requires robots with dexterous manipulation abilities and safe compliant control as well as algorithms for humanrobot interaction during skill acquisition. Today, however, most robots have limited interaction capabilities and are not prepared to appropriately respond to the movements and behaviors of their human partners. The main reason for this limitation is the fact that programming robots for such interaction scenarios is notoriously hard, as it is difficult to foresee many possible actions and responses of the human counterpart. Over the last ten years, the field of imitation learning [12] has made tremendous progress. In imitation learning, a user does not specify the robot's movements using traditional programming languages. Instead, he only provides one or more demonstrations of the desired behavior. Based on these demonstrations, the robot autonomously generates a control program that allows it to generalize the skill to different situations. Imitation learning has been successfully used to learn a wide range of tasks in robotics [2], such as basic robot walking [4], [5], driving robot cars [10], object manipulation [9], and helicopter manoeuvring [1]. A particularly successful approach to imitation learning is based on Dynamic Motor Primitives (DMPs)[6]. DMPs use dynamical systems as a way of representing control policies,
1 Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, 801 Atlantic Drive, Atlanta, GA 30332-0280, USA 2 Intelligent Autonomous Systems, Department of Computer Science, Technical University Darmstadt, Hochschulstr. 10, 64289 Darmstadt, Germany 3 Max Planck Institute for Intelligent Systems Spemannstr. 38, 72076 Tuebingen, Germany

Fig. 1. A human-robot interaction scenario as investigated in this paper. A robot needs to learn when and how to interact with a human partner. Programming such a behavior manually is a time-consuming and errorprone process, as it hard to foresee how the interaction partner will behave.

which can be generalized to new situations. Several motor primitives can be chained together to realize more complex movement sequences. In this paper, we generalize the concept of imitation learning to human-robot interaction scenarios. In particular, we learn interactive motor skills, which allow anthropomorphic robots to engage in joint physical activities with a human partner. To this end, the movements of two humans are recorded using motion capture and subsequently used to learn a compact model of the observed interaction. In the remainder of this paper, we will call such a model an Interaction Primitive (IP). A learned IP is used by a robot to engage in a similar interaction with a human partner. The main contribution of this paper is to provide the theoretical foundations of interaction primitives and their algorithmic realization. We will discuss the general setup and introduce three core components, namely methods for (1) phase estimation, (2) learning of predictive DMP distributions, and (3) correlation the movements of two agents. Using examples from handwriting synthesis and human-robot interaction tasks, we will clarify how these components relate to each other. Finally, we will apply our approach to real-world interaction scenarios using motion capture systems. II. RELATED WORK Finding simple and natural ways of specifying robot control programs is a focal point in robotics. Imitation learning, also known as programming by demonstration, has been proposed as a possible solution to this problem [12]. Most ap-

proaches to imitation learning obtain a control policy which encodes the behavior demonstrated by the user. The policy can subsequently be used to generate a similar behavior that is adapted to the current situation. Another way of encoding policies is to use statistical modeling methods. For example, in the mimesis model [8] a continuous hidden Markov model is used for encoding the teacher's demonstrations. A similar approach to motion generation is presented by Calinon et al. [3] who used Gaussian mixture regression to learn gestures. The methods discussed so far are limited to single agent imitation learning scenarios. Recently, various attempts have been undertaken for using machine learning in human-robotinteraction scenarios. In [13], an extension of the Gaussian process dynamics model was used to infer the intention of a human player during a table-tennis game. Through the analysis of the human player's movement, a robot player was able to determine the position to which the ball will be returned. This predictive ability allowed the robot to initiate its movements even before the human hit the ball. In [7], Gaussian mixture models were used to adapt the timing of a humanoid robot to a human partner in close-contact interaction scenarios. The parameters of the interaction model were updated using binary evaluation information obtained from the human. While the approach allowed for humanin-the-loop learning and adaptation, it did not include any imitation of observed interactions. In a similar vein, the work in [8] showed how a robot can be actively involved in learning how to interact with a human partner. The robot performed a previously learned motion pattern and observed the partner's reaction to it. Learning was realized by recognizing the observed reaction and by encoding the action-reaction patterns in a HMM. The HMM was then used to synthesize similar interactions. In our approach, learning of motion and interaction are not split into two separate parts. Instead, we learn one integrated interaction primitive which can directly synthesize an appropriate movement in response to an observed movement of the human partner. Furthermore, instead of modelling symbolic action-reaction pairs, our approach models the joint movement of a continuous level. This continuous control is realized through the use of DMPs as the underlying representation. By introducing probabilistic distributions and bayesian inference in the context of DMPs, we obtain a new set of tools for predicting and reacting to human movements. III. INTERACTION PRIMITIVES The goal of learning an Interaction Primitive is to obtain a compact representation of a joint physical activity between two persons and use it in human robot interaction. An interaction primitive specifies how a person adapts his movements to the movement of the interaction partner, and vice versa. For example, in a handing-over task, the receiving person adapts his arm movements to the reaching motion of the person performing the handing-over. In this paper, we propose an imitation learning approach for learning such interaction primitives. First, one or several demonstrations of the

interaction task are performed by two human demonstrators in a motion capture environment. Using the motion capture data, we extract an interaction primitive which specifies the reciprocal dependencies during the execution of the task. Finally, the learned model is used by a robot to engage in a similar interaction with a human partner. An IP should also be applicable to a wide variety of related interactions, for example, handing over an object at different locations. An example of an interaction of a humanoid robot with a human partner performing a high-five movement is given in Fig. 2. At the core of our approach is a new representation for interaction tasks. An IP can formally be regarded as a special type of DMP which represents a joint activity of two interaction partners. After an IP is learned from the demonstration data, it is used to control a robot in a similar task. For the sake of notational clarity, we will refer to the first interaction partner, i.e., the human, as the observed agent, while the second interaction partner, i.e., the robot, will be called controlled agent. The IP performs three steps to infer an appropriate way of reacting to the movement of the observed agent. 1) Phase Estimation: The actions of the interaction partners are executed in synchrony. In particular, the robot adapt its timing such that it matches the timing of the human partner. For this synchronization, we need to identify the current phase of the interaction 2) Predictive DMP distributions: As a next step, we compute predictions over the behavior of an agent given a partial trajectory  o of the observed agent. To do so, we use a probabilistic approach and model a distribution p( ) over the parameters of the DMPs. This distribution can be conditioned on a new, partial observation, i.e., to obtain an updated parameter distribution p( | o ). We use samples of this distribution to predict the future behavior of the agent. 3) Correlating both Agents: In order to perform a successful cooperation, the movement of the robot needs to be correlated with the movement of the human. Such operation is a straightforward extension to the predictive DMP distributions. Instead of conditioning on the observation of all DoFs, we only condition on the DoFs of the observed agent, and, hence, we also obtain a distribution over the DMP parameters of the controlled agent, that can be used to control the robot. In the following, we will address each of the steps above in detail. First, we will recapitulate the basic properties and components of DMPs. Subsequently, we will describe how phase estimation, adaptation, and correlation can be realized within the DMP framework in order to produce an interactive motor primitive. A. Dynamic Motor Primitives A DMP is an adaptive representation of a trajectory representing a human or robot movement [6]. In this section, we will give a brief recap of DMPs. The general idea is to encode a recorded trajectory as dynamical systems, which can be used to generate different variations of the original

Motion Capture

Interaction Primitive

Human-Robot Interaction

Fig. 2. An overview of the presented machine learning approach. Left: Using motion capture we first record the movements of two persons during an interaction task. Center: Given the recorded motion capture data, we learn an interaction primitive specifying each persons' movement as well as the dependencies between them. Right: During human-robot interaction, the learned interaction primitive is used by the robot to adapt its behavior to that of his human interaction partner.

movement. As a result, a robot can generalize a demonstrated movement to new situations that may arise. Formally, a DMP can be written as a dynamical system y ¨ = (y (y (g - y ) - ((y  )/ )) + f (x))  2 (1)

B. Phase Estimation by Dynamic Time Warping For a joint activity to succeed, the movement of the interaction partners needs to be temporally aligned. During the execution of human-robot interaction, the robot observes a partial movement of the human counterpart. Given this partial movement sequence, we need to determine the current state of the interaction. This is achieved by determining the current value of the phase variable x. To this end, we will use the dynamic time warping (DTW) algorithm [11]. DTW is a method for the alignment of time series data. Given two time series u = (u1 , · · · , uN ) and v = (v1 , · · · , vM ) of size N and M , DTW finds optimal correspondences between data points, such that a given distance function D is minimized. This task can be formulated as finding an alignment between a reference trajectory u and an observed subsequence v. In our specific case, the reference trajectory is the movement of the observed agent during the original demonstration of the task, and the query trajectory is the currently seen partial movement sequence of the human interaction partner. We define an alignment  as a set of tuples (1 , 2 ) specifying a correspondence between point 1 of the first time series and point 2 of the second time series. To find such an alignment, we first calculate the accumulated cost matrix, which is defined as D(1, m) D(n, 1) D(n, m) = = = m k=1 c(u1 , vk ), m  [1 : M ] n k=1 c(uk , v1 ), n  [1 : N ] (5) (6) (7)

where y is a state variable such as the joint angle to be controlled, g is the corresponding goal state, and  is a time scaling factor. The first set of terms represents a criticallydamped linear system with constant coefficients y and y . The last term is called the forcing function f ( x) =
m i=1 i (x)wi x m j =1 j (x)

= (x)T w

(2)

where i (x) are Gaussian basis functions and w the corresponding weight vectors. The basis functions only depend on the phase variable x, which is the state of a canonical system shared by all degrees of feedom (DoFs). The canonical system acts as a timer to synchronize the different movement components. It has the form x  = -x x , where x0 = 1 at the beginning of the motion and, thereafter, it decays towards zero. The elements of the weight vector w are denoted as shape-parameters, as they determine the acceleration profile of the movement, and, hence, indirectly also the shape of the movement. Typically, we learn a separate set of shape parameters w as well as the goal attractor g for each DoF. The goal attractor g can be used to change the target position of the movement while the time scaling parameter  can be used to change the execution speed of the movement. The weight parameters w of the DMP can be straightforwardly obtained from observed trajectories {y1:T , y  1:T , y ¨1:T } by first computing the forcing function that would reproduce the given trajectory, i.e., Fi = 1 y ¨i - y (y (g - yi ) - y  i / ). 2 (3)

min{D(n - 1, m - 1), D(n, m - 1), . . . D(n - 1, m)} + c(un , vm )

Subsequently, we can solve the system w = F in a least squares sense, i.e., w = (T )-1 T F , (4)

where  is a matrix containing of basis vectors for all time steps, i.e., t = T t = (xt ).

where c is a local distance measure, which is often set to the squared Euclidean distance, i.e., c = ||u - v ||2 . In the original DTW formulation, finding the optimal alignment is cast as the problem of finding a path from (1, 1) to (N, M ) producing minimal costs according to the accumulated cost matrix. This optimization is achieved using a dynamic programming recursion. The DTW approach above assumes that both time series have approximately the same length. However, in our case we want to match a partial movement to the reference movement. To this end, we modify the DTW algorithm and determine the path with minimal distance starting at (1, 1)

with µ =
Training Partial Completion

S j =1

 [j ]

S

,

 =

S [j ] j =1 (

- µ)( [j ] - µ)T S

.

(10)

Fig. 3. Phase estimation and pattern completion using DTW and DMPs. Given the partial observation (black), we estimate the current phase, and use it to generate the unseen part (red) of the letter. The goal does not have to be specified and is estimated alongside the other parameters.

In order to obtain a predictive distribution, we observe a partial trajectory  o = y 1:t up to time point t and our goal is to estimate the distribution p( | o ) over the parameters  of the DMP. These parameters can be used to predict the remaining movement y t :T of the observed agent. The updated distribution p( | o ) can simply be computed by applying Bayes rule, i.e., p( | o )  p( o | )p( ). (11)

and ending at (n , M ), where n is given by n = argmin D(n, M ).
n

(8)

The index n reflects the frame in the reference movement which produces minimal costs with respect to the observed query movement. As a result it can be used to estimate the current value of the phase variable x of the canonical system. More specifically, calculating (n /N ) yields an estimated of the relative time that has passed, assuming a constant sampling frequency. Scaling this term nonlinearly yields an estimate of the phase x of the canonical system x = exp -x n N


In order to compute this operation, we first have to discuss how the likelihood p( o | ) can be implemented. The parameters of a DMP directly specify the forcing function, however, we also include the goal positions gi in the forcing function fi for the ith DoF. Therefore, we reformulate the forcing function, i.e., for a single degree of freedom i, we will write the forcing function fi (t) as fi (xt ) = T t w i + y y gi = [T t , y y ] wi gi . (12) (13)



.

(9)

A simple example for the explained phase estimation algorithm can be see Fig 3. The grey trajectories show the demonstrated handwriting samples for which DMPs have been learned. The black trajectories show a new, partial handwriting sample. Using DTW, we can identify the phase of the DMP and then use it to automatically complete the observed letter. To this end, we can set the starting position of the DMP to the last point in the partial trajectory, and set the phase according to the estimated x. By specifying any goal position g , we can generate the missing part of the observed movement. C. Predictive Distributions over DMPs In this section we will introduce predictive distributions over DMP parameters that can be used to predict the behavior of an agent given a partial observed trajectory. We will first describe how to generate such predictive distribution for a single agent and later show in the next section how this model can be easily extended to infer a control policy for the controlled agent in an interaction scenario. In our probabilistic approach, we model a distribution p( ) over the parameters of a DMP. In order to also estimate the target position of the movement, we include the shape parameters wi as well as the goal attractors gi for all DoFs i in the parameter vector  of the DMP, i.e., T  = [wT 1 , g1 , . . . , w N , gN ], where N is the number of DoFs for the agent. Given the parameter vector samples  [j ] of multiple demonstrations j = 1 . . . S , we estimate a Gaussian distribution over the parameter  , i.e., p( ) = N ( |µ ,  ),

The forcing function can be written in matrix form for all DoFs of the observed agent and for all time steps 1  t  t , i.e.,    ~  w1  0 .....   ~ 0 . .   g1   0     . =  , (14) F = .  . . . . .  .   . . . . . .  .  wN  ~ 0 .....  gN


~ t,· = [T , y y ] of  ~ contains the basis where the t-th row  t functions for time step t and a constant as basis for the goal attractor gi . The matrix  contains the basis functions for all DoFs on its block-diagonal. The vector F contains the value of the forcing function for all time steps and all degrees T T of freedom, i.e., F = [f T 1 , . . . , f N ] , where f i contains the values of the forcing function for all time steps for the ith DoF. By concatenating the forcing vectors f i for the single DoFs and by writing  as a block-diagonal matrix, we achieve that the parameters of all DoF can be concatenated in the vector  . Given an observed trajectory y 1:t , we can also compute the forcing function vector F  from the observation, i.e., the elements of the F  vector are given by 1 ¨i (t) - y (-y oi (t) - o i (t)/ ). (15) fi (xt ) = 2 o  Now, we can use a Gaussian observation model for the likelihood p( o | ) = N (F  | ,  2 I ), (16) where  2 is the observation noise variance which will act as regularizer in the subsequent conditioning.

0.5 0

attraction-point

goal

attraction-point

y-axis [m]
Training Partial Completion
Fig. 4. Given a set of training trajectories (in gray) we learn a predictive distribution over the DMP weights. The distribution can then be used to sample new trajectories with a similar shape. In this example, DTW is used to determine the current phase of a partially observed trajectory (black). The completions of this trajectory are performed by estimating the most likely distribution of DMP weights.

-0.5 -1 -1.5 -2

-2

-1.5

-1

-0.5

0

0.5

1

1.5

2

Observed Agent

x-axis [m]

Controlled Agent

Fig. 5. A simple simulation setup for studying Interaction Primitives. Two opposing (robot) arms of different kinematic structure (2 links vs. 3 links) execute a high-five movement to a specific goal. Using optimal control methods we can calculate optimal trajectories that reach the goal, while at the same time being attracted by "attraction points". The resulting data set contains strong correlations between the movements of the two agents.

In order to perform the conditioning, we first write p( o ,  ) = p( o | )p( ) as a joint Gaussian distribution and, subsequently, condition on  o . The joint distribution is given by p( o ,  ) = N F  A  , µ  T   (17)

with A =  2 I +  T . The conditional distribution p( | o ) is now also Gaussian with mean and variance µ| o  |  o = µ +  T A-1 (F  - µ ), =  -  T A-1  . (18)

~  , with  ~ = [, 0]. The conditioning equations given F = in (18) can now be applied straightforwardly by replacing  ~ . The estimated parameter vector  can, thus, be used with  to to predict the remaining movement of the observed agent (using  o ) but also to control the robot in the current situation (using  c ). Hence, its behavior is always related the behavior of the human interaction partner. IV. E XPERIMENTS To evaluate our approach under controlled and varying conditions, we designed a simple simulation environment that can be used to study interaction scenarios. The simulation consists of two opposing robot arms with different kinematic structures as can be seen in Fig. 5. The robot on the left side (the observed agent) consists of two links which are connected through a hinge joint, while the robot on the right side (the controlled agent) has three links and two joints. The task of the robots is to execute a high-five movement. In order to generate training data for this task we first synthesize a set of training trajectories for both agents. Using optimal control we determine for each agent a joint angle trajectory which brings it from its start position to a specified goal position. Attractor points are added in order to generate realistic-looking high-five movements. During the synthesis of training data, both the goal position and the attractor positions are varied. Given this training data, we learn an IP capturing the mutual dependencies of the agents during the high-five movement. After learning, we the use the IP to control the three linked robot arm. Given partial joint angle trajectories of the observed agent, we use the IP and conditioning to (1) determine the most likely current goal, (2) the ideal joint angle configurations of the controlled agent. Fig. 6 depicts task space trajectories for the controlled agent after conditioning. On the left side we see the results of conditioning when 40% of the movement of the observed agent is seen. On the right side are the results after conditioning with 60% of the data. Each trajectory is a possible

Using the above distribution we can compute the most likely weights µ| o of the DMPs for any observed partial movement. In Fig. 4, we see an example of using the above procedure for estimating weights from partial observations. On the left side, we see the demonstrations that have been used to train the DMPs. The different demonstrations reflect different versions of the same alphabet letter. On the right side, we see the partial observation (black) of a new handwritten sample as well as the automatic reconstruction using a DMP with estimated weights. D. Correlating two Agents with Predictive Distributions Correlating the controlled agent with its interaction partner is now a straightforward extension of the predictive DMP framework. We now assume that we have two agents, the observed and the controlled agent. In order to capture the correlation between the agents, we use a combined parameter T T vector  = [ T o ,  c ] which contains the DMP parameters  o of the observed and the parameters  c of the controlled agent. We can now use the approach for obtaining a predictive distribution p( | o ), however, in contrast to the previous section, the observed trajectory only contains the DoFs of the observed agent. Hence, in order to write the forcing vector F =  o in terms of the complete parameter vector  , we need to append a zero-matrix to the feature matrix , i.e.,

0.0
2

-0.1
y-axis [m]
1

y -a x is [m ]
25 50 75 100 0 25 50 75 100

-0.2 -0.3 -0.4 -0.5 -0.6 1.5 1.6 1.7 1.8 1.9

0

0

x-axis [m]

Fig. 6. The uncertainty over the goal position shrinks with increasing amount of data points. Left: distribution after observing 40% of the partners movements. Right: distribution after observing 60% of the partners movements
16 14 12 10 6 4 2 0 0 10 20 30 40 50 60 70 80 90 100

x - ax i s[ m ]

Controlled Observed

Fig. 8. Difference between ground truth and predicted task space goals. Blue circles show the true positions of the goals. Red circles depict the predicted goals after observing 60% of the interaction.

MSE

Fig. 7. Mean squared error based on different percentage of partially observed trajectories of the interaction partner. The red curve shows the accuracy of predicting the movements of the observed agent from partial trajectories. The black curve shows the accuracy in inferring the right reaction in response to the observed movement.

Middle

Percentage of trajectory [%]

Side

8

(a)

(b)

(c)

prediction of the task space movement of the agent. The figure shows that the uncertainty significantly shrinks when we transition fom 40% to 60% in this example. To further analyze the accuracy of prediction, we generated a new test data set consisting of interactions to different goal positions that were not part of the original training data. We then calculated the mean squared error (MSE) between the predicted joint angle trajectories generated from the IP and the ground-truth data. Fig. 7 shows the evolution of the MSE for observed partial trajectories of increasing size. We can see that the MSE in the prediction of the observed agent significantly drops at around 20% and again at 60%. A similar trend, albeit with higher variance, can also be seen in the predictions for the ideal response of the controlled agent. The plot suggests that after seeing 60% of the movement of the interaction partner we can already roughly infer the goal he is aiming at. We also analyzed the difference in task space between the inferred and the ground-truth data for the controlled agent. More specifically, we evaluated how far the predicted goal is from the true goal of the trajectory. Fig. 8 shows the true goals and the inferred goals in task space after seeing 60% of the movements of the observed agent. We can see that the predicted goals are in close proximity to the true goal locations. Please note, that the depicted positions are in task space while the inferred values are in joint space.

Fig. 9. Two reactions synthesized from an Interaction Primitive for a handing-over task. The humanoid on the left side (controlled agent) is controlled through the IP and has to receive an object from the humanoid on the right side. Depending on the location where the object is handed over, the reaction of the controlled agent is changed. Training of the IP was performed using motion capture data of two humans.

The depicted position is calculated by performing forward kinematics using the inferred joint angle data. As a result, even small errors in the joint angle of the first link will propagate and produce larger errors at the end-effector. Still, the results indicate that the approach can be efficiently used for intention inference in human-robot tasks. By predicting the most likely goal of the human interaction partner, the robot can proactively initiate his own response. We also conducted experiments using real motion capture data collected from the interactions of two human subjects. In a first experiment we asked two humans to demonstrate a "handing-over" in which the first subject hands a cup to the second subject. The demonstrations were then used to learn an IP. However, in order to cope with the high dimensionality of the dataset, we applied Principal Component Analysis (PCA) as a pre-processing step, in order to project the data onto a five-dimensional space. After training, we tested the learned IP on a set of data points that have not been used during training. Fig. 9 shows two example situations, where the controlled agent (left humanoid) automatically infers the optimal reaction to the behavior of his interaction partner

Fig. 10. A frame sequence from a high-five interaction between a human and a humanoid. The robot automatically reacts to the movement of the human and estimates the appripriate location of the executed high-five. The human interaction partner is tracked using an OptiTrack motion capture system.

(right humanoid). In the first sequence the controlled agent correctly turns to the left side to receive an object. In contrast to that, in the second sequence, the agent reaches for the middle in order to properly react to the observed movement. Finally, we performed a set of interaction experiments on a real humanoid robot. The humanoid has two arms with 7 DoF each. During the experiment we used one arm with four DoFs. More specifically, we trained an Interaction Primitive for the high-five. Again, we collected motion capture data from two humans for training this IP. After training, the robot used the IP to predict the joint configuration at the goal position as well as the weight parameters of the DMP. Fig. 10 shows an example interaction realized via the presented approach. Using prediction in this task is important, as it helps the robot to match the timing of the human interaction partner. Notice that the starting location of the robot is quite far from the rest poses in the learning database. V. C ONCLUSION In this paper, we proposed the novel Interaction Primitive framework based on DMPs and introduced a set of basic algorithmic tools for synchronizing, adapting, and correlating motor primitives between cooperating agents. The research introduced here lays the foundation for imitation learning methods that are geared towards multi-agent scenarios. We showed how demonstrations recorded from two interacting persons can be used to learn an interaction primitive, which specifies both the executed movements, as well as the correlations in the executed movements. The introduced phase estimation method based on dynamic time warp proved to very important for applying a learned interaction primitive in new situations. Timing is a highly variable parameter, which varies among different persons, but can also vary depending on the current mood or fatigue. In future work, we plan to concentrate on more complex interaction scenarios, which are composes of several interaction primitives. These primitives could executed in sequence or in a hierarchy in order to produce complex interactions with a human partner. We are also already working on using the interaction primitive framework for predicting the most likely future movements of a human interaction partner. The underlying idea is that the same representation which is

used for movement synthesis can also be used for movement prediction. The predicted actions of the human could then be integrated into the action selection process of the robot, in order to avoid any dangerous situations. VI. ACKNOWLEDGEMENTS The work presented in this paper is funded through the Daimler-and-Benz Foundation and the European Communitys Seventh Framework Programme under the grant agreement n ICT-600716 (CoDyCo). R EFERENCES
[1] P. Abbeel, A. Coates, and A. Ng. Autonomous Helicopter Aerobatics through Apprenticeship Learning. International Journal of Robotic Research, 29:1608­1639, 2010. [2] A. Billard, S. Calinon, R. Dillmann, and S. Schaal. Survey: Robot Programming by Demonstration. In Handbook of Robotics, volume chapter 59. MIT Press, 2008. [3] S. Calinon, E.L. Sauser, A.G. Billard, and D.G. Caldwell. Evaluation of a probabilistic approach to learn and reproduce gestures by imitation. In Proc. IEEE Intl Conf. on Robotics and Automation (ICRA), pages 2381­2388, Anchorage, Alaska, USA, May 2010. [4] R. Chalodhorn, D. Grimes, K. Grochow, and R. Rao. Learning to walk through imitation. In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI'07, pages 2084­2090, San Francisco, CA, USA, 2007. Morgan Kaufmann Publishers Inc. [5] D. Grimes, R. Chalodhorn, and R. Rao. Dynamic imitation in a humanoid robot through nonparametric probabilistic inference. In In Proceedings of Robotics: Science and Systems (RSS). MIT Press, 2006. [6] A. Ijspeert, J. Nakanishi, H. Hoffmann, P. Pastor, and S. Schaal. Dynamical movement primitives: Learning attractor models for motor behaviors. Neural Comput., 25(2):328­373, February 2013. [7] S. Ikemoto, H Ben Amor, T. Minato, B. Jung, and H. Ishiguro. Physical human-robot interaction: Mutual learning and adaptation. IEEE Robotics and Automation Magazine, 19(4):24­35, Dec. [8] D. Lee, C. Ott, and Y. Nakamura. Mimetic communication model with compliant physical contact in human-humanoid interaction. Int. Journal of Robotics Research., 29(13):1684­1704, November 2010. [9] M. M¨ uhlig, M. Gienger, and J. Steil. Interactive imitation learning of object movement skills. Autonomous Robots, 32:97­114, 2012. [10] D. Pomerleau. ALVINN: an autonomous land vehicle in a neural network. In David S. Touretzky, editor, Advances in Neural Information Processing Systems 1, pages 305­313. San Francisco, CA: Morgan Kaufmann, 1989. [11] H. Sakoe and S. Chiba. Dynamic programming algorithm optimization for spoken word recognition. Acoustics, Speech and Signal Processing, IEEE Transactions on, 26(1):43­49, 1978. [12] S. Schaal. Is imitation learning the route to humanoid robots? Trends in Cognitive Sciences, 3:233­242, 1999. [13] Z. Wang, M. Deisenroth, H. Ben Amor, D. Vogt, B. Schoelkopf, and J Peters. Probabilistic modeling of human dynamics for intention inference. In Proceedings of Robotics: Science and Systems (R:SS), 2012.

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/265605707

DynamicModeDecompositionforPerturbation EstimationinHumanRobotInteraction
ConferencePaper·August2014
DOI:10.13140/2.1.2957.7601

CITATIONS

READS

4
5authors,including: ErikBerger TechnischeUniversitätBergakademieFreiberg
18PUBLICATIONS52CITATIONS
SEEPROFILE

103

DavidVogt TechnischeUniversitätBergakademieFreiberg
20PUBLICATIONS86CITATIONS
SEEPROFILE

HeniBenAmor ArizonaStateUniversity
56PUBLICATIONS434CITATIONS
SEEPROFILE

Someoftheauthorsofthispublicationarealsoworkingontheserelatedprojects:

Mining-RoX:AutonomousRobotsinUndergroundMiningViewproject

AllcontentfollowingthispagewasuploadedbyErikBergeron15September2014.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Dynamic Mode Decomposition for Perturbation Estimation in Human Robot Interaction
Erik Berger1 , Mark Sastuba2 , David Vogt1 , Bernhard Jung1 , Heni Ben Amor3
Abstract-- In many settings, e.g. physical human-robot interaction, robotic behavior must be made robust against more or less spontaneous application of external forces. Typically, this problem is tackled by means of special purpose force sensors which are, however, not available on many robotic platforms. In contrast, we propose a machine learning approach suitable for more common, although often noisy sensors. This machine learning approach makes use of Dynamic Mode Decomposition (DMD) which is able to extract the dynamics of a nonlinear system. It is therefore well suited to separate noise from regular oscillations in sensor readings during cyclic robot movements under different behavior configurations. We demonstrate the feasibility of our approach with an example where physical forces are exerted on a humanoid robot during walking. In a training phase, a snapshot based DMD model for behavior specific parameter configurations is learned. During task execution the robot must detect and estimate the external forces exerted by a human interaction partner. We compare the DMDbased approach to other interpolation schemes and show that the former outperforms the latter particularly in the presence of sensor noise. We conclude that DMD which has so far been mostly used in other fields of science, particularly fluid mechanics, is also a highly promising method for robotics.

I. I NTRODUCTION Robots need accurate and efficient sensing capabilities in order to react to influences from the environment. This is particularly true for robots that are engaging in joint physical activities with a human partner. In such scenarios, forces and torques applied by the human can severely perturb the execution of a motor skill and need to be accounted for in the decision making process. In order to appropriately respond to a perturbation, a robot needs to detect both the occurrence of such an event as well as the degree by which it occurred. One way of implementing such detection is to use readings from a special purpose sensor, e.g., force-torque sensor, along with a thresholding method. However, such sensors are often heavy, expensive and prone to error. In practice many sensors return non-zero readings even when the robot merely moves. Distinguishing between external, human perturbations and natural variation in the sensor values can therefore become a challenging task. In this paper, we present a machine learning approach to robot sensing that is well suited for identifying external influences caused by a human partner as shown in Figure 1.
1 Institute of Computer Science, Technical University Bergakademie Freiberg, Bernhard-von-Cotta-Str. 2, 09599 Freiberg, Germany 2 Institute of Mechanics and Fluid Dynamics, Technical University Bergakademie Freiberg, Lampadiusstr. 4, 09599 Freiberg, Germany 3 Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, 801 Atlantic Drive, Atlanta, GA 30332-0280, USA

Fig. 1: A NAO robot detects the existence and amount of external perturbations applied by a human interaction partner. The direction and amount of the perturbation is used to infer human guidance, e.g., walk backwards.

The approach focuses on learning probabilistic, behaviorspecific models of regular oscillations in sensor readings during motor skill execution. These models are used to (1) identify perturbations by detecting irregularities in sensor readings that cannot be explained by the inherent noise, and (2) to generate a continuous estimate of the amount of external perturbation. Due to the data-driven nature of the approach, no detection threshold needs to be provided by the user. The presented perturbation filter can be regarded as a virtual force sensor that produces a continuous estimate of external forces. To this end, we use Sparse Dynamic Mode Decomposition to learn a model of the system dynamics during the robot execution of a specific motor skill. During human-robot interaction, the model is then used to determine the existence and amount of irregularities in the sensor readings. By modeling the correlations as well as the timedependent variation in the original sensor values, our filter can robustly deal with uncertainties in estimating the human physical influence on the robot. During task execution, the estimated perturbation value can be used to compensate for the external forces or infer the intended guidance of a human interaction partner. Experiments on a real robot show that

External Perturbation

Perturbation Filter
Perturbation Detection Perturbation Estimation
Prediction

Reaction

Sensor Data

Behavior Behavior Parameter Parameter

Costs

Perturbation Value

Evaluation

Fig. 2: An overview of the presented machine learning approach. An external perturbation is filtered by using a previously learned predictive model of behavior parameters. After detecting a perturbation the strength and direction is estimated in the behavior parameter space. The resulting perturbation value can be used for an adequate reaction.

learned models can be used to accurately determine even small disturbances. II. R ELATED W ORK A popular approach to the design of human-robot interaction (HRI) is the use of mediating artefacts, such as pendants, joysticks or mobile phones [1]. The approach allows a programmer to pre-specify a set of tasks, commands and corresponding robot reactions. Since communication is mediated through the artefact, no filtering or interpretation of the human commands is required. In recent years, more natural and intuitive approaches to HRI have gained popularity. Various researchers have proposed the so-called soft robotics paradigm: compliant robots that "can cooperate in a safe manner with humans" [2]. An important robot control method for realizing such a compliance is impedance control [3]. Impedance control can be used to allow for touch based interaction and human guidance. To this end, impedance controllers require accurate sensing capabilities, in the form of force-torque sensors. However, such sensors are typically heavy, expensive and suffer from noise. Other sensors, such as current based torque sensors are even more prone to issues related to noise and drift. Still, the ability to sense physical influences is at the core of recent advances made in the field of HRI. For example, Lee et al. [4] use impedance control and force-torque sensors in order to realize human-robot interaction during programming by demonstration tasks. Wang et al. [5] present a robot that can adapt its dancing steps based on the external forces exerted by a human dance partner. Ben Amor et al. [6] uses touch information to teach new motor skills to a humanoid robot. Touch information is therefore only used to collect data for subsequent learning of a robotic motor skill. Robot learning approaches that are based on such kinesthetic teachin have gained considerable attention in the literature, with similar results reported in [7] and [8]. A different approach aiming at joint physical activities between humans and robots has been reported in [9]. Ikemoto et al. use Gaussian mixture models to adapt the timing of a humanoid robot to that

of a human partner in close-contact interaction scenarios. The parameters of the interaction model are updated using binary evaluation information obtained from the human. The approach significantly improves physical interactions, but is limited to learning timing information. St¨ uckler et al. [10] present a cooperative transportation task where a robot follows the human guidance using arm compliance. In doing so, the robot recognizes the desired walking direction through visual observation of the object being transported. A similar setting has been investigated by Yokoyama et al. [11]. They use a HRP-2P humanoid robot equipped with a biped locomotion controller and an aural human interface to carry a large panel together with a human. Forces measured with sensors on the wrists are utilized to derive the walking direction. Similarly, Bussy et al. [12] also use force-torque sensors on the wrists to adapt the robot behavior during object transportation tasks. Lawitzky et al. [13] also shows how load sharing and role allocation can be used to balance the contribution of each interaction partner depending on the current situation. The main drawback of the above approaches is that they require special aural and visual input devices or force sensors which are not present on many robot platforms. Additionally, none of the approaches using force-torque sensors addresses the problem of uncertainty in the provided measurements. As a result, all of these approaches assume high-quality sensing capabilities and low-speed execution of the joint motor task. In contrast to the above approaches, we propose a new filtering algorithm that can learn the natural variation in sensor values during a motor skill. Using predictive models learned by Dynamic Mode Decomposition, the filtering algorithm also estimates the perturbation which best explains an observed set of new sensor values. III. APPROACH In our approach the robot recognizes and automatically differs between strength and direction of external perturbations which may be caused by a human interaction partner. An overview of the approach can be seen in Figure 2. First,

we record training data for a behavior with different parameter configurations, e.g. walking with varying step lengths, in a controlled environment without external perturbations. The training data is used to learn a dynamic model utilizing a state of the art interpolation method from fluid dynamics (DMD). During behavior execution an external perturbation is detected by comparing the recorded training data with the current sensor data. For estimating the perturbation value the current sensor readings are compared to new sensor values generated from the learned model. The perturbation value is then calculated from the difference between the current behavior parameter and the behavior parameter of the sensor characteristic with the highest compliance to the current sensor characteristic. In the following, we will address each step of our approach in more detail. Subsequently, we will describe how perturbation detection, model learning and perturbation estimation are realized in order to allow a whole variety of HRI scenarios. A. Recording Training Data The first step in our approach is to record training data that reflects the evolution of sensor values during regular execution of a motor skill. It is important to record several executions of the behavior, since motor skills can often be executed with different parameters, e.g., varying step lengths during walking. However, since we use machine learning methods, we will later see that the number of required training data can be limited to about five examples. Each recorded example contains training data sampled with 100Hz for one repetition of the modelled robot behavior. In our specific case of training a perturbation filter for walking, we record both the center of mass (CoM) and the proper acceleration of the robot for four seconds. Acquiring training data requires less than one minute in total. B. Phase Estimation Since we are dealing with time-varying data, it is important to estimate the phase of the robot during the execution of a motor skill. Depending on the phase, e.g., the left leg is lifted, the variance in the sensor readings can change drastically. To determine the current phase, a time window of sensor values is captured and temporally aligned to the training data. To this end, we use the dynamic time warping technique (DTW) [14]. DTW is a time series alignment algorithm for measuring the similarity between two temporal sequences X = (x1 , . . . , xN ) and Y = (y1 , . . . , yM ) of length N  N and M  N. In our specific case, the goal is to find the optimal correspondence between the sensor data Y recorded during the training phase and the currently observed sequence X, where M is much larger then N . Due to this significant difference in length of X and Y, we formulate our task as finding a subsequence Y(a : b ) = (ya , ya +1 , . . . , yb ) (1)

a*

p* b* Y X

Fig. 3: Given the recorded data (black) and the partial observation (red), we calculate the optimal warping path p between a and b .

subsequence X. This technique is also known as subsequence dynamic time warping (SDTW) [15]. To find the optimal subsequence we first have to calculate the accumulated cost matrix D, which for SDTW is defined as
n

D(n, 1) =
k=1

c(xk , y1 ), n  [1 : N ],

D(1, m) = c(x1 , ym ), m  [2 : M ], D(n, m) = min{D(n - 1, m - 1), D(n - 1, m), D(n, m - 1)} + c(xn , ym ) where c is a local distance measure, which in our case is defined as c = |x - y |. The goal of the SDTW algorithm is to determine the path with minimal overall costs C ending at (b , M ), where b is given by b = argmin D(N, b).
b[1:M ]

(2)

To determine the warping path p = (p1 , . . . , pL ) starting at p1 = (a , 1) and ending at pL = (b , M ) a dynamic programming recursion is used. As illustrated in Figure 3 the resulting path p represents the optimal subsequence of X in Y. As a result SDTW can be used to estimate the current state of a behavior using a subset of temporally measured sensor values which are mapped to the recorded data. In more detail, we use the subsequence p as prediction of sensor values at the current state. C. Perturbation Detection Due to uncertainties in the real world, a motor skill is never twice executed in exactly the same way. To accommodate for such natural noise in the behavior, we use learned, behaviorspecific information about the temporal evolution of sensor variances. Different approaches can be used to learn such a probabilistic model. One solution is to use Gaussian Process Regression (GPR) [16]. An important advantage of GPR is the ability to learn a probabilistic model from a small set of training

with 1  a  b  M , where a is the starting index and b is the end index that optimally fits to the corresponding

Standard Deviation Sensor Value

Prediction

Measurement

0

10

20

30

40

50

0

10

20

30

40

50

Time Step [1/100s]

Fig. 4: After estimating the current phase of the behavior the deviation between the measured and predicted sensor values can be used to detect external influences. Left: There is no external perturbation. Right: An external perturbation is detected.

knowledge of A is not required for the following variant: xN = a0 x0 + a1 x1 + · · · + aN -1 xN -1 + r. The final snapshot xN can be expressed as a linear combination of the previous ones [x0 , . . . , xN -1 ] by computing the weighting factors [a0 , . . . , aN -1 ], considering the residual r is minimized in a least squares sense, to form the companion matrix   0 a0 1 0 a1      . .. .. .  CN × N . (3) S= . . .      1 0 aN -2 1 aN -1 In [17] the author describes a more robust solution, which is achieved by applying a singular value decomposition on K1 ~  CN × N such that K1 = U W  . The full-rank matrix S is determined on the subspace spanned by the orthogonal ~ = U  K2 W -1 . basis vectors U of K1 , described by S ~ Solving the eigenvalue problem Sµ = µ leads to a subset of complex eigenvectors µ. The DMD modes are defined by  = U µ, which implies a mapping of the eigenvectors µ  CN ×N from a lower dimensional space to a higher dimensional space CM ×N . The complex eigenvalues  contain growth/decay rates  = [log()]/t and frequencies f = [log()]/(2 t) of the corresponding DMD modes . The temporal behavior of the DMD modes is contained in the Vandermonde matrix Vand , which is formed by  -1  1 1 · · · N 1 1 2 · · · N -1  2   (4) Vand =  . . . . . . . . . . . . .  1 N ···
-1 N N

data. The main drawback of this approach is the large computational effort. Another, computationally less expensive solution is to compute the standard deviation  for each time step of the recorded data separately. Given a probabilistic model as described above, we can detect a perturbation by calculating the likelihood of the current sensor readings. In our implementation, we trigger a detection when the sensor values are outside of the computed standard deviation  . Figure 4 shows an example for a regular and a disturbed execution of a behavior. D. Modelling Robot Dynamics using Dynamic Mode Decomposition In this section we use Dynamic Mode Decomposition (DMD) to learn a predictive model describing the change in sensor values under different behavior parameters. DMD is a novel data processing technique from fluid dynamics and was introduced in [17] and [18]. Once a DMD is learned, it can be applied to simulated sensor values under different parameter conditions. DMD presents a modal decomposition for nonlinear flows and features the extraction of coherent structures with a single frequency and growth/decay rate. It computes a linear model which approximates the underlying nonlinear dynamics. Given is an equidistant snapshot sequence N + 1 of an observable x = (u1 , . . . , uM )  CM ×1 which is stacked into two matrices K1 = [x0 . . . , xN -1 ]  CM ×N and K2 = [x1 . . . , xN ]  CM ×N . The matrices K1 and K2 are shifted by one time step t and can be linked via the mapping matrix (system matrix) A  CM ×M such that K2 = AK1 = K1 S . Since the data stem from experiments, the system matrix A is unknown and for a very large system it is computationally impossible to solve the eigenvalue problem directly as well as to fulfill the storage demand [19]. The idea is to solve an approximate eigenvalue problem by projecting A onto an N -dimensional Krylow subspace and to compute the eigenvalues and eigenvectors of the resulting low-rank operator as described in [20]. One type of Krylow methods is the Arnoldi algorithm and the

The DMD modes  must be scaled in order to perform a data recalculation of the first snapshot sequence K1 = D Vand . Therefore, having a look into Vand shows that the first snapshot x0 is independent from temporal behavior since  = [1 , . . . , N ] = 1. The scaling factors  = [1 . . . N ] are calculated by D = x0 , where D = diag {a}. A new solution to find the scaling vectors  was introduced in [21]. Here,  is obtained by considering the temporal growth/decay rates of the DMD modes in order to approximate the entire data sequence K1 optimally. Therefore the problem can be brought into the following form min J () = |W  - µD Vand |F
 2

(5)

which is a convex optimization problem. Its solution leads to
 ))-1 diag (V   = ((µ µ)  (Vand Vand and W  µ)

(6)

where the over line denotes the complex-conjugate of a vector/matrix. However, the key challenge is to identify a subset of DMD modes that captures the most important dynamic structures in order to achieve a good quality approximation. To solve that problem, the sparsity-promoting dynamic mode decomposition (SDMD) [21] was developed. The sparsity structure of the vector of amplitudes  is fixed in order

Standard Deviation Sensor Value

Prediction

Measurement

4

6

8

10

12
-3

400

Trained CoM

x 10

Time Step [1 / 100 s]

300

200

100

C

-4
100 200

-2

0

2

4

Time Step [1/100s]

300

400

500

600

400

Interpolated CoM

Time Step [1 / 100 s]

Fig. 5: External perturbations which differ in strength and direction are increasing the overall warping costs C during behavior execution.

300

200

to determine the optimal values of the non-zero amplitudes. Therefore the objective function J () is extended with an additional term such that
N

100

min J () + 
 i=1

|i | ,

(7)
-4 -2 0 2 4

Step Length [ cm ]

where  denotes a regularization parameter that focuses on the sparsity of the vector . As a result, instead of considering only the modes with largest amplitudes, the sparsity-promoting DMD aims to identify the modes that have the strongest influence on the entire time sequence. The lower the number of non-zero amplitudes, the more the sparse-promoting DMD concentrates on the low-frequency modes. As already mentioned, the data presented here stems from low cost sensors which may be affected by disturbance. Hence, forcing a low number of non-zero amplitudes in  can reduce the influence of noise in the approximation. For our implementation of DMD in a human robot interaction scenario, the snapshot data N +1 is represented by the sensor data recorded during training data acquisition. Each column of the snapshot matrices K1 and K2 contains a fixed number of sensor values, i.e., the longitudinal CoM. E. Calculating a Continuous Measure of Perturbation If the deviation between measured and predicted sensor value is larger than the allowed variance  we assume that an external perturbation is influencing the execution of the behavior. However, the question remains: how strong is the external perturbation? To estimate the strength of the perturbation, we simulate different behavior parameters using the learned DMD model and select the one that produces sensor values similar to our

Fig. 6: DMD is used to generate new sensor values for unknown parameter settings. Top: The training data which consists of five equidistant samples of the longitudinal CoM during walking. Bottom: The longitudinal CoM is interpolated with an interval of 0.01cm resulting in predictions for 800 possible parameter configurations.

current readings. For this task, we make use of the previously described SDTW method. As mentioned the SDTW finds the optimal warping path p for a currently measured subsequence X to a previous recorded dataset Y. Whenever a perturbation is detected, we perform iterative optimization by generating predictions using a DMD model and calculating the warping costs using SDTW. The goal of this optimization process is to identify the behavior parameter that would best explain the currently observed sensor values. Optimization is performed using a stochastic optimization technique, i.e, Covariance Matrix Adaptation Evolution Strategy (CMAES). The warping costs C generated by SDTW are used as objective function. Figure 5 shows the warping costs C calculated during a walking task. The behavior parameter which produces least costs C is regarded as the true behavior parameter if human forces are taken into account. Accordingly, we can generate an estimate

20 DMD SDMD LWR Spline Cubic

Standard Deviation a

Prediction

Measurement b

15 M RE

10

longitudinal CoM

c

d

5

0

CoM

Proper Acceleration

Time Step

Fig. 7: The DMD techniques are compared with a set of classical interpolation schemes. Left: The DMD shows the highest accuracy for the CoM. Right: In presence of high noise, which is the case for proper acceleration estimates, SDMD produces higher accuracy than DMD or classical interpolation schemes.

Fig. 9: Perturbation detection during walking using the robot's longitudinal CoM. Top Left: Slight push from the front. Top Right: Strong push from the front. Bottom Left: Slight push from the back. Bottom Right: Strong push from the back.

for the human forces by calculating the difference between the behavior parameter used to control the robot and the behavior parameter identified by the learned model. IV. E XPERIMENTS In the following experiments we use DMD, SDMD and classical interpolation schemes to learn a model of a robot's walking gait. Furthermore, we evaluate and compare the quality of each of these models. The best model is then used to detect and estimate external perturbations during a human robot interaction task. A. Prediction Quality For the evaluation of DMD and SDMD we make use of a walking dataset recorded on a Nao robot. The longitudinal CoM was recorded for a walking gait with five different equidistant step lengths between -4cm and +4cm. The data is recorded with 100Hz for four seconds. Both the DMD and SDMD algorithms were applied on this dataset, resulting in four DMD modes. Given the learned models, the goal is to generate new sensor values for step lengths that were not recorded during training. Figure 6 shows the five training samples of the longitudinal CoM and the generated model which was interpolated with an interval of 0.01cm. To evaluate the precision of the generated data we additionally recorded test samples with step lengths in an interval of 1cm and measured their mean relative error MRE w.r.t. the corresponding generated data. We also compared the results with a set of classical interpolation schemes. For the CoM, Figure 7 shows that DMD results in the highest accuracy among all methods. SDMD reduces the number of used modes to three and results in a slightly less accurate model. Since, we want to work with low cost sensors which may have significant noise, we additionally recorded the robot's longitudinal acceleration and applied

the same data generation techniques as above. Again, the original DMD uses all extracted modes to predict new sensor values. However, these predictions are corrupted by the fact that some of these extracted modes mainly contain noise. As a result, the prediction performance of DMD deteriorates to about the same level as classical interpolation schemes. In contrast, SDMD concentrates on the three DMD modes that best approximate the sensor data. In this case, one mode was set to zero which obviously contained strong noise. However, because of its smaller MRE, we use the DMD model in conjunction with the CoM for the following experiments. B. Perturbation Detection In the following experiments we detect external perturbations while the robot performs a walking gait with a step length of 0.5cm for 35 seconds. During slowly walking the human perturbs the robot by touching and pushing it as shown in Figure 8. While Figure 8a, 8c show slight pushes, which just marginally disturb the walking gait. We also applied strong pushes as shown in Figure 8b, 8d. Especially, the strong push from the back as shown in Figure 8d significantly affected the robot`s stability during walking. We use the DMD model to generate the predicted sensor values for the current step length. During behavior execution the longitudinal CoM is measured with 100Hz and saved in a sliding window with 10 measurements. To estimate the current walking phase, we calculate the optimal warping path from this subsequence in the predicted data using SDTW. The resulting path is used as time dependent prediction of the longitudinal CoM for the currently measured values. Figure 9 shows the measured and predicted longitudinal CoM for the external perturbations a-d as shown in Figure 8. A perturbation is detected when the measured longitudinal CoM is outside the variance of the predicted one.

(a) Slight Push Front

(b) Strong Push Front

(c) Slight Push Back

(d) Strong Push Back

Fig. 8: The human touches and pushes the robot during the execution of a walking behavior. The estimated perturbation values differ in strength and direction and reflect the amount of force applied on the robot.

Perturbation Value [cm]

2 1 0 -1 -2 0

a

b

c

d

500

1000

Time Step [1 / 100s ]

1500

2000

2500

3000

3500

Fig. 11: The perturbation value for the external perturbation a-d is the difference between the predicted parameter with minimal costs and the current behavior parameter. Perturbation d produces a large oscillation which is dampened over time.
a
0.2

C. Perturbation Estimation
0.15 0.1 0.05 0 0.4 0.3

b

If a perturbation is detected, we have to find another behavior parameter and its corresponding sensor evolution, which has minimal mapping costs C for the SDTW. As mentioned before, there are several different approaches for this minimization problem. However, to prove the correctness of our approach we compute C for each step length of the learned DMD model. Figure 10 shows the overall costs C for all possible step lengths of our DMD model during the peeks of the external perturbations as shown in Figure 8ad. Obviously, pushes from the front produces minimal costs for negative step lengths while pushes from the back lead to positive step lengths. As a result, the parameters with minimal costs can be seen as behavior parameters which counteract the external perturbation. Finally, the perturbation value is calculated from the difference of the current step length of 0.5cm and the predicted step length. Since the behavior parameter is specified in cm the measuring unit for the perturbation value is also in cm. The perturbation value for the complete behavior execution is shown in Figure 11.

C

c

d

C

0.2 0.1 0 -4 -2 0 2 4 -4 -2 0 2 4

Step Length [cm]

Fig. 10: The overall costs C for all possible parameters during the peaks of the external perturbations a-d. The step length which produces the minimal costs (black crosses) is the predicted step length which is used to calculate the perturbation value.

of this approach to industry-grade robots and collaborative assembly tasks. R EFERENCES
[1] P. Rouanet, P.-Y. Oudeyer, F. Danieau, and D. Filliat, "The Impact of Human-Robot Interfaces on the Learning of Visual Objects," IEEE Transactions on Robotics, vol. 29, no. 2, pp. 525­541, Apr. 2013. [2] A. Albu-Sch¨ affer, O. Eiberger, M. Fuchs, M. Grebenstein, S. Haddadin, C. Ott, A. Stemmer, T. Wimb¨ ock, S. Wolf, C. Borst, and G. Hirzinger, "Anthropomorphic soft robotics from torque control to variable intrinsic compliance," in Robotics Research, ser. Springer Tracts in Advanced Robotics, C. Pradalier, R. Siegwart, and G. Hirzinger, Eds. Springer Berlin Heidelberg, 2011, vol. 70, pp. 185­207. [3] S. Haddadin, Towards Safe Robots - Approaching Asimov's 1st Law. Springer, 2014. [4] D. Lee and C. Ott, "Incremental kinesthetic teaching of motion primitives using the motion refinement tube," Autonomous Robots, vol. 31, no. 2-3, pp. 115­131, 2011. [5] H. Wang and K. Kosuge, "Control of a robot dancer for enhancing haptic human-robot interaction in waltz," IEEE Trans. Haptics, vol. 5, no. 3, pp. 264­273, Jan. 2012. [6] H. Ben Amor, E. Berger, D. Vogt, and B. Jung, "Kinesthetic bootstrapping: Teaching motor skills to humanoid robots through physical interaction," in KI 2009: Advances in Artificial Intelligence. Springer Berlin Heidelberg, 2009, pp. 492­499. [7] S. Calinon, Robot programming by demonstration: A probabilistic approach. EPFL Press, 2009. [8] J. Kober and J. Peters, "Policy search for motor primitives in robotics," Machine Learning, vol. 84, no. 1, pp. 171­203, 2011. [9] S. Ikemoto, H. Ben Amor, T. Minato, B. Jung, and H. Ishiguro, "Physical human-robot interaction: Mutual learning and adaptation," IEEE Robotics & Automation Magazine, vol. 19, no. 4, pp. 24­35, 2012. [10] J. St¨ uckler and S. Behnke, "Following human guidance to cooperatively carry a large object," in Humanoids'11, 2011, pp. 218­223. [11] K. Yokoyama, H. Handa, T. Isozumi, Y. Fukase, K. Kaneko, F. Kanehiro, Y. Kawai, F. Tomita, and H. Hirukawa, "Cooperative works by a human and a humanoid robot," in Proceedings. ICRA '03. IEEE International Conference on Robotics and Automation 2003., vol. 3, 2003, pp. 2985­2991 vol.3. [12] A. Bussy, P. Gergondet, A. Kheddar, F. Keith, and A. Crosnier, "Proactive behavior of a humanoid robot in a haptic transportation task with a human partner," in RO-MAN, 2012 IEEE. IEEE, 2012, pp. 962­967. [13] M. Lawitzky, A. Mortl, and S. Hirche, "Load sharing in human-robot cooperative manipulation," in RO-MAN, 2010 IEEE, 2010, pp. 185­ 191. [14] H. Sakoe, "Dynamic programming algorithm optimization for spoken word recognition," IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 26, pp. 43­49, 1978. [15] M. M¨ uller, Information Retrieval for Music and Motion. Secaucus, NJ, USA: Springer-Verlag New York, Inc., 2007. [16] C. E. Rasmussen and C. K. I. Williams, Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning). The MIT Press, 2005. [17] P. J. Schmid, "Dynamic mode decomposition of numerical and experimental data," Journal of Fluid Mechanics, vol. 656, pp. 5­28, 8 2010. [18] C. W. Rowley, I. Igor Mez, I. Shervin Bagher, R. Philipp Schlatte, and D. S. Henningson, "Spectral analysis of nonlinear flows," Journal of Fluid Mechanics, vol. 641, no. -1, pp. 115­127, 2009. [19] S. Bagheri, "Analysis and control of transitional shear flows using global modes," Ph.D. dissertation, KTH, Mechanics, 2010. [20] K. Chen, J. Tu, and C. Rowley, "Variants of dynamic mode decomposition: Boundary condition, koopman, and fourier analyses," Journal of Nonlinear Science, vol. 22, no. 6, pp. 887­915, 2012. [21] M. R. Jovanovi, P. J. Schmid, and J. W. Nichols, "Sparsity-promoting dynamic mode decomposition," Physics of Fluids (1994-present), vol. 26, no. 2, 2014. [22] E. Berger, D. Vogt, N. Haji-Ghassemi, B. Jung, and H. Ben Amor, "Inferring guidance information in cooperative human-robot tasks," in Proceedings of the International Conference on Humanoid Robots (HUMANOIDS), 2013.

Fig. 12: During a cooperative transportation task, a humanoid robot continuously estimates the amount and direction of external perturbations in order to react to human guidance.

D. Reaction Our approach can be used in scenarios where a robot has to detect and react to external perturbations in order to fulfill a specified task. As shown in Figure 12 it can be used to follow the human guidance in a cooperative transportation task as investigated in our previous publication [22]. A video of the functionality can be found under this link1 . Furthermore, our approach can be used to implement collision detection and safety constrains. In addition, the method can also be used to measure the weight of a carried object during a manipulation task. In general, behavior specific filtering allows for a variety of close contact interactions with the environment. V. C ONCLUSION In this paper, we presented a new approach for learning behavior-specific filters that can be used to accurately identify human physical influences on a robot. The approach uses DTW and DMD/SDMD in order to (1) detect an external perturbation, and (2) to quantify the amount of external perturbations. The generated perturbation value can then be used by a robot to adapt its movements to the applied forces or interpret a human command such as "walk backwards". In our experiments we showed that the learned perturbation filter can be used to accurately estimate touch information from noisy, low-cost sensors. Our approach produces a continuous perturbation value that can be used to detect even subtle physical interactions with a human partner. Since we are using a data-driven approach, no thresholds need to be defined by the user. At the core of our approach lies Dynamic Mode Decomposition, which so far has mostly been used in other fields of science, particularly fluid mechanics. We conclude that DMD is also a highly promising method for robotics. For future work, we hope to hierarchically combine several filters in a mixture-of-experts approach, in order to generalize perturbation estimation to new, untrained behaviors. We are currently also investigating the application
1 http://youtu.be/48y0hEix2fY

View publication stats

Probabilistic Movement Modeling for Intention Inference in Human-Robot Interaction
Zhikun Wang1,2 , Katharina Mülling1,2 , Marc Peter Deisenroth2 , Heni Ben Amor2 , David Vogt3 , Bernhard Schölkopf1 , and Jan Peters1,2 Max Planck Institute for Intelligent Systems Spemannstr. 38, 72076 Tübingen, Germany. 2 Technische Universität Darmstadt Hochschulstr. 10, 64289 Darmstadt, Germany. 3 Technical University Bergakademie Freiberg Bernhard-von-Cotta-Str. 2, 09596 Freiberg, Germany.
1

Abstract

Intention inference can be an essential step toward efficient humanrobot interaction. For this purpose, we propose the Intention-Driven Dynamics Model (IDDM) to probabilistically model the generative process of movements that are directed by the intention. The IDDM allows to infer the intention from observed movements using Bayes' theorem. The IDDM simultaneously finds a latent state representation of noisy and highdimensional observations, and models the intention-driven dynamics in the latent states. As most robotics applications are subject to real-time constraints, we develop an efficient online algorithm that allows for real-time intention inference. Two human-robot interaction scenarios, i.e., target prediction for robot table tennis and action recognition for interactive humanoid robots, are used to evaluate the performance of our inference algorithm. In both intention inference tasks, the proposed algorithm achieves substantial improvements over support vector machines and Gaussian processes.

1 Introduction
Recent advances in sensors and algorithms allow for robots with improved perception abilities. For example, robots can now recognize human poses in real time using depth cameras (Shotton et al., 2011), which can enhance the robot's ability to interact with humans. However, effective perception alone may not be sufficient for Human-Robot 1

g x1 z1 x2 z2
(a) GPDM

x3 z3

··· x1 z1 x2 z2
(b) IDDM

x3 z3

···

Figure 1: Graphical models of the Gaussian process dynamical model (GPDM) and the proposed intention-driven dynamics model (IDDM), where we denote the intention by g , state by xt , and observation by zt . The proposed model explicitly incorporates the intention as an input to the transition function (Wang et al., 2012b). Interaction (HRI), since the robot's reactions ideally depend on the underlying intention of the human's action, including the others' goal, target, desire, and plan (Simon, 1982). Human beings rely heavily on the skill of intention inference (for example, in sports, games, and social interaction) and can improve the ability of intent prediction by training. For example, skilled tennis players are usually trained to possess substantially better anticipation than amateurs (Williams et al., 2002). This observation raises the question of how robots can learn to infer the human's underlying intention from movements. In this article, we focus on intention inference from a movement based on modeling how the dynamics of a movement are governed by the intention. This idea is inspired by the hypothesis that a human movement usually follows a goal-directed policy (Baker et al., 2009; Friesen and Rao, 2011). The resulting dynamics model allows to estimate the probability distribution over intentions from observations using Bayes' theorem and to update the belief as additional observation is obtained. The human movement considered here is represented by a time series of observations, which makes discrete-time dynamics models a straightforward choice for movement modeling and intention inference. In a robotics scenario, we often rely on noisy and high-dimensional sensor data. However, the intrinsic states are typically not observable, and may have lower dimensions. Therefore, we seek a latent state representation of the relevant information in the data, and then model how the intention governs the dynamics in this latent state space, as shown in Fig. 1b. The resulting model jointly learns both the latent state representation and the dynamics in the state space. Designing a parametric dynamics model is difficult due to the complexity of human movement, e.g., its unknown nonlinear and stochastic nature. To address this issue, Gaussian processes (GPs), see (Rasmussen and Williams, 2006), have been successfully applied to modeling human dynamics. For example, the Gaussian Process Dynamical Model (GPDM) proposed in (Wang et al., 2008) uses GPs for modeling the

2

(a) Robot table tennis.

(b) Interactive humanoid robot.

Figure 2: Two examples of HRI scenarios where intention inference plays an important role: (a) target prediction in robot table tennis games, and (b) action recognition for human-robot interaction. generative process of human motion with a nonlinear dynamical system, as shown in Fig. 1a. Since the GP is a probabilistic nonparametric model, the unknown structure of the human moment can be inferred from data, while maintaining posterior uncertainty about the learned model itself. As an extension to the GPDM, we propose the Intention-Driven Dynamics Model (IDDM), which models the generative process of intention-driven movements. The dynamics in the latent states are driven by the intention of the human action/behavior, as shown in Fig.1b. The IDDM can simultaneously find a good latent state representation of noisy and high-dimensional observations and describe the dynamics in the latent state space. The dynamics in latent state and the mapping from latent state to observations are described by GP models. Using the learned generative model, the human intention can be inferred from an ongoing movement using Bayesian inference. However, exact intention inference is not tractable due to the nonlinear and nonparametric GP transition model. Therefore, we propose an efficient approximate inference algorithm to infer the intention of a human partner. The remainder of the article is organized as follows. First, in this section, we illustrate the considered scenarios (Section 1.1) and discuss the related work (Section 1.2). Subsequently, we present the Intention- Driven Dynamics Model (IDDM) and address the problem of its training in Section 2. In Section 3, we study approximate algorithms for intention inference and extend them to online inference in Section 4. We evaluate the performance of the proposed methods in the two scenarios, i.e., target prediction in robot table tennis and action recognition, in Section 5 and 6. Finally, we summarize our contributions and discuss properties of the IDDM in Section 7.

1.1 Considered Scenarios
To verify the feasibility of the proposed methods, we discuss two representative scenarios where intention inference plays an important role in human-robot interactions: (1) Target inference in robot table tennis. We consider human-robot table tennis games (Mülling et al., 2011), where the robot plays against a human opponent as shown

3

in Fig. 2a. The robot's hardware constraints often impose strong limitations on its flexibility in such a high-speed scenario; for example, the Barrett WAM robot arm often cannot reach incomining balls due to a lack of time caused by acceleration and torque limits for the biomimetic robot table tennis player described in (Mülling et al., 2011). The robot is kinematically capable of reaching a large hitting plane with pre-defined hitting movements such as forehand, middle, and backhand stroke movements that are capable in returning the ball shot into their corresponding hitting regions. However, movement initiation requires an early decision on the type of movement. In practice, it appears that to achieve the required velocity for returning the ball for the whole kinematically reachable hitting plane, this decision needs to be taken at least 80 ms before the opponent returns the ball (Wang et al., 2011b). Hence, it is necessary to choose the hitting movement before the opponent's racket has even touched the ball. This choice can be made based on inference of the target location where the opponent intends to return the ball from his incomplete stroke movement. We show that the IDDM can improve the prediction of the human player's intended target over a baseline method based on Gaussian process regression, and can thus expand the robot's hitting region substantially by utilizing multiple hitting movements. (2) Action recognition for interactive humanoid robots. In this setting, we use our IDDM to recognize the actions of the human, as shown in Fig. 2b, which can improve the interaction capabilities of a robot (Jenkins et al., 2007). In order to realize natural and compelling interactions, the robot needs to correctly recognize the actions of its human partner. In turn, this ability allows the robot to react in a proactive manner. We show that the IDDM has the potential to identify the action from movements in a simplified scenario. In most robotics applications, including the scenarios discussed above, the decision making systems are subject to real-time constraints and need to deal with a stream of data. Moreover, the human's intention may vary over time. To address these issues, we propose an algorithm for online intention inference. The online algorithm can process the stream data and fulfill the real-time requirements. In the experiments, the proposed online intention inference algorithm achieved over four times acceleration over our previous method in (Wang et al., 2012b).

1.2 Related Work
We review methods for intention inference and for modeling human movements that are related to the proposed IDDM and inference methods. 1.2.1 Intention Inference Inference of intentions has been investigated in different settings. Most of previous work relies on probabilistic reasoning. Intention inference with discrete states and actions has been extensively studied, using Hidden Markov Models (HMMs) to model and predict human behavior where different dynamics models were adopted to the corresponding behaviors (Pentland and Liu, 1999). Online learning of intentional motion patterns and prediction of intentions based on HMMs was proposed in (Vasquez et al., 2008), which allows efficient inference in real time. The HMM can be learned incrementally to cope with new motion

4

patterns in parallel with prediction (Vasquez et al., 2009). Probabilistic approaches to plan recognition in artificial intelligence (Liao et al., 2007) typically represent plans as policies in terms of state-action pairs. When the intention is to maximize an unknown utility function, inverse reinforcement learning (IRL) infers the underlying utility function from an agent's behavior (Abbeel and Ng, 2004). IRL has also been applied to model intention-driven behavior. For instance, maximum entropy IRL (Ziebart et al., 2008) has been used to model goal-directed trajectories of pedestrians (Ziebart et al., 2009) and target-driven pointing trajectories (Ziebart et al., 2012). In cognitive science, Bayesian models were used for inferring goals from behavior in (Rao et al., 2004), where a policy conditional on the agent's goal is learned to represent the behavior. Bayesian models can be used to interpret the agent's behavior and predict its behavior in a similar environment with the learned model (Baker et al., 2006). In a recent work (Friesen and Rao, 2011), a computational framework was proposed to model gaze following, where GPs are used to model the dynamics with actions driven by a goal. These methods assume that the states can be observed. However, in practice the states are often not well-defined or not observable for complex human movement. One can also consider the intention inference jointly with decision making, such as autonomous driving (Bandyopadhyay et al., 2012), control (Hauser, 2012), or navigation in human crowds (Kuderer et al., 2012). For example, when the state space is finite, the problem can be formulated as a Partially Observable Markov Decision Process (Kurniawati et al., 2011) and solved efficiently (Wang et al., 2012a). In contrast, our method assumes that the robot's decision does not influence the intention of the human and considers intention inference and decision making separately, which allows us to efficiently deal with high-dimensional data stream and fulfill the real-time constraints. 1.2.2 Gaussian Process Dynamical Model and Extensions Observations of human movements often consist of high-dimensional features. Determining a low-dimensional latent state space is an important issue for understanding observed actions. The Gaussian Process Latent Variable Model (GPLVM) (Lawrence, 2004) finds the most likely latent variables while marginalizing out the function mapping from latent to observed space. The resulting latent variable representation allows to model the dynamics in a low-dimensional space. For example, the Gaussian Process Dynamical Model (Wang et al., 2008) uses an additional GP transition model for the dynamics of human motion on the latent state space. In robotics applications, the GPLVM can also be used for learning dynamical system motor primitives (Ijspeert et al., 2002) in a low-dimensional latent space, to achieve robust dynamics and fast learning (Bitzer and Vijayakumar, 2009). Nonparametric dynamics models are also applied for tracking a small robotic blimp with two cameras (Ko and Fox, 2009), where GP-Bayes filters were proposed for efficient filtering. In a follow-up work (Ko and Fox, 2011), the model is learned based on the GPLVM, so that the latent states need not be provided for learning. The use of a GP transition model renders exact inference in the GPDM and, hence, 5

in the IDDM, analytically intractable. Nevertheless, approximate inference methods have been successfully applied based on filtering and smoothing in nonlinear dynamical systems. For the GPDM and its extensions, approximate inference can be achieved using Particle Filters (GP-PF), Extended Kalman Filters (GP-EKF), and Unscented Kalman Filters (GP-UKF) as proposed by (Ko and Fox, 2009). GP Assumed Density Filters (GP-ADF) for efficient GP filtering, and general smoothing in GPDMs were proposed in (Deisenroth et al., 2009) and (Deisenroth et al., 2012), respectively. These filtering and smoothing techniques allow the use of Expectation-Maximization (EM) framework for approximate inference (Ghahramani and Roweis, 1999; Turner et al., 2010; Wang et al., 2012b).

2 Intention-Driven Dynamics Model
We propose the Intention-Driven Dynamics Model (IDDM), which is an extension of the GPDM (Wang et al., 2008). The GPDM is a nonparametric approach to learning the transition function in the latent state space and the measurement mapping from states to observations simultaneously. As shown in Fig. 1a, the transition function in the GPDM is only determined by the latent state. However, in the applications considered in this paper, the underlying intention, as an important drive of human movements, can hardly be discovered directly from the observations. Considering that the dynamics can be substantially different when the actions are based on different intentions, we propose the Intention-Driven Dynamics Model. As shown in Fig. 1b, the IDDM explicitly incorporates the intention into the transition function in the latent state space. This dynamics model was inspired by the hypothesis that the human action is directed by the goal (Baker et al., 2009; Friesen and Rao, 2011). For example, in table tennis, the player swings the racket in order to return the ball to an intended target. The target is, hence, a driving factor in the dynamics of the racket. We present the proposed model and address the problem of its training in this section. Later, in Section 3, we study approximate algorithms for intention inference, and extend it for online inference in Section 4.

2.1 Measurement and Transition Models
In the proposed IDDM, one set of GPs models the transition function in the latent space conditioned on the intention g . A second set of GPs models the measurement mapping from the latent states x and the observations z. For notational simplicity, we assume the intention variable g is discrete or a scalar. The model and method can easily generalize to multi-variate intention variables. We detail both the measurement and transition models in the following. This article extensively uses properties of the Gaussian processes, e.g., predictive distribution and marginal likelihood. We refer to (Rasmussen and Williams, 2006) for a comprehensive introduction to GPs.

6

2.1.1 Measurement model The observations of a movement are a time series z1:T [z1 , . . . , zT ], where zt  RDz . In the proposed generative model, we assume that an observation zt  RDz is generated by a latent state variable xt  RDx according to zt = Wh(xt ) + Wnz,t , nz,t  N (0, Sz ) , (1)

where the diagonal matrix W = diag(w1 , . . . , wDz ) scales the outputs of h(xt ). The scaling parameters W allow for dealing with raw features that are measured in different units, such as positions and velocities. We place a GP prior distribution on each dimension of the unknown function h, which is marginalized out during learning and inference. The GP prior GP (mz (·), kz (·, ·)) is fully specified by a mean function mz (·) and a positive semidefinite covariance (kernel) function kz (·, ·). Without specific prior knowledge on the latent state space, we use the same mean and covariance function for the GP prior on every dimension of the unknown measurement function h, and use the noise (co)variance Sz = s2 z I. The predictive probability of the observations zt is given by a Gaussian distribution zt  N (mz (xt ), z (xt )) , where the predictive mean and covariance are computed based on training inputs Xz and outputs Yz , given by
1 mz (xt ) = Yz K- z kz (xt ),

(2) (3)
T 1 K- z kz (xt ) ,

z (xt ) =
2 z (xt )

2 z (xt )I,

= kz (xt , xt ) - kz (xt )

(4)

where, we use the shorthand notation kz (xt ) to represent the cross-covariance vector between h(Xz ) and h(xt ), and use Kz to represent the kernel matrix of Xz . 2.1.2 Transition model We consider first-order Markov transition model, see Fig. 1b, with a latent transition function f , such that xt+1 = f (xt , g ) + nx,t , nx,t  N (0, Sx ) . (5)

The state xt+1 at time t + 1 depends on the latent state xt at time t as well as on the intention g . We place a GP prior GP (mx (·), kx (·, ·)) on every dimension of f with shared mean and covariance functions. Subsequently, the predictive distribution of the latent state xt+1 conditioned on the current state xt and intention g is a Gaussian distribution given by xt+1  N (mx ([xt , g ]), x ([xt , g ])) based on training inputs Xx and outputs Yx , with
1 mx ([xt , g ]) =Yx K- x kx ([xt , g ]),

(6) (7)
T 1 K- x kx ([xt , g ]) ,

x ([xt , g ])
2 x ([xt , g ])

2 = x ([xt , g ])I,

=kx ([xt , g ], [xt , g ]) - kx ([xt , g ])

(8)

where Kx is the kernel matrix of training data Xx = [x1 , g1 ], . . . , [xn , gn ] . The transition function f may also depend on environment inputs u, e.g., controls or motor commands. We assume that environment inputs are observable and omit them in the description of model for notational simplicity. 7

2.2 Covariance Functions
By convention, we use GP prior mean functions that are zero everywhere for notational simplicity, i.e., mz (·)  0 and mx (·)  0. Hence, the model is determined by the covariance functions kz (·, ·) and kx (·, ·), which will be motivated in the following. The underlying dynamics of human motion are usually nonlinear. To account for nonlinearities, we use a flexible Gaussian tensor-product covariance function for the dynamics, i.e., kx ([xi , gi ], [xj , gj ]; ) = kx (xi , xj ; )kx (gi , gj ; ) + knoise = 1 exp
2 - 2

(9)

xi - xj

2

-

3 2 (gi

- gj )

2

+ 4 ij ,

where  = [1 , 2 , 3 , 4 ] is the set of all hyperparameters, and  is the Kronecker delta function. When the intention g is a discrete variable, we set the hyperparameter 3 =  such that kx (gi , gj ; )  ij . The covariance function for the measurement mapping from the state space to observation space is chosen depending on the task. For example, the GPDM in (Wang et al., 2008) uses an isotropic Gaussian covariance function
1 kz (x, x ;  ) = exp -  2 x-x

2

+ 2 x,x ,

(10)

parameterized by the hyperparameters  , as, intuitively, the latent states that generate human poses lie on a nonlinear manifold. Note that the hyperparameters  do not contain the signal variance, which is parameterized by the scaling factors W in Eq. (1). In the context of target prediction in table tennis games, we use the linear kernel kz (x, x ;  ) = xT x + 1 x,x , as the observations are already low-dimensional, but subject to substantial noise. (11)

2.3 Learning the IDDM
The proposed IDDM can be learned from a training data set D = {Z, g } of J movements and corresponding intentions. Each movement Zj consists of a time series of j T observations given by Zj = [zj 1 , . . . , zT ] . We construct the overall observation matrix Z by vertically concatenating the observation matrices Z1 , . . . , ZJ , and the overall intention matrix g from g 1 , . . . , g J . In the robot table tennis example, one movement corresponds to a stroke of the opponent, represented by a time series of observed racket and ball configurations. We assume the intention g can be obtained for training, for example by post-processing the data. In the robot table tennis example, the observed intention corresponds to the target where the opponent returns the ball to (see Fig. 5 for an illustration). In the table-tennis training data, we can obtained the target's coordinates by post-processing. In the action recognition, the label of action is provided directly in the training data. Similar to the GPDM (Wang et al., 2008), we find maximum a posteriori (MAP) estimates of the latent states X. Alternative learning methods and an empirical comparison can be found in (Turner et al., 2010; Damianou et al., 2011). Given the model 8

hyperparameters, the posterior distribution of latent states X can be decomposed into the probability of the observations given the states and the probability of the states given the intention, i.e., p(X|Z, g, ,  , W)  p(Z|X,  , W)p(X|g, ), (12)

both obtained by the GP marginal likelihood (Rasmussen and Williams, 2006). The GP marginal probability of the observations Z given the latent states X is given by a Gaussian distribution p(Z|X,  , W) = 
|W|M (2 )M Dz |Kz |Dz -1 T exp - 1 2 tr Kz ZWW Z

,

(13)

where M JT is the length of observations Z, and Kz is the kernel matrix computed by the kernel function kz (·, ·). Given the intention g , the sequence of latent states X has a Gaussian probability p(X|g, ) = p(X1 )p(X2:T |X1:T -1 , g, ) =
p(X1 ) (2 )mDx |Kx |Dx -1 T exp - 1 2 tr Kx X2:T X2:T

,

(14)

where Xt , t  {1, . . . , T } is constructed by vertically concatenating state matrices J x1 J (T - 1) is the length of X2:T , and Kx is the kernel matrix of X2:T , t , . . . , xt , m computed by the kernel function kx (·, ·). We use a Gaussian prior distribution on the initial states X1 . Based on Eqs. (13)­(14), the MAP estimates of the states are obtained by maximizing the posterior in Eq. (12). In practice, we minimize the negative log-posterior
T -1 1 z - M log |W| L(X) = D 2 log |Kz | + 2 tr Kz ZWW Z

+

Dx 2

-1 T T 1 log |Kx | + 1 2 tr Kx X2:T X2:T + 2 tr X1 X1 + const

(15)

with respect to the states X, using the Scaled Conjugate Gradient (SCG) method (Møller, 1993).

2.4 Learning Hyperparameters
A reliable approach to learning the hyperparameters  = {,  , W} is to maximize the marginal likelihood p(Z|g, ) = p(Z, X|g, )dX, (16)

which can be achieved approximately by using the Expectation-Maximization (EM) algorithm (Bishop, 2006). The EM algorithm computes the posterior distribution of states q (X) = p(X|Z, g, ), given in Eq. (12), in the Expectation (E) step and updates the hyperparameters by maximizing the expected data likelihood Eq [p(Z, X|g, )] in the Maximization (M) step. However, the posterior distribution q (X) is difficult to compute in the IDDM. Following (Wang et al., 2008), we draw samples of the states 9

Algorithm 1: Learning the model hyperparameters ,  , and W by maximizing the marginal likelihood, using the Monte Carlo EM algorithm. Input : Data: D = {Z, g } Input : Number of EM iterations: L Output: Model hyperparameters:  = {,  , W} 1 for l  1 to L do 2 for i  1 to I do 3 Initialize X by its MAP estimate ; 4 Draw sample X(i) from p(X|Z, g, ) using HMC;
5

Maximize

1 I

I i=1

log p(Z, X(i) |g, ) w.r.t.  using SCG;

X(1) , . . . , X(I ) from the posterior distribution using hybrid Monte Carlo (Andrieu et al., 2003), and, hence, the data likelihood is estimated via Monte Carlo integration according to I 1 Eq [p(Z, X|g, )]  p(Z, X(i) |g, ). (17) I i=1 In the M step, we use SCG to update the hyperparameters. In practice, we choose the number of samples I = 50 and the number of EM iterations L = 10. Although this procedure, as described in Algorithm 1, is time-demanding, in practice, we can learn the hyperparameters off-line. In practice, the maximum likelihood estimate of the hyperparameters may lead to over-fitting. For the IDDM, we found that the noise variance 4 in Eq. (9) of the transition model is occasionally underestimated, e.g., 4 < e-6 , as Algorithm 1 estimates it based on only a few samples. The underestimated noise variance may prevent the learned model from generalizing to test data that have significant deviation from the training data. This phenomenon of over-confidence has been discussed in (Lawrence, 2005; Wang et al., 2008). To alleviate this problem, we add a small constant e-3 to the learned noise variance 4 . The model also depends on the hyperparameter Dx , i.e., the dimensionality of the latent state space. Choosing an appropriate Dx is important. If the dimensionality is too small, the latent states cannot recover the observations, which leads to significant prediction errors. On the other hand, a high-dimensional state space results in redundancy and can cause a drop in performance and computational efficiency. Nevertheless, model selection, based on cross-validation for example, is conducted before learning and applying the model. To summarize, the model M = {X, } can be learned from a data set D. Subsequently, we use the model to infer the unobserved intention of a new ongoing movement, as described in the following section.

10

3 Approximate Intention Inference
After learning the model M from the training data set D, the intention g can be inferred from a sequence of new observations z1:T . For notational simplicity, we do not explicitly condition on the model M and the data set D. The measurement model defined in Eq. (1) scales the observations by a diagonal matrix W. Therefore, we pre-process every received observation with the scaling matrix W and omit W hereafter as well. The IDDM models the generative process of movements, represented by observations z1:T , given an intention g . Using Bayes' rule, we estimate the posterior probability (belief) on an intention g from observations z1:T . The posterior is given by p(g |z1:T ) = p(z1:T |g )p(g ) p(z1:T ) p(z1:T , x1:T |g )dx1:T , (18) (19)

 p(g )

where computing the marginal likelihood p(z1:T |g ) requires to integrate out the latent states x1:T . Exactly computing the posterior in Eq. (19) is not tractable due to the use of nonlinear GP transition model. Hence, we resort to approximate inference. In (Wang et al., 2012b), we introduced an EM algorithm for finding the maximum likelihood estimate of intention. However, this point estimate may not suffice for the reactive policies of the robot that also take into account the uncertainty in the intention inference (Wang et al., 2011a,b; Bandyopadhyay et al., 2012). For example, in the table tennis task, the robot may need to choose the optimal time to initiate its hitting movement, and such a choice is ideally made based on how certain the prediction of target is (Wang et al., 2011b). In this article, we extend our previous inference method introduced in (Wang et al., 2012b), such that the uncertainty about the intention is explicitly modeled and taken into account when making decisions. The key challenge in estimating the belief in Eq. (19) is integrating out the latent states x1:T . A common approximation to the log marginal posterior is to compute a lower bound B (g )  log p(g |z1:T ) based on Jensen's inequality (Bishop, 2006). The bound is given by B (g ) Eq [log p(z1:T , x1:T , g )] + H(q ) = log p(g |z1:T ) - KL (q ||p(x1:T |z1:T , g ))  log p(g |z1:T ) , which holds for any distribution q (x1:T ) on the latent states. Here, the KullbackLeibler (KL) divergence KL (q ||p(x1:T |z1:T , g )) determines how well B (g ) can approximate the belief. Based on this approximation, the inference problem consists of two steps, namely, (a) finding an approximation q (x1:T )  p(x1:T |z1:T , g ), and (b) computing the approximate belief B(g ). When using the EM algorithm for the maximum likelihood estimate of the intention g , as in (Wang et al., 2012b), the E-step and M-step correspond to these two steps, respectively. For step (a), we approximate the posterior of latent states p(x1:T |z1:T , g ) by a Gaussian distribution q (x1:T ). For this purpose, we use the forward-backward smoothing method proposed in (Deisenroth et al., 2009, 2012), which is based on moment 11 (20)

matching. Typically, Gaussian moment-matching provides credible error bars, i.e., it is robust to incoherent estimates. The resulting approximate distribution q that we use in the lower bound B in Eq. (20) is given by q (x1:T ) = N (µq , q )  p(x1:T |z1:T , g ), with the mean and block-tri-diagonal covariance matrix  x 1|T  µx  x 1| T 2,1|T  .   µq =  . .  , q =   µx T |T 0  x 1, 2| T .. . .. . 0 .. .. . . x T -1,T |T x T |T    ,   (22) (21)

x T,T -1|T

where we only need to consider the cross-covariance between consecutive states1 . For step (b), based on the approximation q (x1:T ), the posterior belief p(g |z1:T ) can then be approximated by the lower bound B(g ) in Eq. (20). In the following, we first detail step (a), i.e., the computation of q for our IDDM, in Section 3.1. Subsequently, we discuss step (b), i.e., efficient belief estimation, in Section 3.2.

3.1 Filtering and Smoothing in the IDDM
To obtain the posterior distribution p(x1:T |z1:T , g ), approximate filtering and smoothing with GPs are crucial in our proposed IDDM. We place a Gaussian prior on the initial state x1 . Subsequently, Gaussian approximations q (xt-1 , xt ) of p(xt-1 , xt |z1:T , g ) for t = 2, . . . , T are computed. We explicitly determine the marginals p(xt |z1:T , g ) for t = 1, . . . , T , and the cross-covariance terms cov[xt-1 , xt |z1:T , g ], t = 2, . . . , T . These steps yield a Gaussian approximation with a block-tri-diagonal covariance matrix, see Eq. (22). These computations are based on forward-backward smoothing (GPRTSS) as proposed in (Deisenroth et al., 2012). As a first step, we compute the posterior distributions p(xt |z1:T , g ) with t = 1, . . . , T . To compute these posteriors using Bayesian forward-backward smoothing in the IDDM, it suffices to compute both joint distributions p(xt-1 , xt |z1:t-1 , g ) and p(xt , zt |z1:t-1 , g ). The Gaussian filtering and smoothing updates can be expressed solely in terms of means and (cross-)covariances of these joint distributions, see (Deisenroth and Ohlsson, 2011; Deisenroth et al., 2012). Hence, we have
x xz z -1 µx (zt - µz t|t = µt|t-1 + t|t-1 (t|t-1 ) t|t-1 ) ,

(23) (24) (25)

x t|t µx t-1|T x t|T

= = =

xz z -1 zx x t|t-1 t|t-1 - t|t-1 (t|t-1 ) x x x µt-1|t-1 + Jt-1 (µt|T - µt|t-1 ) ,

,

x t-1|t-1

+

Jt-1 (x t|T

-

x t|t-1 )Jt-1

,

(26)

1 We use the short-hand notation ad where a = µ denotes the mean µ and a =  denotes the b|c covariance, b denotes the time step of interest, c denotes the time step up to which we consider measurements, and d  {x, z } denotes either the latent space (x) or the observed space (z ).

12

where we define
x -1 Jt-1 = x . t-1,t|t-1 (t|t-1 )

(27)

In the following, we first detail the computations required for a Gaussian approximation of the joint distribution p(xt-1 , xt |z1:t-1 , g ) using moment matching. Here, we approximate the joint distribution p(xt-1 , xt |z1:t-1 , g ) by the Gaussian N x µx t-1|t-1 t-1|t-1 , x x µt|t-1 t,t-1|t-1 x t-1,t|t-1 x t|t-1 . (28)

x Without loss of generality, the marginal distribution N (xt-1 | µx t-1|t-1 , t-1|t-1 ), which corresponds to the filter distribution at time step t - 1, is assumed known. We compute the remaining elements of the mean and covariance in Eq. (28) in the following paragraphs. We will derive our results for the more general case where we have a joint Gaussian distribution p(xt-1 , xt , g |z1:t-1 ). The known mean and covariance ~ t-1|t-1 = [(µx of distribution p(xt-1 , g |z1:t-1 ) are given by µ t-1|t-1 ) , µg ] and ~ t-1|t-1 , respectively, where the covariance matrix  ~ t-1|t-1 is block-diagonal with  blocks x t-1|t-1 and g . By setting the mean µg = g and g = 0, we obtain the ~ = [x , g ] . results from (Wang et al., 2012b). For convenience, we define x Using the law of iterated expectations, the a-th dimension of the predictive mean of the marginal p(xt |z1:t-1 ) is given as

~ t-1 ]|z1:t-1 (µx xt-1 )|x t|t-1 )a = Ext-1 Efa [fa (~ = ~ t-1 , xt-1 )p(~ xt-1 |z1:t-1 )dx ma x (~

(29)

where we substituted the posterior GP mean function for the inner expectation. Note that if g is given then g = 0. Writing out the posterior mean function and defining 1  a := K- x ya , with yai , i = 1, . . . , M , being the training targets of the GP with target dimension a, we obtain (µx t|t-1 )a = q  a , where we define q = ~ t-1 . kx ([xt-1 , g ], Xx )p(~ xt-1 |z1:t-1 )dx (31) (30)

~ i = [xi , gi ] of the transition Here, Xx denotes the set of the M GP training inputs x GP. Since kx is a Gaussian kernel, we can solve the integral in Eq. (31) analytically and obtain the vector q with entries qi with i = 1, . . . , M as
-1 i , qi = 1 ||- 2 exp - 1 2  i () 1

(32) + I, (33)

~i - µ ~ t-1|t-1 , i = x

 = t-1|t-1 

-1

13

where  is a diagonal matrix of concatenated length-scales 2 I and 3 I. By applying x the law of total variances, the entries ab of the marginal predictive covariance matrix x t|t-1 in Eq. (28) are given by
x ab =

 a (Qx - qq ) b  a (Q - qq ) b + 1 - tr (Kx + 4 I)
x -1

if a = b , Q
x

+ 4

if a = b .

(34)

We define the entries of Qx  RM ×M as Qx ij = with ~ t-1|t-1 (-1 + -1 ) + I , R :=  a b
-1 1 ~ -1 T = - a + b + t-1|t-1 , 1 1 ~ t-1|t-1 ) + - ~ t-1|t-1 ) . zij := - xi - µ xj - µ a (~ b (~ a b ~ t-1|t-1 ])kx ~ t-1|t-1 ]) kx ([xi , gi ], [µ ([xj , gj ], [µ

|R |

exp

-1 1 zij 2 zij T

For a detailed derivation, we refer to (Deisenroth, 2010; Deisenroth et al., 2012). To fully determine the joint Gaussian distribution in Eq. (28), the cross-covariance x t-1,t|t-1 = cov[xt-1 , xt |z1:t-1 , g ] is given as the upper part of the cross-covariance
M

cov[xt-1 , xt , g |z1:t-1 ] =
i=1

~ t-1|t-1 -1 (~ ~ t-1|t-1 ) , ai qai  xi - µ

when we set µg = g and g = 0. Note that q and  are defined in Eq. (32) and (33), respectively. Up to now, we have computed a Gaussian approximation to the joint probability distribution p(xt-1 , xt |z1:t-1 , g ). Let us now have closer look at the second joint distribution p(xt , zt |z1:t-1 , g ), which is the missing contribution for Gaussian smoothing (Deisenroth and Ohlsson, 2011), see Eq. (23)­(26). To determine a Gaussian approximation N x µx t|t-1 t|t-1 , z µt|t-1 zx t|t-1 xz t|t-1 z t|t-1 (35)

to p(xt , zt |z1:t-1 , g ) it remains to compute the mean and the covariance of the marginal distribution p(zt |z1:t-1 , g ) and the cross-covariance terms cov[xt , zt |z1:t-1 , g ]. We omit these computations for the nonlinear Gaussian kernel as they are very similar to the computations to determine the joint distribution p(xt-1 , xt |z1:t-1 , g ). For the linear measurement kernel in Eq. (11), we compute the marginal mean µz t|t-1 in Eq. (35) for observation dimension a = 1, . . . , Dz according to Eh,xt-1 [ha (xt )|z1:t-1 , g ] = = m(xt )p(xt |z1:t-1 , g ) dxt xt Xz p(xt |z1:t-1 , g ) dxt  a = q  a , (36)

q = Xz µx t|t-1 . 14

1 Here, Xz comprises the training inputs for the measurement model and  a = K- z Yza , where Yza are the training targets of the ath dimension, a = 1, . . . , Dz . The elements z ab of the marginal covariance matrix z t|t-1 in Eq. (35) are given as z ab =

a (Qz - qq )a
x x x t|t-1 + µt|t-1 (µt|t-1 )

if a = b , - tr
1 z K- z Q

+  a (Q - qq ) a
z

if a = b , (37)

a, b = 1, . . . , Dz , where we define Qz =
x x Xz xt xt Xz p(xt |z1:t-1 , g ) dxt = Xz (x t|t-1 + µt|t-1 (µt|t-1 ) )Xz .

The cross-covariance xz t|t-1 = cov[xt , zt |z1:t-1 , g ] in Eq. (35) is given as
x xz t|t-1 = t|t-1 Xz  a

(38)

for all observed dimensions a = 1, . . . , Dz . The mean µz t|t-1 in Eq. (36), the covariand the cross-covariance in Eq. (38) fully determine the ance matrix z in Eq. (37), t|t-1 Gaussian distribution in Eq. (35). Hence, following (Deisenroth and Ohlsson, 2011), we can now compute the latent state posteriors (filter and smoothing distributions) according to Eq. (23)­(26). These smoothing updates in Eq. (23)­(26) yield the marginals of our Gaussian approximation to p(x1:T |z1:T , g ), see Eq. (22). The missing cross-covariances x t-1,t|T of p(x1:T |z1:T , g ) that finally fully determine the block-tri-diagonal covariance matrix in Eq. (22) are given by
x x t-1,t|T = Jt-1 t|T ,

(39)

where Jt-1 is given in Eq. (27). For detailed derivations, we refer to (Deisenroth, 2010). These computations conclude step (1) on lower-bounding the posterior distribution on the intention, see Eq. (20), i.e., the computation of the approximate distribution q in Eq. (21). It remains to compute the bound B itself, which is described in the following.

3.2 Estimating the Belief on Intention
For a given intention g , we compute a Gaussian approximation q (x1:T ) to the posterior p(x1:T |z1:T , g ), given by q (xt , xt+1 ) = N x µx t|T t|T , x µx t+1,t|T t+1|T x t,t+1|T x t+1|T (40)

for t = 1, . . . , T - 1. The belief p(g |z1:T )  exp(B (g )) is estimated using Eq. (20), where the computation can be decomposed according to
T -1

B (g ) =
t=1

Eq [log p(xt+1 |xt , g )] +p(g ) + H(q ) + const.
Qt ( g )

(41)

15

Here the smoothing distribution q (x1:T |g )  p(x1:T |z1:T , g ) is computed given the intention g . As we only need to estimate the unnormalized belief, the constant term needs not to be computed. The entropy H(q ) of the Gaussian distribution q can be computed analytically, and is given by H (q ) = We define Qt (g ) = = Eq [log p(xt+1 |xt , g )] q (xt , xt+1 ) log p(xt+1 |xt , g )dxt+1 dxt q (xt , xt+1 ) log (p(xt+1 |xt , g )q (xt )) dxt+1 dxt -
q ~(xt ,xt+1 )

1 (T Dx + T Dx log(2 ) + log |q |) . 2

(42)

(43)

q (xt ) log q (xt )dxt ,

where p(xt+1 |xt , g )q (xt ) can be approximated by a Gaussian distribution q ~(xt , xt+1 ) = N (µq ,  ) based on moment matching (Quiñonero-Candela et al., 2003). Here, we ~ q ~ only compute the diagonal elements in the covariance matrix of q . As a result, ~ Eq. (43) is approximated as Qt (g )  KL q (xt , xt+1 )||q ~(xt , xt+1 ) + H q (xt , xt+1 ) + H q (xt ) , (44)

where H(q ) is the entropy of the distribution q and KL(q ||q ~) is the Kullback-Leibler (KL) divergence between q and q ~, both of which are Gaussians. The KL divergence also has a closed-form expression, given by KL(q ||q ~) = 1 2
1 T -1 tr(- ~) q ~) - log q ~ q ) + (µq - µq ~ (µq - µq

|q | |q ~|

+ const.

As a result, we can compute the unnormalized belief B (g ) for a given intention g approximately according to Eq. (41). We aim to determine the posterior distribution p(g |z1:T ) of the intention g . Using the posterior distribution instead of point estimates allows us to express uncertainty about the inferred intention g . Computing Gaussian approximations of the posterior distributions can be done using the unscented transformation (Deisenroth et al., 2012), for instance. However, when the posterior is not unimodal, a Gaussian approximation may lose important information. Particle filtering can preserve all the modes (Ko and Fox, 2009), but will not be sufficiently efficient due to the real-time constraints. As we focus on one-dimensional intentions in this article, we advocate the discretization of intention. For example, in the table tennis task, the intention (opponent's target position) is a bounded scalar variable g  [gmin , gmax ], where the bounds are given by physical constraints such as the table width and the length of robot arm. We uniformly choose {v1 , . . . , vK } from [gmin , gmax ] and represent intention by the index, i.e., g  {1, . . . , K }.

16

Algorithm 2: Inference of the discretized intentions by computing the posterior probabilities for every value of the intention. Input : Observations x1:T Output: Posterior probabilities for every intention value g  {1, . . . , K } 1 foreach g  {1, . . . , K } do 2 Compute smoothing distribution q (x1:T )  p(x1:T |z1:T , g ) ; 3 Compute the value of B(g ) = Eq [log p(x1:T |g )] + log p(g ) using the approximation in Eq. (44) ;
4

Estimate the posterior p(g |x1:T )  exp B(g )/(

K g =1

exp B(g )).

3.3 Discussion of the Approximate Inference Method
To summarize, the algorithm for computing the posterior distribution over discrete or discretized intentions g is given in Algorithm 2. The smoothing distribution q defined in Eq. (40) depends on the current estimate of intention g . However, it is often time-demanding to enumerate the intention g and compute the smoothing distribution q for each g individually. The computational complexity of the 3 2 3 smoothing step in Algorithm 2 is O(T K (Dz + Dx Dz + N 2 Dx )) when using the linear 2 2 3 ))) + Dz + N 2 Dx (Dx kernel function for the measurement mapping, and O(T K (Dz when using the Gaussian kernel function, where T is the number of observations obtained, K the number of (discretized) intentions, N the number of training data, and Dx and Dz the dimensionality of state and observation. The complexity of com2 ). The computational efficiency can be improved puting the belief is O(T KN 2 Dx to meet the tight time constraints in robotics applications by introducing further approximations, such as adopting GP pseudo inputs to reduce the size of training data N (Snelson and Ghahramani, 2006; Quiñonero-Candela and Rasmussen, 2005), using dimensionality reduction or feature selection techniques to obtain a small number of features Dz (van der Maaten et al., 2009; Ding and Peng, 2005), and reducing the sample size K of intention g . However, the dependence of complexity on the number of observations T still prevents the algorithm from being applied to online scenarios. For these, T keeps growing as new observations come, whereas observations obtained a long time ago do not provide as much information as recent ones. To address this issue, we will introduce an approximation in the online inference method in Section 4.

4 Online Intention Inference
The introduced inference algorithm can be seen as a batch algorithm that relies on the segmentation of human movements. However, in online human-robot interaction, the intention inference algorithm faces new challenges to deal with the stream of observations. The complexity of Algorithm 2 grows with the number of existing observations, which does not fulfill the real-time requirements of an online method. In addition, the intention can vary over time in an online inference scenario. For example, the intended targets in table tennis games vary between strokes. Hence, the online method should model and track the change of intention. To address these issues, we generalize the inference method to an online scenario. 17

g

···

xt - 1

xt

xt + 1

···

zt - 1

zt

zt + 1

Figure 3: The graphical model of the IDDM in an online manner, which can handle a stream of observations. That is, the observations are obtained constantly, and the belief on the intention is reestimated after receiving a new observation. A computational bottleneck in the batch method is that the smoothing distribution q is computed for every value of intention. For efficient inference, we compute a marginal smoothing distribution q according to current belief on intention p(g ), i.e., we integrate out the intention, q (x1:t )
g

p(g )qg (x1:t ).

(45)

The online inference algorithm then estimates the belief Bt (g ) on the intention based on the marginal smoothing distribution q after receiving an observation, which can be sufficiently efficient for real-time intention inference with a small sacrifice in accuracy. Based on the marginal smoothing distribution, we update the belief on intention using dynamic programming. which will be discussed as follows.

4.1 Online Inference using Dynamic Programming
Assuming the marginal smoothing distribution q is given, we develop an online inference method using dynamic programming (see Fig. 3). The method maintains the belief (i.e., log of the unnormalized posterior) of the intention g based on the obtained observations z1:t-1 according to Eq. (41), given by Bt-1 (g )  Eq [log p(g, x1:t-1 )] + const. (46) Here, we consider discretized intentions g  {1, . . . , K }, and write the belief Bt-1 as a vector of length K . For a new observation zt , we decompose p(g, x1:t ) according to p(g, x1:t ) = p(xt |xt-1 , g )p(g, x1:t-1 ). As a result, the belief Bt becomes Bt (g ) = Eq [log p(g, x1:t )] + const = Eq [log p(xt |xt-1 , g )] + Eq [log p(g, x1:t-1 )] + const = Eq [log p(xt |xt-1 , g )] + Bt-1 (g ) + const, 18 (48) (49) (50) (47)

which is in a recursive form and can be computed efficiently using dynamic programming. Given a new observation zt , the belief is updated based on Eq [log p(xt |xt-1 , g )], which is computed according to Eqs. (43)-(44). The belief Bt is then normalized, i.e., g exp(B t (g )) = 1. In addition, the intention can vary over time in an online inference scenario. As the new observation zt can be more informative than the previous observations z1:t-1 , we introduce a forgetting factor to shrink the belief Bt-1 . The recursive formula of the belief is subsequently given by Bt (g ) = Eq [log p(xt |xt-1 , g )] + (1 - )Bt-1 (g ), where the shrinking factor observations. (51)

determines how fast the algorithm forgets the previous

4.2 Marginal Smoothing Distribution
The inference method relies on the smoothing distribution q at time t, which in turn depends on the intention belief Bt-1 . In analogy to the EM algorithm, we iteratively update the belief on intention B and the smoothing distribution q . However, full forward-backward smoothing on x1:t is impractical as the computational complexity grows when we obtain more observations. Full smoothing is also not unnecessary since we do not update the previous belief B1:t-1 on the intention. Hence, given a new observation zt , we only need to compute q (xt-1:t ), which requires a single-step forward filtering and a single-step backward smoothing, based on the current belief Bt-1 . The filtering and smoothing need to integrate out the uncertainty in the intention. For discrete intentions, we can simply compute the smoothing distributions qg for every value of intention gt-1 , and average over them q (xt-1:t ) 
g

qg (xt-1:t )pt-1 (g ),

(52)

where the belief pt-1 (g )  exp(Bt-1 (g )). The resulting distribution q will still be a Gaussian distribution. For continuous intentions, enumerating the discretized intention may be inefficient. To address this problem, we use the moment matching to approximate the distribution on intention by a Gaussian distribution, which is also adopted in the filtering and 2 smoothing method. Specifically, we compute the mean µg and variance g according to the belief Bt-1 . As a result, the marginal smoothing distribution is given by q (xt-1:t ) 
2 qg (xt-1:t )N (g |µg , g )dg,

(53)

which is computed using moment matching.

4.3 Discussion of the Online Inference Method
The online inference algorithm described in Algorithm 3 iteratively updates the belief of intention and latent states. 19

Algorithm 3: The online algorithm for the inference of discrete intention g  {1, . . . , K }.
1 2 3 4 5 6

7

8 9 10 11

Obtain the initial observation z1 ; Initialize the approximate distribution q (x1 ) ; Initialize B1 (g ) = log p(g ) according to the prior ; for t = 2, 3, . . . do Obtain the observation zt ; Compute marginal filtering distribution q (xt ) according to current belief Bt-1 ; Update marginal smoothing distribution q (xt-1 ) according to current belief Bt-1 ; foreach gt = {1, . . . , K } do Compute B0 (g ) = Qt-1 (g ) using the approximation in Eq. (44) ; Update the belief Bt = B0 + (1 - )Bt-1 ; Normalize the belief Bt  Bt - log
g

exp(Bt (g )) ;

3 The computational complexity of the smoothing step in Algorithm 3 is O(Dz + 2 2 3 Dx Dz + N Dx ) when using the linear kernel function for the measurement mapping, 3 2 3 and O(Dz + N 2 ( Dx Dx + Dx )) when using the Gaussian kernel function, which no longer depends on the number of observations T and the number of intentions K . The 2 complexity of computing the belief is O(KN 2 Dx ). Comparing to the batch algorithm, the efficiency is improved by a factor of T . To summarize, we proposed an efficient online method for intention inference from a new movement. The online method updates the belief of the intention by taking into account both the current belief and the new evidence (i.e., new observation). We list the employed approximations in both the batch and online inference methods in Table 1.

5 Target Prediction for Robot Table Tennis
Playing table tennis is a challenging task for robots, and, hence, has been used by many researchers as a benchmark task in robotics (Anderson, 1988; Billingsley, 1984; Fässler et al., 1990; Matsushima et al., 2005; Mülling et al., 2011). Up to now, none of the groups that have been working on robot table tennis ever reached levels of a young child, despite having robots with better perception, processing power, and accuracy Table 1: Important approximations employed in the batch and online inference. belief p(g |z1:T ) approx. belief B(g ) distr. p(x1:T |z1:T , g ) stream of observations batch online Jensen's lower bound B(g ); cf. Eq. (20) moment matching; cf. Eq. (44) q (x1:T |g ) for each g q (x1:T ) for all g ; cf. Eq. (53) sliding window recursive update; cf. Eq. (51)

20

than humans (Mülling et al., 2011). Likely explanations for this performance gap are (i) the human ability to predict hitting points from opponent movements and (ii) the robustness of human hitting movements (Mülling et al., 2011). In this article, we focus on the first issue: anticipation of the hitting region from opponent movements. Using the proposed method, we can predict the where the ball is likely to be shot before the opponent hits the ball, which gives the robot a head start of more than 200 ms additional time to initiate its movement2 . This additional time can be crucial due to robot's hardware constraints, for example, acceleration and torque limits in the considered setting (Mülling et al., 2011). Note that the predicted intention is only used to choose a hitting type, e.g., forehand, middle, or backhand. Fine-tuning of the robot's movement can be done when the robot is adjusted to the forehand/middle/backhand preparation pose and once the returned ball can be reliably predicted from the ball's trajectory alone. Hence, a certain amount of intention prediction error is tolerable since the robot can apply small changes to its basic hitting plan based on the ball's trajectory. However, the robot cannot return the ball outside the corresponding hitting region once it is adjusted to a preparation pose, see the video3 . Therefore, prediction accuracy directly influences the performance of the robot player (Wang et al., 2011b).

5.1 Experimental Setting
Our anticipation system has been evaluated in conjunction with the biomimetic robot table tennis player (Mülling et al., 2011), as this setup allowed exhibiting how much of an advantage such a system may offer. We expect that the system will help similarly or more when deployed within our skill learning framework (Mülling et al., 2013) as well as many of the recent table tennis learning systems (Huang et al., 2013; Yang et al., 2010; Matsushima et al., 2005). We used a Barrett WAM robot arm to play table tennis against human players. The robot's hardware constraints impose strong limitations on its acceleration, which severely restricts its movement abilities. This limitation can best be illustrated using typical table tennis stroke movements as shown in Fig. 4, see (Ramanantsoa and Durey, 1994; Mülling et al., 2011), which consist of four stages, namely awaiting stage, preparation stage, hitting stage, and finishing stage. In the awaiting stage, the ball moves toward the opponent and is returned by the opponent. The robot player moves to the awaiting pose and stays there during this stage. The preparation stage starts when the hitting movement is chosen according to the predicted opponent's target. The arm swings backward to a preparation pose. The robot requires sufficient time to execute a ball-hitting plan in the hitting stage. To achieve the required velocity for returning the ball in the hitting stage, movement initiation to an appropriate preparation pose in the preparation stage is needed, which is often before the opponent hits the ball. The robot player uses different preparation poses for different hitting plans. Hence, it is necessary to choose among them based on modeling the opponent's preference (Wang et al.,
2 Our methods allows the robot to initiate its movement at least 80 ms before the opponent hits the ball. As the ball can usually be reliably predicted more than 120 ms after the opponent returns, the robot could gain more than 200 ms additional execution time by using our prediction method. 3 http://robot-learning.de/Research/ProbabilisticMovementModeling

21

(a) Awaiting Stage

(b) Preparation Stage

(c) Hitting Stage

(d) Finishing Stage

Figure 4: The four stages of a typical table tennis ball rally are shown with the red curve representing the ball trajectories. Blue trajectories depict the typical racket movements of players. The racket of human player is to the left of the table in the pictures. Figures are adapted from (Mülling et al., 2011). 2011a) and inference of the opponent's target location for the ball (Wang et al., 2011b). The robot perceives the ball and the opponent's racket in real-time, using seven Prosilica GE640C cameras. These cameras were synchronized and calibrated to the coordinate system of the robot. The ball tracking system uses four cameras to capture the ball on both courts of the table (Lampert and Peters, 2012). The racket tracking system provides the information of the opponent's racket, i.e., the position and orientation (Wang et al., 2011b). As a result, the observation zt includes the ball's position and velocity as well as the opponent's racket position, velocity, and orientation before the human plays the ball. For the anticipation system described here, we process the observations every 80 ms. Here, the position and velocity of the ball were processed online with an extended Kalman filter, based on a known physical model (Mülling et al., 2011). However, the same smoothing method cannot be applied to the racket's trajectory, as its dynamics are directed by the unknown intended target. Therefore, the obtained states of the racket were subject to substantial noise and the model has to be robust to this noise. The proposed inference method can jointly smooth on the racket's trajectory, given by the smoothing distribution q , and infer the intended target, given by the belief B . In our setting, the robot always chooses its hitting point on a virtual hitting plane, which is 80 cm behind the table, as shown in Fig. 5. We define the human's intended target g as the intersection of the returned ball's trajectory with the robot's virtual hitting plane. As the x-coordinate (see Fig. 5) is most important for choosing among forehand/middle/backhand hitting plans (Wang et al., 2011b), the intention g considered here is the x-coordinate of the hitting point. Physical limitations of the robot restrict the x-coordinate to the range to ±1.2 m from the robot's base (table is 1.52 m

22

Figure 5: The robot's hitting point is the intersection of the coming ball's trajectory and the virtual hitting plane 80 cm behind the table. Figure is adapted from (Mülling et al., 2011). wide). To evaluate the performance of the target prediction, we collected a data set with recorded stroke movements from different human players. The true targets were obtained from the ball tracking system. The data set was divided into a training set of 100 plays and a test set of 126 plays. The standard deviation of the target coordinate in the test set is 102.2 cm. A straightforward approach to prediction is to learn a mapping from the features zt (including the position, orientation, and velocity of the racket and the position and velocity of the ball) to the target g . We compared our method to this baseline Gaussian Process Regression (GPR) using a Gaussian kernel with automatic relevance determination (Rasmussen and Williams, 2006). We considered using a sliding window on the sequence of observations, and conducted model selection to choose the optimal window size. The best accuracy of GPR was achieved when using a sliding window of size two, i.e., the input features consist of zt-1 and zt . The hyperparameters were learned by maximizing the marginal likelihood of training data, following the standard routine (Rasmussen and Williams, 2006). For every recorded play, we compared the performance of the proposed IDDM intention inference and the GPR prediction at 80 ms, 160 ms, 240 ms, and 320 ms before the opponent hits the ball. Note that this time step was only used such that the algorithms could be compared, and that the algorithms were not aware of the hitting time of the opponent in advance. We evaluated both the batch algorithm and online algorithm.

5.2 Results
As demonstrated in Fig. 6, the proposed IDDM model outperformed the GPR baseline. At 80 ms before the opponent hit the ball, the batch algorithm resulted in the mean absolute error of 31.5 cm, which achieved a 11.3% improvement over the GPR, whose average error was 35.6 cm. The online algorithm had a mean absolute error of 32.5 cm,

23

Online 40 Mean absolute error in cm

Batch

GPR

35

30

25

20

320

240 160 time in ms before stroke

80

Figure 6: Mean absolute error of the ball's target with standard error of the mean. The algorithms use the observations obtained before the opponent has hit the ball. which also outperformed GPR by an 8.5% improvement in the accuracy. One modelfree naive intention prediction is to always predict the median of the intentions in the training set. This naive prediction model caused an error of 78.8 cm. Hence, both the GPR and IDDM substantially outperformed naive goal prediction. The online algorithm, with a shrinking factor = 0.2 given in Eq. (51), took on average 70 ms to process every observation, which can potentially fulfill the real-time requirements of 80 ms. The batch algorithm used a sliding window of size 4, and took on average 300 ms to process every observation. The online algorithm was significantly faster than the batch algorithm, with a small loss in accuracy4 . Nevertheless, a certain amount of error is tolerable since the robot can apply small changes to its basic hitting plan based on the ball's trajectory. Therefore, we advocate the use of the online algorithm for applications with tight real-time constraints. We performed model selection to determine the covariance function kz , which can be either an isotropic Gaussian kernel, see Eq. (10), or a linear kernel, see Eq. (11). Furthermore, we performed model selection to find the dimension Dx of the latent states. In the experiments, the model was selected by cross-validation on the training set. The best model under consideration was with a linear kernel and a four dimensional latent state space. Experiments on the test set verified the model selection result, as shown in Table 2.
4 The reason is that the online algorithm only updates the smoothing distribution q (x t-1:t ) instead of the entire smoothing distribution q (x1:T ), and, hence, reduces the time complexity by a factor of T , see Section 3.3 and 4.3

24

Table 2: The mean absolute errors (in cm) with standard error of the mean of the goal inference made 80 ms before the opponent hits the ball, where Dx denotes the dimensionality of the state space. kernel linear Gaussian Dx = 3 41.5 ± 3.0 38.5 ± 2.7 Dx = 4 31.5 ± 2.2 34.2 ± 2.5 Dx = 5 35.4 ± 2.4 34.4 ± 2.7 Dx = 6 37.0 ± 2.6 37.3 ± 2.7

(a) Forehand pose.

(b) Middle pose.

(c) Backhand pose.

Figure 7: Preparation poses of the three pre-defined hitting movements in the prototype system, i.e., (a) forehand, (b) middle, and (c) backhand. The shadowed areas represent the corresponding hitting regions. Our results demonstrated that the IDDM can improve the target prediction in robot table tennis and choose the correct hitting plan. We have developed a proof-of-concept prototype system in which the robot is equipped with three pre-defined hitting movements, i.e., forehand, middle, and backhand movements, with their hitting regions shown in Fig. 7. As exhibited in Fig. 8, our method allows the robot to choose the responding hitting movement before the opponent has hit the ball himself, which is often necessary for the robot to have sufficient time to execute the hitting movement, and substantially expands the robot's overall hitting region to cover almost the entire accessible workspace, see the video. Furthermore, we expect that the method can further enhance the robot's capability when equipped with more and self-improving hitting primitives (Mülling et al., 2013).

6 Action Recognition in Human-Robot Interaction
To realize safe and meaningful human-robot interaction, it is important that robots can recognize the human's action. The advent of robust, marker-less motion capture techniques (Shotton et al., 2011) has provided us with the technology to record the full skeletal configuration of the human during HRI. Nevertheless, recognition of the human's action from this high-dimensional data set poses serious challenges. In this paper, we show that the IDDM has the potential to infer the intention of actions from movements in a simplified scenario. Using a Kinect camera, we recorded the 32-dimensional skeletal configuration of a human during the execution of a set of

25

approx. 320ms 0.5 0.4 posterior 0.3 0.2 0.1 0 backhand middle forehand

approx. 160ms

approx. 80ms (before hit)

-0.2

0.4

X

-0.2

0.4

X

-0.2

0.4

X

Figure 8: Bar plots show the distribution of the target (X coordinate) at approximately 320ms, 160ms, and 80ms before the player hits the ball. The prediction becomes more and more certain as the player finishes the stroke, and the robot later chose the middle hitting movement accordingly.

26

Table 3: Comparison of the accuracy and efficiency using different algorithms for the action recognition task. Here, n denotes the size of sliding windows and is the shrinking factor of the online method. algorithm SVM(n=5) GPC(n=5) batch(n=4) batch(n=5) batch(n=6) online( =0.3) online( =0.2) online( =0.1) accuracy 77.5% 79.4% 79.0% 83.8% 83.0% 83.0% 83.0% 82.6% time(s) <0.01 >1 0.27 0.32 0.39 0.07 0.07 0.07

actions namely: crouching (C), jumping (J), kick-high (KH), kick-low (KL), defense (D), punch-high (PH), punch-low (PL), and turn-kick (TK). For each type of action we collected a training set consisting of ten repetitions and a test set of three repetitions. The system down-sampled the output of Kinect and processes three skeletal configurations per second. In this task, the intention g is a discrete variable and corresponds to the type of action. Action recognition can be regarded as a classification problem. We compared our proposed algorithms to Support Vector Machines (SVMs), see (Schölkopf and Smola, 2001), and multi-class Gaussian Process Classification (GPC), see (Khan et al., 2012). We used off-the-shelf toolboxes, i.e., LIBSVM (Chang and Lin, 2011) and catLGM5 , and followed their standard routines for prediction. The algorithms made a prediction after observing a new skeletal configuration. The batch algorithm used a sliding window of length n = 5, i.e., it recognized actions based on the recent n observations. We chose the IDDM with a linear covariance function for the covariance function kz of the measurement GP and a two-dimensional latent state space. The batch algorithm achieved the precision of 83.8%, which outperformed SVM (77.5%) and GPC (79.4%) using the same sliding windows. The online algorithm achieved the precision of 83.0% with significantly reduced computational time. We observed that both the SVM and GPC confused crouching with jumping, as they were similar in the early and late stages. In contrast, the IDDM could distinguish between crouching (C) and jumping (J) from their different dynamics, which became clearly separable while the human performed the actions. The batch algorithm needs to choose the size of sliding windows, which influences both the accuracy and efficiency. As shown in Table 3, the batch algorithm could yield real-time action recognition in 3 Hz with a sliding window of size 5. The online algorithm, as shown in Table 3, achieved a speedup of over four times compared to the batch algorithm with a sliding window. The online algorithm relies on the shrinking factor in Eq. (51), which describes how likely the type of actions is expected to change. We also found that the performance of online algorithm is not sensitive to this
5 http://www.cs.ubc.ca/~emtiyaz/software/catLGM.html

27

parameter.

7 Discussion
In this article, we have proposed the intention-driven dynamics model (IDDM), a latent-variable model for inferring intentions from observed human movements. We have introduced efficient approximate inference algorithms that allow for real-time inference. Our contributions include: (1) suggesting the IDDM, which simultaneously finds a latent state representation of noisy and high-dimensional observations and models the dynamics that are driven by the intention; (2) introducing an online algorithm to efficiently infer the human's intention from an ongoing movement; (3) verifying the proposed model in two human-robot interaction scenarios. In particular, we have considered target inference in robot table tennis and action recognition for interactive robots. In these two scenarios, we show that modeling the intention-driven dynamics can achieve better predictions than algorithms without modeling the dynamics. The proposed method outperformed the GPR in the robot table tennis scenario and SVM and GPC in the action recognition scenario. Nevertheless, we would not draw the overstated conclusion that IDDM is a better model than SVM or GP based on these empirical results, as this discussion would be a comparison of generative and discriminative models. The performance of IDDM and SVM/GP should be studied on a case-by-case basis. However, two important properties of these approaches should be noticed: (1) computational efficiency and (2) robustness to measurement noise. Firstly, the IDDM is often more computationally demanding than GP and SVM. Nevertheless, the proposed online inference method, and described possible approximations, make the IDDM applicable to real-time scenarios. As demonstrated in the prototype robot table tennis system, the IDDM was successfully used in a real system with tight time constraints. Secondly, the IDDM is generally less prone to measurement noise than SVM/GP, as it models the noise in the generative process of observations. In conclusion, the IDDM takes into account the generative process of movements in which the intention is the driving factor. Hence, we advocate the use of IDDM when the movement is indeed driven by the intention (or target to predict), as the IDDM captures the causal relationship of the intention and the observed movements.

Acknowledgments
Part of the research leading to these results has received funding from the European Community's Seventh Framework Programme under grant agreements no. ICT-270327 (CompLACS) and no. ICT-248273 (GeRT). We thank Abdeslam Boularias for valuable discussions on this work.

References
Abbeel, P. and Ng, A. (2004). Apprenticeship learning via inverse reinforcement learning. In International Conference on Machine Learning. 28

Anderson, R. (1988). A Robot Ping-Pong Player: Experiment in Real-time Intelligent Control. MIT Press. Andrieu, C., De Freitas, N., Doucet, A., and Jordan, M. (2003). An introduction to MCMC for machine learning. Machine Learning, 50(1):5­43. Baker, C., Saxe, R., and Tenenbaum, J. (2009). Action understanding as inverse planning. Cognition, 113(3). Baker, C., Tenenbaum, J., and Saxe, R. (2006). Bayesian models of human action understanding. In Advances in Neural Information Processing Systems. Bandyopadhyay, T., Won, K. S., Frazzoli, E., Hsu, D., Lee, W. S., and Rus, D. (2012). Intention-aware motion planning. In International Workshop on the Algorithmic Foundations of Robotics. Billingsley, J. (1984). Machineroe joins new title fight. Practical Robotics, pages 14­16. Bishop, C. (2006). Pattern Recognition and Machine Learning. Springer New York. Bitzer, S. and Vijayakumar, S. (2009). Latent spaces for dynamic movement primitives. In IEEE-RAS International Conference on Humanoid Robots, pages 574­581. IEEE. Chang, C.-C. and Lin, C.-J. (2011). LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1­27:27. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm. Damianou, A., Titsias, M., and Lawrence, N. (2011). Variational Gaussian process dynamical systems. In Advances in Neural Information Processing Systems. Deisenroth, M. (2010). Efficient Reinforcement Learning using Gaussian Processes. KIT Scientific Publ. Deisenroth, M., Huber, M., and Hanebeck, U. (2009). Analytic moment-based Gaussian process filtering. In International Conference on Machine Learning. Deisenroth, M. and Ohlsson, H. (2011). A general perspective on Gaussian filtering and smoothing. In American Control Conference. Deisenroth, M., Turner, R., Huber, M., Hanebeck, U., and Rasmussen, C. (2012). Robust filtering and smoothing with Gaussian processes. Trans. on Automatic Control. Ding, C. and Peng, H. (2005). Minimum redundancy feature selection from microarray gene expression data. Journal of bioinformatics and computational biology, 3(02):185­205. Fässler, H., Beyer, H., and Wen, J. (1990). A robot ping pong player: optimized mechanics, high perfromance 3d vision, and intelligent sensor control. Robotersysteme, 6(3):161­170.

29

Friesen, A. and Rao, R. (2011). Gaze following as goal inference: A Bayesian model. In Annual Conference of the Cognitive Science Society. Ghahramani, Z. and Roweis, S. (1999). Learning nonlinear dynamical systems using an em algorithm. In Advances in Neural Information Processing Systems. Hauser, K. (2012). Recognition, prediction, and planning for assisted teleoperation of freeform tasks. In Proceedings of Robotics: Science & Systems. Huang, Y., Xu, D., Tan, M., and Su, H. (2013). Adding active learning to LWR for ping-pong playing robot. IEEE Trans. on Control Systems Technology, accepted for publication. Ijspeert, A., Nakanishi, J., and Schaal, S. (2002). Movement imitation with nonlinear dynamical systems in humanoid robots. In IEEE International Conference on Robotics and Automation. Jenkins, O., Serrano, G., and Loper, M. (2007). Interactive human pose and action recognition using dynamical motion primitives. International Journal of Humanoid Robotics, 4(2):365­386. Khan, M., Mohamed, S., Marlin, B., and Murphy, K. (2012). A stick-breaking likelihood for categorical data analysis with latent Gaussian models. In International Conference on Artificial Intelligence and Statistics. Ko, J. and Fox, D. (2009). GP-BayesFilters: Bayesian filtering using Gaussian process prediction and observation models. Autonomous Robots, 27(1):75­90. Ko, J. and Fox, D. (2011). Learning GP-BayesFilters via Gaussian process latent variable models. Autonomous Robots, 30(1):3­23. Kuderer, M., Kretzschmar, H., Sprunk, C., and Burgard, W. (2012). Feature-based prediction of trajectories for socially compliant navigation. In Proceedings of Robotics: Science & Systems. Kurniawati, H., Du, Y., Hsu, D., and Lee, W. (2011). Motion planning under uncertainty for robotic tasks with long time horizons. The International Journal of Robotics Research, 30(3):308­323. Lampert, C. H. and Peters, J. (2012). Real-time detection of colored objects in multiple camera streams with off-the-shelf hardware components. Journal of Real-Time Image Processing. Lawrence, N. (2004). Gaussian process latent variable models for visualization of high dimensional data. In Advances in Neural Information Processing Systems. Lawrence, N. (2005). Probabilistic non-linear principal component analysis with Gaussian process latent variable models. The Journal of Machine Learning Research, 6:1783­1816.

30

Liao, L., Patterson, D., Fox, D., and Kautz, H. (2007). Learning and inferring transportation routines. Artificial Intelligence, 171(5-6). Matsushima, M., Hashimoto, T., Takeuchi, M., and Miyazaki, F. (2005). A learning approach to robotic table tennis. IEEE Trans. on Robotics, 21(4). Møller, M. (1993). A scaled conjugate gradient algorithm for fast supervised learning. Neural networks, 6(4):525­533. Mülling, K., Kober, J., Kroemer, O., and Peters, J. (2013). Learning to select and generalize striking movements in robot table tennis. International Journal of Robotics Research, accepted for publication. Mülling, K., Kober, J., and Peters, J. (2011). A biomimetic approach to robot table tennis. Adaptive Behavior, 19(5):359­376. Pentland, A. and Liu, A. (1999). Modeling and prediction of human behavior. Neural Computation, 11(1). Quiñonero-Candela, J., Girard, A., Larsen, J., and Rasmussen, C. (2003). Propagation of uncertainty in Bayesian kernel models-application to multiple-step ahead forecasting. In IEEE International Conference on Acoustics, Speech, and Signal Processing. Quiñonero-Candela, J. and Rasmussen, C. (2005). A unifying view of sparse approximate Gaussian process regression. The Journal of Machine Learning Research, 6:1939­1959. Ramanantsoa, M. and Durey, A. (1994). Towards a stroke construction model. International Journal of Table Tennis Science, 2:97­114. Rao, R., Shon, A., and Meltzoff, A. (2004). A Bayesian model of imitation in infants and robots. Imitation and Social Learning in Robots, Humans, and Animals. Rasmussen, C. and Williams, C. (2006). Gaussian Processes for Machine Learning. MIT Press. Schölkopf, B. and Smola, A. (2001). Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press. Shotton, J., Fitzgibbon, A., Cook, M., Sharp, T., Finocchio, M., Moore, R., Kipman, A., and Blake, A. (2011). Real-time human pose recognition in parts from single depth images. In IEEE Conference on Computer Vision and Pattern Recognition. Simon, M. (1982). Understanding Human Action: Social Explanation and the Vision of Social Science. State University of New York Press. Snelson, E. and Ghahramani, Z. (2006). Sparse Gaussian processes using pseudoinputs. In Advances in Neural Information Processing Systems.

31

Turner, R., Deisenroth, M., and Rasmussen, C. (2010). State-space inference and learning with Gaussian processes. In International Conference on Artificial Intelligence and Statistics. van der Maaten, L., Postma, E., and van den Herik, J. (2009). Dimensionality reduction: A comparative review. Journal of Machine Learning Research, 10:1­41. Vasquez, D., Fraichard, T., Aycard, O., and Laugier, C. (2008). Intentional motion on-line learning and prediction. Machine Vision and Applications, 19(5):411­425. Vasquez, D., Fraichard, T., and Laugier, C. (2009). Growing hidden Markov models: An incremental tool for learning and predicting human and vehicle motion. The International Journal of Robotics Research, 28(11-12):1486­1506. Wang, J., Fleet, D., and Hertzmann, A. (2008). Gaussian process dynamical models for human motion. IEEE Trans. on Pattern Analysis and Machine Intelligence, 30(2):283­298. Wang, Y., Won, K., Hsu, D., and Lee, W. (2012a). Monte Carlo Bayesian reinforcement learning. In International Conference on Machine Learning. Wang, Z., Boularias, A., Mülling, K., and Peters, J. (2011a). Balancing safety and exploitability in opponent modeling. In AAAI Conference on Artificial Intelligence. Wang, Z., Deisenroth, M., Amor, H., Vogt, D., Schölkopf, B., and Peters, J. (2012b). Probabilistic modeling of human movements for intention inference. In Proceedings of Robotics: Science and Systems. Wang, Z., Lampert, C., Mülling, K., Schölkopf, B., and Peters, J. (2011b). Learning anticipation policies for robot table tennis. In IEEE/RSJ International Conference on Intelligent Robots and Systems. Williams, A., Ward, P., Knowles, J., and Smeeton, N. (2002). Anticipation skill in a real-world task: Measurement, training, and transfer in tennis. Journal of Experimental Psychology, 8(4):259. Yang, P., Xu, D., Wang, H., and Zhang, Z. (2010). Control system design for a 5-DOF table tennis robot. In International Conference on Control Automation Robotics & Vision, pages 1731­1735. Ziebart, B., Dey, A., and Bagnell, J. (2012). Probabilistic pointing target prediction via inverse optimal control. In ACM International Conference on Intelligent User Interfaces, pages 1­10. Ziebart, B., Maas, A., Bagnell, J., and Dey, A. (2008). Maximum entropy inverse reinforcement learning. In AAAI Conference on Artificial Intelligence, pages 1433­ 1438. Ziebart, B., Ratliff, N., Gallagher, G., Mertz, C., Peterson, K., Bagnell, J., Hebert, M., Dey, A., and Srinivasa, S. (2009). Planning-based prediction for pedestrians. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 3931­ 3936. 32

2013 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) November 3-7, 2013. Tokyo, Japan

Learning Responsive Robot Behavior by Imitation
Heni Ben Amor1 , David Vogt2 , Marco Ewerton1 , Erik Berger2 , Bernhard Jung2 , Jan Peters1

Abstract-- In this paper we present a new approach for learning responsive robot behavior by imitation of human interaction partners. Extending previous work on robot imitation learning, that has so far mostly concentrated on learning from demonstrations by a single actor, we simultaneously record the movements of two humans engaged in on-going interaction tasks and learn compact models of the interaction. Extracted interaction models can thereafter be used by a robot to engage in a similar interaction with a human partner. We present two algorithms for deriving interaction models from motion capture data as well as experimental results on a humanoid robot.

I. INTRODUCTION While robots are becoming increasingly better at performing a wide range of motor skills, they are still limited in their human interaction capabilities. To date, most robots are not prepared to appropriately respond to the movements or the behavior of a human partner. However, with application domains of robots coming closer to our everyday life, there is a need for adaptive algorithms that ensure responsive robot behavior for human-robot interaction. We present a new approach to robot learning that allows anthropomorphic robots to learn a library of interaction skills from demonstration. Traditional approaches to modelling interactions assume a pre-specified symbolic representation of the available actions. For example, they model interactions in terms of commands such as wait, pick-up, and place. Instead of such a top-down approach, we want to focus on learning responsive behavior in a bottom-up fashion using a trajectory based approach. The key idea behind our approach is that the observation of human-human collaborations can provide rich information specifying how and when to interact in a particular situation. For example, by observing how two human workmen collaborate on lifting a heavy box, a robot could use machine learning algorithms to extract an interaction model that specifies the states, movements, and situational responses of the involved parties. In turn, such a model can be used by the robot to assist in a similar lifting task. Our approach is as an extension of imitation learning [3] to multi-agent scenarios, in which the behavior and the mutual interplay between two agents is imitated. In this paper, we describe the general multi-agent imitation learning setup for learning interaction models from motion capture data. We also provide two first algorithms that enable a robot to learn such interaction models between interacting
1 Heni Ben Amor, Marco Ewerton and Jan Peters are with the Technische Universitaet Darmstadt, Intelligent Autonomous Systems, Darmstadt, Germany. {amor,ewerton,peters}@ias.tu-darmstadt.de 2 David Vogt, Erik Berger and Bernhard Jung are with the Technische Universitaet Bergakademie Freiberg, Virtual Reality and Multimedia Group, Freiberg, Germany. {david.vogt,bergere,jungb}@tu-freiberg.de

Fig. 1. A humanoid robot receives a book that is handed over by a human interaction partner. The robot learned what to do in this situation by observing a similar situation between two humans.

agents. The first algorithm PPCA-IM (Probabilistic Principal Component Analysis-Interaction Model) frames the task as a missing value estimation problem. The second algorithm called PM-IM (Path Map-Interaction Model) uses a Hidden Markov Model (HMM) [20] to represent the mutual dependencies of the interacting agents. A set of shared latent states is used to map the behavior of one agent to the behavior of the interaction partner. The principal difference between the two algorithms presented in this paper is the representation of the temporal dynamics of interaction. The PPCA-IM uses an implicit representation of time via a temporal embedding of the training data. In contrast, the PM-IM uses an explicit representation of time via a discrete set of hidden nodes. Through a series of experiments, we will show how the two algorithms can be used to create a responsive robot that learns to react to the movements and gestures of humans. We will also provide a comparison of PPCA-IM and PM-IM and discuss the advantages and drawbacks of each approach. II. R ELATED W ORK Finding simple and natural ways of specifying robot control programs is a focal point in robotics. Imitation learning, also known as Programming by Demonstration, has been proposed as a possible solution to this problem [22]. Based on human-provided demonstrations of a specific skill, a robot autonomously generates a control program that allows it to generalize the skill to different situations. Most approaches to imitation learning obtain a control policy which encodes the behavior demonstrated by the user. The policy can subsequently be used to generate a similar behavior that is

978-1-4673-6358-7/13/$31.00 ©2013 IEEE

3257

Fig. 2. Overview of the interaction learning approach presented in this paper. The interaction behavior of two humans is observed, analyzed and imitated in order in human-robot interaction scenarios. Left: The movements of two persons are recorded using motion capture technology. Middle: A compact interaction model specifying the mutual influences and responses is learned. Right: The interaction model enables a robot to compute the best response to the current behavior of a human interaction partner.

adapted to the current situation. For example, the Dynamical Motor Primitive (DMP) [13] approach uses dynamical systems to represent control policies. The DMP approach has been widely accepted in the imitation learning community and has been used to learn various motor skills such as locomotion [16], or drumming [13]. Another way of encoding policies is to use statistical modelling methods. For example, in the Mimesis Model [17] a continuous hidden Markov model is used for encoding the teacher's demonstrations. A similar approach to motion generation is presented by Calinon et al. [7] who used Gaussian Mixture Regression to learn gestures. The advantage of statistical and probabilistic approaches, is the ability to naturally model the spatial and temporal variability of human motion. The methods discussed so far are limited to single agent imitation learning scenarios. Once the behavior is learned, it is executed without taking into account the reaction of an interaction partner. In recent years, various attempts have been undertaken for using machine learning in human-robot interaction scenarios. In [15], a recurrent neural network was used to learn a simple interaction game between a human and a robot. More recently, Wang et al. [24] presented an extension of the Gaussian Process Dynamics Model that was used to infer the intention of a human player during a tabletennis game. Through the analysis of the human player's movement, a robot player was able to determine the position to which the ball will be returned. This predictive ability allowed the robot to initiate its movements even before the human hit the ball. In [14], Gaussian mixture models were used to adapt the timing of a humanoid robot to that of a human partner in close-contact interaction scenarios. The parameters of the interaction model were updated using binary evaluation information obtained from the human. While the approach allowed for human-in-the-loop learning and adaptation, it did not include any imitation of observed interactions. In a similar vein, the work in [17] showed how a robot can be actively involved in learning how to interact with a

human partner. The robot performed a previously learned motion pattern and observed the partner's reaction to it. Learning was realized by recognizing the observed reaction and by encoding the action-reaction patterns in a HMM. The HMM was then used to synthesize similar interactions. In contrast, in our approach, learning of motion and interaction is not split into two parts. Instead, we learn one integrated interaction model which can directly synthesize an appropriate movement in response to an observed movement of the human partner. Further, instead of modelling symbolic action-reaction pairs, our approach is based on modelling the joint dynamics during the execution of a movement. In general, while the learning approaches discussed above are placed within human-robot interaction settings, they only learn from demonstrations by a single actor at a time. In contrast, the work presented here focuses on imitation learning from simultaneously recorded movements by two human interaction partners in order to learn integrated models of the joint interaction. III. L EARNING I NTERACTION M ODELS The goal of learning an interaction model is to derive a compact representation of how two agents behave and, in particular, how they react to each other when they perform a cooperative or competitive task together. The approach followed in this paper derives such a representation from observations of human-human interactions. In Figure 2 we see an overview of this approach. First, the movements of a pair of persons performing a competitive (or cooperative) task are recorded using motion capture technology. Subsequently, an interaction model is learned from the recorded data. The interaction model captures the reciprocal influences during the execution of the task. In turn, the interaction model enables us to predict the state (skeletal configuration) of one human based on the observed states of the second human. Finally, the learned model is used by a robot to engage in a similar interaction with a human partner. In the example depicted in Figure 2, the humanoid robot learns to perfom defensive movements in response to a human performing punching movements.

3258

An interaction model can be regarded as a mapping from the current state of one agent to the state of a second agent. In our particular application, we want to learn a mapping from the state of the opponent agent (i.e. the human) to the state of the controlled agent (i.e. the robot)1 . Input to the learning algorithms are the two data sets A (controlled agent) and B (opponent agent) consisting of joint angle configurations of the two agents. Each point in A contains information about the skeletal configuration of the controlled agent at a particular time step, while B contains a joint angle configurations for the opponent agent. Once a mapping from B to A is learned, it can be used to compute the most appropriate response of the controlled agent, given the observed movements of the opponent agent. In this section, we will present two algorithms that can learn such a mapping. A. Algorithm 1: PPCA-IM The first algorithm that we present is the Probabilistic Principal Component Analysis - Interaction Model. The method exploits the low-dimensional nature of human movement in order to create a compact model of the interaction. It is well known from human motor control, that motor tasks, e.g., grasping [21], walking [9], and also interactions between humans [4] lie on low-dimensional manifolds. Such a manifold typically has a much smaller dimensionality than the total number of joints involved in the motor task. Therefore, instead of finding a mapping in the high-dimensional space involving all joints, we can find a low-dimensional space in which the relationship between the postures and movements can be learned in a more efficient way. After learning is finished, the joint values of the controlled agent are treated as missing values that are estimated by maximizing the likelihood in the low-dimensional latent space. The first step to PPCA-IM is the temporal embedding of the opponent agent's data. Temporal embedding allows us to disambiguate between similar movements by including information from prior time steps. In the second step, namely dimensionality reduction, we then compute a lowdimensional projection of the data set and use this space to estimate the most likely response for the controlled agent. 1) Temporal Embedding: One possible approach to learning an interaction model is to learn a mapping between individual pairs of samples in A and B directly. However, such an approach does not take the temporal offset between action and reaction into account, and is therefore prone to fail for many behaviors. For example, a stretched out arm can either mean that the opponent wants to shake hands or perform a Karate movement. To disambiguate the behavior in such scenarios it is important to take the temporal development of the movement into account. When using PPCA-IM we perform a temporal embedding of the data in
the sake of clarity we will henceforth use the terms controlled agent and opponent agent to refer to the agents involved in an interaction. Note that this naming convention does not restrict the application to competitive tasks only.
1 For

B yielding a new data set B . To each point in B we add joint angles of the  last time steps:   bt ,  .  b (1)   bt  B, t > . t = . . bt- This embedding is comparable to modelling the interaction as a Markov chain of order  , rather than a traditional firstorder Markov chain. A similar preprocessing of the data was proposed in [2]. 2) Dimensionality Reduction: The next step in PPCA-IM is to create a shared latent space that models the interaction dynamics of the two agents. As the name of the algorithm suggests, we use Probabilistic Principal Component Analysis (PPCA) to learn a shared low-dimensional representation of the movements. To this end, we create a combined data set Z which is a concatenation of A and B :
  z = [at , b t ]  at , bt  A, B

(2)

On the new data set Z we can then perform PPCA. PPCA is an iterative version of the original PCA algorithm which uses Expectation-Maximization (EM) [11] to determine the optimal projection matrix C that maps the data set Z onto a lower-dimensional principal component space. The PPCA algorithm used here is based on [19]. An advantage of PPCA over PCA is that it provides a probabilistic framework for performing PCA on data sets that have missing values. In our case, we treat the current joint angles of the controlled agent as missing values that need to be estimated. The EMalgorithm can be used to estimate the missing values of our data. This estimation is done by adding an additional entry z to Z, which consists of an observed part zo and a hidden part zh . The observed part contains (temporally embedded) joint values zo of the opponent agent. The hidden part zh will be estimated during the EM-algorithm and is initially filled with zeros. Before starting the EM algorithm, we initialize the projection matrix C and the variance  2 with random values between zero and one. In the E-step, we first calculate new estimates for the covariance matrix  and matrix Y of projected points: E-Step:  Y  I +  -2 CCT   -2 CZ,
-1

,

Based on these estimates, we can update in the M-step the projection matrix C and the variance  2 : M-Step: C 
2

ZYT (S  + YYT )-1 , 1 S Tr{CT C} + ||zi - CT yi ||2 + Dh  2 ,  SD i=1
S

where S is the number of samples, D is the dimensionality of the samples, and Dh is the total number of missing

3259

200

PC 1

0

-200

-200

0

200

-200

=0

 = 10

PC 2

0

200

-200

0

200

 = 20

Fig. 4. PPCA projections of a defense interaction for different values of  . During the interaction the opponent agent attacks and retracts back to the rest pose while the controlled agent goes to defense stance and then retracts to the rest pose. When  = 0 the temporal context of a posture is not taken into account.

Fig. 3. The projection of high-five motions into a low-dimensional space using Probabilistic Principal Component Analysis. Each point in this space corresponds to specific interaction situation and defines the postures of both agents. Even if we observe the postures of one agent only, we can still infer the most likely posture of the interaction partner using PPCA and missing value estimation.

values in Z. The missing values zh of the matrix Z are re-estimated before performing the next E-Step by first calculating Zestim : Zestim = CT Y, (3)

and then replacing the missing values zh with the newly estimated values from Zestim . The above EM-steps can be iterated until the change in the error of the following objective function is below a given threshold (in our experiments the threshold is 10-5 ):
2 (C,  ) = SD log  2 +  -2 Dh old + S

pose. Figure 4 shows the projected movement for different values of parameter  . When  = 0 (Figure 4 left), the postures for going towards the defensive stance (green) and the postures for retracting back from the defensive stance (red) are mapped onto the same points in the low-dimensional space. As a result a robot cannot distinguish between the two different modes of this particular movement and would produce the same reaction in both situations. We can also see in Figure 4, that with increased value for  the points are more and more disentangled. We can see that  = 20 produces a clear separation between the two modes, i.e. going to and pulling back from the defense stance, of the movement. The trajectory starts at the rest posture in (-250, 0)T , moves to the defense stance at (250, 0)T , and then moves back to a position close to the rest posture. Note, that in this example we used a two-dimensional projection for visualization purposes only. To find a suitable value for the dimensionality of the low-dimensional space, we can use intrinsic dimensionality estimation methods [5]. A simpler approach, which was used in this paper, is to use the number of principal components that insures that 95% of the information in our training data is retained after PPCA. B. Algorithm 2: PM-IM

 -2
i=1 2 old

||zi - CT yi ||2 + Tr CT C

,

where is the previous value for the variance. Once the EM-algorithm is finished, we can use the missing values zh as the new desired joint angles for the controlled agent. Figure 3 shows the low-dimensional projection of a highfive interaction calculated with PPCA. Each point in the low-dimensional space encodes the reaction of the controlled agent with respect to the previous movements of the opponent agent. We can see that the interaction forms a smooth trajectory in the low-dimensional space. To better understand the role of temporal embedding in our learning algorithm, we computed several PPCA projections with different values for the parameter  (from Equation (1)). For this purpose, we recorded a defensive movement in which a human starts in a rest pose, then moves both arms in a defensive stance, and finally goes back to the rest

The second algorithm for learning interaction models is called Path Map-Interaction Model (PM-IM). The algorithm uses a HMM to represent the mutual dependency of the interaction partners at different time steps. A HMM is an efficient tool for modelling probability distributions over time-series'. It assumes that a set of observations was generated by a process with hidden internal states. Given the Markov assumption, any state st only depends on the predecessor state st-1 . Following the notation in [20] and [6], a HMM can be defined as the tuple  = (S , , Pj i , pi (o|si )) where
· ·

S = {s1 , ..., sN } is the set of states si of the HMM  = {1 , ..., N } is a probability distribution specifying the probability i of starting in state i Pj i is the state transition matrix defining the probability p(sj |si )of transitioning from state si to sj

·

3260

a1

a2

a3

S1

S2

S3

...

every situation. An interesting feature of HMMs is the ability to use several HMM models in parallel. For example, assume that we learn two HMMs for different interaction tasks, e.g., punching and handing-over. Given a new observed movement of the attacking agent, we can calculate the likelihood of this movement with respect to each learned HMM and select the model with highest likelihood according to:  = arg max p(bt |).


b1

b2

b3

(4)

Fig. 5. The graphical model of a path map Hidden Markov Model. Each hidden node (white) is connected to two observed nodes (colored) corresponding to each of the interacting agents. Each observed node contains the joint angle configuration of the respective agent which is depicted by a small skeleton. The diagram shows a path map for an punch/defense interaction.

Once the HMM with highest likelihood is selected, we can calculate the emissions for the current situation and use the resulting joint angle values for controlling the robot. The above feature allows us to use the knowledge of several HMMs in order to recognize an interaction scenario and also to respond to the behavior of the human partner. The set of interaction skills can, therefore, be gradually expanded. IV. E XPERIMENTS

·

p(o|si ) is the emission probability distribution which defines the probability of observing output o while in state si . The emission probability distribution is modelled using a Gaussian distribution.

As already mentioned, the nodes of an HMM can be divided into observable nodes and hidden nodes. Typically, an HMM is defined in such a way that each hidden node is connected to one observable node only. In the following, however, we will use an extension of HMM, sometimes also referred to as a path map[6], which has a different graph structure. A path map relates the time-series behavior of a cue system to the behavior a target system. This is achieved by connecting each hidden node to two observables nodes: one observable for the cue system and one observable for the target system. A path map for the task of interaction modelling can be seen in Figure 5. The colored nodes correspond to the observables of the controlled agent (blue) and the opponent agent (red) respectively. Each observable state holds the full joint angle configuration of the respective agent in the current situation. The white nodes depict the hidden states of the interaction task. Each hidden state models a specific context or situation during the interaction. In contrast to the standard HMM, a path map contains two emission probability distributions pA (at |st ) and pB (bt |st ); one for each of the two agents. The training of the path map, however, can be performed using the same approach as for a standard HMM. First, a K-Means[18] clustering algorithm is used to initialize the hidden states of the HMM. Using the EM [11] algorithm, we can then estimate all missing parameters of the HMM. A detailed description of HMM training can be found in [20]. Once it is learned, the path map in Figure 5 allows us to estimate the behavior of one agent by observing the movements of the other. We first calculate the most likely sequence of states given the observed behavior of the opponent agent. Using the emission probability distribution pB (bt |st ) of each state, we can then generate an appropriate response for the controlled agent in

To evaluate the algorithms proposed in this paper, we conducted a set of interaction experiments and analyzed the results. In the following sections, we will report the results achieved by applying the algorithms in simulation as well as on a real robot performing human-robot interaction with a human partner. A. Interaction Data Before training any specific model, we first collected a set of training data representing different competitive and cooperative interaction tasks. Specifically, we collected motion capture data of two interacting humans. The data set consisted of various interaction tasks acquired from the CMU Motion Capture Library2 , as well as additional data gathered using two Kinect cameras and two human subjects. In order to be independent of a specific tracking device, we transformed all motion capture data into the BioVision file format and used the resulting joint angle information as input to the learning algorithms. In total, we used 18 joints, with each joint being parametrized by three joint angles. The final data set consisted of four different interaction tasks: · Boxing: One agent attacks with punches at different heights and from different directions while the other agent defends. · Martial Arts: One agent attacks with punches and kicks while the other defends. · High Five: Both agents perform a high-five movement. · Handing over: One agent hands a book over to the other agent. B. Runtimes In interactive scenarios, a robot needs to quickly respond to the behavior of the human partner. In the following we will, therefore, analyze the computational demands of the proposed algorithms and the runtimes for predicting the appropriate response in the current situation. Figure 6 depicts
2 http://mocap.cs.cmu.edu/

3261

0.03

z-position [m ]

T ime [sec]

0.02

PPCA-IM 20 PPCA-IM 10 PPCA-IM 5

0.4 0.3 0.2

Human
PM-IM 20 cl. PM-IM 40 cl. PM-IM 60 cl.
2.0 1.8 1.6 1.4 1.2 1.0 0 20 40 60 80 100 120

Top Center Bottom

0.01

0.1 0.0 1

0.0 1

T ime step
2 3 4 5
2.0

2

3

4

5

Num. of Behaviors

Num. of Behaviors
z-position [m ]

PPCA-IM
2.0 1.8 1.6 1.4 1.2 1.0 0 20 40 60 80 100 120 0 20 40 1.8 1.6 1.4 1.2 1.0

PM-IM

Fig. 6. The runtime of the PPCA-IM and the PM-IM algorithms. The values indicate the measured time needed for predicting the optimal response of the robot given the human's action at a particular time step. With increasing number of learned behaviors, the time needed to predict the optimal response increases, too. Additionally, the plots also show how the size of the temporal embedding window (20,10,5) affects the runtime of the PPCA. The right plot shows how the number of states/clusters affects the runtime of the PM-IM.

60

80

100

120

T ime step

T ime step

the runtimes of the PPCA-IM and the PM-IM algorithms. For training we used an increasingly complex data set with one to five different behaviors. Each behavior consisted of approx. 120 data samples. The plots show how the number of interactive behaviors affects the response time of the robot when applying an interaction model after learning. As can be seen in Figure 6, PPCA-IM has a significantly faster response time. Especially, with increasing number of clusters/states in the PM-IM, the response times quickly deteriorate. With 60 hidden states, the PM-IM requires about 0.4 seconds to compute a prediction. A smaller number of states can be used to speed up the algorithm. However, this comes at the price of a significantly lower quality of the learned model. In the above example, the PM-IM was only able to produce accurate responses when 55 or more states were used. The PPCA-IM was used with a 7 dimensional latent space. C. Generalization Another important feature of interaction models is the ability to generalize learned behaviors to new situations. To analyze the generalization ability, we conducted a set of experiments in which we trained interaction models for a boxing/defending behavior. The models were trained with high- and low-punches, and were later tested with several other punches that aimed at a position inbetween the trained punches. Figure 7 shows the z-position of the wrist of the controlled agent while trying to defend several punches. The gound truth data gathered from the human clearly shows that the hand needs to be lifted to different levels, in order to defend from the upcoming punch. The trajectories generated with the PPCA-IM model have similar characteristics to the human trajectories. The blue trajectory in Figure 7 corresponds to a movement that was not seen in the training data. Despite that it was not trained, the PPCA-IM was still able to generalize the learned movement to this new situation. Both the shape and height of the trajectory are close to the ground truth of the human demonstrator. In contrast to that, the PM-IM does not exhibit a similar generalization ability. In the depicted case, the PM-IM repeatedly switches between

Fig. 7. The wrist position of the controlled agent for different defenses. The human raises his hand to different heights depending on the type of punch he receives. The defense movement for a center punch was not learned. The PPCA-IM algorithm is still able to generalize to this situation. The PM-IM algorithm does not generalize well in this situation.

states for high-punches and low-punches leading, over time, to oscillations with an increasing amplitude. An interesting property of PPCA-IM is the fact that it automatically produces continuous outputs in every time step. The controlled agent reacts even to small changes in the behavior of the opponent agent. By nature the PM-IM is a discrete model and does not produce different outputs in every time step. Still, a continuous output can be generated by using interpolation (as done in the above examples) or by incorporating velocity information into the model. D. Robot Experiments In order to validate our results on a real robot, we conducted an experiment in which a NAO robot learned a set of interaction skills that can be performed cooperatively or competitively with a human partner. However, in order to replay any of the synthesized movements with the used NAO robot, we first have to find a mapping between the body parts of the human demonstrator and the body parts of the robot. This problem is commonly referred to as the correspondence problem [8] and is a fundamental problem of imitation learning. In this paper, we performed the mapping by using inverse kinematics (IK) on the extremities of the robot. The human skeleton was scaled to the size of the robot and IK was used to ensure that the positions of the feet, hands, pelvis and head of the robot matched the positions of the human extremities. More specifically, we used the iTaSC [10] IK algorithm for fitting the human skeleton to the robot. We have released the software package for IK-based correspondence matching of the NAO robot as an open-source tool for the general public3 . To test our algorithms, we trained interaction models for the martial arts data set. The robot learns to recognize and
3 The software can be downloaded as a Blender -extension from https://bitbucket.org/JuvenileFlippancy/naoblender

3262

Robot

Human

Punch right high Punch right low Kick right low Punch left high Punch left low

Log-likelihood

200

-500

Time

Fig. 8. A martial arts scenario trained and executed with the PM-IM algorithm. Top: The captured movement of the human. Second row: The joint angle configurations generated by the PM-IM after observing the human movement. Third row: The log-likelihoods for the different behaviors. Below: Pictures of the interaction between the human and the robot. The human movement was recorded with a Kinect camera.

defend different types of attacks, e.g., punchrighthigh, punchleftlow, kickrightlow. A set of 12 behaviors was used for training. Depending on the type of attack a different defense behavior is executed. For learning the PM-IM we used 60 to 70 hidden states. The number of hidden states for each behavior was estimated using cross-validation on the training data. For PPCA-IM we used a sampling rate of 10Hz and  = 20.

E. Discussion The results suggest that both PPCA-IM and PM-IM can be used encode and reproduce the joint dynamics of the interaction partners in a shared task. However, the results also show various advantages and shortcomings of these algorithms. The PPCA-IM approach is particularly well suited for modelling continuous reponses and correlations in the movements of the interaction partners. It has limited computational demands and generalizes to some extent to new situations. This shows that dimensionality reduction can be an effective measure for extracting the hidden structure in interaction data. Without dimensionality reduction, the use of the temporal embedding for motion capture data would be computationally prohibitive and the learning would require a significantly larger amount of data. At the same time, PPCA-IM does not provide information about the state or the development of the interaction. In this regard, the HMM-based PM-IM algorithm provides a richer set of tools for recognizing and estimating of the current state of the interaction. Yet, this comes at the price of significantly higher computational demands, as well as limited generalization abilities. Consequently, it would be interesting to combine the approaches presented in this paper by using a HMM with PPCA-IM models as emissions. In such a case, the HMM can be used to realize a spacetime linearization of the training data, while the PPCAIM can take on the role of modelling the correlations in the movements of the interaction partners in a particular temporal context. Recent advances in HMM training [23]

Figure 8 shows the movements of the human and the responses of the NAO robot. Note that the defense posture for an attack with the right hand and attacks with the left hand are different: robot lifts only one arm or both arms for defense. Similarly, the defense stance for a low-kick requires the robot to kneel down and block with one arm. The Figure also shows the log-likelihoods for the different behaviors that are generated by the HMMs. Interestingly the difference in the log-likelihood is very high when the opponent agent executes a kick-right-low movement. The robot can easily disambiguate this case as it is only when of two trained behaviors which use the leg. Both the PM-IM as well as the PPCA-IM model can solve the above task and produce appropriate responses for the NAO robot. The PPCA-IM model was again trained with 7 latent dimensions. Apart from martial arts examples we have also trained interaction models for the other data sets. Figure 1 shows the behavior of the robot after training a handing-over interaction task.

3263

also suggest a closer relationship between dimensionality reduction and temporal models such as HMMs. V. CONCLUSIONS In this paper we presented a new approach for teaching robots how to respond to the movements of a human partner. Using motion capture technology, the movements of a pair of persons are first recorded, and then processed using machine learning algorithms. The result is a model of how each person adapted its behavior to the movements of the respective other. Once an interaction model is learned, it can be used by a robot to engage in a similar task with a human counter part. We have also provided two algorithms called PPCA-IM and PM-IM, that are extensions to known methods, which can be used for learning interaction models from motion capture data. The algorithms allow a robot to learn when and how to respond to the behavior of a human partner. All methods were implemented on a NAO humanoid robot and were evaluated in cooperative and competitive tasks. After learning an interaction model, the NAO robot was able to generate appropriate defense responses in a challenging martial arts scenario. The discussion of the advantages and shortcomings of each of the two algorithms suggests that a combination of temporal models and dimensionality reduction can be an interesting path for developing more sophisticated models of interactions. While the results in this paper are encouraging, there are various aspects of imitation learning in multi-agent scenarios that need further investigation. In particular, it is interesting to investigate how learned models can be used to predict the future behavior of an interaction partner given the actions of the controlled agents. This can be helpful in avoiding decisions that potentially lead to dangerous situations or injuries. In this paper, we did not investigate the aspect of force transfer between a human and a robot. Even small forces that are exchanged between interaction partners can have a significant impact on the execution and success of a joint task. First research results on incorporating force transfer in interaction models can be found in [1]. Another aspect that needs further investigation is task space control. For some interaction tasks it is important that constraints are fulfilled within the task space. We are currently investigating the use of Interaction Meshes [12] for this purpose. Finally, it is also important to include tertiary objects, e.g., a jointly lifted box, into the interaction model. VI. ACKNOWLEDGMENT The work presented in this paper is funded through the CoDyCo project of the European Community's Seventh Framework Programme under the grant agreement n ICT600716 (CoDyCo). R EFERENCES
[1] E. Berger, H. Ben Amor, N. Haji-Ghassemi, D. Vogt, and B. Jung. Inferring guidance information in cooperative human-robot tasks. In IEEE-RAS International Conference on Humanoid Robots (HUMANOIDS). IEEE, 2013 (submitted).

[2] F. Biessmann, F. C. Meinecke, A. Gretton, A. Rauch, G. Rainer, N. Logothetis, and K.-R. M¨ uller. Temporal kernel canonical correlation analysis and its application in multimodal neuronal data analysis. Machine Learning, 79(1-2), 2009. [3] A. Billard, S. Calinon, R. Dillmann, and S. Schaal. Survey: Robot Programming by Demonstration. In Handbook of Robotics, volume chapter 59. MIT Press, 2008. [4] D.P. Black, M.A. Riley, and C.K. McCord. Synergies in intraand interpersonal interlimb rhythmic coordination. Motor Control, 11(4):348­73, 2007. [5] C. Bouveyron, G. Celeux, and G. Stphane. Intrinsic dimension estimation by maximum likelihood in isotropic probabilistic {PCA}. Pattern Recognition Letters, 32(14):1706 ­ 1713, 2011. [6] M. Brand and A. Hertzmann. Style machines. In Proceedings of the 27th annual conference on Computer graphics and interactive techniques, SIGGRAPH '00, pages 183­192, New York, NY, USA, 2000. ACM Press/Addison-Wesley Publishing Co. [7] S. Calinon, E.L. Sauser, A.G. Billard, and D.G. Caldwell. Evaluation of a probabilistic approach to learn and reproduce gestures by imitation. In Proc. IEEE Intl Conf. on Robotics and Automation (ICRA), pages 2381­2388, Anchorage, Alaska, USA, May 2010. [8] K. Dautenhahn and C. L. Nehaniv. Imitation in Animals and Artifacts. MIT Press, Campridge, 2002. [9] A. d'Avella, P. Saltiel, and E. Bizzi. Combinations of muscle synergies in the construction of a natural motor behavior. Nat Neurosci, 6(3):300­8, 2003. [10] J. De Schutter, T. De Laet, J. Rutgeerts, W. Decr´ e, R. Smits, E. Aertbeli¨ en, K. Claes, and H. Bruyninckx. Constraint-based task specification and estimation for sensor-based robot systems in the presence of geometric uncertainty. Int. J. Rob. Res., 26(5):433­455, May 2007. [11] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1­38, 1977. [12] E. Ho, T. Komura, and C. Tai. Spatial relationship preserving character motion adaptation. ACM Transactions on Graphics, 29(4):1­8, 2010. [13] A. Ijspeert, J. Nakanishi, and S. Schaal. Learning attractor landscapes for learning motor primitives. In Suzanna Becker, Sebastian Thrun, and Klaus Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 1523­1530. MIT Press, 2002. [14] S. Ikemoto, H Ben Amor, T. Minato, B. Jung, and H. Ishiguro. Physical human-robot interaction: Mutual learning and adaptation. IEEE Robotics and Automation Magazine, 19(4):24­35, Dec. [15] M. Ito and J. Tani. On-line imitative interaction with a humanoid robot using a dynamic neural network model of a mirror system. Adaptive Behavior, 12(2):93­115, 2004. [16] Z. Kolter, P. Abbeel, and A. Ng. Hierarchical Apprenticeship Learning with Application to Quadruped Locomotion. In John C. Platt, Daphne Koller, Yoram Singer, and Sam T. Roweis, editors, Advances in Neural Information Processing Systems (NIPS). MIT Press, 2007. [17] D. Lee, C. Ott, and Y. Nakamura. Mimetic communication model with compliant physical contact in human-humanoid interaction. Int. Journal of Robotics Research., 29(13):1684­1704, November 2010. [18] J. B. MacQueen. Some methods for classification and analysis of multivariate observations. In L. M. Le Cam and J. Neyman, editors, Proc. of the fifth Berkeley Symposium on Mathematical Statistics and Probability, volume 1, pages 281­297. University of California Press, 1967. [19] J. Porta, J. Verbeek, and B. Krose. Active appearance-based robot localization using stereo vision. Autonomous Robots, 18(1):59­80, 2005. [20] L. Rabiner. A tutorial on HMM and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257­286, February 1989. [21] M. Santello, M. Flanders, and J. F. Soechting. Postural Hand Synergies for Tool Use. The Journal of Neuroscience, 18(23):10105­10115, December 1998. [22] S. Schaal. Is imitation learning the route to humanoid robots? Trends in Cognitive Sciences, 3:233­242, 1999. [23] L. Song, B. Boots, S. M. Siddiqi, G. J. Gordon, and A. J. Smola. Hilbert space embeddings of hidden Markov models. In Proc. 27th Intl. Conf. on Machine Learning (ICML), 2010. [24] Z. Wang, M. Deisenroth, H. Ben Amor, D. Vogt, B. Schoelkopf, and J Peters. Probabilistic modeling of human dynamics for intention inference. In Proceedings of Robotics: Science and Systems (R:SS), 2012.

3264

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/234883726

PhysicalHuman-RobotInteraction:Mutual LearningandAdaptation
ArticleinIEEERobotics&AutomationMagazine·February2012
DOI:10.1109/MRA.2011.2181676

CITATIONS

READS

26
5authors,including: ShuheiIkemoto OsakaUniversity
38PUBLICATIONS171CITATIONS
SEEPROFILE

278

HiroshiIshiguro OsakaUniversity
354PUBLICATIONS5,174CITATIONS
SEEPROFILE

Someoftheauthorsofthispublicationarealsoworkingontheserelatedprojects:

AndroidprojectViewproject

ERATOIshiguroSymbioticHuman-RobotInteractionProjectViewproject

AllcontentfollowingthispagewasuploadedbyHeniBenAmoron13April2014.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Physical Human­Robot Interaction
Mutual Learning and Adaptation
By Shuhei Ikemoto, Heni Ben Amor, Takashi Minato, Bernhard Jung, and Hiroshi Ishiguro

·

lose physical interaction between robots and humans is a particularly challenging aspect of robot development. For successful interaction and cooperation, the robot must have the ability to adapt its behavior to the human counterpart. Based on our earlier work, we present and evaluate a computationally efficient machine learning algorithm that is well suited for such close-contact interaction scenarios. We show that this algorithm helps to improve the quality of the interaction between a robot and a human
Digital Object Identifier 10.1109/MRA.2011.2181676 Date of publication: 29 February 2012

C

caregiver. To this end, we present two human-in-the-loop learning scenarios that are inspired by human parenting behavior, namely, an assisted standing-up task and an assisted walking task. Human­Robot Interaction and Cooperation Until recently, robotic systems mostly remained in the realm of industrial applications and academic research. However, in recent years, robotics technology has significantly matured and produced highly realistic android robots. As a result of this ongoing process, the application domains of robots have slowly expanded into domestic environments, offices, and other human-inhabited locations. In turn, interaction and
MARCH 2012

1070-9932/12/$31.00ª2012 IEEE

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

1

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

cooperation between humans and robots has become an increasingly important and, at the same time, challenging aspect of robot development. Particularly challenging is the physical interaction and cooperation between humans and robots. For such interaction to be successful and meaningful, the following technical difficulties need to be addressed: l guaranteeing safety at all times l ensuring that the robot reacts appropriately to the force applied by the human interaction partner l improving the behavior of the robot using a machinelearning algorithm in a physical human­robot interaction (PHRI). In our previous research [1], we presented a PHRI scenario in which we addressed the above topics. Inspired by the parenting behavior observed in humans, a test subject was asked to physically assist a state-of-the-art robot in a standing-up task. In such a situation, both the human and the robot are required to adapt their behaviors to cooperatively complete the task. However, most machine learning scenarios to date do not address the question of how learning can be achieved for tightly coupled, physical interactions between a learning agent and a human partner. Building on the results in [2], we present an extended evaluation and discussion of such human-in-the-loop learning scenarios. To realize learning and adaptation on the robot's side, we employ a computationally efficient learning algorithm based on a dimensional reduction technique. In particular, after each trial, the human can judge whether the interaction was successful, then the judgment is used in a machine learning algorithm to apply a dimensional reduction technique and update the behavior of the robot. As learning progresses, the robot creates a behavioral model, which implicitly includes the actions of the human counterpart. At the same time, refining the motions of the robot during a physical interaction requires the motions of the human to be improved, because the two motions influence each other. Hence, the human counterpart is part of the learning system and overall dynamics. To analyze the efficiency of the proposed learning algorithm and the effect of human habituation to the robot during such close-contact interactions, we perform a set of PHRI experiments. In addition to the assisted standing-up interaction scenario presented in [2], we also present and discuss the first results based on a novel interaction scenario. More specifically, we present an assisted walking task in which a human caregiver must assist a humanoid robot while walking. We believe that human-in-the-loop learning scenarios, such as that presented herein, will be particularly interesting in the future because they can help to strengthen the mutual relationship between humans and robots. Ideally, this will lead to a higher acceptance of robotic agents in society. Related Research Important aspects of PHRIs have been investigated in a perspective research project conducted by the European
2

Network of Excellence (EURON) [3]. The objective of the project was to present and discuss important requirements for safe and dependable robots involved in PHRIs. Initial approaches for achieving these requirements are currently being addressed in a follow-up research project called PHRIENDS (a PHRI that is dependable and safe). To reduce risks and fatalities in industrial manufacturing workplaces, the primary goal of the PHRIENDS project is to design robots that are intrinsically safe. This requires the development of new actuator concepts, safety measures, and control algorithms, which take the presence of human subjects into account. The results of this project are also relevant to applications outside the manufacturing industry. However, learning and adaptation between humans and robots is not the focus of the PHRIENDS project. Khatib et al. [4] discussed the basic capabilities needed to enable robots to operate in human-populated environments. In particular, they discussed how mobile robots can calculate collision-free paths and manipulate surrounding objects. In their approach, they characterized free space using an elastic strip approach. However, the described robots were not expected to come into direct (physical) contact with the surrounding human subjects. The importance of direct physical interaction was highlighted in the haptic creature project [5], which investigated the role of affective touch in fostering the companionship between humans and robots. In an attempt to improve human­robot interaction, Kosuge et al. presented a robot that can dance with a human by adaptively changing the dance steps according to the force/moment applied to the robot [6]. Amor et al. [7] used kinesthetic interactions to teach new behaviors to a small humanoid robot. Furthermore, the behavior of the robot may be optimized with respect to a given criterion in simulation. In this learning scheme, the robot is a purely passive interaction partner and acts only after the learning process is complete. Similar approaches to teaching new skills have also been reported in [8] and [9] using different learning methods, i.e., continuous time-recurrent neural networks and Gaussian mixture models (GMMs), respectively. Odashima et al. [10] developed a robot that can come into direct physical contact with humans. This robot is intended for caregiving tasks such as carrying injured persons to a nearby physician. The robot can also learn new behaviors and assistive tasks by observing human experts as they perform these tasks. However, this learning does not take place during interactions but rather in offline sessions using immersive virtual environments. In [11], Evrard et al. present a humanoid robot with the ability to perform a collaborative manipulation task together with a human operator. In a teaching phase, the robot is first teleoperated using a force-feedback device. The recorded forces and positions are then used to learn a controller for the collaborative task. The main hypothesis underlying this approach is that the intentions of the human interaction partner can

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

MARCH 2012

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

be guessed from haptic cues. In [12], physical interactions between a robot's hand and the hand of a human are modeled by recording their distances. The distances are then encoded in a hidden Markov model (HMM), which in turn is used to synthesize similar hand contacts. A recent survey on modern approaches to physical and tactile human­robot interaction can be found in [13]. In this article, we present experiments with a flexiblejoint robot that is involved in close physical interaction with a human caregiver. In contrast to the above research, both human and robot play an active role in the interaction to learn and adapt their behaviors to their partner so as to achieve a common goal. This tight coupling of robot and human learning and coadaptation is a unique feature and is the primary contribution of the present study. We assume that it is important to focus on the active role in the interaction because the forces generated during the active behavior of the robot influence the behavior of the human, which in turn influences the passive behavior of the robot. In addition, these active and passive roles cannot be clearly separated because the robot and the human influence each other when they are in physical contact. Physical Interaction Learning Approach The goal of interaction learning is to improve the cooperation of humans and robots while they are working to achieve a common goal. Figure 1 shows an overview of the learning scheme used in this article. After an initial physical interaction between a human and a robot, the human is given the chance to evaluate the behavior of the robot. More precisely, the human can judge whether the interaction was a success or failure (binary evaluation). The feedback can be provided in various ways, such as through touch or through a simple graphical user interface. Once the evaluation information is collected by the robot system, it is stored in a database in the memory. The memory collects information about recent successful interactions and manages the data for the subsequent learning step. This allows us to optimize the set of training examples used for learning to improve learning quality. Figure 1 shows the human-in-the-loop learning system considered in this article, where the behavior of the human influences the behavior of the robot and, simultaneously, the behavior of the robot influences the behavior of the human. Furthermore, the behavior of the robot changes as learning progresses, which in turn influences the behavior of the human and its physical support. This system demonstrates one of the applications of a tightly coupled physical interaction. After a number of interactions, the learning system queries the memory for a new set of training data. The data are then projected onto a low-dimensional manifold using dimensional reduction techniques. There are three justifications for this step. First, dimensional reduction allows a reduction of the space in which learning takes place. Thus, the learning can be much faster and more efficient. In addition, dimensional reduction generally helps to detect

Learning

Memory

DR

Critique

GMM

Physical Interaction (a)

(b)
Figure 1. (a) Overview of the physical interaction learning approach. After physical interaction, the human judges whether the interaction was successful. This information is stored in the robot's memory and used for later learning. (b) Flexible-joint humanoid robot used in the experiments in this study. (Photos courtesy of ERATO Asada Project.)

meaningful low-dimensional structures in high-dimensional inputs. Second, dimensional reduction allows us to visualize and understand the adaptation taking place during interaction. This is particularly helpful for later review and analysis purposes. Finally, dimensional reduction reduces the negative influence of outliers on learning. The inputs to the dimensional reduction step are high-dimensional state vectors describing the postures of the robot during the interaction. The output is a low-dimensional posture space. Once the state vectors are projected onto a low-dimensional manifold, we group the resulting points into sets according to the action performed in that state. Thus, we obtain for each possible action a set of states in which the corresponding action should be triggered. For each action, a GMM is learned. The model encodes a probability density function of the learned state vectors. The ideal number of Gaussian mixtures is estimated using the Bayesian information criterion (BIC) [14]. By computing the likelihood of a given state vector p in a GMM of action A, we can estimate how likely it is
MARCH 2012

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

3

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

that the robot should perform action A when in posture p. The learned models are then used during the next physical interaction trial to determine the actions of the robot. Here, each new posture is projected into the lowdimensional posture space. Then, the likelihood of the projected point for each GMM is computed. Following a maximum-likelihood rationale, the action corresponding to the GMM with the highest likelihood is executed by the robot. With each iteration of the above learning loop, the robot adapts its model more and more toward successful interactions. The result is a smoother and easier cooperative behavior between the human and the robot. The CB2 Robot The robot used in this study is called the child­robot with biomimetic body, or CB2 [15]. The robot has the following features. l Its height is 130 cm, and its mass is approximately 33 kg. l The degree of freedom (DOF) is 56. l The supplied air pressure is 0.6 MPa. l The efficient torque of the knee is theoretically 28.6 NÆm. l All joints, apart from the joints used to move the eyes and eyelids, are driven by pneumatic actuators. l All joints, apart from the joints used to move the fingers, are equipped with potentiometers. The joints have low mechanical impedance because of the compressibility of air. The joints can also be made to be completely passive if the system discontinues air

Behavior System of CB2 Switching Mechanism x *(i ) (i = 1, 2, . . . , n) Desired Posture
  x 2, . x  {x 1,

. ., x  k}

Control System of CB2 d dt ­ d dt ­ P . . . + ­ d dt P D + Joint k + P D D + Joint 1 + + Joint 2 +

x1



+

 x2

+

compression during robot motion. This helps the robot to perform passive motions during physical interaction and helps to ensure the safety of the human partner. This is in contrast to most other robots, in which the joints are driven by electric motors with decelerators. The flexible actuators enable the joints to produce seemingly smooth motions, even when the input signal changes drastically. This feature of the CB2 robot is used to realize complex motions using the simple control architecture [1] depicted in Figure 2. More specifically, full body motions of the robot are realized by switching between a set of successive desired postures. Furthermore, the flexible actuators enable motions generated by this simple control architecture to be adaptively changed in response to an applied force from the human partner. Each posture is described by a posture vector x, with each entry of the vector denoting the angular value of a particular joint. A low-level controller is implemented by the proportional-integral-differential (PID) control of angular values. Each time the desired posture is switched drastically, large drive torques are generated, resulting in an active force being applied to the human caregiver. As the posture of the robot approaches the desired posture, the passive motion gradually becomes the dominant motion of the robot because the amount of error in the angular control gradually becomes smaller. Figure 3 shows how the examined standing-up task is realized using the proposed control architecture. The behavior is realized by switching between three desired postures. At first glance, the specifications of the robot motion appear to be extremely simple. However, the switching times are highly dependent on the human interaction. More specifically, the switching times depend on the anatomy and Current Posture skills of the human. This means that x {x 1, x 2, . . ., x k} the robot has to adapt the switching times to the characteristics of its partner during the period of interacx1 tion. In addition, it must be noted that this motion cannot be performed by the robot if a human does not assist in its execution.
x2

 xk

xk

Figure 2. Control architecture of the CB2 robot. The desired posture is encoded as a vector xÃ of angular values. Using a PID controller, drive torques are generated to attain the desired posture. The switching mechanism changes between a set of different desired postures to achieve complex robot motions.

Learning Method In the standing-up task, the goal of learning is to determine the ideal timing for switching actions between different xÃ 2 X Ã  X desired postures. Here, xÃ is a desired posture, X Ã is a set of desired postures prepared for control, and X is a posture space that is constructed from all joint angles. This is achieved by learning three different probabilistic lowdimensional posture models: 1) for

4

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

MARCH 2012

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Switching 1 When the Hands Are Pulled up by the Human

Switching 2 When the Legs Are Bent More Than in Posture 2 Posture 2 Posture 3

Posture 1

Figure 3. The three desired postures used in the standing-up task of the experiment. The learning task is to determine the ideal switching conditions between the desired postures. (Photo courtesy of ERATO Asada Project.)

the case in which no switching occurs, 2) for the first switching action, and 3) for the second switching action. At each time step of an interaction between the human and the robot, the realized posture and the current desired posture of the robot are recorded. The robot posture r is a 52-dimensional vector that codes the current angular value of each joint. After the interaction is complete, the postures are stored in a database in the memory. The database holds the information for the last ten interactions. Although there are several possible ways to integrate this new data into the database, the general policy used here is this new data overwrite old data, and successful interactions overwrite failed interactions. After ten interactions, the training data from the memory are used for learning. The goal of the learning is to construct a model that indicates when the robot should switch actions by changing the current desired posture. This rule is described by a mapping from the current posture of the robot to the desired posture that the robot should use. To realize this map, we use a GMM that can construct a probabilistic model. Therefore, the objective model of the learning is a probabilistic model that indicates the likelihood of desired postures in the current state. First, dimensional reduction is applied to the data because a 52-dimensional vector has too many dimensions to learn the model. Although a number of methods can be applied for this task, in this article, we used a principal component analysis (PCA). To perform the PCA, the mean rm is subtracted from all recorded posture vectors, and the covariance matrix M of the resulting points is computed. A singular value decomposition (SVD) on M yields matrices U , V , and W , such that M ¼ UWV T : (1)

(PCs), of matrix M . The matrix W is a diagonal matrix containing singular values. Each PC has a corresponding singular value that indicates how much information of the data set is covered by a specific PC. The first few PCs are then used as the axes of the lower-dimensional PCA space. Given a new data point, we can compute its coordinates in PCA space by subtracting the mean and calculating the dot product for each of the PCs. Next, we compute a GMM for each of the three switching classes. Here, we divide the projected data points into distinct sets. If no switching occurred, then the corresponding point is assigned to the first data set. Otherwise, the corresponding point is assigned to one of the other two sets. For each set of projected points, we learn a probability density function by a weighted sum of K Gaussian distributions: p(x) ¼
K X k¼1

pk p(xjk),

(2)

with pk being the weight of the kth Gaussian and p(xjk) being the conditional density function. The conditional density function is a d-dimensional Gaussian distribution:
T À1 1 1 p(xjk) ¼ pffiffiffiffiffid pffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi eÀ2ð(xÀlk ) Ck (xÀlk )Þ , 2p det(Ck )

(3)

The columns of matrix V contain orthonormal vectors, also known as the eigenvectors or principal components

with mean lk and covariance matrix Ck . The above p(xjk) can also be written as N (xjlk , Ck ). The expectation-maximization (EM) [16] algorithm is used to estimate the parameters {lk , Ck , pk } for each of the Gaussian kernels. Fortunately, performing the EM algorithm in low-dimensional spaces improves the convergence of the algorithm. After the learning process, we end up with three GMMs coding three probability density functions,
MARCH 2012

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

5

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

namely, p1 (x), p2 (x), and p3 (x). In our experiments, each GMM had between five and ten Gaussians. Each probability density function can be used to determine the probability of a point in a low-dimensional posture space with respect to a particular switching action. For example, computing p2 (r ) for a given projected robot posture, r , returns the likelihood of the robot having to switch from the second to the third desired posture when the robot is in state r . When the next interaction with the human starts, the robot can use the newly learned model to decide its current state and the desired posture. Here, the current joint values are projected onto the learned low-dimensional posture space. The result is a d -dimensional point. The optimal desired subsequent switching action can be computed in a maximum-likelihood fashion as follows:
Ã ¼ argmax ps (x): xnext xÃ 2X Ã

(4)

In each step of the control loop, the robot calculates snext and sends the angular values of the corresponding desired posture to a low-level controller. The controller then computes the needed joint torques to take on this posture. After the interaction is complete, the human evaluation information is collected and used to update the memory. The learning loop is then repeated. The above algorithm is closely related to HMMs [17]. At the same time, however, our algorithm deviates in various ways from HMM. More specifically, we do not learn the sequencing of states in our system. As a result, no explicit transition probabilities between the states are modeled. Figure 4 shows an example of a set of interactions projected onto a low-dimensional space. Each point in the plot represents one posture of the CB2 robot during an interaction. The points were colored according to the desired posture that was active during that particular time step.

Initial Desired Posture Desired Posture 1 Desired Posture 2

First Principal Component
Figure 4. Interaction data for the standing-up task projected into a low-dimensional posture space. Each point corresponds to one posture of the robot.

Experiment and Results To investigate tightly coupled adaptation and the learning scheme proposed in this article, we conducted a PHRI experiment using the interaction for the standing-up task introduced earlier. In particular, we considered the following question: "Does the learning algorithm lead to a symmetric learning process, in which both human and robot adapt their behaviors?" Furthermore, we wanted to measure the contribution of the learning algorithm to any improvement in the interaction. This required a careful experiment design that would allow us to distinguish between learning-based adaptation and adaptation due to human habituation to the robot. The experiment was split into three independent parts. Throughout the experiment, five subjects were asked to repeatedly assist the robot in standing up. In the first part, after every ten trials, the accumulated data in the memory were used for learning a new model, according to the learning scheme described in the "Physical Interaction Learning Approach: Learning Method" section. In total, 30 interactions with two intermediate learning steps were performed. In the second part of the experiment, learning by the robot was disabled and fixed time steps were used for switching between the postures. In this baseline scenario, the only type of adaptation that was possible was the adaptation of the human to the robot. In the third and final part, learning was once again enabled (the results of the first part were not included; hence, learning started from the beginning again). The experimental design ensures that we have baseline data, allowing us to compare the results of the interactions with and without learning. In addition, by performing the baseline experiment between the learning experiments, we ensure that the user is already familiar with the robot. Thus, we rule out any distortion of the baseline result because of unfamiliarity. To determine the ideal number of PCs on which to project the 52-dimensional posture vector of the robot, intrinsic dimensionality estimation techniques can be used [18] as a criterion. A simple estimation technique is based on the analysis of eigenvalues, which store the amount of information that is captured by each of the PCs. Hence, the eigenvalues determine how many PCs are needed to retain a specific percentage of information found in the data set. In our implementation, we automatically determine the number of PCs that capture more than 85% of the information in the data set. For our standing-up data set, this resulted in a projection onto two PCs. Figure 5 shows sequential photographs of the interactions of two test subjects. Figure 5(a) shows the initial interaction, whereas Figure 5(b) shows the interaction after learning. The white dashed line indicates the height of the hips in each snapshot. In the figures, we can observe a smoother transition of the hip height after the learning interaction, when compared with that before the learning interaction. In particular, the center photographs reveal strong contact between the feet and the ground and an

6

·

Second Principal Component

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

MARCH 2012

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

increased hip height after learning, in contrast to the poor contact with the ungainly leg posture beforehand. Since the degree to which the human helped the robot in the task and the evaluation of the robot performance are somewhat subjective, in our evaluation, we focus only on whether the robot motion is refined to the degree that inefficient and jerky motions are avoided. Figure 6 shows the interaction trajectories for two users before and after learning. Each trajectory was computed by projecting the robot postures into the low-dimensional posture space. Before learning, the trajectories contain loops and are partially linear. These linear pieces of the trajectories are due to jerky movements and large changes in the robot postures. In particular, for the first user, the variance in the trajectory decreases after learning. The trajectories become more similar and take on a V-shaped form. This can be explained by the fact that the interaction consists of three desired postures. Therefore, in successful trials, the interaction leads the robot from a starting posture to an intermediate posture and then to a final posture, as shown in Figure 3. In a low-dimensional space, the result is a V-shaped or triangular-shaped trajectory. This allows us to qualitatively evaluate the efficiency and naturalness of the interaction by analyzing the smoothness and shape of the low-dimensional trajectories. For example, in the case of the second subject, the trajectories before learning contain large loops at the point (1:7, À 1:5)T , which is the lowdimensional coordinate of the second desired posture. This phenomenon can easily be explained if we take into account our previous analysis. In the initial trials, the robot has poor contact with the floor and the legs are often not symmetrically arranged when reaching the second desired posture. As a result, lifting the robot becomes more difficult for the human and involves slight modifications of the

robot posture to make the feet more stable. This interrupts the flow of the standing-up task and increases the interaction burden for the human caregiver. To confirm the above discussion, we quantified the robot motion using the posture change norm. The posture change norm a of the robot motion was calculated using the Euclidean distance between the data of t and t À 1 in the posture space X defined using each joint angle as a base: a(t) ¼ jjx(t ) À x(t À1) jj2 , x 2 X : (5)

Computing the posture change norm at each time step of the interaction results in the time series depicted in Figure 7. The solid line shows the posture change norm during the initial interaction phase. We can see a sudden peak indicating a large change in the robot posture and, consequently, a nonsmooth motion. This is undesirable because large changes in the robot posture result from strong forces acting on the robot. The other lines show the evolution of the norm after each learning step. With each learning step, the number of peaks in the time series is reduced. In other words, the fluctuations in the posture change norm decrease, leading to a smoother and more efficient motion. A statistical analysis of the data further underlines the above hypothesis. Here, we computed the mean and standard deviation of the summation of the posture change norm during the interactions. Figure 8 shows the evolution of these values with each learning step. For all subjects, we see that the mean and standard deviation of the posture change norm decreased as the experiment progressed. In the baseline experiment, only one subject was able to significantly improve the interactions, where statistical significance is computed using a t test. None of the other subjects

(a)

(b)
Figure 5. Sequential photographs of the (a) first and (b) last interactions of the test subjects with the robot. The white curve depicts the change in position of the robot's hips. The center photograph of each sequence shows how the robot learns to maintain firm contact between its feet and the ground for both subjects. (Photos courtesy of ERATO Asada Project.)

MARCH 2012

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

7

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Before Training 2.0 Principal Component 2 1.5 1.0 0.5 0.0 ­0.5 ­1.0 ­0.5 0.0 0.5 1.0 1.5 2.0 Principal Component 1 (a) Before Training 1.5 Principal Component 2 Principal Component 2 1.0 0.5 0.0 ­0.5 ­1.0 ­1.5 ­2.0 ­1.0 ­0.5 0.0 0.5 1.0 1.5 2.0 Principal Component 1 (c) 2.5 3.0 1.5 1.0 0.5 0.0 ­0.5 ­1.0 ­1.5 2.5 3.0 Principal Component 2 2.0 1.5 1.0 0.5 0.0 ­0.5

After Training

­1.0 ­0.5 0.0 0.5 1.0 1.5 2.0 Principal Component 1 (b) After Training

2.5

3.0

­2.0 ­1.0 ­0.5 0.0 0.5 1.0 1.5 2.0 Principal Component 1 (d)

2.5

3.0

Figure 6. Projected interactions in the low-dimensional posture space: (a) and (b) the interaction trajectories for the first subject before and after learning and (c) and (d) the interaction trajectories for the second subject. In both the cases, the trajectories become smoother after learning and sudden jumps and knots are reduced. Furthermore, the trajectories become V-shaped, clearly indicating a smooth transition between the three desired postures.

were able to improve their interactions. In the first experiment, in which the proposed learning system is used, three subjects show significant improvement. Finally, in the second learning experiment, all of the subjects showed significant improvement in their interactions. This indicates that while a human can adapt to a robot and thus improve their interactions (as in the baseline experiment), this adaptation can be significantly improved by empowering the robot with learning capabilities (first and second learning experiments). We also analyzed the maximum values of the posture change norm during the interaction. Figure 9 shows the change in the maximum posture change norm during each learning phase of the baseline experiment and the first learning experiment. No significant difference in the maximum posture change norm is observed in the baseline experiment. On the other hand, in the learning experiment, there are large changes in the maximum posture change norm. For all subjects, the values drastically decrease after learning. Still, one possible implication from above results cannot be ruled out by the experiments performed so far.
8

Specifically, it remains unclear how much the learning system contributes to the improvement of interaction. A possible argument would be that the observed improvements are due to the long-term habituation and experience with the robot. If this argument is true, then we should see a similar improvement of interactions as above, even if we simply repeat the baseline experiment (where learning is disabled) three times in a row. To investigate this question, we performed the aforementioned experiment (three times baseline) with all subjects. For the subjects, the experiment looked exactly the same as the other experiments: the difference was not transparent. Figure 10 compares the summation of posture change norm between the first and the third baseline experiment. In each of the experiments, only one subject made significant improvement during the intermediate learning steps. On the whole, although for some subjects slight improvement was visible (notably Subject 5), the results are not as comprehensive as when learning is enabled. This means that, while long-term habituation and experience aids the learning process, it is not sufficient for a general improvement in PHRI.

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

MARCH 2012

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

2.5 Posture Change Norm 2.0 1.5 1.0 0.5 0.0 Initial After Learning 1 After Learning 2 Summation of Posture Change Norms

20 0.0325 15

10

0

10 20 30 40 50 60 70 80 90 100 Time (a) Initial After Learning 1 After Learning 2

5 1 2 3 4 Subject Number (a) 2.32e­4 1.18e­4 5

2.0 Posture Change Norm 20

Summation of Posture Change Norms

1.5

0.0873

1.0

5.19e­4 4.96e­4 15

0.5

0.0

0

10 20 30 40 50 60 70 80 90 100 Time (b)

10

Summation of Posture Change Norms

Figure 7. Evolution of the posture change norm during one learning experiment. The solid, dotted, and dashed lines show the evolution of the value when the robot has not yet learned, after the first intermediate learning step, and after the second intermediate learning step, respectively. (a) Subject 1 and (b) Subject 2.

5 1 20 2 3 4 Subject Number (b) 5

0.0049 0.022 0.033 0.0073 0.0028 0.0021

Discussion The following observations are based on the results of the above experiments. First, when learning and adaptation were only possible on the side of the human caregiver, generally, little or no improvement could be measured. However, even in this asymmetric learning situation, at least one subject was able to adapt to the robot so as to significantly improve the interaction quality. This shows the human ability to quickly adapt to new situations and motor tasks. The second observation is that the interaction quality significantly improved in the first learning experiment, and the improvement was even more remarkable during the second learning experiment. These results support our working hypothesis that the proposed learning system facilitates PHRI. Another interesting observation is that the human adaptation to the robot occurred in stages throughout the experiment. At the beginning of the experiment, the users were intimidated by the robot and the experimental setup. However, during the course of the experiment, the test subjects became more and more comfortable with the situation and the robot dynamics. As a result, the test subjects found it easier to interact with the robot. This suggests that algorithms for improving PHRI

3.05e­7 1.36e­5 4.63e­4 8.27e­8

15

10

5 1 2 3 4 Subject Number (c) 5

Figure 8. Mean and standard deviation of the summation of the posture change norm of test subjects in the (a) baseline, (b) first learning, and (c) final training experiments. The dark gray, white, and light gray bars indicate the mean and standard deviation values during each of the intermediate learning steps (after every ten trials). In (a), the baseline experiment, only Subject 2 shows a significant improvement after all trials. In (b) the first learning experiment, Subjects 2, 4, and 5 show significant improvements. In (c) the final experiment, the interaction with the robot improved for all subjects. With each learning trial, the indicated values decrease, and the movement of the robot becomes smoother and more synchronized with that of the subject.

MARCH 2012

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

9

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

2.5 2 1.5 1 0.5 0

Summation of Posture Change Norm

3 Maximum Posture Change Norm

20

15

0.0029

10

1

2

3 4 Subject Number (a)

5

0

1

2

3 4 Subject Number (a)

5

2.5 2 1.5 1 0.5 0

Summation of Posture Change Norm

3 Maximum Posture Change Norm

20

0.0313 15

10

0

1

2

3 4 Subject Number (b)

5

1

2

3 4 Subject Number (b)

5

Figure 9. Change in the maximum posture change norm in each phase during (a) the baseline and (b) the first learning experiments. No significant difference in the maximum posture change norm is observed in the baseline experiment. On the other hand, there are large changes in the maximum posture change norm in the learning experiment. For all subjects, the values decrease drastically after learning.

Figure 10. Baseline experiment repeated three times in a row to investigate whether improvement can be made without the robot's learning system enabled. In each of the experiments only one subject made a significant improvement. On the whole, although for some subjects slight improvement is visible (notably Subject 5), the results are not as comprehensive as when learning is enabled. (a) First baseline and (b) third baseline experiments.

can be made more efficient if the familiarization of the human with the robot is taken into account. A special familiarization phase, in which the human caregiver becomes accustomed to the robot before any cooperative tasks, might be one approach. Another method by which to familiarize the human with the robot might be a welldesigned interaction protocol that involves tasks that are intended only to familiarize the human with the robot. An interesting feature of the proposed algorithm is the ability to monitor the progress of learning as trajectories in a lowdimensional space. The results of this study indicate that the trajectories converge toward a V-shaped pattern for the standing-up task. Furthermore, the trajectories, after learning, appear to have particular points or bottlenecks through which they pass. This is reminiscent of the study by Kuniyoshi et al. [19] in which it was shown that the dynamic motions for a particular task often have a
10

bottleneck in the state space. This bottleneck is the result of the interaction of the human body and the environment. Kuniyoshi et al. referred to this property as knack and showed that the knack can be exploited to efficiently control a humanoid robot. In the proposed PHRI scenario, the dynamics of the robot strongly depends on the dynamics of the human caregiver. A knack may be said to appear in PHRI because of the strong coupling between the human and the robot and the resulting joint dynamics. In other words, the human can be regarded as a changing environment that constraints the robot dynamics. Note that, although only the posture of the robot was used to create the trajectories, we can still discern a knack that is based on joint dynamics. However, it can be argued that posture information is not sufficient enough to draw final conclusions about the joint dynamics. To address this question, we are currently investigating a different cooperative

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

MARCH 2012

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

PHRI task, namely that of assisted walking as can be seen in Figure 11. In this scenario, the human caregiver must assist the robot while the latter is trying to walk. Similar to the standing-up task, the assisted walking is realized using three desired postures: left leg up, standing, and right leg up. These postures are repeated in a predetermined order (standing ! left leg up ! Figure 11. The assisted walking task where a human caregiver assists the robot in his or standing ! right leg up) to create a her attempt to perform several walking steps. (Photo courtesy of ERATO Asada Project.) cyclic walking motion. During an interaction session, the human assists the robot in performing four cycles of the latter sequence. For a fast assessment of the applicability of our approach to different scenarios, we 90 performed an experiment using the same setup and para85 meters as for the standing-up task. However, in this case, we had only one test subject performing 30 interactions 80 with learning enabled and 30 interactions as baseline. Figure 12 shows the comparison of posture change norms 75 between each phase (a phase consists of ten trials) in each experiment (one baseline and one learning experiment). 70 As opposed to the baseline experiment, we can see 65 that the posture change norms decrease when learning is enabled. Note that the baseline experiment was per60 formed after the learning experiment to account for the Baseline Learning human's habituation. These early results show that the proposed human-in- Figure 12. Summation of posture change norm for baseline the-loop learning system is not limited to the uprising and learning experiments in the assisted walking task. Each bar interaction and that other types of interactions can be real- corresponds to a phase of ten interactions with the robot. ized. At the same time, in our experiments, we found that the robot often failed to keep up when the human demon- human interaction partner. This method has a low compustrator drastically increased or reduced the speed of his or tational load and can be run online during the interaction her walking gaits. This is due to the reactive nature of esti- with the robot and requires relatively few training data. In mating the joint dynamics from the postures only. To keep contrast to previous research in this field, the robot considup with a human interaction partner in this scenario, the ered in this study is in close physical contact with the robot must be more predictive in its estimation of the joint human partner and plays an active role during the dynamics. One possible approach to overcome this prob- performance of the cooperative task. The CB2 robot, lem is to include sensor information into the probabilistic through its flexible-joint design and soft silicone skin, is low-dimensional posture models. That is, the state of the particularly well suited to such tasks because physical robot would be based on the current joint angles as well as interactions become more natural and lifelike. In an the information gathered from the sensors under the skin. experiment inspired by parenting behavior in humans, we In this case, switching between one posture and another were able to show that the proposed learning method would also be influenced by the amount of pressure results in measurable improvements of interaction. Quanexerted by the human caregiver on the robot's body, e.g., titative evaluations based on the posture change norm conthe arms during assisted walking. Further studies are firm the significance of these improvements. Thus far, the control system used herein has three underway to obtain a conclusive answer to these questions. parameters: the set of desired postures, the feedback gains, and the switching rule. In this article, we focused on learnConclusions In this article, we presented a PHRI scenario in which suc- ing the switching rule only. However, for more complex cessful task completion can only be achieved through coor- interaction scenarios it might be important to adapt all of dinated actions involving physical contact. We introduced these parameters. Another limitation of the proposed a simple machine learning algorithm for adapting the learning algorithm is the use of binary evaluation informabehavior of the robot according to an evaluation by a tion. As a result, optimization of the parameters in a
Summation of Posture Change Norm
MARCH 2012

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

11

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

gradient descent manner is not possible. Another drawback of binary evaluation information is that only positive feedback examples are retained for use in the learning set while negative feedback examples are removed from the learning set. With respect to the first limitation, the desired postures and feedback gains can be regarded as attractors and velocities in a low-dimensional space. Amor et al. [7] have shown that such attractors can be efficiently learned in a low-dimensional space while incorporating kinesthetic assistance provided by the user. In the future, we therefore hope to integrate such a method into the proposed PHRI algorithm. As for the second limitation, we are considering the use of pressure sensors on the body of the robot. The amount of pressure issued by the caregiver can then be used as an approximate evaluation information. This allows for a finer grained reward value and, consequently, the use of modern optimization algorithms. Pressure sensors are also helpful to distinguish whether the human is currently in contact with the robot. In summary, this study provided interesting insights into the dynamics of PHRIs. The combination of a softbody robot and an efficient learning scheme is an important step toward responsive robots that share a common living space with humans. References
[1] S. Ikemoto, T. Minato, and H. Ishiguro, "Analysis of physical human-robot interaction for motor learning with physical help," Appl. Bionics Biomech. (Special Issue on Humanoid Robots), vol. 5, no. 4, pp. 213­223, 2008. [2] S. Ikemoto, H. B. Amor, T. Minato, H. Ishiguro, and B. Jung, "Physical interaction learning: Behavior adaptation in cooperative human-robot tasks involving physical contact," in Proc. IEEE Int. Symp. Robot and Human Interactive Communication (Ro-Man), Sept. 2009, pp. 504­509. [3] A. De Santis, B. Siciliano, A. De Luca, and A. Bicchi, "An atlas of physical human-robot interaction," Mechanism Mach. Theory, vol. 43, no. 3, pp. 253­270, Mar. 2008. [4] O. Khatib, K. Yokoi, O. Brock, K. Chang, and A. Casal, "Robots in human environments," in Proc. 1st Workshop Robot Motion and Control, 1999, pp. 213­221. [5] S. Yohanan and K. E. MacLean, "The haptic creature project: Social human-robot interaction through affective touch," in Proc. AISB Symp. Reign of Catz & Dogs: The 2nd AISB Symp. Role of Virtual Creatures in a Computerised Society, 2008, vol. 1, pp. 7­11. [6] K. Kosuge, T. Hayashi, Y. Hirata, and R. Tobiyama, "Dance partner robot--MS-danSer," in Proc. IEEE/RSJ Int. Conf. Intelligent Robots and Systems, 2003, vol. 3, pp. 3459­3464. [7] H. B. Amor, E. Berger, D. Vogt, and B. Jung, "Kinesthetic bootstrapping: Teaching motor skills to humanoid robots through physical interaction," KI 2009: Advances in Artificial Intelligence (ser. Lecture Notes in Artificial Intelligence), B. Mertsching, Ed. Berlin, Germany: SpringerVerlag, 2009, pp. 492­499. [8] J. Tani, R. Nishimoto, J. Namikawa, and M. Ito, "Codevelopmental learning between human and humanoid robot using dynamic neural

network model," IEEE Trans. Syst., Man, Cybern., vol. 38, no. 1, pp. 43­ 59, 2008. [9] S. Calinon and A. Billard, "What is the teacher's role in robot programming by demonstration?--Toward benchmarks for improved learning" Interaction Studies (Special Issue on Psychological Benchmarks in Human-Robot Interaction), vol. 8, no. 3, pp. 441­464, 2007. [10] T. Odashima, M. Onishi, K. Tahara, K. Takagi, F. Asano, Y. Kato, H. Nakashima, Y. Kobayashi, T. Mukai, Z. W. Luo, and S. Hosoe, "A soft human-interactive robot ri-man," in Proc. IEEE/RSJ Int. Conf. Intelligent Robotics and Systems (IROS, v018), 2006, p. 1­1 (video session). [11] P. Evrard, E. Gribovskaya, S. Calinon, A. Billard, and A. Kheddar, "Teaching physical collaborative tasks: Object-lifting case study with a humanoid," in Proc. IEEE-RAS Int. Conf. Humanoid Robots (Humanoids), Dec. 2009, pp. 399­404. [12] D. Lee, C. Ott, and Y. Nakamura, "Mimetic communication with impedance control for physical human-robot interaction," in Proc. IEEE Int. Conf. Robotics and Automation (ICRA) , 2009, pp. 1535 ­ 1542. [13] B. D. Argall and A. G. Billard, "A survey of tactile human-robot interactions," Robot. Autonom. Syst., vol. 58, no. 10, pp. 1159­1176, 2010. [14] G. Schwarz, "Estimating the dimension of a model," Ann. Statist, vol. 6, no. 2, pp. 461­464, 1978. [15] T. Minato, Y. Yoshikawa, T. Noda, S. Ikemoto, H. Ishiguro, and M. Asada, "Cb2: A child robot with biomimetic body for cognitive developmental robotics," in Proc. IEEE-RAS/RSJ Int. Conf. Humanoid Robots (Humanoids), 2007, pp. 557­562. [16] A. P. Dempster, N. M. Laird, and D. B. Rubin, "Maximum likelihood from incomplete data via the EM algorithm," J. R. Statist. Soc. Series B (Methodological), vol. 39, no. 1, pp. 1­38, 1977. [17] L. R. Rabiner, "A tutorial on hidden Markov models and selected applications in speech recognition," Proc. IEEE, vol. 77, no. 2, pp. 257­ 285, Feb. 1989. [18] K. Fukunaga and D. Olsen, "An algorithm for finding intrinsic dimensionality of data," IEEE Trans. Comput., vol. 20, no. 2, pp. 176­183, 1971. [19] Y. Kuniyoshi, Y. Ohmura, K. Terada, A. Nagakubo, S. Eitoku, and T. Yamamoto, "Embodied basis of invariant features in execution and perception of whole body dynamic actions--Knacks and focuses of rolland-rise motion," Robot. Autonom. Syst., vol. 48, no. 4, pp. 189­201, 2004.

Shuhei Ikemoto, Department of Multimedia Engineering, Osaka University, Japan. E-mail: ikemoto@ist.osaka-u.ac.jp. Heni Ben Amor, Intelligent Autonomous Systems Group, Technische Universitaet Darmstadt, Germany. E-mail: amor@ias.tu-darmstadt.de. Takashi Minato, ATR Hiroshi Ishiguro Laboratory, Kyoto, Japan. E-mail: minato@atr.jp. Bernhard Jung, Virtual Reality and Multimedia Group, Technische Universit at Bergakademie Freiberg, Germany. E-mail: jung@informatik.tu-freiberg.de.

12

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

MARCH 2012

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Hiroshi Ishiguro, Department of Systems Innovation, Osaka University, Japan. E-mail: ishiguro@is.sys.es. osaka-u.ac.jp.

·
Close physical interaction between robots and humans is a particularly challenging aspect of robot development.

·
Until recently, robotic systems mostly remained in the realm of industrial applications and academic research.

·
In recent years, robotics technology significantly matured and produced highly realistic android robots.

·

·

·

·
The human counterpart is part of the learning system and overall dynamics.

·
The joints have low mechanical impedance because of the compressibility of air.

·
The joints have low mechanical impedance because of the compressibility of air.

· ·

·

MARCH 2012
View publication stats

·

IEEE ROBOTICS & AUTOMATION MAGAZINE

·

13

Point Cloud Completion Using Extrusions
Oliver Kroemer, Heni Ben Amor, Marco Ewerton, and Jan Peters
Intelligent Autonomous Systems Technische Universitaet Darmstadt Email: {oli, amor, peters}@ias.tu-darmstadt.de

Abstract--In this paper, we propose modelling objects using extrusion-based representations, which can be used to complete partial point clouds. These extrusion-based representations are particularly well-suited for modelling basic household objects that robots will often need to manipulate. In order to efficiently complete a partial point cloud, we first detect planar reflection symmetries. These symmetries are then used to determine initial candidates for extruded shapes in the point clouds. These candidate solutions are then used to locally search for a suitable set of parameters to complete the point cloud. The proposed method was tested on real data of household objects and it successfully detected the extruded shapes of the objects. By using the extrusion-based representation, the system could accurately capture various details of the objects' shapes.

20 40 60 80 100 120 140 160 0 50 100 150 200

I. I NTRODUCTION In the future, service robots working in everyday environments will need to grasp and manipulate a wide range of different objects. Given these unstructured environments, many of the encountered objects will be novel to the robot, and their complete shapes will initially be unknown. This shape information is however vital for successfully and efficiently manipulating the objects. Hence, the robot will need to autonomously determine the shapes of novel objects. One approach to acquiring a suitable 3D model is to scan the object from different perspectives by either shifting the object or moving around the object [1]. The information from these multiple perspectives can then be accumulated to form a 3D model. Although this approach can acquire accurate object models, the process of acquiring multiple images is a time-consuming and non-trivial task. Alternatively, the robot can attempt to predict the shape of an object from only a single perspective. The partial model acquired from one perspective can be completed by detecting patterns in the observed shape and extending these patterns into the occluded regions [2]. For example, planes of symmetry can be detected and, subsequently, used to complete the point cloud accordingly [3]. In this paper, we show how the partial point clouds of objects can be completed by detecting extruded shapes. Extruded 3D shapes are 2D shapes that have been extended into the third dimension along a particular path, such as a line segment (linear extrusion) or circle (rotational extrusion). For example, a cube is a linearly extruded square, and a sphere is a rotationally extruded circle. Our overall approach to detecting extruded shapes is similar to the shape from symmetry framework proposed by Thrun

Figure 1. The top left image shows a heart-shaped box. The top right image shows the depth data collected from the heart-shaped box. The bottom images show the object models obtained using the extrusion-based approach to point cloud completion.

and Wegbreit [4]. The robot first searches for planes of symmetry in the partial point cloud, which are entailed by both linear and rotational symmetries. The detected symmetries are then used to initialize local searches for suitable extrusion parameters. Finally, the detected extrusions are evaluated according to a scoring system, and used to complete the point cloud when applicable. The main contribution of this paper is therefore the use of extrusion-based, rather than symmetrybased, representations. Extrusion-based representations are well-suited for robots working in everyday environments, wherein many objects are manufactured. Not only are linear and rotational extrusions often used to design objects, but they are also common in the manufacturing process. As a result, many everyday objects have basic extruded shapes. In this paper, we will be focusing on completing the point clouds of basic objects that can be described by single extrusions. Although the proposed approach is similar to the shape from symmetry method, extrusion-based representations can complete some point clouds that the symmetry-based representations cannot. For example, the extrusion-based approach can complete surfaces by extruding edges, as illustrated in Fig. 1. The heart-shaped box has two curved surfaces that were not visible in the original point cloud. However, by

extruding the curved edges observed along the top of the box, the extrusion-based approach could complete these parts of the point cloud. The symmetry-based approach relies on projecting observed surfaces into occluded regions. Given that all of the observed surfaces are flat, this approach cannot complete the curved surfaces. A similar problem can occur when completing the point cloud of a rectangular box when only two of the six sides are observed. The extrusion-based approach can extrude one surface according to the other in order to complete the entire box. The symmetry-based approach would, however, have problems completing the third pair of opposing surfaces. There are obviously also situations in which a symmetrical object cannot be modeled by an extrusion-based representation. Thus, these two representations complement each other and could be used together. The proposed method is explained in Section II, which also includes an overview of related work in completing point clouds for robot applications. The applicability of the extrusion-based approach is demonstrated in Section III, wherein the robot successfully completes the shapes of various household objects from a single perspective. II. E XTRUSION - BASED P OINT-C LOUD C OMPLETION In this section, we explain how the observed point clouds can be completed by detecting extruded shapes in the partial point cloud. We begin by giving a brief overview of methods used in robotics applications to complete partial point clouds. In Section II-B, we detail the extrusion-based representation, which is flexible enough to model a wide range of objects shapes. Sections II-C to II-F explain how one can search for extrusions in a partial point cloud. The proposed method assumes that the robot's vision system acquires structured point clouds; i.e., each point cloud corresponds to a pixel in a 2D grid. This form of data can be acquired from dense stereo, time-of-flight camera, Kinect, or other active stereo cameras. A. Point Cloud Completion in Robotics Determining the shape of an object from a single view is a common problem in robotics. One approach to solving this problem is to provide the robot with a library of previously scanned models, which it can then fit into the observed scene [5, 6, 7, 8]. This approach allows the robot to accurately reconstruct the scene. However, it also relies on the robot having a model of the object, and requires searching through a large library of known objects. In the field of computer vision, Pauly et al. [9] presented a method for completing the shape of objects using the models of objects with similar shapes. Hence, a smaller library of objects could be used, as the objects generalize to novel objects. Another approach is to fit primitive shapes, such as cubes and cylinders or superquadrics, to the partial view [10]. Primitive shapes cannot only be used to represent simple objects, but also parts of more complex objects [11]. The primitive shapes are generally parameterized such that they

3D

Symmetry

Split

ICP

(A)

(B)

(C)

(D)

Figure 2. The figure illustrates how the initial extrusion hypotheses are generated. The top row corresponds to rotational extrusions, and the bottom row demonstrates linear extrusions. (A) The 3D objects of a tube and a box are shown. (B) A top view of the objects, as well as the detected plane of symmetry indicated by the dashed line. (C) The point clouds are divided into two regions, indicated by red and green. The arrows in the top image indicate the observed surface normals for these regions, which are used to divide the points. (D) The ICP algorithm is used to slide one region into the other. The rigid body transformation found by the ICP algorithm is then used to compute the extrusion parameters.

can be adapted to model a range of similar object parts. The ability to adapt the primitives in this manner is important, as the additional flexibility allows the model to capture more details of the object. Point cloud completion can also be performed by predicting the full shape of an object from symmetry [4, 3]. As already mentioned, this approach is the most similar to the one presented in this paper. Thrun and Wegbreit proposed a hierarchy of symmetries that can be detected from a partial view and, subsequently, used to complete the point cloud [4]. They begin by performing a grid search over the entire object for valid local symmetries, followed by a local optimization of the symmetry parameters using the hill climbing algorithm. The resulting candidate symmetries are evaluated using a scoring system based on the probability of observing the completed point cloud. The symmetry-based approach to point cloud completion was extended to robot manipulation by Bohg et al. [3]. B. Extrusion-based Object Representations The goal of the work presented in this paper is to detect extruded shapes of objects from a single perspective. In this manner, the robot can attempt to complete the shapes of the objects in occluded region, such that the model can be used for manipulating the object. An extruded shape consists of two components: the profile and the path. The profile is the basic 2D shape that the 3D extruded shape is based on. The path is the line indicating how the profile is extended into the third dimension. In this paper, we focus on paths defined by straight line segments (linear extrusions) and circles (rotational extrusions). This family of shapes allows us to represent a wide range of different primitive shapes, including spheres, rectangular prisms, cylinders, and cones.

However, the robot will also encounter more complex extruded shapes. Hence, we need the representation to be flexible enough to capture these shapes in detail. We achieve this goal by representing the profile of the object as a 2D point cloud. This low-dimensional point cloud can be used to represent a wide range of shapes. The profile can, thus, also be directly obtained from the observed 3D point cloud. The path of a linear extrusion is defined by a 3D coordinate system and the length of the extrusion. The direction of the extrusion is always in the coordinate frame's z-direction, and the 2D profile defines the shape in the x-y plane. For a rotational extrusion, we define the axis of rotation as a 3D line. The 2D profile points define the location along the axis of rotation, as well as the radial distance from this axis. C. Detecting Planes of Symmetry One important characteristic of both linear and rotational extrusions is that they result in symmetric shapes; i.e. the extrusions entail a mirror symmetry. Therefore, to find extruded shapes in the point cloud, we begin by first searching for planar reflection symmetries. Instead of using a grid search to detect these symmetries [4, 3], we adopt the fast votingbased approach proposed by Mitra et al. [12]. First, the normal vector and curvature are computed for each point in the point cloud. The normal vector can be obtained by computing the eigenvectors for a local neighborhood of points. The normal direction is given by the eigenvector with the smallest eigenvalue, which points towards the camera. The eigenvectors can then be computed for the local neighborhood of normal directions. The resulting two largest eigenvalues are used to approximate the local curvature. Subsequently, each point is compared with every other point in the cloud in order to find pair-wise symmetries. The plane of symmetry for two points xa and xb is located at the midpoint 0.5(xa + xb ), with a normal aligned with the direction xb - xa . However, this pair-wise symmetry is only considered valid if the points' normals are also reflected in this plane, and the difference in curvature values between the two points is below a given threshold. The parameters of each valid plane of symmetry are treated as one vote. The goal is therefore to find parameter settings with many votes, which correspond to large symmetric regions in the point clouds. In order to find these planes of symmetries, the distribution of symmetry plane parameters is modelled as a kernel density estimate [12]. All of the modes of this distribution can then be found using mean-shift clustering [13]. The corresponding symmetry parameters form the basis for local searches for extruded shapes. D. Computing Initial Extrusion Parameters using ICP Given a plane of symmetry, a set of extrusion parameters needs to be computed. The steps used to perform this computation are outlined in Fig. 2. We begin by dividing the point cloud into two parts according to the plane of symmetry. To detect extrusions, we divide the points according to which side of the plane of

symmetry the point lies on. To detect rotations, we separate the points according to their normals n and the symmetry plane's normal p. If the inner product pT n is greater than zero, the point is assigned to the first region and otherwise it is assigned to the second region. The dividing of the point cloud into two regions is illustrated in Fig. 2C. Once the point cloud has been divided into two parts, we need to find a rigid-body transformation that shifts one of the parts to the pose of the other. We use the iterative closest point (ICP) algorithm to compute this transformation [14]. The computed transformation should have the same effect as sliding the part along the extrusion's path. Hence, a linear extrusion should result in a translation, and a rotational extrusion should result in a rotation about an axis (see Fig. 2D). Given the transformation computed using ICP, we can compute the direction of the path for the linear extrusions, and the axis of rotation for rotational extrusions. It should be noted that the symmetry-detection algorithm proposed by Mitra et al. [12] also uses ICP to detect symmetries more accurately. However, their approach searches for symmetries, and then uses ICP to find the same type of symmetry more accurately. Instead, our approach first detects planar reflection symmetries, and then uses ICP to find extrusions, which are a different type of pattern. In this manner, we exploit the self-similarity property of extruded shapes. E. Local Search Given an initial hypothesis for an extrusion, the parameter and profile can usually be further improved using a local search. The 2D point cloud for representing the extrusion's profile also needs to be determined at this stage. For linear extrusions, the profile will be a plane of points that is orthogonal to the path of the extrusion. Hence, we must align the direction of the path d with the normal of the profile plane. We perform this local alignment using an iterative procedure. We first define the set of profile points as those points that have a normal n such that -dT n >  , where 1 >  > 0 is a threshold value. Given this set of profile points, the direction of the path is updated as the negative mean of the profile points' normals. The set of profile points can then again be updated according to the update path direction. In order to improve the robustness of this process, we begin with a low threshold value, e.g.,  = 0.5, and increase the value in each iteration up to a maximum value, e.g.  = 0.95. After the path direction has been aligned with the normal of one of the object's sides, we need to fit a plane to this side. We first find the location of this plane along the path direction by computing the position of the current profile points in this direction and selecting the mode. The points that are near to this plane are then used to define the final set of profile points. The direction of the extrusion path is given by the normal of this plane. The length of the extrusion path is set according to the length of the surfaces that are orthogonal to the direction of the extrusion.

Figure 3. The columns correspond to the results for a cup, a funnel, a pot, a heart-shaped box, a roll of toilet paper, and a box respectively. The first row shows a picture of each object. The second row shows the depth image taken of the object, which corresponds to the partial point cloud. Darker red regions are further away than yellow regions, and blue regions have a depth of zero. The bottom two rows show the reconstructed object shape from different angles. These reconstructions were made from one perspective of the object and the 3D meshes were not post-processed.

For rotational extrusions, we want to set the rotational axis such that many of the profile points are mapped onto each other. We begin by projecting the data into the 2D profile space according to the initial hypothesis, and marking points in dense regions as profile points. For each of the m profile points, located at 3D points x1:m , we compute the position along the rotation axis a1:n and the distance from the rotation axis r1:m to the point. We also compute the normalized direction d1:m from each point to the closest point on the axis. For the ith data point, we now compute the m m locally weighted mean radius r ^i = j =1 wij rj / k=1 wik where the weight wij is given by wij = exp(-(ai - aj )2 /v 2 ) and v is a length scale parameter. This radius value r ^i is an approximation of the desired radius at this point along the axis. Using this desired radius, we create a new 3D point y i = xi + r ^i di . Once all y 1:n have been computed, we fit a line to these points, which then becomes the new axis of rotation. Note that the point y i will be closer to xi than the axis of rotation if ri > r ^i and, thus, will draw the axis of rotation closer to xi . Similar to the linear extrusion case, we iterate over these steps multiple times to acquire a suitable axis of rotation. The final profile for the rotational extrusion can be smoothed by using local averaging over the radii component. F. Visibility Score In the final stage of the extrusion detection process, we assign each candidate extrusion a score in order to select the one that best represents the robot's observations. Similar to the scoring systems used in symmetry-based approaches [4, 3], the score is defined according to how well the

completed point cloud matches the observed scene. Our basic scoring system is based on the depth images obtained from the camera, such as the one shown in Fig. 3. This data structure is similar to a z-buffer in computer graphics, and indicates which regions in space are occluded. Using the 3D positions of the observed points, and their pixel location in the z-buffer, we can compute a projection matrix P between the 3D camera space and the 2D z-buffer pixel space. The partial point cloud is first completed according to the extrusion parameters currently being evaluated. In order to compare different extrusions in a fair manner, each point in the profile should be used to generate the same number of extruded points. The completed point cloud is then projected into the z-buffer space using the projection matrix P . Each projected point is assigned to the nearest pixel in the z-buffer. A projected point is assigned a score according to its z value in the camera coordinate frame zc and the depth value of the assigned z-buffer pixel zp . We also define a length scale parameter h. If the depth values are close together zp - zc < 2h, then the point provides evidence for the extrusion and, hence, is assigned a positive score of exp(-0.5(zp - zc )2 /h2 ). If the point has a depth greater than the z-buffer zp - zc > 2h, its location corresponds to an occluded region and, hence, it is assigned a score of zero. Finally, if the point has a depth that is less than the z-buffer zp - zc < -2h, then it contradicts the observed scene and, hence, it is assigned a negative score, e.g. -3. The score for the entire point cloud is given by the sum of the scores obtained by the individual points, divided by the number of points in the profile. The extrusion with the largest score is used to complete

the partial point cloud, which can then be used to create a 3D model for manipulating the object. III. E XPERIMENT
Probability Density

2 1.8 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2 0 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2

The proposed method was implemented and applied to a set of common household objects. The results show that the method can detect the extruded shapes in the point clouds, and even capture details of the objects' shapes. A. Setup and Results In this experiment, we evaluated the accuracy of the extrusion paths found by the proposed method. In particular, we measured the errors in the radii of rotational extrusions and the path lengths of linear extrusions. Using a standard Kinect camera, we collected 30 partial point clouds of common household objects. The objects were placed individually on a table, and the table was segmented out of the point cloud. The segmented point cloud was then subsampled to obtain around 3000 points. The approach presented in Section II was then used to complete each of the partial point clouds. The desired type of extrusion was pre-specified for each object. The actual lengths and radii of the extrusions were also measured manually using the point cloud. By measuring these distances directly from the point clouds, cameraspecific calibration errors do not affect our results. The measured distances were then compared to those found by the point cloud completion method. The robot found suitable extrusions for 28 of the 30 images (93% success rate). In one of the failed trials, the robot extruded an incorrect surface. In the other failed trial, the robot did not find a suitable axis of rotation for describing the shape of the object. In the successful trials, the error in the computed distances could be measured. The distribution over these errors is shown in Fig. 4. The mode and mean of the distribution are at -1.36 mm and -1.19 mm respectively. The standard deviation of the distribution is 2.23 mm. Detecting the planar symmetries for each object in the experiments took on average 19.9 seconds. Fig. 3 shows a set of models obtained using the proposed method. For the rotationally extruded objects, a stochastic optimization was used to improve the alignment of the axis of rotation and increase the visibility score. The 3D models were generated from the completed point clouds using the ball-pivot algorthim [15]. Some of the objects may seem shorter than the actual object, which is a result of the table segmentation. Standard post-processing methods, such as smoothing, were not applied to the point clouds, in order to display the quality of the profiles more clearly. For a real application, we recommend post-processing the 3D model. B. Discussion The results show that the extrusion-based approach could accurately complete the objects' shapes in most of the trials. In practice, the accuracy of the model would decrease due to other sources of error, such as the camera calibration.

Extrusion Error (cm)
Figure 4. The distribution over errors in extrusion lengths and radii. The distribution was modelled using a kernel density estimate with a Gaussian kernel with a width of  = 0.1cm. A negative value indicates that the extruded shape found by the proposed method was smaller than the actual size.

However, the computed models should still be suffficiently accurate for performing coarse manipulations with the objects. The results also show that the voting-based symmetry detection method performs well even when applied to noisy data. The detected planes of symmetry allowed the robot to find valid extrusions in most of the objects used in this experiment. The computed extrusions are slightly biased towards being too small. This may be a result of the relatively large penalization for overestimating the size of the extrusion. However, a slight bias is also to be expected for some objects, such as the cup. The point cloud of the cup contains points from both the outside and the inside of the cup. Hence, a rotational extrusion that maps the front-outer points onto the back-inner points would achieve a higher visibility score, but would also result in a smaller cup radius than the actual radius. The models in Fig. 3 show the importance of using a flexible profile representation. The extrusions are capable of modelling details, such as the lip of the pot and the hole in the middle of the toilet paper roll. The quality of the profiles could be improved by reincorporating more points from the original point cloud, once a valid set of extrusion parameters has been found. Fine details, such as the texture of the cup, are obviously lost due to noise in the data. One shortcoming of the current method is that it can sometimes detect degenerate extrusions when a linear extrusion is applied to a rotationally extruded object, or vice versa. For example a rotational extrusion applied to a box may detect a cylinder that fits into the shape of the box. We plan to address this problem in the future in order to render the method more robust. The method is also able to cope with additional parts of

R EFERENCES [1] M. Krainin, P. Henry, X. Ren, and D. Fox, "Manipulator and object tracking for in-hand 3d object modeling.," I. J. Robotic Res., vol. 30, no. 11, pp. 1311­1327, 2011. [2] T. P. Breckon and R. B. Fisher, "Amodal volume completion: 3d visual completion," Comput. Vis. Image Underst., vol. 99, pp. 499­526, Sept. 2005. [3] J. Bohg, M. Johnson-Roberson, B. León, J. Felip, X. Gratal, N. Bergström, D. Kragic, and A. Morales, "Mind the Gap - Robotic Grasping under Incomplete Observation," in ICRA 2011, May 2011. [4] S. Thrun and B. Wegbreit, "Shape from symmetry," in ICCV 2005, pp. 1824­1831, IEEE, 2005. [5] S. Savarese and F.-F. Li, "3d generic object categorization, localization and pose estimation," in ICCV 2007, pp. 1­8, 2007. [6] R. Detry, N. Pugeault, and J. Piater, "A probabilistic framework for 3D visual object representation," IEEE Trans. Pattern Anal. Mach. Intell., vol. 31, no. 10, pp. 1790­1803, 2009. [7] A. Aldoma, N. Blodow, D. Gossow, S. Gedikli, R. Rusu, M. Vincze, and G. Bradski, "Cad-model recognition and 6 dof pose," in ICCV 2011, 3D Representation and Recognition (3dRR11), 11/2011 2011. [8] G. Biegelbauer and M. Vincze, "Efficient 3d object detection by fitting superquadrics to range image data for robot's object manipulation," in ICRA 2007, pp. 1086 ­1091, 2007. [9] M. Pauly, N. J. Mitra, J. Giesen, M. Gross, and L. Guibas, "Example-based 3d scan completion," in Symposium on Geometry Processing, pp. 23­32, 2005. [10] Z. C. Marton, L. C. Goron, R. B. Rusu, and M. Beetz, "Reconstruction and Verification of 3D Object Models for Grasping," in ISRR 2009, (Lucerne, Switzerland), 2009. [11] C. Goldfeder, P. K. Allen, C. Lackner, and R. Pelossof, "Grasp planning via decomposition trees," in ICRA'07, pp. 4679­4684, 2007. [12] N. J. Mitra, L. Guibas, and M. Pauly, "Partial and approximate symmetry detection for 3d geometry," ACM Transactions on Graphics (SIGGRAPH), vol. 25, no. 3, pp. 560­568, 2006. [13] D. Comaniciu and P. Meer, "Mean shift: a robust approach toward feature space analysis," IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 24, pp. 603 ­619, may 2002. [14] Y. Chen and G. Medioni, "Object modeling by registration of multiple range images," in ICRA 1991, pp. 2724­2729, 1991. [15] F. Bernardini, J. Mittleman, H. Rushmeier, C. Silva, G. Taubin, and S. Member, "The ball-pivoting algorithm for surface reconstruction," IEEE Trans. Visualization and Computer Graphics, vol. 5, pp. 349­359, 1999.

Figure 5. The top left image show the watering can. The top right image shows the depth image taken of the watering can. Darker red regions are further away than yellow regions, and blue regions have a depth of zero. The bottom images show the results of applying the proposed point cloud completion approach to the watering can's partial point cloud. The watering can is an example of an object that consists of multiple extruded parts.

the object that are not extruded, as shown by the handle of the cup. In the future, we will investigate how a robot can robustly decompose more complex objects into multiple extruded parts, using the method proposed by Mitra et al. [12]. An early result of this approach applied to a watering can is shown in Fig. 5. Although the system failed to complete the top handle, and incorrectly completed the side handle, these initial results are promising. Overall, the experiment has demonstrated that the proposed method could detect most of the extruded shapes and, thus, accurately complete the point clouds. IV. C ONCLUSION In this paper, we investigated how point clouds of basic objects can be represented and completed by linear and rotational extrusions. These extrusions are represented in a flexible manner, which allows them to accurately model a wide range of shapes. By detecting local symmetries in partial point clouds, we can search for extrusions in an efficient manner, and use these extrusions to complete the point cloud. In the experiment, the proposed method was applied to point clouds obtained from real household objects, and successfully completed most of the partial point clouds. In the future, we plan to use the proposed method in order to plan grasps on novel objects. In particular, the proposed method allows us to compute contact points for occluded regions of the object. ACKNOWLEDGEMENTS The project receives funding from the European Community's Seventh Framework Programme under grant agreement n° ICT- 248273 GeRT.

IEEE/RSJ International Conference on Intelligent Robots and Systems -- IROS 2012

Generalization of Human Grasping for Multi-Fingered Robot Hands
Heni Ben Amor, Oliver Kroemer, Ulrich Hillenbrand, Gerhard Neumann, and Jan Peters

Abstract-- Multi-fingered robot grasping is a challenging problem that is difficult to tackle using hand-coded programs. In this paper we present an imitation learning approach for learning and generalizing grasping skills based on human demonstrations. To this end, we split the task of synthesizing a grasping motion into three parts: (1) learning efficient grasp representations from human demonstrations, (2) warping contact points onto new objects, and (3) optimizing and executing the reach-and-grasp movements. We learn low-dimensional latent grasp spaces for different grasp types, which form the basis for a novel extension to dynamic motor primitives. These latent-space dynamic motor primitives are used to synthesize entire reach-and-grasp movements. We evaluated our method on a real humanoid robot. The results of the experiment demonstrate the robustness and versatility of our approach.

I. I NTRODUCTION The ability to grasp is a fundamental motor skill for humans and a prerequisite for performing a wide range of object manipulations. Therefore, grasping is also a fundamental requirement for robot assistants, if they are to perform meaningful tasks in human environments. Although there have been many advances in robot grasping, determining how to perform grasps on novel objects using multi-fingered hands still remains an open and challenging problem. A lot of research has been conducted on robot grippers with few degrees of freedom (DoF) which may not be particularly versatile. However, the number of robot hands developed with multiple fingers has been steadily increasing in recent years. This progress comes at the cost of a much higher dimensionality of the control problem and, therefore, more challenges for movement generation. Hard coded grasping strategies will typically result in unreliable robot controllers that can not sufficiently adapt to changes in the environment, such as the object's shape or pose. Such hard coded strategies will also often lead to unnatural `robotic looking' grasps, that do not account for the increased sophistication of the hardware. Alternative approaches, such as the optimization of grasps using stochastic optimization techniques, are computationally expensive and require the specification of a grasp quality metric [27]. Defining an adequate grasp metric is often hard to do, as it requires specifying intuitive concepts in a mathematical form. Additionally, such approaches typically do not consider the whole reachand-grasp movement but exclusively concentrate on the hand
Heni Ben Amor, Oliver Kroemer, Gerhard Neumann and Jan Peters are with the Technische Universitaet Darmstadt, Intelligent Autonomous Systems, Darmstadt, Germany. {amor, kroemer, neumann, Fig. 1. The Justin robot learns to grasp and lift-up a mug by imitation. The reach-and-grasp movement is learned from human demonstrations. Latentspace dynamic motor primitives generalize the learned movement to new situations.

configuration at the goal. In this paper, we present an imitation learning approach for grasp synthesis. Imitation learning allows a human to easily program a humanoid robot [3], and also to transfer implicit knowledge to the robot. Instead of programming elaborate grasping strategies, we use machine learning techniques to successfully synthesize new grasps from human demonstration. The benefits of this approach are threefold. First, the computational complexity of the task is significantly reduced by using the human demonstrations along with compact lowdimensional representations thereof. Second, the approach allows us to imitate human behavior throughout the entire reach-and-grasp movement, resulting in seamless, naturallooking motions. Typical transitions between a discrete set of hand shapes, as can be found in traditional approaches, are thus avoided. Finally, this approach also allows the user to have control over the type of grasp that is executed. By providing demonstrations of only one particular grasp type, the synthesis algorithm can be used to generate distinct grasps e.g., only lateral, surrounding, or tripod grasp. The use of assorted grasps can considerably improve the robustness of the grasping strategy as the robot can choose a grasp type which is appropriate for the current task. A. Related Work In order to generalize human grasping movements, we need to understand how humans perform grasps. Human grasping motions consist of two components: the reaching motion of the arm for transporting the hand, and the motions

peters}@ias.tu-darmstadt.de Ulrich.Hillenbrand@dlr.de

Ulrich Hillenbrand is with the German Aerospace Center - DLR, Institute of Robotics and Mechatronics, Oberpfaffenhofen, Germany.

Grasp Type Learning

DB
Contact Warping
Human Demonstration Grasp Spaces

Grasp Spaces Grasp Optimizer

Grasp Configuration

LS-DMP

Fig. 2. An overview of the proposed approach. The contact points of a known object are warped on the current object. Using the resulting positions, an optimizer finds the ideal configuration of the hand during the grasp. The optimizer uses low-dimensional grasp spaces learned from human demonstrations. Finally, a latent space dynamic motor primitive robustly executes the optimized reach-and-grasp motion. The approach is data-driven and can be used to train and execute different types grasps.

of the fingers for shaping the hand [16], [17]. These two components are synchronized during the grasping movement [7]. For example, at around 75% of the movement duration, the hand reaches its preshape posture and the fingers begin to close [15]. At this point in time, the reaching motion of the hand shifts into a low velocity movement phase. Early studies into human hand control assumed muscles and joints as being controlled individually by the central nervous system [26], [19]. However, more recent studies have found evidence suggesting that the fingers are controlled using hand synergies [23], [2] -- i.e., the controlled movements of the fingers are synchronized. According to this view, fingers are moved "synergistically" thereby reducing the number of DoF needed for controlling the hand. Such hand synergies can be modeled as projections of the hand configuration space into lower-dimensional subspaces [20] such as the principal components. Movements along the first principal component of this subspace result in a basic opening and closing behavior of the hand. The second and higher-order principal components refine this motion and allow for more precise shaping of the hand [20], [24], see Fig. 3. Although the majority of the variation in the finger configurations is within the first two principal components, higher-order principal components also contain important information for accurately executing grasps [23]. The gain in grasp accuracy does, however, plateau at around five dimensions [22], [20]. Therefore, the space of human hand synergies during grasping can be well represented by a five-dimensional subspace. Following this idea, various researchers have used dimensionality reduction techniques to find finger synergies in recorded human grasps [4], [8]. Once a low-dimensional representation of finger synergies is found, it can be used to synthesize new grasps in a generate-and-test fashion. For example, the authors of [8] use Simulated Annealing to find

an optimal grasp on a new object while taking into account the finger synergies. Common to such approaches is the use of a grasp metric [27] that estimates the quality of a potential solution candidate. However, such metrics can be computationally demanding and rely on having an accurate model of the objects. In general, it is difficult to define a grasp metric that includes both, physical aspects of the grasps (such as the stability) as well as functional aspects that depend upon the following manipulations. Alternative approaches to grasp synthesis predict the success probability of grasps for different parts of the object. For example, good grasping regions are estimated from recorded 2D images of the object in [25]. A labeled training set of objects including the grasping region is subsequently produced by using a ray-tracing algorithm. The resulting dataset is then used to train a probabilistic model of the ideal grasping region. The learned model, in turn, allows a robot to automatically identify suitable grasping regions based on visual features. In a similar vein, Boularias et al. [6] use a combination of local features and Markov Random Fields to infer good grasping regions from recorded point clouds. Given an inferred grasping region, the reach-andgrasp motion still needs to be generated using a set of heuristics. Additionally, this approach does not address the problems of how to shape the hand and where to place the finger contacts. Tegin et. al. [28] also used imitation learning from human demonstration to extract different grasp types. However, they do not model the whole reach-and-grasp movement and circumvent the high-dimensionality problem by using simpler manipulators. II. O UR A PPROACH In our approach, we address the challenges of robot grasping by decomposing the task into three different stages: (1)

learning efficient grasp representations from human demonstrations, (2) warping contact points onto new objects, and (3) optimizing and executing the synchronized reach-andgrasp movements. An overview of the proposed approach can be seen in Fig. 2. The contact points of a known object are first warped onto the current object using the techniques in Sec. II-B. The warped contact points are then used by the optimizer to identify all parameters needed for executing the grasp, i.e., the configuration of the fingers and the position and orientation of the hand. The optimization is performed in low-dimensional grasp spaces which are learned from human demonstrations. Finally, the reach-and-grasp movement is executed using a novel extension to dynamic motor primitive [14] called latent-space dynamical systems motor primitive (LS-DMP). A. Learning Grasp Types from Human Demonstration Using human demonstrations as reference when synthesizing robot grasps can help to narrow down the set of solutions and increase the visual appeal of the generated grasp. At the same time, a discrete set of example grasps can also heavily limit the power of such an approach. To overcome this problem, we use dimensionality reduction techniques on the set of human demonstrations in order to infer the low-dimensional grasp space. To this end, we recorded the movements of nine test-subjects, where each test subject was asked to perform reach-and-grasp actions on a set of provided objects. We subsequently performed Principal Component Analysis (PCA) on the dataset, projecting it onto five principal components. This choice of dimensionality is based on research on the physiology of the human hand [22], [20] which suggested that five principle components are sufficient for accurately modeling the movements of the human hand. The resulting grasp space is a compact representation of the recorded grasps as it models the synergies between the different fingers and finger segments. The first principal component, for example, encodes the opening and closing of the hand. Fig. 3 shows grasps from the space spanned by the first two principal components. The above approach yields general grasp spaces that do not give the user control over the grasp type to be executed by the robot. However, for many tasks it is important to favor a particular grasp type over another when synthesizing the robot movements. For example, for carrying a pen one can use a tip grasp, while for writing with the pen an extension grasp is better suited. Hence, in a second experiment with the same test subjects we learned grasp spaces for specific grasp types, such as lateral grasps or tripod grasps. To determine the grasp space, we devised a grasp taxonomy [10] consisting of twelve grasp types and recorded specific datasets for each of these types. The datasets were subsequently used to learn grasp spaces for the specific grasp type. Due to the differences in kinematics of the human and robot hand, there are multiple ways to map the hand state to the robot state, also known as the correspondence problem

2. Principal Component

1. Principal Component
Fig. 3. The space spanned by the first two principal components of human recorded grasps applied to the robot hand. The first component describes the opening and closing of the hand. The second principal component modulates the shape of the grasp.

in the robotics literature [9]. In this paper, we solve the correspondence problem by dividing the generalization of grasps into two parts, i.e., the reproduction of the hand shape and the adaptation of Cartesian contact points. The reproduction of the hand shape is realized by directly mapping the human joint angles to the robot hand. For the index, middle and ring fingers this results in an accurate mapping with robot hand configurations similar to the demonstrated human hand shapes. In order to map the thumb, an additional offset needed to be added to the carpometacarpal joint. Using this type of mapping, the reproduced hand shapes will be similar to those of the human. The generalization of the Cartesian contact points is achieved by the contact warping algorithm described in Sec. II-B. The two generalizations in Cartesian space and in joint space are then reconciled through the optimization process explained in Sec. II-D. B. Generalizing Grasps through Contact Warping In this section, we introduce the contact warping algorithm. This algorithm allows the robot to adapt given contact points from a known object to a novel object. As a result, we can generalize demonstrated contact points to new situations. Assume that we are given two 3D shapes from the same semantic/functional category through dense sets of range data points. In our approach, the process of shape warping, that is, computing a mapping from the source shape to the target shape, has been broken down into three steps. 1) Rigid alignment of source and target shapes, such that semantically/functionally corresponding points get close to each other.

2) Assignment of correspondences between points from the source shape and points on the target shape. 3) Interpolation of correspondences to a continuous (but possibly non-smooth) mapping. The alignment step involves sampling and aligning many surflet pairs, i.e., pairs of surface points and their local normals, from source and target shapes. The estimation of relative clusters of the pose parameters is obtained from the surflet-pair alignments [11], [12]. Since the alignment of source and target shapes has brought corresponding parts close to each other, we can again rely on the local surface description by surflets to find correspondences, based on proximity of points and alignment of normal vectors. The correspondence assignment that we have used here is an improved version of the method described in [11]. In this approach, correspondences were assigned for each source surflet independently into the set of target surflets. For strong shape variations or unfavorable alignment between source and target, such an approach could result in a confusion of similar parts. In order to cope with larger shape variation, some interaction between assignments of neighboring points has to be introduced. We have, therefore, formulated correspondence search as an optimal assignment problem. In this formulation, interaction between assignments of different points is enforced through uniqueness constraints. Let {x1 , . . . , xN } be points from the source shape, transformed to align with the target shape; let {y1 , . . . , yN } be points from the target shape.1 Assignment of source point i to target point j is expressed as an assignment matrix, aij = 1 0 if i is assigned to j , otherwise. (1)

Fig. 4. Mug warping example. A dense set of surface points from the source mug (top row) and their mappings to the target mug (bottom row) are colored to code their three Cartesian source coordinates (three columns).

Furthermore, let dij = xi - yj be the Euclidean distances between source and target points and cij = ni · mj be the angle cosines between the unit normal vectors ni and mj at source point i and target point j , respectively. The objective is to minimize the sum of distances between correspondences, i.e., mutually assigned points,
N N

i.e., to assign only between points with inter-normal angle of  90 degrees. The two equality constraints (3) and (4) mediate the desired interaction between assignments of different points. The inequality constraint (5) can exclude points from being assigned and, therefore, the problem may become infeasible. Thus, we have to add imaginary source and target points x0 and y0 which have no position and no normal direction. They can be accommodated by appending large entries d0j and di0 to the distance matrix, which larger than all real distances in the data set, as well as zero entries c0j = ci0 = 0 to the angle cosine matrix. These imaginary points can be assigned to all real points with a penalty, which is chosen such that only points without a compatible partner will receive this imaginary assignment. We subsequently minimize the cost function C (a01 , . . . , a0N , a10 , . . . , aN N ) = D(a11 , . . . , a1N , a21 , . . . , aN N )
N N

(6)

+
i=1

di0 ai0 +
j =1

d 0j a 0j .

D(a11 , . . . , a1N , a21 , . . . , aN N ) =
i=1 j =1

dij aij ,

(2)

subject to the constraints
N

aij = 1 j  {1, . . . , N } ,
i=1

(3)

i.e., to assign every target point to exactly one source point,
N

aij = 1 i  {1, . . . , N } ,
j =1

(4)

i.e., to assign every source point to exactly one target point, and cij aij  0 i, j  {1, . . . , N } , (5)
equal number N of points from source and target shapes can always be re-sampled from the original data sets.
1 An

For solving this constrained optimization problem, we use the interior-point algorithm, which is guaranteed to find an optimal solution in polynomial time [30]. Finally, point correspondences are interpolated to obtain a continuous (but possibly non-smooth) mapping of points from the source domain to the target domain. More theory and systematic evaluations of the procedure are given in [13]. Fig. 4 shows an example of a dense set of surface points warped between two mugs. A warp of the contact points of an actual grasp from the source to the target mug is shown on the left of Fig. 2. C. Latent Space Dynamic Motor Primitives In order to execute different grasps, the robot requires a suitable representation of the grasping actions. Ideally, the grasping action should be straightforward to learn from

a couple of human demonstrations and easily adapted to various objects and changes in the object locations. The action representation should also ensure that the components of the grasping movement are synchronized. The dynamical systems motor primitives (DMPs) representation fulfills all of the above requirements [14]. DMPs have been widely adopted in the robotics community, and are well-known for their use in imitation learning [21], [18]. The DMP framework represents the movements of the robot as a set of dynamical systems y ¨ =  z (z  - 2 (g - y ) -  - 1 y  ) + a -2 f (x, 1:N ) where y is a state variable, g is the corresponding goal state, and  is a time scale. The first set of terms represents a critically-damped linear system with constant coefficients z and z . The last term, with amplitude coefficient a = g - y0 , incorporates a shaping function f (x, 1:N ) =
N i=1 i (x)i x , N j =1 j (x)

where i (x) are Gaussian basis functions, and the weight parameters 1:N define the general shape of the movements. The weight parameters 1:N are straightforward to learn from a single human demonstration of a goal directed movement. The variable x is the state of a canonical system shared by all DoFs. The canonical system acts as a timer to synchronize the different movement components. It has the form x  = - x, where x0 = 1 at the beginning of the motion and thereafter decays towards zero. The metaparameters g , a, and  can be used to generalize the learned DMP to new situations. For example, the goal state g of the reaching movement is defined by the position of the object and the desired grasp. We explain how the DMP goal meta-parameters are computed for new objects in Sec. II-D. However, we need first to define how the finger trajectories can be encoded as DMPs, such that they generalize to new situations in a human-like manner. Representing and generalizing the motions of the fingers is a challenging task due to the high dimensionality of the finger-configuration space. A naive solution would be to assign one DMP to each joint [19]. However, as previously discussed in Sec. I-A, humans seem to generalize their movement trajectories within lower-dimensional spaces of the finger configurations, and not at the level of each joint independently [23], [20]. If the robot's generalization of the grasping action does not resemble the human's execution, implicit information contained within the human demonstrations is lost. Therefore, in order to facilitate behavioral cloning of human movements, the DMPs for multi-fingered hands should be realized in a lower dimensional space. In addition, overfitting is avoided by representing the movement in a lower-dimensional space. In particular, the DMPs can be defined in the latent spaces learned in Sec. II-A. As such spaces are learned from complete trajectories of the grasping movements, they also include the finger configurations needed for representing

the hand during the approach and preshaping phases of the action, as well as the final grasps [20]. We use a DMP for each of the latent space dimensions as well as DMPs for the wrist position and orientation. The weight parameters for these DMPs can be learned from human demonstrations by first projecting the tracked motions into the latent space and subsequently learning the weights from the resulting trajectory. Thus, the same data that is used to learn the latent space can be reused for learning the weight parameters. The resulting latent-space DMPs, as well as the reaching movement's DMPs, are linked to the same canonical system, thus, ensuring that they remain synchronized. The output of the latent-space DMPs is afterwards mapped back into the high-dimensional joint space by the PCA projection. In this manner, the grasping action can be executed seamlessly, and the robot can begin closing its fingers before the hand has reached its final position. Thus, we have defined a human-like representation of the grasping movements that can be acquired by imitation learning. Given this DMP representation, the robot still needs to determine the meta-parameters for new situations. This process is described in the next section. D. Estimating the Goal Parameters In order to generalize the latent-space DMPs to new objects, we need to estimate the goal state g for each latentspace dimension, as well as the orientation of the hand for a new set of contact points which we have acquired from contact warping as discussed in Sec. II-B. We use one contact-point per finger, where the contact point is always located at the finger tip. Each point is specified in Cartesian coordinates. As we have four fingers, this results into a 12-dimensional task space vector xC . Additionally, we also want to estimate the position and orientation of the hand in the world coordinate frame. We therefore add six virtual joints v, i.e., three translational and three rotational joints. We will denote the transformation matrix, which is defined by these six virtual joints, as T(v). We define the finger tip position vector x1:4 as the concatenation of all four finger tip positions. This vector is a function of the transformation matrix T(v) and the joint configurations of the fingers q = m + Kg, i.e., W (y) = T(v)H (m + Kg). The vector m represents the mean of the PCA transformation and K is given by the first five eigenvectors. The function H (q) calculates the finger tip-positions in the local hand coordinate frame. This setup is an inverse kinematics problem with the difference that we want to optimize the joint positions of the fingers in the latent space instead of directly optimizing the joint positions q. Thus, the inverse kinematics problem is over-constrained as we have twelve task variables and only eleven degrees of freedom. Therefore, instead of the standard Jacobian pseudo-inverse solution, we need to employ a different approach. Our task is to estimate the optimal configuration y =  [v , g ] of the hand, which consists of the orientation and

the latent space coordinates, such that the squared distance between finger-tip positions x1:4 and the contact points xC is minimal, i.e., y L( y ) = = [v , g ] = argmaxy L(y), -(W (y) - xC )T C-1 (W (y) - xC ) (7) +yT Wy.
Fig. 6. The Justin robot executes a reach-and-grasp movement in simulation. Using the trained LS-DMP a new trajectory (red) to the target object is synthesized. The optimal hand position and orientation (shown as a coordinate system) is estimated along with the optimal hand shape in latent-space.

The matrix W = diag(w) defines a damping or regularization term for the step-size of y, and C = diag(c) defines the inverse precision for each task variable. The Jacobian J =  W / y for this problem can be obtained straightforwardly, i.e., for the derivation w.r.t v it is given by the standard geometric Jacobian and for the derivation w.r.t the latent variable g it is given by Jl = Jq K, where Jq denotes the geometric Jacobian. We will solve the optimization problem given in Equation (7) by iteratively applying a least squares solution. Given the current hand configuration yk and the desired finger-tip positions xC , the update step for the hand configuration is therefore given by yk = (JT CJ + W )-1 JT C (xC - W (yk )) . (8)

As we have to solve an overconstrained inverse kinematics setting, in contrast to the more common underconstrained inverse kinematics setting, we use the left-pseudo inverse in Equation (8). This update equation also corresponds to a Bayesian view on inverse kinematics [29]. We repeat the update until convergence in order to get the optimal hand configuration y . We always start our optimization from an initial posture where the hand is pointing downwards. III. S ETUP AND E VALUATIONS To evaluate the proposed approach, we conducted a set of experiments using the Rollin' Justin robot platform [5]. Justin is a mobile humanoid robot system with an upperbody including 43 actuated degrees-of-freedom (DoF). In our experiments we controlled 22 DoF pertaining to the Torso (3 DoF), the right arm (7 DoF), and the four-fingered right hand (12 DoF). The experiments were performed both in simulation and on the physical robot. A. Simulation Results In the first experiment, we evaluated the performance and the results of our approach in a simulated environment for the Justin robot. As explained in Sec. II, we trained individual LS-DMPs for each of the principal components of the demonstrated reach-and-grasp movement. Fig. 5 shows the latent-space trajectories for three out of the five principal components of the hand shape. The example trajectories are depicted in blue, while the trajectory learned by the LSDMP is depicted in red. This figure reveals an interesting insight into the nature of the recorded human reach-and-grasp movements: many of the example trajectories have a distinct sigmoid shape that has a bell-shaped velocity profile. This insight corresponds to the results in [1] , which showed that

humans perform point-to-point reaching movements such that the velocity profile along the path can be characterized by a symmetric bell-shape. Our results indicate that a similar property holds for the latent space trajectories of the hand shape during a reach and grasp. After learning, we first executed the LS-DMP in simulation. Fig. 6 shows the start and end configuration during one run of the algorithm. The red curve depicts the trajectory of the hand as generated by the LS-DMP, while the displayed coordinate system shows the estimated hand orientation of the robot. To evaluate the accuracy of the produced grasping motions, we repeatedly changed the position and orientation of the target object and measured the distance between the warped contact points on the object and the fingertip positions. Ideally, the fingertips should always coincide with the contact points. Tab. I shows the average distance of the fingers to the warped contact points after executing a reachand-grasp movement. We also varied the grasp spaces in order to evaluate the effect of the grasp type on the the resulting hand shape. The grasp space indicated by Multi in Tab. I was learned using all available human demonstrations. This grasp space encompasses a wide range of variations of the human hand. As can be seen in the table, we achieved the most accurate results by using this grasp space. In this case, the average error is about 7mm. It should be noted that the fingers of the robot are much larger than human fingers and have a width of about 3cm. Given the size of the robot's fingers, the produced error only corresponds to about a quarter of the finger width. The table clearly shows that changing the grasp type results in higher average error. This increased error is to be expected, as we constrained the space of possible solutions to a specific grasp type. At the same time, visual inspection of the resulting grasps shows that this error does not deteriorate the quality of the resulting grasps, as will be seen in the next section. B. Real Robot Experiments We also conducted experiments with the real Rollin' Justin robot. Three different types of mugs were used during the experiments. After placing a mug on a table in front of the robot, all information about the pose of the mug

2

2

2

1.5

1.5

1.5

Eigenvector 1

Eigenvector 3

1

1

Eigenvector 5
0 0.2 0.4 0.6 0.8 1

1

0.5

0.5

0.5

0

0

0

-0.5

0

0.2

0.4

0.6

0.8

1

-0.5

-0.5

0

0.2

0.4

0.6

0.8

1

Time

Time

Time

Fig. 5. The plots show example trajectories (blue), and the mean trajectory (red), for three (out of five) latent space dimensions during the closing of the hand. The trajectories have been shifted and scaled to start at zero and end at one, in order to allow for easier comparison of their shapes. As the scaled trajectories have similar shapes, they can be represented by individual DMPs and easily learned from human demonstrations.

TABLE I AVERAGE DISTANCE BETWEEN WARPED CONTACT POINTS AND FINGERTIPS AFTER GRASPING . Grasp Type Avg. Error (m) Multi 0.007 Tripod 0.013 Surrounding 0.0157 Lateral 0.014
Human

Tripod

Surrounding

Lateral

was estimated using a Kinect camera and the techniques explained in [12]. Subsequently, using the contact warping techniques from Sec. II-B the contact points from a known mug were warped onto the currently seen mug. The resulting contact points were subsequently fed into the optimizer to estimate all parameters that are needed to execute the reachand-grasp movement. The estimation of all parameters using the algorithm in Sec. II-D takes about one to five seconds. We performed about 20 repetitions of this experiment with the different mugs placed at various positions and heights. Additionally, we included a lifting-up motion to our movement, in order to evaluate whether the resulting grasp was stable or not. In all of the repetitions the robot was able to successfully grasp and lift-up the observed object. Furthermore, we also executed the reach-and-grasp movements using grasp spaces belonging to different grasp types. No change was made to the structure or other parameters of the algorithm. The only difference between each execution run was the grasp space to be loaded. Fig. 7 shows three of the grasp types used in our taxonomy along with the result of applying them to the Justin robot. The figure clearly shows that changing the grasp type can have a significant effect on the appearance of the executed grasp. For example, we can see that the use of the tripod-grasp results in delicate grasps with little finger opposition, while surrounding grasps lead to more caging grasps with various finger oppositions. Our approach exploits the redundancy in hand configurations and allows desired grasp types to be set according to the requirements of the manipulation task that is going to be executed. Fig. 8 shows a sequence of pictures captured from one of the reach-and-grasp movements executed on the real robot. Reach-and-grasp movements for different grasp types and situations are shown in the video submitted as supplemental material.

Fig. 7. The three grasp types lateral, surrounding, and tripod from our taxonomy are demonstrated by a human and later reproduced by the Justin robot. All parameters of the reach-and-grasp movement, such as the shape of the hand, its position, and orientation are automatically determined using latent space dynamic motor primitives.

Robot

IV. C ONCLUSION In this paper, we presented a new approach for imitation and generalization of human grasping skills for multifingered robots. The approach is fully data-driven and learns from human demonstrations. As a result, it can be used to easily program new grasp types into a robot ­ the user only needs to perform a set of example grasps. In addition to stable grasps on the object, this approach also leads to visually appealing hand configurations of the robot. Contact points from a known object are processed by a contact warping technique in order to estimate good contact points on a new object. We, furthermore, presented latent-space dynamic motor primitives as an extension to dynamic motor primitives that explicitly models synergies between different body parts. This significantly reduces the number of parameters needed to control systems with many DoF such as the human hand. Additionally, we have presented a principled optimization scheme that exploits the low-dimensional grasp spaces to estimate all parameters of the reach and grasp movement.

Fig. 8. A sequence of images showing the execution of a reach-and-grasp movement by the Justin humanoid robot. The executed latent-space dynamic motor primitive was learned by imitation. The type of the grasp to be executed can be varied according to the requirements of the task to subsequently executed. New grasp types can be trained within minutes by recording a new set of human demonstrations.

The proposed methods were evaluated both in simulation and on the real Justin robot. The experiments exhibited the robustness of the approach with respect to changes in the environment. In all of the experiments on the real, physical robot, the method successfully generated reach-and-grasp movements for lifting up the seen object. ACKNOWLEDGMENT We thank Florian Schmidt and Christoph Borst from the DLR - German Aerospace Center for their help with the Justin robot and for their valuable comments and suggestions. H. Ben Amor was supported by a grant from the Daimlerund-Benz Foundation. The project receives funding from the European Community's Seventh Framework Programme under grant agreement n ICT- 248273 GeRT. R EFERENCES
[1] W. Abend, E. Bizzi, and P. Morasso. Human arm trajectory formation. Brain : a journal of neurology, 105(Pt 2):331­348, jun 1982. [2] M. Arbib, T. Iberall, and D. Lyons. Coordinated control programs for movements of the hand. Experimental brain research, pages 111­129, 1985. [3] H. Ben Amor. Imitation learning of motor skills for synthetic humanoids. PhD Thesis, Technische Universitaet Bergakademie Freiberg, Freiberg, Germany, 2011. [4] H. Ben Amor, G. Heumer, B. Jung, and A. Vitzthum. Grasp synthesis from low-dimensional probabilistic grasp models. Comput. Animat. Virtual Worlds, 19(3-4):445­454, sep 2008. [5] C. Borst, T. Wimbock, F. Schmidt, M. Fuchs, B. Brunner, F. Zacharias, P. R. Giordano, R. Konietschke, W. Sepp, S. Fuchs, C. Rink, A. AlbuSchaffer, and G. Hirzinger. Rollin' justin - mobile platform with variable base. In Robotics and Automation, 2009. ICRA '09. IEEE International Conference on, pages 1597 ­1598, may 2009. [6] A. Boularias, O. Kroemer, and J. Peters. Learning robot grasping from 3-d images with markov random fields. In 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2011, pages 1548­1553, 2011. [7] S Chieffi and M Gentilucci. Coordination between the transport and the grasp components during prehension movements. Experimental Brain Research, pages 471­477, 1993. [8] M. T. Ciocarlie and P. K. Allen. Hand posture subspaces for dexterous robotic grasping. Int. J. Rob. Res., 28(7):851­867, July 2009. [9] K. Dautenhahn and C. L. Nehaniv. Imitation in Animals and Artifacts. MIT Press, Campridge, 2002. [10] G. Heumer. Simulation, Erfassung und Analyse direkter Objektmanipulationen in virtuellen Umgebungen. PhD Thesis, Technische Universitaet Bergakademie Freiberg, Freiberg, Germany, 2011. [11] U. Hillenbrand. Non-parametric 3d shape warping. In Pattern Recognition (ICPR), 2010 20th International Conference on, pages 2656 ­2659, 2010.

[12] U. Hillenbrand and A. Fuchs. An experimental study of four variants of pose clustering from dense range data. Computer Vision and Image Understanding, 115(10):1427 ­ 1448, 2011. [13] U. Hillenbrand and M. A. Roa. Transferring functional grasps through contact warping and local replanning. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS, 2012. [14] A.J. Ijspeert, J. Nakanishi, and S. Schaal. Movement imitation with nonlinear dynamical systems in humanoid robots. In Robotics and Automation, 2002. Proceedings. ICRA '02. IEEE International Conference on, volume 2, pages 1398 ­1403, 2002. [15] M Jeannerod. The timing of natural prehension movements. Journal of Motor Behavior, 16(3):235­254, 1984. [16] M. Jeannerod. Perspectives of Motor Behaviour and Its Neural Basis, chapter Grasping Objects: The Hand as a Pattern Recognition Device. 1997. [17] M. Jeannerod. Sensorimotor Control of Grasping: Physiology and Pathophysiology, chapter The study of hand movements during grasping. A historical perspective. Cambridge University Press, 2009. [18] J. Kober, B. Mohler, and J. Peters. Learning perceptual coupling for motor primitives. In Intelligent Robots and Systems, 2008. IROS 2008. IEEE/RSJ International Conference on, pages 834 ­839, sept. 2008. [19] R. N. Lemon. Neural control of dexterity: what has been achieved? Exp Brain Res, 128:6­12+, 1999. [20] C. R. Mason, J. E. Gomez, and T. J. Ebner. Hand Synergies During Reach-to-Grasp. Journal of Neurophysiology, 86(6):2896­ 2910, December 2001. [21] J. Nakanishi, J. Morimoto, G. Endo, G. Cheng, S. Schaal, and M. Kawato. Learning from demonstration and adaptation of biped locomotion. Robotics and Autonomous Systems, 47:79?­91, 2004. [22] M. Saleh, K. Takahashi, and N.G. Hatsopoulos. Encoding of coordinated reach and grasp trajectories in primary motor cortex. J Neurosci, 32(4):1220­32, 2012. [23] M. Santello, M. Flanders, and J. F. Soechting. Postural Hand Synergies for Tool Use. The Journal of Neuroscience, 18(23):10105­10115, December 1998. [24] M. Santello and J. F. Soechting. Gradual molding of the hand to object contours. Journal of neurophysiology, 79(3):1307­1320, March 1998. [25] A. Saxena, J. Driemeyer, and A. Y. Ng. Robotic grasping of novel objects using vision. Int. J. Rob. Res., 27(2):157­173, feb 2008. [26] M H Schieber. How might the motor cortex individuate movements? Trends Neurosci, 13(11):440­5, 1990. [27] R. Su´ arez, M. Roa, and J. Cornella. Grasp quality measures. Technical report, Technical University of Catalonia, 2006. [28] Johan Tegin, Staffan Ekvall, Danica Kragic, Jan Wikander, and Boyko Iliev. Demonstration-based learning and control for automatic grasping. Intelligent Service Robotics, 2009. [29] M. Toussaint and C. Goerick. A bayesian view on motor control and planning. In From Motor Learning to Interaction Learning in Robots, pages 227­252. 2010. [30] R. J. Vanderbei. Linear Programming: Foundations and Extensions. Springer, 2001.

In this paper we propose an approach to robot grasp prioritization based on a combined arm-and-hand metric. Most traditional approaches evaluate grasps based on hand-centric metrics such as force-closure, finger spread, contact surface area and similar measures. While these are certainly important factors to predict the robustness of a grasp, they do not carry information on the feasibility of the reaching action needed to execute the grasp. Based on our observations of physical pick-up experiments, we suggest that the execution success of a pick-up task is partially dependant on the easiness of the reaching movement. We present our metric, which combines 2 measures involving arm-kinematics and an existing hand heuristic metric. Results of simulated as well as physical experiments in our robot, Crichton, are presented.Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/303802736

Experience-basedTorqueEstimationforan IndustrialRobot
ConferencePaper·May2016
DOI:10.1109/ICRA.2016.7487127

CITATIONS

READS

0
5authors,including: ErikBerger TechnischeUniversitätBergakademieFreiberg
18PUBLICATIONS52CITATIONS
SEEPROFILE

62

DavidVogt TechnischeUniversitätBergakademieFreiberg
20PUBLICATIONS86CITATIONS
SEEPROFILE

HeniBenAmor ArizonaStateUniversity
56PUBLICATIONS434CITATIONS
SEEPROFILE

Someoftheauthorsofthispublicationarealsoworkingontheserelatedprojects:

Mining-RoX:AutonomousRobotsinUndergroundMiningViewproject

AllcontentfollowingthispagewasuploadedbyErikBergeron04June2016.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Experience-based Torque Estimation for an Industrial Robot
Erik Berger1 , Steve Grehl1 , David Vogt1 , Bernhard Jung1 , Heni Ben Amor2

Abstract-- Robotic manipulation tasks often require the control of forces and torques exerted on external objects. This paper presents a machine learning approach for estimating forces when no force sensors are present on the robot platform. In the training phase, the robot executes the desired manipulation tasks under controlled conditions with systematically varied parameter sets. All internal sensor data, in the presented case from more than 100 sensors, as well as the force exerted by the robot are recorded. Using Transfer Entropy, a statistical model is learned that identifies the subset of sensors relevant for torque estimation in the given task. At runtime, the model is used to accurately estimate the torques exerted during manipulations of the demonstrated kind. The feasibility of the approach is shown in a setting where a robotic manipulator operates a torque wrench to fasten a screw nut. Torque estimates with an accuracy of well below ±1 N m are achieved. A strength of the presented model is that no prior knowledge of the robot's kinematics, mass distribution or sensor instrumentation is required.

I. I NTRODUCTION Physical interaction between a robot and its environment requires accurate approaches for measuring and assessing forces applied by the robot to external objects and vice versa. For example, in human-robot interaction scenarios, exchanged forces may indicate unwanted collisions or result from intended human guidance. Accurate measurement of external forces is also important in manipulation tasks involving tool-usage. Humans often rely on sensory stimuli in order to estimate the state of a manipulation process, e.g., how hard a screw has been tightened. This paper addresses the problem of measuring forces during a manipulation task. Force-torque (FT) sensors are often the first choice for measuring the forces exerted in interactions with external objects. However, these sensors are limited to a specific location. The spatial distribution and cost of these sensors therefore need to be carefully balanced. In dynamic tasks, such as manipulation tasks, it may also become challenging to distinguish between different kinds of forces such as, for instance, forces caused by the task, sensor noise, or external perturbations of the current behavior. Furthermore, modern robots, such as the UR5 robotic arm, come with built-in, purely software-based capabilities for force sensing utilizing a mass-acceleration model. However, such methods require detailed knowledge about the kinematics and mass distribution of the robot. A drawback of such a method is that it quickly deteriorates in estimation accuracy when the model is imprecise, e.g. due to additionally attached equipment. In particluar, when the
1 Institute of Computer Science, Technical University Bergakademie Freiberg, Bernhard-von-Cotta-Str. 2, 09599 Freiberg, Germany 2 School of Computing, Informatics and, Decision Systems Engineering, Arizona State University, 699 S Mill Ave, Tempe, AZ 85281, USA

Fig. 1. A 3-finger gripper mounted on an UR5 robotic arm is utilizing a usual torque wrench.

robot is extended by a gripper or a tool, the mass distribution needs to be re-calibrated accurately. In case of the UR5 robot, the manufacturer specifies a force accuracy of ±25 N for the robot's tool center point (TCP). Also, the manufacturer warns that such a kind of force estimation provides no protection against momentum. In contrast to the approaches mentioned above, and motivated by the ability of a trained mechanic to estimate a screw nut's tightening torque from prior experience, this paper proposes an experience-based approach that does not require prior information about the robot platform. Task models are learned during a training phase based on the available sensor data, without relying on any further knowledge, such as mass distribution or robot kinematics. During runtime, expected and measured sensor values are compared and detected discrepancies are turned into force estimates. A specific manipulation scenario is investigated, in which a UR5 learns to adjust a screw nut using a torque wrench. Figure 1 shows the principle setup which is elaborated below.

II. R ELATED W ORK Robots that engage in physical interactions with humans and objects need to regulate the forces exchanged with their

Data Acquisition
Time Torque Sensor Data

Model Generation
Phase Feature Space Torque Feature Space

Feature Space Projection
Phase Estimation

Realtime Sensor Data

Torque Estimation

Fig. 2. An overview of the presented machine learning approach. During an offline training phase, sensor data together with information about the actual time and torque is recorded (section III-A) to generate two low-dimensional Feature Spaces (section III-B). In order to estimate the actual torque, realtime sensor data is projected into this spaces and compared with the training data (section III-C). The most similar matching is used as estimation of the actual torque.

environment. This requires methods for estimating the occurring forces as well as methods for adapting the robot behavior accordingly. Recent developments in compliant control have lead to the emergence of robots with joint torque sensing and feedback control [1]. For measuring external forces and perturbations, however, typically additional force sensors, e.g. force-torque sensors are used. Such sensors are often expensive, add weight to the robot, and are limited in their spatial resolution. Hence, various authors have suggested using algorithmic approaches for inferring applied forces. In [2], a depth camera is used in order to estimate applied forces. Using a depth camera allows for generic contact locations on the robot. In [3] machine learning methods are used to extract an inverse dynamics model for a cabledriven robot manipulator. Measuring the difference between predicted controls from the inverse dynamics model and the executed controls provides an estimate for external forces applied on the robot. A major challenge for such approaches however, is training an inverse dynamics model that is general enough to be applied to different situations. Using machine learning for force-based robot control has also been suggested a number of other publications. In [4] a robot learns to adapt its motion by anticipating human intentions from force measurements only. In [5], a method for learning force-based manipulation skills from demonstrations was presented. The approach generated variable-impedance control strategies thereby producing the necessary compliance for handling deformable objects. The work presented in the remainder of our paper focuses on different aspect: how can a robot learn to estimate forces from experience? In contrast to earlier discussed papers, we learn behavior-specific models for force estimation, that are narrower in scope, but accurate in results. III. A PPROACH An overview of the presented approach is shown in Figure 2, while the setup is further explained in Figure 3. The physical interaction considered is the tightening of a screw nut by using a wrench. Hence, the torque results in a preload force which is countered by a pressure spring. To secure a

Fig. 3. A 3-finger gripper mounted on a UR5 robotic arm turns a usual torque wrench about 45 . The applied torque (black) results in a preload force (blue) which is countered by the force conducted exerted by a pressure spring (green).

given tightening torque, a 3-fingered gripper mounted on a robotic arm is used to adjust the torque utilizing a custom torque wrench. The goal is to generate a behavior-specific model, which is able to estimate the torque from previous experience. The first step of the presented approach is to record example data representing the evolution of sensor values for different torque values. In the training phase, the values of all sensors available on the robot together with the actually exerted torque and time stamps are recorded during a 45 tightening movement. This is performed multiple times for different preconfigured torques. The recorded data is used to generate a model consisting of two components, the PhaseFeature Space (P-FS), and the Torque-Feature Space (TFS). These components are low-dimensional embeddings of the original high-dimensional data, that are extracted using Transfer Entropy [6] (TE) and Principal Component Analysis (PCA). During task execution, the P-FS is used to identify the phase of the behavior while the T-FS estimates the

applied torque based on the selected phase. To this end, the real-time sensor data is compared to sensor readings acquired during the training phase. Finally, the tightening torque is estimated, by determining the training data point with the highest similarity to the actual sensor readings. In the following, each step of the presented approach will be explained in more detail. A. Data Acquisition Similar to a skilled mechanic who is able to estimate a screw nuts torque from previous experience, the robot needs training data of the behavior. The realtime interface of the UR5 provides overall 105 different sensors, containing less useful information, e.g., the actual mainboard voltage and redundant data as control, target and actual joint values. However, the goal is to identify the sensors which are most significant for estimating the behavior phase and the exerted torque. For this, the m = 105 sensor values r = (r1 , . . . , rm ), are recorded during behavior execution. Additionally, the relative time w and the preconfigured torque v are recorded and stored in a m + 2 dimensional vector s = (r, w, v ). In contrast to the torque v , the relative time w increases during behavior execution and is reset after. Consequently, as the time for each training phase remains the same, w contains always the same data for each single recording. However, one behavior execution is recorded for two seconds with 125 Hz. The resulting training data S = (s1 ; . . . ; sn ) represents the training data for one possible torque v consisting of n = 250 equidistant samples. To estimate the tightening torque, multiple behavior executions for varying configurations are recorded. First, the screw nut was tightened with 6.0 N m and equidistantly increased by 1.0 N m up to 15.0 N m. In the following, the recorded training data D = (S1 ; . . . ; Sk ) for k = 10 different torque configurations is used to estimate the torque. To achieve a higher level of accuracy without resulting in a time consuming training phase, virtual training sets are interpolated. For this, Dynamic Mode Decomposition (DMD) [7] is utilized . A detailed explanation of DMD and how it is used in the context of sensor data interpolation can be found in [8]. B. Model Generation The recorded data D is investigated in order to extract relevant features. For this, two relevance values o = (o1 , . . . , om )T and p = (p1 , . . . , pm )T are computed for each sensor. On the one hand o describes how strong the sensor is influenced by the preconfigured torque v = D·,m+2 and on the other hand p describes how strong a sensor is influenced by the relative time w = D·,m+1 . The main idea is that sensor with a high TE are beneficial for estimating the corresponding value. In previous work [9], TE has been used to solve related tasks for perturbation detection during human-robot interaction. In the present work, TE is used as a measure of predictability and information flow between the relative time or torque and the evolution of sensor readings.

o[33...50] p[33...50]

Angle Sensors

Velocity Sensors

Current Sensors

Fig. 4. The Transfer Entropy for the actual angle, velocity and current sensors is calculated for the torque applied to the screw nut (blue) and the relative time (orange).

6Nm

10Nm

15Nm

Ampere Radian

4 3 2 1

-1.3 -1.4 -1.5 -1.6 0 125 250

Timestep
Fig. 5. The peak TE sensor streams. Top: Different torques result in varying sensor readings for the current sensor. Bottom: The angle sensor is not affected by the torque but increases over time.

The main idea is that sensors with a high TE w.r.t. the robot's behavior are deemed more influential and relevant. The formula for the TE between j and i is defined as
|i|-1

T E (j, i) =
t=1

p(it+1 , it , jt )log2

p(it+1 |it , jt ) , p(it+1 |it )

(1)

where the function p describes the conditional probability. Utilizing Formula 1 the TE between the relative time and the sensors p(q) and for the torque and the sensor o(q) is calculated by p(q ) = T E (w, D·,q ) o(q ) = T E (v, D·,q ), (2)

where q  [1 . . . m]. Figure 4 shows the resulting TE for the actual angle, velocity and current sensors contained in p33...50 and o33...50 . For o the highest TE values was found for the current sensors. The sensor with the highest TE o49 is visualized in the top of Figure 5. Obviously, the sensor stream differs for varying torques. The angle and velocity sensor share likewise less TE. Though, for p the TE is almost completely contained in two angle sensors. The sensor with the peak TE o35 is visualized in the bottom of Figure 5. As can be seen, the sensor stream is nearly the same for

different training data. Hence, independent from the applied torque, the sensor values are increasing over time. Therefore, the sensor is suitable for estimating the actual phase of the motor skill. Next, the sensors with at least 90% of the overall TE are used to select a subset of sensors from D in order to build a feature space with
^  : R m  Rm

(3)

where m ^  m. The threshold is determined empirically but can be changed in order to adapt the computational effort. However, sensors which are not influenced by the relative time are ignored during phase estimation while sensor not related to the torque are ignored during torque estimation. Finally, the dimensionality of the feature spaces is reduced even further by applying well-known PCA. During the further procedure, the low dimensional embedding of  (o) is used for the phase estimation and therefore is called P-FS. In contrast to that, the dimensional reduced  (p) is used to predict the torque and is called T-FS. C. Phase Estimation Due to small time shifts, occurring between multiple executions of a motor skill, the relative time is not sufficient to estimate the current phase of a behavior. Because of that, in contrast to the training data, the realtime data does not contain information about the relative time. Instead, the current phase is estimated from the P-FS which among other sensors is strongly affected from joint positions. For the estimation of the current phase of the behavior a time window with size t of the actual sensor stream is projected into the P-FS resulting in X = (x1 , . . . , xt ). In previous work [8], the Subsequence Dynamic Time Warping technique (SDTW) [10] was used for measuring the similarity between two sequences. In the case presented here, the low dimensional sequence X is compared with the low dimensional training data in the rows of the P-FS   y1,1 · · · y1,n  .  . .. . Y= . (4) . . .  yk,1 ··· yk,n where n = 250, k = 10, n  t and y is the low dimensional projection of a single time step. Since the UR5 provides equidistant sensor readings, the optimization problem of the SDTW is reduced to finding the optimal path p = (b , . . . , b + t - 1), where b is given by
t

are almost the same. This effect is utilized in order to further decrease the computational effort. For this, the training data in the first row k = 1 of Y is compared with X by applying Formula 5. The resulting path p 1 represents the current phase of the behavior and the last element in p 1 the estimated actual state of the robot. Due to the similarity of the phases, the search space for all other rows in Y is reduced by applying a hill climbing approach. The starting point of the search space for the remaining training sets is at b  p 1 . By applying Formula 5, the hill climbing method is searching the neighbors of b and stops when no reduced costs can be found. Since p 1 contains the global minimum it is assumed  that, due to their similarity, p 2 , . . . , pk only contain global  minima. Finally, all phases are stored in P = (p 1 , . . . , pk ). D. Torque Estimation In the following, the torque is estimated based on the previous phase estimation. By projecting the current time ^ = (x window into the T-FS resulting in X ^1 , . . . , x ^t ) the data can be compared to the low dimensional training data   y ^1,1 · · · y ^1,n  . . .. ^ = . (6) Y  . . . .  y ^k,1 ··· y ^k,n where n = 250, k = 10 and n  t and y ^ is the low dimensional projection of a single timestep. Comparing the ^ is time consuming. time window with the complete matrix Y In order to reduce the computational effort the comparison is shrunk to the actual phases P calculated in the previous section. This reduces the number of computations from (n - t) · k to k . In order to estimate the torque, the training ^ data l with the highest correspondence to the actual data X is calculated by
t

l = argmin
l[1:k] i=1

c(^ xi , y ^l,P (k,i) )

(7)

b = argmin
b[0:(n-t)] i=1

c(xi , yb+i )

(5)

and c is a local distance measure, which in our case is the Euclidean distance c = |x - y|. In contrast to SDTW no accumulated cost matrix or warping need to be computed what results in significant less computational effort. As illustrated in the bottom of Figure 5, an advantage of the presented phase estimation method is that the phases between the k recordings are just slightly shifted and therefore

where l  [1 . . . k ]. The preconfigured torque recorded during execution of the training data vl is used as estimation of the actual one. Since, the robot performs the movement during the estimation phase, the torque continuous increases. This makes a realtime approach necessary. As mentioned above, the training data is interpolated which effectivley increases the number of datasets k . In order to further decrease the computational demands, the previous mentioned hill climbing method is utilized. Instead of computing k possible matches the algorithm has a maximal computation time in which it searches l from random start positions in l. This time is set to a value less than the data rate provided by the robot, which in the case of the UR5 is 125 Hz. This ensures that the robot always know its actual state and is able to stop when a certain torque is reached. Obviously, the computational demands depend to the size of the time window t. However, in the following experiments 8 ms are sufficient for estimating the torque utilizing the introduced method.

2

Relative Time in [s]

P-FS ¬P-FS P-FS Actual Phase

TABLE I T HE MEAN ABSOLUTE ERRORS IN N m RESULTING FROM DIFFERENT
FEATURE SPACES FOR DIFFERENT SIGNAL - TO - NOISE RATIOS .

T HE

ERROR SIGNAL WAS GENERATED UTILIZING WHITE NOISE .

SN R
1

T-FS MAE 0.0 0.0 0.0 0.01 0.07

¬T-FS MAE 1.05 1.34 1.42 1.74 2.51

T-FS MAE 0.53 0.89 1.03 1.18 1.75

20 10 5 2.5 1.0
11

25

125

250

Timestep
Fig. 6. Phase estimation during behavior execution utilizing different sensor selection schemes. P-FS is generated from the sensors with 90% of the overall Transfer Entropy (orange) while ¬P-FS is generated from the remaining sensors (blue) and P-FS from all sensors (green). As can be seen, only P-FS is able to predict the actual phase (black) accurately.

Estimated Torque Configured Torque

Torque in [Nm]

10

IV. E XPERIMENTS To validate the proposed method different experiments are conducted. Therefore ten training data sets are recorded. For a more accurate torque estimation, the training data sets are further interpolated after feature extraction. The resulting data set contains 901 equidistant samples for torques in between [6.0 N m . . . 15.0 N m]. The sensors selection process is evaluated and the reliability is proven by investigating the absolute error. Consequently, the accuracy of the presented results is 0.01 N m. A. Feature Selection Evaluation The evaluation of the P-FS is done by comparing it to all sensors P-FS and the negation of it ¬P-FS. The torque is randomly chosen and low-dimensional projections of the last 25 recordings is provided. Figure 6 shows the resulting phase estimation for the three different groups. As presumed, the original P-FS is able to estimate the current phase accurately while both other groups fail at certain time steps. Although, there is nearly no difference between the result of ¬P-FS and P-FS. This is due to the fact that suitable sensors form a small subsets, identified by the introduced feature selection. This shows that the resulting feature space P-FS is suitable, to estimate the current phase of the behavior. Next, the quality of the selected sensors for the torque estimation is evaluated. Therefore, the data set for a torque of 8.0 N m is selected from the training data. In order to prove the robustness of the T-FS it is disturbed with white noise of the following signal-to-noise ratio: SN R =
2 Y ^
k,n

9

0

Timestep

125

250

Fig. 7. The torque estimation during realtime behavior execution. The screw nut is preconfigured with 9.6 N m (green). The torque estimation (blue) fails at the beginning because of the small size of the captured realtime data. After 0.2 s (highlighted area) the estimation gets close to the correct value. The accuracy of the estimation directly depends to the size of the recorded realtime data and gets close to the real torque after 2 s.

mations. That proofs that the selected sensors are a suitable choice for torque estimation. B. Realtime Torque Estimation In the final experiment a torque of 9.6 N m was set. The goal is to identify this torque as fast as possible in order to avoid an additional tightening. As mentioned the realtime interface of the UR5 provides 105 different sensors 125 times per second. Each sample is first projected into the P-FS and appended to the previous ones. For the P-FS the size of this low-dimensional segment is set to a size of 25 values what, as illustrated in Figure 6, results in accurate phase estimation results. However, as illustrated in Figure 6 before the segment contains 25 elements, no phase estimation is solved. Instead, the relative time is used in order to allow at least a approximate torque estimation. Utilizing the actual phase, the T-FS is used to estimate the actual torque by comparing the realtime projection with the training data. As can be seen in Figure 7 the torque estimation fails at the beginning, due to the small size of the projected segment. In contrast to the estimation of the phase, the torque estimation accuracy is increased by using larger segments. In more detail, the size of the low-dimensional projection window is not restricted. As can be seen in Figure 7 after 0.2 s the estimated torque matches the preconfigured one. In order to

2 noise

,

(8)

where  is the standard deviation. As done previously T-FS and ¬T-FS are generated in order to evaluate the selected sensors in the T-FS. Table I shows the resulting mean absolute errors for different levels of noise. Similar to the phase estimation, the T-FS produces the best results while ¬T-FS and T-FS result in incorrect esti-

avoid an additional tightening of the screw nut, the robot may stop the behavior execution or perform a reverse motion to adjust the screw nut to the initial value. However, after 2 s the torque was estimated with sufficient accuracy. This confirms the assumption, that the generated feature spaces can be applied in order to estimate the actual torque from previous experience. In the following section, the experimental results are discussed. C. Discussion As can be seen in Figure 6 the accuracy of the torque estimation is time dependent. This comes from the size and the actual values within the recorded time window. From time step 5 to about 60 the estimation overshoots the correct torque and converges slowly afterwards. This is due to the quality of the data recorded during the training phase. Taking into account Figure 5 top, one can see that the data model up to time step 40 contains overlapping sensor data. This leads to a less accurate model for this subsequence of the behavior. Past time step 40 the sensor data is clearly separated and therefore has a better estimation quality. Consequently, as the behavior continues, the phase shifts to a region with higher accuracy and improves the overall estimation results. The overall estimation converges slowly, since the time window still contains the less accurate data from the beginning of the estimation. Taking a closer look at Figure 6, the offset after 0.5 s is 0.3 N m and after 2.0 s still about 0.1 N m. The length of the torque wrench from the original tool center point is exactly .3 N m 0.23 m. This results in an accuracy of 1.3 N = 0 0.23 m after 0.1 N m 0.5 s and 0.43 N = 0.23 m after 2.0 s. Thus, the proposed model learning approach is more accurate than the mentioned force accuracy of ±25 N specified by the manufacturer. A limitation of the approach is that the robot needs to be configured exactly the same way between offline training and realtime estimation. However, in industrial settings this usually is the case. V. C ONCLUSION In order to provide a robot with a sense of force, an approach for torque estimation from prior experience was presented. Instead of using dedicated force sensors, data from all sensors available on the robot is recorded, including e.g. angle, velocity and current sensors. Using Transfer Entropy, a model consisting of two feature spaces is learned from the recorded sensor data. During runtime, sensor data is captured and projected into these feature spaces. The first

feature space is used to estimate the actual phase of the motor skill by comparing realtime and training data. The second feature space estimates the actual torque, again by comparing realtime with training data. By taking into account the previously calculated phase estimate, the computational effort for torque estimation is reduced. As explained in the discussion, the quality of the extracted models varies over time. In some cases, this could lead to TE values that vary for different phases of the behavior. Thus, a phase-dependent selection of the optimal subset of samples could be a beneficial extension of the proposed approach in order to increase the overall accuracy of the torque estimate. Nonetheless, with the present implementation torques were accurately estimated, achieving a quality that compares to force estimation capabilities achieved by humans. R EFERENCES
[1] R. Bischoff, J. Kurth, G. Schreiber, R. Koeppe, A. Albu-Sch¨ affer, A. Beyer, O. Eiberger, S. Haddadin, A. Stemmer, G. Grunwald, and G. Hirzinger, "The KUKA-DLR lightweight robot arm - a new reference platform for robotics research and manufacturing," in ISR/ROBOTIK 2010, Proceedings for the joint conference of ISR. VDE Verlag, 2010, pp. 1­8. [2] E. Magrini, F. Flacco, and A. De Luca, "Control of generalized contact motion and force in physical human-robot interaction," in Robotics and Automation (ICRA), 2015 IEEE International Conference on, May 2015, pp. 2298­2304. [3] A. Colome, D. Pardo, G. Alenya, and C. Torras, "External force estimation during compliant robot manipulation," in Robotics and Automation (ICRA), 2013 IEEE International Conference on, May 2013, pp. 3535­3540. [4] E. Gribovskaya, A. Kheddar, and A. Billard, "Motion learning and adaptive impedance for robot control during physical interaction with humans." in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2011, pp. 4326­4332. [5] A. Lee, H. Lu, A. Gupta, S. Levine, and P. Abbeel, "Learning force-based manipulation of deformable objects from multiple demonstrations," in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2015. [6] T. Schreiber, "Measuring information transfer," Physical Review Letters, vol. 85, no. 2, pp. 461­464, 2000. [7] M. R. Jovanovi, P. J. Schmid, and J. W. Nichols, "Sparsity-promoting dynamic mode decomposition," Physics of Fluids (1994-present), vol. 26, no. 2, 2014. [8] E. Berger, M. Sastuba, D. Vogt, B. Jung, and H. B. Amor, "Estimation of perturbations in robotic behavior using dynamic mode decomposition," Advanced Robotics, vol. 29, no. 5, pp. 331­343, 2015. [9] E. Berger, D. M¨ uller, D. Vogt, B. Jung, and H. Ben Amor, "Transfer entropy for feature extraction in physical human-robot interaction: Detecting perturbations from low-cost sensors," in Humanoids'14, 2014. [10] H. Sakoe, "Dynamic programming algorithm optimization for spoken word recognition," IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 26, pp. 43­49, 1978.

View publication stats

Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/305775316

EstimatingPerturbationsfromExperienceusing NeuralNetworksandInformationTransfer
ConferencePaper·October2016

CITATIONS

READS

0
5authors,including: ErikBerger TechnischeUniversitätBergakademieFreiberg
18PUBLICATIONS52CITATIONS
SEEPROFILE

51

DavidVogt TechnischeUniversitätBergakademieFreiberg
20PUBLICATIONS86CITATIONS
SEEPROFILE

HeniBenAmor ArizonaStateUniversity
56PUBLICATIONS434CITATIONS
SEEPROFILE

Someoftheauthorsofthispublicationarealsoworkingontheserelatedprojects:

Mining-RoX:AutonomousRobotsinUndergroundMiningViewproject

AllcontentfollowingthispagewasuploadedbyHeniBenAmoron02August2016.
Theuserhasrequestedenhancementofthedownloadedfile.

Estimating Perturbations from Experience using Neural Networks and Information Transfer
Erik Berger1 , David Vogt1 , Steve Grehl1 , Bernhard Jung1 , Heni Ben Amor2
Abstract-- In order to ensure safe operation, robots must be able to reliably detect behavior perturbations that result from unexpected physical interactions with their environment and human co-workers. While some robots provide firmware force sensors that generate rough force estimates, more accurate force measurements are usually achieved with dedicated force-torque sensors. However, such sensors are often heavy, expensive and require an additional power supply. In the case of lightweight manipulators, the already limited payload capabilities may be reduced in a significant way. This paper presents an experience-based approach for accurately estimating external forces being applied to a robot without the need for a forcetorque sensor. Using Information Transfer, a subset of sensors relevant to the executed behavior are identified from a larger set of internal sensors. Models mapping robot sensor data to force-torque measurements are learned using a neural network. These models can be used to predict the magnitude and direction of perturbations from affordable, proprioceptive sensors only. Experiments with a UR5 robot show that our method yields force estimates with accuracy comparable to a dedicated force-torque sensor. Moreover, our method yields a substantial improvement in accuracy over force-torque values provided by the robot firmware.

Fig. 1. For safe behavior execution, robots must be able to reliably detect perturbations resulting from unexpected physical interactions with their environment and human co-workers.

I. I NTRODUCTION The ability to sense the environment is a vital requirement for intelligent and safe robotics. Modern sensors, such as force-torque sensors (FT) can be used to measure external influences on a robot and, in turn, generate adequate responses. However, it is often difficult to distinguish between natural, behavior-related fluctuations in the sensor readings and external perturbations that are caused by forces or collisions applied by the outside world. Dynamic tasks in particular can cause significant variation in sensor readings that could potentially be mistaken for external influence. In recent years, various methods have been proposed for behavior-specific estimation of external perturbations [1][2]. In prior work, we have introduced a new methodology for estimating forces from experience [3]. We have shown that nonlinear state prediction and machine learning can be used to generate accurate estimates of external perturbations, even in the absence of force measuring sensors. First, a model of the expected sensory feedback during a physical task is learned. During runtime, expected sensations are compared to measured sensor values and the difference is transformed into a perturbation estimate. Our results showed that feature extraction is a key component to the above methodology.
1 Institute of Computer Science, Technical University Bergakademie Freiberg, Bernhard-von-Cotta-Str. 2, 09599 Freiberg, Germany 2 School of Computing, Informatics and, Decision Systems Engineering, Arizona State University, 699 S Mill Ave, Tempe, AZ 85281, USA

Recent advances in deep learning research have produced powerful neural network models for feature extraction and nonlinear regression [4]. In this paper, we investigate such deep learning techniques as the core component of our methodology. To this end, we will contrast feature extraction using autoencoders to our previous approach using Transfer Entropy [5]. For modeling the evolution of sensor values in time, we will employ different neural network architectures, including feedforward and recurrent neural networks. One potential advantage of neural networks over previous approaches, is their scalability to large numbers of input dimensions and large numbers of data points, as well as their ability to represent both feature extraction and regression within the same framework. In the remainder of this paper, we will introduce our methodology for perturbation estimation and show how neural networks can be used to model the evolution of sensor values in time. We will present a number of experiments to identify performance of different neural network architectures in this application domain and evaluate the accuracy of these results. II. R ELATED W ORK Robots that engage in physical interactions with humans and objects need to regulate the forces exchanged with their environment. This requires methods for estimating the occurring intrinsic and external forces to allow for appropriate robot responses. Recent developments in compliant

Data Acquisition
Time FT Data Robot Data

Model Generation
Intrinsic FT- Model Total FT-Model

Perturbation Estimation
Intrinsic FT-Prediction Total FT-Prediction

Realtime Robot Data

Perturbation Value

Fig. 2. An overview of the presented machine learning approach. During an offline training phase, the robot sensor data together with information about the time, force, and torque are recorded (Section III-A). The recorded data is analyzed for features and used to learn two models (Section III-B). In order to estimate external influences, the robot's realtime sensor data is used to predict intrinsic (Section III-C) and total force-torque measurements(Section III-D). The difference between both is used as estimation of the actual external perturbation.

control have lead to the emergence of robots with joint torque sensing and feedback control [6]. For measuring external forces and perturbations, however, typically additional force-torque (FT) sensors are used. Such sensors are often expensive, add weight to the robot, and require additional power supply. Hence, various authors have suggested using algorithmic approaches for inferring applied forces. In [7], a depth camera is used in order to estimate applied forces. Using a depth camera allows for generic contact locations on the robot. In [1] machine learning methods are used to extract an inverse dynamics model for a cable-driven robot manipulator. Measuring the difference between predicted controls from the inverse dynamics model and the executed controls provides an estimate for external forces applied on the robot. Using machine learning for force-based robot control has also been suggested a number of other publications. In [8] a robot learns to adapt its motion by anticipating human intentions from force measurements only. In [9], a method for learning force-based manipulation skills from demonstrations was presented. The approach generated variable-impedance control strategies thereby producing the necessary compliance for handling deformable objects. The work presented in the remainder of our paper focuses on different aspect: how can a robot learn to estimate forces from experience? In contrast to earlier discussed papers, we learn behavior-specific models for intrinsic and total force estimation. To increase the accuracy, an additional FT-sensor is used during training these models. During runtime the FTsensor is not available anymore and therefore is predicted by the learned models from the robot's sensor data only. Finally, the difference between intrinsic and total forces is used to predict the magnitude and direction of external perturbations. III. A PPROACH The goal of the presented approach is to learn a behaviorspecific perturbation model which is able to predict the readings of a FT-sensor from previous experience. Specifically, a FT-sensor with six degrees of freedom (Robotiq FT150) is mounted close to the tool center point (TCP) of a robotic arm (Universal Robots UR5). Figure 1 shows the principal setup including a gripper for pick-and-place operations.

The depicted robot provides 104 different measurements from a multitude of internal sensors, e.g., the applied current or the joint encoder state. In our approach, sensor readings generated from these sensors of the UR5 are used to learn a model that predicts FT-values measured by the FT150. The force-torque sensor is used in this context to provide ground truth data about all measured forces acting on the robot. The basic rationale underlying our approach is that accurate estimates of forces applied on the robot can be generated by fusing information and evidence from a large number of low-cost sensors. Note that these sensors do not have to be related to force estimation. A crucial component in our methodology is the identification of relevant features that are used for learning the predictive models. Figure 2 show an overview of the approach. The first step of the presented approach is to record training data representing the evolution of sensor values for the particular behavior. In contrast to our previous work [3], [2] no labeled data is recorded. Instead, the values of the FT150 are used as ground truth data for the actual force and torque. Next, the recorded data is analyzed for relevant features. As presented in [10], classical dimensionality reduction methods such as Principal Component Analysis fail to identify behavior-specific features. Instead, Transfer Entropy [5] (TE) is utilized to extract the most relevant features. In turn, these features are used to train two neural network models. The first model is used to predict the natural, behavior-related dynamics and is therefore called intrinsic FT-model. The second network is trained to predict total FT-values acting on the robot (intrinsic dynamics + external perturbations) and is called total FT-model. The difference between total and intrinsic prediction is used to estimate the magnitude and direction of external perturbations applied by a human or through a collision. Since both models require only robot sensor data to predict the actual FT-values, no expensive and heavy FT-sensor is required during runtime. In the following, each step of the presented approach will be explained in more detail. A. Data Acquisition The realtime interface of the UR5 provides 104 different sensors, containing a variety of different information

Force [N]

sources, e.g., the mainboard voltage, control, target and actual joint values. Some of these sources are relevant to the FT prediction task while others are redundant or noninformative. Hence we first identify a smaller set of relevant, informative sensors. To this end, the robot sensor values s = (s1 , . . . , s104 ) and the FT-values f = (f1 , . . . , f6 ) are recorded during the execution of a behavior. Since the FT150 data rate is less than the UR5's 125Hz , FT-values need to be interpolated to generate intermediate values. We use Dynamic Mode Decomposition (DMD) [11], a nonlinear interpolation method as described in [2]. Additionally, the time index r is recorded. During behavior execution the time index increases and is reset after each repetition. This time index will be relevant for the later feature selection step of the intrinsic FT-model. For all following experiments a pick and place behavior was executed on the robot for 60 seconds. During this process, the behavior was repeated about 16 times. The recorded data R = ((s, f , r)1 ; . . . ; (s, f , r)n ) represents training data consisting of n = 7500 equidistant samples with no external perturbations. In addition, two other data sets T1 and T2 with a length of 60 seconds were recorded. In contrast to the training data, T1 was perturbed by a human during the last half of its execution, while T2 was continuously perturbed. B. Model Learning

Measured
All sensors
50

FF

TD

NARX

REC

25

Force [N]

0

-25

-50

Autoencoder features
50

25

Force [N]

0

-25

-50

TE features
50

25

0

-25

-50

In the following section, different neural network architectures for dynamical systems modeling are used to model the recorded robot dynamics. More specifically, a feedforward (FF), a time-delay (TD), a recurrent (REC), and a nonlinear autoregressive network with exogenous inputs (NARX) are employed. For the sake of reproducibility, each network was generated with one hidden layer containing six neurons. Training was performed with the LevenbergMarquardt method. In order to allow for a dynamic response, all but the FF network require setting a temporal delay parameter d. The TD network has a delay on the input weights while the REC network layer has a delayed recurrent connection to itself. In addition to delayed input weights, the NARX network makes use of previous predictions. For a more detailed description of the different neural network architectures, the reader is referred to [12]. All network architectures have been trained to map input data s to output data f . C. Intrinsic FT-Prediction In the following section, all neural networks are trained with R and a delay of d = 2. The goal is to predict the intrinsic state for the semi-perturbed data set T1. First, the neural networks is trained using all sensors. The resulting predictions for f2 can be seen in Figure 3. As long as no perturbation occurs, all networks are approximately predicting the correct value. However, during the second half of T1 all networks fail to predict the correct intrinsic state. This is due to the fact that the 104 input sensors contain non-relevant information that may obfuscate important features. Hence,

0

10

20

30

40

50

60

Time [s]

Fig. 3. The different predictions of f2  T1 for different neural network architectures trained with the data set R. Using all sensors (top), autoencoders (middle), or TE feature selection (bottom) influences the accuracy of the predictions. Only the sensors selected by utilizing TE was able to suppress the external perturbations in the second half of the data set.
Discarded Selected

0.12 0.1

TEr

0.08 0.06 0.04 0.02 0 0 10 20 30 40 50 60 70 80 90 100

Sensors

Fig. 4. 10 sensors (highlighted green) with 50% of the overall TE are selected as features for training. The remaining 94 sensors (highlighted blue) are classified as less important for predicting the actual phase and therefore being discarded.

feature extraction methods are needed to identify relevant features. A feature extraction approach that is gaining popularity, is the use of autoencoders [4]. An autoencoder consists of an encoder and a decoder component, both of which are neural networks. Broadly speaking, the encoder maps the input data to a smaller set of hidden neurons while the decoder tries to

reconstruct the original input. This process can be stacked to reduce the dimensionality of the input data through a stepwise layering. Using autoencoders, we reduce 104 sensors to 50 values using the first encoder, and then to 10 values by stacking a second encoder. For the encoding process, a logistic sigmoid function was used while a linear transfer function was utilized for the decoders. As a result, the 104 dimensional input data s is reduced to a 10 dimensional feature data set. Next, this feature data was used to train the neural networks. The resulting predictions for f2  T1 can be seen in Figure 3 middle. Especially during the perturbation phase, the overall accuracy increases since irrelevant information is discarded. However, a problem of autoencoders is that the temporal influence and correlation of variables is not taken into account. To predict the time-dependent intrinsic values of the FTsensor the actual phase of the behavior is taken into account. TE is used as a measure of predictability and information flow between the robots sensor values and the relative time by TEr = T E (s, r). The TE from J to I is defined as T E (I, J) =
iI,j J

0.08

Discarded Selected

0.06

TEf

0.04

0.02

0 0 10 20 30 40 50 60 70 80 90 100

Sensors

Fig. 5. 9 sensors (highlighted green) with 50% of the overall TE are selected as features for training. The remaining 95 sensors (highlighted blue) are classified as less important for predicting the actual total FT-values and therefore being discarded.

40

20

Force [N]

0

-20

-40

p(i + 1|i, j ) , p(i + 1, i, j )log2 p(i + 1|i)

-60

Measured All Sensors Autoencoders TE features
0 10 20 30 40 50 60

where (i1 , . . . , iq )  I and (j1 , . . . , jq )  J are the possible states of quantized time-series data and the function p(·|·) describes the conditional probability. For a more detailed derivation the interested reader is referred to [5]. The resulting TEr describes how strong each sensor of the robot is influenced by the relative time. Sensors with a high TE are assumed to be beneficial for predicting the relative time and therefore the actual phase of the behavior. Figure 4 shows the normalized and ordered TE values of TEr. As can be seen, only 10 sensors with at least 50% overall TE are selected for training the neural networks. These sensors are, in particular, target and control values of the robot's joint states (e.g. position and velocity), which are independent of external influences and therefore are good predictors for the actual phase of the behavior. The resulting predictions for this subset of sensors can be seen in Figure 3 bottom. In contrast to the previous results, the selected sensors are able to accurately predict the intrinsic fluctuations. Also during external perturbations the predicted intrinsic forces are not influenced. The difference between the actual measured total FT-values and the intrinsic ones can be used to estimate the magnitude and direction of an external perturbation. D. Total FT-Prediction In contrast to the prediction of the intrinsic FT-values explained in the previous section the total FT-values are not exclusively dependent on the state of the robot. Consequently, it is important that the neural networks are additionally trained with information about how external perturbations influence the sensors of the robot. To this end, the neural networks are trained with R and the perturbed data set T2. Furthermore, feature selection is not calculate with respect to relative time anymore. Instead, the TE is calculated between the robots sensor values and each FT-value by TEf = |T E (s, f )|.

Time [s]

Fig. 6. The different predictions of f2  T1 for a recurrent neural network which was trained with the data sets R and T2. Using all sensors (red), autoencoders (yellow), or TE feature selection (purple) influences the accuracy of the predictions.

Figure 5 shows the normalized and ordered TE values of TEf . As can be seen in the figure, a small set of 9 sensors contains more than 50% of the overall TE information. These are especially measured values (e.g. current values close to the TCP) and no more target and control values. This is due to the fact that external perturbations does not perturb target/control values but the measured actual ones instead. The resulting mean squared errors for the different network predictions of f  T1 are shown in Table I. As can be seen, utilizing TE for feature selection outperforms the usage of autoencoders or all sensors. Furthermore, for NARX and REC networks using autoencoders further deteriorates the results. A possible explanation for this effect is that the objective function of autoencoders only focuses on the amount of information retained by using the generated features. This may be detrimental in various physical tasks in which some
TABLE I T HE MEAN SQUARED ERRORS RESULTING FROM DIFFERENT INPUTS AND NEURAL NETWORK ARCHITECTURES . All Sensors FF TD NARX REC 155.24 95.48 22.43 13.74 Autoencoder 72.62 81.45 62.34 42.48 TE Features 25.14 14.82 7.33 3.41

sensors have limited variability but strong influence on the task. The best result is obtained by using TE features and the REC network architecture. Predictions of f2  T1 generated by the REC network are compared to using all sensors and autoencoders in Figure 6. Given these results, the following experiment utilizes the proposed TE feature selection method combined with recurrent neural networks. IV. E XPERIMENTS Different experiments have been conducted to validate the proposed method. To this end, the data set T1 introduced in Section III-A was used.

Firmware

Measurement

Prediction

50

Force [N] Force [N] Force [N]

0

-50

50

A. Accuracy of Estimates First, the RECo model is evaluated by investigating the mean absolute error (MAE) in comparison to ground truth data of the FT150 and an intrinsic FT-sensor included in the firmware of the UR5. This sensor makes use of joint torques and a kinematic model of the robot to predict the FT-values at the TCP. In order to get comparable results, the firmware was calibrated to the mass and size of the FT150. For the FT-sensor, the manufacturer specifies a force accuracy of 25 N at the TCP and a detection time of 250 ms. The different force estimates for each dimension can be seen in Figure 7. The estimates provided by the firmware sensor follow the general trend but exhibit significant noise. By contrast, the estimates of the RECo model are close to the ground truth data of the FT150. The accuracy of the firmware sensor resulted in a MAE of 26.7401 N which is slightly below the 25 N specified by the manufacturer. In addition, considering the mentioned detection time delay of 250 ms did not decrease the MAE score. In comparison, the MAE of the RECo model is 3.8336 N. Furthermore, the detection time is less than 10 ms (on a dual core with 3.2 GHz) and scales with the performance of the computer system. Increasing the network delay from d = 2 to d = 125 further reduces the MAE to 1.9417 N. Consequently, the model requires one second of continuous sensor data to start the prediction while the detection time slightly increases to 15 ms. However, in order to keep the robot reactive from the beginning, a minimal delay of d = 2 was used for all experiments. Additionally, the RECo model also provides better torque estimates (1.0761 N m) when compared to the firmware sensor (5.6735 N m). B. Perturbation Estimation The intrinsic forces need to be predicted in order to dissect external perturbations from the predicted total forces. For this task, the RECi model is used. Figure 8 shows the resulting intrinsic force predictions. As can be seen in Figure 8, the intrinsic forces are not affected by perturbations in the second half of the recording. Finally, the perturbation value shown in Figure 9 is defined as the difference between total and intrinsic FT-values. The length and direction of the perturbation value is used to estimate the magnitude and direction of external perturbations. Generated estimates only represent the external perturbations applied by humans,

0

-50

-100

60

40

20

0

-20

0

10

20

30

40

50

60

Time [s]

Fig. 7. The x (top), y (middle) and z (bottom) force values of the firmware FT-sensor (green), the FT150 sensor (blue), and the prediction of the learned model (purple). The predicted values provide a tighter approximation of the measured ground truth data.
x-direction
25 20 15

y-direction

z-direction

Force [N]

10 5 0 -5 -10

0

10

20

30

40

50

60

Time [s]

Fig. 8. The intrinsic force predictions are not influenced by the external perturbations.

collisions or other external factors. As a result, the proposed method can be applied during runtime without making use of a FT-sensor in order to estimate total, intrinsic and in consequence external FT-values from previous experience. A video of some further experiments can be found here 1 .
1 https://youtu.be/60ue0X25S6k

40

x-direction y-direction z-direction

20

0

-20

-40

-60

0

10

20

30

40

50

60

Time [s]

Fig. 9. The difference between the total and intrinsic forces represent the perturbation value which is the FT-value applied to the robot from its environment.
Perturbation
50

light-weight manipulators with a limited payload.A further strength of the presented approach is that no prior knowledge of the robot kinematics, dynamics, or sensor characteristics is required. As a result, the approach generalizes to arbitrary robot platforms. We have shown that, without further adjustment, usual neural network architectures produce adequate estimates of the intrinsic, external, and total FT-values. Adapting the network properties, for instance by increasing the network delay, could further increase the estimation accuracy. A limitation of the approach is that the robot needs to perform the same behavior during training and runtime estimation. However, early results on the generalization capability of this approach show that it generalizes to mild variations of the behavior. A more in-depth evaluation of the generalization ability will be conducted in future work.

Perturbation [N]

No Perturbation

40

R EFERENCES
[1] A. Colome, D. Pardo, G. Alenya, and C. Torras, "External force estimation during compliant robot manipulation," in Robotics and Automation (ICRA), 2013 IEEE International Conference on, May 2013, pp. 3535­3540. [2] E. Berger, M. Sastuba, D. Vogt, B. Jung, and H. B. Amor, "Estimation of perturbations in robotic behavior using dynamic mode decomposition," Advanced Robotics, vol. 29, no. 5, pp. 331­343, 2015. [3] E. Berger, S. Grehl, D. Vogt, B. Jung, and H. Ben Amor, "Experiencebased torque estimation for an industrial robot," in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2016. [4] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol, "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion," J. Mach. Learn. Res., vol. 11, pp. 3371­3408, Dec. 2010. [5] T. Schreiber, "Measuring information transfer," Physical Review Letters, vol. 85, no. 2, pp. 461­464, 2000. [6] R. Bischoff, J. Kurth, G. Schreiber, R. Koeppe, A. Albu-Sch¨ affer, A. Beyer, O. Eiberger, S. Haddadin, A. Stemmer, G. Grunwald, and G. Hirzinger, "The KUKA-DLR lightweight robot arm - a new reference platform for robotics research and manufacturing," in ISR/ROBOTIK 2010, Proceedings for the joint conference of ISR. VDE Verlag, 2010, pp. 1­8. [7] E. Magrini, F. Flacco, and A. De Luca, "Control of generalized contact motion and force in physical human-robot interaction," in Robotics and Automation (ICRA), 2015 IEEE International Conference on, May 2015, pp. 2298­2304. [8] E. Gribovskaya, A. Kheddar, and A. Billard, "Motion learning and adaptive impedance for robot control during physical interaction with humans." in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2011, pp. 4326­4332. [9] A. Lee, H. Lu, A. Gupta, S. Levine, and P. Abbeel, "Learning force-based manipulation of deformable objects from multiple demonstrations," in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2015. [10] E. Berger, D. M¨ uller, D. Vogt, B. Jung, and H. Ben Amor, "Transfer entropy for feature extraction in physical human-robot interaction: Detecting perturbations from low-cost sensors," in Humanoids'14, 2014. [11] M. R. Jovanovi, P. J. Schmid, and J. W. Nichols, "Sparsity-promoting dynamic mode decomposition," Physics of Fluids (1994-present), vol. 26, no. 2, 2014. [12] M. Nrgaard, O. E. Ravn, N. K. Poulsen, and L. K. Hansen, Neural Networks for Modelling and Control of Dynamic Systems: A Practitioner's Handbook, 1st ed. Secaucus, NJ, USA: Springer-Verlag New York, Inc., 2000.

Force [N]

30

20

10

0

0

5

10

15

20

Time [s]

Fig. 10. behavior.

Non-perturbed and perturbed execution of a pick and place

C. Discussion As can be seen in Figure 7 (bottom), the force applied on the robot is estimated to be about 23 N. This is due to the weight of the gripper mounted on top of the FT-sensor ­ the manufacturer specifies a weight of 2.3 kg. A strength of the presented approach is that, no prior knowledge of the robots kinematics, dynamics, sensor instrumentation, or parameters is required. As a result, our approach can quickly be applied to any kind of robot platform. In addition, no hard thresholds for detecting external perturbations need to be set. Figure 10 illustrates this point. The green trajectory shows the force readings during a normal execution of a pick and place behavior. As can be seen, forces of 22 ± 3N are generated. In contrast to that, the blue trajectory shows an execution with different degrees of perturbations. V. C ONCLUSION In this paper, we have described a methodology for estimating forces from experience and showed how to use neural network models in order to generate accurate estimates of external perturbations. The main advantage of the presented approach is that the learned models are able to map low-cost robot sensor data to accurate FT measurements. Hence, no FT-sensor is required during runtime and consequently the robot's weight is reduced. This is particularly beneficial for

View publication stats

Grasping for a Purpose: Using Task Goals for Efficient Manipulation Planning
Ana Huam´ an Quispe Heni Ben Amor Henrik I. Christensen M. Stilman
Abstract--In this paper we propose an approach for efficient grasp selection for manipulation tasks of unknown objects. Even for simple tasks such as pick-and-place, a unique solution is rare to occur. Rather, multiple candidate grasps must be considered and (potentially) tested till a successful, kinematically feasible path is found. To make this process efficient, the grasps should be ordered such that those more likely to succeed are tested first. We propose to use grasp manipulability as a metric to prioritize grasps. We present results of simulation experiments which demonstrate the usefulness of our metric. Additionally, we present experiments with our physical robot performing simple manipulation tasks with a small set of different household objects.

arXiv:1603.04338v1 [cs.RO] 14 Mar 2016

I. I NTRODUCTION The ability to grasp objects in order to accomplish a task is one of the hallmarks of human intelligence. Numerous psychological studies show that humans grasp selection depends on the goal to be accomplished [14]. Decision making during grasping is therefore not only based on stability during manipulation, but also based on task requirements. If a specific grasp does not facilitate the execution of the upcoming subtasks, it is omitted from the reasoning process. In contrast to that, research on robot grasp synthesis has been tilted towards optimizing stability metrics only. A prominent approach is to generate a set of physically stable grasps, one of which is then selected by the high-level planner. If a high-level task planner cannot achieve the goals of the task, it has to back track and try a different grasp. Since no information is flowing between high-level planning and lowerlevel grasp generation, a large number of grasps may have to evaluated. If the required grasp is not within the optimized set of candidates, the entire task will fail. In this paper, we introduce a method for manipulation planning which uses foresight to identify tasks constraints. Constraints extracted from subsequent sub-tasks are used to synthesize grasps that facilitate overall task completion. Our goal is to derive a fast planning algorithm that can efficiently generate manipulation sequences for previously unseen objects. These latter properties, hence, allow a robot to perform manipulation tasks in new environments without resorting to prior 3D models of the object or pre-calculated grasp sets. This ability to generalize is realized by using a super-quadric representation of objects. We show how super-quadrics can be extracted from a single depth image and how they can be used to generate a large set grasp candidates.
Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA 30332, USA. ahuaman3@gatech.edu,

Fig. 1: Example of grasp selection based on goal constraints: The goal location of the chips box is surrounded by nearby objects, hence an overhead grasp must be selected for manipulation. If not for the obstacles present, a different grasp (overlayed in red) from the side would be easier to execute. Short planning times are realized by introducing a heuristics that efficiently guides the search by incorporating arm kinematics. Inspired by the end-comfort effect [20] in humans, grasps are preferred which lead to a comfortable arm configuration at the end of a task. We borrow ideas from this work and propose to use a metric based on manipulability as a measurement of end-comfort. The contributions of this paper are threefold, namely (1) a framework for online grasp planning that incorporates future task constraints into the grasp synthesis process, (2) an efficient grasp generation approach based on super-quadrics that works with previously unseen objects, (3) the end-comfort heuristics for efficient search during manipulation planning. The rest of this paper is organized as follows: Section II present relevant work in the area of grasp synthesis and grasp selection. Section IV presents our grasp generation method using a primitive-based approach and in Section V we introduce our manipulability-based strategies to prioritize the generated grasps. Section VI shows the results of the comparisons in simulation and the metrics we used to compare their performance. Finally, we present the application of our approach in our physical robot. We conclude this paper with section VII, where we provide some discussion regarding future work, and the advantages and shortcomings of our approach.

hbenamor@cc.gatech.edu, hic@cc.gatech.edu

Superquadric Fitting

Grasp Generation at Goal Pose

Validation at Start Pose

Prioritization

End Yes

Found Solution?

Plan Pick and Place

Pop First Grasp

No

Fig. 2: Manipulation planning pipeline: a partial point cloud of the object is first analyzed for symmetries and then turned into a superquadric representation. Grasps at the goal and the start position are generated and then prioritized according to end-comfort. Potential grasps are then analyzed within the plan and then executed. II. R ELATED W ORK In this section we review work concerning grasp synthesis and grasp selection. For a more detailed review of previous research in the area, we suggest the interested reader to consult the excellent reviews from Bohg [3] and Sahbani [21]. Pioneering work on grasp selection was developed by Cutkosky [5], who observed that humans select grasps in order to satisfy 3 main types of constraints: Hand, object and taskbased constraints. As pointed out by Bohg et al. in [3], there is little work on task-dependent grasping when compared to work focused on the first two constraints. Hence, the main goal for a planner is to find a grasp such that the robot can approach the object and execute the said grasp, without further regard of what the robot will do once the object is picked. Grasp generation methods vary widely depending on the assumptions considered. In the case of grasp planning for known objects, Ciocarlie et al. [4] presented the concept of eigengrasps, which was exploited to generate candidate grasps searching in a low dimensional hand posture space using their GraspIt! simulator. Diankov generated grasps by sampling the surfaces of object meshes and using the normals at the sample points to guide the approach direction of the hand [6]. Approaches using primitive representations were also proposed such that the grasp generation depends on the particular primitive characterization: Miller et al. [15] proposed to use a set of primitive shapes (cylinder, box, ball) to decompose complex objects. Huebner and Kragic [12] used bounding boxes, Przybylski et al. proposed the Medial Axis representation [18], Goldfeder et al. [9] used superquadrics due to their versatility to express different geometry types with only 5 parameters. In all the cases mentioned, the grasps are generated offline and stored in a database for future use. These grasps are usually ranked based on their force-closure properties, which theoretically express the robustness and stability of a grasp. One of the most popular metrics ( ) was proposed by Ferrari and Canny [7]. However, it has been noted by different authors that analytical metrics do not guaranteee a stable grasp when executed in a real robot. This can be explained by the fact that these classical metrics consider assumptions that don't always hold true in real scenarios (dynamics, perceptual and modelling inaccuracies, friction conditions). On the other hand, studies that consider human heuristics to guide grasp search have shown remarkable results, outperforming classical approaches. In [1], Balasubramanian observed that when humans kinestetically teach a robot how to grasp objects, they strongly tend to align the robotic hand along one of the object's principal axis, which later results in more robust grasps. The author termed skewness to the metric measuring the axis deviation. In [19], Przybylski et al. combine the latter metric with and use it to rank grasps produced with GraspIt!. Berenson et al. [2] proposed a score combining 3 measures: , object clearance and the robot relative position to the object. In this work we are interested in manipulation of unknown objects. Multiple approaches of this kind have flourished during the last few years, particularly due to the advent of affordable RGB-Depth sensors. Since the 3D information is partial and noisy, classical approaches to grasp generation cannot be directly used. Rather, most of the current work uses heuristics to guide grasp generation based on local representation of the object geometry features (or global features if the object shape is approximated). In [10], Hsiao et al. use the bounding box of the object segmented pointcloud to calculate grasp approach directions using a set of heuristics. We should notice that for most of these approaches, their effectiveness can only be verified empirically. As we mentioned at the beginning of this section, the metrics we discussed above do not consider the task to be executed after the grasp is achieved. Some authors, however, have investigated this issue at some level. In pioneering work, [13] Li and Sastry proposed the concept of the task ellipsoid, which maximizes the forces to be applied in the direction of the task. More recently, Pandey et al. [16] proposed a framework to select a grasp such that the object grasped can be manipulated in a human-robot interaction scenario in which the goal pose of the object is not entirely constrained. Finally, although one of our main concerns is to select a grasp that is suitable for the task to be executed, we also consider important to use a grasp that allows for a simple, easy

Fig. 3: System setup and problem description.

a complete point cloud, multiple images can be acquired by either iteratively moving the camera or the object. This process is time-consuming and introduces new challenges such as the precise matching of the individual point clouds of each view. To solve this problem, we use a super-quadric representation of objects and reason about symmetry in order to infer the shape of any invisible part. Superquadrics are a family of geometric shapes that can represent a wide range of diverse objects using a limited set of parameters. Superquadrics can be expressed with their implicit equation: x a
2 2

arm execution. Interestingly, the problem of grasp planning is usually considered isolated from arm planning. In some recent work, Vahrenkamp et al. proposed Grasp-RRT [22] in order to perform both grasp and arm planning combined. In a similar vein, Roa et al. also proposed an approach that solve both problems simultaneously [8]. Both approaches focus on reaching tasks. Our approach tackles pick-and-place tasks in which reaching is only the first half of the solution (object placing being the second). We make use of our proposed heuristics to solve the complete pick-and-place problem in a manner as efficient as possible by means of grasp prioritization. III. P ROBLEM D EFINITION AND A SSUMPTIONS Our problem description can be explained as follows: Given a bimanual manipulator R and a simple object O, the manipulation task consists on transporting O from a given start pose w Ts to a final pose w Tg . Figure 3 depicts the problem described. The following constraints are considered: · A 3D model of O is not available beforehand. · A one-view pointcloud of the scenario is available from the Kinect sensor mounted on top of the robot shoulders. · Each limb of R consists of a 7-DOF arm (AL , AR ) and a 3-fingered hand (HL , HR ). A semi-analytical IK solver is available for AL and AR In the following sections we will describe our basic approach for problems in which only the use of one arm is required to solve the manipulation task described. IV. G RASP G ENERATION FOR U NKNOWN O BJECTS As our problem description stated, our approach must find a plan such that O can be grasped at w Ts , transported and finally repositioned at w Tg . Our approach consists on 4 main steps, shown in Figure 2. The first 3 steps (object fitting, grasp generation and grasp validation) will be explained in the rest of this section. A. Object Representation using Superquadrics Requiring complete 3D models of objects before grasp synthesis severely limits the application domains of robot manipulation. Modern depth cameras partly solve the problem, since they allow the robot to estimate the surface of an object. Yet, since the point clouds are acquired from a specific perspective, they only hold partial shape information about the visible frontal part. To fill any gaps and produce

y + b

2 2

2 1

+

z c

2 1

=1

(1)

In our approach, we generate a super-quadric representation using a single depth image. Fitting of the parameters can be performed online by minimizing the difference between the model and the partial point cloud [11]. However, since only one side of the object is visible, a standard approach to fitting will result in erroneous approximations of the object. To reproduce the entire shape from a partial point cloud, we added an additional pre-processing step to the superquadric fitting process. Instead of using the original point cloud as input, we generate a mirrored version by finding an optimal symmetry plane [11]. The goals of this step is to exploit symmetries to infer invisible parts of an object. The output of this process for a given object O consists on a transformation w To in world coordinates and the parameters, psq = { a, b, c,
1, 2

}

defining its approximated geometry. A good number of household objects can be easily described with generic shapes such as boxes, cylinders and ovoids, for which we can further bound the shape parameters considered:
1, 2

 [0.1, 1.9]

Figure 4 shows different geometric shapes corresponding to superquadrics with different values for 1 and 2 . The superquadric approach turns the pointcloud-based representation into a parametric representation, which can be much more efficiently used during grasp synthesis. Calculations of principal-axes, normals and other features are much faster and less susceptible to noise. B. Generating Valid Candidates Once the shape of O is approximated, we can proceed to generate candidate grasps gi using a simulation of the robot, and the object O, whose mesh is reconstructed by using the superquadric parameters found in the previous section. The candidates grasps must be kinematically feasible to execute with O located at both start and goal conditions (w Ts and w Tg ). Our approach accomplish this with Algorithm 1. First, we set O at its goal pose w Tg and generate a set of kinematically feasible grasps for it (G ). Next, we set O at its start pose w Ts . Finally, we test each of the grasps gi from G in this scenario, discarding the grasps for which there exist

1.2

superquadrics equation. We use the method proposed by Pilu and Fischer [17] to obtain an evenly-spaced set of points and normals. To define a grasp we calculate the transformation of the hand H w.r.t. O(o Th ). We use the samples to generate this transformation (lines 3 to 7 of Algorithm 2). After positioning the TCP of the H at w Tp , we close the fingers. If there are not collisions with the environment, we proceed to evaluate if there exists at least an arm configuration that allows the hand to execute the grasp. If so, then a corresponding grasp is stored.
1.0

Fig. 4: Examples of superquadrics with different shapes. A variety of shapes can be represented using a small number of parameters.

Algorithm 2: GenerateGrasps(H, A, w To ,O,psq ) Input: H, A , w To , O, psq Output: A feasible set of grasps G
/* psq = { 1 , 2 ,a,b,c}
1

*/

S = sample_SQ(psq ) foreach (pi ,ni )  S do
/* p: TCP point in the hand H */ */ */
o

not a single IK solution. The surviving grasps in G are then grasps that can be executed for the object O at both w Ts and w Tg . Algorithm 1: get Valid Candidates Input: H, A , w Ts , w Tg , O, psq Output: Set of Candidate grasps G
1

2

3

Tp .trans = pi Tp .z = -ni

/* z: Approach direction of H
4 o

/* x: Fingers closing direction
5 6 o

set_Pose(O,w Tg )
/* Generate grasps with O at
wT g

7

*/
8

Tp .x = smallest_Axis(a,b,c) o Tp .y = o Th .z × o Th .x w Tp = w To ·o Tp
/* h: Origin of hand H
w w p

2 3

G  generate_Grasps(H, A ,w Tg ,O, psq ) set_Pose(O,w Ts )
/* Discard grasps invalid with O at
wT s

*/

*/

9 10 11 12 13

4 5 6 7 8

foreach gi  G do if exist_IK_sol(gi , H, A ) is false then G .erase(gi ) return G

Th = Tp · Th setHand_Tcp(H, w Tp ) close_Hand(H ) if check_collision(H ) is false then if exist_IK_conf(H, A ,w Th ) is true then G  Grasp(H, o Tp ·p Th )

14 15

return G

1) Grasp Generation at Goal Pose: The function generate_Grasps, which we use to produce grasps exploiting the shape parameters of O is shown in Algorithm 2. First, we uniformly sample the surface of O. This is easily done by using the explicit equation defining the points in a superquadric and their corresponding normals:     x a cos 1  cos 2    y  =  b cos 1  sin 2   with 2 <  < 2 (2) << z c sin 1    1 2- 1 2- 2    cos   a cos nx 1  2 - ny  =  cos2- 1  sin 2   (3) b    nz 1 sin2- 1  c Sampling uniformly  and  does not produce a uniform sampling of surface points due to the high nonlinearity of the

Algorithm 2 generates at most one grasp per each sampled point. Optionally, we generated 2 additional possible grasps per each point by rotating the hand an angle ± around the x axis of o Tp . We added this since we noticed that, when executing the grasps on the physical robot, a slight inclination usually made the grasp much easier to reach. In this paper we used  = 30o . An example of the variated grasps generated using  is shown in Figure 5. 2) Validation at Start Pose: Once a set of grasps feasible to execute on O at w Tg is obtained, our algorithm discards the grasps that cannot be executed with O at w Ts (lines 4 to 6). V. G RASP P RIORITIZATION Once a set of feasible grasps G is generated, paths for reaching and placing the object must be produced. A brute-force approach would be to exhaustively try each grasp in a random

Fig. 5: Generating grasps varying the approach direction by rotating the hand around the local x axis(pointing out the page): Left: Original. Middle,right: After rotating by ± order until a solution is found. However, arm planning can be a time-consuming process, particularly when using samplingbased methods. It is therefore desirable to first evaluate grasps that are more likely to produce a solution. Since there is likely more than one solution in G , it is preferable to choose grasps such that the solution is quickly found. We propose to use situated grasp manipulability as a metric to prioritize the grasps and, hence, as a heuristic for guiding the search process. Manipulability(m) measures how dexterous the end-effector of a robotic arm is at a given joint configuration q . Initially proposed by Yoshikawa [23], m(q ) is defined as: m(q ) = J (q )J T (q ) . (4)

From the shown example, it becomes evident that for a pick-and-place manipulation problem, there are at least two possible metrics to use per grasp gi : mg measured either at w Ts or w Tg . Choosing the first option means that we prioritize grasps in which the pick phase is executed comfortably (w Ts ), whereas by choosing the latter, we favor grasps in which the arm configuration used at placing the object (w Tg ) is more relaxed. In section VI, we present the results of experiments comparing these two metrics and an additional control measure to analyze their performance and choose which one is best suited for our problem. VI. E XPERIMENTS AND R ESULTS In this section, we perform a set of experiments in simulation and on the real robot in order to evaluate the introduced manipulation planning algorithm. The simulation experiments are used to analyze the situated grasp manipulability using a large number of trials. Experiments on the real robots are performed to show the generation of manipulations based on task goals and previously unknown objects. Generation of superquadric object models was performed on the spot within 1 second. A. Simulation Experiments In this experiment, we consider three alternatives: w · Measure mg for grasp situated at Ts w · Measure mg for grasp situated at Tg · Average of both measures above We use 3 measures to compare the performance of the 3 evaluated metrics. · First success: The main goal of the metrics evaluated is to prioritize the grasps such that the first one tried is the most likely to succeed. This metric indicates the number of times that a solution is found by evaluating only the grasp with the biggest value for the evaluated metric. · Planning time: It measures the average planning time of the successing grasps. The planning time is the total time to plan a reach and transport path for the given grasp. · Path length: It indicates the number of steps required for the pick-and-place solution. The step length is a normalized value in joint space, so this metric compare the paths in configuration space. The scenario we used is depicted in Figure 7. We fix the w Tg to the middle of the table (red marker) and vary the start pose w Ts to 35 positions, each separated 0.1 m (green markers). We devised 2 kind of experiments: In the first, w Ts and w Tg have the same orientation, with only the position being changed (35 scenarios). In the second case, w Ts presents a rotation around  steps, the Z axis w.r.t. w Tg in the interval [0, 2 ] at each 4 so in total 35 × 8 = 280 scenarios are tested. We used an standard IK-BiRRT to perform the arm planning. To account for its randomness, the results presented in Table I and Table II are an average of 5 runs per each experiment. From the tables, we can observe that using mg evaluated at w Tg produces the best results in terms of success at the first

Manipulability is typically defined for a single joint configuration. In our scenario, we describe a situated grasp gi for which multiple q might exist, due to redundancy. This naturally leads to the definition of situated grasp manipulability(mg ). Given a target object O located at w To , and its corresponding grasp gi , we define mg as the average manipulability of a uniform set of collision-free arm configurations qi that allow executing gi : mg = 1 N
N

m(qi )
i=1

(5)

Please note that mg depends on both gi , w To and the environment (for collisions) since only collision-free grasps that reach the object are considered. Figure 6 shows an example of a pick-and-place task wherein the green and red markers indicate the w Ts and w Tg . In this case, mg at w Tg is bigger than w Ts (where Ns = 76 and Ng = 108 are the number of IK solutions for both situations). When the object is at w Tg , the arm movement requires less effort.

Fig. 6: Examples of mg measured at w Ts and w Tg

TABLE II: Evaluation with rotation change
Metric Type mg at mg at
wT s wT g

Path Steps 100.7 99.6 100.4

Planning Time 4.70 4.49 3.83

Success 255/278 275/278 260/278

Avg. mg

Fig. 7: Setup for unimanual evaluation experiments TABLE I: Evaluation with no rotation change
Metric Type mg at mg at
wT s wT g

Path Steps 82.92 89.28 92.92

Planning Time 2.17 2.218 2.29

Success 21.8/35 33/35 31.4/35

Avg. mg

trial, whereas w Ts present the worst results. The average path length for the general case of Table II is rather similar for the 3 cases. Regarding planning times, the average mg gives better results. Given the results presented, we chose to use the mg at w Tg . Its next best competitor (the avg. mg ) was not considered since in order to calculate it, the mg at both w Ts and w Tg must be calculated, which increases the computation time (for the examples presented, the computation time of mg was 2 seconds). Given that the advantage of planning time is not significant, we chose mg at w Tg . B. Robot Experiments Next, we perform a set of experiments on the real robot. All performed experiments are pick-and-place tasks. However, in some tasks we add environmental constraints at the goal location which limit the range of applicable grasps. Figure 8 depcits two trials without any environmental constraint. The robot has to pick an object at the starting location (green) and move it successfully to the goal location (orange). The robot has no prior knowledge of the object and needs to extract shape information from a single depth image produced by a depth sensor mounted in the head. As can be seen in the figure, the grasp direction and the hand shape is adapted to suit the object. A different set of experiments can be seen in Figure 9. Here, environmental constraints at the goal are introduced. In the top row, the object has to be placed in a box. Accordingly, the robot has to choose a grasp that allows it to place the object in the box without colliding with it. Hence, the selected grasps are mostly from above. The middle row show a different scenario, in which the object has to be placed on a box which is farther away. Choosing the wrong approach direction, e.g. from above, would prevent the robot from successfully finishing the manipulation process, due to workspace limitations. The bottom row shows normal runs without any environmental constraints.

Fig. 8: Two examples of a pick-and-place without constraints. The robot can identify suitable grasp for novel objects using the superquadric representation. VII. C ONCLUSION In this paper, we introduced a new method for manipulation planning with task constraints. Given a previously unknown object and goals of the task, the method synthesizes online a grasp that facilitates task completion. Planning and grasp synthesis are effectively merged to efficiently produce manipulation sequences. Object acquisition, representation, grasp synthesis and planning can be performed within a couple of seconds, i.e., 2-5 seconds, for the presented examples. We showed how superquadrics and a new heuristic, i.e., the situated grasp manipulability can be used towards this end. These properties, hence, allow a robot to perform successful, goal-driven manipulation tasks in new environments without resorting to prior 3D models of the object or pre-calculated grasps. While superquadrics can be efficiently calculated, they lack accuracy when representing complex shapes and objects. In this paper, we showed that many objects, in particular household objects can be represented by superquadrics. In our future work, we want to explore extensions of this representation that can model a larger set of objects. In particular, we want to build upon our previous research on the identification of rotational and linear extrusions [11] to represent more complex shapes. In addition, we want to verify the introduced planning approach on longer manipulation sequences.

Fig. 9: Final grasp configuration during manipulation with goal constraints (top and middle) and without goal constraints (bottom).

R EFERENCES
[1] R. Balasubramanian, L. Xu, P. D Brook, J.R. Smith, and Y. Matsuoka. Physical human interactive guidance: Identifying grasping principles from human-planned grasps. In The Human Hand as an Inspiration for Robot Hand Development. Springer, 2014. [2] D. Berenson, R. Diankov, K. Nishiwaki, S. Kagami, and J. Kuffner. Grasp planning in complex scenes. In 7th IEEE-RAS Int. Conf. on Humanoid Robots, 2007. [3] J. Bohg, A. Morales, T. Asfour, and D. Kragic. Data-driven grasp synthesis: A survey. IEEE Transactions on Robotics, 2014. [4] M. Ciocarlie, C. Goldfeder, and P. Allen. Dimensionality reduction for hand-independent dexterous robotic grasping. In IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), pages 3270­3275, 2007. [5] M.R. Cutkosky. On grasp choice, grasp models, and the design of hands for manufacturing tasks. IEEE Transactions on Robotics and Automation, 1989. [6] R. Diankov and J. Kuffner. Openrave: A planning architecture for autonomous robotics. Robotics Institute, Pittsburgh, PA, CMU-RI-TR08-34, 79, 2008. [7] C. Ferrari and J. Canny. Planning optimal grasps. In IEEE Int. Conf. on Robotics and Automation (ICRA), 1992. [8] J. Fontanals, B.A. Dang-Vu, O. Porges, J. Rosell, and M. Roa. Integrated grasp and motion planning using independent contact regions. 14th IEEE-RAS Int. Conf. on Humanoid Robots (Humanoids), 2014.

[9] C. Goldfeder, P.K. Allen, C. Lackner, and R. Pelossof. Grasp planning via decomposition trees. In IEEE Int. Conf. on Robotics and Automation (ICRA), pages 4679­4684, 2007. [10] K. Hsiao, S. Chitta, M. T. Ciocarlie, and E. G. Jones. Contact-reactive grasping of objects with partial shape information. In IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), volume 99, page 124, 2010. [11] A. Huam´ an Quispe, B. Milville, MA. Guti´ errez, C. Erdogan, M. Stilman, HI. Christensen, and H. Ben Amor. Exploiting symmetries and extrusions for grasping household objects. IEEE Int. Conf. on Robotics and Automation (ICRA)(to appear), 2015. [12] K. Huebner and D. Kragic. Selection of robot pre-grasps using boxbased shape approximation. In IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2008. [13] Z. Li and S.S. Sastry. Task-oriented optimal grasping by multifingered robot hands. IEEE Journal of Robotics and Automation, 1988. [14] J. Loucks and J. A. Sommerville. The role of motor experience in understanding action function: The case of the precision grasp. Child development, 83(3):801­809, 2012. [15] A.T. Miller, S. Knoop, H.I. Christensen, and P.K. Allen. Automatic grasp planning using shape primitives. In IEEE Int. Conf. on Robotics and Automation,(ICRA), 2003. [16] A.K. Pandey, J-P Saut, D. Sidobre, and R. Alami. Towards planning human-robot interactive manipulation tasks: Task dependent and human

[17] [18] [19] [20] [21] [22] [23]

oriented autonomous selection of grasp and placement. In 4th IEEE RAS & EMBS Int. Conf. on Biomedical Robotics and Biomechatronics (BioRob), 2012. M. Pilu and R.B. Fisher. Equal-distance sampling of superellipse models. University of Edinburgh, Department of Artificial Intelligence, 1995. M. Przybylski, T. Asfour, and R. Dillmann. Unions of balls for shape approximation in robot grasping. In IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2010. M. Przybylski, T. Asfour, R. Dillmann, R. Gilster, and H. Deubel. Human-inspired selection of grasp hypotheses for execution on a humanoid robot. In 11th IEEE-RAS Int. Conf. on Humanoid Robots, 2011. D. A. Rosenbaum, C. M. van Heugten, and G. E. Caldwell. From cognition to biomechanics and back: The end-state comfort effect and the middle-is-faster effect. Acta psychologica, 94(1):59­85, 1996. A. Sahbani, S. El-Khoury, and P. Bidaud. An overview of 3d object grasp synthesis algorithms. Robotics and Autonomous Systems, 60(3):326­336, 2012. N. Vahrenkamp, T. Asfour, and R. Dillmann. Simultaneous grasp and motion planning: Humanoid robot armar-iii. Robotics & Automation Magazine, 2012. T. Yoshikawa. Manipulability of robotic mechanisms. The International Journal of Robotics Research, 4(2):3­9, 1985.

Physical humanârobot interaction tasks require robots that can detect and react to external perturbations caused by the human partner. In this contribution, we present a machine learning approach for detecting, estimating, and compensating for such external perturbations using only input from standard sensors. This machine learning approach makes use of Dynamic Mode Decomposition (DMD), a data processing technique developed in the field of fluid dynamics, which is applied to robotics for the first time. DMD is able to isolate the dynamics of a nonlinear system and is therefore well suited for separating noise from regular oscillations in sensor readings during cyclic robot movements. In a training phase, a DMD model for behavior-specific parameter configurations is learned. During task execution, the robot must estimate the external forces exerted by a human interaction partner. We compare the DMD-based approach to other interpolation schemes. A variant, sparsity promoting DMD, is particularly well suited for high-noise sensors. Results of a user study show that our DMD-based machine learning approach can be used to design physical humanârobot interaction techniques that not only result in robust robot behavior but also enjoy a high usability.BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

1

arXiv:1507.07882v1 [cs.CV] 27 Jul 2015

Occlusion-Aware Object Localization, Segmentation and Pose Estimation
Samarth Brahmbhatt
http://www.cc.gatech.edu/~sbrahmbh

Heni Ben Amor
http://henibenamor.weebly.com

School of Interactive Computing Georgia Institute of Technology Atlanta, GA USA

Henrik Christensen
http://www.cc.gatech.edu/~hic

Abstract We present a learning approach for localization and segmentation of objects in an image in a manner that is robust to partial occlusion. Our algorithm produces a bounding box around the full extent of the object and labels pixels in the interior that belong to the object. Like existing segmentation aware detection approaches, we learn an appearance model of the object and consider regions that do not fit this model as potential occlusions. However, in addition to the established use of pairwise potentials for encouraging local consistency, we use higher order potentials which capture information at the level of image segments. We also propose an efficient loss function that targets both localization and segmentation performance. Our algorithm achieves 13.52% segmentation error and 0.81 area under the false-positive per image vs. recall curve on average over the challenging CMU Kitchen Occlusion Dataset. This is a 42.44% decrease in segmentation error and a 16.13% increase in localization performance compared to the state-of-the-art. Finally, we show that the visibility labelling produced by our algorithm can make full 3D pose estimation from a single image robust to occlusion.

1

Introduction and Related Work

In this paper we address the problem of localizing and segmenting partially occluded objects. We do this by generating a bounding box around the full extent of the objects, while also segmenting the visible parts inside the box. This is different from semantic segmentation, which typically does not provide information about the spatial position of labelled pixels inside the object. While a lot of progress has been made in object detection [9, 13, 21], occlusion by other objects still remains a challenge. A common theme is to model occlusion geometrically or appearance-wise, thereby allowing it to contribute to the detection process. Wang et al. [19] use a holistic Histogram of Oriented Gradients (HOG) template [6] to scan through the image and use specially trained part templates for instances where some cells of the holistic template respond poorly. Girshick et al. [11] force the Deformable Part Model detector to place a trained `occluder' part in regions where the original parts respond weakly. The object masks produced by both of these algorithms are only accurate up to the parts and hence not usable for many applications e.g. edge-based 3D pose estimation. Xiang and Savarese [20]
c 2015. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.

2

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

approximate object structure in 3D using planar parts. A Conditional Random Field (CRF) is then used to reason about visibility of the parts when the 3D planes are projected to the image. However, such methods work well only for large objects that can be approximated with planar parts. Our approach is entitled Segmentation and Detection using Higher-Order Potentials (SDHOP). It is based on discriminatively learned HOG templates for objects and occlusion. Whereas the object templates model the objects of interest, the occlusion templates provide discriminative support and do not model a specific occluder. Segmentation is done by considering not only the response of patches to these templates, but also the segmentation of neighbouring patches through a CRF with higher-order connections that encompass image regions. We will compare our approach to two existing approaches that have been designed to handle partial occlusion. Hsiao and Hebert [14] approximate all occluders by boxes and generate occlusion hypotheses by finding locations of mismatch between image gradient and object model gradient. These hypotheses are then validated by the visibility of other points of the object and by an occlusion prior which assumes all objects rest on the same planar surface. Our algorithm does not need such assumptions which reduce the segmentation accuracy. Gao et al. [10] learn discriminative appearance models of the object and occlusion seen during training. Segmentation is achieved by defining a CRF to assign binary labels to patches based on their response to these two filters. We build on their work but add several important modifications that lead to better localization and segmentation performance. Firstly, we replace the edge-based pairwise terms with 4-connected pairwise terms that are better able to propagate visibility relations. Secondly, we introduce the use of higher-order potentials defined over groups of patches, allowing us to reason at the level of image segments which contain much more information than pairs of patches. We also introduce a new loss function for structured learning that targets both localization and segmentation performance but is still decomposable over the energy terms. Lastly, we introduce a simple procedure to convert the granular patch-level object mask produced by the algorithm to a fine pixel-level mask that can be used to make 3D pose estimation of detected objects robust to partial occlusion. Our algorithm outperforms these approaches (Hsiao and Hebert [14], Gao et al. [10]) at both object localization and segmentation on the CMU Kitchen Occlusion dataset as shown in Section 3. The rest of the paper is organized as follows. Section 2 describes our proposed approach. We present evaluations on standard datasets and our own laboratory dataset in Section 3 and summarize in Section 4.

2

Method

The training phase for SD-HOP requires a set of images with different occlusions of the object(s) of interest. Each training sample is (1) over-segmented and (2) annotated with a bounding box around the full extent of the object and a binary segmentation of the area inside the box into object vs. non-object pixels. Given these training images and labels, we train a structured Support Vector Machine (SVM) that produces the HOG templates and CRF weights. Figure 1 shows an overview of our approach. Object segmentation is done by assigning binary labels to all HOG cells within the bounding box, 1 for visible and 0 for occluded. Instead of making independent decisions for every cell, we allow neighbouring cells to influence each other. Neighbour influence can

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION
Training Object 1 Object 2 Feature Extraction Feature Vector Find Most Violated Constraint Segmentation Pyramid Testing Model Learning Solve Constrained QP Labels

3

HOG Feature Pyramid

Inference

Figure 1: Overview of our approach. Top: During training, images are segmented and features are extracted from pyramids of segmentations and HOG features. An SVM model is learned by max-margin learning. Bottom: After training, the model can be used to infer a bounding box and visible segments of the object. take two forms: (1) pairwise terms (Rother et al. [17]) that impose a cost for 4-connected neighbours to have different labels and (2) higher-order potentials (Kohli et al. [16]) that impose a cost for cells to have a different label than the dominant label in their segment of the image. These segments are produced separately by an unsupervised segmentation algorithm.

2.1

Notation

The label for an object in an image x is represented as y = (p, v, a), where p is the bounding box, v is a vector of binary variables indicating the visibility of HOG cells within p and a  [1, A] indexes the discrete viewpoint. p = ( px , py , p ) indicates the position of the top left corner and the level in a scale-space pyramid. The width and height of the box are fixed per viewpoint as wa and ha HOG cells respectively. Hence v has wa · ha elements. All training images are also over-segmented to collect statistics for higher-order potentials. Any unsupervised algorithm can be used for this, e.g. Felzenszwalb and Huttenlocher [8] and Arbelaez et al. [2].

2.2

Feature Extraction

Given an image x and a labelling y, a sparse joint feature vector (x, y) is formed by stacking A vectors. Each of these vectors has features for a different discretized viewpoint. All vectors except for the one corresponding to viewpoint a are zeroed out. Below, we describe the components of this vector. 1. 31-dimensional HOG features are extracted for all cells of 8x8 pixels in p as described in Felzenszwalb et al. [9]. The feature vector is is constructed by stacking two groups which are formed by zeroing out different parts, similarly to Vedaldi and Zisserman [18]. The visible group v (x, p) has the HOG features zeroed out for cells labelled 0 and the occlusion group nv (x, p) has them zeroed out for cells labelled 1. 2. Complemented visibility labels, to learn a prior for a cell to be labelled 0: [1wh - v].

4

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

3. Count c(p) of cells in bounding box p lying outside the image boundaries, to learn a cost for truncation by the image boundary, similarly to Vedaldi and Zisserman [18]. 4. Number of 4-connected neighbouring cells in the bounding box that have different labels, to learn a pairwise cost. 5. Each segment in the bounding box obtained from unsupervised segmentation defines a clique of cells. To learn higher-order potentials, we need a vector HOP that captures the distribution of 0/1 label agreement within cliques. A vector c  RK +1 is constructed for each clique c as (c )k = 1 if ic vi = k. The sum of all c within p gives HOP . In practice, since cliques do not have the same size we employ the normalization strategy described in Gould [12] and transform statistics of all cliques to a standard clique size K (K = 4 in our experiments). 6. The constant 1, used to learn a bias term for different viewpoints.

2.3

Learning

Suppose w is a vector of weights for elements of the joint feature vector. We define wT (x, y) as the `energy' of the labelling y. The aim of learning is to find w such that the energy of the correct label is minimum. Hence we define the label predicted by the algorithm as f (x; w) = y = argmin wT (x, y)
y

(1)

We use a labelled dataset (xi , yi )N i=1 and learn w by solving the following constrained Quadratic Program (QP) N 1 min w 2 + C  i (2) w, 2 i=1 s.t. ^ i ) - (xi , yi )) + i  (yi , y ^ i ) i, y ^  Yi wT ((xi , y i  0 i D2 w  0 Intuitively this formulation requires that the score wT (xi , yi ) of any ground truth labelled ^ i ) of any other labelling y ^ i by the distance image xi must be smaller than the score wT (xi , y ^ i ) minus the slack variable i , where w 2 and i are between the two labellings (yi , y minimized. The regularization constant C adjusts the importance of minimizing the slack variables. The above formulation has exponential constraints for each training image. For tractability, training is performed by using the cutting plane training algorithm of Joachims et al. [15] which maintains a working set Yi of most violated constraints (MVCs) for each image. Gould [12] adapts this algorithm for training higher-order potentials. It uses D2 as a second order curvature constraint on the K + 1 weights for the higher-order potentials, which forces them to make a concave lower envelope. This encourages most cells in the image segments to agree in visibility labelling. D2 is an appropriately 0-padded (to the left and right) version of   -1 2 1 0 ...  .  . . .. .  . . . . . . . 0 . . . -1 2 -1

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

5

^ is called the loss function. It depends on the amount The distance between two labels y and y of overlap between the two bounding boxes and the Hamming distance between the visibility labellings ^) ^) area(p p area(p p ^) = 1 - ^) + · H (v, v (3) (y, y ^) ^) area(p p area(p p ^ ) between two labellings v and v ^ (potentially having difThe mean Hamming distance H (v, v ferent sizes as they might belong to different viewpoints) is calculated after projecting them to the lowest level of the pyramid. By construction of the loss function, the difference in segmentation starts contributing to the loss only after the two bounding boxes start overlapping each other. It also has the nice property of decomposing over the energy terms, as described in Section 2.4.1.

2.4

Inference

To perform the inference as described in Eq. 1 we have to search through Y = A × P × V where A is the set of viewpoints, P is the set of all pyramid locations and V is the exponential set of all combinations of visibility variables. We enumerate over A and P and use an s - t mincut to search over V at every location. By construction, the feature vector w can be decomposed into weight vectors for the different viewpoints i.e. w = [w1 , w2 , . . . , wA ]. In the following description, we will consider one viewpoint and omit the superscript for brevity of notation. w can also be decomposed as [wv , wnv , w pr , wtrunc , W , wHOP , wbias ] into the six components described in Section 2.2. We define the following terms that are used to construct the graph shown in Figure 2(b). i (x, p) are the vectorized HOG features extracted at cell i in bounding box p. Unary terms Fi (p) = wv,i T i (x, p) and Bi (p) = wnv,i T i (x, p) are the responses at cell i for object and occlusion filters respectively. Ri = w pr,i is the prior for cell i to be labelled 0. Constant term C(y) = wtrunc · c(p) + wbias is the sum of image boundary truncation cost and bias. E is the set of 4-connected neighbouring cells in p and W is the pairwise weight. C (p) is the set of all cliques in p and c (vc ) is the higher-order potential for clique c having nodes with visibility labels vc . Combining these terms, the energy for a particular labelling is formulated as
wh

E (x, y) = wT (x, y) =  Fi (p)vi + Bi (p)(1 - vi ) + Ri (1 - vi )
i=1

(4) W |vi - v j | +
cC (p)

+

(i, j)E





c (vc ) + C(y)

c (vc ), the higher-order potential for clique c is defined as mink=1...K (sk ic vi + bk ), following Gould [12]. Intuitively, it is the lower envelope of a set of lines whose slope is defined as sk = M K ((wHOP )k - (wHOP )k-1 ) and intercept as bk = (wHOP )k - sk k (recall that wHOP is a K + 1 dimensional weight vector). M is the size of the clique. The normalization in sk makes the potential invariant to the size of the clique (refer to Gould [12] for details). Figure 2(a) shows a sample higher-order potential curve for a clique of K cells. Given an image, a location, and a viewpoint we use s - t mincut on the graph construction shown in Figure 2(b) to find the labelling v that minimizes the energy in Eq. 4. Each variable vi , i  {1, 2, . . . , wh} defines a node and each clique has K - 1 auxiliary nodes in the graph, z1 . . . zK -1 . For a detailed derivation of this graph structure please see Boykov and Kolmogorov [3] and Gould [12]. After the maxflow algorithm finishes, the nodes vi still connected to s are labelled 1 and others are labelled 0.

6

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

(a)

(b)

Figure 2: (a): Concave higher-order potentials encouraging cells in a clique to have the same binary label, (b): Construction of graph to compute the energy minimizing binary labelling of cells by s - t mincut. Algorithm 1 Response-transfer between object detectors in overlapping regions for all o  [1, L] do {L is the number of objects,  denotes the Hadamard product} for all p  P do B(p) = C(p)  1[V (p) = 0] {Transfer equation for all cells in p} end for y = argminy wT (x, y) C(y (p)) = F (y (p))  y (v) {Update equations for all cells in p} V (y (p)) = o · y (v) end for

2.4.1

Loss-augmented Inference

Loss-augmented inference is an important part of the cutting plane training algorithm (`separation oracle' in Joachims et al. [15]) and is used to find the most violated constraints. It T ^ ) - (y, y ^ ), where y is the ground-truth labelling. is defined as yMVC = argminy ^ w (x, y Our formulation of the loss function makes it solvable with the same complexity as normal inference (Eq. 1) by decomposing the loss over the terms in Eq 4. The first term of Eq. 3 is added to C(y), while the second term is distributed across Fi (p) and Bi (p) in Eq. 4.

2.5

Detection of Multiple Objects

Multiple objects of interest might overlap. Running the individual object detectors separately leaves regions of ambiguity in overlapping areas if multiple detectors mark the same location as visible. We find that running one iteration of  -expansion (see Boykov et al. [4]) in overlapping areas resolves ambiguities coherently. The detectors are run sequentially. We maintain a label map V that stores for each cell the label of the object that last marked it visible, and a collected response map C that stores for each cell the object filter response (Fi (p)) from the object that last marked it visible. While running the location search for object o, we transfer object filter responses from C to the occlusion filter response map (B(p)) for the current object as described in Algorithm 1. This is effectively one iteration

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

7

of  -expansion (see supplementary material for details). It causes decisions in overlapping regions to be made between responses of well-defined object filters rather than between responses of an object filter and a generic occlusion filter. Such response-transfer requires the object models to be compatible with each other. We achieve this by training the object models together as if they were different viewpoint components of the same object. The bias term in the feature vector makes the filter responses of different components comparable.

2.6

3D Pose Estimation

The basic principle of many model based 3D pose estimation algorithms is to fit a given 3D model of the object to its corresponding edges in the image e.g. in Choi and Christensen [5], the 3D CAD model is projected into the image and correspondences between the projected model edges and image edges are set up. The pose is estimated by solving an Iterative Re-weighted Least Squares (IRLS) problem. However, partial occlusion causes these approaches to fail by introducing new edges. We make the algorithm robust to partial occlusion by first identifying visible pixels of the object using SD-HOP and discarding correspondences outside the visibility mask. We call our extension of the algorithm Occlusion Reasoning-IRLS (OR-IRLS).

3

Evaluation

We implemented SD-HOP in Matlab, with MVC search and inference implemented in CUDA since they are massively parallel problems. Inference on a 640x480 image with 11 scales takes 3s for a single object with a single viewpoint on our 3.4 GHz CPU and NVIDIA GT730 GPU.

3.1

Localization and Segmentation

We evaluated our approach on the CMU Kitchen Occlusion Dataset from Hsiao and Hebert [14]. This dataset was chosen because (1) it provides extensive labelled training data in the form of images with bounding boxes and object masks, and (2) the dataset is challenging and offers the opportunity to compare against an algorithm designed specifically to handle occlusion. For the localization task we generated false positives per image (FPPI) vs. recall curves, while for the segmentation task we measured the mean segmentation error against ground truth as defined by the Pascal VOC segmentation challenge in Everingham et al. [7]. C = 25 (see eq. 2) was chosen by 5-fold cross-validation. While both results are presented for the single pose part of the dataset, multiple poses are easily handled in our algorithm as different components of the feature vector. Figure 3 shows FPPI vs. recall curves compared with those reported by the rLINE2d+OCLP algorithm of Hsiao and Hebert [14] and those generated from our implementation of Gao et al. [10]. Table 1 presents segmentation errors compared with Gao et al. [10]. Hsiao and Hebert [14] do not report a segmentation of the object. Figure 3 shows that while both SD-HOP and Gao et al. [10] have similar recall at 1.0 FPPI, SD-HOP consistently preforms better in terms of area under the curve (AUC). Averaged over the 8 objects, SD-HOP achieves 16.13% more AUC than Gao et al. [10]. Table 1

8
Bakingpan 1 0.8

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION
Colander 1 1 Thermos 1 Pitcher

0.8

0.8

0.8

Recall

Recall

Recall

0.4

0.4

0.4

Recall rLINE2D+OCLP SD-HOP Gao et al. 0 0.2 0.4 FPPI 0.6 0.8 1

0.6

0.6

0.6

0.6

0.4

0.2 0 0 0.2

rLINE2D+OCLP SD-HOP Gao et al. 0.4 FPPI 0.6 0.8 1

0.2

rLINE2D+OCLP SD-HOP Gao et al. 0 0.2 0.4 FPPI 0.6 0.8 1

0.2

0.2

rLINE2D+OCLP SD-HOP Gao et al. 0 0.2 0.4 FPPI 0.6 0.8 1

0

0

0

Saucepan 1 0.8 1 0.8

Cup 1

Scissors 1

Shaker

0.8

0.8

Recall

Recall

Recall

0.4

0.4

0.4

Recall rLINE2D+OCLP SD-HOP Gao et al. 0 0.2 0.4 FPPI 0.6 0.8 1

0.6

0.6

0.6

0.6

0.4

0.2 0 0 0.2

rLINE2D+OCLP SD-HOP Gao et al. 0.4 FPPI 0.6 0.8 1

0.2 0 0 0.2

rLINE2D+OCLP SD-HOP Gao et al. 0.4 FPPI 0.6 0.8 1

0.2

0.2

rLINE2D+OCLP SD-HOP Gao et al. 0 0.2 0.4 FPPI 0.6 0.8 1

0

0

Figure 3: Object localization results on the CMU Kitchen Occlusion dataset Table 1: Mean object segmentation error Object Gao et al. [10] SD-HOP Bakingpan Colander Cup Pitcher Saucepan Scissors Shaker Thermos 0.2904 0.2095 0.2144 0.2499 0.1956 0.2391 0.2654 0.2271 0.1516 0.1249 0.1430 0.1131 0.1103 0.1649 0.1453 0.1285

Table 2: Mean 3D pose estimation error Pose parameter IRLS OR-IRLS X (cm) Y (cm) Z (cm) Roll (degrees) Pitch (degrees) Yaw (degrees) 1.6874 1.4953 8.228 1.1711 7.9100 5.7712 0.5774 0.6516 2.1506 0.7152 2.3191 2.6055

shows that SD-HOP consistently outperforms Gao et al. [10] in terms of segmentation error, achieving 42.44% less segmentation error averaged over the 8 objects. Figure 5 shows examples of the algorithm's output on various images from the CMU Kitchen Occlusion dataset.

3.2

Ablation Study

We conducted an ablation study on the `pitcher' object of the CMU Kitchen Occlusion dataset to determine the individual effect of our contributions. Using the loss function from Gao et al. [10] caused the segmentation error to increase from 0.1131 to 0.1547 and area under curve (AUC) of FPPI vs. recall to drop from 0.7877 to 0.7071. To discern the effect of 4-connected pairwise terms we removed the higher order terms from the model too. Using the pairwise terms as described in Gao et al. [10] caused the segmentation error to increase from 0.1547 to 0.2499 and AUC to decrease from 0.7071 to 0.6414. Lastly, to quantify the effect of higher order potentials, we compared the full SD-HOP model against one with higher order potentials removed. Removing higher order potentials

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

9

Figure 4: 3D pose estimation. Left to right: Pose estimation with IRLS, SD-HOP raw segmentation mask, SD-HOP refined segmentation mask, Pose estimation with OR-IRLS. Best viewed in colour. caused the segmentation error to increase from 0.1131 to 0.1430 and AUC to drop from 0.7877 to 0.7544. We hypothesize that for small objects like the ones in the CMU Kitchen Occlusion dataset, 4-connected pairwise terms are almost as informative as higher order terms. To check this hypothesis we tested the effect of removing higher order potentials on a close-up dataset of 41 images of a pasta-box occluded by various amounts through various household objects. Removing the higher order potentials caused the segmentation error to increase from 0.1308 to 0.1516 and area under curve AUC to drop from 0.9546 to 0.9008. This indicates that higher order terms are more useful for objects with larger and hence more informative segments.

3.3

3D Pose Estimation

We collected 3D pose estimation results produced by IRLS and OR-IRLS on a dataset which has 17 images of a car-door in an indoor environment. The ground truth pose for the cardoor was obtained by an ALVAR marker alv [1]. Table 2 shows the mean errors in the six pose parameters. To discern the effect of errors inherent in the pose estimation process from the effect of occlusion reasoning, the pose of the cardoor was constant throughout the dataset, with various partial occlusions being introduced. The granular HOG cell-level mask produced by SD-HOP caused some important silhouette edges to be missed for pose estimation. To solve this problem we utilized the unsupervised segmentation done earlier for defining higher order terms. If more than 80% of the area within a segment was marked 1, we marked the whole segment with 1. Since segments follow object boundaries, this produced much cleaner masks for pose estimation. Figure 4 shows the masks and pose estimation results for an example image from the dataset, with more such examples presented in the supplementary material. Note that the segmentation errors mentioned in Table 1 use the raw masks.

4

Conclusion

We presented an algorithm (SD-HOP) that localizes partially occluded objects robustly and segments their visible regions accurately. In contrast to previous approaches that model occlusion, our algorithm uses higher order potentials to reason at the level of image segments and employs a loss function that targets both localization and segmentation performance. We demonstrated that our algorithm outperforms existing approaches on both tasks, when evaluated on a challenging dataset. Finally, we have shown that the segmentation output from

10

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

Figure 5: Object localization and segmentation results on the CMU Kitchen Occlusion dataset. Left: Image, Center: Raw mask from SD-HOP, Right: Refined mask from SD-HOP SD-HOP can be used to improve pose estimation performance in the presence of occlusion. Avenues of future research include (1) training from weakly labelled data i.e. without segmentations, (2) a post-training algorithm to make object models comparable without having to train them together, and (3) using the occlusion information to reason about interactions between objects in scene understanding applications. We would like to acknowledge Ana Huamán Quispe's help with implementing this system on a bimanual robot. The system was used to enable the robot to pick up partially visible objects lying on a table.

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

11

References
[1] ALVAR tracking library. http://virtual.vtt.fi/virtual/proj2/ multimedia/alvar/index.html. Acessed: 2015-05-03. [2] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. Contour detection and hierarchical image segmentation. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 33(5):898­916, 2011. URL http://ieeexplore.ieee. org/xpl/login.jsp?tp=&arnumber=5557884. [3] Yuri Boykov and Vladimir Kolmogorov. An experimental comparison of mincut/max-flow algorithms for energy minimization in vision. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 26(9):1124­1137, 2004. URL http: //ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1316848. [4] Yuri Boykov, Olga Veksler, and Ramin Zabih. Fast approximate energy minimization via graph cuts. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 23 (11):1222­1239, 2001. URL http://ieeexplore.ieee.org/xpl/login. jsp?tp=&arnumber=969114. [5] Changhyun Choi and Henrik I Christensen. Robust 3D visual tracking using particle filtering on the special Euclidean group: A combined approach of keypoint and edge features. The International Journal of Robotics Research, 31(4):498­519, 2012. URL http://ijr.sagepub.com/content/early/2012/03/01/ 0278364912437213. [6] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886­893. IEEE, 2005. URL http: //ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=1467360. [7] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The Pascal Visual Object Classes (VOC) Challenge. International Journal of Computer Vision, 88 (2):303­338, June 2010. URL http://link.springer.com/article/10. 1007%2Fs11263-009-0275-4. [8] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficient graph-based image segmentation. International Journal of Computer Vision, 59(2):167­ 181, 2004. URL http://link.springer.com/article/10.1023%2FB% 3AVISI.0000022288.19776.77. [9] Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object detection with discriminatively trained part-based models. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(9):1627­1645, 2010. URL http: //ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5255236. [10] Tianshi Gao, Benjamin Packer, and Daphne Koller. A segmentation-aware object detection model with occlusion handling. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1361­1368. IEEE, 2011. URL http: //ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5995623.

12

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

[11] Ross B Girshick, Pedro F Felzenszwalb, and David A Mcallester. Object detection with grammar models. In Advances in Neural Information Processing Systems, pages 442­ 450, 2011. URL http://citeseerx.ist.psu.edu/viewdoc/summary? doi=10.1.1.231.2429. [12] Stephen Gould. Max-margin learning for lower linear envelope potentials in binary markov random fields. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 193­200, 2011. URL http://ieeexplore.ieee. org/xpl/articleDetails.jsp?arnumber=6945904. [13] Stefan Hinterstoisser, Cedric Cagniart, Slobodan Ilic, Peter Sturm, Nassir Navab, Pascal Fua, and Vincent Lepetit. Gradient response maps for real-time detection of textureless objects. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34 (5):876­888, 2012. URL http://ieeexplore.ieee.org/xpls/abs_all. jsp?arnumber=6042881. [14] Edward Hsiao and Martial Hebert. Occlusion reasoning for object detection under arbitrary viewpoint. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 3146­3153. IEEE, 2012. URL http://ieeexplore.ieee. org/xpl/login.jsp?tp=&arnumber=6248048. [15] Thorsten Joachims, Thomas Finley, and Chun-Nam John Yu. Cutting-plane training of structural SVMs. Machine Learning, 77(1):27­59, 2009. URL http://link. springer.com/article/10.1007%2Fs10994-009-5108-8. [16] Pushmeet Kohli, Philip HS Torr, et al. Robust higher order potentials for enforcing label consistency. International Journal of Computer Vision, 82(3):302­ 324, 2009. URL http://ieeexplore.ieee.org/xpl/login.jsp?tp= &arnumber=4587417. [17] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. Grabcut: Interactive foreground extraction using iterated graph cuts. In ACM Transactions on Graphics (TOG), volume 23, pages 309­314. ACM, 2004. URL http://dl.acm.org/ citation.cfm?id=1015720. [18] Andrea Vedaldi and Andrew Zisserman. Structured output regression for detection with partial truncation. In Advances in neural information processing systems, pages 1928­1936, 2009. [19] Xiaoyu Wang, Tony X Han, and Shuicheng Yan. An HOG-LBP human detector with partial occlusion handling. In Computer Vision, 2009 IEEE 12th International Conference on, pages 32­39. IEEE, 2009. URL http://ieeexplore.ieee.org/ xpl/login.jsp?tp=&arnumber=5459207. [20] Yu Xiang and Silvio Savarese. Object Detection by 3D Aspectlets and Occlusion Reasoning. In Computer Vision Workshops (ICCVW), 2013 IEEE International Conference on, pages 530­537. IEEE, 2013. URL http://ieeexplore.ieee.org/xpl/ articleDetails.jsp?reload=true&arnumber=6755942. [21] Menglong Zhu, Konstantinos G Derpanis, Yinfei Yang, Samarth Brahmbhatt, Mabel Zhang, Cody Phillips, Matthieu Lecce, and Kostas Daniilidis. Single image 3D object detection and pose estimation for grasping. In Robotics and

BRAHMBHATT et al.: OCCLUSION-AWARE OBJECT DETECTION

13

Automation (ICRA), 2014 IEEE International Conference on, pages 3936­3943. IEEE, 2014. URL http://www.cis.upenn.edu/~menglong/papers/ icra2014_object_grasping.pdf.

Downloaded from orbit.dtu.dk on: Dec 21, 2016

Measuring and Modelling Delays in Robot Manipulators for Temporally Precise Control using Machine Learning.

Andersen, Thomas Timm; Amor, Heni Ben; Andersen, Nils Axel; Ravn, Ole Published in: Proceedings of IEEE ICMLA'15

Publication date: 2015 Document Version Publisher's PDF, also known as Version of record Link to publication

Citation (APA): Andersen, T. T., Amor, H. B., Andersen, N. A., & Ravn, O. (2015). Measuring and Modelling Delays in Robot Manipulators for Temporally Precise Control using Machine Learning. In Proceedings of IEEE ICMLA'15. IEEE.

General rights Copyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright owners and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights. · Users may download and print one copy of any publication from the public portal for the purpose of private study or research. · You may not further distribute the material or use it for any profit-making activity or commercial gain · You may freely distribute the URL identifying the publication in the public portal ? If you believe that this document breaches copyright please contact us providing details, and we will remove access to the work immediately and investigate your claim.

Measuring and Modelling Delays in Robot Manipulators for Temporally Precise Control using Machine Learning
Thomas Timm Andersen , Heni Ben Amor , Nils Axel Andersen and Ole Ravn  Department of Automation and Control, DTU Electrical Engineering Technical University of Denmark, DK-2800 Kgs. Lyngby, Denmark. {ttan, naa, or}@elektro.dtu.dk  Institute for Robotics and Intelligent Machines, College of Computing Georgia Tech, Atlanta, GA 30332, USA. hbenamor@cc.gatech.edu

Abstract--Latencies and delays play an important role in temporally precise robot control. During dynamic tasks in particular, a robot has to account for inherent delays to reach manipulated objects in time. The different types of occurring delays are typically convoluted and thereby hard to measure and separate. In this paper, we present a data-driven methodology for separating and modelling inherent delays during robot control. We show how both actuation and response delays can be modelled using modern machine learning methods. The resulting models can be used to predict the delays as well as the uncertainty of the prediction. Experiments on two widely used robot platforms show significant actuation and response delays in standard control loops. Predictive models can, therefore, be used to reason about expected delays and improve temporal accuracy during control. The approach can easily be used on different robot platforms. Keywords-Robot control; Automation; Machine learning algorithms;
Figure 1. Temporally precise control of an industrial robot is realized by modelling the inherent delay in the system. The picture depicts a fast robot movement during data acquisition. Recorded data is processed using machine learning algorithms to generate predictive models for system and response delay.

I. I NTRODUCTION For robots to engage in complex physical interactions with their environment, efficient and precise action generation and execution methods are needed. Manipulation of small objects such as screws and bolts, for example, requires spatially precise movements. However, in dynamically changing environments, spatial precision alone is often insufficient to achieve the goals of the task. In order to intercept a rolling ball on the table, for instance, a robot has to perform temporally precise control--the right action command has to be executed at the right time. Yet, by their very nature, actuation commands are never instantaneously executed. Delays and latencies, therefore, play an important role in temporally precise control and can occur at different locations in the robot control loop. Actuation delay is the delay type that most roboticists are aware of. When an action command is sent to the robot's controller, it takes a short while to process the command and calculate the required joint motor input. Imagine a welding robot with an uncompensated actuation delay of 50 ms, working an object on a fairly slow-moving conveyor belt with a speed of 0.5 m/s. The incurred delay would result in a tracking error of 2.5 cm, which could easily destroy a product, or at the very least result in a suboptimal result.

A different type of delay is the response delay which measures the amount of time until a real-world event is sensed, processed and updated in memory. Response delay is usually assumed zero, as one would naturally assume that this is sampled and transmitted instantaneously whenever a motion occurs. However, since there is a sampling clock and since the controller also needs some time to pack the data for transmission, the response delay can be a non-negligible amount of time. An important implication of the response delay is the discrepancy between the robot's belief of its own state and the true value of that state. When data is received from the robot, indicating that the robot is at a certain position moving with some velocity, the data is in reality describing a state in the past. In order to effectively act in dynamic environments and reason about timing, a robot has to be aware of both the actuation delay as well as the response delay. Sadly, such information is not readily accessible form the robotics company, and no method is currently available for identifying it. This has lead many researches to develop their own

controllers, but this is rarely an opportunity for industrial users. Safety during operation is the most crucial issue for robot controllers, but each robotic company may has different strategies which affect the architecture of the robot controller. It is therefore necessary to consider the controller as a black box from which we must learn the controllerdependent delay characteristics. Direct measurement of these delays is typically difficult, since the different delay types are convoluted and hard to separate. An important challenge is therefore the question of how to separate these two delays as, depending on the executed task, a robot has to compensate for a different type of delay. In this paper, we present a methodology for measuring and modelling the inherent delays during robot control. We introduce an experimental setup which allows us to collect evidence for both the actuation delay, as well as the response delay. The collected data is then used to learn controllerdependent predictive models of each type of delay. The learned predictive models can be used by a robot to reason about timing and perform temporally precise control. The contributions of this publication are three-fold. First, we provide a generic method for measuring the actuation and response delay of a robot manipulator. Due to its datadriven nature, the method can be used on a variety of actuators. Second, we show how existing machine learning methods can be used to model and predict the inherent delay. Finally, we show modelling results for two widely used robot platforms, namely the Kuka KR 5 Sixx and the Universal Robots' UR10 robot. The acquired data is made publicly available to the robotics community [1]. II. R ELATED WORK Modelling time delays is a vital research topic in computer network engineering. In order to ensure fast communication over large computer networks, various models have been put forward to model the mean delay experienced by a message while it is moving through a network [15]. These analytic models typically require the introduction of assumptions, e.g. Kleinrock's independence assumption [11], to make them tractable. Yet, since the network communication is based on a limited number of communication protocols, it is reasonable to use and constantly refine such analytic approaches. Another domain in which latencies and delays play a vital role is virtual reality (VR). As noted in [6], latencies lead to a sensory mismatch between ocular and vestibular information, can reduce the subjective sense of presence, and most importantly, can change the pattern of behavior such that users make more errors during speeded reaching, grasping, or object tracking. In VR applications, measuring and modelling delays can be very challenging, since the delay can heavily vary based on the involved software components, e.g., rendering engine, as well as highly heterogeneous hardware components, e.g., data gloves, wands, tracking

devices etc. In [6] a methodology for estimating delays is presented, which focuses on VR application domains. In robotics, the delay inherent to control loops can have a detrimental impact on system performance. This is particularly true for sensor-based control used in autonomous robots. Visual servoing of a robot, for example, can be sensitive to the delays introduced through image acquisition and processing [9]. Similarly, delays in proprioception can produce instabilities during dynamic motion generation. In [2], a dynamically smooth controller has been proposed that can deal with delay in proprioceptive readings. However, the approach assumes constant and known time-delay. A major milestone in robot control with time-delay was the ROTEX experiment [8]. Here, extended Kalman filters and graphical representation were used to estimate the state of objects in space, thereby enabling sensor-based long-range teleoperation. How to effectively deal with such communication delays has been a central research question in robotic tele-operation. Delays in robot control loops are not limited to sensor measurements only. A prominent approach for dealing with actuation delays is the Smith Predictor [17]. The Smith Predictor assumes a model of the plant, e.g. robot system, and can become unstable in the presence of model inaccuracies. A different approach has been proposed in [3]. A neural network was first trained to predict the state of mobile robots based on positions, orientations, and the previously issued action commands. The decision making process was, then, based on predicted states instead of perceived states, e.g. sensor readings. The approach presented in our paper follows a similar line of thought. However, instead of predicting specific states of the robot, we are interested in predicting the delay occurring at different parts of the control loop. III. M ETHODOLOGY In this section, we describe a data-driven methodology for modelling delays in robotic manipulators. We show how to acquire evidence for different types of delays and how this information can be used in conjunction with machine learning methods to produce predictive models for control. A. Measuring the delay The purpose of the presented method is to establish the actuation and response delay that a high-level control program can expect when issuing commands to a robotic controller. To measure these delays, we need to synchronize the issuing of commands with the control loop of the robot controller. To this end, we use the published current state of the robot, which most controllers send out in each control cycle. The overall system setup which will be used in the remainder of the paper is depicted in Figure 2 (left). A highlevel control program is running on a computer, which sends the commands to the robot control box. The control box,

Transmission Delay

Actuation Delay

Computer
Transmission Delay

Control Box
Response Delay

Robot

Gyro/IMU

Processing Delay

Figure 2. Left: Delays during the control of a robot manipulator. Transmission delay affects information flow between main control computer and the robot control box. Actuation delay and response delay are introduced in the communication between the control box and the physical robot. Right: For delay modelling an external sensor is mounted, e.g. a gyroscope, to measure discrepancies between command times and execution times.

in turn, calculates and issues the low-level commands that drive the robot. The delay between the high-level controller and the control box will be referred to as the transmission delay. The transmission delay has already been extensively studied in computer networking [15] and will thus not be treated in this paper. It is particularly crucial in tele-operation scenarios, in which the high-level controller and the robot control box may be separated by thousands of kilometers. In this paper, however, we focus on the delays incurred between the control box and the robot manipulator. A command that is received by the control box from the high-level program at time t = 0 is typically only executed after a delay of 1 . This is the actuation delay. Similarly, once a command is executed by the robot at time t = 1 , it takes another delay of 2 until the motion is reflected in the controllers memory and transmitted to the high-level program running on the central computer. This is the response delay. The fundamental idea of our approach is to compare time stamps at the moment a command is issued, the moment the command is executed, and the moment the command gets reflected in the published state of a robot. To this end, it is important to know the ground truth about the true timing of the robot movement. This is realized using an external apparatus in our setup, e.g., a gyroscope or accelerometer, see Figure 2 (right). 1) Determining ground truth: Since we want to measure the delay of the robot, we need a reliable and accurate method of measuring robot motion. The method needs to measure the current motion without adding a significant delay of its own. This can be achieved by imposing a significantly higher sampling rate than the robot controller. We use microelectromechanical (MEMS) gyroscopes, or angular rate sensors, for the revolving joints, and MEMS accelerometers for prismatic joints. They offer very high sampling rates of several orders of magnitude higher than many robot controllers publish (e.g. several kHz for affordable sensors), and practically no delay from motion to available measurement. Such sensors cannot readily be

used to infer where in the kinematical chain a motion has occurred, hence measurements have to be performed a single joint at a time. Gyroscope measurements often come with significant noise, while accelerometer measurements suffer from drift. However, both of these issues can be compensated for using simple offline filtering in-between measurement and training the model. 2) Acquiring measurements: As mentioned before, our approach is based on comparing time stamps throughout the robot control loop. To this end, we use the published state from the robot as the main sample clock and reference. An experimental trial starts at t = 0 upon reception of a first package from the controller. The system time stamp is recorded as soon as data is read, and the byte-encoded package is stored for later parsing to extract the current joint state. Upon reception, a command is sent instructing the robot to start moving a single joint, which we monitor with our angular rate sensor or accelerometer. The commanded movement consists of a rapid acceleration in one direction, followed by a fast deceleration before returning to return to the starting pose. The entire motion trial takes about a second, and all packages received until the robot stands still are stored. Sensor readings from the external sensor are stored by the central computer in order to identify the ground truth time stamp of the moment in which the robot moved. There are several perturbations that can lead to variations in the incurred delays, in particular physical perturbations. For instance, the force resulting from the gravitational pull on the robot varies with the joint configuration of the robot, just as the direction of motion effects whether the motor needs to work against or along gravity. The different size of motors and gearing in the robot also yields varying results. These perturbations lead to varying static and kinetic friction in the moving parts of a robot. This variation in turn leads to a varying actuation delay. As the magnitude of the static friction is usually larger than that of the kinetic friction, we assume that the delay is

mostly affected by the robot's joint configuration when the motion starts. We assume that the effect by the other joints during a motion after the static friction has been overcome can be neglected. A similar assumption of joint independence if often employed on the joint position controller when using Independent joint control [14]. To acquire a representative data set for modelling delays, we therefore need to map out the delay of each joint for all the different joint combinations, moving in both positive and negative direction. To capture variance in the delay, each combination of joint configuration and direction should be measured several times. 3) Filtering data and computing delays: When extracting the delays, we evaluate the difference between the recorded data. Before doing that, thought, we use a high order lowpass FIR filter (Figure 3) on the data from the angular rate sensor and correct for any drifting of the accelerometer, based on data recorded while the sensor was held stationary on the robot.
Frequency (Hz)
0 5
Speed unfiltered Speed filtered PSD Unfiltered PSD Filtered

B. Learning Predictive Models of Delay Next, we want to use the recorded data in order to learn predictive models of robot latencies. Once a predictive model is learned, it can be used by a robot to infer the most likely delay in a given situation. A common approach in robot control is to use a path planner running on the central computer to generate a starting joint configuration and an execution time of the trajectory. To find the actual real time that the robot will use to get to the goal state, we can query the learned predictive models for each moving joint. The individual delay is then added to the execution time of each joint to identify the real execution time. As input features for the model we use the starting joint configuration of the robot. As mentioned before, forces acting on the robot vary depending on the joint configuration and impact in particular the actuation delay. The output of the model is the expected delay. We learn individual models for the actuation delay and the response delay, since these two delays are unrelated. In line with the assumption of independence between the joints, a separate model is learned for each joint. Introducing the above structured approach, allows for accurate predictions of the delay. To evaluate how a unified model, predicting the delay of all joints, performs, such a model is also trained. The goal of learning is to generate predictive models that can generalize to new situations and lead to accurate predictions of the expected latencies. To this end, we use three different machine learning methods, namely neural networks (NN) [4], regression trees (RT) [5], and Gaussian processes (GPR) [12]. We use these methods as they can all effectively recover nonlinear relationships between input and output data. In our specific implementation, we used a feed-forward neural network with 30 neurons in a single hidden layer. Learning was performed using the Levenberg-Marquardt [10] algorithm. In contrast, the regression tree method hierarchically partitions the training data into a set of partitions each of which is modelled through a simple linear model. Both NNs and the RTs produce a single result and do not provide information about the uncertainty in the predicted value. In contrast to that, GPR can learn probabilistic, nonlinear mappings between two data sets. Due to the inherent noise and related phenomena, uncertainty handling is a crucial issue when dealing with delays. By providing the mean and the variance of any prediction, the GPR approach allows us to reason about uncertainty of our prediction. Together, mean and variance form Gaussian probability distribution indicating the expected range of predictions. This information can potentially be exploited to generate upper- and lower-bounds for the expected delays, which is in contrast to both NN and RT. As both NN, RT, and GPR are well known machine learning methods and we do not add anything to these

500

1000

1500

2000

2500

3000

3500 30 20

0 0 -10 -20 -30 -5 0 10 20 30 40 50 -40 60

Time (sec)

Figure 3. Gyroscope readings are filtered using a FIR filter. A 60 second datastream (green), recorded without moving the robot, is passed through the filter to remove noise (blue). The frequency component of the data before and after filtering is shown in red and black, calculated using Welch's Power Spectral Density (PSD) estimate [18]

To calculate the delay, we evaluate our two data series generated in each trial; the speed output from the robot controller and the filtered sensor data. The actuation delay is the difference between the moment a command is sent to the robot and the moment a sensor registers the motion, while the response delay is the difference between the moment a sensor registers motion and the moment it is reflected in the robot's current speed data. Both are calculated while taking into account the transmission delay from Figure 2 (left). Even when filtering out the noise, it can be challenging to establish the exact moment in time when the sensor determines that a motion has started as the measured speed is hardly ever zero. Instead we identify extrema of our recorded data to detect the time difference between the set target speed, the measured speed, and the reported current speed.

Magnitude (dB)

Speed (deg/sec)

10

Figure 4. The Universal Robot UR10 with mounted measuring equipment. The enclosure keeps the sensor at a stable temperature thus avoiding temperature-related drift in measurements.

methods, the theory behind them will not be covered further in this paper. IV. R ESULTS A. Experimental setup In our experiments, we model the performance of both a Kuka KR 5 sixx (Figure 1) and a Universal Robot UR10 (Figure 4). To generate the training data, we mounted a MPU6000 combined angular rate sensor and accelerometer to the end-effector. To avoid temperature-related drifts, we
100
Commanded speed Measured speed Reported speed Actuation delay Response Delay

moving 10 times in both positive and negative direction in 1,920 different joint configurations. A total of 33,500 trials were performed on each robot in order to generate a comprehensive dataset, to be released to the public [1]. For purposes of machine learning, only a subset of the data was later used. To be able to compare the performance of the two robots, we used the same 1,920 physical joint configuration (i.e. all links vertical) for both robots rather than using the same joint values. This is a necessity since the DenavitHartenberg parameters of the robots are not identical and the home position varies, thus positive joint rotation on one robot might lead to negative joint rotation on the other. Sampling only a subspace of the robots' total workspace does not introduce bias in the data, but rather limits the model to predict delays within that subspace. By sampling more poses, the model can routinely be extended to cover the entire workspace if needed. B. Delay output As explained in Section III-A3, delays are determined by evaluation of the extrema of the recorded motions. An example of how the delays vary for the two robots can be seen on Figure 6. The distribution of the actuation delays can be seen on Figure 7, while a boxplot showing the individual delays per joint is shown on Figure 8. The same plots for the reaction delays can be seen on Figure 9 and Figure 10, respectively. C. Model comparison

Speed (deg/sec)

50

0

-50

-100 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7

Time (sec)

Figure 5. Typical plot of logged data from a single trial. 33,500 trials were completed on each robot.

mount the sensor on the robot in an enclosure with low heat conductivity and then let the sensor warm up before measurements. Since these robots only have revolute joints, only the angular rate sensor is used, which outputs data at a rate of 8 kHz. To collect the data used for training the model, we perform a series of short trials, wherein the robot is commanded to perform a fast acceleration and deceleration motion. For controlling the Kuka robot, the Kuka RSI [7] protocol is used. It operates with a sample rate of 83.3 Hz (12 ms). As argued in [13], the UR10 is controlled using URScript SpeedJ commands. It operates with a sample rate of 125 Hz (8 ms). An example trajectory, along with typical outcome of a trial, can be seen on Figure 5. The plots clearly show a significant time difference in the commanded speed, the reported speed and the measured speed. To capture the variations of the delays, we performed trials on 4 different joints,

The extracted delays were used to train and validate models based on different machine learning algorithms, namely NN, RT, and GPR. For the NN and RT algorithms, we used the standard MatLab implementation, while we used GPstuff [16] for the GPR implementation. The starting joint configuration, the actuated joint, and the rotational direction were used as input. The delays that were measured at each input combination were used for training and testing, using k-fold cross validation with 10 folds to limit overfitting the data and to give an insight on how the model will generalize to an independent dataset. The mean squared error (MSE) from each fold were averaged together and used as a measure of how well the model predicts delays. Models for predicting both the delay of individual joints, as well as a combined model that can predict the delay of all joints were trained. The mean error of each model is derived by taking the square root of the MSE and is shown in Table I and II. The tables also shows the resulting mean error if the delay was assumed that of the median of the corresponding boxplots. This gives an indication of the performance of the trained models. Lower values indicate better generalization capabilities, while larger mean error values indicate poor prediction performance.

Joint 2 Joint 3 angle (deg) angle (deg)

-20 -50 -80 -110 65 25 -15 -55 0.12 0.10 0.08 0.06 0.03 0.02 0.01 0

Joint 2 Joint 3 angle (deg) angle (deg) Actuation Delay (s) Response Delay (s) Number of trials in bin
0.08 0.085 0.09 0.095 0.1 0.105

-70 -100 -130 -160 45 -5 -35 -75 0.065 0.045 0.025 0.005 0.03 0.02 0.01 0

Figure 6. Actuation and response delay for joint 3 moving in positive direction as a function of varying joint 2 and 3. The red graph is the mean and the gray area is ±2 standard deviations, corresponding to a 95% confidence interval. Note the different y axis interval. Left: Kuka. Right: Universal Robot.
6000 6000 5000 4000 3000 2000 1000 0 0 0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04

Number of trials in bin

Response Delay (s)

Actuation Delay (s)

5000 4000 3000 2000 1000 0 0.075

Delay (s)

Delay (s)

Figure 7.

Combined distribution of the actuation delay of all joints. Note the different x axis interval. Left: Kuka. Right: Universal Robot.
0.05

0.115 0.04 0.105

Delay (s)

Delay (s)

0.03

0.095

0.085

0.02

0.075 Joint 1 Joint 2 Joint 3 Joint 5 All joints

0.01 Joint 1 Joint 2 Joint 3 Joint 5 All joints

Figure 8.
6000

Boxplot of individual joint's actuation delay. Note the different y axis interval. Left: Kuka. Right: Universal Robot.

Number of trials in bin

Number of trials in bin
0 0.005 0.01 0.015 0.02 0.025 0.03 0.035

8000 6000 4000 2000 0 0 0.005 0.01 0.015 0.02 0.025 0.03 0.035

5000 4000 3000 2000 1000 0

Delay (s)

Delay (s)

Figure 9.

Combined distribution of the response delay of all joints. Left: Kuka. Right: Universal Robot.

0.03

0.03

Delay (s)

0.02

Delay (s)
Joint 1 Joint 2 Joint 3 Joint 5 All joints

0.02

0.01

0.01

0.00

0.00 Joint 1 Joint 2 Joint 3 Joint 5 All joints

Figure 10.

Boxplot of individual joint's response delay. Left: Kuka. Right: Universal Robot.

Table I M EAN ERROR IN MILLISECONDS OF MODEL FIT FOR ACTUATION DELAY. Kuka Median NN RT GPR UR Median NN RT GPR Joint 1 2.27 1.79 1.99 1.85 Joint 1 6.18 4.08 4.63 5.01 Joint 2 2.68 2.55 2.48 2.27 Joint 2 4.64 5.32 5.41 4.89 Joint 3 4.61 4.74 3.74 4.30 Joint 3 6.08 3.86 4.28 3.68 Joint 5 3.51 3.37 3.76 3.39 Joint 5 2.59 2.49 2.72 2.47 Combined 3.67 3.21 3.33 3.48 Combined 5.33 3.12 3.42 3.36 Average 3.35 3.13 3.06 3.06 Average 4.96 3.77 4.09 3.88

the delay and assuming the delay constant at the median of each boxplot would thus decrease the error to within 0.3 cm for a delay within ±6 ms. If we include the whiskers of the boxplot, corresponding to  ±2.7 or 99.3% of the data, the worst case error would within 0.85 cm for a delay within ±17 ms. Figure 8 also shows that on both robots, it is joint 2 that has the highest delay. This is the shoulder joint, and the one that lifts the most. This supports our theory that gravity influences the actuation delay. Figure 10 suggests that the response delay on the other hand is not varying between the joints. This is not surprising, as the response delay, as mentioned previously, is largely incurred by the sampling clock, packing of data and transmission. This most likely happens simultaneously for each joint. The seemingly correlation between actuation and response delay on Figure 6 is a consequence of the relatively low temporal resolution of the robot controller data. This is also why it is more dominant on the Kuka robot. As the sum of the actuation and response delay will always be a multiple of the sample period, an actuation delay a few ms below the mean at a specific pose will result in a response delay a few ms above the mean at that pose. A surprising finding on Figure 9 is that the response delay for the Kuka robot is more than one sample period, which suggests that sampling and transmission of data takes place in separate sample clock cycles. B. Evaluating the models' performance All of the models are able to predict the delays very accurately to within a mean error of 5ms and it is thus difficult to say anything conclusive about which model is best. Though all of the models would have a mean error less than 0.35 cm if used for a typical task like welding, which is an improvement of more than a factor 12 for the Kuka robot and almost a factor 3 for the Universal Robot, compared to using the controllers and not assuming any delay. Comparing the learned models with measuring the delay and assuming it to be static shows an improvement

Table II M EAN E RROR IN MILLISECONDS OF MODEL FIT FOR REACTION DELAY. Kuka Median NN RT GPR UR Median NN RT GPR Joint 1 2.13 1.70 1.86 1.63 Joint 1 4.82 4.11 4.66 3.88 Joint 2 2.37 2.32 2.21 2.17 Joint 2 2.00 2.48 2.44 2.44 Joint 3 4.30 4.68 3.62 4.23 Joint 3 4.08 4.24 4.47 3.75 Joint 5 3.09 2.22 2.31 3.11 Joint 4 4.90 5.22 5.84 5.20 Combined 3.33 2.36 2.37 2.63 Combined 5.09 4.84 5.33 5.05 Average 3.05 2.65 2.47 2.75 Average 4.18 4.01 4.35 3.82

V. D ISCUSSION A. Evaluating the two robots' delays As it can be seen on Figure 7, the actuation delay of the Kuka is significantly higher than on the Universal Robot, even factoring in the higher sample period; the average delay for the Kuka is 7.5 sample periods vs. 2.5 sample periods for the UR. If we relate the figure to the example from the introduction, where a welding robot need to weld an object on a conveyor belt moving at 0.5 m/s, our claim that it is important to compensate for the delay is clearly justified. The Kuka robot would, without compensation, make a welding seam displaced 4.5cm ± 0.5cm from the target, while the Universal Robot would miss with 0.75cm - 1.25cm. A deeper look into the actuation delays, which is on Figure 8, shows that the delays in general only vary with a few ms for each joint. Using our method for measuring

between 6 and 24%. The response delay for the Universal Robot shows the least benefit from modeling. This is most likely due to the fact that the spread of the delays are so small. The missing improvement with machine learning is thus a result of the median delay yield a very good guess, and not a result of the models being poor at learning those delays. It is worth noticing that the mean error in some cases are significantly higher for the Universal Robot models than those of the Kuka robot. This correspond with Figure 6, where the confidence interval is much broader for the Universal Robot than for the Kuka. It should also be noted that GPR does not only supply a prediction of the delay, but also outputs a measure of uncertainty, which is not reflected in the tables. For the Universal Robot's large variance, this is certainly an added bonus. VI. C ONCLUSION In this paper we presented a methodology for measuring and separating actuation and response delays in robot control loops. In addition, we introduced a data-driven approach for modelling inherent delays using machine learning algorithms. We showed that the introduced models can be efficiently used to predict occurring delays during temporally precise control. Real world experiments were used to identify latencies in two widely used robot platforms. The measured delay showed a large potential for improving temporal precision, with more than a factor 12 improvement for one of the robots. All the employed machine learning algorithms showed similar abilities to further improve the accuracy, with no algorithm showing significantly better accuracy than the others. Still, Gaussian processes seem to be better suited for this task, since they provide a probability distribution over the expected delay. In turn, such a distribution can be used to reason about upper- and lower-bounds in temporal precision. In our future work we will investigate how inverse models of time delay can be learned. Given a specific time constraint during a control task, an inverse model can be queried for the most appropriate action which will meet the goals of the task while ensuring time constraints. R EFERENCES
[1] http://aut.elektro.dtu.dk/staff/ttan/delay.html. [2] S. Bahrami and M. Namvar. Motion tracking in robotic manipulators in presence of delay in measurements. In Robotics and Automation (ICRA), 2010 IEEE International Conference on, pages 3884­3889, May 2010.

[3] S. Behnke, A. Egorova, A. Gloye, R. Rojas, and M. Simon. Predicting away robot control latency. In RoboCup 2003: Robot Soccer World Cup VII, Lecture Notes in Computer Science, pages 712­719. Springer Berlin Heidelberg, 2004. [4] C. M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, Inc., New York, NY, USA, 1995. [5] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth and Brooks, Monterey, CA, 1984. [6] M. Di Luca. New method to measure end-to-end delay of virtual reality. Presence: Teleoper. Virtual Environ., 19(6):569­ 584, December 2010. [7] KUKA Robot Group. KUKA.Ethernet RSI XML 1.1, kst ethernet rsi xml 1.1 v1 en edition, 12 2007. [8] G. Hirzinger, K. Landzettel, and Ch. Fagerer. Telerobotics with large time delays-the rotex experience. In Intelligent Robots and Systems '94. 'Advanced Robotic Systems and the Real World', IROS '94. Proceedings of the IEEE/RSJ/GI International Conference on, volume 1, pages 571­578 vol.1, Sep 1994. [9] A.J. Koivo and N. Houshangi. Real-time vision feedback for servoing robotic manipulator with self-tuning controller. Systems, Man and Cybernetics, IEEE Transactions on, 21(1):134­142, Jan 1991. [10] D. W. Marquardt. An algorithm for least-squares estimation of nonlinear parameters. SIAM Journal on Applied Mathematics, 11(2):431­441, 1963. [11] A. Popescu and D. Constantinescu. On kleinrocks independence assumption. In DemetresD. Kouvatsos, editor, Network Performance Engineering, volume 5233 of Lecture Notes in Computer Science, pages 1­13. Springer Berlin Heidelberg, 2011. [12] C. E. Rasmussen and Ch. K. I. Williams. Gaussian Processes for Machine Learning. The MIT Press, 2005. [13] O. Ravn, N. A. Andersen, and T. T. Andersen. Ur10 performance analysis. Technical report, Technical University of Denmark, Department of Electrical Engineering, 2014. [14] M. W. Spong, S. Hutchinson, and M. Vidyasagar. Robot modeling and control. John Wiley & Sons New York, 2006. [15] A. S. Tanenbaum and D. J. Wetherall. Computer Networks. Prentice Hall, 5th edition, 2011. [16] Jarno Vanhatalo, Jaakko Riihim¨ aki, Jouni Hartikainen, Pasi Jyl¨ anki, Ville Tolvanen, and Aki Vehtari. Gpstuff: Bayesian modeling with gaussian processes. The Journal of Machine Learning Research, 14(1):1175­1179, 2013. [17] P.D. Welch. A controller to overcome dead time. ISA Journal, 6(2):28­33, 1959. [18] P.D. Welch. A direct digital method of power spectrum estimation. IBM Journal of Research and Development, 5(2):141­156, 1961.

