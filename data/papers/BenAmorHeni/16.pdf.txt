International Conference on Humanoid Robots (HUMANOIDS), 2014

Online Multi-Camera Registration for Bimanual Workspace Trajectories
Neil T. Dantam

Heni Ben Amor

Henrik I. Christensen

Mike Stilman

Abstract— We demonstrate that millimeter-level bimanual
manipulation accuracy can be achieved without the static
camera registration typically required for visual servoing. We
register multiple cameras online, converging in seconds, by
visually tracking features on the robot hands and filtering the
result. Then, we compute and track continuous-velocity relative
workspace trajectories for the end-effector. We demonstrate the
approach using Schunk LWA4 and SDH manipulators and Logitech C920 cameras, showing accurate relative positioning for
pen-capping and object hand-off tasks. Our filtering software
is available under a permissive license.1

I. I NTRODUCTION
Visual feedback of hand movements provides rich information that can be used to correct for errors and improve manipulation accuracy. Recent evidence suggests that humans
use visual feedback of the hand to guide reach and grasp
tasks [15]. Continuously tracking and monitoring the state of
the hand allows us to dynamically accommodate to internal
and external perturbations, e.g., muscle impairments, thereby
achieving a high degree of robustness during manipulation
tasks.
In robotics, using visual feedback depends on a kinematic registration between the camera and the manipulators.
Typically, this is viewed as a static task: registration is
computed offline and assumed to be constant. In reality,
camera registration changes during operation due to external
perturbations, wear and tear, or even human repositioning.
For example, during the recent DARPA Robotics Challenge
trials, impacts from falls resulted in camera issues which
significantly affected the robot behavior [10]. The pose registration process should be treated as a dynamic task in which
the involved parameters are continuously updated. Such an
online approach to pose registration is challenging, since it
requires the constant visibility of a calibration reference and
sufficient accuracy to perform manipulation tasks.
Bimanual manipulation requires accurate coordination of
both end-effectors. To perform smooth and accurate bimanual manipulation, we propose an online estimation and
control approach that combines (1) visual tracking of the
manipulators, (2) co-estimation of poses for cameras and
end-effectors using a special Euclidean group median and
extended Kalman filter, and (3) continuous geometric interpolation on the special Euclidean group. Our key insight is
to combine perception and control online, using the robot

The authors are with the Institute for Robotics and Intelligent
Machines, Georgia Institute of Technology, Atlanta, GA 30332,
USA.
ntd@gatech.edu, hbenamor@cc.gatech.edu,

hic@cc.gatech.edu
1 software

available at http://github.com/golems/reflex

Fig. 1. Bimanual Schunk LWA4/SDH capping a pen using visual feedback
from online camera registration and end-effector tracking.

body frame as a reference. This work extends the singlecamera and manipulator registration presented in [5] to multicamera and multi-manipulator estimation, and it integrates
the spherical blending approach of [6] to enable continuous
motion of the manipulator in the workspace.
II. R ELATED W ORK
Typical camera registration methods collect a set of calibration data using an external reference object, compute
the calibration, then proceed assuming the calibration is
static. OpenCV determines camera registration from point
correspondences, typically using a chessboard [12]. Pradeep,
et. al, develop a camera and arm calibration approach based
on bundle adjustment and demonstrate it on the PR2 robot
[13]. This approach requires approximately 20 minutes to
collect data and another 20 minutes for computation, a
challenge for handling changing pose online.
Visual servo control incorporates camera feedback into
robot motion control [1], [2]. The two main types of visual
servoing are image-based visual servo control (IBVS), which
operates on features in the 2D image, and position-based
visual servo control, which operates on 3D parameters. Both
of these methods assume a given camera registration. While
IBVS is locally stable with regard to pose errors, under
PBVS, even small pose errors can result in large tracking
error [1]. Compared to both IBVS and PVBS, our method
requires no initial camera registration, instead estimating
the registration online. Additionally, compared to IBVS,
we estimate the full kinematics of the camera and robot,

and thus can directly follow workspace reference poses and
trajectories, such as [6], rather than being limited to imagespace reference points and trajectories.
Other recent work has explored online visual parameter
identification. A single-camera and single-arm version of the
approach in this paper was presented in [5]. [11] tracks a
robot arm to identify encoder offsets. This method assumes
a given camera registration, but is also tolerant to some
registration error. In contrast, our work identifies the camera
registration online. Though we do not explicitly consider
encoder offsets, our method is empirically robust to offsets
◦
of even 30 (see Sect. V). [9] considers bimanual arm and
object tracking with vision and tactile feedback. Though the
hardware and implementation differ from work presented in
this paper, we obtain similar accuracy using inexpensive webcams and without tactile sensing. [16] uses maps generated
from a Simultaneous Localization and Mapping (SLAM)
algorithm to calibrate a depth sensor. In our approach, unlike
typical environments for SLAM, the object to which we
are trying to register our camera – the manipulator – will
necessarily be in motion.

A. Asynchronous Pose Co-Estimation
Each camera image provides pose measurements for visible end-effector features. To reduce estimation latency, we
process and filter the measurements from each camera asynchronously as they arrive rather than collecting images from
all cameras at a fixed timestep.
The kinematic chain through the manipulator, feature, and
camera is defined as:
Swi ⊗ wiSwi0 ⊗ wiSfp = bScj ⊗ cjSfp

We estimate poses of multiple cameras and manipulators
by visually detecting the 3D pose of each manipulator. First,
we detect texture features on the end-effector and fit a
transform, providing an instantaneous estimate of camera and
hand pose. To obtain sufficient accuracy for manipulation we
then combine median and extended Kalman filtering of these
poses.
To use the robot body as a reference for camera registration, we track the 3D pose of features on the end-effector.
These 3D poses can be estimated with marker-based [14]
and model-based approaches [3]. Marker-based approaches
require attaching fiducials to known locations on the robot,
such as the fingers. Model-based tracking, on the other hand,
requires accurate polygon meshes of the tracked object. In
our implementation, we use the ALVAR library [14] for
marker-based tracking.
For computational reasons, we used the dual quaternion
representation for the special Euclidean group SE(3). Compared to matrices, the dual quaternion has lower dimensionality and is more easily normalized, both advantages for our
filtering implementation. Compared to the unit quaternion
plus translation vector representation, dual quaternions are
more convenient for algebraic manipulation because they are
chained through multiplication. For Euclidean transformations, we use the conventional coordinate notation where
the leading superscript denotes the parent frame and the
following subscript denotes the child frame, i.e., aSb gives
the origin of b relative to a. The transformation aSb followed
by bSc is given as the dual quaternion multiplication aSb ⊗
b
Sc = aSc . We represent an orientation quaternion as aqb , a
translation vector as axb , a rotational velocity as aωb , and the
combined translational and rotational velocity as aχ˙ b .

(1)

where bSwi is the encoder-measured pose of wrist i in body
0
frame, wiSwi0 is the estimated offset pose of wrist i, wiSfp is
the encoder-measured transform from wrist i to feature p on
the hand, bScj is the estimated pose registration of camera j,
and cjSfp is the visually-measured pose feature p in camera
j. For a depiction of the setup see Fig. 3.
Based on (1), we produce measurements for wrist offset
wi 0
Swi and camera registration bScj :
w
iS 0
]
wi
bg
Scj

III. E STIMATION : P OSES OF C AMERAS AND H ANDS

0

b

0

= (bSwi )−1 ⊗ bd
Scj ⊗ cjSfp ⊗ (wiSfp )−1

(2)

wi0

(3)

b

= Swi ⊗

w
iS 0
[
wi

⊗

cj

−1

Sfp ⊗ ( Sfp )

iS 0 is the wrist offset measurement from this image
where w]
wi
and feature, bg
Scj is the camera registration measurement,
d
w
iS 0 is the currently estimated wrist offset, and b
[
Scj is the
wi
currently estimated camera pose.

B. SE(3) Median and Extended Kalman Filter
We apply median and extended Kalman filtering in the
special Euclidean group SE(3) to the measurements for
g
iS 0 and camera registration b
Scj , similar to
wrist offset w]
wi
the approach in [5]. First, to reject outliers, we compute the
median measurement over a sliding time window. Then, we
use an extended Kalman Filter over time to compute optimal
pose estimates under a Gaussian noise assumption.
To compute the median of orientation q over the sliding window, the structure of rotations in SO(3) offers a
convenient distance metric between two orientations: the
angle between them. Using this geometric interpretation, the
median orientation q is the orientation with minimum angular
distance to all other orientations.
n
X
q = arg min
| ln(qi∗ ⊗ qj )|
(4)
qi ∈Q

j=0

The median translation v is the conventional geometric
median, the translation with minimum Euclidean distance to
all other translations:
n
X
v = arg min
|vi − vj |
(5)
vi ∈V

j=0

In the extended Kalman filter, we consider state x composed of a quaternion q, a translation vector v, and the
translational and rotational velocities, v̇ and ω:
x = (q, v) = [qx , qy , qz , qw , vx , vy , vz , ωx , ωy , ωz , v̇x , v̇y , v̇z ]

er
S

er

Se` (t1 )

Relative Trajectory

erS˙
e`

e` ,

e`
Sw`
b
Sw`
bS ˙
w`

w
\
iS 0 , bd
Sc j
w
i

EKF

=
=

0

Sw0 ⊗ w`Sw`

e`

=

`

b

Ser ⊗ erSe` ⊗ e`Sw`
Ser ⊗ erS˙e` ⊗ e`Sw`

Sr , Ṡr

J+



b

ẋr
ωr



Workspace Control


x − xr
− kx
− kφ (J + J − I)φ
∗
ln (q ⊗ qr )
φ̇r (joint velocity)



EKF

wiS


w0
i



bS

wiS
w0
i



bS

median



cj

median

cj

w
^
iS 0
w
i



bg
Sc j



...

wiS
w0
 ^
i

0



...



bg
Sc j

0

0

(bSwi )−1 ⊗ bScj ⊗ cjSf` ⊗ (wiSf` )−1

ROBOT

0

image

Swi ⊗ wiSw0 ⊗ wiSf` ⊗ (cjSf` )−1

b

i

cj

Sf0 . . . cjSfn

Feature Detector

^
iS 0 and camera
Block diagram of the control system. 3D feature poses cjSfp are detected from visual data. Instantaneous wrist offsets w
wi
registrations bg
Scj are computed. Then the median of these poses is taken over a sliding window and subsequently Kalman-filtered. The filtered poses are
used to track a relative left-right workspace trajectory, and the Jacobian damped-least squares gives the reference joint velocities φ̇r .
Fig. 2.

b

Sc 0

b

Swr

wr
S

0
wr

0
wr
Ser

0
wr
Sf

c0

Sf3

3

Frame Source:
Encoders
Visions
Filter

Fig. 3.

Setup for a dual-arm and dual-camera system. The kinematic frames are shown for one of the arms and cameras.

The measurement z is the median pose from the sliding
window:
z = (q, v) = [qx , qy , qz , qw , vx , vy , vz ]

Now, we find the process Jacobian F . The translation
portion is a diagonal matrix of the translational velocity. For
the orientation portion, we find the quaternion derivative q̇
from the rotational velocity:

The general EKF prediction step for time k is:
x̂k|k−1 = f (x̂k−1 )

∂f 
Fk−1 =
∂x x̂k−1|k−1
T
Pk|k−1 = Fk−1 Pk−1|k−1 Fk−1
+ Qk−1

(6)

q̇ =

1
ω⊗q
2

(10)

(7)
(8)

where x̂ is the estimated state, f (x) is the process model, F
is the Jacobian of f , P is the state covariance matrix, and
Q is the process noise model.
The process model integrates the translational and rotational velocity, staying in the SE(3) manifold using the dual
quaternion exponential of the twist Ω:


Ω(ω, v̇, v) = ω, v × ω + v̇ 


∆t
f (x) = exp
Ω ⊗ (q, v)
(9)
2

This quaternion multiplication can be converted into the
following matrix multiplication:
1
1
ω ⊗ q = Mr (q) ω
2
2

qw
qz
−qz qw
Mr (q) = 
 qy −qx
−qx −qy


−qy
qx 

qw 
−qz

(11)

Note that we omit the w column of the typical quaternion
multiplication matrix because the w element of rotational
velocity ω is zero.

This gives the following process 13 × 13 Jacobian F :



1
I4×4
0
0
2 ∆tMr q
 0
I3×3
0
∆tI3×3 

F =
(12)
 0
0
I3×3
0 
0
0
0
I3×3
Now we consider the EKF correction step. The general
form is:
ẑk = h(x̂k|k−1 )

∂h 
Hk =
∂x x̂k|k−1

(13)
(14)

yk = v(zk , ẑ)
Sk =
Hk Pk|k−1 =

Hk Pk|k−1 HkT
Sk KkT

(15)
+ Rk

(16)
(17)

x̂k|k = p(x̂k|k−1 , Kk yk )

(18)

Pk|k = (I − Kk Hk )Pk|k−1

(19)

where z is the measurement, h is the measurement model,
H is the Jacobian of h, ẑ is the estimated measurement, R
is the measurement noise model, and K is the Kalman gain,
v is a function to compute measurement residual, and p is a
function to compute the state update.
We compute the EKF residuals and state updates using
relative quaternions to remain in SE(3) without needing
additional normalization. The observation h(x) is a pose
estimate:
h(x) = (q, v)
H = I7×7

(20)

We compute the measurement residual based on the relative rotation between the measured and estimated pose:
v(z, ẑ) = (yq , yv )
(21)

where yq is the orientation part of the residual and yv the
translation part. Note that ln zq ⊗ ẑq∗ corresponds to a
velocity in the direction of the relative transform between
the actual and expected pose measurement and that we can
consider yq as a quaternion derivative. Then, the update
function will integrate the pose portion of y, again using
the exponential of the twist. First, we find the twist corresponding to the product of the Kalman gain K and the
measurement residual y:
(Ky)φ = (Ky)q ⊗ q ∗
Ω(Ky, v) = ((Ky)φ , v × (Ky)φ + (Ky)v )

(22)

Then, we integrate estimated pose using the exponential
of this twist:


∆t
Ω ⊗ (q, v)
(23)
(x(q,v) )k|k = exp
2
Finally, the velocity component of innovation y is scaled
and added:
(xω,v̇ )k|k = xω,v̇ + (Ky)ω,v̇

To perform smooth, bimanual motion, we compute a
relative workspace trajectory between the two manipulators,
transform the relative pose and velocity of the trajectory
to the body frame, then compute joint velocities using the
Jacobian damped least squares pseudoinverse.
We compute a relative trajectory for the two endeffectors using the spherical parabolic blends described
in [6]. This provides a straight-line, constant-axis, and
continuous-velocity workspace path for the end-effector by
blending subsequent spherical linear interpolation segments.
Given a list of relative left-right waypoint poses and times,
er
Se` (t0 ), . . . , erSe` (tn ), we compute the reference left-right
pose and velocity as a function of time: erSe` (t), erχ˙ e` (t).
From the relative reference pose erSe` and velocity erχ˙ el
between the left and right end-effectors, we control the left
arm in workspace, by first converting the relative pose and
velocity to the body frame, then computing the Jacobian
damped-least-squares inverse kinematics solution.
The left-arm wrist pose bSw` follows directly from the
kinematic chain through the right arm:
b

Sw` = bSer ⊗ erSe` ⊗ e`Sw`

e`

0

Sw` = e`Sw`0 ⊗ w`Sw`

(25)

Next, we compute the body-frame feedforward reference
velocity, aχ˙ b . Since there is only one changing frame, erSe` ,
we could find the corresponding body frame motion by
rotating the velocity. However, the typical computation is
notationally cumbersome [4, p140].2 Instead, we find an
elegant and more general solution by merely taking the
derivative of the pose:
b


yq = ln zq ⊗ ẑq∗ ⊗ q
yv = zv − ẑv

IV. C ONTROL : C ONTINUOUS W ORKSPACE
T RAJECTORIES

(24)

Sw` = bSer ⊗ erSe` ⊗ e`Sw`
0
d er

˙
˙
b
b
( Se ⊗ e`Sw` )
⇒ Sw` = Ser ⊗ (erSe` ⊗ e`Sw` ) + bSer ⊗
dt !`
*0 e

e`
˙ = bSe ⊗ erSe ⊗ e`S˙
⇒ bSw
r
`
`
 w` + rS˙ e` ⊗ Sw`
˙ = bSe ⊗ erS˙ e ⊗ e`Sw
⇒ bSw
r
`
`
`

(26)

0

7 indicates that S cancels to zero, and we assume the
where 
S
right arm and left fingers are stationary (0 = bS˙er = e`S˙w` ).
Relative motion with both arms moving could be computed
by including the nonzero derivative bS˙er in the computation.
We can use the product rule for this derivation because
dual quaternion poses are chained through multiplication.
Using the quaternion plus translation vector representation,
2 The complexity of the velocity transformation notation in [4, p140]
stems from its representation using Gibbs’s vector calculus which decouples the quaternion multiplication into separate dot and cross products.
Hamilton’s and Study’s classical quaternion and dual quaternion notation is
simpler and more elegant for this kinematic computation. A similar computation is also possible using transformation matrices and their derivatives, but
these matrices are more difficult to normalize than quaternions, increasing
numerical error.

Fig. 4. Manipulation error using only encoders for position feedback.
Without using visual feedback, there is a 15mm relative positioning error
between the two end-effectors.

chaining is not a multiplication, so an equivalent derivation
would be more complex.
Velocity and the dual quaternion derivative are related as
follows:

1

where R(S) is the real part of S, D(S) is the dual part of
S, ω is rotational velocity, and x is translation.
Finally, we compute reference joint velocities using the
Jacobian damped least squares with a nullspace projection
to keep joints near the zero position:
 


ẋr
x − xr
+
φ̇r = J
− kx
−kφ (J + J −I)φ (28)
ωr
ln (q ⊗ qr∗ )
where x is the actual translation, q is the actual orientation
quaternion, xr is the reference translation, qr is the reference
orientation quaternion, ω is the actual rotational velocity,
ωr is the reference rotational velocity, kx is the workspace
position error gain, kφ is the null-space projection gain, and φ
is the configuration. We then use joint-level velocity control
to track the reference joint velocities φ̇r . A block diagram
depicting the components of the control system and their
interplay can be found in Fig. 2.
V. E XPERIMENTS
We implement this approach on a pair of Schunk LWA4
manipulators with SDH end-effectors, and use a pair of
Logitech C920 webcams to track the robot and objects.
Our estimation and control software is implemented as a
distributed system using the Ach real-time communication
library [7]. The Schunk LWA4 has seven degrees of freedom and uses harmonic drives, which enable repeatable
positioning precision of ±0.15mm [8]. However, absolute
positioning accuracy is subject to encoder offset calibration
and link rigidity. In practice, we achieve ±15mm accuracy
when using only the joint encoders for feedback, as can be
seen in Fig. 4. The Logitech C920 provides a resolution of
1920x1080 at 15 frames per second. To measure ground-truth
distances, we use a ruler and meter-stick.
To test the relative positioning accuracy of our implementation, we servo the end-effectors to a reference zero relative
alignment, Fig. 5, and then measure the actual relative error
between the two end-effectors. We conduct this test using

quaternion, q

(27)

x
y
z
w

0.5

0

-0.5

0

1

2

3

4 5 6
time (s)

7

0.5

translation, x

dR(S)
1
= ω ⊗ R(S)
dt
2

1
dR(S)
dD(S)
=
ẋ ⊗ R(S) + x ⊗
dt
2
dt

Fig. 5. Testing relative positioning accuracy by aligning the end-effectors.
Incorporating visual feedback and online registration reduces manipulation
error from 15mm to ≈ 2mm.

8

9 10

8

9 10

x
y
z

0

-0.5

0

1

2

3

4 5 6
time (s)

7

Fig. 6. Relative trajectory of erSe` between left and right end-effectors
for pen-capping. The trajectory has constant acceleration, constant velocity,
and constant deceleration segments.

only encoder feedback, then with visual feedback. We also
◦
repeat the test injecting encoder error of 15 at the initial
◦
◦
shoulder joint, 30 at the shoulder, and 15 at both the
shoulder and elbow. The results of this test are summarized
in Table I.
In addition, we use this method to perform the pen-capping
task shown in Fig. 3 and the object hand off task shown in
Fig. 7. The relative trajectory of erSe` for the pen-capping
task is plotted in Fig. 6
VI. D ISCUSSION
The results of Sect. V show that this method achieves
bimanual positioning accuracy of a few millimeters without
◦
static camera registration and even with significant (30 )
error in the joint encoders.

TABLE I
P OSITIONING T EST R ESULTS (mm)

No Offset
◦
shoulder: 15
◦
shoulder: 30
◦
shoulder & elbow: 15

Fig. 7.

Mean
encoder
visual
16.5
2.2
155
2.8
280
1.3
240
0.95

Std. Dev.
encoder
visual
0.5
0.94
0.6
0.78
0
0.95
0
1.1

An object hand-off task.

There are a variety of error sources that we address in this
system. For the kinematics, error from encoder offsets in the
arm, imprecise link lengths, and flexing of links all contribute
inaccurate kinematic pose estimates. For perception, error
from inaccurate camera intrinsics, imprecise fiducial sizes,
offsets in object models, and noise in the image all contribute to error in visual pose estimates. To achieve accurate
manipulation, we must account for these potential sources of
error.
The position of the tracked features on the robot has
an important effect on error correction. Kinematic errors
between the robot body origin and the tracked features, e.g.,
due to flex or encoder offsets, are incorporated into the
camera registration and handled through the servo loop. Error
between the observed features and the end-effector cannot be
corrected. Thus, it is better to track features as close to the
end-effector as possible. Consequently, we placed the fiducial
markers on the fingers of the SDH end-effector.
One source of error for manipulation that we do not
address is error in grasping. Because we track only the robot
hand, any error in the relative pose between the hand and
grasped object is not corrected. In reality, when grasping
an object, the object itself becomes the robot’s end-effector.
Thus, to accurately manipulate in-hand objects, it would be
better to track the objects themselves. Since a grasped object
is likely to be partially occluded, model-based tracking such
as [3], which is robust to occlusions, is a potential approach.
A crucial additional consideration in manipulation is force
and tactile sensing. Using visual feedback without force and
tactile sensing already reduces the error to a few millimeters
and allows the robot to perform tasks such as pen capping
and object hand-off. However, considering the generated
contact forces during the manipulation would further improve
performance and allow even more accurate operation, in
particular during the post-contact phase. This is a key area
for improvement in this approach.

VII. C ONCLUSION
We have presented an online method to identify multiple
camera and manipulator poses and track continuous relative
trajectories for bimanual manipulation tasks. This is useful
for the typical case where camera registration is not static
but changes due to model error, disturbances, or wear and
tear. The key point is to track both manipulators, and follow
a trajectory based on the visually estimated relative 3D
pose between the end-effectors. By combining median and
Kalman filtering, we are able to achieve millimeter-level
manipulation accuracy. We have shown in our experiments
that online registration can be used to improve positioning
accuracy during bimanual manipulation tasks where successful operation depends on relative end-effector pose.
This method uses feedback only from joint encoders and
visual tracking of the robot hand. Further improvements
could be made by including force and tactile sensing and
by visually tracking in-hand objects.
R EFERENCES
[1] François Chaumette and Seth Hutchinson. Visual servo control, part I:
Basic approaches. Robotics and Automation Magazine, 13(4):82–90,
2006.
[2] François Chaumette and Seth Hutchinson. Visual servo control,
part II: Advanced approaches. Robotics and Automation Magazine,
14(1):109–118, 2007.
[3] Changhyun Choi and Henrik I Christensen. Robust 3d visual tracking
using particle filtering on the special euclidean group: A combined
approach of keypoint and edge features. The International Journal of
Robotics Research, 31(4):498–519, 2012.
[4] J. Craig. Introduction to Robotics: Mechanics and Control. Pearson,
3rd edition, 2005.
[5] N. Dantam, H. Ben Amor, H. Christensen, and M. Stilman. Online
camera registration for robot manipulation (presented). In International Symposium on Experimental Robotics, 2014.
[6] N. Dantam and M. Stilman. Spherical parabolic blends for robot
workspace trajectories (accepted). In International Conference on
Intelligent Robots and Systems, 2014.
[7] Neil Dantam, Daniel Lofaro, Ayonga Hereid, Paul Oh, Aaron Ames,
and Mike Stilman. Multiprocess communication and control software
for humanoid robots (accepted). Robotics and Automation Magazine,
2014.
[8] Schunk GmbH. Dextrous lightweight arm LWA 4D, technical data.
http://mobile.schunk-microsite.com/en/produkte/
produkte/dextrous-lightweight-arm-lwa-4d.html.
[9] Paul Hebert, Nicolas Hudson, Jeremy Ma, and Joel W Burdick. Dual
arm estimation for coordinated bimanual manipulation. In Intl. Conf.
on Robotics and Automation, pages 120–125. IEEE, 2013.
[10] Sungmoon Joo and Michael Grey. DRC-Hubo retrospective, January
2014. Personal Communication.
[11] Matthew Klingensmith, Thomas Galluzzo, Christopher Dellin,
Moslem Kazemi, J. Andrew (Drew) Bagnell, and Nancy Pollard.
Closed-loop servoing using real-time markerless arm tracking. In Intl.
Conf. on Robotics and Automation (Humanoids Workshop), May 2013.
[12] OpenCV API Reference. http://docs.opencv.org/master/
modules/refman.html.
[13] Vijay Pradeep, Kurt Konolige, and Eric Berger. Calibrating a multiarm multi-sensor robot: A bundle adjustment approach. In Experimental Robotics, pages 211–225. Springer, 2014.
[14] Kari Rainio and Alain Boyer. ALVAR – A Library for Virtual and
Augmented Reality User’s Manual. VTT Augmented Reality Team,
December 2013.
[15] Jeffrey A. Saunders and David C. Knill. Humans use continuous visual
feedback from the hand to control both the direction and distance of
pointing movements. Experimental Brain Research, 162(4):458–473,
2005.
[16] A. Teichman, S. Miller, and S. Thrun. Unsupervised intrinsic calibration of depth sensors via slam. In Robotics: Science and Systems
(RSS), 2013.

