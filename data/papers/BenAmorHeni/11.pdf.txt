Seediscussions,stats,andauthorprofilesforthispublicationat:https://www.researchgate.net/publication/273134654

ExploitingSymmetriesandExtrusionsfor GraspingHouseholdObjects
ConferencePaperinProceedings-IEEEInternationalConferenceonRoboticsandAutomation·May2015
DOI:10.1109/ICRA.2015.7139713

CITATIONS

READS

2
7authors,including: AnaHuamanQuispe GeorgiaInstituteofTechnology
7PUBLICATIONS4CITATIONS
SEEPROFILE

265

MarcoA.Gutierrez UniversidaddeExtremadura
11PUBLICATIONS23CITATIONS
SEEPROFILE

HenrikIskovChristensen UniversityofCalifornia,SanDiego
478PUBLICATIONS6,419CITATIONS
SEEPROFILE

HeniBenAmor ArizonaStateUniversity
56PUBLICATIONS434CITATIONS
SEEPROFILE

AllcontentfollowingthispagewasuploadedbyHeniBenAmoron05March2015.
Theuserhasrequestedenhancementofthedownloadedfile.Allin-textreferencesunderlinedinblueareaddedtotheoriginaldocument andarelinkedtopublicationsonResearchGate,lettingyouaccessandreadthemimmediately.

Exploiting Symmetries and Extrusions for Grasping Household Objects
Ana Huam´ an Quispe1 Beno^ it Milville1 Marco A. Guti´ errez2 Can Erdogan1 Henrik Christensen1 Heni Ben Amor1 Mike Stilman

Abstract--In this paper we present an approach for creating complete shape representations from a single depth image for robot grasping. We introduce algorithms for completing partial point clouds based on the analysis of symmetry and extrusion patterns in observed shapes. Identified patterns are used to generate a complete mesh of the object, which is, in turn, used for grasp planning. The approach allows robots to predict the shape of objects and include invisible regions into the grasp planning step. We show that the identification of shape patterns, such as extrusions, can be used for fast generation and optimization of grasps. Finally, we present experiments performed with our humanoid robot executing pick-up tasks based on single depth images and discuss the applications and shortcomings of our approach.

I. I NTRODUCTION The ability to grasp and manipulate objects is an important skill for autonomous robots. Many important tasks, e.g., assisting humans in household environments, require robots to reliably plan and execute grasps on surrounding objects. To generate plans for manipulation tasks, information about the shape of the object is required. A frequent approach to grasp planning is to use a database of polygonal meshes representing the different objects that the robot can manipulate [8]. Such information about object geometry can be used by grasp planners to synthesize an appropriate hand shape and orientation for physical interaction. While this approach is valid for structured domains with a small set of different objects, it does not scale to unstructured environments in which many objects may have never been seen before. Other approaches to grasp planning employ depth cameras to acquire 3D point clouds of new objects, which in turn are used to generate grasps. Since the point clouds are acquired from a specific perspective, they only hold partial shape information about the visible frontal part. Using only partial point clouds to plan manipulation tasks can be very limiting, since many grasps involve placing fingers on opposite sides of an object. To fill any gaps and produce a complete point cloud, multiple images can be acquired by either iteratively moving the camera or the object. This process is time-consuming and
1 Institute

Fig. 1: Extracted information of rotational symmetries in the object is used to create a complete shape from a partial point cloud. The generated mesh is used by a grasp planner to generate a continuous set of grasps around the symmetry axis.

for of

gia

Institute

Robotics and Intelligent Machines, Technology, Atlanta, GA 30332,

GeorUSA.

ahuaman3@gatech.edu, cerdogan@cc.gatech.edu, benoit.milville@gadz.org, hic@cc.gatech.edu, hbenamor@cc.gatech.edu
2 Robotics and Artificial Laboratory, Univesity of Extremadura, C´ aceres, 10003, Spain. marcog@unex.es

introduces new challenges such as the precise matching of the individual point clouds of each view. Alternatively, the robot can use geometric cues to predict the shape of the object in unseen regions. Through the analysis of inherent shape properties such as mirror symmetries and rotational extrusions, estimates of the complete point cloud can be generated from a single image. The extracted symmetry parameters can be used to extend observed shape patterns, e.g., the profile curve of an object, to occluded regions. In this paper, we show how compact object representations for manipulation tasks can be generated from a partial point cloud. Given a single RGB-D image, we generate a complete mesh model of the observed object as well as additional shape information, e.g., axis of symmetry or superquadric approximations. We show that these compact representations can be later exploited for the fast synthesis of a continuous set of grasps. In turn, the set is used to plan robot manipulation tasks. Our approach builds both upon recent developments in symmetry-based [3, 18], as well as extrusion-based object representations [16]. Symmetry-based representations mirror observed object parts into occluded regions. Extrusion-based approaches, on the other hand, try

to identify a two-dimensional profile which can be linearly or rotationally extruded to complete an object. In this work we show how symmetries and extrusions can be used to extract two different types of object representations, namely superquadric approximations and 2D shape profiles. We also show how these representations can used to generate grasps on the object. The rest of this paper is organized as follows: Section II summarizes relevant literature. Section III introduces two compact object representations that are based on detecting symmetries and extrusions. Section IV shows how compact object representations based on extrusion patterns can be exploited for fast grasp planning with a small number of parameters. Section V presents experimental results of the object completion, as well as its application to robot grasping tasks. Finally in Section VI we discuss our approach and its advantages and shortcomings. II. R ELATED W ORK For a robot to physically interact with its environment, algorithms for both grasp planning and perception are required. Traditional approaches for grasp generation are often based on fitting 3D CAD models to the observed scene [14, 15]. Such an approach, however, cannot be used to grasp novel objects since it requires accurate, prior knowledge about the shape. With the advent of depth cameras, various researchers have turned towards point cloud representations for perception and grasp planning. Huebner et al. [11] showed that bounding boxes computed from point clouds can be used to grasp novel objects. In a similar vein, Jiang et al. [12] proposed a socalled grasping-rectangle representation which can be used to infer the best grasp parameters given an RGB-D image of a novel object (given an offline training step). Przybylski et al. [21] showed simulation results in which a medial axis representation of objects can be used to find successful grasps without compromising on the approximation quality. Other than boxes and spheres [17], superquadrics [9] have also been considered for grasping applications given their compactness and ability to represent many diverse shapes with a limited number of parameters. Recently, Duncan et al. presented a fast hierarchical approach to fit superquadrics online [5]. On the side of grasp generation, a popular metric used to predict grasp robustness is the  metric proposed by Ferrari and Canny [6]. While many popular grasp generators, such as GraspIt! use this metric to evaluate and refine the grasp search, it has been noted [4] that a grasp with a good metric does not translate to a robust grasp in a real-world execution. Researchers such as Hsiao [10] and Balasubramanian [1] have shown that grasps obtained using simple human heuristics can produce comparable or even better results when evaluated in a real, non-simulated environment. A real world scenario - contrary to a simulated one presents its own set of challenges: errors in perception, control and modeling must be considered and might render an optimal simulated grasp into an infeasible one. Regarding incomplete perceptual information, such as one-view point clouds for a

given object, Bohg et al. [3] proposed a simple approach that exploits the symmetry of most common household objects to predict the full shape of an object on a tabletop scenario. Following Bohg's observation that most common household objects present similar characteristics (such as symmetry, extrusion-like geometry and primitive shapes), we use them to approximate the shape of objects. This is also useful in the event of occlusion, in which a complete point cloud is not available. III. G ENERATING C OMPACT O BJECT R EPRESENTATIONS FROM S INGLE RGB-D I MAGES In this section, we present two compact representations of objects that can be generated from partial point clouds. These representations can be used to plan grasps on objects involving regions of the point cloud that are currently invisible. As a result, a wider range of grasps can be planned, including, for example, side grasps which are based on an opposition of fingers placed at the front (seen) and the back (unseen) of the object. We will first present a superquadric representation which is based on determining symmetries in point clouds. After that, we will turn towards a more detailed representation which makes use of rotational symmetries and linear extrusions to characterize an object. A. Superquadric Representation Superquadrics are a family of geometric shapes that can represent a wide range of diverse objects. The equation describing superquadrics in their canonical form can be written as x a
2 2

F (x) =

y + b

2 2

2 1

+

z c

2 1

= 1.

(1)

where a,b,c are the scaling factors along the principal axes, 1 is the shape factor of the superquadric cross section in a plane orthogonal to XY containing the axis Z, and 2 is the shape factor of the superquadric cross section in a plane parallel to XY. If a general transformation is considered, then the total number of parameters required to define a superquadric is 11 (the 6 additional being the rotational and translational degrees-of-freedom (DoFs) {x, y, z, , , }). By minimizing the error between each point and the general superquadric equation, a shape that best fits the point cloud can be obtained:
n

min
k k=0



abcF 1 (x; ) - 1

2

(2)

As mentioned in Section II, superquadrics have previously been used to generate grasp configurations for simple objects [2, 22]. Most of these approaches assume that the complete shape of the object is given or that the parameters can be learned beforehand. However, when working with depth cameras this is not a reasonable assumption to make. In recent work, Duncan et al. [5] presented a superquadric fitting

Hypotheses

Initial Estimation

Optimization

Fig. 3: The three steps used for optimizing the axis of extrusion. First, we generate hypotheses by analysing pairs of points. The resulting estimates are used to produce an initial estimate of the axis of extrusion. Finally, optimization is used to improve the extrusion axis.

choose suitable candidates for task execution. For example, detecting the axis of symmetry in a rotationally symmetric object allows us to rotate any feasible grasp around this axis. In this paper, extrusion detection is performed using a threestep approach, see Fig. 3 for an overview of the approach using rotational extrusions. In the first step, we use points from the partial point clouds to generate hypotheses for the extrusion axis. In the case of rotational extrusions, we randomly sample pairs of points and use the normal of each point to create a line. Each pair of lines is intersected and the resulting point is used as a hypothesis for the axis of extrusion. Fig. 3 shows an example for points sampled from a cylindrical object. To account for noise, we use the midpoint of the line connecting the closest points, in case the two lines do not intersect. The collected hypotheses points are then used to create an initial estimate of the axis of extrusion. To this end, we fit a line into the set of hypotheses using linear least-squares. The RANSAC [7] algorithm is further used to reduce the influence of outliers. Given this initial estimate, we perform optimization to produce a more accurate axis of extrusion. Specifically, we use the dynamic hill climbing algorithm [23] to search for an axis of extrusion which reduces the dispersion of points along the profile of the object. In every iteration, the axis of extrusion is used to rotate all points of the partial point cloud back onto a plane. We then estimate the density of the points using a kernel density estimator [20]. By maximizing the density using the hill climbing algorithm, we can reduce the dispersion of the projected points, thereby recreating the profile of the object. However, performing a kernel density estimation in each step of the optimization process is computationally expensive and does not scale to large point clouds. The following method is, therefore, a discrete approximation of the kernel density, which produced accurate results in practice while at the same time being fast. We create an approximation of the kernel density estimator by creating a grid over the projected point cloud. The number of cells used in our experiments varied between 5 and 30 cells in each dimension. For each cell i  {1, .., M } we count the number of points ci that lie within. We then calculate the average of the differences to neighbouring cells j  {1, .., N }. The overall objective function of the optimization can be

Fig. 2: An example for the superquadric fitting with symmetry analysis (middle) and without it (bottom). approach which uses a voxel representation to reduce the computational complexity of the task. We found that this approach worked well when the segmented point cloud of the object had a good viewing point (i.e. the front, side and top of the object were seen). For point clouds in which only one side of the object was seen (i.e. only front), the performance quickly deteriorated, producing fitting parameters that in many cases exceeded greatly the original dimensions of the objects. While this could be partially alleviated by hardcoding limits in the dimension of the axes, this is not practical when dealing with novel objects, for which we might not know the dimensions beforehand. Inspired by work presented by Bohg et al. [3], we added an additional pre-processing step to the superquadric minimization process. Instead of using the original point cloud as input, we generated a mirrored version (see Fig. 2) by finding an optimal symmetry plane perpendicular to the table where the object resides (for more details of this process, please refer to the original paper [3]). B. Object Completion from Extrusions Planning task-specific grasps requires information about the complete shape of the object to be manipulated. Many household objects are based on extrusions. Indeed many modelling and manufacturing systems use linear and rotational extrusions in a hierarchy to generate the models used for manufacturing. Uncovering extrusions in partial point clouds can therefore help to generate a complete point cloud from a partial observation. In addition, this knowledge can be used to create a large set of feasible grasps from which a planner can

Iteration 2
25 20 15 10

Iteration 10

Iteration 50

20

15

10

5 5 0 0 1 2 3 4 5 6 7 0

Fig. 4: Density estimation at different stages of optimization. At the beginning of the optimization, the projected points are highly dispersed. The axis of extrusion is then changed to minimize the dispersion, such that the outer profile of the object emerges as can be seen in iteration 50. On the right side we can see the object to which the profile belongs. written as E= 1 1 MN Fig. 6: Extracted object profile for the linearly extruded objects. The extracted profiles are used to create a complete point cloud. ||ci - cj || (3) and evaluate grasp quality using existing metrics. In contrast, traditional grasp quality metrics cannot be directly applied to partial point clouds. Similarly, having a complete mesh allows a grasp planner to evaluate a large variety of grasps, which can then be pruned based on task constraints. However, generating many grasps often involves repeated applications of grasp optimization methods which can be computationally demanding, in particular in the presence of many degrees-of-freedom in the robot arm and hand. Extracted shape information from extrusions can be used to improve the efficiency of this process by significantly reducing the number of degrees-of-freedom of the problem. The main insight of this section is that hand shapes during object grasping are invariant to movements along the axis of extrusion. As long as the robot hand moves along the axis of extrusion, no expensive replanning of the hand shape is necessary. In the case of linear extrusions, the robot hand can move up and down the axis of extrusion without having to change the hand shape. Similarly, in the case of rotational extrusions, the hand can be rotated around the axis of extrusion. This knowledge can be exploited during grasp generation in order to turn each single detected grasp into a continuous set of grasps. Subsequently, we present a specific example how information about extrusions can be used to reduce the dimensionality and complexity of a grasp re-planning task. Fig. 7a shows a scenario, in which a grasp is executed on a rotationally symmetric object. The grasp has a low manipulability index which is not sufficient to achieve the task constraints. Typically, this means that a new grasp and arm pose needs to be planned, which involves (sampling-based) optimization in the high-dimensional space of joint angles. Given that the grasp is performed on a rotationally symmetric object, the grasp generation can be modeled as an inverse kinematics problem where the goal is to determine an arm configuration q that is collision free. The output is constrained by the end-effector position on the object and the corresponding inverse kinematics solution. The end-effector pose x can be parametrized by (1) the rotation around the axis of extrusion  and (2) the distance along the axis of extrusion

M

N

i

j

where E is the energy to be minimized. Fig. 4 shows three iterations during the optimization of the axis of extrusion. Dark areas correspond to regions of high density of points, while lighter areas correspond to low density regions. In early iterations, the estimate of the axis does not produce a clear profile when points are projected (rotationally) onto a plane. In iteration ten, we can see that high density regions start forming. After fifty iterations, an approximate profile of the object starts to emerge. After optimization is finished, we regard the projected points as the profile of the object and rotationally extrude them around the axis of extrusion to generate a complete point cloud. Fig. 5 shows a set of household objects, the recorded depth images, as well as the reconstructed complete meshes. Given the completed point cloud, we reconstructed the meshes using Poisson surface reconstruction [13]. For the case of linear extrusions along an axis, a different method for the estimation of the initial axis of extrusion needs to be used. For linear extrusions, we compare the normal vectors of pairs of points and generate a hypothesis if the difference between the normals is below a threshold. The resulting set of hypothesis can then be clustered, such that each cluster represents a possible axis of extrusions. For example, for a box, up to six clusters can be found. Note, that in our approach we use a point cloud to represent the profile of an extrusion. For revolute objects, the profile defines the outer curve of the object, which can be rotated around the axis of extrusion to generate the complete shape. For linear extrusions, the cloud represents the basic 2D shape which can be extruded to form the object. Fig. 6 shows the extracted object profiles for objects with linear extrusions. IV. U SING C OMPACT O BJECT R EPRESENTATIONS FOR G RASP P LANNING Grasp planning greatly benefits from the completed point clouds. A complete point cloud can be triangulated and used as an input to existing grasp generation and planning algorithms. In contrast to the partial point cloud, the completed and triangulated mesh can be used to perform collision checks

Fig. 5: Reconstruction of rotationally symmetric household objects. The top row shows a photo of the object. The middle row shows the corresponding depth image recorded using a Microsoft Kinect. The bottom row shows the completed mesh. Reconstruction was performed from a single image through the analysis of extrusions. , x = pose(, ). The inverse kinematics solution q with a 7-DoF arm for an end-effector pose x can be parametrized by an additional variable  which represents the angle between the wrist-elbow-shoulder plane and the ground, q = IK (x, ). At each iteration i, the new arm position is computed using an updated grasp position from the parameter space {i-1 ±  , i-1 ±  } and the corresponding inverse kinematics parametrized by {i-1 -  , i-1 , i-1 +  }. Let P represent the full space of the variables , , and . The algorithm iteratively updates these parameters by determining which tuple leads to the maximum manipulability [19]. This is realized by solving for the following objective q i = argmax det(J (q )J T (q )) (4) set of experiments focuses on the complexity and accuracy of point cloud completion when generating compact object representations. The second set of experiments shows the application of the approach to grasp planning on a humanoid robot. The used humanoid robot is based on Schunk LWA3 arms with 7 DoF. A Schunk gripper with a maximum aperture of 7cm was used. Partial point clouds were recorded using a Microsoft Kinect camera. A. Accuracy of Fit We first analyzed the accuracy of fit of the two presented compact object representations. For extrusions, we collected a set of rotationally symmetric meshes from internet databases from which we generated partial point clouds. We then cut out a partial point cloud representing 30% of the data and simulated Kinect-like noise by adding holes and noise to the dataset. The partial cloud was then completed using the extrusion detection methods from Sec. III-B. To measure the accuracy, we compared the completed clouds to the original mesh of the object. On average, the approach produced an error (distance of points to mesh) of 2mm, where objects had a diameter between 10 - 20cm. Analysis of the extrusions required on average 200ms. For superquadric fitting we conducted a similar experiment. However, in this case we noticed larger variations in the reconstructed shapes depending on the perspective of the camera to the object. We therefore placed each object at one of five different locations in front of the camera and measured the run time of the algorithm including symmetry analysis and without it. As depicted in Tab. I, the fitting time is shorter when additional points are added via symmetry analysis. While this

{,, }P

where q = IK (pose(, ), ). The sequence in Fig. 7 shows several snapshots during this optimization. In this scenario, the robot grasps a rotationally symmetric bottle. The initial random grasp sample in Fig. 7a yields a manipulability of 0.268 which is then improved in Fig. 7d leading to a value of 0.540. To optimize the manipulability, the planner iteratively changes the grasp position on the robot with the  and  parameters, and the inverse kinematics parameter . This optimization can be performed efficiently since, the highdimensional configuration space of the hand does not need to be represented thanks to the extracted symmetries. Instead, a three-dimensional space of parameters {, , }  P is used. V. E XPERIMENTAL R ESULTS In this section, we present a set of experiments which we conducted to evaluate the proposed approach. The first

Fig. 7: Grasp manipulability optimization along the axis of extrusion. Since the object is symmetric, the same hand configuration can be rotated around the object (A-B, C-D). At the same time, the extra DOF in the inverse kinematics solution is also utilized to maximize manipulability (B-C). may seem unintuitive, we found that the superquadric shape has more constraints when considering mirrored points. As a result, the optimization process required for fitting quickly settles on a good solution. TABLE I: Comparison of fitting times
Object Apple Milk Jam Raisins Creamer Input Symmetry Plain Symmetry Plain Symmetry Plain Symmetry Plain Symmetry Plain P1 0.02 0.14 0.20 0.42 0.06 0.08 0.29 0.36 0.15 0.65 P2 0.13 0.17 0.07 0.56 0.11 0.29 0.25 0.40 0.15 0.09 P3 0.01 0.06 0.03 0.27 0.13 0.10 0.31 0.43 0.22 0.39 P4 0.06 0.06 0.05 0.53 0.08 0.08 0.14 0.43 0.13 0.26 P5 0.07 0.06 0.04 0.06 0.21 0.11 0.27 0.32 0.14 0.29 Avg. Time 0.05s 0.098s 0.078s 0.368s 0.118s 0.132s 0.252s 0.388s 0.158s 0.336s Extrusion Success SQ

to the invariance along the axis of extrusion. Images of the executed grasp and the experimental setup can be found in Fig. 8. TABLE II: Experimental results, 3 trials per object per location
Location B4 C3 C4 B4 C3 C4 B4 C3 C4 B4 C3 C4 Creamer 100% 0% 100% 100% 100% 100% 1040 800 1270 11 7 10 Dove 100% 0% 100% 100% 100% 100% 900 400 320 11 3 5 Roll 0% 0% 100% 0% 66% 0% 1200 2200 800 7 1 5 Micro 100% 0% 100% 100% 100% 100% 640 800 1020 11 7 13

Extrusion Grasps SQ

B. Robot Grasping Experiments Next, we conducted an experiment in which a humanoid robot was used to grasp household objects located in front of it. We also placed several other objects as clutter on the table. Given the depth image all objects were reconstructed using compact object representations. After that, the robot planned and executed grasps using the normal at a point as an approach direction and the method described in Sec. IV for ensuring manipulability and obstacle avoidance. We conducted trials with 4 objects which were placed at 4 different locations on the table. Each trial was repeated three times. A grasp was regarded successful if the robot was able to lift the object. Tab. II summarizes the results of the experiment. We can see that the approach using superquadrics performs well on most objects with the exeption of the roll. In contrast, the extrusionbased approach seems to have difficulties with a specific location (C3). Analyzing the robot executions, we found that superquadric approach typically leads to approximate shapes which are slightly larger than the original object. Hence, the executed grasp includes a "buffer" zone that allows it to succeed in the presence of sensor and calibration noise. Grasps planned for the shapes generated by the symmetry detection, however, are tighly fit to the object. This often lead to premature contact with the object during grasp execution. In Tab. II we also see the number of different grasps found using the two approaches. We can see that the symmetry based approach leads to a larger number of different grasps, due VI. D ISCUSSION AND C ONCLUSION In this paper we introduced methods for generating compact and complete object representations that are particularly useful for robot grasping applications. The approach exploits natural patterns found in many shapes, e.g., symmetries, linear extrusions, and rotational extrusions to generate a complete mesh from a single depth image. We also showed that the extraction of this information can be used to improve the efficiency and quality of the grasp planning step. The work presented in this paper can be seen as a first step towards shape priors that can be used by a robot to generate hypotheses about the shape of an object in invisible regions. Other cues, such as curvature and texture may also be helpful in predicting the complete shape from partial observations. At the moment the introduced approach is limited to household objects, which are often based on linear and rotational extrusions. However, it can also be extended to work in a hierarchy to complete more complex objects. In future work, we hope to investigate this aspect in more detail. The performed robot experiments showed that the approach can be used to create a variety of grasps. In particular, we can generate grasps that extend to parts of the object that are not seen. This is in contrast to other methods which limit the approach direction of the robot to the visible part of the object. We have shown in the experiments that the method can be used to reconstruct objects in a cluttered scene without prior

Fig. 8: Grasps on household objects generated via grasp planning on compact object representations. All objects on the table were reconstructed. Objects that were not grasps were regarded as obstacles to be avoided during the manipulation task. information. Yet, the additional information gained by creating complete meshes also imposes additional requirements on the accuracy of the robot controller. Planning grasps with more accurate reconstructions of the observed object means that the robot needs to be very precise in the task execution. So far, we do not have a model of the inherent sensor and actuation noise. We hope to investigate Bayesian approaches to object fitting, which would allow us to use information about the uncertainty during task execution. ACKNOWLEDGMENTS This work is dedicated to the memory of Mike Stilman, whose enthusiasm for making robots do cool things will always be remembered. R EFERENCES
[1] R. Balasubramanian, L. Xu, P. D. Brook, J.R. Smith, and Y. Matsuoka. Human-guided grasp measures improve grasp robustness on physical robot. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2010. [2] G. Biegelbauer and M. Vincze. Efficient "3d" object detection by fitting superquadrics to range image data for robot's object manipulation. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2007. [3] J. Bohg, M. Johnson-Roberson, B. Le´ on, J. Felip, X. Gratal, N. Bergstrom, D. Kragic, and A. Morales. Mind the gap: Robotic grasping under incomplete observation. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2011. [4] R. Diankov. Automated Construction of Robotic Manipulation Programs. PhD thesis, Robotics Institute, Carnegie Mellon University, 2010. [5] K. Duncan, S. Sarkar, R. Alqasemi, and R. Dubey. Multi-scale superquadric fitting for efficient shape and pose recovery of unknown objects. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2013. [6] C. Ferrari and J. Canny. Planning optimal grasps. In IEEE Int. Conf. on Robotics and Automation (ICRA), 1992. [7] M. A. Fischler and R. C. Bolles. Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 1981. [8] C. Goldfeder and P. Allen. Data-driven grasping. Autonomous Robots, 2011. [9] C. Goldfeder, P. Allen, C. Lackner, and R. Pelossof. Grasp planning via decomposition trees. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2007. [10] K. Hsiao, S. Chitta, M. Ciocarlie, and E. Jones. Contact-reactive grasping of objects with partial shape information. In IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2010. [11] K. Huebner and D. Kragic. Selection of robot pre-grasps using boxbased shape approximation. In IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2008. [12] Y. Jiang, S. Moseson, and A. Saxena. Efficient grasping from rgbd images: Learning using a new rectangle representation. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2011. [13] M. Kazhdan, M. Bolitho, and H. Hoppe. Poisson surface reconstruction. In Proc. of the Fourth Eurographics Symposium on Geometry Processing, 2006. [14] U. Klank, D. Pangercic, R.B. Rusu, and M. Beetz. Real-time cad model matching for mobile manipulation and grasping. In 9th IEEE-RAS Int. Conf. on Humanoid Robots (Humanoids), 2009. [15] D. Kragic, A. Miller, and P. Allen. Real-time tracking meets online grasp planning. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2001. [16] O. Kroemer, H. Ben Amor, M. Ewerton, and J. Peters. Point cloud completion using extrusions. In Int. Conf. on Humanoid Robots(Humanoids), 2012. [17] A. Miller, S. Knoop, H. I. Christensen, and P. Allen. Automatic grasp planning using shape primitives. In IEEE Int. Conf. on Robotics and Automation (ICRA), 2003. [18] Niloy J. Mitra, Leonidas J. Guibas, and Mark Pauly. Partial and approximate symmetry detection for 3d geometry. In ACM SIGGRAPH 2006 Papers, SIGGRAPH '06, pages 560­568, New York, NY, USA, 2006. ACM. [19] Y. Nakamura and H. Hanafusa. Inverse kinematic solutions with singularity robustness for robot manipulator control. Journal of dynamic systems, measurement and control, 1986. [20] E. Parzen. On estimation of a probability density function and mode. The Annals of Mathematical Statistics, 1962. [21] M. Przybylski, T. Asfour, and R. Dillmann. Unions of balls for shape approximation in robot grasping. In IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2010. [22] F. Solina and R. Bajcsy. Recovery of parametric models from range images: The case for superquadrics with global deformations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1990. [23] D. Yuret and M. de la Maza. Dynamic hill climbing: Overcoming the limitations of optimization techniques. In Second Turkish Symposium on Artificial Intelligence and Neural Networks, 1993.

View publication stats

