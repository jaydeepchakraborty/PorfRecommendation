RATT-ECC: Rate Adaptive Two-Tiered Error Correction Codes
for Reliable 3D Die-Stacked Memory
HSING-MIN CHEN and CAROLE-JEAN WU, Arizona State University
TREVOR MUDGE, University of Michigan
CHAITALI CHAKRABARTI, Arizona State University

This article proposes a rate-adaptive, two-tiered error-correction scheme (RATT-ECC) that provides strong
reliability (1010 x reduction in raw FIT rate) for an HBM-like 3D DRAM system. The tier-1 code is a strong
symbol-based code that can correct errors due to small granularity faults and detect errors caused by large
granularity faults; the tier-2 code is an XOR-based code that corrects errors detected by the tier-1 code. The
rate-adaptive feature of RATT-ECC enables permanent bank failures to be handled through sparing. It can
also be used to significantly reduce the refresh power consumption without decreasing reliability and timing
performance.

r

CCS Concepts:
Computer systems organization → Reliability;
cuits; Dynamic memory; Transient errors and upsets;

r

Hardware → 3D integrated cir-

Additional Key Words and Phrases: 3D memory, reliability, performance, error control coding
ACM Reference Format:
Hsing-Min Chen, Carole-Jean Wu, Trevor Mudge, and Chaitali Chakrabarti. 2016. RATT-ECC: Rate adaptive
two-tiered error correction codes for reliable 3D die-stacked memory. ACM Trans. Archit. Code Optim. 13, 3,
Article 24 (September 2016), 24 pages.
DOI: http://dx.doi.org/10.1145/2957758

1. INTRODUCTION

The 3D die-stacked DRAM is an excellent candidate for high-performance computer
systems. It has lower access latency, lower power consumption and higher bandwidth
compared to 2D DRAM [Loh 2008; Meng et al. 2011]. There are several prototypes for
die-stacked DRAM, including high bandwidth memory (HBM) [JEDEC HBM 2013],
hybrid memory cube (HMC) [Jeddeloh and Keeth 2012] and Octopus from Tezzaron
[2014]. In this article, we consider an HBM-like structure with 8 data dies and 1 error
correction code (ECC) die stacked on top of the logic die; stacking is achieved by the
use of through silicon vias (TSVs).
Unfortunately, 3D DRAM is likely to be less reliable than 2D DRAM because of additional errors due to its higher integration density. The increase of errors in 2D DRAM
due to higher density has been well documented in Hwang et al. [2012], Sridharan
and Liberty [2012], and Sridharan et al. [2013, 2015]. The 2D DRAM errors that were
due to transient and permanent single-bit, column, row and bank failures are expected
This work was supported in part by the DARPA PERFECT program, NSF CNS 18183, ARM and DOE.
Authors’ addresses: H.-M. Chen, C.-J. Wu, and C. Chakrabarti, School of Electrical, Computer and Energy
Engineering, Arizona State University, Tempe, AZ, 85287-5706 USA; emails: {hchen136, carole-jean.wu,
chaitali}@asu.edu; Trevor Mudge, Department of Electrical Engineering and Computer Science, University
of Michigan, Ann Arbor, MI, 48109-2121 USA; email: tnm@umich.edu.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for profit or commercial advantage and that
copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior specific permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c 2016 ACM 1544-3566/2016/09-ART24 $15.00

DOI: http://dx.doi.org/10.1145/2957758

ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

24

24:2

H.-M. Chen et al.

to be present in 3D DRAM as well. In addition, 3D DRAM will incur errors caused
by TSV failures. Thus, designing a reliable memory system using an error-prone 3D
die-stacked DRAM is quite a challenge.
Added to this challenge is the fact that, in a 3D DRAM (such as HBM), a data block
from a lower-level cache is stored in a single bank and not striped across multiple chips,
as in 2D DRAM. While this results in lower power and higher performance, it makes
the design of a reliable memory system even more challenging. For instance, if a bank
fails, an entire cache line is corrupted. Traditional ECC schemes for 2D DRAMs, such as
Chipkill [Locklear 2000], virtualized ECC [Yoon and Erez 2011], LOT-ECC [Udipi et al.
2012], and Multi-ECC [Jian et al. 2013], were all based on data being striped across
different chips. Hence, these schemes cannot be extended to handle 3D DRAM directly.
Furthermore, in large-sized DRAMs, refresh power accounts for a significant portion
of the power consumption [Liu et al. 2012]. Reducing the refresh power by increasing
the refresh interval indiscriminately makes the memory even more unreliable.
In recent years, several schemes have been proposed for improving the reliability of
3D DRAM. These include Subarraykill [Giridhar et al. 2013], Resilient-DRAM-Caches
[Jaewoong et al. 2013a], E-RAS [Jeon et al. 2014], Citadel-1 [Nair et al. 2014], and
Citadel-2 [Nair et al. 2016]. Subarraykill [Giridhar et al. 2013] proposes an ECC scheme
for the specialized Tezzaron Octopus structure, which allows the data to be striped
across several subarrays. The ECC design in Jaewoong et al. [2013a] focuses on the
protection of the last-level cache; the single-bit error correction code and small size of
cyclic redundancy code (CRC) [Lin and Costello 2004] are not strong enough for 3D
DRAM memory directly. E-RAS [Jeon et al. 2014] and Citadel [Nair et al. 2014, 2016]
are contemporary systems that have been designed for HBM-like memory. Both use
two-tiered codes, in which the first tier is used only to detect [Nair et al. 2014], or
detect as well as correct [Jeon et al. 2014; Nair et al. 2016]; the second tier is used to
correct errors detected, but not corrected, by the first tier. Finally, while there are no
schemes to explicitly reduce the refresh power of 3D DRAM, the existing 2D DRAM
techniques—such as RAIDR [Liu et al. 2012], which uses different refresh intervals for
different rows, or Hi-ECC [Wilkerson et al. 2010], which uses a strong ECC to correct
errors due to retention failures—can be extended to 3D DRAM.
In this article, we propose a rate-adaptive, two-tiered error-correction scheme (RATTECC) that provides very high reliability for an HBM-like 3D DRAM system. The tier-1
code is based on a Reed-Solomon (RS) code [Lin and Costello 2004] that provides
strong error detection and correction capability. The strong detection capability reduces
error leaks and the strong error correction capability decreases the frequency of tier-2
activation. RATT-ECC can correct errors due to all small granularity faults such as
single-bit, column or TSV failures by using tier-1 code. It needs tier-2 code only for
correcting errors due to large granularity faults such as row or bank failures. RATTECC uses RS (70,64) as the tier-1 code. This code is used to correct one symbol error and
detect five symbol errors. Since the undetected error probability of tier-1 code is as low
as 2.4 · 10−10 , almost all errors can be corrected by the tier-2 code. The tier-1 code can
also be used as an erasure code, and errors due to permanent failures in data TSVs can
be easily corrected. Furthermore, RATT-ECC has a mechanism to free up banks in the
ECC die so that they can be used as spares for faulty data banks. It alters the rate of tier1 and tier-2 codes so that these codes need less parity storage, allowing for 1 or 2 spare
banks. Finally, use of a low latency tier-1 code that can correct small granularity faults
allows RATT-ECC to operate at large refresh intervals, thereby reducing refresh power.
Overall, this article makes the following key contributions:
—We design RATT-ECC, a rate-adaptive two-tiered ECC scheme with a total storage
overhead of 12.5%, to provide strong reliability for an HBM-like 3D DRAM system. It
ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

Rate Adaptive Two-Tiered Error Correction Codes for Reliable 3D Die-Stacked Memory

24:3

Fig. 1. (a) 3D HBM-like structure with 8 data dies and 1 ECC die; each die has 8 banks. (b) Data of size
512b or 64 symbols are read from one bank in 4 beats (128b per beat); the red box represents an 8b symbol
and a TSV failure results in 1 symbol error.

corrects all errors due to small granularity faults and reduces the errors due to large
granularity faults significantly. Overall, this scheme reduces the raw failure-in-time
(FIT) rate by more than 1010 x.
—RATT-ECC uses a strong symbol-based code with small latency in tier-1 (0.52ns for
syndrome calculation in 28nm node) to provide very high reliability with minimal
performance degradation.
—The rate-adaptive feature of tier-1 code and tier-2 code can be used to dynamically
free up to two spare banks in the ECC die. Thus, the memory system can reliably
function even after two data banks are marked faulty with very little overhead.
—The erasure correction capability of the tier-1 code can be used to handle permanent
failures in the data TSVs with no performance overhead.
—The low-latency tier-1 code can also be used to correct additional errors that are introduced by increasing the refresh interval with minimal performance degradation.
Simulation-based evaluation with SPEC2006 benchmarks [SPEC2006 2011] demonstrates that, if the refresh interval increases from 64ms to 256ms, the refresh power
reduces by 75% while the total power consumption is reduced by 7.1%, 15.1%, 17.6%,
and 21.6% when the DRAM size is 8GB, 16GB, 32GB, and 64GB, respectively.
The rest of the article is organized as follows: Section 2 describes the 3D DRAM
memory system followed by error characteristics and presents a brief review of
existing schemes. Section 3 gives a high-level overview of our proposed ECC scheme
and Section 4 presents the detailed design. The simulation infrastructure, hardware
overhead, and timing performance are analyzed in Section 5, and the reliability part
is analyzed in Section 6. We present our conclusions in Section 7.
2. BACKGROUND AND RELATED WORK
2.1. 3D DRAM Architecture

In this article, we present an ECC-based scheme for improving the reliability of an
HBM-like memory. Figure 1(a) shows the internal stack organization of such a memory.
There are eight dies to store data; each die has eight banks. We also assume that
there is an additional ECC die to store the ECC bits [JEDEC HBM 2013]. The parity
check symbols for tier-1 and tier-2 code are stored in the ECC die, making the storage
overhead of ECC 12.5%. Each die is assigned an independent channel. There is one
controller for each channel and the memory controller (MC), which is housed in the
logic die, coordinates the activities of the nine channel controllers.
ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

24:4

H.-M. Chen et al.

In each memory request, 512b of data is accessed from a certain bank of a single data
die. The data is read over four beats with 128b being read in each beat (see Figure 1(b)).
In the HBM standard [JEDEC HBM 2013], each channel has a data width of 128b;
thus, there are 128 data TSV lines per channel. Since there are eight channels for the
eight data dies, there are a total of 1024 data TSV lines. Eight data bits form a symbol;
thus, a single TSV failure leads to only one symbol error, as shown in Figure 1(b).
2.2. Error Characteristics

DRAM errors can be broadly classified into soft errors and hard errors [Sridharan and
Liberty 2012]. Soft errors are caused by transient faults that occur randomly and cause
incorrect data to be read from a memory location; they disappear when the location
is overwritten. Hard errors are caused by permanent faults or intermittent faults. A
permanent fault causes a memory location to consistently return an incorrect value.
Note that a single fault can result in multiple error instances [Sridharan and Liberty
2012].
DRAM errors can also be classified into those that are due to small granularity faults
such as single bit or single column that account for 62.8% to 84.8% of all faults, and
large granularity faults such as row and bank failures [Sridharan and Liberty 2012;
Sridharan et al. 2013, 2015]. 3D DRAM also has errors due to TSV failures [Nair et al.
2014; Jeon et al. 2014], which fall under small granularity faults. In this article, we
analyze the capability of the memory system to handle errors due to small-granularity
and large-granularity faults (transient and permanent).
To handle permanent faults such as TSV faults (small granularity) and bank faults
(large granularity), we utilize the error-recoding mechanism of machine-check architecture (MCA) [AMD 2011]. The MCA has registers to store the error address, time,
and type (corrected or uncorrected) of error. Error events are recorded both during
memory scrubbing and during normal read operation [Intel 2011]. Now, if the number
of errors is larger than a threshold value (the threshold value is system dependent),
the specific TSV line or bank can be marked as faulty by the MC. If a TSV is marked
as faulty, we utilize the erasure correction capability of the tier-1 code (Section 4.4); if
the data banks are marked as faulty, we use the rate-adaptive feature of the tier-1 and
tier-2 codes to free up banks in the ECC die (Sections 4.2 and 4.3).
2.3. Related Work in Reliability

This section contains a brief description of prior approaches on improving the reliability
of 3D Die-stacked DRAM [Nair et al. 2014; Jeon et al. 2014; Giridhar et al. 2013;
Jaewoong et al. 2013b]. Of these schemes, E-RAS [Jeon et al. 2014] and Citadel [Nair
et al. 2014] also focus on an HBM-like structure and are closest to our proposed scheme.
Subarraykill Giridhar et al. [2013] correct errors due to a subarray failure in the
Tezzaron Octopus structure. A data block is striped across a subset of subarrays; the
number of subsets depends on the page size. Due to this striped pattern of data storage,
the ECC schemes based on symbol-based code can be used.
Resilient-DRAM-Caches Jaewoong et al. [2013b] use ECC codes to protect tags and
cache lines in 3D DRAM caches. Single-bit error correction (SEC) code is used to correct
single-bit errors CRCs are used to detect multiple-bit errors. To recover multiple-bit
errors due to row or bank failures in a contaminated cache line, a duplicate dirty cache
line that is stored in another DRAM cache bank is utilized. This approach has high
storage overhead and is not strong enough to handle errors in 3D DRAM.
E-RAS Jeon et al. [2014] propose a two-tier error-correction scheme with higher
than 12.5% storage overhead. The first tier uses a symbol-based ECC code to correct
errors due to small-granularity faults, while the second tier is an XOR-based correction
code (XCC) to correct the detectable but uncorrectable errors due to large-granularity
ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

Rate Adaptive Two-Tiered Error Correction Codes for Reliable 3D Die-Stacked Memory

24:5

faults. It proposes a strategy to deal with permanent TSV (or row) failures by using
spare TSVs (or rows). However, it does not have any strategy to handle permanent
bank failures.
Citadel-1 Nair et al. [2014] also use two-tier ECC protection to handle errors due to
small- and large-granularity faults. For the first tier, Citadel-1 uses CRC-32 to provide
strong error detection capability. After errors are detected, it relies on multiple levels of
parity (3DP) to recover errors. It identifies TSV faults through additional computation
and repairs them at runtime through TSV swapping. In addition, it provides for two
spare banks to handle permanent bank failures. While Citadel-1 is optimized to handle
errors due to permanent faults (TSV, row, bank), it has a very large overhead for
correcting errors due to transient faults, including those that affect only a single bit.
Recently, Citadel-2 [Nair et al. 2016] was proposed, which uses SEC along with CRC-30
to reduce the decoding latency of single-bit errors. Both Citadel-1 and Citadel-2 have
a storage overhead of 14%.
2.4. Related Work in DRAM Refresh

While refresh operations are essential for DRAM operation, they degrade timing performance, energy consumption, and I/O throughput; the degradation increases with
the increase in DRAM size. According to Liu et al. [2012], for a large 64GB DRAM
device, refresh operations account for 50% of the DRAM power, while degrading memory throughput by 50%. Several techniques have been proposed to reduce the refresh
power of 2D DRAM by selectively increasing the refresh interval from the default 64ms
that is specified by JEDEC [2010]. The technique in Kim and Papaefthymiou [2003]
selects a subset of DRAM blocks to be refreshed using a larger refresh interval. The
interval is chosen based on the retention capability of the cells in that block. RAIDR
[Liu et al. 2012] and REFLEX [Bhati et al. 2015] also adjust the refresh intervals of
DRAM blocks based on their retention capability. RAPID [Venkatesan et al. 2006] is an
OS-based solution that allocates pages to the DRAM rows with longer retention time.
Another approach is to extend the refresh interval and handle the extra errors due
to retention failures by using a strong ECC code. Hi-ECC [Wilkerson et al. 2010] is
one such approach. To reduce storage overhead, it increases the cache-line size to 1KB,
resulting in significant bandwidth overhead. Another scheme, Morphable ECC [Chou
et al. 2015], uses a strong ECC code with long latency in idle mode to compensate for
errors due to long refresh intervals and a weak code during active mode. Our approach
to reducing refresh power is also through use of strong ECC code. However, our design
reduces refresh power when the computer system is in active mode.
3. OVERVIEW OF RATT-ECC

We propose RATT-ECC, a robust and low-overhead ECC scheme that can cope with
all types of faults (transient and permanent, small- and large-granularity faults) as
described in Section 2. A high-level view of RATT-ECC is shown in Figure 2. RATTECC is a two-tiered scheme, like E-RAS [Jeon et al. 2014], Citadel-1 [Nair et al. 2014],
and Citadel-2 [Nair et al. 2016]. For each memory-read request, along with the data,
the corresponding tier-1 ECC parity symbols are read out and used to perform error correction and detection. The tier-1 code is a strong symbol-based code that can
correct all small-granularity faults and can detect large-granularity faults with high
probability. The tier-2 code is an XOR-based correction code that is used to correct
large-granularity faults that cannot be corrected by the tier-1 code. Choosing a strong
code for tier-1 reduces the number of activations of tier-2 code, and helps reduce error
leaks. Furthermore, tier-1 code and tier-2 code can be modified to provide for an extra
spare bank when a data bank has been marked as faulty. In the following, we explain
how RATT-ECC deals with different types of faults.
ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

24:6

H.-M. Chen et al.

Fig. 2. RATT-ECC overview. Tier-1 codeword is formed by concatenating 512b or 64 symbols (that are read
from one of 64 data banks) with tier-1 ECC symbols (that are read from one of banks in the ECC die).
Tier-1 is used for correcting errors due to small-granularity faults. Tier-2 is used for correcting errors due to
large-granularity faults. Tier-1 and Tier-2 can each provide one spare bank (shaded) to handle permanent
bank failures.

3.1. Strategies for Handling 3D DRAM Faults

Transient small-granularity faults: We use a strong tier-1 code that can correct
errors due to transient small-granularity faults, such as single-bit, single-column and
single-TSV faults, without triggering tier-2, and which has strong detection capability
to avoid error leaks.
Permanent small-granularity faults: If the small-granularity faults, such as TSV
faults, have been marked as permanent by the MCA, we can treat the errors due to
these faults as erasures and use erasure correction to correct them.
Transient large-granularity faults: We rely on the strong detection capability of
tier-1 code and the correction capability of tier-2 code to recover from errors due to
transient large-granularity faults such as row or bank failures. The strong detection
capability of tier-1 is required to avoid error leaks since tier-2 code only performs
correction (no detection).
Permanent large-granularity faults: Since correcting errors due to permanent
large-granularity faults by tier-2 code has a large timing overhead, we propose the use
of spare rows/banks. To handle the case in which there are a few row failures in a single
bank, we allocate a few spare rows per bank and house them in the MC. When there
is a permanent bank failure, we propose modifying tier-1 or tier-2 code so that fewer
banks are required to store ECC bits and these banks can now serve as spare banks.
3.2. Design of Tier-1 and Tier-2 Codes

In order to keep the storage overhead at 12.5%, we use only one extra ECC die to protect
eight data dies. Thus, tier-1 and tier-2 parity symbols have to fit into eight banks in
the ECC die. Tier-1 code plays a more important role than tier-2 code because tier1 code needs to have strong error-detection and error-correction capabilities. If more
ECC banks are allocated for tier-2 parity, errors due to large-granularity faults would
be corrected faster. This would imply use of a weaker tier-1 code, however, resulting in
weaker correction and detection capabilities, which is undesirable. Thus, we allocate
at most 2 ECC banks for tier-2 parity storage.
For tier-1 code, to achieve strong correction and detection capability, we propose
using RS code [Lin and Costello 2004]. If we use RS (72,64) code over GF(28 ), the 12.5%
ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

Rate Adaptive Two-Tiered Error Correction Codes for Reliable 3D Die-Stacked Memory

24:7

storage overhead budget would go toward supporting this code and there would be no
room for tier-2 parity, which is essential to recover from errors due to large-granularity
faults. Therefore, RS (72,64) is not a suitable candidate. We can choose RS (71,64) or
RS (70,64) as tier-1 code since both codes can correct errors due to small-granularity
faults and detect errors with very high probability (the undetected error rate of both
codes is as strong as CRC-32).
If RS (71,64) code is used, seven parity symbols need to be distributed among seven
ECC banks (one ECC bank is reserved for tier-2). The minimum number of additional
accesses with power-of-two mapping is 3 (three ECC banks store 4 + 2 + 1 parity
symbols). This assignment hurts timing performance; thus, we consider only RS (70,64)
code as our tier-1 code. In this case, tier-1 parity is distributed among six ECC banks
and tier-2 parity across two ECC banks.
Finally, we choose RS code because of the embedded structure of these codes, which
allows for rate adaptation – a feature that is exploited when the memory system has
faulty banks. The ECC code rate R is defined as R = nk , where k is the size of information
symbols and n is the size of an ECC codeword [Lin and Costello 2004]. For example,
the code rate for RS (70,64) code is 64
.
70
As shown in the Appendix, RS (70,64) can be used to derive RS (69,64) and RS
(68,64) codes. The encoding of RS (68,64) code is embedded in the RS (69,64) code and
the encoding of RS (69,64) code is embedded in the RS (70,64) code. The decoders of
these three codes have a similar embedded structure. Thus, the codeword of RS (68,64)
can be obtained from the codeword of RS (69,64) by removing the last symbol, and the
codeword of RS (69,64) can be obtained from the codeword of RS (70,64) by removing
the last symbol. Finally, the syndrome vector due to RS (70,64) can be used as the
syndrome vector of RS (69,64) by removing the last symbol and the syndrome vector
due to RS (70,64) can also be used as the syndrome vector of RS (68,64) by removing
the first and last symbols.
In the next section, we show how this embedded structure can help partition the
tier-1 parity symbols into two parts, resulting in less memory access without affecting
the reliability. Also, the embedded structure is used to implement a rate-adaptive tier-1
code that helps provide a spare bank when needed.
4. DETAILS OF RATT-ECC

In this section, we describe how RATT-ECC handles the following scenarios: (i) no
data banks are marked as faulty (Section 4.1), (ii) one data bank is marked as faulty
(Section 4.2), (iii) two data banks are marked as faulty (Section 4.3), and (iv) permanent
data TSV failures (Section 4.4).
4.1. Scenario 1: No Banks are Marked as Faulty

Here, none of the data banks is marked as faulty; thus, all eight ECC banks can be
used by the tier-1 and tier-2 codes. Since we choose RS (70,64) code over GF(28 ) as the
tier-1 code, six ECC banks are reserved for storing parity symbols of tier-1 code; the
remaining two ECC banks are used for tier-2 code.
RS (70,64) has a minimum distance of 7 and supports the following error correction
and detection cases [Lin and Costello 2004]: (i) detects six errors, (ii) corrects one error
and detects five errors, (iii) corrects two errors and detects four errors, and (iv) corrects
three errors. Table I lists the detectable and correctable error (DCE) rate, detectable
but uncorrectable error (DUE) rate and silent data corruption (SDC) rate [Rao and
Fujiwara 1989; Sullivan et al. 2015] for different types of failures for the four cases.
We utilize the formula given in Kasami and Lin [1984] to derive the probability of
undetected error or SDC rate of RS (70,64) code.
ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

24:8

H.-M. Chen et al.
Table I. Error Handling Performance of the Different Decoding Cases of RS (70,64)

Failure Type

Single bit

Single column

Single TSV

Double bit/
column/TSV
Row or Bank

Case (i)
Detects 6
errors
DCE: 0%
DUE: 100%
SDC: 0%
DCE: 0%
DUE: 100%
SDC: 0%
DCE: 0%
DUE: 100%
SDC: 0%
DCE: 0%
DUE: 100%
SDC: 0%
DCE: 0%
DUE: 1 − 3.6 · 10−15
SDC: 3.6 · 10−15

Case (ii)
Corrects 1 error &
detects 5 errors
DCE: 100%
DUE: 0%
SDC: 0%
DCE: 100%
DUE: 0%
SDC: 0%
DCE: 100%
DUE: 0%
SDC: 0%
DCE: 0.2%
DUE: 99.8%
SDC: 0%
DCE: 0%
DUE: 1 − 2.4 · 10−10
SDC: 2.4 · 10−10

Case (iii)
Corrects 2 errors &
detects 4 errors
DCE: 100%
DUE: 0%
SDC: 0%
DCE: 100%
DUE: 0%
SDC: 0%
DCE: 100%
DUE: 0%
SDC: 0%
DCE: 100%
DUE: 0%
SDC: 0%
DCE: 0%
DUE: 1 − 7.2·−6
SDC: 7.2 · 10−6

Case (iv)
Corrects 3
errors
DCE: 100%
DUE: 0%
SDC: 0%
DCE: 100%
DUE: 0%
SDC: 0%
DCE: 100%
DUE: 0%
SDC: 0%
DCE: 100%
DUE: 0%
SDC: 0%
DCE: 0%
DUE: 0%
SDC: 100%

Case (i) has the highest error detection probability (with SDC rate 3.6 · 10−15 ), but
cannot correct any errors. Once the errors are detected by tier-1 code, case (i) relies on
tier-2 code to recover from the errors. Case (ii) can correct a single error, detect up to
five errors, and has 2.4 · 10−10 SDC, while case (iii) can correct double errors, detect four
errors, and has a higher 7.2 · 10−6 SDC. Case (iv) can only correct errors without detecting errors, which is a serious problem in case there are row or bank failures. We choose
case (ii) over case (iii) due to its stronger error-detection probability. Furthermore, case
(ii) can correct single symbol errors, which is important since small-granularity faults
account for a large fraction of all faults (84.8% in 2D DRAM [Sridharan et al. 2015]).
During a memory read request, the 64B block is read from a data bank and its
corresponding tier-1 code is read from ECC banks in the ECC die. Since each 64B
block is encoded with six parity symbols of tier-1 code, if we put all six symbols in a
single row of an ECC bank, it incurs two disadvantages. First, it increases address
decoding complexity due to a non-power-of-two computation. Also, six symbols cannot
fit perfectly if the row size is 2KB or 4KB, resulting in a need for a larger ECC bank
size. Second, if that ECC bank fails, it could lead to six symbol errors that cannot be
detected even by tier-1 code. Thus, we propose distributing the six symbols of tier-1 code
across multiple banks to support power-of-two address mapping and higher reliability.
We can distribute the six symbols in one of the following ways: (i) 1+1+1+1+1+1,
(ii) 2+2+2, or (iii) 4+2, all of which support power-of-two address mapping. In distribution (i), the six symbols are stored in six ECC banks; thus, six additional accesses
are needed to retrieve tier-1 parity symbols. If a single ECC bank fails, it only leads to
one symbol error, which can be easily corrected by tier-1 code. In distribution (ii), the
six symbols are stored in three ECC banks; thus, each memory request leads to three
extra accesses. If an ECC bank fails, it leads to double errors, which can be detected
by tier-1 code but require tier-2 code for correction. In distribution (iii), the 6 symbols
are stored in two different ECC banks, leading to two additional accesses. Of the 6
symbols, 4 symbols are stored in one ECC bank and another 2 symbols are stored in
the second ECC bank. Thus, if an ECC bank fails, it either leads to four symbol errors
(first bank) or double symbol errors (second bank). Both error types can be detected by
tier-1 code and corrected by tier-2 code. We choose distribution (iii) since it requires the
ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

Rate Adaptive Two-Tiered Error Correction Codes for Reliable 3D Die-Stacked Memory

24:9

Fig. 3. Scenario 1: High-level diagram illustrating access of data block, tier-1 code, and tier-2 code during
read.

lowest number of accesses among the three distributions. The parity symbols of tier-1
code now consist of two parts: tier-1a (four symbols) and tier-1b (two symbols).
The tier-2 code is a RAID5-like code based on XOR operations. Since two ECC banks
are reserved for storing the parity symbols of tier-2 code, data in a set of 32 data banks
are XORed and stored in a single tier-2 bank. For example, data in banks 0 to 3 from
dies 0 to 7 are XORed and stored in one tier-2 bank, and data in banks 4 to 7 from dies
0 to 7 are XORed and stored in the second tier-2 bank.
In each memory read request, tier-1a and tier-1b parity symbols stored in two ECC
banks have to be accessed. In order to reduce the number of accesses, we propose
reading only tier-1a parity symbols. The 64B data symbols, along with the 4 tier-1a
parity symbols, form a codeword of RS (68,64) over GF(28 ). As described earlier, RS
(68,64) is obtained by puncturing RS (70,64); thus, the syndrome of this code can be
used to detect errors with SDC as low as 2.3 · 10−10 . If errors are detected, then tier1b parity symbols are retrieved by a second access and RS (70,64) decoder is used to
correct the errors.
To reduce the timing performance overhead of these extra accesses, we use a cache
to store the ECC bits, referred to as ECC cache. Existing schemes such as Nair et al.
[2014] or Jeon et al. [2014] also utilize ECC caches. We cache tier-1a, tier-1b and tier2 parity symbols in the ECC cache; however, we have to check only tier-1a for each
memory read request in the error-free case.
READ: The sequence of operations during read have been described in Figures 3 and
4. First, a 64B block is read from one of the data dies (step 1). The MC checks whether
tier-1a parity symbols are in the ECC cache or not. If it is not cached, the MC reads
the corresponding ECC symbols of tier-1a from the ECC bank (step 2) and calculates
the syndrome corresponding to the RS (68,64) code. If the syndrome vector is zero, no
additional accesses for tier-1b are required; otherwise, the ECC symbols of tier-1b have
to be accessed. If the ECC symbols of tier-1b are not cached, the MC signals an extra
read (step 3). The parity symbols of tier-1a and tier-1b and the 64 symbols from the
data cache line form a codeword of RS (70,64), which can correct a single error and
detect five errors. If RS (70,64) flags a multiple-error event, the MC activates tier-2
code to correct the errors generated by large-granularity faults (step 4). When tier-2 is
launched, 32 data blocks (31 data blocks + 1 ECC data block) have to be read from 32
ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

24:10

H.-M. Chen et al.

Fig. 4. Scenario 1: Sequence of operations corresponding to a read request.

Fig. 5. Three scenarios of RATT-ECC.

ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

Rate Adaptive Two-Tiered Error Correction Codes for Reliable 3D Die-Stacked Memory

24:11

banks (31 data banks + 1 ECC bank). Thus, at most 4 data blocks have to be read from
each die, resulting in 4 sequential reads per channel.
WRITE: When a dirty cache line is evicted from L3 cache to 3D DRAM, it is written
back to one of the data dies, and the corresponding tier-1a, tier-1b, and tier-2 ECC
symbols are written back from the ECC cache to the ECC die. The MC needs to
issue 3 sequential operations to update the ECC symbols. Although we have one
extra write compared to Nair et al. [2014] and Jeon et al. [2014] for updating
tier-1 parity, the extra write operations can be well hidden and our simulation results
(see Section 5) show that the overhead of this extra write is quite small.
4.2. Scenario 2: 1 Data Bank is Marked as Faulty

Once the MC marks a bank as faulty, every read access from that bank would require
activation of both tier-1 and tier-2 codes. This would result in unnecessary performance
overhead. We propose using a spare bank to achieve high system reliability without
degradation in timing performance.
We have two ways to free up a spare bank from the set of eight banks in the ECC
die. We can either modify the rate of tier-1 code or modify the rate of tier-2 code.
(a) Modification of tier-1 code: If the tier-1 RS(70,64) code is punctured to RS (69,64)
code, five banks are needed to store the tier-1 code (instead of six banks); thus, one
bank can be used as the spare bank. The tier-2 code is still stored in two banks.
Unfortunately, RS (69,64) code has weaker detection capability; thus, this modification
is not our first choice. (b) Modification of tier-2 code: If only one bank is used to store
tier-2 code, then the second bank can be used as the spare; the tier-1 code is still stored
in 6 banks. Here, data from all 64 banks (instead of 32 banks) have to be XORed and
stored in one XCC bank. Since tier-2 code data was previously stored in 2 XCC banks,
data in the two XCC banks has to be XORed and stored in one XCC bank. Thus, the
modification of tier-2 changes the error correction latency for only large-granularity
faults. We therefore choose modification (b) over modification (a). Thus, in Scenario 2,
we still use RS (70,64) code as the tier-1 code. Six ECC banks are still used to store the
tier-1 code, one ECC bank is used to store tier-2 code, and one ECC bank is utilized as
a spare bank.
READ: The sequence of operations is essentially the same as in Scenario 1. However,
to recover from errors in a block, row, or bank failures, 64 blocks, rows, or banks have
to be accessed instead of 32 in Scenario 1. To recover from errors in a 64B data block,
64 data blocks (63 data blocks + 1 ECC data block) have to be read from 64 banks (63
data banks + 1 ECC bank); thus, there are 8 sequential read accesses per channel.
WRITE: The write-back operation is the same as in Scenario 1.
4.3. Scenario 3: 2 Banks are Marked as Faulty

If there are two bank failures, we use RS (69,64) code that is obtained by puncturing
RS (70,64) code to free up the second spare bank. We choose to perform single-error
correction and quadruple-error detection with 5.9 · 10−8 SDC as the decoding strategy.
Although RS (69,64) code can also be used to support double-error correction and
triple-error detection, the corresponding SDC drops to 1.26 × 10−4 , which is too low for
a reliable tier-1 code.
The five parity symbols due to RS (69,64) code cannot be stored in the same ECC
bank due to poor storage efficiency. Also, if we store five ECC symbols in the same ECC
bank, a single row or bank failure of an ECC bank can lead to five errors, which cannot
be detected and corrected by tier-1 code. Thus, we distribute five ECC parity symbols
across two ECC banks (four symbols in one ECC bank and the fifth symbol in another
ECC bank). This distribution has two advantages: first, tier-1a parity of RS (69,64)
is the same as that of RS (70,64). Thus, there is no change in the storage pattern of
ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

24:12

H.-M. Chen et al.

Fig. 6. Decoding flowchart of erasure and error correction of RS (70,64) code to handle permanent TSV data
failures.

tier-1a across all three scenarios. Second, the tier-1a continues to have high detection
probability. Finally, for Scenario 3, we need to store only the tier-1b parity symbols
corresponding to RS (69,64). These fit into one ECC bank, freeing up the second bank.
This implies that the tier-1b parity symbols of RS (69,64) that were distributed across
two ECC banks now have to be mapped onto one ECC bank.
READ: When a 64B block is read from one of the dies, the MC checks whether tier-1a
parity symbols are in ECC cache, then sends the 64 + 4 symbols to the RS (68,64) unit.
If the syndrome vector is not zero, the MC reads one tier-1b parity symbol to form the
RS (69,64) code. If the RS (69,64) decoder flags a multiple-error event, the MC activates
tier-2 code to correct the errors generated by large-granularity faults.
WRITE: The write-back operation is the same as in Scenarios 1 and 2.
4.4. Handling Permanent Data TSV Failures

In the organization described in Figure 1, a single data TSV failure leads to only one
symbol error, which can easily be corrected by tier-1 code. However, both tier-1a and
tier-1b parity symbols are needed to perform correction; in the worst case, this leads
to two additional memory accesses. Now, if the TSV failure is marked as a permanent
failure by the MC, the corresponding symbol can be treated as an erasure. Since the
error location of an erasure is known (by definition), correcting erasures is a lot simpler
compared to correcting random errors.
Figure 6 gives the decoding flow chart for this case. First, 64 symbols from the data
bank and 4 tier-1a parity symbols are read and sent to the RS (68,64) unit (which
is embedded in the RS (70,64) unit). After the syndrome vector is generated, the RS
(68,64) decoder checks whether the syndrome vector is a single-erasure event. In the
case of an erasure, it corrects it. When there are more errors, two tier-1b symbols are
read out from an ECC bank and a 70 (=68+2) symbol codeword is sent to the RS (70,64)
decoder, which then checks whether it is a single-error and single-erasure event. If so,
it corrects the errors; otherwise, it launches tier-2. The decoding procedures of singleerasure correction of RS (68,64) and single-erasure and single-error correction of RS
(70,64) are shown in the Appendix. Note that our tier-1 has been designed to correct
single erasure (due to TSV fault) along with a single error (which could be caused by
TSV fault) because single-bit errors have a high probability of occurrence. Thus, RATTECC can handle errors due to two data TSV faults (one of which is marked faulty)
with no performance overhead. In case there are faults in the address TSV lines, we
could utilize the method in Jaewoong et al. [2013b], which folds the row index into the
ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

Rate Adaptive Two-Tiered Error Correction Codes for Reliable 3D Die-Stacked Memory

24:13

Table II. Simulation Configuration
Core
L1 I-cache
L1 D-cache
L2 cache
L3 cache
Size
Bus frequency
Channels
Memory controller
Banks
Data block size
Scheduling policy
Row buffer
tRAS-tCAS-tRP

Processors
8 cores, 3.2GHz out-of-order, 128 ROB
4-way, 32KB, 1 cycle; 64B cache line
4-way, 32KB, 2 cycle; 64B cache line
8-way, 256KB, 4 cycle; 64B cache line
16-way, shared 8MB, 24 cycles; 64B cache line
3D Stacked DRAM
9GB, 8 data layers + 1 ECC layer
800MHz (DDR3 1.6GHz), 128b per channel
8 + 1 channels per stack
9 channel controllers
8 banks per channel
64B
FR-FCFS
2KB
27-9-9

data and computes the exclusive-OR of the data with several copies of the row index to
detect row decoder failures.
5. EVALUATION

Section 5.1 describes the methodology for our experimentation. We analyze the hardware overhead of RATT-ECC in Section 5.2. The timing performance with ECC cache
is elaborated in Section 5.3. The refresh power reduction achieved by increasing the
refresh interval is described in Section 5.4.
5.1. Simulation Infrastructure and Workloads

To evaluate the timing performance of our proposed ECC scheme, we utilize macsim
[HPArch 2009], a cycle-level x86 simulator. We model an eight-core processor, each
core having a private 32KB(I) cache + a 32KB(D) L1 cache and a unified 256KB L2
cache. The size of the shared L3 cache is 8MB. The memory system uses 3D-stacked
DRAM with 8 data dies + 1 ECC die (1GB per die). There are 9 channel controllers
with each controller controlling an independent channel. The detailed configuration of
our system is described in Table II.
We use 17 SPEC CPU2006 benchmarks and simulate a representative set of 250M
instructions identified with SimPoint [Perelman et al. 2003]. We categorize the applications into four different groups based on the misses per thousand instructions
(MPKI) in the L3 cache. We simulate 4 high-memory intensity workloads (MPKI is
above 27), 2 high-median-intensity workloads (MPKI is between 15 and 27), 4 medianintensity workloads (MPKI is between 1 and 15) and 7 low workloads (MPKI is below 1).
Table III summarizes the workload combinations.
For power-consumption simulation, we use a cycle-accurate simulator, DRAMSim2
[Rosenfeld et al. 2011] to obtain the DRAM power consumption. We simulate the 9channel structure (8 channels for 8 data dies + 1 channel for 1 ECC die). Since there
is no power model for HBM-like 3D DRAM, we utilize the DDR3 power model (1GB to
8GB) provided by Micron [JEDEC 2010] for each of the dies.
We extract the memory traces from macsim for the 17 SPEC CPU2006 benchmarks
and feed the memory traces to DRAMSim2. We also add the additional memory accesses
to the ECC die for the case in which there is an ECC cache miss or extra correction is
needed due to refresh errors.
ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

24:14

H.-M. Chen et al.
Table III. Simulation Workloads
Mix
WL-1
WL-2
WL-3
WL-4
WL-5
WL-6
WL-7
WL-8
WL-9
WL-10
WL-11
WL-12
WL-13
WL-14
WL-15
WL-16
WL-17

Workload
8 x astar
8 x bwaves
8 x bzip2
8 x dealII
8 x gromacs
8 x h264ref
8 x lbm
8 x libquantum
8 x mcf
8 x namd
8 x omnetpp
8 x perlbench
8 x povray
8 x sjeng
8 x soplex
8 x tonto
8 x zeusmp

MPKI
13.51
30.8
2.15
0.68
0.68
1.24
43.34
23.05
83.85
0.1
22.23
0.06
0.02
0.33
27.73
0.02
6.16

Group
M
H
M
L
L
M
H
HM
H
L
HM
L
L
L
H
L
M

Table IV. Synthesis Results for Existing ECC Schemes
Rotational code
Syndrome Calculation
Rotational code
SSC-DSD
RS(70,64)
Syndrome Calculation
RS(70,64)
Single-Error Correction
RS(70,64)
Single-Erasure Correction
RS(70,64)
Single-Erasure and SingleError Correction

Area
2,000um2

Latency
0.41ns

2,760um2

0.41ns

11,337um2

0.52ns

2,024um2

0.47ns

1,336um2

0.33ns

5,658um2

1.01ns

Power
0.0083 mW (Static)
14.5 mW (Dynamic)
0.0015 mW (Static)
12.5 mW (Dynamic)
0.0645 mW (Static)
36.345 mW (Dynamic)
0.0195 mW (Static)
22.53 mW (Dynamic)
0.0144 mW (Static)
25.62 mW (Dynamic)
0.0271 mW (Static)
21.44 mW (Dynamic)

5.2. Hardware Overhead of RATT-ECC

The overhead of the RATT-ECC scheme consists of ECC decoding units, ECC cache,
and the modification required to support rate-adaptive codes.
ECC Decoding Units: We synthesized the syndrome calculation and the error correction units for rotational code [Rao and Fujiwara 1989] that is used for E-RAS and
RS(70,64) code using 28nm industry library. Syndrome and error-correction units
for RS(69,64) and RS(68,64) are derived from the RS(70,64) unit. Basically, small
additional logic is used to support decoding of the punctured codes (RS(69,64) and
RS(68,64)). Table IV gives the area, latency, and power information for rotational code
and RS(70,64). Since syndrome calculation is on the critical path of memory read, its
decoding latency is made as small as possible. The decoding latency of syndrome calculation for a single rotational code is 0.41ns; since 4 rotational codes are used per
channel, the area increases to 8000 um2 . The decoding latency of syndrome calculation
of RS(70,64) code is slightly higher at 0.52ns and its area is 11,337 um2 . If there is
ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

Rate Adaptive Two-Tiered Error Correction Codes for Reliable 3D Die-Stacked Memory

24:15

a faulty TSV line, the MC launches single-erasure correction, which has a latency of
0.33ns or single-erasure and single-error correction, which has a latency of 1.01ns.
Considering that an average latency of a memory read is around 30ns to 40ns [Meng
et al. 2011] and the lower clock frequency of L3 cache (1ns), the latency of syndrome
calculation of RS(70,64) code incurs only one extra cycle in L3 cache or DRAM cycle.
Since the logic die area of the existing computer system is around 200-500 mm2 , the
size of the ECC unit consumes negligible extra area in the logic die.
Rate-Adaptive Code Implementation: The 64 data symbols and 4 symbols of the
tier-1a code are decoded using the RS(68,64) decoder, which is embedded in the RS
(70,64) decoder. If an error is detected, 2 additional symbols are read and sent to the
RS(70,64) decoder. When there is a faulty bank marked by the MCA (Scenario 2),
data in two ECC banks of tier-2 are XORed and stored in one ECC bank. This step
can be done in parallel with storing corrected data in the spare bank. However, when
the second spare bank is required (Scenario 3), we move from RS(70,64) to RS(69,64)
to free up a bank and only the tier-1b parity symbols that are common to RS(69,64)
and RS(70,64) have to be stored. The tier-1b parity symbols of RS(69,64) that were
originally stored in the bank marked spare have to be routed to the other tier-1b bank.
Finally, to handle remapping of faulty banks, we utilize the bank remap table in the
MC, which associates a faulty bank address with the spare bank address.
We assume that there are up to 4 row failures in each bank [Nair et al. 2014]. Since
each row is of size 2KB, and there are 64 data banks, the MC has a memory of size
4 × 64 × 2 KB = 512 KB to store spare rows. To handle remapping of rows, we utilize a
row remap table in the MC, which associates a faulty row address with the spare row
address.
ECC cache: We consider a simple direct mapped ECC cache with sizes ranging from
256KB to 2MB; the effect of the cache size is analyzed in Section 5.3. The ECC cache
size could be reduced with a set associative cache organization.
5.3. Timing Performance Analysis

5.3.1. ECC Cache Implementation. The proposed ECC cache stores tier-1a, tier-1b, and
tier-2 parity symbols. We allocate 50% of the ECC cache to tier-1a, 25% to tier-1b, and
25% to tier-2. This distribution is motivated by the fact that, in the ECC die, four
banks are used to store tier-1a parity symbols, two banks are used to store tier-1b
parity symbols, and another two banks are used to store tier-2 parity symbols. The
ECC cache is housed in the logic die. It is implemented as a directed mapped cache
with size varying from 256KB to 2MB.
In an error-free system, for each memory read request, the MC checks whether the
corresponding parity symbols of tier-1a are cached or not. Since the ECC cache is direct
mapped, it takes a few cycles to determine whether it is a hit or a miss. If it is a hit, we
assume that there is no delay; if there is a miss, a read request is launched by the MC
to retrieve the data from the tier-1a ECC banks. Every ECC cache miss costs one read
to the ECC die. This corresponds to tRC = tRAS + tRP (worst case), in which tRAS
is 27 cycles and tRP is 9 cycles in our simulation configuration; thus, for a memory
frequency of 800MHz, a read operation takes 45ns. For each write-back request, the
MC has to update the data block in the data bank along with the tier-1a, tier-1b, and
tier-2 parity symbols in the ECC banks. This takes 3 sequential writes to the ECC die,
which corresponds to 135ns in our simulation.
5.3.2. Timing Performance of RATT-ECC. We assume that the system is error-free. Also,
we simulate only for Scenario 1 (no banks are marked faulty) since it is projected to
have the worst timing performance. The ECC cache-line size is also set to 64B, which is
consistent with the rest of the memory system. Figure 7 gives the hit rate for different
ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

24:16

H.-M. Chen et al.

Fig. 7. Hit rate of different-sized ECC caches.

Fig. 8. Normalized execution time for different ECC cache sizes.

ECC cache sizes. When ECC cache size is larger than 1MB, the hit rate is stable at 70%.
Figure 8 compares the execution time for different ECC cache sizes; the execution time
is normalized to the baseline that does not have any ECC scheme. These results show
that, for ECC cache sizes that are less than 512KB, the execution time is increased
by 5%, which is too large. However, if we use ECC cache sizes of 1MB or 2MB, the
execution time increases by 2.4% and 1.8%, respectively, which is acceptable.
Although we did not perform the simulations for Scenarios 2 and 3, we argue that
Scenario 1 captures the worst-case timing performance. In Scenario 1, a cache line
of the tier-1a part of the ECC cache covers 16 blocks of one data bank, a cache
line in the tier-1b part covers 32 blocks of one data bank, and a cache line in the
tier-2 part of the ECC cache covers 32 data blocks across 32 banks. In Scenario 2,
the tier-2 part of the ECC cache covers 64 blocks across 64 banks. Thus, the hit rate
of the ECC cache will be higher in Scenario 2 compared to Scenario 1. Similarly, in
Scenario 3, the tier-1b part of the ECC cache covers 64 blocks of a data bank (instead of
32 blocks in Scenario 1) and the hit rate of the ECC cache will be higher. Thus, Scenario
1 will have the worst timing performance, followed by Scenario 2 and Scenario 3.
In the error-free case, RATT-ECC and the competing schemes have similar timing
performance when the ECC cache is used. All schemes utilize ECC caches to reduce the
extra reads/writes in the ECC die; thus, there is no performance difference. However,
if there are errors, RATT-ECC, E-RAS [Jeon et al. 2014], and Citadel-2 [Nair et al.
2016] have comparable performance that is better than Citadel-1 [Nair et al. 2014]
since Citadel-1 takes 0.7s to correct transient errors, including single-bit errors.
If there is no ECC cache, our simulation results show that there is a 14% performance degradation. This is because each tier-1a miss during a read costs 45ns and the
writes to tier-1a, tier-1b, and tier-2 banks have to be done sequentially and cost a total
of 45 × 3 = 135ns.
ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

Rate Adaptive Two-Tiered Error Correction Codes for Reliable 3D Die-Stacked Memory

24:17

Table V. Number of DRAM Cell Retention Errors as a
Function of Refresh Interval in a 60nm Process
Refresh interval
128ms
256ms

8GB
8
250

Memory size
16GB
32GB
15
30
500
1000

64GB
60
2000

Note: The default refresh interval is 64ms based on
Kim and Lee [2009].

Fig. 9. Number of additional error corrections for 8GB DRAM when the refresh interval is increased.

5.4. Refresh Power Evaluation

Current 3D DRAM memories, such as HBM, are 8GB [JEDEC HBM 2013] and are projected to become larger in the near future. As the 3D DRAM size increases, the refresh
power accounts for a larger part of DRAM power. In order to reduce the refresh power,
we increase the refresh interval and correct the errors caused by the larger interval
to maintain the same reliability level as the baseline (64ms). In our simulation, we
utilize the error rates due to the increase in refresh interval from Kim and Lee [2009].
Table V shows the significant increase in the number of single-bit errors (retention
failures) as the refresh interval increases for different memory sizes. According to Kim
and Lee [2009], weak cells caused by increasing the refresh interval are randomly
distributed throughout the DRAM array. Thus, for each memory read access, there is
at most one bit error (among 512b). This error can be corrected by tier-1 code with low
decoding latency.
The default refresh rate is set to 64ms (as DDR3), which corresponds to the error-free
case; thus, we normalize all the results to this case. For 8GB DRAM, if we increase the
refresh interval to 128ms, there would be 8 additional errors, as indicated in Table V.
We inject errors in 8 random locations in the memory trace of each benchmark corresponding to the worst-case consideration. Similarly, if the refresh interval is 256ms,
we inject 250 errors in random locations in the memory trace. We run the simulations
100 times and take the average for each benchmark. The number of additional error
corrections for each benchmark is shown in Figure 9.
When the memory read access contains data from the leaky cell (single-bit error),
the MC activates the correction operation to correct this error. RATT-ECC relies on
tier-1 code to correct this error, and the overhead is quite small. The overhead is due
to the tier-1 ECC cache miss, which has a penalty of 100ns to read data. The syndrome
calculation and single-error correction takes only 0.52ns and 0.47ns (from Table IV),
respectively.
Figure 10 shows the power reduction of the RATT-ECC scheme for refresh intervals
of 128ms and 256ms compared to when the refresh interval is 64ms. While the refresh
power reduces by 50% when the refresh interval is increased to 128ms and by 75% when
the refresh interval is increased to 256ms, the overall power reduction is a lot lower.
For instance, the power consumption is reduced by an average of 4.1% (maximum 7.1%
ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

24:18

H.-M. Chen et al.

Fig. 10. Power reduction of 8GB memory.

Fig. 11. Power reduction of 8GB to 64GB memory.

and minimum 2%) and 7.1% (maximum 11.2% and minimum 3.2%) when the refresh
interval is 128ms and 256ms, respectively.
We also increase the DRAM memory size from 8GB to 64GB and change the corresponding DRAM parameters in the power consumption simulation of DRAMSim2.
The result is shown in Figure 11. For memory size of 16GB, 32GB, and 64GB, the
power consumption reduces by an average of 11.9%, 14.6%, and 16.3%, respectively,
if the refresh interval is 128ms, and reduces by an average of 15.1%, 17.6%, and
21.6%, respectively, if the refresh interval is 256ms. Thus, for large memory size,
the total power consumption reduces significantly, and since all additional singlebit errors are corrected by tier-1, this is achieved with negligible loss in timing
performance.
Finally, we also compare the timing and power performance of RATT-ECC to ERAS [Jeon et al. 2014], Citadel-1 [Nair et al. 2014], and Citadel-2 [Nair et al. 2016]
for larger refresh intervals. When the refresh interval is increased, RATT-ECC, ERAS, and Citadel-2 have very little overhead in timing performance, since they can all
correct the additional single-bit errors using the tier-1 code. However, Citadel-1 takes
0.7s to correct every single-bit error, which increases its total execution time and power
consumption significantly.
6. RELIABILITY

In this section, we analyze the reliability of RATT-ECC and compare it with respect to
the existing methods. We consider several failure modes. A single-bit failure leads to
only a single-bit error in a data block. A single-column failure also leads to a single-bit
error in a data block (since data is read out along a row). A single-TSV failure leads to
4b errors in a data block but, due to the access alignment, a TSV failure leads to only
one symbol error. A single-row or bank failure leads to multiple bit errors (up to 64
symbol errors) in a single data block.
We begin by analyzing the performance of the tier-1 codes using the three metrics:
DCE, DUE, and SDC. RS(70,64), used in Scenarios 1 and 2, has strong correction and
ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

Rate Adaptive Two-Tiered Error Correction Codes for Reliable 3D Die-Stacked Memory

24:19

Table VI. Comparison of the Error-Handling Performance of Tier-1 Codes Used in the Existing Schemes
E-RAS
Error Type

Citadel

4 x Rotational code CRC-32

CRC-30 & SEC

[Jeon et al. 2014]

[Nair et al. 2016]

[Nair et al. 2014]

RATT-ECC
1 x RS (69,64)

1 x RS (70,64)

Single-bit

DCE: 100%

DCE: 0%

DCE: 100%

DCE: 100%

DCE: 100%

failure

DUE: 0%

DUE: 100%

DUE: 0%

DUE: 0%

DUE: 0%

SDC: 0%

SDC: 0%

SDC: 0%

SDC: 0%

SDC: 0%

Single-column DCE: 100%

DCE: 0%

DCE: 100%

DCE: 100%

DCE: 100%

failure

DUE: 0%

DUE: 100%

DUE: 0%

DUE: 0%

DUE: 0%
SDC: 0%

SDC: 0%

SDC: 0%

SDC: 0%

SDC: 0%

Single-TSV

DCE: 100%

DCE: 0%

DCE: 0%

DCE: 100%

DCE: 100%

failure

DUE: 0%

DUE: 100%

DUE: 100%

DUE: 0%

DUE: 0%
SDC: 0%

SDC: 0%

SDC: 0%

SDC: 0%

SDC: 0%

Row or Bank

DCE: 0%

DCE: 0%

DCE: 0

DCE: 0%

DCE: 0%

failure

DUE: 97%

DUE: 1 - 2.3 · 10−10

DUE: 1 - 10−9

DUE: 1 − 5.9 · 10−8

DUE: 1 - 2.4 · 10−10

SDC: 3%

SDC: 2.3 · 10−10

SDC: 10−9

SDC: 5.9 · 10−8

SDC: 2.4 · 10−10

detection capabilities. Although we read only the first 4 symbols for detection, the
corresponding RS(68,64) code still has a very low SDC of 2.3 · 10−10 . Once RS(68,64)
code detects errors, the other two ECC symbols are read and the RS(70,64) decoder
is activated. It corrects all errors caused by the small-granularity faults and detects
errors caused by large-granularity faults with 2.4 · 10−10 SDC. RS(69,64) code, used
in Scenario 3, can correct one symbol error and detect four symbol errors, and has
slightly higher SDC (5.9 · 10−8 ). The DCE, DUE, and SDC rates of RS(70,64) (RATTECC Scenarios 1 and 2) and RS(69,64) (RATT-ECC Scenario 3) are summarized in
Table VI.
The capability of RATT-ECC to handle errors due to different types of faults is as
follows:
Transient small-granularity faults: RATT-ECC can correct all errors due to
transient small-granularity faults without activating tier-2.
Permanent small-granularity faults: RATT-ECC uses erasure-correction capability of the RS code to directly correct errors due to permanent data TSV faults
by only using tier-1a. The latency of single-erasure correction is only 0.33ns (see
Table IV). Since single errors have a high probability of occurrence, RATT-ECC also
supports single-erasure and single-error correction. The additional error can be due
to a single-bit, single-column, or single-TSV failure, and our design can correct this
additional error without activating tier-2.
Transient large-granularity faults: The strong tier-1 code detects almost all errors due to large-granularity faults with a very small SDC rate. These errors are
corrected by tier-2 code.
Permanent large-granularity faults: Large-granularity faults tend to have a bimodal distribution as indicated by Nair et al. [2014]. There are either a few row failures
(less than four rows) or a large number of row failures (more than 4K rows) in a single bank. Row failures are handled by utilizing spare rows that are stored in the
MC. The rates of tier-1 and tier-2 codes are used to free up ECC banks to be used as
spare banks. Thus, RATT-ECC allows for ECC banks to be used as spares only when
needed.
FIT Analysis: Real-world field data from Sridharan and Liberty [2012] provides
DRAM FIT rate for a 1GB 2D DRAM device. Since field data are not available for 3D
DRAM, we scaled the error rates by 10× to account for higher error rates in 3D DRAM,
as in Jeon et al. [2014]. The TSV failure rate is also borrowed from Jeon et al. [2014],
ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

24:20

H.-M. Chen et al.
Table VII. FIT Analysis

Resultant FIT
Raw FIT
E-RAS
Citadel-1
Citadel-2
RATT-ECC
Failure Mode Trans.(Perm.) [Jeon et al. 2014] [Nair et al. 2014] [Nair et al. 2016]
Single-bit
142(186)
0(0)
0*(0)
0(0)
0(0)
Single-column
14(56)
0(0)
0*(0)
0(0)
0(0)
Single-row
2(82)
0.06*(2.46*)
4.6 · 10−10 *(0)
2 · 10−9 *(0)
4.8 · 10−10 *(0)
Single-bank
20(142)
0.6*(4.26*)
4.6 · 10−9 *(0)
2 · 10−7 *(0)
4.8 · 10−9 *(0)
Single-TSV
20(21)
0(0)
0*(0)
0*(0)
0(0)
Summary
685
7.38
5.06 · 10−9
2.02 · 10−7
5.28 · 10−9
*means that the correction should be performed by tier-2.

and we assume that the transient and permanent TSV faults are each 50%. Table VII
presents the final FIT rate of the competing schemes along with the breakdown of
transient/permanent FIT rates.
Comparison with existing schemes: Since E-RAS [Jeon et al. 2014] uses a rotational code (in Rao and Fujiwara [1989]) to correct errors due to small-granularity
faults and to detect errors due to large-granularity faults, its SDC rate for row or
bank failures is quite large. While the CRC-32-based scheme in Nair et al. [2014]
has very good detection capability, it cannot correct any errors without launching
tier-2. The more recent scheme in Nair et al. [2016] can correct single-bit errors
and still has good detection capability since it uses CRC-30. Table VI compares
the error-handling performance of all the schemes for errors due to each type of
failure.
Overall, all schemes can completely remove the errors due to small-granularity
faults. For errors due to large-granularity faults, RATT-ECC and Citadel-1 [Nair et al.
2014] can reduce the raw FIT rates to around 5 · 10−9 , which improves the FIT rate
by more than 1010 x compared to the baseline (no ECC) scheme. Since RATT-ECC and
Citadel both provide for spare rows and banks, we assume that all permanent rows
or bank faults can be removed. Citadel-2 [Nair et al. 2016] has a slightly larger FIT
rate of 2 · 10−7 compared to Nair et al. [2014] since it uses CRC-30. E-RAS [Jeon et al.
2014] decreases the overall raw FIT rate to only 7.38 because of use of a weaker code
in tier-1.
7. CONCLUSION

In this article, we presented RATT-ECC, a two-tiered error-correction scheme that
provides very high reliability with minimal performance degradation for HBM-like
3D DRAM memory systems. Unlike existing schemes that either use weaker codes or
have large decoding latency, we use RS(70,64), a strong tier-1 code with small decoding
latency, that can correct all errors due to small-granularity faults and reliably detect
errors due to large-granularity faults (with SDC 2.4 · 10−10 ). Being able to correct
errors due to small-granularity faults (caused by an increase in refresh interval) with
low decoding latency also allows RATT-ECC to reduce refresh power with minimal
performance degradation. Finally, RATT-ECC does not earmark banks as spare banks
as in existing schemes; instead, it allows two banks that are used for parity storage
during normal operation to be used as spares. The dynamic sparing is implemented
by utilizing the rate-adaptive feature of the RS and XCC codes. Thus, RATT-ECC is a
low-cost solution to significantly improve the reliability of 3D die-stacked DRAM with
minimal loss in performance through strong error correction, detection, and dynamic
sparing.
ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

Rate Adaptive Two-Tiered Error Correction Codes for Reliable 3D Die-Stacked Memory

24:21

APPENDIX

Embedded RS code structure: We show how the codewords of the three RS
codes—RS(70,64), RS(69,64) and RS(68,64)—are related. Specifically, the codeword
of RS(69,64) can be obtained by removing the last symbol of the codeword of RS(70,64).
Similarly, the codeword of RS(68,64) can be obtained by getting rid of the last two symbols of the codeword of RS(70,64). We utilize this embedded code structure to implement
the rate-adaptive tier-1 code.
The RS(68,64) code can be obtained by shortening the RS(255,251) code [Lin and
Costello 2004]. After shortening, the parity check matrices of RS(69,64) and RS(70,64)
can be derived from that of RS(68,64) [Joiner and Komo 1996]. The parity check matrix
of RS(68,64), H, is given by
⎛

α
α2
α3
α4

1
⎜1
⎜
H=⎝
1
1

α2
α4
α6
α8

···
···
···
···

⎞
α 67
α 134 ⎟
⎟,
α 201 ⎠
α 13



and the parity check matrix, H , for RS(69,64) is given by




H =

1

··· 1 1
H4×68
0

	
.


The parity check matrix for RS(70,64) code can be obtained from H as


H =





1 α5

0
H5×69
80
··· α 0 1

	
.

Assume
that
the generator matrix of RS(68,64) code is G. Then, the generator matri

ces
G
and
G
of
RS(69,64) and RS(70,64) can be derived from Joiner and Komo [1996].

G is given by




G = ( G g ),


and G is given by






G = ( G g ),




where g and g are given in Joiner and Komo [1996].

For encoding, the codeword of RS(70,64) code can be expressed in the
form c =

(c0 , c1 , . . . , c67 , c68 , c69 ), which is generated by using G . The codeword c of RS(69,64)
is simply obtained by puncturing c69 in c . Similarly, the codeword c of RS(68,64) is
obtained by puncturing c68 and c69 in c . This demonstrates that all three codes can
share the same encoder.
For decoding
RS(70,64),
the syndrome vector s = (s0 , s1 , s2 , s3 , s4 , s5 ) is obtained by


T
computing c · (H ) . Because of the structure of H , the syndrome vector of RS(69,64)
code can be obtained by removing s5 from the syndrome vector of RS(70,64). Similarly,
the syndrome vector of RS(68,64) code, which is s = (s1 , s2 , s3 , s4 ), can be obtained by
removing s0 and s5 from s . Thus, the syndrome calculation unit of RS(70,64) can be
used to calculate the syndrome of RS(69,64) and RS(68,64). Similarly, the decoding
units of RS(70,64) can be reused for RS(69,64). For example, for single-error correction,
ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

24:22

H.-M. Chen et al.

RS(70,64) decoder checks whether ss10 = ss21 = ss32 = ss43 = ss54 and the controller unit just
turns off the last condition, that is, it only checks ss10 = ss21 = ss32 = ss43 .
Note that if we do not use the embedded RS codes as given here, multiple generator
matrices or parity check matrices would have to be used to implement the rate-adaptive
RS codes, resulting in a significant increase in ECC hardware overhead.
Single-erasure correction: To correct one symbol error due to permanent TSV
failure, we use the single-erasure correction mode of RS(68,64) code. Here, the faulty
TSV symbol, which is marked as an erasure symbol, is replaced with the zero symbol in
the received codeword and fed to the syndrome calculation unit. The syndrome vector
of RS (68,64), s, is compared with the column in the parity check matrix corresponding
to the erasure address. If the erasure address is i, the decoder checks if s = ei hi for
column hi in H. If it holds, the decoder recovers the erasure value ei in address i of the
codeword. Otherwise, tier-1b symbols are read and the single-erasure and single-error
correction unit of RS(70,64) is activated.
Single-erasure correction and single-error correction: Assume that the erasure address is i and the erasure value is ei ; similarly, the error address is j and error
value is e j , where i = j. The decoder needs to check whether the syndrome vector, s ,
is a linear combination of hi and h j , where j is from 0 to 69 and i = j. The hardware
consists of 70 subdecoders, with the j th decoder having h j embedded in it. The MC
feeds column hi to all but the i th subdecoder. If the i th and j th columns of the parity
check matrix are hi = (hi0 , hi1 , hi2 , hi3 , hi4 , hi5 )T and h j = (h j0 , h j1 , h j2 , h j3 , h j4 , h j5 )T ,
then s0 = ei · hi0 + e j · h j0 , s1 = ei · hi1 + e j · h j1 equations are used to obtain ei and e j .
The decoded ei and e j values are substituted back to calculate s˜2 = ei · hi2 + e j · h j2 ,
s˜3 = ei · hi3 + e j · h j3 , s˜4 = ei · hi4 + e j · h j4 and s˜5 = ei · hi5 + e j · h j5 . If s˜2 = s2 , s˜3 = s3 , s˜4 = s4 ,
and s˜5 = s5 all hold, the decoder declares that the error is located in location j of the
received codeword and corrects it. Otherwise, it declares that there are more errors.

REFERENCES
AMD. 2011. AMD64 Architecture Programmer’s Manual Volume 2: System Programming, May, 2013.
http://developer.amd.com/wordpress/media/2012/10/24593_APM_v21.pdf.
I. Bhati, Z. Chishti, S.-L. Lu, and B. Jacob. 2015. Flexible auto-refresh: Enabling scalable and energyefficient DRAM refresh reductions. In ACM/IEEE 42nd Annual International Symposium on Computer
Architecture (ISCA’15). 235–246.
C. Chou, P. Nair, and M. K. Qureshi. 2015. Reducing refresh power in mobile devices with morphable ECC.
In 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN’15).
355–366.
B. Giridhar, M. Cieslak, D. Duggal, R. Dreslinski, H.-M. Chen, R. Patti, B. Hold, C. Chakrabarti, T. Mudge,
and D. Blaauw. 2013. Exploring DRAM organizations for energy-efficient and resilient exascale memories. In International Conference on High Performance Computing, Networking, Storage and Analysis
(SC’13).
HPArch. 2009. Macsim simulator. (2009). https://code.google.com/p/macsim/downloads/list.
A. Hwang, I. Stefanovici, and B. Schroeder. 2012. Cosmic rays don’t strike twice: Understanding the nature of
DRAM errors and the implications for system design. SIGARCH Computer Architecture News 111–122.
Intel. 2011. Intel xeon processor e7 family: Reliability, availability and serviceability: Advanced data integrity
and resiliency support for mission-critical deployment. http://www.intel.com/content/dam/www/public/
us/en/documents/white-papers/xeon-e7-family-ras-server-paper.pdf.
S. Jaewoong, Gabriel H. Loh, Vilas Sridharan, and M. O’Connor. 2013a. Resilient die-stacked DRAM caches.
In Proceedings of the 40th Annual International Symposium on Computer Architecture. 416–427.
S. Jaewoong, G. H. Loh, V. Sridharan, and M. O’Connor. 2013b. Resilient die-stacked DRAM caches. In
Proceedings of the 40th Annual International Symposium on Computer Architecture (ISCA’13). 416–427.
J. Jeddeloh and B. Keeth. 2012. Hybrid memory cube new DRAM architecture increases density and performance. In Symposium on VLSI Technology (VLSIT’12). 87–88.

ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

Rate Adaptive Two-Tiered Error Correction Codes for Reliable 3D Die-Stacked Memory

24:23

JEDEC. 2010. DDR3 SDRAM specification. http://www.jedec.org/sites/default/files/docs/JESD79-3E.pdf, July
2010.
JEDEC HBM. 2013. High bandwidth memory (HBM) DRAM, JESD235. https://www.jedec.org/sites/default/
files/docs/JESD235A.pdf, Nov. 2015.
H. Jeon, G. Loh, and M. Annavaram. 2014. Efficient RAS support for die-stacked DRAM. In IEEE International Test Conference (ITC’14). 1–10.
X. Jian, H. Duwe, J. Sartori, V. Sridharan, and R. Kumar. 2013. Low-power, low-storage-overhead Chipkill
correct via multi-line error correction. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis (SC’13).
L. L. Joiner and J. J. Komo. 1996. Time domain decoding of extended Reed-Solomon codes. In Proceedings of
the IEEE Southeastcon’96. Bringing Together Education, Science and Technology. 238–241.
T. Kasami and S. Lin. 1984. On the probability of undetected error for the maximum distance separable
codes. IEEE Transactions on Communications 32, 9, 998–1006.
J. Kim and M. C. Papaefthymiou. 2003. Block-based multiperiod dynamic memory design for low dataretention power. IEEE Transactions on Very Large Scale Integration (VLSI) Systems 11, 6, 1006–1018.
K. Kim and J. Lee. 2009. A new investigation of data retention time in truly nanoscaled DRAMs. IEEE
Electron Device Letters 30, 8, 846–848.
S. Lin and D. J. Costello. 2004. Error Control Coding (2nd ed.). Pearson, New York.
J. Liu, B. Jaiyen, R. Veras, and O. Mutlu. 2012. RAIDR: Retention-aware intelligent DRAM refresh. In 39th
Annual International Symposium on Computer Architecture (ISCA’12). 1–12.
D. Locklear. 2000. Chipkill correct memory architecture. Dell Enterprise System Group. http://www.ece.umd.
edu/courses/enee759h.S2003/references/chipkill.pdf, Aug. 2000.
G. H. Loh. 2008. 3d-stacked memory architectures for multi-core processors. In 35th International Symposium on Computer Architecture (ISCA’08). 453–464.
J. Meng, D. Rossell, and A. K. Coskun. 2011. 3D systems with on-chip DRAM for enabling low-power highperformance computing. In IEEE High Performance Embedded Computing, HPEC.
P. Nair, D. Roberts, and M. Qureshi. 2014. Citadel: Efficiently protecting stacked memory from large granularity failures. In 47th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO’14).
51–62.
P. J. Nair, D. Roberts, and M. Qureshi. 2016. Citadel: Efficiently protecting stacked memory from TSV and
large granularity failures. ACM Transactions on Architecture and Code Optimization (TACO) 12, 4,
49:1–49:24.
E. Perelman, G. Hamerly, M. Biesbrouck, T. Sherwood, and B. Calder. 2003. Using SI’mPoint for accurate
and efficient simulation. In Proceedings of the 2003 ACM International Conference on Measurement and
Modeling of Computer Systems (SIGMETRICS’03).
T. R. Rao and E. Fujiwara. 1989. Error-Control Coding for Computer Systems. Prentice-Hall, Upper Saddle
River, NJ.
P. Rosenfeld, E. Cooper-Balis, and B. Jacob. 2011. DRAMSim2: A cycle accurate memory system simulator.
IEEE Computer Architecture Letters 10, 1, 16–19.
SPEC2006. 2011. SPEC CPU2006 benchmark suite. Retrieved July 23, 2016 from http://www.spec.org/
cpu2006/.
V. Sridharan, N. DeBardeleben, S. Blanchard, K. B. Ferreira, J. Stearley, J. Shalf, and S. Gurumurthi. 2015.
Memory errors in modern systems: The good, the bad, and the ugly. Proceedings of the 20th International
Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS’15)
297–310.
V. Sridharan and D. Liberty. 2012. A field study of DRAM errors. In International Conference on High
Performance Computing, Networking, Storage and Analysis (SC’12).
V. Sridharan, J. Stearley, N. DeBardeleben, S. Blanchard, and S. Gurumurthi. 2013. Feng Shui of supercomputer memory: Positional effects in DRAM and SRAM faults. International Conference on High
Performance Computing, Networking, Storage and Analysis (SC’13).
M. Sullivan, J. Kim, and M. Erez. 2015. Bamboo ECC: Strong, safe, and flexible codes for reliable computer
memory. In IEEE International Symposium on High Performance Computer Architecture (HPCA’15).
Tezzaron. 2014. Octopus. Retrieved July 23, 2016 from http://www.tezzaron.com/.
A. N. Udipi, N. Muralimanohar, R. Balsubramonian, A. Davis, and N. P. Jouppi. 2012. LOT-ECC: Localized
and tiered reliability mechanisms for commodity memory systems. In International Symposium on
Computer Architecture (ISCA’12). 285–296.

ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

24:24

H.-M. Chen et al.

R. K. Venkatesan, S. Herr, and E. Rotenberg. 2006. Retention-aware placement in DRAM (RAPID): Software
methods for quasi-non-volatile DRAM. In 12th International Symposium on High-Performance Computer
Architecture (HPCA’06). 155–165.
C. Wilkerson, A. R. Alameldeen, Z. Chishti, W. Wu, D. Somasekhar, and S.-L. Lu. 2010. Reducing cache
power with low-cost, multi-bit error-correcting codes. In Proceedings of the 37th Annual International
Symposium on Computer Architecture (ISCA’10).
D. Yoon and M. Erez. 2011. Virtualized ECC: Flexible reliability in main memory. MICRO 31, 1, 11–19.
Received March 2016; revised May 2016; accepted June 2016

ACM Transactions on Architecture and Code Optimization, Vol. 13, No. 3, Article 24, Publication date: September 2016.

3766

IEEE TRANSACTIONS ON COMPUTERS,

VOL. 65,

NO. 12,

DECEMBER 2016

Using Low Cost Erasure and Error Correction
Schemes to Improve Reliability of Commodity
DRAM Systems
Hsing-Min Chen, Supreet Jeloka, Akhil Arunkumar, Student Member, IEEE, David Blaauw, Fellow, IEEE,
Carole-Jean Wu, Trevor Mudge, Life Fellow, IEEE, and Chaitali Chakrabarti, Fellow, IEEE
Abstract—Most server-grade systems provide Chipkill-Correct error protection at the expense of power and performance. In this
paper we present a low overhead solution to improving the reliability of commodity DRAM systems with no change in the existing
memory architecture. Specifically, we propose five erasure and error correction (E-ECC) schemes that provide at least Chipkill-Correct
protection for x4 (Schemes 1, 2 and 3), x8 (Scheme 4) and x16 (Scheme 5) DRAM systems. All schemes have superior error correction
performance due to the use of strong symbol-based codes. Synthesis results in 28 nm node show that the decoding latency of these
codes is negligible compared to the DRAM access latency. In addition, we make use of erasure codes to extend the lifetime of the
DRAM systems. Specifically, once a chip is marked faulty due to persistent errors, all E-ECC schemes correct erasures due to that
faulty chip and also correct an additional random error in a second chip. Evaluation with SPEC2006 workloads show that compared to
x4 Chipkill-Correct schemes, Scheme 5 has the highest IPC improvement (mean of 7 percent) and Scheme 4 has the largest power
reduction (mean of 18 percent) and the largest increase in energy efficiency (mean of 25 percent).
Index Terms—DRAM Memory system, reliability, chipkill-correct, error control coding (ECC), erasure and error correction

Ç
1

INTRODUCTION

M

reliability is a major challenge in the design of
large scale computing systems. More than 40 percent
of hardware related failures are attributed to memory systems [1], and this number is projected to increase in the
future. Memory systems are vulnerable to different kinds of
faults (e.g., hard, intermittent or random) [2], [3]. These
faults manifest as single bit errors, multiple errors along a
row and/or along a column of a chip and even a whole chip
failure. The challenge is in designing schemes that have
higher reliability than current systems but with lower
power and performance overhead.
High performance servers are typically expected to have
Chipkill-Correct level protection, that is, the ability to correct errors due to failures of a DRAM chip with 12.5 percent
storage overhead [4], [5]. Chipkill-Correct was first implemented by striping data across multiple chips so that single
bit error correction and double bit detection (SEC-DED)
code could be used to correct errors due to a chip failure [4].
The bit-level Chipkill-Correct code had high power
EMORY



H.-M. Chen and C. Chakrabarti are with SECEE, Arizona State University, Tempe, AZ 85287-5706. E-mail: {hchen136, chaitali}@asu.edu.
 A. Arunkumar and C.-J. Wu are with SCIDSE, Arizona State University,
Tempe, AZ 85287-5706.
E-mail: {akhil.arunkumar, carole-jean.wu}@asu.edu.
 S. Jeloka, D. Blaauw, and T. Mudge are with the Department of Electrical
Engineering and Computer Science, University of Michigan, Ann Arbor,
MI 48109-2121. E-mail: {sjeloka, blauuw, tnm}@umich.edu.
Manuscript received 22 May 2015; revised 15 Mar. 2016; accepted 18 Mar.
2016. Date of publication 4 Apr. 2016; date of current version 14 Nov. 2016.
Recommended for acceptance by C. Metra.
For information on obtaining reprints of this article, please send e-mail to:
reprints@ieee.org, and reference the Digital Object Identifier below.
Digital Object Identifier no. 10.1109/TC.2016.2550455

consumption and low system performance and so symbolbased Chipkill-Correct was proposed for x4 DRAM systems
in [6], [7]. Such a scheme activated 36 chips across two ranks
for every memory access and thus also had high power
consumption.
To reduce the power consumption, many systems moved
to x8 or x16 DRAMs, which activate fewer chips per memory access. For instance, V-ECC [8] activates 188 chips
across two ranks while LOT-ECC [9] and Multi-ECC [10]
only activate nine chips per rank. Although LOT-ECC and
Multi-ECC can reduce the memory power by an average of
more than 25 percent compared to Chipkill-Correct, they
cannot fully correct a chip failure at run time.
In order to activate fewer chips and maintain high reliability, many systems employ two-tier schemes [8], [9], [10], [11],
[12], where the first tier is used for error detection and the
second tier is used for error correction. For example, V-ECC
[8] uses two check symbols in the first tier to perform error
detection and uses the third check symbol in the second tier
to perform error correction. The second tier is usually cached
to reduce the read latency for error correction and to reduce
the number of writes for updating the tier-2 ECC symbols.
Existing schemes such as those in [9], [10] also rely on
replacement of faulty chips to extend the lifetime of the
DRAM memory systems. Some commercial systems use
memory sparing or bit-steering [10], [13], [14] to re-route the
faulty bits or re-route data from a faulty chip to a healthy
chip. This not only reduces the usable physical memory size
but also increases the overhead required to perform re-routing or mapping.
In this paper, we propose a very different approach to
providing at least Chipkill-Correct error protection for

0018-9340 ß 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

CHEN ET AL.: USING LOW COST ERASURE AND ERROR CORRECTION SCHEMES TO IMPROVE RELIABILITY OF COMMODITY DRAM...

commodity DRAM systems. Our approach is based on the
use of stronger symbol based codes which are chosen to
handle the constraints of the different memory systems. Use
of a stronger code adds very little overhead to existing systems. Synthesis results in 28 nm node show that the decoding latencies of these codes are very small and do not affect
the DRAM timing performance. Moreover, unlike the existing multi-tiered schemes, our schemes do not require extra
memory read/write operations to access data for error
correction.
Furthermore, instead of employing chip sparing to
increase the lifetime of memory systems, we make use of
erasure correction, where an erasure is defined as an error
whose location is known. We utilize the machine check
architecture (MCA) [2], [14], [15] to record the error information of each chip. Once the number of errors in a certain
chip increases beyond a threshold, this chip is marked as
faulty and errors due to this chip are treated as erasures.
In the rest of this paper, we present five erasure and error
correction (E-ECC) schemes that provide at least ChipkillCorrect reliability for x4, x8 and x16 DRAM systems. All the
proposed E-ECC schemes can correct errors due to a chip
failure on the fly and can correct one more random error
when the chip is marked as faulty. We analyze the tradeoffs
between reliability and system performance (timing, power
and energy) of these proposed schemes. Overall, this paper
makes the following key contributions:


For x4 DRAM systems, we present three schemes
that all have 12.5 percent storage overhead but differ
in the number of ranks being activated (one or two).
The specific ECC codes used in these three schemes
are rotational (144,128) code over GF(24 ) and RS
(36,32) code over GF(28 ). The schemes that activate
two ranks per memory access have lower timing,
power and energy performance but higher reliability
compared to the one that activates only one rank.
 For x8 DRAM systems, we propose a scheme which
is also based on the RS (36,32) code over GF(28 ); it
has the lowest power consumption and highest
energy efficiency among all five schemes.
 For x16 DRAM systems, we propose a scheme which
is based on the RS (20,16) code over GF(28 ); its storage overhead is 25 percent but has the highest timing
performance among all the schemes.
Finally, compared to existing schemes, the proposed EECC schemes have superior reliability. They all achieve at
least Chipkill-Correct reliability and one of our x4 E-ECC
schemes can even correct errors due to two chip failures.
Our x8 E-ECC scheme has similar timing and power performance compared to [8] but with higher reliability and lower
storage overhead. Compared to [9] and [10], it has slightly
lower power/energy efficiency but stronger reliability since
LOT-ECC cannot handle row failures and Multi-ECC cannot handle column failures.
The rest of the paper is organized as follows. Section 2
introduces the DRAM architecture, DRAM error characteristics, existing methods and our strategy. Section 3 presents
the proposed schemes; details of the decoding algorithms
are given in the Appendix. Section 4 includes the synthesis
results of the E-ECC decoders. Section 5 discusses the

3767

Fig. 1. Logical view of a DRAM based memory system.

timing, power, energy and reliability of the proposed
schemes. Section 6 concludes this paper.

2

BACKGROUND

2.1 DRAM Memory Systems
A DRAM memory system is organized into channels, ranks,
chips and banks [16]. The DRAM memory controller (MC)
acts as an interface between the last level cache and the
DRAM. It can access data from one or more channels. Each
channel consists of dual in-line memory modules (DIMMs),
each of which consists of one or more ranks. A rank is the
minimum unit that is activated in a read or write access.
Each rank is composed of multiple chips (also called devices) and the number of chips to be activated depend on the
size of the data bus width. For DDR3, the I/O width (N) is
typically 4, 8 or 16 bits. Since the 64 bit data path is fixed, a
rank consists of 64/N chips. A DRAM system built with sixteen x4 DRAM chips is referred to as a 16 x4 system. The
common DRAM system configurations are 16 x4, 8 x8 and
4 x16 (number of chips per rank x data I/O width). In an x4
system, there are 8 extra ECC bits for every 64 bits data
resulting in two extra ECC chips per rank. In an x8 system,
one extra chip is used for ECC chip per rank. The DRAM
architecture for a x8 system is shown in Fig. 1.
In DDR3 systems, data is accessed in the burst mode; typically, burst length is eight or four (chopped burst) [17]. A
burst length of eight means that eight beats of data are
transferred per memory access [16]. Some DRAM systems
operate in the lock-step mode [8], [16]. In such a mode, two
physical channels operate as a single logical channel. A single 64B cache line is fetched using two memory channels;
one half of the cache line is accessed from the first channel
while the second half is obtained from the second channel.
2.2 DRAM Error Characteristics
DRAM errors can be broadly classified into soft errors and
hard errors. Soft errors are caused by transient faults which
occur randomly and cause incorrect data to be read from a
memory location; they disappear when the location is overwritten. Hard errors are caused by permanent faults or
intermittent faults. A permanent fault causes a memory
location to consistently return an incorrect value, such as a
stuck-at-0 fault. An intermittent fault causes a memory

3768

location to sometimes return incorrect values. Note that a
single fault can result in multiple error instances [2], [3].
DRAM errors have been analyzed in detail in [2], [3],
[18], [19], [20]. The study in [2], [18], [19] shows that a large
fraction of errors are hard errors and these manifest as
repeating errors occurring at the same address, row, column
or chip. In addition, permanent faults tend to be clustered
[18]; these errors have strong correlations in space and time.
The repeated errors contaminate the nearby rows and columns and increase dramatically in the presence of prior
errors. The study in [18], [19] also shows that the number of
errors in any memory system increase over time. A more
recent analysis of DRAM faults performed over a period of
15 months shows that while the failure rate due to transient
faults increases mildly, the failure rate due to permanent
faults is higher in the beginning and becomes almost the
same as transient faults around months six to eight [3]. It is
projected that the failure rate would again increase towards
the end of the device’s lifetime.
In general, if a chip has persistent errors, then that chip
can be marked as faulty and all data from that chip can be
treated as erasures. Note that erasures are defined as errors
whose locations are known [21]. Thus if erasures can be corrected, the faulty chip can continue to be used instead of
being retired. In this work, we utilize the error recording
mechanism of machine-check architecture [2], [14], [15] to
mark a chip as faulty. The MCA has registers to store the
error address, time or type (corrected or uncorrected) of
errors. Error events are recorded both during memory
scrubbing [13], [22] and during normal read operation.
Now if the number of errors is larger than a threshold value
(the threshold value is system-dependent), the chip is
marked as faulty by the MC.

2.3 Existing ECC Mechanisms
Chipkill-correct is the most common error protection scheme
for DRAM memory systems [4], [5], [19], [23]. It can correct
errors due to failure of one chip and also detect errors due
to two chip failures. The original Chipkill-Correct solution
from IBM [4] used single bit error correction and double bit
error detection code (SEC-DED). Current systems such as
Sun UltraSPARC-T1/T2 [24] and AMD Opteron systems
use symbol based Chipkill-Correct codes. An example of
such a code is the rotational (144,128) code [6], which is a
(36,32) code over GF(24 ). In an x4 memory system, this code
results in activation of 36 devices across 2 ranks and thus
consumes a lot of power. Next we describe several methods
that try to achieve a balance between reduction in power
consumption and Chipkill-Correct reliability.
Virtualized ECC (V-ECC) [8] provides Chipkill-Correct
capability for x4 and x8 DRAM systems. It is based on a 3
check symbol code, where 2 check symbols are used for detection (tier-1) and a third check symbol is used for correction
(tier-2). In an x8 system, V-ECC activates 18 chips in 2 ranks. It
caches the tier-2 symbols to reduce the read latency and write
frequency. However, it still incurs extra read/write operations to perform error correction or to update the ECC bits.
The storage overhead of V-ECC is 18.75 percent.
Localized and tiered ECC (LOT-ECC) [9] activates only nine
chips per rank in x8 DRAM systems to reduce power consumption. It uses multiple layers of XOR operations to deal

IEEE TRANSACTIONS ON COMPUTERS,

VOL. 65,

NO. 12,

DECEMBER 2016

with memory errors. Data along with local and global ECC
parity bits are stored in the same DRAM row to improve
access efficiency. If errors are detected, global parity bits are
read by a second access. LOT-ECC is not a Chipkill-Correct
solution since it can only correct a stuck-at-0 or stuck-at-1
chip failure. The storage overhead of LOT-ECC is 26.5 percent, which is higher than the existing schemes.
Multi-line error correction (Multi-ECC) [10] also activates
only nine chips per rank in x8 DRAM systems. Multi-ECC
uses a different approach where errors are first detected
along rows and then column checksums are used to locate
the errors. The row parity bits are then used to correct these
errors. This method requires a large number of data reads
when an error is detected. In addition, since Multi-ECC
uses one’s complement for column checksums, it cannot
fully detect errors due to column failures. The storage overhead of this method is only 12.9 percent, which is a small
increase compared to Chipkill-Correct.
Adaptive reliability chipkill correct (ARCC) [25] also provides two tiers of error protection. It reduces the power consumption by activating only one rank when there are no
errors. When errors are detected, ARCC adaptively adjusts
the ECC strength by combining adjacent codewords to perform error correction. Once two ranks are combined, the
cache line size is increased from 64 to 128B. ARCC does not
increase the ECC storage overhead; the only overhead is
that the last level cache needs to be modified to accommodate both 64 and 128B cache lines.
Bamboo-ECC [26] is a recently proposed single-tier error
protection scheme that provides good system reliability
with low storage overhead. It uses a 8-bit symbol based RS
code for x4 DRAM systems to handle error events ranging
from correcting single bit errors with 3.1 percent storage
overhead to correcting errors due to two chip failures with
25 percent storage overhead. Furthermore, by grouping
per-pin data to form ECC symbols, it is able to correct double pin failures and thus provides higher error protection
compared to Chipkill-Correct.
Apart from the academic solutions described above, there
are several commercial solutions. Many server systems use
page retiring or chip sparing to improve reliability [13], [18],
[27]. For example, Intel systems use double device data correction (DDDC) to correct double device errors sequentially.
In each rank, one DRAM device is reserved as a spare chip;
when a chip is marked faulty, the spare chip is utilized. IBM
ProteXion [14] uses redundant bit steering to re-route the
faulty bits to backup bits to deal with bit failures. Instead of
using eight bits to protect 64 bits of data it uses only six bits
and uses the remaining two bits as backup bits.
Our method does not use any spare bits/chips or re-routing, instead we handle errors due to faulty chips through
erasure correction. The decoding circuit for erasure correction is quite small and its latency is negligible compared to
the DRAM access latency (Section 4). Thus we believe erasure correction is a more efficient way to deal with errors
due to chips that have been marked faulty.

3

PROPOSED E-ECC SCHEMES

In this section we describe the proposed E-ECC schemes for
x4 DRAM systems in Section 3.1, for x8 DRAM systems in

CHEN ET AL.: USING LOW COST ERASURE AND ERROR CORRECTION SCHEMES TO IMPROVE RELIABILITY OF COMMODITY DRAM...

3769

Fig. 2. Overview of the decoding algorithm for Scheme 1.

Section 3.2 and for x16 DRAM systems in Section 3.3. For each
scheme, we describe the data access pattern and the decoding
flowchart. The decoding algorithms are described in the
Appendix and the corresponding synthesis results are
included in Section 4.

3.1 x4 DRAM Systems
3.1.1 E-ECC Scheme 1
Chipkill-Correct uses 4-check symbol codes to correct errors
due to a single chip failure and detect errors due to two
chip failures. We use rotational (144,128) code [6] as the representative Chipkill-Correct code in this paper. Here, 36
devices are activated across two ranks in each access. Each
device provides 4 bits of data per beat, that is, 36  4 = 144
bits per beat, to the ECC decoding unit. Each set of 144 bits
is decoded to obtain 128 data bits and a total of 4  128 =
512 data bits is sent to the last level cache.
The rotational (144,128) code has a minimum distance of
4 and so this code can support the following cases: (i) single
symbol correction and double symbol detection, (ii) single
erasure correction, (iii) single erasure and single error correction, (iv) double erasure correction and (v) double erasure and single error detection. Current Chipkill-Correct x4
systems implement only single symbol correction and double symbol detection (case (i)). Here, we propose an
enhancement which makes use of the same (144,128) code
to handle erasures; we refer to it as E-ECC Scheme 1.
If a chip is marked as faulty, it leads to a single erasure
and Scheme 1 performs single erasure correction (case (ii)).
When one more random error occurs in another chip,
Scheme 1 can still correct it (case (iii)). The decoder first
checks if it is a single erasure event. If so, case (ii) is executed; otherwise, case (iii) is activated. If a second chip fails,
the MC marks it as faulty. The decoder first checks if it is a
double erasure event. If so, double erasure correction (case
(iv)) is implemented; otherwise, double erasure and single
error detection (case (v)) is activated. The decoding flowchart for Scheme 1 is shown in Fig. 2 and the details of the
decoding algorithm are included in the Appendix.
3.1.2 E-ECC Scheme 2
To enhance the error correction capability of x4 DRAM systems, we investigate ECC codes operating in higher finite

Fig. 3. Scheme 2. (a) x4 DRAM access pattern of two ranks (top), (b)
Overview of the decoding algorithm (bottom).

field. We combine data from two beats to obtain 256 data bits
and 32 ECC bits. Since there is no RS (72,64) code over GF
(24 ), we move to GF(28 ). We propose using RS (36,32) in GF
(28 ), which can be derived from RS (255,251). RS (36,32) can
provide double error correction, that is, it can correct double
chip failures on the fly instead of only detecting them as in
Scheme 1. We refer to this method as E-ECC Scheme 2.
In Scheme 2, two ranks (with 18 chips per rank) are activated per access. In each read/write, two consecutive 4-bit
symbols from the same bank form a single 8-bit symbol,
and thus a total of 36 symbols are read out. Fig. 3a illustrates
the data access pattern. The proposed RS (36,32) E-ECC
code has a minimum distance of 5 and supports the following cases: (i) single error correction, (ii) double error correction, (iii) single erasure correction, (iv) single erasure and
single error correction (v) double erasure correction and (vi)
double erasure and single error correction.
The default state of Scheme 2 is single error and double error correction. Since RS code has a special algebraic
structure, the decoder can use the syndrome to distinguish between case (i) and case (ii) efficiently [28]. When
a chip fails, it leads to a single error in the E-ECC codeword and a single error can be corrected easily. When
MC marks this chip as faulty, the error becomes an erasure in an ECC codeword and the decoding circuitry for
single erasure correction (case (iii)) is activated. Correcting single erasure is simpler compared to correcting single error (see Appendix). Furthermore, if there is an
additional error in another chip, it can be corrected as
well (case (iv)). We assume that the errors build up over
time and so a second faulty chip can start generating

3770

IEEE TRANSACTIONS ON COMPUTERS,

VOL. 65,

NO. 12,

DECEMBER 2016

repeated errors. When MC marks the second chip as
faulty, the E-ECC decoder activates double erasure correction (case (v)). Once two chips are marked as faulty,
it can correct one more random error (case (vi)). The
decoding flowchart is shown in Fig. 3b and details of
the decoding algorithm for each case is given in the
Appendix.

3.1.3 E-ECC Scheme 3
Although Scheme 2 improves the reliability compared to
Chipkill-Correct and E-ECC Scheme 1, it still activates 36
devices and consumes significant amount of power. This
motivates us to find another scheme that activates fewer
chips at the cost of some loss in reliability. We investigate
codes in GF(24 ) and GF(28 ) with the constraint that 18 chips
can be activated in each access. Under this constraint, in
each beat, 72 bits (64 data bits + 8 ECC bits) are accessed
from 18 chips. Since there is no RS (18,16) code over GF(24 ),
we move to GF(28 ).
In GF(28 ), the only available code is RS (9,8), which has a
minimum distance of 2 and so cannot even correct one single error. However, if we combine data from two beats (144
bits), there are two candidates: one is the rotational
(144,128) code and the other is the RS (18,16) code over GF
(28 ). Rotational code cannot correct one chip failure (there
are 2 x4 symbol errors due to a chip failure) and hence it is
not suitable. The RS (18,16) code has minimum distance of 3
and can perform single error correction. It seems to be used
in AMD Chipkill-Correct [26]. Although this code provides
Chipkill-Correct capability, we choose a stronger code
whose error correction capability is competitive with the
other proposed schemes. Specifically, we choose RS (36,32)
code over GF(28 ); the corresponding scheme is referred to
as Scheme 3.
In Scheme 3, four consecutive 4-bit symbols from the
same bank contribute towards a codeword. Fig. 4a describes
the corresponding data access pattern. As mentioned earlier, the RS (36,32) code has a minimum distance of 5 and
supports the following cases: (i) single error correction, (ii)
double error correction, (iii) double erasure correction, (iv)
double erasure and single error correction (v) double erasure and double error detection.
The default state of Scheme 3 is also single error and double error correction. When a chip fails, it leads to two errors
in the E-ECC codeword and these two errors can be corrected
on the fly. When MC marks a chip as faulty, the decoding circuitry for double erasure correction (case (iii)) is activated.
The double erasure correction methods for Schemes 2 and 3
are different. In Scheme 2, two chip failures lead to two erasure symbols in a codeword, which may not be adjacent. In
Scheme 3, one chip failure leads to double erasures and these
two erasure symbols are adjacent in a codeword.
Correcting two erasures is simpler compared to correcting two errors (see Appendix) and these two erasure
addresses are consecutive in Scheme 3. Furthermore, if there
is an additional error in another chip, it can be corrected as
well (case (iv)). If a second chip fails, there are two erasures
(from the chip marked faulty) and two errors (from the
other faulty chip) which can be detected but not corrected
(case (v)). Note that the system cannot use case (iv) and case

Fig. 4. Scheme 3. (a) x4 DRAM access pattern of one rank (top), (b)
Overview of the decoding algorithm (bottom).

(v) at the same time and a choice has to be made. For
DRAM systems whose errors increase over time, the MC
can activate case (iv) decoder, and once the number of single errors is larger than a threshold, it can activate the circuitry for case (v). The decoding flowchart is shown in
Fig. 4b and details of the decoding algorithm for each case
is given in the Appendix.

3.2 x8 DRAM systems - E-ECC Scheme 4
In an x8 DRAM system, nine chips (8 data chips + 1 ECC chip)
are accessed every time and since fewer chips are activated
compared to x4 DRAM, the power consumption is lower.
Hence, a lot of research effort has focused on designing Chipkill-Correct x8 DRAM systems [8], [9], [10], [25], [29].
Deriving a Chipkill-Correct solution for x8 DRAM with
low overhead is still quite a challenge. If only one rank is
activated, in each beat, nine symbols are accessed from
nine chips. A possible choice is the RS (9,8) code over GF
(28 ). Unfortunately, this code can only detect a single symbol error and thus is not suitable. If two beats of data are
combined, RS (18,16) code can be used over GF(28 ) to perform single error correction. However, since one faulty
chip leads to two error symbols per codeword, this code
can not correct them. To provide Chipkill-Correct protection, either an extra ECC chip (3-check symbol code) has to
be used or the extra ECC symbols have to be stored in
another rank like [8].
Our solution is to operate the x8 DRAM system in lockstep mode as in Intel [13], HP [22] and Dell [27] systems. In
the lock-step mode, two ranks are activated and in each
beat, 144 bits (128 data bits + 16 ECC bits) are accessed from

CHEN ET AL.: USING LOW COST ERASURE AND ERROR CORRECTION SCHEMES TO IMPROVE RELIABILITY OF COMMODITY DRAM...

3771

Fig. 5. x8 DRAM access pattern for Scheme 4.

Fig. 6. x16 DRAM access pattern for Scheme 5.

18 chips across two ranks. If we use the rotational (144,128)
code, when a chip fails, the faulty chip leads to 2 x4 error
symbols in a codeword and since this code cannot correct
two random errors, it is not Chipkill-Correct. In GF(28 ), the
RS (18,16) code is a possible choice. It can correct errors due
to a chip failure. When MC marks a chip as faulty, it can correct single erasure and detect one more random error in
another chip. However, we choose a stronger code, namely,
RS (36,32), since it has better error correction capability compared to RS (18,16) and the same 12.5 percent storage overhead. We refer to this scheme as E-ECC Scheme 4 and this
scheme was also presented in [30].
In each beat, 2  9 = 18 symbols are read out from two
ranks and a total of 2  18 = 36 symbols are accessed in two
beats. Fig. 5 demonstrates the data access pattern of Scheme
4. As mentioned earlier, the RS (36,32) code supports: (i) single error correction, (ii) double error correction, (iii) double
erasure correction, (iv) double erasure and single error correction (v) double erasure and double error detection.
Scheme 4 uses the same RS (36,32) code as Scheme 3, though
their access patterns are different (Scheme 4 is designed for
x8 DRAM systems while Scheme 3 is for x4 DRAM systems). The decoding flow of Scheme 4 is the same as Scheme
3 (shown in Fig. 4b). When none of the chips are marked as
faulty, Scheme 4 performs cases (i) and (ii). When a chip is
marked as faulty, both symbols from that chip are treated as
erasures. The decoder checks whether it is a double erasure
event. If so, it launches double erasure correction (case (iii));
otherwise, it activates double erasure and single error correction (case (iv)) or double erasure and double error detection (case (v)). Note that case (iv) and case (v) cannot be
chosen at the same time.

Unfortunately, both these codes cannot correct errors due to
a faulty chip. Hence, we move to x16 DRAM systems that
operate in the lock-step mode.
When two ranks are activated, in each beat, 160 bits (128
data bits + 32 ECC bits) are accessed. If RS (20,16) code over
GF(28 ) is used, double errors due to a single faulty chip can
be corrected. Although RS (10,8) code over GF(216 ) is also a
Chipkill-Correct code, RS (20,16) over GF(28 ) provides
stronger and more flexible error correction capability with
the same 25 percent storage overhead. It can correct two
random symbol errors in two different chips and also can
correct two symbol errors in a single chip. Hence, we choose
RS (20,16) code over GF(28 ) as our E-ECC Scheme 5.
Fig. 6 illustrates the data access pattern. Here two ranks
with five chips per rank are activated each time. The 16 bits
per chip are organized into two 8-bit symbols in a codeword. The proposed RS (20,16) code also has a minimum
distance of 5 and supports the following cases: (i) single
error correction, (ii) double error correction, (iii) double erasure correction, (iv) double erasure and single error correction (v) double erasure and double error detection. Scheme
5 has the same decoding flow as Schemes 3 and 4 (also
shown in Fig. 4b), though its access pattern is quite different. As before, when none of the chips are marked as faulty,
Scheme 5 performs cases (i) and (ii). When a chip is marked
as faulty, one 16-bit symbol failure from that chip is treated
as two 8-bit erasures. The decoder checks whether it is a
double erasure event. If so, Scheme 5 launches double erasure correction (case (iii)); otherwise, it activates double erasure and single error correction (case (iv)) or double erasure
and double error detection (case (v)). Note that case (iv) and
case (v) cannot be used at the same time.

3.3 x16 DRAM Systems - E-ECC Scheme 5
x16 DRAM activates fewer number of chips per rank compared to x4 and x8 DRAM systems and thus has even lower
power. Since each rank has four chips for data, there has to
be at least one additional chip per rank to provide ChipkillCorrect reliability. Thus, such a scheme results in a storage
overhead of 25 percent. If only one rank is activated, five 16bit symbols are read out in each beat. This can be configured
into ten 8-bit symbols and the RS (10,8) code over GF(28 ) can
be used. While the RS (10,8) code can correct a single symbol
error, it cannot correct double errors due to a faulty chip.
The RS (5,4) code over GF(216 ), on the other hand, can detect
a faulty chip but cannot correct the errors due to that faulty
chip. If two beats of data are combined, the RS (20,16) code
over GF(28 ) or the RS (10,8) code over GF(216 ) can be used.

4

SYNTHESIS RESULTS

We implemented the E-ECC decoders in Verilog hardware
description language and synthesized them with a 28 nm
industrial process. Recall that Scheme 1 is based on the rotational (144,128) code, Schemes 2, 3, 4 are based on the RS
(36,32) code over GF(28 ), and Scheme 5 is based on the RS
(20,16) code over GF(28 ). The latency, power consumption
and area of each block ini the three E-ECC codes are presented in Tables 1, 2 and 3, respectively.
The two main hardware components are finite field multiplication and finite field inversion. We used a fully parallel
implementation for finite field multiplication. We implemented finite field inversion using a look-up table of size
16  4 for GF(24 ) and 256  8 for GF(28 ). The syndrome

3772

IEEE TRANSACTIONS ON COMPUTERS,

VOL. 65,

NO. 12,

DECEMBER 2016

TABLE 1
Synthesis Results Using a 28 nm Library - Latency

Syndrome Calculation
Corrects 1 error
Corrects 1 erasure
Corrects 1 erasure & 1 error
Corrects 2 errors
Corrects 2 erasures
Corrects 2 erasures & 1 error

RS (36,32) Code
over GF(28 )
0.48 ns (L)
L + 0.47 ns
L + 0.33 ns
L + 0.33 ns + 0.68 ns
L + 1.07 ns
L + 0.78 ns
L + 0.78 ns + 0.87 ns

Rotational (144,128)
Code over GF(24 )
0.41 ns (D)
D + 0.41 ns
D + 0.30 ns
D + 0.30 ns + 1.1 ns
N/A
D + 0.39 ns
N/ A

RS (20,16) Code
over GF(28 )
0.42 ns(n)
n + 0.47 ns
N/A
N/A
n + 1.07 ns
n + 0.79 ns
n + 0.79 ns + 0.86 ns

TABLE 2
Synthesis Results Using a 28 nm Library - Power
dynamic power (static power)
Syndrome Calculation
Corrects 1 error
Corrects 1 erasure
Corrects 1 erasure & 1 error
Corrects 2 errors
Corrects 2 erasures
Corrects 2 erasures & 1 error

RS (36,32) Code
over GF(28 )
14.6 mW (0.0192 mW)
22.53 mW (0.0195 mW)
25.62 mW (0.0144 mW)
21.44 mW (0.0271 mW)
16.54 mW (0.0374 mW)
19.1 mW (0.0277 mW)
19.97 mW (0.0353 mW)

Rotational (144,128)
Code over GF(24 )
14.5 mW (0.0083 mW)
12.5 mW (0.0015 mW)
12.6 mW (0.001 mW)
9.48 mW (0.0043 mW)
N/A
15.93 mW (0.0131 mW)
N/A

RS (20,16) Code
over GF(28 )
11.2 mW (0.0141 mW)
13.6 mW (0.0158 mW)
N/A
N/A
11.1 mW (0.0326 mW )
12.2 mW (0.0239 mW)
13.7 mW (0.0306 mW)

TABLE 3
Synthesis Results Using a 28 nm Library - Area

Syndrome Calculation
Corrects 1 error
Corrects 1 erasure
Corrects 1 erasure & 1 error
Corrects 2 errors
Corrects 2 erasures
Corrects 2 erasures & 1 error

Rotational (144,128)
Code over GF(24 )
2,001 um2
2,760 um2
980 um2
5,658 um2
N/A
1,394 um2
N/A

calculation unit is activated in every read operation and so it
is important that its latency be minimized. We implemented
this unit using 144 GF(24 ), 144 GF(28 ) and 80 GF(28 ) multiplications for rotational (144,128) code, RS(36,32) code
and RS (20,16) code, respectively, followed by a tree of XOR
gates.
Latency. The syndrome calculation unit of Scheme 1
(based on the (144,128) code) has a latency of 0.41 ns. If
the syndrome vector is not zero and the error event is
classified as single error, correcting the error takes an
additional 0.41 ns. This low latency comes at the cost of
additional area due to parallelization. If a chip is marked
as faulty, single erasure correction takes 0.3 ns. In addition to the erasures, if there is one more random error, it
takes another 1.1 ns. If two chips are marked as faulty,
Scheme 1 takes 0.39 ns to do double erasure correction. If
there is one more random error, Scheme 1 can detect this
error but can not correct it, and the corresponding timing
delay is also 0.39 ns.
The syndrome calculation unit of Scheme 2 (based on RS
(36,32) code) takes 0.48 ns. If the syndrome vector is not
zero, RS (36,32) can classify whether it is a single error event
or a double error event. Single error correction takes an
additional 0.47 ns, making the total latency 0.95 ns. Double

RS (36,32) Code
over GF(28 )
3,970 um2
2,024 um2
1,336 um2
3,490 um2
5,043 um2
3,311 um2
6,041 um2

RS (20,16) Code
over GF(28 )
2,533 um2
1,842 um2
N/A
N/A
4,575 um2
2,935 um2
4,883 um2

error correction takes an additional 1.07 ns, making the total
latency 1.55 ns. When a chip is marked as faulty, Scheme 2
checks whether it is a single erasure event. If so, it implements single erasure correction with an additional 0.33 ns;
otherwise, it implements single erasure and single error correction with an additional 0.33 + 0.68 ns. In Schemes 3 and 4,
when a chip is marked as faulty, E-ECC decoder checks for
double erasures. If so, it implements double erasure correction with an additional 0.78 ns; otherwise, it implements
double erasure and single error correction with an additional 0.78 + 0.87 ns.
The syndrome calculation unit in Scheme 5 takes only
0.42 ns because Scheme 5 is based on a smaller RS code.
When none of the chips are marked faulty, Scheme 5 performs single error correction with an additional 0.47 ns and
double error correction with an additional 1.07 ns. When a
chip is marked as faulty, double erasure correction takes an
additional 0.79 ns. If there is one more random error, it performs double erasure and single error correction with an
additional 0.79 + 0.86 ns.
Power. We list the dynamic power and static power consumption for each E-ECC unit in Table 2. The dynamic
power is based on input switching probability of 50 percent.
The syndrome calculation unit, which is activated in every

CHEN ET AL.: USING LOW COST ERASURE AND ERROR CORRECTION SCHEMES TO IMPROVE RELIABILITY OF COMMODITY DRAM...

TABLE 4
Simulation System Parameters
Processor

L1-I Cache
L1-D Cache
Shared L2 Cache
Main Memory

2 GHz single core (quad-core for
multiprogrammed workloads), 4-way
out-of-order, 128-entry reorder buffer
32 KB, 4-way assoc., 1 cycle latency, 64B
cache line
32 KB, 8-way assoc., 1 cycle latency, 64B
cache line
1MB (4 MB for multiprogrammed workloads), 16-way assoc., 10 cycle latency,
64B cache line
DDR3-1600 (800 MHz), FCFS, open page
policy, 13.75 ns precharge time, 13.75 ns
CAS latency, 13.75 ns RAS to CAS
latency, 8 banks per chip, 1 kB row
buffers

memory access, also has very small power consumption
compared to the off-chip DRAM power consumption.
Area. We also estimate the area of the E-ECC decoders.
Schemes 1 to 5 have an area of 12,793, 25,215, 20,389, 20,389
and 16,768 um2 respectively. Scheme 2 has the largest number of decoding units and hence it has the largest area.
Schemes 3 and 4 both have the same area since they activate
the same set of decoding units. Scheme 5 has smaller area
compared to Schemes 3 and 4 because it uses a smaller sized
RS code. Overall the area of the E-ECC decoders is quite
small. The largest E-ECC has an area of only 0.025 mm2 ,
which is fairly small compared to a typical die size ( 100
mm2 ).

5

EVALUATION

We evaluate the timing performance of the different ECC
schemes by using an open source full-system simulator,
gem5 [31]. We model a 4-way out-of-order processor with a
two-level cache hierarchy. The L1 instruction and data
caches are private to each core while the L2 caches are
shared. We also model a detailed DDR3 DRAM with data
rate 1,600 (MT/s). Table 4 describes the configuration of our
setup.
We use a detailed cycle-based DRAM simulator, DRAMSim2 [32], to evaluate DRAM power consumption. We generate DRAM access traces from the aforementioned gem5
simulation setup. We then estimate the power consumption
of the five different schemes by simulating at least 2 million
memory accesses from these traces. We use the Micron 4 Gb
data sheets [33] to obtain the input parameter values for
DRAMSim2.

5.1 Workloads
We evaluate the timing and power performance impact of
the E-ECC schemes using both sequential and multiprogrammed workloads. We use 11 DRAM-sensitive sequential
applications from the SPEC2006 benchmark suite (Table 5).
We use Simpoints [34] to identify a single, 250-million
instruction representative region for each sequential workload. In addition, we also evaluate the multi-core system
performance. We create two 4-core multiprogrammed
workload mixes of the SPEC2006 benchmarks to realistically

3773

TABLE 5
Sequential and Multiprogrammed Workloads
Benchmarks
bwaves, bzip2, cactusADM, h264ref, lbm,
libquantum, mcf, omnetpp, soplex, sphinx3, zeusmp
mix1(bzip2, mcf, omnetpp, libquantum)
mix2(libquantum, bwaves, zeusmp, cactusADM)

model the multiprogrammed application execution scenarios. The workload mixes are also summarized in Table 5.
For our multiprogrammed simulations, in order to enable
the start of different benchmarks in each workload at the
same time, we fast-forward two billion instructions from
the program start and simulate in detail until all the benchmarks have simulated for at least 250 million instructions.
We collect the statistics after the slowest benchmark has
completed simulating 250 million instructions.

5.2 Timing, Power and Energy Results
In this part, we analyze the timing, power and energy performance of the five E-ECC schemes and the three existing
schemes (V-ECC, LOT-ECC and Multi-ECC). In order to
obtain the timing performance of V-ECC, LOT-ECC and
Multi-ECC, we also synthesized their corresponding ECC
units in 28 nm technology. The delay of the syndrome calculation unit in V-ECC (using RS (19,16) code over GF(28 )) is
0.42 ns, LOT-ECC (using multi-level one’s complement
addition) is 0.52 ns, and Multi-ECC (using RS (9,8) code
over GF(216 )) is 0.24 ns. Since the latencies of syndrome calculation of all schemes are within one memory cycle
(1.25 ns), we add one additional cycle in the memory read
access time in gem5 simulation.
We present the simulation results of all schemes in an
error-free system. V-ECC activates two x8 ranks per access
and has the same configuration as E-ECC Scheme 4. LOTECC and Multi-ECC both activate one x8 rank per access
and so we have added simulation results for this configuration. V-ECC, LOT-ECC and Multi-ECC all use ECC cache to
store the tier-2 ECC symbols. We provide the timing performance for the best case scenario when the ECC cache has a
hit rate of 100 percent. All performance values are normalized to those of the x4 Chipkill-Correct baseline scheme.
Since Schemes 1 and 2 activate 36  4 chips in two ranks as
in the baseline, they have identical timing, power and
energy performance and are not shown in the figures.
5.2.1 Timing Performance Comparison
Fig. 7 shows the IPC performance of all schemes for a subset
of the sequential and multiprogrammed workloads. We
measure the performance of the multiprogrammed workloads using the weighted speedup, which is given by
P (IPCishare Þ
single
and IPCishare correspond to the
i (IPC single Þ, where IPCi
i
number of instructions executed per cycle in program i
under single-program and multi-program execution,
respectively [35], [36]. We use the geometric mean to report
the average values.
Overall, the IPC performance increases as the data width
increases from x4 to x8 to x16. The performance

3774

IEEE TRANSACTIONS ON COMPUTERS,

VOL. 65,

NO. 12,

DECEMBER 2016

Fig. 7. IPC performance of sequential and multiprogrammed applications. Scheme 1 and 2 have the same IPC performance as Chipkill-Correct
(baseline). LOT-ECC and Multi-ECC have performance comparable to ECC x8 one rank and V-ECC has performance comparable to E-ECC x8
Scheme 4.

Fig. 8. Power consumption of sequential and multiprogrammed workloads normalized to Chipkill-Correct. Schemes 1 and 2 have the same power
performance as Chipkill-Correct (baseline). LOT-ECC and Multi-ECC have performance comparable to ECC x8 one rank and V-ECC has
performance comparable to E-ECC x8 Scheme 4.

improvement can be attributed to higher rank-level parallelism. For example, Schemes 1 and 2 operate on one logical
rank whereas Schemes 3 and 4 operate on two logical ranks
and thus have better IPC performance. Fig. 7 shows that
Scheme 5, which operates on four logical ranks, has the best
IPC performance among all the schemes. The improvement
is 7 percent average, 21 percent maximum, in multiprogrammed workloads and 18 percent in sequential workloads.
Of the existing schemes, since V-ECC has the same configuration as E-ECC Scheme 4 and V-ECC uses ECC cache to
reduce the number of write operations, we project that V-ECC
has timing performance similar to E-ECC Scheme 4. LOTECC and Multi-ECC both activate one x8 rank per memory
access. While the tier-2 ECC symbols of LOT-ECC are located
in the same row as the accessed data, they are located in
another row in Multi-ECC. Since both schemes also use ECC
cache, their performance is likely to be the same. Overall, we
can expect the order of timing performance improvement
(high to low) to be Scheme 5, followed by LOT-ECC and
Multi-ECC, followed by Scheme 4 and V-ECC, followed by
Scheme 3, followed by Scheme 1 and 2 and Chipkill-Correct.

5.2.2 Power Performance Comparison
We compare the DRAM power consumption of the candidate ECC schemes; the memory configurations are provided in detail in Table 6. We generate the power
consumption for each scheme by running the simulations
in DRAMSim2.
Fig. 8 shows the power consumption of the sequential and
multiprogrammed SPEC workloads normalized to baseline.
The power consumption is due to the off-chip DRAM since
the power consumption due to ECC decoding in logic die is
negligible (20 mW compared to 3 W for DRAM). Scheme 4
obtains the best power reduction among the five schemes; it
achieves an average of 18 percent, a maximum of 23.5 percent
in multiprogrammed workloads and a maximum of 21.3 percent in sequential workloads power reduction compared to
baseline. Scheme 5 does not perform as well due to its higher
storage overhead (25 percent compared to 12.5 percent) and
its 2 kB row buffer compared to 1 kB row buffer used in x4
and x8 DRAM systems. Even though x16 DRAM activates
fewer number of DRAM chips, the larger row buffer size
adversely affects its power performance.

TABLE 6
Memory Configurations of the Five E-ECC Schemes
ECC
E-ECC x4 Scheme 1,2
E-ECC x4 Scheme 3
E-ECC x8 Scheme 4
E-ECC x16 Scheme 5
ECC x8 One rank

Channel

I/O pin

Ranks/Channel

Chips/Rank

Burst length

Total Size

1
1
1
2
2

x4
x4
x8
x16
x8

1
2
2
2
2

36
18
18
10
9

4
8
4
4
8

18 GB
18 GB
18 GB
20 GB
18 GB

Data Size
16 GB
16 GB
16 GB
16 GB
16 GB

CHEN ET AL.: USING LOW COST ERASURE AND ERROR CORRECTION SCHEMES TO IMPROVE RELIABILITY OF COMMODITY DRAM...

3775

Fig. 9. Energy efficiency of different E-ECC schemes. Schemes 1 and 2 have the same energy efficiency performance as Chipkill-Correct (baseline).
LOT-ECC and Multi-ECC have performance comparable to ECC x8 one rank and V-ECC has performance comparable to E-ECC x8 Scheme 4.

Of the existing schemes, V-ECC has power performance
comparable to our E-ECC Scheme 4. LOT-ECC and MultiECC have the lowest power consumption since they only
activate one x8 rank per memory access. Overall, we can
expect that the power efficiency (high to low) to be LOTECC and Multi-ECC, followed by Scheme 4, followed by
Scheme 3, followed by Scheme 5m followed by Scheme 1
and 2 and Chipkill-Correct.

5.2.3 Energy Performance Comparison
We next present our energy efficiency comparison for the different ECC schemes. We derive the energy number by multiplying cycle per instruction (CPI) and power for each
benchmark with 250 million instructions and normalizing it
to the energy number of the baseline. Overall, Scheme 4 outperforms all other proposed schemes in energy efficiency.
Fig. 9 shows that Scheme 3, Scheme 4 and Scheme 5 improve
the energy efficiency by a mean of 17.4, 25.4 and 22 percent,
respectively. Scheme 5 outweighs Schemes 3 or 4 only when
the system is heavily used. However, if the system is under
utilized, Scheme 5 performs worse compared to other
schemes. We conclude that even though Scheme 5 has the
best timing performance, it does not have the best power performance and thus not the best energy performance.
Of the existing schemes, V-ECC has energy efficiency
comparable to our E-ECC Scheme 4. LOT-ECC and MultiECC have the best energy efficiency; they outperform EECC Scheme 4 by around 15 percent. Although LOT-ECC
and Multi-ECC have attractive energy performance, they
are not as reliable as Chipkill-Correct schemes.

5.3 Reliability
In this section, we analyze the reliability of the competing
schemes including Chipkill-Correct, V-ECC [8], LOT-ECC
[9], Multi-ECC [10] and our E-ECC Schemes. Table 7 summarizes the error detection and correction capability of all
schemes. For each error event, we provide the rates for
detectable and correctable errors (DCE), detectable but
uncorrectable errors (DUE) and silent data corruption
(SDC). These rates are calculated by performing 10 million
Monte Carlo simulations.
First of all, all schemes can correct single bit error with
100 percent probability. If there are multiple bit errors in a
row, LOT-ECC uses 7-bit checksum to perform local error
detection and can only detect a whole row being stuck-at-0
or stuck-at-1. However, if there are multiple random bit
errors in a row, LOT-ECC cannot fully detect it. The DCE
rate of this scheme is 87.5 percent and the SDC rate is 12:5
percent as given in [9]. The rest of the schemes can correct
multiple random bit errors in a single row.
Next we consider multiple bit errors in a single column.
Multi-ECC uses one’s complement to generate the column
check sum and so if there are even number of random errors
or combinations of stuck-at-0 and stuck-at-1 failures in a single column, Multi-ECC can detect the errors using row parity bits but cannot use the column checksums to locate them
correctly. If every bit in a single column is flipped with 50
percent probability, then for 10 million runs, Multi-ECC has
DCE of 75.01 percent and DUE of 24.99 percent. In contrast,
the rest of the schemes can deal with any combination of
random errors or permanent errors within a single column.

TABLE 7
Error Detection and Correction Comparison

Single bit
failure
Row
failures
Column
failures
Single chip
failure

ChipkillCorrect

V-ECC
[8]

LOT-ECC
[9]

Multi-ECC
[10]

E-ECC
Scheme 1

E-ECC
Scheme 2

E-ECC
Scheme 3

E-ECC
Scheme 4

E-ECC
Scheme 5

DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%

DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%

DCE:100%
DUE:0%
SDC:0%
DCE:87.5%
DUE:0%
SDC:12.5%
DCE:100%
DUE:0%
SDC:0%
DCE:87.5%
DUE:0%
SDC:12.5%

DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%
DCE:75.01%
DUE:24.99%
SDC:0%
DCE:75.01%
DUE:24.99%
SDC:0%

DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%

DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%

DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%

DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%

DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%
DCE:100%
DUE:0%
SDC:0%

3776

IEEE TRANSACTIONS ON COMPUTERS,

When a chip fails, it results in multiple column failures or
row failures. LOT-ECC can detect a chip failure with DCE
less than 87.5 percent, which is the same as its row failure
event. Similarly, Multi-ECC has a chip failure probability
which is the same as its column failure probability. In contrast, all five E-ECC schemes can correct errors due to a chip
failure. When one chip fails and a single bit failure occurs in
another chip, Chipkill-Correct and V-ECC can fully detect
this event. LOT-ECC can detect this event with less than
87:5 percent probability while Multi-ECC can detect with
1  216  99:9985% probability. Scheme 1 can detect this
event with 100 percent probability and Scheme 2 can correct
with 100 percent probability. However, Schemes 3, 4 and 5
cannot handle this error event.
If the MC marks a chip as faulty and there is one more
random error in another chip, all 5 E-ECC Schemes can correct it. V-ECC can also be designed to correct one erasure
and one random error. LOT-ECC, which uses XOR operation to recover from a faulty chip, cannot handle this error
event. Multi-ECC can correct the extra error if a spare chip
is used to replace the faulty chip. If the MC marks a chip as
faulty and there is one more chip failure, Schemes 1 and 2
can fully correct the errors.
Of all the schemes, E-ECC Scheme 2 has the highest error
protection capability since it can correct double chip failures
on the fly. In addition, when two chips are marked as faulty,
it can correct errors due to a third chip failure.

5.4 Overhead Comparison
All proposed E-ECC Schemes (except for Scheme 5) have
12.5 percent storage overhead while V-ECC, LOT-ECC and
Multi-ECC have storage overheads more than 12.5 percent
(18.5, 26.5 and 12.9 percent). Although Scheme 5 has 25 percent storage overhead, it only utilizes one additional chip
per four chips in x16 DRAM. Since direct implementations
of V-ECC, LOT-ECC and Multi-ECC incur extra reads or
writes to access or update tier-2 ECC symbols, all three
schemes use ECC cache to store the tier-2 ECC symbols and
avoid performance degradation. To support use of ECC
cache, OS needs to be modified and extra hardware units
(ECC address mapping) have to be added as indicated in
[8], [9], [10].
The E-ECC schemes rely on MCA to record the corrected
errors in each chip. MCA is already used in several servers
[2], [14], [15] and does not add to the overhead. When errors
are caused by permanent faults, V-ECC, LOT-ECC and
Multi-ECC also rely on MCA logs to decide when to use
spare rows, page retirement and chip replacement. However, use of these methods results in significant overhead to
reroute, remap or retire the data in DRAM. Overall, the proposed E-ECC schemes use more logic die area but have the
lowest storage and infrastructure overhead compared to the
existing schemes. A comparison of the overhead of the competing schemes is given in Table 8.

6

CONCLUSION

In this paper, we present five erasure and error correction
(E-ECC) schemes that provide superior error protection for
x4, x8 and x16 DRAM systems. All our schemes use strong
symbol based codes and provide higher error protection

VOL. 65,

NO. 12,

DECEMBER 2016

TABLE 8
Overhead Comparison
Infrastructure Overhead
E-ECC

V-ECC

LOT-ECC

Multi-ECC

 12.5% storage overhead (25% for Scheme 5)
 ECC logic die size from 12,793 to 25,215 um2
 Utilizes MCA to mark chip as faulty
 18.5% storage overhead
 ECC logic die size of 4,375 um2
 ECC cache to store tier-2 ECC symbols
 26.5% storage overhead
 ECC logic die size of 4,107 um2 (only tier-1)
 ECC cache to store tier-2 ECC symbols
 Relies on MCA to record errors and implement
spare rows, page retirement and chip replacement
 12.9% storage overhead
 ECC logic die size of 762 um2 (only tier-1)
 ECC cache to store tier-2 ECC symbols
 Relies on MCA to record errors and implement
spare rows, page retirement and chip replacement

compared to existing systems. Furthermore, when a chip is
marked faulty, our schemes make use of erasure correction
to increase the lifetime of the memory system with no additional cost. Synthesis results show that the decoding latency
of these codes is very small and the additional latency does
not affect the timing performance of the memory system.
Also, our schemes require no extra memory accesses to perform error correction and more importantly, do not require
any change in the memory architecture.
All the proposed schemes can correct errors due to a chip
failure, and when the chip is marked faulty, they can correct
one more random error. Of all these schemes, Scheme 2 that
is designed for x4 systems and uses RS(36,32) code, has the
highest reliability. Simulations on SPEC 2006 benchmarks
show that Schemes 3, 4 and 5 have better timing, power and
energy performance compared to x4 Chipkill-Correct solutions. Of these schemes, Scheme 4 that is designed for x8
systems and uses the RS(36,32) code, has the lowest power
consumption and highest energy performance while
Scheme 5 that is designed for x16 systems and uses the RS
(20,16) code, has the best timing performance. Overall, our
proposed schemes provide a low cost solution to increasing
the lifetime of commodity DRAM memory systems with
lower power and performance overhead.

APPENDIX
Decoding algorithm of the E-ECC schemes based on the
(144,128) rotational code: This is a (36,32) code over GF(24 )
that has a minimum distance of 4 and supports the following cases: (i) single error correction and double error detection, (ii) single erasure correction, (iii) single erasure and
single error correction, (iv) double erasure correction and
(v) double erasure and single error detection.
The first step is syndrome calculation, where the syndrome vector S ¼ ðs0 ; s1 ; s2 ; s3 ÞT is calculated by multiplying
the parity check matrix H with the codeword. The parity
check matrix of the rotational (144,128) code is of size 4  36,
where each entry is a 4 bit symbol, and is given in [6]. Case (i)
is the traditional single symbol correction and double symbol
detection. The decoder compares the syndrome vector with
all columns of the parity check matrix to determine the error

CHEN ET AL.: USING LOW COST ERASURE AND ERROR CORRECTION SCHEMES TO IMPROVE RELIABILITY OF COMMODITY DRAM...

location and the corresponding error value. Let the jth column of the parity check matrix, H, be hj ¼ ðhj0 ; hj1 ;
hj2 ; hj3 ÞT , j ¼ 0; 1; . . . ; 35. If S ¼ ej hj holds, then the error
occurred in position j and has a value ej . If there is no j
for which S ¼ ej hj , then the decoder declares it as a double
error event.
Cases (ii) to (v) involve correction of erasure symbols.
Note that since the location of the faulty chip is known, the
corresponding symbols are marked as erasure symbols.
Each erasure symbol is replaced with the zero symbol in the
received codeword and then the syndrome vector is generated. For case (ii), the syndrome vector is compared with
the column in the parity check matrix corresponding to the
erasure address. If the erasure address is i, the decoder
checks if S ¼ ei hi for column i in H. If it holds, the decoder
recovers the erasure value ei in address i of codeword. Otherwise, the single erasure and single error correction (case
(iii)) unit is activated.
For case (iii), assume that the erasure address is i and the
erasure value is ei ; similarly, the error address is j and error
value is ej , where i 6¼ j. The decoder needs to check whether
the syndrome vector, S, is a linear combination of hi and hj ,
where j is from 0 to 35 and i 6¼ j. The hardware consists of
36 sub-decoders, where the jth decoder has hj embedded in
it. The MC feeds column hi to all but the ith sub-decoder. If
the ith and jth columns of the parity check matrix are hi ¼
ðhi0 ; hi1 ; hi2 ; hi3 ÞT and hj ¼ ðhj0 ; hj1 ; hj2 ; hj3 ÞT , then s0 ¼ ei 
hi0 þ ej  hj0 , s1 ¼ ei  hi1 þ ej  hj1 equations are used to
obtain ei and ej . The decoded ei and ej values are substituted back to calculate s~2 ¼ ei  hi2 þ ej  hj2 and s~3 ¼ ei 
hi3 þ ej  hj3 . If s~2 ¼ s2 and s~3 ¼ s3 both hold, the decoder
declares that the error is located in location j of the received
codeword and corrects it.
For case (iv), the MC sends two erasure addresses
(assume i and j) to the decoder and the decoder extracts the
two columns, hi and hj , corresponding to the two erasure
addresses. It uses these two columns to check whether the
syndrome is a linear combination of hi and hj . If the condition holds, the decoder recovers the two erased symbols.
Otherwise, it declares that there are two erasures and one
error, which is case (v).
Decoding algorithm of the E-ECC schemes based on the
RS (36,32) over GF(28 ): This code has minimum distance of
5 and supports the following cases: (i) single error correction, (ii) double error correction, (iii) single erasure correction, (iv) single erasure and single error correction, (v)
double erasure correction, (vi) double erasure and single
error correction. The parity check matrix, H, of this code is
given as follows
0

1 1
B1 a
H¼B
@ 1 a2
1 a3

1
a2
a4
a6

::: 1
::: ai
::: a2i
::: a3i

:::
:::
:::
:::

1
1
a35 C
C,
a70 A
a105

where a is a primitive element of GF(28 ). For case (i) and
case (ii), the decoder implements single error and double
error correction. The decoder can classify the two cases by
using the syndrome vector [28]. Let the syndrome vector be
S ¼ ðs0 ; s1 ; s2 ; s3 ÞT . The condition ss32 ¼ ss21 ¼ ss10 corresponds to

3777

a single error event; otherwise, it is a double error event. We
implement the double error correction based on the method
used in [37]. To solve the error locator polynomial, we use a
deterministic way to solve the roots rather than using the
Chien search method. The error locator polynomial can be
simplified to y2 þ y þ K. To solve y, a deterministic way by
using linearlized polynomials is shown in [37].
Case (iii) is trivial and easier than case (i). Suppose hi is
the ith column of H and the erasure position is at i. The
decoder compares S ¼ ei  hi or not. If it holds, the decoder
declares it is single erasure event. If it does not hold, it activates case (iv). Suppose the erasure address is i (known)
and the error address is j (unknown). The decoder checks
whether the syndrome vector, S, is a linear combination of
hi and hj . Let S ¼ ei  hi þ ej  hj , where hi ¼ ð1; ai ; a2i ; a3i ÞT
i

i

þs2
and hj ¼ ð1; aj ; a2j ; a3j ÞT , then aj ¼ ss1 a
, ej ¼ s0aai þaþsj 1 and
ai þs
0

1

ei ¼ s0 þ ej . The decoder first finds the error address j, then
derives ei and ej .
In E-ECC Scheme 2, if two chips are marked faulty, then
there are two erasures (case (v)). Assume the positions of two
erasure symbols are i and j, where i and j are from 0 to 35 and
i 6¼ j. The relation between syndrome vector and the columns
corresponding to the erasure positions in H is as follows:
0

1
0
1
0
1
s0
1
1
B s1 C
B iC
B jC
B C ¼ ei B a2i C þ ej B a2j C,
@ s2 A
@a A
@a A
s3
a3i
a3j
where ei and ej are the erasure values for positions ith and
i

þs1
jth. The value ej is calculated by ej ¼ s0aai þa
and ei is
j
obtained by ei ¼ s0 þ ej . The decoded ei and ej values are
used to calculate s~2 and s~3 , namely, s~2 ¼ ei  a2i þ ej  a2j
and s~3 ¼ ei  a3i þ ej  a3j . If s~2 ¼ s2 and s~3 ¼ s3 hold, the
decoder declares a double erasure event and corrects these
two erased symbols (case (v)). Otherwise, the decoder activates the double erasure and single error correction unit
corresponding to case (vi).
In case (vi), let the address of the error symbol be k, the
error value be ek and the addresses of the erasure symbols
are i and j, where i 6¼ j, j 6¼ k and i 6¼ k. The relation
between syndrome vector and double erasure and one error
is given as follows:

0

1
0
1
0
1
0
1
1
1
1
s0
B s1 C
B iC
B jC
B kC
B C ¼ ei B a2i C þ ej B a2j C þ ek B a2k C
@ s2 A
@a A
@a A
@a A
3i
3j
s3
a
a
a3k
iþj

j

i

2 a þs2 a þs3
.
The error address ak is derived by ak ¼ ss1 aaiþj þs
þs aj þs ai þs

The error value ek is obtained by

0
1
s0 aiþj þs1 aj þs1 ai þs2
ðak þaj Þðai þaj Þ
j

j

k

1

2

The era-

þek a þs1
sure value ej is obtained by ej ¼ s0 a þekaai þa
. Finally, ei
j
is obtained by ei ¼ s0 þ ej þ ek .
In E-ECC Schemes 3 and 4, if a single chip is marked
faulty, there are two erasures but those erasures are in consecutive locations. So if the position of the first erasure is 2i
the position of the second erasure is 2i þ 1, where
i ¼ 0; 1; . . . ; 17. The procedure for case (v) and case (vi) in

3778

IEEE TRANSACTIONS ON COMPUTERS,

E-ECC Schemes 3 and 4 are very similar to these in E-ECC
Scheme 2 and are not described here.
The RS (20,16) code over GF(28 ) has smaller parity check
matrix but it has the same decoding algorithm as the RS
(36,32) code. Hence, we skip the description of its decoding
units of RS (20,16) here.

ACKNOWLEDGMENTS
This work was supported in part by the DARPA-PERFECT
program, NSF CNS 18183, ARM and DOE.

REFERENCES
[1]

[2]
[3]

[4]
[5]
[6]
[7]
[8]
[9]

[10]

[11]

[12]

[13]

[14]
[15]
[16]
[17]
[18]

[19]
[20]

S. Li, K. Chen, M.-Y. Hsieh, N. Muralimanohar, C. Kersey,
J. Brockman, A. Rodrigues, and N. Jouppi, “System implications
of memory reliability in exascale computing,” in Proc. Int. Conf.
High Perform. Comput., Netw., Storage Anal., 2011, pp. 1–11.
V. Sridharan and D. Liberty, “A field study of DRAM errors,” in
Proc. Int. Conf. High Perform. Comput., Netw., Storage Anal., 2012,
pp. 1–11.
V. Sridharan, J. Stearley, N. DeBardeleben, S. Blanchard, and
S. Gurumurthi, “Feng Shui of supercomputer memory: Positional
effects in DRAM and SRAM faults,” in Proc. Int. Conf. High Perform. Comput., Netw., Storage Anal., 2013, pp. 22:1–22:11.
T. J. Dell, “A white paper on the benefits of Chipkill-Corret ECC
for PC server main memory,” in Proc. IBM Microelectron., 1997.
D. Locklear, “Chipkill correct memory architecture,” Dell Enterprise System Group, Aug. 2000.
T. Rao and E. Fujiwara, Error-Control Coding for Computer Systems.
Upper Saddle River, NJ, USA: Prentice, 1989.
C. L. Chen, “Symbol error correcting codes for memory
applications,” in Proc. Annu. Symp. Fault Tolerant Comput., 1996,
pp. 200–207.
D. Yoon and M. Erez, “Virtualized ECC: Flexible reliability in
main memory,” IEEE Micro, vol. 31, no. 1, pp. 11–19, Jan. 2011.
A. N. Udipi, N. Muralimanohar, R. Balsubramonian, A. Davis,
and N. P. Jouppi, “LOT-ECC: Localized and tiered reliability
mechanisms for commodity memory systems,” in Proc. Int. Symp.
Comput. Archit., Jun. 2012, pp. 285–296.
X. Jian, H. Duwe, J. Sartori, V. Sridharan, and R. Kumar, “Lowpower, low-storage-overhead Chipkill correct via multi-line error
correction,” in Proc. Int. Conf. High Perform. Comput., Netw., Storage
Anal., 2013, pp. 24:1–24:12.
X. Jian, J. Sartori, H. Duwe, and R. Kumar, “High performance,
energy efficient Chipkill correct memory with multidimensional
parity,” Trans. Comput. Archit. Lett., vol. 12, no. 2, pp. 39–42,
Jul. 2013.
X. Jian and R. Kumar, “ECC Parity: A technique for efficient memory error resilience for multi-channel memory systems,” in Proc.
Int. Conf. High Perform. Comput., Netw., Storage Anal., Nov. 2014,
pp. 1035–1046.
“Intel Xeon Processor E7 Family: Reliability, Availability and Serviceability: Advanced data integrity and resiliency support for
mission-critical deployment,” 2011, http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/xeone7-family-ras-server-paper.pdf
“Reliability, Availability, and Serviceability. Features of the IBM
eX5 Portfolio,” 2012.
“AMD64 architecture programmer’s manual revision 3.17,” 2011.
B. Jacob, S. W. Ng, and D. T. Wang, Memory Systems Cache, DRAM,
Disk. Morgan Kaufmann Pub, 2007.
“DDR3 SDRAM Standard, JESD79-3F,” 2012.
A. Hwang, I. Stefanovici, and B. Schroeder, “Cosmic rays don’t
strike twice: Understanding the nature of DRAM errors and the
implications for system design,” SIGARCH Comput. Archit. News,
vol. 40, pp. 111–122, Mar. 2012.
B. Schroeder, E. Pinheiro, and W. Weber, “DRAM errors in the
wild: A large-scale field study,” in Proc. 11th Int. Joint Conf. Meas.
Modeling Comput. Syst., 2009, pp. 193–204.
N. DeBardeleben, S. Blanchard, V. Sridharan, S. Gurumurthi,
J. Stearley, K. Ferreira, and J. Shalf, “Extra Bits on SRAM and
DRAM Errors - More data from the field,” in Proc. Silicon Errors in
Logic - System Effects (SELSE-10), Apr. 2014.

VOL. 65,

NO. 12,

DECEMBER 2016

[21] S. Lin and D. J. Costello, Error Control Coding. London, U. K.: Pearson, 2004.
[22] “HP: How memory RAS technologies can enhance the uptime of
HP Proliant servers,” 2013.
[23] T. J. Dell, “System RAS implications of DRAM soft errors,” IBM J.
Res. Develop., vol. 52, no. 3, pp. 307–314, 2008.
[24] Sun Microsystems, Inc., “T2 core microarchitecture specification,”
2007.
[25] X. Jian and R. Kumar, “Adaptive Reliability Chipkill Correct
(ARCC),” in Proc. IEEE Int. Symp. High Perform. Comput. Archit.,
Feb. 2013, pp. 270–281.
[26] J. Kim, M. Sullivan, and M. Erez, “Bamboo ECC: Strong, safe, and
flexible codes for reliable computer memory,” in Proc. IEEE Int.
Symp. High Perform. Comput. Archit., 2015, pp. 101-112.
[27] “Memory for Dell Poweredge 12th Generation Servers,” 2012.
[28] R.-H. Deng and D. J. Costello, “Decoding of DBEC-TBED
Reed-Solomon codes,” IEEE Trans. Comput., vol. C-36, no. 11,
pp. 1359–1363, Nov. 1987.
[29] A. N. Udipi, N. Muralimanohar, N. Chatterjee, R. Balsubramonian,
A. Davis, and N. P. Jouppi, “Rethinking DRAM design and organization for energy-constrained multi-cores,” in Proc. Annu. Int.
Symp. Comput. Archit., Jun. 2010, pp. 175–186.
[30] H.-M. Chen, A. Arunkumar, C.-J. Wu, T. Mudge, and
C. Chakrabarti, “E-ECC: Low power erasure and error correction schemes for increasing reliability of commodity DRAM
systems,” in Proc. Int. Symp. Memory Syst., 2015, pp. 60–70.
[31] N. Binkert, B. Beckmann, G. Black, S. Reinhardt, A. Saidi, A. Basu,
J. Hestness, D. Hower, T. Krishna, S. Sardashti, R. Sen, K. Sewell,
M. Shoaib, N. Vaish, M. Hill, and D. Wood, “The Gem5 Simulator,” SIGARCH Comput. Archit. News, vol. 39, no. 2, pp. 1–7,
Aug. 2011.
[32] P. Rosenfeld, E. Cooper-Balis, and B. Jacob, “DRAMSim2: A cycle
accurate memory system simulator,” IEEE Comput. Archit. Lett.,
vol. 10, no. 1, pp. 16–19, Jan.–Jun. 2011.
[33] Micron. (2011). DDR3 SDRAM System-Power Calculator. [Online].
Available:
http://www.micron.com/-/media/documents/
products/power/%20calculator/ddr3_power_calc.xlsm
[34] E. Perelman, G. Hamerly, M. Biesbrouck, T. Sherwood, and
B. Calder, “Using Si’mPoint for accurate and efficient simulation,”
in Proc. ACM Int. Conf. Meas. Model. Comput. Syst., 2003, pp. 318–319.
[35] A. Snavely and M. Tullsen, “Symbiotic jobscheduling for a simultaneous multithreaded processor,” in Proc. Int. Conf. Archit. Support Program. Lang. Operating Syst., Dec. 2000, pp. 234–244.
[36] S. Eyerman and L. Eeckhout, “System-level performance metrics
for multiprogram workloads,” IEEE Micro, vol. 28, no. 3, pp. 42–
53, May/Jun. 2008.
[37] R. Berlekamp, Algebraic Coding Theory. New York, NY, USA:
McGraw-Hill, 1968.
Hsing-Min Chen received the BS and MS
degrees in computer science from National Chiao
Tung University, Taiwan. Currently, he is working
toward the PhD degree in electrical engineering
at Arizona State University. He is supervised by
Dr. Chakrabarti. His research interests include—
error correction codes, reliable memory design,
memory system performance evaluation, and
front-end RTL design.

Supreet Jeloka received the BTech degree from
NIT, Warangal, India and the MS degree in electrical engineering in 2013, from the University of
Michigan, Ann Arbor, where he is currently working toward the PhD degree. His current research
interests include—low power circuits, memory
design, memory-based computing, interconnect
fabrics, and hardware security.

CHEN ET AL.: USING LOW COST ERASURE AND ERROR CORRECTION SCHEMES TO IMPROVE RELIABILITY OF COMMODITY DRAM...

3779

Akhil Arunkumar received the MS degree in
electrical engineering from the University of North
Carolina at Charlotte in 2012. He is currently
working toward the PhD degree in computer science at Arizona State University. His main
research interests include memory hierarchy
design and computer architecture. He is a student
member of the IEEE and ACM.

Trevor Mudge received the PhD degree in computer science from the University of Illinois, Urbana.
He is now the Bredt family professor of computer
science for pioneering contributions to low-power
computer architecture and received the University
of Illinois Distinguished Alumni Award. He is a life
fellow of the IEEE, a member of the ACM, the IET,
and the British Computer Society.

David Blaauw received the BS degree in physics
and computer science from Duke University in
1986, and the PhD degree in computer science
from the University of Illinois, Urbana, in 1991. After
his studies, he was with Motorola, Inc. in Austin,
TX, where he was the manager of the High-Performance Design Technology group. Since August
2001, he has been a professor at the University of
Michigan. He has published more than 450 papers
and holds 40 patents. His work has focussed on
VLSI design with particular emphasis on ultra-low
power and high-performance design. He was the technical program chair
and general chair for the International Symposium on Low Power Electronic and Design. He was also the technical program cochair of the ACM/
IEEE Design Automation Conference and a member of the ISSCC Technical Program Committee. He is a fellow of the IEEE.

Chaitali Chakrabarti received the BTech degree
in electronics and electrical communication engineering from the Indian Institute of Technology,
Kharagpur, India, in 1984, and the PhD degree in
electrical engineering from the University of Maryland, College Park, in 1990. He is a professor with
the School of Electrical, Computer and Energy
Engineering, Arizona State University, Tempe. Her
research interests include VLSI algorithm-architecture co-design of signal processing and communication systems and all aspects of low-power
embedded systems design. He is a fellow of the IEEE.

Carole-Jean Wu received the BS degree in electrical and computer engineering from Cornell University in 2006, and the MA and PhD degrees in
electrical engineering from Princeton University, in
2008 and 2012, respectively. She is currently an
assistant professor with the School of Computing,
Informatics, and Decision Systems Engineering,
Arizona State University. Her research interests are
in the areas of processor architectures and memory hierarchy designs to achieve high performance
and Improved energy efficiency.

" For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

CAWS: Criticality-Aware Warp Scheduling for GPGPU Workloads
Shin-Ying Lee and Carole-Jean Wu
Computer Science and Engineering
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University
Tempe, AZ 85281
{lee.shin-ying,carole-jean.wu}@asu.edu

ABSTRACT

Keywords

The ability to perform fast context-switching and massive
multi-threading is the forte of modern GPU architectures,
which have emerged as an efficient alternative to traditional
chip-multiprocessors for parallel workloads. One of the main
benefits of such architecture is its latency-hiding capability.
However, the efficacy of GPU’s latency-hiding varies significantly across GPGPU applications.
To investigate this, this paper first proposes a new algorithm that profiles execution behavior of GPGPU applications. We characterize latencies caused by various pipeline
hazards, memory accesses, synchronization primitives, and
the warp scheduler. Our results show that the current roundrobin warp scheduler works well in overlapping various latency stalls with the execution of other available warps for
only a few GPGPU applications. For other applications,
there is an excessive latency stall that cannot be hidden by
the scheduler effectively. With the latency characterization
insight, we observe a significant execution time disparity
for warps within the same thread block, which causes suboptimal performance, called the warp criticality problem.
To tackle the warp criticality problem, we design a family of criticality-aware warp scheduling (CAWS) policies by
scheduling the critical warp(s) more frequently than other
warps. Our results on the breadth-first-search, B+tree search,
two point angular correlation function, and K-means clustering show that, with oracle knowledge of warp criticality,
our best-performing scheduling policy can improve GPGPU
applications’ performance by 17% on average. With our
designed criticality predictor, the various scheduling policies can improve performance by 10-21% on breadth-firstsearch. To our knowledge, this is the first paper to characterize warp criticality and explore different criticality-aware
warp scheduling policies for GPGPU workloads.

GPGPU; warp/wavefront scheduling; GPU performance characterization

1.

INTRODUCTION

The ability to perform fast context-switching and massive multi-threading has been the forte of modern Graphics
Processing Unit (GPU) architectures, which have emerged
as an efficient alternative to traditional chip multiprocessors
(CMPs) for parallel workloads. Therefore it is becoming
more and more common today to execute general-purpose
GPU (GPGPU) workloads on the highly-parallel architecture. GPUs are based on the Single Instruction Multiple
Thread (SIMT) computing paradigm where multiple threads
are grouped together to form a warp or wavefront. Threads
in a warp are mapped to a Single Instruction Multiple Data
(SIMD) execution unit such that all threads execute the
same instruction, but with different data. For example,
NVIDIA Kepler GPU supports up to 64 concurrent warps
within a streaming multiprocessor (SM).
The benefit of fast context-switching and the large number
of warps is latency-hiding: whenever the execution of a warp
is stalled, it can be swapped out and another warp can be
swapped in for immediate execution to maximize resource
utilization without any penalty. These stalls could be caused
by pipeline hazards, such as control or structural hazards,
or memory accesses.
To investigate GPU’s latency hiding ability, we develop an
algorithm that attributes latencies caused by data dependency, control and structural hazards, as well as latencies
from instruction and data cache accesses. Our results show
that the current round-robin warp scheduler in the baseline
GPU architecture works well in overlapping various types
of latency stalls experienced by a warp with executions of
other available warps in the warp pool for some GPGPU
applications. In these applications, all warps are created
equal, with homogeneous distribution of latency breakdown
and execution time.
However, for certain applications, e.g., breadth-first-search
(bfs), not all warps are created equal. We observe a significant execution time disparity for warps belonging to the
same thread group (thread block ). Since a thread block
cannot be scheduled to execute until all warps in the previous thread block have finished executing, this results in
sub-optimal computing resource utilization, as the warps
which finished executing earlier have to wait for the longestrunning warp to finish before another thread block can be
swapped in for execution. We call this the warp criticality

Categories and Subject Descriptors
C.1 [Processor Architectures]: Parallel Architectures;
C.4 [Performance of Systems]: Design Studies
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
PACT’14, August 24–27, 2014, Edmonton, AB, Canada.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2809-8/14/08 ...$15.00.
http://dx.doi.org/10.1145/2628071.2628107.

175

problem, which is similar to its multi-threaded CPU application counterpart [2] and is due to the internal synchronization of the application. For some thread blocks in bfs, we
observe warp execution time disparity to be over 50% compared to the ideal scenario, where all warps in the thread
block finish execution at the same time. This indicates that
there is ample room for execution time improvement if we
can reduce the warp execution time disparity.
The critical warp behavior is exacerbated by additional
scheduling delay that is imposed upon the critical warp (the
slowest-running warp) and, as a result, overall application
performance is degraded.
To address the warp criticality problem, we delve deeper
into bfs’s application code to identify the source of warp
criticality and explore a set of warp scheduling algorithms to
improve the execution time of the longer running warps in a
thread block. Many prior works have looked at various warp
scheduling algorithms to improve GPU performance, e.g.,
prefetch-aware scheduling [12], memory-aware scheduling [9,
10, 12, 11, 16, 18]. However, to the best of our knowledge,
this work is the first to characterize warp criticality and
explore different criticality-aware warp scheduling (CAWS)
algorithms for modern GPU architectures.
The concept behind CAWS is simple: by scheduling the
critical warps more frequently than other warps, we can
effectively reduce the execution time of the critical warp,
and hence reduce the total execution time of an application,
since the next thread block can now be scheduled to execute
and finish earlier. The ultimate goal of the criticality-aware
scheduling policies is to equalize the execution time of all
warps.
With the oracle knowledge of criticality for all warps in a
thread block, our CAWS policies improve the performance
of GPGPU workloads by 17% on average. In addition to
the oracle knowledge, with application program hints, our
average-based CAWS policy achieves performance improvement on bfs of 21%. Overall, our CAWS policies can improve
performance by 10-21% with the warp criticality prediction.
Contributions: The key contributions of this paper are:
1. We characterize the stall cycles for GPU warps due
to pipeline hazards and memory subsystem latencies,
which gives insights into the latency-hiding ability of
the massively parallel GPU architecture.
2. We identify and characterize the critical warp behavior of GPGPU applications on a modern GPU architecture, which has not been addressed in prior work.
3. We show that the critical warp behavior can be alleviated by scheduling warps according to their criticalities.
4. We design, implement, and evaluate a family of criticalityaware warp scheduling algorithms to address the critical warp issue. These new scheduling algorithms increase computing resource utilization and improve execution performance for GPGPU applications by 17%.
The rest of the paper is organized as follows. Section 2
describes our simulation infrastructure and the methodology we use to evaluate warp-criticality. Section 3 characterizes the latency-hiding ability of the modern GPU architectures and outlines the execution latency breakdown. We
present and formalize the warp-criticality problem of bfs in
Section 4 and explore a range of criticality-aware scheduling algorithms in Section 5. Section 6 presents our result
analysis for the scheduling policies explored in this paper.

176

Architecture
Num. of SMs
Max. # of Warps per SM
Max. # of Blocks per SM
# of Schedulers per SM
# of Registers per SM
Shared Memory
L1 Data Cache
L1 Inst Cache
L2 Cache
Min. L2 Access Latency
Min. DRAM access Latency
Warp Size (SIMD Width)

NVIDIA Fermi GTX480
15
48
8
2
32768
48KB
16KB per SM (32-sets/4ways)
2KB per SM (4-sets/4-ways)
768KB (64-sets/16-ways/6banks)
120 cycles
220 cycles
32 threads

Table 1: GPGPU-sim configurations.
Section 7 presents related work in this area. Section 8 discusses various challenges we faced in designing the CAWS
algorithms and known-issues in other GPGPU applications,
and Section 9 concludes the paper.

2.
2.1

METHODOLOGY
Simulation Infrastructure

In order to investigate the warp criticality problem, we
use GPGPU-sim (version 3.2.0) [1] to profile the behavior of
GPGPU applications. GPGPU-sim is a cycle-level performance simulator that models a general-purpose GPU architecture supporting NVIDIA CUDA and its PTX ISA. We
run GPGPU-sim with the default configuration representing NVIDIA Fermi GTX480 architecture. Figure 1 shows
the relevant microarchitecture components modeled in our
simulation environment and Table 1 describes the simulation configuration and parameters. In addition to the baseline round-robin warp scheduler modeled in GPGPU-sim,
we implement a range of criticality-aware warp schedulers
which can be employed to reduce the execution time of the
critical warps. Details of the criticality-aware warp schedulers are described in Section 5.1.

2.2

Workload

We choose 18 GPGPU applications from the Rodinia [4]
and Parboil [19] benchmark suites to characterize the latencyhiding feature in modern GPUs. Table 2 lists the details
of these 18 benchmarks along with the description, the input data set used for our characterization study, and the
computation/memory-intensive characters.

3.
3.1

CHARACTERIZING THE LATENCY HIDING
ABILITY OF GPU WARP SCHEDULER
Background

At every cycle, GPU’s warp scheduler selects an available
warp from the warp pool for execution (Figure 1). If there is
a ready instruction in the selected warp’s instruction buffer
without any pipeline hazards, this warp instruction is ready
for execution; otherwise it is stalled due to one of the following factors: instruction fetch scheduling policy, instruction
cache miss, data cache miss, data hazard, control hazard,
structural hazard, or delay caused by the warp scheduling

Warp	
  
Scheduler	
  
Warp	
  Pool	
  

Fetch	
  Unit	
  

Inst.	
  
Decoder	
  

L1	
  Data	
  Cache	
  

Scoreboard	
  

Warp	
  
0	
  
	
  
Warp	
  1	
  
Warp	
  2	
  
.	
  .	
  
Warp	
  (N-­‐1)	
  

.	
  
.	
  .	
  

...

SIMD	
  Lanes	
  (adder)	
  
.	
  .	
  .	
  
SIMD	
  Lanes	
  (mult.)	
  
.	
  .	
  .	
  
SIMD	
  Lanes	
  (divider)	
  
.	
  .	
  .	
  

Inst.	
  	
   Reg.	
  	
  
Buﬀers	
   Files	
  

CDB	
  

L1	
  Inst.	
  Cache	
  

Streaming	
  Mul.processor	
  (SM)	
  
Streaming	
  Mul.processor	
  (SM)	
  
Streaming	
  Mul.processor	
  (SM)	
  

To	
  L2	
  Cache	
  

..

Memory	
  Ports	
  

Figure 1: GPU pipeline modeled.
Benchmark

Description

Data Set

Category [5, 11]

b+tree
backprop
bfs
gaussian
heartwall
hotspot
kmeans
lavaMD
leukocyte
lud
myocyte
needle
particle
pathfinder
srad 1
srad 2
streamcluster(small)
streamcluster(mid)
tpacf

graph algorithm
pattern recognition
graph algorithm
matrix operations
medical image processing
physics simulation
data mining algorithm
physics simulation
medical image processing
matrix operations
biological simulation
data mining algorithm (bioinformatics)
medical image processing
graph algorithm
image processing
image processing
data mining algorithm
data mining algorithm
geometry and physics computation

1 million nodes
65536 nodes
65536 nodes
1024x1024 matrix
656x744 grey scale AVI
512x512 nodes
494020 nodes
10 nodes
640x480 grey scale AVI
2048x2048 matrix
100 nodes
1024x1024 nodes
128x128x10 nodes
100000 nodes
502x458 nodes
2048x2048 nodes
32x4096 nodes
64x8192 nodes
487x100 nodes

memory intensive
memory intensive
memory intensive
computation intensive
computation intensive
computation intensive
memory intensive
computation intensive
computation intensive
computation intensive
computation intensive
memory intensive
computation intensive
computation intensive
computation intensive
memory intensive
memory intensive
memory intensive
memory intensive

Table 2: GPGPU Benchmarks.
policy. In order to evaluate the effectiveness of the GPU
warp scheduler’s latency hiding ability, we first need to identify the sources of warp execution time delays. In the following, we explain each of the potential factors that can delay
a warp’s instruction from getting executed:
Warp scheduler. At every cycle, the warp scheduler depicted in Figure 1 selects a warp for execution based on the
warp scheduling policy. In the baseline GPU architecture,
we use a fair round-robin scheduler to select warps in the
warp pool. For instance, if there are 48 active warps in the
pool, the warp scheduler will iterate over the 48 warps cycleby-cycle issuing one instruction per warp. A warp is selected
for instruction issue and execution every 48 cycles and will
have up to 47 cycles delay from the round robin scheduling
policy to hide any latency stall cycles from itself.
Instruction buffer and instruction cache miss. In order to store instructions fetched from the instruction cache,
each warp has a two-entry instruction buffer. When a warp
selected for execution has no instruction(s) in its instruction buffer, additional latency penalty is paid because of the
empty instruction buffer caused by instruction cache misses.
Structural hazard. If there is an available instruction in
the selected warp’s instruction buffer, the instruction will
be placed in the scoreboard while also accessing the source
operands in the register file. This is when structural hazards caused by contentions in the register file banks and at

the various functional units are examined. If the decoded
instruction cannot proceed for execution due to the unavailability of the register file bank or due to the unavailability
of the required functional unit, the warp needs to be stalled.
Control hazard. Unlike CMPs that are often equipped
with advanced branch predictor units, GPUs do not currently implement branch prediction logics and rely on the
massive multi-threading feature to overlap the latency caused
by control hazards. However, it is possible for a warp to experience additional control hazard stalls. For example, if a
branch or function call instruction falls onto the taken path
for a particular warp, and there is no other active warp in the
pool at this time instance to help hide the branch address
resolution latency, the particular warp then has to spend
additional cycle(s) until its target address is calculated.
Data hazard. In addition to the stalls coming from structural and control hazards, data dependency can introduce
additional penalty to an active warp. Currently, there is
no data forwarding logics implemented in GPUs. Therefore,
warps have to wait until data dependency is resolved before it can proceed execution. We highlight here that if an
instruction is dependent on an older load instruction experiencing a cache miss, this particular dependent instruction
will spend a significant amount of stall cycles until the data
hazard is cleared.
MSHR and data cache miss. Memory load and store

177

Algorithm 1: Baseline Fair Round-Robin Scheduling.
1
2
3
4
5

Algorithm 2: Warp Latency Breakdown (the probe
function in Algorithm 1).
Input: w: warp
1 w.Scheduling + = CurrT ime − w.P revT ime;
w.P revT ime = CurrT ime;
2 if Sync then
3
w.Sync++;

while N oReadyW arp AND N otV isitedAllW arps do
warp = f indN extW arp();
probe(warp);
if readyW arp then
issue(warp);

4
5

instructions can experience additional latency stalls if the
miss status holding registers (MSHRs) for the data cache
are highly contended or if a data cache miss is encountered.
Depending on the availability of the MSHRs and where the
requested data reside, the amount of latency can vary.
Synchronization primitives. In addition to pipeline hazards, cache miss, and scheduling latencies, implicit and explicit synchronization primitives in GPU programs can stall
warp execution as well. This delay mainly comes from how
the communication between the parallel threads is structured.

3.2

Latency Attribution Algorithm

To illustrate how the different stall factors can contribute
to a warp’s execution time, we develop an algorithm to count
and reason about where stall cycles come from for warps.
Because latencies caused by the various stall factors can be
significantly overlapped in GPUs, it is a challenging task to
accurately and faithfully attribute stall cycles to the corresponding cause.
At every cycle, the baseline warp scheduler looks for a
ready warp by iterating over the active warps in the instruction issue stage at the GPU pipeline (Algorithm 1). For
each of the visited warps, whether they are ready or not, we
record the execution status of the warp by instrumenting
the baseline warp scheduler’s implementation with a monitoring function, called probe(), in Line 3 of Algorithm 1. If
the visited warp is not ready, the monitoring function will
investigate the sources of stalls and update the corresponding latency counting logics; otherwise, the visited warp is
ready for execution.
Next, we delve deeper into the monitoring function and
present our latency attribution algorithm. First, we want
to calculate the time during which a warp does not execute
any new instruction because of the scheduling policy. This
is determined to be the difference between the time when
a warp is checked and the time when this particular warp
was last checked. The time difference signifies how long the
selected warp needs to wait until it can potentially issue the
next instruction, i.e., the scheduling delay.
After the scheduling latency is determined, the monitoring
function next examines whether the selected warp is ready
or not. If ready, the w.Exec counter is incremented (Line 19
of Algorithm 2). Otherwise, the monitoring function investigates the stall factors sequentially – synchronization primitives, no available instructions in the instruction buffer or
instruction cache misses, control hazard, data hazard, data
cache misses, and structural hazard. Because a warp can be
stalled due to multiple stall factors at the same time (the latency hiding feature by the massive multi-threading GPUs),
we want to be cautious about not double-counting latencies
overlapped by several stall factors. For example, if a warp
is stalled because of data dependency on an older, load instruction that misses in the data cache, we attribute only the

6
7
8
9
10

11

12
13

else if EmptyInstF etchBuf f er then
w.F etch++;
else if prevBranchT aken then
w.CtrlHazard++;
else if DataDependency then
if onOlderDataCacheM iss then
if
w.CurrP endingAddr 6= w.P revP endingAddr
then
w.DataHazard++; w.P revP endingAddr =
w.CurrP endingAddr;
else
w.DataCacheM iss++;
else
w.DataHazard++;

14
15
16
17
18
19

else if F U unavailable then
w.StructuralHazard++;
else
w.Exec++;

first stall cycle to the data hazard factor and any additional,
subsequent stall cycle(s) to the data cache miss factor. This
is because the underlying reason for the warp stall is the
data cache miss encountered by the previous memory miss.
If there is no following dependent instruction on a previous
load cache miss (while the cache miss is being serviced), our
algorithm does not attribute any stall cycle for this data
cache miss. Therefore, it is important to note that the latency shown under data cache miss category represents only
the latency penalties that stalled pipeline execution.
We also want to highlight that since GPUs rely on the fast
switching between the massive number of available warps to
improve pipeline resource utilization, even if a warp encounters no delay from pipeline hazards, memory accesses, or
synchronization overhead, it still suffers from delays caused
by the scheduling policy. For example, if a fair round-robin
scheduling policy is applied for a GPU application that has
48 warps running on an SM core, each warp will spend 47 cycles waiting until its next ready instruction can be executed.
These 47 scheduling cycles will be used to overlap with other
latencies caused by e.g., structural hazards, for a particular
warp. These stall cycles are shown in the scheduling latency
instead of structural hazards, since the additional latencies
are hidden by the warp scheduler which overlaps the scheduling latency with the execution of other ready warps.

3.3

Characterization Results

Figure 2 presents the latency characterization results for
the Rodinia and Parboil applications. The x-axis presents
the latency breakdown of the GPGPU applications sorted
by the degree of the scheduler’s latency-hiding ability while

178

1	
  
0.9	
  
0.8	
  
0.7	
  
0.6	
  
0.5	
  
0.4	
  
0.3	
  
0.2	
  
0.1	
  
0	
  

Strcl.	
  Hzrd.	
   Ctrl.	
  Hzrd.	
  
Sync.	
  
Execu9on	
  

Data$	
  
MSHR	
  
Scheduling	
  

Algorithm 3: The bfs algorithm
1
2
3
4
5
6
7
8
9

myocyte	
  
needle	
  
par9cle_ﬂoat	
  
strmcltr	
  (med.)	
  
srad_2	
  
gaussian	
  
bfs	
  
lud	
  
strmcltr	
  (small)	
  
lavaMD	
  
leukocyte	
  
B+tree	
  
hotspot	
  
srad_1	
  
heartwall	
  
backprop	
  
tpacf	
  
kmeans	
  
pathﬁnder	
  

Latency	
  Breakdown	
  

Data	
  Hzrd.	
  
Inst.	
  Fetch	
  

Figure 2: Latency breakdown for the Rodinia and
Parboil applications.

10
11
12
13
14

15
16

the y-axis shows the latency stalls attributed by the various
factors discussed in Section 3.1. The latency stall results are
across all warps in each of the applications. The sections
of the bars illustrate the stall cycles contributed by each
stalling factor. Applications toward the right are the ones
that benefit from the baseline round-robin (RR) scheduling
policy whereas the applications toward the left are the ones
whose latency stall cycles cannot be hidden by the scheduler.
First, we investigate the effectiveness of latency hiding
ability of the baseline RR scheduling policy by focusing on
the Scheduling bars. The RR scheduling policy is able to
hide the majority of the warp stall cycles in applications,
such as backprop, tpacf, kmeans, and pathfinder. These applications are considered as well-behaving GPGPU applications. On the other hand, the latency-hiding ability of the
RR scheduling policy is poor for applications such as myocyte and needle. This is because there lacks warp-level parallelism in these two applications. In other words, there are
not sufficient active warps in the warp pool.
The next two factors dominating stall cycles are from Inst.
Fetch and Data$ – stalls caused by waiting for ready instructions in the instruction buffer and by waiting for data to be
served from the cache memory. Applications, such as myocyte, needle, and tpacf, suffer from instruction fetch stalls
significantly. For myocyte and needle, this is again due to the
lack of warp-level parallelism. When there are enough active
warps in the pool, warps will “prefetch” instructions for each
other. However, since the number of active warps in these
two applications is small, instruction fetch often results in a
cache miss. Furthermore, the scheduler cannot overlap the
instruction fetch latency with other concurrent warps. As a
result, the applications spend a significant amount of time
waiting for available instructions for execution.
On the other hand, the reason that tpacf suffers from long
instruction fetch stall cycles is because it has a high static
instruction working set that does not fit to the instruction
cache entirely. Despite having a high degree of warp-level
parallelism, tpacf still encounters high instruction fetch latencies. Moreover, having a high degree of warp-level parallelism makes the instruction cache contention problem even
more critical. Fast warps evict cache lines from the instruc-

while not yet visited all nodes do
kernel 1() { / ∗ running on GP U ∗ /
if node[tid].mask == true then
node[tid].mask = f alse;
i = 0;
while i < node[tid].no of connected nodes do
id = node[tid].child[i]
if node[id].visited == f alse then
node[id].update = true;
}
/ ∗ an implicit barrier here ∗ /
kernel 2() { / ∗ running on GP U ∗ /
if node[tid].update == true then
node[tid].update = f alse;
node[tid].mask = true;
node[tid].visited = true;
}
/ ∗ an implicit barrier here ∗ /

tion cache that will be re-referenced by slow warps later. As
a result, slow warps take a large amount of time to resolve
instruction cache misses.
Applications such as streamcluster(small) and streamcluster(medium) suffer from long-latency data cache misses. As
explained in Section 3.1, this Data$ miss latency reflects the
amount of cycles dependent instructions need to wait for,
but not the amount of cycles memory accesses in the application take. The significant amount of stall cycles caused
by data cache misses indicate that streamcluster heavily accesses the memory hierarchy and suffer significantly from its
low degree of memory-level parallelism.
We next look at the amount of latency stalls contributed
by the three classic types of pipeline hazards, i.e., data,
structural, and control hazards. Data hazard stalls are caused
by the particular instruction ordering within an application.
We observe that applications, e.g., myocyte, needle, and particle float, suffer from data hazard stalls more than other
GPGPU applications. Structural hazard stalls are caused by
the competition for pipeline resources, in this case, the functional units in the pipeline. Applications, such as lavaMD,
spend a significant amount of time waiting for the structural
hazard to be resolved. This is because the warps in lavaMD
heavily compete for the load/store unit in the pipeline. Over
all GPGPU applications, we do not see much stall penalty
caused by control hazards. Compared with CPU applications, the Rodinia GPGPU applications contain less branch
instructions [13]. Furthermore, since the branch resolution
latency is relatively small, it can often be easily hidden by
the scheduler.
In addition to structural hazard caused by the unavailability of functional units, contention in the miss status holding registers (MSHRs) can cause additional penalty. lud, in
particular, experiences a significant amount of delay by the
unavailability of MSHRs. This is because warps in lud, a
memory-bandwidth intensive program [4, 11], often request
data from the memory in a bursty manner. As a result, the
performance of lud is significantly degraded by MSHR contention. Such contention happens to srad 2 and bfs as well.
Our studies show that by increasing the number of MSHR

179

1.2
1

Unbalanced Case with
Baseline RR Scheduling

0.8

Stall Time

Runtime Normalized to the Critical Warp

Compute Time

Ideal Warp Scheduler with
Criticality-aware Scheduling

1.35x Speedup

0.6
0.4
0.2
0

0 – 15

0 – 15

Thrd. Blk. 2

Thrd. Blk. 2

SM 3

SM 3

0.6
0.5

0.4
0.3
0.2

0.1
0

criticality, resulting in a speedup of 35%.
We further investigate the latency breakdown for all warps
in this thread block that is executed in lockstep. Figure 4
compares the latency breakdown of each warp in the thread
block. We observe that the critical warp (Warp 15) in this
particular thread block (Thread Block 2, SM3) suffers from
longer Scheduling, MSHR, Data$, and Strcl. Hzrd. latency
delays than other warps. As we will later see in the paper, our proposed criticality-aware scheduling policies can
reduce these latencies for the critical warp and result in
faster thread block execution time.

MOTIVATION: WARP CRITICALITY PROBLEM IN THE BFS APPLICATION
The Breadth-First Search Application

4.3

Identification of the Source of Criticality in bfs
To root-cause the sources of warp criticality in bfs, we
investigate several important contributing factors that can
potentially slow down warps in a thread block. We explore
the impact of instructions per cycle, the impact of the number of instruction and data cache misses in different levels of
the GPU cache hierarchy, code size, etc. Figure 5 compares
the degree of correlation between the warp criticality and
the different factors. The bars represent the magnitude of
the different factors explored in this work normalized to that
of the critical warp. Comparing the trend of the different
bars, it is apparent that CPI and data cache misses do not
correlate well with warp execution time, whereas the number of iterations and the number of dynamic instructions in
a warp do, implying that these are potential indicators for
warp criticality.
In the bfs algorithm, based on the input data set, each
node has a different number of child nodes as illustrated in
Algorithm 3. A warp has to traverse through all connected
nodes in the graph. Because the number of child nodes varies
in each node level, the amount of work distributed to the
warps in the same thread block varies as well. This causes
an imbalanced workload distribution among the warps. The
warp which needs to traverse through more child nodes (the
iteration count in Line 6 of Algorithm 3) finishs more slowly
and becomes the critical warp in the thread block. In fact,
the warp execution time is directly proportional to the number of iterations of the algorithm. Therefore, we believe that
the number of iterations specified in bfs could be a good indicator for predicting the degree of warp criticality for this

We use the breadth-first search (bfs) application from the
Rodinia and Parboil benchmark suites as a motivating example to highlight the warp criticality problem. bfs performs
breadth-first graph traversal to search for a specific item in
a tree.
The CUDA version of bfs contains two kernels that are
called repeatedly in a loop. Both kernels have a thread block
size of 512 threads that are grouped into 16 warps. These
warps are then mapped to the available SMs on the GPU.
The first kernel expands the search frontier from the current node to the next node level that contains multiple child
nodes. The second kernel performs the actual visit and then
sets up the conditions for the next iteration of the first kernel. All threads are synchronized at the end of each kernel.
Algorithm 3 illustrates the breadth-first-search algorithm.
At the end of two kernels, all warps are synchronized implicitly, without an explicit use of barriers, before all warps
can proceed.

4.2

Execution
Scheduling
Inst. Fetch
MSHR
Data$
Ctrl. Hzrd.
Strcl. Hzrd.
Data Hzrd.

0.7

Figure 4: Latency breakdown for the bfs algorithm
(sorted by execution time).

entries from 32 to 64, MSHR stall cycle is halved for bfs.
Finally, the Sync. component in Figure 2 indicates the
amount of time warps wait on barriers (e.g., the syncthread()
API calls) in the kernel. b+tree, particle float, and tpacf are
three applications whose warps spend a large amount of time
synchronizing on barriers. This significant synchronization
time is caused by workload imbalance between the warps.

4.1

0.8

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Warp Number

Figure 3: Speedup for ideal criticality-aware warp
scheduler.

4.

1
0.9

The Warp Criticality Problem

Figure 3 shows the amount of idle time the non-critical
warps spend waiting for the critical warp to arrive at the
end of the first kernel in bfs. The bars plot the compute and
idle time normalized to the critical warp, Warp 15 (sorted
by execution time), in Thread Block 2, SM 3. These show
large warp stall times, with a worst-case stall time of 53%
for Warp 0. In contrast, the right set of bars depict an oracle
criticality prediction, in which the warp scheduler preferentially selects and executes warps based on their degrees of

180

Counts	
  Normalized	
  to	
  	
  
The	
  Cri3cal	
  Warp	
  

#	
  of	
  Iter.	
  

Inst.	
  Count	
  

CPI	
  

D$	
  Miss	
  

Exe.	
  Time	
  

cution time of the critical warp(s) within the same SM but
across different thread blocks. The idea behind this policy is
to equalize the execution time of all warps on an SM. CAWSSM selects the top N critical warps from the warp pool to
accelerate, where N equals to the number of thread blocks
on an SM in the design. The SM-based policy is particularly
helpful when some of the thread blocks mapped to the same
SM do not contain any critical warp and other thread blocks
contain one or more critical warps.
Average based CAWS (CAWS-avg) scheduling. Instead of giving more time resource to the top critical warp
in a particular thread block (e.g., CAWS-blk) or within a
particular SM (e.g., CAWS-SM), we design a CAWS-avg
scheduling policy that identifies a number of critical warps
(ranging from none to m) within either a thread block or
a SM which require more time resource, where m is determined by the average execution time of all warps scaled by a
factor. CAWS-avg evaluates warp criticality by the average
execution time of all warps. When the disparity of execution
time for warps in an SM is insignificant, giving the slowest
warp a higher priority may cause criticality inversion, which
occasionally happens in CAWS-SM and CAWS-blk. The advantage of CAWS-avg is to avoid the occurrence of criticality
inversion.

1.6	
  
1.4	
  
1.2	
  
1.0	
  
0.8	
  
0.6	
  
0.4	
  
0.2	
  
0.0	
  

0	
   1	
   2	
   3	
   4	
   5	
   6	
   7	
   8	
   9	
   10	
  11	
  12	
  13	
  14	
  15	
  
Warps	
  Ranked	
  by	
  Execu3on	
  Time

Figure 5: The comparison of execution time and
different factors (normalized to the critical warp).
application.
Since the major contributing factor causing the warp execution speed disparity in bfs is the dynamic code size, and
the knowledge of the code structure becomes available when
the compiler analyzes and transforms the high-level C/C++
code to the PTX intermediate representation, we can insert
this hint to predict the warps criticality in each thread block
by including the feature into the LLVM compiler [14].

5.

5.2
5.2.1

Priority Counter

We design a per-warp priority counter that indicates the
degree of warp criticality. The counter value determines
when a warp is going to be selected by the scheduler for
execution. For example, given there are 48 active warps
in the pool in the baseline RR scheduling policy, all warps
would initially have counter values from 0 to 47. The warp
with a zero counter value is to be scheduled and its counter
is reset based on its priority. In this case, the counter for
the selected warp is always reset to be 47 since all warps are
treated equally and have the same priority in the baseline
policy. In addition to the selected warp, counters for other
warp are decremented by one.

CRITICALITY-AWARE WARP SCHEDULING
POLICIES

To reduce warp execution time disparity, we explore a
family of criticality-aware warp scheduling (CAWS) policies and seek to bridge the execution time gap between the
warps. We first explain how each scheduling policy works
and present the performance results of the scheduling policies when oracle knowledge of warp criticality is available.
Then we investigate the kind of information that can be
useful to the design of a warp criticality predictor and help
us identify warp criticality at runtime. Finally, we compare
the performance results for the baseline round-robin and the
family of CAWS policies designed in this paper.

5.1

Implementation of Scheduling Policies

5.2.2

Counter Value

Since the value of the priority counter determines how
often a warp is scheduled for execution (a smaller value indicates a warp is more important and needs to be scheduled
more often), we have to carefully select the counter value for
the critical warp(s). For example, to give critical warp(s)
20% more time resource, the scheduler will have to schedule
the critical warp(s) 20% more frequently compared to other
warps in the pool. This means that, in a pool of 48 warps,
the counter value of the critical warps should be reset to 0.8
* 48 and so on.

Exploration of Scheduling Policies

Round-robin (RR) scheduling. As the name suggests,
the round-robin (RR) policy schedules warps in an iterative
manner. All warps in the warp pool are treated equally
regardless of their degree of criticality and are given the
same amount of time resource in the baseline GPU.
Thread block based CAWS (CAWS-blk) scheduling.
This scheduling policy aims to improve the execution time of
a thread block (limited by the execution time of the longest
running, critical warp in the thread block) by giving more
time resource to the most critical warp in the thread block.
By giving higher priority to the most critical warp at the
thread block granularity, CAWS-blk allows thread blocks to
finish faster, such that hardware resources become available
to other thread blocks earlier. Consequently, the resource
contention is alleviated.
Streaming-multiprocessor based CAWS (CAWS-SM)
scheduling. Instead of improving the execution time of the
critical warp local to a particular thread block, we design a
global, the CAWS-SM policy that aims to improve the exe-

5.2.3

Counter Configuration for the Scheduling Policies

In bfs, we recognize that the execution disparity between
the fastest and the slowest warps is approximately 20% on
average. To compare the effectiveness of the CAWS policies
explored in this paper, we experiment with a counter configuration that guarantees 20% more time resource to the
high-priority, critical warp(s) identified in each policy by resetting the value of the priority counters to 40 for critical
warps and to 48 for the remaining warps
For the CAWS-avg policy in particular, we divide all warps
in the pool into three priority levels. Warps with execution

181

1.50	
  

1.25	
  

1.00	
  

bfs	
  

b+tree	
  

0.75	
  

tpacf	
  

kmeans	
  

geomean	
  

Oracle

0.74	
  

CAWS-avg

1.75	
  

CAWS-SM

1.25
1.20
1.15
1.10
1.05
1.00
0.95
0.90

2L-­‐CMU	
  

CAWS-blk

GTRR	
  

CAWS-avg

GTO	
  

CAWS-SM

CAWS-­‐avg	
  

CAWS-blk

CAWS-­‐SM	
  

Round-Robin (RR)

Speedup	
  over	
  the	
  Baseline	
  RR	
  

CAWS-­‐blk	
  

Criticality-Guided Predictor

Figure 6: Speedup comparison for the different warp
scheduling policies (normalized to the round-robin
scheduler).

Figure 7: Speedup comparison for the different
CAWS policies on bfs (normalized to the round-robin
scheduler).

time more than 20% of the average are determined to be
critical warps and are given 20% more time resource. As discussed in the previous section, this implies that the counter
value for these warps are reset to 40 for a pool of 48 warps.
Warps with execution time less than 20% of the average are
determined to be fast warps and are given 20% less time resource by setting the counter value accordingly. Finally, the
remaining warps in the pool will adopt the default counter
value as the baseline RR policy.

6.1

6.

6.2

Criticality Encoding

We encode warp criticality information based on the warp
execution time with the baseline round-robin policy to guide
the CAWS schedulers. Furthermore, in order to keep the
simulation region consistent and to create a fair comparison
metric, in this paper, we only use the first round of kernel execution to present and analyze the performance of different
warp scheduling policies.

RESULT ANALYSIS

In this section, we choose four benchmarks from the Rodinia and Parboil benchmark suites to represent four distinct GPGPU workload behavior. We use the benchmarks
to highlight the specific characteristics of the workloads and
to evaluate the effectiveness of our proposed CAWS schemes
in detail.
1. Rodinia/Parboil Breadth-First-Search (bfs) kernel#1 A workload imbalance GPGPU application depicted in Section 4. Warps have disparate execution
path resulting from different number of iterations. Warps
with heavier workloads experience more instructions
and take longer time to finish.
2. Rodinia B+tree Search (b+tree) An application
that has multiple small parallel regions (a region between two consecutive barrier instructions). Warps in
b+tree reach barrier instructions frequently and spend
a large amount of time on waiting for synchronization
with warps from the same thread block.
3. Parboil Two Point Angular Correlation Function (tpacf ) A GPGPU application whose static instruction count is larger than the capacity of instruction cache. Faster warps force the instruction cache
to evict cache lines that will soon be reused by other
warps. Its instruction fetch and instruction cache access pattern causes slow warps to wait for the next instruction, which degrades the overall system throughput.
4. Rodinia K-means Clustering (kmeans) kernel#1
A memory intensive GPGPU application where warps
require large memory space to store private data and
have a high degree of intra-warp data access [18]. This
memory access behavior creates memory contention at
the data cache, resulting in frequent cache misses.

182

Overall Performance

We first apply the oracle criticality knowledge on top of
CAWS to observe the performance improvement. Figure 6
shows the speedup comparison for different state-of-the-art
GPU warp scheduling schemes with CAWS using the oracle criticality encoding. Compared with the baseline roundrobin policy, CAWS schedulers can achieve an average of
22% speedup by equalizing warp execution time, which outperform the state-of-the-art warp schedulers, such as GTO [18],
GTRR [18], and 2L-CMU [16].
Imbalance GPGPU Workload. For bfs, CAWS tends
to accelerate warps with heavy workloads or having longer
execution path. The critical warps receive more time slots to
process their tasks. As a result, the execution time within a
thread block or SM becomes more balanced, minimizing the
warp idle time. By adopting GTO and 2L-CMU schedulers,
instead of making improvement, the performance degrades.
This is because GTO and 2L-CMU greedily dispatch time
slots to one warp and/or a group of warps. Without considering warps’ criticality, the critical warps do not always
reside in the high priority groups and will experience performance degradation. Consequently, GTO and 2L-CMU
worsen bfs’s warp execution time disparity and criticality
problem.
Fine-grained Parallel Region Workload. b+tree comprises multiple parallel regions which have only 5-10 instructions. Warps in b+tree do not exhibit a clear execution time
disparity because the application frequently perfoms synchronization instructions. For this type of GPGPU applications, employing CAWS to accelerate the critical warps
does not receive speedup as much as applications in the
Imbalance GPGPU Workload category. Nevertheless, our
average-based (CAWS-avg) policy still improves performance
by 5% on b+tree, which outperforms all other warp schedul-

Runtime Normalized to Critical Warp in
RR policy

1

dictor tailored for bfs. As discussed in Section 4.3, the loop
iteration count serves as a good indicator for the amount of
workload assigned to each warp.
Figure 7 compares the performance of various CAWS policies with the baseline round-robin policy when the oracle
knowledge of the critical warp(s) is available versus when
using the criticality-guided predictor.
When oracle knowledge of warp criticality is available, the
average-based (CAWS-avg) scheduling policy improves bfs’s
execution time the most, by 21%. The thread-block-based
(CAWS-blk) and streaming-multiprocessor-based (CAWSSM) scheduling policies also improve the performance of bfs
by 16% and 18%, respectively. Overall, the iteration-based
CAWS prediction policies all improve the execution time
for bfs. The performance speedup under the CAWS policies, i.e., the CAWS-blk and the CAWS-SM policies, is less
when compared to using the oracle warp criticality knowledge. This is not surprising since the loop iteration counts
may not correlate perfectly with warp criticality. Table 3
compares the speedup with respect to the predictor accuracy. Given the prediction accuracy of 87.5% and 79.2%
for the CAWS-blk and the CAWS-SM policies, bfs still sees
10-12% performance speedup over the baseline RR policy.
The CAWS-avg policy still performs the best, even with the
iteration-based criticality predictor, and improves the performance of bfs by 21%.

0.9
0.8

Execution
Scheduling
Inst. Fetch
MSHR
Data$
Ctrl. Hzrd.
Strcl. Hzrd.
Data Hzrd.

0.7
0.6

0.5
0.4
0.3

0.2
0.1
0
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Warp Number

Figure 8: Latency breakdown for the bfs algorithm
under the oracle, CAWS scheduling policy.

ing policies.
Large Instruction Working Set Workload. The criticality problem of tpacf stems from insufficient instruction
cache capacity. Slow warps spend more time waiting for the
next instruction to be fetched from the instruction cache.
Employing CAWS, warps reach and execute a particular instruction roughly at the same time. Therefore CAWS reduces the instruction cache miss rate and shortens the average wait time resulting from instruction fetch latency. Overall, tpacf gains approximately 1.5x speedup by applying the
various CAWS schemes.
Large Data Working Set Workload. The fourth class
of applications represented by kmeans does not benefit much
from CAWS. This is because the execution time difference
and the criticality problem for kmeans is due to memory contention in the data cache. Our experiment results show that
the critical warps in kmeans mainly suffer from the high L2
cache miss count. In this case, giving higher priority to the
slow warps and prioritizing critical warps does not alleviate
its memory contention nature. Moreover, it may introduce
criticality inversion. While critical warps receive more time
slots to run, it implies that the fast warps’ private data gets
evicted from data cache more frequently. Eventually, the
fast warps turn into critical warps due to longer memory access latencies. For GTRR, because it applies a round-robin
manner to greedily schedule a new warp, the data locality
is hardly kept. Therefore, GTRR degrades kmeans’s performance significantly. On the other hand, GTO and 2L-CMU
scheduler tend to maintain data locality which relieves memory contention at the data cache and thereby significantly
improves the performance for kmeans.
In summary, the GPGPU applications with an imbalanced
warp workloads benefit the most from the CAWS schemes
proposed here. While we use four applications to represent
and evaluate the diverse, heterogeneous characteristics of
the GPGPU workloads here, our results on other GPGPU
applications show similar behavior for each category. Taking
a step further, we next show how the criticality information
can be predicted and expressed to the scheduler in practice
for bfs, the application that benefits the most from CAWS.
To realize a practical implementation for the family of
CAWS policies, we design the iteration-based criticality pre-

Policy

Speedup

Accuracy

CAWS-blk
CAWS-SM
CAWS-avg

1.10
1.12
1.21

87.50%
79.17%
95.4%

Table 3: Prediction accuracy of the criticalityguided predictor.
The varying degree of performance improvement by the
these scheduling policies comes from (1) the distribution pattern of warp execution time and (2) the priority counter implementation. For example, if there exists one obvious critical warp among all warps within a thread block or within
a SM, all three scheduling policies would perform equally
well. However, if the execution time difference between the
most critical warp (the longest-running warp) and the next
critical warp (the second longest-running warp) is not significant, the occurrence of criticality inversion is likely. In
other words, by giving more resources to the critical warp in
the three criticality-aware scheduling policies, it is likely for
the second critical warp to become the most critical warp
instead.
Furthermore, the priority counter implementation may
worsen the criticality inversion problem. For example, if
the counter value is not configured carefully, the most critical warp may receive more resources than needed. As a
result, it increases the likelihood of criticality inversion. Table 4 compares the frequency of criticality inversion in each
of the three scheduling policies with respect to the execution
time improvement and shows that the increasing frequency
of criticality inversion hinders the amount of performance
gain in the scheduling policy. When there is no criticality
inversion, e.g., in the CAWS-avg policy, we can achieve the
most performance benefit with the CAWS policy.
Next, to illustrate how the CAWS policies can effectively
reduce the execution time of the critical warp(s), we compare the execution time of all warps in the particular thread

183

1.00	
  
0.80	
  
0.60	
  
0.40	
  
0.20	
  
0.00	
  

RR	
  
CAWS-­‐blk	
  
CAWS-­‐SM	
  
CAWS-­‐avg	
  
GTO	
  
GTRR	
  
2L-­‐CMU	
  
RR	
  
CAWS-­‐blk	
  
CAWS-­‐SM	
  
CAWS-­‐avg	
  
GTO	
  
GTRR	
  
2L-­‐CMU	
  
RR	
  
CAWS-­‐blk	
  
CAWS-­‐SM	
  
CAWS-­‐avg	
  
GTO	
  
GTRR	
  
2L-­‐CMU	
  
RR	
  
CAWS-­‐blk	
  
CAWS-­‐SM	
  
CAWS-­‐avg	
  
GTO	
  
GTRR	
  
2L-­‐CMU	
  

Latency	
  Breakdown	
  Normalized	
  
to	
  Baseline	
  RR	
  

1.20	
  

bfs	
  

b+tree	
  

tpacf	
  

Scheduling	
  
ExecuKon	
  
Sync.	
  
Inst.	
  Fetch	
  
MSHR	
  
Data$	
  
Ctrl.	
  Hzrd.	
  
Strcl.	
  Hzrd.	
  
Data	
  Hzrd.	
  

kmeans	
  

Figure 9: Latency breakdown of different warp scheduling policies at first round of kernel execution (normalized to the round-robin scheduler).
Policy

Speedup

Criticality Inversion

CAWS-blk
CAWS-SM
CAWS-avg

1.16
1.18
1.21

8.89%
2.22%
0%

cies are coming from warps generating a burst of memory
requests and competing for memory resources including the
load/store units. Applying CAWS or GTRR does not reduce
memory traffic in the memory system and thereby the latencies of these factors do not change a lot. GTO and 2L-CMU
schedulers exploit a small number of active warps to lessen
the amount of outstanding memory requests, consequently
reducing the memory latency.
b+tree suffers from memory system and synchronization
time. The reason b+tree’s MSHR and Data$ latency cannot
be overlapped by scheduling is its fine-grained parallel regions. Fast warps arrive at the barrier quickly and become
idle. These fine-grained parallel regions result in insufficient
warp-level parallelism. The latency breakdown shows that
applying CAWS does not further improve the latency hiding
ability of the scheduler.
For tpacf, observing from the latency breakdown, the baseline RR scheduler hides tpacf’s execution latency well. Indeed, the main factor limiting tpacf’s performance is instruction fetch latency. As mentioned in Section 3.3, the instruction fetch latency can be hidden well unless an application
lacks warp-level parallelism. Under all scheduling policies,
i.e., CAWS, GTRR, GTO, and 2L-CMU, the scheduling latency is significantly reduced. This is due to less instruction cache miss count with these advanced warp scheduling
algorithms. Furthermore, benefiting from the significantly
reduced instruction fetch latency, the sync. time also decreases. This implies that the criticality problem is alleviated. Furthermore, Figure 9 also shows that the time for
warps to reach a barrier instruction becomes negligible under the advanced scheduling policies.
The latency breakdown of kmeans under the RR scheduler
demonstrates that this application suffer from memory access latency primarily. A significant portion of the execution
delay is at the MSHR and Data$. In addition to RR, adopting CAWS or GTRR policies does not alleviate its memory
contention nature. Therefore the latency characterization
does not show a significant change. On the other hand, unsurprisingly, using GTO or 2L-CMU scheduler, the MSHR
and Data$ latencies are reduced. This is because GTO and
2L-CMU scheduling policies keep data locality in the data
cache better and, as a result, reduce the latencies related to
data memory references.

Table 4: The speedup and frequency of criticality
inversion within a thread block.

block (Thrd Blk 2, SM3) used previously in Section 4 and
Figure 4. Figure 8 shows the execution time and the latency
breakdown for all the warps in the thread block under the
oracle, CAWS-avg scheduling policy. The execution time
of the critical warp (Warp 15 ) is reduced by 28.4% when
compared to using the baseline RR scheduling policy. This
execution time improvement comes primarily from the reduction in the scheduling delay (Scheduling), the contention
in the MSHR entries (MSHR), and the contention in the
functional units (Strcl. Hzrd ). These latency components
are exactly the ones that the CAWS policy intends to improve. When comparing to the warp latency breakdown under the baseline RR scheduling policy in Figure 4, the Data$
latency component is increased under the oracle, CAWS-avg
policy. This is because the Data$ stall cycles of the critical
warp are better hidden by the RR scheduling policy. Overall, by applying the oracle, CAWS-avg policy, the execution
time of this particular thread block is improved by 27%.

6.3

Latency Characterization and CAWS

We revisit the latency breakdown approach (Algorithm 2)
to analyze the behavior of different GPU warp scheduling
policies. Figure 9 shows the average latency of each warp in
the first round kernel execution. Note that because the latency breakdown counts the average of all warps, it is not directly proportional to the overall execution time, especially
for a workload-imbalanced application such as bfs.
As shown in Figure 9, a significant amount of bfs’s execution time is spent at the MSHR and data cache. This is
because bfs has a large dataset, which does not completely
fit into the data cache. Moreover, in the data cache, the data
re-reference rate is very low. Hence it requires high memory
bandwidth to accommodate the data requests. This insight
reflects on the latency characterization. bfs substantially
suffers from Strcl. Hzrd., MSHR, and Data$. These laten-

184

7.

RELATED WORK

and showed that TCP can be used to guide task stealing
as well as to improve energy efficiency of the CMP. Many
other mechanisms, e.g., [3, 6], were inspired by the thread
criticality concept for parallel applications running on CMPs
and proposed techniques to reduce the execution time of the
critical thread(s) and thereby improve overall parallel application performance.
While the warp criticality problem presented in this paper
is similar to the thread criticality problem that is observed
in parallel applications executing on CMPs, due to the distinct difference between GPU and CPU architectures, the
effects introduced by the critical warp(s) and thread(s) vary
as well. Because of the fast context-switching and massive
multi-threading in modern GPUs, our characterization results have shown that a significant amount of warp stall
cycles can be well hidden in GPUs (but this is not the case
in CMPs).
To the best of our knowledge, this work is the first that
performs warp latency characterization in detail and presents
the warp criticality problem in GPGPU applications. Taking a step further, we design and evaluate a range of criticalityaware scheduling policies with both oracle and an iterationbased warp criticality predictor for improving the performance of the breadth-first-search algorithm on modern GPUs.

Prior work have looked at various warp scheduling policies
to improve GPU performance. In this section, we discuss the
scheduling policies that are most related to this work.
Many prior work [9, 10, 12, 11, 16, 18] focused on improving the memory aspect of the GPU warp scheduling
algorithm. Gebhart et al. [9] and Narasiman et al. [16] proposed to split warps into different subgroups and keep only
one group of warps active at a time. The warp scheduler
is only able to issue instructions from the active group of
warps. These algorithms prevent all warps from stalling at
the same time by ensuring the warps in the pool do not
encounter long latency memory operations or compete for
the register file banks at the same time. These scheduling
policies allow the GPU warp scheduler to tolerate memory
latency better while reducing the idle time of GPU cores.
In contrast, this paper aims to reduce the delay caused by
scheduling policies as well as the delay caused by the structural hazards in particular. These additional latency penalties worsen the execution time of the critical warp(s) and can
degrade the overall performance of GPGPU applications.
Rogers et al. [18] proposed a family of static and dynamic
greedy scheduling approaches, GTRR, GTO, and CCWS, to
improve intra-warp data locality. These greedy scheduling
algorithms aim to improve the hit rate at D-cache. However,
the greedy approaches are only apt for improving memory
intensive applications. For imbalance GPGPU workloads,
such as bfs, employing the greedy scheduling policies, e.g.
GTO and GTRR, does not help improve performance. On
the other hand, the proposed CAWS policies designed for
imbalance GPGPU workloads are shown to improve performance for such workloads while not hurting the performance
of memory intensive applications.
In addition to improving the memory behavior of GPGPU
applications, prior studies proposed to use scheduling policies to reduce the likelihood of branch and memory divergence occurrence on GPUs. When branch or memory divergence happens, the utilization of GPU resources becomes
sub-optimal. Therefore, algorithms, e.g., [7, 8, 15, 16, 20],
are designed to dynamically select and combine cross-warp
threads that are executing the same instruction to the same
warp, such that cross-warp threads can now execute on the
same SIMD unit, and thereby improve the utilization of the
SIMD lanes. Instead of applying such compaction-based algorithms, Rhu and Erez [17] proposed to overlap the execution of a divergent warp to further improve the latency
hiding ability of the scheduler. All of these scheduling algorithms attempt to improve GPU performance by increasing
the utilization of functional units but are not designed to
address the warp criticality problem studied in this paper.
Besides the various warp scheduling policies discussed here
for modern GPUs, scheduling is a rich research field in CPUs.
While we cannot cover all scheduling policies, we focus on a
few that are most related to our work here. Bhattacharjee
and Martonosi [2] observed the thread criticality problem
for parallel threads executing on CMPs. Since the performance of a parallel application is constrained by the critical (the slowest running) thread, the execution time of the
parallel application is significantly affected by the execution
time of the critical thread. To alleviate the execution time
disparity between the different parallel threads, Bhattacharjee and Martonosi designed the thread criticality predictor
(TCP) that is largely based on per-thread cache behavior

8.

DISCUSSION

The performance of the criticality-aware scheduling policys depends significantly on the performance of the criticality predictor, and it is challenging to design the criticality
predictor for modern GPUs executing GPGPU applications.
First, as shown in Section 6.1, the performance improvement
seen in bfs depends on how frequently criticality inversion
occurs, which is governed by the implementation of the perwarp priority counters. If the priority counter is not configured appropriately, it is easy to overestimate the amount
of resources that should be given to the critical warp and,
consequently, other non-critical warps will be slowed down.
Second, the performance improvement of bfs is also affected by the prediction accuracy. Because of the diverse
GPGPU application behavior, the factors causing the warp
execution time disparity vary from application to application. Hence, it is a non-trivial task to design a warp criticality predictor which works well for all GPGPU applications.
Lastly, we also find that the parallel regions in between
two consecutive synchronization points in GPGPU applications are much more fine-grained than CPU applications.
This implies that the time for the criticality predictor to
learn and to accelerate the execution of the critical warp(s)
is tight. We find this especially true for applications, e.g.,
backprop, hotspot, particle float, etc., in the fine-grained parallel region application represented by b+tree.

9.

CONCLUSION

This paper presents a detailed characterization and evaluation for the latency-hiding capability of modern GPU architectures, highlighting the different factors that comprise the
execution latency in the GPU pipeline, across a wide range
of GPGPU applications. We find that the fast contextswitching and massive multi-threading architecture can effectively hide much of the latency by swapping in and out
warps. However, for certain GPGPU applications, such as
breadth-first-search, the performance of the overall application is limited by certain critical warps. To address such a

185

performance issue, we identify the sources of criticality and
design a family of simple and lightweight scheduling policies
that aim to equalize the execution of all warps to achieve
maximum hardware resource utilization and minimize the
overall application execution time. Our results show that,
given oracle knowledge of warp criticality, the overall performance can be improved by as much as 17% on average. We
design practical warp criticality predictors that can improve
performance by 10-21% on bfs. To the best of our knowledge, this paper is the first to identify the warp criticality
problem in modern GPU architectures and to design CAWS
policies for managing massively parallel threads. We hope
that, with the significant performance improvement for bfs
and potentially other GPGPU workloads, this work inspires
better criticality counter designs tailored to the warp criticality problem in modern GPUs.

10.

[9]

[10]

[11]

ACKNOWLEDGEMENTS

We would like to thank Wenhao Jia, Margaret Martonosi,
and the anonymous reviewers for their feedback. This work
is supported by Arizona State University.

11.

[12]

REFERENCES

[1] A. Bakhoda, G. Yuan, W. W. L. Fung, H. Wong, and
T. M. Aamodt. Analyzing cuda workloads using a
detailed gpu simulator. In Proc. of the 2009
International Symposium on Analysis of Systems and
Software (ISPASS-2009), Apr 2009.
[2] A. Bhattacharjee and M. Martonosi. Thread criticality
predictors for dynamic performance, power, and
resource management in chip multiprocessors. In Proc.
of the 36th International Symposium on Computer
Architecture (ISCA-36), Jun 2009.
[3] K. D. Bois, S. Eyerman, J. B. Sartor, and L. Eeckhout.
Criticality stacks: Identifying critical threads in
parallel programs using synchronization behavior. In
Proc. of the 40th International Symposium on
Computer Architecture (ISCA-40), Jun 2013.
[4] S. Che, M. Boyer, J. Meng, D. Tarjan, J. W. Sheaffer,
S.-H. Lee, and K. Skadron. Rodinia: A benchmark
suite for heterogeneous computing. In Proc. of the
2009 International Symposium on Workload
Characterization (IISWC-2009), Oct 2009.
[5] S. Che, J. W. Sheaffer, M. Boyer, L. G. Szafaryn,
L. Wang, and K. Skadron. A characterization of the
rodinia benchmark suite with comparison to
contemporary cmp workloads. In Proc. of the 2010
International Symposium on Workload
Characterization (IISWC-2010), Dec 2010.
[6] E. Ebrahimi, R. Miftakhutdinov, C. Fallin, C. J. Lee,
J. A. Joao, O. Mutlu, and Y. N. Patt. Parallel
application memory scheduling. In Proceedings of the
44th International Symposium on Microarchitecture
(MICRO-44), Dec 2011.
[7] W. L. W. Fung and T. M. Aamodt. Thread block
compaction for efficient simt control flow. In Proc. of
the 17th International Symposium on High
Performance Computer Architecture (HPCA-17), Feb
2011.
[8] W. L. W. Fung, I. Sham, G. Yuan, and T. M.
Aamodt. Dynamic warp formation and scheduling for
efficient gpu control flow. In Proc. of the 37th

[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]

186

International Symposium on Computer Architecture
(ISCA-37), Jun 2010.
M. Gebhart, R. D. Johnson, D. Tarjan, S. W. Keckler,
W. J. Dally, E. Lindoholm, and K. Skadron.
Energy-efficient mechanisms for managing thread
conteext in throughput processors. In Proc. of the 38th
International Symposium on Computer Architecture
(ISCA-38), Jun 2011.
W. Jia, K. A. Shaw, and M. Martonosi. MRPB:
Memory request prioritization for massively parallel
processors. In Proc. of the 20th International
Symposium on High Performance Computer
Architecture (HPCA-20), Feb 2014.
A. Jog, O. Kayiran, N. Chidambaram, A. K. Mishra,
M. T. Kandemir, O. Mutlu, R. Iyer, and C. R. Das.
Owl: Cooperative thread array aware scheduling
techniques for improving gpgpu performance. In Proc.
of the 18th International Conference on Architectural
Support for Programming Languages and Operating
Systems (ASPLOS-13), Mar 2013.
A. Jog, O. Kayiran, A. K. Mishra, M. T. Kandemir,
O. Mutlu, R. Iyer, and C. R. Das. Orchestrated
scheduling and prefetching for gpgpus. In Proc. of the
40th International Symposium on Computer
Architecture (ISCA-40), Jun 2013.
A. Kerr, G. Diamos, and S. Yalamanchili. A
characterization and analysis of ptx kernels. In Proc.
of the 2009 International Symposium on Workload
Characterization (IISWC-2009), Oct 2009.
C. Lattner and V. Adve. Llvm: A compilation
framework for lifelong program analysis &
transformation. In Proc. of the 2004 International
Symposium on Code Generation and Optimization
(GCO-2004), Mar 2004.
J. Meng, D. Tarjan, and K. Skadron. Dynamic warp
subdivision for integrated branch and memory
divergence tolerance. In Proc. of the 37th
International Symposium on Computer Architecture
(ISCA-37), Jun 2010.
V. Narasiman, M. Shebanow, C. J. Lee,
R. Miftakhutdinov, O. Mutlu, and Y. N. Patt.
Improving gpu performance via large warps and
two-level warp scheduling. In Proc. of the 44th
International Symposium on Microarchitecture
(MICRO-44), Dec 2011.
M. Rhu and M. Erez. The dual-path execution model
for efficient gpu control flow. In Proc. of the 19th
International Symposium on High Performance
Computer Architecture (HPCA-19), Feb 2013.
T. G. Rogers, M. O’Connor, and T. M. AAmodt.
Cache-conscious wavefront scheduling. In Proc. of the
45th International Symposium on Microarchitecture
(MICRO-45), Dec 2012.
J. A. Stratton, C. Rodrigues, I.-J. Sung, N. Obeid,
L.-W. Chang, N. Anssari, G. D. Liu, and W.-M. W.
Hwu. The Parboil technique report, Mar 2012.
A. S. Vaidya, A. Shayesteh, D. H. Woo, R. Saharoy,
and M. Azimi. Simd divergence optimization through
intra-warp compaction. In Proc. of the 40th
International Symposium on Computer Architecture
(ISCA-40), Jun 2013.

-


  , %


 '
</
 - '


8
K1 $
  .

 .

7J  6
 6
  8

 (
+
 &	



L 
;
J


455;
-5;

)  
 
 
 -%"	=  
<
 
	
 

 

	 
 
 


   
  
 

	= 
 
 
 

	 
 	
  	
 
	; ' 

 			= 

	  	 

   	
 	<  @&&-?;
 
  
	 
 

 	 

    	=  
	  
 
 

< 

	   

  	 




 		 
 			=   

  
	

 
 
		    	 	  &. 			;
	 
 

	  
   
<



 
	 	 	   	  



	
&&- 

  	
  
  
 
<

	 

	  		   
  &&- 			= 


 
    	 	 
	

&&- 

;
.	 
 	 


	=  


	  <
 
	 
 
 
<
 

; *
	=
    # <

<
  	

 

 
	 

  

	 	
 '"-	  	  	
91A @5A 
 
?; 
= 
  
< &&

=  


	= =    


 
  		 ' "+. 	 
 
	 
< 

  
 	 

				 
 

 

	 	 


	 
; #
=


 


	 
 
    	

 
<
 

=  

 

  	



/5;
15;
35;
5;

*
%%($			
"
	

8	8

5J GJ //8  
> &
 	J (J ' 
 O
N 

 
 

  
   
 
  


 
 ON 

  
  
 

L JJ 
L
   
  '/9 
L J 7   2 8
 AJ

  
   
 

 

    J 5
 L 5

 G 
 
  //8  L  
 


 

 
 

   
 2
- O@BN J O( F 	  JN
:  //8 L 


   
 

  

J '  
  

 


  
 L    

 '
 / 9

 O'/9N L 
	

 L  	 
    



J
3	   
K 

 


  

  
 

   	


   L  
  
 
	
   
  
 

J 2 


L 

 
 
 
  
 

   

 

K 

  
 J '    
 



  

   	
   

 
  
    

J $
L  
 

 

 
	   

 G?P    	
  CPJ ' 

  GHP

 

 
	  EP  	
J
2 L  
 
 
 

 
K

  

   
 

	 

 
J ,

  
 K

L 
 
 2 +
 6	 9 ( O+69(N
L 
   
  //8 

K
 
  
 

J 9   L

9 '



2     
 
 L 
  

	
L 	K
L 
K	L L  
 
L  K


 O8.+N L 

 
 
   	 K  O//8NJ
'  
 

 
  

 
K

J :
 
K 

  

   K
 DL ?L GGL FGL FDL   
5 

     
 
	 
	   J (  
 


  ,( 

  GL FE L 
 

 
  	
 
     
   
  
 

  

 

 
J 5



L 
      



K 

     
L 
	


 

J
,

 
 
	   
K 


  

 

   
 
 



978-1-61284-368-1/11/$26.00 ©2011 IEEE

2

':9/6 2
: )832'68'&):/ +:):.6'6)( ,5 '36 )6:/ ("('6. O2 -'6/ - 63:/6. NJ
#
 	

 "

&9 '<&.
&9 ,<&.
&8 &.	
&9 ' -
&9 , -
&8 -
&7 - @&&-?

':9/6 22
: )832'68'&):/ +:):.6'6)( ,5 '36 9:(6/2-6 (2.&/:'67 +/:'5,).
O( 2.28( I46.(NJ

&

 ?JGH 7
2 8
 A 8+& ?CH
GF@K
L DK
BDK
 O JNL EFK
 O
 JNL DK
CGFK
 OK NL DKL +
	
EF09L DKL +
	L E 
EF09L @KL +
	L D 
FCB09L @KL +
	L GG 
G.9 
KK
L GBKL (
L EHKDH K


#
 	

 "

&9 '<&.
&9 ,<&.
&9 ':, -	
&8 - @&&-?

  
 
  
	  



    

   


O.()NJ ,

 
  
 

 
 
 KJ
$   
 
L 
K 


 
  
	 

 
  
K

  FCP 
    
 O 


 
 NJ . 
L  
 

 
 
	   

  
  	 
  
    
  

    
J :  

L 
 OJJ 
N  
 
 


 

  FHP

      
  
J
,	
L  
   
K 


  	
  

   
 
 
 


 
  
   





  
    
K
 

J
' 

   
  


  J 2 (K
 FL  
  
K  
 





  
 
  
 
J '  (
EL  
 

 

     
K

 
K //8 

J '  
 ( D 
  
  
  

  
   

J ( C 

   

  
  
 
	
    
 
 J '   
( B 
  
 
 
J 5L ( A

 
J

(
 (
 GH
&
(+:)8 222 8

GBK
L 
 J OI
 N=
GF@K
L FK O
 N
GBK
L 
 J OI
 N=
CGFK
 
 GHFDK
 FK J O

N
BD09L DKL +
	L E 
G.9 
K
L GBKL (
L EC 



 
 
   

 +.8
  
 
J $ 
  

  //8 
 
   
 
 

 
 

 	 $*$%+*'*+/%%,$'J $ 

 
 

  //8   #,%%,$'J
'  
 %-$'*/.%&,"$!%**+ 
 


     
  '/9 L
%4+!*)*,(/$' 
  

  /G7 

L  %3 /!*)*,($' 
  

 /F 
 J .
  
 
 


 
 	   
   2
(
 7	
 .
 AJ

8;8 *< 	 

2 

  

  

  //8   

L  
  
K 

 

  
(I46.( GFL G?J ' 
 

  (

&
(+:)8 222 8
 

  O' 22N   K

  
 O..&N 

J , 


    CGF 
L 
  F FCBK
 /G 7K
'/9J ' 

 

  K


L   '/9 I
 
  
- 
 
  ( FJGJ $  	
 


 '/9 
      GHFD 
L 
  (
 5
 E@HH O   
 '/9 I

 NJ 9 ..& 


 	 
 GBK
 
K
 	 /G 2  7K'/9 
   
 

  J
9
    
  '/9   


  

 

L  
  (


   
 
  
 

 K



 
  '/9  J ,

  
 


 
  
  ( EJFJ

8 %



'  

 

 
  
 

K 

  
K 
J

8;9 !< 	 %	
 '
	


$ 
  2 -K 8
 A 8+& ?CH O9N

 
  
K 

J ' 


 D  

 
J 6 
  
	 /G
 /F L   
 
  /E J 6  

    
	 /G 2K'/9L  
	 /G 7K'/9L  

	 /F 
 '/9J '  


 


  
 
 
 
  ' 2J
2 
 L 

 

 
 


O+.8N  
  

   
 

J
O'  
   
  
	 

L 

 
  
 
JN $ 
  

3 GB

8;7 .
	  ' 	
$ 

 

 	
 
 
K //8 
K

 
 
    (+68 8+&FHHB

 
 FHJ :  
 
   

 J 5
 
K 

L  
 


 J ( 
K 
 
 

 

 
  
K L  
  
  (+68 8+&FHHB  
   
 GHH.

     

 
   G9

J

3

	

455;

-5;

	

/5;

15;

	

35;

	

5;

	

	
7

		7




5J FJ
//8 

K  K 

 
J , 	
L

  CHP  //8 

 
 KJ 7  
(I46.( 
K 
J





5J EJ
'/9  
  
  (+:)8J )K


 

   2K-K 
 
 /

 
 	
J

7 -

 !
	 
  &	<
& -


 
  
 

J 9
  
 

 


  '/9   
 
 
  K 
 

L 
	 
  '/9    
  
   (+:)8  @B O2 -N 



  
   
K 
  
K


 
   
J

:  

  

 
   
	  
K

  8.+ 
 

 	
 
 




  
 L   


 
 

K 

  
 
 ,( 
K

 
 
 
 

J $ 
 
 
K


  	  	
	   	
  


  J $ 
K 

 
 

 
  
 
K
 
L    
  


  
  
   

 


	J ,

 

 

 
	 


 

K
   
K 
J

7;8 , ,
0 -

 	 !<

	
( 	
 
 

 


 	

KK K

 
L '/9  

   

  
  
J $   


  '/9L  	

   
 
 
 
 
J 5
  '/9 L    
	
 
 
J +

 	
 

 
 
    
  

 
 
L 
 

  

 
 //8 

  

J '
 
 
  
 
 
 
 (+:)8



J
04142 !'  #  (& () $
 '/9  

L  
   	J 2 

  
 
 

 O'(9NL  
 
 
   	

   
 
J 2 
 
    '(9L  '/9   
	   '(9 
J
2 L  
       
	
 
 
J 7

  
   

  '/9 L   
  


 
 


L  
  
 

J
04141 "    
  

) : 


    
  


 

       


 O
   L K
 L  
 
  
 

N


	   //8  KH 

J $ 
 
'/9  
   ( ..&  
 
 

 

 
   J $ 
 
..& 
 
  
 

  	  

 '/9   
	J 3	
L 

  
  L
 
  

   
  
  
   
 
 
J ' 

 



 
  '/9   L  
 

7;9 #
 -


	
04242 5
 &  " 
)
$ 

 +.8 	     L
    
  
K //8 K


 
  

 
  
K


J $ 

  

  

 
//8 
  O+.8 	> $*$%+*'*+/%%,$'N
 
    
 

  //8

  O#,%%,$'/*.+NJ ' 
 
$*$%+*'*+/%%,$'  #,%%,$' 


 

  //8 
  
 
	 L

 

L 
 
  
 

J
$     
  

  
 
K

 
  

 

   2 - K

J 9   
	   

 

L
 

   //8 L   
 //8
 
L    
  //8 

 

   
 

  
 J
04241 &  
3$% ) $ 
 
K
 
  


 	  

  //8


   J (
  

 
K


 

L 5

 F  L  	
L 
 CHP  //8 

 
 
 

 J : K
 
  //8 

 
 K
 

J
( EJF 

 
 
K //8 
K

 
   

L  
  
 
 '/9 L K  L 

4








	




	

5J BJ (K 

 
J , 	
L @HP  K


 
    
  '/9 J

5J DJ -

  
 

 


 
 '/9  J ,
	
L 
  @HH 
 

 
 


    '/9
 O  

  
NJ '  

 

 
 
   //8  

   
 
K
//8 

J

04140 !'  #  +- () ( 


K 

 

    @B 
K


L  
 
 '/9    @BL 
		 

   L 
 L  
 
 

 

  
K //8


J '
 
  

 	    
BDK J $  '/9  

L  8+&  


K	  L   '/9  

 





 

 
 

   

 

 J 2 L  
    


  '/9   J :  

L 
'/9  
  
 

L  
 
 
 
 	 

 '/9 
 

  8)E 

J '     
    
 
  (+:)8  

J

5J CJ -

  
 

 


 
 '/9  J ,
	
L 
  FHH 
 

 
 


    '/9
 O  

  
NJ '  

 

 
 
   //8  

   
 
K
//8 

J

7;7 %
 !	 -

		 /		 

	
<  	<%
 !
	

 27   
 
  
 



       
J
5

 E   

  '/9  
 

 
 (+68 8+&FHHB J 2  CGFK

'/9L 
  GHH '/9  

 
 	
 
 
J 
L  
 
 
 K
L    '/9  
   J
5
  

 GHFDK
 '/9 
L  

 


J
5

 D  C 

 L  	
L  
'/9  

 
 @HH 
  FHH  



J 5
  L 
  
L 
L 
L  

     
  


  
	 J '  
 
  
 '/9     '(9L    
    


J
,	
L     
  
 
K

 
   
   K


J 5

 B 

 L  	
L @HP  K
 

 
 
  '/9  J 2 
    
  '/9 L  

FHP    

  5

 B   

K  L 
  
 
K
L J (  

    
 
 K 

L    


 
 


  
 
J

2 

  	 	    
 
K //8 

  

 
 
 

L   
  


 

 
  

   

J $

  
 L 

  
 


   

 

   

 J 7 

   //8 
 

 	

 
  	J 3	
L   

 	
 
  

   

 
  //8J
$ 

   

 
 	 
L 
 

    	
 

 O
K

N 

 	 
  //8J
' 

 L 5

 A   

 

 
 

 

   
  //8 
	 O&
,N    
  //8   


O9NJ ' 
    

   

K
 
  //8    L ABP  

 
 
 	
 

 
  	 
  //8J
3	
L  

 

 	 
	   
//8L  

  
K

 

   
  EBPJ
'  
  

    
  

 

 	 
 
  
K //8 



  

J
6	 
L   
   

 K


  

 

 	   



5

455;
-5;
/5;
15;
35;
5;

)	


	
2

	


3

*6

	
"
(	

	+				
	

5J AJ .
 

 

 
 

 

> +
  
 
 
K

J '  

 
  

 

 
 

 //8 

 
  

J

5J ?J )
 

   

J $ @FP  


 
 
K

L  
 G@P  

  


 	
J CP     
 

 
  FH J

455;
-5;


  

 
L  
 	 

 
2 -K 

 
J '  



 
K 

 

 
 
K
//8 

 
   K	  

J

/5;
15;
35;
5;

)	


	
2

	


3

7;5 (

 "

	 
 ' $

*6

'
 
 

  

 

  2 -

>   
 O78&N 2+K

L 78& 

K


L K	  O./8N K

L  ./8


K

J
' 
K
 78& 2+K

  
 
 

  
 
  
     /G
=  78& 

 

  
 
 
      
 
     
       /G  J $ 	
  78& 

   
 2  
  
 
  J
,  
 L   
  ./8 K


  

K

    

K
   

 O.()N J ' ./8 K


  

   
	   

 

       
 J '
./8 

K

 
 
   78& 

K


L  
   


  

   


   
J

	"
(		
	+			

	

5J @J .
 

 

 
  

> +
 
  
 
K

J



J 5

 @  L   



  //8  

 

L @DP    

 
K

J 3	
L 	   

 	

	    //8 O( ,NL @FP  
  
  	
 

J '    //8
 
  
 

  

K 

  
 	  K 

L    	
   	
  

J
4	    

 
 
K

L  
      

  
 //8 

  

   

J
3	
L    

   

  

 
 

 

J 5

 ?
  

 

    J ' K
 

 

 

   K 

 


	  
  

 

J $ @FP
  

 	 
 

 

L CP  


 
 

 	     



 J

7;4 '
</
 '

 -	 
(

 "

$ 
   
	   ./8 

 
 (+68 8+&FHHB J ' 	   
 
  
K //8 

 
 
- 

 

J
5

 GH 

  
    //8 

 
 /F 
 J :   @CP   //8


 
 
   ./8 

J (   
 
 

  
  //8   /F L   
   

 

J -L   
 
 //8 
  	 
  ./8 

	
L  
 
 

  
 
//8 

J
-L  
    

    ./8


J 2 

  

  

 

7;6 -

 (

 "

2    

L 
 

 

 
 
  //8 

J $  
L 
  
	 
    

J 2 
  L
 	  


 

 
 CL BL
GHL GEL GCL FFL FB  
  
	 
 K
  
 

 

 I
 

 

J 3	
L 
     

 
 


 
K 
K
 //8 


 J
2 

  

 
K //8 



6

2
360
3
460
4
560
5

*
%%($			

*
6%%($			

5J GGJ 2
K
 //8 

 
  ./8 
 

J '

 
  
 OK 
 N  

 


L 
    

 

 J

5J GHJ 9	
  - .K/	 8 O./8N 

J '

 

  
  //8 

  
 
  ./8 

O    

 
    /G 
 /F NJ

      
	
 
 
L
  
  O72+N GAJ
,

 
 
     
 	
 

    
  

  



 
  J 2 //8L   	 

    GB 
 EFJ '   
 
 
 
 
  
     L 

 
     
    
K
 
 O.)&N J +

 
  


    
 .)& 
 /)&   	

K	  
  
  


J ,

 
     
 

 
 

K  K 

J $
	 
  
  	
  
K

 
 >  <&!L  <%',L   <,$J
$ 
  
 L 

  
  



 

   .)&   	
 

 
  

J

 <&!> '  
 

 

   .)&
     L 
  
  
K

   /)&  J ( 



    27 < HJ

 <%',> :   <&!L 

 

 
 

  .)& J ( 

 
L 	
L

      .)&  /)& J
5
 

 GBK K	 //8L    @J

 <,$> '  
 ( 7
 ? 
 
( DJGJG  
 
 

  
 
 
   
 L  <&! 
 <%',J 2     
 
 

  
  J

//8  
L 
K //8 

 
 

 

  
 J ,
L 
K
  	  

   	J
5

 GG   

   //8 
  

  
 //8  O
 
  
 

    //8N 
./8 

 

 J '  
 

  

  //8  
   L 
 ./8 

 
 J ' 
 
  

//8  
L  
     




 J 5
 (+68 8+&FHHB L 
  
L

L 
L 
L 
3L L  301L 


 

  
  //8 

J '    ./8 

  	
 

   	J
5
 L 
  
  
L 

  ./8


   
  

  
//8 J '   ./8 

 
  	

   
	  //8 J 5
 L

  


L L  L  ./8 



  	
 	 L 

L   

   


 J
/ 
  
  	
  
2L
 
 

  ./8 

J : 

 
5

 GGL  

   //8   


 
 	
J .
 L 
 
2L


   ./8 

 

   

 
 //8  O 
N   
 

 
//8  O
 
N J '   


  ./8 

  	
  
 
2  
 
 	
 
K
 //8 

J

 <,$       

  


 

  
J 2 

 
L  <,$ K

 

    
 
  
  
	
 GHHH //8 J 5



L  

   

 

  

L  <,$ 
 O	
 BD
 //8 N 
  

   .)&
    

  
  //8J '

  
    
  

 GAJ
'  <,$ 
  
  :
 GJ 2 

L  
  L L    
  

 
  

    
J 2 

 
L 

6 ! # < - '

<

6;9 # </
 , - '	



,

     

 
K //8 
K

 
  
K

    O@FP 
  

N  
    


 

  
  ( EJEJ '
L  
L  
 

  
    
     
 

 


 

K  K 

J ,

  K

7


	

  < GJ '   <,$ 
 
  <&! 
 
    //8  
 
  


 

 //8  
 

J '  <,$ 
 
  <%',J 5
  

  GL   
 

 
L   <,$ 
  <%', 
 

L
 	 	
 
  
  GJ


" !
" !







/
 9 :
 
  <,$J

!


 	   

  
   <&! K 

  
 
 <%',

 	     
   <&! K  

   <%',
 O
 	  M 
 	 N P GHHH << H 
 
 	   

 	    
 <,$   <%',
	
 <,$   <&!
 
 


	


!


" 
" 

!


	


!


	
	
5J GFJ 3

  
  <,$J 2 

 
L  
 G
  
 FJ

/4242 # "

) '  
 
   <,$   

 


K
J $ 
   
   
 

   //8  
 
  <&! 
 <%', J 2   L   EF 

  GHFD        
 >
 <&!   <%',J '   GL  EEL JJJL  	

EF    GHFDK //8 
   
  <&!J
(
L  FL  EDL JJJL  	
 EF  
  

  <%',J
$ 


 

    

  



   //8    J : 
 
:
 GL   
   

  
 //8
 
  
 ?BH  J
5

 GF 

  

    <
,$J ' 
 C    
   

 
       J
+
L  <,$ 
   

  

 
  
  
  J

4650
4
56,0
56,
56-0
5656.0

7% 
	


7$&'
	


7'#*$&(

5J GEJ &
 //8  
 

 
  <&!L  <%',L   <
,$J ' 

  
 
 
      

 
L 
  

 
 
   .)& J


  <&! 
	 

 2+8L  



  
  

 

 
	J : 


L  	
 2+8 
 
J  <%', 	 

J
5

 GE  L 
  <%',   


//8  
  
   <&!L   
	


 //8  
 
  J 2 L 
 
  5

 GC  

 2+8 
 
	 
DP  	
J $    
 2+8 
 
	
  
  GHP 
 
    	
  EJCPJ
'  
 
   

  
   /)&  
  <%',  
 
 
 G@P  

    

 	
  

 //8 

 
   


 

J
9
    <&!   <%',    	K
  

 //8 

  

  


L    
 L  <,$J 2L
 <,$    	    
 

J  <,$ 

 

 //8  
   K

	  

  
 2+8 

J ' 


  5

 GE    <,$L 
   

6;8 "

 !		

5

 GEGC   

 
  <&!L  <%',L 
 <,$J 2 


L 5

 GE  

 //8  
K

 
  
 J $ 
 

 //8 

 L  <&! O  
  5

 GEN 




 //8  
 
   
  J &
 
 
 
   
  GDP 
 
3 
  	
  BPJ '  
  <&! 
	

  

   /)& J :  

L 
  //8 

 
   


 	J
' //8  

   <&! 

  

  FFP 

 2+8 
	L   	
  CPJ
5



L 5

 GD    
 2+8 O

 

   
 

N  
	  

  GFP 
 
J $  <&!  	  
K

 

 //8  
L    
 	
 
 	
J 5
 L 
 
L 


L L 

L  <&! 
 
 2+8J '  


8

463
464
4
56,
5656.

5;9 , "
 %

7% 
	


7$&'
	


' 


 
  
 

 
 K

L  
L L  	
   K
 
  
 - 

 

 
  
  
K
 //8 

 
	
 
J
5
 L    
 
	

K 

 
  
 

L 

  

   

   


K
 

 J $  
K
	    
 
  

 L 
 
 

    

J
.4242 "

) ,
 

  	  
K
  

 
K
  

L JJ GEL
GCL FBJ 3	
L     


 


J 2 
L  
   	L
 
 
    
   



K 
  	 
K


  
  

 

J
2 


L   	  - +
 6	
9 ( O+69(N     
 //8  
 
J 5
 L  
 
    #L  

   
 
	
 #  //8 L  
 

 
  
K


 J $ 
  

3 
   
 L     L  
  
 

   +, 
J 5



L 
    

  
 
 
L 
  

 
    


O.()N  
  

 

J
-L  
 

  
 	  


 
J '
 
  
   

 

 
>   
    



 J 2   
 L  

  
./8 

  

     
	 J $  
  

   ./8


 

 J 2       	 
 

 
 

 L  	  

 


 
  

 
 J ,
L  

  
 

J ' 
K 
 

  



  
  :
 FJ '  


   
  GHLHHHJ ' L  	
 GHLHHH
 //8 L    

J $  	 
CH  

   GHLHHHJ '   

  
 
   GHH    

  


 
   ?L?HH J ,

 
  
 
 GHH  OGP   

 
N 
 
 
   
 
  



 
 J
( 

  
 
  
 
 

3L
 
  

 
 
 	
J '

  	
L  

     +69(
J 5
   

 
  

 
L
   
 	
 GHLHHH  //8 L
 	
  J 5
  
 
L   K
   HJCP  

  
 

   
   
 J

7'#*$&(

5J GDJ 2+8 

 
	 
  <&!L  <%',L   <
,$J ' 
 2+8 
 
    L 
 


 
 
   .)& J

5J GCJ 2+8 

 
	 
 

 

 
  <&!L
 <%',L   <,$J ' 

 2+8 
 
    L

  

 
 
   .)& J



 

 //8  
  
   <&!L  


 
 

  
   <%',J 5
  (+68
8+&FHHB L  <,$ 

 

 //8  
 
  GEPJ 5



L  
	 

 2+8  

  G?P    	
  CPJ
.
 
L  <,$   
	 

 2+8 

 
  J 5
 
L 
  
  L  <,$ 
  
  
 
 J '  
  <,$
 	   <&!L   

 

 //8 

 
 
	J 9
     
 
  
L  <,$     <
%', J :  

L  

   //8  

	J ,	
L 5

 GD 

   <,$ 
	

 

   
  GHP  EP  	


  J

5 % "
<'
/
 - '



'
<

2   K 

L 

 



  
K //8 

  J ,


K 

  ( EJB   -
./8 

  
 
 

  
	

  J ' 

  

 //8 

K
  

 
 

    K
J '  
 
K
  



  

 
L ( EJB   


  ./8 

  
 L 
 
	
  
2L  
	  //8
 
J

9

/
 8 :
 
  
 
J  -   


J
  - P 

 << H 
II  
 > 

 

	  < 


 =
'

   

 

=
	   - P 

 << 	 
II  
 > 

 
'  < 


  K 
	 =

	  < 


 =
'

   

 

=
	   - P 

 << 8>	 
II 

 
 
'  < 


  K 
	 =
 '  
 '  
'

   

 

=
	
'

   

 

=
 
 

5J GAJ

5;7

2

4
560
5

!

	"




' 

L  
	  
 
K 

K
 
 
K 

 
  



  

   
  
  

J &  +.8   
L  


  
   
 

 

 


 
 

J $   

 

 
 

 
 L 
  	
 
2L 
  
K
 
K
//8 

J 2  L 

  

 


	  
 

J
'  
K
  

 
K
L    
 
 
  K
   
  //8 

 
 

 

  
  
	  




 

 

 
J $ 
 
 
  
 
  


 
	
 

 
       


 J

360
3
460

!

	"
:	'9

+
     
 
 
J

'
$



5J GBJ : //8  

 
  
 
 
 >
+

 , O( 7
NL +

 ,L  7 .J

5;8 "

 !		

' 
  
 

  

 K
  
   
  FCPJ 5

 GB 


  //8  
 
   

L    

 
 

 J ' 

 

   
  O GNL  

 

  

K L   
 



   
 
J
5
   O
L 


L L 
L

L L 
3L L 
L  301N 
 
 
  

   

  

  
  //8  
J
5
 	
  
2L  
 
	

K
  

L   
 K

  

 

 L  

L 

 
 
  CP  FCP 
	J
2
L   
 
 
	 
 
 

   
 
  
J

  //8  

    



 

   

    (
7
  +

 , J '  
  

 
 
	    

  

  
	  

 

 
J
5

 GA 

  
 
  	
 



 
J 9 
  
	  -K
 

      L 

  
   
   

 
J

10

4 ! 

4;9


 &&- - %

//8 
    
 
 


  8.+
J ' 	 

 L 
  
	L
 
  


 
  //8L 


 



 	     DL ?L GGL GDL
GAL G@L FGL FDL FCJ
' 
L 	
L   
 

 



  

     
 



 
 
 
 

J ,

 
 

 
K //8 

 
   

 	    
    
 //8  J ' 

 L  
 

     
K //8 



   
 

  
 



  
   
K 
 
J
5



L   
 
K 

 
 

  	  J

4;8 # :	
 &&- !
 -




$  ,(   	  
  
 GL FEL 

 
 

  
  

     
 8.+  
 
L
 KKK
 

 

J 5



L 
 

 
 
 ,( 

L    
K
 

  
J 2 
L 

 

  
 
 
K  



 
     
  

J

4;7 &. %		 (

1  .
 @ 
 	
 
 

       

  
 

 

     
 
 

J
' 
	      	 
 
 
  
  
  

 
J 9
	
 J E 
 K OF7N    

 
 
   	

 	
J
:
  F7    
 '/9  K
L   

 
  

  
 

 
 

 	 
J ' 

  	


    L 9

  J F 
 	
  


   
	  

  



 
    J 9 
 FL E

  
	    
  

'/9  J ' 
L 	
L 
  


J ,

 
   
   


  FL E  


  //8 



   O
 	 
N 

    

J

3 -
	


,	
L 

 
  
   
 

K 

  


 
  J
,

 

  
 
K 


  2 - 

 

 

 

L   
  (I46.( 
K K

   
    

 



J 5
    
 

  //8 

L 

 
 
 

  J ,

 


  
	 

 2+8   
  G?PJ ,


 
 
     FCP  //8
 
    
J ' 
 

  
 
   
  
	 

 


    
 
     
   //8 

   

 J

2 /
	

$  ( 6
  1 .
 
 
 


 
 
  

3 
  -J $
  "
K"
 8L :
 1L 1 6
L  

 
	
 
 
 

   

   
J ' 
   
 
 


  - ( 5
 

 4
 -J 8-(K
HBFABCH  8-(KHAFHCBBGJ ' 

  
 

   4 ( )
 8
L  
 
 
 

  5
 8
 )
 +


O58)+NL  (

 )
 8

 J

!
	
G :J :
L 1J 3L  .J 3
J 8 

 

   


 
J (& 
 &
 
L G?@@J
F 'J 9

L :J 8L  (J )
J '
 > (L  
O  NJ 2    0, " 

 
&
 (L FHGHJ

11

E )J 9
	L 9J (

L 5J (L  (J .J :
 K
   
 	

 J 2   
20 " &  ( 

  


!  
 
L FHH@J
D 7J 8
L 5J 4
L (J 0L  "J (J +
 
K

     
K

 


J 2 
  22 " 

  #5
 &

(L FHHCJ
C 2J 3

  8J /J .
 
 
 	 
 J
2    0* " 

  L
FHHBJ
B (J 2	L /J (
L (J 0L "J 8
L  (J 4J :
J
6	 
K  
K  
J 2 5
   2+ " &  

L FHHDJ
A 2 BD  2:EF :


 (
 7	
 .
J
>IIJJI

I

I
IJ
@ 9J 1  'J .
J :   	
 
  
L
'/9K
 L    
J 2  
 22 " &  ( 

  
5

 !  
 
L G??@J
? :J 1L $J 3
L .J *

L 1J (L (J (L 1
JL 
1J 6
J :	 
  
  
 J 2
   2, " &   (5
  &
 	L FHH@J
GH -J 1
J 2
	 
K  

   
   
K	   
 

J 2 
  2, " 

  &
 (L G??HJ
GG (J 0L 7J 8
L  "J (J 5
  
  

   


 


J 2    20 "5
 &   (  &
 	L
FHHDJ
GF .J .J 0J .
L 7J 1J (
L 9J .J 9L .J )J .
L .J #
L
:J )J :L 0J 6J .
L .J 7J 3L  7J :J $J .

46.( 

 J "$(&# &
 ( L FHHCJ
GE 0J -L :J 7
L  1J (J :8I78>  	  


J 2    20 " & 
 (  &
 	L FHHDJ
GD 0J -L 1J /
L  1J (J %

 
	 J 2 
  00 " 

  &
 (L FHHAJ
GC 1J +
L (J /L (J /
L 1J (
L  0J /J 9 
  

 

  
  
J 2    2-
 &  

L FHHFJ
GB +
F> 3

K +

 .
 2
 
 /
J
>II
FJ


JIJ
GA .J 0J *

L :J 1L "J -J +L (J 8J (L  1J 6
J :	

  
  

 J 2   
0/ " 

  &
 (L FHHAJ
G@ .J 0J *

  "J -J +J &K  
> :
K	
L K

L 
   
 

J 2    0* " 

  5
L FHHBJ
G? ( (
 +
J >IIJ	

JIJ
FH (+68 9
 (
J >IIJJ
I
FHHBIJ
FG (J (
L .J 0
L  .J 1J 2
J :	  >
 
    


J 2   
20 " &  ( 

  


!  
 
L FHH@J
FF (J (
L ,J .

L 3J 0L  "J -J +J 5 


> 2
	  

  K 


 

J 2    20 " 

 # 
 &
 (L FHHAJ
FE 1J '

L :J 4
L  1J 3J 8

   

 

   


 
 J 2
   . " &  ( 


 

 !  
 
L G??FJ
FD "J #  4J /J +2++> 
I
 
K
 

K
 
 J 2    0. "


  &
 (L FHH?J
FC /J !L )J 2
L )J 2L 1J .L (J .L  7J -J
8(
> 5K
 
  
   8.+ 
J
2    2- " &   (5
  &
 	L FHHAJ
FB #J !
  3J /J )
  
 	  

 
J "%%%   &
L FHHAJ

ID-Cache: Instruction and Memory Divergence
Based Cache Management for GPUs
Akhil Arunkumar, Shin-Ying Lee and Carole-Jean Wu
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University
{akhil.arunkumar,lee.shin-ying,carole-jean.wu}@asu.edu

I. I NTRODUCTION
Modern GPUs are not only able to perform graphics rendering, but also used to accelerate general-purpose computations.
Such kind of GPUs are also known as general-purpose GPUs
(GPGPUs). Since GPGPUs have the ability to process massive
number of threads concurrently, they are considered as an
efficient alternative to accelerating parallel workloads for high
performance computing systems.
Similar to CPUs, modern GPGPUs are often equipped
with cache memories primarily to reduce off-chip bandwidth
demand and in some cases also to mitigate the performance
gap between the processor and main memory. Caches improve
a processor’s overall performance by means of exploiting
temporal and spatial locality. However, in modern GPGPU
designs, temporal and spatial localities can be filtered out
by the large register file, scratchpad (shared memory), and/or
memory coalescer [1]. Moreover, due to the massive multithreading computation paradigm, the per-thread cache capacity
is relatively small to accommodate the working dataset. Consequently, GPGPU caches are not utilized effectively. Recent
prior works, such as [2], [3], [4], have pointed out that
employing cache memories in GPGPUs may rather degrade
performance significantly for some GPGPU workloads. This
is due to pipeline stalls incurred by resource contention (e.g.,
MSHR entries) and additional queuing latencies introduced
by unnecessary data traffic. Therefore, a simply solution is

978-1-5090-3896-1/16/$31.00 ©2016 IEEE

2.0	
Speedup	over	Baseline	

Abstract—Modern graphic processing units (GPUs) are not
only able to perform graphics rendering, but also perform general
purpose parallel computations (GPGPUs). It has been shown
that the GPU L1 data cache and the on chip interconnect
bandwidth are important sources of performance bottlenecks
and inefficiencies in GPGPUs. Through this work, we aim to
understand the sources of inefficiencies and possible opportunities
for more efficient cache and interconnect bandwidth management
on the GPUs. We do so by understanding the predictability
of reuse behavior and spatial utilization of cache lines using
program level information such as the instruction PC, and
runtime behavior such as the extent of memory divergence.
Through our characterization results, we demonstrate that a)
PC, and memory divergence can be used to efficiently bypass
zero reuse cache lines from the cache; b) memory divergence
information can further be used to dynamically insert cache lines
of varying size granularities based on their spatial utilization.
Finally, based on the insights derived through our characterization, we design a simple Instruction and memory Divergence
cache management method that is able to achieve an average of
71% performance improvement for a wide variety of cache and
interconnect sensitive applications.

Baseline	

L1D$	Turned	Oﬀ	

1.5	
1.0	
0.5	
0.0	
PF	 BC	 MIS	 PVR	 BFS	 SS	 CLR	 CSR	 STR	 FLD	 MM	 ELL	 PRK	 KMN	

Fig. 1. Performance gain can be achieved when the L1 data cache is simply
turned off.

to bypass memory requests and turn off the L1 data caches
completely. Figure 1 shows that turning off the L1 caches
completely can help improve the performance of a number of
GPGPU applications that are sensitive to cache capacity and
interconnect bandwidth. However, this performance gain is not
consistent across all applications because bypassing memory
requests completely from the L1 caches also eliminates the
performance opportunities that data with locality brings.
First, to quantify the performance sensitivities of GPGPU
workloads to cache and bandwidth capacities, we conduct
a detailed study over a wide range of GPGPU workloads.
Our reuse distance analysis suggests that using additional
information at the application program level and at the runtime
can help distinguish memory references into different reuse
categories effectively. Our characterization results also suggest
that a significant performance gain can be achieved if an
optimal cache line size is used.
The characterization results in this paper show that GPGPU
applications suffer from low spatial locality in general. This
along with an application’s inherent degree of memory divergence results in bulk of the default 128B cache line being
unused. That is, GPGPU applications suffer from low spatial
utilization. We observe that the spatial utilization of cache lines
is highly correlated with the degree of memory divergence of
instructions that insert these cache lines into the cache.
With the observations presented in this paper, we propose
an Instruction and memory Divergence-based Cache management technique, called ID-Cache. ID-Cache bypasses data
that have low likelihood of reuse hits and insert data into
the cache with varying size granularities, i.e., 32-byte, 64byte, or 128-byte. We observe that instruction program counter
and the memory divergence pattern correlate well with reuse
distance characteristics. To take advantage of this insight, we

158

build a simple predictor that learns the reuse distance patterns
of instructions and memory divergence degrees, and bypass
data that are not likely to receive future hits from the L1
data cache. Furthermore, we also observe that applications
prefer different cache line size configurations and the memory
divergence patterns correlate well with the optimal cache line
size configuration. To take advantage of this insight, we insert
cache lines with varying size granularities based on the degree
of memory divergence.
We evaluate ID-Cache with a wide variety of GPGPU workloads on GPGPU-Sim v3.2.2. Overall, ID-Cache improves the
performance of high cache and bandwidth sensitive workloads
by an average of 71% and by as much as 4.48x (KMN). IDCache has negligible impact on the performance of other
GPGPU workloads that are not sensitive to either cache or
bandwidth capacities. By managing the on-chip resources
effectively, ID-Cache achieves 90% of the performance improvement made by doubling the L1 cache sizes and the
interconnect bandwidth capacity.
The key observations and contributions of this paper are:
• Our characterization shows that the reuse behavior of
cache lines is highly correlated to program instructions
and an instruction’s memory divergence pattern. These
information, when used intelligently, can improve the performance of the GPGPU memory subsystem significantly
(Section V).
• We also demonstrate that cache lines in GPGPU applications suffer from low spatial utilization and the
spatial utilization of cache lines is well correlated to
the degree of memory divergence of an instruction. This
can be leveraged to dynamically insert cache lines of
varying size granularities to improve cache capacity and
interconnect bandwidth utilization (Section VI).
II. BACKGROUND
GPU instructions are executed in the SIMT manner. That
is, depending on the width of the SIMD unit, multiple threads
execute the same instruction on different data concurrently. For
example, in nVIDIA Fermi GPUs, 32 threads are grouped into
a single warp (wavefront in AMD terminology) for execution
concurrently. The SIMT execution model presents a number of
challenges in designing efficient memory systems for GPUs.
In case of memory load or store instructions, every single
thread in a warp could potentially request for data located in a
different cache line from the memory system simultaneously.
In other words, up to 32 individual pieces of data could be
demanded and brought into the cache (memory divergence).
This places immense pressure on the memory subsystem
of GPUs. To mitigate some of this pressure, GPUs employ
coalescers that try and merge multiple requests from a single
warp or a few warps that are executed close in time.
On one hand, due to the large number of memory requests
that are presented to the GPGPU memory systems, the data
caches in GPGPUs tend to get thrashed often. On the other
hand, GPGPU applications experience minimal spatial locality
and spatial utilization of cache lines. These factors result
in glaring inefficiencies in the current GPGPU cache and

TABLE I
GPGPU- SIM SIMULATION CONFIGURATIONS .

Architecture
Num. of SMs
Max. # of Warps per SM
Max. # of Blocks per SM
# of Schedulers per SM
# of Registers per SM
Shared Memory
L1 Data Cache
L1 Inst Cache
L2 Cache
Min. L2 Access Latency
Min. DRAM Access Latency
Warp Size (SIMD Width)
Warp Scheduler

NVIDIA Fermi GTX480
15
48
8
2
32768
48KB
16KB per SM (32-sets/4-ways),
LRU
2KB per SM (4-sets/4-ways), LRU
768KB unified cache (64-sets/16ways/6-banks), LRU
120 cycles
220 cycles
32 threads
GTO [5]

TABLE II
B ENCHMARK INFORMATION .

Abbr
BO
PTH
HOT
FWT
DCT
BP
NW
SR1
HTW
SC
BT
SR2
WC
PF
BC
MIS
PVR
BFS
SS
CLR
CSR
STR
FLD
MM
ELL
PRK
KMN

Application
Binomial Options [6]
Path Finder [7]
Hotspot [7]
Fast Walsh Trans. [6]
Discreet Cosine Trans. [6]
Back Propagation [7]
Needleman-Wunsh [7]
SRAD1 [7]
Heartwall [8]
Streamcluster [7]
B+Tree [7]
SRAD2 [7]
Word Count [9]
Particle Filter [7]
Betweenness Centrality [10]
Maximal Ind. Set [10]
Page View Rank [9]
Breadth First Search [7]
Similarity Score [9]
Graph Coloring [10]
Dijkstra-CSR [10]
String Match [9]
Floyd Warshall [10]
Matrix Multiplication [9]
Dijkstra-ELL [10]
Pagerank (SPMV) [10]
K-Means [7]

Input
512 Options
100k nodes
512x512 nodes
32k samples
10 blocks
65536 nodes
1024x1024 nodes
502x458 nodes
656x744 AVI
32x4096 nodes
1M nodes
2048x2048 nodes
86kB text file
28x128x10 nodes
1K (V), 128K (E)
ecology
1M data entries
65536 nodes
1024x256 points
ecology
USA road NY
165k words
256(V), 16K (E)
1024x1024
USA road NY
Co-Author DBLP
494020 objects

Cat.

C/I-L

C/I-H

interconnect management methods. In this paper, we focus
on these important sources of inefficiencies and demonstrate
ways, through which data cache capacity and the interconnect
bandwidth on GPGPUs can be utilized more efficiently.
III. M ETHODOLOGY
A. Simulation Infrastructure and Configurations
We use GPGPU-sim simulator (version 3.2.2) [11] to characterize the behavior of the GPGPU memory subsystems.
GPGPU-sim is a cycle-level performance simulator that models a general-purpose GPGPU architecture. We run GPGPUsim with the default configuration representing the NVIDIA
Fermi GTX480 architecture [12]. Table I shows the detailed
configuration of our experimental setup.

159

Speedup over Baseline

3.5
3.0

Baseline

2x L1D$

2x BW

2x L1D + 2x BW

2.5
2.0
1.5

1.0
0.5

C/I-H
C/I-H

C/I-L
C/I-L

Fig. 2. GPGPU application performance sensitivity to data cache capacity and interconnect bandwidth. The different columns follow the format of “Data
Cache Capacity/Interconnect Bandwidth”.

B. GPGPU Benchmarks
We select a broad set of GPGPU applications from the
Mars [9], NVIDIA SDK [6], Pannotia [10], and Rodinia [7],
[8] benchmark suites to represent the diverse behavior present
in GPGPU workloads. We take these GPGPU applications
to quantify and evaluate the efficiency of memory subsystem designs. We classify the applications into two categories based on the speedup achieved when both the L1
data cache capacity and interconnect bandwidth are doubled
— cache/interconnect insensitive (C/I-L) (speedup < 1.2x),
and highly cache/interconnect sensitive (C/I-H) (speedup >
1.2x). Table II lists the details of these benchmarks and their
input data sets. We simulate all benchmarks, except three,
to completion1 . In the interest of space, we will present detailed characterization and analysis for the C/I-H benchmarks
throughout this paper, while only presenting results for C/I-L
benchmarks when necessary for completion.
IV. S ENSITIVITY TO C ACHE C APACITY AND
I NTERCONNECT BANDWIDTH
GPGPUs operate within the massive multithreading
paradigm. For example, on the Kepler and Maxwell architectures released in 2012 and 2014 [13], [14], more than 2000
concurrent threads are supported on a streaming multiprocessor. These threads share a 16KB L1 data cache resulting in
as few as 8B data capacity per thread. Similarly, the massive
multithreading operation also puts immense pressure on other
parts of the memory system, such as the on-chip and off-chip
interconnect bandwidth. This makes data cache capacity and
on-chip interconnect critical resources for GPGPUs. Figure 2
shows the performance sensitivity of GPGPU workloads to L1
data cache capacity and interconnect bandwidth. The x-axis
represents the wide variety of GPGPU applications studied in
this paper and the y-axis represents the execution time speedup
normalized to the baseline configuration (16KB L1 data cache
with 32B Flits interconnect configuration). The first bar represents the baseline configuration (always 1). The second and
third bars represent settings where the data cache capacity
and interconnect bandwidth are doubled separately, 2x L1D$
and 2x BW (with 64B Flits interconnect configuration) respectively. Finally, the last bar represents a configuration where
1 To keep the simulation times manageable, we restrict CSR, ELL and KMN
to one billion instructions.

both the data cache capacity and interconnect bandwidth are
doubled together (2x L1D$ + 2x BW).
As one would expect, the C/I-L applications in Table II
experience negligible performance impact with the increase in
cache capacity or bandwidth. The C/I-H applications (Table II)
on the other hand are highly sensitive to data cache capacity
and interconnection bandwidth. For these applications, 89%
performance improvement is obtained when both the cache
and interconnection bandwidth capacities are doubled.
Data streaming applications are more sensitive to the bandwidth than cache capacity. Streaming applications such as SC,
SR2, and PVR receive negligible performance improvement
with the 2x L1D$ + 2x BW configuration while receiving
significant speedup with the 2x BW configuration shown in
Figure 2. On the other hand, applications that possess cache
friendly access patterns benefit significantly from the increased
cache capacity. Applications such as BC and STR in Figure 2
exemplify this scenario. Nonetheless, most applications in the
C/I-H category are sensitive to both data cache capacity and interconnect bandwidth resources Since simply increasing cache
and bandwidth capacities is not always a viable option, we
explore and propose simple designs to optimize the utilization
of the two critical resources together in this paper.
V. T OWARDS E FFECTIVE C ACHE B YPASSING
A. Inefficient Cache Utilization in GPGPUs
In the default 16KB L1 data cache, about 70% of the lines
allocated in the data cache do not receive reuse before being
evicted. Although this might seem to indicate that the GPGPU
applications do not possess data locality that can be exploited
by the caches, our analysis suggests otherwise. Figure 3 shows
the comparison of the fraction of zero reuse cache lines in
the default 16KB L1 data cache (black bars) and that in a
4x larger 64KB cache (orange diagonal bars). On average,
the fraction of zero reuse lines reduces from 75% to 40%
for the C/I-H benchmarks. The large disparity between the
number of zero reuse cache lines in the two configurations
illustrates that data locality exists in GPGPU workloads but
the baseline architecture is unable to effectively capture it. The
main reason is the commonly-observed mixed reuse patterns
in GPGPU applications. That is, due to massive multithreading
execution, lines that possess excellent reuse behavior are often
interleaved with lines that do not. This results in lines being
evicted before they can be reused, wasting precious cache
capacity and interconnect bandwidth.

160

16KB	L1	Data	Cache	

64KB	L1	Data	Cache	

Distribu(on	of	Reuse	
Distance	

Frac%on	of	Zero	Reuse	
Cache	Lines	

1	

0.5	

0	

65536	
16384	
4096	
1024	
256	
64	
16	
4	
1	

Load	Instruc(ons	

Fig. 3. The fraction of zero reuse cache lines in the baseline 16KB and 4x
larger 64KB data cache configurations. Zero reuse cache lines are cache lines
that are brought to the cache but never reused.

Fig. 5. The distribution of L1 data cache reuse distance with different load
instructions in ELL. The grey diamonds represent the median reuse distances.
The high-low bars represent 75th and 25th percentile reuse distances.
14	

4096	
Distribu(on	of	Reuse	
Distance	

Reuse	Distance	

1024	
256	
64	
16	
4	
1	

12	
10	
8	
6	
4	
2	
0	
Degree	of	Memory	Divergence	

Fig. 4. The distribution of L1 data cache reuse distances seen in GPGPU
benchmarks. The grey diamonds represent the median reuse distances. The
high-low bars represent 75th and 25th percentile reuse distances.

Fig. 6. The distribution of L1 data cache reuse distance with degree of
memory divergence of a load instruction, i.e., ELL’s PC_3. The grey diamonds
represent the median reuse distances. The high-low bars represent 75th and
25th percentile reuse distances.

B. Diverse Reuse Distance Patterns
To further investigate the memory reuse patterns, we analyze
the reuse distances seen in GPGPU applications. Figure 4
shows the distribution of reuse distances for all memory
requests in the C/I-H category. The median reuse distance is
depicted by the diamonds whereas the 75th and 25th percentile
reuse distances are shown by the high and low markers
respectively. We can observe that across the wide variety of
applications, the reuse distance of data is extremely diverse.
Due to the long reuse distances, some GPGPU applications
might appear as streaming applications under the default
cache configuration e.g., PVR and CSR. Furthermore, Figure 4
demonstrates the mixed reuse behavior discussed above. That
is, for many applications, some of the requests have reuse
distances less than four which is the associativity of the
baseline data cache, while other requests do not. Under such
a scenario, the requests that have low reuse distance could
experience interference from other requests that have long
reuse distances resulting in a loss of locality leading to higher
cache misses. Therefore, it is important to protect these low
reuse distance requests from the interference originating from
other long reuse distance requests. This turns out to be a nontrivial task in the massive multithreading operating paradigm.
C. Using Instruction Program Counter to Classify Requests
The problem of identifying, separating, and protecting
memory references that have excellent locality from other
references that do not, has been investigated in the context
of CMP caches. A common piece of information that has
been well explored for this purpose is the instruction program
counter (PC) [15], [16], [17], [18], [19], [20] All of the above
works exploit the property that the reuse behavior, or the
lack thereof, of cache lines brought by a particular load is
typically homogeneous. This property is a result of the typical
features of structured programming. For example, during each

iteration of a loop, a load instruction will access data that is
indexed by some function of the iteration number in a strided
fashion. If this access stride is smaller than the cache line
size, all the cache lines brought by this load will receive a
reuse hit due to spatial locality. Similarly, if we consider any
producer-consumer relationship, an instruction that is part of
the producer would typically bring a cache line into the cache
and update it before the data is used by the consumer. This
process could result in cache hits due to temporal locality. On
the other hand, if the access stride is too long, or if there is
a large number of conflicting accesses between when a piece
of data is produced and consumed, the data that is brought to
the cache will not be reused.
Following the same insight, a few recent works in GPGPU
cache designs have also proposed to use instruction specific
information to identify the reuse behavior of different load
instructions. Many of these works use compiler analysis to
learn the reuse behavior of specific load instructions [2],
[21], [22]. These works either require significant support from
the compiler or offline profiling to work well. The adaptive
bypassing method proposed by Tian et al. [23] utilized the last
touched PC information to predict zero reuse cache lines and
bypass them from the cache. Their design achieves a modest
3% performance improvement.
To understand how PC information can potentially be used
to segregate memory references in C/I-H kind of GPGPU
applications, we analyze the reuse distance distribution of
memory references from the different PCs. Figure 5 shows
the reuse distance distribution of memory references from the
different PCs in an example application, ELL. From the figure,
we can make a couple of important observations. The reuse
distances of PCs in an application vary widely. A significant
part of memory references brought in by a small set of

161

Speedup over Baseline

16KB L1D $ (baseline)
2.5

PC-Based

PC+Div-Based

32KB L1D $

2
1.5
1

0.5

Fig. 7. Performance of GPGPU applications with offline trained PC-Based
and PC+Div-Based bypassing methods.

PCs have long reuse distances, e.g., PC_4, PC_5, and PC_6.
Such reuses cannot be captured by caches of realistic sizes
and hence should be bypassed from the cache. On the other
hand, the applications also possess certain PCs whose memory
requests have short reuse distances which could potentially
result in cache hits. Memory references from such PCs should
be preserved in the cache.
Furthermore, while memory references associated with a
particular PC exhibit “similar” reuse distance for a majority
of PCs in an application, there are other PCs that do not
follow this pattern, e.g., PC_1 and PC_2 in ELL. For these
instructions, some references exhibit short reuse distances
that could result in cache hits, while others, although in the
same PC bucket, exhibit long reuse distances. Therefore, we
need additional information to achieve a finer resolution in
segregating memory references, such that, references that can
potentially receive reuse hits are protected in the cache while
others are bypassed more intelligently.
D. Using Memory Divergence Behavior to Classify Requests
Memory divergence is a property unique to GPGPU applications, that could have a pronounced impact on the reuse
behavior of an application. As the degree of memory divergence increases, the number of memory requests sent to the
cache also increase. Consequently, the reuse distances of lines
inserted by divergent instructions are typically longer. We
use this insight to segregate memory references that exhibit
short reuse distances from others that do not. Specifically we
analyze the relationship between reuse distance and degree
of memory divergence of load instructions. Figure 6 shows
the reuse distance distribution for memory references from
a specific PC in ELL. We can see that with the help of
the memory divergence information, we can further separate
memory references in a finer granularity such that we can
identify specific memory references that have similar reuse
distances. That is, in the example of ELL’s PC_3 (Figure 6),
we observe that a memory load instruction with a high degree
of memory divergence, e.g., PC_3 that generates more than 15
requests in the case of ELL, typically all have reuse distances
greater than four, i.e., the set associativity of the baseline
cache. Such requests are unlikely to receive reuse hits in the
cache and therefore are candidates for bypassing.
E. PC and Memory Divergence Pattern Guided Bypassing
The observations made in the previous sections motivate the
exploration of using PC and the memory divergence patterns to

manage cache bypassing in order to improve cache utilization
in GPGPUs. We design and evaluate two simple bypassing
techniques — a PC (PC-only) based method and a combined
PC and degree of divergence (PC+Div) based method.
1) Offline-Trained Bypassing: To quantify the performance
potential enabled by the consideration of the PC and memory divergence information, we obtain the reuse distance
distribution for PC-only and PC+Div offline. Then, in the
second pass, we use the reuse distance information obtained
to guide the cache line bypassing decision. To compensate
the effect of bypassing on the reuse distance distribution
obtained offline, the bypassing decision is made when the
reuse distance of a PC is larger than 8 for the 4-way set
associative L1 data caches. Then, for the PC+Div bypassing
technique, a second filter is applied by using the instruction’s
degree of memory divergence. That is, for each PC, if the
reuse distance for memory references with a particular degree
of memory divergence d, is higher than 6, then the future
memory references from that PC, having degree of memory
divergence greater than d are bypassed from the cache.
2) Performance Results for Offline-Trained Bypassing:
Figure 7 shows the performance of the PC-only and PC+Div
based bypassing methods. The PC-only and PC+Div methods
result in an average speedup of 14% and 17%, respectively, for
C/I-H workloads. As can be expected, both techniques have
negligible impact on the C/I-L workloads. When compared to
a 32KB cache, the PC-only and PC+Div based methods can
bridge the performance gap between 16KB and 32KB caches
by 29% and 35%, respectively.
Adding an additional layer of information, namely, the
memory divergence patterns, can help prune the incoming
memory requests in a finer granularity. The benefit of doing
so can be witnessed in workloads such as STR, ELL and
PRK. For these workloads, PC+Div based method improves the
performance by a significant extent than the PC-only method.
In the case of STR, while the PC-only approach does not
bypass any requests and performs exactly the same as the
baseline, the PC+Div based approach bypasses 10% of the
requests, translating to a significant 19% performance gain.
The performance of the PC-only and PC+Div based approaches also depends on the aggressiveness of bypassing that
is carried out. This aggressiveness is dictated by the reuse
distance thresholds used in the bypassing decision. If the
aggressiveness is too low, then not many requests are bypassed
from the cache and this results in applications performing
exactly the same as the baseline, e.g., MIS, FLD, and KMN
in Figure 7. On the other hand, if the aggressiveness is too
high, it could result in useful requests being bypassed from the
cache. Due to this reason, applications such as PF experience
an 11% performance degradation with both the PC-only and
PC+Div based methods.
Thus far, we have used the offline profiled reuse distance
information to evaluate the performance potential for using
PC and memory divergence information to guide bypassing
decisions. We go a step further and implement simple online
PC-only and PC+Div-based bypassing designs which learn the

162

Speedup	
  over	
  Baseline	
  

16KB	
  L1D	
  $	
  (baseline)	
  
2.5	
  

PC-­‐Based	
  

PC+Div-­‐Based	
  

32KB	
  L1D	
  $	
  

2.0	
  
1.5	
  
1.0	
  
0.5	
  
0.0	
  

Fig. 8. Performance of GPGPU applications with online PC-Based and
PC+Div-Based bypassing designs. These designs require no offline training.

reuse distance characteristics dynamically.
3) Dynamic Bypassing: For the PC-only and PC+Div based
bypassing techniques to be practical, the reuse distance information must be learned dynamically at runtime. To do so, we
design a 128-entry reuse distance prediction table of saturating
counters for the PC-only scheme. In case of the PC+Div
scheme, for each PC, we use four bins to learn the reuse
behavior of instructions with different divergence degrees.
Therefore, we use a 512-entry reuse distance prediction table
of saturating counters for the PC+Div scheme. The table learns
and predicts a PC’s reuse characteristics in a similar manner as
a recent prior work [17]. The prediction table is indexed by the
lower 7 bits of an instruction PC and the entry value indicates
the predicted reuse behavior of the instruction. Algorithm 1
describes the learning and prediction steps of the design.
4) Performance Results for Dynamic Bypassing: Figure 8
shows the performance improvement achieved by the online PC-only and PC+Div based bypassing designs described
above. On average, the online PC-only and PC+Div-based
designs improve performance by 29% and 22%, respectively
for C/I-H workloads and have negligible impact on the C/IL workloads. Since the online PC-only and PC+Div based
designs can adapt to the runtime changes in reuse behavior
of different load instructions, the online approaches perform
better than the offline trained ones in Figure 7 for a number

100%	
%	of	Cache	Lines	

Algorithm 1 The reuse prediction algorithm for PC-only and
PC+Div based bypassing.
1: function P REDICTORU PDATE (request, access.status)
2:
if P C − only then
3:
index ← hash(request.Inst.P C)
4:
else
5:
index ← hash(request.Inst.P C,
request.Inst.deg_divergence)
6:
if access.status = M ISS then
7:
if table[index] > 0 then
8:
Decision ← IN SERT
9:
else
10:
Decision ← BY P ASS
11:
else if access.status = HIT then
12:
table[index] ← table[index] + 1
13:
else if access.status = EV ICT then
14:
if Evicted_line.Reused = F alse then
15:
table[index] ← table[index] − 1

80%	

75-100%	Used	

60%	

50-75%	Used	

40%	
20%	

25-50%	Used	
0-25%	Used	

0%	

Fig. 9. The distribution of spatial utilization of cache lines. The stacks
represent different spatial utilization categories measured at the granularity
of 32B and the y-axis represents the percentage of cache lines that belong to
a particular utilization category, e.g., “0-25% Used” represents the fraction of
cache lines with 0-25% spatial utilization.

of applications, such as PF, SS, CSR, and KMN. Furthermore,
both the online PC-only and PC+Div based designs achieve
more than 50% of the performance benefit brought by a 32KB
cache for the C/I-H workloads.
The benefit from the degree of divergence information is
more modest in the case of the online PC+Div based design.
The workloads that received the highest benefits from the degree of divergence information in the offline trained approach
(STR, ELL, and PRK) fail to benefit from the same information
in our simple online design described here. A more advanced
design that accurately captures the reuse distance behavior
across the different divergence degrees is necessary.
VI. T OWARDS E FFICIENT C ACHE L INE S IZE S ELECTION
A. Inefficient Cache and Bandwidth Utilization due to Default
Fixed Cache Line Size Configuration
Having addressed the problem of zero reuse cache lines in
Section V, we focus this section on another key contributor
to suboptimal memory system performance in GPGPUs. We
observe that a large part of the bytes in the default 128B cache
line remain unused. We refer to this problem as low spatial
utilization problem. Low spatial utilization results in wastage
in precious cache capacity and bandwidth resources.
To understand the spatial utilization of cache lines, we analyze the fraction of bytes utilized in each cache line throughout
the execution period of GPGPU applications. Figure 9 shows
the utilization distribution of data in the cache line granularity
for the C/I-H GPGPU applications. From Figure 9, we can
observe that while the cache line spatial utilization is fairly
high for C/I-L applications, for C/I-H applications, an opposite
behavior is observed—more than 70% of cache lines have less
than 25% spatial utilization. For an overwhelming majority of
the cache lines, at least 75% of the data is brought to the cache
and is stored unnecessarily. This inefficient use of cache and
bandwidth capacities presents an opportunity for performance
optimization for GPGPUs.
B. Performance Gain with Finer Cache Line Granularity
One way to minimize cache capacity and bandwidth
wastage due to low spatial utilization is to reduce the cache
line size. For the same purpose, the L1 data cache can even
be turned off.Figure 10 shows the performance of GPGPU
applications with different cache line sizes and with the L1
data caches turned off. Each of the configuration shows bipolar
behavior across the different kinds of workloads. For example,

163

Speedup	over	Baseline	

4	
3.5	
3	
2.5	
2	
1.5	
1	
0.5	
0	

L1D$	Turned	Oﬀ	

32B	

64B	

128B	(baseline)	

C/I-L	
C/I-I	

C/I-H	

the 32B line size configuration provides performance speedup
that varies from -50% (FWT) to as much as 3.7X (KMN).
Similar behavior can be observed for the 64B cache line size
configuration as well as when L1 data cache is turned off. This
performance variability is undesirable.
The performance variability arises from the fact that the
spatial utilization of cache lines varies significantly from one
application to another, as seen in Figure 9. Furthermore, as
Figure 11 shows, the spatial utilization of cache lines can vary
significantly within an application over its execution as well.
Therefore, there is not a simple way to select a cache line size
that minimizes cache capacity wastage and bandwidth usage
while ensuring there is no performance degradation. Therefore,
there is a need for a dynamic method that predicts and inserts
cache lines with an optimal line size configuration.
C. Minimal Spatial Locality in GPGPUs
We first analyze the sources of cache line spatial utilization.
In a GPGPU system, the data within a cache line could be
consumed in two ways: 1) Future reuses to the cache line
consume data that was not consumed upon the line’s insertion (traditionally, spatial locality) 2) Requests from multiple
threads in a warp are coalesced together to consume adjacent
data upon a line’s insertion. To delve deeper, we look at the
significance of each of these in GPGPU applications.
In Figure 12 we show a breakdown for all cache accesses
by separating them into misses, temporal hits, spatial hits and
mixed hits. Temporal hits signify the situation where all data
that is reused on a particular access have been touched before.
Spatial hits on the other hand represent the hits where all
data that is being touched for the first time. Similarly, mixed
hits refer to the case where a part of the data that is being
reused has been touched before and the rest has not. From
Figure 12, we can observe that the fraction of accesses that
result in hits due to spatial locality (spatial hits and mixed
hits) are a mere 7% on average for both C/I-L and C/I-H
workloads, respectively. That is, the amount of spatial locality
that is exploited in a GPGPU system is minimal.
D. High Correlation between Cache Line Spatial Utilization
and Memory Divergence Patterns
Although the coalescer attempts to combine requests from
multiple threads that access adjacent data together, it might
not always be successful in doing so. This results in memory
divergence. The degree of memory divergence is a property
of GPGPU applications that affects the spatial utilization of

%	of	Cache	Lines	

Fig. 10. The performance of GPGPU applications with different L1 cache line sizes and turning off L1 data caches.
0	-	25%	
100%	

25	-	50%	

50	-	75%	

75	-	100%	

80%	
60%	
40%	
20%	
0%	
Time	(every	10,000	misses)	

Fig. 11. Cache line spatial utilization over time (an example from PVR).

cache lines significantly. For example, when the degree of
memory divergence is one (i.e., a convergent instruction) and
each thread accesses 4B data, all 128 bytes of the cache line
are utilized, leading to 100% spatial utilization. However, on
the other extreme, when the degree of memory divergence is
32, 1 to 8 bytes (depending on the access data type) of the
cache line would be used, leading to low spatial utilization.
We investigate the relationship between cache line spatial
utilization and the memory divergence patterns by analyzing
the variation of spatial utilization for cache lines that are
inserted by instructions of different degrees of divergence.
Figure 13 shows the variation of spatial utilization of cache
lines with varying degree of memory divergence for one
example application — PVR. We notice that most of the cache
lines brought to the cache by instructions having lower degrees
of divergence (e.g. 1-8) have higher spatial utilization. On
the other hand, most of the cache lines that are brought in
by instructions with high degrees of divergence have much
lower utilization. It is apparent that the spatial utilization has
a fairly predictable behavior with respect to the degree of
divergence2 . Therefore, we leverage this piece of information
to dynamically optimize both cache capacity utilization and
bandwidth consumption.
E. Divergence Guided Adaptive Line Size Insertion (ALSI)
Based on the observations made in the previous sections, we
arrive at the intuition that both cache capacity and interconnect
bandwidth utilization can be optimized together by inserting
cache lines of different sizes based on their spatial utilization.
A similar intuition was used by Rhu et al. to optimize for
bandwidth consumption [24]. We will highlight the differences
between their work and ours in detail in Section VIII. In
2 We also evaluate the correlation between PC and spatial utilization and
find that degree of low spatial utilization to be more closely related to the
degree of memory divergence.

164

Temporal	
  Hit	
  

Spa7al	
  Hit	
  

Baseline	
  

Mixed	
  Hit	
  
Speedup	
  over	
  Baseline	
  

	
  %	
  of	
  All	
  Accesses	
  

Miss	
  
100%	
  
80%	
  
60%	
  
40%	
  
20%	
  
0%	
  

%	of	All	Cache	Lines	

Fig. 12. The distribution of cache hits and misses in GPGPU applications.

0-25%	Used	
100%	

25-50%	Used	

50-75%	Used	

3	
  
2.5	
  
2	
  
1.5	
  
1	
  
0.5	
  
0	
  

ALSI	
  

Sta4c	
  Best	
  Line	
  Size	
  

4.48x	
   3.76x	
  

Fig. 14. The performance improvement under Adaptive Line Size Insertion.

75-100%	Used	

Algorithm 2 Algorithm for Divergence Based Adaptive Line
Size Selection (ALSI)
function L INE S IZE S ELECTION(request)
if request.Inst.divergence_deg = 1 then
Line_Size ← 128B
if request.Inst.divergence_deg < 4 then
Line_Size ← 64B
else
Line_Size ← 32B

75%	
50%	
25%	
0%	
1	 3	 5	 7	 9	 11	 13	 15	 17	 19	 21	 23	 25	 27	 29	 31	
Degree	of	Memory	Divergence	

Fig. 13. The distribution of L1 data cache utilization vs degree of memory
divergence (x-axis) for PVR application. The stacks represent different spatial
utilization categories measured at the granularity of 32B and the y-axis
represents the percentage of cache lines that belong to a category.

order to store data of variable size granularities, e.g., 32B,
64B, or 128B, we modify the L1 data cache architecture. We
use a previously-proposed cache architecture, called Amoeba
cache [25]. Amoeba cache is a cache architecture proposed
for CMP LLCs that treats each cache set as an array of small
blocks (8B size each) that can be used to hold either tag or
data information. Therefore, a cache line of any size can be
held in the cache using a set of contiguous blocks.
We design a simple divergence based approach for Adaptive
Line Size Insertion (ALSI). We modify the cache line size
dynamically at runtime based on the degree of memory
divergence. Specifically, we assume that as the degree of
memory divergence increases, the spatial utilization of cache
lines reduces and therefore cache lines are inserted using a
smaller line size configuration, e.g., 32B. On the other hand,
when the load instruction is convergent, the spatial utilization
of cache lines is likely to be 100% and therefore cache lines are
inserted using the 128B line size configuration. The algorithm
for ALSI is described in Algorithm 2.
F. Performance Results for ALSI
Figure 14 shows the performance improvement achieved
by ALSI. On average, ALSI improves the performance of
the C/I-H applications by 64% and does not affect/degrade
the performance of the C/I-I applications. Furthermore, we
compare the performance of ALSI to a static, best line size
configuration i.e., a per-application line size configuration
(32B, 64B, or 128B) which gives the best per-application
performance. ALSI performs almost as well as the static best
line size configuration and achieves 96% of the performance
gain given by the optimal setting. This shows that degree of

divergence information can be used effectively to predict the
spatial utilization and hence the insertion cache line size. Furthermore, a runtime adaptive system such as ALSI would be
able to capture the change in spatial utilization over different
application phases. This results in ALSI outperforming the
static best line size configuration for a number of workloads
such as PVR, SS, and KMN. In summary, we demonstrate that
a simple divergence based line size insertion can give robust
performance improvement.
VII. P UTTING I T A LL T OGETHER
Thus far, we describe two designs—PC+Div-based bypassing and adaptive line size insertion (ALSI)—that improve
the efficiency of the memory subsystem by minimizing zero
reuse lines (Section V) and increasing spatial utilization (Section VI), respectively. Since cache bypassing and variable line
size insertion are closely related to each other, we integrate
the two designs together to jointly optimize the performance
of the memory subsystem. We propose ID-Cache to improve
the performance of the L1 data caches and the interconnect bandwidth utilization. ID-Cache is a simple design that
optimizes the GPU performance by using instruction-related
information, i.e., the reuse distance characteristics and memory
divergence behavior of instructions. ID-Cache improves the
cache capacity utilization by bypassing memory requests from
instructions that generate long reuse requests and have high
degree of memory divergence. This component is the PC+Div
based bypassing design described in Section V and is called
ID-Cache Bypass. Furthermore, for lines to be inserted into
the cache, ID-Cache uses the degree of memory divergence
to determine the size configuration and thereby improves the
utilization of precious cache capacity and the interconnect
bandwidth. This component is the ALSI design described in

165

ID-Cache Bypass

Speedup over Baseline

Algorithm 3 Algorithm for ID-Cache Bypass and ALSI
selection logic
1: function B YPASS /I NSERT L INE(request, access)
Bypass ← P redictBypass()
2:
. Predict whether to bypass or not based on Algorithm 1
3:
if Bypass = T RU E then
4:
access.bypass ← T RU E
5:
. Take bypass path for this access
6:
else
7:
. Insert line to cache
8:
LineSize ← P redictLineSize()
9:
. Predict Line Size according to ALSI Algorithm
2
10:
access.line_size ← LineSize
11:
. Complete cache line insertion with predicted line
size

ID-Cache

2x L1D + 2x BW
2.7x

2.50

4.48x
4.48x
3.4x

2.25
2.00
1.75
1.50

1.25
1.00
0.75
0.50

Fig. 15. Performance improvement with ID-Cache.

ID-­‐Cache	
  

Interconnect	
  Busy	
  Stall	
  
1	
  

Interconnect	
  Busy	
  Stall	
  	
  
Normalized	
  to	
  Baseline	
  

Hit	
  Rate	
  (%)	
  

Baseline	
  
100	
  

Section VI. The pseudo-code implementation of ID-Cache is
described in Algorithm 3.
A. Performance Evaluation for ID-Cache
Overall, when compared with the baseline architecture, IDCache achieves an average of 71% performance improvement
for the cache and bandwidth capacity sensitive workloads, as
Figure 15 shows. The significant performance gain from IDCache matches 90% of that from a GPU with doubled cache
and bandwidth capacities (2x L1D$ + 2x BW). Figure 15 also
shows the performance of ID-Cache’s component policies—
ID-Cache Bypass and ALSI—individually. Generally, IDCache performs better than its component policies. Combining
a more intelligent bypassing scheme and adaptive cache line
size insertion results in added performance gain for most of the
workloads. For a few workloads, i.e., SS, CSR and MM, while
ID-Cache improves the performance, the performance gain is
lower than that of its component policy, ALSI. This is because
the reuse behavior changes with the varying cache line sizes
and a simple bypass predictor (ID-Cache Bypass) is unable to
learn the changing reuse behavior introduced by the varying
cache line sizes inserted by ALSI, leading to bypassing cache
lines too aggressively.
To understand the source of the large performance gain
brought by ID-Cache, we take a closer look at the cache and
interconnect performance. Figure 16 shows that L1 data cache
hit rate improvement and the reduction in interconnect busy
stalls, that is achieved by ID-Cache. The hit rate improvement
signifies the improved utilization of cache capacity under IDCache. On the other hand, the reduction in interconnect busy
stalls demonstrates ID-Cache’s improved bandwidth utilization. ID-Cache increases L1D cache hit rate by 10% and
reduces interconnect busy stalls by 60% for C/I-H workloads.
This shows that, by utilizing program level information such
as instruction PC and runtime information such as memory
divergence patterns intelligently, the performance of GPGPU
applications can be significantly enhanced.

ALSI

80	
  

0.8	
  

60	
  

0.6	
  

40	
  

0.4	
  

20	
  

0.2	
  

0	
  

0	
  

Fig. 16. L1 data cache hit rate improvement and interconnect busy stall
reduction provided by ID-Cache.

VIII. R ELATED W ORK
In order to alleviate cache thrashing and resource contention, many prior works focused on designing cache bypassing algorithms tailed-made for GPGPUs. Jia et al. [2], Xie
et al. [3], and Liang et al. [21] proposed using compilers to
perform offline analysis and identify memory regions which
have long reuse distances. These memory regions are then
bypassed from the cache. Jia et al. [4], Chen et al. [26],
and Khairy et al. [27] demonstrated that bypassing memory
accesses whenever resource contention is detected could effectively improve the performance of GPGPUs. Tian et al. [23]
built additional hardware in L2 caches to collect and predict
the reuse pattern of L1 cache, whereas Li et al. [28] proposed
using decoupled tag arrays to calculate the reuse distance.
Lee et al. [29] proposed CAWA which uses instruction level
information to predict reuse distance. The goal of CAWA is
to accelerate the performance of the critical, i.e., the slowest
running, warp within a thread block. In contrast, all these
cache bypassing and modified insertion/replacement schemes
only take temporal locality into account. Our work characterizes the efficacy of utilizing program level information such
as insertion PCs and runtime information such as memory
divergence to predict reuse behavior and the spatial utilization
patterns of cache lines.
Rhu et al. [24] observed that data caches in GPU usually
have low cache line utilization, i.e., only a small portion
of data within a cache line are referenced during the line’s
lifetime. As a result, a large amount of data traffic across
the interconnection is redundant. Thus, the authors proposed

166

LAMAR, a low overhead bloom filter and sectored cache
based design to (1) reduce data traffic by bringing and storing
segments of cache lines into the cache and (2) improve the
energy efficiency by turning off the unused portion of the
caches. Our paper, on the other hand, demonstrates that there
is minimal amount of spatial locality that can be exploited
in a wide variety of GPGPU applications and the spatial
utilization of cache lines is highly correlated with the degree
of divergence. Thus, instead of bringing a smaller amount of
data based on first touched patterns, ID-Cache determines the
amount of data to bring and store in the L1 caches based on
memory divergence patterns.
IX. C ONCLUSION
In this paper we identified the sources of inefficiencies in
the memory subsystem of GPUs. Our analysis indicated that
there is an ample room for performance improvement which
can be achieved by effective management of GPU L1 data
caches and the interconnect bandwidth. We showed that the
reuse behavior of cache lines is well correlated with program
level information such as memory load/store instructions and
runtime information such as memory divergence patterns.
Based on the insights from our characterization results, we
design ID-Cache, a simple, yet effective, cache management
mechanism. ID-Cache identifies and bypasses zero reuse cache
lines intelligently while inserts data into caches with appropriate size granularities (ALSI). ID-Cache achieves a significant
71% performance improvement by improving the utilization of
the L1 cache and bandwidth capacities on GPUs. We hope the
insights, the detailed characterization results, and the designs
presented in this paper can be used to motivate future designs
and to improve the cache and bandwidth utilization efficiency
for GPGPU workloads.
ACKNOWLEDGMENT
The authors would like to thank the anonymous reviewers
for their insightful feedback. This work is supported in part
by the National Science Foundation (Grant #CCF-1618039)
and by Science Foundation Arizona under the Bisgrove Early
Career Scholarship. The opinions, findings and conclusions or
recommendations expressed in this manuscript are those of the
authors and do not necessarily reflect the views of the Science
Foundation Arizona.
R EFERENCES
[1] J. Hestness, S. W. Keckler, and D. A. Wood, “A comparative analysis of
microarchitecture effects on CPU and GPU memory system behavior,”
in International Symposium on Workload Characterization, 2014.
[2] W. Jia, K. A. Shaw, and M. Martonosi, “Characterizing and improving
the use of demand-fetched caches in GPUs,” in International Conference
on Supercomputing, 2012.
[3] X. Xie, Y. Liang, G. Sun, and D. Chen, “An efficient compiler framework for cache bypassing on GPUs,” in International Conference on
Computer-Aided Design, 2013.
[4] W. Jia, K. A. Shaw, and M. Martonosi, “MRPB: memory request prioritization for massively parallel processors,” in International Symposium
on High Performance Computer Architecture, 2014.
[5] T. G. Rogers, M. O’Connor, and T. M. Aamodt, “Cache-conscious
wavefront scheduling,” in International Symposium on Microarchitecture, 2012.
[6] NVIDIA, “CUDA C/C++ SDK code samples v4.0,” 2011. [Online].
Available: http://docs.nvidia.com/cuda/cuda-samples

[7] S. Che, M. Boyer, J. Meng, D. Tarjan, J. W. Sheaffer, S.-H. Lee, and
K. Skadron, “Rodinia: A benchmark suite for heterogeneous computing,”
in International Symposium on Workload Characterization, 2009.
[8] S. Che, J. W. Sheaffer, M. Boyer, L. G. Szafaryn, L. Wang, and
K. Skadron, “A characterization of the Rodinia benchmark suite with
comparison to contemporary CMP workloads,” in International Symposium on Workload Characterization, 2010.
[9] B. He, W. Fang, N. K. Govindaraju, Q. Luo, and T. Wang, “Mars:
A mapreduce framework on graphics processors,” in International
Conference on Parallel Architectures and Compilation Techniques, 2008.
[10] S. Che, B. M. Beckmann, S. K. Reinhardt, and K. Skadron, “Pannotia:
Understading irregular GPGPU graph applications,” in International
Symposium on Workload Characterization, 2013.
[11] A. Bakhoda, G. Yuan, W. W. L. Fung, H. Wong, and T. M. Aamodt,
“Analyzing CUDA workloads using a detailed GPU simulator,” in
International Symposium on Analysis of Systems and Software, 2009.
[12] NVIDIA, “NVIDIA GeForce GTX 480/470/465 GPU datasheet,” 2010.
[Online]. Available: http://www.nvidia.co.uk/docs/IO/90201/GTX-480470-Web-Datasheet-Final4.pdf
[13] NVIDIA,
“NVIDIA’s
next
generation
CUDA
compute
architecture:
Kepler
GK110,”
2012.
[Online].
Available: https://www.nvidia.com/content/PDF/kepler/NVIDIA-KeplerGK110-Architecture-Whitepaper.pdf
[14] NVIDIA, “NVIDIA GeForce GTX 750 Ti,” 2014. [Online].
Available:
http://international.download.nvidia.com/geforcecom/international/pdfs/GeForce-GTX-750-Ti-Whitepaper.pdf
[15] S. Khan, Y. Tian, and D. A. Jiménez, “Sampling dead block prediction
for last-level caches,” in International Symposium on Microarchitecture,
2010.
[16] S. Khan, D. A. Jiménez, D. Burger, and B. Falsafi, “Using dead blocks
as a virtual victim cache,” in International Conference on Parallel
Architectures and Compilation Techniques, 2010.
[17] C.-J. Wu, A. Jaleel, W. Hasenplaugh, M. Martonosi, S. C. S. Jr., and
J. Emer, “SHiP: Signature-based hit predictor for high performance
caching,” in International Symposium on Microarchitecture, 2011.
[18] A.-C. Lai, C. Fide, and B. Falsafi, “Dead-block prediction & deadblock correlating prefetchers,” in International Symposium on Computer
Architecture, 2001.
[19] R. Manikantan, K. Rajan, and R. Govindarajan, “NUcache: an efficient
multicore cache organization based on next-use distance,” 2011.
[20] G. Tyson, M. Farrens, J. Matthews, and A. R. Pleszkun, “A modified
approach to data cache management,” in International Symposium on
Microarchitecture, 1995.
[21] Y. Liang, Y. Wang, and G. Sun, “Coordinated static and dynamic cache
bypassing for GPUs,” in International Symposium on High Performance
Computer Architecture, 2015.
[22] X. Xie, Y. Liang, G. Sun, and D. Chen, “An efficient compiler framework
for cache bypassing on gpus,” in International Conference on ComputerAided Design, 2013.
[23] Y. Tian, S. Puthoor, J. L. Greathouse, B. M. Bechmann, and D. A.
Jiménez, “Adaptive GPU cache bypassing,” in Workshop on General
Purpose Processing Using GPUs, 2015.
[24] M. Rhu, M. Sullivan, J. Leng, and M. Erez, “A locality-aware memory
hierarchy for energy-efficient gpu architectures,” in International Symposium on Microarchitecture, 2013.
[25] S. Kumar, H. Zhao, A. Shriraman, E. Matthews, S. Dwarkadas, and
L. Shannon, “Amoeba-cache: Adaptive blocks for eliminating waste in
the memory hierarchy,” in International Symposium on Microarchitecture, 2012.
[26] X. Chen, L.-W. Chang, C. I. Rodrigues, J. Lv, Z. Wang, and W.-M. Hwu,
“Adaptive cache management for energy-efficient GPU computing,” in
International Symposium on Microarchitecture, 2014.
[27] M. Khairy, M. Zahran, and A. G. Wassal, “Efficient utilization of
GPGPU cache hierarchy,” in Workshop on General Purpose Processing
Using GPUs, 2015.
[28] C. Li, S. L. Song, H. Dai, A. Sidelnik, S. K. S. Hari, and H. Zhou,
“Locality-driven dynamic gpu cache bypassing,” in International Conference on Supercomputing, 2015.
[29] S.-Y. Lee, A. Arunkumar, and C.-J. Wu, “CAWA: Coordinated warp
scheduling and cache prioritization for critical warp acceleration of
GPGPU workloads,” in International Symposium on Computer Architecture, 2015.

167

Adaptive Timekeeping Replacement: Fine-Grained Capacity
Management for Shared CMP Caches
CAROLE-JEAN WU and MARGARET MARTONOSI, Princeton University

In chip multiprocessors (CMPs), several high-performance cores typically compete for capacity in a shared
last-level cache. This causes degraded and unpredictable memory performance for multiprogrammed and
parallel workloads. In response, recent schemes apportion cache bandwidth and capacity in ways that offer
better aggregate performance for the workloads. These schemes, however, focus primarily on relatively
coarse-grained capacity management without concern for operating system process priority levels.
In this work, we explore capacity management approaches that are both temporally and spatially more
fine-grained than prior work. We also consider operating system priority levels as part of capacity management. We propose a capacity management mechanism based on timekeeping techniques that track the
time interval since the last access to cached data. This Adaptive Timekeeping Replacement (ATR) scheme
maintains aggregate cache occupancies that reflect the priority and footprint of each application. The key
novelties of our work are (1) ATR offers a complete cache capacity management framework taking into account application priorities and memory characteristics, and (2) ATR’s fine-grained cache capacity control is
demonstrated to be effective and important in improving the performance of parallel workloads in addition
to sequential ones.
We evaluate our ideas using a full-system simulator and multiprogrammed workloads of both sequential
and parallel applications. This is the first detailed study of shared cache capacity management considering
thread behaviors in parallel applications. ATR outperforms an unmanaged system by as much as 1.63X
and by an average of 1.19X. ATR’s fine-grained temporal control is particularly important for parallel
applications, which are expected to be increasingly prevalent in years to come.
Categories and Subject Descriptors: B.3.0 [Memory Structures]: General
General Terms: Design, Performance
Additional Key Words and Phrases: Cache decay, capacity management, shared resource management
ACM Reference Format:
Wu, C.-J. and Martonosi, M. 2011. Adaptive timekeeping replacement: Fine-grained capacity management
for shared CMP caches. ACM Trans. Architec. Code Optim. 8, 1, Article 3 (April 2011), 26 pages.
DOI = 10.1145/1952998.1953001 http://doi.acm.org/10.1145/1952998.1953001

1. INTRODUCTION

It is common today to run multiple heterogeneous applications, such as web-server,
video streaming, graphic-intensive, scientific, and data mining workloads, on chipmultiprocessor (CMP) systems, where multiple cores share the last level on-chip cache.
This material is based on work supported by the National Science Foundation under Grants No. CNS-0627650
and CNS-07205661. The authors also acknowledge the support of the Gigascale System Research Center,
one of six centers funded under the Focus Center Research Program (FCRP), a Semiconductor Research
Corporation entity.
Authors’ addresses: C.-J. Wu, Department of Electrical Engineering, 34 Olden Street, Princeton University,
Princeton, NJ 08544; email; carolewu@princeton.edu; M. Martonosi, Department of Computer Science, 34
Olden Street, Princeton University, Princeton, NJ 08544.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for profit or commercial advantage and that
copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior specific permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c 2011 ACM 1544-3566/2011/04-ART3 $10.00

DOI 10.1145/1952998.1953001 http://doi.acm.org/10.1145/1952998.1953001
ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

3

3:2

C.-J. Wu and M. Martonosi

Fig. 1. The bars show normalized instructions per cycle (IPC) when applications run as an unmanaged
group for each individual workload. Performance degradation is experienced when multiple applications
are running simultaneously. More importantly, performance degradation of a high-priority application, for
example, gcc in (a), fft in (b), or lbm in (c), is more significant than others in the workloads. However the
unmanaged scheme (LRU) does not take into account process priorities while allocating cache capacity.

Conventionally, on-chip caches implement pseudo Least-Recently-Used (LRU) replacement policies. However, commonly used LRU replacement policies do not distinguish
between processes and their different memory needs. In addition, current systems cannot explicitly assign shared cache capacity effectively based on process priority and
characteristics of each process memory footprint. Consequently, while multiple processes have access to the shared cache, it is possible, for example, that a large-footprint
but low-priority process uses the shared cache intensively, such that other high-priority
processes are left with insufficient cache share throughout execution. This can result
in performance degradation and unpredictability. In order to provide performance predictability, particularly for high-priority processes, we have to prioritize accesses to the
shared cache and mitigate interprocess interference in the shared cache resource.
Not surprisingly, when multiple applications are running simultaneously, the performance of all applications is consistently degraded. Figure 1 shows this for three
heterogeneous workloads. The bars show normalized instructions per cycle (IPC) when
applications run as an unmanaged group for each individual workload over when
each individual application runs alone. Performance suffers because all concurrent
applications contend for the shared resources simultaneously in each workload. More
importantly, performance degradation of a high-priority application is more significant
than others in the workloads. This is because the unmanaged scheme (LRU) does not
take into account application priorities while allocating cache capacity.
Furthermore, there are no detailed studies on shared resource management for
parallel applications. Most of the existing proposals only target sequential applications.
As mainstream workloads consist of more and more parallel applications today, it
is important to explore the needs of shared cache capacity management for parallel
applications. In particular, management approaches must acknowledge the differences
between references coming from cooperating versus competing threads. This work is
the first to do so.
In order to provide shared cache capacity management on CMPs, various shared
resource management techniques have previously been proposed for shared caches
[Bitirgen et al. 2008; Chang and Sohi 2007; Hsu et al. 2006; Iyer 2004; Iyer et al.
2007; Kim et al. 2004; Nesbit et al. 2007; Petoumenos et al. 2006; Qureshi and Patt
2006; Rafique et al. 2006; Zhao et al. 2007]. However, most of these techniques focus on
studies of the high level policies in their frameworks and often ignore the effectiveness
offered by different possible underlying capacity management mechanisms. If the chosen cache capacity management technique partitions the shared cache space in coarse
granularity, the cache may not be utilized as efficiently. For example, way partitioning
is a commonly-used coarse-grained shared cache partitioning technique [Bitirgen et al.
2008; Nesbit et al. 2007; Qureshi and Patt 2006]. The major advantage of using way
ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

ATR: Fine-Grained Capacity Management for Shared CMP Caches

3:3

partitioning is its moderate overhead and simple implementation, but it has drawbacks
as well. First, in order to have sufficient control in implementing performance isolation, it is preferable to have more ways in the set-associative cache than the number of
concurrent processes. As the number of concurrent processes increases in today’s CMP
systems, this is difficult to achieve. In addition, way partitioning results in inefficient
cache space utilization due to its coarse granularity in space allocation.
Other papers, which offer finer-grained cache capacity management, modify cache
insertion or replacement policies to utilize shared caches more efficiently [Jaleel et al.
2008, Suh et al. 2001, 2002]. These proposals often relate the amount of cache space
allocated to a process to its performance improvement directly but take into account
temporal behavior of cached data implicitly (for example, with modified cache insertion/replacement policies). Our work is distinct in offering finer temporal control. Furthermore, these proposals aim at improving aggregate system performance, but do not
distinguish between process priorities. Finally, there has been no work studying the
shared cache capacity problem for parallel applications.
In this work, we propose a novel shared cache capacity management scheme called
Adaptive Timekeeping Replacement (ATR). ATR is a fine-grained mechanism that
assigns a lifetime to cache lines according to operating system process priority and
application memory reference patterns. ATR observes the memory needs of an application during different program phases and adjusts cache space allocation accordingly, by
varying the cache lifetime of each line. It not only minimizes unnecessary cache memory interference from running processes, but also allows more fluid apportioning of the
shared cache space. Consequently, the shared cache space is utilized more effectively.
As such, data remaining in the cache exhibit two critical characteristics: high priority and temporal locality. Finally, ATR is the first shared cache capacity management
considering thread behaviors in parallel applications. This fine-grained cache capacity
control improves the performance of parallel workloads by speeding up critical threads
in parallel applications. We demonstrate ATR’s fine-grained time-keeping feature is
effective and important, particularly for parallel workloads.
The contributions of our work over the previously proposed capacity management
techniques are the following.
(1) Our proposed Adaptive Timekeeping Replacement (ATR) scheme is effective in
managing shared caches. We run full-system simulation to evaluate the ATR
scheme and its performance effects, taking into account operating system influence on the problem. We demonstrate that ATR, which takes into account application priorities and manages capacity in a fine-grained manner, outperforms a
baseline unmanaged system by as much as 1.63X and by an average of 1.19X for
multiprogrammed workloads with less than 3% hardware overhead.
(2) ATR’s fine-grained cache capacity management feature is particularly important
for parallel threads. We are the first to investigate capacity management on a
mixture of parallel applications. We show that ATR causes critical threads in a
parallel application to be sped up by 7%, resulting in better overall application
performance.
(3) The ATR scheme can be used to implement a variety of high-level shared resource
management policies implemented by the operating system, because of its flexibility
and effectiveness. It can also be viewed as a building block and used along with
prior work on cache and network bandwidth management.
The structure of this article is as follows. In Section 2, we give an overview of shared
resource management. In Section 3, we introduce timekeeping techniques for ordering
cache replacement. Section 4 offers a detailed description of ATR’s algorithm and
enforcement. In Sections 5 and 6, we describe our simulation framework, evaluate
ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

3:4

C.-J. Wu and M. Martonosi

the shared resource management schemes, and analyze their performance in detail.
Section 7 discusses related work on shared resource management. This is followed by
Section 8, which presents design issues and future work. Finally, Section 9 offers our
conclusions.
2. SHARED RESOURCE MANAGEMENT OVERVIEW

Shared resource management refers to partitioning common resources among multiple requesters. In the case of multiple applications running on CMP systems, multiple
requests are made to the shared cache simultaneously. However, current systems cannot explicitly assign shared cache resources effectively based on process priority and
characteristics of each process’ memory footprint. Consequently, in order to arbitrate
memory accesses from all running processes, while taking into account heterogeneity
and process priority, shared resource management is critical.
An important goal in shared resource management is performance predictability.
While multiple processes have access to the shared cache in CMP systems, it is possible that a process, for example a memory-bound one, uses the shared cache intensively
and other processes are left with insufficient cache share throughout execution. This
can result in performance degradation and unpredictability. In order to provide performance predictability, particularly for high-priority processes, we have to prioritize
accesses to the shared cache and isolate interprocess interference in the shared caches.
Typically, shared resource management can be divided into two parts: a shared resource management policy and its capacity management mechanism. Shared resource
management policies define a performance goal, such as overall system throughput,
fair sharing, and/or quality of service, for a targeted system, by specifying the amount
of resources allocated to all running processes in the system. Then an underlying capacity management mechanism implements the specified policies to achieve the desired
performance goal. Our proposed Adaptive Timekeeping Replacement (ATR) scheme fits
into the category of the capacity management mechanism. ATR is a hardware mechanism that can control shared caches effectively and enforce the high-level shared
resource management policy.
There are some major design challenges faced by existing shared resource management techniques. One of these challenges is to preferentially partition shared caches
based on process priorities assigned by operating systems. Another challenge is to
design new management techniques for collaborative threads in parallel workloads,
which are becoming more prevalent today. Next we discuss the two problems in more
detail.
2.1. Process Priorities Assigned by Operating Systems

Traditionally operating systems play the role of assigning priorities to active processes
and allocating system resources based on process priorities. This is typically done by
allocating larger CPU time slices to high-priority processes and shorter time slices
to low-priority processes. However, these operating system assigned priorities are not
taken into account in today’s shared cache capacity management schemes. Currently,
existing schemes mainly focus on exploiting application memory characteristics while
managing the shared CMP caches, but ignore application priorities. To address this
issue, ATR accounts for application priorities in its capacity management while also
responding to application memory characteristics.
2.2. Collaborative Parallel Threads in a Parallel Application

Another issue faced by shared cache capacity management techniques is that collaborative threads are treated equally in a parallel application. In existing techniques, for
multiprogrammed workloads that consist of several parallel applications, all parallel
ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

ATR: Fine-Grained Capacity Management for Shared CMP Caches

3:5

threads freely compete for the shared cache. This causes sibling threads (threads in
the same parallel application) to compete for shared CMP cache space rather than
acting cooperatively. In order to tackle this issue, we need novel techniques that recognize collaborative threads in a parallel application and help to reduce the shared
cache contention among all active threads. The proposed ATR scheme offers a solution
for this issue by identifying high-priority critical threads in a parallel application and
allocating the shared cache space preferentially to these threads.
3. TIMEKEEPING TECHNIQUES FOR ORDERING CACHE REPLACEMENT

ATR builds on the cache decay concept [Kaxiras et al. 2001], which was originally
proposed to reduce leakage power consumption of cache memories on chip. We first
give background on cache decay’s original use for leakage control.
3.1. Cache Decay

Cache decay exploits the generational behavior of data stored in a cache. That is, often
data in the cache have a burst of frequent reuse due to data temporal locality followed
by a period of “dead time” before they are evicted. Although implementations vary,
conceptually cache lines are associated with a decay counter which is decremented
periodically over time. Each time a cache line is referenced, its decay counter is reset
back to a decay interval, T, which is the number of idle cycles this cache line is allowed
to remain in the cache. In other words, if a cache line is frequently referenced, its
decay counter will not reach 0. When the decay counter reaches 0, it implies that the
associated cache line has not been referenced for the past T cycles and its data are
unlikely to be referenced again in the near future. In the original cache decay proposal,
this timer would signal when the power supply of the cache line should be turned off
(with loss of data) to save leakage power. Our proposal does not turn off the cache
line nor lose data; rather, we use the generational timer construct to prioritize data
for upcoming evictions, as discussed next. Furthermore, as Section 4.4 describes, our
implementation has a much lower overhead than the conceptual example’s full counter
per cache line.
3.2. Shared Cache Capacity Management

In ATR, the generational behavior exploited by timekeeping leakage control helps
determine which lines of a cache should be considered for early replacement. ATR
suggests evicting the data of low-priority processes more aggressively than that of
high-priority processes. This approach can be used to control cache space occupancy of
multiple processes.
When a cache line has not been referenced for the past T cycles, it can be thought
of as moving to the LRU position in a replacement queue. That is, it becomes an
immediate candidate for replacement regardless of its LRU status. The key observation
is that we can use different decay intervals for each process. The decay counters of
cache lines associated with high-priority processes receive a longer decay interval,
so over time more cache resources are allocated to these processes. Similarly, the
system may assign a shorter decay interval to cache lines associated with streamingmemory applications or low-priority processes, in order to release their cache space
more frequently. Consequently, higher-priority data tend to stay in the shared cache
for a longer period of time.
In Figure 2, assuming a 4-way set-associative cache, we show how cache decay can
be individualized to different applications in a mixed workload. Suppose memory addresses X and Y are referenced by the lower-priority process. After some T cycles
without being rereferenced, X’s and Y’s cache lines will decay and become candidates
for replacement whether or not they are least recently used. With an LRU replacement
ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

3:6

C.-J. Wu and M. Martonosi

Fig. 2. An illustrative case study, where the timekeeping replacement policy outperforms the LRU replacement policy for a 4-way set-associative cache. Suppose memory addresses X and Y are referenced by the
lower-priority process. After some T cycles, X’s and Y’s cache lines will decay and become candidates for
replacement. With the LRU replacement policy, all memory references will be misses. In contrast, 3 out of 7
memory references are hits with the timekeeping replacement policy. More significantly, these hits are for
the high-priority process.

policy for the example shown in Figure 2, all memory references would be misses. In
timekeeping management, 3 out of 7 memory references are hits. More importantly,
these hits are for the high-priority process. This is because cache lines associated with
the high-priority process do not decay. In this example, LRU replacement works poorly
because it treats all memory references equally. In contrast, the timekeeping replacement has more hits because it adjusts replacement based on process priority while
responding to data temporal locality. Its fine-grained control in time and in space enables the effectiveness of the enforcement. Furthermore, unlike a strict priority-based
eviction scheme, timekeeping control lets low-priority applications make use of cache
space at times when the high-priority applications are not actively referencing their
cache lines.
While Petoumenos et al. [2006] explored some of the basic trends for such a technique, they focused on a statistical model that predicts thread behaviors. Our work
employs the timekeeping concept of cache decay to cache capacity management considering process priority. We offer direct performance studies, implementation details, and
consider operating system effects. Finally and importantly, our work further studies
the impact on workloads with parallel applications. Timekeeping replacement ordering
coupled with adaptive decay intervals forms the core of ATR.
4. ATR IMPLEMENTATION

This section first introduces the key components in the ATR framework. Then we
explain ATR’s algorithm and its enforcement for selected performance targets. Finally
we discuss the hardware implementation and overhead for ATR.
4.1. Implementation Overview

In order to adapt to memory needs during different program phases of an application, ATR monitors performance of applications of interest based on process priorities
and aims to provide performance expectation defined by shared resource management
policies. ATR observes a metric of choice and adjusts decay intervals of applications
accordingly. Depending on the metric chosen, the ATR scheme can support different
high-level capacity management policies, such as to maximize overall system throughput, to satisfy quality of service goals, and/or to provide fairness in cache sharing.
ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

ATR: Fine-Grained Capacity Management for Shared CMP Caches

3:7

Fig. 3. (a) Region classification with an IPC target described in Section 4.3.1. (b) Pseudocode for ATR’s decay
interval calculation.

Possible metrics include cache space allocation per process, the number of cache misses,
or instructions committed per cycle.
4.2. ATR Algorithm and Enforcement

This section explains ATR’s parameters and algorithm in detail. There are three priority levels in the ATR framework: high-priority (HP), medium-priority (MP), and
low-priority (LP). Cache lines associated with HP applications never decay, whereas
those associated with MP and LP do.
With the three priority levels, there are five parameters. α M and α L are weighting
factors, which determine how fast decay intervals change for MP and LP applications.
Cache lines associated with lower priority processes have larger α values, so the decay intervals are decremented faster. Thus the lower-priority cache lines move more
quickly to the LRU position in the shared cache. For a system that must support
more intermediate priority levels, the ATR scheme can be simply extended to provide
more intermediate decay factors. In addition, we define βU , β M , and β L as the tolerance thresholds for a target metric. The β values determine how much performance
degradation is tolerable for HP applications.
Typically, the operating system annotates its policies to the underlying hardware
mechanisms. As a result, it is the most suitable candidate to set α and β values for active
processes based on their priorities. Then the underlying ATR hardware mechanism
interprets the policies defined by the operating system and guarantees the quality of
service goal for processes in different priority levels. In the case of the ATR scheme
in this work, we set the α and β parameters according to application priorities to
demonstrate how ATR can effectively enforce the performance goal defined by the
preset parameters.
Next, we explain ATR’s algorithm. Figure 3(a) depicts four regions into which the
performance of the application(s) of interest can fall at any point in time. Figure 3(b)
provides the pseudo code for the ATR algorithm. In the example, we use application
IPC to represent the application performance. The different regions in Figure 3(a)
correspond to how the HP application’s performance compares to a target level.
Region 1 represents a better-than-target performance region, where I PChp approaches the performance of the application running alone, I PCalone , and is greater
than the upper performance tolerance threshold, βU *I PCalone . Because the performance of the HP application(s) is close to the performance upper bound, ATR can
allocate more cache space to the MP and LP applications running concurrently, by
increasing their associated decay intervals with factors of α L and α M respectively.
Region 2 represents a controlled performance region, where I PChp is between the
upper performance degradation tolerance, βU *I PCalone , and the midperformance degraACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

3:8

C.-J. Wu and M. Martonosi

dation tolerance, β M *I PCalone . When the performance of the HP application(s) falls in
region 2, decay intervals of the MP and LP applications can remain unchanged. This
is because the performance of the HP application(s) lies in the controlled performance
region and meets the QoS goal defined by the β values.
Region 3 represents a slight-degradation performance region. When I PChp falls in
region 3, the HP application(s) experience performance degradation: I PChp falls in
between the midperformance degradation tolerance, β M *I PCalone , and the lower performance degradation tolerance, β L*I PCalone . In order to catch up with I PCalone , cache
lines associated with LP applications are to be evicted more frequently by a factor of
α M under the ATR scheme. As a result, HP applications can utilize more shared cache
space.
Finally, region 4 represents a severe-degradation performance region. When in region
4, I PChp is classified as severely degraded. Here, ATR will decrease decay intervals of
both MP and LP applications by factors of α M and α L respectively, so the associated
cache lines are evicted more frequently. As a result, the HP application(s) are allocated
more shared cache space.
ATR decay interval adjustments continue dynamically either until I PChp increases
or until the MP and LP intervals reach their preset lower bounds. These lower bounds
provide worst-case guarantees for MP and LP applications. In general, the HP applications are allocated more shared cache space and can recover from the performance
loss caused by the interference of others. On the other hand, the lower bounds stabilize
the control algorithm and ensure minimal cache space for MP and LP applications.
Although IPC is used as the performance metric for the performance target in this
example, the ATR scheme is flexible and can enforce other performance metrics defined
by a specific shared resource policy. Section 4.3 discusses this in more detail.
4.3. Performance Target

The previous example used IPC as the target metric, but ATR is general and can use
other performance metrics as well, such as cache miss counts, or cache occupancies.
A performance target based on an IPC metric tracks application performance more
directly, whereas performance targets based on cache resource metrics, such as cache
miss counts and cache occupancies, are less direct, but sometimes easier to obtain
dynamically. In Sections 4.3.1 and 4.3.2, we explain how the ATR framework obtains
its performance target using IPC and cache resource metrics respectively.
4.3.1. IPC Target. Deducing the target IPC for an application can come from an application performance goal (e.g. a soft-realtime video game or media player) or from
profiling. We use profiling in our work. The application of interest is first run alone,
owning the entire shared cache. Every 100 million cycles, its performance in IPC is
recorded. This is essentially the most direct method for knowing how the application
would run in isolation. When the target application runs concurrently with other applications, its instruction counts are lined up with the counts obtained when it runs
alone, so I PCalone is used for the same program section. Nevertheless, the I PCalone can
also be obtained by dynamic performance prediction using, for example, the techniques
proposed in Isci et al. [2006] and Tam et al. [2009] with some modification.
For a parallel application, we set the performance target based on the IPC of the
critical thread(s). This is because the performance of a parallel application is often
bounded by the slowest running threads. There has been other work [Bhattacharjee
and Martonosi 2009] that focuses on predicting critical threads in a parallel application
based on thread cache behavior at runtime. Our work uses a similar but simpler
technique, which determines thread criticality by the lowest-IPC thread.

ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

ATR: Fine-Grained Capacity Management for Shared CMP Caches

3:9

Fig. 4. (a) Region classification with a cache resource target described in Section 4.3.2. (b) Pseudo code for
ATR’s decay interval calculation.

4.3.2. Other Targets. An alternative to using profiled IPC as ATR’s performance target
is to use other dynamic metrics, such as the number of L2 misses or cache occupancy.
Work, such as Kim et al. [2004] and Jaleel et al. [2008], also uses a dynamic cache
miss count metric to guide the proposed cache partitioning algorithm. Here we offer an
example policy that uses cache miss counts to guide ATR’s enforcement mechanism.
Similarly, we can fit the performance curves of MP and LP applications at any point in
time in Figure 4(a). Here γ M and γ L represent the L2 miss count ratio of MP and HP
applications and that of LP and HP applications respectively. When an interprocess
conflict miss occurs to an HP process, ATR decreases MP’s and LP’s decay intervals by
factors of α M and α L. In return, the HP process can utilize more shared cache space. ATR
then periodically checks whether the L2 miss proportions of MP/LP and HP processes
exceed γ M and γ L, the preset target. If it does, the performance of MP or LP processes
is degraded more than desired, so ATR will increase MP’s and LP’s decay intervals to
allow more cache utilization. As a result, miss count ratios of MP/LP and HP processes
stay close to γ M and γ L. Figure 4(b) provides the pseudocode for the decay interval
calculation with a cache resource metric.
4.4. Hardware Implementation and Overhead

This section discusses the hardware implementation and overhead for ATR in detail.
The ATR hardware interprets process priorities assigned by the operating system to
decay threshold with an N-entry lookup table, where N is the number of processors.
This small hardware lookup table maps PIDs to decay intervals for all active processes.
Then, at every sampling interval, the ATR scheme compares the performance of HP
applications to their performance targets and the tolerance thresholds as described in
Section 4.2. It also calculates the next decay intervals based on the comparison results
and the α values. Since we only allow α factors to be multiples of 2, the calculation
for new decay intervals is a simple shift, and as a result, this calculation is not a
performance bottleneck. Alternatively, the decay interval calculation is only performed
every 10 million cycles, so this calculation can be done in software. In this work, we
simulate all modifications in hardware with a full-system simulator (Section 5.1).
For decay counter implementation, the ATR scheme primarily relies on a single
global cycle counter. Some of the global counter’s middle or high-order bits are used to
control the per-line or regional counters in the cache, as discussed in Hu et al. [2002].
Only a small number of coarse-grained decay bits, for example, 2 bits, are stored per
cache entry along with a subset/hash of PID. In this two-level scheme, when four
decay intervals are required (64K, 256K, 1M, and 4M), the 4th, 6th, 8th, and 10th bits
of the global counter are used to control the local counter. The single global counter
ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

3:10

C.-J. Wu and M. Martonosi

Fig. 5. Decay counter implementation with 2-bit speed counter per cache line incurs less than 1% overhead
for 64-byte cache lines. The overhead is even less for longer cache lines: 0.1% for 256-byte cache lines.

decrements every cycle, but local per-entry counters only decrement every 4096 cycles.
When a local counter reaches zero, its associated cache line becomes an immediate
candidate for replacement. Our work has explored design options and determined that
four different decay intervals are sufficient.
When only a few representative decay intervals are required, ATR can be implemented with a 2-bit speed counter per cache line as shown in Figure 5. This incurs
less than 1% hardware overhead for 64-byte cache lines. For longer cache lines, the
overhead is even less. If more fine-grained decay interval increments are desired, the
two-level counter scheme incurs no more than 3% overhead for 64-byte cache lines,
which is still reasonably low overhead.
Last but not least, although we do not model this, the same decay counter hardware
can be used both for ATR’s capacity management and for timekeeping leakage energy
control. This amortizes overhead over two benefits and allows systems to optimize for
power, cache capacity management, or both.
5. EXPERIMENTAL SETUP
5.1. Simulation Framework

We evaluate the proposed ATR scheme using GEMS [Martin et al. 2005]. GEMS is built
upon Virtutech Simics,1 a full-system multicore processor simulator. We simulate both
4-core (for sequential workloads) and 16-core (for parallel workloads) CMP systems
based on the Sparc architecture running the Solaris 10 operating system. We elaborate
the workload setup in Section 5.2.
The microarchitectural parameters for our baseline memory subsystem are based on
Intel’s Core2 Quad chips clocked at 2.83GHz on a 45nm technology node [Intel Corp.
2010]. Each processor core has a 4-way set-associative, 32KB private L1 cache with
64B cache lines. All cores share the two 2MB L2 cache banks (4MB in total). The L2
cache banks are 16-way set-associative, with 64B cache lines.
CACTI 5.3 determines the L2 cache access latency to be 2.87ns. This means it takes
8.12 cycles to access the L2 cache. We round up to a higher access latency of 10 cycles for
the experiments in the article to represent CMP systems running at a higher frequency.
GEMS memory modules add 4 more cycles to determine and access the cache banks.
Furthermore, a simple interconnection network is modeled, which includes 2 more
cycles for network link latency. Thus, overall, the L2 cache access latency is 16 cycles.
This also matches well with Core2 Quad’s design parameters. The measured L2 cache
access latency for Core2 Quad machines is 15 cycles [Intel Corp. 2011]. Similarly, we
use CACTI 5.3 to determine the access latencies for other memory components in
the system, where L1 cache access takes 3 cycles and DRAM access takes 268 cycles
respectively.

1 http://www.virtutech.com/

ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

ATR: Fine-Grained Capacity Management for Shared CMP Caches

3:11

Fig. 6. Application L2 cache miss ratios of sequential and parallel applications used in our workloads.
Table I. Workloads Reflect General Sequential and General Parallel Scenarios. The Application Priority in All
Workloads is Randomly Chosen to Mimic the OS Priority Assignment
SPEC2006
Priority Level
SPEC2006
Priority Level
SPLASH-2
Priority Level
Input Dataset
Domain
PARSEC
Priority Level
Input Dataset
Domain

Workload 1: General, Sequential
gobmk
MP
Workload 2: General, Sequential
bzip2
gcc
HP
MP
Workload 3: General, Parallel
barnes
fft
HP
MP
65,536 particles
4,194,304 points
scientific computing
scientific computing
Workload 4: General, Parallel
streamcluster
canneal
HP
MP
simlarge
simlarge
data mining
engineering
gcc
HP

lbm
LP
mcf
LP
radix
LP
8,388,608 points
sorting
fluidanimate
LP
simlarge
animation

5.2. Workload Construction

We use a combination of sequential and parallel applications to evaluate shared resource management mechanisms. The sequential applications come from the SPEC
2006 benchmark suite; the parallel applications are from the SPLASH-2 [Woo et al.
1995] and PARSEC [Bienia et al. 2008b] benchmark suites. To better suit today’s
CMP cache sizes and configurations, the SPLASH-2 applications use the scaled input
datasets suggested in Bienia et al. [2008a]. The PARSEC applications use the simlarge
input datasets. Figure 6 shows the L2 cache miss ratios of sequential and parallel applications used in our workloads. The L2 cache miss ratio represents an application’s
memory characteristics and is provided as background information to help understand
the mixed memory characteristics for the entire aggregate workloads.
Tables I and II then summarize the seven workloads we study. Five of these are multiprogrammed mixtures of sequential applications (SPEC2006); the other two workloads
are multiprogrammed mixtures of parallel applications. The workloads representing
multiprogrammed mixes of sequential and parallel applications are intended to illustrate several scenarios.
First, Workloads 1 and 2 are general sequential scenarios. Here, we generate heterogeneous memory requests to the shared cache by running gcc, gobmk, and lbm in
Workload 1 and bzip2, gcc, and mcf in Workload 2. Each of the three applications
is pinned to an individual core (in the 4-core CMP system). The remaining core is
ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

3:12

C.-J. Wu and M. Martonosi

Table II. Workloads Reflect High-Contention Scenarios. The Application Priority in All Workloads is Randomly
Chosen to Mimic the os Priority Assignment
SPEC2006
Priority Level
SPEC2006
Priority Level
SPEC2006
Miss Ratio

Workload 5: High Contention, Sequential
gcc
mcf
HP
LP
Workload 6: High Contention, Sequential
gcc
mcf
mcf
HP
LP
LP
Workload 7: High Contention, Sequential
hmmer
bzip2
gobmk
HP
MP
MP
bzip2
HP

lbm
LP
mcf
LP
gcc
LP

Table III. ATR Configuration for All Workloads. In a Full Implementation, the OS Would Set and Adjust These
Parameters
IPC Control
Sequential Workloads
Parallel Workloads
Cache Miss Control

αL
4
4
αL
4

αM
2
2
αM
2

βL
0.93
0.90
γL
4

βM
0.96
0.95
γM
2

βH
0.97
0.97
γH
N/A

dedicated for operating system activities, so the OS interference on the running applications is minimized.
Workloads 3 and 4 are general purpose parallel workloads. Workload 3 consists of
three parallel applications, fft, barnes, and radix, from the SPLASH-2 benchmark
suite. Each is run with four threads and each thread is pinned to an individual core in
the 16-core CMP system. The second multithreaded workload includes three different
parallel applications, canneal, streamcluster, and fluidanimate, from the PARSEC
benchmark suite. Similarly, each application is run with four threads and each thread
is pinned to an individual core.
Finally, Workloads 5–7 model the high-contention scenarios to the shared cache.
These high-contention scenarios are the ones that most fundamentally motivate cache
resource management approaches. Workload 5 models high contention memory access
to the shared cache by running bzip2, gcc, mcf, and lbm, where both mcf and lbm are
memory-bound applications. Workload 6 also models the high contention scenario by
including one instance of gcc along with three instances of mcf. The first three applications are pinned to individual cores in the 4-core CMP system and the last application
shares its core with OS activities. Finally, the last high-contention workload, Workload
7, consists of four SPEC2006 applications, bzip2, gcc, gobmk, and hmmer. This workload
is used to demonstrate ATR’s enforcement for both an IPC and a dynamic cache miss
metric. Table III summarizes ATR’s configurations for all workloads.
5.3. Performance Metric

In the multiprogrammed sequential workloads, Workloads 1, 2, 5, 6, and 7, we measure
the performance in IPC of each application for the same program section following the
method discussed in Section 4.3.1. We show speedup for each application individually. We also compute a weighted speedup metric related to application priorities. The
speedup of high-, medium-, and low-priority applications are weighted 3:2:1. Furthermore, for Workload 7, we also use a dynamic cache resource metric (number of L2 cache
misses) to measure the performance of each application individually in addition to the
weighted mean.
For Workloads 3 and 4, which consist of parallel applications, we run each application to completion and use execution time speedup as the performance metric. If one
of the applications finishes earlier than other applications in the workload, it restarts
ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

ATR: Fine-Grained Capacity Management for Shared CMP Caches

3:13

Table IV. STR Configuration for All Workloads. Application Priorities are Given in Tables I and II. HP
Application Cache Lines are Never Decayed
Workload 1
Workload 2
Workload 3
Workload 4
Workload 5
Workload 6
Workload 7

LP Decay Interval (in cycles)
4096
4096
1,048,576
1,048,576
4,096
4,096
262,144

MP Decay Interval (in cycles)
8192
1,048,576
8,388,608
4,194,304
8,192
N/A
4,194,304

Fig. 7. Performance comparison for all workloads under the unmanaged baseline, STR, and ATR schemes.
We also illustrate the performance the workload would attain if each application ran alone. ATR outperforms
the unmanaged baseline by as much as 1.63X and by an average of 1.19X.

and reruns until all applications finish at least once. This ensures a degree of multiprogrammed contention throughout the full simulation.
6. PERFORMANCE EVALUATION

We investigate a static, nonadaptive scheme, called Static Timekeeping Replacement
(STR). Then we compare the performance results for STR and ATR to a baseline
system, that uses the LRU replacement policy for its level-two cache. Furthermore, we
implement another recently proposed cache capacity management scheme, TADIP-F
[Jaleel et al. 2008]. We extend TADIP-F to account for application priorities and give a
detailed comparison to ATR in Section 6.4.
6.1. STR Scheme

We first consider STR, a static version of ATR, in order to show the importance of ATR’s
ability to adaptively change decay intervals at runtime. For STR, we experiment with
36 pairs of decay intervals for MP and LP applications ranging from 4K cycles to 64M
cycles. Based on these results, we pick the single decay interval combination giving
the best weighted average performance for applications. Thus, the STR results show
an idealized best-case performance for a static decay interval, and can be used as a
comparison point. Table IV summarizes the chosen decay interval combinations.
6.1.1. General, Sequential Workloads: Workloads 1 and 2. Figure 7 shows that, for all workloads except Workload 2, STR significantly improves performance compared to the
unmanaged baseline. In Workload 1, STR preferentially allocates more shared cache
space to the high-priority application, gcc. In return, the performance of gcc and gobmk
is improved by 17% and 16% relative to the unmanaged case although the performance
of lbm is degraded by 1%, as shown in Figure 8(a). This minimal performance tradeoff
of lbm and significant performance gain of gcc and gobmk comes from the fine-grained
capacity control of STR and its ability to eliminate memory interferences in the shared
cache between lbm and the other applications.
ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

3:14

C.-J. Wu and M. Martonosi

Fig. 8. Performance comparison for (a) Workload 1, and (b) Workload 2.

Fig. 9. Cache space utilization for Workload 3 under (a) the baseline scheme, (b) STR, and (c) ATR. During
time T1, ATR prefers to retain barnes (the HP application) cache lines, then fft’s (the MP application), and
last radix’s (the LP application), as shown in (c), whereas the allocation in the baseline scheme is on-demand
(by LRU).

However, for Workload 2, STR is not flexible enough and cannot improve the performance of the high-priority application, as illustrated in Figure 8(b). For workload 2,
the performance of bzip2, the high-priority application, remains the same as it is in the
unmanaged baseline. Furthermore, the performance of mcf, the low-priority application, is degraded by 9%. This is because STR is unable to adjust the decay intervals to
adapt to changes in memory needs over the program execution. ATR’s dynamic decay
interval selection addresses this problem.
6.1.2. General, Parallel Workloads: Workloads 3 and 4. Next, we investigate STR’s effectiveness for workloads comprised of parallel applications, Workloads 3 and 4. As illustrated
in Figure 9, when the shared cache is unmanaged, radix uses most of the shared cache
over the program execution, leaving almost no cache space for fft and very little for
barnes, the high-priority application. In STR, radix’s cache space occupancy is constrained when other higher priority applications request more shared cache space. In
return, the performance of fft and barnes is improved by 5% and 1%. More importantly, because of the fine-grained control of the STR scheme, the performance of radix
experiences no additional degradation compared to the baseline scheme. Similarly,
in Workload 4, the STR scheme can improve the performance of streamcluster, the
high-priority application, and canneal by 4% and 13% respectively without additionally harming the performance of fluidanimate, the low-priority application. Figure 10
illustrates this.
6.1.3. High Contention, Sequential Workloads: Workloads 5–7. Finally, we illustrate that
STR can improve the performance of Workloads 5–7 in the high contention scenarios
significantly. In Workload 5, STR preferentially allocates more shared cache space to
the high-priority applications, bzip2 and gcc, while retaining data that exhibit good
temporal locality from lbm and mcf. In return, the performance of bzip2 and gcc is
ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

ATR: Fine-Grained Capacity Management for Shared CMP Caches

3:15

Fig. 10. Performance comparison for Workloads 3 and 4.

Fig. 11. Performance comparison for (a) Workload 5, (b) Workload 6, and (c) Workload 7.

improved by 50% and 27% relative to the unmanaged case, although the performance
of lbm is degraded by 5%, as shown in Figure 11(a). More interestingly, the performance
of the other low-priority application, mcf, is improved by 28%. This results in a net average performance improvement. If lbm’s small performance degradation is problematic,
the operating system could categorize it as an MP application, which would help protect it from mcf’s heavy use of the cache. Another alternative is to let the operating
system decrease the α values at runtime. Thus lbm, the LP application in the workload, is allowed to use more shared cache. The minimal performance tradeoff of lbm
and significant performance gain of mcf comes from the fine-grained capacity control
of STR and its ability to eliminate memory interferences in the shared cache between
lbm and all other applications. We see similar performance results for Workloads 6 and
7, as illustrated in Figures 11(b) and (c).
While STR improves the performance for all workloads except Workload 2, its inability to adjust decay intervals on the fly results in less effective memory allocation. As
a result, the performance of the high- and the medium-priority applications is slightly
degraded in Workload 2. Furthermore, to determine the optimal combination of the
decay intervals for MP and LP applications in the STR scheme requires multiple profiling runs. This imposes a constraint on STR’s feasibility. In the following section, we
will demonstrate ATR can detect the change of memory needs for all applications at
runtime, dynamically adjust decay intervals, and as a result, allocate the shared cache
space more effectively to all running applications.
6.2. ATR Scheme

ATR’s dynamic decay interval adjustments reduce the need for profiling and increase
its ability to exploit application program phases. When the shared cache is managed
under the ATR scheme, as shown in Figure 7, ATR offers a significant performance
ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

3:16

C.-J. Wu and M. Martonosi

Fig. 12. Decay interval variation by ATR for Workload 1.

Fig. 13. Cache space distribution over time for Workload 1: (a) under the baseline scheme, and (b) under
the ATR scheme.

Fig. 14. Decay interval variation by ATR for Workload 2.

improvement across all workloads. The performance benefit is especially large for
workloads where memory needs change over different program phases.
6.2.1. General, Sequential Workloads: Workloads 1 and 2. When compared to the STR
scheme, ATR can further improve the performance of gcc, the high-priority application
in Workload 1, by an additional 6% without degrading the performance of the other
applications. Figure 12 shows that this is because ATR can allocate cache memory effectively based on its performance monitoring by quickly adjusting the decay intervals
of the MP and LP applications. As a result, the high-priority application receives more
cache space in the ATR technique (Figure 13).
With Workload 2, we demonstrate, ATR’s ability to adapt to memory needs of all
applications over different program phases is critical and essential. When bzip2, the
high-priority application, is running along with gcc and mcf in Workload 2, its performance is degraded by 31%. This performance degradation is caused by the memory
interference in the shared cache. When bzip2’s memory requirement decreases, ATR
immediately increases the decay intervals associated with gcc’s and mcf’s cache lines,
as illustrated in Figure 14, so more shared cache can be utilized by gcc and mcf.
Figure 15 illustrates that when the shared cache is managed under the ATR scheme,
bzip2, the high-priority application, immediately receives more shared cache space
as required throughout the execution. Then, when its memory need decreases, ATR
ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

ATR: Fine-Grained Capacity Management for Shared CMP Caches

3:17

Fig. 15. Cache space distribution over time for Workload 2: (a) under the baseline scheme, and (b) under
the ATR scheme.

Fig. 16. (a) The IPC performance of barnes critical thread, the high-priority application in Workload 3, over
time in the baseline and ATR schemes. (b) The decay interval variation for fft and radix in Workload 3.

immediately allows the other applications to use more shared cache space. As a result,
the performance of bzip2 and gcc is improved significantly, by 28% and 11% respectively, in the ATR scheme. In contrast, STR’s inability to adapt to different program
phases results in slight performance degradation in Workload 2.
6.2.2. General, Parallel Workloads: Workloads 3 and 4. Next, we explore how the ATR
scheme manages the shared cache space to improve the performance of workloads
comprised of parallel applications, especially for the high-priority ones. Figures 16 and
17 depict the performance of critical threads in Workloads 3 and 4 over time. As discussed in Section 4.3.1, the ATR logic monitors the performance of the critical threads
for the high-priority applications, barnes in Workload 3 and streamcluster in Workload 4. Once it observes a slowdown beyond the tolerance threshold, the ATR hardware
immediately decreases the decay intervals of other running applications.
Figure 17(b) illustrates that, during time interval T1, for Workload 4, the performance of the critical thread of streamcluster is rising, so the ATR logic increases the
decay intervals for canneal and fluidanimate. This allows both applications to utilize
more shared cache space. Then, during time interval T2, the performance of the critical thread is declining. As a result, ATR decreases the decay intervals of canneal and
fluidanimate, so cache lines associated with the two applications move to the LRU
position more quickly. As a result, more shared cache space is allocated to the highpriority applications, as requested. Consequently, the performance of the high-priority
applications in Workloads 3 and 4 is improved by 4% and 7% respectively.
Although the performance of fft in Workload 3 and canneal in Workload 4 is improved more in the STR scheme, the performance improvement of the high-priority
applications is more significant in the ATR scheme. This is because with statically
ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

3:18

C.-J. Wu and M. Martonosi

Fig. 17. (a) The IPC of streamcluster’s critical thread over time under the baseline and ATR schemes.
During time T1, the performance of the critical thread of streamcluster is rising, so the ATR logic increases
decay intervals for canneal and fluidanimate, as shown in (b). During time T2, the performance of the
critical thread is declining, so the ATR logic decreases the decay intervals of other running applications.
As a result, cache lines associated with canneal and fluidanimate move to the LRU position more quickly,
similarly for times T3 and T4.

Fig. 18. Cache space distribution over time for Workload 6 (a) Under the baseline scheme, and (b) under
the ATR scheme.

assigned decay intervals in the STR scheme, fft and canneal are able to use more
shared cache space, leaving barnes and streamcluster with insufficient cache share
over certain program phases. In contrast, ATR is able to observe these subtle changes
in memory needs over time and adjust the decay intervals accordingly. Consequently,
the performance of both barnes and streamcluster, the high-priority applications, is
improved by an additional 3% compared to the improvement in the STR scheme.
6.2.3. High Contention, Sequential Workloads: Workloads 5–7. Finally we demonstrate that
compared to STR, ATR can further improve the performance of Workloads 5–7 in the
high contention scenario. ATR works better than (or as well as) STR and improves
the performance of Workloads 5–7 by 1.64X, 1.07X, and 1.06X respectively (Figure 7).
ATR generally better protects the performance of the high-priority applications in the
workloads.
The additional performance gain for the high-priority applications comes from ATR’s
ability to adapt to the memory needs of all applications at runtime. The ATR mechanism
first assigns an initial decay interval to each MP and LP application. Then during the
workload execution, ATR quickly settles to the decay intervals that can be used to
achieve the performance target. When the IPC target is met, ATR increases decay
intervals for the medium- and low-priority applications. Figure 18 illustrates that ATR
can preferentially allocate more shared cache space to the high-priority application in
ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

ATR: Fine-Grained Capacity Management for Shared CMP Caches

3:19

Fig. 19. Performance comparison for ATR under the guidance of a cache resource metric: L2 miss counts.

Workload 6. Consequently, ATR further improves the performance of the high-priority
application in the workloads.
6.3. Dynamic Miss Tracking

As discussed in Section 4.3.2, ATR can also be guided by target metrics other than
IPC. Here as one example, we consider cache miss counts. Figure 19 illustrates the L2
miss reduction for Workload 7, which consists of four SPEC2006 applications, bzip2,
gcc, gobmk, and hmmer. The left bar shows the number of L2 cache misses under no
management, the middle bar shows the management scheme under ATR’s enforcement
with β M = 2 and β L = 4, and the right bar shows the number of L2 misses when each
application is running alone. ATR reduces the number of L2 misses of the high-priority
application, hmmer, by 27%. Furthermore, the overall L2 misses are reduced by 11%.
Here ATR is demonstrated as an effective, and yet flexible, mechanism for shared cache
capacity management using dynamic cache miss tracking.
6.4. Comparison to TADIP-F and TADIP-FP

Another cache capacity management scheme, called Thread-Aware Dynamic Insertion
Policy with Feedback (TADIP-F) has been proposed recently [Jaleel et al. 2008]. Both
TADIP-F and ATR perform capacity management for shared caches by modifying the
cache insertion/replacement policy. The original TADIP-F work does not take into
account application priorities in managing the shared cache. Here we take a step
further to extend the original TADIP-F work and propose a version of TADIP-F that
encodes application priorities (TADIP-FP). Then we compare the three schemes to
demonstrate ATR’s strengths.
6.4.1. TADIP-F. TADIP-F observes the memory requirements of all running applications and determines the insertion position of an incoming cache line by set-sampling.
It proposes to insert all incoming cache lines to two possible insertion positions: either
LRU or MRU. After insertion, LRU management occurs. For example, for a cachethrashing application, the majority of its incoming cache lines are placed in the LRU
position and a few in the MRU position. This insertion policy is called bimodal insertion
policy (BIP). On the other hand, for an LRU-friendly workload, TADIP-F always inserts
its incoming cache lines in the MRU position. This policy is called most-recently-used
insertion policy (MIP).
TADIP-F uses a few cache sets to sample workloads running simultaneously and
varies a per-core policy selection (PSEL) counter to track misses in each insertion
policy. A miss in the MRU-insertion cache sets increments the PSEL counter and a
miss in the bimodal-insertion cache sets decrements the PSEL counter. The rest of the
cache sets use the most-significant-bit (MSB) of the PSEL counters to determine the
insertion policy: MSB = 1 uses BIP and MSB = 0 uses MIP. In contrast, the ATR scheme
takes access timing into account by keeping track of the time interval since the last
access to cached data. We compare TADIP-F with the ATR scheme and demonstrate
ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

3:20

C.-J. Wu and M. Martonosi

Fig. 20. Comparison of TADIP-F and the ATR scheme. (a) and (b), The ATR scheme outperforms TADIP-F
for parallel applications in Workloads 3 and 4. (c) The performance of bzip2, the high-priority application in
Workload 5, is doubled in the ATR scheme compared to TADIP-F. (d) The ATR scheme performs better than
TADIP-F on average for the studied workloads.

the importance of this temporal aspect of the ATR scheme, particularly for the studied
parallel workloads.
6.4.2. TADIP-FP. Since the original TADIP-F does not consider process priorities, we
also implement a priority-aware version of TADIP-F, called TADIP-FP. TADIP-FP
varies how fast PSEL counters are incremented and decremented based on application OS priorities. The PSEL counter for a high-priority application is incremented
by 1 as it is in TADIP-F, but it is decremented faster by a constant larger than 1. As a
result, TADIP-FP uses MIP for the high-priority application more frequently. On the
other hand, the PSEL counter for a low-priority application is incremented faster by a
constant larger than 1 and is decremented as it is in TADIP-F. This means TADIP-FP
is more inclined to use BIP for the low-priority application.
In our experiments, we use the weights for high-, medium-, and low-priority applications to determine the rate of change for the per-core PSEL counters. For high-priority
applications, we increment the PSEL counters by 1 when a miss occurs in the cache set
sampling for MIP, and decrement the PSEL counters by 3 when a miss occurs in the
cache set sampling for BIP. As a result, TADIP-FP is more likely to select MIP for the
high-priority applications. For medium-priority applications, TADIP-FP increments
the PSEL counters by 2 and also decrements the PSEL counters by 2 also. Finally, for
low-priority applications, TADIP-FP increments the PSEL counters by 3 when a miss
occurs in its cache sets sampling for MIP and decrements the PSEL counters by 1 when
a miss occurs in its cache sets sampling for BIP. As a result, TADIP-FP selects BIP for
the low-priority applications more frequently than TADIP-F.
6.4.3. Comparative Performance Results. Figure 20 compares the overall performance for
all workloads under TADIP-F, TADIP-FP, and ATR. ATR performs better than TADIPF by 9% on average. More importantly, TADIP-F does not account for OS-imposed
priorities as ATR does. Figures 20(a) and (b) show that ATR outperforms TADIP-F for
parallel applications in Workloads 3 and 4 by 15% and 4% respectively. Furthermore,
the performance of two parallel applications in Workload 3, fft and radix, is degraded
severely under TADIP-F. This is because (1) not all cache sets in TADIP-F use the
optimal insertion policy, and (2) threads in the same parallel application also have to
compete for shared cache in TADIP-F. Figure 20(c) illustrates that the performance of
bzip2, the high-priority application in Workload 5, is 21% better in the ATR scheme
than in TADIP-F. This is because ATR preferentially allocates more shared cache space
ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

ATR: Fine-Grained Capacity Management for Shared CMP Caches

3:21

Fig. 21. (a) Cumulative density function for the PSEL counter of gcc, the high-priority application, in
Workload 1. TADIP-FP elects to use MIP for the high-priority application more frequently than TADIPF. (b) Performance comparison for TADIP-F and TADIP-FP. TADIP-FP improves the performance of the
high-priority application by 2.5% more than TADIP-F.

Fig. 22. (a) Cumulative density function for the PSEL counter of bzip2, the high-priority application, in
Workload 2. TADIP-FP elects to use MIP for the high-priority application more frequently than TADIPF. (b) Performance comparison for TADIP-F and TADIP-FP. TADIP-FP improves the performance of the
high-priority application by 1.5% more than TADIP-F.

to the high-priority application(s) whereas TADIP-F only observes memory footprint
characteristics and optimizes for the overall system throughput, but does not aim to
provide preferential cache allocation based on process priority.
Next, we investigate the performance results in the priority-aware TADIP-FP technique. For Workloads 1 and 6, TADIP-FP improves the aggregate performance the most,
by 28% and 14%. For all other workloads, ATR performs better. Although TADIP-FP
prioritizes high-priority cache lines, for a low-priority cache line inserted to the LRU
position followed by a cache hit, TADIP-FP promotes this low-priority cache line to
the MRU position. From here on, such low-priority cache lines are treated no differently than high-priority cache lines. As a result, the performance benefit gained by
high-priority applications is still limited in the TADIP-FP scheme.
Figure 21(a) shows the cumulative density function (CDF) for the PSEL counter
values of the high-priority application, gcc, in Workload 1. TADIP-F elects to use BIP
for gcc’s cache lines for 55% of the program run while TADIP-FP always elects to use
MIP for gcc’s cache lines. Figure 21(b) illustrates that as a result, TADIP-FP improves
the performance of the high-priority application, gcc, by 2.5% over TADIP-F. This is
because more high-priority cache lines are inserted in the MRU position.
Similarly, Figure 22(a) shows the CDF for the PSEL counter values of the highpriority application, bzip2, in Workload 2. TADIP-F elects to use BIP for bzip2’s cache
lines for 19% of the program run while TADIP-FP again always elects to use MIP
for bzip2’s cache lines. As a result, TADIP-FP improves the performance of the highpriority application by 1.5% over TADIP-F, as illustrated in Figure 22(b). Although
TADIP-FP can help improve the performance of the high-priority applications in both
Workloads 1 and 2, the overall weighted performance does not improve much compared
to TADIP-F. On average, TADIP-FP improves workload performance by 1% more than
TADIP-F.
ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

3:22

C.-J. Wu and M. Martonosi

Fig. 23. Comparison of the ATR and ATR-reduced schemes. Performance here is normalized to the ATR
scheme, and the performance of ATR already represents an improvement over the baseline in all cases.

In summary, although TADIP-F can be extended for priority level consideration by
weighting misses of high-priority applications more heavily (for example, TADIP-FP),
there are only two insertion positions for incoming cache lines: MRU or LRU. If a
low-priority application’s cache line is first inserted to the LRU position and then rereferenced, TADIP-F promotes it to the MRU position. It then must step back through
the positions to LRU before eviction. On the other hand, with ATR, low-priority cache
lines become immediate candidates for eviction based on their decay counters even
when they are in the MRU position. Therefore, ATR can better manage the shared
cache capacity taking into account the temporal behavior of data and process priority
while also improving the system throughput.
6.5. Sensitivity to Decay Intervals

Although temporal adaptivity is clearly important, we observe that decay intervals
used in most of the workloads settle to a few possible values in various program phases.
Thus, it may be sufficient for the ATR scheme to support just these representative decay
intervals, which can ease ATR’s temporal control for cached data. This AT Rreduced
scheme implements four possible decay interval options for all cache lines: 4K cycles,
32K cycles, 1M cycles, and never decay. Figure 23 shows the performance comparison of
the ATR and AT Rreduced schemes. On average the AT Rreduced scheme performs similarly
as the ATR scheme for all sequential workloads. We note, however, that the coarsergrained control does not improve the performance of the high-priority application(s) as
much as the ATR scheme.
ATR also performs better than the AT Rreduced scheme (7% and 6% respectively) for
the parallel workloads, Workloads 3 and 4. This is because the memory needs in parallel applications change more frequently and more diversely, as illustrated previously
in Figures 16(b) and 17(b). Nonetheless, overall AT Rreduced offers performance improvement competetive with ATR. For all workloads, the performance gain under the
AT Rreduced mechanism is only 3% worse than ATR’s. In general, AT Rreduced represents
a simpler control implementation and similar performance to ATR.
6.6. Summary

This work proposes a priority-aware capacity management approach, ATR. We demonstrate that ATR’s fine-grained cache capacity management in space and time is effective and important in improving the performance of both sequential and parallel
workloads. ATR outperforms an unmanaged baseline by as much as 1.63X and by
ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

ATR: Fine-Grained Capacity Management for Shared CMP Caches

3:23

an average of 1.19X. We take a step further to implement a priority-aware version
of TADIP-F, called TADIP-FP. Our results show that ATR performs better than both
TADIP-F and TADIP-FP by an average of 9% and 8% respectively. Finally, we show,
with a simpler ATR implementation featuring four decay intervals, AT Rreduced can sustain ATR’s significant performance improvement with only 3% performance tradeoff
on average.
7. RELATED WORK

There has been a significant amount of research in capacity management for shared
CMP caches [Bitirgen et al. 2008; Hsu et al. 2006; Iyer 2004; Jaleel et al. 2008; Kim
et al. 2004; Nesbit et al. 2007; Petoumenos et al. 2006; Qureshi and Patt 2006; Rafique
et al. 2006; Suh et al. 2001, 2002; Zhao et al. 2007]. However, the underlying capacity
control mechanisms used in these proposals are often based solely on spatial partitioning of shared caches. To the best of our knowledge, our proposed ATR scheme is
the first shared cache capacity management that takes into account not only spatial
allocation of shared caches and temporal characteristics of cached data but also application priorities. Furthermore, this is the first detailed study of shared cache capacity
management considering thread behaviors in parallel applications.
Qureshi and Patt [2006] proposed Utility-based Cache Partitioning which monitors
resource utilization among all running processes in flight and distributes the shared
cache, accordingly using way-partitioning. CacheScouts [Zhao et al. 2007] offered a
finer-grained monitoring for shared caches but it also uses spatial partitioning for its
shared caches. Similarly, Nesbit et al. [2007] proposed virtual private caches, which consist of a bandwidth manager and a capacity manager implementing way-partitioning.
Furthermore, Bitirgen et al. [2008] also uses spatial partitioning as its underlying
mechanism for the shared cache while coordinating other on-chip shared resources:
shared caches, cache bandwidth, and power budgets. To investigate the effectiveness
of spatial partitioning, Iyer [2004] demonstrates that spatial partitioning of shared
caches in various granularities can help achieve QoS goals more effectively for CMP
platforms. Moreover, Kim et al. [2004] discussed five performance metrics for CMP
cache sharing, which help the operating system determine shared cache allocation
better, and offered two algorithms that also suggest spatially partitioning the shared
cache. We argue that the proposed ATR scheme can provide a more effective shared
cache capacity allocation because not only is it capable of managing shared caches
spatially in the granularity of cache lines, but it also takes the timing behavior of
cached data into account. It is promising both as a stand alone capacity management
scheme, as well as in concert with other resource management schemes discussed in the
preceding.
Srikantaiah et al. [2008] proposed adaptive set pinning to identify and eliminate
interprocess misses in CMP systems. This work is complementary to the proposed
ATR scheme and can be used to further improve system throughput. Petoumenos
et al. [2006] offered a statistical model to predict thread behaviors in a shared cache,
based on cache decay, while no direct performance studies nor implementation details
are given for managing the shared cache. Jaleel et al. [2008] proposed TADIP-F, as
discussed in Section 6.4. Xie and Loh [2009] extended the idea in TADIP-F for shared
cache capacity management and showed that the proposed pseudopartitioning scheme
allows better utilization of the shared cache. Finally, Jaleel et al. [2010] proposed RRIP,
which modifies cache replacement policies based on cache line reuse interval prediction.
While this work is similar to ours, ATR is distinct in its application priority handling.
Furthermore, this work is the first that quantities the need for fine-grained capacity
management for parallel workloads.
ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

3:24

C.-J. Wu and M. Martonosi

8. DISCUSSION AND FUTURE WORK

In this section, we describe the role of the operating system in general for capacity
management schemes and specific to the proposed ATR scheme. In addition, we discuss
other design choice for cache memory organizations and the impact for cache capacity
management studies. Finally we list the future direction for this work.
8.1. Operating Systems

Operating systems apportion system resources to running processes based on the assigned priorities. Traditionally that has included CPU time slices, but our work shows
that shared CMP caches must similarly be managed. While the operating system offers more flexibility in defining quality of service goals, it plays a significant role in
annotating its policies or specific goals to the underlying hardware mechanisms. The
underlying hardware mechanisms then interpret the policies defined by the operating
system and guarantee some level of quality of service by adjusting shared resource
allocation.
In the case of ATR, the operating system can specify the α and β values based on its
QoS goals. The underlying hardware can vary decay intervals for each process on the
fly to satisfy OS policies. It is interesting to consider whether there is benefit to having
more priority levels than what is currently supported by ATR. On the other hand, for
processes with equal priorities, ATR does not treat them differently. A potential future
work is to explicitly assign the α and β values based on application cache utilization
for processes in the same priority level.
8.2. Nonuniform Cache Access Design

Nonuniform cache access (NUCA) design is becoming more prevalent for today’s memory systems and poses an interesting challenge in cache capacity management techniques. The traditional monolithic last-level cache (LLC) is separated into a few cache
banks, enabling more parallelism in accessing the cache bank modules. However, this
results in nonuniform cache access latencies for the shared LLC. Accessing cache banks
located closer to a particular processor takes less time than other banks located further
away.
This NUCA design certainly influences the cache capacity management problem
and motivates new approaches to cache partitioning. Cache capacity management
schemes should take into account cache bank access time when apportioning the shared
LLC. This can further optimize for application performance and system throughput.
However, NUCA or the traditional uniform cache access (UCA) design is orthogonal
to both ATR and TADIP-F, which focus on improving application performance via
modifying cache insertion and replacement policies. As a result, we do not further
investigate this cache design choice in this work, but leave it as potential future work.
8.3. ATR for Shared versus Private Data in Parallel Applications

The proposed ATR scheme is the first work quantifying an approach for cache capacity
management for parallel applications. Currently, ATR without special treatment, can
effectively improve parallel thread performance with its fine-grained capacity allocation. However, a more complicated ATR scheme can be designed to take into account the
data sharing characteristics: shared versus private data in a parallel application. For
example, assigning a longer decay interval to shared data can help reduce the eviction
rate of shared L2 cache blocks and their corresponding L1 cache blocks. As a result,
the amount of coherence messages in the interconnection network between L1 and L2
caches can be significantly reduced. Furthermore, this can also reduce the overall application cache miss rate. Although the current ATR scheme does not take advantage of
ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

ATR: Fine-Grained Capacity Management for Shared CMP Caches

3:25

the data sharing property in parallel applications, this opportunity remains for future
work.
9. CONCLUSION

In this work, we propose ATR, a fine-grained capacity management for shared caches,
based on timekeeping techniques. ATR monitors the performance of high-priority application(s) and dynamically apportions shared cache space among running processes
based on memory footprints and process priorities. We evaluate the proposed approach,
including operating system effects. We demonstrate that ATR outperforms a baseline
system by as much as 1.63X and by an average of 1.19X for all studied workloads. Furthermore, we study ATR’s effectiveness on workloads consisting of parallel applications.
ATR can improve application performance over unmanaged scenarios by accelerating
critical threads; overall application performance is improved by as much as 7%. ATR’s
fine-grained temporal control is particularly important for parallel applications. We
take a step further to implement a priority-aware version of TADIP-F and show that
ATR performs better than both TADIP-F and TADIP-FP by 9% and 8% respectively.
Overall, ATR is an effective mechanism for capacity management in shared caches,
because of its fine temporal and spatial control of the shared cache space. While we
have demonstrated ATR’s effectiveness in isolation, it can also be viewed as a building
block to be used along with prior work on cache and network bandwidth management.
ACKNOWLEDGMENTS
We thank Konstantinos Aisopos, Yu-Yuan Chen, Daniel Lustig, Jakub Szefer, and the anonymous reviewers
for their feedback. We also thank Aamer Jaleel and Joel Emer for their useful feedback and insights related
to this work.

REFERENCES
BHATTACHARJEE, A. AND MARTONOSI, M. 2009. Thread criticality predictors for dynamic performance, power,
and resource management in chip multiprocessors. In Proceedings of the 36th Annual International
Symposium on Computer Architecture (ISCA).
BIENIA, C., KUMAR, S., AND LI, K. 2008a. PARSEC vs. SPLASH-2: A quantitative comparison of two multithreaded benchmark suites on chip-multiprocessors. In Proceedings of the IEEE International Symposium on Workload Characterization (IISWC).
BIENIA, C., KUMAR, S., SINGH, J. P., AND LI, K. 2008b. The PARSEC benchmark suite: characterization and
architectural implications. In Proceedings of the 17th International Conference on Parallel Architectures
and Compilation Techniques (PACT).
BITIRGEN, R., IPEK, E., AND MARTINEZ, J. 2008. Coordinated management of multiple interacting resources in
chip multiprocessors: A machine learning approach. In Proceedings of the 41st Annual International
Symposium on Microarchitecture (MICRO).
CHANG, J. AND SOHI, G. S. 2007. Cooperative cache partitioning for chip multiprocessors. In Proceedings of the
21st Annual International Conference on Supercomputing (ICS).
HSU, L. R., REINHARDT, S. K., IYER, R., AND MAKINENI, S. 2006. Communist, utilitarian, and capitalist cache
policies on CMPs: caches as a shared resource. In Proceedings of the 15th International Conference on
Parallel Architectures and Compilation Techniques (PACT).
HU, Z., KAXIRAS, S., AND MARTONOSI, M. 2002. Let caches decay: Reducing leakage energy via exploitation of
cache generational behavior. ACM Trans. Comput. Syst.
INTEL CORP. 2010. Intel Core2 Extreme Processor QX9000 Series and Intel Core2 Quad Processor Q9000,
Q9000S, Q8000 and Q8000S Series Datasheet.
INTEL CORP. 2011. Intel 64 and IA-32 Architectures Optimization Reference Manual.
ISCI, C., CONTRERAS, G., AND MARTONOSI, M. 2006. Live, runtime phase monitoring and prediction on real systems with application to dynamic power management. In Proceedings of the 39th Annual International
Symposium on Microarchitecture (MICRO). 359–370.
IYER, R. 2004. CQoS: A framework for enabling QoS in shared caches of CMP platforms. In Proceedings of
the 18th Annual International Conference on Supercomputing (ICS).

ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

3:26

C.-J. Wu and M. Martonosi

IYER, R., ZHAO, L., GUO, F., ILLIKKAL, R., MAKINENI, S., NEWELL, D., SOLIHIN, Y., HSU, L., AND REINHARDT, S. 2007.
QoS policies and architecture for cache/memory in CMP platforms. ACM SIGMETRICS Perform. Eval.
Rev.
JALEEL, A., HASENPLAUGH, W., QURESHI, M., SEBOT, J., STEELY, JR., S., AND EMER, J. 2008. Adaptive insertion
policies for managing shared caches. In Proceedings of the 17th International Conference on Parallel
Architectures and Compilation Techniques (PACT).
JALEEL, A., THEOBALD, K. B., STEELY, JR., S. C., AND EMER, J. 2010. High performance cache replacement using
re-reference interval prediction (RRIP). In Proceedings of the 37th Annual International Symposium on
Computer Architecture (ISCA).
KAXIRAS, S., HU, Z., AND MARTONOSI, M. 2001. Cache decay: exploiting generational behavior to reduce cache
leakage power. In Proceedings of the 28th Annual International Symposium on Computer Architecture
(ISCA).
KIM, S., CHANDRA, D., AND SOLIHIN, Y. 2004. Fair cache sharing and partitioning in a chip multiprocessor architecture. In Proceedings of the 13th International Conference on Parallel Architectures and Compilation
Techniques (PACT).
MARTIN, M. M. K., SORIN, D. J., BECKMANN, B. M., MARTY, M. R., XU, M., ALAMELDEEN, A. R., MOORE, K. E., HILL,
M. D., AND WOOD, D. A. 2005. Multifacet’s general execution-driven multiprocessor simulator (GEMS)
toolset. SIGARCH Comput. Archit. News.
NESBIT, K. J., LAUDON, J., AND SMITH, J. E. 2007. Virtual private caches. In Proceedings of the 34th Annual
International Symposium on Computer Architecture (ISCA).
PETOUMENOS, P., KERAMIDAS, G., ZEFFER, H., KAXIRAS, S., AND HAGERSTEN, E. 2006. Modeling cache sharing on
chip multiprocessor architectures. In Proceedings of the IEEE International Symposium on Workload
Characterization (IISWC).
QURESHI, M. K. AND PATT, Y. N. 2006. Utility-based cache partitioning: A low-overhead, high-performance, runtime mechanism to partition shared caches. In Proceedings of the 39th Annual International Symposium
on Microarchitecture (MICRO).
RAFIQUE, N., LIM, W. T., AND THOTTETHODI, M. 2006. Architectural support for operating system-driven CMP
cache management. In Proceedings of the 15th International Conference on Parallel Architectures and
Compilation Techniques (PACT).
SRIKANTAIAH, S., KANDEMIR, M., AND IRWIN, M. J. 2008. Adaptive set pinning: Managing shared caches in
CMPs. In Proceedings of the 13th International Conference on Architectural Support for Programming
Languages and Operation Systems (ASPLOS).
SUH, G., DEVADAS, S., AND RUDOLPH, L. 2002. A new memory monitoring scheme for memory-aware scheduling
and partitioning. In Proceedings of the 8th International Symposium on High-Performance Computer
Architecture (HPCA).
SUH, G. E., RUDOLPH, L., AND DEVADAS, S. 2001. Dynamic cache partitioning for simultaneous multithreading
systems. In Proceedings of the International Conference on Parallel and Distributed Computing and
Systems (IASTED).
TAM, D. K., AZIMI, R., SOARES, L. B., AND STUMM, M. 2009. RapidMRC: Approximating L2 miss rate curves
on commodity systems for online optimizations. In Proceeding of the 14th International Conference on
Architectural Support for Programming Languages and Operating Systems (ASPLOS).
WOO, S. C., OHARA, M., TORRIE, E., SINGH, J. P., AND GUPTA, A. 1995. The SPLASH-2 programs: Characterization
and methodological considerations. In Proceedings of the 22nd Annual International Symposium on
Computer Architecture (ISCA).
XIE, Y. AND LOH, G. H. 2009. PIPP: Promotion/insertion pseudo-partitioning of multi-core shared caches. In
Proceedings of the 36th Annual International Symposium on Computer Architecture (ISCA).
ZHAO, L., IYER, R., ILLIKKAL, R., MOSES, J., MAKINENI, S., AND NEWELL, D. 2007. CacheScouts: Fine-grain monitoring of shared caches in CMP platforms. In Proceedings of the 16th International Conference on Parallel
Architecture and Compilation Techniques (PACT).
Received September 2009; revised September 2010; accepted December 2010

ACM Transactions on Architecture and Code Optimization, Vol. 8, No. 1, Article 3, Publication date: April 2011.

E-ECC: Low Power Erasure and Error Correction Schemes
for Increasing Reliability of Commodity DRAM Systems
Hsing-Min Chen

Akhil Arunkumar

Carole-Jean Wu

Arizona State University

Arizona State University

Arizona State University

hchen136@asu.edu
akhil.arunkumar@asu.edu carole-jean.wu@asu.edu
Trevor Mudge
Chaitali Chakrabarti
University of Michigan

Arizona State University

tnm@umich.edu

chaitali@asu.edu

ABSTRACT

Keywords

Most server-grade memory systems provide Chipkill-Correct
error protection at the expense of power and/or performance
overhead. In this paper we present low overhead schemes for
improving the reliability of commodity DRAM systems with
better power and IPC performance compared to ChipkillCorrect solutions. Specifically, we propose two erasure and
error correction (E-ECC) schemes for x8 memory systems
that have 12.5% storage overhead and do not require any
change in the existing memory architecture. Both schemes
have superior error performance due to the use of a strong
ECC code, namely, RS(36,32) over GF (28 ). Scheme 1 activates 18 chips per access and has stronger reliability compared to Chipkill-Correct solutions. If the location of the
faulty chip is known, Scheme 1 can correct an additional
random error in a second chip. Scheme 2 trades off reliability for higher energy efficiency by activating only 9 chips per
access. It cannot correct random errors due to a chip failure
but can detect them with 99.9986% probability, and once a
chip is marked faulty due to persistent errors, it can correct
all errors due to that chip. Synthesis results in 28nm node
show that the RS (36,32) code results in a very low decoding latency that can be well-hidden in commodity memory
systems and, therefore, it has minimal effect on the DRAM
access latency. Evaluations based on SPEC CPU 2006 sequential and multi-programmed workloads show that compared to Chipkill-Correct, the proposed Schemes 1 and 2
improve IPC by an average of 3.2% (maximum of 13.8%)
and 4.8% (maximum of 31.8%) and reduce the power consumption by an average of 16.2% (maximum of 25%) and
26.8% (maximum of 36%), respectively.

DRAM Memory System, DRAM errors, Reliability, ChipkillCorrect, Erasure and Error Correction

Categories and Subject Descriptors
•Computer systems organization → Reliability;

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

MEMSYS ’15, October 05-08, 2015, Washington DC, DC, USA
c 2015 ACM. ISBN 978-1-4503-3604-8/15/10. . . $15.00

DOI: http://dx.doi.org/10.1145/2818950.2818961

1.

INTRODUCTION

As the capacity of today’s server systems increases, the
reliability of these systems has become a very important
concern [1, 2, 3, 4]. The increase of errors in today’s memory
systems has been well documented in [5, 6, 7]. Such errors
cause machine crashes and lead to increase in the operational
cost.
Many of the server systems are built with commodity
DRAMs. These memory systems can have multiple errors
along a row and/or along a column of a chip and can also
have whole chip failures [5, 6, 7]. Today’s memory systems
are expected to provide Chipkill-Correct reliability [1, 8] the ability to correct errors due to failure of a whole DRAM
chip with 12.5% storage overhead. Bit-level Chipkill-Correct
spreads the DRAM access across multiple chips so that errors caused by a chip failure can be corrected using ECC
[8]. However, such an implementation needs activations of
4 ranks with 18 chips per rank and incurs very high power
consumption. Hence, many memory systems have moved to
symbol-based Chipkill-Correct code [9, 10] that can correct
single symbol errors and detect double symbol errors. However for x4 DRAM systems, symbol-based Chipkill-Correct
code requires 2 ranks with (2x18 = 36) chips to be activated
in each access, thereby also incurring high power consumption.
To maintain Chipkill-Correct capability and reduce the
power consumption, several techniques have been proposed,
namely, V-ECC [11], SSA[12], LOT-ECC [13], ARCC[14]
and Multi-ECC [15]. While these techniques access fewer
chips and operate on x8 DRAMs, they either have higher
storage overhead or require additional accesses for memory write and for error correction. To reduce power and
performance overhead during normal error-free operation,
several of these systems separate error detection from error
correction. However, when errors occur, these systems need
additional reads to perform error correction and additional
writes when the data is updated in DRAM.
We present a very different approach to provide error protection for x8 systems while keeping the storage overhead
to 12.5%. Our approach is based on the use of a stronger
symbol-based Reed-Solomon code, namely, RS(36,32) over
GF(28 ). This code was specifically chosen to handle the con-

straints of an x8 DRAM system. Synthesis results in 28nm
technology (Section 4.3) show that the decoding latency of
this code is significantly smaller than DRAM access latency
[16], making this code based design approach practical for
commodity DRAM systems. We show how RS (36,32) code
can be used in the design of two erasure and error correction
(E-ECC) schemes that have high reliability due to efficient
handling of errors caused by both transient and permanent
faults.
Scheme 1 activates 18 chips per access and has very strong
error correction capability. It can fully correct errors due
to one chip failure and if the location of the faulty chip is
known, it can correct errors due to the faulty chip and an
additional error in a second chip. Scheme 2, which activates
only 9 chips per access, has higher IPC and lower power
consumption compared to Scheme 1 but weaker reliability. If
a chip fails due to random errors, it has very high probability
to detect these errors and if the errors are persistent, and
the chip is marked as faulty, Scheme 2 can correct all errors
due to that chip.
We evaluate the proposed E-ECC schemes by simulating
SPEC2006 benchmarks on a full system simulator. We analyze the trade-offs among reliability, IPC, power and energy
performance for the two proposed schemes. Both schemes
have minimal hardware and OS overhead - they only affect
the ECC decoding circuitry in the memory controller (MC).
Overall, we make the following key contributions:
• We show that a stronger decoding algorithm for error
detection and correction for modern DRAM chips is
now a plausible solution because the decoding latency
can be well-hidden in today’s memory system with a
negligible amount of performance overhead.
• We propose, design, and implement two erasure and
error correction (E-ECC) schemes that provide different levels of error protection with power and performance trade-offs. Scheme 1 has better reliability,
higher IPC, lower power consumption and better energy performance than Chipkill-Correct. Scheme 2 has
even better IPC, power and energy performance compared to Scheme 1 and good reliability compared to
existing x8 single rank systems. Overall, both E-ECC
schemes achieve higher energy efficiency - 18.9% and
30.3% better than Chipkill-Correct.
The rest of the paper is organized as follows: Section 2 describes a typical DRAM memory system followed by error
characteristics and a summary of existing schemes. Section
3 gives the details of our proposed E-ECC schemes. Section
4 presents the evaluation methodology and Section 5 compares the performance with the competing schemes with respect to timing, power consumption and reliability. Section
6 concludes this paper.

2.

BACKGROUND AND RELATED WORK

In this section, we first introduce the basic configuration
of commodity DRAMs (in Section 2.1) and then describe the
error characteristics of DRAM memory systems (in Section
2.2). Next, we present the various existing ECC schemes (in
Section 2.3).

2.1

DRAM Memory Systems

Processor

Channel 2

Channel 1
DIMM1

DIMM3

DIMM2

DIMM4

Rank 0

Rank 1

Chip 0 Chip 1

Chip 8

Banks in a chip
bank 7

bank 0

Figure 1: Logical view of a DRAM based memory
system
A DRAM memory system is organized into channels, ranks,
chips and banks [17]. The DRAM memory controller (MC)
acts as an interface between the processor’s last level cache
and the DRAM. It can access data from one or more channels. Each channel consists of dual in-line memory modules
(DIMM) and each of the DIMMs consists of one or more
ranks. A rank is the minimum unit that is activated in a
read or write access. Each rank is composed of multiple
chips (also called devices) and the number of chips depends
on the size of the data bus width. The bus width is typically
64 bits with another 8 extra bits for ECC [17]. A wider configuration has bus width of 128 bits with 16 extra bits for
ECC.
A single DRAM chip has a narrow data path, typically
N bits, where N is 4, 8 or 16; the corresponding DRAM is
referred as x4, x8 or x16, respectively. In an x8 system with
bus width of 64+8 bits, a rank consists of 8 chips for data and
an additional chip for ECC bits. Each chip typically consists
of 4 to 16 banks. Figure 1 illustrates a typical organization
of a DRAM memory system. In this system, the MC has two
channels, where each channel can access two DIMMs. Each
DIMM consists of two ranks, where each rank consists of 9
chips. Furthermore, each DRAM chip consists of 8 internal
banks.
In each access, one row of a bank is activated; the column
address strobe (CAS) signal provides the column address of
the data that is to be read out. In an x8 system, a total of
8+1 bytes are accessed at a time from the 8+1 chips. In the
DDR3 setting, the default burst length is 8, which means
that 8 words (1 word per beat) along the same row of a
bank are accessed one after the other. This leads to 64B
(typically, one cache line size) for 8 beats. In some memory
systems, in order to provide stronger reliability, multiple
channels work together in a lockstep manner to provide for
a wider memory data bus (for example, 128 bits). In this
mode, data is retrieved over 4 beats (referred to as burst
chop mode) [17] to match the 64B cache line size.

2.2

Error Characteristics

DRAM errors can be broadly classified into soft errors
and hard errors. Soft errors are caused by transient faults
which occur randomly and cause incorrect data to be read
from a memory location; they disappear when the location
is overwritten. Hard errors are caused by permanent faults

or intermittent faults. A permanent fault causes a memory
location to consistently return an incorrect value, such as
a stuck-at-0 fault. An intermittent fault causes a memory
location to sometimes return incorrect values. Note that a
single fault can result in multiple error instances [7, 18].
DRAM errors have been analyzed in detail in [5, 6, 7, 18,
19, 20]. The study in [5, 6, 7] shows that a large fraction of
errors are hard errors and these manifest as repeating errors
occurring at the same address, row, column or chip. The
study in [5, 6] also shows that the number of errors in any
memory system increase over time. A more recent analysis of DRAM faults done over a period of 15 months shows
that while the failure rate due to transient faults increases
mildly, the failure rate due to permanent faults is higher in
the beginning of the 15-month period. The failure rate of
permanent faults becomes almost the same as the rate of
transient faults around months 6 to 8 [18] and then reduces
mildly. However, it is projected that the failure rate would
again increase towards the end of the device’s lifetime. Permanent faults tend to be clustered [5] and so if the chip with
such faults can be identified, then that chip can be marked
as faulty and all data from that chip can be treated as erasures.
Erasures are easier to correct than random errors since
the location of the errors is known. Bit-level erasure codes
have recently been proposed in [21] to improve the correction capability from single bit to double bit in STT-MRAM.
Also the new DIMM architecture proposed in [22] uses erasure codes to deal with chip failures and pin failures. The
assumption is that the memory controller (MC) can identify
these failures by analyzing the corrupted data patterns.
In this work, we utilize the error recording mechanism of
machine-check architecture (MCA) [23, 24] to mark a chip
as faulty. The MCA has registers to store the error address,
time and type (corrected or uncorrected) of error. Error
events are recorded both during memory scrubbing [25, 26]
and during normal read operation. Now if the number of
errors is larger than a threshold value (the threshold value is
system-dependent), the chip is marked as faulty by the MC.
Again if the faulty chip has no errors over a predetermined
time period, it can revert back to normal mode, i.e., be
considered as a healthy chip.

2.3

Related Work

The ECC storage overhead for commercial memory systems is around 12.5%. This is the storage overhead of ChipkillCorrect systems and is taken to be the golden standard. The
data access granularity for Chipkill-Correct based x4 systems is 128B for burst length of 8. However if the system
supports burst chop with 4 beats (instead of 8), the data
access granularity can be reduced to 64B which matches the
cache line size of most systems. Chipkill-Correct activates
36 devices with 2 ranks which is wasteful in terms of energy.
However, activating fewer ranks while maintaining high reliability typically comes with higher storage overhead (more
than 12.5%) or extra DRAM read/writes.
Baseline Scheme - Chipkill-Correct: Chipkill-Correct
is the most common error protection scheme for DRAM
memory systems [1, 2, 6, 8]. It can correct errors due to
failure of one chip and also detect errors due to two chip
failures. The ECC code for Chipkill-Correct used in [1, 9] is
the symbol-based code (144,128) code over GF(24 ) [9]. For
an x4 memory system, use of this code results in activation

of 36 devices across 2 ranks in lockstep mode. Although the
storage overhead of Chipkill-Correct is 12.5%, it still consumes substantial energy due to activation of 36 devices.
While an x8 memory system is known to be more energyefficient compared to an x4 system, direct implementation
of Chipkill-Correct with symbol based ECC code either has
very large storage overhead or incurs large data access granularity as shown in [13, 17]. For example, [13] showed that
use of a 3-check symbol code (64b data + 24b ECC) had a
storage overhead of 37.5% in order to support 64B cache line
size. Hence, several schemes have been proposed to address
this problem for the x8 systems and we list 5 well known
solutions in the rest of this section.
Virtualized ECC (V-ECC) [11] provides Chipkill-Correct
capability for x4 and x8 DRAM memory systems while activating fewer chips than the baseline implementation. It is
a two-tiered scheme where error detection is separated from
error correction. The ECC code used here is RS (19,16)
code over GF(28 ). Error detection is done by using 2 check
symbols and error correction is done after reading out a 3rd
check symbol. For the read operation, 18 chips are activated
in a rank. For error correction or for the write operation,
another 18 chips in a rank have to be activated. Thus, this
procedure is expensive both in terms of latency and energy.
The storage overhead of V-ECC is 18.75%, which is larger
than the desired overhead of 12.5%.
Localized and tiered ECC (LOT-ECC) [13] activates
nine x8 DRAM chips within one rank to support 64B granularity when the burst length is 8. Instead of using traditional error correction code, it uses multiple layers of XOR
operations to deal with the errors. Each time, it only activates 64B and does error detection using the local checksums. However, if an error is detected, global error parity
bits (GEC) are read using a second access. LOT-ECC needs
to activate additional memory accesses for both error correction as well for as write operation. The storage overhead
of this scheme is 26.5%, which is the largest of all existing
schemes.
Multi-line error correction (Multi-ECC) [15] uses a
2-dimensional ECC scheme where row decoding is first used
to detect row errors and column checksums are used to locate
the error locations. After the error is located, the row parity
check bits are used again to correct the errors. To keep the
overhead of column checksums low, every checksum group
is obtained by combining data from 256 rows. Multi-ECC
incurs large latency for error correction and for storing the
new checksums back to the DRAM device. When errors are
detected by the row decoder, all the lines in the checksum
group have to be read out to identify the error location.
Although the storage overhead is only 12.9%, this system
also has a large overhead when errors have to be corrected.
Single Subarray Access (SSA) [12] lowers energy consumption in an x8 DRAM system by storing the entire cache
line in a subarray in a single DRAM chip. As a result, for
read, only one chip is activated instead of 9 chips. Each
subarray has 8 bits to store local checksums for error detection. For error correction, global checksums, which are
stored across all chips in a rank, are used. Thus, when errors
are detected, 8 other chips have to be read out, resulting in
increase in energy consumption and latency. Also for the
write operation, two DRAM chips have to be read followed
by write, which leads to 256B granularity. While the storage overhead of this scheme is small at 14.3%, using only 8

Chip 0

Chip 8

Chip 0

Chip 8

Codeword

Syndrome Calculation
No

Banks in a chip

Syndrome
Classification

8

Two symbols from a bank are
sent to an ECC codeword

8

Error Free

Banks in a chip

8

8

Yes

Chip Marked
Faulty?

RS (36,32) code over GF(28)

Double Erasure
Correction
Case (iii)

One more
Error ?

No

Yes

Single
Error
Correction
Case (i)

Double
Error
Correction
Case (ii)

Double Erasure and
Single Error
Correction
Case (iv)

Data Out

Figure 2: E-ECC Scheme 1. (a) x8 DRAM access pattern for RS (36,32) code (b) Overview of the decoding
algorithm
Table 1: Proposed schemes for x8 DRAM systems
RS (36,32) code over GF(28 )
E-ECC Scheme 1

# of chips per access
18

Granularity
64B (4 beats)

Storage overhead
12.5%

E-ECC Scheme 2

9

64B (8 beats)

12.5%

bits to detect errors in 64B may not be strong enough for
DRAM systems. SSA also requires a change in the DRAM
microarchitecture so that cache lines are localized in each
DRAM chip.
Adaptive Reliability Chipkill correct (ARCC) [14]
provides two levels of error protection. It assumes that the
errors are few in the beginning and so uses a smaller sized
code with 2 check symbols for error detection. By using
the smaller code, only 64B per channel is activated. However, when an error is detected, ARCC adaptively adjusts
the ECC strength by combining adjacent codewords to do
error correction. In this scenario, 36 devices across 2 channels are activated. This increases the granularity to 128B.
An additional overhead is that the last level cache needs to
be modified to accommodate both 64B and 128B cache lines.
Bamboo ECC [27] is a recently proposed single-tier error
protection scheme that provides good system reliability with
low storage overhead. It uses 8-bit symbol based RS codes
for x4 DRAM memory to handle error events ranging from
correcting single bit errors with 3.1% storage overhead to
correcting errors due to two chip failures with 25% storage
overhead. Furthermore, by grouping per-pin data to form
ECC symbols, it is able to correct double pin failures and
thus provides higher error protection compared to ChipkillCorrect.
Many commercial servers employ page retiring, chip sparing or memory mirroring to improve reliability [5, 25, 28].
For example, Intel systems use double device data correction (DDDC) to correct double device errors sequentially.
In each rank, one DRAM device is reserved as a spare chip;
when a chip is marked faulty, the spare chip is utilized. IBM
ProteXion [24] uses redundant bit steering to re-route the
faulty bits to backup bits to deal with bit failures. Instead
of using 8 bits to protect 64 bits of data it uses only 6 bits
and uses extra two bits as backup bits.
In the next section we describe two schemes that can correct or detect a combination of erasures and random er-

ECC capability
Corrects 2 errors or 2 erasures;
Corrects 2 erasures and 1 error
Corrects 1 error and detects 3 errors;
Corrects 4 erasures

rors without using hardware sparing or page retiring. Our
schemes do not bypass a faulty chip; if the location of the
faulty chip is known, data from that chip is treated as erasures and are corrected.

3.

PROPOSED E-ECC SCHEMES

We propose two erasure and error correction (E-ECC)
schemes for x8 DRAM memory systems that achieve the
target 12.5% storage overhead and require no additional accesses for error correction (Table 1). Both schemes are based
on RS (36,32) code over GF(28 ). Scheme 1 is designed for
systems that require very high reliability while Scheme 2 is
designed for lower power and higher performance systems.

3.1

E-ECC Scheme 1

Scheme 1 operates in lockstep mode as in [17, 25, 26].
Here two ranks (2x9 chips) are activated in each access and
if the system operates in the burst chop mode with 4 beats
[17], the data access size is 64B. In each beat, 144 bits (128
data bits + 16 data bits) are read out from 18 chips. If
rotational (144,128) code over GF(24 ) is used as in some
Chipkill-Correct x4 systems [9, 10], when a chip fails, it
generates 2 error symbols which cannot be corrected. If we
move to a higher Galois field such as GF(28 ), we can use
the shortened RS (18,16) code, which is Chipkill-Correct.
This code can correct an erasure from a chip that has been
marked faulty and can detect one more random error in
another chip. However, for higher reliability, it is important
that the random error in the second chip be corrected and
so a stronger code is necessary.
In this paper, we propose using RS (36,32) code over
GF(28 ) that has the same 12.5% storage overhead as the
rotational (144,128) code but higher error correction capability. This code is derived from RS (255,251) code over
GF(28 ) and can support the following cases: (i) single error
correction, (ii) double error correction, (iii) double erasure

Codeword
Chip 0

Chip 1

Chip 8

Syndrome Calculation

No

Chip Marked
Faulty?

Yes

Banks in a chip

8 8 8 8

Four symbols from a bank
contribute to the ECC codeword 8

Banks in a chip

Error Free

Syndrome
Classification

Single Error
Correction and Triple
Error Detection
Case (v)

Four Erasure
Correction
Case (vi)

8 8 8

Data out

RS (36,32) code over GF(28)

Figure 3: E-ECC Scheme 2. (a) x8 DRAM access pattern for RS (36,32) code, (b) Overview of the decoding
algorithm
correction, (iv) double erasure and single error correction.
Figure 2 (a) illustrates the data access mechanism of Scheme
1. Two consecutive 8-bit symbols are read out from the same
bank in two consecutive beats. Since there are 18 chips, a
total of 2 × 18 = 36 symbols are read out and sent to the
RS (36,32) decoder.
The decoding flowchart for Scheme 1 is shown in Figure 2
(b). The syndrome vector is calculated every time. If none
of the chips has been marked faulty by the MC, the left path
is taken. Since RS code has a special algebraic structure, the
syndrome vector can be used to classify whether it is a single
error (case i) or a double error (case ii) event efficiently.
After the classification, the corresponding single error and
double error correction units are activated.
If a chip fails, it generates two error symbols and this code
can correct these two errors. Now if the location of the faulty
chip is known, the two error symbols from that chip can be
treated as erasures and corrected. Correcting two erasures
is simpler compared to correcting two errors. Furthermore,
if a chip is marked faulty, an additional error in another chip
can also be corrected. To handle double erasure correction
(case iii) and double erasure and one error correction (case
iv), the E-ECC decoder has to do double erasure correction
first. If the decoder finds that it is not a double erasure
event, the decoder activates the double erasure and single
error correction unit. We explain the decoding algorithm
for all cases in the Appendix. We present the latency, area
and power results of the synthesized decoder in Section 4.3
and show that the hardware overhead is very small.

3.2

E-ECC Scheme 2

To achieve higher reliability, E-ECC Scheme 1 activates
two ranks and thus has higher power. In order to reduce
power consumption, we propose E-ECC Scheme 2, which
activates only one rank with 9 chips. The proposed scheme
has lower reliability compared to Chipkill-Correct solutions
(including E-ECC Scheme 1) but lower power and higher
IPC performance (shown in Section 5). In this scheme, four
consecutive 8-bit symbols are read out from the same bank
in four consecutive beats. Since there are 9 chips, a total of
9 × 4 = 36 symbols are read out and fed to the RS (36,32)
based decoder. Figure 3 (a) illustrates the data access pattern and Figure 3 (b) illustrates the corresponding decoding

flowchart.
If no chips are marked faulty (left path in Figure 3 (b)),
and the syndrome vector is non-zero, the decoder does single error correction and triple error detection. While RS
(36,32) can be used to provide either (a) single and double
error correction or (b) single error correction and triple error detection, we choose (b) since it has significantly higher
detection capability. In fact, this code can fully detect up to
3 errors and can detect 4 errors with 99.9986% probability
(Section 5.5). In contrast, (a) cannot detect more than 2 errors and thus will lead to miss error correction in case of row,
bank/chip failures. We refer to the single error correction
and triple error detection as case v.
Scheme 2 can correct errors due to single bit failure, single
word failure and single column failure with 100% probability and these failures account for 62.8% [7] and 84.8% [20] of
all the failures. In case of row failure due to random errors,
Scheme 2 detects the errors with 99.9986% probability (Section 5.5) and relies on higher level protection mechanisms
such as roll back to recover the data. Thus, in such cases,
Scheme 2 will incur performance and energy loss due to use
of the higher level protection. In contrast, Scheme 1 can
correct transient errors due to row or bank failures on the
fly.
If a chip is marked faulty (right path in Figure 3 (b)),
all symbols from that chip are treated as erasures and the
4 consecutive erasure symbols are corrected by (case vi).
Thus, row failures due to persistent errors are treated as
erasures and corrected with 100% probability. While RS
(36,32) code can be used to correct 4 erasure symbols whose
addresses are not consecutive, the decoding is cumbersome.
However, correcting 4 consecutive erasure symbols is a lot
simpler and can be implemented by multiplication of a 4x4
matrix and a 4x1 vector in GF(28 ). The decoder needs to
store 9 inverse matrices corresponding to 9 different chip
failure locations. The decoding algorithm for correcting 4
consecutive erasures is given in the Appendix.

4.
4.1

METHODOLOGY
Simulation Infrastructure

We evaluate the timing performance of E-ECC schemes
using an open source full-system simulator, gem5 [29]. We

4.2

Workload Construction

We evaluate the performance impact of E-ECC schemes
using both sequential and multiprogrammed workloads. We
use 11 DRAM sensitive sequential applications from the
SPEC2006 benchmark suite (Table 3). We use Simpoints [32]
methodology to identify a single, 250-million instruction representative region for each sequential workload. We study
the memory access characteristics of these applications. Figure 4 shows the characterization of the DRAM memory access intensities as well as the read/write access intensities
for the applications under study. We discuss the performance implications based on these memory intensity characteristics in Section 5 in greater detail. In addition, to
evaluate the multi-core system performance, we create two
4-core multiprogrammed workload mixes of the aforementioned SPEC2006 benchmarks to realistically model the multiprogrammed application execution scenarios. The workload mixes are summarized in Table 3.
For our multiprogrammed simulations, in order to enable
the start of different benchmarks in each work load at the
same time, we fast-forward two billion instructions from the
program start and simulate in detail until all the benchmarks
have simulated for at least 250 million instructions. We collect the statistics after the slowest benchmark has completed
simulating 250 million instructions.

4.3

Decoder Synthesis Results

We implemented the E-ECC decoders in Verilog hardware
description language and synthesized with a 28nm industrial
process. Table 4 presents a summary of the latency of the

Table 2: Simulated system architectural details
2 GHz single core (quad-core for
Processor
multiprogrammed workloads), 4-way
out-of-order, 128-entry reorder buffer
32KB, 4-way assoc., 1 cycle access
L1-I Cache
latency, 64B block size
32KB, 8-way assoc., 1 cycle access
L1-D Cache
latency, 64B block size
1MB (4MB for multiprogrammed
Shared L2
workloads), 16-way assoc., 10 cycle
Cache
access latency, 64B block size
DDR3-1600 (800MHz DDR), FCFS
scheduling, open page policy, 13.75ns
Main Memory
precharge time, 13.75ns CAS latency,
13.75ns RAS to CAS latency, 8 Banks
per chip, 8KB row buffers

Reads / K Cycle

Writes / K Cycle

Average

zeusmp

soplex

sphinx3

omnetpp

mcf

libquantum

lbm

h264ref

cactusADM

12
10
8
6
4
2
0

bzip2

Mem Access / K Cycle

bwaves

Counts / Kilo Cycle

model a 4-way out-of-order processor with a two-level cache
hierarchy. We also model a detailed DDR3 DRAM. The
L1 instruction and data caches are private to each core and
the L2 cache is shared among the cores. We use the LRU
replacement policy for all caches. The configurations of our
setup is summarized in Table 2.
For our power estimations we use a detailed DRAM simulator DRAMsim2 [30]. We generate DRAM access traces
from the gem5 simulator setup. We then estimate the power
consumption of different schemes by simulating 2 million
DRAM access from these traces. The parameters of the
simulated DRAM used for power estimations match those
used for our timing performance evaluation and that of a
standard DDR3 DRAM as described in [31].

Figure 4: Read write intensities of SPEC2006 applications
different units. We parallelized the implementation as much
as possible to obtain a shorter latency. The two main important hardware components are finite field multiplication and
finite field inversion. We used a fully parallel implementation for finite field multiplication. For finite field inversion,
we implemented it with a look up table of size 256 × 8.
We implemented the syndrome calculation unit using 144
GF(28 ) multiplications followed by a tree of XOR gates.
Such a parallel implementation results in a latency of only
0.48ns (see Table 4). For comparison, we also synthesized
the syndrome calculation unit of (144,128) rotational code;
its latency is around 0.42ns.
For Scheme 1, single error correction (case i) takes an
additional 0.47ns making the total latency 0.95ns. Similarly,
double error correction (case ii) takes 1.55ns and correcting
two erasures (case iii) takes 1.26ns. When the decoder fails
to correct two erasures, it takes an additional 0.87ns to do
double erasure and single error correction (case iv).
For Scheme 2, single error correction and triple error detection (case v) takes the same additional time as case i.
The decoder first checks whether it is single error event. If
so, it activates single error correction; otherwise, it declares
there are errors. Thus, the critical path of case v is that of
single error correction and its latency is similar to case i.
When a chip is marked faulty and 4 consecutive errors have
to be corrected (case vi), it takes an additional 0.42ns.
The power consumption of syndrome calculation of RS
(36,32) code is 0.0192 mW (static) and 14.6 mW (dynamic)
for input switching probability 50%. In contrast, the power
consumption of syndrome calculation of (144,128) rotational
code used in Chipkill-Correct is 0.0083 mW (static) and
14.5mW (dynamic). Thus the ECC decoding power is less
than 0.1% of the DRAM power (see Section 5.2). The total area of Scheme 1 and Scheme 2 are 20,389 um2 and
8,994 um2 respectively which is larger than the area of the
(144,128) rotational code (5,000 um2 ) but is quite small
compared to that of a logic die.

Table 3: Sequential and multiprogrammed workloads.
Benchmarks
bwaves, bzip2, cactusADM, h264ref, lbm, libquantum,
mcf, omnetpp, soplex, sphinx3, zeusmp
mix1(bzip2, mcf, sphinx3, bwaves)
mix2(omnetpp, soplex, zeusmp, bwaves)

Table 4:
node)

E-ECC decoder latency results (28nm

RS (36,32) code
over GF(28 )
Syndrome Calculation
Corrects 1 error (case i)
Corrects 2 errors (case ii)
Corrects 2 erasures (case iii)
Corrects 2 erasures
and 1 error (case iv)
Corrects 1 error and
detects 3 errors (case v)
Corrects 4 consecutive
erasures (case vi)

5.

Scheme 1

Scheme 2

0.48ns (Λ)
Λ + 0.47ns
Λ + 1.07ns
Λ + 0.78ns
Λ + 0.78ns
+ 0.87ns
N/A

0.48ns (Λ)
N/A
N/A
N/A
N/A
Λ + 0.47ns

N/A

Λ + 0.42ns

PERFORMANCE EVALUATION AND
ANALYSIS

We evaluate the two E-ECC schemes by comparing the
timing performance to Chipkill-Correct (baseline). For our
evaluations, in order to achieve a consistent data output of
64B per request across all ECC schemes, we set the burst
length to 4 for Chipkill-Correct and E-ECC Scheme 1. For
E-ECC Scheme 2, we set the burst length to 8. Since the decoding latency of syndrome calculation of baseline, Scheme
1 and Scheme 2 are 0.42ns, 0.48ns and 0.48ns respectively,
we add one extra (memory) cycle penalty for both reads and
writes in our gem5 simulation.

5.1

Timing Performance Comparison

Figure 5 shows the IPC performance of the two ECC
schemes for a subset of the sequential and multiprogrammed
workloads. The IPC performance is normalized to the baseline. We measure the performance of multiprogrammed
workloads using the normalized average throughput metric.
The normalized average throughput is given by
GM (IP Ci ECCscheme )
, for (i = 0, 1, 2, 3). This metric is
GM (IP Ci Chipkill−Correct )
indicative of the overall throughput of the system.
The IPC improvement of E-ECC Scheme 1 is an average
of 3.2%, a maximum of 13.8% among the multiprogrammed
workloads and a maximum of 8.7% among the sequential
workloads. The IPC improvement of Scheme 2 is higher
with an average of 4.8%, a maximum of 31.5% for multiprogrammed workloads and a maximum of 15.6% for the
sequential workloads. Both Scheme 1 and Scheme 2 perform better than baseline due to the increase in rank-level
parallelism. Furthermore, Scheme 1 and Scheme 2 have close
performance for low memory access workloads, such as bzip2
and h264ref. However, Scheme 2 consistently performs better than Scheme 1 for high memory access workloads.
Qualitative comparison with existing schemes: Scheme 1
is likely to have slightly higher IPC performance compared
to V-ECC [11] since V-ECC has to update tier 2 ECC bits
even when ECC caching is used. LOT-ECC [13], Multi-ECC
[15] and E-ECC Scheme 2 will have better IPC performance
than V-ECC and E-ECC Scheme 1 due to better rank-level
parallelism. However, Multi-ECC has slightly lower IPC
than LOT-ECC (as pointed out in [15]) because Multi-ECC
updates the tier 2 checksum in another row while LOT-ECC
updates the global error correction bits in the same row. EECC Scheme 2 is likely to have better IPC since it does not
need to perform tier 2 updates.

5.2

Power Performance Comparison

We compare the DRAM power consumption of E-ECC
Schemes 1 and 2 with respect to the baseline system in a
fault-free memory system. Table 5 gives the details of the
memory configurations for each method.
Figure 6 shows the power consumption of the sequential
and multiprogrammed SPEC workloads normalized to the
baseline. The power consumption is only due to the DRAM
access; the power consumption due to ECC decoding is negligible (as shown in Section 4.3). E-ECC Schemes 1 and
2 have significantly lower power consumption compared to
the baseline. Scheme 1 reduces power consumption by an
average of 16.2%, a maximum of 21% among the multiprogrammed workloads and a maximum of 18.7% among the
sequential workloads. Scheme 2 reduces power consumption
by an average of 26.8%, a maximum of 36% among the multiprogrammed workloads and a maximum of 31% among the
sequential workloads. The reduction in power consumption
is mainly due to fewer chips being activated per access. Basically, Chipkill-Correct activates 36 chips per memory access
while Scheme 1 activates 18 chips per access and Scheme
2 activates 9 chips per access. We expect Scheme 1 to perform slightly better than V-ECC because Scheme 1 does not
need to update tier 2 ECC bits. This is also the reason why
we expect Scheme 2 to perform better than LOT-ECC and
Multi-ECC.
An analysis of the power consumption behavior of the
benchmarks shows that the power reduction achieved is a
function of the memory access intensity of the benchmarks.
For example, Figure 4 shows that bizp2 and h264ref have
lower memory access intensities. As a consequence of this,
we can see that power reduction achieved by both the ECC
schemes is lesser than that of the other benchmarks which
have higher memory access intensities such as bwaves, lbm.
Figure 8 shows power consumption of the different components, namely, background, refresh, burst and activate/
precharge (Act/Pre) averaged across all workloads. Background power is the largest and is related to the total memory size. The power consumption of Act/Pre is affected by
the number of activated chips. For each Act/Pre, one row
of data is loaded to sense amplifier. Activating more chips
means activating more number of rows. Hence, ChipkillCorrect consumes the largest Act/Pre power consumption
and E-ECC Scheme 2 consume the least Act/Pre power consumption. The burst power corresponds to the read/write
power and it is consumed when data moves in or out of the
memory devices. The refresh power is the power dissipated
from refreshing the data array and is mainly affected by the
total size of memory. Since the memory size is set to 18GB
for all schemes, the refresh power consumption is the same
for all.

5.3

Energy Performance Comparison

In this section, we present energy results (shown in Figure
7) that were calculated based on timing and power performances. Since energy is given by power x time, we multiply
the CPI and power consumption for each benchmark for 250
million instructions and normalize it to the baseline energy
results. Scheme 1 has an average energy efficiency of 18.9%,
a maximum of 28.7% among the multiprogrammed workloads and a maximum of 22.5% among the sequential workloads. Scheme 2 has an even higher energy efficiency with
an average of 30.3%, a maximum of 49.5% among the mul-

Geomean

mix2

mix1

zeusmp

E-ECC Scheme 2

sphinx3

soplex

omnetpp

mcf

libquantum

lbm

h264ref

cactusADM

bzip2

E-ECC Scheme 1

bwaves

IPC Normalized to Baseline

1.4
1.3
1.2
1.1
1
0.9
0.8

Figure 5: IPC performance of sequential and multiprogrammed applications

Table 5: Memory configurations of E-ECC Schemes and Baseline
ECC
Chipkill-Correct
E-ECC Scheme 1
E-ECC Scheme 2

Channel
1
2
2

I/O pin
x4
x8
x8

Ranks/Channel
1
1
2

tiprogrammed workloads and a maximum of 36.8% among
the sequential workloads.

5.4

Performance of Systems with Errors

Although we do not simulate the competing schemes when
there are errors in a chip or complete chip failure, here, we
briefly compare the performance of the systems under this
condition. When one chip fails or there are multiple errors
in a chip, Chipkill-Correct and E-ECC Scheme 1 can correct
these errors without additional memory reads. In contrast,
V-ECC, LOT-ECC and Multi-ECC all require additional
reads to correct errors. E-ECC Scheme 2 can detect multiple random errors in a chip with a very high probability
(99.9986% given in Section 5.5) and can correct all errors
from a chip that has been marked faulty without additional
reads.
In V-ECC x8, when an error event is detected by T1EC,
the memory controller has to retrieve the corresponding T2EC
symbols before error correction. If T2EC is not in the cache,
an additional memory access is needed to fetch T2EC. In
LOT-ECC, the GEC needs to be read out after the local
checksum detect errors. Hence, two additional reads are
required to perform error correction if ECC caching is not
used. In Multi-ECC, 256 additional memory accesses are
required to perform error correction. Multi-ECC also uses
ECC caching to reduce the frequency of column checksum
updates.
To reduce the performance overhead due to correction of
permanent errors, Multi-ECC relies on page retiring or chip
sparing. In contrast, in our scheme, the MC sends the address of the faulty chip to the E-ECC decoder and the corresponding erasures are corrected. Thus the proposed schemes
increase the lifetime of the device with very little overhead.

5.5

Reliability

In this section, we analyze the reliability of the competing schemes. For each error event, we provide the rates for
detectable and correctable errors (DCE), detectable but uncorrectable errors (DUE) and silent data corruption (SDC).
These rates are calculated by performing Monte Carlo simulations 10 million times. Table 6 summarizes the error
detection and correction capability for each scheme. The
most common error event is single bit failure. Since all the

Chips/Rank
36
18
9

Burst length
4
4
8

Total Size
18GB
18GB
18GB

Data Size
16GB
16GB
16GB

schemes can correct it with 100% probability, we do not include it in this table.
We first discuss the reliability of the three schemes that
activate two ranks, namely, Chipkill-Correct, V-ECC and
E-ECC Scheme 1. All these schemes have very good error
correction performance. They can all correct errors due to
a row failure (corresponds to multiple bit failures in a row),
a column failure (corresponds to multiple bit failures in a
single column) and a chip failure. When a chip fails and
there is an additional random error in another chip, ChipkillCorrect and V-ECC can detect the errors but cannot correct
them. If this failed chip is marked as faulty, E-ECC Scheme
1 can correct the errors due to the faulty chip and correct
the additional random error. E-ECC Scheme 1 can also be
designed to detect two additional random errors (when a
second chip fails) but has to choose between correcting one
random error and detecting two errors in the second chip.
Thus, of all the schemes that activate two ranks, E-ECC
Scheme 1 has the highest reliability.
Next, we consider the reliability of systems, which activate
on rank, such as LOT-ECC [13], Multi-ECC [15] and E-ECC
Scheme 2. In case of row failure, LOT-ECC detects the
errors with 87.5% probability [13] and Multi-ECC corrects
them with 100% probability. Since a row failure from a
single chip might lead to 1 to 4 errors in a codeword of EECC Scheme 2, we simulate two cases. In the first case, we
inject errors from 3 bits to 32 bits [33] in one of the nine
chips. For this case, the DCE rate is 0.8%, DUE rate is
99.1993% and SDC rate is 0.0007%. Next we simulate for
the worst case by injecting errors in 4 consecutive symbols
obtained from one of the nine chips. The DUE rate is now
99.9986% and the SDC rate is 0.0014% (listed in Table 6).
Thus in both cases, E-ECC Scheme 2 has very low SDC rate
for row failures.
In the case of a column failure, both LOT-ECC and EECC Scheme 2 can correct 100% of errors because a single
column failure is equivalent to a single bit error for both
schemes. Since Multi-ECC uses one’s complement to compute column checksums, if there are even number of bit failures along a column, the column checksum cannot locate
the errors. If every bit in a single column is flipped with
50% probability, then for 10 million runs, Multi-ECC has
75.0052% DCE, 24.9948% DUE and 0% SDC. The 0% SDC

Geomean

mix2

zeusmp

mix1

E-ECC Scheme 2

sphinx3

soplex

omnetpp

mcf

libquantum

lbm

h264ref

cactusADM

bzip2

E-ECC Scheme 1

bwaves

Power Consumption
Normalized to Baseline

1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5

Figure 6: Power consumption of sequential and multiprogrammed workloads normalized to Chipkill-Correct

Geomean

mix2

mix1

zeusmp

sphinx3

E-ECC Scheme 2

soplex

omnetpp

mcf

libquantum

lbm

h264ref

cactusADM

bzip2

E-ECC Scheme 1

bwaves

Energy Efficiency Normalized to
Baseline

1
0.9
0.8
0.7
0.6
0.5
0.4

Figure 7: Energy efficiency of E-ECC Scheme 1 and Scheme 2
3
2.5

Refresh

Burst

Act/Pre

Background

Watts

2
1.5
1

0.5
0

Chipkill-Correct E-ECC Scheme1 E-ECC Scheme2

Figure 8: Component power consumption breakdown
is because even though the column checksum cannot locate
errors, the row check symbols correctly detect the errors.
A single chip failure is a combination of multiple row or
column failures. Since LOT-ECC and E-ECC Scheme 2 can
not fully deal with row failures, both schemes have the same
correction and detection coverage as row failures. Similarly,
since Multi-ECC can not fully cope with column failures, it
has the same correction and detection coverage as column
failures.

6.

CONCLUSION

In this paper, we present two erasure and error correction
(E-ECC) schemes that improve the reliability of x8 memory systems while keeping the storage overhead at 12.5%
and requiring no extra memory access to perform error (and
erasure) correction. The increased reliability is due to the
use of a stronger code, namely, RS(36,32) over GF(28 ) and
activation of erasure correction mode when a chip is marked
faulty due to persistent errors. Our detailed performance
and power consumption evaluations show that the use of
this code does not degrade performance since the decoding

latency of this code is negligible and can be hidden in the
DRAM access latency. Of the two schemes, Scheme 1 activates two ranks, provides stronger reliability than ChipkillCorrect and has higher performance (increases IPC performance by a mean of 3.2% and reduces power consumption
by a mean of 16.2%). Scheme 2 has even better performance
(increases IPC by a mean of 4.8% and reduces power consumption by a mean of 26.8%) but still has good reliability
compared to the existing x8 DRAM single rank systems.
Overall, the proposed schemes have minimal overhead; they
do not require any change in the existing memory architecture and only affect the decoding circuitry in the memory
controller.

7.

ACKNOWLEDGMENTS

We would like to thank Supreet Jeloka for help with synthesis. This work was supported in part by the DARPAPERFECT program.

8.

APPENDIX

The two proposed schemes are based on the Reed Solomon
RS (36,32) code over GF(28 ). This code supports the following cases: (i) single error correction, (ii) double error
correction, (iii) double erasure correction (iv) double erasure and single error correction, (v) single error correction
and triple error detection (vi) four erasure correction. The
parity check matrix, H, of this code is given as follows


1
 1
H=
 1
1

1
α
α2
α3

1
α2
α4
α6

...
...
...
...

1
αi
α2i
α3i

...
...
...
...


1
35

α
,
α70 
105
α

where α is a primitive element of GF(28 ). Before a chip
is marked faulty, the decoder implements single error correction (case i) and double error correction (case ii). The
decoder can classify the two cases by using the syndrome
vector [34]. Let the syndrome vector be S = (s0 , s1 , s2 , s3 )T .

Table 6: Error detection and correction comparison
Error Type
Row failure

Column failure

One chip failure

ChipkillCorrect
DCE: 100%
DUE: 0%
SDC: 0%
DCE: 100%
DUE: 0%
SDC: 0%
DCE: 100%
DUE: 0%
SDC: 0%

Activates 2 ranks
V-ECC
E-ECC
[11]
Scheme 1
DCE: 100% DCE: 100%
DUE: 0%
DUE: 0%
SDC: 0%
SDC: 0%
DCE: 100% DCE: 100%
DUE: 0%
DUE: 0%
SDC: 0%
SDC: 0%
DCE: 100% DCE: 100%
DUE: 0%
DUE: 0%
SDC: 0%
SDC: 0%

The condition ss23 = ss21 = ss10 corresponds to a single error
event; otherwise, it is a double error event. We implement
the double error correction based on the method used in
[35, 36]. To solve the error locator polynomial, we use a
deterministic way to solve the roots rather than using Chien
search method. The error locator polynomial can be simplified to y 2 + y + K [35, 36], which can be solved by using
linearlized polynomials as shown in [36].
When there are two persistent errors in a chip, the MC
marks this chip as faulty and the corresponding symbols are
treated as erasures. For case iii, a failed chip leads to two
adjacent erased symbols in an E-ECC codeword. Assume
that the positions of two erased symbols are i and i + 1,
where i = 0, 2, ..., 34. The relation between syndrome vector
and the columns corresponding to the erased positions in H
is:






1
1
s0
 αi+1 
 αi 
 s1 



 s  = ei 
 α2i  + ei+1  α2(i+1) ,
2
3i
3(i+1)
s3
α
α

where ei and ei+1 are the erasure values for positions ith and
αi +s1
(i+1)th respectively. ei+1 can be derived by ei+1 = αs0i +α
i+1
and ei is obtained by ei = s0 +ei+1 . The decoded ei and ei+1
are substituted back into the s2 and s3 equations, namely,
s2 = ei · α2i + ei+1 · α2(i+1) and s3 = ei · α3i + ei+1 · α3(i+1) .
If these two conditions hold, the decoder classifies this as
a double erasure event and corrects these two erasure symbols. Otherwise, the decoder activates the double erasure
and single error correction unit corresponding to case iv.
The procedure for handling double erasure and single error
correction is as follows (case iv). Let the address of the error
symbol be j and the error value be ej , where j 6= i or i + 1.
The relation between syndrome vector and double erasure
and one error is given as follows:


1
s0
 αi
 s1 

 s  = ei  α2i
2
s3
α3i





1
i+1


 + ei+1  α

 α2(i+1)
α3(i+1)




1

 j
 + ej  α2j

 α
α3j






s1 α2i+1 +s2 αi +s2 αi+1 +s3
.
s0 α2i+1 +s1 αi +s1 αi+1 +s2
s0 αi+j +s1 αj +s1 αi +s2
The erasure value ei+1 is obtained by ei+1 = (αi+j +α2i+1 )(1+α)
j
αj +s1 +e1 αi+1
and the erasure value ei is obtained by ei = s0 α +e1 α
.
i +αj
Finally, ej is obtained by ej = s0 + ei + ei+1 .

The error address αj is derived by αj =

Scheme 2 performs case v when the chip is not marked
as faulty. The decoder first checks whether the condition
s3
= ss21 = ss10 holds. If the condition holds, it is a single
s2
error event and is corrected; otherwise, it reports errors.
When the chip is marked as faulty, the four symbols from

LOTECC [13]
DCE: 87.5%
DUE: 0%
SDC: 12.5%
DCE: 100%
DUE: 0%
SDC: 0%
DCE: 87.5%
DUE: 0%
SDC: 12.5%

Activates 1 rank
MultiECC [15]
DCE: 100%
DUE: 0%
SDC: 0%
DCE: 75.0052%
DUE: 24.9948%
SDC: 0%
DCE: 75.0052%
DUE: 24.9948%
SDC: 0%

E-ECC
Scheme 2
DCE: 0%
DUE: 99.9986%
SDC: 0.0014%
DCE: 100%
DUE: 0%
SDC: 0%
DCE: 0%
DUE: 99.9986%
SDC: 0.0014%

that chip are treated as erasures and corrected (case vi).
Recall that due to the access pattern (see Figure ??), the
four erasure symbols are in consecutive locations. Hence,
when chip i is marked as faulty, there are four erasures in
positions: 4i, 4i + 1, 4i + 2 and 4i + 3. The relation between
the syndrome and the four erasures are

s0
s


~s =  1  = A · ~e =
s2
s3
1
1
1
α4i+1
α4i+2
α4i+3
α2(4i+1) α2(4i+2) α2(4i+3)
α3(4i+1) α3(4i+2) α3(4i+3)




1
 α4i
 2(4i)
 α
α3(4i)

 

e4i
  e4i+1 
·

e4i+2 
e4i+3

Thus, ~e can be derived by A−1 · ~s, where A−1 matrices can
be pre-computed. Since there are only 9 chips in x8 DRAM,
9 different inverse matrices can be pre-computed and stored.
Once the address of the failed chip is known, the corresponding inverse matrix is fed into the E-ECC decoder. This computation is very simple; it requires only 16 GF(28 ) multiplications and 4 4-symbol-XOR computations.

9.

REFERENCES

[1] T. J. Dell. A white paper on the benefits of
Chipkill-Corret ECC for PC server main memory. In
IBM Microelectronics, 1997.
[2] T. J. Dell. System RAS implications of DRAM soft
errors. IBM Journal of Research and Development,
52(3):307–314, 2008.
[3] C. W. Slayman. Cache and Memory Error Detection,
Correction, and Reduction Techniques for Terrestrial
Servers and Workstations. IEEE Transactions on
Device and Materials Reliability, 5(3):397–404, 2005.
[4] S. Li, K. Chen, M.-Y. Hsieh, N. Muralimanohar,
C. Kersey, J. Brockman, A. Rodrigues, and N. Jouppi.
System Implications of Memory Reliability in Exascale
Computing. In Proceedings of the International
Conference on High Performance Computing,
Networking, Storage and Analysis, SC 11, pages 1–12,
Nov 2011.
[5] A. Hwang, I. Stefanovici, and B. Schroeder. Cosmic
Rays Don’t Strike Twice: Understanding the Nature
of DRAM Errors and the Implications for System
Design. SIGARCH Computer Architecture News,
pages 111–122, Mar. 2012.
[6] B. Schroeder, E. Pinheiro, and W. Weber. DRAM
Errors in the Wild: A Large-Scale Field Study. In
SIGMETRICS, pages 193–204, 2009.

[7] V. Sridharan and D. Liberty. A Field Study of DRAM
Errors. In International Conference for High
Performance Computing, Networking, Storage and
Analysis (SC), pages 1–11, Nov 2012.
[8] D. Locklear. Dell Enterprise System Group, Aug, 2000.
[9] T. R. Rao and E. Fujiwara. Error-Control Coding for
Computer Systems. Prentice-Hall Inc., 1989.
[10] C. L. Chen. Symbol error correcting codes for memory
applications. In Proceedings of Annual Symposium on
Fault Tolerant Computing, pages 200–207, 1996.
[11] D. Yoon and M. Erez. Virtualized ECC: Flexible
Reliability in Main Memory. MICRO, 31(1):11–19,
Jan. 2011.
[12] A. N. Udipi, N. Muralimanohar, N. Chatterjee,
R. Balsubramonian, A. Davis, and N. P. Jouppi.
Rethinking DRAM Design and Organization for
Energy-Constrained Multi-Cores. In 2010 Annual
International Symposium on Computer Architecture
(ISCA), pages 175–186, Jun. 2010.
[13] A. N. Udipi, N. Muralimanohar, R. Balsubramonian,
A. Davis, and N. P. Jouppi. LOT-ECC: Localized and
Tiered Reliability Mechanisms for Commodity
Memory Systems. In International Symposium on
Computer Architecture (ISCA), pages 285–296, Jun.
2012.
[14] X. Jian and R. Kumar. Adaptive Reliability Chipkill
Correct (ARCC). In IEEE International Symposium
on High Performance Computer Architecture, HPCA,
pages 270–281, Feb. 2013.
[15] X. Jian, H. Duwe, J. Sartori, V. Sridharan, and
R. Kumar. Low-power, Low-storage-overhead Chipkill
Correct via Multi-line Error Correction. In Proceedings
of the International Conference on High Performance
Computing, Networking, Storage and Analysis, SC13,
pages 1–12, 2013.
[16] Y. Son, S. O, Y. Ro, J. Lee, and J. Ahn. Reducing
Memory Access Latency with Asymmetric DRAM
Bank Organizations. In 2013 40th Annual
International Symposium on Computer Architecture
(ISCA), volume 41, pages 380–391, 2013.
[17] B. Jacob, S. Ng, and D. Wang. Memory Systems
Cache, DRAM, Disk. Morgan Kaufmann; first edition,
2007.
[18] V. Sridharan, J. Stearley, N. DeBardeleben,
S. Blanchard, and S. Gurumurthi. Feng Shui of
Supercomputer Memory: Positional Effects in DRAM
and SRAM Faults. Proceedings of the International
Conference on High Performance Computing,
Networking, Storage and Analysis, SC13, pages 1–11,
2013.
[19] N. DeBardeleben, S. Blanchard, V. Sridharan,
S. Gurumurthi, J. Stearley, K. Ferreira, and J. Shalf.
Extra Bits on SRAM and DRAM Errors - More Data
From the Field. Silicon Errors in Logic - System
Effects (SELSE-10), Stanford University, Apr. 2014.
[20] V. Sridharan, N. Debardeleben, S. Blanchard,
K. Ferreira, J. Stearley, J. Shalf, and S. Gurumurthi.
Memory Errors in Modern Systems: The Good, The
Bad and The Ugly. In Proceedings of the Twentieth
International Conference on Architectural Support for
Programming Languages and Operating Systems,
ASPLOS ’15, pages 297–310, 2015.

[21] S. Evain and V. Gherman. Error-correction schemes
with erasure information for fast memories. In Test
Symposium (ETS), 2013 18th IEEE European, pages
1–6, May 2013.
[22] D. Yoon, J. Chang, N. Muralimanohar, and
P. Ranganathan. BOOM: Enabling mobile memory
based low-power server DIMMs. In 39th Annual
International Symposium on Computer Architecture
(ISCA), pages 25–36, Jun. 2012.
[23] AMD64 architecture programmer’s manual revision
3.17. 2011.
[24] Reliability, Availibility, and Serviceability. Features of
the IBM eX5 Portfolio. 2012.
[25] Intel Xeon Processor E7 Family: Reliability,
Availability and Serviceability: Advanced data
integrity and resiliency support for mission-critical
deployment. 2011.
[26] HP: How memory RAS technologies can enhance the
uptime of HP Proliant servers. 2013.
[27] M. Sullivan J. Kim and M. Erez. Bamboo ECC:
Strong, Safe, and Flexible Codes for Reliable
Computer Memory. In IEEE International Symposium
on High Performance Computer Architecture, HPCA,
pages 101–112, 2015.
[28] Memory for Dell Poweredge 12th Generation Servers.
2012.
[29] N. Binkert, B. Beckmann, G. Black, S. Reinhardt,
A. Saidi, A. Basu, J. Hestness, D. Hower, T. Krishna,
S. Sardashti, R. Sen, K. Sewell, M. Shoaib, N. Vaish,
M. Hill, and D. Wood. The Gem5 Simulator.
SIGARCH Computer Archittecture News, 39(2), Aug.
2011.
[30] P. Rosenfeld, E. Cooper-Balis, and B. Jacob.
DRAMSim2: A Cycle Accurate Memory System
Simulator. IEEE Computer Architecture Letters,
10(1):16–19, Jan. 2011.
[31] Micron. DDR3 SDRAM System-Power Calculator,
2011.
[32] E. Perelman, G. Hamerly, M. Biesbrouck,
T. Sherwood, and B. Calder. Using Si’mPoint for
Accurate and Efficient Simulation. In Proceedings of
the 2003 ACM International Conference on
Measurement and Modeling of Computer Systems, In
SIGMETRICS, 2003.
[33] H. Jeon, G. Loh, and M. Annavaram. Efficient RAS
support for die-stacked DRAM. In IEEE International
Test Conference (ITC), pages 1–10, Oct. 2014.
[34] R.-H. Deng and D. J. Costello. Decoding of
DBEC-TBED Reed-Solomon Codes. IEEE
Transactions on Computers, C-36(11):1359–1363,
1987.
[35] S. Fenn, M. Benaissa, and D. Taylor. Decoding
double-error-correcting Reed-Solomon codes. IEE
Proceedings on Communications, 142(6):345–348,
1995.
[36] R. Berlekamp. Algebraic Coding Theory. McGraw-Hill,
1968.

ReMAP: Reuse and Memory Access Cost Aware
Eviction Policy for Last Level Cache Management
Akhil Arunkumar and Carole-Jean Wu
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University, Tempe, AZ 85281, USA
{Akhil.Arunkumar, Carole-Jean.Wu}@asu.edu
Abstract—To mitigate the signiﬁcant main memory
access latency in modern chip multiprocessors, multilevel on-chip caches are used to bridge the gap by
retaining frequently used data closer to the processor
cores. Such dependence on the last-level cache (LLC)
has motivated numerous innovations in cache management schemes. However, most prior works focus
their eﬀorts on optimizing cache miss counts experienced by applications, irrespective of the interactions
between the LLC and other components in the memory
hierarchy such as the main memory. This results in
sub-optimal performance improvements, since reducing
miss rates does not directly translate to increased IPC
performance.
In this paper, we show that in addition to the
recency information provided by the cache replacement
policy, post eviction reuse distance (PERD) and main
memory access latency cost are useful to make betterinformed eviction decisions at the LLC. We propose
ReMAP, Reuse and Memory Access Cost aware eviction policy, that takes reuse characteristics and memory
access behavior into consideration when making eviction decisions. ReMAP achieves higher performance
compared to prior works. Our full-system simulation
results show that ReMAP reduces the number of misses
of SPEC2006 applications by as much as 13% over
the baseline LRU replacement and by an average of
6.5% while MLP-aware replacement and DRRIP reduce
the miss counts by -0.7% and 5% respectively. More
importantly, ReMAP achieves an average of 4.6% IPC
performance gain across the SPEC2006 applications
while MLP-aware replacement and DRRIP see only
1.8% and 2.3% respectively.

I. Introduction
In modern computing systems, DRAM access latency is
the most signiﬁcant bottleneck hindering performance due
to the widening gap between the speed of the main memory
and the processor core. Multi-level on-chip SRAM caches
are used to bridge the gap by storing frequently-used data
closer to the core. Across the memory hierarchy, last level
caches (LLCs) play a crucial role in avoiding fetching data
from the main memory, and therefore it is important to
eﬀectively manage the performance of the LLC.
The dependence on LLC to support the growth of
next generation chip multiprocessors has resulted in numerous innovations in LLC management [1], [2], [5], [6],
[8], [12], [15], [18], [20], [24], [26], [31]. Most previous
works focus their eﬀorts on optimizing various aspects of
LLC performance, e.g., reducing miss rate [5], [14], [24]
or guaranteeing Quality-of-Service (QoS) [11], [23], [30].
These cache management policies use variations of recency
information of cache lines to determine the cache line
eviction/replacement policy. However, these policies often
optimize the performance of the LLC in isolation without
considering its interaction with the upper level caches, or
978-1-4799-6492-5/14/$31.00 ©2014 IEEE

110

the main memory. This results in sub-optimal performance
improvements since reducing miss rates does not directly
translate to increased Instruction-per-Cycle (IPC). In fact,
our studies show that such policies leave ample room
for improvement. In this paper, we show that additional
performance improvement can be achieved by taking the
LLC interaction with main memory into account while
making cache management decisions.
In the open page DRAM architectures, the cost of a
memory access can vary by a large amount between 30
and 300 cycles, depending on whether the memory access
experiences a row buﬀer hit or a bank conﬂict. Therefore,
not all LLC misses experience the same memory access
cost. This variation in the memory access latency presents
us with an opportunity to optimize the LLC management
scheme such as eviction or replacement policy by taking
the varying memory access cost into consideration.
Qureshi et al. [25] highlighted the potential of coordinating LLC management with DRAM access behavior.
Their proposal, MLP-aware cache management, uses the
varying memory access behavior to guide cache line replacement decisions to improve system performance. For
example, if the data in a particular cache line would
experience a longer memory access latency, the cache line is
less likely to be evicted from the LLC. However, this design
does not take into account the reuse distance information
for the cache line. If a cache line is not going to be reused
in the near future, even though it has a longer memory
access latency, such a cache line should be replaced by
data that will be reused by the processor to better utilize
the capacity of the LLC. Through our study, we observe
that the likelihood of reuse after a line is evicted and
the corresponding reuse distance (Post Eviction Reuse
Distance) provides information that can be utilized to
make better-informed eviction decisions at the LLC.
We propose ReMAP, Reuse and Memory Access Cost
aware eviction policy, that takes cache line reuse characteristics and memory access behavior into consideration when
making cache line eviction decision. ReMAP combines
recency, post eviction reuse distance, and memory access
cost information together to achieve higher performance
compared to prior works. We compare ReMAP with two
closely-related state-of-the-art prior works, RRIP [14] and
MLP-aware replacement [25]. Our full-system simulation
results show that ReMAP reduces the number of misses
of SPEC2006 applications by as much as 13% over the
baseline LRU replacement and by an average of 6.5%
while MLP-aware replacement and DRRIP reduce the miss
counts by -0.7% and 5% respectively. More importantly,
ReMAP achieves an average of 4.6% IPC performance
gain across the SPEC2006 applications while MLP-aware
replacement and DRRIP see only 1.8% and 2.3% respec-

tively.
Our paper makes the following contributions:
1) We identify the limitations of prior-art LLC management schemes that do not take into account crosshierarchy memory subsystem interactions, such as
non-uniform memory access time due to bank conﬂict
and row buﬀer access characteristics.
2) We develop a simple yet eﬀective cross-hierarchy
memory subsystem model to estimate the overall
access latency to the memory and use this estimation
to guide LLC replacement.
3) We devise a novel LLC management scheme that
determines the lifetime of cache lines using crosshierarchy information, including cache line reuse distance, recency, and memory access cost.
II. Background and Motivation
There has been a wide body of research literature that
is directed towards improving cache management. This
has led to many innovations in insertion, promotion and
replacement policies, and dead block prediction [1], [2], [5],
[6], [12], [15], [18], [20], [24], [31]. Most of the aforementioned studies focus on optimizing the LLC performance
in isolation. Though the improvement in LLC performance
leads to a signiﬁcant reduction in the gap between memory
and processor speeds, LLC is going to be most eﬀective
when its working is well coordinated with the levels of
memory above and below it, i.e., the L1 and L2 private
caches and the DRAM.
In the widely prevalent open page DRAM designs, not
all LLC misses experience a ﬁxed memory access cost. The
memory access cost can vary from approximately 15ns
to 150ns (for a 2GHz processor, this corresponds to 30
cycles to 300 cycles). This is owing to the fact that LLC
misses could result in row buﬀer hits, row buﬀer misses,
or map to conﬂicting banks in the memory. To address
this, Qureshi et al. [25] made a compelling case for taking
memory level parallelism into consideration while making
the LLC eviction decision. The memory access cost is lower
for parallel LLC misses because the cost is amortized over
multiple misses.
LLC misses that incur the least memory access cost
are the ones that 1) do not cause bank conﬂict and 2)
hit in the row buﬀer. The memory access cost incurred
by such a reference is proportional to the time taken to
place the data on the data lines from the sense ampliﬁers.
This is called Column Address Strobe (CAS) latency (CL).
Typically CL is about 15ns for a DDR3 SDRAM [7].
Therefore, for a 2GHz processor, the overall memory cost
to fetch data from the memory is,
M emoryCostrow buf f er hit ∝ CL ≈ 15ns = 30 cycles
(1)
However, when the row buﬀer of the bank does not
contain the row of data for the referenced address, a
row buﬀer miss is incurred. In this case, when the data
request is presented to the memory, the row that is open
in the row buﬀer is closed (row is precharged) and the row
corresponding to the new reference is brought to the row
buﬀer. Finally, the data is placed on the data lines. The
time taken to bring data into the row buﬀer is referred to
as Row Address Strobe (RAS) to Column Address Strobe
(CAS) delay. In this case, the overall memory access cost
becomes the sum of the time taken for row precharge
(tRP), time to bring data into row buﬀer (RAS to CAS
111

delay or tRCD), and CAS latency. Typically for a DDR3
SDRAM, tRCD and tRP are about 15ns [7]. Therefore for
a 2GHz processor, the overall cost to fetch data from the
memory is,
M emoryCostrow buf f er miss ∝ tRP + tRCD + CL
≈ 15ns + 15ns + 15ns = 45ns = 90 cycles (2)
When an LLC miss is mapped to a conﬂicting bank, the
memory access cost experienced varies (depending on the
memory access costs of earlier misses that are waiting to
be serviced by this particular bank), and a cascading eﬀect
inﬂuences the memory access cost. If two requests map
to a bank when another bank is idle, the second request
experiences a memory access cost that is the sum of the
memory access cost experienced by the ﬁrst access and
the second request’s own memory access cost. If both the
ﬁrst and second requests experience a row buﬀer miss, this
could be as high as 180 cycles (90 cycles for the ﬁrst request
and 90 cycles for this request).s
Next we use our characterization results to highlight
that we can choose the eviction candidates in the LLC
more intelligently, if the knowledge of the memory access
cost and reuse pattern is available at the time of cache line
replacement. For various SPEC2006 benchmarks, Figure 1
shows the memory access cost breakdown of the lines
that are evicted from a LRU-based LLC. Utilizing oracle
information, we also show the fraction of evicted lines that
would have been reused in the future. We notice that live
cache lines are being evicted from the LLC while
there are one or more other cache lines in the same
set, that are dead. Among all evicted cache lines, the
fraction of cache lines that are indeed dead (with no future
or distant reuse) is represented by the black bars labeled
“Dead Lines” in Figure 1. The adjacent grey bars show
the fraction of evictions when there is at least one cache
line in the same set, that is dead. The diﬀerence between
these two bars gives the fraction of times where a live line
was evicted even though a dead line was available in the
cache. We can see that this diﬀerence is signiﬁcant for
many benchmarks – 15% on an average and as much as
50%, of all evictions. This is because using only recency
information is ineﬀective in identifying the best cache
eviction candidates.
While rescuing cache lines that are still useful in the
near future can improve the LLC performance, we observe
that the memory access penalties for these cache lines
can vary signiﬁcantly. For example, some cache lines that
could be reused in the near future have longer memory
access cost than others. This leads to our second important
observation: cache lines being evicted from the LLC
are not always the ones that have the least memory
access cost. Often there are one or more cache lines in the
same set whose memory access cost is lesser than memory
access cost of the chosen eviction candidate. The darker
bars in Figure 1 represent the fraction of evictions where
live lines with higher memory access cost were evicted.
This undesirable behavior occurs to 10% of the evictions
on average (and can happen to as much as 60% of the evictions). For most of these occasions, there is opportunity to
convert higher memory access cost evictions to dead line
evictions or lower memory access cost evictions. Therefore,
we need a cache replacement policy that prioritizes cache
lines with longer memory access cost and farther or no
reuse over other lines in the set at the eviction time.
From the above insights, we can see that standard
recency-based cache replacement policies leave suﬃcient

Input: EvictionSet
//make eviction decision
for all lines in EvictionSet do
cache.line.Ef f ectiveCost = α ∗ cache.line.R +
β ∗ cache.line.PERD + γ ∗ cache.line.MAC;
if cache.line.Ef f ectiveCost ¡ cache.M inCost
then
cache.M inCost = cache.line.Ef f ectiveCost;
cache.EvictionCandidate = cache.line;
end
end
//insert to victim buﬀer
insert to vb(cache.EvictionCandidate);
Algorithm 1: ReMAP eviction decision algorithm.

room for improvement. Leveraging on post eviction reuse
distance (PERD) and memory access cost (MAC) information along with recency information can provide additional
performance beneﬁts. In this paper, we propose ReMAP,
Reuse and Memory Access Cost aware eviction policy,
that takes cache line reuse characteristics and memory
access behavior into consideration when making cache line
eviction decision. This allows ReMAP to mitigate the
two undesirable eﬀects described above and achieve higher
performance compared to other recency based policies.
III.

Reuse and Memory Access Cost Aware
Cache Replacement
Conceptually in an LRU-based cache replacement policy, each cache line in a cache set is given a reuse counter
that records how long ago the particular cache line was last
reused. For example in a 16-way cache, each cache line in
a set is assigned a number between 0 and 15. Every time a
cache line is accessed, its counter is reset to zero while all
other cache lines’ counters increase by 1. When a cache line
needs to be replaced, the eviction candidate is selected by
choosing the cache line that has the largest counter value.
In ReMAP, instead of assigning a predetermined counter
value as in LRU, we assign a cost to each cache line,
indicating the cost of evicting a particular cache line versus
keeping it in the cache. For example, the cost is higher if
a cache line to be evicted will experience a longer memory
access latency when it is accessed next time. Therefore
when selecting an eviction candidate, ReMAP looks at the
cost for each cache line in the set and picks the line that
has the least cost, contrary to an LRU-based system.
ReMAP determines the cost of the eviction candidate
by considering a cache line’s recency (R), predicted post
eviction reuse distance (PERD), and memory access cost
(MAC). While the recency information gives us an insight
into the line’s liveliness when the line is still in the cache,
PERD provides us with additional information about how
soon a line would be recalled into the cache after it has
been evicted. Finally, MAC provides additional information on the associated latency for main memory access
when the line gets recalled. These three vital pieces of
information help in assessing the worthiness of a cache
line. At the time of eviction, an eﬀective cost of each cache
line is determined using a linear relationship between the
aforementioned parameters.
Ef f ectivecost = α ∗ R + β ∗ P ERD + γ ∗ M AC (3)
Intuitively, the cache line with the least Ef f ectivecost
is less important. Therefore, ReMAP always selects the
cache line with the least Ef f ectivecost for eviction. The
pseudo code for cache line eviction selection is illustrated
in Algorithm 1.

A. Recency Estimation
Recency (R) is typically available from the underlying
cache replacement policy. For example, the recency counter
in a LRU based cache indicates how recently a cache line is
or will be used. Similarly, in another state-of-the-art cache
replacement policy, RRIP [14], the re-reference interval
predicted value (RRPV) provides a measure of the recency
of a cache line. We use RRPV in estimating a cache line’s
recency (R) component of the eﬀective cost calculation.
B. Post Eviction Reuse Distance Estimation
To learn a cache line’s reuse behavior and predict the
post eviction reuse distance, we use a bloom ﬁlter [4] based
victim buﬀer that records the address of every cache line
that is evicted from the cache. Upon every cache miss, the
victim buﬀer is looked up for the missing line address. We
empirically design the victim buﬀer to hold cache entries
entries. In order to facilitate our PERD estimation with a
practical hardware overhead, we design the victim buﬀer
by cascading three bloom ﬁlters in a hierarchical fashion as
shown in Figure 2. Upon eviction, a cache line is inserted
into the ﬁrst stage of the victim buﬀer. When the number
of entries in the victim buﬀer is equal to one-third the
number of entries in the cache, the entries in each stage is
ﬂushed down to the stage below it.
To classify the PERD for the entries in the victim
buﬀer, we use the following heuristics. If the missing
address is found within the ﬁrst 1/3∗cache entries (Stage
1 BF in Figure 2) in the victim buﬀer, the line’s PERD
is predicted to be “near”. If the missing address is found
within 2/3 ∗ cache entries (Stage 2 BF in Figure 2),
the line’s PERD is predicted to be “intermediate”. If the
missing address is found within cache entries (Stage 3
BF in Figure 2), the line’s PERD is predicted to be “far”.
If the missing address is not found within cache entries
entries, the line is predicted to have no reuse, or “dead”.

100
60

Dead Line Availability at
Eviction
Highest Cost Live Lines

40

Higher Cost Live Lines

Fig. 1.

20
Lower Cost Live Lines
Average

zeusmp

xalancbmk

sphinx3

soplex

sjeng

omnetpp

mcf

libquantum

lbm

hmmer

h264ref

cactusADM

bzip2

0
bwaves

% of All Evictions

80

Memory access cost and oracle reuse information of evicted lines at LLC for SPEC2006 benchmarks.

112

Lowest Cost Live Lines
Dead Lines

reuse distance, and memory access cost, for estimating the
cost of LLC misses. We qualitatively explore diﬀerent combinations of values for these parameters exhaustively. We
evaluated ReMAP for setups that give equal importance
to R, PERD, and MAC, higher importance to one of the
three, and lastly, giving higher importance to two of the
three, pieces of knowledge. Our results show that while
some applications beneﬁt from α = 1, β = 1, γ = 4
and some other applications beneﬁt from α = 1, β = 4,
γ = 1. Therefore, we implement a set dueling mechanism
that dynamically selects the eviction policy that minimizes
the total memory access cost (as compared to number of
misses in traditional set dueling schemes).
Fig. 2. Cascading bloom ﬁlter as victim buﬀer for PERD estimation

This predicted PERD is recorded with each cache line
using two additional bits. The PERD encoding is described
in Table I. The hardware overhead of the victim buﬀer is
described in the Section 4 and 6.2.3.
Post Eviction Reuse Distance Encoding.
Encoding
3
2
1
0

C. Memory Access Cost Determination
In addition to obtaining the recency and PERD information, we need to obtain the associated memory access
cost. To obtain the MAC, we use a small auxiliary structure called MAC estimation table. The MAC estimation
table holds a small memory access trace of current reference and previous references by storing the row addresses
of two references preceding the current reference for each
bank, and the number of references waiting to be serviced
by each bank. At the time of insertion, based on whether
the bank has requests waiting and if the current row
address matches the previous two row addresses, the MAC
is determined as described in Table II. The predicted MAC
of the new inserted cache line is then recorded and used to
calculate the overall eﬀective cost at the time of eviction.

MAC

Encoding

Lowest

0

Lower

1

Higher

2

Highest

3

Algorithm 2 shows the ReMAP algorithm and Figure 3
shows the hardware structures used in ReMAP. ReMAP
uses the PERD estimation victim buﬀer which is a multilevel bloom ﬁlter. Each cache line has two 2-bit ﬁelds to
record PERD and MAC estimations.
The PERD estimation victim buﬀer is implemented as
a set of three bit arrays as shown in Figure 2. These bit
arrays are of size 5 ∗ c bits, where c is 10. Each cache line
inserted into the victim buﬀer is represented by “k” bits.
These “k” bits are identiﬁed by a set of “k” hash functions.
The victim buﬀer hardware overhead for the above setup
is 18.75 KB. This additional hardware requirement is
reasonable given the signiﬁcant performance gain and is
comparable to the hardware overhead of recently proposed
state-of-the-art replacement policies [6][28][29].

Memory Access Cost Classification and
Encoding.

Access Type
No bank conﬂict;
row buﬀer hit
Bank conﬂict;
row buﬀer hit;
No bank conﬂict;
row buﬀer miss
Bank conﬂict;
at least one row buﬀer miss from earlier
misses

Algorithm 2: ReMAP algorithm.

We want to point out that this mechanism for estimating the MAC of an incoming cache line relies on the
similarity of row access behavior of the current and the
previous reference. As a previous study [25] showed, the
row buﬀer access patterns of the current and the previous
references in the LLC are highly correlated. We estimate
MAC based on this insight – row access behavior of current
and its previous references are similar. Table II shows
the diﬀerent memory access cost classiﬁcation used in this
study for diﬀerent bank and row access scenarios. We store
the predicted MAC per cache line, as a 2-bit value.
D. α, β, and γ Parameters in EﬀectiveCost Computation
The parameters α, β, and γ in the Ef f ectivecost calculation (Equation (3)) represent the importance of each
of the three pieces of knowledge, i.e recency, post eviction



	
 #, #, !


*
(-+' *
(.+'
-
.


 
	


$)&
&
	&
'''



("

Fig. 3.

113

"



TABLE II.

//Upon cache insertion
1. Attach PERD value to new cache line
2. Attach MAC value to new cache line
3. Complete cache insertion




PERD
0 < PERD <= 4
5 < PERD <= 9
10 < PERD <= 15
PERD > 15




Reuse Dist.
near
intermediate
far
distant

//Upon cache miss
1. Issue request to DRAM
2. Compute Eﬀective Cost for all cache lines (Oﬀ
critical path)
3. Find eviction candidate with minimum eﬀective
cost (Oﬀ critical path)
4. Perform PERD estimation victim buﬀer access
for missing address (Oﬀ critical path)
5. Perform MAC estimation for missing address
(Oﬀ critical path)



!

TABLE I.

IV. Implementation and Hardware Overhead
Input: cache access

ReMAP cache organization.

TABLE III.

TABLE V.

Simulated System Architectural Details

Processor
L1-I Cache
L1-D Cache
Uniﬁed L2
Cache
Shared L3
Cache
Main Memory

1 GHz, 4-way out of order, 128 entry reorder
buﬀer
32KB, 4-way associative, 1 cycle access
latency, 64B block size
32KB, 8-way associative, 1 cycle access
latency, 64KB block size
256KB, 8-way associative, 10 cycle access
latency, 64B block size
1MB per core, 16-way associative, 30 cycle
access latency, 64B block size
DDR3, FCFS scheduling, open page policy,
13.75ns precharge time, 13.75ns CAS
latency, 13.75ns RAS to CAS latency, 1
Channel, 8 Banks, 8 KB row buﬀers

TABLE IV.
Category
Memory Sensitive
Streaming or Large
Working Set

WL#
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

Sequential Workloads.
Benchmarks
h264ref, lbm, mcf, omnetpp,
soplex, sphinx3
cactusADM, libquantum

18

Apart from the PERD estimation victim buﬀer,
ReMAP consists of negligible logic overhead from the
eﬀective cost calculation and 4 bits per cache line to store
the line’s MAC and PERD values.
V. Experimental Methodology
A. Simulation Infrastructure
We evaluate ReMAP using an open source full system simulator, gem5 [3]. We model a 4-way out-of-order
processor with a 128-entry reorder buﬀer, a three-level
non-inclusive cache hierarchy, and a multi-channel, multibank DRAM. The memory hierarchy is based on an Intel
Core i7 system [10]. The L1 and L2 caches are private to
each core and implement the LRU replacement policy. The
conﬁgurations of our setup is summarized in Table 3. This
setup is similar to the setup used in other recent studies
[1], [5], [6], [12], [15], [18], [20], [24], [31].
We build ReMAP on top of a recently proposed cache
replacement policy, Static Re-Reference Interval Prediction (SRRIP)[14] because SRRIP requires less hardware
overhead than LRU and outperforms LRU [14].
B. Workload Construction
We evaluate ReMAP for both sequential and multiprogrammed workloads.
1) Sequential Workloads: For our study with sequential
workloads, we use 8 memory sensitive (MS), and streaming
or large working set (Str) benchmarks from the SPEC2006
benchmark suite. We use Simpoints [22] methodology to
identify single 250 million instruction representative region
for each benchmark and use this for our study. Table 4
shows the benchmarks used in our study.
2) Multiprogrammed Workloads: For our study with
multiprogrammed workloads, we add one memory sensitive
(MS), two streaming (str), and three compute intensive
(CI) benchmarks to model realistic multiprogrammed application execution scenarios. We choose 4-core combinations of the workload types and create 18 workload
mixes such that all combinations of the diﬀerent types are
covered. The workload mixes are shown in Table 5.
For our multiprogrammed simulations we fast-forward
two billion instructions from the program start and simulate in detail until all the benchmarks have simulated for
at least 250 million instructions. The benchmarks continue
to run after they have ﬁnished executing 250 million
instructions until all other benchmarks within that set
have completed simulating 250 million instructions. This
114

Multiprogrammed Workload Mixes.

Benchmarks
bzip2, mcf, omnetpp, soplex
bzip2, mcf, lbm, sphinx3
bzip2, mcf, omnetpp, sphinx3
bzip2, soplex, omnetpp, sphinx3
bzip2, mcf, sphinx3, bwaves
bzip2, mcf, omnetpp, libquantum
omnetpp, soplex, zeusmp, bwaves
sphinx3, mcf, libquantum, bwaves
mcf, sphinx3, cactusADM, zeusmp
zeusmp, libquantum, cactusADM,
bwaves
bzip2, mcf, sphinx3, hmmer
omnetpp, mcf, sphinx3, h264ref
bzip, soplex, h264ref, hmmer
bzip2, mcf, sjeng, hmmer
sphinx3, sjeng, h264ref, hmmer
sjeng, xalancbmk, h264ref, hmmer
xalancbmk, bwaves, h264ref, hmmer
libquantum, bwaves, cactusADM,
hmmer

Category
MS, MS, MS, MS
MS, MS, MS, MS
MS, MS, MS, MS
MS, MS, MS, MS
MS, MS, MS, Str
MS, MS, MS, Str
MS, MS, Str, Str
MS, MS, Str, Str
MS, MS, Str, Str
Str, Str, Str, Str
MS, MS, MS, CI
MS, MS, MS, MS
MS, MS, MS, CI
MS, MS, CI, CI
MS, CI, MS, CI
CI, CI, MS, CI
CI, Str, MS, CI
Str, Str, Str, CI

is done so that the faster benchmark continues to oﬀer
cache contention while the slower benchmark is running.
However, in such a case, the statistics are collected only
for the ﬁrst 250 million instructions.
VI. Experimental Results
We evaluate ReMAP by comparing its performance
with LRU, DRRIP [14] and the MLP-aware replacement
policy (MLP-aware) [25] as these policies are most closely
related to ReMAP. We perform sensitivity studies to
determine the weights given to R, MAC, and PERD
in the eﬀective cost computations. We observe that the
optimal conﬁguration varies from application to application. Therefore, we use set dueling [24] to determine the
weights for MAC, R, and PERD dynamically at runtime.
Speciﬁcally, for the eﬀective cost computation, we employ
set dueling between “α = 1, β = 1, γ = 4” and “α = 1,
β = 4, γ = 1” for our performance studies. Having values
which are powers of two for β and γ makes the hardware
implementation simpler. Unlike in [24], the policy selector
counter is updated based on the total MAC incurred by the
component policies instead of the total number of misses
incurred.
A. Sequential Workloads
Figures 4 and 5 compare the cache miss reduction
and IPC improvement experienced by the sequential workloads under the diﬀerent policies: DRRIP, MLP-aware,
ReMAP-16, ReMAP-best-VB. ReMAP-16 represents the
performance of ReMAP with 16 entry per set victim
buﬀer and ReMAP-best-VB represents the performance
of ReMAP when victim buﬀer size is ﬁne tuned for each
benchmark. To identify the best victim buﬀer setup for
each benchmark, we search through victim buﬀers having 8
through 64 entries per cache set. Though most applications
are found to perform best when the victim buﬀer sizes are
less than 24 entries, applications such as cactusADM, mcf,
and soplex perform best with victim buﬀers containing
upto 48 entries per cache set.
While all three policies reduce application LLC misses,
the miss reduction does not always translate to IPC performance improvement. For example, for sphinx3, DRRIP
reduces the number of misses the most, but because of
the memory access cost disparity among the misses, the
beneﬁt of miss reduction is not reﬂected in application IPC
performance improvement. Overall, ReMAP reduces the
number of misses of SPEC2006 applications by as much
as 13% over the baseline LRU replacement and by an

sphinx3
sphinx3

Average

soplex
soplex

mcf

omnetpp

ReMAP-best-VB

libquantum

ReMAP-16

lbm

MLP Aware

h264ref

cactusADM

MPKI Reduction Compared
to LRU (%)

1.12

DRRIP

MLP Aware

ReMAP-16

ReMAP-best-VB

1.08
1.04
1
Average

omnetpp

mcf

libquantum

lbm

h264ref

cactusADM

0.96

ReMAP: Improvement in IPC over LRU

ReMAP adopts a holistic approach towards LLC management and this is highlighted in the cases of libquantum
and soplex. For both these applications all three policies achieve similar MPKI reduction. However, ReMAP
achieved superior IPC improvement compared to DRRIP
and MLP-Aware policies.
B. ReMAP Sensitivity to Various Design Choices
1) Beneﬁt of Using PERD and MAC Information in
Isolation: In order to understand the contributions of
each of the individual components of ReMAP, i.e. the
post eviction reuse distance and memory access cost, we
study the performance beneﬁt achieved by each component
in isolation for a few interesting applications. Figure 6
shows the performance results of using PERD and MAC
information in isolation, on top of SRRIP. SRRIP-PERD
represents the setup where only PERD information is
115

SRRIP

SRRIP - MAC

SRRIP - PERD

ReMAP

1.08
1.04
1
Average

sphinx3

libquantum

0.96
h264ref

To illustrate the importance of considering the post
eviction reuse distance and memory access cost in LLC
management, we take a closer look at cactusADM. Figure 4
shows that all three replacement policies reduce the LLC
miss count for cactusADM by 3-5%. However, because
not all cache lines are equally important, the LLC miss
count reduction does not translate to IPC performance improvement linearly. Figure 5 shows that ReMAP improves
the performance of cactusADM by 2% while DRRIP and
MLP-aware improve its performance by 0.1% and 0.5%
respectively. The reason for the IPC performance gap can
be explained by Figure 1. The ﬁgure shows that, for 40% of
cache line evictions, a live line with higher memory access
cost is chosen as the eviction candidate under LRU. In
contrast, ReMAP is able to identify and prioritize cache
lines with higher memory access cost over those with lower
memory access cost.

1.12

cactusADM

average of 6.5% while MLP-aware replacement and DRRIP
reduce the miss counts by -0.7% and 5% respectively. More
importantly, when looking at application IPC performance
improvement, ReMAP-best-VB achieves an average of
4.6% performance gain across the SPEC2006 applications
while MLP-aware replacement and DRRIP see only 1.7%
and 2.3% respectively. Here onward, to maintain generality
we only discuss the results of ReMAP-16.

IPC Normalized to LRU

Fig. 5.

DRRIP

ReMAP: Reduction in LLC Misses over LRU
IPC Improvement Compared
to LRU

Fig. 4.

15
10
5
0
-5
-10

Fig. 6. ReMAP: performance using PERD and MAC information
in isolation

used to make eviction decisions along with the recency
information. Similarly SRRIP-MAC represents the setup
where only MAC information is used to make eviction decisions along with recency information. Figure 6 highlights
the importance of the considering all three parameters,
recency, PERD, and MAC, together while making the
eviction decision.
2) Sensitivity to Victim Buﬀer Storage: The multi-level
bloom ﬁlter based victim buﬀer consists of three stages
of bloom ﬁlters cascaded together. It can be seen that
the hardware overhead depends largely on the number of
bit array size of the bloom ﬁlter. As the bit array size
increases, the bloom ﬁlter false positive rate decreases. We
observe that we can achieve a reasonable accuracy (false
positive rate 1%) when the bit array size is 10x the number
of entries in the victim buﬀer. Furthermore, as the number
of entries in the victim buﬀer increase, we will be able
to capture the reuse behavior more accurately. However if
the number of entries is very large, the reuse behavior can
be misguided and in turn hurt performance. We observe
a similar trend in our experiments. This behavior can be
seen in Figure 7.
C. ReMAP Sensitivity to Various System Parameters
ReMAP performance can be inﬂuenced by system
parameters such as the baseline replacement policy and
memory scheduling policy. In addition to RRIP, we conducted experiments with ReMAP built on top of the LRU

Average IPC Improvement
Compared to LRU

1.05

Fig. 7.

[18], [20], [21], [26], [24], [31], we discuss prior works that
closely resemble ReMAP in this section.

1.04
1.03

1.02
1.01
8
16
48
64
Best
Number of VB Entries Per Cache Set (16-way LLC)

ReMAP:sensitivity to victim buﬀer storage

policy. We observe that ReMAP shows similar performance
improvement when the recency information is provided by
LRU.
We also conducted studies to understand the sensitivity
of ReMAP to architectural parameters such as cache sizes
and cache set associativities. We conduct our sensitivity
study with cache sizes from 1MB through 32MB, and
associativities from 16-way, through 64-way conﬁgurations.
We observe that ReMAP continues to provide signiﬁcant
performance beneﬁt ranging from 2% to 7% on average and
as high as 25% in case of benchmarks such as libquantum
and mcf.
The performance of ReMAP can be sensitive to the
underlying memory scheduling policy. Diﬀerent memory
scheduling policies can alter the MAC of the cache lines
diﬀerently. Furthermore, estimating MAC under more sophisticated policies can be non trivial. In such cases, MAC
value can be communicated from the main memory to the
last level cache with negligible overhead on the bandwidth
demand. We expect that the MAC information and PERD
information will continue to be important pieces of information that can assist LLC management even under more
sophisticated memory scheduling policies as well.
D. Multiprogrammed Workloads
We evaluate the heterogeneous multiprogrammed
workloads for overall system throughput and fairness.
We measure overall system throughput using the normalized average throughput and normalized average weighted
speedup metrics. The normalized average throughput is
GM (IP C
)
, for (i = 0, 1, 2, 3). This metric
given by GM (IP Cii policy
LRU )
indicates the overall throughput improvement of the system. We use the minimum normalized throughput achieved
across all threads as a fairness metric. This metric gives
a conservative measure of fairness of the new policy relative to the fairness of the baseline. This is given by
Ci policy)
M ini ( (IP
(IP Ci LRU ) ).
Figure 8 summarizes the normalized average throughput achieved by ReMAP, MLP-Aware [25], and TADRRIP [13] policies for diﬀerent workload mixes. Across
all workload mixes, ReMAP improves average throughput
by 2.5% compared to LRU and TA-DRRIP improves by
1.8%. MLP-Aware policy by -14% compared to LRU.
Overall ReMAP performs better than both TA-DRRIP
and MLP-aware policies on the fairness metric as well.
ReMAP gives a normalized minimum throughput of 0.9
compared to LRU while TA-DRRIP and MLP-aware policies give 0.8 and 0.7 respectively.
VII. Related Work
Though we cannot summarize all the innovations in
cache management research [1], [2], [5], [6], [8], [12], [15],
116

A. Reuse Distance Prediction
Jaleel et al. [14] proposed SRRIP and DRRIP to
learn reuse behavior of applications and manage the last
level cache accordingly. DRRIP provides both scan and
thrashing resistance by performing set-dueling [24] between its two component policies, SRRIP and BRRIP.
SRRIP provides scan resistance by inserting cache lines
with “long” reference interval prediction. BRRIP on the
other hand, provides both thrashing resistance by predicting “distant” re-reference interval most of the times
and “long” re-reference interval infrequently. The recently
proposed EAF-cache [28] predicts whether a cache line
will receive reuse or not at the time of insertion, based
on it’s own past behavior. Though RRIP and EAF-cache
predict the reuse behavior of cache lines, their predictions
are limited to insertion time. On the contrary, ReMAP uses
post eviction reuse distance prediction. This helps ReMAP
to predict if a cache line will be recalled to the cache or
not, and also how soon would a line be recalled once it is
evicted from the cache.
Rajan et al. proposed shepherd cache [27] to emulate
optimal replacement in the cache. They use four shepherd
ways in a 16-way cache that will keep track of partial
reuse distances and allowing to evict the the lines that are
reused farther into the future. Their proposal allows for
limited look ahead and ReMAP on the other hand is able
to predict reuse distances much which are much farther,
upto three times the associativity of the cache.
B. Dead Block Prediction
Many works have also used a variation of recency,
instruction traces, or address traces to predict blocks that
are dead [9], [16], [19]. Sampling Dead Block Prediction
[17] proposed by Khan et al. predicts cache blocks that are
dead in the cache based on the last touched instructions.
They replace these predicted dead blocks prior to the LRU
replacement candidate. Chaudhuri et al. proposed cache
hierarchy-aware replacement (CHAR) [6] policy where
they mined the private L2 cache eviction stream for information that identiﬁes certain blocks to be dead and passed
the hint to the LLC.
While ReMAP’s PERD estimation is similar to dead
block prediction, it is important to note that ReMAP
predicts the reuse distance in a ﬁner grainularity, and
this paper shows that the ﬁner-grained PERD prediction
contributes to further performance improvement.
C. Coordinating LLC Management with DRAM
Qureshi et al. ﬁrst identiﬁed the potential in considering DRAM access costs in managing the LLC in [25].
They assign cost to each cache line based on the amount
of memory level parallelism the cache line would present
when it misses in the cache. They adopt a linear relationship between recency and the MLP cost of cache lines to
determine the total cost. We present a more ﬁne-grained
memory access cost analysis and combine that with post
eviction reuse distance along with recency information
to assign eﬀective costs to cache lines. This coordinated
approach enables ReMAP to provide higher performance
improvement than MLP-aware replacement.

Fig. 8.

[11]

Acknowledgment
We would like to thank the anonymous reviewers for
their feedback. This work is supported by Arizona State
University.

[20]

References

[22]

[2]
[3]

[4]
[5]
[6]

[7]
[8]

[9]

[10]

Average

mix18

mix17

mix16

mix15

mix14

mix13

0.69

1.5

ReMAP: Performance for multiprogrammed workloads

VIII. Conclusion
The miss rate reduction achieved by most state-of-theart cache management policies does not translate to corresponding IPC improvement at all times. This is because
of the wide disparity in memory access costs experienced
by diﬀerent LLC misses. Hence, it is vital to manage the
last level cache while considering memory access behavior
of cache lines. Furthermore, the system performance is
understandably aﬀected by the current and future reuse
of cache lines. With this insight, we proposed ReMAP, a
reuse and memory access cost aware eviction policy that
takes recency, post eviction reuse distance, and memory
access costs to make better-informed eviction decisions at
LLC. ReMAP is able to preserve most valuable cache lines,
i.e lines that have near reuse and high memory access cost,
in the LLC and thereby providing superior performance.
We demonstrate with extensive performance evaluation
using wide variety of workloads that ReMAP performs
consistently better than related state-of-the-art policies
such as MLP-aware replacement, DRRIP and TA-DRRIP.

[1]

1.4

1.4

mix11

0.65

mix10

mix9

0.62

mix8

mix7

1.4

mix12

ReMAP

mix6

mix5

mix3

mix4

MLP Aware

0.69 0.68 0.66 0.66

mix2

mix1

Average Throughput
Normalized to LRU

TA-DRRIP
1.25
1.2
1.15
1.1
1.05
1
0.95
0.9
0.85
0.8
0.75

A. Basu, N. Kirman, M. Kirman, and M. Chaudhuri. Scavenger:
A new last level cache architecture with global block priority.
In MICRO 2007.
L. A. Belady. A study of replacement algorithms for a virtual
storage computer. In IBM Syst. J., volume 5, June 1966.
N. Binkert, B. Beckmann, G. Black, S. K. Reinhardt, A. Saidi,
A. Basu, J. Hestness, D. R. Hower, T. Krishna, S. Sardashti,
R. Sen, K. Sewell, M. Shoaib, N. Vaish, M. D. Hill, and D. A.
Wood. The gem5 simulator. SIGARCH Comput. Archit. News,
39(2):1–7, Aug. 2011.
B. H. Bloom. Space/time trade-oﬀs in hash coding with
allowable errors. Commun. ACM, 13(7):422–426, July 1970.
M. Chaudhuri. Pseudo-LIFO: the foundation of a new family
of replacement policies for LLCs. In MICRO 2009.
M. Chaudhuri, J. Gaur, N. Bashyam, S. Subramoney, and
J. Nuzman. Introducing hierarchy-awareness in replacement
and bypass algorithms for last-level caches. In PACT 2012.
Micron DDR3 SDRAM http://www.micron.com/products/
dram/ddr3-sdram.
H. Dybdahl and P. Stenstrom. An adaptive shared/private nuca
cache partitioning scheme for chip multiprocessors. In HPCA
2007.
Z. Hu, S. Kaxiras, and M. Martonosi. Timekeeping in the
memory system: Predicting and optimizing memory behavior.
In ISCA 2002.
Intel Core i7 Processors http://www.intel.com/products/processor/corei7/.

117

[12]

[13]

[14]

[15]
[16]
[17]
[18]

[19]

[21]

[23]

[24]

[25]
[26]

[27]
[28]

[29]

[30]

[31]

R. Iyer. CQoS: A framework for enabling qos in shared caches
of cmp platforms. In ICS 2004.
A. Jaleel, E. Borch, M. Bhandaru, S. Steely, and J. Emer.
Achieving non-inclusive cache performance with inclusive
caches: Temporal locality aware (TLA) cache management
policies. In MICRO 2010.
A. Jaleel, W. Hasenplaugh, M. Qureshi, J. Sebot, S. Steely, Jr.,
and J. Emer. Adaptive insertion policies for managing shared
caches. In PACT 2008.
A. Jaleel, K. B. Theobald, S. C. Steely, Jr., and J. Emer.
High performance cache replacement using re-reference interval
prediction (RRIP). In ISCA 2010.
G. Keramidas, P. Petoumenos, and S. Kaxiras. Cache replacement based on reuse-distance prediction. In ICCD 2007.
S. M. Khan, D. A. Jiménez, D. Burger, and B. Falsaﬁ. Using
dead blocks as a virtual victim cache. In PACT 2010.
S. M. Khan, Y. Tian, and D. A. Jimenez. Sampling dead block
prediction for last-level caches. In MICRO 2010.
M. Kharbutli and Y. Solihin. Counter-based cache replacement
and bypassing algorithms. In IEEE Trans. Comput., volume 57,
April 2008.
A.-C. Lai, C. Fide, and B. Falsaﬁ. Dead-block prediction &amp;
dead-block correlating prefetchers. In ISCA 2001.
H. Liu, M. Ferdman, J. Huh, and D. Burger. Cache bursts: A
new approach for eliminating dead blocks and increasing cache
eﬃciency. In MICRO 2008.
J. Meng and K. Skadron. Avoiding cache thrashing due to
private data placement in last-level cache for manycore scaling.
In ICCD 2009.
E. Perelman, G. Hamerly, M. Van Biesbrouck, T. Sherwood,
and B. Calder. Using simpoint for accurate and eﬃcient
simulation. In SIGMETRICS 2003.
R. Prabhakar, S. Srikantaiah, R. Garg, and M. Kandemir. Qos
aware storage cache management in multi-server environments.
In PPoPP 2011.
M. K. Qureshi, A. Jaleel, Y. N. Patt, S. C. Steely, and J. Emer.
Adaptive insertion policies for high performance caching. In
ISCA 2007.
M. K. Qureshi, D. N. Lynch, O. Mutlu, and Y. N. Patt. A case
for MLP-Aware cache replacement. In ISCA 2006.
M. K. Qureshi, D. Thompson, and Y. N. Patt. The v-way cache:
Demand based associativity via global replacement. In ISCA
2005.
K. Rajan and R. Govindarajan. Emulating optimal replacement
with a shepherd cache. In MICRO 2007.
V. Seshadri, O. Mutlu, M. A. Kozuch, and T. C. Mowry. The
evicted-address ﬁlter: A uniﬁed mechanism to address both
cache pollution and thrashing. In PACT 2012.
C.-J. Wu, A. Jaleel, W. Hasenplaugh, M. Martonosi, S. C.
Steely, Jr., and J. Emer. SHiP: Signature-based hit predictor
for high performance caching. In MICRO 2011.
C.-J. Wu and M. Martonosi.
Adaptive timekeeping replacement: Fine-grained capacity management for shared cmp
caches. ACM Trans. Archit. Code Optim., 8(1):3:1–3:26, Feb.
2011.
Y. Xie and G. Loh.
PIPP: Promotion/insertion pseudopartitioning of multi-core shared caches. In ISCA 2009.

Improving Smartphone User Experience by Balancing
Performance and Energy with Probabilistic QoS Guarantee
Benjamin Gaudette, Carole-Jean Wu, and Sarma Vrudhula
School of Computing, Informatics, & Decision Systems Engineering
Arizona State University
Tempe, AZ 85281
Email: {bgaudett,carole-jean.wu,vrudhula}@asu.edu

ABSTRACT

delay could potentially cost you $2.5 million in lost sales
every year." From this perspective, it is imperative to ensure
user satisfaction even at the cost of additional power; however, due to ﬁnite energy capacity, it is not typically optimal
for mobile platforms to sustain high performance states, and
as this paper will demonstrate, providing guarantees on QoS
requires a different approach that takes into account the distribution of page load times.
Over the past decade, a large body of research has been
published on optimizing energy efﬁciency [10, 13, 16, 18, 23,
24, 38, 43, 44, 47]. However, most, if not all of the work,
have assumed that the underlying quantities (e.g., execution
times) are deterministic. In [27], Isci et al. considered voltage and frequency (DVFS) as a means to dynamically tune
a processor’s power consumption based on an application’s
computation and memory characteristics. Others considered
thermal constraints and leakage power when optimizing energy efﬁciency [20, 22, 28, 31, 33, 35].
While most of the prior works focus on optimizing system energy efﬁciency for the application domain of chipmultiprocessors (CMPs), a few more recent works have proposed energy efﬁciency optimization algorithms for usercentric, interactive smartphone applications. Egilmez et al. [14]
and Singla et al. [50] proposed temperature-aware energy efﬁciency optimization while Zhu et al. [52] focused on meeting interactive application QoS. Egilmez et al. [14] specifically aimed at user satisfaction through DVFS-based control knobs to maintain a low and comfortable temperature.
Singla et al. [50] developed power and thermal models for a
modern mobile platform and proposed a closed loop thermal
control to adjust processor frequencies. Zhu et al. [52] proposed eQoS to improve the energy efﬁciency of web browsers
for smartphones while meeting certain QoS constraints.
Although the problems addressed in the existing literature
are complex in and of themselves, they have assumed that
the execution time of an application is deterministic. In reality, execution times vary substantially, depending on the
continuously varying states of the system. This is due to factors such as operating system (OS) preemption events, page
faults, interrupts, processor pipeline stalls, branch prediction, cache misses, context switches, data input variations,
network delays, etc. While considering only the average
case may result in higher energy efﬁciency on average, not
accounting for the variations in execution time can lead to
situations where user satisfaction is poor. For these reasons,
the execution time of an application must be modeled as a
stochastic value, whose distribution ideally should depend
on the control variables (voltage and frequency) and other
parameters that capture the characteristics of the data and

User satisfaction is pivotal to the success of a mobile application. A recent study has shown that 49% of users would
abandon a web-based application if it failed to load within 10
seconds. At the same time, it is imperative to maximize energy efﬁciency to ensure maximum usage of the limited energy source available to smartphones while maintaining the
necessary levels of user satisfaction. An important factor to
consider, that has been previously neglected, is variability of
execution times of an application, requiring them to be modeled as stochastic quantities. This changes the nature of the
objective function and the constraints of the underlying optimization problem. In this paper, we present a new approach
to optimal energy control of mobile applications running on
modern smartphone devices, focusing on the need to ensure
a speciﬁed level of user satisfaction. The proposed statistical
models address both single and multi-stage applications and
are used in the formulation of an optimization problem, the
solution to which is a static, lightweight controller that optimizes energy efﬁciency of mobile applications, subject to
constraints on the likelihood that the application execution
time meets a given deadline. We demonstrate the proposed
models and the corresponding optimization method on three
common mobile applications running on a real Qualcomm
Snapdragon 8074 mobile chipset. The results show that the
proposed statistical estimates of application execution times
are within 99.34% of the measured values. Additionally, on
the actual Qualcomm Snapdragon 8074 mobile chipset, the
proposed control scheme achieves a 29% power savings over
commonly-used Linux governors while maintaining an average web page load time of 2 seconds with a likelihood of
90%.

1

Introduction

In recent years, there has been an explosive growth in the use
of mobile platforms—especially smartphones—for our everyday computing needs. Applications developed for these
devices serve as gateways for many businesses to interact
with their clients; however, in a recent survey it has been
found that quality of service (QoS) and user satisfaction of
web-based applications plays a pivotal role in the ﬁnancial
success of a business [4]. In fact, the study found that 19%
of the users would abandon a page requiring more than 5
seconds to load and 49% of the users would abandon the
page after 10 seconds. Perhaps worse than this, 79% of the
participants stated that if they are dissatisﬁed with their user
experience, they are less likely to shop from the site again.
By extrapolating this data, the study concluded that “If an ecommerce site is making $100,000 per day, a 1 second page
c
978-1-4673-9211-2/16/$31.00 2016
IEEE

52

PPW- Energy Efficiency Execution Time (sec)

(a)

0%

30%

60%

100%

100%

100%

Δ20

deadline
20 FPS

Δ30

deadline
30 FPS

stochastic quantities. Each application consists of some number of computational segments or basic units of computation
(UoC), with each UoC being mapped to a core. Variations
in execution times are due to different sources of interruptions in the normal ﬂow of computation, as well as intrinsic variations in the complexity of the data being processed.
Note that the notion of a UoC varies among applications. We
ﬁrst hypothesize, and then experimentally validate a model
(distribution function) of the execution times of UoCs. The
model parameters are functions of the control variables (e.g.,
voltage and frequency), as well as characteristics of the data
being processed (e.g., complexity of the web page component being processed).
The model of the total execution time of an application depends on how its constituent UoCs are mapped to the cores.
For example, some applications have independent UoCs executing in parallel on separate cores, while in others, the
UoCs interact. This leads to different ways of modeling the
total execution time for different applications. The proposed
statistical models are used in the formulation of an optimization problem, the solution to which is a static, lightweight
controller that optimizes energy efﬁciency of mobile applications, subject to constraints on the likelihood that the application execution time meets given deadlines.
The statistical models for single and multi-stage applications and the design of the controller are demonstrated on
three commonly used smartphone applications: web browsing, image similarity search, and video playback, and on two
different real platforms: an Intel Quad-Core Sandy-Bridge
(SNB) processor and a Qualcomm Snapdragon 8074 chipsetbased mobile device. The results show that the proposed statistical estimates of application execution times are within
99.34% of the measured values. We then demonstrate an
application case study for which these estimates are applied
to select the frequency setting when accounting for device
energy efﬁciency and an execution time deadline. Our proposed optimization achieved a 29% power savings over commonlyused Linux governors while maintaining an average web page
load time of 2 seconds with a likelihood of 90%—a quality
that the Linux governors do not consider. We achieve similar
ﬁndings for the other mobile applications: image similarity
search and video playback.
In summary, this paper proposes an accurate execution
time model that provides a quantative measure of the likelihood of meeting a deadline – an overlooked dimension of
execution time modeling. To demonstrate the importance of
the developed model, we build the model for a modern, highperformance real smartphone device to improve user experience while providing a probabilistic guarantee on QoS. By
doing so, on average the QoS is improved by 2.18x at a cost
of a 25% degradation in power efﬁciency when compared to
the optimal energy efﬁciency setting.

*Low QoS

0%

0%

0%

25%

50%

99.9%

f1

f2

f3

f4

f5

f6

Frequency Setting (GHz)

(b)

Optimal f setting (f4)

f1

f2

f3

f4

f5

Frequency Setting (GHz)

*High QoS

f6

Figure 1: Conceptual plots of mean execution time and PPW of an application running at various clock frequencies, with error bars depicting range of
variation, and probability of meeting a given deadline at each frequency.

the computing devices. With this paradigm change, the constraints in the optimization problem can no longer be simply
viewed as being satisﬁed or not being satisﬁed, but instead
must be expressed as a likelihood of being satisﬁed.
To illustrate the importance of considering the likelihood
of satisfying constraints, and the potential tradeoffs, consider video playback, which is a very common smartphone
application. Suppose that a processor allows multiple frequency settings, f1 - f6 . Figure 1(a) shows a hypothetical
plot of the average execution time (dotted curve) of the application (which involves loading, decoding, and displaying
a video frame) over multiple invocations, at different frequency settings. As stated earlier, the different invocations
at a ﬁxed frequency lead to different execution times due to
various factors, such as the frame complexity, and the nondeterministic nature of the system states. The vertical bars
at each frequency indicate the observed minimum and maximum execution times. Figure 1(b) shows the average energy
efﬁciency, expressed as performance per watt (PPW) of the
application at each frequency.
The QoS here is a composite measure of how stringent the
deadline is for processing a frame, as well as the likelihood
of meeting the deadline. Suppose that to satisfy the high
QoS requires the video to be played at 30 frames-per-second
(fps), which translates to a frame execution time 33.3 ms.
Let Δ30 denote this deadline (see Figure 1(a)). To meet this
deadline with a likelihood of 99.9%, will require running the
core at frequency f6 . However, at this frequency the average
energy efﬁciency or PPW will be very low. This may result,
for instance, in substantially reducing the residual battery
charge, raising the possibility of an inoperable phone until
the next charging opportunity. However if the user is willing
to sacriﬁce image quality for a longer lasting battery, then
the deadline can be increased to 50 ms, which corresponds
to 20 fps. Then the phone can be operated at f4 , at which the
energy efﬁciency is maximum, with a near certainty of meeting the larger deadline. Another viewpoint of the tradeoff is
that at both f4 and f6 , there is a very high likelihood that the
video will be completed at 20 fps, but at f4 , a substantially
larger number of frames will be dropped as compared to frequency f6 , resulting in a low image quality, while ensuring
a larger residual battery charge.
Overview: In this paper, we present a new approach to improve mobile user experience by balancing performance and
energy with probabilistic guarantee of QoS. The approach
is based on modeling the execution time of applications as

2

Problem Description

In this section, we introduce some basic terminologies and
make a precise statement of the optimization problem. Then,
we describe two different scenarios that ﬁt into the general
optimization framework, but with different models of execution time.
An application consists of a collection of basic UoCs, each
of which is mapped to a core. For instance, a web browser
receives an HTML document that consists of a number of
URIs (Uniform Resource Index). The URIs are queued, pro2

53

the large number of available frequency settings in modern
processors and the increasing number of cores in the application processor, a brute force evaluation for identifying
the optimal frequency combination for all cores is infeasible. For example, there are fourteen available frequency settings per core in the Qualcomm Snapdragon chipset (ranging from 300MHz to 2.15GHz). For an Octa-core application processor, it requires 148 experiments to determine the
frequency combination that maximizes system energy efﬁciency for any application. For this reason, the more sophisticated modeling mechanisms presented herein, are needed.
The proposed approach addresses two distinct scenarios, one
where all the cores are operating concurrently and independently, and the other where there are interactions among the
cores. These two scenarios require a different approach to
modeling the execution times.
While this work only examined core frequencies due to
the lack of control over other SoC components (GPU, DSP,
etc.), incorporating accelerators will require no changes to
the models presented. It is only necessary to monitor the
runtime of the code segment(s) ofﬂoaded and develop probability distributions as a function of their controls.

cessed and ultimately displayed on screen. Therefore, in this
instance, a UoC would simply be a URI or a collection of
URIs. In a video playback, a UoC refers to a single video
frame that undergoes several stages of processing before it
is displayed.
The execution time of a UoC on a single core is a random
variable X, with a distribution function FX (x | s) = Prob(X ≤
x | s), where s represents the core frequency. In general,
the distribution function can also include a set of uncontrollable parameters that affect the execution time, reﬂecting the
complexity of the data being processed. Examples of such
parameters would be number of URIs or the complexity of
URIs in a webpage as well as the operating frequencies of
each processor core. Note that in general the core voltage
setting is determined by the frequency, and hence we consider only the core frequency as the main control variable.
In this work we limit our study to the situation where only
one foreground application is being executed at a given time.
Currently, this seems to be the most common application use
case on smartphones; however, FX (x | s) can be extended
to handle multiple applications with enough training to expose the variability caused by the interfering applications.
For example in [48] (deterministic) the execution time is parameterized as a function of the memory interference due to
concurrently running applications. We could similarly deﬁne the execution time pdf parameters to be functions of the
memory interference.
A given dataset (e.g., a webpage or a video frame) is a
collection of UoCs, U1 ,U2 , · · · ,Un , whose corresponding individual execution times are independent random variables
X1 , X2 · · · Xn , with distribution function FX . In a CMP with n
cores, let Zn (ss) denote the total execution time of an application, where s denotes the vector of core frequencies. The
relation between Zn (ss), and the execution time of the UoCs
that make up the application depends on the application and
how they are mapped onto the cores. For instance, if all its
UoCs execute concurrently and independently, then Zn (s)
would be the maximum of the individual execution times,
whereas in the case of a single core Zn (ss) would be the sum
of the individual execution times.
The objective function to be maximized is energy efﬁciency, also referred to as performance-per-watt, denoted by
E (PPW (ss)), where E () denotes the expected value. PPW
is the ratio of the performance to power, and performance
is simply the reciprocal of the execution time. The measure
of performance depends on the speciﬁc application, but is
in general, a function of all the core frequencies s . In the
case of a video playback, PPW (ss) would denote the average
number of fps per Watt, and in the case of a web browser,
it would be the number of webpages displayed per second
per Watt. Hence, PPW denotes the number of items processed per joule of energy. Let Δ denote a deadline and
Q ∈ (0, 1) denote a lower bound on the likelihood of satisfying the given deadline. Then, with these notations, the
optimization problem can be stated as follows:
max E (PPW (s))
s

s.t.

FZn (s) (Δ|s) = Prob(Zn (ss) ≤ Δ) ≥ Q.

3 Execution Time Models
3.1 Computations on a Single Core
We start this section with some evidence that supports the
need to model execution times as random variables. As stated
earlier, during the course of execution, ﬂow of computation will be subjected to numerous interruptions, which contribute to its execution time. To better understand the sources
of interruptions, we carried out an experiment on the Intel
SNB processor1 in which the architectural sources of interrupts were monitored using the available performance counters on the processor [25] (the complete methodological detail is given in Section 5). It is important to note, that the severaity of each source of delay can vary greatly. In our work
we focus on the performance of web page rendering (web
pages are loaded ofﬂine) and thus network delay is not included. That said, network delay is not necessarily the dominating factor on mobile phones. A study from [29] showed
that under a 2Mbps network connection halving the CPU
frequency results in a 50%-100% increase in the page load
times, thus the bottleneck becomes the client CPU. To avoid
the correlation between cores, we disabled all but one application processor core. The Firefox web browser was executed, along with the eleven most visited web pages from the
BBench suite [19], including Amazon, BBC, CNN, Craiglist,
eBay, ESPN, Google, MSN, Slashdot, Twitter, and YouTube.
The various sources of delays were monitored during the
course of the eleven webpage loads. This was repeated 1,500
times, and the execution times and the associated delays incurred in the processor and the memory subsystem were
recorded. Table 1 shows the statistical information gathered from the experiment, and Figures 2(a-h) show the histograms of the durations of each of the monitored sources
of interruptions. The data clearly demonstrates a substantial

(1)

1 The Intel SNB processor was selected over other platforms due to
the availability of advanced performance counters to measure the
numerous sources of delays. While the mobile platforms may have
some performance counters, the available information is not sufﬁciently detailed as that given by the Intel SNB processor. Although
the parameters of the distributions may change from platform to
platform, the underlying distributions will not.

(2)

A naive approach to solving the above optimization problem would be to experimentally evaluate the application program at all combinations of per-core frequencies. Given
3

54

L2 Cache Stalls

Store Buffer Delay

0.3

0.3

0.15

0.15

0.2

0.2

0
0

0.2

(a) L1 Cache Delay

0.05
Seconds

0
0

0.1

Floating Point Unit Delay

Frequency

Frequency

0.1
0.05

0.05

0.01
0.02
Seconds

(e) Full IQ Delay

0.03

x 10

0.5
Seconds

0.25

0.2

0.2

0.15
0.1

0
2
Seconds

−7

x 10

(f) FPU Delay

0.15
0.1
0.05

0
−2

1

10
−3

x 10

Branch Delay

0.25

0.05

0
0

0
5
Seconds

(d) SB Delay

Reorder Buffer Delay

0.15

0.1

0
−5

1.5
−3

(c) LB Delay

0.2

0.15

0
0

0.5
1
Seconds

(b) L2 Cache Delay

Full Instruction Queue Delay
0.2

0.1
0.05

Frequency

0.1
Seconds

0.1
0.05

0.1

0
0

Frequency

0.2

Frequency

0.2

0.1

Frequency

Load Buffer Delay

0.4

Frequency

Frequency

L1 Cache Stalls
0.4

0
0

4
−3

x 10

(g) ROB Delay

0.05
Seconds

0.1

(h) Branch Delay

Figure 2: Distributions of the non-deterministic portions of sources of delay (Minimum delays subtracted).

Execution Time

Table 1: Summary of Sources of Delays
Std. Deviation
0.2117

Min
0.2862

Source of delay
L2 Cache Stalls (s)
L1 Cache Stalls (s)
LB Stalls (s)
SB Stalls (s)
Full IQ Stalls (s)
FPU Stalls (s)
ROB Stalls (s)
Branch Stalls (s)

Mean
0.0828
0.1651
0.0012
0.0063
0.0326
8.80E-8
0.0008
0.0858

Std. Deviation
0.0048
0.0095
0.0001
0.0005
0.0021
6.71E-9
0.0002
0.0049

Min
0.0276
0.0546
0.0003
0.0020
0.0116
3.51E-8
0.0007
0.0280

50
40
Frequency

Execution Time (s)

Mean
0.5532

30
20
10

variations in the durations of different types of interruptions.
In particular, the standard deviation of the total execution
time (0.2117) is about 38% of the mean.
The interruptions to the execution ﬂow can be viewed as
intervals of idle periods, whose endpoints are random points
in time, and whose lengths are random variables. Thus the
total execution time may be viewed as the sum of some minimum execution time (not random, but unknown) and the
total duration of all the interruptions. A model for sum of
random interval lengths often used in the literature is the
Gamma distribution [6]. Figure 3 shows the histogram of
the total execution of the web browser compared to a 3parameter Gamma distribution. It shows that the Gamma
distribution provides an adequate model to explain the variations in the total execution time for this application on a single core. Note that the dependence of the probability distribution function (pdf) on the core frequency will be modeled
by relating it to the parameters of the Gamma distribution.
The pdf of the Gamma is given by
fX(s) (x) = Gamma(x; K(s), θ (s), λ (s))


x − λ (s) K(s)−1
1
=
,
θ (s)Γ(K(s))
θ (s)

Experimental Data
Gamma Dist. Fit

0
0.5

0.55
0.6
Seconds
Figure 3: Histogram of execution times of an application running on a single core, compared to a pdf of a 3-parameter Gamma pdf.

the distribution are given by: μ(s) = K(s)θ (s) and σ 2 (s) =
K(s)θ 2 (s).
It is important to note that the parameters K(s), θ (s), and
λ (s) are modeled as functions of the core frequencies s. This
provides a simple and effective means to relate the distribution function of the execution times to the core frequencies.
In addition, it is also possible2 to make K and θ depend
on application speciﬁc parameters. For instance, for web
browsing we could include quantiﬁable characteristics of the
web page such as the number of URIs and the types of objects in the URIs that affect the execution time. In this way,
the execution time of a browser in processing a web page
can be made sensitive to the complexity of the webpage.

3.2

Parallel Computations, Independent Cores

We now describe the execution time model of an application
in which the UoCs are executed in parallel, and independently on multiple cores. Although the description is based
on a speciﬁc multi-threaded web browser, it is easily made
applicable to other browsers and data parallel applications.

(3)

s is the core frequency, K(s) is the shape parameter, θ (s) is
the scale parameter, and λ (s) is the left endpoint or minimum value of X(s). The mean μ(s) and variance σ 2 (s) of

2 Not

4

55

considered in this article

To load a web page, a browser must process a collection
of URI elements. The web browser analyzes the collection
of URI elements in a page and determines a subset of elements to be serviced by a secondary instance of the browser,
assuming it deems the subset of elements to be “sufﬁciently
complex”. The servicing of this subset of URIs mapped to
an application processor core denotes a unique UoC with
its own execution time distribution. This process can be repeated until all URI elements are assigned a UoC. In practice the web pages of most web sites are partitioned to 1 or
3 UoCs.
With this view, the total time to load a page will be the
latest completion time among all the UoCs. Thus if a page
results in N UoCs, each mapped to a core, and the execution
time of UoC n is a random variable Xn , whose distribution
function at a given core frequency sn is given by (3), the
N X (s ), where
total execution time will be ZN (ss) = maxn=1
n n
sn is the core frequency at which UoC n is processed. The
distribution function of ZN (ss) is given by

‫ ݐ ܫ‬ൌ൜

data

‫ݐ ܫ‬

ܵ ‫ݐ‬

ܱ ‫ݐ‬

ܺெ஺௑
ܱ ‫ ݐ‬ൌ ‫(ܵ ٔ ݐ ݔ‬t)
‫ݐ ݎ‬ǡ
‫ ݐ‬൑ ܶଶ
ൌ൜ ை
ܺெ஺௑ ǡ
‫ ݐ‬൐ ܶଶ

ܶଵ

ܶଶ

time

ܶଶ ൌ ܺெ஺௑ Ȁ‫ݎ‬ை

Figure 4: A classical network calculus example, the leaky bucket. The leaky
bucket drains water via a hole in the bottom at a constant rate, rO (i.e. the
service characteristics of the system). The system receives an input ﬂow of
rI t + b until time T at which time no more ﬂow is received. The output ﬂow
can therefore be calculated via min-plus convolution of the service curve
and the input curve.

as follows.
• I(t) – the total amount of data which has entered S
during the interval [0,t).
• O(t) – the total amount of data which has exited S
during the interval [0,t).
• S(t) – the service curve of system S . This curve represents a lower bound on the total amount of data which
could be serviced during the interval [0,t).
The goal of this section is to provide a method to compute
the pdf of O(t), the throughput of a system, denoted by
fO(t) (x). In the case of a pipelined computation ﬂow, this
would be the pdf of the output curve of the last stage. The
output ﬂow of a system with a given service curve and input
ﬂow is given by

(4)

n=1

3.3

‫ ݐ‬൑ ܶଵ
‫ ݐ‬൐ ܶଵ

ܵ ‫ ݐ‬ൌ ‫ݎ‬ை ‫ݐ‬

N

FZ(ss) (z) = ∏ FXn (sn ) (z).

‫ݎ‬ூ ‫ ݐ‬൅ ܾǡ
ܺெ஺௑ ǡ

Parallel Computations, Interacting Cores

The previous sections described how the execution time of
independent UoCs can be accurately modeled as simple functions of one or more independent random variables. While
this is perfectly suitable to model many applications, there
are many others in which UoCs possess a pipelined data ﬂow
structure such as image similarity search and video playback. In this section, we describe how to model the execution time of a collection of UoCs constituting an application
that are running in parallel on a network of interacting cores,
where the network is a cascade of stages.
The naive approach is to model the total execution time of
the application as the sum of execution times of the stages.
This leads to very pessimistic results (demonstrated in Section 5, see Figure 6) due to the fact that it doesn’t accurately
model the temporal dependencies among stages. A more appropriate model is the mathematical framework of Network
Calculus [15] that can be used to describe the computation as
ﬂows of data through a network. With nodes exhibiting statistical variations in their execution times, the ﬂows, which
are represented by functions of time, are now stochastic processes (as opposed to simple random variables). Here, we
describe how the combination of network calculus and probability calculus can be used to predict the execution time of
parallel interacting tasks.
In its original formulation, Network Calculus (NC) was
aimed at modeling deterministic queuing systems for computer networks and is analogous to system theory used in
circuit analysis and other domains. It captures the intricacies associated with buffering systems and pipelining in networked systems. The fundamental distinguishing feature of
NC is the use of min-plus algebra as the basis for its calculations. As such, data ﬂows are modeled as non-decreasing
functions of time.
NC works on the notion of an observable node or system, S , and derives properties (e.g., backlog, delay, etc.)
based on the ﬂow of data entering, exiting, and being serviced by the node. S in the present case is a cascade of
one or more nodes. The ﬂows are represented by the nondecreasing functions I(t), O(t), and S(t) which are deﬁned

O(t) = I(t) ⊗ S(t),

(5)

where ⊗ represents the min-plus convolution operation, deﬁned by
O(t) = inf (I(s) + S(t − τ)).
0≤τ≤t

(6)

Figure 4 illustrates a classical example of this operation.
NC provides an algebraic representation of concatenation
of simple systems to form a complex networked system. As
in system theory, this greatly simpliﬁes the calculation of
ﬂows between multiple connected systems. Consider the
situation in which two systems S1 and S2 possess service
curves S1 (t) and S2 (t) respectively. Additionally, S1 has input ﬂow I1 (t) and output ﬂow O1 (t) while S2 has input ﬂow
I2 (t) and output ﬂow O2 (t). The outﬂow of the cascade of
S1 and S2 is given by
O2 (t) = I1 (t) ⊗ S1 (t) ⊗ S2 (t).

3.3.1

(7)

Distribution of the Service Process S(t)

In deterministic systems, a service process is a monotonic
function which describes the potential cumulative output of
the system given that the system is never starved for input.
In other words, it is akin to the impulse response function
in system theory. When extending into systems with nondeterminism, a single service curve no longer represents the
system. More precisely, it is a stochastic process, which is
an inﬁnite, non-denumerable set of service curves, as illustrated in Figure 5. At each ﬁxed time t, the sampled space
of outcomes are the points on all the curves at time t, representing the random variable S(t). The objective now is to
compute the distribution of the output variable O(t) given by
Equation 6, in terms of the input random variables I(t) and
5

56


	
	

		

	








4






Figure 5: An illustration of Service curve S(t) being a stochastic process.
Each trial represents the algorithm being executed under the same input
and system controls; however due to random system variations, the service
times can vary, thus the observed output process can vary from trial to trial.
At each t, the set of points on all the possible (inﬁnite) curves represents the
sample space of the random variable S(t).

the service random variable S(t). We may view this as pdfs
being propagated through the network to ﬁnally compute the
pdf of the number of UoCs processed at the system output
for each t.
Let S(t | s) denote the service curve of some stage, which
represents the number of UoCs processed in the interval [0,t).
We assume that a stage is synonymous with a core, whose
frequency is s. Let Tn (s) denote the time to process n UoCs
by a stage. Tn (s) is equal to the sum of the times to process
each UoC. Then using the previous notation, let X(s) be the
random variable that represents the time to process one UoC
by the given stage. The pdf of X(s) is given by Equation (3).
The the pdf of Tn (s) is then given by
fTn (s) (t | n, s) = fX(s) ∗ fX(s) ∗ . . . ∗ fX(s) .




Power Model

A vital part of the optimization process is accurate power
models. In this work power is represented as the sum of the
dynamic power Pdyn , the leakage power Plkg and static system power Pstc . While Pstc represents a constant offset, the
dynamic power varies linearly with the clock frequency, as
a circuit is operated only when the clock is high, while dynamic power varies quadratically with the voltage, as power
of a transistor is the product of transistor current and voltage,
and current of a transistor is also a function of voltage. The
components of the dynamic power vector are expressed as

	
	

	


	

puting fON (t) (x | s ) using the approximation given in Equation 10, depending on whether it is a collection of independent UoCs running on cores or a pipeline computation.






max
Pdyn,c,b (t) = Pdyn,c,b
(t)sc (t)v2c (t), ∀c, b,t

max
where Pdyn,c,b
is the dynamic power dissipated by block b
of core c when the core is at the maximum speed and voltmax
is obtained by proﬁling the time-varying power
age. Pdyn,c,b
consumption of the task to be run on core c.
The leakage power is known to have exponential dependence on the die temperature and supply voltage. The exact
equation is hard to derive analytically. Hence it is usually
derived based on data ﬁtting of the simulated power values
for various components like adders, multipliers, memories,
etc. An example empirical equation for leakage power in 65
nm is given [37].

(8)

Plkg,c,b (t) =

n-fold convolution

3.3.2

Distribution of the Output Process O(t)

Consider the situation of a single UoC running on a core
at frequency s, with an input ﬂow, I(t), service curve, S(t |
s), and output ﬂow, O(t | s). The computation of O(t | s)
given by Equation 6 is approximated by partitioning the time
interval into a discrete set of time points (0,t1 ,t2 , . . .tr = t).
O(t | s) =

min

τ∈(0,t1 ,t2 ,...tr )

I(τ) + S(t − τ | s)

αc,b vc (t)+βc,b

1
2
kc,b
vc (t)Tc,b
(t)e Tc,b (t) +
2 (γc,b vc (t)+δc,b )
kc,b
e
, ∀c, b,t.

(12)

1 , k2 , α , β , γ , and δ
kc,b
c,b
c,b c,b
c,b are parameters that depend
c,b
on circuit topology, size, technology and design. It should
be noted that (12) can be simpliﬁed to a piece-wise linear
approximation as shown in [23]. Thus we use the following
model to estimate leakage power for each core/block on the
processor.

(9)

= min {I(0) + S(t), I(t1 ) + S(t − t1 | s),
· · · , I(ti−1 ) + S(t − ti−1 | s)}

v
vc (t), ∀c, b,t. (13)
Plkg,c,b (t) = Plkg0,c,b + GTc,b Tc,b (t) + kc,b
v represent the temperature and the voltage
where GTc,b and kc,b
coefﬁcients. Plkg0,c,b represents the leakage power for block
b in core c corresponding to Tc,b = 0 and vc = 0 (i.e. the
ambient temperature and minimum voltage).

The density of each sum within the min, i.e. I(tk ) + S(t − tk |
s), is computed by the convolution of the individual densities. The density of the minimum of some n random variables, is approximated by iteratively computing the density
of the minimum of pairs of random variables, which is given
by

5

Experimental Setup

In this section, we describe the setup of the experiments and
the validation results for the execution time and power models, evaluated on real platforms.

fmin(U,V ) (x) = fU (x) + fV (x)
− fU (x)FV (x) − FU (x) fV (x).

(11)

(10)

5.1

For an N stage pipeline, we are interested in the distribution of the output process of the Nth stage, namely ON (t | s ).
Equation 9 is expanded, with the output process of the stage
i serving as the input process for stage i + 1. Then the resulting expression will be the minimum among a potentially
large number of service curves, along with the distribution
of the initial I(t), which is assumed to be known. The distribution function of such a minimum has to be computed
numerically, using the approximation given in Equation 10.
In summary, estimation of execution times is done by numerically computing its distribution using Equation 3 or com-

Real-Device Experimental Platform

The experiments were conducted on two different platforms:
the Intel Quad-Core SNB processor and a DragonBoard development board based on the Qualcomm Snapdragon 8074
chipset. The Intel SNB processor includes four cores, each
of which has a private L1 instruction cache (32KB), a private L1 data cache (32KB), and a private uniﬁed L2 cache
(256KB). All four cores share the last-level cache of 6144KB.
The frequency settings available in the Intel SNB processor
range from 0.8GHz to 2.1GHz in steps of 100MHz. Since
this processor is not a mobile chipset, it is only used in
the event when detailed performance counters are needed
6

57

(see Section 3). On the other hand, the Qualcomm Snapdragon 8074 chipset, which is in the US versions of Samsung Galaxy S5 smartphones and many other modern smartphone devices, is used for all results presented in the evaluation section (Section 6). It has four 2.15GHz cores with
independent frequency control. Each core hosts a private L0cache (4KB) and L1-cache (16KB). All cores share a 2MB
L2 cache. Since the Dragonboard offers no onboard power
sensors, an NI DAQ unit was used with a 1MHz sampling
rate for current and voltage measurements. The readings
were taken after AC to DC conversion.

1600

Expected Execution Time

1400

1000

800

400
0.8

Cortex-A15
ARM v7, Krait
0.3-2.15GHz
4KB I & 4KB D
16KB I & 16KB D
2MB

1

1.2
1.4
1.6
Frequency (GHz)

1.8

2

Figure 6: Predicted versus measured execution time of the image similarity
search application on Intel SNB quad core processor.

control binding of speciﬁc threads to cores in order to reduce
variability in testing and to ensure that the proper cluster of
cores were being tested. The load/input, feature extraction,
and output stages were bound to the same core due to their
relatively light computation, whereas the segmentation, indexing, and ranking stages were each mapped to an individual core. The application was run with the sim-large input
set.
The third application, was a video playback, implemented
as a data parallel streaming application. We used the open
source VLC video player after making minor modiﬁcations
to the source code in order to observe the per-frame service
time and to construct the empirical distributions. The workload was selected from MobileBench [42].3 Hardware acceleration was disabled so that all video decoding was done
by the application processor. The choice was made for two
reasons: (1) reaching 30 fps with GPU assisted acceleration
would be trivial, and (2) the platforms provide a greater level
of control over the application processor, including per-core
DVFS in the case of the Snapdragon chipset.

The experimental platforms run a rooted Android 4.4 KitKat
OS. The applications of interest web browsing [2], image
similarity search [8], video playback [3], were written in
C and cross-compiled on the host machine with the ARMAndroid NDK toolchains [1]. The binary is pushed to the
device and is launched from the host machine via the adb
terminal.

5.2

1200

600

Table 2: Parameters for the DragonBoard experimental platform.
Architecture
Frequency
L0 Cache Size
L1 Cache Size
L2 Cache Size

Measured
Predicted
Sum of Delays

Benchmark Applications

Three applications common to the mobile domain were evaluated. The ﬁrst is web browsing. Following [53], a web
page is viewed as a collection of elements (or URIs) which
are then serviced by the browser’s threads. Each thread is
treated as an independent server and acts in parallel with
other threads. The speciﬁc web browser was Firefox. Minor modiﬁcations were made to the source code in order to
observe the service time of each web page and each URI, to
compute the empirical distributions. The BBench 2.0 benchmark [19] was used as it contains a collection of the eleven
most widely viewed webpages. It should be noted that this
benchmark is limited to ofﬂine browsing due to the limitations of implementating timing analysis code into the web
pages. Although not explored in this work, it is possible to
capture the effects of network delay by either (1) decreasing
the value of δ by the expected value of the network delay or
(2) model the network as UoC cascading into the remainder
of the stages. Effectively this creates an arrival process for
the web browser. The second approach is explored in more
detail in [34]. That said, network delay is not necessarily the
dominating factor on mobile phones.
The second application was image similarity search, which
is an image similarity ranking algorithm from the PARSEC
benchmark suite [8]. The algorithm is used for contentbased similarity search of feature-rich data, such as images
or videos, and is an important building block for many image
recognition and augmented reality Apps running on modern
smartphones or Google Glass-like devices. The algorithm
consists of six stages of processing. Given an image, image similarity search searches a database of images to ﬁnd
the closest match. After the image is loaded, it is ﬁrst fragmented into segments based on its contents. Then the feature extraction stage assigns each fragment a numerical feature vector. Finally this feature vector is compared against a
central database and the most similar results are produced as
the output. Minimal source code modiﬁcations were made to

5.3

Validation of Execution Time and Power
Models

We demonstrate here the appropriateness and value of NC
model for modeling the execution time of a pipeline that involves interacting cores. Figure 6 plots the expected execution time for image similarity search that was obtained using
the proposed statistical models in combination with NC, and
the actual measured values on the Intel SNB processor. It
also shows the results of the naive approach, which is simply the sum of the execution times of each stage. The experiment demonstrates the value and accuracy of the proposed
approach. The naive approach is a signiﬁcant overestimate
of the actual execution time, which would result in a suboptimal solution of core frequencies when attempting to maximize performance or energy efﬁciency. It is due to the fact
that a simple summation does not account for the overlapping of computations involving different data sets. On average, difference between the model predicted execution times
and the observed values was less than 0.66%. The average
error using the naive approach was 26%.
The parameters of the model of power described in Section 4, were estimated using data gathered from synthetic
3 Although all input ﬁles were evaluated, we reports results for
big_buck_bunny_720p_stereo.avi as these are the few formats that require signiﬁcant amount of rendering time (i.e. obtaining 60 FPS was not trivially done at the lowest speed).

7

58

For web browsing and image similarity search, the selected levels of QoS (high, low, and unconstrainted) resulted
in greater energy efﬁciency than the linux governors. The
frequency governor, interactive, tends to over-react to processor utilization and attempts to set the processor to the
highest available frequency (i.e. assumes it is most energy
efﬁcient to ﬁnish the job as quickly as possible and then
slow/disable the processor). As shown in Table 3, the proposed optimization method determines independent frequency
levels for each application processor core. This is due to imbalances in the workloads between the cores. For example in
the case of web browsing the ﬁrst core receives a signiﬁcant
portion of the total number of URIs which constitute that
core’s UoC. Therefore its frequency is set to be the highest of 1.72GHz. In contrast, the other cores receive very
few URIs which are typically not too computationally demanding. Thus the frequency of these latter cores are reduced. Because there was never a case when four UoCs were
simultaneously being executed, the last application processor core’s frequency was set to zero, i.e., automatically disabled, to minimize power consumption. This agrees with
the observation made in a recent mobile device utilization
study [17]—the processor utilization of web browsing has
peaks and valleys between one to three cores of a four-core
application processor with an average utilization to be below
two cores. Similarly, the image similarity search application
we see that the optimization determines frequencies based
on the imbalances in the application pipeline. Speciﬁcally
we can see that the most computationally demanding stage
is mapped to the fourth core and thus always requires a substantially larger frequency than the ﬁrst three cores.
An interesting situation occurred in the case of video playback. As seen in Figure 7 and Table 3, the lowest frequency
setting (i.e. Powersave Governor) results in the optimal PPW
point for the system. The unconstrained optimal point of operation occurs at the same frequency setting. For this reason
it is very tempting to choose a very slow speed to yield the
highest energy efﬁciency; however, the data shows that the
QoS would dramatically drop (given a Q value of 0.9). Thus
the video would be playing at 10 fps or lower with a likelihood of at least 90%. It is quite reasonable to assume this to
be intolerable for a large user base. This issue highlights the
importance of adding constraint (2) to the PPW optimization
problem.
There is a strong non-linear relation between PPW and
QoS (= Δ). This is more prominent in video playback. Table 3 shows that increasing the required fps from unconstrained to 20 fps only degrades PPW by 2.9%, whereas increasing it from 20 to 30 fps degrades PPW by 51.3%. This
must be considered when balancing user experience and energy savings.
We reiterate the key points found in our analysis:
• The unconstrained (no consideration to user satisfaction) optimization problem results in the highest possible energy efﬁciency.
• Including a QoS constraint reduces the possible optimal energy efﬁciency. Alternatively, reducing Δ (i.e.
requiring the application to ﬁnish earlier) or increasing
Q (i.e. requiring a larger spread of execution times to
achieve the chosen deadline) requires a greater amount
of power thus reducing energy efﬁciency.
Next, we examine the accuracy or tightness of the likelihood constraint given in Equation 2, by plotting the observed

benchmarks aimed to stress the various portions of the microarchitectural and memory components. 2000 samples of
power data were collected while running image similarity
search. The difference between the model predicted power
and measured values was 1.72%.

5.4

Optimal Selection of Core Frequencies

The key component of the model creation is that only the
independent tasks are characterized. This avoids a full experimental exploration of the state space such that each core
only needs to be evaluated at each frequency (rather than
evaluating the system at every possible frequency-core combination). This was used to construct the individual densities fX(s) (x; K(s), θ (s)) (Equation 3) for each of the service
threads.
With each fX(s) (x; K(s), θ (s)), the statistical model was
used to compute the joint distribution of multiple random
variables, where each corresponds to a core at an individual frequency. For the web browser and video playback, the
joint distribution was computed using Equation 4. For image
similarity search the joint distributions were computed using
the NC approach described in Sections 3.3.1 and 3.3.2.
To ﬁnd the appropriate optimal frequency combination to
solve Equations 1–2 an exhaustive enumeration of the operating frequencies was performed. The problem can then
be resolved for various levels of QoS and stored in a table.
This table is then used at run time to select the optimal frequency. While this a fairly heavy overhead (in our case 4
cores, 14 frequencies), it is a one-time static analysis which
can bring signiﬁcant improvements in energy efﬁciency (see
Section 6).
In terms of practicality, our current method is very similar to characterizing a standard cell for a VLSI library. For
minor application updates or platform differences, the old
models could be used-the exact loss of efﬁciency however
is difﬁcult to quantify. Additionally, much like a standard
cell needs to be re-characterized for different process technologies, our model needs to be constructed on a per-device
basis or given signiﬁcant application changes. While this is
a limitation we believe that the resulting increase in energy
efﬁciency makes the process worthwhile. That said, one of
our current efforts is to adapt the methodology to different
platforms.

6

Case Study of Balancing Performance and
Energy with Probabilistic Guarantee of QoS

In this section we compare the results of using the proposed
methodology to maximize the PPW with several standard
schedulers available on Android OS: powersaver, performance, and interactive [41]. As stated earlier, the QoS =
(Δ, Q), is a composite metric that includes both the deadline Δ and the likelihood Q (see Equation 2). For the following experiments we set Q = 0.9. That is, the selected
frequencies should be such that the probability of meeting
the deadline Δ is at least 0.9. For this set of experiments,
since Q is ﬁxed, QoS is effectively the same as the deadline
Δ. For each application two levels of QoS where selected
to represent a high and low state. Additionally, we provide
an unconstrained solution as well—Maximum Energy Efﬁciency (Max Energy Eff.). Together these three points can
represent different tradeoffs between energy efﬁciency and
user satisfaction, as shown in Figure 7 and Table 3.
8

59

Powersave

Performance

Interactive

High QoS

Low QoS

Max Energy Eff.

Normalized PPW

1
0.8
0.6
0.4
0.2
0
Web Browsing

Image Similarity Search

Video Playback

Figure 7: PPW scores of all frequency governors. Higher is better.
Table 3: Performance of several frequency governors on various mobile applications.
Governor
Powersave
Performance
Interactive
High QoS
Low QoS
Max Energy Eff.
Governor
Powersave
Performance
Interactive
High QoS
Low QoS
Max Energy Eff.
Governor

Q - Observed

Powersave
Performance
Interactive
High QoS
Low QoS
Max Energy Eff.

Req. QoS
Lower is better
—
—
—
2.0 Sec
4.0 Sec
—
Req. QoS
Higher is better
—
—
—
4 Images/Second
2 Images/Second
—
Req. QoS
Higher is better
—
—
—
30 FPS
20 FPS
—

Web Browsing
Freq. Vector
300MHz, 300MHz, 300MHz, 300MHz
2.15GHz, 2.15GHz, 2.15GHz, 2.15GHz
varies over time
1.72GHz, 1.57GHz, 1.57GHz, off
1.26GHz, 960MHz, 960MHz, off
960MHz, off, off, off
Image Similarity Search
Freq. Vector
300MHz, 300MHz, 300MHz, 300MHz
2.15GHz, 2.15GHz, 2.15GHz, 2.15GHz
varies over time
652MHz, 422MHz, 422MHz, 1.49GHz
422MHz, 422MHz, 300Mhz, 960MHz
422MHz, 422MHz, 300MHz, 960MHz
Video Playback
Freq. Vector
300MHz, 300MHz, 300MHz, 300MHz
2.15GHz, 2.15GHz, 2.15GHz, 2.15GHz
varies over time
1.72GHz, 1.72GHz, 1.72GHz, 1.72GHz
652MHz, 652MHz, 652MHz, 652MHz
300MHz, 300MHz, 300MHz, 300MHz

Obs. QoS
Lower is better

Power

PPW

7.55 Sec
0.97 Sec
1.52 Sec
1.36 Sec
2.12 Sec
4.24 Sec

0.90 W
4.99 W
3.56 W
2.76 W
1.61 W
0.75 W

0.15
0.21
0.20
0.27
0.29
0.32

Obs. QoS
Higher is better

Power

PPW

0.84 Images/Second
6.06 Images/Second
6.05 Images/Second
4.22 Images/Second
2.05 Images/Second
2.05 Images/Second

0.85 W
5.42 W
4.96 W
3.32 W
1.94 W
1.94 W

0.99
1.12
1.22
1.28
1.39
1.39

Obs. QoS
Higher is better

Power

PPW

22.4 FPS
39.2 FPS
39.1 FPS
31.7 FPS
23.3 FPS
22.4 FPS

0.95 W
5.15 W
5.03 W
2.81 W
1.00 W
0.95 W

23.9
7.61
7.77
11.3
23.2
23.9

We now demonstrate the potential tradeoff analysis (hypothetical plot shown in Figure 1) that is made possible with
the proposed approach. Speciﬁcally we explore web browsing in greater detail to better demonstrate the beneﬁts of
modeling execution time as a non-deterministic value. Figure 9(b) presents the energy efﬁciency of web browser at the
corresponding frequency settings. The green circles in Figure 9(a) represent the average execution time needed to load
the web page. The range for each point represents the statistical distribution of the execution time for web browsing in
the various frequency settings.

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

0.5
1
Q - Required
Figure 8: Observed Q versus speciﬁed Q (lower bound on likelihood),
showing tightness of constraint in Equation 2.

In addition to the expected load times of the web browser,
the proposed framework also provides a quantitative measure for the likelihood of performance guarantee. This is
shown in Figure 9(c) where a deadline on the web page load
times is set to 2 seconds. The likelihood of reaching this
deadline at a frequency setting of 1.728GHz is represented
by the grey shaded area under the distribution, and the likelihood of meeting that deadline is 95.48%. The data also
presents tradeoffs between likelihood and PPW. While a frequency of 1.728GHz provides a high likelihood of satisfying the deadline, it is suboptimal in terms of PPW (see ﬁgure 9(b)). If the a frequency of 960 MHz was selected, the
likelihood is much less (55%) but the PPW is close to the optimal (0.315). The reduced likelihood implies the possibility
of the image quality being degraded.

or actual value of Q versus the required or speciﬁed value of
Q. This is shown in Figure 8, using web browsing as an example. The Firefox web browser was repeatedly executed
100 times with Q set to 0.1, 0.5, and 0.9, and Δ = 2.0s. Then
the number of page loads that exceeded Δ (2 second load
time) was recorded. Since the speciﬁed value of Q is a lower
bound on the likelihood, and the observed Q is simply a sample estimate of the likelihood, all the points in the plot should
be at or above the y = x line. The observed results are very
consistent with the expected, and show that the constraint on
the likelihood does indeed restrict the set of optimal solutions.

Figure 10 shows this result. It’s clear that as the frequency
9

60

Av
Average
Web Page
Load Time (s)

(a)

15
10
5

deadlinee

0

Energy Efficiency
(PPW)

0.2

0.7

1.2
1.7
Frequency (GHz)

2.2
(b))

0.3
0.2
0.1
0.2

0.7

1.2
1.7
Frequency (GHz)
Probability

2.5

0.015
0.010
0.005
0.000

Average Web Page Load Time (s)

the mobile user experience by balancing performance and
energy with probabilistic guarantee on QoS.
We summarize some of the most closely related works to
this paper [7, 10, 12, 13, 16, 18, 20–24, 26, 27, 36, 38, 43, 44,
47, 52]. Isci et al [27] considered DVFS as a means to dynamically tune a processor’s power consumption based on an
application’s computation and memory characteristics. Others considered thermal constraints and leakage power when
optimizing energy efﬁciency [10,13,20,22–24,28,30,31,33,
35, 47]. The control-theoretic techniques, e.g. [7, 23, 26, 36],
for multicore processors usually involve a model-predictive
controller to determine optimal control for one or more control time steps in the future, by solving a constrained optimization problem. A signiﬁcant contribution to extending
the traditional DVFS-based multicore controller is adding
deadline constraints. For example, a few recent works [12,
21,22] proposed learning algorithms for the temperature and
power models. However, these works assume deterministic
runtimes for applications; therefore, deadline constraints are
only a deterministic function of processor core frequency.
While most of the prior works focus on optimizing system
energy efﬁciency for the application domain of CMPs, a few
more recent works started examining and designing energy
efﬁciency optimization algorithms for user-centric, interactive smartphone applications. Egilmez et al. [14] speciﬁcally
aimed at improving user satisfaction through DVFS-based
control knobs to maintain a low and comfortable device skin
temperature. Singla et al. [50] developed power and thermal
models for a modern mobile platform and proposed a closed
loop thermal control to adjust processor frequencies.
Zhu et al. [52] proposed an event-based scheduling approach to improve the energy efﬁciency of web browsers for
smartphones while meeting speciﬁed QoS constraints. Their
method displayed a 33% improvement in energy efﬁciency
over the linux Interactive governor; however, this work only
targeted web browser workloads. Additionally [39] found
that with detailed knowledge of the application domain an
average of 16% energy efﬁciency improvement can be achieved
over the existing linux governors.
Another very relevant work appears in [32]. The authors
characterized the tradeoffs between power management and
tail latency for server applications. From this analysis they
determined that the commonly used service techniques were
not sufﬁcient to reach the stringent server QoS requirements
and did not provide the most energy efﬁcient method of service either. Instead they suggest a careful, workload speciﬁc
frequency scaling policy. Our work takes advantage of this
insight by creating workload speciﬁc performance models
which help identify the optimal frequency setting.
The previous works have demonstrated that additional application knowledge can yield higher energy-efﬁciency results. As such, the probabilistic guarantee on performance
QoS proposed in this paper (Section 3) can be used to guide
existing dynamic energy and thermal management techniques
for applications with pre-speciﬁed deadlines. This paper has
demonstrated an average energy efﬁciency improvement of
34% over existing linux governors and displays this over
a larger variety of applications than previously mentioned
works.

2

2.2
(c)

deadline

1.5
1
0.5
95.48%

0
1.7

1.8

1.9
2
Frequency (GHz)

2.1

2.2

Web Page Load Time (sec)

Figure 9: (a) Execution time, (b) energy efﬁciency results at all available
frequencies for web browsing on the Dragonboard. (c) Example calculation
of the probability of meeting the speciﬁed deadline in 9(a).
300Mhz

422.4MHz

652.8MHz

729.6MHz
20

1036.8MHz

1267.2MHz

15
10
5

0
0

0.2
0.4
0.6
0.8
Liklihood of Meeting Load Time

1

Figure 10: Web page load time versus the likelihood of reaching those load
times for various frequency settings.

increases, the variance in web page load times decreases, and
the difference in mean execution time reduces. There are diminishing returns on average web page load time; however,
the likelihood of reaching any given deadline increases as
well. When coupled with the non-linear power model, the
data shows that the optimal point may not necessarily be one
which gives a necessary level of QoS.

7 Related Works
7.1 DVFS-based Energy Efﬁciency Control
Over the past decade, a large body of research has been published on optimizing energy efﬁciency; however, most if not
all of the works have assumed that the underlying execution
time quantities are deterministic. While there is an extensive
amount of prior work on using the available DVFS control
knobs to optimize for processor energy efﬁciency, this work
takes into account the probabilistic nature of execution times
to ensure performance QoS leading to improved user experiences, that is particularly important for smartphone applications. Furthermore, this work that demonstrates how realdevice per-core DVFS control feature can be used to improve

7.2

Performance Modeling and Analysis

It is common to represent tasks bound to processing elements as a collection of linear equations based on the ex10

61

wards ensuring user experience in non-deterministic workloads. However, we recognize that there is signiﬁcant room
for expanding the work. For example, the most desirable
case would be to dynamically tune the processor based on
the changing characteristics of the user, input, background
task noise, and other system state. Despite our static nature,
our proposed optimization method achieved a 29% power
savings over commonly used Linux governors while maintaining an average web page load time of 2 seconds with
90% likelihood—a quality that the Linux governors do not
consider for user-centric mobile applications.

pected throughput of each task [5, 40, 49]. These methods
are advantageous in their simplicity; however, they typically
overlook crucial characteristics of the system and application which can lead to inaccurate results. To overcome some
of the limitations caused by the use of linear equations, Bogdan et al. [9] proposed a performance prediction model which
leveraged fractional calculus. This extension proved to be
effective in deterministic routing systems. Another common methodology for performance analysis involves sampling. Similar to the problem setup of this paper, Wernsing
and Stitt [51] suggested that the execution time of a task is a
function of some work metric or characteristic. In this case
the authors limited this function to a deterministic, monotonically non-decreasing function which is determined via
regression analysis. From these models, the expected function execution time is directly predicted based on the work
metric and proved to be an effective means of performance
prediction in systems with low execution time variance.
To model pipelined parallel applications, scenario-aware
dataﬂow models offer an accurate albeit, complex solution
[11]. Real-time calculus and network calculus [15] offer an
alternative which provides some solutions to the complexities of scenario-aware dataﬂow modeling. For example, a
round robin scheduler which used real-time calculus for data
ﬂow prediction was developed for image processing on multiprocessor system-on-chip [46]. Qian et al. proposed a network calculus performance model for a general multi-router
system with contention [45]. However, these prior network
calculus-based works assumed that the system is a deterministic queuing system.
On the other hand, this work
• considers non-determinism in the architecture and application software,
• formally characterizes and expresses the execution time
as a statistical distribution and propose the resulting
QoS constrained energy efﬁcient optimization problem, and
• performs an in-depth energy-efﬁciency evaluation on
a real-system platform and demonstrate a use case for
quantitative measure of the probability of meeting an
execution time deadline.

8

Acknowledgment
The authors would like to thank the anonymous reviewers
for their insightful feedback and Qualcomm Research for
the generous equipment donation and tool support. Additionally, we would like to thank David Brooks and Margaret
Martonosi for their time reviewing this work. This work was
partially supported by the NSF I/UCRC Center for Embedded Systems (NSF grants 1361926, 1432348) and by Science Foundation Arizona under the Bisgrove Early Career
Scholarship. The opinions, ﬁndings and conclusions or recommendations expressed in this manuscript are those of the
authors and do not necessarily reﬂect the views of the Science Foundation Arizona.

References
[1] “Android NDK,”
https://developer.android.com/tools/sdk/ndk/index.html, accessed:
2014-8-11.
[2] “Mozilla ﬁrefox.” [Online]. Available: https://www.mozilla.org
[3] “Videolan organization.” [Online]. Available:
http://www.videolan.org
[4] “How loading time affects your bottom line,”
https://blog.kissmetrics.com/loading-time, 2011, Accessed:
2015-05-14.
[5] A. Alimonda, S. Carta, A. Acquaviva, A. Pisano, and L. Benini, “A
feedback-based approach to dvfs in data-ﬂow applications,” the IEEE
Transactions on Computer-Aided Design of Integrated Circuits and
Systems, vol. 28, no. 11, pp. 1691–1704, 2009.
[6] M.-S. Alouini, A. Abdi, and M. Kaveh, “Sum of gamma variates and
performance of wireless communication systems over
nakagami-fading channels,” Proceedings of the IEEE Transactions
on Vehicular Technology, vol. 50, no. 6, pp. 1471–1480, 2001.
[7] A. Bartolini, M. Cacciari, A. Tilli, and L. Benini, “A distributed and
self-calibrating model-predictive controller for energy and thermal
management of high-performance multicores,” in Proceedings of the
Design, Automation Test in Europe Conference Exhibition, march
2011, pp. 1–6.
[8] C. Bienia, S. Kumar, J. P. Singh, and K. Li, “The parsec benchmark
suite: Characterization and architectural implications,” in
Proceedings of the 17th international conference on Parallel
architectures and compilation techniques, 2008, pp. 72–81.
[9] P. Bogdan, R. Marculescu, S. Jain, and R. T. Gavila, “An optimal
control approach to power management for multi-voltage and
frequency islands multiprocessor platforms under highly variable
workloads,” in Proceedings of the Sixth IEEE/ACM International
Symposium on Networks on Chip, 2012, pp. 35–42.
[10] D. Brooks and M. Martonosi, “Dynamic thermal management for
high-performance microprocessors,” in Proceedings of the Seventh
International Symposium on High-Performance Computer
Architecture, 2001, pp. 171–182.
[11] M. Damavandpeyma, S. Stuijk, T. Basten, M. Geilen, and
H. Corporaal, “Throughput-constrained DVFS for scenario-aware
dataﬂow graphs,” in Proceedings of the IEEE 19th Real-Time and
Embedded Technology and Applications Symposium, 2013, pp.
175–184.
[12] G. Dhiman and T. S. Rosing, “Dynamic voltage frequency scaling for
multi-tasking systems using online learning,” in Proceedings of the
International Symposium on Low Power Electronics and Design,
2007, pp. 207–212.
[13] J. Donald and M. Martonosi, “Techniques for multicore thermal
management: Classiﬁcation and new exploration,” in ACM
SIGARCH Computer Architecture News, vol. 34, no. 2, 2006, pp.
78–88.
[14] B. Egilmez, G. Memik, S. Ogrenci-Memik, and O. Ergin,

Conclusion

This work provides a new and necessary step toward developing a methodology for optimizing energy efﬁciency subject to user satisfaction. We develop a statistical framework
for characterizing, proﬁling, and predicating the execution
time of parallel applications running on mobile platforms.
The proposed statistical framework provides additional and
valuable information that can be used to guide the control of
voltage/frequency settings available on modern processors.
This is particularly helpful for today’s energy-constrained
smartphones that execute real-time apps with execution time
constraints. With the detailed performance and energy efﬁciency characterization on web browsing, we show how the
proposed statistical framework is used to consider the energy efﬁciency for smartphones while accounting for a probabilistic guarantee on QoS.
Speciﬁcally, the optimization method creates a set of static
tables containing frequencies corresponding to optimal energy efﬁciency, subject to speciﬁed QoS levels. Once the
application is started, the processor state is altered based on
the desired QoS level. This is an important ﬁrst step to11

62

[15]
[16]
[17]

[18]

[19]

[20]

[21]

[22]
[23]
[24]

[25]
[26]

[27]

[28]
[29]
[30]

[31]

[32]

[33]

[34]
[35]
[36]
[37]

“User-speciﬁc skin temperature-aware dvfs for smartphones,” in
Proceedings of the 2015 Design, Automation & Test in Europe
Conference & Exhibition, 2015, pp. 1217–1220.
M. Fidler, “Survey of deterministic and stochastic service curve
models in the network calculus,” Communications Surveys &
Tutorials, IEEE, vol. 12, no. 1, pp. 59–86, 2010.
K. Flautner and T. Mudge, “Vertigo: Automatic performance-setting
for linux,” in Proceedings of the 5th Symposium on Operating
Systems Design and Implementation, 2002.
C. Gao, A. Gutierrez, M. Rajan, R. Dreslinski, T. Mudge, and C.-J.
Wu, “A study of mobile device utilization,” in Proceedings of the
International Symposium on Performance Analysis of Systems and
Software, 2015.
D. Grunwald, C. B. Morrey, III, P. Levis, M. Neufeld, and K. I.
Farkas, “Policies for dynamic clock scheduling,” in Proceedings of
the 4th Conference on Symposium on Operating System Design &
Implementation - Volume 4, 2000.
A. Gutierrez, R. G. Dreslinski, T. F. Wenisch, T. Mudge, A. Saidi,
C. Emmons, and N. Paver, “Full-system analysis and characterization
of interactive smartphone applications,” in Proceedings of the IEEE
International Symposium on Workload Characterization, 2011, pp.
81–90.
V. Hanumaiah, S. Vrudhula, and K. S. Chatha, “Performance optimal
online DVFS and task migration techniques for thermally constrained
multi-core processors,” IEEE Transactions on Computer-Aided
Design, vol. 30, no. 11, pp. 1677–1690, November 2011.
V. Hanumaiah, D. Desai, B. Gaudette, C.-J. Wu, and S. Vrudhula,
“STEAM: A Smart Temperature and Energy Aware Multicore
Controller,” ACM Transactions on Embedded Computing Systems,
vol. 13, no. 5s, Sept. 2014.
V. Hanumaiah and S. Vrudhula, “Temperature-aware dvfs for hard
real-time applications on multicore processors,” IEEE Transactions
on Computers, vol. 61, no. 10, pp. 1484–1494, 2012.
——, “Energy-efﬁcient operation of multicore processors by dvfs,
task migration, and active cooling,” IEEE Transactions on
Computers, vol. 63, no. 2, pp. 349–360, 2014.
M. Huang, J. Renau, S.-M. Yoo, and J. Torrellas, “A framework for
dynamic energy efﬁciency and temperature management,” in
Proceedings of the 33rd annual ACM/IEEE International Symposium
on Microarchitecture, 2000, pp. 202–213.
R
64 and IA-32 Architectures Software
Intel Corporation, Intel
Developer’s Manual, September 2014, no. 325462-052US.
C. Isci, A. Buyuktosunoglu, C.-Y. Cher, P. Bose, and M. Martonosi,
“An analysis of efﬁcient multi-core global power management
policies: Maximizing performance for a given power budget,” in
Proceedings of the International Symposium of Microarchitecture,
2006, pp. 347–358.
C. Isci, G. Contreras, and M. Martonosi, “Live, runtime phase
monitoring and prediction on real systems with application to
dynamic power management,” in Proceedings of the 39th Annual
IEEE/ACM International Symposium on Microarchitecture, 2006, pp.
359–370.
R. Jejurikar, C. Pereira, and R. Gupta, “Leakage aware dynamic
voltage scaling for real-time embedded systems,” in Proceedings of
the 41st Annual Design Automation Conference, 2004.
C. G. Jones, R. Liu, L. Meyerovich, K. Asanovic, and R. Bodik,
“Parallelizing the web browser,” in Proceedings of the First USENIX
Workshop on Hot Topics in Parallelism, 2009.
D. Kadjo, R. Ayoub, M. Kishinevsky, and P. V. Gratz, “A
control-theoretic approach for energy efﬁcient CPU-GPU subsystem
in mobile platforms,” in Proceedings of the 52nd Annual Design
Automation Conference, 2015, p. 62.
A. Kahng, S. Kang, R. Kumar, and J. Sartori, “Enhancing the
efﬁciency of energy-constrained DVFS designs,” IEEE Transactions
on Very Large Scale Integration (VLSI) Systems, vol. 21, no. 10, pp.
1769–1782, Oct 2013.
S. Kanev, K. Hazelwood, G.-Y. Wei, and D. Brooks, “Tradeoffs
between power management and tail latency in warehouse-scale
applications,” in IEEE International Symposium on Workload
Characterization, 2014, pp. 31–40.
K. Kang, J. Kim, S. Yoo, and C.-M. Kyung, “Temperature-aware
integrated DVFS and power gating for executing tasks with runtime
distribution,” IEEE Transactions on Computer-Aided Design of
Integrated Circuits and Systems, vol. 29, no. 9, pp. 1381–1394, Sept
2010.
J.-Y. Le Boudec and P. Thiran, Network calculus: a theory of
deterministic queuing systems for the internet. Springer Science &
Business Media, 2001, vol. 2050.
J. S. Lee, K. Skadron, and S. W. Chung, “Predictive
temperature-aware DVFS,” IEEE Transactions on Computers,
vol. 59, no. 1, pp. 127–133, Jan 2010.
W.-Y. Liang, P.-T. Lai, and C. W. Chiou, “An energy conservation
DVFS algorithm for the android operating system,” Journal of
Convergence, vol. 1, no. 1, pp. 93–100, December 2010.
W. Liao, L. He, and K. M. Lepak, “Temperature and supply voltage
aware performance and power modeling at microarchitecture level,”

[38]

[39]

[40]

[41]
[42]

[43]
[44]
[45]
[46]

[47]
[48]

[49]
[50]

[51]
[52]

[53]

12

63

Proceedings of the IEEE Transactions on Computer-Aided Design of
Integrated Circuits and Systems, vol. 24, no. 7, pp. 1042–1053, 2005.
J. R. Lorch and A. J. Smith, “Improving dynamic voltage scaling
algorithms with pace,” in Proceedings of the ACM SIGMETRICS
International Conference on Measurement and Modeling of
Computer Systems, 2001.
N. C. Nachiappan, P. Yedlapalli, N. Soundararajan,
A. Sivasubramaniam, M. T. Kandemir, R. Iyer, and C. R. Das,
“Domain knowledge based energy management in handhelds,” in
Proceedings of the IEEE 21st International Symposium on High
Performance Computer Architecture, 2015, pp. 150–160.
U. Y. Ogras, P. Bogdan, and R. Marculescu, “An analytical approach
for network-on-chip performance analysis,” IEEE Transactions on
Computer-Aided Design of Integrated Circuits and Systems, vol. 29,
no. 12, pp. 2001–2013, 2010.
A. S. V. Palladi and A. Starikovskiy, “The ondemand governor: past,
present and future,” in Proceedings of Linux Symposium, vol. 2, no.
223-238, 2001, pp. 3–3.
D. Pandiyan, S.-Y. Lee, and C.-J. Wu, “Performance, energy
characterizations and architectural implications of an emerging
mobile platform benchmark suite-mobilebench,” in Proceedings of
the IEEE International Symposium on Workload Characterization,
2013, pp. 133–142.
T. Pering, T. Burd, and R. Brodersen, “Voltage scheduling in the
lpARM microprocessor system,” in Proceedings of the International
Symposium on Low Power Electronics and Design, 2000.
P. Pillai and K. G. Shin, “Real-time dynamic voltage scaling for
low-power embedded operating systems,” in Proceedings of the
Eighteenth ACM Symposium on Operating Systems Principles, 2001.
Y. Qian, Z. Lu, and W. Dou, “Analysis of communication delay
bounds for network on chips,” in Proceedings of the 2009 Asia and
South Paciﬁc Design Automation Conference, 2009, pp. 7–12.
S. Schliecker, M. Negrean, G. Nicolescu, P. Paulin, and R. Ernst,
“Reliable performance analysis of a multicore multithreaded
system-on-chip,” in Proceedings of the 6th IEEE/ACM/IFIP
international conference on Hardware/Software codesign and system
synthesis, 2008, pp. 161–166.
J. S. Seng, D. M. Tullsen, and G. Z. Cai, “Power-sensitive
multithreaded architecture,” in Proceedings of the 2000 International
Conference on Computer Design, 2000, pp. 199–206.
D. Shingari, A. Arunkumar, and C.-J. Wu, “Characterization and
throttling-based mitigation of memory interference for heterogeneous
smartphones,” in Proceedings of the IEEE International Symposium
on Workload Characterization, 2015, pp. 22–33.
J. Sim, A. Dasgupta, H. Kim, and R. Vuduc, “A performance analysis
framework for identifying potential beneﬁts in GPGPU applications,”
in ACM SIGPLAN Notices, vol. 47, no. 8, 2012, pp. 11–22.
G. Singla, G. Kaur, A. K. Unver, and U. Y. Ogras, “Predictive
dynamic thermal and power management for heterogeneous mobile
platforms,” in Proceedings of the 2015 Design, Automation & Test in
Europe Conference & Exhibition, 2015, pp. 960–965.
J. R. Wernsing and G. Stitt, “Elastic computing: A portable
optimization framework for hybrid computers,” Parallel Computing,
vol. 38, no. 8, pp. 438–464, 2012.
Y. Zhu, M. Halpern, and V. J. Reddi, “Event-based scheduling for
energy-efﬁcient qos (eqos) in mobile web applications,” in
Proceedings of the IEEE 21st International Symposium on High
Performance Computer Architecture, 2015, pp. 137–149.
Y. Zhu and V. J. Reddi, “Webcore: architectural support for
mobileweb browsing,” in Proceeding of the 41st annual international
symposium on Computer architecuture, 2014, pp. 541–552.

Ctrl-C: Instruction-Aware Control Loop Based
Adaptive Cache Bypassing for GPUs
Shin-Ying Lee and Carole-Jean Wu
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University
{lee.shin-ying,carole-jean.wu}@asu.edu
Abstract—The performance of general-purpose graphics processing units (GPGPUs) is often limited by the efﬁciency of the
memory subsystems, particularly the L1 data caches. Because of
the massive multithreading computation paradigm, signiﬁcant
memory resource contention and cache thrashing are often
observed in GPGPU workloads. This leads to high cache miss
rates and substantial pipeline stall time. In order to improve
the efﬁciency of GPU caches, we propose an instruction-aware
control loop based adaptive cache bypassing design (Ctrl-C).
Ctrl-C applies an instruction-aware algorithm to dynamically
identify per-memory instruction cache reuse behavior. Ctrl-C
then adopts feedback control loops to bypass memory requests
probabilistically in order to protect cache lines with short reuse
distances from early eviction. GPGPU-sim simulation based
evaluation shows that Ctrl-C improves the performance of cache
sensitive GPGPU workloads by 41.5%, leading to higher cache
and interconnect bandwidth utilization with only an insigniﬁcant
3.5% area overhead.

I. I NTRODUCTION
Graphics processing units (GPUs) are widely used in
high performance computing systems to accelerate generalpurpose parallel computation. General-purpose GPUs (GPGPUs) achieve a signiﬁcant throughput by processing massive
number of concurrent threads in the single instruction multiple
thread (SIMT) manner. Similar to traditional chip multiprocessors (CMPs), modern GPUs are also equipped with
caches to reduce the average memory access latency and
the interconnect bandwidth requirement. However, because
caches are often shared across all concurrent threads, the perthread cache capacity is often too small for GPUs. Threads
contend for the cache storage with each other, resulting in
cache thrashing, i.e., cache lines are frequently swapped in/out
without receiving any reuse. Consequently, GPUs do not
utilize caches efﬁciently, L1 data caches, in particular.
The cache inefﬁciency in GPUs raises two critical problems
which often limit the performance of GPUs. First, due to
the cache thrashing problem, many GPGPU applications have
high data cache miss rates. GPU caches are not able to
effectively reduce the average memory access latency, leading
to extra pipeline stalls. Second, a large amount of adjacent
data elements brought into caches with the demanded data is
never referenced before being evicted. This injects additional
interconnect trafﬁc that can increase the queuing latencies in
the interconnect. Because of the unnecessary data trafﬁc, using
caches to preserve spatial localities may signiﬁcantly degrade
the performance in some GPGPU workloads [1].

c
978-1-5090-5142-7/16/$31.00 2016
IEEE

To alleviate the degree of cache thrashing in GPUs, many
prior works proposed to apply cache bypassing techniques. A
widely used approach is to employ compilers to perform offline analysis and identify data that are unlikely to receive any
reuse in the near future [1], [2], [3]. However, the compilerbased algorithms are not ﬂexible for input dependent applications. Aside from the compiler-based schemes, a number of
prior works proposed to use additional hardware components
to count and predict the reuse distances of cache lines at
runtime [4], [5]. The reuse distances of GPGPU cache lines
can be extremely long and exhibit a disperse distribution. It
is challenging to accurately predict reuse characteristics of
GPGPU cache lines with limited storage requirement. These
dynamic prediction algorithms, thus, require a large number
of counters and incur signiﬁcant implementation overhead.
To tackle the cache inefﬁciency problem in GPUs, we
propose a low implementation overhead design: InstructionAware Control Loop Based Adaptive Cache Bypassing (CtrlC) to accurately predict the cache reuse behavior in the
granularity of an instruction and statistically bypass memory
requests to prevent cache lines from early eviction. Ctrl-C ﬁrst
applies an instruction program counter (PC) based algorithm
to identify the cache reuse behavior. Ctrl-C then employs feedback control loops to learn the per-instruction reuse patterns.
By doing so, a part of an instruction’s active working set is
retained in the cache, receiving reuses. Furthermore, since data
inserted in the cache starts getting reuses, the reuse predictor
learns this updated re-reference information in the control loop
at runtime and bypasses requests accordingly. Eventually, it
reaches a stable state by settling at the optimal bypassing
aggressiveness. Our evaluation results show that Ctrl-C can
improve the performance of GPGPU workloads by an average
of 41.5% over the baseline conﬁguration while the stateof-the-art PC-based adaptive bypassing scheme improves the
performance by 22.6% [4], [6]. Overall, Ctrl-C’s performance
speedup is close to that of doubling the L1 data cache capacity
with only 3.5% hardware overhead.
II. BACKGROUND AND M OTIVATION
A. GPU Data Cache Inefﬁciency
GPUs execute programs in a massive multithreading manner. Thousands of threads run simultaneously and compete for
hardware resources, such as the L1 data caches, L2 caches,
and the interconnect bandwidth. This makes cache capacity
and interconnect bandwidth critical resources for GPUs.

133

64k L1D$

	
	
		
	
	

134































	




	

	












	



	

	






















	


















	

	

	



	

 

 

! 

 

	









"






		


























Fig. 2. An example of thrashing in GPU caches. The data structure in is
accessed sequentially by different threads and each access reads a word of
a cache line. A cache line will never receive a hit since the working set is
larger than the cache capacity. In this example, the references are mapped to
a particular cache set in a 2-way set-associative cache.



Figure 1 shows the performance sensitivity of GPGPU applications to the L1 data cache capacity. The x-axis represents
the wide range of GPGPU applications studied in this paper
(more methodology detail is given in Section IV-B) whereas
the y-axis represents the speedup normalized to the baseline
16kB L1 data cache conﬁguration. We observe that a large
number of GPGPU applications—the cache sensitive (CS)
workloads—gain a signiﬁcant speedup with the increase in
the L1 data cache capacity. When the L1 data cache size is
quadrupled from the baseline 16kB conﬁguration to 64kB,
an average of 2.29x performance speedup is gained for the
CS GPGPU applications. This indicates that a signiﬁcant
performance improvement can be gained if the L1 data cache
capacity is increased or is managed more efﬁciently, such that
the large working sets common in GPGPU workloads can be
accommodated more effectively.
Moreover, from Figure 1, we also notice that there are
a number of GPGPU applications beneﬁting from turning
off the L1 data caches completely, e.g., MM, PRK and KMN.
This is because in these GPGPU applications, a large amount
of data elements adjacent to the demanded data in a cache
line are brought into the cache but are not reused during
its cache lifetime before being evicted from the cache (early
eviction). Thus, spatial locality is not exploited efﬁciently,
resulting in poor cache line utilization that wastes interconnect
bandwidth and introduces additional queuing latency [1], [7].
Since simply increasing cache capacities and interconnect
bandwidth to speed up the execution of GPGPU applications
costs tremendous storage overhead, it is an impractical solution. A more sophisticated cache management approach is
needed to improve the resource utilization of the memory
subsystem in GPUs.





1: function KERNEL(out[], in[], int m)
2:
for i in [0 : m] do
3:
 tid = the unique thread id from 0 to (n - 1)
4:
v ← in[tid ∗ m + i]
5:
perf orming computations on v
6:
out[tid ∗ m + i] ← v
7:
end for
8: end function





Algorithm 1 An example of GPGPU kernel source code from
KMN.





Fig. 1. Speedup of different L1 data cache conﬁgurations over the baseline
16kB L1 data cache.



	









Avg (All)

Avg (NS)

KMN

Avg (CS)

PRK

MM

ELL

STR

SS

BFS

FLD

BC

CSR

PVR

PF








	


16kB L1D$ (Baseline)




CLR

Speedup over the Baseline

L1D$ Off
3.0
2.5
2.0
1.5
1.0
0.5
0.0

Fig. 3. The distribution of L1 data cache reuse distance. The stacked bars
show the distribution whereas the black curve indicates the median value.

B. GPU Data Cache Reuse Behavior
In order to understand the root-cause of inefﬁciency in GPU
caches, we delve deeper to investigate the cache access behavior of GPGPU applications. We use the cache access pattern
directly from a cache sensitive application from the Rodinia
benchmark suite [8]—k-means clustering (KMN)—to
illustrate the cache thrashing behavior generated as a result of
the SIMT execution paradigm.
Algorithm 1 shows the kernel source code from KMN. This
kernel iteratively reads data from the in array and performs
computations on array elements with n concurrent threads,
where each thread works on m array elements. At each
iteration, array elements (in[i], in[m + i], in[2 ∗ m + i], ...,
in[n ∗ m + i]) are accessed sequentially by different threads
and these array elements are mapped to multiple cache lines as
shown in Figure 2. Since n is usually very large for GPGPU
workloads, we would expect to see an access pattern like
(a0 , a1 , ..., ak )N in a single cache set, where ai represents
a unique access to the cache set and N represents the number
of repeats. When k (or equivalently the reuse distance) is
greater than the cache set associativity (S), cache thrashing
occurs. Cache lines are evicted before they are re-referenced
and all memory accesses result in cache misses. Because GPUs
process thousands of threads in parallel with large working
sets, a similar cache access pattern with a large k is observed
in GPGPU workloads. Cache thrashing is a primary bottleneck
limiting the performance of GPUs.

2016 IEEE 34th International Conference on Computer Design (ICCD)





























Fig. 4. The distribution of L1 data cache reuse distance per insertion PC of
BFS. The stacked bars show the distribution whereas the black curve indicates
the median value.

For such thrashing access behavior, it has been proved that
inserting exactly S cache lines to a particular cache set and
bypassing all other memory requests can achieve an optimal
cache hit rate [9], [10]. However, it is virtually impossible
to identify k in advance because of the diverse behavior of
GPGPU workloads. Figure 3 shows the distribution of L1 data
cache reuse distances (k) for GPGPU applications. The stacked
bars show the distribution of reuse distances whereas and the
black curve indicates the median reuse distance.
Figure 3 provides three important insights for cache access behavior of GPGPU workloads. First, since a common
conﬁguration for the GPU L1 data cache is 4 or 8-way
set associative, a huge portion of cache lines will never be
reused during its lifetime. Second, GPGPU applications have
a diverse reuse behavior. For example, PVR has a long median
reuse pattern whereas FLD has a short median reuse pattern.
Finally, the reuse distance can be extremely long and the
distribution is dispersed within an application. For instance,
in PVR, 29% of cache lines have reuse distances longer than
128 while 17% of cache lines have reuse distances less than 4
and are expected to receive cache hits. It is difﬁcult to design a
static cache management policy that can achieve the optimal
hit rate for all GPGPU applications.
Figure 4 shows an example of the distribution of L1
data cache reuse distances with different memory instructions
from BFS. Figure 4 reveals that cache lines inserted by the
same load instructions often have a similar reuse pattern. For
example, most cache lines inserted by PC_0, PC_1, PC_6,
and PC_8 in BFS have short reuse distances. On the other
hand, the cache lines from the other memory instructions have
long reuse distances that are likely to incur cache misses.
This indicates that the unique program counter (PC) values of
memory instructions can be a signature to predict and identify
the behavior of cache lines.
C. Cache Bypassing for GPUs
Cache bypassing is a common technique to mitigate cache
thrashing. Nevertheless, rather than bypassing all requests,
bypassing only a selective portion of memory requests can
achieve a better cache hit rate and execution time speedup [9],
[10]. While we have identiﬁed that different memory instructions have different cache reuse patterns, we ﬁnd that, instead

N=3

N=5

N=7

Bypassing Complete

BFS

PC_3





N=1

PC_2





1.5
1.4
1.3
1.2
1.1
1.0
0.9
0.8

PC_1

	


PC_6



PC_4


 

PC_3


 

Speedup over No Bypassing


 








 













2.6
2.4
2.2
2.0
1.8
1.6
1.4
1.2

PC_0
KMN

ELL

Fig. 5. Performance speedup results for varying an instruction’s insertion/bypassing ratio within three GPGPU applications, i.e., BFS, ELL, and
KMN.The bars represent different insertion/bypassing ratios. N indicates the
insertion probability of memory requests from an instruction—A memory
request has 21N probability to be inserted into the L1 data cache.

of bypassing all memory references from an instruction with
long reuse behavior, bypassing only a selective portion of the
memory references offers additional performance gain.
Figure 5 shows the application execution time speedup
with different per-instruction bypassing probabilities. The xaxis represents memory instructions from different GPGPU
applications and the y-axis represents the speedup normalized
to no cache bypassing. Each bar in Figure 5 represents
a bypassing probability—(1- 21N )—imposed on the important
memory instructions in three applications, BFS, ELL, and
KMN. With a greater N , a larger portion of memory requests
are bypassed stochastically whereas, with a smaller N , a larger
portion of memory requests are inserted into the cache.
Not all memory instructions beneﬁt from the same degree
of cache bypassing. The optimal bypassing probability varies
from instruction to instruction, and from application to application. The optimal N for ELL’s PC_3 is 3 and for BFS’s PC_4
is 7 (or bypassing all) whereas the optimal N for KMN’s PC_0
is 5. Moreover, not all instructions beneﬁt from bypassing, as
illustrated by BFS’s PC_6. To capture the signiﬁcant performance improvement potential with bypassing, a design must
be able to predict the reuse patterns dynamically and adopt a
variable bypassing probability on the basis of instructions.
III. C TRL -C: I NSTRUCTION -AWARE C ONTROL L OOP
BASED A DAPTIVE C ACHE B YPASSING
A. Overview of the Ctrl-C Design
In order to improve the efﬁciency of GPU caches, an
Instruction-Aware Control loop based adaptive Cache bypassing design, called Ctrl-C, is devised. By taking the insights
from Section II, Ctrl-C (1) dynamically learns and predicts
the reuse patterns of instructions in GPGPU programs, and (2)
statistically bypasses memory requests from the L1 data caches
for instructions that generate requests with a low likelihood of
reuse. Ctrl-C applies feedback control loops to train the entries
of an instruction reuse prediction table (Section III-B) with the
reuse history of evicted cache lines. When the fraction of zeroreuse cache lines inserted by a particular instruction is high,
Ctrl-C starts bypassing memory requests from this instruction
and increases its bypassing aggressiveness until a stable state

2016 IEEE 34th International Conference on Computer Design (ICCD)

135

Feedback
Control Loop

Cache Memory
Reuse

Insertion
Instruction

Valid

iReuse

Data

bypass?

bypass?

……

……

(at insertion)

if (BYP == 2AGG) then
insert
reset BYP = 0
else
bypass
endif

௓ாோை

AGG

if (ூேௌாோ் ൐ ܶ‫ܦܮܱܪܵܧܴܪ‬ு ) then
AGG++
௓ாோை
elif (ூேௌாோ் ൏ ܶ‫ܦܮܱܪܵܧܴܪ‬௅ ) then
AGG-endif

BYP
BYP++
INSERT++

Memory
Request

insertion instruction, reuse
(at eviction)

if (reuse == FALSE) then
ZERO++
endif

ܼ‫ܱܴܧ‬
‫ܴܶܧܵܰܫ‬

Fig. 6. The system diagram of Ctrl-C design.

is reached—cache lines inserted with the instruction start
receiving hits (Section III-C). By doing so, a portion of the
instruction’s working set is retained in the cache, leading to
increased cache utilization.
B. The Instruction Reuse Prediction Table (iReuse Table)
The goal of the instruction reuse prediction table, or iReuse
Table, is to predict whether a cache line inserted by an instruction will receive future reuses and to determine if a memory
request should be bypassed from the cache. The intuition
is that if a cache line inserted by a particular instruction
receives hits, future cache insertions by the same instruction
will similarly receive hits. To explicitly correlate the reuse
patterns to the insertion instructions, we implement iReuse
Table as a hash table indexed by the lower bits of an instruction
PC 1 . iReuse Table is a simple hash table, where each table
entry is connected to a feedback control loop III-C.
iReuse Table is trained with the reuse histories of cache
lines in the L1 data caches. To track the reuse histories,
Ctrl-C augments the meta data ﬁeld of a cache line by two
additional parameters—a 1-bit reuse parameter and a 7-bit
insertion instruction parameter. The reuse bit is set to 0 when
a cache line is ﬁrst inserted into the cache and is set to 1
when it receives a cache hit. The insertion instruction is set
to the lower 7-bit of the instruction PC that causes the cache
insertion. When a cache miss occurs and a new cache line is
inserted into the cache, an eviction candidate is selected based
on the underlying cache replacement policy. The reuse history
of the evicted cache line—(insertion instruction, reuse)—is
used to train the corresponding iReuse Table entry.
C. Feedback Control Loop for Modulating Per-Instruction
Bypass Aggressiveness
The goal of the per-instruction feedback control loops in
iReuse Table is to learn the reuse distances and to modulate
the aggressiveness of per-instruction bypassing until a portion
of the instruction’s inserted data is retained in the cache. While
1 iReuse Table is implemented as a 128-entry hash table indexed by the
lower 7-bit of instruction PCs. A 128-entry table is sufﬁcient to cover distinct
instructions in GPGPU workloads that typically contain only tens of memory
load/store instructions.

136

iReuse Table separates instructions that generate memory
requests with high and low likelihood of reuses, for this
instruction-based reuse learning and prediction mechanism to
perform well, a design must handle bypassing appropriately.
First, memory requests from an instruction, if bypassed, must
have a chance to be inserted into the cache such that its
reuse pattern can be captured by iReuse Table. Second, for
instructions with long reuse distances, if the corresponding
reuse distance can be predicted well, a portion of the instruction’s active working set can be retained in the cache to
start receiving cache reuses. Ctrl-C achieves this by inserting
cache lines statistically into the cache and by modulating the
aggressiveness of bypassing for each individual instruction
with a feedback control loop.
The feedback loop controller applies four counters to keep
track of the cache utilization behavior: a 3-bit AGG saturating
counter, a 7-bit BYP counter, a 10-bit ZERO counter, and a
10-bit INSERT counter. AGG represents the aggressiveness of
bypassing and controls the probability of cache line insertion
and BYP counts the total bypassing requests. AGG and BYP
are used to regulate the aggressiveness of bypassing whereas
ZERO and INSERT are used to keep track of the cache
utilization. Speciﬁcally, cache lines of an instruction have a
1
to be inserted into the cache.
probability of 2AGG
D. Ctrl-C Algorithm
Figure 6 illustrates the design structure of Ctrl-C. Upon a
cache miss, the corresponding iReuse Table entry is accessed
for a decision on either inserting or bypassing the cache line. If
a cache line is to be bypassed, the instruction’s corresponding
controller increments BYP and bypasses the cache line. If a
cache line is to be inserted into the cache, an eviction candidate
is selected and its reuse history (insertion instruction, reuse)
is used to train the iReuse Table. The controller increments
INSERT to keep track of the number of insertions that have
occurred thus far, and if the evicted cache line’s reuse bit
is 0, indicating that it has not received a reuse during its
cache lifetime, the controller also increments ZERO to keep
track of the number of zero-reuses. Then, the per-instruction
controller for the corresponding instruction increments BYP

2016 IEEE 34th International Conference on Computer Design (ICCD)

Algorithm 2 The operations of Ctrl-C at a cache miss –
determining bypassing or insertion.
1: function AT M ISS(memRequest)
2:
 determine whether bypassing or not
3:
ctrl ← iReuse[request.P C].ctrl
4:
if ctrl.BY P < ((1  ctrl.AGG) − 1) then
5:
 bypassing the request
6:
bypass(memRequest)
7:
ctrl.BY P ← ctrl.BY P + 1
8:
else
9:
 inserting a new cache line
10:
evictedLine ← insert(memRequest)
11:
atEviction(evictedLine)
12:
ctrl.BY P ← 0
13:
end if
14: end function

Algorithm 3 The operations of Ctrl-C at a cache line eviction
– updating the bypassing aggressiveness.
1: function AT E VICTION(evictedLline)
2:
 update iReuse table
3:
ctrl ← iReuse[evictedLine.insertP C].ctrl
4:
ctrl.IN SERT ← ctrl.IN SERT + 1
5:
if evictedLine.reuse == F ALSE then
6:
ctrl.ZERO ← ctrl.ZERO + 1
7:
end if
8:
if ctrl.IN SERT ≥ SAM P LE_P ERIOD then
9:
f ract ← ctrl.ZERO/ctrl.IN SERT
10:
 NOTE: ctrl.AGG is a saturating counter
11:
if f ract > T HRESHOLD_H then
12:
 predict long reuse distance
13:
ctrl.AGG ← ctrl.AGG + 1
14:
else if f ract < T HRESHOLD_L then
15:
 predict short reuse distance
16:
ctrl.AGG ← ctrl.AGG − 1
17:
end if
18:
ctrl.IN SERT ← 0
19:
ctrl.ZERO ← 0
20:
end if
21: end function

with a bypassing decision. If a cache line gets evicted, iReuse
also increments INSERT and updates ZERO based on whether
the evicted cache line has received a reuse or not during the
line’s lifetime in the cache.
The feedback controller learns the optimal bypassing aggressiveness by periodically examining the current cache
utilization, namely, the number of zero-reuse cache lines
ZERO
out of the total inserted cache lines ( IN
SERT ). Speciﬁcally,
ZERO
the controller aims to keep IN SERT within a target range
(THRESHOLD_H and THRESHOLD_L) for cache perforZERO
mance improvement. If IN
SERT shifts from the target range,
the feedback controller tunes the bypassing aggressiveness
by modulating AGG. Speciﬁcally, during a time period, if
ZERO
IN SERT is greater than a certain target threshold (THRESHOLD_H), the number of zero-reuse cache lines is too high.
This implies that the reuse distance of this instruction’s cache
lines is too long—more memory requests shall be bypassed
from the cache to prevent data from early eviction. Thus, the
controller increments AGG by 1 to increase the probability of
ZERO
bypassing for this instruction. On the other hand, if IN
SERT
is lower than the low threshold (THRESHOLD_L), the controller decrements AGG by 1 to bypass memory requests less
ZERO
aggressively. When IN
SERT settles between THRESHOLD_H

and THRESHOLD_L, the controller is in a stable state with the
optimal degree of bypassing. Consequently, the cache retains a
portion of the instruction’s inserting data probabilistically and
its utilization is more efﬁcient. Algorithm 2 and 3 describe the
pseudo-code of Ctrl-C.
E. Ctrl-C Design Parameters
The target thresholds should provide the following features.
ZERO
First, the number of zero-reuse cache lines or IN
SERT should
be as low as possible in order to eliminate the cache thrashing
problem and guarantee no early eviction. Second, the controller should be able to detect and recover from an incorrect
ZERO
prediction. When IN
SERT is low, it is likely that the controller
bypasses memory requests too aggressive and loses the opportunity to recover cache lines that are mistakenly bypassed.
Therefore, THRESHOLD_L should be slightly higher than zero
to leave room for incorrect prediction detection and recovery.
Additionally, THRESHOLD_H represents the number of zeroZERO
reuse lines allowed, it should be small to keep IN
SERT low.
Finally, the bypassing aggressiveness should reach a stable
state quickly to attain as much performance gain as possible.
For our study, the target threshold range, [THRESHOLD_L,
THRESHOLD_H], is set as [0.1, 0.4] 2 .
Another critical parameter in the feedback controller design
is the sampling period. The feedback controller uses the
number of evictions as the unit of the sampling period to
evaluate the status of cache utilization and to update bypassing aggressiveness accordingly. However, when bypassing
is frequent with a high AGG value, the frequency of cache
evictions decreases and causes the controller a longer learning
time. Thus, the bypassing aggressiveness may not get updated
quickly enough. In order to adjust the learning rate according
to the bypassing/insertion rate, we adjust the sampling period
based on the bypassing aggressiveness as follows (Equation 1).
SAM P LE _P ERIOD = BASE _P ERIOD >> AGG
(1)
F. Hardware Implementation Overhead
Ctrl-C has a low implementation overhead. Its implementation overhead includes the feedback controllers, and the two
additional metadata ﬁelds per cache line. Overall, with the
baseline 16kB data cache (32-set, 4-way set associative), CtrlC needs only 608 bytes of additional storage. Compared to
the baseline 16kB cache, this corresponds to 3.5% storage
overhead. As this paper later shows, this extra storage brings
a signiﬁcant 41.5% performance improvement for cache sensitive GPGPU workloads (Section V-A).
IV. M ETHODOLOGY
A. Simulation Infrastructure
To evaluate the performance improvement of the Ctrl-C,
we use GPGPU-sim (version 3.2.2) [15]. GPGPU-sim is a
cycle-level simulator that simulates a GPGPU and supports
NVIDIA CUDA [16] as well as the PTX ISA [17]. We
implement the proposed Ctrl-C design on top of GPGPU-sim
and run with the default conﬁguration to simulate NVIDIA
2 The

parameters are chosen empirically through sensitivity tests.

2016 IEEE 34th International Conference on Computer Design (ICCD)

137

TABLE III
B ENCHMARKS FOR C TRL -C PERFORMANCE EVALUATION .

TABLE I
GPGPU- SIM SIMULATION CONFIGURATIONS .

3 We simulate all benchmarks to completion, except CSR, ELL, and KMN.
We simulate the three applications with one billion instructions to keep the
simulation times manageable.

138

16kB L1D$ (Baseline)

Dataset
512 Options
100k nodes
512x512 nodes
65536 nodes
32k samples
656x744 AVI
502x458 nodes
1024x1024 nodes
2048x2048 nodes
32x4096 nodes
1M nodes
10 blocks
86kB text ﬁle
ecology
ecology
28x128x10 nodes
1M data entries
1K (V), 128K (E)
USA road NY
256(V), 16K (E)
1024x256 points
65536 nodes
165k words
USA road NY
Co-Author DBLP
1024x1024
494020 objects

Adaptive Bypass [4]

2.0

Ctrl-C

Category

NS

CS

32kB L1D$

2.39x 2.38x

1.5

1.0
0.5
Avg (All)

Avg (NS)

KMN

Avg (CS)

PRK

MM

ELL

STR

SS

BFS

FLD

0.0
BC

Fermi GTX480 [18]. As a comparison, we also implement
the PC-based Adaptive Bypassing, which uses per-instruction
conﬁdence counters to predict the reuse patterns [6], [4]. When
detecting cache lines inserted by an instruction do not have
any reuse, it bypasses all memory requests generated by this
particular instruction. Table I and II show the conﬁgurations
of the simulation setup and the control loop parameters for
Ctrl-C in detail.
B. GPGPU Benchmarks
We select a wide range of GPGPU applications from the
Mars [13], NVIDIA SDK [11], Pannotia [14], and Rodinia [8],
[12] benchmark suites to represent the diverse behavior of
GPGPU workloads. Based on the performance sensitivity to
the cache size increase from the baseline 16kB to 64kB, we
classify these applications into two categories: (1) CacheSensitive (CS) applications which achieves a speedup greater
than 1.2x with the 64kB L1 data caches and (2) Non-CacheSensitive (NS) applications which has less than 1.2x speedup
with the 64kB L1 data caches. Table III lists the details of the
benchmarks and their input datasets 3 .
V. E XPERIMENTAL R ESULT AND A NALYSIS
A. Overall Performance Improvement
Our simulation results show that compared to the baseline 16kB L1 data cache conﬁguration, the proposed Ctrl-C
improves GPGPU performance by an average of 41.5% for

CSR

Conﬁguration
128 entries
3-bit counter per iReuse entry
7-bit counter per iReuse entry
10-bit counter per iReuse entry
10-bit counter per iReuse entry
(1024 » AGG) cache evictions
[0.1, 0.4]

PF

Design
Size of iReuse Table
AGG
BYP
REF
ZERO
SAMPLE_PERIOD
Target Threshold

Application
Binomial Options [11]
Pathﬁnder [8]
Hotspot [8]
Back Propagation [8]
Fast Walsh Trans. [11]
Heartwall [12]
SRAD1 [8]
Needleman-Wunsh [8]
SRAD2 [8]
Streamcluster [8]
B+Tree [8]
Discreet Cos Trans. [11]
Word Count [13]
Maximal Ind. Set [14]
Graph Coloring [14]
Particle Filter [8]
Page View Rank [13]
Betweenness Central [14]
Dijkstra-CSR [14]
Floyd Warshall [14]
Similarity Score [13]
Breadth First Search [8]
String Match [13]
Dijkstra-ELL [14]
Pagerank-SPMV [14]
Matrix Mul [13]
K-Means [8]

PVR

TABLE II
PARAMETERS FOR THE C TRL -C CONTROL LOOP DESIGN .

Abbr
BO
PTH
HOT
BP
FWT
HTW
SR1
NW
SR2
SC
BT
DCT
WC
MIS
CLR
PF
PVR
BC
CSR
FLD
SS
BFS
STR
ELL
PRK
MM
KMN

CLR

Min. L2 Access Latency
Min. DRAM Access Latency
Warp Size (SIMD Width)

NVIDIA Fermi GTX480
15
48
8
2
32768
48kB
16kB per SM (32-sets/4-ways)
2kB per SM (4-sets/4-ways)
768kB uniﬁed cache (64-sets/8ways/12-banks)
120 cycles
220 cycles
32 threads

Speedup over Baseline

Architecture
# of SMs
Max. # of Warps per SM
Max. # of Blocks per SM
# of Schedulers per SM
# of Registers per SM
Shared Memory
L1 Data Cache
L1 Inst Cache
L2 Cache

Fig. 7. The performance improvement comparison.

CS applications and 20.7% across all applications as shown
in Figure 7. With Ctrl-C, all CS applications, except FLD,
can obtain more than 1.1x speedup and the performance gain
can be as high as 2.39x (KMN). We notice that FLD does not
achieve a good speedup. This is because FLD does not have
a high fraction of zero-reuse lines (Section V-C). Ctrl-C does
not bypass any memory request and hence the performance is
the same as the baseline.
The PC-based Adaptive Bypassing scheme improves the
performance of the cache sensitive workloads by an average of
22.6%, that is 19% lower than that of Ctrl-C. This is because,
instead of bypassing cache lines of an instruction probabilistically, when the PC-based Adaptive Bypassing scheme detects
a portion of cache lines of an instruction do not receive
reuse hits, it starts bypassing all memory references from this
instruction. As a result, this design loses the opportunity to

2016 IEEE 34th International Conference on Computer Design (ICCD)

Fig. 8. The L1 data cache MPKI reduction with Ctrl-C.

Adaptive Bypass [4]

Ctrl-C

0.8
0.6
0.4

0.2
Avg (All)

Avg (NS)

KMN

Avg (CS)

MM

PRK

ELL

STR

BFS

SS

FLD

BC

CSR

PVR

PF

0.0
CLR

20%
Avg (All)

Avg (CS)

Avg (NS)

MM

KMN

ELL

PRK

STR

SS

BFS

FLD

0%

Fig. 10. The fraction of zero-reuse cache lines with Ctrl-C.

1.0
Interconnect Traffic
Normalized to Baseline

40%

BC

Avg (All)

Avg (NS)

KMN

Avg (CS)

MM

ELL

PRK

STR

SS

BFS

FLD

BC

CSR

PF

PVR

0.2

60%

CSR

0.4

80%

PVR

0.6

Ctrl-C

100%

PF

0.8

16kB L1D$ (Baseline)

16kB L1D$ (Baseline)

32kB L1D$

CLR

Ctrl-C

2.83x

Fraction of Zero-Reuse Lines

Adaptive Bypass [4]

1.0

CLR

MPKI Normalized to
Baseline

16kB L1D$ (Baseline)
1.2

Fig. 9. The L1 to L2 caches interconnect trafﬁc reduction with Ctrl-C.

learn and capture the cache lines that can receive potential
reuses once it has learned the per-instruction reuse history.
Moreover, it is difﬁcult to detect an incorrect prediction in
such a design if it bypasses all requests and therefore, with
the PC-based Adaptive Bypassing, an application may instead
experience performance degradation, e.g., FLD. Overall, CtrlC achieves a signiﬁcant speedup that is close to using a double
sized (32kB) L1 data cache, and outperforms the PC-based
Adaptive Bypassing.
B. MPKI and Interconnect Trafﬁc Reduction
To investigate where the signiﬁcant performance improvement comes from, we next investigate the ability of Ctrl-C to
reduce the number of misses per kilo instructions (MPKI) and
interconnect trafﬁc.
Figure 8 shows the normalized MPKI with Ctrl-C for all CS
applications. For CS applications, the working set is typically
much larger than the data cache capacity. A large amount
of data is evicted from the data caches before receiving
any reuse. Therefore, the performance of CS applications is
mainly restricted by the cache thrashing problem. However,
Ctrl-C adaptively protects cache lines from early eviction by
bypassing a part of the memory requests and thereby some
cache misses turn into hits. Overall, with Ctrl-C, the MPKI
of CS applications is reduced 9.9% that effectively translates
into 41.5% performance speedup.
The interconnect bandwidth is another critical resource
limiting GPU performance. Figure 9 shows the interconnect
trafﬁc reduction with Ctrl-C. Ctrl-C is able to achieve 43.7%
trafﬁc reduction for CS applications compared to the baseline.
It ﬁlters out data trafﬁc by reducing the MPKI and the zeroreuse data elements.

C. Fraction of Zero-reuse Lines
While CS applications have a signiﬁcant portion of zeroreuse lines due to the severe cache thrashing problem in the
baseline 16kB L1 data caches, Ctrl-C can effectively reduce
the number of zero-reuse lines. Figure 10 shows the percentage
of zero-reuse lines with Ctrl-C. For CS applications, the percentage of zero-reuse lines reduces signiﬁcantly by an average
of 48.7%. We notice that PF still has a high fraction of zeroreuse lines with Ctrl-C. This is because the kernel execution
time of PF is too short. Ctrl-C does not have sufﬁcient time to
learn the optimal bypassing aggressiveness for this particular
application. Overall, with Ctrl-C, the average fraction of zeroreuse lines for all applications reduces to 44.1%.
VI. R ELATED W ORK
Many cache management policies have been proposed to
mitigate cache thrashing in CPUs, such as [19], [20], [21].
However, due to the small per-thread cache capacity, employing these algorithms is not able to accurately predict the data
reuse patterns in GPUs.
To improve the cache hit rate with a thrashing access
pattern, Qureshi et al. [10] proposed the BIP policy to bypass
memory requests probabilistically. However, the bypassing
probability of BIP is ﬁxed across all memory instructions and
is trained off-line. BIP is not ﬂexible for different applications.
Wu et al. [6] proposed a PC-based reuse prediction algorithm
to predict the cache reuse behavior and use the prediction to
guide cache insertion positions. While SHiP can be extended
for cache bypassing, it does not use different aggressive levels
for bypassing cache lines. In contrast, the proposed CtrlC scheme trains the per-instruction bypassing aggressiveness
adaptively and achieve a better performance improvement.
In order to alleviate the resource contention in GPUs, many
cache bypassing algorithms have been proposed. Jia et al. [22]
designed a FIFO queue (MRPB) to reorder requests targeted
at reducing inter-warp contention. Additionally, MRPB bypasses requests if intra-warp contention is detected. Chen et
al. [23] designed an adaptive resource management scheme
that monitors cache contention and interconnect congestion.
If the degree of cache contention or bandwidth demand is too
high, memory requests will be bypassed. However, MRPB and
the adaptive resource management designs do not distinguish
reuse patterns among memory requests. Cache lines with near
reuse distances may be bypassed, losing an opportunity to

2016 IEEE 34th International Conference on Computer Design (ICCD)

139

improve the cache hit rate. In contrast, Ctrl-C only bypasses
requests having long reuse patterns.
Xie et al. [2] and Liang et al. [3] modiﬁed compilers to
analyze GPGPU applications and guide GPUs to bypass data
which are unlikely to receive reuse. However, the compiler
based schemes are not able to predict the reuse behavior of
input dependent applications. Tian et al. [4] proposed the PCbased Adaptive Bypassing that uses conﬁdence counters to
predict zero-reuse lines and bypasses all requests if detecting
cache lines will not receive any reuse. Instead of bypassing
all requests, Ctrl-C bypasses more intelligently by inserting
only a selective portion of requests which mitigate the degree of cache thrashing. Furthermore, Ctrl-C enables perinstruction bypassing probabilities. By doing so, Ctrl-C can
more effectively learn the reuse patterns of instructions in
GPUs. Lee et al. [24] designed a technique called CCBP to
predict cache reuse behavior and prioritize cache resources for
cache lines required by critical warps. While CCBP targets
to accelerate the critical warp execution, the proposed Ctrl-C
can further improve the performance of GPUs by reducing the
overall MPKI and execution time of GPUs for all warps. Li
et al. [5] suggested adding additional tag array entries to track
the data reuse patterns. Nevertheless, the reuse distance can
be extremely long for GPUs. It is challenging to accurately
predict the data reuse patterns with a limited number of tag
array entries. In contrast, Ctrl-C uses feedback controllers to
learn the reuse pattern with low storage overhead.
VII. C ONCLUSION
In this paper, we present a dynamic scheme to perform
cache bypassing speciﬁcally for GPUs without the need of offline analysis. This paper ﬁrst identiﬁes GPU cache line reuse
patterns and then introduces Ctrl-C to mitigate the data cache
inefﬁciency problem in modern GPUs. Our evaluation results
show that Ctrl-C is able to signiﬁcantly reduce the MPKI and
interconnect bandwidth demand. With Ctrl-C, cache sensitive
applications can achieve a 1.42x speedup.
ACKNOWLEDGMENT
The authors would like to thank Dr. Amrit Panda and the
anonymous reviewers for their insightful feedback. This work
is supported in part by the National Science Foundation (Grant
#CCF-1618039) and by Science Foundation Arizona under the
Bisgrove Early Career Scholarship. The opinions, ﬁndings and
conclusions or recommendations expressed in this manuscript
are those of the authors and do not necessarily reﬂect the views
of the Science Foundation Arizona.
R EFERENCES
[1] W. Jia, K. A. Shaw, and M. Martonosi, “Characterizing and improving
the use of demand-fetched caches in GPUs,” in Proc. of the 26th ACM
International Conference on Supercomputing (ICS), Jun 2012.
[2] X. Xie, Y. Liang, G. Sun, and D. Chen, “An efﬁcient compiler framework
for cache bypassing on GPUs,” in Proc. of the 2013 IEEE/ACM
International Conference on Computer-Aided Design (ICCAD), Nov
2013.
[3] Y. Liang, Y. Wang, and G. Sun, “Coordinated static and dynamic cache
bypassing for GPUs,” in Proc. of the 21st IEEE Symposium on High
Performance Computer Architecture (HPCA), Feb 2015.
[4] Y. Tian, S. Puthoor, J. L. Greathouse, B. M. Bechmann, and D. A.
Jiménez, “Adaptive GPU cache bypassing,” in Proc. of the 8th Workshop
on General Purpose Processing Using GPUs (GPGPU), Feb 2015.

140

[5] C. Li, S. L. Song, H. Dai, A. Sidelnik, S. K. S. Hari, and H. Zhou,
“Locality-driven dynamic GPU cache bypassing,” in Proc. of the 29th
ACM International Conference on Supercomputing (ICS), Jun 2015.
[6] C.-J. Wu, A. Jaleel, W. Hasenplaugh, M. Martonosi, S. C. S. Jr., and
J. Emer, “SHiP: Signature-based hit predictor for high performance
caching,” in Proc. of the 44th IEEE/ACM International Symposium on
Microarchitecture (MICRO), Dec 2011.
[7] M. Rhu, M. Sullivan, J. Leng, and M. Erez, “A locality-aware memory hierarchy for energy-efﬁcient GPU architectures,” in Proc. of the
46th Annaul IEEE/ACM International Symposium on Microarchitecture
(MICRO), Dec 2013.
[8] S. Che, M. Boyer, J. Meng, D. Tarjan, J. W. Sheaffer, S.-H. Lee, and
K. Skadron, “Rodinia: A benchmark suite for heterogeneous computing,”
in Proc. of the 2009 IEEE International Symposium on Workload
Characterization (IISWC), Oct 2009.
[9] L. A. Belady, “A study of replacement algorithms for a virtual-storage
computer,” IBM Systems Journal, vol. 5, pp. 78–101, Apr 1966.
[10] M. K. Qureshi, A. Jaleel, Y. N. Patt, S. C. S. Jr., and J. Emer, “Adaptive
insertion policies for high performance caching,” in Proc. of the 34th
IEEE/ACM International Symposium on Computer Architecture (ISCA),
Jun 2007.
[11] NVIDIA, “CUDA C/C++ SDK code samples v4.0,” May 2011.
[Online]. Available: http://docs.nvidia.com/cuda/cuda-samples
[12] S. Che, J. W. Sheaffer, M. Boyer, L. G. Szafaryn, L. Wang, and
K. Skadron, “A characterization of the Rodinia benchmark suite with
comparison to contemporary CMP workloads,” in Proc. of the 2010
IEEE International Symposium on Workload Characterization (IISWC),
Dec 2010.
[13] B. He, W. Fang, N. K. Govindaraju, Q. Luo, and T. Wang, “Mars:
A MapReduce framework on graphics processors,” in Proc. of the
17th IEEE/ACM International Conference on Parallel Architectures and
Compilation Techniques (PACT), Oct 2008.
[14] S. Che, B. M. Beckmann, S. K. Reinhardt, and K. Skadron, “Pannotia:
Understading irregular GPGPU graph applications,” in Proc. of the 2013
IEEE International Symposium on Workload Characterization (IISWC),
Sep 2013.
[15] A. Bakhoda, G. Yuan, W. W. L. Fung, H. Wong, and T. M. Aamodt,
“Analyzing CUDA workloads using a detailed GPU simulator,” in Proc.
of the 2009 IEEE International Symposium on Analysis of Systems and
Software (ISPASS), Apr 2009.
[16] NVIDIA, “NVIDIA CUDA C programming guide v4.2,” Apr 2012.
[Online]. Available: http://developer.nvidia.com/nvidia-gpu-computingdocumentation
[17] NVIDIA, “NVIDIA compute PTX: parallel thread execution ISA v1.4,”
Mar 2009. [Online]. Available: http://www.nvidia.com/content/CUDAptx_isa_1.4.pdf
[18] NVIDIA, “NVIDIA GeForce GTX 480/470/465 GPU datasheet,” Mar
2010. [Online]. Available: http://www.nvidia.co.uk/docs/IO/90201/GTX480-470-Web-Datasheet-Final4.pdf
[19] A. Jaleel, K. B. Theobald, S. C. S. Jr., and J. Emer, “High performance
cache replacement using re-reference interval prediction (RRIP),” in
Proc. of the 37th IEEE/ACM International Symposium on Computer
Architecture (ISCA), Jun 2010.
[20] S. M. Khan, Y. Tian, and D. A. Jiménez, “Sampling dead block prediction for last-level caches,” in Proc. of the 43rd IEEE/ACM International
Symposium on Microarchitecture (MICRO), Dec 2010.
[21] N. Duong, D. Zhao, T. Kim, R. Cammarita, M. Valero, and A. V. Veidenbaum, “Improving cache management policies using dynamic reuse
distances,” in Proc. of the 45th IEEE/ACM International Symposium on
Microarchitecture (MICRO), Dec 2012.
[22] W. Jia, K. A. Shaw, and M. Martonosi, “MRPB: memory request prioritization for massively parallel processors,” in Proc. of the 20th IEEE
International Symposium on High Performance Computer Architecture
(HPCA), Feb 2014.
[23] X. Chen, L.-W. Chang, C. I. Rodrigues, J. Lv, Z. Wang, and W.-M. Hwu,
“Adaptive cache management for energy-efﬁcient GPU computing,”
in Proc. of the 47th Annaul IEEE/ACM International Symposium on
Microarchitecture (MICRO), Dec 2014.
[24] S.-Y. Lee, A. Arunkumar, and C.-J. Wu, “CAWA: coordinated warp
scheduling and cache prioritization for critical warp acceleration of
GPGPU workloads,” in Proc. of the 42nd IEEE/ACM International
Symposium on Computer Architecture (ISCA), Jun 2015.

2016 IEEE 34th International Conference on Computer Design (ICCD)

A Study of Mobile Device Utilization
Cao Gao1 , Anthony Gutierrez1 , Madhav Rajan2 , Ronald G. Dreslinski1 , Trevor Mudge1 , and Carole-Jean Wu2
1

University of Michigan, Ann Arbor, {caogao, atgutier, rdreslin, tnm}@umich.edu
2
Arizona State University, {mrajan, carole-jean.wu}@asu.edu

Abstract—Mobile devices are becoming more powerful and
versatile than ever, calling for better embedded processors.
Following the trend in desktop CPUs, microprocessor vendors
are trying to meet such needs by increasing the number of cores
in mobile device SoCs. However, increasing the number does
not translate proportionally into performance gain and power
reduction. In the past, studies have shown that there exists little
parallelism to be exploited by a multi-core processor in desktop
platform applications, and many cores sit idle during runtime.
In this paper, we investigate whether the same is true for current
mobile applications.
We analyze the behavior of a broad range of commonly
used mobile applications on real devices. We measure their
Thread Level Parallelism (TLP), which is the machine utilization
over the non-idle runtime. Our results demonstrate that mobile
applications are utilizing less than 2 cores on average, even
with background applications running concurrently. We observe
a diminishing return on TLP with increasing the number of
cores, and low TLP even with heavy-load scenarios. These studies
suggest that having many powerful cores is over-provisioning.
Further analysis of TLP behavior and big-little core energy
efﬁciency suggests that current mobile workloads can beneﬁt from
an architecture that has the ﬂexibility to accommodate both high
performance and good energy-efﬁciency for different application
phases.

I.

I NTRODUCTION

Nowadays, mobile devices are gradually taking over the
functions of traditional desktop applications. High-deﬁnition
video playback, interactive games and web browsing are
commonly supported by the latest smartphones and tablets.
These performance-intensive tasks need powerful hardware
support, which drives microprocessor vendors to continuously
produce better mobile CPUs. Given the strict power budget
of mobile devices, vendors reached the limits of frequency
scaling quickly and turned to multi-processors. The ﬁrst dualcore smartphones, such as Galaxy S II and HTC Sensation,
came to market in 2011. Most of the high-end smartphones
released in 2012 were dual-core or quad-core; in April 2013,
the Samsung Galaxy S4 was released with the Exynos 5 Octa,
which uses ARM’s big.LITTLE architecture and has a total
of eight cores. Mediatek shipped their octa-core SoC in late
2013, and Qualcomm announced their octa-core CPU with
eight A53s in early 2014 as well.
However, some recent smartphones are still equipped with
dual cores. Apple’s new A8 Chip for the iPhone 6, released
in September 2014, uses a dual-core CPU and still provides
satisfactory performance. This leads to the question: How
much of the computation potential residing in multi-core CPUs
is actually being utilized? On the desktop end, Blake et al. [1]
did a study on Thread Level Parallelism (TLP) on a suite of

978-1-4799-1957-4/15/$31.00 ©2015 IEEE

225

representative desktop applications. Their work was to measure
the core utilization in modern multi-core CPUs, and they
suggested that the number of cores that can be proﬁtably
used is less than 3 for most commonly used applications.
It is possible that mobile device applications have similar
characteristics and cannot effectively utilize a quad-core CPU,
let alone hexa- and octa-core. Moreover, the GPU, DSPs,
and ASICs in these systems already exploit much of the
parallelism, leaving little for the CPU.
To make some observations about the beneﬁt of multi-core,
we performed two preliminary experiments on an up-to-date
quad-core mobile device platform. First, we measured how
many cores are actually activated by the Android OS when
running an application. We found that the fourth core was
only activated in 2 out of 21 apps, and the OS also shut off
the third core for nearly half of the apps. Note that activation
does not mean the core is in use; it only means the OS thinks
that the core might be used. Second, we overrode system setup
and manually set the number of activated cores in the system.
Then we ran a browser benchmark [2]; we saw a signiﬁcant
performance improvement from single-core to dual-core, but
negligible improvements from dual-core to triple- and quadcore. Both of these results show rather modest gains from
high numbers of cores (here more than 2). In all, to measure
how much parallelism actually exists is helpful to: a) inform
vendors and prevent them from over-provisioning hardware
that cannot be effectively used, b) highlight the need to ﬁnd
more parallelism, c) provide suggestions for a better design.
In this work, we analyze a broad range of popular mobile
applications to determine how the growing number of cores
are utilized. We measure the Thread Level Parallelism (TLP)
of these applications. The results show that mobile apps are
utilizing less than 2 cores on average, which means multiple
cores are used rather infrequently. A small TLP scalability
is observed for most applications, and increasing the number
of cores has diminishing return on TLP. Even in heavyload real-world scenarios with background applications or
multi-tab browsing, there is still not enough work to keep
utilization high. Due to the physical constraint and interactive
user pattern, mobile applications tend to have less parallelism
to exploit than desktop applications. The GPU and mobile coprocessors on chip also reduce CPU load. All these factors,
and the history of the slow pace of exploiting parallelism
in desktop and mobile software environments [1, 3], indicate
that having many powerful cores is over-provisioning. Further
analysis suggests that current mobile applications can beneﬁt
from a system with the ﬂexbility to satisfy high performance
and good energy-efﬁciency for different application phases. We
ﬁnd that TLP behavior exhibits short peaks and long valleys
rather than remaining constant. Peaks require high perfor-

•

We construct a suite containing representative Android
applications from a variety of categories, as well as
their corresponding test actions.

•

We measure the Thread Level Parallelism (TLP) of
mobile applications on current mobile device platforms and show it is less than 2 on average.

•

•

We observe diminishing returns of TLP when increasing the number of cores. Heavy-load test cases also
show low TLPs, which suggests there is not a lack
of hardware resources. Both demonstrate that having
many powerful cores is over-provisioning.
We make the case for the need of a ﬂexible system that
can accommodate both high performance and good
energy-efﬁciency for different program phases.
II.

M OTIVATION

We perform two preliminary experiments on an Origen
board, a current mobile device platform with a quad-core
1.4GHz ARM Cortex-A9 CPU. First, we measure how many
cores are actually activated by the OS when running an application. The Android system employs a CPU governor which
turns individual cores on or off and changes their frequencies
based on CPU loads. When it ﬁnds that a core sits idle for most
of the time, it will turn it off to save power. We run a suite
of commonly used applications with the default governor—
ondemand. We ﬁnd that the fourth core is only activated in 2
out of the 21 apps we tested. For nearly half of the apps, the
OS also shuts off the third core for most of the time. We show
part of our results in Fig. 1. We plot a breakdown of the time
percentage that each system conﬁguration spends for these
applications. Different colors represent system conﬁgurations
with different numbers of cores activated. The only app in this
graph that activates the fourth core is Google Maps. Browser
and Jetpack (a game) do activate three cores for most of the
time, but Facebook does so only for half of the time, and Email
never. Note that activation does not mean utilization; it only
means the OS thinks that this core might be utilized. The core
could still sit activated and idle at the same time. We will show
the core utilization in the results section.
In the second experiment, we override system setup and
manually set the number of cores activated in the system. Since
web-browsing is among the most commonly used features on
mobile devices [4], we run a browser benchmark, BBench [2]
on two browsers, and compare the performance of different
CPU conﬁgurations. We plot the results in Fig. 2. The score
is the time taken to render the complete set of webpages
in BBench, and lower score means better performance. It

226

	




	


*$-
)$-

(
'
&
%

($-
&$-
$-


!





 


!










Fig. 1: Time breakdown of number of cores activated by the
Android OS. For most of the applications, the fourth core is
always shut down, For some of them, namely Email, Facebook,
Music and Gallery in this graph, most of the time the third core
is not activated as well.

			
	

To summarize, we make the following contributions:

%$$-


			

mance, but not necessary good energy-efﬁciency because these
peaks are usually short, meaning that power has less affect
on overall energy consumption. Valleys, on the other hand,
desire better energy-efﬁciency because they do not require high
performance but usually dominate the application execution.
There is also a number of other research opportunities that
arise, such as building accelerators or customized hardware
to further reduce the thread’s TLP peaks with better energy
efﬁciency, or building better OS infrastructure to utilize mobile
heterogeneous systems.




























 
 
 



 
 
 



	




	


Fig. 2: BBench score for different number of cores. The scores
here are webpage rendering time, so lower is better. Note the
tiny difference in performance among 2 core, 3 core and 4
core conﬁgurations.
is clear from the graph that a single-core system suffers
from poor performance. However, the performance gain is
negligible from dual-core to triple-core and quad-core. It may
seem confusing why the OS activates 3 cores for BBench
when there is such little performance improvement. Actually
it again proves that activation does not mean utilization; the
ondemand CPU governor in Android OS is performance-aware
and will keep the core activated unless it is very unlikely to
be utilized [5].
Both of these tests suggest a more thorough quantitive
investigation of quad-core CPU utilization.
III.

BACKGROUND

A. TLP
To evaluate the utilization of a multi-core system, we
need a metric for system proﬁling. A commonly used metric
would be CPU utilization, which is simply the overall average
CPU usage during runtime. However, it would underestimate
the parallelism in mobile applications. Most of these apps
are interactive, and there is a large portion of idle time for
interactive applications. The program itself could be highly
parallelized with a high utilization during busy time. However,
it sits in the idle state waiting for user input for most of the total
running time, which would drag down the average utilization
number. To avoid this bias, we use Thread Level Parallelism

(TLP) [1, 6]. TLP is deﬁned as the machine utilization over the
non-idle portions of the benchmark’s execution. The formula
for TLP is given by Equation 1:
n
ci i
T LP = i=1
(1)
1 − c0
where ci is the fraction of time that i cores are concurrently
running different threads, and n is the number of cores.
Speciﬁcally, c0 represents idle time fraction, which is excluded
because it does not count towards the program’s parallelism.
Note that TLP is not a performance metric; the software
could still spawn threads that do not perform useful work.
Nevertheless, it is the natural metric to measure multi-core
utilization, especially for interactive applications like the ones
on a smartphone. The TLP serves as a good indicator of the
number of processors needed to support the execution of a
parallelized workload.
B. Early Studies on TLP
Flautner et al. [6] proposed the deﬁnition of TLP in 2000.
At that time, multi-core was mostly exploited in research labs
and appeared only in workstations and servers. They performed
a study of TLP on desktop applications and found that a
dual-core system improves the responsiveness of interactive
programs. However, they also showed that desktop applications
leveraged TLP very sparingly. This result was echoed 10 years
later by Blake et al. [1] with a similar study of TLP of
contemporary software and hardware, when multi-core had
become the norm rather than the exception in home and
ofﬁce desktops. They reported that 2-3 cores were more than
adequate for almost all but a few domain speciﬁc applications
like Video Authoring. After observing low single-thread performance could have a small impact on the TLP, they claimed
that software is lagging behind and is the main limiting factor
in TLP.
Smartphones were already becoming popular during the
time when Blake et al. presented their results, and they have
continued to supplant desktops for many applications. To
reﬂect this it is important to analyze TLP behavior on mobile
devices because the original studies did not. Besides exploring
a different hardware platform, we are also using a slightly
different set of benchmarks from the original work. Some
categories of desktop applications are rarely seen on mobile
devices, such as Video Authoring and professional Image
Authoring.
IV.

M ETHODOLOGY

A. System Setup
We use the Odroid XU+E board. It has a Samsung Exynos
5410 SoC, which contains an ARM big.LITTLE octa core of
four 1.6GHz A15s and four 1.2GHz A7s. Each core has its own
32KB/32KB L1 instruction and data cache; the four A15s share
a 2MB L2 cache and the A7s share a 512KB L2 cache. Either
four A15s or four A7s can be enabled at the same time, but not
a mixture of them. The Odroid board has a PowerVR tri-core
GPU running at 480MHz and with 2GB main memory. It also
has an on-board current/power semiconductor sensor which
measures the current/power consumption of CPUs, GPU and
memory separately1 . We run Android version 4.4.2 (Kitkat)
1 For

CPU power, we measure the sum of power of big and little clusters.

227

and Linux kernel version 3.4.5. We choose the use the ART
runtime instead of the older Dalvik.
B. Measurement
1) TLP: To get the TLP number, we track all the context
switches that happen in the system, which reveals the information about the status of each core. For instance, a context
switch from SurfaceFlinger to swapper on Core #0 indicates
this core has turned from busy to idle. This information gives
us the number of running cores at any time, which is sufﬁcient
to calculate TLP. Moreover, we can get information about
which thread is running and ﬁlter out observation overhead
threads. For example, we treat adbd, the Android Debug
Bridge thread, as swapper. The core that is running adbd
would then be treated as idle, preventing an overestimation
of TLP. We use ftrace, a Linux kernel internal trace, to get
context switches. The data we gathered contains task names,
ids, CPU Number, and timestamp.
2) GPU utilization: For the PowerVR on the Odroid board,
we directly read GPU utilization numbers from the sysfs
interface provided.
C. Benchmarks
In this work we test a diverse range of real-world Android applications. We prefer applications that are: a) most
widely used by users; b) from a broad range of diversiﬁed
categories. Based on these requirements, we choose 18 toppick applications from the Google Play Android App store,
and 4 native ones in the Android OS. This means they are
the applications commonly used in their category and are thus
representative of current mobile software. They come from 10
different categories: browser, video player, music player, image
viewer, communication, games, social networking, navigation,
ofﬁce, and ﬁle browser. They make use of important hardware
resources on a mobile device (CPU, GPU, co-processors, etc).
We then perform test actions on each of the testing
applications. Three applications (browser, Adobe reader and
MX Player) are so widely used that they have already been
included in some benchmarks [2, 7, 8], therefore we leverage
the existing work and use their implementation directly. For
other applications, we design a series of actions that cover most
typical functions of the application under test. We also refer
to the study in [9] on mobile applications usages, including
what the popular applications are and how long each session
(from opening to closing) normally last. Test actions on the
Odroid board are automated using android adb commands and
RERAN, a record and replay tool for Android OS [10]. All
experiments are repeated at least 5 times for more accurate
results, and applications that require an internet connection
are repeated for at least 10 runs. We observe a low standard
deviation of TLP results as shown in Section. V-A.
It is also important to test TLP of scenarios with background applications, to reﬂect common daily usage. We also
test three applications with a set of other applications running
in the background. The three applications under test are Angry
Birds, Adobe reader, and Chrome, while the background
applications are Hangout, Spotify, and Email.
We brieﬂy introduce each application, and its corresponding test actions in the following subsections.

1) Web browser: We use the Realistic General Web Browsing (R-GWB)[8], an automatic webpage rendering benchmark.
It comprises ofﬂine pages of several most popular webpages,
all of which utilize modern web technology such as CSS,
HTML, ash, and multi-media. During the experiment, MobileBench uses JavaScript to load each webpage and then scroll
over it with a pre-set speed. By doing so, it simulates an
actual web browsing scenario. We run MobileBench on three
popular browsers: the Android stock web browser, Firefox, and
Chrome. For each test, we iterate through the MobileBench
webpage set ﬁve times, and proﬁle the third one. We do not
impose any other user input as MobileBench is automatic itself.
2) Video Player: We use two applications: MXPlayer, a
video playback application and Netﬂix, an online streaming
application. We test both applications by playing a video for
30 seconds, with a 1 second pause at the 15th second of each
of the tests.
3) Music Player: We use the Android stock music player
and Spotify for testing music players. Spotify is a music player
that supports online music streamling. We test both of the two
apps by running a series of actions including open new song,
jump to an arbitrary position of the song (not in spotify), and
open another song.
4) Image Viewer: We use Android stock image viewer
(Gallery) and Instagram for testing image viewers. We test it
by opening images, scrolling between images, opening another
image and new folders, and playing a slideshow for a couple
of seconds. Instagram is mobile photo-sharing service. Users
take pictures share them on a variety of social networking
platforms. We test it by scrolling new feeds, opening a picture,
applying Amaro ﬁlter and changing brightness to 75, and then
sharing the picture.
5) Communication: We use Google Hangout and Skype
here. We test both of these applications by initiaing a 1
minute video call, 30 seconds in foreground and 30 seconds
in background (approximating an audio call).
6) Games: The three games we choose are Angry Birds,
Fruit Ninja, and Jetpack. Angry Bird is a puzzle video game;
in the game, players use a slingshot to launch birds at pigs
stationed on or within various structures, with the intent of
killing all the pigs on the playing ﬁeld. We play this game
by entering the ﬁrst stage, ﬁring two birds with one miss and
one hit. Fruit Ninja is an action game with lots of ﬂoating
objects and high-frequency user input. It represents a more
intensive mobile game comparing to Angry Birds. We play
this game in the “zen mode”, where fruit keeps spawning
for 90 seconds. During testing, the tester keeps sliding with
a constant frequency horizontally in the upper middle of the
screen. Jetpack Joyride is a game where the player tries to
control the rider to avoid barriers and collect coins. We play
this game by tapping the screen at a regular frequency for 45
seconds.
7) Social Networking: We choose the apps from two major
Social networking service providers, Facebook and Twitter.
When testing Facebook, we scroll feeds, open up the pictures
in the feeds and browse the proﬁle of the user. The action of
testing Twitter includes clicks on tweets, checking out picture
in the tweets and looking at the proﬁle of the tweeter.

228

8) Navigation: The most popular navigation application on
Android platform is Google Maps. We test Google Maps by
searching directions between airports in New York city: we
search driving directions from Newark to JFK, then public
transportation from JFK to LaGuardia. We also save an ofﬂine
map of New York city to avoid fetching map data from the
internet during testing.
9) Ofﬁce: The ofﬁce category contains a broad range of
commonly used apps. We test the following: 1) Android stock
Email — we test it by writing an email and save that as a draft,
sending it, checking and downloading new email. 2) Adobe
reader — actions here include opening a pdf, zooming in and
out, scrolling pages and searching for a keyword. 3) Amazon
Kindle — actions here include opening a book, scrolling and
jumping to arbitrary positions.
10) File Browser: We test Dropbox and ES File Browser.
For Dropbox, we open and change folders, sort the content
in the folder, do searching and editing new text ﬁle. For ES
File Browser, we open the folders on the SD cards, scroll the
images in it, sort and change the view of the folder.
11) Background: The background applications we choose
are Google Hangout (video chatting), Spotify (playing music),
Email (checking emails)2 . With those applications we test Fruit
Ninja, Maps, and Adobe Reader.
V.

R ESULTS

In this section, we present our experimental results and
analysis of mobile device utilization, speciﬁcally on CPU and
GPU. First, we show that current mobile applications have a
rather low average TLP on modern mobile device platforms.
We show that increasing the number of cores has diminishing
returns on TLP. Even some heavy-load real-world scenarios do
not use many cores. High GPU utilization also indicates that
some of the parallelism is already ofﬂoaded from the CPU
to the GPU. All these factors, and the history of the slow
pace of exploiting parallelism in desktop environments [1],
suggests that having many powerful cores is likely to be overprovisioning.
A. Overall Results
We list a summary of the results in Table I. Each row
in the table shows the TLP and standard deviation (σ) for
an application type. The ﬁrst line, “System”, refers to the
plain Android OS testing environment without any application
running; the last line, “Average”, is the average of the statistics
of all tested applications. The standard deviation of the TLP
over runs for each application is low. This indicates the tests
are reproducible and insensitive to user input variation.
We make two observations based on these results:
1) All the applications demonstrate some, but quite
limited TLP.
For Android, even in a case where a developer writes
code with no awareness of multi-threading, a number of
threads are still created for external I/O, garbage collection,
graphics rendering, etc. This means that even a programmer
2 During the test we automatically send an email to the account on board
from the host machine every half minute

Category
System
Browser

Video Player
Music Player
Image Viewer
Communication
Games

Social Network
Navigation
Ofﬁce

File Browser
Background

Average

App
[None]
Stock Browser
Firefox
Chrome
MXPlayer
Netﬂix
Stock Music
Spotify
Stock Gallery
Instagram
Google Hangout
Skype
Angry Birds
Fruit Ninja
Jetpack
Facebook
Twitter
Google Maps
Stock Email
Adobe Reader
Kindle
Dropbox
ES ﬁle browser
Back Fruit
Back Maps
Back Adobe

TLP
1.03
1.47
1.31
1.66
1.34
1.53
1.29
1.23
1.46
1.31
1.82
1.55
1.31
1.40
1.54
1.43
1.32
1.59
1.52
1.30
1.45
1.33
1.22
1.65
1.91
1.60
1.46

σ (TLP)
0.00
0.03
0.02
0.01
0.01
0.07
0.03
0.05
0.03
0.03
0.15
0.13
0.08
0.12
0.09
0.04
0.04
0.06
0.04
0.05
0.01
0.02
0.02
0.12
0.11
0.14
0.06













	



 


	
































 

 












	

Fig. 3: Time breakdown of how the multi-core is utilized.
The big pie chart on the right shows an average result of all
application categories we tested. The smaller pie charts show
the breakdown of three representative kinds of apps: Google
Maps with the highest TLP, stock Browser, and stock Music
with a low TLP.

TABLE I: TLP results for the Odroid board — using ondemand governor.

for TLP. For each category, the leftmost bar shows the TLP
when 4 cores are kept activated. The middle one shows the
TLP when the fourth core is shut down and the remaining 3
cores are kept activated. Similarly, the rightmost bar shows the
system conﬁguration with 2 cores. The results demonstrate:

with no idea about multithreading could beneﬁt from parallel
processing on different CPU cores. Moreover, many software
developers are aware of the multi-core hardware they are using
and write applications explicitly with multiple threads.

1) Increasing the number of cores has little impact on
TLP.

However, the parallelism we observed is generally still
quite low. On average, we see a TLP of 1.46. The application
with the highest TLP, Google Hangout, has a TLP of just 1.8.
Applications like Music and File Browser have rather low TLP,
around 1.2 to 1.3. This result shows, on average, the system
is using less than 2 cores.

2) Most applications show some scalability, but not
much.

2) Multi-core is utilized infrequently.
We present a time breakdown of how the multi-core system
is utilized in Fig. 3. The big pie chart on the right shows
an average result of all application categories we tested. The
smaller pie charts show the breakdown of three representative
application categories. We observe a very low 4-core and 3core utilization; on average only 0.68% of all the non-idle time
is the system fully utilized (all four cores running), and 5.81%
of the time when three cores are used — this means there is
only a small amount of time that four or three cores are being
utilized at the same time.
B. Core Scaling
Microprocessor vendors put more cores on a chip to exploit
more parallelism in the system. Therefore, it is important to
consider the change of TLP as the system scales from 2 to 3
to 4 cores.
We show our TLP results in Fig. 4a. We change the number
of active cores in the system and repeat the same experiments

229

On average, TLP increased by 7.03% when we switch from
a 2-core system to a 3-core system, and only 3.47% from a
3-core system to a 4-core system.

Particularly, Games, Navigation, Ofﬁce and Social apps
show over a 10% increase of TLP from a 2-core to a 4-core
system. File manager only shows a 5.6% increase. Most apps
show small increases in TLP from 3-core to 4-core which are
below 4%. This indicates that the software does not generate
many concurrently parallel threads during its execution.
C. Heavy Load Scenarios
Intuitively, a multi-core system is beneﬁcial when the CPU
load is high. In this section we test the TLP of a couple of
heavy load scenarios.
1) Background Applications: It is now common to have
several applications like music or email checking running
in background concurrently with a foreground application.
One argument that favors having more cores is that they can
boost performance of such scenarios. We measure the TLP
of several applications with a set of background applications
running concurrently (described in Section. IV-C), and present
our result in Table I and Fig. 4b. The results demonstrate
that background applications only lead to limited increase in
TLP, and we are still not fully utilizing all four cores with
background activities.

#)

&


#(

%


$














#'
#&
#%
#$
##
#"








 













	





(a) TLP
























	

(b) The red regions show the increase of TLP in the presence of
background apps

Fig. 4: Overall TLP result
2) Multi-tab Web Browsing: We test multi-tab scenarios
to see how mobile browser applications exploit parallelism
under high load circumstances. We measure the TLP and
performance of MobileBench on different CPU conﬁgurations.
We run one, two and three MobileBench tabs concurrently
and measure the average of the metrics. We manually switch
between tabs constantly to make them appear in the foreground
for similar amount of time. Fig. 5 shows the results. The
maximum TLP is still below 2 for big cores and 2.2 for little
cores, and there is signiﬁcant performance degradation when
increasing the number of tabs.
The reason for low TLP here is that even for these two
cases, we do not see a real “multi-tasking” scenario; instead,
we see a main task and several light-load tasks, and that does
not exhibit a high TLP. For instance, for multi-tab browsing,
only the visible web pages will be loaded at regular speed,
and all background pages will be given much less priority
and use less CPU. Energy and thermal constraints are tight
in mobile devices. It might be that the developers realize
there is not enough need for implementing a fast but energyhungry multi-tab browser for mobile phones. Reasons may
include small display size or typical user behavior. In other
words, the physical constraints and use pattern could reduce
the amount of parallelizable work of mobile applications. In
the future, we may have phones with bigger screens and higher
resolutions, but human user perception will not change. On
the other hand, the desktop TLP study by Blake et al. [1]
showed the TLP of desktop applications remained relatively
low, even after 10 years of effort writing parallelized software.
Similarly, we have not seen a signiﬁcant increase in TLP
compared work from one year ago [3]. Clearly, parallelizing
software is an extremely challenging problem, particularly for
desktop/mobile applications.
D. Little Cores
We also perform the same set of TLP tests on the little
cores in order to see how a less powerful CPU would affect
TLP. We present our results in Fig. 6. Both the TLP and the
average percentage of time that four or three cores are utilized
has increased when using little cores compared to big cores.
One reason is that tasks on more powerful cores run faster and
ﬁnish earlier, reducing the overlap between them. This result
suggests that as CPU architecture designs improve, exploiting
TLP will be harder. The CPUs will be less utilized if software
developers fail to produce better parallelized program.
Nevertheless, we still have TLP less than 2. Further

230

suggesting that software is still the main limiting factor in
exploiting TLP.
E. GPU
Applications like games and web browsers require large
amount of graphical computation. On the hardware side, almost all mobile device SoCs now contain their own GPU units.
On the OS side, the Android 2D rendering pipeline has started
to support hardware acceleration in Android 3.0 (Honeycomb).
Hardware acceleration is enabled by default from Android 4.0
(Ice Cream Sandwich). Therefore, it is important to analyze
the actual utilization of mobile device GPUs.
We measure the GPU utilization of the same suite of
applications on the Odroid board. We show our experimental
result of GPU usage in Fig. 7. For each category, the leftmost
bar shows the average GPU utilization when 4 big cores are
kept activated, then 3 big cores, 2 big cores, and little cores. We
have not found much variance when we change the number
and type of CPU cores. Average GPU utilization is 24.1%,
and some speciﬁc applications such as games, communcation
(chats in the graph) and navigation utilize a considerable
amount of the GPU. This is an indication that a part of the
parallelism is already ofﬂoaded from the CPU to the GPU,
which reduces the amount of parallelism that the CPU can
exploit.
Given the availability of programmable GPUs, an increasing amount of general-purpose, non-graphics work can be
ofﬂoaded from the application cores to the GPU or other
accelerators for performance and energy efﬁciency. This led us
to examine the energy efﬁciency of computation ofﬂoading for
mobile platforms. We analyze several applications on both the
CPU and the GPU and present the results in Fig. 8. We run the
OpenMP and OpenCL versions of three machine learning algorithms – kmeans, backpropagation (BP), and nearest neighbor
(NN) as well as a streaming algorithm Daxpy on a Qualcomm
Snapdragon board, one of the few development boards which
support ofﬂoading for general-purpose GPU applications. We
evaluate the machine learning algorithms because these are
the important building blocks of application domains such as
audio recognition, image recognition/processing, and recommendation algorithms.
The programs running on the Krait CPU are written in
OpenMP whereas the programs running on the Adreno GPU
use OpenCL to exploit the heterogeneous GPU compute unit.
We ﬁnd that for Daxpy, the Krait CPU achieves higher energy

	



	




























	







	







	



















	






	



	



	





	














	









	



	

	


	






	



	


	

	







	


	


























	





	










	


(a) Big cores / Chrome

(b) Little cores / Chrome

Fig. 5: Performance and TLP results for browser. Performance are shown in columns and TLP in lines. Performance scores
are calculated by taking the inverse of MobileBench rendering time then normalize against the worst score in each graphs. For
instance, score bC4 stands for performance of four big cores, lC2 for two little cores, etc. For each CPU conﬁguration, we test
three scenarios: 1, 2 and 3 tabs running MobileBench on Chrome.
$,
$+
$*
$)
$(
$'
$&
$%
$$
$#

'


&


%



		


 



	












  







  	





(a) TLP

(b) Average CPU Time breakdown for a 4-core conﬁgurations

Fig. 6: TLP result for little cores

'


&


%


'

&

!

%

'#+
&(+
&#+
%(+
%#+
$(+
$#+
(+
#+












   	 !

	


























!

	





  
!

'(+




	
"#

(#+




	










Fig. 7: GPU utilization of different category of apps. Columns
with different colors represent system conﬁguration with different number and kind of cores activated.

Fig. 8: Energy consumption and energy efﬁciency (deﬁned as
performance per watt) comparison for Krait CPU vs. Adreno
GPU.

efﬁciency than the GPU (less than 1). This means that when
the parallelism can be well exploited by the software, Daxpy,
and when the instructions are simple enough for the CPU, the
energy efﬁciency of the multicore CPU can be equivalent or
slightly higher than that of the GPU. On the other hand, for
the machine learning algorithms, GPU offers higher energy
efﬁciency of varying degrees. This suggests that, for software
where there is an ample amount of thread- and data-level
parallelism, e.g., the machine learning algorithms, and where
there are instructions that can be accelerated by the GPU,

e.g., the multiplication, sqaure root mathematical functions,
it is more beneﬁcial to ofﬂoad the computation to the GPU
or other accelerators for performance and energy efﬁciency.
For workloads with more branch-divergence, or with a small
amount of parallelism, it is more efﬁcient to run them on the
CPU. In short, the variety of mobile workloads suggests that
we should look deeper into building a suitable heterogeneous
system to take advantage of the different types and the varying
degree of parallelism and better utilizing the existing hardware
real estate.

231

"#

S UGGESTIONS

In the previous section, we demonstrate that current mobile
applications are not fully utilizing mobile devices, and simply
adding more cores can be over-provisioning. However, it is
not clear yet what kind of system is more desired for mobile
devices. In this section, we try to shed light on this question
by further analyzing the TLP behavior and energy efﬁciency
of current CPUs. We make the following two observations:

	

VI.


	







	
	

"
!#

$

 


	






	


%

!
 #

 

 

%

	


		
"	

!	

	

	

"	

!	

	

	

a) TLP behavior exhibits short peaks and long valleys
rather than staying constant. This suggests that during peak
TLP times, higher performance is desired: the system needs
to be kept responsive for better user experience, also these
peaks are short so the extra computation power will not have
a big impact on total energy consumption. During low TLP
times when the performance requirement is low, better energyefﬁciency is required as any extra power will be a waste and
will affect battery life.

Fig. 10: Performance and power under different frequency and
cores (tested using MobileBench). Lines represent different
cores conﬁgurations; dots on lines represent different frequencies, with lower frequencies (thus poor performance) on the
left. Error bars are drawn to a show range of scores for each
dot.

b) For current systems, there is a distinct energy-efﬁciency
difference between the big and little cores.

B. Energy Efﬁciency of Big and Little Cores

Based on these observations, we argue that current mobile
applications can beneﬁt from a system that has ﬂexibility
to accommodate both high performance and good energyefﬁciency under different applications as well as different
program phases. Architectures including heterogenous multicores[11, 12] and ﬂexible core architectures[13–15] might be
among the possible solutions.

A. TLP vs. Time
We record the TLP over time for applications and present
the results in Fig. 9. We choose the 20 seconds3 of the test
starting from launching the applications. For Browser, pronounced peaks can be seen in TLP (Fig. 9a). In MobileBench
these occur when the application is launched and new browser
webpages are opened (these actions are labeled in circled numbers in Fig. 9a). We also see a shift of peaks towards the right
from a 4 core system to a 2 core system corresponding with the
actions, which reﬂects a quicker webpage load time in a 4 core
system. For MXPlayer, there are more signiﬁcant peaks during
application launch and when starting, pausing and resuming a
video. For Angry Birds, there are less pronounced peaks during
runtime but it still shows one when the application launches
as well as few others during the game.
These results show that the interactive nature of most
mobile application can cause TLP to ﬂuctuating above 2, but
the average TLP still remains low. The peaks do suggest a
need for multiple cores for quicker response time, which is
critical for better user experience. However, multiple cores are
used only during brief bursts, mostly at the application launch
time. Moreover, even during these peaks, we do not observe
a constant high peak TLP (above 3). This suggests that the
idea of keeping many big cores (four or even more) for short
bursts may not be a good choice for mobile devices. Instead,
a system that can provide high performance during peaks and
good energy-efﬁciency ﬁts better with the ﬂuctuate TLP pattern
of mobile applications.

232

#



#

 

"

$



%



 

The processor takes up a substantial portion of power consumption in mobile devices [16]. It is meaningful to analyze
the energy efﬁciency between big cores and little cores. We run
MobileBench on the Odroid board for different types of cores,
different number of cores, and three different frequencies4
on each cluster (big and little). We show the results on
two different clusters in Fig. 10. For every core/frequency
combination, we did four repeated runs. The results show a
distinct energy-efﬁciency difference between the big and little
cores: big cores have better performance, but little cores only
use roughly a quarter of the power consumption than big cores.
Though big cores have approximately 25% less execution time,
their power consumption is 3× more than little cores so they
consume more total energy. In a system with ﬂexbility, we can
use little cores as much as possible, and big cores in situations
that are both computational intensive.
Additionally, the importance of user experience makes
such architecture more desirable. Human users want to deliver
a response within a user acceptable timeframe rather than
ﬁnishing the task as fast as possible. For instance, literature
shows that a latency less than 0.1s is not perceivable by a
human [17]. Any extra resources that are used in accelerating
the program to ﬁnish faster than 0.1s is unnecessary. In the case
of browser performance, as shown in Fig. 10, we may or may
not need to switch to a big core depending on the workload of
the webpage and the quality of experience demanded by the
user. More ﬂexibility in such scenarios will be beneﬁcial for
both performance and energy-efﬁciency.
VII.

R ELATED WORKS

A. Mobile Workload Characterization
Extensive research has been done on characterizing mobile
workloads. Gutierrez et al. [2] measure the microarchitectural behavior of a set of mobile applications. Hayenga et
3 20
4 We

cores.

seconds is enough for apps to reach steady state.
use 1.6, 1.2 and 0.8GHz for big cores, and 1.2, 0.8, 0.5GHz for little








	

	

	


	













	

	

                    

 	 
   
     	 		 	
 	 	 	
 	 	 	 	 


(b) Chrome

(a) Browser (4 cores)























	


	


	





	

 	 
   
     	 		 	
 	 	 	
 	 	 	 	 


 	 
   
     	 		 	
 	 	 	
 	 	 	 	 


(d) Angry Birds

(c) MXPlayer

Fig. 9: TLP vs. time in seconds for mobile applications. For Browser, the solid lines represent TLP, and the dashed vertical lines
show when there is an action, such as application startup or opening a new webpage. Actions are labeled by circled numbers.

al. [18] present a workload characterization and memorylevel analysis of internet and media-centric applications for
an embedded system used for mobile applications. Sunwoo
et al. [19] propose a methodology to tractably explore the
processor design space and to characterize applications in a
full-system simulation environment. Zhang et al. [20] study
the performance of mobile applications using multicore CPUs
and develop a new CPU power model with a high accuracy.
Their results also show that even large applications like web
browsers with multi-threading acceleration cannot fully utilize
the multicore CPUs. Narancic et al. [21] evalute the memory
system behavior of smartphone workloads and show that
many workloads are memory throughput bound, especially
with specialized compute engines providing enough compute
performance. We directly measure multi-core utilization of
mobile applications and analyze its implications.
B. Mobile Benchmarks
There has been literature which argue traditional desktop
benchmarks such as PARSEC or SPEC are not suitable for
mobile devices [2]. Several mobile benchmarks have been
proposed. MobileBench [8] is a collection of applications,
including a revised version of BBench, Adobe PDF reader,
Photo viewer and video playback. Mobybench [7] is also
comprised of popular applications, which has already been
ported to the gem5 simulator [22]. AM-Bench [23] is an
open source based mobile multimedia benchmark for Android
platform. Some more application-speciﬁc benchmarks have
also been proposed [24, 25]. Compared to these benchmarks,
we have a boarder coverage of common categories with more
applications such as social networking or communication.
C. Mobile CPU Architectures
Zhu and Reddi [4, 26] propose two specialized hardware
and an event-based scheduling for mobile web applications.
Several papers have presented work on addressing the Dark
Silicon problem [27–30]. They propose specialized, energyefﬁcient hetergeneous co-processors which provides much

233

better energy-efﬁciency. By providing quantitive analysis of
mobile workload CPU utilization, we provide a written record
of information that can beneﬁt researchers pursuing better
mobile CPU designs.
VIII.

C ONCLUSION AND D ISCUSSION

In this paper, we considered how multi-core processors in
mobile devices are being used. We have shown that current
mobile applications cannot effectively use a large number of
cores. Instead, we suggest that a ﬂexible system that can accommodate both high performance and good energy-efﬁciency
is a more preferable choice for current mobile applications.
We have analyzed a wide range of common mobile applications, and calculated the Thread Level Parallelism (TLP)
of these applications. The average TLP across all categories
is 1.46, which shows that mobile apps are utilizing less than
2 cores on average. The applications with the highest TLP,
Google Hangout, only has a TLP of just 1.8. We have also
evaluated a number of different CPU conﬁgurations, including
different numbers of cores, core frequencies, and CPU types.
We observe a diminishing return on TLP when the number of
cores increases. Even in those heavy-load real-world scenarios
with background applications or multi-tab browsing, there is
still not enough work to keep utilization high. Both these
results suggests that having many powerful cores is overprovisioning. Due to physical constraint and interactive user
patterns, mobile applications tend to have less parallelism to
exploit than desktop applications. The GPU and mobile coprocessors on chip also takes off work from CPU. Historically,
the desktop TLP study by Blake et al. [1] showed the TLP of
desktop applications remained relatively low, even after a 10
year gap. It indicates that parallelizing software is an extremely
challenging problem. All these contribute to the low TLP for
current mobile applications.
On the other hand, we ﬁnd out that TLP behavior exhibits
peaks and valleys rather than remaining constant. User experience, which is critical for mobile applications, also varies by
different application scenarios and different users. A system

with the ﬂexiblity to satisfy both high performance and good
energy-efﬁciency for different program phases is a good choice
for mobile devices.

[13]

K. Khubaib, M. A. Suleman, M. Hashemi, C. Wilkerson, and Y. N.
Patt, “Morphcore: An energy-efﬁcient microarchitecture for high performance ilp and high throughput tlp,” in Proceedings of the 45th Annual
IEEE/ACM International Symposium on Microarchitecture, 2012.

We believe this work can motivate new research directions.
TLP is a utilization metric rather than a performance metric.
We have only used web browser benchmarks [2, 8] as performance metrics in this paper; the research community can
beneﬁt from having benchmarks that quantitatively measure
the performance for popular applications of various categories
such as games, social, ofﬁce, etc. Responsiveness is another
metric worth noting, especially for user experience of interactive mobile applications [5]. This is also where the peak
TLP mainly comes from. Building an accelerator or speciﬁc
processor architecture tailored for such phases may be an
interesting avenue of research into.
IX. ACKNOWLEDGEMENT

[14]

P. Petrica, A. M. Izraelevitz, D. H. Albonesi, and C. A. Shoemaker,
“Flicker: A dynamically adaptive architecture for power limited multicore systems,” in Proceedings of the 40th Annual International Symposium on Computer Architecture (ISCA), 2013.

[15]

A. Lukefahr, S. Padmanabha, R. Das, F. M. Sleiman, R. Dreslinski,
T. F. Wenisch, and S. Mahlke, “Composite cores: Pushing heterogeneity
into a core,” in Proceedings of the 2012 45th Annual IEEE/ACM
International Symposium on Microarchitecture, 2012.

[16]

A. Carroll and G. Heiser, “An analysis of power consumption in
a smartphone,” in Proceedings of the 2010 USENIX conference on
USENIX annual technical conference, 2010.

[17]

R. B. Miller, “Response time in man-computer conversational transactions,” in Proceedings of Fall Joint Computer Conference, Part I, 1968.

We would like to thank our shepherd, Vijay Janapa Reddi,
and the anonymous reviewers for their valuable feedback. This
work is supported in part by a grant from ARM Ltd and NSF
I/UCRC Center for Embedded Systems (NSF grant #0856090).

[18] M. Hayenga, C. Sudanthi, M. Ghosh, P. Ramrakhyani, and N. Paver,
“Accurate system-level performance modeling and workload characterization for mobile internet devices,” in Proceedings of the 9th workshop
on MEmory performance: DEaling with Applications, systems and
architecture, 2008.
[19]

D. Sunwoo, W. Wang, M. Ghosh, C. Sudanthi, G. Blake, C. D.
Emmons, and N. C. Paver, “A structured approach to the simulation,
analysis and characterization of smartphone applications,” in Workload
Characterization (IISWC), IEEE International Symposium on, 2013.

[20]

Y. Zhang, X. Wang, X. Liu, Y. Liu, L. Zhuang, and F. Zhao, “Towards
better cpu power management on multicore smartphones,” in Proceedings of the Workshop on Power-Aware Computing and Systems, 2013.

[21]

G. Narancic, P. Judd, D. Wu, I. Atta, M. Elnacouzi, J. Zebchuk,
J. Albericio, N. Enright Jerger, A. Moshovos, K. Kutulakos, and
S. Gadelrab, “Evaluating the memory system behavior of smartphone
workloads,” in Embedded Computer Systems: Architectures, Modeling,
and Simulation (SAMOS XIV), 2014 International Conference on, 2014.

[22]

N. Binkert, B. Beckmann, G. Black, S. K. Reinhardt, A. Saidi, A. Basu,
J. Hestness, D. R. Hower, T. Krishna, S. Sardashti, R. Sen, K. Sewell,
M. Shoaib, N. Vaish, M. D. Hill, and D. A. Wood, “The gem5
simulator,” in SIGARCH Comput. Archit. News, 2011.

[23]

E. K. Chayong Lee and H. Kim, “The am-bench: An android multimedia benchmark suite,” in Technical Report, School of Computer Science,
Georgia Institute of Technology, 2012.

[24]

Aurora
softworks:
Quadrant.
[Online].
http://www.aurorasoftworks.com/products/quadrant

[25]

Gfxbench: uniﬁed graphics benchmark based on dxbenchmark
(directx) and glbenchmark (opengl es). [Online]. Available:
http://gfxbench.com/result.jsp

[26]

Y. Zhu, M. Halpem, and V. J. Reddi, “Event-based scheduling for
energy-efﬁcient qos (eqos) in mobile web applications,” in Proceedings
of the 42th annual International Symposium on Computer Architecture
(ISCA), 2015.

[27]

G. Venkatesh, J. Sampson, N. Goulding, S. Garcia, V. Bryksin, J. LugoMartinez, S. Swanson, and M. B. Taylor, “Conservation cores: Reducing
the energy of mature computations,” in Proceedings of the 15th international conference on Architectural support for programming languages
and operating systems (ASPLOS), 2010.

[28]

G. Venkatesh, J. Sampson, N. Goulding-Hotta, S. K. Venkata, M. B.
Taylor, and S. Swanson, “Qscores: Trading dark silicon for scalable energy efﬁciency with quasi-speciﬁc cores,” in Proceedings of the Annual
IEEE/ACM International Symposium on Microarchitecture, 2011.

R EFERENCES
[1]

[2]

[3]

[4]

[5]

[6]

[7]

[8]

[9]

[10]

[11]
[12]

G. Blake, R. G. Dreslinski, T. Mudge, and K. Flautner, “Evolution of
thread-level parallelism in desktop applications,” in Proceedings of the
37th annual International Symposium on Computer Architecture (ISCA),
2010.
A. Gutierrez, R. G. Dreslinski, T. F. Wenisch, T. Mudge, A. Saidi,
C. Emmons, and N. Paver, “Full-system analysis and characterization
of interactive smartphone applications,” in Workload Characterization
(IISWC), 2011 IEEE International Symposium on, 2011.
C. Gao, A. Gutierrez, R. Dreslinski, T. Mudge, K. Flautner, and
G. Blake, “A study of thread level parallelism on mobile devices,” in
Performance Analysis of Systems and Software (ISPASS), 2014 IEEE
International Symposium on, 2014.
Y. Zhu and V. J. Reddi, “Webcore: architectural support for mobileweb
browsing,” in Proceedings of the 41th annual International Symposium
on Computer Architecture (ISCA), 2014.
L. Yang, R. Dick, G. Memik, and P. Dinda, “Happe: Human and
application driven frequency scaling for processor power efﬁciency,”
in Mobile Computing, IEEE Transactions on, 2013.
K. Flautner, R. Uhlig, S. Reinhardt, and T. Mudge, “Thread-level
parallelism and interactive performance of desktop applications,” in
Proceedings of the ninth international conference on Architectural
support for programming languages and operating systems (ASPLOS),
2000.
Y. Huang, Z. Zha, M. Chen, and L. Zhang, “Moby: A mobile benchmark
suite for architectural simulators,” in Performance Analysis of Systems
and Software (ISPASS), 2014 IEEE International Symposium on, 2014.
D. Pandiyan, S.-Y. Lee, and C.-J. Wu, “Performance, energy characterizations and architectural implications of an emerging mobile platform
benchmark suite: Mobilebench,” in Workload Characterization (IISWC),
2013 IEEE International Symposium on, 2013.
M. Böhmer, B. Hecht, J. Schöning, A. Krüger, and G. Bauer, “Falling
asleep with angry birds, facebook and kindle: A large scale study on
mobile application usage,” in Proceedings of the 13th International
Conference on Human Computer Interaction with Mobile Devices and
Services, 2011.
L. Gomez, I. Neamtiu, T. Azim, and T. Millstein, “Reran: Timing- and
touch-sensitive record and replay for android,” in Software Engineering
(ICSE), 2013 35th International Conference on, 2013.
big.little processing with arm cortex-a15 & cortex-a7. [Online]. Available: http://www.arm.com/ﬁles/download/big LITTLE Final Final.pdf
Variable
smp
a
multi-core
cpu
architecture
for
low
power
and
high
performance.
[Online].
Available:
http://www.nvidia.com/content/PDF/tegra white papers/tegrawhitepaper-0911b.pdf

234

Available:

[29] A. Raghavan, Y. Luo, A. Chandawalla, M. Papaefthymiou, K. P. Pipe,
T. F. Wenisch, and M. M. K. Martin, “Computational sprinting,” in
Proceedings of the 2012 IEEE 18th International Symposium on HighPerformance Computer Architecture (HPCA), 2012.
[30] H. Esmaeilzadeh, E. Blem, R. St Amant, K. Sankaralingam, and
D. Burger, “Dark silicon and the end of multicore scaling,” in Proceedings of the 38th annual International Symposium on Computer
Architecture (ISCA), 2011.

PACMan: Prefetch-Aware Cache Management
for High Performance Caching
§

Carole-Jean Wu∗ Aamer Jaleel† Margaret Martonosi∗ Simon C. Steely Jr.† Joel Emer†‡
Princeton University ∗
Princeton, NJ
{carolewu,mrm}@princeton.edu

Intel Corporation, VSSAD †
Hudson, MA
{aamer.jaleel,simon.c.steely.jr,
joel.emer}@intel.com

Massachusetts Institute of Technology ‡
Cambridge, MA

ABSTRACT

1. Introduction

Hardware prefetching and last-level cache (LLC) management are
two independent mechanisms to mitigate the growing latency to
memory. However, the interaction between LLC management and
hardware prefetching has received very little attention. This paper
characterizes the performance of state-of-the-art LLC management
policies in the presence and absence of hardware prefetching. Although prefetching improves performance by fetching useful data
in advance, it can interact with LLC management policies to introduce application performance variability. This variability stems
from the fact that current replacement policies treat prefetch and
demand requests identically.
In order to provide better and more predictable performance, we
propose Prefetch-Aware Cache Management (PACMan). PACMan
dynamically estimates and mitigates the degree of prefetch-induced
cache interference by modifying the cache insertion and hit promotion policies to treat demand and prefetch requests differently.
Across a variety of emerging workloads, we show that PACMan
eliminates the performance variability in state-of-the-art replacement policies under the influence of prefetching. In fact, PACMan improves performance consistently across multimedia, games,
server, and SPEC CPU2006 workloads by an average of 21.9% over
the baseline LRU policy. For multiprogrammed workloads, on a 4core CMP, PACMan improves performance by 21.5% on average.

Technology trends show that main memory speeds significantly
lag behind processor speeds. In response, both industry and academia have invested significant efforts in improving a processor’s
memory subsystem. One approach relies on efficiently managing the on-chip last-level cache (LLC) through intelligent cache
replacement. Such proposals reduce LLC interference by dynamically modifying the cache insertion policy to prioritize the most
reused and important data. Another popular approach is to prefetch
data into the cache hierarchy before the actual reference. While
prefetching can hide memory latency and improve performance
significantly, it can severely degrade performance in the event of
untimely and/or inaccurate prefetch requests. To avoid prefetcher
pollution, recent studies have proposed alternative cache insertion
policies for prefetch requests [18, 31]. Unfortunately, these proposals incur significant hardware overhead. In contrast, this paper
proposes simple and low-overhead dynamic prefetch-aware LLC
management to provide consistent performance improvements in
the presence of hardware prefetching.
State-of-the-art cache replacement policies improve performance
over the baseline least-recently-used (LRU) replacement policy by
preserving the useful working set in the cache [2, 5, 12, 16, 17, 23,
26, 27, 30, 32, 35]. However, the majority of recent replacement
policy proposals are evaluated in the absence of hardware prefetching. In the presence of prefetching, we find that intelligent replacement policies may provide minimal performance improvements or
unexpectedly degrade performance. This stems from the fact that
the majority of replacement policy proposals make the same replacement policy decisions for prefetch and demand requests.
This paper proposes Prefetch-Aware Cache Management (PACMan). The goals of PACMan are (1) avoid cache pollution due to
harmful prefetch requests and (2) retain cache lines that cannot be
easily prefetched, and hence are more valuable in the cache. PACMan accomplishes both goals by recognizing that prefetch and demand requests can be treated differently on cache hits and cache insertions. In doing so, we show that PACMan reduces prefetcher induced cache pollution and combines the performance benefits from
hardware prefetching and intelligent cache management.
We apply PACMan to the recent Dynamic Re-Reference Interval Prediction (DRRIP) [12] replacement policy. We show that the
additional information on the type of request (demand or prefetch)
can improve the re-reference predictions made by DRRIP. In the
presence of hardware prefetching, PACMan significantly improves
application performance by an average of 21.9% while DRRIP alone
improves performance by only 5.8%. For server workloads, where
prefetcher pollution is particularly problematic, PACMan improves
performance by an average of 27.5% over LRU.
Furthermore, our evaluations with 4-core multi-programmed work-

Categories and Subject Descriptors
B.8.3 [Hardware]: Memory Structures

General Terms
Design, Performance

Keywords
Prefetch-Aware Replacement, Reuse Distance Prediction, Shared
Cache, Set Dueling
§A large part of this work was performed while Carole-Jean Wu
was an intern at Intel/VSSAD.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
MICRO ’11 December 3-7, 2011, Porto Alegre, Brazil
Copyright 2011 ACM 978-1-4503-1053-6/11/12 ...$10.00.

442

Figure 1: Hardware prefetching significantly improves application performance. See Section 5 for methodology details.

Figure 2: Performance of individual applications under cache management policies without prefetching: Cache management
schemes, DRRIP, Seg-LRU, and SDBP, effectively improve performance in the absence of prefetching.
stream prefetcher that closely models the mid-level cache (MLC)
stream prefetcher of an Intel Core i7 [8]. In addition, we implement three recent replacement policy proposals: DRRIP [12],
Segmented-LRU (Seg-LRU) [5], and Sampling Deadblock Prediction (SDBP) [16] and evaluate their performance in the presence of
hardware prefetching.

loads show that PACMan effectively eliminates inter-core, as well
as intra-core, prefetch-induced interference in shared LLCs. In
the presence of hardware prefetching, PACMan improves multiprogrammed workload performance by an average of 21.5% while
DRRIP alone improves performance by 7.8% over LRU.
Overall, the contributions of the paper are as follows:
• This paper provides a comprehensive performance evaluation of state-of-the-art cache management policies in the presence and absence of hardware prefetching. While an intelligent cache management technique can improve application
memory performance, prefetching can increase performance
even more by fetching useful data in advance to the cache.
However, we show that prefetching alone, without the help of
a prefetch-aware cache management technique, can interact
poorly with LLC management, often causing performance
degradation and variability.
• In addition to improving performance significantly, PACMan
effectively eliminates performance variability introduced by
prefetching. PACMan’s ability to provide quality of service
is particularly important for server applications, for which
performance guarantee is a high priority.
• Finally, PACMan insights presented in this paper are independent from the underlying replacement policy. PACMan
can be built on top of any ordered replacement policy. PACMan’s simple and elegant prefetch-aware decision offers significant performance improvements without incurring additional overhead.

2.

2.1 Performance Variability under Various
Cache Replacement with Prefetching
Figure 1 illustrates a summary of application performance (for
each workload category) in the presence of hardware prefetching.
Overall, the figure shows that prefetching significantly improves
application performance by roughly 35%. However, when we look
at the performance of individual applications from each workload
category more closely, we find that the performance of prefetching varies significantly across the different cache management policies. Figures 2 and 3 highlight this performance variability in the
absence and presence of prefetching respectively. In both figures,
the x-axis represents the different workloads under study while the
y-axis represents the performance relative to the baseline LRU replacement policy without prefetching.
Figure 2 shows that, in the absence of prefetching, intelligent
cache management schemes can improve application performance
for a variety of applications. However, in the presence of prefetching, we observe very different behavior. For example, finalfantasy receives 2-8% performance improvement in the absence
of prefetching. However, as Figure 3 shows, in the presence of
prefetching, its performance varies from -12% to 8% across the
four different underlying cache replacement policies.
In addition to performance variability across cache replacement
policies, prefetching can severely degrade performance. This is
particularly true for server applications where performance degrades
by more than 20% (e.g. tpc-c). For some workloads (e.g. IB and

Motivation

To understand the effects of hardware prefetching thoroughly,
we evaluate replacement policies in the presence and absence of
hardware prefetching. For the purpose of this paper, we model a

443

0//8
538
3/8
138
/8

+,#

"&"

# (2

$



!%$%

* 1

'&#

$ -



	









'"




1


&"#"&$

-$#)


	


Figure 3: Performance of individual applications under cache management policies in the presence of prefetching: Prefetcher cache
interference not only can cause performance degradation but also introduces performance variability. Note: Y-axes use different
scales.

#)!%#$#

1
4

Figure 4: Reuse of prefetched cache lines at the L2 and last-level L3 caches. This is in part due to prefetcher LLC pollution on
inaccurate prefetches, but also due to the temporal filtering of effective prefetches by references in smaller caches.
(e.g. homeworld and SPEC CPU2006 workloads).
In conclusion, the appearance of zero-reuse prefetched cache
lines at the LLC and the reduced gains from intelligent replacement
policies in the presence of hardware prefetching motivate alternative policies for handling prefetch requests at the LLC. The next
section proposes such a policy.

tpc-c), intelligent cache replacement policies can address some
of the performance degradation caused by prefetcher pollution.
Most importantly, Figure 3 illustrates that the performance improvements from intelligent replacement policies drop significantly
in the presence of prefetching. For example, in the absence of
prefetching, gemsFDTD and sphinx3 receive roughly 7% and
24% performance improvements respectively from an intelligent
replacement policy. However, in the presence of prefetching, intelligent replacement yields negligible performance benefits for
gemsFDTD while the benefits for sphinx3 reduce to roughly
15% over the baseline LRU. This raises the question as to whether
there exists a prefetch-aware cache management policy that can further improve performance of such applications in the presence of
prefetching.

3. Prefetch-Aware Cache Management

2.2 Low LLC Reuse of Prefetch Requests
To further motivate the need for a prefetch-aware cache management policy, Figure 4 illustrates the reuse characteristics of lines
prefetched into the LLC. The x-axis represents the different workloads while the y-axis represents the fraction of cache lines inserted
by a prefetcher that are re-referenced by a demand request. On average, 47% of prefetched cache lines are never reused by a demand
request.
Low reuse of prefetched cache lines at the LLC can be due to
prefetcher pollution. Such behavior is indicative of workloads whose
performance degrades in the presence of prefetching (e.g. tpc-c).
Alternatively, low reuse also occurs because of timely prefetched
data in the smaller, upper-levels of the cache hierarchy (L2 cache
in our case). Subsequent demand requests are directly serviced by
the prefetched cache lines inserted into the L2 cache, and never
reach the LLC. As a result, the filtering of temporal locality by the
smaller caches causes low reuse of the prefetched lines in the LLC

444

Despite the ubiquitous industry use of hardware prefetching, existing cache management policies do not distinguish between prefetch and demand requests. This leaves a significant opportunity
to improve cache management by making Prefetch-Aware Cache
Management (PACMan) decisions.
From a cache management perspective, prefetch requests have
different properties than a demand request. In general, cache lines
inserted into the LLC by demand requests are more likely to be
performance-critical than prefetch requests. Thus, a replacement
policy may benefit by giving more preference to items fetched by
demand requests versus those from prefetch requests. This work
explores different replacement policies for prefetch requests.
Cache replacement policies essentially predict the re-reference
behavior of cache lines [12]. The natural opportunity to make rereference predictions is on cache insertions and cache hits. For example, LRU predicts that a missing cache line will be re-referenced
soon and always inserts the missing line at the “MRU position”
of the LRU chain. Similarly, LRU predicts that a line receiving a
cache hit will also be re-referenced soon and moves the line to the
MRU position.
Traditionally, the majority of replacement policies assign the same
re-reference predictions for demand and prefetch requests. To tackle
the problem of prefetch-induced cache interference, we propose to
make re-reference predictions at the granularity of a request type

Figure 5: PACMan RRPV Assignment under DRRIP. We use the BRRIP configuration in [12] for all PACMan schemes under
DRRIP: 95% of cache lines are inserted with an RRPV of 3, and 5% are inserted with an RRPV of 2.

4.1 DRRIP Background

(i.e. demand or prefetch). Specifically, we investigate replacement
policies that apply the baseline re-reference predictions for demand
requests, but apply modified re-referenced predictions for prefetch
requests. We propose four types of prefetch-aware cache management policies:
• PACMan on Misses (PACMan-M): PACMan-M modifies the
re-reference prediction only when a prefetch request misses
in the cache. In particular, PACMan-M predicts that all
prefetch requests will be re-referenced in the distant future.
For example, like previous LRU-based studies, PACMan-M
can be implemented by inserting all prefetch requests at the
LRU position [18, 31]. In doing so, PACMan-M reduces the
average in-cache lifetime of a prefetched cache line, in effect prioritizing demand requests over prefetch requests. In
the event the line is subsequently re-referenced, the replacement state is updated on the cache hit. However, if there
is no subsequent request, PACMan-M is useful for avoiding
prefetcher pollution.
• PACMan on Hits (PACMan-H): PACMan-H modifies the rereference prediction only when a prefetch request hits in the
cache. Unlike a conventional replacement policy that always
updates re-reference predictions on cache hits, PACMan-H
chooses not to update the re-reference prediction on prefetch
cache hits. In doing so, PACMan-H treats “prefetchable” requests with lower priority. In doing so, PACMan-H enables
useful, but hard to prefetch, demand requests to be retained
in the cache.
• PACMan on Hits and Misses (PACMan-HM): PACMan-HM
modifies the re-reference predictions for prefetch request cache
hits and misses. PACMan-HM gives shorter lifetime to all
prefetch requests by using the PACMan-M policy on prefetch
misses and the PACMan-H policy on prefetch hits. PACManHM is useful for avoiding prefetcher pollution and retaining
hard to prefetch lines.
• Dynamic PACMan (PACMan-DYN): Statically predicting
the re-reference pattern of prefetch requests can significantly
degrade performance of workloads that benefit from prefetching. To ensure robust prefetch-aware replacement, we propose to dynamically detect the re-reference behavior of
prefetch requests. PACMan-DYN uses Set Dueling [27] to
dynamically predict the re-reference behavior of prefetch requests.

4.

PACMan Implementation

Instead of using a 4-bit LRU counter per cache line for a 16-way
set-associative cache, DRRIP uses an M-bit counter per cache line
to store the lines re-reference prediction value (RRPV). The RRPV
in essence controls the lifetime of a cache line. In general, large
RRPVs cause insertion closer to the “LRU position” and, therefore, have a short lifetime in the cache. In contrast, small RRPVs
comparatively have a longer lifetime in the cache.
We base PACMan on 2-bit DRRIP, where each cache line can
have RRPV between 0 and 3. DRRIP uses Set Dueling [28] to
choose between its two component policies: SRRIP and BRRIP.
SRRIP always inserts cache lines with an RRPV of 2 while BRRIP
inserts most of cache lines with an RRPV of 3. On hits, DRRIP
updates the RRPV of a cache line to 0. On replacements, DRRIP
chooses a cache line with RRPV of 3 for replacement. If no block
with RRPV of 3 is found, all RRPVs in the set are incremented and
the search for a replacement block is repeated.

4.2 Applying PACMan to DRRIP
As summarized in Figure 5, PACMan’s RRPV assignment is
based on the DRRIP policy as follows:
PACMan-M: PACMan-M modifies DRRIP’s insertion policy on
cache misses. If a miss is due to a demand request, PACMan-M
follows the baseline DRRIP insertion policy and inserts these cache
lines with an RRPV of 2 (or 3). However, if the cache miss is due
to a prefetch request, PACMan-M always inserts these lines with
an RRPV of 3.
PACMan-H: PACMan-H modifies DRRIP’s policy on cache hits.
If the hit is due to a demand request, PACMan-H follows the baseline DRRIP policy and updates the line’s RRPV to 0. However,
if the cache hit is due to a prefetch request, PACMan-H does not
modify the line’s RRPV state. In doing so, PACMan-H explicitly
prioritizes retaining more “valuable” (harder to prefetch) blocks instead of “prefetchable” blocks in the cache1 .
PACMan-HM: PACMan uses the PACMan-M policy at cache
misses and follows the PACMan-H policy at cache hits.

4.3 PACMan-DYN
To differentiate whether a cache line is brought into the LLC by
a demand versus prefetch request, prior work [1] proposed using
an additional prefetch bit per cache block. To avoid this overhead,
PACMan-DYN instead uses Set Dueling Monitors (SDMs) to determine which applications benefit from prefetching. An SDM estimates the number of cache misses for any given policy by permanently dedicating a few cache sets to follow that policy. The rest of
1
DRRIP [12] assigns re-reference predictions based on the expected re-reference pattern. PACMan-H, however, assigns rereference predictions based on the expected value of the cache line.
For example, it is more “valuable” to retain cache lines that are
harder to prefetch.

PACMan can be applied to any existing state-of-the-art cache
management policy. We implement PACMan upon the recently
proposed Dynamic Re-Reference Interval Prediction (DRRIP) replacement policy [12] because of its simple design and low implementation complexity.

445

Existing cache module

SDM
SRRIP+PACMan-H

SDM
SRRIP+PACMan-HM

index

SDM
BRRIP+PACMan-H

cntSRRIP+PACMan-H
cntSRRIP+PACMan-HM

MIN

cntBRRIP+PACMan-H

Follower sets
…

Figure 7: PACMan-DYN Implementation.

5. Experimental Methodology

Figure 6: PACMan-DYN Algorithm.

5.1 Simulation Infrastructure
the cache sets then follow the policy whose SDM gives the fewest
cache misses.
A brute force approach to use Set Dueling for determining the
best PACMan prefetching and cache replacement combination requires 8 SDMs. As you will see in the result section, PACMan-H
always performs better and provide more consistent gains than the
underlying DRRIP replacement policy. As a result, we switch our
baseline to PACMan-H and use Set Dueling to determine dynamically whether to use the PACMan-M component.
PACMan-DYN implements 3 SDMs: SDMSRRIP +P ACM anH ,
SDMSRRIP +P ACM anHM , and SDMBRRIP +P ACM anH over the
2-bit implementation of DRRIP. SDMSRRIP +P ACM anH follows
the cache insertion and update policy based on SRRIP and PACManH, SDMSRRIP +P ACM anHM follows the cache insertion and update policy based on SRRIP and PACMan-HM, and
SDMBRRIP +P ACM anH follows the cache insertion and update
policy based on BRRIP and PACMan-H. PACMan-DYN does not
implement the additional SDMBRRIP +P ACM anHM policy because,
for cache misses, most references, regardless of prefetch or demand, are inserted with an RRPV of 3. In other words, the
SDMBRRIP +P ACM anHM and SDMBRRIP +P ACM anH policies
are essentially identical.
The PACMan-DYN algorithm is described in Figure 6 and its
implementation is presented in Figure 7. Each SDM has an associated counter. When a demand cache miss2 occurs at a set within a
particular SDM, the associated counter is incremented by 2 and the
counters associated with the other two SDMs are decremented by 1.
If any counter is saturated, all counters remain the same. This policy selection process is based on a scheme proposed in [21] where
the goal is to choose the best out of 3 different policies. For cache
misses occurring at follower sets, PACMan-DYN finds the minimum of the three SDM counters and uses the corresponding policy
for inserting and updating the references.
2

Prefetch and writeback misses do not update the counters.

446

We evaluate PACMan using the the simulation framework released by the First JILP Workshop on Computer Architecture Competitions [13]. This Pin-based [20] CMP$im [10] simulation framework models a 4-way out-of-order processor with a 128-entry reorder buffer and a three-level cache hierarchy. The three-level cache
hierarchy is based on an Intel Core i7 system [8]. The L1 and L2
caches use LRU replacement and our replacement policy studies
are limited to the LLC. We model a per-core streamer prefetcher
with 16 stream detectors that trains on L2 cache misses. The prefetcher issues prefetch requests from the L2 cache and inserts prefetched cache lines into the L2 and LLC.
Table 1: Architectural parameters of the simulated system.
MSHR: 32 entries allowing up to 32 outstanding misses
L1 Inst. Caches: 32KB, 4-way, Private, 1 cycle
L1 Data Caches: 32KB, 8-way, Private, 1 cycle
Mid-Level L2 Caches: 256KB, 8-way, Private, 10 cycles
LLC: 1MB per-core, 16-way, Shared, Non-inclusive, 30 cycles
Main Memory: 32 outstanding requests, 200 cycles
We also model an interconnect with a fixed average latency. Traffic entering the interconnect is modeled using a fixed number of
miss status handling registers (MSHRs). Modeling MSHR contention reflects the latency penalty of excess traffic in the system.
All transactions, including all data demand and prefetch requests,
contend for the MSHRs. Table 1 lists the architectural parameters
for the memory hierarchy and Table 2 summarizes the configurations used for the comparison replacement policy proposals.

5.2 Workload Construction
We perform our evaluation for state-of-the-art cache replacement
schemes and PACMan using both sequential and multiprogrammed
workloads. For single-program studies, we use a collection of applications from multimedia and PC games (Mm.), enterprise server
(Srvr.), and SPEC CPU2006 (Spec) [29] applications. While these
applications are quite diverse, most of our performance analysis focuses on 18 applications (which span the three categories) that are

Table 2: Description and configuration for the three state-of-the-art cache management techniques under study: DRRIP, Seg-LRU,
and SDBP. The configuration is selected to represent the best performance in each scheme.
Policies
DRRIP
Seg-LRU

SDBP

Description and Parameters
A cache insertion/replacement scheme using re-reference interval prediction to assign lifetime to cache blocks [12].
10-bit policy selector; 32 sets per set-dueling monitor; 2-bit RRIP counters.
A cache replacement and bypassing scheme based on a segmented-LRU cache [5].
Adaptive bypassing enabled for Seg-LRU and LRU; Aging enabled for Seg-LRU; Initial bypassing probability is 64;
Second minimum bypassing probability is 1/4096.
A cache replacement and bypassing scheme based on instruction-aware deadblock prediction [16].
32-set 12-way sampler; Three 4096-entry skew table; True 16-way LRU policy.
nally, e.g. gemsFDTD, experience performance degradation under
PACMan-HM. PACMan-DYN resolves this negative performance
impact. This is because PACMan-DYN eliminates performance
degradation for applications that actually benefit from aggressive
prefetching by giving beneficial prefetch requests higher priority.
Finally, while PACMan-HM is effective, PACMan-DYN improves
performance even more for applications like sphinx3 because of
its dynamic adjustment. Most importantly, PACMan-DYN consistently improves performance over the baseline for all applications.

cache intensive. We conclude with a performance summary that
includes all 65 applications.
The SPEC CPU2006 reference traces were collected using PinPoints [24] for the reference input set, while other application traces
were collected on a hardware tracing platform. The hardwaretraced workloads include both operating system and user-level activity while the SPEC CPU2006 workloads only include user-level
activity. These workloads were run for 250 million instructions.
To evaluate cache management techniques for the shared LLC,
we also construct 161 heterogeneous mixes of multiprogrammed
workloads. First, we use 35 mixes of Mm. applications, 35 mixes
of Srvr. applications, and 35 mixes of Spec programs. Finally, we
create another 56 combinations of 4-core workloads comprising
randomly-chosen applications that are mixed from different categories. The heterogeneous mixes of different workload categories
are used as a proxy for a virtualized system. We run each application for 250 million instructions and collect the statistics with
the first 250 million instructions completed. If the end of the trace
is reached, the model rewinds the trace and restarts automatically.
This simulation methodology is similar to recent work on shared
caches [3, 11, 12, 19, 35].

6.

6.2 Results for Sequential Workloads
Figure 9 compares the overall performance results for sequential applications. While DRRIP under prefetching improves performance by an average of 5.8% over the baseline LRU, PACManDYN improves performance by 21.9%. PACMan’s prefetch-aware
management is particularly important for server applications. It further improves server workload performance by an average of 27.5%
over LRU. Looking more closely at SPEC CPU2006 applications,
although application performance is less sensitive to replacement
policies in the presence of prefetching, we find that PACMan-DYN
still further improves SPEC CPU2006 workload performance. While
DRRIP gives 1% performance benefit compared to LRU in the
presence of prefetching, PACMan-DYN improves performance for
SPEC CPU2006 applications more significantly by an average of
14.3%. Overall, PACMan-DYN demonstrates an effective cache
management by differentiating prefetch and demand requests at
cache hits and cache misses.

Performance Evaluation

6.1 Overview Results for PACMan
Figure 8 compares the sequential application performance results for PACMan-M, PACMan-H, PACMan-HM, and PACManDYN with the baseline LRU scheme (LRU) and DRRIP. In general, PACMan-M or PACMan-H alone improves performance significantly over both LRU and DRRIP under prefetching across all
types of workloads. PACMan-HM, combining the performance
benefits from PACMan-M and PACMan-H, shows further performance improvement, except for bwaves and gemsFDTD. This is
because PACMan-HM actively de-prioritizes prefetch requests over
demand requests at cache hits and misses. As a result, applications
which do not suffer from prefetch-induced cache pollution origi-

6.3 Performance Consistency
We categorize the sequential applications into three distinct classes, as summarized in Table 3. Class I applications represent those
experiencing prefetch-induced cache interference and, as a result,
seeing prefetch-induced performance degradation. Class II applications represent those that receive performance gains under intelligent cache management schemes [33] and can also benefit from
prefetching. Finally, Class III applications represent those that are

Figure 8: Performance comparison for PACMan-M, PACMan-H, PACMan-HM, and PACMan-DYN.

447

and by as much as 1.67X for tpc-c.
For Class II applications, which not only are sensitive to the underlying cache management schemes but also receive benefit from
prefetching, we show that both PACMan-HM and PACMan-DYN
can further improve the performance significantly and consistently
for all applications in this category. In particular, among all Class
II applications, PACMan-DYN improves sphinx3’s performance
the most: 1.36X over the baseline LRU scheme under the influence
of prefetching.
Finally, for Class III applications, which neither suffer from
prefetch-induced interference nor receive performance benefit with
larger cache sizes, PACMan-DYN can sustain the performance improvement from prefetching by accurately identifying beneficial
prefetching.
On average, in the presence of prefetching, PACMan-DYN improves application performance consistently by an average of 21.9%
over the baseline LRU scheme. More importantly, PACMan-DYN
is able to provide performance predictability in the presence of
hardware prefetching. This performance guarantee is particularly
important for server applications, for which performance quality of
service is an important requirement.

Figure 9: Performance comparison for sequential workloads.

not cache sensitive but benefit from prefetching. Typically, streaming and compute-bound applications fit into Class III.
Overall, PACMan-DYN consistently and effectively improves
performance for applications in a diverse range of workloads. Figure 10 shows the consistent performance improvement under
PACMan-HM (except for bwaves and gemsFDTD) and PACManDYN for all multimedia, games, server, and SPEC CPU2006 applications. This is because PACMan-DYN can exploit the distinct
access characteristics of prefetch versus demand references and dynamically determine better cache insertion and cache update policies based on its observation. As a result, PACMan-DYN can manage the degree of prefetch-induced LLC interference more effectively.
PACMan-DYN’s ability to eliminate performance variability
caused by prefetch-induced cache interference is particularly important for applications in Class I, which experience the most significant cache pollution among the three categories. While DRRIP
under prefetching improves the performance of Class I applications
by an average of 1.13X over the baseline LRU scheme, PACManDYN improves the performance even more by an average of 1.44X

6.4 PACMan for Multiprogrammed Workloads
Prefetch-induced cache interference becomes more complicated
and severe for multiprogrammed workloads. Applications not only
experience intra-application cache interference from their own
prefetch requests, but also cross-application prefetch-prefetch and
prefetch-demand interference. As a result, the degree of cache contention is also higher than that observed in sequential workloads.
To implement PACMan-DYN for multiprogrammed workloads,
we investigate two approaches: PACMan-DYN-Local and
PACMan-DYN-Global. PACMan-DYN-Local duplicates the group
of SDMs directly from PACMan-DYN for sequential applications.
Similar to the adaptive insertion policy [11], each application has
its own SDMs for the three policies described in Section 4.3. The
follower sets then use the best of the three policies based on the
counters associated with each application. Based on each indi-

Table 3: Applications selected to represent 3 distinct categories under prefetching influence.
Class I: Harmful Prefetching
final-fantasy (Mm.), halflife2 (Mm.), SA (Srvr.), IB (Srvr.), SW (Srvr.), tpc-c (Srvr.)
Class II: Beneficial Prefetching, Cache Sensitive
doom3 (Mm.), NOLF (Mm.), homeworld (Mm.), PP (Mm.), MS (Srvr.); bzip2 (Spec), omnetpp (Spec), sphinx3 (Spec)
Class III: Beneficial Prefetching, Cache Insensitive
GG (Srvr.), bwaves (Spec), gemsFDTD (Spec), libquantum (Spec)

Figure 10: Performance comparison for applications from 3 different classes (See Table 3). Both PACMan and PACMan-DYN can
effectively and consistently improve application performance when compared to other state-of-the-art LLC management schemes.

448

	



	


)%&"!#' 

"	



"	


"&
"


"	


"&
"


"	


"&
"


"	


")

)%&"!#' 





"	


"	


"	


"&
"




"	


"&
"


"	


"&
"


"



"


"&
"

	
"


"


"&
"


"


"&
"

	
"

"&
"

	
"

"&
"

	
"

"&
"






"


	

	
"
	

"






# #($%&%

")

	

"














	



"

# #($%&%

Figure 11: PACMan-DYN implementation for the 4-core workloads. (a) PACMan-DYN-Local: A group of 3 SDMs is used to
determine the best per-application-basis PACMan policy. For example, the three top-most SDMs are dedicated to App0. References
of App0 follow the PACMan policy pre-assigned to each of its SDMs. References from the other three applications follow the policies
determined by the per-core policy selection counters: P1 , P2 , or P3 . (b) PACMan-DYN-Global: A group of 2 SDMs is dedicated to
each application. Similar to how the default DRRIP works, application i follows the policy determined by the per-application policy
counter, Pi . In addition, two SDMs are used to determine which of the PACMan policies (PACMan-H or PACMan-HM) the follower
sets use.
vidual application’s prefetching preference, PACMan-DYN-Local
follows the best local, per-application basis policy. Figure 11(a)
depicts PACMan-DYN-Local’s implementation.
To explore a more global prefetch decision for the shared LLC,
we also propose PACMan-DYN-Global. PACMan-DYN-Global
chooses a uniform global prefetch cache insertion and update decision for all per-core prefetchers. In PACMan-DYN-Global, each
application in the multiprogrammed workload has two SDMs that
follow the baseline DRRIP policies: SRRIP and BRRIP. To determine whether prefetching is beneficial globally to the LLC, PACManDYN-Global implements two additional SDMs: SDMP ACM anH
and SDMP ACM anHM . SDMP ACM anH follows the better of the
two policies for each application together with the PACMan-H policy. Similarly, SDMP ACM anHM follows the better policy for each
application along with the PACMan-HM policy for prefetch requests. Note, as Figure 11(b) shows, since each application in
PACMan-DYN-Global uses only two SDMs, only one saturating
counter (instead of three for PACMan-DYN-Local) is required for
each application. Furthermore, PACMan-DYN-Global requires
fewer total SDMs than in PACMan-DYN-Local.

6.5 Results for Multiprogrammed Workloads
Figure 12 shows that, in the presence of prefetching, DRRIP
can improve performance by an average of 7.8% over the baseline LRU already. This is because although DRRIP does not directly distinguish between demand and prefetch requests, its in-

trinsic scan- and thrash-resistant properties can identify aggressive,
but low-reuse, prefetch requests and reduce interfering prefetch requests in the LLC. However, PACMan-HM with explicit prefetchaware cache management improves performance even more: an average of 22.9% for multiprogrammed workloads. For the multiprogrammed workloads studied in this paper, PACMan-DYN-Local
and PACMan-DYN-Global did not perform better than PACManHM because of the SDM learning overhead; however, PACManDYN-Local and PACMan-DYN-Global are still more robust when
prefetching preference varies over time in the workload. Nonetheless, for all 161 multiprogrammed workloads, both PACMan-DYNLocal and PACMan-DYN-Global still offer significant performance
improvements over LRU by an average of 20.9% and 21.5% respectively. Since PACMan-DYN-Global requires less SDM learning overhead while offering slightly better performance gain over
PACMan-DYN-Local, it is preferable to use PACMan-DYN-Global
for the shared LLC. For the rest of the paper, for the shared LLC,
we will refer PACMan-DYN-Global as PACMan-DYN.
If we focus on the performance improvement for the multiprogrammed mixes of SPEC CPU2006 applications, we do not see
much performance difference across the different state-of-the-art
cache management policies when considering prefetching. This is
similar to what we observe for sequential SPEC CPU2006 applications. However, both PACMan-HM and PACMan-DYN demonstrate their effective management by using prefetch-aware cache
insertion and cache hit policies. As a result, PACMan-HM and

Figure 12: Performance comparison for multiprogrammed workloads.

449

Figure 13: PACMan’s impact on memory bandwidth.

PACMan-DYN improve the performance of multiprogrammed SPEC
CPU2006 workloads by an average of 12.3% and 12.0% over the
baseline LRU under prefetching. Finally, similar to sequential server
workloads, PACMan-DYN’s prefetch-aware cache management decisions improve multiprogrammed server workloads the most: an
average of 47.8% (46.1% for PACMan-DYN-Local). This is a considerable improvement over the 20.8% offered by DRRIP.

6.6 Impact on Memory Bandwidth
Prefetching can adversely affect the bandwidth requirements to
memory. This section evaluates the influence of PACMan’s cache
management decision on memory bandwidth. Figure 13 compares
PACMan’s impact on bandwidth between the LLC and the main
memory. The upper portion of the figure plots the number of prefetch
requests to memory per instruction, normalized to the baseline LRU
scheme. The lower portion of the figure plots the number of data
demand requests to memory per instruction normalized to the baseline LRU scheme under prefetching. On average, PACMan-HM
increases the bandwidth usage from hardware prefetchers by 22%
while PACMan-DYN incurs less memory bandwidth overhead (by
an average of 20%). This increase in memory bandwidth due to
prefetching is because PACMan-HM does not cache many prefetched
cache lines in the LLC that are considered harmful. This results in
more memory traffic for prefetched requests.
When comparing PACMan’s impact on data demand memory
bandwidth usage, we see a significant reduction for all PACMan
schemes. This is because PACMan can significantly decrease the
number of demand cache misses at the LLC, and, as a result, improve the data demand memory bandwidth between the LLC and
the main memory. For the selected applications, while DRRIP under prefetching can reduce demand memory bandwidth by an average of 7.6%, PACMan-HM and PACMan-DYN improve demand
memory bandwidth more significantly by an average of 29.5% and
31.3% respectively.
In particular, for applications that do not suffer much from prefetchinduced cache pollution i.e. gemsFDTD, PACMan-DYN’s dynamic
detection and adjustment for cache insertion and update policies
help to minimize the memory bandwidth consumption. While
PACMan-HM increases gemsFDTD’s demand memory bandwidth
by 12.6%, PACMan-DYN can reduce this significant bandwidth
overhead to merely 2%. Finally, we note that PACMan-DYN is

450

Figure 14: PACMan-DYN sensitivity to cache sizes for (a) sequential and (b) multiprogrammed workloads.

essentially a lightweight dynamic method for detecting prefetchinduced cache pollution. As such, we foresee future methods where
PACMan-DYN is used to modulate prefetcher aggressiveness to
further reduce memory bandwidth requirement caused by today’s
aggressive hardware prefetchers.

6.7 PACMan-DYN Sensitivity to Cache Sizes
and Various Cache Hierarchies
To analyze the effectiveness of PACMan under various cache
sizes, we perform a cache size sensitivity study for PACMan-DYN.
Figure 14(a) shows that for sequential workloads, PACMan-DYN
consistently outperforms DRRIP. The performance benefit levels
off for an 8MB LLC. This is because an 8MB cache is adequate to
accommodate both prefetch and demand references. For multiprogrammed workloads, the LLC experiences higher cache contention.
Figure 14(b) illustrates that, for cache sizes ranging from 2MB to
16MB, PACMan consistently outperforms LRU and DRRIP for all
shared cache sizes.
Although this work evaluates the various PACMan insertion and
promotion policies for non-inclusive cache hierarchy, the core idea
of the prefetch-aware cache management applies to strictly inclusive [9] and exclusive [15] cache hierarchies as well. For the inclusive cache hierarchy, we apply exactly the same PACMan-DYN
insertion and hit promotion policies. For the exclusive cache hierarchy, we retain the same capacity in the three-level hierarchy by

tiprogrammed workloads by an average of 9.2% and 21.5% respectively over the baseline LRU scheme while DRRIP improves performance by 2.9% and 5.8%. This work shows the need of cache
management schemes to recognize the unique access behavior for
demand and prefetch requests in future design. Finally, while we
build PACMan-DYN upon DRRIP, our insights to the interaction
between demand and prefetch requests are not limited to a DRRIPbased cache; the same ideas hold regardless of underlying cache
management policies.

doubling the L2 cache size and halving the LLC size. Since exclusion requires lines evicted from the L2 cache to be filled into the
LLC, to implement PACMan-DYN, the L2 cache must remember
which L2 cache lines were brought in by prefetch requests. This
can be accomplished by adding a prefetch bit per line in the L2
cache to remember whether the L2 cache line was brought in by
a demand or a prefetch request. To distinguish between demand
and prefetched lines, this prefetch bit at the L2 cache is used by
PACMan-DYN to determine the LLC insertion policy. Across all
workloads, our evaluation of PACMan-DYN for the strictly inclusive and exclusive cache hierarchies see as much as 28% performance improvement (and 5% on average) over LRU: this doubles
the performance gain over DRRIP.

7. Related Work
7.1 Dynamic Prefetching Control

6.8 Hardware Overhead
Table 4 compares hardware overhead and performance for several prior state-of-the-art schemes, along with PACMan-HM and
PACMan-DYN. We show performance for the 18 sequential applications considered thus far, and also for the full 65-application
suite from which they were drawn. PACMan-DYN uses the same
amount of hardware as DRRIP but, on average, it offers 16.2%
more performance improvement for the selected sequential applications and 13.8% more for multiprogrammed workloads. Seg-LRU
offers the best performance of the prior schemes, but uses much
more hardware implementation overhead. In contrast, PACManDYN improves performance even more than Seg-LRU—an additional 10.6%. Also importantly, PACMan-DYN offers a much simpler implementation than Seg-LRU, with little hardware beyond its
underlying replacement policy. Finally, we note that while the performance improvements for the full 65-application suite (including non-cache-intensive applications) are lower than those for the
18 cache-intensive applications, PACMan-DYN’s performance improvements are still considerable. They still greatly exceed that
of the other possible approaches. Overall, PACMan-DYN offers a
simple and practical cache design for managing LLC prefetching
influence.

6.9 Results Summary
To summarize, we provide the first characterization for prefetchinduced LLC interference within an application, as well as across
applications, under state-of-the-art cache capacity management techniques. We illustrate that, in the presence of hardware prefetching, performance benefits using prior proposals become more varied. Some applications experience significant performance degradation. To tackle this problem, we propose a simple prefetchaware cache management scheme, PACMan-DYN. PACMan-DYN
offers a more consistent performance improvement across applications for a diverse range of workloads: multimedia, games, server,
and SPEC CPU2006. In the presence of prefetching, PACManDYN improves the performance of the 65 sequential and 161 mul-

Prefetching takes advantage of available memory bandwidth to
reduce memory latencies. However, the benefits of aggressive
prefetching heavily rely on its accuracy. Other prior work [1, 4, 6,
7, 14, 18, 22, 25, 31, 34, 36] improves prefetching accuracy and
reduces cache pollution caused by prefetch requests. All of these
work mainly focus on designs that reduce per-core interference between prefetch and demand requests for different cores in CMP
systems.
Lin, Reinhardt, and Burger [18] proposed a mechanism to reduce
DRAM latencies by modulating prefetcher issue rate, maximizing
DRAM low hit rate, and always inserting prefetch requests with the
lowest priority in the LLC. Their proposed mechanism is static and
always gives prefetch references lower priority by inserting them in
the LRU position. Our paper, on the other hand, introduces several
policy variations including a dynamic prefetch-aware cache management solution.
Alameldeen and Wood [1] proposed to use an additional prefetch
bit per cache block to estimate the degree of prefetch-induced cache
pollution and adjust the aggressiveness of hardware prefetchers accordingly. Our cache pollution estimation scheme proposed here
accurately approximates when prefetching harms performance, but
at negligible hardware overhead.
Srinath et al. [31] built a feedback-directed prefetching mechanism which uses an additional prefetch bit per cache block to help
identify prefetcher accuracy. Furthermore, additional hardware is
required to help identify the degree of prefetch-induced cache pollution. Again, the light-weight cache pollution estimation mechanism proposed in PACMan can be used in [31] and the insights
discovered in PACMan can help further improve the feedback directed prefetching scheme’s decision on cache fill and hit policies
for demand and prefetch cache blocks.
Wu and Martonosi [34] characterized real-system cache pollution caused by hardware prefetchers and proposed a dynamic
prefetcher manager that modulates the aggressiveness of prefetchers at runtime. Their real-system prefetch manager implementation offers an orthogonal and coarser-grained approach to mitigate
prefetch-induced cache pollution without hardware changes.

Table 4: Hardware overhead and performance comparison for prior state-of-the-art schemes, PACMan-HM, and PACMan-DYN.
Policies
%IPC vs. LRU (18 rep. apps) %IPC vs. LRU (65 apps)
Hardware Overhead**
LRU
33.0
29.3
n ∗ log2 n
DRRIP
38.8
32.2
2n
Seg-LRU***
44.4
33.6
5n + 5.68KB
SDBP***
40.4
32.6
5n + 13.75KB
PACMan-HM
53.6
38.0
2n
PACMan-DYN
55.0
38.5
2n
**Assuming an n-way set-associative cache, HW overhead is measured in bits required per cache set.
***Hardware overhead for Seg-LRU and SDBP scales up as the number of cores sharing the LLC increases.

451

16-way 1MB LLC
8KB
4KB
15.68KB
23.75KB
4KB
4KB

workload performance is significantly improved by an average of
27.9% over the baseline LRU scheme. Moreover, PACMan can
further improve the performance of SPEC CPU2006 and other applications which benefit hardware prefetching originally. On average, while the best of the state-of-the-art techniques improves the
performance of the diverse range of applications by an average of
11.4% over LRU, PACMan can improve performance by an average
of 21.9% over LRU and by 16.2% over DRRIP under prefetching.
Furthermore, we evaluate PACMan with multiprogrammed workloads, for which prefetch-induced interference is a more complicated problem. Workloads not only experience intra-core demandprefetch interference, but aggressive per-core prefetchers worsens
the problem by introducing inter-core prefetch-prefetch and prefetchdemand interference. We show PACMan can eliminate intra-core,
as well as inter-core, prefetch-induced interference for shared LLCs.
As a result, the performance of multiprogrammed workloads is improved by an average of 21.5% over LRU and by 13.8% over DRRIP under prefetching. Finally by more effectively caching the
most useful demand- and prefetch-requested data, PACMan also
reduces demand memory bandwidth considerably: an average of
roughly 31.3%. Overall, the detailed analysis of state-of-the-art
cache management policies in this paper can guide future work
in memory hierarchy architecture and design regarding the importance and challenges of prefetching.

On the other hand, Ebrahimi et al. [4] proposed coordinated control of CMP prefetchers to reduce inter-core prefetcher-prefetcher
LLC interference. However, this approach requires modification
to prefetcher organization in hardware whereas PACMan leverages
hardware implementation of the underlying cache replacement policy and with little additional overhead.
To our knowledge, there has not been any research that analyzes
cache management schemes under prefetching in detail. We are
the first to compare the performance of three state-of-the-art cache
management techniques, DRRIP, SLRU, and SDBP, in the presence
of prefetching. We further propose PACMan, a novel and practical
prefetch-aware cache management scheme to address both intraand inter-core prefetch-induced cache interference.

7.2 LLC Interference Characterization and
Cache Capacity Management
The LLC is one of the most critical shared resources in CMPs.
For performance isolation, quality of service, and better system
throughput for the LLC, there has been research addressing cache
capacity management problems [2, 5, 12, 16, 17, 23, 26, 30, 32,
35]. However, real systems have many contributing factors to LLC
interference including per-core prefetchers. This interference becomes more complicated as multiple applications and multiple percore prefetchers share the LLC simultaneously.
Prior work often focuses on characterizing inter-core cache interference only; they neglect other contributing factors to LLC interference in real systems. As characterized in the paper, hardware prefetching implemented in all of today’s high performance
systems influences memory sub-system performance significantly.
To the best of our knowledge, we are the first to analyze prefetchinduced interference for private and shared LLCs, offer a comprehensive study on the effect of prefetching for the state-of-the-art
cache management techniques, and propose a novel management
solution addressing this problem.

8.

9. Acknowledgements
We thank the entire Intel VSSAD group for their support and
feedback during this work. We also thank Yu-Yuan Chen, Daniel
Lustig, and the anonymous reviewers for their useful insights related to this work. This material is based upon work supported by
the National Science Foundation under Grant No. CNS-0509402
and CNS-0720561. The authors also acknowledge the support of
the Gigascale Systems Research Center, one of six centers funded
under the Focus Center Research Program (FCRP), a Semiconductor Research Corporation entity. Carole-Jean Wu is supported in
part by an Intel PhD Fellowship.

Conclusion

The most important research impact of our work is to highlight
the need for cache research studies to consider prefetching and
other real-system effects when evaluating management schemes
and other design proposals. As characterized in the paper, hardware prefetching implemented in today’s high performance systems significantly influences memory sub-system performance. In
the presence of prefetching, performance benefits using prior proposals become more varied. Some applications experience significant performance degradation. Other applications, which originally
see significant performance improvement from an intelligent cache
management scheme in the absence of prefetching, do not continue
to see these gains when prefetching effects are accounted for.
To address the challenges of performance degradation and variability under prefetching, we propose a novel Prefetch-Aware Cache
Management technique (PACMan) to mitigate prefetch-induced LLC
interference. We build PACMan on top of a state-of-the-art replacement policy, DRRIP, by modifying cache insertion and cache
hit promotion policies differently for prefetch and demand requests
to (1) avoid cache pollution due to harmful prefetch requests, and
(2) retain cache lines that cannot be easily prefetched in the cache.
The proposed dynamic PACMan policy implements Set Dueling to
determine when prefetching is beneficial to running applications.
Thus, unlike prior work, PACMan can exploit benefits of hardware
prefetching without additional hardware overhead.
PACMan can effectively and consistently reduce the degree of
prefetch-induced cache pollution, a prominent issue particularly in
server workloads. As a result, in the presence of prefetching, server

452

10. References

[1] A. Alameldeen and D. Wood. Interactions between
compression and prefetching in chip multiprocessors. In
Proceedings of the 35th International Symposium on
Computer Architecture, 2007.
[2] D. Chandra, F. Guo, S. Kim, and Y. Solihin. Predicting
inter-thread cache contention on a chip multi-processor
architecture. In Proceedings of the 11th International
Symposium on High-Performance Computer Architecture,
2005.
[3] M. Chaudhuri. Pseudo-LIFO: The foundation of a new
family of replacement policies for LLCs. In Proceedings of
the 42nd International Symposium on Microarchitecture,
2009.
[4] E. Ebrahimi, O. Mutlu, C. J. Lee, and Y. N. Patt. Coordinated
control of multiple prefetchers in multi-core systems. In
Proceedings of the 42nd International Symposium on
Microarchitecture, 2009.
[5] H. Gao and C. Wilkerson. A dueling segmented LRU
replacement algorithm with adaptive bypassing. In
Proceedings of the 1st JILP Workshop on Computer
Architecture Competitions, 2010.
[6] I. Hur and C. Lin. Memory prefetching using adaptive stream
detection. In Proceedings of the 39th International
Symposium on Microarchitecture, 2006.
[7] S. Iacobovici, L. Spracklen, S. Kadambi, Y. Chou, and S. G.

[8]
[9]

[10]

[11]

[12]

[13]
[14]

[15]

[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23] K. Nesbit, J. Laudon, and J. Smith. Virtual private caches. In
Proceedings of the 35th International Symposium on
Computer Architecture, 2007.
[24] H. Patil, R. Cohn, M. Charney, R. Kapoor, A. Sun, and
A. Karunanidhi. Pinpointing representative portions of large
Intel Itanium programs with dynamic instrumentation. In
Proceedings of the 37th International Symposium on
Microarchitecture, 2004.
[25] J.-K. Peir, S.-C. Lai, S.-L. Lu, J. Stark, and K. Lai. Bloom
filtering cache misses for accurate data speculation and
prefetching. In Proceedings of the 16th International
Conference on Supercomputing, 2002.
[26] M. Qureshi and Y. Patt. Utility-based cache partitioning: A
low-overhead, high-performance, runtime mechanism to
partition shared caches. In Proceedings of the 39th
International Symposium on Microarchitecture, 2006.
[27] M. K. Qureshi, A. Jaleel, Y. N. Patt, S. C. Steely Jr., and
J. Emer. Adaptive insertion policies for high performance
caching. In Proceedings of the 35th International Symposium
on Computer Architecture, 2007.
[28] M. K. Qureshi, D. N. Lynch, O. Mutlu, and Y. N. Patt. A
case for MLP-aware cache replacement. In Proceedings of
the 34th International Symposium on Computer Architecture,
2006.
[29] Standard Performance Evaluation Corporation CPU2006
Benchmark Suite. http://www.spec.org/cpu2006/.
[30] S. Srikantaiah, M. Kandemir, and M. J. Irwin. Adaptive set
pinning: Managing shared caches in chip multiprocessors. In
Proceedings of the 13th International Conference on
Architectural Support for Programming Languages and
Operating Systems, 2008.
[31] S. Srinath, O. Mutlu, H. Kim, and Y. N. Patt. Feedback
directed prefetching: Improving the performance and
bandwidth-efficiency of hardware prefetchers. In
Proceedings of the 13th International Symposium on High
Performance Computer Architecture, 2007.
[32] E. Suh, S. Devadas, and L. Rudolph. A new memory
monitoring scheme for memory-aware scheduling and
partitioning. In Proceedings of the 8th International
Symposium on High-Performance Computer Architecture,
2002.
[33] C.-J. Wu, A. Jaleel, W. Hasenplaugh, M. Martonosi, S. C.
Steely Jr., and J. Emer. SHiP: Signature-based hit predictor
for high performance caching. In Proceedings of the 44th
International Symposium on Microarchitecture, 2011.
[34] C.-J. Wu and M. Martonosi. Characterization and dynamic
mitigation of intra-application cache interference. In
Proceedings of the International Symposium on Performance
Analysis of Systems and Software, 2011.
[35] Y. Xie and G. Loh. PIPP: Promotion/insertion
pseudo-partitioning of multi-core shared caches. In
Proceedings of the 37th International Symposium on
Computer Architecture, 2009.
[36] X. Zhuang and H.-H. Lee. Reducing cache pollution via
dynamic data prefetch filtering. In IEEE Trans. Comput.,
volume 56, January 2007.

Abraham. Effective stream-based and execution-based data
prefetching. In Proceedings of the 18th International
Conference on Supercomputing, 2004.
Intel Core i7 Processors
http://www.intel.com/products/processor/corei7/.
A. Jaleel, E. Borch, M. Bhandaru, S. C. Steely Jr., and
J. Emer. Achieving non-inclusive cache performance with
inclusive caches: Temporal locality aware (TLA) cache
management policies. In Proceedings of the 43rd
International Symposium on Microarchitecture, 2010.
A. Jaleel, R. S. Cohn, C.-K. Luk, and B. Jacob. CMP$im: A
Pin-based on-the-fly multi-core cache simulator. In
Proceedings of the 4th Workshop on Modeling,
Benchmarking and Simulation, 2008.
A. Jaleel, W. Hasenplaugh, M. Qureshi, J. Sebot, S. C. Steely
Jr., and J. Emer. Adaptive insertion policies for managing
shared caches. In Proceedings of the 17th International
Conference on Parallel Architecture and Compilation
Techniques, 2008.
A. Jaleel, K. B. Theobald, S. C. Steely Jr., and J. Emer. High
performance cache replacement using re-reference interval
prediction (RRIP). In Proceedings of the 38th International
Symposium on Computer Architecture, 2010.
JILP Workshop on computer architecture competitions
http://jilp.org/jwac-1/.
N. P. Jouppi. Improving direct-mapped cache performance
by the addition of a small fully-associative cache and
prefetch buffers. In Proceedings of the 17th International
Symposium on Computer Architecture, 1990.
N. P. Jouppi and S. J. E. Wilton. Tradeoffs in two-level
on-chip caching. In Proceedings of the 21st International
Symposium on Computer Architecture, 1994.
S. M. Khan, Y. Tian, and D. A. Jiménez. Dead block
replacement and bypass with a sampling predictor. In
Proceedings of the 43rd International Symposium on
Microarchitecture, 2010.
S. Kim, D. Chandra, and Y. Solihin. Fair cache sharing and
partitioning in a chip multiprocessor architecture. In
Proceedings of the 13th International Conference on Parallel
Architectures and Compilation Techniques, 2004.
W.-f. Lin, S. Reinhardt, and D. Burger. Reducing DRAM
latencies with an integrated memory hierarchy design. In
Proceedings of the 7th International Symposium on High
Performance Computer Architecture, 2001.
G. Loh. Extending the effectiveness of 3D-stacked DRAM
caches with an adaptive multi-queue policy. In Proceedings
of the 42nd International Symposium on Microarchitecture,
2009.
C.-K. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser,
G. Lowney, S. Wallace, V. J. Reddi, and K. Hazelwood. Pin:
Building customized program analysis tools with dynamic
instrumentation. In Proceedings of the ACM SIGPLAN
Conference on Programming Language Design and
Implementation, 2005.
P. Michaud. The 3P and 4P cache replacement policies. In
Proceedings of the 1st JILP Workshop on Computer
Architecture Competitions, 2010.
K. Nesbit, A. Dhodapkar, and J. Smith. AC/DC: An adaptive
data cache prefetcher. In Proceedings of the 13th
International Conference on Parallel Architectures and
Compilation Techniques, 2004.

453

SHiP: Signature-based Hit Predictor
for High Performance Caching
Carole-Jean Wu∗

§

Aamer Jaleel† Will Hasenplaugh†‡ Margaret Martonosi∗ Simon C. Steely Jr.† Joel Emer†‡

Princeton University∗
Princeton, NJ
{carolewu,mrm}@princeton.edu

Intel Corporation, VSSAD†
Hudson, MA
{aamer.jaleel,william.c.hasenplaugh,
simon.c.steely.jr,joel.emer}@intel.com

ABSTRACT

memory speeds increase the importance of a high performing LLC.
Recent studies, however, have shown that the commonly-used LRU
replacement policy still leaves significant room for performance
improvement. As a result, a large body of research work has focused on improving LLC replacement [5, 10, 15, 16, 17, 20, 27,
29, 31, 32]. This paper focuses on improving cache performance
by addressing the limitations of prior replacement policy proposals.
In the recently-proposed Re-Reference Interval Prediction (RRIP)
framework [10], cache replacement policies base their replacement
decisions on a prediction of the re-reference (or reuse) pattern of
each cache line. Since the exact re-reference pattern is not known,
the predicted re-reference behavior is categorized into different
buckets known as re-reference intervals. For example, if a cache
line is predicted to be re-referenced soon, it is said to have a nearimmediate re-reference interval. On the other hand, if a cache line
is predicted to be re-referenced far in the future, it is termed to have
a distant re-reference interval. To maximize cache performance,
cache replacement policies continually update the re-reference interval of a cache line. The natural opportunity to predict (and update) the re-reference interval is on cache insertions and cache hits.
The commonly-used LRU replacement policy (and its approximations) predict that all cache lines inserted into the cache will
have a near-immediate re-reference interval. Recent studies [10,
27] have shown that always predicting a near-immediate re-reference
interval on cache insertions performs poorly when application references have a distant re-reference interval. This situation occurs
when the application working set is larger than the available cache
or when the application has a mixed access pattern where some
memory references have a near-immediate re-reference interval
while others have a distant re-reference interval. For both of these
access patterns, LRU replacement causes inefficient cache utilization. Furthermore, since LLCs only observe references filtered
through the smaller caches in the hierarchy, the view of re-reference
locality at the LLCs can be skewed by this filtering of upper-level
caches.
In efforts to improve cache performance, many studies have proposed novel ideas to improve cache replacement [5, 15, 16, 17, 20,
31]. While these proposals address the limitations of LRU replacement, they either require additional hardware overhead or require
significant changes to the cache structure. Alternatively, studies
have also shown that simply changing the re-reference prediction
on cache insertions [10, 27, 29, 32] can significantly improve cache
performance at very low hardware overhead. However, a fundamental challenge today is how to best design a practical mechanism
that can accurately predict the re-reference interval of a cache line
on cache insertions.
Recent proposals [10, 27, 32] use simple mechanisms to predict
the re-reference interval of the incoming cache line. Specifically,
thes mechanisms predict the same re-reference interval for the ma-

The shared last-level caches in CMPs play an important role in
improving application performance and reducing off-chip memory
bandwidth requirements. In order to use LLCs more efficiently, recent research has shown that changing the re-reference prediction
on cache insertions and cache hits can significantly improve cache
performance. A fundamental challenge, however, is how to best
predict the re-reference pattern of an incoming cache line.
This paper shows that cache performance can be improved by
correlating the re-reference behavior of a cache line with a unique
signature. We investigate the use of memory region, program
counter, and instruction sequence history based signatures. We also
propose a novel Signature-based Hit Predictor (SHiP) to learn the
re-reference behavior of cache lines belonging to each signature.
Overall, we find that SHiP offers substantial improvements over the
baseline LRU replacement and state-of-the-art replacement policy
proposals. On average, SHiP improves sequential and multiprogrammed application performance by roughly 10% and 12% over
LRU replacement, respectively. Compared to recent replacement
policy proposals such as Seg-LRU and SDBP, SHiP nearly doubles
the performance gains while requiring less hardware overhead.

Categories and Subject Descriptors
B.8.3 [Hardware]: Memory Structures

General Terms
Design, Performance

Keywords
Replacement, Reuse Distance Prediction, Shared Cache

1.

Massachusetts Institute of Technology‡
Cambridge, MA

Introduction

The widespread use of chip multiprocessors with shared lastlevel caches (LLCs) and the widening gap between processor and
§A large part of this work was performed while Carole-Jean Wu
was an intern at Intel/VSSAD.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
MICRO ’11 December 3-7, 2011, Porto Alegre, Brazil
Copyright 2011 ACM 978-1-4503-1053-6/11/12 ...$10.00.

430

Table 1: Common Cache Access Patterns.
Recency-Friendly
(a1 , ..., ak−1 , ak , ak , ak−1 , ..., a1 )N
Thrashing (k > C)
(a1 , ..., ak−1 , ak )N
Streaming (k = ∞)
(a1 , ..., ak−1 , ak )
Mixed (k < C, m > C)
[(a1 , ..., ak )A P! (b1 , ..., bm )]N
C represents the cache set associativity.
ai represents a cache line access.
(a1 , a2 , ..., ak−1 , ak ) is a temporal sequence of k unique
addresses to a cache set.
(a1 , a2 , ..., ak−1 , ak )N represents a temporal sequence that
repeats N times.
P! (a1 , a2 , ..., ak−1 , ak ) is a temporal sequence that occurs
with some probability P! .
jority of cache insertions and, as a result, make replacement decisions at a coarse granularity. While such proposals are simple and
improve cache performance significantly, we show that there is opportunity to improve these predictions. Specifically, we show that
re-reference predictions can be made at a finer granularity by categorizing references into different groups by associating a signature
with each cache reference. The goal is that cache references that
have the same signature will have a similar re-reference interval.
This paper investigates simple, high performing, and low overhead mechanisms to associate cache references with a unique signature and to predict the re-reference interval for that signature. We
propose a Signature-based Hit Predictor (SHiP) to predict whether
the incoming cache line will receive a future hit. We use three
unique signatures to predict the re-reference interval pattern: memory region signatures (SHiP-Mem), program counter signatures
(SHiP-PC), and instruction sequence history signatures (SHiP-ISeq).
For a given signature, SHiP uses a Signature History Counter Table (SHCT) of saturating counters to learn the re-reference interval for that signature. SHiP updates the SHCT on cache hits and
cache evictions. On a cache miss, SHiP indexes the SHCT with the
corresponding signature to predict the re-reference interval of the
incoming cache line.
SHiP is not limited to a specific replacement policy, but rather
can be used in conjunction with any ordered replacement policy.
SHiP is a more sophisticated cache insertion mechanism that makes
more accurate re-reference predictions than recent cache insertion
policy proposals [10, 27, 32]. Our detailed performance studies
show that SHiP significantly improves cache performance over prior
state-of-the art replacement policies. Of the three signatures, SHiPPC and SHiP-ISeq perform the best across a diverse set of sequential applications including multimedia, games, server, and the
SPEC CPU2006 applications. On average, with a 1 MB LLC, SHiP
outperforms LRU replacement by 9.7% while existing state-of-theart policies like DRRIP [10], Seg-LRU [5] and SDBP [16] improve
performance by 5.5%, 5.6%, and 6.9% respectively. Our evaluations on a 4-core CMP with a 4 MB shared LLC also show that
SHiP outperforms LRU replacement by 11.2% on average, compared to DRRIP (6.5%), Seg-LRU (4.1%), and SDBP (5.6%).

2.

tions in replacement policies include improvements in cache insertion and promotion policies [10, 27, 32], dead block prediction [15,
16, 18, 20], reuse distance prediction [10, 14, 17], frequency-based
replacement [19] and many more [1, 2, 6, 11, 24, 28, 30, 31].
To better understand the need for more intelligent replacement
policies, a recent study [10] summarized frequently occurring access patterns (shown in Table 1). LRU replacement (and its approximations) behaves well for both recency-friendly and streaming access patterns. However, LRU performs poorly for thrashing and
mixed access patterns. Consequently, improving the performance
for these access patterns has been the focus of many replacement
policy proposals.
Thrashing occurs when the application working set is larger than
the available cache. In such cases, preserving part of the working
set in the cache can significantly improve performance. Recent
proposals show that simply changing the re-reference predictions
on cache insertions can achieve this desired effect [10, 27].
Mixed access patterns on the other hand occur when a frequently
referenced working set is continuously discarded from the cache
due to a burst of non-temporal data references (called scans). Since
these access patterns are commonly found in important multimedia,
games, and server applications [10], improving the cache performance of such workloads is of utmost importance.
To address mixed access patterns, a recent study proposed the
Dynamic Re-Reference Interval Prediction (DRRIP) replacement
policy [10]. DRRIP consists of two component policies: Bimodal
RRIP (BRRIP) and Static RRIP (SRRIP). DRRIP uses Set Dueling [27] to select between the two policies. BRRIP is specifically
targeted to improve the performance of thrashing access patterns.
SRRIP is specifically targeted to improve the performance of mixed
access patterns. DRRIP performance is shaped by how well it performs on the two component policies.
The performance of the SRRIP component policy is dependent
on two important factors. First, SRRIP relies on the active working
set to be re-referenced at least once. Second, SRRIP performance is
constrained by the scan length. Table 2 illustrates scan access patterns using the scan notation from Table 1, and their performance
under SRRIP. For short scan lengths, SRRIP performs well. However, when the scan length exceeds the SRRIP threshold or when
the active working set has not been re-referenced before the scan,
SRRIP behaves similarly to LRU replacement. We focus on designing a low overhead replacement policy that can improve the
performance of mixed access patterns.

3. Signature-based Hit Predictor
Most replacement policies attempt to learn the re-reference interval of cache lines by always making the same re-reference prediction for all cache insertions. Instead of making the same rereference predictions for all cache insertions, we associate each
cache reference with a distinct signature. We show that cache replacement policies can be significantly improved by dynamically
learning the re-reference interval of each signature and applying
the information learned at cache insertion time.

3.1 Signature-based Replacement

Background and Motivation

The goal of signature-based cache replacement is to predict whether the insertions by a given signature will receive future cache
hits. The intuition is that if cache insertions by a given signature

Increasing cache sizes and the use of shared LLCs in CMPs has
spurred research work on improving cache replacement. Innova-

Table 2: n-bit SRRIP Behavior with Different Mixed Access Patterns
Mixed Access Patterns
[(a1 , a2 , ..., ak−1 , ak )A P! (b1 , b2 , ..., bm )]N
Example
Short Scan
m <= (C − K) ∗ (2n − 1) and A > 1
(a1 , a2 ), (a1 , a2 ), b1 , b2 , b3 , (a1 , a2 ), ...
(a1 , a2 ), (a1 , a2 ), b1 , b2 , b3 , b4 , b5 , b6 , (a1 , a2 ), ...
Long Scan
m > (C − K) ∗ (2n − 1) and A > 1
Exactly One Reuse
A = 1 regardless of m
(a1 , a2 ), b1 , b2 , b3 , b4 , (a1 , a2 ), ...
m: the length of a scan; C: the cache set associativity; K: the length of the active working set.

431

Figure 1: (a) SHiP Structure and (b) SHiP Algorithm.

Figure 2: (a) Memory region signature: 393 unique 16KB address regions are referenced in the entire hmmer program run. The
most-referenced regions show high data reuse whereas the other regions include mostly cache misses. (b) Instruction PC signature:
reference counts per PC for zeusmp
are re-referenced, then future cache insertions by the same signature will again be re-referenced. Conversely, if cache insertions by
a given signature do not receive subsequent hits, then future insertions by the same signature will again not receive any subsequent
hits. To explicitly correlate the re-reference behavior of a signature,
we propose a Signature-based Hit Predictor (SHiP).
To learn the re-reference pattern of a signature, SHiP requires
two additional fields to be stored with each cache line: the signature
itself and a single bit to track the outcome of the cache insertion.
The outcome bit (initially set to zero) is set to one only if the cache
line is re-referenced. Like global history indexed branch predictors
[33], we propose a Signature History Counter Table (SHCT) of saturating counters to learn the re-reference behavior of a signature.
When a cache line receives a hit, SHiP increments the SHCT entry
indexed by the signature stored with the cache line. When a line
is evicted from the cache but has not been re-referenced since insertion, SHiP decrements the SHCT entry indexed by the signature
stored with the evicted cache line.
The SHCT value indicates the re-reference behavior of a signature. A zero SHCT value provides a strong indication that future
lines brought into the LLC by that signature will not receive any
cache hits. In other words, references associated with this signature always have a distant re-reference interval. On the other hand,
a positive SHCT counter implies that the corresponding signature
receives cache hits. The exact re-reference interval is unknown because the SHCT only tracks whether or not a given signature is
re-referenced, but not its timing.
Figure 1 illustrates the SHiP structure and SHiP pseudo-code.
The hardware overhead of SHiP includes the SHCT and the two
additional fields in each cache line: signature and outcome. SHiP
requires no changes to the cache promotion or victim selection policies. To avoid the per-line overhead, Section 7.1 illustrates the use
of set sampling [27] to limit the hardware overhead to only a few
cache lines for SHCT training.
SHiP can be used in conjunction with any ordered replacement

policy. The primary goal of SHiP is to predict the re-reference interval of an incoming cache line. For example, on a cache miss,
the signature of the missing cache line is used to consult the SHCT.
If the corresponding SHCT entry is zero, SHiP predicts that the
incoming line will have a distant re-reference interval, otherwise
SHiP predicts that the incoming line will have an intermediate rereference interval. Given the re-reference prediction, the replacement policy can decide how to apply it. For example, LRU replacement can apply the prediction of distant re-reference interval
by inserting the incoming line at the end of the LRU chain (instead
of the beginning). Note that SHiP makes re-reference predictions
only on cache insertions. Extensions of SHiP to update re-reference
predictions on cache hits are left for future work.
For our studies, we evaluate SHiP using the SRRIP replacement
policy. We use SRRIP because it requires less hardware than LRU
and in fact outperforms LRU [10]. In the absence of any external information, SRRIP conservatively predicts that all cache insertions
have an intermediate re-reference interval. If the newly-inserted
cache line is referenced quickly, SRRIP updates the re-reference
prediction to near-immediate; otherwise, SRRIP downgrades the
re-reference prediction to distant. In doing so, SRRIP learns the
re-reference interval for all inserted cache lines upon re-reference.
SHiP on the other hand explicitly and dynamically predicts the rereference interval based on the SHCT. SHiP makes no changes to
the SRRIP victim selection and hit update policies. On a cache
miss, SHiP consults the SHCT with the signature to predict the rereference interval of the incoming cache line. Table 3 summarizes
the cache insertion and hit promotion policies for the 2-bit SRRIP
and 2-bit SHiP schemes.
Table 3: Policies for the 2-bit SRRIP and SHiP.
SRRIP
SHiP
Insertion
always 2
if (SHCT[Signature] == 0) 3; else 2;
Promotion always 0
always 0

432

tions have a distant re-reference interval. A PC-based signature can generate accurate re-reference predictions if most
references from a given PC have similar reuse behavior.
• Instruction Sequence History Signature: Cache references
can also be grouped using an instruction sequence history for
that memory reference. We define instruction sequence history as a binary string that corresponds to the sequence of
instructions decoded before the memory instruction. If a decoded instruction is a load/store instruction, a ‘1’ is inserted
into the sequence history, else a ‘0’ is inserted into the sequence history. Figure 3 illustrates this using an example.
Instruction sequences can capture correlations between references and may be more compact than PC-based signatures.

Figure 3: Example of Encoding Instruction Sequence History

3.2 Signatures for Improving Replacement

While there are many ways of choosing a signature for each
cache reference, we evaluate SHiP using the following three signatures.

4. Experimental Methodology
4.1 Simulation Infrastructure
We evaluate SHiP using the the simulation framework released
by the First JILP Workshop on Computer Architecture Competitions [12]. This Pin-based [22] CMP$im [8] simulation framework
models a 4-way out-of-order processor with a 128-entry reorder
buffer and a three-level cache hierarchy. The three-level cache hierarchy is based on an Intel Core i7 system [7]. The L1 and L2
caches use LRU replacement and our replacement policy studies
are limited to the LLC. Table 4 summarizes the memory hierarchy.
For the SHiP scheme, we use a default 16K-entry SHCT with
3-bit saturating counters1 SHiP-PC uses the 14-bit hashed instruction PC as the signature indexing to the SHCT. SHiP-ISeq uses
the 14-bit hashed memory instruction sequence as the signature.
The instruction sequence is constructed at the decode stage of the
pipeline. Like all prior PC-based schemes, the signature is stored
in the load-store queue and accompanies the memory reference
throughout all levels of the cache hierarchy. SHiP-Mem uses the
upper 14-bit of data addresses as the signature. Table 3 summarizes re-reference predictions made by SHiP upon cache insertion.

• Memory Region Signature: Cache references can have signatures based on the memory region being referenced. Specifically, the most significant bits of the data address can be
hashed to form a signature. Figure 2(a) illustrates the reuse
characteristics for hmmer. The x-axis shows a total of 393
unique 16KB memory regions referenced in the entire program (ranked by reference counts) while the y-axis shows
the number of references in each region. Data references to
certain address regions have “low-reuse” and always result in
cache misses. On the other hand, references to other address
regions are reused more often. A memory-region-based signature can generate accurate re-reference predictions if all
references to a given memory region have a typical access
pattern (e.g. scans).
• Program Counter (PC) Signature: Cache references can be
grouped based on the instructions which reference memory.
Specifically, bits from the instruction Program Counter (PC)
can be hashed to form a signature. Figure 2(b) illustrates the
reference counts per instruction PC for a SPEC CPU2006 application, zeusmp. The x-axis shows the 70 instructions that
most frequently access memory (covering 98% of all LLC
accesses), while the y-axis shows the reference counts. The
bars show, under LRU replacement, whether these memory
references receive cache hits or misses. Intuitively, SHiP can
identify the frequently-missing instructions (i.e. instructions
1-4) and predict that all memory references by these instruc-

L1 Inst. Caches
L1 Data Caches
L2 Caches

4.2 Workload Construction
Our evaluations use both sequential and multiprogrammed workloads. We use 24 memory-sensitive applications from multimedia and PC games (Mm.), enterprise server (Srvr.), and the SPEC
1
Each SHCT entry is a measure of confidence. When the entry is
zero, it gives high confidence that the corresponding references will
not be re-referenced.

Table 4: Architectural parameters of the simulated system.
32KB, 4-way, Private, 1 cycle
MSHR
32 entries allowing up to 32 outstanding misses
32KB, 8-way, Private, 1 cycle
LLC
1MB per-core, 16-way, Shared, 30 cycles
256KB, 8-way, Private, 10 cycles
Main Memory
32 outstanding requests, 200 cycles

Figure 4: Cache Sensitivity of the Selected Applications.

433

Figure 5: Performance comparison of DRRIP, SHiP-PC, SHiP-ISeq, and SHiP-Mem.

Figure 6: SHiP miss count reduction over LRU.
CPU2006 categories. From each category, we select eight benchmarks for which the IPC performance doubles when the cache size
increases from 1MB to 16MB. Figure 4 shows the cache sensitivity of the selected applications. The SPEC CPU2006 workloads
were collected using PinPoints [25] for the reference input set while
the other workloads were collected on a hardware tracing platform.
These workloads were run for 250 million instructions.
To evaluate shared LLC performance, we construct 161 heterogeneous mixes of multiprogrammed workloads. We use 35 heterogeneous mixes of multimedia and PC games, 35 heterogeneous
mixes of enterprise server workloads, and 35 heterogeneous mixes
of SPEC CPU2006 workloads. Finally, we create another 56 random combinations of 4-core workloads. The heterogeneous mixes
of different workload categories are used as a proxy for a virtualized system. We run each application for 250 million instructions
and collect the statistics with the first 250 million instructions completed. If the end of the trace is reached, the model rewinds the
trace and restarts automatically. This simulation methodology is
similar to recent work on shared caches [4, 9, 10, 21, 32].

5.

In particular, SHiP-PC and SHiP-ISeq are especially effective in
managing a commonly found access pattern in applications such
as halo, excel, gemsFDTD, and zeusmp. Figure 7 captures a
stream of memory references to a particular cache set in gemsFDTD.
For this particular cache set, addresses A, B, C, and D are brought
into the cache by instruction P1. Before getting re-referenced again,
A, B, C, and D are evicted from the cache under both LRU and DRRIP because the number of distinct interleaving references exceeds
the cache associativity. As a result, the subsequent re-references to
A, B, C, and D by a different instruction P2 result in cache misses.
However, under SHiP-PC, the SHCT learns the re-reference interval of references associated to instruction P1 as intermediate and
the re-reference interval of other references as distant. As a result,
when the initial references A, B, C, and D are inserted to the LLC at
P1, SHiP predicts these to have the intermediate re-reference interval while other interleaving references have the distant re-reference
interval. This example illustrates how SHiP-PC and SHiP-ISeq can
effectively identify the reuse pattern of data brought into the LLC
by multiple signatures.
Regardless of the signature, SHiP outperforms both DRRIP and
LRU. Though SHiP-Mem provides gains, using program context
information such as instruction PC or memory instruction sequence
provides more substantial performance gains. Among the three,
SHiP-PC and SHiP-ISeq perform better than SHiP-Mem. On average, compared to LRU, SHiP-Mem, SHiP-PC and SHiP-ISeq improve throughput by 7.7%, 9.7% and 9.4% respectively while DRRIP improves throughput by 5.5%. Unless mentioned, the remainder of the paper primarily focuses on SHiP-PC and SHiP-ISeq.

Evaluation for Private Caches

Figures 5 and 6 compare the throughput improvement and cache
miss reduction of the 24 selected applications. For applications that
already receive performance benefits from DRRIP such as finalfantasy, IB, SJS, and hmmer, all SHiP schemes further improve performance. For these applications, performance gains primarily come from SHiP’s ability to accurately predict the re-reference interval for incoming lines.
More significantly, consider applications, such as halo, excel,
gemsFDTD, and zeusmp, where DRRIP provides no performance
improvements over LRU. Here, SHiP-PC and SHiP-ISeq can provide performance gains ranging from 5% to 13%. The performance
gains is due to the 10% to 20% reduction in cache misses. The results show that SHiP-PC and SHiP-ISeq both dynamically correlate
the current program context to an expected re-reference interval.

5.1 Coverage and Accuracy
To evaluate how well SHiP identifies the likelihood of reuse, this
section analyzes SHiP-PC coverage and prediction accuracy. Table
5 outlines the five different outcomes for all cache references under
SHiP. Figure 8 shows results for SHiP-PC. On average, only 22% of
data references are predicted to receive further cache hit(s) and are

434

Figure 7: Cross-instruction reuse pattern: SHiP-PC can learn and predict that references A, B, C, and D brought in by instruction P1
receive cache hits under a different instruction P2. SHiP learns the data references brought in by P1 are reused by P2 and, therefore,
assigns an intermediate re-reference interval to A, B, C, and D. As a result, the second occurrences of A, B, C, and D by P2 hit in the
cache whereas, under either LRU- or DRRIP-based cache, A, B, C, and D all miss in the cache.
inserted to the LLC with the intermediate re-reference prediction.
The rest of the data references are inserted to the LLC with the
distant re-reference prediction.
The accuracy of SHiP’s prediction can be evaluated by comparing against the actual access pattern. For cache lines filled with the
distant re-reference (DR) interval, SHiP-PC is considered to make a
misprediction if a DR-filled cache line receives further cache hit(s)
during its cache lifetime. Furthermore, a DR-filled cache line under
SHiP-PC could have received cache reuse(s) if it were filled with
the intermediate re-reference interval. To fairly account for SHiPPC’s misprediction for DR-filled cache lines, we implement an 8way first-in-first-out (FIFO) victim buffer per cache set2 . For cache
lines that are filled with the distant re-reference prediction, Figure
8 shows that SHiP-PC achieves 98% prediction accuracy. Few DRfilled cache lines see further cache reuse after insertion. Consider
applications like gemsFDTD and zeusmp for which SHiP-PC offers large performance improvements. Figure 8 shows that SHiPPC achieves more than 80% accuracy for cache lines predicted to
receive further cache hit(s).
On average, for the cache lines filled with the intermediate re-

reference (IR) prediction, SHiP-PC achieves 39% prediction accuracy. SHiP-PC’s predictions for cache lines that will receive
no further cache reuse(s) are conservative because the misprediction penalty for DR-filled cache lines is performance degradation,
while the misprediction penalty for IR-filled cache lines is just the
cost of missing possible performance enhancement. For the mispredicted IR-filled cache lines, SHiP learns that these cache lines
have received no cache hit(s) at their eviction time and adjusts its
re-reference prediction accordingly. Consequently, SHiP-PC trains
its re-reference predictions gradually and more accurately with every evicted cache line.
Over all the evicted cache lines, SHiP-PC doubles the application hit counts over the DRRIP scheme. Figure 9 illustrates the
percentage of cache lines that receive at least one cache hit during
their cache lifetime. For applications such as final-fantasy,
SJB, gemsFDTD, and zeusmp, SHiP-PC improves the total hit
counts significantly. This is because SHiP-PC accurately predicts
and retains cache lines that will receive cache hit(s) in the LLC.
Consequently, the overall cache utilization increases under SHiPPC. A similar analysis with SHiP-ISeq showed similar behavior.

2
A victim buffer is used for evaluating SHiP prediction accuracy.
It is not implemented in the real SHiP design. The per-set FIFO
victim buffer stores DR-filled cache lines that have not received a
cache hit before eviction.

5.2 Sensitivity of SHiP to SHCT Size

Re-Reference Interval Prediction
Distant Re-Reference (DR)
Intermediate Re-Reference (IR)

The SHCT size in SHiP should be small enough to ensure a practical design and yet large enough to mitigate aliasing between un-

Table 5: Definition of SHiP accuracy.
Outcome
Definition
accurate
DR cache lines receive no cache hit(s) before eviction.
inaccurate
DR cache lines receive cache hit(s) before eviction.
inaccurate
DR cache lines receive cache hit(s) in the victim buffer.
accurate
IR cache lines receive cache hit(s) before eviction.
inaccurate
IR cache lines receive no cache hit(s) before eviction.

Figure 8: SHiP-PC coverage and accuracy.

435

Figure 9: Comparison of application hit counts under DRRIP and SHiP-PC.

Figure 10: SHCT utilization and aliasing under the SHiP-PC scheme.
related signatures mapping to the same SHCT entry. Since the degree of aliasing depends heavily on the signature used to index the
SHCT, we conduct a design tradeoff analysis between performance
and hardware cost for SHiP-PC and SHiP-ISeq separately.
Figure 10 plots the number of instructions sharing the same SHCT
entry for a 16K-entry SHiP-PC. In general, the SHCT utilization is
much lower for the multimedia, games, and SPEC CPU2006 applications than the server applications. Since the instruction working
set of these applications is small, there is little aliasing in the 16Kentry SHCT. On the other hand, server applications with larger instruction footprints have higher SHCT utilizations.
For SHiP-PC, we varied the SHCT size from 1K to 1M entries.
Very small SHCT sizes, such as 1K-entries, reduced SHiP-PC’s
effectiveness by roughly 5-10%, although it always outperforms
LRU even with such small table sizes. Increasing the SCHT beyond
16K entries provides marginal performance improvement. This is
because the instruction footprints of all our workloads fit well into
the 16K-entry SCHT. Therefore, we recommend an SCHT of 16K
entries or smaller.
Given the same 16K-entry SHCT size, SHiP-ISeq exhibits a different utilization pattern because it uses the memory instruction sequence signature to index to the SHCT. For all applications, less
than half of the 16K-entry SHCT is utilized. This is because certain
memory instruction sequences usually do not occur in an application. Hence, this provides an opportunity to reduce the SHCT of
SHiP-ISeq.
Instead of using the 14-bit hashed memory instruction sequence
to index to the SHCT directly, we further compress the signature
to 13 bits and use the compressed 13-bit signature to index an
8K-entry SHCT. We call this variation SHiP-ISeq-H. Figure 11(a)
shows that the utilization of the 8K-entry SHCT is increased significantly. While the degree of memory instruction sequence aliasing
increases as well, in particular for server workloads, this does not
affect the performance as compared to SHiP-ISeq. Figure 11(b)
compares the performance improvement for the selected sequential

436

applications under DRRIP, SHiP-PC, SHiP-ISeq, and SHiP-ISeqH over LRU. While SHiP-ISeq-H uses the 8K-entry SHCT (half of
the default 16K-entry SHCT), it offers a comparable performance
gain as SHiP-PC and SHiP-ISeq and improves performance by an
average of 9.2% over LRU.

6. Evaluation for Shared Caches
6.1 Results for SHiP-PC and SHiP-ISeq
For shared caches, on average, DRRIP improves the performance
of the 161 multiprogrammed workloads by 6.4%, while SHiP-PC
and SHiP-ISeq improve the performance more significantly by 11.2%
and 11.0% respectively. For an in-depth analysis, we randomly selected 32 multiprogrammed mixes of sequential applications representative of the behavior of all 161 4-core workloads3 . With the default 64K-entry SHCT scaled for the shared LLC, Figure 12 shows
that the performance improvement of the selected workloads under
SHiP-PC and SHiP-ISeq is 12.1% and 11.6% over LRU respectively while it is 6.7% under DRRIP. Unlike DRRIP, SHiP-PC and
SHiP-ISeq performance improvements are primarily due to finegrained re-reference interval predictions.
Section 5.2 showed that aliasing within a sequential application
is mostly constructive and does not degrade performance. However, for the multiprogrammed workloads, aliasing in the SHCT
becomes more complicated. In a shared SHCT, aliasing not only
comes from multiple signatures within an application but it also
stems from signatures from different co-scheduled applications. Constructive aliasing helps SHiP to learn the correct data reuse patterns
quickly because the multiple aliasing signatures from the different
applications adjust the corresponding SHCT entry unanimously.
Consequently, SHiP’s learning overhead is reduced. On the other
hand, destructive aliasing can also occur when the aliasing signa3

The performance improvement of the randomly selected workloads is within 1.2% difference compared to the performance improvement over all 161 multi-programmed workloads (Figure 12).

Figure 11: SHiP-ISeq-H (a) utilization and aliasing in SHCT (b) performance comparison with SHiP-ISeq.
tures adjust the same SHCT entry in opposite directions. This can
affect SHiP accuracy and reduce its performance benefits.
To investigate the degree of constructive versus destructive aliasing among the different co-scheduled applications, we evaluate three
different SHCT implementations: the unscaled default 16K-entry
SHCT, the scaled 64K-entry SHCT, and the per-core private 16Kentry SHCT for each of the four CMP cores. The former two designs propose a monolithic shared SHCT for all four co-scheduled
applications while in the latter design, each core has its own private
SHCT to completely eliminate cross-core aliasing.

well and aliasing worsens. This problem can be alleviated by scaling the shared SHCT from 16K-entry to 64K-entry. However, the
per-core private SHCT is the most effective solution.
Unlike the multimedia, games, and server workloads, the multiprogrammed SPEC CPU2006 application mixes receive the most
performance improvement from the shared SHCT designs. As discussed in Section 5.2, a significant portion of the 16K-entry SHCT
is unused for any SPEC CPU2006 application alone. When more
SPEC CPU2006 applications are co-scheduled, the utilization of
the shared SHCT increases but the shared 16K-entry SHCT is still
sufficient. While the per-core private 16K-entry SHCT eliminates
destructive aliasing completely, it improves the performance for the
SPEC CPU2006 workloads less because of the learning overhead
each private SHCT has to pay to warm up the table.
Overall, for the shared LLC, the two alternative SHCT designs,
the shared 16K-entry SHCT and the per-core private 16K-entry
SHCT, perform comparably to the shared 64K-entry SHCT in the
SHiP-PC and SHiP-ISeq schemes.

6.2 Per-core Private vs. Shared SHCT
Figure 13 illustrates the sharing patterns among all co-scheduled
applications in the shared 16K-entry SHCT under SHiP-PC. The
No Sharer bars plot the portion of the SHCT used by exactly one
application. The More than 1 Sharer (Agree) bars plot the portion
of the SHCT used by more than one application but the prediction
results among the sharers agree. The More than 1 Sharer (Disagree) bars plot the portion of the SHCT suffering from destructive
aliasing. Finally, the Unused bars plot the unused portion of the
SHCT. The degree of destructive aliasing is fairly low across all
workloads: 18.5% for Mm./Games mixes, 16% for server mixes,
only 2% for SPEC CPU2006 mixes, and 9% for general multiprogrammed workloads.
Figure 14 compares the performance improvement for the three
SHCT implementations in the SHiP-PC and SHiP-ISeq schemes.
Although destructive aliasing does not occur frequently in the shared
16K-entry SHCT, multimedia, games, and server workloads still
favor the per-core 16K-entry SHCT over the other shared SHCT
designs. This is because, as shown in the private cache performance analysis, multimedia, games, and server applications have
relatively larger instruction footprints. When the number of concurrent applications increases, the SHCT utilization increases as

7. SHiP Optimization and Comparison with Prior Work
While the proposed SHiP scheme offers excellent performance
gains, to realize a practical SHiP design, we present two techniques
to reduce SHiP hardware overhead: SHiP-S and SHiP-R. Instead of
using all cache sets in the LLC, SHiP-S selects a few cache set samples to train the SHCT. Then, in the SHiP-R scheme, we explore the
optimal width of the saturating counters in the SHCT. Finally, we
compare the performance of all SHiP variants with the three stateof-the-art cache replacement policies: DRRIP, Segmented LRU
(Seg-LRU), and Sampling Dead Block Prediction (SDBP) and discuss the design tradeoff for each of the schemes.

Figure 12: Performance comparison for multiprogrammed workloads under DRRIP, SHiP-PC, and SHiP-ISeq.

437

Figure 13: Utilization and aliasing for the shared 16K-entry SHCT under SHiP-PC.
less hardware for the SHCT. We see a similar trend for the default
SHiP-ISeq and SHiP-ISeq-R2.
For the shared LLC, using the 2-bit saturating counters in the
SHCT accelerates SHiP’s learning of signature reuse patterns. As
a result, SHiP-PC-R2 and SHiP-ISeq-R2 both perform better than
the default SHiP-PC and default SHiP-ISeq schemes.

7.3 Comparison with Prior Work
In addition to DRRIP, we compare the proposed SHiP scheme
with two recently proposed cache management techniques,
Segmented-LRU (Seg-LRU) [5] and Sampling Dead Block Prediction (SDBP) [16]. DRRIP, Seg-LRU, and SDBP are the top three
best-performing cache replacement policies from JILP Cache Replacement Championship Workshop. Among the three, SDBP also
uses additional information, such as instruction PCs, to assist its
LLC management.
Figure 16 compares the performance improvement for sequential applications under DRRIP, Seg-LRU, SDBP, and our proposed
SHiP schemes. For applications such as SJS, the additional instruction level information in SDBP, SHiP-PC, and SHiP-ISeq helps
improve LLC performance significantly over the LRU, DRRIP, and
Seg-LRU schemes. While SDBP, SHiP-PC, and SHiP-ISeq all improve performance for applications, such as excel, SHiP-PC and
SHiP-ISeq outperforms SDBP for other applications, such as gemsFDTD and zeusmp. Furthermore, while SDBP performs better
than DRRIP and Seg-LRU, its performance improvement for sequential applications varies. For example, SP and gemsFDTD receive no performance benefits under SDBP. Although SDBP and
SHiP use instruction-level information to guide cache line insertion and replacement decisions, SHiP-PC and SHiP-ISeq improve
application performance more significantly and more consistently
by an average of 9.7% and 9.4% over LRU while SDBP improves
performance by only 6.9%. For the shared LLC, SHiP-PC and
SHiP-ISeq outperforms the three state-of-the-art cache replacement
schemes by 11.2% and 11.0% over the LRU baseline while DRRIP, Seg-LRU, and SDBP improve performance by 6.4%, 4.1%,
and 5.6%.

Figure 14: Performance comparison for per-core private vs.
shared SHCT.

7.1 SHiP-S: Reducing Per-line Storage
Using every cache line’s reuse outcome for SHiP training requires each cache line to store two additional information, 14-bit
signature_m and 1-bit outcome, for the SHCT to learn the
reuse pattern of the signature. We propose to use set sampling
to reduce the per-cache line storage overhead of the default SHiPPC and SHiP-ISeq schemes. SHiP-PC-S and SHiP-ISeq-S select a
number of cache sets randomly and use cache lines in the selected
cache sets to train the SHCT.
For the private 1MB LLC, using 64 out of the total 1024 cache
sets is sufficient for SHiP-PC-S to retain most of the performance
gain from the default SHiP-PC scheme. This reduces SHiP-PC’s total storage in the LLC from 30KB to only 1.875KB. For the shared
4MB LLC, more sampling cache sets are required for SHCT training. Overall, 256 out of the total 4096 cache sets offers a good
design point between performance benefit and hardware cost. Figure 15 shows that with set sampling, SHiP-PC and SHiP-ISeq retain most of the performance gains while the total per-line storage
overhead is reduced to less than 2% of the entire cache capacity.

7.2 SHiP-R: Reducing the Width of Saturating Counters in the SHCT
We can further reduce SHiP hardware overhead by decreasing
the width of SHCT counters. In the default SHiP scheme, SHCT
uses 3-bit saturating counters. Using wider counters requires more
hardware but ensures higher prediction accuracy for SHiP because
only re-references with a strongly-biased signature are predicted
to have the distant re-reference interval. On the other hand, using
narrower counters not only requires less hardware but it also accelerates the learning time of the signatures.
Figure 15 compares the performance of the default SHiP-PC and
SHiP-PC-R2 based on 2-bit saturating counters in the SHCT. For
the private LLC (Figure 15(a)), the default SHiP-PC and the SHiPPC-R2 schemes perform similarly while SHiP-PC-R2 uses 33%

7.4 Sensitivity to Cache Sizes
To evaluate the effectiveness of SHiP-PC and SHiP-ISeq for various cache sizes, we perform a sensitivity study for both private and
shared LLCs. We find that larger caches experience less contention,
so the differences in replacement approaches are reduced. However, both SHiP-PC and SHiP-ISeq still continue to improve sequential application performance over the DRRIP and LRU schemes.
For a typical 4MB shared LLC on a 4-core CMP system, SHiPPC improves the performance of all 161 multiprogrammed workloads by an average of 11.2% over the LRU scheme while DR-

438

Figure 15: Performance comparison of SHiP realizations and prior work.

Figure 16: Comparing SHiP with DRRIP, Seg-LRU, and SDBP.
RIP improves the performance by 6.3%. As the shared cache sizes
increase, SHiP-PC and SHiP-ISeq performance improvement remains significant. For a shared 32MB cache, the throughput improvement of SHiP-PC (up to 22.3% and average of 3.2%) and
SHiP-ISeq (up to 22% and average of 3.2%) over LRU still doubles the performance gain under the DRRIP scheme (up to 5.8%
and average of 1.1%).

all, for a diverse variety of multimedia, games, server, and SPEC
CPU2006 applications, the practical SHiP-PC-S-R2 and SHiP-ISeqS-R2 designs use only slightly more hardware than LRU and DRRIP, and outperform all state-of-the-art cache replacement policies.
Furthermore, across all workloads, the simple and low-overhead
SHiP-PC-S-R2 and SHiP-ISeq-S-R2 schemes provide more consistent performance gains than any prior schemes.

7.5 Comparison of Various SHiP Realizations:
Performance and Hardware Overhead

8. Related Work
While we cannot cover all innovations in cache replacement research [1, 2, 3, 6, 10, 11, 14, 16, 18, 19, 20, 24, 26, 27, 28, 30, 31],
we summarize prior art that closely resembles SHiP.

In summary, this paper presents a novel Signature-based Hit Predictor (SHiP) that learns data reuse patterns of signatures and use
the signature-based information to guide re-reference prediction assignment at cache line insertion. The full-fledged SHiP-PC improves sequential application performance by as much as 34% and
by an average of 9.7% over LRU.
In addition to the detailed performance analysis for the proposed
SHiP scheme, we present two techniques that lead to practical SHiP
designs. Figure 15 compares the performance improvement for the
various implementations for SHiP-PC and SHiP-ISeq. Among the
SHiP-PC variants, while using much less hardware, set sampling
(SHiP-PC-S) reduces SHiP-PC performance gain slightly. Overall,
SHiP-PC-S and SHiP-PC-S-R2 still outperform the prior art. The
various SHiP-ISeq practical designs show similar trends.
Furthermore, while reducing the hardware overhead from 42KB
for the default SHiP-PC to merely 10KB for SHiP-PC-S-R2, SHiPPC-S-R2 can retain most of SHiP-PC’s performance benefits and
improve sequential application performance by as much as 32%
and by an average of 9%. Table 6 gives a detailed comparison of
performance improvement versus hardware overhead for the various cache replacement policies investigated in this paper. Over-

8.1 Dead Block Prediction
Lai et al. [18] proposed dead block correlating prefetchers (DBCP)
that prefetch data into dead cache blocks in the L1 cache. DBCP
encodes a trace of instructions for every cache access and relies on
the idea that if a trace leads to the last access for a particular cache
block the same trace will lead to the last access for other blocks.
The proposed DBCP scheme can identify more than 90% of deadblocks in the L1 cache for early replacement; however, a recent
study [16] shows that DBCP performs less well at the last-level
cache of a deep multi-level memory hierarchy.
Instead of using instruction traces to identify cache deadblocks,
Liu et al. [20] proposed Cache-Burst that predicts dead blocks
based on the hit frequency of non-most-recently-used (non-MRU)
cache blocks in the L1 cache. Similarly, cache blocks that are
predicted to have no more reuses become early replacement candidates. Cache-Burst, however, does not perform well for LLCs
because cache burstiness is mostly filtered out by the higher-level
caches. In addition, Cache-Burst requires a significant amount of

439

Table 6: Performance and hardware overhead comparison for prior work and SHiPs.
LRU DRRIP Seg-LRU[5] SDBP[16] SHiP-PC* SHiP-ISeq*
For 1MB LLC
8
4
8+7.68
8+13.75
4+6
4+6
Total Hardware (KB)
8
4
15.68
21.75
10
10
Selected App.
1
1.055
1.057
1.069
1.090
1.086
All App.
1
1.021
1.019
1.024
1.036
1.032
Max. Performance
1
1.28
1.21
1.33
1.32
1.33
Min. Performance
1
0.94
0.87
0.95
1.02
1.02
SHiP-PC* and SHiP-ISeq* use 64 sampling sets to train its SHCT with 2-bit counters (S-R2).
meta-data associated with each cache block.
To eliminate dead cache blocks in the LLC, Khan et al. [16]
proposed Sampling Dead Block Prediction (SDBP) which predicts
dead cache blocks based on the last-referenced instructions and replaces the dead blocks prior to the LRU replacement candidate.
SDBP implements a three-level prediction table trained by a group
of sampled cache sets, called sampler. Each cache block in the sampler remembers the last instruction that accesses the cache block.
If the last-referenced instruction leads to a dead block, data blocks
associated with this instruction are likely to be dead in the LLC.
A major shortcoming of SDBP is that its deadblock prediction
relies on a low-associativity LRU-based sampler. Although Khan
et al. claim that the LRU-based sampler is decoupled from the underlying cache insertion/replacement policy, our evaluations show
that SDBP only improves performance for the two basic cache replacement policies, random and LRU. SDBP also incurs significant
hardware overhead.
One can potentially describe SDBP as a signature-based replacement policy. However, the training mechanisms of both policies are
fundamentally different. SDBP updates re-reference predictions on
the last accessing signature. SHiP on the other hand makes rereference predictions based on the signature that inserts the line
into the cache. Correlating re-reference predictions to the “insertion” signature performs better than the “last-access” signature.
Finally, Manikantan, Rajan, and Govindarajan proposed NUcache [23] which bases its re-use distance prediction solely on instruction PCs. In contrast, SHiP explores a number of different signatures: instruction PC, instruction sequence, and memory region.
Furthermore, while NUcache results in performance gains across a
range of SPEC applications, this paper shows that there are significantly fewer unique PCs in SPEC applications (10’s to 100’s) than
in multimedia, games, and server workloads (1,000’s to 10,000’s).
This hinders NUcache’s effectiveness for these workloads.
While NUcache is relevant to SHiP, NUcache requires significant modification and storage overhead for the baseline cache organization. Furthermore, SHiP’s reuse prediction SHCT, on the
other hand, is elegantly decoupled from the baseline cache structure. Overall, SHiP requires much less hardware overhead. Last but
not least, this work explores three unique signatures to train SHiP.
Instruction sequence is a novel signature, and others like instruction
PC and memory address are novel in how they are applied.

tern of incoming cache blocks. Instead of storing the recency with
each cache line, both SRRIP and DRRIP store the re-reference prediction with each cache line. Both use simple mechanisms to learn
the re-reference interval of an incoming cache line. They do so by
assigning the same re-reference prediction to the majority of cache
insertions and learning the re-reference interval on subsequents.
While simple, there is no intelligence in assigning a re-reference
prediction. SHiP improves re-reference predictions by categorizing references based on distinct signatures.
Gao and Wilkerson [5] proposed the Segmented LRU (Seg-LRU)
replacement policy. Seg-LRU adds a bit per cache line to observe
whether the line was re-referenced or not. This is similar to the
outcome bit stored with SHiP. Seg-LRU modifies the victim selection policy to first choose cache lines whose outcome is false. If no
such line exists, Seg-LRU replaces the LRU line in the cache. SegLRU also proposes additional hardware to estimate the benefits of
bypassing modifications to the hit promotion policy. Seg-LRU requires several changes to the replacement policy. On the other hand
SHiP is higher performing, only modifies the insertion policy, and
requires less hardware overhead.

9. Conclusion
Because LLC reference patterns are filtered by higher-level caches,
typical spatial and temporal locality patterns are much harder to
optimize for. In response, our approach uses signatures—such as
memory region, instruction PC, or instruction path sequence—to
distinguish instances where workloads are mixes of some highly
re-referenced data (which should be prioritized) along with some
distant-reuse data. In this paper we have presented a simple and effective approach for predicting re-referencing behavior for LLCs.
The proposed SHiP mechanism, which accurately predicts the rereference intervals for all incoming cache lines, can significantly
improve performance for intelligent cache replacement algorithms.
Over a range of modern workloads with high diversity in data and
instruction footprint, we have demonstrated performance that is
consistently better than prior work such as Seg-LRU and SDBP
with much less hardware overhead. Although we evaluated our
method on top of SRRIP, the re-reference predictor is a general
idea applicable to a range of LLC management questions.

8.2 Re-Reference Prediction

10. Acknowledgements

Instead of relying on a separate prediction table, Hu et al. [6]
proposed to use time counters to keep track of the liveness of cache
blocks. If a cache block has not been referenced for a specified period of time, it is predicted to be dead. These “dead” blocks become
the eviction candidates before the LRU blocks [6, 31] for cache utilization optimization or can be switched off to reduce leakage [13].
In addition to the LRU counters, the proposed scheme keeps additional coarser-grained counters per cache line, which incurs more
hardware requirement than SHiP.
Jaleel et al. [10] proposed SRRIP and DRRIP to learn reuse pat-

We thank the entire Intel VSSAD group for their support and
feedback during this work. We also thank Yu-Yuan Chen, Daniel
Lustig, and the anonymous reviewers for their useful insights related to this work. This material is based upon work supported by
the National Science Foundation under Grant No. CNS-0509402
and CNS-0720561. The authors also acknowledge the support of
the Gigascale Systems Research Center, one of six centers funded
under the Focus Center Research Program (FCRP), a Semiconductor Research Corporation entity. Carole-Jean Wu is supported in
part by an Intel PhD Fellowship.

440

11. References

International Symposium on Computer Architecture, 2001.
[19] D. Lee, J. Choi, J. H. Kim, S. H. Noh, S. L. Min, Y. Cho, and
C. S. Kim. LRFU: A spectrum of policies that subsumes that
least recently used and least frequently used policies. In
IEEE Trans. Comput., volume 50, December 2001.
[20] H. Liu, M. Ferdman, J. Huh, and D. Burger. Cache bursts: A
new approach for eliminating dead blocks and increasing
cache efficiency. In Proc. of the 41st International
Symposium on Microarchitecture, 2008.
[21] G. Loh. Extending the effectiveness of 3D-stacked DRAM
caches with an adaptive multi-queue policy. In Proc. of the
42nd International Symposium on Microarchitecture, 2009.
[22] C.-K. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser,
G. Lowney, S. Wallace, V. J. Reddi, and K. Hazelwood. Pin:
Building customized program analysis tools with dynamic
instrumentation. In Proc. of the 2005 ACM SIGPLAN
Conference on Programming Language Design and
Implementation, 2005.
[23] R. Manikantan, K. Rajan, and R. Govindarajan. NUcache:
An efficient multicore cache organization based on next-use
distance. In Proc. of the 17th International Symposium on
High Performance Computer Architecture, 2011.
[24] N. Megiddo and D. S. Modha. A self-tuning low overhead
replacement cache. In Proc. of the 2nd USENIX Conference
on File and Storage Technologies, 2003.
[25] H. Patil, R. Cohn, M. Charney, R. Kapoor, A. Sun, and
A. Karunanidhi. Pinpointing representative portions of large
Intel Itanium programs with dynamic instrumentation. In
Proc. of the 37th International Symposium on
Microarchitecture, 2004.
[26] P. Petoumenos, G. Keramidas, and S. Kaxiras.
Instruction-based reuse-distance prediction for effective
cache management. In Proc. of the 9th International
Conference on Systems, Architectures, Modeling and
Simulation, 2009.
[27] M. K. Qureshi, A. Jaleel, Y. N. Patt, S. C. Steely Jr., and
J. Emer. Adaptive insertion policies for high performance
caching. In Proc. of the 35th International Symposium on
Computer Architecture, 2007.
[28] K. Rajan and G. Ramaswamy. Emulating optimal
replacement with a shepherd cache. In Proc. of the 40th
International Symposium on Microarchitecture, 2007.
[29] S. Srinath, O. Mutlu, H. Kim, and Y. N. Patt. Feedback
directed prefetching: Improving the performance and
bandwidth-efficiency of hardware prefetchers. In Proc. of the
13th International Symposium on High Performance
Computer Architecture, 2007.
[30] R. Subramanian, Y. Smaragdakis, and G. Loh. Adaptive
caches: Effective shaping of cache behavior to workloads. In
Proc. of the 39th International Symposium on
Microarchitecture, 2006.
[31] C.-J. Wu and M. Martonosi. Adaptive timekeeping
replacement: Fine-grained capacity management for shared
CMP caches. In ACM Trans. Archit. Code Optim., volume 8,
February 2011.
[32] Y. Xie and G. Loh. PIPP: Promotion/insertion
pseudo-partitioning of multi-core shared caches. In Proc. of
the 37th International Symposium on Computer Architecture,
2009.
[33] T. Yeh and Y. N. Patt. Two-level adaptive training branch
prediction. In Proc. of the 24th International Symposium on
Microarchitecture, 1991.

[1] S. Bansal and D. S. Modha. CAR: Clock with adaptive
replacement. In Proc. of the 3rd USENIX Conference on File
and Storage Technologies, 2004.
[2] A. Basu, N. Kirman, M. Kirman, and M. Chaudhuri.
Scavenger: A new last level cache architecture with global
block priority. In Proc. of the 40th International Symposium
on Microarchitecture, 2007.
[3] L. A. Belady. A study of replacement algorithms for a virtual
storage computer. In IBM Syst. J., volume 5, June 1966.
[4] M. Chaudhuri. Pseudo-LIFO: the foundation of a new family
of replacement policies for LLCs. In Proc. of the 42nd
International Symposium on Microarchitecture, 2009.
[5] H. Gao and C. Wilkerson. A dueling segmented LRU
replacement algorithm with adaptive bypassing. In Proc. of
the 1st JILP Workshop on Computer Architecture
Competitions, 2010.
[6] Z. Hu, S. Kaxiras, and M. Martonosi. Timekeeping in the
memory system: Predicting and optimizing memory
behavior. In Proc. of the 29th International Symposium on
Computer Architecture, 2002.
[7] Intel Core i7 Processors
http://www.intel.com/products/processor/corei7/.
[8] A. Jaleel, R. S. Cohn, C.-K. Luk, and B. Jacob. CMP$im: A
Pin-based on-the-fly multi-core cache simulator. In Proc. of
the 4th Workshop on Modeling, Benchmarking and
Simulation, 2008.
[9] A. Jaleel, W. Hasenplaugh, M. Qureshi, J. Sebot, S. C. Steely
Jr., and J. Emer. Adaptive insertion policies for managing
shared caches. In Proc. of the 17th International Conference
on Parallel Architecture and Compilation Techniques, 2008.
[10] A. Jaleel, K. B. Theobald, S. C. Steely Jr., and J. Emer. High
performance cache replacement using re-reference interval
prediction (RRIP). In Proc. of the 38th International
Symposium on Computer Architecture, 2010.
[11] S. Jiang and X. Zhang. LIRS: An efficient low inter-reference
recency set replacement policy to improve buffer cache
performance. In Proc. of the International Conference on
Measurement and Modeling of Computer Systems, 2002.
[12] JILP Workshop on computer architecture competitions
http://jilp.org/jwac-1/.
[13] S. Kaxiras, Z. Hu, and M. Martonosi. Cache decay:
Exploiting generational behavior to reduce cache leakage
power. In Proc. of the 28th International Symposium on
Computer Architecture, 2001.
[14] G. Keramidas, P. Petoumenos, and S. Kaxiras. Cache
replacement based on reuse-distance prediction. In Proc. of
the 25th International Conference on Computer Design,
2007.
[15] S. M. Khan, D. A. Jiménez, D. Burger, and B. Falsafi. Using
dead blocks as a virtual victim cache. In Proc. of the 19th
International Conference on Parallel Architecture and
Compilation Techniques, 2010.
[16] S. M. Khan, Y. Tian, and D. A. Jiménez. Dead block
replacement and bypass with a sampling predictor. In Proc.
of the 43rd International Symposium on Microarchitecture,
2010.
[17] M. Kharbutli and Y. Solihin. Counter-based cache
replacement and bypassing algorithms. In IEEE Trans.
Comput., volume 57, April 2008.
[18] A. Lai, C. Fide, and B. Falsafi. Dead-block prediction &
dead-block correlating prefetchers. In Proc. of the 28th

441

Quantifying the Energy Cost of Data Movement for Emerging
Smart Phone Workloads on Mobile Platforms
Dhinakaran Pandiyan and Carole-Jean Wu
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University
Tempe, Arizona 85281
Email: {dpandiya,carole-jean.wu}@asu.edu
Abstract—In portable computing systems like smartphones,
energy is generally a key but limited resource where application
cores have been proven to consume a signiﬁcant part of it. To
understand the characteristics of the energy consumption, in
this paper, we focus our attention on the portion of energy
that is spent to move data to the application core’s internal
registers from the memory system. The primary motivation
for this focus comes from the relatively higher energy cost
associated with a data movement instruction compared to that
of an arithmetic instruction. Another important factor is the
distributive computing nature among different units in a SoC
which leads to a higher data movement to/from the application
cores.
We perform a detailed investigation to quantify the impact
of data movement on overall energy consumption of a popular, commercially-available smart phone device. To aid this
study, we design micro-benchmarks that generate desired data
movement patterns between different levels of the memory
hierarchy and measure the instantaneous power consumed
by the device when running these micro-benchmarks. We
extensively make use of hardware performance counters to
validate the micro-benchmarks and to characterize the energy
consumed in moving data. We take a step further to utilize this
calculated energy cost of data movement to characterize the
portion of energy that an application spends in moving data
for a wide range of popular smart phone workloads. We ﬁnd
that a considerable amount of total device energy is spent in
data movement (an average of 35% of the total device energy).
Our results also indicate a relatively high stalled cycle energy
consumption (an average of 23.5%) for current smart phones.
To our knowledge, this is the ﬁrst study that quantiﬁes the
amount of data movement energy for emerging smart phone
applications running on a recent, commercial smart phone
device. We hope this characterization study and the insights
developed in the paper can inspire innovative designs in smart
phone architectures with improved performance and energy
efﬁciency.

Among all the components in a mobile SoC platform,
the device display has been shown to be the most energyhungry in several studies, e.g., [1], [2], when operating in
full brightness. In the energy characterization study [2],
Pandiyan et al. showed that when the brightness of the screen
display is at 25%, the general-purpose application processor
becomes the most energy-hungry component, consuming
more than 50% of the total energy capacity of the device.
This has motivated our work to delve deeper into the energy
consumption characteristics of the application cores.
Most prior work that characterize the power proﬁle of
smart phones focus on component-level results. For example, Carroll and Heiser [1] used sensors on a smart
phone to measure the power usage of the individual computation components under different workloads. Similarly,
Murmuria et al. [3] demonstrated a power usage characterization and develop a power-modeling framework, based
on the component-level power consumption. However, these
component-level energy results do not give the full picture
of how energy is spent in the entire system since the
information of where data reside in the system and how
data is moved across the system are not considered.
Recent work have highlighted the signiﬁcance of the data
movement energy cost in the desktop and server computing
environment. Moving data present in the cache requires as
much energy as a ﬂoating point computation itself and cost
much more if the data is not in the cache hierarchy [4].
For scientiﬁc applications executed on high-performance
desktop/server processors, 28-40% of total energy cost is
spent in moving data [5]. The gap between the energy cost
of moving data from memory to registers and the energy cost
of performing ﬂoating point computations is widening for
future systems. The energy cost of double-precision ﬂoating
point operations is expected to reduce by ten times by 2018
while the energy cost of moving data from memory to
register is expected to remain the same [6], [7]. This trend
indicates the signiﬁcance of data movement energy.

I. I NTRODUCTION
In recent years, there has been an explosive growth in
the use of mobile phones, especially smart phones, for our
everyday computing needs. To cover the wide range of
application requirements running on the smart phones, it
is becoming the trend for mobile System-on-Chip (SoC) to
include an increasing number of general-purpose application
cores (up to eight cores to date), hundreds of graphic processing cores, and many special purpose accelerators such
as digital signal processing cores and video encoder/decoder
cores, as exempliﬁed by the recent NVIDIA Tegra K1.

978-1-4799-6454-3/14/$31.00 ©2014 IEEE

To better understand the energy cost of data movement
in one of the most energy-hungry elements in smart phones,
i.e., the multi-core application processor, this paper performs
a detailed investigation to quantify the overall energy cost

171



	

	





			




	

	









			



	

	

















	

















	


	






		





	




	

Figure 1.
Dynamic power consumption comparison for benchmarks
that perform continuous load instruction execution with and without data
dependency, and add instruction execution utilizing one or both pipelines
of the dual-issue ARM Cortex-A9 processor.

Figure 2. Architecture of the ARM Cortex-A9 processor in a Samsung
Galaxy S3 smart phone.

different levels of the memory hierarchy on a commercial smart phone platform. The hardware performance
counter- and power meter-based measurements allow
us to estimate the energy cost of data movement in the
different levels of the memory hierarchy in a Samsung
Galaxy S3 device.
2) With the unit cost of data movement energy, we perform
a detailed characterization to show the signiﬁcance
of data movement energy and stalled cycle energy of
the entire device when running realistic smart phone
applications, including various web browsing activities,
video playback, photo browsing, and a video game.
Based on the result, we offer our insights for future
smart phone architecture designs.
The remainder of the paper is organized as follows: In
Section II, we give background information of the memory
hierarchy for modern smart phone architectures and we describe the design of the micro-benchmarks that characterize
the data movement energy for a speciﬁc memory hierarchy.
Section III outlines our energy measurement methodology.
Section IV describes our real-device experimental setup for
measuring the energy consumption. Section V presents our
experimental results and insights gained from the results.
Section VI summarizes prior work in this area and Section VII concludes the paper.

of data movement for modern smart phone applications on a
real, commercially available smart phone. We ﬁnd that even
when the mobile application processor is solely working on
fetching data from the memory and idling most of the cycles,
its power consumption is on par with that when it is busy
executing arithmetic operations under 100% utilization.
Figure 1 compares the application processor power consumption under four different scenarios: continuous load
instruction execution with and without data dependency, and
add instruction execution utilizing one or both pipelines
of the dual-issue ARM Cortex-A9 processor. The dynamic
power consumption of the benchmark which performs continuous independent load instruction execution is far more
signiﬁcant than that when performing continuous add operations. This indicates the signiﬁcance of the data movement
energy cost relative to the ALU operations and to the total
application energy consumption.
To quantify and evaluate the impact of data movement
energy on modern smart phone platforms, this paper ﬁrst
presents a methodology to measure the energy of data
movement across the multi-level memory hierarchy in a
mobile application processor. We show that the energy cost
to perform a memory load instruction whose data is not
found in the processor caches is 115 times higher than
that of an add operation. With the unit energy cost for the
arithmetic and memory operations, we take a step further
to characterize modern smart phone workloads running on
a Samsung Galaxy S3 platform. On average, a signiﬁcant
portion (34.6%) of the total device energy consumption
is spent on moving data from one level of the memory
hierarchy to the next level for interactive smart phone
workloads. The data movement energy is particularly high
(41%) for realistic web browsing.
In summary, this paper makes the following contributions:
1) We develop a micro-benchmark methodology to characterize in detail the data movement energy across

978-1-4799-6454-3/14/$31.00 ©2014 IEEE

II. M ICRO - BENCHMARKS : I SOLATING DATA ACCESSES
TO A SPECIFIC LEVEL OF THE MEMORY HIERARCHY

A. Background
Modern smart phone processor architectures feature a hierarchical memory structure. Figure 2 illustrates the memory
hierarchy of the ARM Cortex-A9 processor in a Samsung
Galaxy S3 smart phone. Other commonly-available mobile
processors, e.g., Intel Atom-based Clover Trail processors,
also implement a similar memory hierarchy. When data in
the memory is accessed, it will be moved across different

172

	



	


Initialize benchmark
Start Timer
for i = 0 TO i < iterations/x do
Unroll x times { benchmark operation; }
end for
Stop Timer
Algorithm 1: Pseudo-code of the micro-benchmark which
performs the desired data movement in a target level of the
memory hierarchy iteratively.






	

	



	









	









	








		



tion and other architectural optimizations. The design of the
micro-benchmark methodology is inspired by a recent work
that quantiﬁes the data movement energy cost for scientiﬁc
applications running on desktop and server processors [5].
The goal for the benchmark design is to consistently
bring data from a particular level of memory hierarchy. The
program has to overcome a number of micro-architectural
and compiler optimizations to accomplish that. Between the
two, hardware optimizations are relatively harder to combat
as they occur at runtime, are not visible to the software
and only can be deduced based on performance counter
values. On the other hand, compiler optimizations can be
investigated by reviewing the assembly code which the
compiler generates and be selectively disabled with compiler
ﬂags and appropriate programming methods. We summarize
the workings of our micro-benchmarks as follows:
1) Each micro-benchmark ﬁrst requests the operating system to allocate a size of memory that ﬁts within
the capacity of the level of memory system we are
interested in.
2) The memory region is then accessed as an array of
pointers. Pointer chasing is performed so that each array
element access corresponds to only one architecturally
executed load to avoid overhead due to array index
increment. We utilize a temporary pointer variable to
hold an address that refers to a word in the data set
and a subsequent dereference of the pointer provides
the address to another word. The sequence of addresses
dereferenced can either be random or strided depending
on the speciﬁc benchmark. The following snippet of C
code illustrates pointer chasing.
tmp_ptr = *(void**)tmp_ptr;
3) By continuously performing these dereferences in a
loop, we traverse different portions of the allocated
memory. The loop is timed to enable the calculation of
the average latency per load instruction. The average
latency is used to validate that the micro-benchmark
indeed performs the intended memory accesses in the
target level of the memory hierarchy. Algorithm 1
outlines the pseudo-code of the micro-benchmark.
We create three micro-benchmarks to study the data
movement energy across the different levels of the memory
hierarchy, namely from the L1 cache to registers, from the

Figure 3. Latency measurement of the designed micro-benchmarks with
varying active data working set sizes.

levels of memory: from DRAM to the level-two (L2) cache,
from the L2 cache to the level-one (L1) cache, and from
the L1 cache to the register ﬁle. To accurately quantify
the data movement energy cost, we design a set of microbenchmarks to continuously access data in a speciﬁc level of
the memory hierarchy. We correlate the data references from
the micro-benchmark with power readings obtained from an
external power meter to compute the energy cost of each
data movement operation.
ARM Cortex-A9 speciﬁcations: The designed microbenchmarks are executed on an ARM Cortex-A9 processor
and are used to carry out speciﬁc data movement patterns.
Since the intended data movement patterns are closely coupled with the architectural parameters of the experimental
platform, we give the parameter details here. The ARM
Cortex-A9 processor used in this study has four cores. Each
core has a private 32KB instruction cache and a private
32KB data cache. All cores share the uniﬁed L2 cache
whose size is 1MB. In addition, each core also has a twolevel translation lookaside buffer (TLB) – L1 instruction
and data TLBs (Micro TLBs) are fully set-associative (each
with 32 entries) and the L2 TLB (Main TLB) is 2-way
set-associative with 128 entries. When virtual-to-physical
address translation is not present in the TLB resulting a TLB
miss, page table walk needs to be performed. This can cause
up to two additional memory accesses in the cache. We use
the default page size of 4KB in the experimental platform.
B. Design of the Micro-Benchmarks
Isolating data accesses to a speciﬁc level of the memory
hierarchy and quantifying the energy cost of a speciﬁc
data movement are challenging in modern processors. Outof-order execution and other important architectural optimization features, such as data prefetching and speculation,
have worked well in hiding memory latencies but, at the
same time, make the energy cost benchmarking for an
individual instruction difﬁcult. We design a set of microbenchmarks that minimize the effect of out-of-order execu-

978-1-4799-6454-3/14/$31.00 ©2014 IEEE

173

benchmarks which perform iterative data movement
from a speciﬁc level of the memory hierarchy to
the processor register, we design two more microbenchmarks that execute integer addition and NOP
instructions continuously to understand their relative
impact on energy consumption with respect to that of
data movement.

L2 cache to registers, and from main memory to registers.
• L1
cache
to
CPU
register
(M icrobenchmarkL1→Reg ): This micro-benchmark
performs random accesses within the data set. Each
access brings a word from the L1 data cache to the
register and all data references to the L1 cache are
hits. We choose a data set of 24KB which comfortably
ﬁts in the 32KB L1 data cache.
• L2
cache
to
CPU
register
(M icrobenchmarkL2→Reg ): This micro-benchmark
performs random accesses within the data set that
ﬁts into the 1MB L2 cache of the experimental
platform. The selection of the data set size for this
micro-benchmark requires more considerations than
M icrobenchmarkL1→Reg . This is because additional
TLB misses can be incurred by the random-walk
references performed in the larger memory region. In
order to ensure the majority of data movement happens
between the L2 cache and the register, we need to
choose a data set size that ﬁts in the 1MB L2 cache
but does not ﬁt in the 32KB L1 cache. In addition,
we also want to keep the TLB miss rate as small
as possible1 . To fulﬁll these constraints, we select
the data set size of M icrobenchmarkL2→Reg to be
125KB. As shown in Figure 3, the selected data set
size performs memory accesses that move data from
the L2 cache to the register as intended since the load
latency closely tracks the L2 cache access latency.
• Main
memory
to
CPU
register
(M icrobenchmarkmemory→Reg ):
This
microbenchmark is designed to bring data from the
memory to the processor register. This means that
the data set size needs to be larger than the 1MB
L2 cache resulting in high L1 and L2 cache miss
rates. Since the data set size needs to be larger than
1MB, this micro-benchmark inevitably thrashes the
L2 TLB. Because the additional memory references
related to page table walks also access the cache
hierarchy, these additional references can signiﬁcantly
lower the high L1 and L2 cache miss rates expected
for the random access micro-benchmark. Currently,
there is not a known, effective method to differentiate
cache accesses related to page table walks from those
made by application load instructions. As a result,
we experimentally choose a data set size that has a
high L2 miss rate while keeping the TLB allocations
as few as possible. In this paper, we choose the data
set size to be 2MB. Therefore, this causes more data
movement for each memory instruction compared to
one with a smaller data set.
• Integer
and NOP (M icrobenchmarkadd and
M icrobenchmarknop ): In addition to the micro1 The

C. Discussion
There are other challenges and considerations in the
process of designing and running the micro-benchmarks,
which we discuss below.
• Compiler Optimization: It is possible for the compiler
to optimize away the register variable allocated for
the temporary pointer storage. Because of this, we
manually verify the designed data movement patterns
by looking at the assembly code generated for the
micro-benchmarks.
• Loop Unrolling Effect: The main loop of memory
accesses in the micro-benchmarks can be unrolled such
that a larger number of memory loads are executed
for each loop iteration to reduce the frequency of
branch instruction execution. We carefully select the
loop size/iterations such that the loop body is large
enough for the reduced overhead of loop index increment and branch instructions but not too large to cause
unintended, additional instruction cache misses.
• Priority of the Micro-benchmarks, Task Migration,
and Dynamic Frequency Control in the Operating
System: To minimize interference with other running
processes, we give the micro-benchmarks the highest priority by setting the nice number2 to −20.
Furthermore, to prevent task migration between the
different, available cores, we pin the micro-benchmarks
to a speciﬁc core at the beginning of the program
execution. Finally, the micro-benchmark power consumption highly depends on the core frequency setting,
and the default Android/Linux operating system varies
the core frequency dynamically based on utilization.
To eliminate the inﬂuence of frequency variation, we
use the performance governor to control and set the
frequency of the cores at 1.4 GHz.
III. DATA M OVEMENT E NERGY M EASUREMENT
In this section, we describe the techniques we used to
measure the energy cost of individual instructions and to
isolate the components of the system that do not contribute
to the data movement energy, e.g., idle energy and stalled
cycles. Then we derive the energy cost of data movement
and cache prefetchers.
2 nice is a Linux program to give a process more or less CPU time than
other processes. A niceness of −20 is the highest priority and 19 is the
lowest priority.

128-entry TLB effectively covers the memory region of 512KB

978-1-4799-6454-3/14/$31.00 ©2014 IEEE

174

register

L1 cache

L2 cache

memory
Emem

value. Thus, the power consumption attributed to the microbenchmarks is
Pmicrobenchmark = Pdevice − Pidle

EL2

Furthermore, the total energy consumption of the microbenchmarks is
 EndT ime
Emicrobenchmark =
Pmicrobenchmark dt

EL1
Ereg

StartT ime

B. Stall Cycles
ΔEnergyL1

Figure 4.
hierarchy.

ΔEnergyL2

Depending on where data resides, it takes from 4 to 200
cycles to bring the requested data to processor registers.
This means that the processor spends a majority of time
waiting for data, resulting in signiﬁcant stall cycles. The
stall cycles increase when the memory instruction in an
application depends on the data requested by the previous
instruction, e.g., in the pointer-chasing micro-benchmarks.
In order to separate the energy cost of stall cycles
from the energy cost of moving data from one level to
another level of the memory hierarchy, we design a matching micro-benchmark, M icrobenchmarkL1→Reg,no−dep. ,
which performs exactly the same data movement as in
M icrobenchmarkL1→Reg , except that all data dependencies in the original micro-benchmark have been removed.
This is done by replacing the pointer chasing-based data
movement with pre-calculated memory addresses. Since
all memory addresses in M icrobenchmarkL1→Reg,no−dep.
are known, the memory accesses are independent of
each other and, thus, the stall cycles caused by data
dependency is removed. The number of stall cycles in
M icrobenchmarkL1→Reg is obtained from the performance counters.
By
comparing
the
energy
consumption
of
M icrobenchmarkL1→Reg,no−dep. with the pointer chasing
M icrobenchmarkL1→Reg , we can compute the stall cycle
energy consumption. Using values measured from the
hardware performance counters, it can be deduced that
M icrobenchmarkL1→Reg creates three pipeline stalls for
each load that is issued. Therefore, Stall Cycle energy can
be computed as:

ΔEnergymem

Energy measurement for data movement across the memory

Figure 4 illustrates our approach in determining the energy
cost for accessing data in different levels of the memory
hierarchy. We ﬁrst determine the energy cost for moving
data from the L1 cache to the registers (ΔEnergyL1 ), and
repeat this process to determine the energy cost for moving
data from the L2 cache to the L1 cache (ΔEnergyL2 ) and
from the memory to the L2 cache (ΔEnergymem ). Since
the number of instructions performed in the body loop of the
micro-benchmarks is in the order of billions, we ensure that
the majority of the processor energy consumption is spent
on the designed data movement.
A. Dynamic Energy Measurement of the Micro-Benchmarks
Working with a commercial product does not offer the
ﬂexibility of a product development board with exposed test
points/voltage rails or even that of a PC motherboard. The
only access points our system offers for power measurement
are the battery terminals. The entire system that consists
of the display, LEDs, speakers, DRAM, SoC, sensors,
eMMC etc. is powered via this set of terminals. For the
data movement power measurements at this terminal to be
meaningful, we keep all the peripheral components turned
off or inactive through options that the OS provides. When
the display, Wi-Fi, mobile radio and other peripherals are
turned off, the application processor and DRAM consume
most of the power [2] since the micro-benchmarks used in
our measurements do not make use any peripheral components. To minimize potential interference, we also manually
terminate unnecessary background service processes in our
smart phone experimental platform that can potentially skew
measurements of the micro-benchmarks. The power measurements are made in stable memory access phases (regions
of interest) when the program is fully loaded into memory
and executes only data movement instructions.
In order to calculate the power consumption of the microbenchmarks, the baseline power or the idle power consumption of the device before the micro-benchmarks begin
to execute is recorded and subtracted from the measured

978-1-4799-6454-3/14/$31.00 ©2014 IEEE

EStall = (EL1toReg→Reg − EL1toReg→Reg,no−dep )/NStalls
C. Long Latency Memory Operations
and
The
M icrobenchmarkL2→Reg
M icrobenchmarkRAM →Reg benchmarks with the pointer
chasing logic like M icrobenchmarkL1→Reg cause several
stall cycles due to the data dependency between loads. The
stall cycle energy has to be subtracted from the energy
measurement values for both benchmarks to isolate the
data movement energy. We use the same formula below for
these long latency operations.
EL2→Reg = (EL2→Reg −EStall ∗NStalls )/NM emoryAccesses

175

Table I
E NERGY COST OF DATA MOVEMENT.

D. Cache Prefetcher
Hardware prefetching is a commonly-used latency mitigation technique in modern processors. Cache prefetchers
bring data into the cache hierarchy before the actual reference, thereby shortening the memory latency of application
demand requests. While often helpful, the beneﬁts of aggressive prefetching hinge on its accuracy. When effective, memory performance can be signiﬁcantly improved. However,
inaccurate or untimely prefetched cache lines can result in
additional data to be brought into the cache, which amounts
to wasted energy. As energy is a key limited resource in
mobile platforms, it is of critical importance to evaluate the
energy cost of prefetching. We approximate this energy cost
of prefetching as ERAM →L2 , i.e., same as moving data from
the memory to L2 cache. The rationale is that the energy cost
of prefetching a line from memory is mostly expended on the
actual data movement [5]. Isolating the energy consumption
of prefetch engine’s overhead from this is not apparent, due
to a combination of the energy measurement methodology
we adopt and the limited access to component level power
measurement on a production smartphone.
There are three distinct prefetchers on the Samsung
Exynos-based SoC which houses the ARM Cortex A9 processor: per-core L1 cache stride prefetchers, L2 double lineﬁll cache prefetcher, and the L2 cache stride prefetcher [8].
The per-core L1 cache prefetchers monitor cache references to the L1 cache based on the program counter
(PC) value and address and is capable of tracking multiple
prefetch streams. The L1 cache prefetchers bring data from
the lower levels of the cache hierarchy in advance by placing
the prefetched cache lines into a dedicated prefetch buffer.
Upon hits, prefetched data are brought from the prefetch
buffers to the L1 caches. In the case of inaccurate prefetch
requests, the prefetcher throttles down its aggressiveness to
reduce the degree of potential interference in the prefetch
buffer. Apart from this, the L1 prefetcher also sends prefetch
hints to the L2 cache controller for prefetching lines into
the L2 cache. These lines that are allocated in the L2 cache
are not sent to the L1 cache. The L2 double line-ﬁll cache
prefetcher observes the L2 cache misses and fetches two
cache lines – the one that caused a miss and the next
line from the memory. The L2 controller implements stride
prefetching mechanism that fetches a pre-conﬁgured number
of cache lines based on the references it receives.

Operation
NOP
ADD
LOAD L1→ Reg
LOAD L2→ Reg
LOAD DRAM→ Reg
Stall cycle

Δ Energy
(nJ)
0.192
0.611
11.228
-

Equivalent
ADD Ops
1
1.83
7.65
114.6
-

are separate L1 instruction and data caches for each of
the four cores and a shared uniﬁed L2 cache. The device
runs a rooted Android 4.3 Jelly Bean OS. The Linux kernel
is conﬁgured to enable performance proﬁling with ARM
Streamline [9]. The micro-benchmarks are written in C,
cross-compiled on the host machine with ARM-Android
NDK toolchains [10]. The binaries are pushed to the device
and are launched from the host machine via the adb terminal.
Cortex-A9 provides 6 hardware performance counters
which can be used to measure six performance events
simultaneously. Similarly, the L2 cache controller that is
integrated with the application cores provides two counters.
The L2 prefetcher is turned on/off by modifying a part of
architecture-speciﬁc kernel code that runs during the OS
initialization. We extend the Linux kernel modules in [11]
to read the L2 cache controller registers and to validate the
conﬁguration that has been set by the kernel.
B. Energy Measurement for the Experimental Platform
We remove the smart phone’s battery and power the
device with a DC power supply set to 4.0V. The National
Instruments DAQ 6251 [12] is used for voltage and current
measurement along with a small shunt resistor. The DAQ
periodically samples voltages which is graphically represented in the NI Signal Express [13] tool. The data logged by
the DAQ is minimally processed to eliminate noise by time
averaging, histogram analysis and DC component extraction.
Both current and voltage are sampled at 100KHz with a resolution of 10−6 . All energy measurement results presented
in this paper are obtained with the lowest brightness setting
for the display. And, the results are the average of three
individual runs of each experiment.
C. Workloads
In addition to the set of micro-benchmarks used to
quantify the energy cost of data movement, we execute
a set of commonly-used smart phone workloads for data
movement energy characterization. We use smart phone
applications from the MobileBench suite [2], including an
image processing application (PhotoView), video playback (VideoPlayback), general web browsing (GWB),
realistic web browsing (RWB), and education-oriented web
browsing (EWB).
PhotoView is a photo-browsing workload that starts a
slide show of full-screen high resolution images, each with

IV. E XPERIMENTAL S ETUP
This section introduces our experimental methodology for
real-system energy measurements.
A. Real-Device Experimental Platform
We perform our experiments on a Samsung Galaxy S3
smart phone which houses a Samsung-made Exynos4 Quad
4412 SoC. The SoC has four Cortex-A9 application cores
with a 1GB Low Power DDR (LPDDR) memory. There

978-1-4799-6454-3/14/$31.00 ©2014 IEEE

Energy
Cost (nJ)
0.105
0.105
0.192
0.803
12.032
0.068

176

#&

	

	

	
	

&




"&




	

!&



&









&

Figure 6.
Figure 5.

Validation error rate.

Instruction mix for workloads

from Streamline to identify a) the number of data movement
operations that occur between the different levels of the
memory hierarchy, b) the number of stall cycles, and c) the
number of arithmetic operations.
The energy consumed by each of these operations over
the benchmark execution is calculated by multiplying the
energy cost per unit operation (Table I) with the raw counts
given by the performance counters. For example, the total
energy consumption of the L2 → Reg + Add validation
benchmark is

a resolution of 4912x3264 and is of size between 4 to 6 MB,
whereas VideoPlayback renders a full-screen HD (720p)
MPEG-4 video of size 80 MB. GWB and RWB sequentially
load and render a set of popular websites, including Amazon,
BBC, CNN, Craiglist, eBay, ESPN, Google, MSN, Slashdot,
Twitter, and YouTube, from the secondary storage. The
benchmarks differ in the browsing activities performed after
the web pages are fully loaded. RWB mimics realistic user
browsing behavior by scrolling the web pages up and down
with random delays and web page zooming. EWB is based
on the popular Blackboard course web platform. It combines
Blackboard page browsing and document browsing by invoking a document reader to skim through a set of Portable
Document Files (PDFs). Furthermore, we evaluate a gaming
workload, Frozen Bubbles, which models an interactive
game application [14].
Figure 5 shows the instruction composition for these
workloads. About 60% of the instructions (i.e., integer,
ﬂoating point and NEON instructions) perform computations
and a signiﬁcant 25% of the instructions perform memory
loads and stores. With the energy consumption of a single
instruction or data memory access3 potentially costing upto
115 times of the cost of an integer instruction, data movement energy cost evaluation for mobile workloads is critical.
We present a detailed analysis of the data movement energy
cost in Section V.

EL2→Reg+Add = EL2→L1 ∗ NL2 + EL1→Reg ∗ NL1
+EStall ∗ Nstall + EAdd ∗ NAdd
The estimated energy consumption is compared with the
actual energy consumption measurement of the experimental platform. Figure 6 shows the accuracy of our microbenchmark methodology for data movement energy. The
error rate is calculated as follows:
Emeasured − Eestimated
Accuracy =
Emeasured
We ﬁnd that our micro-benchmark approach can accurately
estimate the energy consumption of data movement. The
error rate is 3.4% on average and is generally under 3.5%.
Only for the L2 → Reg+Add benchmark, we over-estimate
the benchmark energy consumption by 8.6%.
V. E NERGY C OST A NALYSIS FOR M OBILE W ORKLOADS
We obtain the total energy consumption measurement
for a diverse set of mobile workloads (Section IV-C) by
sampling the dynamic power consumption, Pi , of a mobile
application with the DAQ continuously. The total energy
consumption of the application is derived from the dynamic
power consumption samples as follows:

D. Validation
We create another set of benchmarks to validate our
energy estimation methodology and the measured energy
cost values. Each of these validation benchmarks essentially performs a combination of the data movement and
arithmetic operations described earlier in Section II. Performance counter values are collected for the benchmarks



Pt dt =
StartT ime

3 For example, the execution of a memory load instruction needs to access
the memory twice – the instruction itself and data in the requested memory
location.

978-1-4799-6454-3/14/$31.00 ©2014 IEEE

EndT ime

Energy =

k


Pt ti

i=0

The total energy consumption includes the dynamic energy consumption of the entire experimental device but not

177

	






		




use, e.g., texting, viewing pictures, searching for restaurants,
followed by a long period of idle time. While today’s Android OS already adopts smart energy management policies
that aggressively modulate down the operating frequency
of the application processor or even puts the application
processor into the sleep mode, the stalled cycle energy in the
processor cannot be completely eliminated by such coarsegrained energy management. This urges architects for mobile
processors to integrate more, but simpliﬁed, cores into the
application processor to reduce the amount of stall cycles
which can translate to improved energy efﬁciency. Finally,
the energy cost of computations, hardware accelerators, etc.,
in the Other category varies from 31.3% to 54.1%. We
expect a signiﬁcant portion of the Other energy consumption to come from the smart phone display, which has been
shown as one of the most power-hungry components in
modern smart phones [1], [2].
In addition to the energy breakdown, we also perform
the energy analysis for data moving from one level of the
memory hierarchy to another level. Figure 8 shows the
relative energy cost for moving data from the L1 cache
to processor register (L1 → Reg), from the L2 cache to
the L1 instruction cache (L2 → L1Instruction), from the
L2 cache to the L1 data cache (L2 → L1Data), from the
memory to the L2 cache by the processor (M em → L2)
and by the cache prefetchers (P ref etches). Depending
on the memory access patterns of the mobile workloads,
the energy consumption dedicated to each level of the
memory hierarhcy varies. For all studied workloads except
for VideoPlayback, L1 → Reg is the most signiﬁcant. The reason for the relatively lower L1 → Reg for
VideoPlayback is because the active working set of this
benchmark does not completely ﬁt in the cache hierarchy,
having a higher L2 cache miss rate of 24.86%. Thus, a
considerable amount of energy is spent moving data from
the memory to the L2 cache.
Another interesting observation for the studied mobile
workloads is the relatively higher data movement energy
to bring instructions to the L1 instruction cache than to
bring data to the L1 data cache. This is because mobile
workloads often heavily rely on built-in libraries and system
calls in the OS (Android Jelly Bean 4.3 in our case) and thus
exhibit larger instruction working sets that can exceed the
size of the L1 instruction cache. This is similarly shown
in an older study by Guitierrez et al. [15]. Overall, the
data movement energy of M em → L2 is dominating. This
motivates mobile processor and SoC architects to optimize
the data path between the memory and the L2 cache, which
will translate to signiﬁcant energy consumption reduction
and improved energy efﬁciency gains.








Figure 7.

Energy breakdown for the experimental device.

considering the idle energy portion. The data movement
energy is estimated by multiplying the number of accesses to
the L1, L2 caches, and the main memory with the respective
unit energy cost for moving data between the different
levels (Table I). Similarly, we evaluate the energy spent
on processor stall cycles. By separating the data movement
and stall cycle energy consumption from the total energy
consumption of the experimental device, we can attribute the
rest of the energy consumption to the application processor,
other SoC accelerators which may be active and performing
computations concurrently with the application processor,
other system peripherals (e.g., SD card access), as well as
the display.
Figure 7 shows the energy breakdown for the mobile
workloads. The Data Movement Energy bars represent the
portion of the total device energy consumption attributed to
the data movement in the application processor’s memory
hierarchy and the Stall Cycle Energy bars represent the
portion of the total device energy consumption attributed
to processor stall cycles while the Others bars represent the
rest of the energy consumption of other active components
in the device. On average, a signiﬁcant portion (34.6%) of
the total device energy consumption is spent on moving
data from one level of the memory hierarchy to the next
level for mobile workloads. The data movement energy is
particularly high (41%) for realistic web browsing (RWB).
Relatively PhotoView spends less amount of energy in
data movement. This is likely due to the behavior of using
hardware acceleration for jpeg decoding. As a result, more
energy is spent on the Others category for PhotoView.
What we also observe is that there is a considerable
amount of energy spent on stall cycles in the application
processor. On average, 23.5% of the total device energy is
spent on stalled cycles, e.g., resolving data dependencies,
waiting for long latency memory operations, etc. This stall
cycle energy is expected to increase when we consider
realistic user behavior for mobile devices. Users typically
do not use their smart phones for continuous computations.
Typical smart phone usage reveals a pattern of a short-term

978-1-4799-6454-3/14/$31.00 ©2014 IEEE

VI. R ELATED W ORK
There have been a number of efforts that develop power
models to estimate smart phone energy consumption. Most

178

L1 -> Reg

L2 -> L1 Data

L2 -> L1 Instruction

GWB

Mem -> L2

Prefetches

VideoPlayback
27%

39%
30%

PhotoView

24%

33%

47%

3%
18%

3%

7%

30%

21%

6%

10%
EWB-Blackboard

Frozen Bubble

RWB

28%

2%

31%

39%

29%

37%

44%

15%

10%

3%

16%

11%

3%

24%

2%
8%

Figure 8. Relative energy cost for moving data from one level to another level of the entire memory hierarchy in the ARM Cortex-A9 processors for
mobile workloads.

of these power modeling and energy estimation techniques
are based on application processor utilization. Shye et
al. [16] described a regression-based power estimation technique that can predict the power consumed by different
components on a smart phone. Pathak et al. [17] developed a
power model based on system call tracing and demonstrated
that energy estimation is possible using their developed
power model. Zhang et al. [18] leveraged the battery sensor
on a smart phone to construct a model for power estimation.
Li et al. [19] took a different approach to energy estimation
by proﬁling the Java byte code of an application, aiming at
providing energy consumption data to application developers. However, none of these works focus on data movement
energy consumption which is a major contributor to the
overall device energy consumption, as we show in this paper.

scientiﬁc applications were performed. Nonetheless, both
studies were performed on server-class processors. This
work is the ﬁrst that attempts to quantify the data movement
energy cost for modern smart phone workloads running
on a commercial smart phone device. The smart phone
applications and the mobile processors are vastly different
from their server or desktop counterparts. With the increasing popularity of smart phones, the data movement energy
cost presented in this paper along with the detailed data
movement energy characterization for the diverse range of
smart phone applications provide insights into which data
paths to improve upon, in order to further increase the energy
efﬁciency of next generation smart phones.

There have been a few studies that quantify data movement energy via micro-benchmarking for server processors.
Molka et al. [20] quantiﬁed instruction level and data transfer
energy consumption for Intel and AMD server processors.
Kastor et al. [5] took a step further by proposing an energy
cost evaluation methodology for data movement between the
different memory levels. With the estimated data movement
energy cost, the data movement energy characterization for

This paper presents a detailed methodology for quantifying the data movement energy cost on a commercial
smart phone and summarizes the energy cost for moving
data from one to another level of the memory hierarchy
in a mobile processor, ARM Cortex-A9. Given the energy
cost, we take a step further to characterize the portion of the
energy that is spent on data movement for a diverse set of
smart phone applications. Overall, the energy spent on data

978-1-4799-6454-3/14/$31.00 ©2014 IEEE

VII. C ONCLUSION

179

movement in mobile processors is signiﬁcant. 34.6% of the
total device energy consumption is spent on moving data
from one level of the memory hierarchy to the next level
for interactive smart phone workloads. The data movement
energy is particularly high (41%) for realistic web browsing
that is commonly performed on smart phones. Our results
also indicate a relatively high stalled cycle energy consumption (an average of 23.5%) for current smart phones. This
motivates mobile processors to include more but simpler
cores in the design. Furthermore, the considerable amount
of energy spent on moving data from the memory to the L2
cache encourages more researches into low-power emerging memory technologies for embedded devices. With the
detailed energy characterization and the insights provided
in this paper, we hope to inspire innovative designs that
lower power consumption of memory instructions through
more optimized data paths and simpler architecture for smart
phone architectures.

[7] B. Dally, “Power, programmability, and granularity: The
challenges of ExaScale computing,” in Proceedings of International Parallel and Distributed Processing Symposium,
2011.
[8] “PL310 Cache Controller Technical Reference Manual,”
http://goo.gl/GUP7xf.
[9] “ARM STREAMLINE ANALYZER,” http://ds.arm.com/ds5/optimize/.
[10] “Android NDK,” https://developer.android.com/tools/sdk/ndk/index.html.
[11] “Cortex A9 prefetch disable,” https://github.com/deater/uarchconﬁgure/tree/master/cortex-a9-prefetch.
[12] “National
Instrumens
PXI-6251
DAQ,”
http://sine.ni.com/nips/cds/view/p/lang/en/nid/14125.
[13] “NI SignalExpress,” http://www.ni.com/labview/signalexpress.
[14] Y. Huang, Z. Zha, M. Chen, and L. Zhang, “Moby: A mobile
benchmark suite for architectural simulators,” in Proceedings
of IEEE International Symposium on Performance Analysis
of Systems and Software, 2014.

ACKNOWLEDGMENT
The authors would like to thank the anonymous reviewers
for their feedback. This work was partially supported by the
NSF I/UCRC Center for Embedded Systems (NSF grant
#0856090) and by Science Foundation of Arizona under
the Bisgrove Early Career Scholarship. The opinions, ﬁndings and conclusions or recommendations expressed in this
manuscript are those of the authors and do not necessarily
reﬂect the views of the Science Foundation of Arizona.

[15] A. Gutierrez, R. Dreslinski, T. Wenisch, T. Mudge, A. Saidi,
C. Emmons, and N. Paver, “Full-system analysis and characterization of interactive smartphone applications,” in Proceedings of International Symposium on Workload Characterization, 2011.
[16] A. Shye, B. Scholbrock, and G. Memik, “Into the wild: Studying real user activity patterns to guide power optimizations
for mobile architectures,” in Proceedings of the International
Symposium on Microarchitecture, 2009.

R EFERENCES

[17] A. Pathak, Y. C. Hu, M. Zhang, P. Bahl, and Y.-M. Wang,
“Fine-grained power modeling for smartphones using system
call tracing,” in Proceedings of European Conference on
Computer Systems, 2011.

[1] A. Carroll and G. Heiser, “An analysis of power consumption
in a smartphone,” in Proceedings of USENIX annual technical
conference (USENIX-ATC), 2010.

[18] L. Zhang, B. Tiwana, R. Dick, Z. Qian, Z. Mao, Z. Wang,
and L. Yang, “Accurate online power estimation and automatic battery behavior based power model generation for
smartphones,” in Proceedings of International Conference on
Hardware/Software Codesign and System Synthesis, 2010.

[2] D. Pandiyan, S.-Y. Lee, and C.-J. Wu, “Performance, energy characterization and architectural implications of an
emerging mobile platform benchmark suite – MobileBench,”
in Proceedings of International Symposium on Workload
Characterization, 2013.

[19] D. Li, S. Hao, W. G. J. Halfond, and R. Govindan, “Calculating source line level energy information for android
applications,” in Proceedings of the International Symposium
on Software Testing and Analysis, 2013.

[3] R. Murmuria, J. Medsger, A. Stavrou, and J. Voas, “An
analysis of power consumption in a smartphone,” in Proceedings of International Conference on Software Security and
Reliability, 2012.

[20] D. Molka, D. Hackenberg, R. Schone, and M. S. Muller,
“Characterizing the energy consumption of data transfers and
arithmetic operations on x86-64 processors,” in Proceedings
of International Conference on Green Computing, 2010.

[4] S. Keckler, “Life after dennard and how i learned to love the
picojoule (keynote speech),” in Proceedings of International
Symposium on Microarchitecture, 2011.
[5] G. Kestor, R. Gioiosa, D. Kerbyson, and A. Hoisie, “Quantifying the energy cost of data movement in scientiﬁc applications,” in Proceedings of International Symposium on
Workload Characterization, 2013.
[6] S. Amarasinghe, M. Hall, R. Lethin, K. Pingali, D. Quinlan,
V. Sarkar, J. Shlf, R. Lucas, K. Yelick, P. Balaji, P. C.
Diniz, A. Koniges, M. Snir, and S. R. Sachs, “Report of the
workshop on exascale programming challenges,” in Technical
report, US Department of Energy, 2011.

978-1-4799-6454-3/14/$31.00 ©2014 IEEE

180

IEEE COMPUTER ARCHITECTURE LETTERS, VOL. 13, NO. 2, JULY-DECEMBER 2014

65

Architectural Thermal Energy Harvesting
Opportunities for Sustainable Computing
Carole-Jean Wu, Member, IEEE
Abstract—Increased power dissipation in computing devices has led to a sharp rise in thermal hotspots, creating thermal
runaway. To reduce the additional power requirement caused by increased temperature, current approaches apply cooling
mechanisms to remove heat or apply management techniques to avoid thermal emergencies by slowing down heat generation.
This paper proposes to tackle the heat management problem of computing platforms with a fundamentally new approach –
instead of heat removal using cooling mechanisms and heat avoidance using dynamic thermal/power management techniques,
this work investigates the mechanisms to recover wasted heat into reusable energy for sustainable computing.
Through recent advancements in thermoelectric materials, we allow wasted heat energy generated by computing devices to be
recovered, transformed, and harvested as electricity that can be directly used within the system. We demonstrate a real-system
setup where we recover 0.3 to 1 watt of power with the CPU running at 70 to 105◦ C, using a COTS thermoelectric device on top
of the CPU. Through this research, we hope to motivate more in-depth efforts to explore heat energy harvesting opportunities on
computing devices and inspire plausible solutions to overcome the technical challenges discussed in this paper.

!

1

I NTRODUCTION

Power has been the stumbling block in our path to derive
continued performance growth for modern computing systems. In modern data centers, ﬂoor space and electricity are
wasted due to a combination of power and cooling requirements. At the other end of the computing spectrum, e.g.,
laptops, tablet or smart phones, temperature and battery
life have always been the top concerns for these platforms.
The fundamental challenge of the power and energy
issue is two-fold. First, for any computing system, a rapid
increase in power dissipation results in thermal runaway – a
vicious cycle where high power dissipation creates thermal
hotspots; higher temperature leads to higher leakage power,
which heightens power consumption, creating more thermal
hotspots. Frequent thermal hotspots and high power dissipation not only degrade the overall system energy efﬁciency,
but can also affect the functional and timing correctness
of the hardware. The second challenge is in the usage
of the energy itself – energy utilization efﬁciency. As we
increase the power consumption, a large fraction of the
actual energy is wasted in the form of heat. Removing
the wasted heat from the system requires more power
for cooling, which would further lower the overall energy
efﬁciency, exacerbating the vicious cycle.
In order to address these challenges and ensure that all
transistors can operate simultaneously, function correctly,
and perform computations efﬁciently, it is important to
implement proper temperature control and energy management techniques that reduce the risk of failure due to thermal hotspots, while increasing parallelism and utilization
as a result of increased transistor counts. Prior approaches
proposed to aggressively remove the heat generated from
computations using cooling mechanisms and applying dynamic thermal management (DTM) techniques.
Removing Heat. Cooling mechanisms are commonly
applied to dissipate the heat generated by high temperatures of computing devices. Although common cooling
techniques such as mechanical fans can effectively reduce
the device temperature by removing heat, they require extra
• C. Wu is with the Department of Computer Science Engineering,
School of Computing, Informatics, Decision Systems Engineering,
Arizona State University, Tempe, AZ, 85281. carole-jean.wu@asu.edu
D ate of publication 18 Ju ne . 2013 ; date of current version 31 Dec . 2014.
Digital Object Identifier 10.1109/LL -CA.201 3 . 16

energy to just remove the already wasted heat energy. For
example, using mechanical fans to cool down the CPU temperature in the desktop setting is effective, but it requires
signiﬁcant space for the heat sink and fan installation, and
it draws some power to propel the fan system.
Avoiding Heat. In addition to devising effective cooling
mechanisms to remove heat externally, modern processors
are governed by a collection of DTM techniques to minimize the frequencies of thermal emergencies. In contrast to
removing heat, DTM techniques avoid processor core overheating by slowing down heat generation. When processor
cores are operating at a temperature higher than a prespeciﬁed temperature that may cause a potential thermal
hazard, the DTM governor triggers thermal throttling to
modulate the core frequency. Since DTM techniques address
the problem by avoiding it, the computation power of the
processor is not maximally utilized.
Distinct from the concepts of heat removal using cooling
mechanisms and heat avoidance using DTM techniques, this
paper tackles the heat management problem of computing
platforms using an entirely new approach. This work investigates the mechanisms to recover wasted heat into reusable
energy for sustainable computing. This new approach is
fueled by the following two critical observations:
Recent Advancements in Thermoelectric Material: The
ability to generate electricity from a temperature differential
was shown to be possible at a practical usage level in recent
material science breakthroughs [5]. Thepmanee et al. [7]
showed the use of thermoelectric devices to power a wireless transmitter. Other work [2], [6] have shown the use of
thermoelectric materials to harvest ambient heat energy to
supply power to body-worn electronics and environmental
sensors. The challenges faced in such application domains
are (1) the fast-changing ambient conditions that directly
introduce variations in the harvested energy, and (2) the
small amount of voltage harvested (5̃0mV) because the
temperature difference between the body and ambience for
body-wearable applications is only 1 to 2◦ C. In addition, the
energy conversion efﬁciency of thermoelectrical materials
is still relatively low, about 30%. While the efﬁciency is
seemingly small, the materials can easily operate on small
heat sources and with small temperature differential. In
particular, for small electronic applications, the material is
competitive because it is compact, simple (inexpensive), and

15 5 6-605 6 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

66

IEEE COMPUTER ARCHITECTURE LETTERS, VOL. 13, NO. 2, JULY-DECEMBER 2014

scalable. Distinct from the application domain of body-worn
electronics and low-power sensors, this paper proposes
to explore the potential of heat energy harvesting at the
system as well as at the architectural level and takes a
step further to propose mechanisms to put thermal energy
harvesting technology into practical use. While thermoelectric materials have been around and been well-studied for
decades, especially in aerospace and automobile industries
for energy harvesting, this paper is the ﬁrst that explores its
applicability in computer systems.
Thermal Characteristics of Modern Computing Systems: Studying the thermal energy distribution of computing platforms at all scales has interesting implications. At
the platform level, the temperature difference between the
hottest and the coldest components can be more than tens
of degrees [4]. Such temperature differences create a spatial
thermal gradient across a computing device. In the scale
of a processor chip in the desktop setting, the temperature
difference between the CPU chip and the fan can easily be
40◦ C. For example, when the CPU is under high loads, it
can operate at 68◦ C, resulting in a temperature difference
of about 42◦ C compared to the ambient temperature. (Data
collected on an Intel Ivy-Bridge processor.)
In light of these observations, this papers proposes the
Thermal Energy Harvesting (TEHar) framework that explores the opportunities for harvesting thermal energy from
the available spatial thermal gradients in a computing platform into reusable electrical energy by using thermoelectric
generators (TEGs). The TEG generates a voltage on a material when there is a temperature difference. This voltage can
then supply a current that can be directly used within the
computing system for charging batteries or supplementing
the power supply. Therefore, the heat generated by the
circuitry of the computing devices is not wasted but is rather
harvested for reuse. Our prototype setup demonstrates that
we are able to recover 0.3 to 1 watt of power with the
CPU running at 70 to 105◦ C, using a Commercial Offthe-Shelf (COTS) thermoelectric device on top of the CPU.
Speciﬁcally, this paper makes the following contributions:
1) We explore the potential for energy harvestability, particularly in the steep thermal gradients commonly observed in computing systems.
2) We perform real-system evaluations for the harvesting
wasted heat energy on modern computing systems.
3) We investigate applications that can reuse this recovered energy within the computing system.
The rest of the paper is organized as follows. Section 2
describes the TEHar framework in detail. Section 3 outlines
our real-system experimental setup and Section 4 highlights
and analyzes the energy harvesting experiments. We discuss
the technical difﬁculties and implications of general wasted
heat harvesting on modern computing systems in Section 5
and Section 6 concludes the paper.

2

TEH AR – T HERMAL E NERGY H ARVESTING

The fundamental goal of the TEHar framework is to explore
the usability potential of heat energy harvesting materials
for efﬁcient heat-to-electric energy conversion opportunities.
As mentioned previously, the energy harvesting techniques
proposed in this paper rely on the critical thermal characteristics – the steady-state spatial temperature difference
between the cold and hot components. In order to exploit
the spatial temperature difference, the TEHar framework
harvests the heat energy based on a 3-step process:

Hot
T+ΔT

+

-

e

+

e

e

-

e
+

e

+

-

e

p-type

n-type

−

Cold
T

Fig. 1: Seebeck effect.

Fig. 2: A typical TEG.

1) Perform comprehensive temperature and heat distribution characterization for the system.
2) Identify appropriate thermal points within the system
and apply thermoelectric devices to generate electricity
from the temperature difference.
3) Seek for native applications existing in the system to
directly utilize the harvested energy.
2.1 System-wide Temperature Characterization
In order to capture the steady-state temperature variation
characteristics in a system, the TEHar framework performs
a system-side temperature and heat characterization, taking
into account the run-time workload effects.
A steep steady-state temperature difference can exist between the coldest and the hottest components of a computing system at any time. Each component typically reaches a
certain working temperature that corresponds to how much
power it consumes. The TEHar framework measures and
records the temperature variations of the critical thermal
points on a computing system in order to determine the
optimal harvesting strategy.
For instance for a portable computing device like a laptop, the steady-state spatial temperature difference between
system components could be pronounced, since the components are more tightly-packed and the choice of cooling
mechanisms is limited. An earlier study [3] has shown
that the temperature difference between the hottest and
the coldest components on a laptop can reach 44◦ C. Our
measurement shows that, on a desktop PC, the temperature
difference between the major heat source, the CPU, and the
ambient environment can easily be more than 40◦ C. Given
the observed spatial thermal gradients on a computing platform, we next describe the technique TEHar uses to harvest
the heat energy from spatial temperature distributions.
2.2 Spatial Temperature Variation for Thermoelectric
Energy Harvesting
To exploit the spatial steady-state thermal gradients between the hottest and the coldest components, the TEHar
framework leverages the thermoelectric effect by employing
TEGs to generate electricity from the temperature difference.
The thermoelectric effect is the phenomenon where a difference in temperature creates an electric voltage difference,
and vice versa. When a thermoelectric material is applied a
voltage difference, the splitting and combination of electronhole pairs results in a temperature difference on the material, called the Peltier effect. Conversely, if the material is
subjected to a difference in temperature, a voltage difference
is created, called the Seebeck effect. We leverage the Seebeck
effect to recover the otherwise wasted heat energy on a
computing platform and transform it into electricity.
Figure 1 shows the basic element of generating electricity
from a heat source using the Seebeck thermoelectric effect. A

IEEE COMPUTER ARCHITECTURE LETTERS VOL 13 NO 2 JULY-DECEMBER 2014

)!+

	
.)+




	
.*'
	
		

	
.+'



	
.+'",'

	


)
(!+
(
'!+
'

,'
-'
(''
()'
 #$




	






	

		


	



Fig. 3: (a) Thermal-to-electrical conversion speciﬁcation and
(b) the setup of the TEG used in this paper.
thermoelectric semiconductor material is mounted between
two electrodes and generates a ﬂow of electron (hole) current when the two ends of the device are subjected to a
difference in temperature. In the case for Figure 1, one ptype and one n-type semiconductor materials are connected
in series to generate a larger thermoelectric current. Such a
Seebeck thermoelectric device is also called a thermoelectric generator (TEG), with a typical construction shown in
Figure 2. A typical construction of the TEG contains many
basic thermoelectric semiconductor elements connected in
series to generate a larger voltage and current.

3

and report the steady-state CPU temperature with the fan
turned on and off, as shown in Figure 4. To control the
experiments, we ensure that applications start execution at
an initial CPU temperature between 40 and 45◦ C.

4


E XPERIMENTAL S ETUP

The typical usage of a thermoelectric generator requires the
heat source and sink to be in contact with the top and
bottom plate of the device, respectively. The TEG module
used in this study is manufactured by Custom Thermoelectric [1] and is of the dimension 30mm (W) by 30mm
(L) by 3.75mm (H). The thermal-to-electrical conversion
speciﬁcation is given in Figure 3(a). Given a temperature
difference of 50◦ C (Tcold = 25◦ C and Thot = 80◦ C), 1W (about
1.25V) of electrical power can be harvested by this particular
TEG module. To compare the amount of electrical energy
harvested from our setup with the spec sheet numbers, we
assemble a number of stand-alone TEG modules, shown in
Figure 3(b), for our measurement.
We sandwich the TEG between two heat sinks and
produce varying temperature differences by providing a
constant resistive heat source at the hot side and attaching
a small fan at the cold side. Figure 3(a) shows the amount
of electricity harvested from our measurement compared
to the number reported by the spec sheet. By providing a
temperature difference between the constant resistive heat
source and the ambient air, our measurement shows a match
of more than 80% of the electricity reported by the spec.
Note that as the temperature of the hot side increases to
120◦ C, the temperature of the code side increases to 60◦ C.
To explore the energy harvesting opportunities, we sandwich the TEG directly between the heat source, in this
case, the CPU chip, and the CPU fan of the ATX-size
motherboard. The CPU used is an Intel Ivy-Bridge processor. To read the CPU temperature, we instrument the
hardware performance monitoring counters and log the
per-core CPU temperature readings. We then measure the
electricity generated by the temperature difference using an
auto-ranging digital multimeter.
We exercise the CPU by running SPEC2006 applications

67

E VALUATION AND R ESULT A NALYSIS

4.1 CPU temperature characterization for SPEC2006
Figure 4 shows the CPU temperature for running SPEC2006
application on one core versus on all four cores (one application instance on each core) with the two different fan
conﬁgurations. Fan on represents a computing platform with
advanced cooling capabilities whereas Fan Off simulates the
scenario where no cooling mechanisms are available, e.g.,
laptops and mobile smartphones. The CPU in a system
with advanced cooling mechanisms stays cool even when
the computation load increases. When comparing the temperature under light load (1-Core; Fan On) with that under
heavy load (4-Core; Fan On), we observe an increase of
8.5◦ C (from 56◦ C to 64.5◦ C). In contrast, for systems without
cooling features, the CPU often operates at a much higher
temperature (from 77◦ C under light loads, 1-Core; Fan Off to
100.5◦ C under heavy loads, 4-Core; Fan Off). This suggests
a signiﬁcant energy harvesting opportunity.
4.2 Harvested energy and thermal applications
With our TEHar setup, an average of 0.5V (0.3W) of electrical energy is harvested for the 1-core workloads (an average
of 1.2V and 1.05W for 4-core). Figure 5 shows the amount of
electricity harvested under the various SPEC2006 workloads
by layering the TEG between the CPU and the heat sink.
Our current experimental result shows that, while being
able to harvest the otherwise wasted heat into reusable
electricity, we increase the CPU temperature at the same
time. For the 1-core workloads, we observe an increase
of 14◦ C (from 56◦ C to 70◦ C) for the CPU when the TEG
is applied, while the CPU still operates well below the
allowed temperature limit (under 105◦ C). A higher temperature increase of 20 to 25◦ C is observed for the 4-core
workloads. While the CPU occasionally operates at near
105◦ C for the 4-core workloads (i.e., h264ref, libquantum,
and sjeng), the amount of electrical energy harvested is
signiﬁcantly increased. This implies that the decision for
optimal TEG placement does not simply depend on the CPU
operating temperature – the temperature increase affects
the placement decision as well. To avoid having the CPU
running near its temperature limit, our experiments show
that the optimal TEG placement is at the chip location where the
operating temperature is at 60 to 75◦ C.
To demonstrate the usability of the electrical energy, we
power a 5.5in portable fan, that runs on two 1.5V batteries,
with the energy harvested from our TEG modules. we
connect three TEG modules (the stand-alone module shown
in Figure 3(b)) in series to boost the voltage to a sufﬁcient
level and let the fan run on the electricity harvested from
the TEG modules with a temperature difference of 50◦ C.

5

D ISCUSSION

5.1 Reliability Impact
As mentioned before, the current setup of TEHar increases
chip temperature which affects IC failure rate. The relationship between IC failure rates, time, and temperature
has been well-established and can be represented by the
Arrhenius Model which assumes the degradation is linear
with time and that the mean time between failure (MTBF)
is a function of temperature stress. For example, for the

IEEE COMPUTER ARCHITECTURE LETTERS, VOL. 13, NO. 2, JULY-DECEMBER 2014

	




68







	

	

	

	


Fig. 4: The CPU temperature when running a variety of SPEC2006 benchmarks with fan on/off conﬁgurations.

*$.

/*
.*

*$-

-*

*$,

,*
*

*$/



	
	

+**

	
&+%#
'
	
&+%#
#	'
!

&+%#
#	'



",


,.-













	

+,*

*

Fig. 5: CPU temperature vs. the harvested energy from the
thermoelectric device when running SPEC2006 benchmarks.

high temperature (compared to the ambient environment)
and, therefore, are good candidates for TEHar. To leverage
all usable heat generated as a result of computations, we
should integrate TEGs at all levels. While the energy harvesting design at the CPU (and GPU) level requires more
thought because of potential reliability concerns discussed
here, the application at the PSU and hard disk is much
simpler. With the system-level integration of the TEHar
framework across components, we envision a novel selfsustainable cooling mechanism. For example, when the
heat generating sources, e.g., the CPU, PSU, or GPU, are
operating at a high enough temperature, the TEG modules
deployed on a system will be able to generate enough
usable electricity to activate additional cooling devices.
Consequently, the system temperature can be maintained
without cooling, making the system self-sustainable.

6
0.4eV device, a 30 degree rise in junction temperature, say
from 90 to 120◦ C, results in a 2 to 1 increase in failure rate,
whereas for the 0.7dV device, this 30 degree rise results in a
4 to 1 increase in failure rate. However, such the Arrhenius
Model-based failure prediction is often too conservative; in
practice, most failures are not activated or accelerated in a
constant rate by any temperature increase.
Despite the increase in MTBF, the semiconductor industry
has developed a defecto standards based on the device
package type. These have been well accepted as numbers
that relate to reasonable device lifetimes, thus MTBF. For
the Intel Ivy Bridge processor in our experimental system,
the maximum junction temperature is speciﬁed to be 105◦ C.
As discussed in Section 4.2, for our 1-core workloads, the
CPU temperature increase is from 56◦ Cto 70◦ C, which is still
well under the maximum allowed operating temperature
limit. Therefore, while the CPU MTBF is increased, the
operating temperature with the TEG layer is well within
the reasonable operational range. While with the 4-core
workloads, the CPU temperature becomes higher but is still
below the allowed maximum 105◦ C.
5.2 System-Level Thermal Energy Harvesting
There are many other heat sources within a computing
system that can be leveraged for a larger aggregate electrical
energy, other than the CPU. For example, the graphics card
(e.g., nVidia Quadro NVS) typically runs at 45 to 50◦ C when
idle and at 75 to 90◦ C under gaming loads. This operating
temperature range presents another potential opportunity
for thermal energy harvesting on a system.
The power supply unit (PSU) of a computing system is
often another large heat source. Typically when the CPU is
in an idle state, the PSU runs at 45 to 50◦ C. When the system
is under load, e.g., the CPU operates at 100% utilization,
more power is drawn and the PSU could easily run at 70◦ C
(with the maximum allowed temperature at 85 to 100◦ C).
These system components often operate at a relatively

C ONCLUSION AND F UTURE W ORK

While the concept of thermoelectricity has been established
for quite some time, their potential in the modern computing domain have yet to be realized. This paper explores the
heat energy harvestability of modern computing systems,
using COTS TEGs. We have shown that with a single TEG,
we can recover the otherwise wasted heat from the CPU
to signiﬁcant electrical energy on a real-system. We also
demonstrate that with three TEG modules, the harvested
electrical energy is signiﬁcant enough to power a fan. The
harvested electrical energy of 1W per TEG may seem small
on desktop PC but is large on other platforms, e.g., mobile
handheld devices and wearable electronics. This paper also
shows that while current real-system setup could induce
a thermal transfer bottleneck, a thoughtful TEG placement
can eliminate this possibility. To allow for more advanced
system integration for TEHar, we will explore customized
TEG and the placement of the TEG at the functional unit
or other microarchitectural components that operate at the
optimal temperature range for thermal energy harvesting.

R EFERENCES
[1] Custom Thermoelectric. www.customthermoelectric.com/.
[2] E. Carlson, K. Strunz, and B. Otis. 20mV input boost converter
for thermoelectric energy harvesting. In IEEE Symp. VLSI
Circuits Dig. Tech. Papers, 2009.
[3] E. Dallago and G. Venchi. Thermal characterization of compact
electronic systems: a portable PC as a study case. IEEE Trans.
on Power Electronics, 2002.
[4] N. El-Sayed and et al. Temperature management in data centers:
why some (might) like it hot. In ACM Conf. on Measurement and
Modeling of Computer Systems, 2012.
[5] C. A. Hewitt and et al. Multilayered carbon nanotube/polymer
composite based thermoelectric fabrics. Nano Letters, 2012.
[6] Y. Ramadass and A. Chandrakasan. A batteryless thermoelectric
energy-harvesting interface circuit with 35mV startup voltage.
In IEEE Solid-State Circuits Conference, 2010.
[7] T. Thepmanee, P. Julsereewong, and N. Taratanaphol. Wasteheat thermoelectric power source for industrial wireless transmitters. In IEEE Conf. on Electrical Engineering/Electronics Computer Telecommunications and Information Technology, 2010.

CAWA: Coordinated Warp Scheduling and Cache Prioritization for Critical Warp
Acceleration of GPGPU Workloads
Shin-Ying Lee Akhil Arunkumar Carole-Jean Wu
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University
{lee.shin-ying,akhil.arunkumar,carole-jean.wu}@asu.edu

Abstract

for executing general purpose parallel workloads (GPGPU).
Modern GPU architecture designs are based on the Single Instruction Multiple Thread (SIMT) computing paradigm where
multiple threads are grouped together to form a warp or wavefront. Threads within a warp are mapped to a vector functional
unit or Single Instruction Multiple Data (SIMD) unit such that
all threads execute the same instruction, but with different data,
simultaneously [22]. Furthermore, the concept of ﬁne-grained
multi-threading is implemented in modern GPUs by enabling
a number of warps to be executed in a GPU core in the timemultiplexing manner. For example, NVIDIA’s Maxwell GPUs
support up to 64 concurrent warps (equivalent to 2048 threads)
within a GPU streaming multiprocessor (SM) [27].
GPUs are able to hide the stall latency from a warp with
useful instruction execution from another warp through fast
context-switching—whenever the execution of a warp is
stalled, it is swapped out and another warp will be swapped in
for immediate execution to increase resource utilization. However, it has been recently shown that the latency hiding ability
of GPUs has signiﬁcant room for improvement [21]. The limited latency hiding ability of GPU schedulers is commonly
caused by imbalanced workloads in parallel warps [20], diverging branch behavior [9, 10, 23, 32], irregular memory access
patterns [4, 28], and shared cache contention [24, 34]. This
could result in a signiﬁcant execution time disparity between
different parallel warps in many GPGPU applications—the
warp criticality problem. All warps within a thread-block
have to ﬁnish execution before the particular thread-block can
commit and another thread-block can be dispatched to the
SM. A recent prior work has performed a case study for an
implementation of the breadth-ﬁrst-search (bfs) algorithm,
demonstrating the signiﬁcant warp execution time gap between
the fastest and the slowest (critical) warps to be over 50% [20].
If oracle knowledge of warp criticality were available, the
Criticality-Aware Warp Scheduler (CAWS) could prioritize
the critical warps over other faster running warps, resulting in
21% performance improvement. However, the CAWS requires
oracle warp criticality information; therefore, their proposed
solution is not practical and cannot adapt to runtime behavior.
To address the warp criticality problem, this paper proposes
a runtime Coordinated criticality-Aware Warp Acceleration
(CAWA) solution to reduce the execution time disparity between parallel warps. CAWA manages shared compute and
memory resources on a GPU in order to accelerate the execution of slower-running warps. The proposed CAWA design

The ubiquity of graphics processing unit (GPU) architectures has made them efﬁcient alternatives to chipmultiprocessors for parallel workloads. GPUs achieve superior performance by making use of massive multi-threading
and fast context-switching to hide pipeline stalls and memory access latency. However, recent characterization results
have shown that general purpose GPU (GPGPU) applications
commonly encounter long stall latencies that cannot be easily
hidden with the large number of concurrent threads/warps.
This results in varying execution time disparity between different parallel warps, hurting the overall performance of GPUs –
the warp criticality problem.
To tackle the warp criticality problem, we propose a coordinated solution, criticality-aware warp acceleration (CAWA),
that efﬁciently manages compute and memory resources to
accelerate the critical warp execution. Speciﬁcally, we design
(1) an instruction-based and stall-based criticality predictor
to identify the critical warp in a thread-block, (2) a criticalityaware warp scheduler that preferentially allocates more time
resources to the critical warp, and (3) a criticality-aware
cache reuse predictor that assists critical warp acceleration
by retaining latency-critical and useful cache blocks in the
L1 data cache. CAWA targets to remove the signiﬁcant execution time disparity in order to improve resource utilization for
GPGPU workloads. Our evaluation results show that, under
the proposed coordinated scheduler and cache prioritization
management scheme, the performance of the GPGPU workloads can be improved by 23% while other state-of-the-art
schedulers, GTO and 2-level schedulers, improve performance
by 16% and -2% respectively.

1. Introduction
Graphics Processing Units (GPUs) have recently become an
efﬁcient alternative to traditional chip-multiprocessors (CMPs)
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from Permissions@acm.org.
ISCA’15, June 13-17, 2015, Portland, OR, USA
Copyright 2015 ACM 978-1-4503-3402-0/15/06 ...$15.00
http://dx.doi.org/10.1145/2749469.2750418

515

consists of three main components:
• Dynamic Warp Criticality Prediction with CAWACPL :
To identify critical warps on the ﬂy, we design the
instruction-based and stall-based criticality prediction
logic (CPL). CAWACPL ’s warp criticality prediction outcome is used to guide shared resource prioritization between the faster and slower-running warps at runtime by
the criticality-aware warp scheduler and criticality-aware
cache management (Section 3.1).
• Greedy Criticality-Aware Warp Scheduler with
CAWAgCAW S : Building on top of two existing warp
scheduling policies [20, 34], we propose a greedy
Criticality-Aware Warp Scheduler (gCAWS) to preferentially allocate more time resources to slower-running
warps for critical warp acceleration at the scheduling
level (Section 3.2).
• Criticality-Aware
Cache
Prioritization
with
CAWACACP : To further accelerate the execution of
critical warps, we propose a Criticality-Aware Cache
Prioritization (CACP) scheme, which consists of a
Critical Cache Block Predictor that predicts whether
an incoming cache block should be inserted into the
critical cache partition reserved for critical data in the
L1 data cache and a modiﬁed Signature-based Cache
Hit Predictor [38] that predicts the reuse pattern of an
incoming cache block. CAWACACP proactively predicts
and retains data that is performance-critical with higher
priorities—data useful to slower-running, critical warps
and data that exhibits locality (Section 3.3).
The main concept of CAWA design is to manage both compute resources at the warp scheduler’s level and memory resources at the L1 data caches to accelerate the execution of
critical warps. By doing so, the large execution time gap
between the fastest-running and the slowest-running warps
in a thread block is signiﬁcantly reduced leading to a much
reduced synchronization overhead in the massively-parallel
GPU architecture. Our evaluation results show that the performance of target GPGPU applications can be improved by an
average of 23% and by as much as 3.13 times for kmeans.
In summary, this paper makes the following contributions:
• We present an in-depth characterization to quantify the
degree of warp criticality caused by workload imbalance,
diverging branch behavior, memory subsystem behavior,
and the warp scheduler for GPGPU workloads.
• We propose a coordinated warp scheduling and cache prioritization scheme to accelerate the execution of critical
warps in GPGPU workloads. This is the ﬁrst work that
offers a practical solution for the warp criticality problem
with an accurate warp criticality prediction technique as
well as a criticality-aware warp scheduler and a cache
prioritization scheme.
The rest of the paper is organized as follows. Section 2
gives an overview of the warp criticality problem in GPGPUs
and presents in-depth performance characterization results
for understanding the sources of warp criticality. We present
the proposed coordinated criticality-aware warp acceleration

design in Section 3. Next, we show the experimental methodology in Section 4, followed by the evaluation results and
analysis in Section 5. Section 6 presents related work in this
area and Section 7 concludes the paper.

2. Background and Motivation
2.1. Background on GPU Architecture
A modern uniﬁed GPU consists of multiple streamingmultiprocessors (SMs) or computation units (CUs). An SM
is similar to a Single Instruction Set Multiple Data (SIMD)
processor, having one or multiple vector functional units, data
caches, instruction caches, and instruction fetch/decode units.
An SM also has a warp pool to store the context of all running threads. It implements a hardware warp scheduler that
dispatches and allocates resources for thread execution, and
has a large register ﬁle which is shared across all threads and
accommodates the private data of all threads.
A GPGPU application comprises massive number of threads
that process the same piece of code named a kernel. At
run time, multiple threads form a thread-block or a cooperative thread array (CTA). A thread-block is the basic unit
of GPGPU application execution. All threads within a threadblock have the same life-cycle and are dispatched onto as well
as swapped out from an SM at the same time. Depending on
the width of the vector functional units/SIMD lanes, threads
within a thread-block are also grouped into small batches (typically 32 or 64 threads together) called a warp or a wavefront.
At every cycle, the hardware warp scheduler selects a ready
warp—a warp that does not stall due to factors such as cache
misses or pipeline hazards—from the warp pool for execution.
With this massive multi-threading paradigm and the large
register ﬁle design, whenever a thread/warp stalls, GPUs are
able to quickly perform context-switching to hide the execution latency with minimal penalty. Hence, modern GPUs
are able to achieve a higher average pipeline utilization and
throughput compared to traditional CMP designs.
2.2. The Warp Criticality Problem
Warps within a thread-block are tightly coordinated. They are
bound to the same synchronization barrier (either explicit or
implicit) and have the same life-cycle. That is, warps from
the same thread-block must start execution at the same time,
be blocked at a synchronization barrier until all warps have
arrived, and wait at the kernel exit point until all warps have
ﬁnished execution. Nevertheless, not all warps ﬁnish their
tasks at the same time. There is execution time disparity
observed among warps [20]. Consequently, fast warps are idle
at a synchronization barrier or at the end of kernel execution
until the slowest warp arrives. This fact results in two problems
which degrade a GPU’s overall throughput.
• A thread-block’s execution time is dominated by the slowest warp or the so-called critical warp. Although fast
warps have ﬁnished all the tasks, they are suspended at an
explicit (e.g., an synchronization instruction) or implicit
(e.g., kernel exit point) synchronization barrier without

516

Execution Time Breakdown
(The Fastest Warp Normalized to
the Critical Warp)

Computation

Algorithm 1 bfs searching algorithm
1: function BFS (w)
 w: node (thread/warp)
2:
while notYetVisitedAllNeighbors do
3:
n ← nextNode
4:
if n.hasNotBeenVisited then
 a child node
5:
n.Cost ← w.Cost
6:
n.hasNotBeenVisited ← False
7:
w.nChild ← w.nChild + 1
8:
else
 a non-Child node
9:
w.nNonChild ← w.nNonChild + 1
10:
end if
11:
end while
 kernel exit point/implicit barrier
12: end function

Idle Time

100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%

Figure 1: Warp execution time disparity across different
GPGPU applications. The ﬁgure shows the highest warp execution time disparity across thread blocks.

waiting time for the fastest running warp.
2.2.2. Diverging Branch Behavior In addition to the unbalanced workload scenario for parallel warps in a thread block,
diverging branch behavior could also cause varying execution
time for warps. At runtime, warps can undergo different control paths leading to different number of dynamic instructions
across different warps. This problem could be worsened if
threads in a warp also take diverging control paths, i.e., the
branch divergence problem, leading to a larger instruction
execution gap between warps. Prior studies [9, 10, 23, 32]
showed that the branch divergence problem can signiﬁcantly
degrade the performance of GPGPU applications.
To remove the workload imbalance effect and focus on the
impact of the diverging branch behavior, we modify the data
input provided to bfs to represent a balanced tree. Figure 2(b)
shows the warp execution time for the same thread-block with
a balanced workload. While the computation workload is
equally distributed across warps, we observe varying warp
execution time. The execution time difference between the
fastest and the slowest warps is signiﬁcant 40%. This is because while a node traverses through its neighbors in the input
graph, only the data stored in child nodes (nodes that have
not yet been visited) need to be processed. Visiting child and
non-child nodes (nodes that have been visited before) fall onto
different if-else blocks (lines 4–10 in Algorithm 1). This introduces a varying number of per-warp dynamic instruction
counts, since some warps will execute the taken path while
others execute the not-taken path.
In the worst-case scenario, when thread-level branch divergence occurs [23, 32], instructions in both the taken and
not-taken paths need to be executed in some warps resulting in
a higher number of dynamic instruction executed, while other
warps only have to execute one of the two paths. The dynamic
instruction count disparity between warps could be as high as
20% (number of instructions) in bfs as illustrated by the red
curve in Figure 2(b).
2.2.3. Contention in the Memory Subsystem Hardware resource contention, particularly in the memory subsystem,
can exacerbate the warp criticality problem. Jog et al. observed that the memory subsystem has a signiﬁcant impact
on GPGPU applications [15, 16]. Poor data alignment and

doing any useful computation. As a result, it wastes lots
of hardware resources such as the registers.
• When fast warps have ﬁnished execution and are in the
idle state, the number of active warps decreases. In such
a scenario, the GPU may not have sufﬁcient warps to hide
the execution latency. When a warp stalls, its execution
latency will be exposed and thereby the GPU cannot
achieve the maximum throughput.
We call this sub-optimal performance problem as the warp
criticality problem. To highlight the importance of the warp
criticality problem in GPGPU applications, we evaluate the
execution time difference between the fastest and the slowest
running warps in a thread-block. Figure 1 shows that the average execution time difference across a number of GPGPU
applications is 45% and the difference could be as high as
70% (srad_1). Such signiﬁcant execution time disparity between parallel warps is caused by four factors—(1) workload
imbalance, (2) diverging branch behavior, (3) contention in
the memory subsystem, and (4) warp scheduling order. We
use bfs as an example application to illustrate the degree of
warp criticality for each factor.
2.2.1. Workload Imbalance In a GPGPU kernel function,
tasks are not always uniformly distributed to each thread/warp,
and thereby some threads/warps have heavier workloads than
others. Intuitively, the threads/warps with heavier workloads
require longer time to process their tasks. Consequently,
warps with heavier workloads often become the slowestrunning/critical warps.
In bfs, workload imbalance comes from building and
traversing an unbalanced tree data structure. Each node has to
traverse all of its neighboring nodes to build a tree (line 2 in
Algorithm 1). Depending on data inputs, the number of child
nodes of a particular node mapped to a thread/warp could
vary, resulting in different per-warp workloads. Figure 2(a)
shows the per-warp execution time for all warps in a particular
thread-block (Thread-Block 2 on SM 3) sorted based on warp
execution time. The execution time disparity between the
fastest and the slowest running warps is approximately 20%
of the fastest warp’s execution time, leading to a signiﬁcant

517

	
!

"



!







$

"

#



!

$





"

!

!

%


		
	
	
	





#



"









	


$

 







!



$



#

!

%



%"

!


$



(c)

#



 

#

"

!

!



		
	



"

(b)

"









	


"







	


(a)

 

	
!

"

	
!

"

	

		

Figure 3: Reuse distance analysis for the critical warps in bfs.

warp scheduling design can introduce extra stall cycles which
signiﬁcantly reduce the performance of GPUs. Jia et al. also
pointed out that interference in the L1 data cache as well as
in the interconnection between the L1 data caches and the
L2 cache are the major factors that limit GPU performance
[13, 14]. This is because the data cache capacity and the
memory bandwidth are scarce resources servicing the memory
demand of the massively parallel GPUs.
The cache interference, in particular in the L1 data caches,
could worsen the warp criticality problem. Figure 2(c) shows
the portion of a warp’s execution time caused by delays in the
memory subsystem. We can observe that the slower-running
warps often also experience higher memory access latencies.
Figure 3 shows the reuse distance analysis of critical warp
cache lines1 . More than 60% of the cache blocks that could be
reused by the slower-running, critical warps are evicted before
the re-references by the critical warps. This is caused by the
interference between critical and non-critical cache blocks in
the L1 data cache.
2.2.4. Latency Introduced by the Warp Scheduler The execution of warps can experience additional delay in the warp
scheduler. Because of the particular warp execution order
determined by the scheduler, when a warp becomes ready for
1 Data

is based on an L1 data cache of 16KB, 4-way set-associative, 128B
cache block.





































	






	














	
	










	




	




	




	
	





	


	
	

Figure 2: Warp execution time disparity caused by (a) workload imbalance, (b) diverging branch behavior, and (c) additional
memory subsystem delay for bfs.

	

	 

Figure 4: Warp scheduler could introduce additional stall cycles to warps.

execution, it can experience up to N cycles of scheduling delay,
where N represents the number of warps. State-of-the-art warp
scheduling policies are criticality-oblivious and introduce additional delay that could further degrade the performance of
critical warps. As Figure 4 shows, scheduling policies, such
as the baseline round robin scheduler, can contribute as much
as 52.4% additional wait time for the critical warp.

3. CAWA: Coordinated Criticality-Aware Warp
Acceleration
We propose a coordinated warp scheduling and cache prioritization solution, called Coordinated criticality-Aware Warp
Acceleration (CAWA). CAWA consists of three major components: (1) a dynamic warp criticality predictor, (2) a greedy
criticality-aware warp scheduler, and (3) a criticality-aware
cache prioritization technique. Figure 5 illustrates the different components in CAWA in the context of a modern GPU
pipeline (NVIDIA Fermi architecture).
CAWA identiﬁes slower-running warps that have a
high likelihood to become critical warps at runtime
(CAWACPL ). CAWA allocates more compute resources to the
predicted-to-be-critical warps with higher scheduling priorities (CAWAgCAW S ). This alleviates the dynamic workload
imbalance-caused warp criticality as well as the additional

CAWACPL
CAWAgCAWS
[Section 3.1] [Section
[
3.2]]

.
.
.

Warp
p0
Warp
p1
Warp
p2
..

...

...

IInst.
nst.
t
Reg.
Buffers
s Files

Warp
p (N-1)
((N-1))

L1 Data Cache

Scoreboard

Warp Pool

CAWACACP

Inst. Decoder

Fetch Unit

L1 Inst. Cache

Streaming Multiprocessor (SM)
Streaming Multiprocessor (SM)
Streaming Multiprocessor (SM)
SM)
SIMD Lanes
an (adder))
.. .
SIMD Lanes (mult.))
.. .
SIMD Lanes (divider
r)
(divider)
.. .

CAWACACP
[Section 3.3]
Fig. 7

To L2 Cache

..

Common Data Bus

Memory Ports

Figure 5: Modern GPU Architecture with CAWA.

Algorithm 2 An example of the instruction-based CPL.
1: [PC1 ] $L0 : @p0 bra $L2  jump to PC4 if p0 is true
(ΔInst = PC4 - PC1 + 1)
2: [PC2 ] $L1 : add.u64 %r1, %r1, %1
 jump to PC5
(ΔInst = PC5 - PC1 + 1)
3: [PC3 ]
bra $L3
4: [PC4 ] $L2 : sub.u64 %r1, %r1, %1
5: [PC5 ] $L3 : mov.u64 %r2, %r1, %r2

if(n.hasNotBeenVisited)
Branch path Per-warp
inst-1
#inst.
…
Thread
m+n
Branch
inst-m
Divergence:
else
Yes
inst-1
not-taken
m
Thread
…
Branch
inst-n
Divergence:
taken

n

No

Figure 6: The instruction count disparity caused by branch.

Depending on the values of m and n, even without branch
divergence, warps could face a signiﬁcantly different amount
of instructions for execution. This could translate to diverging
warp execution time.
Based on the branch outcome, CPL updates the per-warp
criticality counter accordingly with the inferred size of the
basic block determined with the current branch instruction
pointer (currPC) and the target instruction pointer (nextPC).
By doing so, CPL would increment or decrement the per-warp
criticality counter. In addition to the branch outcomes, CPL
also decrements the criticality counter whenever an instruction
is committed in order to balance the execution progress.
Algorithm 2 demonstrates an example of how the
instruction-based CPL works. If branch divergence occurs
at PC1 for a particular warp, the warp has to run through all
three instructions (PC2 , PC3 , and PC4 ). On the other hand,
there is no branch divergence and depending on the branch outcome, the warp will execute either one (PC4 ) or two (PC2 and
PC3 ) instructions. After executing PC1 , the target address become available and is used to calculate the additional dynamic
instructions per-warp by CPL.
Stall Latency from Shared Resource Contention and
Scheduling In addition to updating the per-warp criticality
counters based on dynamic execution progress, CPL records
additional stall latencies experienced due to shared resource
contention as well as scheduler delays, while updating the
criticality counter. CPL monitors the stall cycles between the
current and the next instruction execution for each warp and
increment the criticality counter accordingly for all warps. Algorithm 3 presents the criticality counter update mechanism
based on stall cycles in CPL, where the stallCycle represents
the total stall time between executing two consecutive instructions.

scheduling delay imposed onto the critical warps.
In addition to reducing critical warp execution time by allocating more compute resources, CAWA also proactively
reserves a certain amount of cache capacity for data that is
useful to critical warps (CAWACACP ). By doing so, CAWA ensures a certain degree of performance guarantee for the critical
warps by reducing the amount of long latency cache misses
experienced by the critical warps. Next, we present the three
major components in CAWA in detail.
3.1. Critical Warp Identiﬁcation with Criticality
Prediction Logic (CPL)
To identify critical warps at runtime, we develop a criticality
prediction logic (CPL) to monitor the execution progress of
individual warps in the scheduler’s pool by implementing a
criticality counter per warp. The per-warp criticality counter
represents the execution progress of each warp and is updated
based on (1) the degree of instruction count disparity caused
by diverging branch behavior, and (2) stall latencies caused by
shared resource contention. The per-warp criticality counters
are used by our proposed scheduler and cache prioritization
schemes for critical warp acceleration.
Dynamic Instruction Count Disparity To consider the
inﬂuence of workload imbalance and diverging branch behavior on warp criticality, we design CPL to update per-warp
criticality counters based on the number of instructions in
the executed branch path. Figure 6 shows a simple example
to highlight the possibility of diverging dynamic instruction
counts per warp based on the branch path behavior. Warps
that experience thread-level branch divergence will have to
execute m+n number of instructions while other warps will execute either m or n instructions based on the branch outcome.

519

(CCBP)
Critical Cache
Block
ock Predictor
Pre
(SHiP)
Signature Cache
Hit P
Pred
Predictor

tag

data

Evicted Cache Line
Critical Partition?

Evicted Cache Line
Reuse?

recency_cnt;
c_reuse;
nc_reuse;
signature;
...

...

...

...

Critical Cache Ways

Non-Critical Cache Ways

L1 Data Cache

Figure 7: Criticality-Aware Cache Prioritization.
Reuse Critical Line

40%
20%

PC 1

PC 2

PC 3

PC 4

PC 5

16KB L1D $

256KB L1D $

16KB L1D $

256KB L1D $

16KB L1D $

256KB L1D $

16KB L1D $

256KB L1D $

0%

16KB L1D $

3.2. greedy Criticality-Aware Warp Scheduler (gCAWS)
With the critical warp identiﬁed by CPL, gCAWS is designed
to give more compute resources to critical warps by prioritizing the execution of critical warps over other warps and
by providing a larger time slice to warps in a greedy manner. gCAWS combines the strengths of the state-of-the-art
schedulers–Greedy-Then-Oldest (GTO) [34] and CriticalityAware Warp Scheduling (CAWS) [20]. At each cycle, gCAWS
selects a ready warp for execution based on the degree of warp
criticality determined by the per-warp criticality counter in
CPL. If there are multiple warps having the same criticality,
the warp scheduler will selects the oldest one based on the
GTO algorithm. Then gCAWS greedily executes instructions
from the selected critical warp until this particular warp has no
further available instructions. Consequently, the critical warp
not only receives a higher scheduling priority but also beneﬁts
from a larger time slice.

60%

256KB L1D $

where nCriticality represents the value of the per-warp criticality counter, nInst represents the relative instruction count
disparity between the parallel warps, w.CPIavg represents the
per-warp average CPI, and nStall is the stall cycles incurred
by shared resource contention and the scheduler.

No Reuse

80%

16KB L1D $

(1)

Reuse Non Critical Line

100%

256KB L1D $

Out of All Cache Block Insertions

Summary for CPL Update The per-warp criticality
counter is updated as follows:
nCriticality = nInst ∗ w.CPIavg + nStall;

setIndex

Criticality-Aware Cache
Prioritization (CACP)

Algorithm 3 The criticality counter with stall cycles.
1: function WARP S CHEDULER
 select a ready warp to
execute
2:
while NotVisitedAllWarps do  select warps in the
order based on the scheduling policy
3:
w ← f indNextWarp()
4:
if w.isRready() then
 stallCycles = total stall
time between two consecutive instructions.
5:
w.nStall ← w.nStall + stallCycles
6:
InstructionExecute(w)
7:
break
8:
end if
9:
end while
10: end function

PC 6

Figure 8: Reuse behavior of different PCs for bfs.

cache block predictor, and retains these critical cache lines
in the cache partition reserved for critical warps. Figure 7
shows the proposed CACP scheme and the interface between
the different components and the L1 data cache.
Cache Partitioning for Cache Block Criticality CACP
partitions the L1 data cache into two parts in the granularity
of ways: critical cache ways and non-critical cache ways.
The number of ways dedicated to critical versus non-critical
cache blocks are determined through experimental analysis
of various benchmarks. Through sensitivity analysis, we ﬁnd
that we achieve the best overall performance when 8 out of 16
ways are dedicated to critical cache blocks. A design similar
to [31] can be integrated to dynamically tune the size of the
critical and non-critical cache partitions based on the run-time
needs of an application.
Critical Cache Block Predictor (CCBP) for L1 D-Cache
Based on the unique characteristics of GPGPU—relatively
small instruction footprint but diverse reuse behavior—we
carefully design a Critical Cache Block Predictor (CCBP) to
differentiate critical cache lines from non-critical cache lines
such that an incoming cache block will either be inserted in the
cache partition reserved for critical or non-critical cache lines.
Figure 8 shows that there are six memory instructions in this
particular bfs kernel, which all warps execute. The left bar

3.3. Criticality-Aware Cache Prioritization (CACP)
In addition to allowing the critical warps to access pipeline
resources more often and for a longer time duration, CriticalityAware Cache Prioritization (CACP) is designed to allocate a
certain ﬁxed amount of the L1 data cache capacity to data that
will be used by the critical warps for performance guarantee.
The insight which CACP builds upon is that not all cache lines
have equal importance—Data that will be used by critical
warps is latency-critical and should be treated with higher
priority at the L1 data cache. To do so, among all incoming
cache lines, CACP ﬁrst predicts critical cache lines—cache
lines that will be used by critical warps—with the critical

520

Algorithm 4 The CACP pseudo code
1: function C ACHE F ILL(req)  select partition and insert
cache line
2:
line.signature ← (req.pcXORreq.addr)
3:
if CCBP[line.signature] > T hreshold then

predicted to be critical line
4:
L1DCache.CriticalPartition.insert(line)
5:
L1DCache.CriticalPartition.setInsertion
Position(line)
 set SHiP insertion position
6:
else
 predicted to be non-critical line
7:
L1DCache.NonCriticalPartition.insert(line)
8:
L1DCache.NonCriticalPartition.setInsertion
Position(line)
 set SHiP insertion position
9:
end if
10: end function
11: function C ACHE H IT (req)
12:
if InCriticalPartition(line) then
13:
L1DCache.CriticalPartition.setPromotion
Position(line)
 set SRRIP promotion position
14:
else
15:
L1DCache.NonCriticalPartition.setPromotion
Position(line)
 set SRRIP promotion position
16:
end if
17:
if IsCriticalWarp(req.WarpID) then
 Correct
prediction; increment CCBP
18:
line.c_reuse ← true
19:
CCBP[line.signature] + +
20:
SHiP[line.signature] + +
21:
else
 Hit is from non-critical warp
22:
line.nc_reuse ← true
23:
SHiP[line.signature] + +
24:
end if
25: end function
26: function E VICT L INE(line)
27:
if (line.c_reuse = f alse) AND (line.nc_reuse =
true) AND (L1DCache.Partition = CriticalPartition)
then
 Incorrect prediction, line should have been
inserted in Non-Critical Partition; decrement CCBP
28:
CCBP[line.signature] − −
29:
else if (line.c_reuse = f alse AND line.nc_reuse =
f alse) then
 No reuse from this signature
30:
SHiP[line.signature] − −
31:
end if
32: end function

within each memory instruction represents the reuse pattern
of critical cache blocks in a 256KB data cache whereas the
right bar represents the reuse pattern of critical cache blocks
in the baseline 16KB data cache. First, when the cache is
large enough, cached blocks have a high likelihood of reuse
by both critical or non-critical warps. However, the L1 data
cache in GPUs are often too small to accommodate the active
working set of the entire application. The second observation
from Figure 8 is that the reuse patterns for the various memory
instructions are different. For instance, the majority of the
cache blocks brought by PC-5 never receive any reuse before
being evicted from the cache. With the two key observations,
we will design CCBP that learns the reuse patterns for cache
blocks based on the insertion instructions and predicts which
cache blocks will be reused by critical warps at runtime.
CCBP is built upon the idea of the signature-based cache hit
predictor (SHiP) that was originally proposed for the last-level
CMP cache [38]. CCBP learns whether an incoming cache
line will be used by critical warps or not based on a signature. We design the signature for CCBP to be a combination
of instruction program counters (PCs) and memory address
regions as the two pieces of information have been shown to
be useful in learning and correlating cache block reuse patterns [17, 18, 19, 38]. A signature is formed by xor-ing the
lower 8 bits of an instruction PC and the lower 8 bits of the
memory address, and is used to index into the CCBP which is
a simple array of 2-bit saturating counters. The algorithm for
CCBP is outlined in Algorithm 4 in detail.
Modiﬁed Signature-based Cache Hit Predictor In addition to CCBP, CACP includes an additional signature-based
cache hit predictor (SHiP) that learns and predicts the reuse
pattern of any incoming cache blocks based on the same signature used for CCBP. The outcome of SHiP is used to guide
the insertion position of the cache block. For example, if a
2-bit re-reference interval prediction replacement policy is
used [12], the outcome of SHiP will guide a cache block insertion position to be in the long (re-reference prediction value
= 2) versus in the distance (re-reference prediction value = 3)
re-reference prediction.
CACP Summary To protect the critical warp data from
getting thrashed in the L1 data cache, we partition the L1
data cache into critical cache partition and non critical cache
partition. We capture the limited but important re-references
available in the L1 data cache by designing two predictors,
CCBP, and SHiP. Both CCBP and SHiP are indexed using signatures formed using PC information augmented with memory
address region information.
CCBP plays the crucial role of identifying cache lines that
will be reused by critical warps and inserts these cache lines
in the critical partition of the cache. To maximize hits, SHiP
is used so that only the cache lines that receive re-references
are retained in the cache.
With the help of CCBP and SHiP, CACP is able to capture
both cache lines that have intra-warp and inter-warp localities.
Furthermore, CACP complements the gCAWS warp scheduling scheme by reserving larger cache space to critical warps

that are being identiﬁed and prioritized by gCAWS. Thus,
CAWA is able to take the coordinated approach of criticality
prediction, scheduling, and cache prioritization to provide the
best performance gain.

4. Methodology
To evaluate our coordinated CAWA design, we use GPGPUsim (version 3.2.0) [1] to explore the behavior of GPGPU applications. GPGPU-sim is a cycle-level performance simulator
that models a general-purpose GPU architecture supporting

521

Min. L2 Access Latency
Min. DRAM Access Latency
Warp Size (SIMD Width)

NVIDIA Fermi GTX480
15
48
8
2
32768
48KB
16KB per SM (8-sets/16-ways)
2KB per SM (4-sets/4-ways)
768KB uniﬁed cache (64sets/16-ways/6-banks)
120 cycles
220 cycles
32 threads

2L
IPC Normalized to Baseline RR

Architecture
Num. of SMs
Max. # of Warps per SM
Max. # of Blocks per SM
# of Schedulers per SM
# of Registers per SM
Shared Memory
L1 Data Cache
L1 Inst Cache
L2 Cache

GTO

CAWA

3.13x

1.3
1.25
1.2
1.15
1.1
1.05
1
0.95
0.9

2.54x

Table 1: GPGPU-sim conﬁgurations.
Figure 9: Performance of CAWA design.

Data Set

Category

bfs
b+tree
heartwall
kmeans
needle
srad_1
streamcluster (small)

65536 nodes
1 million nodes
656x744 grey scale AVI
494020 nodes
1024x1024 nodes
502x458 nodes
32x4096 nodes

Sens
Sens
Sens
Sens
Sens
Sens
Sens

backprop
particle
pathﬁnder
streamcluster (mid)
tpacf

65536 nodes
128x128x10 nodes
100000 nodes
64x8192 nodes
487x100 nodes

Non-sens
Non-sens
Non-sens
Non-sens
Non-sens

2L
MPKI Normalized to Baseline RR

Benchmark

Table 2: GPGPU Benchmarks and their data set sizes. Benchmarks are classiﬁed into Sens and Non-sens based on their
execution time disparity and sensitivity to L1D cache performance.

GTO

CAWA

1.2
1.1
1
0.9
0.8
0.7
0.6
0.5

Figure 10: L1D cache MPKI performance of CAWA design.

scheduler for Sens applications and by an average of 9.2%
over all applications, as Figure 9 shows. In particular, CAWA
speeds up the performance of kmeans (which suffers from
severe cache thrashing) the most, by a signiﬁcant 3.13 times
over the baseline. We also compare the performance of CAWA
with two other state-of-the-art warp schedulers, i.e., GTO and
the 2-level warp scheduler [24]. Across the seven GPGPU
applications that are scheduling policy- and cache sensitive,
CAWA improves the performance the most by an average
of 23% while GTO and the 2-level scheduler improves the
performance by 16% and -2% respectively.
For memory-intensive GPGPU applications, such as
kmeans, performance is mainly restricted by the efﬁciency
of the data caches. Because of the large amount of data that is
streamed over the relatively small L1 data caches, the cached
data is evicted from the L1 caches before it receives any additional reuses. GTO alleviates the cache thrashing problem
by limiting the number of warps that could be active such
that the active working set is kept small and the intra-warp
data locality can be better captured in the L1 data cache. The
signiﬁcant performance improvement from CAWA is achieved
for a similar reason. The greedy criticality-aware scheduler,
CAWAgCAW S , is able to limit the number of active warps such
that the aggregate working set can well ﬁt into the L1 data

NVIDIA CUDA[26] and its PTX ISA[25]. We run GPGPUsim with the default conﬁguration representing the NVIDIA
Fermi GTX480 architecture and conﬁgure the per-SM L1
data cache as 16-way set-associative. Table 1 describes the
simulation conﬁguration and parameters used for the design
evaluation for CAWA.
We select representative GPGPU applications from the Rodinia [5, 6] and Parboil [36] benchmark suites to evaluate the
performance improvement of the coordinated CAWA design
in GPUs. Table 2 lists the details of the benchmarks and their
data sets used to evaluate the CAWA design. Since CAWA
mainly aims to improve the performance of those applications
with irregular execution behavior as well as cache utilization,
we categorize these benchmarks into two sets based on their
execution time disparity and sensitivity to L1D cache performance as sensitive (Sens) or non-sensitive (Non-sens).

5. Evaluation Results and Analysis
5.1. Performance Overview
Overall, CAWA improves GPGPU performance by an average
of 23% compared to the baseline Round-Robin (RR) warp

522

CAWACPL Prediction Accuracy



	



	
	

100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%

Figure 11: The criticality prediction accuracy of CAWACPL .








	
	
	
	











Figure 12: Critical warp’s scheduling priority over time of bfs.

cache. Furthermore, CAWACACP reserves a ﬁxed cache capacity for data useful to the critical warps thereby reducing the
degree of interference between critical and non-critical cache
blocks.
Figure 10 shows the MPKI results for the GPGPU workloads under the 2-level scheduler, GTO, and CAWA, respectively. Overall, CAWA reduces the L1 cache miss rates the
most when compared to the other two schemes. For memoryintensive applications, such as kmeans, CAWA signiﬁcantly
reduces the cache miss rate by 26.2%. For other applications,
such as heartwall and streamcluster-small, MPKI is increased under CAWA. This is because CAWACACP prioritizes
critical cache blocks over non-critical cache blocks over the
baseline cache replacement policy that is designed to minimize
cache misses. Although MPKI is increased for heartwall
and streamcluster-small, the corresponding IPC performance is improved by 3.3% and 3.6% respectively. This is
because CAWA trades off cache blocks that may be used more
often (with better locality) with cache blocks that are critical.

IPC Normalized to Baseline RR

CAWS

1.3
1.25
1.2
1.15
1.1
1.05
1
0.95
0.9

gCAWS

CAWA

3.13x
2.63x

Figure 13: The performance improvement of CAWAgCAW S .

is able to adjust its warp criticality prediction outcomes at
runtime.
5.3. Performance Analysis for CAWAgCAW S
At runtime, CAWAgCAW S prioritizes warps based on their criticality updated by CAWACPL . To understand how CAWAgCAW S
accelerates warp execution, we investigate the scheduler’s decision based on CAWACPL ’s critical warp prediction over time.
Figure 12 shows the per-warp criticality counter of the static
critical warp over time (bfs) under the baseline scheduler
and under CAWAgCAW S . In this particular bfs thread block,
there are a total of 16 warps; therefore, the critical warp could
be ranked in the warp pool with the lowest priority (0 of the
y-axis) or with the highest priority (15 of the y-axis) based on
CAWACPL ’s prediction outcomes. When the critical warp has
the highest priority, CAWAgCAW S will schedule it for immediate execution. When compared to a criticality-oblivious scheduler, e.g., the baseline RR, the critical warp is treated equally
with all other warps such that it has an equal chance to be selected for execution. In contrast, CAWAgCAW S will proactively
schedule the critical warp until its execution progress has improved (CAWACPL adjusts the criticality counter accordingly).
As Figure 12 shows, CAWAgCAW S ranks the critical warp with
a higher priority and schedules the critical warp for execution more frequently than the baseline round robin scheduler.

5.2. Performance Analysis for CAWACPL
The critical warp prediction mechanism is a vital component
in CAWA and is used to guide compute and memory resource
prioritization. To evaluate the accuracy of CAWACPL , we compare the periodic prediction outcomes with the slowest, critical
warp based on its total execution time. To calculate the prediction accuracy, we deﬁne that if a warp’s criticality is larger
than 50% of warps in a thread-block, this warp is a slow warp.
Since warp criticality could change at runtime which is not
captured by the static warp execution time analysis, it is difﬁcult to calculate the prediction accuracy. Thus we count the
prediction accuracy as the frequency that the critical warp is
identiﬁed as a slow warp. Figure 11 shows the warp criticality
prediction accuracy comparison. On average, CAWACPL can
accurately identify critical warps as a slow warp with a prediction accuracy of 73%2 . Since CAWACPL learns the sources and
degree of delay dynamically and reﬂects the delay and execution progress in the per-warp criticality counters, CAWACPL
2 CPL results in an 100% prediction accuracty for needle because it is an
application which lacks warp-level parallelism, i.e., a thread-block has only
one or two warps

523

Critical Warp Line Hit Rate
Normalized to Baseline

CAWA
7.22x

Percentage of Zero Reuse
Critical Warp Lines

GTO
3
2.5
2
1.5
1
0.5
0

Figure 14: L1D cache critical warp hit rate of CAWA design.

Baseline

100
80
60
40
20
0

GTO

CAWA

Figure 15: L1D cache zero reuse critical warp lines.

Consequently, with the additional compute resources made
available by CAWAgCAW S , the execution time of the critical
warp is signiﬁcantly improved.
Figure 13 shows the performance improvement comparison
for CAWS [20] with the oracle warp criticality knowledge,
CAWAgCAW S , and CAWA. With the oracle warp criticality information obtained ofﬂine, CAWS performs best on small
GPU kernels such as bfs, b+tree and needle. This is because for these small kernels, the prediction and training overhead of CAWACPL is relatively high. Although CAWACPL has
a good prediction accuracy, CAWAgCAW S and CAWA are not
able to achieve the potential speedup under using the oracle
knowledge. On the other hand, for large kernel code such
as heartwall and srad_1, our proposed CAWAgCAW S and
CAWACPL can further improve performance compared with
CAWS.
We also note that CAWAgCAW S and CAWA achieve a greater
performance improvement on kmeans than CAWS. This is
because kmeans has heavy memory contention and prefers to
run with fewer number of threads/warps. CAWAgCAW S adopts a
greedy scheme to temporarily limit the number of active warps
while minimizing warp criticality and resource contention.
Because CAWS does not limit the number of active warps
to mitigate the memory contention, CAWAgCAW S and CAWA
outperforms CAWS on kmeans.
Overall CAWA can obtain an additional 5% IPC improvement on selected benchmarks compared with CAWAgCAW S .
This additional performance improvement is due to the
cache prioritization with CAWACACP . However, b+tree and
strcltr_small have a slight performance degradation under
CAWA. This is because these two particular applications have
high degree of inter-warp data reference and spatial locality.
While memory requests from the critical warps have higher
priority to be allocated, CAWACACP does not take the interwarp reference pattern into account. Therefore, warps may
encounter longer memory access latency with CAWACACP .

MPKI Normalized to Baseline RR

RR+CACP

2L + CACP

GTO+CACP

CAWA

1.5
1.25

1
0.75

0.5

Figure 16: L1D cache MPKI performance of CAWACACP design
with different warp scheduling policies.

diction outcome from the Critical Cache Block Predictor and
from the Signature-based Cache Hit Predictor, cache blocks
are inserted into either the critical or the non-critical cache
partitions with the appropriate insertion positions. Figure 14
shows the normalized cache hit rate received by critical warp
memory requests under CAWA when compared to the baseline. The explicit cache partitioning with CCBP signiﬁcantly
improves the hit rate for critical warps, by an average of 2.46
times and by as much as 7.22 times for kmeans, which outperforms other state-of-the-art warp schedulers. Schedulers, e.g.,
GTO, that are criticality-oblivious, will not speciﬁcally improve the memory performance speciﬁcally for critical warps;
therefore, the cache hit rate performance is less consistent
across the applications.
To examine the utilization of the cache partition dedicated
to critical warps, we compare the number of cache blocks that
have never been reused before being evicted from the L1 data
cache under the baseline and CAWA. Figure 15 shows that,
in the baseline, a majority of cache blocks useful to critical
warps, i.e., 44.3%, have never been reused before being evicted
from the cache. This is because of the severe interference in
the L1 data cache shared between critical and non-critical
cache blocks. CAWACACP is able to reduce such interference
by explicitly partitioning and reserving the cache for critical
cache blocks; therefore, data retained in the cache is critical
and has high locality.
Next, to understand how well CAWACACP performs in isolation from the scheduler (CAWAgCAW S ) and in the presence

5.4. Performance Analysis for CAWACACP
Next, we perform analysis for the cache prioritization scheme.
CAWA relies on the criticality-aware cache prioritization
scheme, CAWACACP , to accelerate the execution of critical
warps, thereby reducing latencies coming from the memory
subsystem. CAWACACP separates the data cache to the critical
and the non-critical partitions explicitly. Based on the pre-

524

IPC Normalized to Baseline RR

RR + CACP

1.3
1.25
1.2
1.15
1.1
1.05
1
0.95
0.9
0.85
0.8

2L + CACP

GTO + CACP

sensitive applications.

CAWA

3.13x

6. Related Work

3.00x

Prior work have looked at various issues that limit GPU performance, including warp scheduling policies and memory
subsystem design. In this section, we discuss prior works that
are most related to this paper.
6.1. Thread/Warp Criticality Prediction
The thread criticality problem on CMP has been addressed in
many works. Bhattacharjee and Martonosi [2] observed that
the thread criticality problem for parallel threads executing
on CMPs. Since the performance of a parallel application is
constrained by the critical (the slowest running) thread, the execution time of the parallel application is signiﬁcantly affected
by the execution time of the critical thread. Bhattacharjee and
Martonosi designed the thread criticality predictor (TCP) to
identify the critical thread based on per-thread cache behavior.
TCP is then used to guide the task stealing as well as dynamic
voltage and frequency scaling (DVFS) of CMPs. Inspired by
the thread criticality concept, Ebrahimi et al. exploited the
resource contention at a spin lock as the metric to evaluate
the critical thread [8], whereas Bois et al. proposed a stack
based approach to measure thread criticality by monitoring the
number of waiting threads in a certain interval [3]. Although
the concept of thread criticality in CPU parallel applications
is similar to warp criticality in GPGPU workloads, due to the
distinct difference between CPU and GPU architectures, the
effects introduced by the critical thread(s) and warp(s) vary
as well. Because of the fast context-switching and massive
multi-threading in modern GPUs, dynamic warp criticality
prediction is challenging. Lee and Wu [20] were the ﬁrst to
show that the performance of modern GPU architectures is
also strictly constrained by the performance of critical warps.
However, there still lacks a robust approach to dynamically
identify the critical thread/warp on GPU platforms, which is
the focus of the work performed in this paper.

Figure 17: L1D cache IPC performance of CAWACACP design
with different warp scheduling policies.

of other state-of-the-art warp scheduling algorithms, we apply
CAWACACP to the baseline RR, GTO, and the 2-level schedulers. Independent from the warp schedulers, CAWACACP uses
the warp criticality prediction from CAWACPL for cache prioritization. Figure 16 shows the MPKI reduction for the different warp schedulers under the inﬂuence of CAWACACP and
Figure 17 shows the corresponding IPC performance results.
When CAWACACP is used in conjunction with the various stateof-the-art schedulers, additional performance improvement is
achieved from 2% to 16.5% while our proposed coordinated
management still performs the best.
5.5. Discussion for Warp Criticality
5.5.1. Criticality in the Context of a Scheduler There have
been warp scheduling algorithms proposed to alleviate resource contention by greedily executing a small group of
warps with a higher priority, e.g., [24, 34]. In this type of
unfair warp schedulers, warps are scheduled with different
priorities based on the cache utilization and warp cache sharing behavior. Such schedulers could intentionally create warp
criticality, depending on the scheduling order of warps in a
thread block, and thus the warp criticality inherited from applications can be skewed, while the concept of warp criticality
proposed in this paper is most directly applicable to fair warp
schedulers.
5.5.2. Applying Criticality to Existing Schedulers The idea
of warp criticality is orthogonal to the warp scheduler’s design,
particularly for applications whose warp criticality stems from
software code implementation (workload imbalance/branch
divergence). Different schedulers may experience different
degrees of warp criticality since criticality is often caused by
multiple sources as described in Section 2. The notion of warp
criticality can also be integrated into existing, advanced schedulers with careful design modiﬁcation to further improve GPU
performance by eliminating warp execution time disparity,
leading to improved pipeline utilization. As this paper shows,
CAWAgCAW S applies CPL on top of GTO [34] and improves
performance by 7% for the selected, scheduling and cache

6.2. GPU Warp Scheduling
Many recent works [11, 15, 16, 24, 34, 35] focused on improving the memory aspect of GPUs by modifying warp scheduling
algorithms. Gebhart et al. and Narasiman et al. designed a 2level scheduler to split warps into different subgroups and keep
only one group of warps active at a time [11, 24]. The warp
scheduler is only able to issue instructions from the active subgroup of warps, so that the resource contention problem can
be alleviated. Jog et al. further improved the 2-level scheduler
by splitting warps with continuous IDs to different subgroups.
Because memory requests from continuous warps have high
probability to fall into the same L2 cache/DRAM bank, resulting in bank conﬂicts. With this warp grouping algorithm, the
GPU performance can be improved by avoiding bank conﬂicts
at the L2 cache and DRAM. Rogers et al. proposed a warp
scheduler design by monitoring memory contention [34]. The
warp scheduler dynamically tunes the number of active warps
based on the degree of data cache contention. If the cache con-

525

troller detects cache lines in the L1 data cache are frequently
evicted due to the interference from inter-warp accesses, the
warp scheduler will limit the number of active warps. These
algorithms prevent all warps from stalling at the same time by
ensuring the warps in the pool do not encounter long latency
memory operations or compete for register ﬁle banks at the
same time. These scheduling policies allow the GPU warp
scheduler to tolerate memory latency better while reducing
the idle time of GPU cores. In contrast, the gCAWS design in
this paper aims to resolve resource contention by designing a
novel warp scheduling order based on warp criticality and by
allowing critical warps to execute with larger time slices.

MRPB, requests coming from the same warp or thread-block
are serviced together to maximize locality. MRPB is able to
effectively improve data cache locality and alleviate memory
contention. However, all of these schemes do not take criticality into account. The critical warp may wait for longer time
to be served. In addition to the previously proposed cache
management policies designed for GPUs, this paper presents a
cache prioritization scheme to improve the cache performance
and tailors the design for critical warp acceleration.

7. Conclusion
This work introduces a new coordinated compute and memory resource prioritization design for GPGPU critical warp
acceleration. We quantify the sources of the warp criticality
problem in the massively-parallel GPU environment.
Built upon the insights observed from the warp criticality
characterization results, this work proposes CAWA to dynamically predict critical warps and accelerate the execution of
the critical warps with a higher scheduling priority and with a
larger scheduling time slice. Furthermore, CAWA reserves a
partition of the L1 data cache for data predicted-to-be-useful
for the critical warps; therefore, the interference between critical and non-critical cache blocks is minimized. Our simulation
results shows that the proposed design improves performance
by an average of 23% on highly scheduler-sensitive and cachesensitive workloads.
This work demonstrates that coordinated management of
warp scheduler and cache prioritization is necessary for a
holistic GPGPU performance improvement. With the knowledge of warp criticality learned and predicted at runtime, warp
schedulers and cache insertion/replacement techniques can be
designed more intelligently to extract additional performance
improvement from modern GPUs. We hope the detailed characterization presented in this paper can offer new insights
into the important warp criticality problem and motivate novel
solutions for future GPGPU architectures.

6.3. Branch Divergence
Many warp scheduling algorithms [9, 10, 23, 24, 35, 37] have
been proposed to alleviate the effects of branch and memory
divergence on GPUs. When branch or memory divergence happens, the utilization of GPU resources becomes sub-optimal.
A common algorithm to reduce the degree of branch divergence is dynamically re-packing threads into different warps
at runtime [9, 10, 23, 37], so that threads having the same
execution path can be executed together. Instead of re-forming
warps, Rhu and Erez [32] proposed an interleaving execution
technique to overlap a divergent warp’s execution. This line of
prior works target to increase the pipeline utilization of functional units instead of alleviating the warp criticality problem
which our proposed CAWA aims to tackle. With the previously proposed designs, the branch diverging behavior will be
improved. However, the diverging warp execution time still
persists, which will beneﬁt from the CAWA design presented
in this paper.
6.4. GPU Memory Subsystem and Resources Contention
Studies relevant to CMP cache designs mainly target to maximize overall cache hit rates [12, 17, 18, 29, 30, 38]. However,
the architecture of modern GPUs is massively-parallel when
compared to its CMP counterpart. The per-thread cache capacity in GPUs is relatively small, resulting in serious data
thrashing problems [12, 29, 30]. Recent works, e.g, [7, 33],
looked at how well cache insertion/replacement policies proposed for CMP caches perform in the massively-parallel GPU
environment and found that directly employing state-of-theart cache management policies for CMPs does not effectively
alleviate the data thrashing problem in the GPU caches.
Instead of aiming to improve cache hit rates, many works,
e.g. [7, 13, 14, 39], investigated techniques to improve GPU’s
memory subsystems by cache bypassing, which can signiﬁcantly reduce the demand of memory/bus bandwidth. Jia et
al. [13] and Xie et al. [39] proposed using compilers to perform off-line analysis to ﬁnd out the load/store instructions or
memory regions which may have poor spatial locality. GPUs
take these hints generated by compilers to bypass data cache
in order to mitigate the bus bandwidth contention. On the
other hand, Jia et al. [14] designed a memory request prioritization buffer (MRPB) which reorders the memory requests
and/or bypasses the requests from the L1 data caches. With

Acknowledgements
We would like to thank Wenhao Jia (Qualcomm) and the
anonymous reviewers for their feedback. The work was supported, in part, by the Science Foundation Arizona under the
Bisgrove Early Career Scholarship and by Arizona State University. The opinions, ﬁndings and conclusion or recommendations expressed in this manuscript are those of the authors and
do not necessarily reﬂect the views of the Science Foundation
Arizona and ASU.

References
[1] A. Bakhoda, G. Yuan, W. W. L. Fung, H. Wong, and T. M. Aamodt,
“Analyzing CUDA workloads using a detailed GPU simulator,” in Proc.
of the 2009 IEEE International Symposium on Analysis of Systems and
Software (ISPASS’09), Boston, MA, USA, April 2009.
[2] A. Bhattacharjee and M. Martonosi, “Thread criticality predictors for
dynamic performance, power, and resource management in chip multiprocessors,” in Proc. of the 36th IEEE/ACM International Symposium
on Computer Architecture (ISCA’09), Austin, TX, USA, June 2009.
[3] K. D. Bois, S. Eyerman, J. B. Sartor, and L. Eeckhout, “Criticality
stacks: Identifying critical threads in parallel programs using synchronization behavior,” in Proc. of the 40th IEEE/ACM International

526

[4]

[5]

[6]

[7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]
[20]

[21]

Symposium on Computer Architecture (ISCA’13), Tel Aviv, Israel, June
2013.
M. Burtscher, R. Nasre, and K. Pingali, “A quantitative study of irregular programs on GPUs,” in Proc. of the 2012 IEEE International
Symposium on Workload Characterization (IISWC’12), San Diego, CA,
USA, November 2012.
S. Che, M. Boyer, J. Meng, D. Tarjan, J. W. Sheaffer, S.-H. Lee,
and K. Skadron, “Rodinia: A benchmark suite for heterogeneous
computing,” in Proc. of the 2009 IEEE International Symposium on
Workload Characterization (IISWC’09), Austin, TX, USA, October
2009.
S. Che, J. W. Sheaffer, M. Boyer, L. G. Szafaryn, L. Wang, and
K. Skadron, “A characterization of the Rodinia benchmark suite
with comparison to contemporary CMP workloads,” in Proc. of the
2010 IEEE International Symposium on Workload Characterization
(IISWC’10), Atlanta, GA, USA, December 2010.
X. Chen, L.-W. Chang, C. I. Rodrigues, J. Lv, and W. mei Hwu,
“Adaptive cache management for energy-efﬁcient GPU computing,”
in Proc. of the 47th Annual IEEE/ACM International Symposium on
Microarchitecture (MICRO’14), Cambridge, UK, December 2014.
E. Ebrahimi, R. Miftakhutdinov, C. Fallin, C. J. Lee, J. A. Joao,
O. Mutlu, and Y. N. Patt, “Parallel application memory scheduling,”
in Proc. of the 44th International Symposium on Microarchitecture
(MICRO’11), Porto Alegre, Brazil, December 2011.
W. L. W. Fung and T. M. Aamodt, “Thread block compaction for
efﬁcient SIMT control ﬂow,” in Proc. of the 17th IEEE International
Symposium on High Performance Computer Architecture (HPCA’11),
San Antonio, TX, USA, February 2011.
W. L. W. Fung, I. Sham, G. Yuan, and T. M. Aamodt, “Dynamic
warp formation and scheduling for efﬁcient GPU control ﬂow,” in
Proc. of the 37th IEEE/ACM International Symposium on Computer
Architecture (ISCA’10), Saint-Malo, France, June 2010.
M. Gebhart, R. D. Johnson, D. Tarjan, S. W. Keckler, W. J. Dally,
E. Lindoholm, and K. Skadron, “Energy-efﬁcient mechanisms for
managing thread context in throughput processors,” in Proc. of the
38th IEEE/ACM International Symposium on Computer Architecture
(ISCA’11), San Jose, CA, USA, June 2011.
A. Jaleel, K. B. Theobald, S. C. Steely, Jr., and J. Emer, “High performance cache replacement using re-reference interval prediction
(RRIP),” in Proc. of the 37th IEEE/ACM International Symposium on
Computer Architecture (ISCA’10), Saint-Malo, France, June 2010.
W. Jia, K. A. Shaw, and M. Martonosi, “Characterizing and improving
the use of demand-fetched caches in GPUs,” in Proc. of the 20th ACM
International Conference on Supercomputing (ICS’12), Venice, Italy,
June 2012.
W. Jia, K. A. Shaw, and M. Martonosi, “MRPB: memory request prioritization for massively parallel processors,” in Proc. of the 20th IEEE
International Symposium on High Performance Computer Architecture
(HPCA’14), Orlando, FL, USA, February 2014.
A. Jog, O. Kayiran, A. K. Mishra, M. T. Kandemir, O. Mutlu, R. Iyer,
and C. R. Das, “Orchestrated scheduling and prefetching for GPGPUs,”
in Proc. of the 40th IEEE/ACM International Symposium on Computer
Architecture (ISCA’13), Tel-Aviv, Isreal, June 2013.
A. Jog, O. Kayiran, N. C. Nachiappan, A. K. Mishra, M. T. Kandemir,
O. Mutlu, R. Iyer, and C. R. Das, “OWL: cooperative thread array
aware scheduling techniques for improving GPGPU performance,” in
Proc. of the 18th IEEE/ACM International Conference on Architectural Support for Programming Languages and Operating Systems
(ASPLOS’13), Houston, TX, USA, March 2013.
G. Keramidas, P. Petoumenos, and S. Kaxiras, “Cache replacement
based on reuse-distance prediction,” in Proc. of the 25th IEEE International Conference on Computer Design (ICCD’07), Lake Tahoe, CA,
USA, October 2007.
S. Khan, Y. Tian, and D. Jimenez, “Sampling dead block prediction
for last-level caches,” in Proc. of the 43rd IEEE/ACM International
Symposium on Microarchitecture (MICRO’10), Atlanta, GA, USA,
December 2010.
A.-C. Lai, C. Fide, and B. Falsaﬁ, “Dead-block prediction & dead-block
correlating prefetchers,” in Proc. of the 28th IEEE/ACM International
Symposium on Computer Architecture (ISCA’01), 2001.
S.-Y. Lee and C.-J. Wu, “CAWS: Criticality-aware warp scheduling
for GPGPU workloads,” in Proc. of the 23rd IEEE/ACM International
Conference on Parallel Architectures and Compilation (PACT’14),
Edmonton, AB, Canada, August 2014.
S.-Y. Lee and C.-J. Wu, “Characterizing the latency hiding ability
of GPUs,” in Proc. of the 2014 IEEE International Symposium on
Performance Analysis of Systems and Software (ISPASS’14) as Poster
Abstract, Monterey, CA, USA, March 2014.

[22] E. Lindholm, J. Nickolls, S. Oberman, and J. Montrym, “NVIDIA
Tesla: A uniﬁed graphics and computing architecture,” IEEE Micro,
vol. 28, pp. 39–55, March 2008.
[23] J. Meng, D. Tarjan, and K. Skadron, “Dynamic warp subdivision for
integrated branch and memory divergence tolerance,” in Proc. of the
37th IEEE/ACM International Symposium on Computer Architecture
(ISCA’10), Saint-Malo, France, June 2010.
[24] V. Narasiman, M. Shebanow, C. J. Lee, R. Miftakhutdinov, O. Mutlu,
and Y. N. Patt, “Improving GPU performance via large warps and twolevel warp scheduling,” in Proc. of the 44th International Symposium
on Microarchitecture (MICRO’11), Porto Alegre, Brazil, December
2011.
[25] NVIDIA, “PTX ISA,” 2009. Available: http://www.nvidia.com/
content/CUDA-ptx_isa_1.4.pdf
[26] NVIDIA,
“NVIDIA
CUDA
C
programming
guide
v4.2,”
2012.
Available:
http://developer.nvidia.com/
nvidia-gpu-computing-documentation
[27] NVIDIA, “NVIDIA GeForce GTX 980: Featuring Maxwell, the most
advanced GPU ever made,” September 2014.
[28] M. A. O’Neil and M. Burtscher, “Microarchitectural performance
characterization of irregular GPU kernels,” in Proc. of the 2014 IEEE
International Symposium on Workload Characterization (IISWC’14),
Raleigh, NC, USA, October 2014.
[29] M. K. Qureshi, A. Jaleel, Y. N. Patt, S. C. Steely Jr., and J. Emer, “Adaptive insertion policies for high performance caching,” in Proc. of the
34th IEEE/ACM International Symposium on Computer Architecture
(ISCA’07), San Diego, CA, USA, June 2007.
[30] M. K. Qureshi, A. Jaleel, Y. N. Patt, S. C. Steely Jr., and J. Emer, “Setdueling-controlled adaptive insertion for high-performance caching,”
IEEE Micro, vol. 28, no. 1, pp. 91–98, January 2008.
[31] M. K. Qureshi and Y. N. Patt, “Utility-based cache partitioning: A lowoverhead, high-performance, runtime mechanism to partition shared
caches,” in Proc. of the 39th IEEE/ACM International Symposium on
Microarchitecture (MICRO’06), Orlando, FL, USA, December 2006.
[32] M. Rhu and M. Erez, “The dual-path execution model for efﬁcient
GPU control ﬂow,” in Proc. of the 19th IEEE International Symposium
on High Performance Computer Architecture (HPCA’13), Shenzhen,
China, February 2013.
[33] M. Rhu, M. Sullivan, J. Leng, and M. Erez, “A locality-aware memory
hierarchy for energy-efﬁcient GPU architecture,” in Proc. of the 46th
International Symposium on Microarchitecture (MICRO’13), Davis,
CA, USA, December 2013.
[34] T. G. Rogers, M. O’Connor, and T. M. Aamodt, “Cache-conscious
wavefront scheduling,” in Proc. of the 45th IEEE/ACM International Symposium on Microarchitecture (MICRO’12), Vancouver, BC,
Canada, December 2012.
[35] T. G. Rogers, M. O’Connor, and T. M. Aamodt, “Divergence-aware
warp scheduling,” in Proc. of the 46th IEEE/ACM International Symposium on Microarchitecture (MICRO’13), Davis, CA, USA, December
2013.
[36] J. A. Stratton, C. Rodrigues, I.-J. Sung, N. Obeid, L.-W. Chang,
N. Anssari, G. D. Liu, and W.-M. W. Hwu, “The Parboil technical
report,” in IMPACT Technical Report (IMPACT-12-01), University of
Illinois Urbana-Champaign, Champaign, IL, USA, March 2012.
[37] A. S. Vaidya, A. Shayesteh, D. H. Woo, R. Saharoy, and M. Azimi,
“SIMD divergence optimization through intra-warp compaction,” in
Proc. of the IEEE/ACM 40th International Symposium on Computer
Architecture (ISCA’13), Tel Aviv, Israel, June 2011.
[38] C.-J. Wu, A. Jaleel, W. Hasenplaugh, M. Martonosi, S. C. Steely, Jr.,
and J. Emer, “SHiP: signature-based hit predictor for high performance
caching,” in Proc. of the 44th IEEE/ACM International Symposium on
Microarchitecture (MICRO’11), Porto Alegre, Brazil, December 2011.
[39] X. Xie, Y. Liang, G. Sun, and D. Chen, “An efﬁcient compiler framework for cache bypassing on GPUs,” in Proc. of the 2013 IEEE/ACM
International Conference on Computer-Aided Design (ICCAD’13), San
Jose, CA, USA, November 2013.

527

2015 IEEE International Symposium on Workload Characterization

Characterization and Throttling-based Mitigation of
Memory Interference for Heterogeneous Smartphones
∗ School

Davesh Shingari∗ , Akhil Arunkumar† , Carole-Jean Wu†
of Electrical, Computer and Energy Engineering † School of Computing, Informatics, and Decision Systems Engineering
Arizona State University, Tempe, Arizona 85281
Email: {dshingar,aarunkum,carole-jean.wu}@asu.edu

Abstract—The availability of a wide range of generalpurpose as well as accelerator cores on modern smartphones
means that a signiﬁcant number of applications can be executed
on a smartphone simultaneously, resulting in an ever increasing
demand on the memory subsystem. While the increased computation capability is intended for improving user experience,
memory requests from each concurrent application exhibit
unique memory access patterns as well as speciﬁc timing
constraints. If not considered, this could lead to signiﬁcant
memory contention and result in lowered user experience.










" 

 $ !
$#'((%
 %!!

	*


&'	

In this paper, we design experiments to analyze the performance degradation caused by the interference at the memory
subsystem for a broad range of commonly-used smartphone
applications. The characterization studies are performed on
a real smartphone device – Google Nexus5 – running an
Android operating system. Our results show that user-centric
smartphone applications, such as web browsing and media
player, suffer upto 34% and 21% performance degradation,
respectively, from shared resource contention at the application
processor’s last-level cache, the communication fabric, and
the main memory. Taking a step further, we demonstrate the
feasibility and effectiveness of a frequency throttling-based
memory interference mitigation technique. At the expense of
performance degradation of interfering applications, frequency
throttling is an effective technique for mitigating memory
interference, leading to better QoS and user experience, for
user-centric applications.

$$$





 




!"
%
%'

#)& 
 #%




$$$

$" "&



 #

Figure 1. The LPDDR3 main memory is a major shared resource serving
memory requests from heterogeneous accelerators and the application
processor in modern smartphone SoC.

back via a speciﬁc accelerator, and web browsing on the
application cores, generating a heterogeneous combination
of memory requests at the main memory. Furthermore, as
GPUs are becoming more and more programmable, generalpurpose computations are increasingly ofﬂoaded to mobile
GPUs, e.g., [2], to achieve higher performance and improved
energy efﬁciency. Similarly, more and more accelerators
have been added to modern smartphone SoCs to execute
special functions as energy efﬁciently as possible, e.g.,
Qualcomm’s programmable digital signal processor (DSP)
released in 2013, and many more other low-power accelerators in the years to come [3], [4].
Memory requests from each application exhibit its own
unique memory access pattern as well as its speciﬁc timing constraint. The main memory, thus, becomes a highly
contended shared resource. Without explicit management,
applications running concurrently on the smartphone will
experience performance degradation in an unpredictable
manner, leading to frequent violation of real-time application
deadlines and lowered user experiences. Figure 1 depicts the
scenario where multiple smartphone applications are running
simultaneously on a modern smartphone SoC sharing the
LPDDR3 main memory.
To maximize system throughput, the ﬁrst-ready ﬁrstcome-ﬁrst-serve (FR-FCFS) memory scheduler prioritizes

Keywords-Smartphone; Memory interference; Frequency
throttling.

I. I NTRODUCTION
There has been an explosive growth in the use of mobile
computing devices, especially smartphones, for our everyday
computing needs. According to International Telecommunication Union, there were nearly 7 billion mobile subscriptions as of May 2014 [1]. To meet the high performance
expectation from users, more and more chip multiprocessor
(CMP) as well as graphics processing unit (GPU) cores
are being integrated into the system-on-chip (SoC) in each
generation of the smartphone chip. The availability of a
wide range of general-purpose as well as accelerator cores
means that a signiﬁcant number of applications can be now
executed on the smartphone simultaneously, resulting in an
ever-increasing demand on the main memory. For example,
a typical smartphone multi-programmed use case consists
of ﬁle downloading via the Wi-Fi/LTE antenna, music play978-1-5090-0088-3/15 $31.00 © 2015 IEEE
DOI 10.1109/IISWC.2015.9




22



	







degradation directly violates user-centric deadlines—user
satisfaction is determined by the absolute execution time.
According to recent studies based on massive mobile web
user experiences, 40% of users abandon web pages that do
not load within 3 seconds [7]. In other words, the 3-second
mark is the performance quality-of-service (QoS) target for
interactive web browsing. As marked in Figure 2, the 3second mark corresponds to 9% performance degradation
of web browsing, which can be tolerated without sacriﬁcing
user experience. However, in all seven realistic workloads,
web browsing experiences signiﬁcant performance degradation that exceeds the latency tolerance of the workload. We
envision this performance degradation to worsen as more
and more GPU cores and accelerators are being integrated
into the smartphone chip, leading to even more signiﬁcant
contention at the main memory for concurrent workloads and
resulting in additional performance loss and unpredictability.
This paper ﬁrst quantiﬁes the degree of memory locality in
a wide range of emerging smartphone workloads. We evaluate a set of user-centric, interactive workloads, such as web
browsing and video playback, as well as machine-learning,
image-based computations, compression algorithms, and
many other applications representative of future smartphone
workloads. We design experiments to focus on memory
interference-caused performance degradation in the shared
cache memory of the application processor as well as in the
main memory shared between the application processor and
other accelerators. Our real-device performance characterization results on a Google Nexus5 smartphone show that
the performance of user-centric web browsing and media
player is signiﬁcantly degraded by the memory interference
at the application processor’s last-level cache at 27% and
21%, respectively (Section IV-A). If focusing on the memory
interference at the shared communication fabric and the
main memory, the performance degradation experienced by
web browsing is signiﬁcant (by as much as 34%) while
the media player experiences no performance degradation
(Section IV-B). The performance characterization results
presented in this paper motivate and offer insights into novel
memory scheduling designs as well as QoS-aware resource
management techniques.
Taking a step further, to demonstrate the feasibility and
effectiveness of a throttling-based memory interference mitigation technique, we leverage the per-core dynamic voltage
and frequency scaling (DVFS) control knob implemented in
the Qualcomm Snapdragon 800 chipset. Our results using
a statically selected frequency setting show that frequency
throttling is an effective technique for mitigating memory
interference, leading to better QoS and user experiences.
Overall, this paper has two major contributions.
1) We characterize the degree of memory access interference patterns in modern smartphone workloads, which
has distinct user experience requirements and has not
been studied in detail before. To our knowledge, this is








	





Figure 2. A signiﬁcant 10-33% performance slowdown is experienced
by user-centric, interactive web browsing that is co-scheduled with other
mobile workloads.

the service of memory requests residing in the memory
buffer based on row buffer locality [5]. FR-FCFS has been
demonstrated to perform well for the main memory of traditional CMPs running homogeneous, sequential workloads
by reducing the degree of DRAM row conﬂicts. However,
since FR-FCFS only takes into account row buffer locality,
its scheduling decisions could adversely affect the performance of latency-critical, high-priority workloads. To control the unpredictability of performance degradation, stalltime fair memory scheduling (STF) [6] used an empirically
determined threshold to prioritize requests from memorysensitive applications if the performance degradation exceeds a pre-deﬁned threshold; otherwise, FR-FCFS is used.
This is to strike a balance between row buffer locality
and performance loss caused by the memory interference
in the CMP. Unlike requests serviced at the CMP’s main
memory, memory requests arriving at the main memory
of modern smartphone SoCs are heterogeneous in access
patterns as well as in latency criticality. Therefore, stateof-the-art memory scheduling policies, such as FR-FCFS
and STF, may not work effectively in smartphone memory
controllers.
To quantify the degree of performance degradation caused
by the interference in the shared cache as well as in the main
memory, we construct a number of scenarios that represent
different user behavior on smartphones. Our characterization
results in Figure 2 show that a signiﬁcant 10-33% performance slowdown on user-centric, interactive web browsing
is experienced when it is co-scheduled with other mobile
workloads, ranging from concurrent ﬁle download, transfer,
and compression to scientiﬁc algorithms that are fundamental building blocks of future mobile workloads. Some
degree of the performance degradation can be tolerated
without sacriﬁcing user satisfaction while other performance

23











	




 

 



	



	




	

the ﬁrst work that performs memory interference and
performance characterization studies for user-centric
and forward-looking applications running on a real
smartphone device.
2) We show that frequency throttling is an effective technique for maintaining the performance QoS of usercentric applications. The results presented in this paper
can be used to motivate memory interference mitigation
techniques that are designed to strike balance between
delivering high performance and meeting the performance QoS goals of latency-critical mobile workloads.
The remainder of the paper is organized as follows: In
Section II, we give background information for modern
smartphone SoCs and the memory architecture. Section III
outlines our real-device measurement infrastructure. Section IV presents the real-device memory interference characterization results while Section V describes the DVFS
throttling-based memory interference mitigation technique
and presents the evaluation results. Section VI summarizes
prior work in this area and Section VII concludes the paper.


	
	




	
   






Figure 3.
DRAM architecture showing memory request buffer with
requests from latency critical and non-latency critical cores

tors, the LLC is a shared resource for concurrent applications
that run on the application processor. The contention at the
shared LLC, as Section IV-A shows, is also a major factor of
performance degradation in modern smartphones. Therefore,
it is important to understand the shared last-level cache effect
on performance in smartphones.

II. BACKGROUND AND M OTIVATION
A. SoC Architecture Overview

B. Memory Scheduling and DRAM Architecture

Modern smartphone architecture consists of a plethora of
accelerators sharing the main memory and communication
fabric. Figure 1 illustrates the architecture of a typical
modern smartphone SoC. Due to the large number of accelerators that perform computations with the general-purpose
application processor, a mixture of memory requests arrive
at the main memory concurrently, leading to high contention
in the communication fabric as well as in the shared main
memory.
For example, the Low Power Audio Subsystem (LPASS)
is the hardware accelerator for audio decoding in Qualcomm’s SoC. As the decoding computations migrates from
the application processor to LPASS, the application core
could switch to a low power state, reducing the total power
consumption. However, since LPASS has an limited amount
of memory, the application processor has to wake up periodically and manage the data and computation transfer to
the accelerator. Similarly for other accelerators such as the
GPU or digital signal processors (DSP), data is transferred
frequently and periodically between the application processor and the accelerators through the shared communication
fabric and main memory. This leads to a high, bursty
memory bandwidth requirement for the memory.
Another commonly used accelerator is the modem that
receives data and places the data directly into the main
memory when users download ﬁles or receive emails. Thus,
the modem introduces additional contention to the communication fabric and the memory and can potentially cause
performance degradation to other tasks.
In addition to the contention at the communication fabric
shared between the application processor and the accelera-

In the widely prevalent open page DRAM designs, not
all LLC misses experience a ﬁxed memory access cost.
The memory access cost can vary from approximately 30
to 300 cycles (for a 2GHz processor, this corresponds to
15 to 150nS). This is owing to the fact that LLC misses
could result in row buffer hits, row buffer misses, or address
mapping to conﬂicting banks in the memory. When the row
buffer of the bank does not contain the row of data for the
referenced address, a row buffer miss is incurred. In this
case, when the data request is presented to the memory,
the row that is open in the row buffer is closed (row is
precharged) and the row corresponding to the new reference
is brought to the row buffer. Finally, the data is placed on
the data lines.
To maximize the performance of the main memory, FRFCFS memory scheduler prioritizes the service of memory
requests residing in the memory buffer based on row buffer
locality. Figure 3 illustrates the memory scheduling orders
for the ﬁrst-come ﬁrst-serve (FCFS) and the FR-FCFS
schedulers. The ﬁgure gives an example stream of memory
requests awaiting to be serviced at the memory request
buffer, where there are three references (A, B, C) from
non-latency critical applications and three references (X,
Y, Z) from latency critical applications. In the case of the
FCFS scheduler, the memory requests are serviced in the
arrival order at the memory request buffer. i.e (X, A, B,
Y, C, Z) whereas, under the FR-FCFS scheduler, memory
requests that exhibit high row buffer locality are prioritized
to maximize the overall system throughput. In this particular
example, while FR-FCFS maximizes the degree of row
buffer locality, it degrades the performance of latency-critical

24



Table I Device Speciﬁcation






	



	






applications by de-prioritizing latency-critical memory requests, leading to potential violation of QoS. In either schedulers that modern smartphones implement, the interleaving
references at the memory request buffer alters memory row
buffer localities observed by individual applications and
introduce un-controlled performance degradation that could
result in lowered user experience.



%"-," 
 

Figure 5.

"&%","/"%


&/(#


&&

#.(#&"#,##

#%

&"#%%-!

"&"(#

&"#%%- 

We perform real device experiments on a Google Nexus
5 smartphone which houses a Qualcomm MSM8974 Snapdragon 800 SoC. The SoC has four Krait cores in the application processor with a 2GB Low Power DDR (LPDDR)
memory. There are separate L1 instruction and data caches
for each core and a shared uniﬁed L2 cache of 2MB. The
device runs a rooted Android 4.4 KitKat OS. There are
two programmable accelerators—the Adreno GPU and the
Hexagon Digital Signal Processor (DSP). The Adreno GPU
supports OpenGL ES 3.0 and OpenCL whereas the Hexagon
DSP encompasses aDSP (application) and mDSP (modem).
The speciﬁcation is outlined in Table I.
The Linux kernel is conﬁgured to enable performance
proﬁling with Qualcomm’s Snapdragon Performance Visualizer and Trepn Proﬁler [8], [9]. To eliminate the possibility
of thermal emergencies that cause unpredictable frequency
throttling for the application processor, we manually set
the frequency of the application processor to operate at
1.574GHz1 .


%#",'%%

A. Real Device Measurement Infrastructure

*"%"



"#(#+


%&#&()(#

This section introduces our experimental methodology for
real device measurement to quantify the degree of memory
interference in current and future smartphone workloads.




&'"

III. M ETHODOLOGY

#&"%%#

	


Figure 4. The application processor and GPU utilization under the usercentric smartphone applications. Note: the application processor utilization
is the average utilization over the four processor cores.
!
	 0
%%,
	 0
%%,
!
	!0
%%,

"#$%

Operating System
Chipset
Application Processor
ISA
L1 I/D Caches
L2 Uniﬁed Cache
GPU
Advanced Graphics API
DSP
Memory
Year of Release

Google Nexus5
Android Kit Kat
MSM8974 Snapdragon 800
Quad-core Krait
ARMv7
private 16KB per core
shared 2MB
Adreno 330
OpenGL ES 3.0/OpenCL
Hexagon DSP
LPDDR3 2GB
2014

#

L1 and L2 cache miss rates for the applications under study.

B. Benchmarks and Input Sets
We use frequently-executed interactive smartphone workloads to characterize the memory interference-caused performance degradation. We perform in-depth evaluations for the
interactive smartphone workloads—web browsing [10], [11]
and an open source media player (VLC) [12]. Web browsing
mainly executes on the application cores and occasionally
ofﬂoads computations to the Adreno GPU. The average
GPU utilization hovers at 4.97%. The VLC media player
could either execute on the application processor or on the
DSP. When focusing on the performance impact coming
from the shared LLC interference, we execute VLC on the
application processor (under the disabled acceleration mode)
whereas when focusing on the performance impact coming
from the shared communication fabric and the memory,
we execute VLC on the DSP (under the full-acceleration
mode). This represents a realistic execution behavior for

1 The highest application processor frequency of the Google Nexus5
smartphone is 2.26GHz. When operating at this frequency, the application
processor encounters frequent thermal emergencies, leading to unpredictable changes in memory interference behavior. While the experients
conducted in this paper are based on a lower frequency of 1.5GHz, the
characterization results are representative to smartphone workloads. We
expect to see even higher memory interference at a higher operating frequency, leading to more signiﬁcant performance degradation for interactive
smartphone workloads.

25

Table II Descriptions for the applications and the corresponding application domains.
App Domain

Representative
Algorithm

Description

User-centric
Web Browsing

bbench/GWB [10],
[11]

Media Player

VLC [12]

SPEC [13]
File
Compression/Decompression
Vehicle Space Optimization

Web browsing sequentially loads and renders a set of popular websites, including
Amazon, BBC, CNN, Craiglist, eBay, ESPN, Google, MSN, Slashdot, Twitter, and
YouTube, from the secondary storage iteratively.
VLC is an open source media player that is used to render a full-screen HD (720p)
H264 MPEG-4 video of size 29MB.

bzip2

A compression/decompression algorithm based on Julian Sewards bzip2 version 1.0.3.

mcf

An algorithm used for single-depot vehicle scheduling in public mass transportation.

Pattern Search

hmmer

An algorithm that performs sensitive database searching, using statistical description
of protein sequences and is typically used in computational biology to search for
patterns in DNA sequences.

PARSEC [14]
Image Recognition
and
Augmented
Reality
Rodinia [15]

ferret

An image recognition application that is commonly used for content-based similarity
search of feature-rich data, such as images or videos.

Sensor Data Analysis

back
(bp)

Image Processing

heartwall; srad

Thermal Prediction
and Management

hotspot

Video Games

lavamd

Medical App
Communication Protocols
Map and Navigation
Graph
Search/Traversal

propagation

needleman-wunsch
(nw)
LU-decomposition
(lu)
nearest neighbors
(nn)
breadth ﬁrst search
(bfs)

A machine learning algorithm that trains the weights of connecting nodes on a layered
neural network.
A movement tracking algorithm of a heart over a sequence of 104 ultrasound images;
a diffusion method for ultrasonic and radar imaging applications based on partial
differential equations.
An iterative processor thermal modeling algorithm that solves a collection of differential equations.
An algorithm that calculates particle potential and relocation due to mutual forces
between particles within a large 3D space.
A non-linear global optimization method for DNA sequence alignments.
A numerical method that factors a matrix as the product of a lower triangular matrix
and an upper triangular matrix for solving a system of linear equations.
An algorithm that ﬁnds the k-nearest neighbors from an unstructured data set.
An algorithm that traversing/searching tree data structures.

mobile workloads. We also verify that the full-acceleration
VLC media player requires minimum utilization on the
application processor and the Adreno GPU. Figure 4 shows
the corresponding application processor and GPU utilization
for the user-centric smartphone applications.
In addition, we use algorithms from various benchmark
suites, including Rodinia [15], SPEC [13], and PARSEC [14]
to represent future smartphone workloads. The Rodinia
applications used are written in OpenMP and OpenCL2 and
the remaining applications used are written in C/C++. All
benchmarks are cross-compiled on the host machine with
ARM-Android NDK tool chains [16], and are statically
assigned to a speciﬁc core. The binaries are pushed to the
device and are launched from the host machine via the

adb terminal. Table II describes the application domains
represented by the algorithms used in this paper. Figure 5
shows the memory intensities of the applications in L1 and
L2 cache miss rates.
C. Metrics
Performance degradation is used as a metric to quantify
memory interference and is deﬁned as follows –
1) For web browsing, performance degradation is the
webpage loading time when the browser runs with other
co-scheduled applications normalized to the time when
it runs alone.
2) For media player, performance degradation is the
frames rendered per second (fps) when media player
runs with other co-scheduled applications normalized
to the fps when it runs alone.
3) For all other workloads, performance degradation is
the execution time when an individual application runs

2 We run the OpenMP version of the Rodinia applications on the application processor whereas the OpenCL version of the Rodinia applications are
run on the programmable GPU to represent GPGPU workload scenarios.

26

with other co-scheduled applications normalized to the
execution time when it runs alone.
We consider the performance QoS target of web browsing
as the 3-second mark which corresponds to a tolerable 9%
performance degradation level whereas the target for media
player is 30-fps, which corresponds to zero dropped frames.

The overall system throughput degradation for the ﬁve
representative workloads studied in this paper varies from 1
to 7% (geometric means), much lower than the observed
performance degradation in shared chip-multiprocessor
caches [17], [18], [19]. We infer this to the relatively
large L2 cache in the Nexus5 device (2MB), the lower
application processor operating frequency (1.5GHz), and the
unique memory intensity characteristics of the smartphone
workloads. While the degree of the performance degradation
in the sustained computation-bound smartphone workloads
is less pronounced, it is still important to devise effective
designs to bound the performance degradation and improve
the system throughput. This is especially useful if some applications in the workload combinations are latency-critical,
e.g., the medical app-related algorithms.
b) Interactive, real-time smartphone workloads: In addition to compute-intensive workloads, we also evaluate the
performance degradation experienced by interactive, realtime smartphone workloads, which are representative of
today’s smartphone user-oriented scenarios. We run web
browsing and the VLC media player individually with two
other randomly chosen applications from Table II on the
four-core Krait processor. To minimize compute resource
contention and to focus on memory interference-induced
performance degradation, we reserve two cores for web
browsing. The setup is based on the observation from a
recent study [3] that shows the utilization of web browsing
hovers at around two cores. Similarly, we reserve two of the
four Krait cores to the VLC media player. When the media
player is executed on the application processor, it requires
a minimum of two cores to achieve the desired frame rate,
i.e., 30 frames per second (fps).
When web browsing and other compute-oriented workloads are co-scheduled on the application processor, the
performance of web browsing is degraded by 6% to 27%
(Figure 7). This performance degradation comes from interference at the memory subsystem. Some of the performance
degradation can be tolerated while further performance
degradation directly translates to lowered user experiences.
In the case of web browsing, the performance degradation
under 9% can be tolerated such that the web page loading/rendering can still be achieved under the 3-second mark3
As Figure 7 shows, the performance of web browsing is
degraded beyond the 3-second mark (9%) in most of the
workloads.
We perform similar experiments to quantify the degree
of memory interference-caused performance degradation for
media player. Figure 8 shows that, the performance of VLC
is degraded by up to 21%. In contrast to web browsing,
most of the time the performance of the media player
is unaffected by the co-scheduled workloads. Its performance, however, can be signiﬁcantly degraded (by 21%)
when the co-scheduled applications are memory-intensive.

IV. R EAL D EVICE M EMORY I NTERFERENCE
C HARACTERIZATION
In this section, we present the experiments designed to
characterize the results of performance degradation caused
by memory interference in two different scenarios—memory
interference in the application processor, and in the shared
communication fabric and main memory. Characterization
results presented here are based on the Nexus 5 smartphone.
A. Scenario 1: Application Processor Memory Interference
To quantify the degree of performance degradation caused
by the memory interference within an application processor,
we design experiments to evaluate the performance effects
in A) sustained computation-bound smartphone workloads,
and B) interactive, real-time smartphone workloads.
a) Sustained computation-bound smartphone workloads:
Compute-intensive workloads form the building blocks for
future mobile workloads such as augmented reality and
navigation, which requires image processing, image recognition algorithms, machine learning algorithms, optimization
algorithms, and many others.
We construct ﬁve representative workloads for different
computation scenarios—Augmented Reality (WL1), Video
Game (WL2), Sensor-based Medical App (WL3), Vehicle
Navigation (WL4), and Medical DNA Sequencing App
(WL5), described in Table III. We evaluate the performance
degradation in each scenario in the presence of application
processor last-level cache and main memory interference.
Figure 6 shows that, when compared to the performance of
each application running alone on the smartphone application processor, applications experience 0.5% to 17.1% performance slowdown. This performance degradation comes
from the memory interference in the shared L2 cache of
the application processor and the main memory used by all
co-scheduled applications.
In particular, the performance degradation is most pronounced in the Augmented Reality workload (WL1). The
performance of the compression algorithm is signiﬁcantly
affected by the memory intensive sensor data analysis algorithm when both are executed on the general purpose
application processor. This suggests that, in order to ensure performance guarantee of the compression algorithm
used for images collected with sensors, explicit memory
management is required. If the two algorithms were run on
accelerators, explicit data communication techniques should
be devised to ensure the two algorithms coordinate their
computations to minimize memory interference.

3 We

27

assume one second delay from the network latency.

Table III Workload combinations for Scenario 1a

Augmented Reality

Video Game

Sensor-based Medical App

Vehicle Navigation

GMEAN

image processing

medical app

sensor data analysis

pattern search

GMEAN

communication protocol

graph search

map navigation

GMEAN

communication protocol

medical app

graph search

sensor data anlysis

GMEAN

compression

image processing

communication protocol

video game

GMEAN

thermal management

image recognition

sensor data analysis

18
15
12
9
6
3
0

vehicle space…

Use Case (Workload Combination)
Compression, sensor data analysis, image recognition, thermal management (bzip2, bp, ferret, hotspot)
Video game, communication protocol, image processing, compression (lavamd, lu, srad, bzip2)
Sensor data analysis, graph search/traversal, medical app, communication protocol (bp, bfs, nw, lu)
Map navigation, graph search, communication protocol, vehicle space optimization (nn, bfs, lu, mcf)
Pattern search, sensor data analysis, medical app, image processing (hmmer, bp, nw, heartwall)

Augmented Reality
Video Game
Sensor-based Medical App
Vehicle Navigation
Medical DNA Sequencing App

compression

% Performance Degradation (Compared to
Each Application Running Alone)

WL1:
WL2:
WL3:
WL4:
WL5:

Medical DNA Sequencing
App

Figure 6. Scenario 1a: Application processor memory interference. Performance degradation of each application compared to its performance when
running alone on the application processor.

			
	
	







	


	

	






	

	















	

	



	


	

	









	
Figure 7. Scenario 1b: Performance degradation S-curve for web browsing
when it is co-scheduled with other applications on the application processor.










	




Figure 8. Scenario 1b: Performance degradation S-curve for media player
when it is co-scheduled with other applications on the application processor.

This is exempliﬁed by the workload combination under
which the media player is executed concurrently with the
most memory-intensive computation-oriented applications,
i.e., sensor data analysis (bp) and medical app (nw). The
21% performance degradation corresponds to a sub-25fps
frame rate, leading to poor video quality and lowered user
experience.

a) Sustained computation-bound smartphone workloads:
To quantify the performance impact when the communication fabric and the memory are shared by sustained
computation-oriented co-scheduled workloads, we design
experiments to perform computations on the application processor and the programmable GPU. We construct ﬁve representative workloads for different computation scenarios—
Thermal Management (WL6), Mobile Medical App (WL7),
Online Medical App (WL8), Vehicle Routing (WL9), and
Augmented Reality (WL10). We run one application on
the application processor and another application on the
programmable GPU to evaluate the effect of shared memory

B. Scenario 2: Memory Interference in the Shared Communication Fabric and Main Memory
This section presents the characterization results for workloads that utilize smartphone accelerators and focuses the
memory interference results at the shared main memory.

28

Table IV Workload combinations for Scenario 2a
WL6: Thermal Management
WL7: Mobile Medical App
WL8: Online Medical App
WL9: Vehicle Routing
WL10: Augmented Reality

Use Case
WL11: Browsing +
music player
WL12: Browsing +
video conferencing
Use Case
WL13: VLC + augmented reality
WL14: VLC + vehicle routing
WL15: VLC +
medical app
WL16: VLC +
Skype

25

Workload Combination
web browsing, audio playback
web browsing, Skype
Workload Combination [DSP,cpu,gpu]
Video
playback,
image
recognition/augmented reality, sensor data
analysis [VLC, ferret, bp]
Video playback, vehicle space optimization, map/navigation [VLC, mcf, nn]
Video playback, communication protocol,
medical app [VLC, lu, nw]
VLC, Skype

WL8

WL9

WL10
!! !"

Figure 9. Scenario 2a: Memory interference in the shared communication
fabric and main memory.

!<

!$


	

	

!%

		

		



		

			


		


	


	

		

GMEAN

image recognition

map and navigation

GMEAN

sensor data analysis

GMEAN

medical app

GMEAN

communication protocol

GMEAN

medical app

WL7





WL6

map and navigation

communication protocol

0

vehicle space…

5

	

10

!$
!<%
!<
!"%
!"
!!%
!! 

	
! %
!
	


15

	




	


20

thermal management

% Performance Degradation (Compared to
Each Application Running Alone)

Table V Workload combinations for scenario 2b

Use Case (Workload Combination
[cpu,gpu])
Thermal management/prediction,
communication protocol [hotspot,
lu]
Map and navigation, medical app
[nn, nw]
Communication protocol, medical
app [lu, nw]
Vehicle space optimization, sensor
data analysis [mcf, bp]
Image recognition, map and navigation [ferret, nn]

!&

Figure 10. Scenario 2b: Performance degradation of interactive, real-time
smartphone workloads.

contention in smartphone SoCs. Table IV describes the
workload combinations in more detail.
The performance degradation results vary across the
different workload combinations. Figure 9 shows that the
performance degradation of the application running on the
application processor is signiﬁcant (21.7% for the map
and navigation application in WL7) while the performance
degradation of the application running on the GPU is below
4.18% (the medical app in WL8). In general, we observe
that the application which runs at the application processor is
more sensitive to the main memory contention. We attribute
this to the higher memory trafﬁc requirement of the application running on the programmable GPU. This agrees with the
observation from prior studies that focus on the shared cache
and memory between a chip-multiprocessor (CMP) and a
discrete GPU [20], [21]—without explicit management, the
bandwidth demanding GPU could signiﬁcantly degrade the
performance of CMPs while the GPU performance is largely
unaffected.
b) Interactive, real-time smartphone workloads: To quantify the performance degradation experienced by web browsing and VLC media player, we design experiments to cause
memory interference at the main memory between the

application processor and the programmable accelerators,
i.e., the Adreno GPU and the Hexagon DSP. Speciﬁcally,
we run web browsing together with one other sustained
computation-bound application on the application processor
and a GPGPU workload on the Adreno GPU. In addition,
we run VLC that is fully accelerated on the Hexagon
DSP together with two other sustained computation-bound
applications on the application processor and a GPGPU
workload on the Adreno GPU4 .
Overall, web browsing experiences signiﬁcant performance degradation caused by memory interference at the
main memory whereas VLC media player does not observe
any performance degradation in the workload combinations
4 The design principle behind the experimental methodology is to
maximize the computation resources while minimizing the likelihood of
computation resource contention. For example, when running an OpenCL
application on the programmable GPU, we allocate one of the four Krait
cores to the OpenCL application for executing the host-side serial part of the
code. Similar, when running the VLC media player on the programmable
DSP, we allocate one of the four Krait cores for handling the control and
accelerator management. Then we maximize the utilization on the Krait
processor with additional co-scheduled workloads.

29


!!!$!#
$&

described in Table V56 . Figure 10 compares the performance
of the user-centric, interactive smartphone applications –
web browsing and media player in the co-scheduled workloads. The web browsing performance is signiﬁcantly degraded by 34% when users browse web pages and make
Skype calls at the same time. We attribute the reason for the
observed performance drop to the intensive data streaming
rate of the co-scheduled Skype call. On the other hand, web
browsing experiences 2% performance slowdown when it is
co-scheduled with music playback. This performance degradation can be tolerated under the 3-second mark without
affecting user experience. In addition, Figure 10 shows that
the performance of media player is never affected by the coscheduled applications when it is executed on the dedicated
Hexagon DSP while the performance of the co-scheduled applications could experience degradation. The un-affected fps
performance could be attributed to the relatively intensive
memory access rate of the media player when running on the
dedicated accelerator compared to that of other applications.
For web browsing and other user-interactive workloads,
we envision the performance degradation to worsen as more
and more GPU cores and accelerators are being integrated
into the smartphone chip, leading to even more signiﬁcant
contention at the main memory and resulting in additional
performance loss and unpredictability. Based on the detailed
performance characterization results here, we present a frequency throttling-based technique to mitigate the memory
interference and to control the performance degradation
experienced by the latency-critical, user-centric workloads
next.
V. T HROTTLING - BASED M EMORY I NTERFERENCE
M ITIGATION
Throttling-based memory interference mitigation technique based on the DVFS control knob has been investigated
in many prior works [22], [23], [24]. These previous studies
are mainly simulation-based and the designs are tailored
for reducing shared cache and main memory contention in
the CMP domain. By leveraging the per-core DVFS control
knob that is available on the Google Nexus5 device, we
explore the effectiveness of the throttling-based technique
for reducing the memory interference between user-centric
applications, i.e., web browsing and the media player, and
other interfering co-scheduled applications.
The principle behind the frequency throttling-based technique is that the memory intensity of an application monotonically increases with the core operating frequency. Thus,
to reduce the memory interference between a user-centric,
latency-critical application and other co-scheduled applications, one potentially plausible solution is to reduce the






!!
!#!
!'",#0+"!





!!
!#!
	
(!,#0/+"!
















! &(-	).
	
	


	
	


Figure 11. Performance degradation of interactive, real-time smartphone
workloads when DVFS is applied on interfering applications. Media player
is able to meet performance QoS target when interfering applications are
scaled to 300MHz while web browsing is unable to meet its performance
QoS target.

operating frequency of the processor cores servicing the coscheduled workloads.
To explore whether the throttling-based technique can effectively lower memory interference and, therefore, improve
the performance degradation observed by web browsing and
the VLC media player, we focus our study on two workloads
– the workload combinations under which web browsing and
the VLC media player receive the most performance degradation in our characterization results presented in Section IV:
Figure 7 and Figure 8. The applications in the corresponding
workload combinations are (1) web browsing, sensor data
analysis, and medical app and (2) VLC media player, sensor
data analysis, and medical app.
Figure 11 depicts the performance of web browsing and
media player when the other two co-scheduled applications are running at different frequency settings. The x-axis
represents the core frequencies while the y-axis represents
the performance degradation of web browsing and media
player when running with the sensor data analysis and the
medical app. As the operating frequency of the interfering applications decreases, the performance degradation of
web browsing and media player reduces, as expected. At
300MHz, the performance degradation of web browsing is
11%, still higher than the tolerable 9% performance degradation (which corresponds to the 3-second mark providing
satisfactory user experience). This is the lowest frequency
setting for the co-scheduled applications under which the
best possible performance QoS target of web browsing can
be met. On the other hand, if we increase the operating
frequency of web browsing to 1.7GHz at the risk of potential thermal emergency, the 3-second mark required by
web browsing can be fulﬁlled. As Figure 12 shows, the
QoS target of web browsing is achieved at the expense of
performance degradation of the other two applications.

5 Web browsing utilizes both the application processor and the Adreno
GPU; therefore, the workload combinations are designed to utilize the
remaining available accelerators, e.g. LPASS, Hexagon DSP, and CP.
6 VLC is executed on the Hexagon DSP in the full acceleration mode
whereas the other co-scheduled applications are run on the application
processor and the Adreno GPU respectively.

30

In summary, the evaluation results presented here serve
as an initial investigation for throttling-based memory interference mitigation techniques. The real system evaluation
results indicate that DVFS-based throttling techniques are
effective and will motivate full-ﬂedged dynamic frequency
throttling controller designs servicing heterogeneous, highlyparallel smartphone architectures. We foresee two key design
challenges – ﬁrst, the mapping of workload deadlines to
performance degradation needs to be expressed to the dynamic frequency throttling manager. In the case described
in this paper, the performance degradation is controlled
by varying the frequency of the running cores. Second,
there could be cases where a DVFS-based mechanism does
not always reduce performance degradation caused by the
interference in the memory effectively. The effectiveness is
tightly coupled with memory access intensity as well as
memory access patterns, as shown in the web browsing and
media player results presented here – we can see that web
browser does not receive the same amount of beneﬁt as the
media player does.

Performance Degradation
Compared to Each Application
Running Alone at 1574MHz

4
3.5
3
2.5
2
1.5
1
0.5
0
300
Web Browser

652

883
1036
1267
CPU Frequency [MHz]

Sensor Data Analysis

1574

Medical App

Performance Degradation
Compared to Each Application
Running Alone at 1574MHz

Figure 12.
Performance degradation of web browser and interfering
applications when DVFS is applied on interfering applications.
4
3.5
3
2.5
2
1.5
1
0.5
0
300
652
883
1036
1267
1574
CPU Frequency [MHz]

Media Player

Sensor Data Analysis

VI. R ELATED W ORK
To the best of our knowledge, this is the ﬁrst work
that performs in-depth performance characterization on real
smartphone devices, considering the memory request behavior of the application processor as well as other available
accelerators. Most of prior works focused their studies on
memory scheduling for applications running simultaneously
on homogeneous, symmetric multiprocessors. Given the
rise of heterogeneous SoC architectures and its increasing
importance, a few recent works investigated the resource
sharing implications of the numerous accelerators present in
today’s smartphones. Nachiappan et al. presented a latency
criticality-aware design for the application processor that
is shared by the accelerators on smartphone SoCs with a
simulation-based evaluation [25]. Narancic et al. showed that
the memory subsystem is a major performance bottleneck
for mobile systems handling both application processor and
accelerator memory trafﬁc [26]. By varying the memory
addressing schemes, their trace-based simulation results
showed that the memory subsystem performance can be improved. Pandiyan et al. showed that the memory subsystem
performance of the application processor can also be significantly improved with common memory latency mitigation
techniques, such as advanced cache insertion/replacement
policies [27], [28], [29], [30], [31], prefetching, and appropriate TLB conﬁgurations [11]. By doing so, the data movement energy is expected to be reduced signiﬁcantly, leading
to more energy efﬁcient smartphone architectures [32].
In addition to the few previous works tailored for smartphones, in the past decade, lots of research effort has been
focused on designing shared resource management for CMP
systems – especially for the memory subsystem, e.g., [6],
[33], [34], [35], [36], and for CPU/GPU heterogeneous
computing platforms, e.g., [20], [21]. In addition, some prior

Medical App

Figure 13.
Performance degradation of media player and interfering
applications when DVFS is applied on interfering applications

Similarly, we investigate the effectiveness of the frequency
throttling-based technique for the VLC media player. Figures 11 and 13 show that if all applications operate at the
default 1.574GHz, the media player experiences signiﬁcant
performance slowdown by 13%, which corresponds to a
frame rate of 26-fps. To achieve 30-fps, the highest operating frequency that the two interfering applications should
operate at is 300MHz. In contrast to web browsing, the
throttling-based memory interference mitigation technique is
more effective for the media player. This is because media
player typically streams over a larger amount of data that
represent pixels from one frame to another. Thus, its memory
access pattern is far more intensive than web browsing.
Furthermore, the memory requests of media player typically
exhibits excellent row buffer locality. When running with
other co-scheduled applications, the high degree of memory
row buffer locality is destroyed by the interleaved memory references, leading to 1-21% performance degradation.
Throttling-based technique effectively controls the performance degradation by reducing the memory injection rate
from the co-scheduled applications and, thus, minimizing
the contention at the memory subsystem. Furthermore, the
performance degradation experienced by the co-scheduled
workloads is far less while the 30-fps frame rate requirement
is achieved.

31

R EFERENCES

works have looked at the use of DVFS or prefetcher control
knobs to modulate memory interference at the shared memory subsystem of CMPs [22], [23], [24], [37]. Most of these
works are simulation-based and are designed for applications
running on CMPs or CPU/GPU platforms. While some of
the previously proposed design concepts can be used to
manage the shared resources in the smartphone domain,
none of prior works has investigated and designed QoSaware shared resource management solutions speciﬁcally for
smartphones. Given the unique memory access intensities
and patterns of smartphone applications execute on the
wide range of heterogeneous processing elements in modern
smartphone SoCs, we need solutions tailored for smartphone
applications executing on highly heterogeneous smartphone
SoCs. We hope that the real device-based memory interference characterization results presented here could motivate
QoS-aware, latency criticality-aware solutions for this space.

[1] “Smartphone
market
share.”
[Online].
Available:
http://mobiforge.com/research-analysis/global-mobilestatistics-2014-part-a-mobile-subscribers-handset-marketshare-mobile-operators
[2] K.-T. Cheng and Y.-C. Wang, “Using mobile GPU for
general-purpose computing: a case study of face recognition
on smartphones,” in Proceedings of the International Symposium on VLSI Design, Automation and Test, 2011.
[3] C. Gao, A. Gutierrez, M. Rajan, R. Dreslinski, T. Mudge,
and C.-J. Wu, “A study of mobile device utilization,” in
Proceedings of the International Symposium on Performance
Analysis of Systems and Software, 2015.
[4] Y. Zhu and V. J. Reddi, “WebCore: Architectural support
for Mobileweb browsing,” in Proceeding of the International
Symposium on Computer Architecuture, 2014.
[5] S. Rixner, W. Dally, U. Kapasi, P. Mattson, and J. Owens,
“Memory access scheduling,” in Proceedings of the International Symposium on Computer Architecture, 2000.

VII. C ONCLUSION

[6] O. Mutlu and T. Moscibroda, “Stall-time fair memory access
scheduling for chip multiprocessors,” in Proceedings of the
International Symposium on Microarchitecture, 2007.

This paper presents a detailed performance characterization for the interference between co-scheduled smartphone
applications in the memory subsystem on a real mobile
platform. Given the abundance of heterogeneous computing units, the shared memory resource becomes highlycontended, leading to QoS violations for the feature-rich yet
timing-critical smartphone applications. Our results indicate
a signiﬁcant 34% and 32% performance degradation for
web browsing and media player, respectively, when they
are co-scheduled with other applications. To reduce the
interference, we implement and evaluate a simple yet effective memory interference mitigation scheme that throttles
the trafﬁc injection rate into the memory subsystem of
the co-scheduled applications, such that the performance
degradation for web browser is reduced to 11% and for
media player is completely eliminated. The characterization
results and the investigated mitigation scheme offer valuable
insights and understandings of the unique memory subsystem behavior of current and future smartphone applications
that we could leverage the insights presented in this paper
to build more intelligent mobile architectures that satisfy
user performance requirements while delivering high performance at the same time.

[7] Y. Zhu, M. Halpern, and V. Reddi, “Event-based scheduling
for energy-efﬁcient QoS (eQoS) in mobile web applications,”
in Proceedings of the International Symposium on High
Performance Computer Architecture, 2015.
[8] “Snapdragon
Performance
Visualizer.”
[Online].
Available:
https://developer.qualcomm.com/mobiledevelopment/increase-app-performance/snapdragonperformance-visualizer
[9] “Qualcomm
Trepn
Proﬁler.”
[Online].
Available:
https://developer.qualcomm.com/mobiledevelopment/increase-app-performance/trepn-proﬁler
[10] A. Gutierrez, R. Dreslinski, T. Wenisch, T. Mudge, A. Saidi,
C. Emmons, and N. Paver, “Full-system analysis and characterization of interactive smartphone applications,” in Proceedings of IEEE International Symposium on Workload
Characterization, 2011.
[11] D. Pandiyan, S.-Y. Lee, and C.-J. Wu, “Performance, energy
characterization and architectural implications of an emerging mobile platform benchmark suite – MobileBench,” in
Proceedings of IEEE International Symposium on Workload
Characterization, 2013.

ACKNOWLEDGMENT
The authors would like to thank the anonymous reviewers
for their feedback. This work was partially supported by
the NSF I/UCRC Center for Embedded Systems (NSF
grant 1361926) and by Science Foundation Arizona under
the Bisgrove Early Career Scholarship. The opinions, ﬁndings and conclusions or recommendations expressed in this
manuscript are those of the authors and do not necessarily
reﬂect the views of the Science Foundation Arizona.

[12] “VLC
media
player.”
[Online].
http://www.videolan.org/vlc/index.html

Available:

[13] “Standard
performance
evaluation
benchmark
suite,”
2006.
[Online].
https://www.spec.org/benchmarks.html

corporation
Available:

[14] C. Bienia, S. Kumar, J. P. Singh, and K. Li, “The PARSEC
benchmark suite: Characterization and architectural implications,” in Proceedings of the International Conference on
Parallel Architectures and Compilation Techniques, 2008.

32

[15] S. Che, M. Boyer, J. Meng, D. Tarjan, J. W. Sheaffer, S.H. Lee, and K. Skadron, “Rodinia: A benchmark suite for
heterogeneous computing,” in Proceedings of the IEEE International Symposium on Workload Characterization, 2009.

[28] A. Jaleel, K. B. Theobald, S. C. Steely, Jr., and J. Emer, “High
performance cache replacement using re-reference interval
prediction (RRIP),” in Proceedings of the Annual International Symposium on Computer Architecture, 2010.

[16] “Android
NDK
toolset.”
[Online].
Available:
https://developer.android.com/tools/sdk/ndk/index.html

[29] C.-J. Wu, A. Jaleel, M. Martonosi, S. C. Steely, Jr., and
J. Emer, “PACMan: Prefetch-aware cache management for
high performance caching,” in Proceedings of the Annual
IEEE/ACM International Symposium on Microarchitecture,
2011.

[17] A. Jaleel, W. Hasenplaugh, M. Qureshi, J. Sebot, S. Steely, Jr.,
and J. Emer, “Adaptive insertion policies for managing shared
caches,” in Proceedings of the International Conference on
Parallel Architectures and Compilation Techniques, 2008.

[30] S. M. Khan, Y. Tian, and D. A. Jimenez, “Sampling Dead
Block Prediction for last-level caches,” in Proceedings of the
Annual IEEE/ACM International Symposium on Microarchitecture, 2010.

[18] Y. Xie and G. H. Loh, “PIPP: Promotion/insertion pseudopartitioning of multi-core shared caches,” in Proceedings of
the International Symposium on Computer Architecture, 2009.
[19] M. K. Qureshi and Y. N. Patt, “Utility-based cache partitioning: A low-overhead, high-performance, runtime mechanism
to partition shared caches,” in Proceedings of the International Symposium on Microarchitecture, 2006.

[31] C.-J. Wu, A. Jaleel, W. Hasenplaugh, M. Martonosi, S. C.
Steely, Jr., and J. Emer, “SHiP: Signature-based hit predictor
for high performance caching,” in Proceedings of the Annual
IEEE/ACM International Symposium on Microarchitecture,
2011.

[20] J. Lee and H. Kim, “TAP: A TLP-aware cache management policy for a CPU-GPU heterogeneous architecture,”
in Proceedings of the International Symposium on HighPerformance Computer Architecture, 2012.

[32] D. Pandiyan and C.-J. Wu, “Quantifying the energy cost
of data movement for emerging smart phone workloads on
mobile platforms,” in Proceedings of the IEEE International
Symposium on Workload Characterization, 2014.

[21] V. Mekkat, A. Holey, P.-C. Yew, and A. Zhai, “Managing
shared last-level cache in a heterogeneous multicore processor,” in Proceedings of the International Conference on
Parallel Architectures and Compilation Techniques, 2013.

[33] L. R. Hsu, S. K. Reinhardt, R. Iyer, and S. Makineni, “Communist, utilitarian, and capitalist cache policies on CMPs:
caches as a shared resource,” in Proceedings of the International Conference on Parallel Architectures and Compilation
Techniques, 2006.

[22] E. Ebrahimi, C. J. Lee, O. Mutlu, and Y. N. Patt, “Fairness
via source throttling: A conﬁgurable and high-performance
fairness substrate for multi-core memory systems,” in Proceedings of the international symposium on Architectural
Support for Programming Languages and Operating Systems,
2010.

[34] K. J. Nesbit, N. Aggarwal, J. Laudon, and J. E. Smith, “Fair
queuing memory systems,” in Proceedings of the International Symposium on Microarchitecture, 2006.
[35] N. Raﬁque, W.-T. Lim, and M. Thottethodi, “Effective management of DRAM bandwidth in multicore processors,” in
Proceedings of the International Conference on Parallel Architecture and Compilation Techniques, 2007.

[23] A. Herdrich, R. Illikkal, R. Iyer, D. Newell, V. Chadha, and
J. Moses, “Rate-based QoS techniques for cache/memory in
CMP platforms,” in Proceedings of the International Conference on Supercomputing, 2009.

[36] C.-J. Wu and M. Martonosi, “Adaptive timekeeping replacement: Fine-grained capacity management for shared CMP
caches,” ACM Trans. Archit. Code Optim., vol. 8, no. 1, 2011.

[24] X. Zhang, R. Zhong, S. Dwarkadas, and K. Shen, “A ﬂexible framework for throttling-enabled multicore management
(TEMM),” in Proceedings of the International Conference on
Parallel Processing, 2012.

[37] ——, “Characterization and dynamic mitigation of intraapplication cache interference,” in Proceedings of the IEEE
International Symposium on Performance Analysis of Systems
and Software, 2011.

[25] N. Chidambaram Nachiappan, P. Yedlapalli, N. Soundararajan, M. T. Kandemir, A. Sivasubramaniam, and C. R. Das,
“Gemdroid: A framework to evaluate mobile platforms,” in
Proceedings of the International Conference on Measurement
and Modeling of Computer Systems, 2014.
[26] G. Narancic, P. Judd, D. Wu, I. Atta, M. Elnacouzi, J. Zebchuk, J. Albericio, N. Enright Jerger, A. Moshovos, K. Kutulakos, and S. Gadelrab, “Evaluating the memory system
behavior of smartphone workloads,” in Proceedings of the
International Conference on Embedded Computer Systems:
Architectures, Modeling, and Simulation, 2014.
[27] M. K. Qureshi, A. Jaleel, Y. N. Patt, S. C. Steely, and J. Emer,
“Adaptive insertion policies for high performance caching,”
in Proceedings of the Annual International Symposium on
Computer Architecture, 2007.

33

Quantitative Analysis of Control Flow Checking Mechanisms
for Soft Errors
Aviral Shrivastava, Abhishek Rhisheekesan, Reiley Jeyapaul, and Carole-Jean Wu
School of Computing, Informatics and Decision Systems Engineering
Arizona State University

Aviral.Shrivastava@asu.edu, Arhishee@asu.edu, Reiley.Jeyapaul@asu.edu, and
carole-jean.wu@asu.edu
ABSTRACT
Control Flow Checking (CFC) based techniques have gained a
reputation of providing effective, yet low-overhead protection
from soft errors. The basic idea is that if the control flow –
or the sequence of instructions that are executed – is correct,
then most probably the execution of the program is correct. Although researchers claim the effectiveness of the proposed CFC
techniques, we argue that their evaluation has been inadequate
and can even be wrong! Recently, the metric of vulnerability has been proposed to quantify the susceptibility of computation to soft errors. Laced with this comprehensive metric,
we quantitatively evaluate the effectiveness of several existing
CFC schemes, and obtain surprising results. Our results show
that existing CFC techniques are not only ineffective in protecting computation from soft errors, but that they incur additional power and performance overheads. Software-only CFC
protection schemes (CFCSS [14], CFCSS+NA [2], and CEDA
[18]) increase system vulnerability by 18% to 21% with 17% to
38% performance overhead; Hybrid CFC protection technique,
CFEDC [4] also increases the vulnerability by 5%; While the
vulnerability remains almost the same for hardware only CFC
protection technique, CFCET [15], they cause overheads of design cost, area, and power due to the hardware modifications
required for their implementations.

1.

BACKGROUND AND INTRODUCTION

Continuous and exponential technology scaling is responsible
for the information revolution and the unprecedented integration of computing systems into our everyday lives. A negative consequence of technology scaling is that the transistors
within modern processors have become increasingly susceptible to transient faults. Among the many sources of transient
faults (e.g., electrical noise, external interference, cross-talk,
etc.) the strike of sub-atomic particles, mainly low and high
energy neutrons, cause majority of soft errors in electronic devices [9]. At the current technology node, everyday computing
systems, e.g., laptops, smart-phones, tablets, etc., experience a
Soft Error Rate (SER) of about once-a-year, but is expected
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from Permissions@acm.org. DAC’14, June 01 - 05 2014, San
Francisco, CA, USA.
Copyright 2014 ACM 978-1-4503-2730-5/14/06 $15.00.
http://dx.doi.org/10.1145/2593069.2593195.

Figure 1: CFCSS[14]: A software CFC scheme. It inserts a few instructions at the beginning of each basic
block. These instructions set the outgoing signature,
and check the incoming signature register. Checking
the incoming signature ensures the correctness of execution flow.
to increase exponentially to about once-a-day in a decade [7].
Reliability is thus rapidly emerging as a primary design metric.
Most techniques to protect programs from soft errors are
built around time and/or space redundancy (detailed in [11]).
The idea is to perform the same computation twice and see
if the results match; if not, then there is an error. Although
redundancy based methods have been considered effective in
providing system reliability, they are generally considered to
have high overhead (around 2X). In particular, even though
it may be possible to minimize/hide the performance overhead
of redundancy based techniques (through parallelization), their
power overheads cannot be hidden.
Control Flow Checking (CFC) techniques were proposed to
provide efficient protection from soft errors. The main idea is
that most soft errors will eventually manifest as errors in the
sequence of instruction execution. Therefore, just by making
sure that the sequence of instructions executed (or the control
flow of the program) is correct, significant protection can be
achieved. Figure 1 shows the basic CFC mechanism. Basically,
each basic block in the program is assigned a signature, which
is written in a register when the basic block is executed, and
when execution flows to another basic block, the signature is
checked to confirm if the execution is coming from the right basic block. Note that a CFC technique by itself does not provide
any protection – it merely provides the capability of detecting
errors. Combined with a scheme to recover from errors (e.g.,

restart from the beginning; or in the case of regularly created
checkpoints, continue from the last checkpoint when an error
is detected), CFC techniques can provide protection from soft
errors. In this paper, since we are interested in estimating the
protection achieved by CFC techniques, we assume that there
is some (but it does not matter which one) scheme to recover
from the control flow error detected.
The arsenal of control flow based soft error protection techniques span across design layers from hardware [3, 8, 10, 15],
software [1, 5, 13, 14, 19, 18, 20], and hardware-software hybrid
techniques [4, 16, 17, 21]. CFC techniques are attractive since
they can often be implemented with much less overhead (as
compared to full scale redundancy) and arguably provide a decent error coverage. Papers proposing CFC techniques perform
fault injection tests and conclude that their techniques are quite
effective in combating soft errors. However, we argue that the
experimentation of these previous papers has been inadequate
and even flawed!
Recently, the metric of Vulnerability has been proposed [12]
to quantitatively estimate the susceptibility of program execution on a processor. A bit is vulnerable in a certain cycle of
execution, if a fault in it may cause a wrong result, otherwise,
it is not vulnerable. Adding up the number of vulnerable bits
in each cycle of the execution of a program, gives us the total
vulnerability of the program execution. Higher vulnerability of
program execution implies that the program execution is more
susceptible to soft errors.
The metric of vulnerability is more comprehensive, and does
not require detailed compute-intensive fault injection experiments. In fact, the vulnerability of a program execution can
be estimated in a single simulation run by tracking the events
on each bit of the processor, and counting the number of vulnerable bits. In this work, we use this metric to evaluate the
effectiveness of CFC techniques. While schemes to compute
the vulnerability of a program without CFC are known, techniques to estimate the vulnerability of a program with CFC is
not known. And that is the primary contribution of this paper.
In this paper, we:
(i) expose the shortcomings in the evaluation of protection capabilities of previous CFC techniques,
(ii) propose a systematic methodology to quantitatively estimate the protection achieved by CFC schemes, and
(iii) explain why existing CFC techniques are not effective.
The basic idea is to analyze each vulnerable < bit, cycle >
in the original execution, and determine the control flow errors
that it can cause. If any of the generated control flow error
can be detected by the CFC, then the < bit, cycle > is deemed
to be not vulnerable in the presence of CFC. We estimate the
vulnerability before and after applying CFC techniques. Our
results reveal that existing CFC techniques not only do not
protect execution from soft errors, but in fact incur additional
power and performance overheads. In particular, software only
CFC protection schemes (CFCSS [14], CFCSS+NA [2], CEDA
[18]) increase system vulnerability by 18% to 21% with 17% to
38% performance overhead. Hybrid CFC protection (CFEDC
[4]) increases vulnerability by 5%. Even though the vulnerability remains almost the same for hardware-only CFC protection
scheme (CFCET [15]), they incur overheads of design cost, area,
and power due to the hardware modifications required for their
implementation.

2.

WHAT WAS WRONG IN THE EVALUATION
OF CFC TECHNIQUES?

Although there are several ways to evaluate the effectiveness
of a CFC technique, most researchers have used some form
of “targeted fault injection” to do this. This is because most
other ways are either inaccessible, difficult to set up, or highly
compute-intensive. Even in targeted fault injection, there is a
question of how and where to insert faults. There have been 3
main approaches of targeted fault injection: i) Assembly code
instrumentation (most popular), used in CFCSS[14], CFCSSNA[2], CFCSS-IS[22], ii) gdb-based runtime fault injection used
in ACCE[19], CEDA[18], ACFC[20], and iii) fault injection in
memory bus, used in OSLC[8], SIS[17]. In addition researchers
have attempted to use probability based expressions to increase
the coverage of fault injections or even just use analytical subjective arguments to claim the superiority of their CFC technique over others. Due to space constraints, we will explain
the problems with assembly code instrumentation approach.
Similar problems exist with other evaluation schemes also.
In the assembly code instrumentation scheme, the binary or
assembly code of the benchmark program is instrumented to
corrupt bits in instructions, essentially simulating a soft error
in the processor pipeline during program execution. Depending
on which instruction the fault is injected into, and which bit
in the instruction is toggled several control flow error types,
e.g., a branch becoming a non-branch (branch deletion), a nonbranch becoming a branch (branch creation), error in branch
offset (target address error), branch predicate error (branching
error) can be simulated. The effectiveness of a CFC technique
is estimated by counting the number of these control flow errors
caught by the CFC technique divided by the number of faults
inserted.
There are at least three problems in this approach, because
of which the number reported does not represent the protection
afforded by the CFC scheme against soft errors:
i) Distribution of the faults in space (across microarchitectural components) is not correct: Inserting faults in
the binary or assembly simulates the effect of fault injection in
instruction cache, or the instruction register (or IF/ID pipeline
stage). It does not model the effect of faults in other processor
components, e.g., fault in the program counters, other pipeline
stages, data cache, register file, or load store buffer. Control
flow errors can occur due to faults in these microarchitectural
components also. For example, if there is a soft fault in the link
register (which contains the address to return from the function
call), control flow becomes wrong.
ii) Distribution of faults in time (how many faults
in a micraorchitectural component) is not correct: In
binary or assembly instrumentation scheme, researchers randomly choose a bit in the instruction at a randomly chosen
address, and flip it. Such a scheme will result in even probabilities of fault insertion in any bit in the instructions. This does
not accurately represent the distribution of transient faults.
Since transient faults are evenly distributed in space and time,
the probability of a transient fault in bits in a microarchitectural component is proprotional to the amount of time the bit
is present in the component. For example, the probability of a
fault being inserted in an instruction in cache is proportional to
the time it is resident in the cache. And therefore instructions
that are inside a loop have a much higher probability of being
hit by a fault than an instruction that executes only once, and
is quickly replaced by another in the cache.

iii) Only insert faults that cause not-successor control flow errors: Finally, and the most important point is
that inserting faults in binary/assembly only results in soft errors that cause, what we term as, “not-successor control flow
errors,” and insert very few faults that cause “wrong-successor
control flow errors.”
Definition 1: not-successor control flow errors happen when
execution flows from an instruction to one that is not a legitimate successor of the instruction. For example, if the execution
jumps from the last instruction of a basic block to a basic block
that is not a legitimate successor, or to the non-first instruction
of a legitimate successor basic block.
Definition 2: wrong-successor control flow errors are those
that cause execution to go from an instruction to a legitimate,
but incorrect successor. For example, if soft error happens in a
register altering its value. If a branch decision could be made
on wrong value of the register. The execution could still go to
a legitimate, but incorrect target.
When a fault is inserted in binary/assembly, then most often
it will cause a not-successor control flow error. Wrong-successor
control flow errors are typically caused by faults in data. For
example, if there is a fault in a value in data cache; it cannot
cause a not-successor control flow error. The only way it can
affect control flow is if it affects the outcome of a branch, then
it has caused a wrong-successor control flow error. Since this
methodology does not insert faults in data, such errors are not
inserted, and the effectiveness of CFCs in catching these errors
is not evaluated.

3.

OUR APPROACH: ESTIMATING VULNERABILITY IN THE PRESENCE OF CFC

Vulnerability is a comprehensive metric for estimating the
susceptibility of a the execution of a program on a processor.
As opposed to fault injection based techniques, it does not depend on accurate distribution of fault injections – which is a
very difficult thing to achieve. Vulnerability estimation systematically analyzes every < bit, cycle >, and declares it vulneable,
if a fault in can cause the program output to go wrong.
CFC techniques are implemented as a set of control flow
checks. A control flow check makes several < bit, cycle > that
were vulnerable before, not-vulnerable. For example, in figure
1, if there is a fault in PC during the fetch of the first instruction of B2, then it will be caught. Therefore the bits in the PC
at that cycle are not vulnerable after CFCSS is implemented.
In fact, the bits of PC of any instruction, which can become the
address of an instruction in basic block B2 have now become
non-vulnerable (because they will be caught by the CFC check).
Plus bits in the branch offsets of the other jump instructions
in the program that can become the address of an instruction
in B2 through a 1-bit flip are also no longer vulnerable.
The objective of vulnerability modeling in the presence of
CFC technique is to find the < bit, cycle >s that were vulnerable, but are no longer vulnerable after the implementation of
CFC – essentially calculating the protection afforded by the
CFC technique. We do this by breaking it into two steps: i)
for each vulnerable < bit, cycle >, find out which control flow
error it causes, and ii) find out if that control flow error can
be caught by the CFC that we are evaluating. The first step
is relatively CFC independent, and captures the impact of soft
errors in architectural bits on the control flow of the program,

while the second step is relatively architecture independent and
captures the capabilities of the CFC technique. We first start
with the second step.

3.1

Which control flow errors can the CFC catch?

Although CFC techniques attempt to detect control flow errors, most CFC schemes can only detect a subset of control
flow errors. A control flow error is identified by a pc → npc
transition, where pc and npc are two consecutive PCs that are
executed, such that in the correct execution npc should not
have been executed after pc. For example, if the execution
flows from a basic block to a basic block that is not a legitimate successor, then CFCSS can detect it, but it cannot detect
when the execution jumps to an instruction in the same basic
block.

Figure 2: In CFCSS[14], checking-instructions are
added to each basic block to set a variable to the basic
block signature; and to check if the execution is coming
from a legitimate parent basic block. When execution
flows to B2 from B1 (a legal parent of B2) the signature
checks out, but if the execution flows from B3 (not a
legal parent of B2), a control flow error happens.
To comprehensively model the protection achieved by a CFC
technique implementation, we build a CFC Protection Table –
a table of all the categories of pc → npc transitions, that may
lead to control flow errors in the system. The CFC Protection
Table for the CFCSS protection technique is shown in figure
3. The first column shows the location of pc, while the second column shows the location of npc. The third column lists
whether the erroneous control flow that occurs represented by
the pc → npc transition, where pc is at the location specified in
the first column, and npc is at the location specified in the second column, will be detected by the CFC or not. For protection
modeling for CFCSS, we label the code in the program with a
software based CFC implementation as: “original source code”
– that was part of original the unprotected program; and “CFC
code” – that is part of the additional code inserted during the
CFC implementation (respective sections of a basic block are
labeled in figure 2). The first row in figure 3 shows that CFCSS
cannot detect if the control flow error causes a pc → npc transition from an instruction in original source code of a basic block
(O), to the original source code of the same basic block (OS).
On the other hand, the second row shows that if the control
flow error causes an erroneous pc → npc transition from an instruction in original source code of a basic block. (O) to the
original source code of any other (different) basic block (OD),
then CFCSS will catch it. This is because, such a control flow
error will essentially bypass the signature update in the other

Figure 3: For each CFC technique, we build this table,
which shows whether the CFC technique can detect
each kind of erroneous pc → npc transition. This table
shows the protection model of CFCSS.

basic block, and then the signature will not match in the next
basic block. CL refers to the checking code of a legal target
of the basic block, and CA refers to the checking code of an
aliased target of the basic block.

3.2

What control flow errors are caused by a fault
in <bit,cycle>?

In general, modeling the effects of a fault, i.e., which control
flow errors will a flip in < bit, cycle > will cause is hard. This is
not only because the number of bits (and cycles) is very large,
but also because to find the effect of each fault, we essentially
have to follow the dependencies of the fault (analyze error propagation), and and see if it can affect the sequence of instructions
executed. Fortunately, our analysis is simplified by the observation that a fault in a < bit, cycle > can be of two types: i)
not-successor control flow error, and ii) wrong-successor control
flow error, and existing CFC techniques cannot detect wrongsuccessor control flow errors; they can only detect not-successor
control flow errors.
We perform a component-wise analysis. For each < bit, cycle >
tuple in each component, we find out, which control flow error
(in the last section) will it cause, and then we see, if the control
flow error is can be detected by the CFC technique or not. For
lack of space, we will discuss the fault analysis on PC for a 5stage in-order DLX pipeline [?], but our approach and analysis
is not restricted to the architecture. In fact, our experiments
are on the ARM v7-a architecture.
A bit flip in PC can cause an erroneous value to be written into the PC (updated next-PC); if the select bit for the
multiplexer (MUX) that chooses between PC+4, and branch
target indicates a non-branch instruction. For instance, let us
assume a non-branch instruction in the current cycle. The bit
flip in the PC causes it to change to a value that is at 1-bit
hamming distance from the current value. Therefore the PC
in the next cycle will become npc = H1(pc) + 4, where the
function H1(value v) calculates all the possible values that are
1-bit hamming distance from v. We calculate npc values for
every such 1-bit hamming distance values of pc, and then use
those pc → npc transition to find if the CFC technique will be
able to catch the erroneous transition or not. If the MUX select
bit indicates a branch instruction, the erroneous PC values are
overwritten in the next cycle with values from the branch target address field in the execute-memory pipeline register. Since
the erroneous PC value is not used to fetch instructions, the PC

bits in the current cycle can be considered not vulnerable.
In summary, most bit flips in the PC will most probably
cause a not-successor control flow error. However, some bit
flips in the PC can cause a wrong-successor control flow error.
Since not-successor control flow errors are typically caught by
existing CFC schemes, we conclude that PC is relatively well
protected by CFC schemes. However a bit flip in the register file
will in most cases result in a wrong-successor control flow error
by changing a branch outcome. As a result, it will be detected
by existing CFCs. There are exceptions though – for example if
the program jumps to an address specified in a register. Then
a bit-flip will most probably cause a not-successor control flow
error. As a result, most bit flips in the register file will not
be caught by CFC schemes. Similar is the story with other
components like caches, load store queues etc. We perform
detailed analysis of the impact of bit-flip in each bit of the
microarchitectural component. It is worth mentioning that bit
flips in several bits of the pipeline registers, e.g., the branch
target address can cause not-successor control flow errors.

4.

EXPERIMENTS AND ANALYSIS
Simulation Environment
Architecture
Pipeline
L1 D-Cache
L1 I-Cache
D-TLB / I-TLB
Physical Reg (INT/FP)
Architecture Reg (INT/FP)

ARM v7-a
5-stages (Out-Of-Order)
64KB (2-way)
32KB (2-way)
64 entries
128/128
16/32

Table 1: Experimental setup for the case-study application of the gemV+llVm framework.

4.1

Experimental Setup

We perform our experiments and quantitatively analyze the
effectiveness of CFC techniques using our gemV+llVm framework configured for the ARM v7-a architecture on MiBench
benchmarks [6] (more details in table 1). We have implemented
various state-of-the-art software CFC techniques, i.e., CFCSS,
CFCSS+NA, and CEDA, and the software part of the hybrid
technique CFEDC in the llVm compiler. The llVm compiler
also generates the information about the location of CFC instructions within each basic block, and the location of the original source code, as shown in figure 2. This information is
needed by the gemV simulator to model the protection achieved
by the CFC techniques. For each CFC technique, we construct
the CFE Protection Table (like that in figure 3 for CFCSS),
for each of the implemented CFC techniques, and provide it as
input to the gemV+llVm framework.

4.2

Vulnerability increases after software CFC!

Figure 4 plots the vulnerability of benchmarks when CFC
is applied, normalized to their vulnerability without CFC. For
this plot, the data cache is ECC protected. The plot shows
this vulnerability ratio for 5 CFC protection schemes, CFCSS,
CFCSS+NA, and CEDA (software techniques), CFCET (hardware technique), and CFEDC (hybrid technique). The last set
of bars show the vulnerability ratio averaged over all the benchmarks. The most important observation to be made from this
plot is that the vulnerability does not reduce after applying

Figure 4: The effective system vulnerability (normalized over original program vulnerability) of the various
CFC techniques, for a ECC protected L1 data cache.

the CFC protection. Software based CFC protection (CFCSS,
CFCSS+NA, and CEDA) increases system vulnerability for
most benchmarks (13 out of 14), and on an average increase
it by 18%-21%. For the gsm-u, the application of CEDA increases the vulnerability of the benchmarks by more than 50%.
Hybrid CFC technique, CFEDC, also results in an increase in
the vulnerability of most of the benchmarks, and on an average results in 5% increase of system vulnerability. Although
the vulnerability after applying the hardware scheme CFCET
does not increase, note that hardware techniques will have additional design, area, power and cost overheads.
Figure 5 plots the overhead in vulnerability and runtime
added by the CFCSS code inserted by the compiler. If the
vulnerability of the application after the application of CFCSS
is 1, then the dark portion of the left bar for each application
shows the vulnerability of the extra code added by CFCSS,
while the light portion of the bar shows the vulnerability of the
original code after the application of CFCSS (this is less than
the vulnerability of the original code before the application of
CFCSS). We can see that CFCSS code has a quite significant
vulnerability (on avaerage 32.5%). The right bar is the performance bar, and also has light and dark portions. If the runtime
of the application after applying CFCSS is 1, then the dark portion is the fraction of runtime spent in executing CFCSS code,
while the light portion is fraction of runtime spent in executing
the original code (which is approximately the same as the runtime of the original code before the application of CFCSS). The
reason we plot both the bars together is to show the correlation
in the increase in vulnerability and the increase in the runtime.
We can see that in the cases where the CFC code contributes a
larger fraction of the system vulnerability, it also contributes a
larger fraction towards the runtime. The correlation coefficient
between the vulnerability increase and runtime increase is 0.75.

4.3

Why are CFCs ineffective?

The reason exisinting CFCs do not protect computation from
soft errors is because they only protect from faults in some
microarchitectural components, e.g., PC, some bits of some
pipeline registers (ones before the branch address generation),
and branch target buffer, the link register, and the processor
status register. Faults in all other microarchitectural components, e.g., the data cache, all other pipeline register bits, register file, and all the buffers and queues in the processor are

Figure 5: The proportion of vulnerability and execution time contributed by the added CFC code and
original program code in the CFCSS implementation
is represented for the benchmarks.

not protected by existing CFC schemes. This is because faults
in these components rarely cause “not-successor control flow
errors.” Most often they cause “wrong-successor control flow
errors.” For example, a fault in the data cache will cause a
control flow error if the data value affects the outcome of the
branch, and that results in a “wrong successor control flow error”, and not a “not successor control flow error.”
The reason why vulnerability actually increases is because
the extra code added to implement CFC adds to the execution time, this in turn increases the time variables spend in
microarchitectural components, and that increases the system
vulnerability. In fact that increase more than outshines any
reduction achieved by CFC.
To evaluate the potential of protection possible through a
classicial CFC scheme, we assume a hypothetical CFC scheme
that can detect all the illegitimate control flow errors. Even
such a scheme can reduce the vulnerability of the execution by
only 4.04% – assuming no extra instructions to achieve such a
scheme.
We have modeled and experimented upon several CFC schemes,
and observed that they do not reduce the vulnerability. However, there are more CFC schemes that we have seen but not
implemented and tested. We believe that all of those schemes
will also result in an increase in the vulnerability – and are
therefore not useful. This is because existing CFC techniques
only compete with each other to detect more kinds and larger
fractions of “not-successor control flow errors.”. For example,
CFCSS cannot detect control flow erros when there is an aliased
block. CFCSS+NA fixes this problem. These two did not detect the error in the conditional execution. CEDA fixes that
loophole. The problem is that, as the techniques try to cover
more and more cases, they add additional instructions – which
backfires, and actually increases the vulnerability even more.
In fact, we observe that the more sophisticated the technique
is, higher is the increase in vulnerability.
Finally, there are some schemes (YACCA[5] and CEDA[18]),
that attempt to detect errors in branch direction. They use additional instructions to re-evaluate the branch condition; but
such schemes can only detect faults that (happened in the
branch operands) between the two computations. Therefore
they cannot have a significant impact.

5.

SUMMARY

This paper has a very surprising conclusion. We show that
existing CFC techniques (that have been developed to protect
programs from soft errors), actually make matters worse âĂŞ
they make programs more susceptible to soft errors. We show
this by estimating the vulnerability of the program “without
CFC”, and “with CFC”. We find that the vulnerability of the
program “with CFC” is higher than “without CFC” for software
and hybrid CFC schemes. This is because the small reduction
in program vulnerability achieved by CFC is overwhelmed by
the additional vulnerability due to the extra instructions that
implement the CFC. Also the reduction in CFC is quite small,
because the total number of bits even partially protected by
CFC are a small fraction of the total number of bits in the
processor.

6.

ACKNOWLEDGEMENTS

This work was partially supported by NSF grant CCF 1055094
(CAREER).

7.

REFERENCES

[1] Alkhalifa, Z., Nair, V. S. S., Krishnamurthy, N.,
and Abraham, J. A. Design and Evaluation of
System-Level Checks for On-Line Control Flow Error
Detection. IEEE Trans. Parallel Distrib. Syst. 10, 6
(June 1999), 627–641.
[2] Chao, W., Zhongchuan, F., Hongsong, C., Wei, B.,
Bin, L., Lin, C., Zexu, Z., Yuying, W., and Gang,
C. CFCSS without Aliasing for SPARC Architecture. In
International Conference on Computer and Information
Technology (CIT) (29 2010-july 1 2010), pp. 2094 –2100.
[3] Eifert, J., and Shen, J. Processor Monitoring Using
Asynchronous Signatured Instruction Streams. In
Twenty-Fifth International Symposium onFault-Tolerant
Computing (jun 1995), p. 106.
[4] Farazmand, N., Fazeli, M., and Miremadi, S. FEDC:
Control Flow Error Detection and Correction for
Embedded Systems without Program Interruption. In
Third International Conference on Availability,
Reliability and Security (march 2008), pp. 33 –38.
[5] Goloubeva, O., Rebaudengo, M., Sonza Reorda,
M., and Violante, M. Soft-error detection using
control flow assertions. In IEEE International Symposium
on Defect and Fault Tolerance in VLSI Systems (nov.
2003), pp. 581 – 588.
[6] Guthaus, M. R., Ringenberg, J. S., Ernst, D.,
Austin, T. M., Mudge, T., and Brown, R. B.
MiBench: A free, commercially representative embedded
benchmark suite. In Proceedings of the IEEE
International Workshop on Workload Characterization
(Washington, DC, USA, 2001), IEEE Computer Society,
pp. 3–14.
[7] Kayali, S. Reliability Considerations for Advanced
Microelectronics. In Proceedings of the 2000 Pacific Rim
International Symposium on Dependable Computing
(Washington, DC, USA, 2000), PRDC ’00, IEEE
Computer Society, p. 99.
[8] Madeira, H., and Silva, J. On-line signature learning
and checking: experimental evaluation. In Proceedings of
5th Annual European Computer Conference (may 1991),
pp. 642 –646.

[9] May, T. Soft Errors in VLSI: Present and Future. IEEE
Transactions on Components, Hybrids, and
Manufacturing Technology 2, 4 (dec 1979), 377 – 387.
[10] Michel, T., Leveugle, R., and Saucier, G. A New
Approach to Control Flow Checking without Program
Modification. In Twenty-First International Symposium
on Fault-Tolerant Computing (jun 1991), pp. 334 –341.
[11] Mukherjee, S. Architecture Design for Soft Errors.
Morgan Kaufmann Publishers Inc., San Francisco, CA,
USA, 2008.
[12] Mukherjee, S. S., Weaver, C., Emer, J.,
Reinhardt, S. K., and Austin, T. A Systematic
Methodology to Compute the Architectural Vulnerability
Factors for a High-Performance Microprocessor.
IEEE/ACM International Symposium on
Microarchitecture 0 (2003), 29.
[13] Nicolescu, B., Savaria, Y., and Velazco, R.
Software detection mechanisms providing full coverage
against single bit-flip faults. IEEE Transactions on
Nuclear Science 51, 6 (dec. 2004), 3510 – 3518.
[14] Oh, N., Shirvani, P., and McCluskey, E.
Control-flow checking by software signatures. IEEE
Transactions on Reliability 51, 1 (mar 2002), 111 –122.
[15] Rajabzadeh, A., and Miremadi, S. CFCET: A
Hardware-based Control Flow Checking Technique in
COTS Processors using Execution Tracing.
Microelectronics Reliability 46, 5 (2006), 959–972.
[16] Saxena, N. R., and McCluskey, W. K. Control-Flow
Checking Using Watchdog Assists and
Extended-Precision Checksums. IEEE Transactions on
Computing 39, 4 (Apr. 1990), 554–559.
[17] Schuette, M. A., and Shen, J. P. Processor Control
Flow Monitoring Using Signatured Instruction Streams.
IEEE Trans. Comput. 36, 3 (Mar. 1987), 264–276.
[18] Vemu, R., and Abraham, J. CEDA: Control-Flow
Error Detection Using Assertions. IEEE Transactions on
Computers 60, 9 (Sept. 2011), 1233–1245.
[19] Vemu, R., Gurumurthy, S., and Abraham, J. ACCE:
Automatic correction of control-flow errors. In Test
Conference, 2007. ITC 2007. IEEE International (oct.
2007), pp. 1 –10.
[20] Venkatasubramanian, R., Hayes, J., and Murray,
B. Low-cost on-line fault detection using control flow
assertions. In On-Line Testing Symposium, 2003. IOLTS
2003. 9th IEEE (july 2003), pp. 137 – 143.
[21] Wilken, K., and Shen, J. P. Continuous Signature
Monitoring: Efficient Concurrent-Detection of Processor
Control Errors. In Proceedings of the 1988 International
Conference on Test (Washington, DC, USA, 1988),
ITC’88, IEEE Computer Society, pp. 914–925.
[22] Wu, Y., Gu, G., Huang, S., and Ni, J. Control Flow
Checking Algorithm using Soft-based Intra-/Inter-block
Assigned-Signature. In Computer and Computational
Sciences, 2007. IMSCCS 2007. Second International
Multi-Symposiums on (Aug.), pp. 412–415.

STEAM: A Smart Temperature and Energy Aware
Multicore Controller
VINAY HANUMAIAH, DIGANT DESAI, BENJAMIN GAUDETTE, CAROLE-JEAN WU,
and SARMA VRUDHULA, Arizona State University

Recent empirical studies have shown that multicore scaling is fast becoming power limited, and consequently,
an increasing fraction of a multicore processor has to be under clocked or powered off. Therefore, in addition
to fundamental innovations in architecture, compilers and parallelization of application programs, there
is a need to develop practical and effective dynamic energy management (DEM) techniques for multicore
processors.
Existing DEM techniques mainly target reducing processor power consumption and temperature, and only
few of them have addressed improving energy efficiency for multicore systems. With energy efficiency taking
a center stage in all aspects of computing, the focus of the DEM needs to be on finding practical methods
to maximize processor efficiency. Towards this, this article presents STEAM – an optimal closed-loop DEM
controller designed for multicore processors. The objective is to maximize energy efficiency by dynamic
voltage and frequency scaling (DVFS). Energy efficiency is defined as the ratio of performance to power
consumption or performance-per-watt (PPW). This is the same as the number of instructions executed per
Joule. The PPW metric is actually replaced by P α PW (performanceα -per-Watt), which allows for controlling
the importance of performance versus power consumption by varying α.
The proposed controller was implemented on a Linux system and tested with the Intel Sandy Bridge
processor. There are three power management schemes called governors, available with Intel platforms. They
are referred to as (1) Powersave (lowest power consumption), (2) Performance (achieves highest performance),
and (3) Ondemand. Our simple and lightweight controller when executing SPEC CPU2006, PARSEC, and
MiBench benchmarks have achieved an average of 18% improvement in energy efficiency (MIPS/Watt) over
these ACPI policies. Moreover, STEAM also demonstrated an excellent prediction of core temperatures and
power consumption, and the ability to control the core temperatures within 3◦ C of the specified maximum.
Finally, the overhead of the STEAM implementation (in terms of CPU resources) is less than 0.25%. The
entire implementation is self-contained and can be installed on any processor with very little prior knowledge
of the processor.
Categories and Subject Descriptors: C.4 [Performance of Systems]
General Terms: Design, Algorithms, Performance
Additional Key Words and Phrases: Multicore, power and temperature modeling, closed-loop controller,
dynamic voltage and frequency scaling, dynamic thermal management, dynamic energy management, energy
efficiency, performance optimal, leakage power dependence on temperature, Kalman filter, least-squares
estimation

This work was supported in part by NSF grant CSR-EHS 0509540, NeTS 0905035; Center for Embedded
Systems grant DWS-0086; Science Foundation Arizona (SFAz) grant SRG 0211-07 and by the Stardust
Foundation.
Authors’ address: Arizona State University, 411 North Central Avenue, Phoenix, AZ 85004; email: {vinayh,
digantdesai, bgaudett, carole-jean.wu, vrudhula}@asu.edu.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for profit or commercial advantage and that
copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior specific permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c 2014 ACM 1539-9087/2014/09-ART151 $15.00

DOI: http://dx.doi.org/10.1145/2661430
ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

151

151:2

V. Hanumaiah et al.

ACM Reference Format:
Vinay Hanumaiah, Digant Desai, Benjamin Gaudette, Carole-Jean Wu, and Sarma Vrudhula. 2014. STEAM:
A smart temperature and energy aware multicore controller. ACM Trans. Embedd. Comput. Syst. 13, 5s,
Article 151 (September 2014), 25 pages.
DOI: http://dx.doi.org/10.1145/2661430

1. INTRODUCTION

The relatively recent transition to multicores has allowed the performance of processors to continue to improve within a limited power budget by exploiting concurrent execution of threads on multiple cores. As device scaling continues, keeping
the number of transistors within a core relatively constant (thereby reducing the
power per core) allows for increasing the number of cores, and improving performance by more aggressively exploiting parallelism. This strategy of increasing the
number of cores initially allowed circumventing the power wall of single-core processors, and this success raised the possibility of increasing the core count as the new
scaling strategy. Unfortunately, recent empirical studies [Esmaeilzadeh et al. 2011]
have shown that multicore scaling is fast becoming curtailed by rising power dissipation, whose limits are determined by the maximum possible temperature a chip
can tolerate. Because high temperatures lead to functional and timing errors, significantly reduce operational lifetimes, and increase power dissipation (which in turn
increases the temperature), an increasing fraction of the silicon will have to become
dark – under clocked or powered off, as the number of cores increases. One study
[Esmaeilzadeh et al. 2011] predicts that at 8nm, the fraction of dark silicon is expected
to exceed 50%. Therefore, besides increasing the parallelism, proper temperature and
energy management are now critical to circumvent this “utilization wall” of multicore
processors.
One way to overcome the utilization wall is to improve the energy efficiency of the
processors. Improving processor energy efficiency can be addressed at various design
stages, including architectural enhancements, compiler design and energy efficient algorithms. In addition to these, dynamic control of the cores can further improve the
processor’s energy efficiency. System level runtime techniques that perform energy
management under thermal constraints are referred to as Dynamic Energy Management (DEM), which is the focus of this work.
A common metric for measuring energy efficiency of a processor is performance/Watt
(PPW), which is equivalent to the number of instructions executed per Joule of energy. This article defines performance as the number of operations completed per
second. It is desirable to execute a processor at the most energy efficient operating
point. Figure 1 shows a plot of PPW versus frequency for the case of two SPEC
benchmark applications executing on the Intel quad-core Sandy Bridge processor.
One benchmark is characterized as a high instruction per cycle (IPC) workload, while
the other has a low IPC. Notice the concave nature of the curves. This concave nature guarantees the existence of a unique operating point that maximizes the energy efficiency. The PPW metric has been shown to be a quasiconcave function of
core frequency in Hanumaiah and Vrudhula [2012b]. Note that the maximum PPW
operating point shown in Figure 1 depends on the IPC of the workload as well as
other parameters such as temperature, power consumption, etc. As a result, determining the operating point is a nontrivial process. Much of the existing work aimed
at determining the optimal operating point has been done with offline simulations.
This has limited value for most modern applications and platforms. The DEM controller described in this article determines the most energy efficient operating point at
runtime.
ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

STEAM: A Smart Temperature and Energy Aware Multicore Controller

151:3

Fig. 1. Variation of PPW of high IPC (453.povray) and low IPC (450.soplex) of SPEC CPU2006 benchmark
suite on an Intel Sandy Bridge processor.

1.1. Challenges in Multicore DEM Controller Design

DEM for multicore processors involves a complex, multidimensional, constrained optimization problem on a multi-input, multioutput (MIMO) system. Dynamic voltage
and frequency scaling (DVFS) is one of the more commonly used DEM techniques to
adjust the performance and power consumption of the target processor. Despite the
dominance of multicore systems, most DEM mechanisms are still based on methods
developed for uniprocessors like proportional-integral-derivative (PID) controllers [Fu
et al. 2010; Skadron et al. 2002]. A PID controller is a generic control loop feedback
mechanism that calculates an “error” value as the difference between a measured output and a desired reference output value. The controller attempts to minimize the error
by adjusting the process control variables. The PID controller algorithm involves three
separate constant parameters: the proportional (depends on the present error), the
integral (accumulation of past error), and derivative (prediction of future error) values,
denoted P, I, and D, respectively. The weighted sum of these three actions is used to
control the process via. process control variables.
However, the use of PID controllers for optimizing DEM in multicore processors
is highly suboptimal. A multicore DEM controller needs to achieve globally optimal
core-frequency settings while accounting for cross-core temperature distribution. It
must be flexible enough to achieve various global objectives such as maximizing energy efficiency, maximizing performance with a given power/thermal constraint, or
guaranteeing deadlines under a power budget, when multiple tasks are executing concurrently. Being a MIMO system, the design of a multicore DEM controller with the
aforementioned characteristics while assuring stability and robustness is a much more
challenging task than the design of a single-core DEM controller.
1.2. Overview of Proposed STEAM Controller

We describe the design of a closed-loop controller called STEAM, which stands for
a Smart Temperature and Energy-Aware Multicore controller. STEAM can predict
the optimal voltage/frequency setting to achieve maximum energy efficiency, through
online power and thermal model estimations and the minimization of prediction error
using an recursive least squares (RLS) and Kalman filters.
Figure 2 depicts the structure of the STEAM controller. The structure consists of a
DEM controller that generates optimal DVFS states for the next time interval based
on the computed power and thermal models, current package temperature and core
utilization. An RLS and a Kalman filter (KF) is used to reduce the prediction errors
caused either by model inaccuracies or by sensor noise. The adaptive filters utilizes
the current prediction errors and the power and the thermal models to compute either
filter weights (RLS) or the correction factor to the state variables (KF). In this work,
ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

151:4

V. Hanumaiah et al.

Fig. 2. Structure of the closed-loop controller with its various components.

based on the assumed model (see Section 2), the state variable is the processor package
temperature, which the KF predicts and updates based on temperature measurements.
To predict the optimal voltage/frequency setting in real time, STEAM incorporates
detailed power and thermal models which consider the coupled heat interaction between all cores, as well as temperature-dependent transistor leakage. In order to reduce the noise from the thermal sensors, STEAM incorporates RLS and Kalman filters
to recursively eliminate the noisy sensor inputs. Therefore, STEAM’s prediction model
is self-optimizing over time. STEAM optimizes a generalized form of energy efficiency
– Perf α/Power, where α is a parameter that is used to assign a greater importance
to performance or power in the optimization. In summary, STEAM is a lightweight
closed-loop controller that coordinates the multiple, independent DVFS settings of all
cores on a multicore platform.
STEAM is implemented and evaluated on a Linux-based system running on an
Intel Quad-Core Sandy Bridge architecture based processor. Our experimental results
on maximizing energy efficiency demonstrate that the proposed controller design can
achieve an 18% improvement in energy efficiency when compared with three other
DEM techniques available on Linux, which are commonly referred to as Performance,
Ondemand and Powersave governors. In these set of experiments, STEAM was able to
regulate core temperatures to be within 3◦ C of the specified maximum temperature.
Experiments analyzing STEAM’s ability to track power consumption and temperatures
has shown that STEAM’s prediction accuracy for temperature is within 0.1◦ C mean
error and 2.5◦ C standard deviation error. STEAM tracks power consumption with a
0.04 W mean error and 0.6 W standard deviation error for average workload power
consumption of 30 W. In comparison, the standard deviation of noise for temperature
sensors is 2.1◦ C and 0.7 W for power sensor. Since the prediction error is close to
the sensors noise range, the proposed STEAM controller has demonstrated excellent
tracking ability on a real processor.
Overall, the STEAM controller design described in this article advances DEM for
multicore processors in the following ways:
(1) it achieves better power and temperature prediction, which takes into account the
workload characteristics;
(2) the prediction is very fast as it employs simple yet accurate power and thermal
models;
(3) it estimates the dynamic and leakage power of processor cores using just the total
power and the core temperatures;
(4) it can optimize various objective functions, for example, maximizing energy efficiency, maximizing performance, minimizing the peak temperature, etc.

ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

STEAM: A Smart Temperature and Energy Aware Multicore Controller

151:5

1.3. Related Work

There is an extensive body of literature on DEM [Rao and Vrudhula 2009; Murali et al.
2007; Coskun et al. 2008; Cochran et al. 2011; Chantem et al. 2008; Hanumaiah et al.
2011] for managing power and temperature of processors. In this section, we discuss
the recent work that is most closely related to this article.
The design of DEM controllers for single-core processors is often straight forward and
usually involve using a simple control implementation like PID controllers [Skadron
et al. 2002; Fu et al. 2010]. However, there are no standard techniques available for
multicore DEM implementations.
Several new controller designs have been proposed in the recent years for multicore
DEM [Bartolini et al. 2011; Jung and Pedram 2006; Wang et al. 2009; Zanini et al.
2009]. These controllers can be broadly classified based on their approaches, viz. statistical and control-theoretic. Jung and Pedram [2006] propose a statistical technique
based on partially observable Markov chains to predict an optimal processor frequency
setting. While their approach avoids the need for learning of the processor power or
thermal models, their algorithm requires complex computations, and the number of
DVFS state searches grow exponentially with the number of processor cores. Moreover,
such statistical approaches are not practical for online DEM.
The control-theoretic techniques [Bartolini et al. 2011; Wang et al. 2009; Zanini et al.
2009] for multicore processors usually involve a model-predictive controller (MPC) to
determine optimal control states for one or more control time steps in the future, by
solving a constrained optimization problem. Some of the limitations of these works are:
(i) ignoring workload characteristics; (ii) including power and thermal models that are
either simplistic, leading to suboptimal controller actions or using higher order models
than necessary that increases computational complexity; (iii) neglecting the leakage
power dependence on temperature; (iv) lack of flexibility in handling objectives functions and constraints of various kinds, for example, a highly nonliner objective function
like performance/Watt, where the control variables are present both in the numerator
and the denominator; (v) high-computation complexity of controller action determination, which increases exponentially with number of cores. Bartolini et al. [2011]
addressed the last limitation by developing a distributed system, where each core or
group of cores determine the control action for their own group, with minimal communication with other cores. However, their policy is mainly a heuristic and does not
guarantee either the optimality, or the bounds on the optimality of the controller action.
By exploiting the workload characteristics, Isci et al. [2006] were able to improve
energy efficiency by applying simple heuristics to select an optimal frequency setting
based on the degree of memory-boundedness of workloads. Similarly, Dhiman and
Rosing [2007] proposed an online learning model to take into account the IPC and
cache misses for determining an optimal frequency setting. While both techniques
consider workload characteristics, the simple heuristics can be significantly improved
with detailed power and temperature models. To do so, Cochran et al. [2011] also took
advantage of the DVFS controls available in most modern processors to optimize for
performance and energy efficiency by computing an energy model offline, which is used
to compute the necessary DVFS states during the online phase. However, since there is
no feedback mechanism and no accounting of process temperatures and leakage power,
it is possible that the system can quickly deviate from its optimality.
To account for cache power consumption, Liang et al. [2010] propose a method to
dynamically select the P-state for energy conservation based on cache miss rates. The
model does not include thermal or leakage parameters, and the approach is not flexible
to support multicore processors and user space policies. Since they compute the model
periodically, the approach can cause high system overhead. In another work, Snowdon

ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

151:6

V. Hanumaiah et al.

Fig. 3. Hardware functional unit classification for a CMP.

et al. [2009] proposed an approach that uses a pre-defined model and determines the
parameters from the statistics available from the operating system. Similar to the
previous work, the model only accounts for the basic characteristic of the processor (no
leakage power). Also, the work is targeted mainly for single-core processors with no
feedback mechanism.
In contrast of existing work on DEM controllers, STEAM is a lightweight, practical
online controller that achieves optimal energy efficiency over a diverse range of applications. It incorporates a feedback control mechanism that corrects for inaccuracies in
the models and sensor measurements. It includes a set of algorithms for constrained
optimization of multidimensional objective functions. STEAM can accurately predict
an optimal DVFS setting that results in a maximum performance-per-watt energy efficiency, without the need for any a priori knowledge of the system. Finally, its high
efficiency makes it amenable to be incorporated directly in the kernel of an operating
system.
2. MODEL IDENTIFICATION METHODOLOGY
2.1. Identification of Power Models

The power consumption in a standard chip multiprocessor (CMP) depends mainly on
two things: (i) the clock frequency of the processor, and (ii) the subsets of the functional
units that are accessed and the rate of their access, which is a characteristic of the
program code. Figure 3 classifies the functional units in a CMP. Some of them are part
of cores, and the remaining are in peripheral units, which are collectively termed as
“uncore”.
As seen in Figure 3, there can be many functional units in a processor, with different
functional units consuming different amounts of power per access. Hence, it is necessary to monitor the access rates of these functional units. Many modern processors
provide an option of monitoring such events called performance counters. A single performance counter can be programmed to capture any one of a large set of hardware
events. However, due to the limitations on the number of available performance counters, only a few of the many possible hardware events can be monitored simultaneously.
Therefore, it is important to identify the most prominent sources of power consumption
based on the knowledge of processor architecture, such that these sources can account
for most of the processor power consumption.
Table I lists a set of factors that were selected for the experiments on an Intel Sandy
Bridge processor, and the corresponding notations used in this article. The choice
of these factors is based on the knowledge of the architecture, as well as, extensive
ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

STEAM: A Smart Temperature and Energy Aware Multicore Controller

151:7

Table I. Hardware Events Used in Modeling Processor
Power Consumption
Factor
P-state
T-state
Temperature
Instructions retired
Loads and stores
L2 accesses
Last-level cache access
Memory controller access

Granularly
chip
core
core
core
core
core
chip
chip

Notation
ρ
τ
T
μ
l , s
2
3
m

experimental exploration of the system. Our approach to identifying these factors was
based on the classification of the functional units shown in Figure 3.
In our design of experiments for obtaining power models, all factors are required to
be controllable independently. In Table I, except for the P-state and the T-state (will be
explained later in Section 2.1.2), which are equivalent to DVFS and DFS mechanisms,
respectively, most events are not directly controllable, but can be indirectly controlled
by modifying a program code to selectively stress one particular functional unit, while
limiting access to all other units. Since temperature contributes to leakage power,
which is significant in submicron designs, it is also monitored in our experiments and
is listed in the table.
In the following sections, we will derive empirical relationships of several important
factors with respect to total power consumption, starting with analyzing heterogeneity
among cores.
2.1.1. Analyzing Variation in Power Consumption of Cores. There can be a significant degree
of variation in power consumption among the cores. This is especially prominent in
heterogeneous multicores like ARM’s big.LITTLE architecture [ARM 2012]. Even for
a homogeneous multicore processor, there can be variation in core power consumption
due to process variations. In order to analyze this variation, we designed an experiment,
where a known application was executed on one core at a time, leading to n trials for
an n-core processor, and measured the resulting power consumption. Analysis of the
variation in the power consumption of the n trials is required to determine the ratios
of power consumption of the various cores.
The specific experiment on the quad core SandyBridge processor involved running
a single application on a core, and measuring the power consumption for each combination of P-states (14 values) and T-states (8 values). The average power consumption
at each P-state and T-state over 100 time instants was computed. The deviation of the
112 power consumption values from the mean for each core are shown as a scatter plot
in Figure 4. The average deviation was less than 0.05 W. Due to the low variation in
power consumption between cores, we just use one core to characterize the core power
consumption, instead of repeating all experiments for each core.
Proposed methodology can be extended for heterogeneous processors by grouping
the similar cores and repeating the experiments for every group to derive its power
characteristics.
Next, we analyze the relationship of various factors listed in Table I with respect
to the total power consumption. Let P be the measured total power consumption of
the processor. The notation for each of the factors are listed in Table I. The subscript
c denotes that the variable is involved with core c. Let Pρ |τ,μ,T ,l ,s ,2 ,3 ,m denote the
power consumption of P with respect to P-state ρ while keeping other parameters
ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

151:8

V. Hanumaiah et al.

Fig. 4. Scatter plot of deviation of power consumption of cores from the mean power consumption of all cores
for the Intel Sandy Bridge quad core processor.

τ, μ, T , l , s , 2 , 3 , m constant. We first start with analyzing the effect of P-states
and T-states on the total power consumption.
2.1.2. Effect of P-States and T-States on Power Consumption. The power consumption of a
processor is typically modeled as the sum of dynamic and leakage power:
max
P = pdyn
f v 2 + plkg (v, T ),

(1)

max
where pdyn
is the maximum possible dynamic power consumption, and f and v are
the frequency and the voltage of the processor, respectively. The leakage power, denoted by plkg , is a function of the operational voltage and the die temperature. Note
that the clock frequency and the voltage in (1) are not independent. In general, the
maximum frequency is constrained by the supply voltage and the temperature. This is
because temperature decreases the carrier mobility degradation leading to increased
delay [Liao et al. 2005], which can be compensated by increasing the voltage to meet
the frequency specifications. This complex dependency is difficult to model as well as
to use. Consequently, in practice, for a given clock frequency, the range of operational
voltages, which satisfies the circuit timing constraints are determined empirically for
worst-case corners. As a result, most manufacturers only allow the control of predetermined “voltage-frequency” pairs under which the timing constraints are satisfied.
These pairs are called P-states.
On the other hand, for a given voltage, a processor’s clock can be throttled to a desired
level to reduce power consumption. This is usually achieved by either inserting halt
instructions or by clock modulation. On some platforms, this is implemented using
clock modulation, and the resulting states are called T-states. Unlike a P-state, a Tstate can be changed on a per-core basis. An example of allowed P-states and T-states
on the Intel Sandy Bridge processor is given in Table II.
P-states are applied globally to all cores and T-states can be controlled locally per
core (as in Intel Sandy Bridge). Another popular industry practice is to have per-core
P-state control (as in Qualcomm Krait). Comparing both these implementations, percore DVFS provides more energy savings over global DVFS, while global DVFS claims
to have a simpler design and lesser overhead of having just one voltage island.
From (1), we expect that a P-state to have a cubic effect on dynamic power consumption. This is true only when both the frequency and the voltage of a processor are
changed independently. However, P-states being voltage-frequency pairs, when the frequency changes by a constant amount, the voltage changes nonlinearly (see Table II).
Therefore, the power consumption varies quadratically with changes in P-states. This

ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

STEAM: A Smart Temperature and Energy Aware Multicore Controller

151:9

Table II. P-State and T-State Specifications for the Intel Sandy Bridge
Processor Used in this Article
P-state
0
1
2
3
4
5
6
7
8
9
10
11
12
13

Voltage (V)
1.000732
0.975708
0.955688
0.930664
0.910645
0.910645
0.885620
0.865601
0.840576
0.820557
0.795532
0.775513
0.775513
0.775513

Fmax (GHz)
2.1
2
1.9
1.8
1.7
1.6
1.5
1.4
1.3
1.2
1.1
1
0.9
0.8

T-state
0
1
2
3
4
5
6
7

% throttling
00.0%
12.5%
25.0%
37.5%
50.0%
62.5%
75.0%
87.5%

Fig. 5. Quadratic effect of P-states on processor power consumption for the Intel Sandy Bridge quad core
processor.

is shown in Figure 5, which is a plot of the power consumption vs. P-states for the Intel
Sandy Bridge processor.
Equation (2) captures the relation between the power consumption of a core and its
P-states using a second-degree polynomial.
Pρ |τ,μ,T ,l ,s ,2 ,3 ,m =

n 
2


xρ,i,c ρci ,

(2)

c=0 i=0

where ρc is the P-state of core c, and n is the total number of cores. Note that the
association between the maximum frequency Fmax and voltage is a function, while
the converse is not. Consequently, we use Fmax to identify a P-state. The coefficients
xρ,2,c , xρ,1,c , and xρ,0,c are processor specific, which needs to be determined through
experiments.
Although T-states behave similar to DFS (dynamic frequency scaling), T-states do
not linearly affect the power consumption like in DFS. This can be seen in Figure 6,
the processor’s power consumption is plotted versus the T-states (measured as the
% of throttling), we see that under a low throttling state, the relation between the
processor power consumption and T-states is linear, but it changes to superlinear at
higher throttling settings.
ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

151:10

V. Hanumaiah et al.

Fig. 6. Quadratic effect of T-states on processor on power consumption for the Intel Sandy Bridge quad core
processor.

Similar to Eq. (2), Eq. (3) expresses the total power consumption of a core with respect
to T-states as a quadratic relation.
Pτ |ρ,μ,T ,l ,s ,2 ,3 ,m =

n 
2


xτ,i,c τci ,

(3)

c=0 i=0

where τc is the T-state of core c, and xτ,2,c , xτ,1,c , and xτ,0,c are coefficients to be estimated. The combined effect of P-states and T-states can be determined by finding the coefficients of (2), that is, [xρ,2,c , xρ,1,c , xρ,0,c ] for the various T-states, and
observing the relation between [xρ,2,c , xρ,1,c , xρ,0,c ] and the T-states. The experiment
to determine the combined effects can also be conducted by observing how T-state
coefficients [xτ,2,c , xτ,1,c , xτ,0,c ] vary by varying the P-states. From our experiments,
[xρ,2,c , xρ,1,c , xρ,0,c ] vary quadratically with T-states. Overall, the combined effect of
P-states and T-states can be represented as follows:
Pτ,ρ |μ,T ,l ,s ,2 ,3 ,m =

n 
2 
2


xρ,τ,i, j,c ρci τcj .

(4)

c=0 i=0 j=0

This equation contains all possible combinations of P-state and T-state exponent terms
from (2) and (3). Note that in general, xρ,τ,c,i, j = xρ,c,i xτ,c, j .
From now on, we only show the partial relationship of a factor with the total power
consumption, and in the end of this section, we present a unified power model which includes all the factors. In general, combining the relationships of two factors follows the
product rule shown in (4), that is, multiplying the terms belonging in a factor relationship with the total power with the other factors. In order to avoid this representation
from getting more unwieldy as more and more factors are added, we introduce a simpler representation of each factor’s relationship with the total power consumption. This
simpler representation mentions only the factors considered, and not the rest of the
factors that are held constant, for example, we will represent Pτ |ρ,μ,T ,l ,s ,2 ,3 ,m as
just Pτ .
2.1.3. Effect of IPC on Processor Power. The power consumption of a task increases with
its instructions committed per cycle or IPC. In general, the processor power increases
linearly with the IPC, but at a fairly slow rate (see Figure 7). It is true that the IPC can
be further refined by distinguishing the different types of instructions (e.g., integer or
floating point). However, this did not substantially improve the accuracy of the power
prediction on our platform. Based on the results from our experiments on stressing
ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

STEAM: A Smart Temperature and Energy Aware Multicore Controller

151:11

Fig. 7. Linear effect of increasing IPC over power consumption for the Intel Sandy Bridge quad core processor.

execution units, it is reasonable to assume that in a realistic instruction mixture scenario higher IPC leads to higher power consumption. And this can be assumed valid
for processors with different ISA or different pipeline design, for example, RISC or
in-order pipeline processors. The relationship between the IPC of a task running on a
core and the total power is given by
Pμ =

n 
1


xμ,i,c μi .

(5)

c=0 i=0

2.1.4. Analysis of Memory Power. Memory power is the most difficult to analyze among
all factors. The primary reason is that it is hard to control various memory related
factors such as cache accesses/misses and accesses to DRAM from a program. The
second reason is that the static power consumption of the processor (including the
leakage power consumption of the memory hierarchy) is much higher than the dynamic
power consumption of the memory hierarchy, for example, in Figure 8, we see that
the static power consumption of the processor at 0 memory access is 26 W, while
the dynamic power consumption contributed by the memory hierarchy with a program
working set size of 30 MB is just 3 W. Thus, cache and memory controller (MC) accesses
do not produce a significant change in the total power consumption. We identified four
factors that are related to memory access and have an impact on the total power
consumption. These are (loads + stores)/cycle, L2 accesses/cycle, L3 accesses/cycle, and
MC accesses/cycle. The values for these four factors are obtained by combining various
performance counters associated with memory access.
The power consumption associated with the memory hierarchy is sum of power consumption of various components in the memory hierarchy, for example, L1 instruction/
data caches, L2, L3 and memory controller (MC) for DRAM access. Figure 8 shows the
dependency of total power consumption as a function of access rates of various components in the memory hierarchy for different working set sizes of programs. Loads
and stores account for the accesses to the L1 data and instruction caches. Although
loads and stores account for most of the memory accesses, they alone do not account for
total memory power consumption. In the figure, we see that total power consumption
is a direct result of loads and stores for working set sizes up to 1MB. Beyond that,
although the loads and stores remain constant and reduce after 1MB working set size,
the memory power consumption continues to increase, but slowly. The memory power
consumption that cannot be related to loads and stores can only be explained by access
rate of other components in the memory hierarchy. As the working set size is increased,
ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

151:12

V. Hanumaiah et al.

Fig. 8. Effect of working set size of programs on memory related factors for the Intel Sandy Bridge quad
core processor.

lower level caches and the MC get accessed and their access adds to the total memory
power consumption, as can be seen from the figure.
In order to analyze the relationship of each of the factors shown in Figure 8 to the
total power consumption, we constructed a set of microbenchmarks which selectively
stress a factor, while isolating all other factors. For example, to understand the effect
of working set size on memory power consumption, we wrote a microbenchmark which
consisted of random memory access instructions, whose address space was limited for
a certain working set size. In such an experiment, all other factors are kept constant,
for example, temperature is not allowed to increase and no change either in P-state
or T-state. Based on the experiments, we have modeled the power consumption as a
linear function of each of these individual factors. The relation between this memory
access factors to the total power consumption is formulated as
Pl ,s ,2 ,3 ,m =

n 
1




xls ,i,c (l + s )i + x2 ,i,c i2 + x3 ,i,c i3 + xm,i,c im .

(6)

c=0 i=0

ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

STEAM: A Smart Temperature and Energy Aware Multicore Controller

151:13

Fig. 9. Linear effect of increasing core temperatures over power consumption for the Intel Sandy Bridge
quad core processor.

2.1.5. Effect of Temperature on Leakage Power. Temperature plays a significant factor in
the leakage power consumption. The relationship between the temperature and the
leakage power has been well studied in the literature, and is usually approximated
by an exponential relationship [Liao et al. 2005]. However, the ratio of the leakage
power to the dynamic power is low. As a result, this exponential relationship will not
be observed when measuring only the total processor power, which is a combination of
both the leakage power and the dynamic power. In fact, we observed an approximate
linear relationship of total power with the mean core temperatures as can be seen in
Figure 9. The relationship between a core temperature and the total power is given by

PT =

n 
1


xT ,i,c T i .

(7)

c=0 i=0

The combined equation that relates all the factors with the total power consumption
is given by
P = Pρ ∗ Pτ ∗ Pμ ∗ PT ∗ Pls ,2 ,3 ,m .

(8)

The coefficients for the above equation are determined using the linear least-square
(LLS) method [Whittle and Sargent 1983] as all the terms have linear coefficients. The
LLS equation is given by
Ax = P,

(9)

where P is a vector form of P for various time instants, x is the vector of coefficients to be
determined, where all xs in (9) are arranged serially in a predetermined order, and A is
j
a matrix that contains the elements ρci τc μkc Tcl (l , s , 2 , 3 , m)n whose arrangement
matches the arrangement of xs. The rows of matrix A correspond to the time instants
where the measurements of the factor terms are taken. In the current set of experiment
on the Sandy Bridge platform, the number of runs were 10,000. The columns of A
correspond to the coefficients of the terms to be estimated. This was 81. Hence, A is a
10000 × 81 matrix and x is a 18 × 1 vector.
2.2. Identification of Thermal Models

2.2.1. Compact Thermal Models. Among the existing models for characterizing processor temperature, compact thermal models [Huang et al. 2006] are the simplest, yet
provide sufficiently accurate models of power-temperature relationship. The compact
thermal models use the electrothermal analogy relating heat generation, spreading and
storing concepts to current sources, resistors and capacitors, respectively, as shown in
ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

151:14

V. Hanumaiah et al.

Fig. 10. Compact thermal model used in this work.

Figure 10. The thermal model used in this work is a simplified version of HotSpot compact thermal model [Huang et al. 2006]. The simplifications are (i) each core has only
one thermal node, as a processor usually has at most one power or thermal sensor per
core; (ii) the capacitance associated with the die thermal nodes are ignored, as the die
thermal constants are usually a few milliseconds and are not noticeable when sensors
have very low sampling rate (≈1 sample per 100 ms); (iii) the package is lumped into
a single thermal node as sensor measurements for the package temperature are not
available.
Every core c has an input power source Pc . Each core is connected to the package
through a vertical resistance and to every adjacent core through a horizontal/lateral
resistance as shown in Figure 10. These set of resistances form the symmetric resistance matrix R, where Ri j denotes the resistance connected between core i and core
j. The package is lumped into a single node connected to the ambient with resistance
Rp and capacitance C p, which are in parallel. T p and Tc are the temperatures of the
package and of core c, respectively.
From the figure, the power-temperature relationship is given by
n


Pc (ρc , τc , Tc , t)
dT p(t)
1
c=1
=−
T p(t) +
,
dt
RpC p
Cp
T(t) = T p(t) + RP(ρ, τ , T, t).

(10)
(11)

The notations ρc and τc represent the P-state and the T-state of core c, respectively.
The notation n denotes the total number of cores. The power consumption per core Pc
is derived from the model equations described in Section 2.1 and is given by (8). Note
that the vectors and the matrices are denoted in bold, for example, P denotes a vector
of Pc of all cores.
Since the power and the temperature measurements are obtained in discrete time
steps, these equations are discretized and the corresponding equations are given here.
T p(k + 1) = aT p(k) + b

n


Pc (ρc , τc , Tc , k),

(12)

c=1

T(k) = T p(k) + RP(ρ, τ , T, k),
t
Rp C p

(13)

t
.
Cp

and b 
The notation t denotes the length of the discrete time
where a  1 −
step and k refers to time kt.
ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

STEAM: A Smart Temperature and Energy Aware Multicore Controller

151:15

2.2.2. Model Identification. The model identification process consists of parameter identification for the thermal model, viz. a, b and R of (12) and (13). For the model identification, a set of benchmarks are chosen and allocated randomly to various cores. Then
the cores operate with varying core speeds for every few milliseconds, and are continued for several minutes. During this time, power and temperature measurements are
noted. These measurements are used to identify the thermal model parameters.
A single equation for temperature T can be obtained by substituting T p(k) = T(k) −
RP(k) from (13) into (12). The resultant equation is

T(k + 1) = aT(k) + RP(k + 1) + b

n


Pc (k) − aRP(k).

(14)

c=1

This equation has an autoregressive (AR) term T, and also a moving average (MA)
noise component e from the previous time interval. Hence, the necessary model that
describes this equation is autoregressive moving average with exogenous input (ARMAX) model [Box et al. 1994]. P is the exogenous input in the previous equation.
Several approaches are available for identifying parameters of ARMA models. We use
the iterative linear least square (ILLS) method to identify the previous ARMAX model.
The details of the ILLS method is provided in Appendix A.
3. ROBUST MULTICORE DEM CONTROLLER
3.1. Description of the Optimization Process

The role of a DEM optimizer is to determine the appropriate P-states and T-states
to achieve a stated objective using the developed power and thermal models, while
ensuring that all the specified constraints are satisfied. The advantage of the proposed
closed-loop controller design (see Figure 2) is that the operation of the DEM optimizer
is not tied with the design of the feedback loop. Thus, the DEM optimizer is not bound
to any specific type of objective or constraint functions. Also, there are no restrictions
on the design of the DEM controller based on the objective type (linear/nonlinear) or
the complexity of the solution required to achieve the objective. The only input that
the DEM controller needs is the updated weights from the RLS filter that corrects the
prediction error of the power consumption, and the updated package temperature from
the Kalman filter, which corrects the prediction errors in temperature. The working
of the RLS and the Kalman filters will be described in the following sections. The
following example illustrates the operation of the DEM controller with the objective of
maximizing overall processor energy efficiency.
c=n

(wc ρc (k)τc (k))α

max

ρ(k),τ (k)

c=0
c=n


,

(15)

Pc (ρc , τc , k)

c=0

such that

T(k + 1) = T p(k + 1) + RP(k),
T(k + 1) ≤ Tmax ,
P = Pρ ∗ Pτ ∗ Pμ ∗ PT ∗ Pl ,s ,2 ,3 ,m ,
ρmin ≤ ρ(k) ≤ ρmax ,
τmin ≤ τ (k) ≤ τmax .

(16)
(17)
(18)
(19)
(20)

The objective in (15) is to maximize the energy efficiency. The notation wc denotes the
weighting factor that is specified at design time. It provides a controllable means of
ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

151:16

V. Hanumaiah et al.

defining the importance of a specific task running on a core (this is important in heterogeneous core design). Additionally, α represents another design parameter which
determines the importance of performance in relation to power consumption. Performance of a core is given by the product of a P-state and a T-state of that core. The
computation of core temperatures for next time interval is given by (16). Note that, in
(16), to compute the temperature in the next time interval, the power consumption of
the current time interval is used, as it is not possible to estimate the future performance counter values. The constraint on the maximum temperature is given by (17).
Equation (18) is same as (8). The range of allowable P-states and T-states are specified
in (19) and (20), respectively.
This formulation can be shown to be a quasiconvex (monotonic) optimization problem
with respect to P-states and T-states. Interested readers can find the proof of quasiconvexity for a similar formulation in Hanumaiah and Vrudhula [2012a]. Hence, the
solution to be this formulation can be obtained using any of the standard convex optimizers. However, the convex optimization approach is not practical as its run time does
not scale well with the increase in the number of cores.1 Moreover, any implementation in the kernel space of an operating system has to be extremely simple, consuming
less than 1% of the kernel resources. For this reason, we propose a two-stage, per-core
binary search of near-optimal P-states and T-states. In the first stage, each core selects
the best P-state assuming previous configuration of P-states and T-states of all other
cores. With the P-states for all the cores set, in the second stage, each core selects the
best T-state for its own core. The experimental results for maximizing energy-efficiency
using the optimization framework are presented in Section 4.3.
3.2. Error Correction Mechanisms

The model parameters derived Section 2 are valid only for the measurements that were
used in the derivation. Moreover, the models proposed in Section 2 are not exact, but a
simplification of more accurate models. As such, the predictions can be inaccurate and
this requires a feedback of the past measurements to correct the future predictions.
Also note that the measurements are almost always corrupted by noise. Hence, there
is a need for error correction mechanisms to minimize the power and temperature
prediction errors.
3.2.1. Recursive Least Squares (RLS) Filter. Recursive least squares (RLS) is used in this
work to minimize the prediction errors in power consumption. RLS is an adaptive
filter which recursively determines the filter coefficients that is used to minimize a
weighted linear least squares cost function of the input signals. The advantage of RLS
is extremely fast convergence of filter coefficients, and also there is no need to invert
matrices, thus less computational overhead. Filter weights w are updated according to
the RLS algorithm as shown here, which minimize power prediction error e p.

e p(k) = Pm(k) − PTp (k)w(k − 1),
g(k) =

C p(k − 1)P p(k)
,
λ + PTp (k)c p(k − 1)P p(k)

C p(k) = λ−1 C p(k − 1)[1 − g(k)PTp (k)],
w(k) = w(k − 1) + e p(k)g(k).

(21)
(22)
(23)
(24)

1 The general convex optimization problem belongs to the class of NP-Hard problems [Nocedal and Wright
2006].

ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

STEAM: A Smart Temperature and Energy Aware Multicore Controller

151:17

Fig. 11. Pictorial description of working of Kalman state estimator.

The vector P p(k) is the vector of predicted power from current time interval k till the
past time interval k − q for a qth order RLS filter. The measured power is represented
by Pm and λ is the constant forgetting factor with 0 < λ ≤ 1. A smaller λ allows the
estimator to forget the history sooner. The covariance matrix is represented by C p. The
initial values of weights and the power prediction values are set to zero. This RLS filter
algorithm is invoked during every control period.
3.2.2. Kalman State Estimator. Unlike in the prediction of power consumption, which
has no state, the temperature prediction is state dependent. In the case of our thermal
models, the state is the package temperature. Hence, we use a Kalman filter [Maybeck
1979] for error correction rather than RLS. A Kalman filter is one of the well-known
methods used for prediction of linear systems corrupted by noise. A Kalman filter
minimizes the prediction errors caused by model inaccuracies and updates the state of
the system such that the accuracy of future predictions can be improved. Due to the
cyclical relationship between the core power consumption and the core temperature
(refer to (16) and (8)), which makes the system nonlinear, we need to use an Extended
Kalman filter [Maybeck 1979], instead of a Kalman filter.
Figure 11 intuitively describes the working of a Kalman filter (KF) in producing
better estimates of state variables using measurements. At time k−1, KF uses the value
of the current state which, in our case, is the package temperature T p(k − 1), to predict
the value of T p at time k, T p(k|k − 1) and also the outputs, that is, temperature of all
cores T(k). The notation k|k−1 denotes prediction at time k based on the measurements
made at time k − 1. In the next time step k, KF obtains the measurement of T(k) and
uses the prediction error in the core temperatures T(k) to estimate better T p at time
k, that is, T p(k|k). Note that T p unlike core temperatures T is never measured, it is
an estimated value. Since package temperature affects output core temperatures, it
is necessary to correct T p(k) estimate to minimize the prediction errors of T(k + 1).
Like RLS, KF is run at every control step. The notation Ce in the figure is the error
covariance of the estimate, which intuitively “remembers” the weighted sum of the
error between the predicted output and the measured output, and is used to improve
the estimate of the package temperature. The details of the Extended Kalman Filter
as applied in this article is available in Appendix B.
A summary of the procedure to maximize energy efficiency using the proposed closedloop controller is described in Algorithm 1.

ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

151:18

V. Hanumaiah et al.

ALGORITHM 1: Overall procedure of closed-loop control for multicore DEM
Input: Number of cores n;
Sensor measurements: processor power Pm, core temperatures Tm, and performance
counters values ν;
Objective to maximize;
Output: P-states ρ and T-states τ
Build power and thermal models by executing benchmarks and measuring Pm, Tm and ν
(Section 2);
Let package temperature T p(0) = Tambient ;
for every kts , k ∈ K, ts ∈ [50 ms, 100 ms] do
1. Using models, compute (ρ(k), τ (k)) that maximizes the objective while ensuring predicted
T(k) ≤ Tmax (Section 3);
2. Predict future T p(k + 1) using current time T p(k) and P(k)
(12);
3. Compute prediction errors eT (k) = T(k) − Tm(k), e P (k) = c Pc (k) − Pm(k);
4. Adjust T p(k + 1) based on e(k) (Section 3.2.1) and compute RLS weights based on e P (k)
(Section 3.2.2) to minimize future prediction errors;
end

4. EVALUATION AND RESULT ANALYSIS
4.1. Experimental Setup

4.1.1. Implementation Environment. The proposed STEAM controller was implemented
on an experimental quad-core Intel Sandy Bridge processor [Rotem et al. 2012]. The
processor has the capability of measuring per-core temperatures and the total energy
consumption through reading model-specific registers (MSR) [Intel Corporation 2012].
The temperature measurements are based on the digital temperature sensor (DTS)
present in each core, while the energy values are the estimates computed based on
the instructions executed. STEAM is written in C++ and deployed on Ubuntu Linux.
The APIs of the Advanced Configuration and Power Interference (ACPI) are used to
adjust processor core speeds. The Sandy Bridge processor on our experimental board
supports 14 different P-states from 0.8 GHz to 2.1 GHz in steps of 100 MHz. It requires
all cores to operate at the same voltage/frequency setting. Though the temperature and
the power measurements could be obtained at time intervals as small as 1 ms, due to
the P-state control latency, our scheduling interval for DVFS optimization was set to
100 ms. The overhead of our STEAM controller is less than 0.25% of total processor
usage. This demonstrates that STEAM is computationally very efficient to be included
in the operating system scheduler.
4.1.2. Workload Construction. The workload for the experimental validation of STEAM
and the comparison with existing governors consisted of SPEC CPU2006 [SPEC], PARSEC [Bienia et al. 2008], and MiBench [Guthaus et al. 2001] benchmarks. Since SPEC
and MiBench are single-threaded benchmark suites, we ran identical copies of each application from those benchmarks on all four cores to fully utilize the processor. However,
as PARSEC applications are multithreaded, they were executed as a single instance.
We ran all applications with the reference input data set to completion and report the
execution time to represent performance.
4.1.3. Other Existing Policies. We compare STEAM with three other DEM policies on
Linux, that are referred to as Performance, Powersave, and Ondemand [Pallipadi and
Starikovskiy 2006]. As the name implies, the Performance policy attempts to achieve
the maximum performance by always running at the maximum P-state. The Powersave
policy aims to minimize the overall energy consumption by running the processor at the
ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

STEAM: A Smart Temperature and Energy Aware Multicore Controller

151:19

Fig. 12. Mean and standard deviation plots of the proposed profiler prediction error for SPEC and PARSEC
benchmarks.

minimum P-state. The Ondemand policy reacts to the workload activity and attempt
to maximize performance, while minimizing energy consumption.
4.2. Model Validation

To validate the proposed power models, we ran both SPEC CPU2006 and PARSEC 2.0
benchmarks and varied the execution rates of the benchmarks by varying P-states and
T-states randomly. The power consumption of each core was computed using the models
derived in Section 2.1, and the RLS method. However, since we can only measure the
total processor power, the per-core power is summed at each time and compared with
the measure value of the total power. Figure 12 plots the mean and the standard
deviation of the prediction error of our proposed power models when compared with
measured total power consumption.
The mean error refers to the average prediction error on the entire run of a benchmark, which in an ideal scenario should be zero, while the standard deviation error
refers to the average deviation of a prediction from the actual measurement for every
sample, which cannot be smaller than the measurement noise of the sensor. The mean
error for the SPEC and PARSEC benchmarks was ≤0.04 W. The standard deviation of
measured noise for power estimator on our platform is 0.7 W. The standard deviation
of power prediction error is generally close to the standard deviation of noise, except
for 429.mcf, which is a memory intensive benchmark. This clearly demonstrates the
high accuracy of the proposed power models. The average power consumption values
of SPEC and PARSEC benchmarks varied between 10 W and 40 W. The temperature
ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

151:20

V. Hanumaiah et al.
Table III. Comparison of Various ACPI Policies with the STEAM Policy
Policies
Performance
Ondemand
Powersave

Improvement of STEAM in MIPS/Watt (%)
SPEC PARSEC
MiBench
18.2
8.1
26.8
19.1
10.2
29.3
31.3
45.2
56.7

SPEC
1.03
0.93
1.67

STEAM speedup
PARSEC
MiBench
0.96
1.08
1.14
1.12
2.33
2.27

All values are normalized with respect to STEAM.

prediction errors had a mean error of 0.1◦ C and 2.4◦ C standard deviation error, which
is close to the thermal sensors’ noise of 2.1◦ C.
4.3. Energy-Efficient Processor Operation using the STEAM Controller

In an energy-constrained environment, users can manually adjust the voltage/
frequency setting using the Powersave policy to prolong the battery life. We argue
that while the Powersave policy indeed minimizes the energy consumption by running
at the lowest P-state setting, it unnecessarily sacrifices application performance. In
contrast, STEAM, which operates at the voltage/frequency setting for optimal energy
efficiency, not only achieves better energy efficiency (performance-per-watt) compared
to Powersave but also improves application execution time. Figure 13 provides a comparison of STEAM controller execution for the different benchmarks. The results of all
policies under study are normalized to the STEAM policy. The topmost figure shows
the comparison of energy efficiency or performance-per-watt expressed as MIPS/Watt.
Our experimental results agree with the conventional belief that running at low
voltage/frequency settings results in suboptimal performance-per-watt. As the figure
shows, Powersave has much worse energy efficiency compared to other policies. Table III
summarizes the results of comparison of MIPS/Watt among different policies and also
the speedup compared with the STEAM governor. The observations are summarized
here.
(1) STEAM does better than Powersave both in terms of MIPS/Watt and Speedup for
all benchmarks.
(2) STEAM achieves a substantially higher MIPS/Watt than Performance and Ondemand for the SPEC and MiBench applications.
(3) The improvement in MIPS/Watt of STEAM over the Performance governor for the
PARSPEC is not as high (8.1%). This is due the lack of per-core P-state support
in our experimental platform. Unlike SPEC and MiBench workloads, where we
executed the identical copies of the workloads on all four cores, just one instance
of a PARSEC application is executed and it spans multiple threads. Ideally, each
thread of PARSEC executing on a core requires an independent P-state control, as
the threads have very different execution rates. We confirmed this by simulating the
PARSEC benchmarks using the models derived from model identification, with percore P-state and without per-core P-state. We found using per-core P-state provides
an additional improvement in the MIPS/Watt by as much as 11%. Hence, we believe
that STEAM is ideally suited for platforms which support per-core P-state control.
(4) STEAM also does better than Performance and Ondemand in terms of execution
speedup. The reason that Performance or Ondemand governors does worse than
STEAM in terms of latency is because they tend to quickly increase the processor
temperature to very high levels, which leads to hardware throttling of processor
frequencies. Hardware throttling is also one of the reasons why the Performance
and Ondemand governors tend to do worse in terms of MIPS/Watt.
ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

STEAM: A Smart Temperature and Energy Aware Multicore Controller

151:21

Fig. 13. Comparison of MIPS/Watt of STEAM controller execution with Performance, Ondemand, and
Powersave for SPEC, PARSEC, and MiBench benchmarks.

In these experiments, we measured the margin of temperature violations and noted
that the maximum thermal violation by the proposed STEAM controller is less than
3◦ C. The thermal violation are primarily due to (i) sensor noise, which can lead to wrong
temperature measurements and also hamper modeling accuracy; (ii) limitations on the
part of the proposed models to account for transients whose time constants are shorter
than the package thermal time constants. Even with these limitations, the maximum
observed thermal violation was at 3◦ C, which is 1.4 times the standard deviation of
thermal sensor noise (2.1◦ C).
The results discussed here are based on the experiments conducted on an Intel Sandy
Bridge architecture processor, which has a global DVFS control. Unfortunately, due to
lack of availability of processors that support per-core DVFS, we were not able to explore
the full potential of our proposed energy optimal DVFS policy for processors that support per-core DVFS. In spite of this constraint, we have seen significant improvement
ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

151:22

V. Hanumaiah et al.

in energy efficiency in the above results from using global DVFS alone. The proposed
policy would result in additional and significant improvement in energy efficiency if
it were implemented on a processor that supports per-core DVFS. Based on simulations using models constructed from the Sandy Bridge processor data, we observed
a 11% improvement in the energy efficiency using per-core DVFS over global DVFS.
This improvement was demonstrated (in simulation) only by per-core T-state control.
The per-core DVFS implementation is not much different than the implementation of
per-core T-state control.
5. CONCLUSION

Thermal management has become indispensable with the advent of multicore processors. The design of a closed-loop controller is necessary in order for a practical
implementation of any thermal management solution. Towards this, we propose and
implement a robust closed-loop DEM controller STEAM and also propose a methodology to model the power and thermal characteristics of a processor. The STEAM is not
restrictive to any class of objective functions. STEAM achieved an average improvement of 18% over SPEC, PARSEC and MiBench benchmarks compared with existing
ACPI governors Performance, Ondemand, and Powersave, giving optimal tradeoff between energy savings and performance. STEAM also provided excellent prediction of
power consumption with a standard deviation of error less than 1.6 W. STEAM was
also able to ensure the maximum temperature constraint is satisfied within 3◦ C of the
specified limit.
APPENDIEXES
A. ITERATIVE LINEAR LEAST SQUARE METHOD FOR IDENTIFYING THERMAL ARMAX
MODEL

Since the value of the noise component is not known a priori, we first have to determine
approximate values of a, b and R by solving the following LLS problem (obtained from
reorganizing (14)), which minimizes the noise error e(k) + qe(k − 1) (current noise +
some combination of the noise from the previous time interval).
⎡

⎤
a
⎢ R11 ⎥
⎢
⎥
⎢
⎥
⎤
⎡
⎤ ⎢ R12 ⎥ ⎡
⎢
⎥
T(2)
T(1) P D(2) P D(1)
..
⎢
⎥
.
⎥ ⎢ T(3) ⎥
⎢ T(2) P D(3) P D(2) ⎥ ⎢
⎥
⎥ ⎢
⎢
⎥⎢
R
⎢
⎥ = ⎢ . ⎥.
⎢
⎥
nn
..
..
..
.
⎢
⎥
⎣
⎣
⎦
. ⎦
.
.
.
⎢b − aR11 ⎥
⎢
⎥
T(K)
T(K − 1) P D(K) P D(K − 1) ⎢b − aR ⎥
12 ⎥
⎢
⎢
⎥
..
⎣
⎦
.
b − aRnn

(25)

P D is defined as follows:
⎤
PT (k) 01×n · · · 01×n
⎢ 01×n PT (k) · · · 01×n ⎥
⎥
⎢
P D(k)  ⎢ .
.
..
.. ⎥
..
⎣ ..
.
.
. ⎦
01×n 01×n · · · PT (k) n×n2
⎡

(26)

ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

STEAM: A Smart Temperature and Energy Aware Multicore Controller

151:23

Using the solution of (25), an estimate for error e is computed (e = R.H.S − L.H.S of
(25)). With the estimated e, the moving average component q is obtained by including
the error term in this equation as shown in (27) (see the last column of the first matrix
on the left) and solving (27) iteratively until the difference in the value of a between
two consecutive iterations is less than 10−5 . Note that the first element of the error
column contains the average error. This is because there is no prior knowledge of the
error at time k = 0. This procedure provides accurate values of a, b and R.
⎡
⎤
a
⎢ R
⎥
11
⎢
⎥
⎢
⎤ ⎢ R12 ⎥
⎡
⎥
K
⎢
⎥ ⎡

⎤
.
1
⎢
⎥
..
⎢ T(1) P D(2) P D(1) K−1
e(k)⎥ ⎢
T(2)
⎥
⎥⎢
⎢
⎥
⎥⎢ R
⎢
k=1
T(3) ⎥
⎥ ⎢
⎥
⎥⎢
⎢
nn
⎥=⎢
.
(27)
e(1)
⎢
⎥⎢
⎢ T(2) P D(3) P D(2)
⎥ ⎣ .. ⎥
⎥
⎢
b
−
aR
⎦
11
⎢
⎥
.
..
..
..
..
⎥⎢
⎢
⎦ ⎢b − aR12 ⎥
⎣
.
.
.
.
⎥
T(K)
⎢
⎥
..
T(K − 1) P D(K) P D(K − 1) e(K − 2)
⎢
⎥
.
⎢
⎥
⎢
⎥
⎣b − aRnn⎦
q
B. MINIMIZING SENSOR NOISE WITH A KALMAN STATE ESTIMATOR

Consider the following example of a nonlinear state space system:
x(k + 1) = f (x(k), i(k)) + w(k)
z(k) = h(x(k), i(k)) + v(k).

(28)
(29)

x is the state variable, i and z are the input and the output variables, respectively. w
and v are the process and the observation noises, both of which are assumed to be zero
mean multivariate Gaussian noises with covariance Cw and Cv , respectively.
The predict and the update equations corresponding to the Extended Kalman filter
are given here:
Predict Equations
x̂(k + 1|k) = f (x̂(k|k), i(k))
Ce (k + 1|k) = F(k)Ce (k|k)F (k) + Cw (k).
T

(30)
(31)

Equations (30) and (31) correspond to computing the predicted state estimate and
predicted estimate covariance, respectively. Estimate of a variable y is denoted by ŷ.
Ce is called the error covariance matrix. The state transition matrix F is given by
F(k) = |x̂(k|k),i(k) .
Update Equations
e(k + 1) = z(k + 1) − h(x̂(k + 1|k), i(k + 1))

(32)

Cr (k + 1) = H(k + 1)Ce (k + 1|k)HT (k + 1) + Cv (k + 1)

(33)

1)Cr−1 (k +

1)
G(k + 1) = Ce (k + 1|k)H (k +
x̂(k + 1|k + 1) = x̂(k + 1|k) + G(k + 1)e(k + 1)
Ce (k + 1|k + 1) = (I − G(k + 1)H(k + 1))Ce (k + 1|k).
T

(34)
(35)
(36)

Equations (32) and (33) compute residual measurement and residual covariance, respectively. Equation (34) gives the Kalman gain G, which is used to correct the
ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

151:24

V. Hanumaiah et al.

estimated state and the covariance as shown in (35) and (36), respectively, thereby,
minimizing future predicted errors. In this way, the model errors are taken care of
through adjusting state variables. In is the identity matrix of size n. The observation
matrix H is given by H(k) = |x̂(k|k),i(k) .
Now, we will compute these quantities specific to our controller problem. The state
and the output equations of our controller are:
T p(k + 1) = aT p(k) + b


T(k)
Ptot




=

n


Pc (k)

c=0

T p(k)1n×1
0



+




R
P(k)
11×n

Pc (k) = Pρc ∗ Pτc ∗ Pμc ∗ PT ,c ∗ Pls,c ,2,c ,3,c ,m,c .

(37)
(38)
(39)

1n×1 is a column vector of ones of length n. The corresponding state transition and
observation matrices are given by
⎤
⎡
1 + R ∂∂TPcp
 ∂ Pc
⎥
⎢
F(k) = a + b
(40)
, Hc (k) = ⎣  ∂ Pc ⎦ .
∂
T
p
c
∂
T
p
c
From (13),

∂ Pc
∂ Tp

=

∂ Pc
∂ Tc

and

∂ Pc
∂ Tp

can be computed from (39).

ACKNOWLEDGMENTS
We are indebted to Mr. Chakravarthy Akella (Power Management Team Lead) and Mr. Christopher Lucero
(Director of Platform Customer Enabling) of the Intelligent Systems Group (ISG) at Intel in Chandler,
Arizona for their technical and material support, and their valuable guidance. The important experimental
work would not have been possible without their assistance and cooperation.

REFERENCES
Advanced Configuration and Power Interface Specification. http://www.acpi.info/spec.htm.
ARM. 2012. Big.LITTLE Processing with ARM Cortex-A15 and Cortex-A7. http://www.arm.com/files/
downloads/big.LITTLE F inal.pdf.
A. Bartolini, M. Cacciari, A. Tilli, and L. Benini. 2011. A distributed and self-calibrating model-predictive
controller for energy and thermal management of high-performance multicores. In Proceedings of the
Design, Automation Test in Europe Conference Exhibition (DATE’2011). 1–6.
C. Bienia, S. Kumar, J. P. Singh, and K. Li. 2008. The PARSEC benchmark suite: Characterization and
architectural implications. In Proceedings of PACT. 72–81.
G. Box, G. M. Jenkins, and G. C. Reinsel. 1994. Time Series Analysis: Forecasting and Control. Prentice-Hall.
T. Chantem, R. P. Dick, and X. S. Hu. 2008. Temperature-aware scheduling and assignment for hard real-time
applications on MPSoCs. In Proceedings of DATE. 288–293.
R. Cochran, C. Hankendi, A. K. Coskun, and S. Reda. 2011. Pack & cap: Adaptive DVFS and thread packing under power caps. In Proceedings of the 44th Annual IEEE/ACM International Symposium on
Microarchitecture (MICRO-44’11). ACM, New York, 175–185.
A. Coskun, T. Rosing, K. Whisnant, and K. Gross. 2008. Static and dynamic temperature-aware scheduling
for multiprocessor SoCs. IEEE Trans. VLSI Syst. 16, 1127–1140.
G. Dhiman and T. S. Rosing. 2007. Dynamic voltage frequency scaling for multi-tasking systems using online
learning. In Proceedings of the 2007 International Symposium on Low Power Electronics and Design
(ISLPED’07). ACM, New York, 207–212.
H. Esmaeilzadeh, E. Blem, R. St. Amant, K. Sankaralingam, and D. Burger. 2011. Dark silicon and the end
of multicore scaling. In Proceedings of the International Symposium on Computer Architecture.
Y. Fu, N. Kottenstette, Y. Chen, C. Lu, X. Koutsoukos, and H. Wang. 2010. Feedback thermal control for realtime systems. In Proceedings of the 16th IEEE Real-Time and Embedded Technology and Applications
Symposium (RTAS’2010). 111–120.

ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

STEAM: A Smart Temperature and Energy Aware Multicore Controller

151:25

M. R. Guthaus, J. S. Ringenberg, D. Ernst, T. M. Austin, T. Mudge, and R. B. Brown. 2001. MiBench: A free,
commercially representative embedded benchmark suite. In Proceedings of WWC. 3–14.
V. Hanumaiah and S. Vrudhula. 2012a. Energy-efficient operation of multi-core processors by DVFs, task
migration and active cooling. IEEE Trans. Comput. 99, 1.
V. Hanumaiah and S. Vrudhula. 2012b. Temperature-aware DVFs for hard real-time applications on multicore processors. IEEE Trans. Comput. 61, 10, 1484–1494.
V. Hanumaiah, S. Vrudhula, and K. S. Chatha. 2011. Performance optimal online DVFS and task migration
techniques for thermally constrained multi-core processors. IEEE Trans. Comput.-Aid. Des. 30, 11,
1677–1690.
W. Huang, M. R. Stan, K. Skadron, K. Sankaranarayanan, and S. Ghosh. 2006. HotSpot: A compact thermal
modeling method for CMOS VLSI systems. IEEE Trans. VLSI Syst. 14, 501–513.
Intel Corporation 2012. Intel 64 and IA-32 Architectures Software Developers Manual. Intel Corporation.
C. Isci, G. Contreras, and M. Martonosi. 2006. Live, runtime phase monitoring and prediction on real systems
with application to dynamic power management. In Proceedings of the International Symposium on
Microarchitecture (MICRO).
H. Jung and M. Pedram. 2006. Stochastic dynamic thermal management: A Markovian decision-based
approach. In Proceedings of ICCD. 452–457.
W.-Y. Liang, P.-T. Lai, and C. W. Chiou. 2010. An energy conservation DVFs algorithm for the android
operating system. J. Converg. 1, 1, 93–100.
W. Liao, L. He, and K. M. Lepak. 2005. Temperature and supply voltage aware performance and power
modeling at microarchitecture level. IEEE Trans. Comput.-Aid. Des. 24, 1042–1053.
P. S. Maybeck. 1979. Stochastic Models, Estimation, and Control. Mathematics in Science and Engineering.
Academic Press.
S. Murali, A. Mutapcic, D. Atienza, R. Gupta, S. Boyd, and G. D. Micheli. 2007. Temperature-aware processor
frequency assignment for MPSoCs using convex optimization. In Proceedings of CODES. 111–116.
J. Nocedal and S. J. Wright. 2006. Numerical Optimization 2nd Ed. Springer.
V. Pallipadi and A. Starikovskiy. 2006. The ondemand governor - past, present, and future. In Proceedings
of the Linux Symposium, Vol. 2, 223–238.
R. Rao and S. Vrudhula. 2009. Fast and accurate prediction of the steady state throughput of multi-core
processors under thermal constraints. IEEE Trans. Comput.-Aid. Des. 28, 1559–1572.
E. Rotem, A. Naveh, D. Rajwan, A. Ananthakrishnan, and E. Weissmann. 2012. Power-management architecture of the intel microarchitecture code-named sandy bridge. Micro, IEEE 32, 2, 20–27.
K. Skadron, T. Abdelzaher, and M. R. Stan. 2002. Control-theoretic techniques and thermal-RC modeling for
accurate and localized dynamic thermal management. In Proceedings of HPCA. 17–28.
D. C. Snowdon, E. Le Sueur, S. M. Petters, and G. Heiser. 2009. Koala: A platform for OS-level power
management. In Proceedings of the European Conference on Computer Systems (EuroSys’09). ACM,
New York, 289–302.
SPEC CPU2006 Benchmarks. http://www.spec.org/cpu2006.
Y. Wang, K. Ma, and X. Wang. 2009. Temperature-constrained power control for chip multiprocessors with
online model estimation. SIGARCH Comput. Archit. News 37, 314–324.
P. Whittle and T. J. Sargent. 1983. Prediction and Regulation by Linear Least-Square Methods 2nd Ed.
University of Minnesota Press.
F. Zanini, D. Atienza, L. Benini, and G. De Micheli. 2009. Multicore thermal management with model
predictive control. In Proceedings of the European Conference on Circuit Theory and Design, 2009.
ECCTD 2009. 711–714.
Received June 2013; revised February 2014; accepted April 2014

ACM Transactions on Embedded Computing Systems, Vol. 13, No. 5s, Article 151, Publication date: September 2014.

Characterizing the Latency Hiding Ability of GPUs
Shin-Ying Lee and Carole-Jean Wu
Arizona State University
{lee.shin-ying, carole-jean.wu}@asu.edu

1. I NTRODUCTION
The ability to perform fast context-switching and massive multi-threading has been the forte of modern Graphics
Processing Unit (GPU) architectures, which have emerged
as an efficient alternative to traditional chip multiprocessors
(CMPs) for parallel workloads. Therefore it is becoming more
and more common today to execute general-purpose GPU
(GPGPU) workloads on the highly-parallel architecture. GPUs
are based on the Single Instruction Multiple Thread (SIMT)
computing paradigm [1] where multiple threads are grouped
together to form a warp or wavefront. The threads in the
warp are then mapped to the Single Instruction Multiple Data
(SIMD) execution unit such that all threads execute the same
instruction, but with different data.
The benefit of fast context-switching and the large number
of warps is latency-hiding: whenever the execution of a warp
is stalled, it can be readily swapped out and another warp
can be swapped in for immediate execution to maximize the
computing resource utilization. These stalls could be caused
by pipeline hazards, such as control or structural hazards, or
memory accesses. To investigate GPU’s latency-hiding ability,
we develop an algorithm that attributes latencies caused by
pipeline hazards as well as latencies from instruction and data
cache accesses.
2. M ETHODOLOGY
In order to perform detailed study of the latency-hiding
ability of modern GPU architecture, we simulate the microarchitecture and run a suite of GPGPU applications with
GPGPU-sim simulator [2]. In our study, we run GPGPU-sim
version 3.2.0 with the default configuration of NVIDIA Fermi
GTX480 architecture [3].
The performance of GPUs heavily depends on the massively
multi-threading feature and fast context switching. At every
cycle, the GPU warp scheduler selects an available warp from
the warp pool for execution. If there is an instruction ready
in the selected warp’s instruction buffer without any pipeline
hazards, this warp instruction is ready for execution; otherwise
it is stalled due to one of the following factors: cache miss,
data hazard, control hazard, structural hazard, or delay caused
by the warp scheduling policy.
To evaluate the effectiveness of the GPU warp scheduler’s
latency hiding ability, we need to root-cause the sources of
warp execution time delays. At every cycle, the warp scheduler
looks for a ready warp with an order based on the scheduling
policy, e.g, round-robin (RR). For each of the visited warps,
we examine and record the execution status of the warp by
Algorithm 1.

978-1-4799-3606-9/14/$31.00 ©2014 IEEE

Input: w: warp
w.Scheduling = CurrT ime − w.P revT ime;
w.P revT ime = CurrT ime;
if Sync then
w.Sync++;
end
else if EmptyInstF etchBuf f er then
w.F etch++;
end
else if prevBranchT aken then
w.CtrlHazard++;
end
else if DataDependency then
if onOlderDataCacheM iss then
if w.CurrP endingAddr 6=
w.P revP endingAddr then
w.DataHazard++;
w.P revP endingAddr =
w.CurrP endingAddr;
end
else
w.DataCacheM iss++;
end
end
else
w.DataHazard++;
end
end
else if F U _unavailable then
w.StructuralHazard++;
end
else
w.Exec++;
end
Algorithm 1: Warp Latency Breakdown.
We first calculate the time during which a warp is not
executing any new instruction because of the scheduling delay.
This is determined to be the difference between the time when
a warp is visited and the time when this particular warp was
last visited. The time difference signifies how long the selected
warp needs to wait until it can potentially issue the next
instruction, i.e., the scheduling delay.
After the scheduling latency is determined, we examine
whether the selected warp is ready or not. If not ready, we
investigate each stall factors sequentially. Because a warp can
be stalled due to multiple stall factors at the same time (the
latency hiding feature by the massive multi-threading GPUs),
we want to be cautious about not double-counting latencies
overlapped by several stall factors. For example, if a warp is
stalled due to data dependency on an older load instruction

145

Latency Breakdown

1
0.9

Scheduling

0.8

Execution
Sync.

0.7

Inst. Fetch

0.6

MSHR

0.5

Data$

0.4

Ctrl. Hzrd.

0.3

Strcl. Hzrd.

0.2

Data Hzrd.

0.1
0

Fig. 1.

Latency breakdown for the Rodinia applications.

which misses in the data cache, we attribute only the first
stall cycle to the data hazard and any additional, subsequent
stall cycle(s) to the data cache miss factor. This is because
the underlying reason for the warp stall is the data cache
miss encountered by the previous memory miss. If there is
no following dependent instruction on a previous load cache
miss (while the cache miss is being serviced), our algorithm
does not attribute any stall cycle for this data cache miss.
Since GPUs rely on the fast switching between the massive
number of warps to improve pipeline resource utilization, even
if a warp encounters no delays from pipeline hazards, memory
accesses, or synchronization overhead, it still suffers from
delays caused by the scheduling policy. For example, if the RR
scheduling policy is applied, each warp will spend X cycles
waiting until its next ready instruction can be executed, where
X is equal to the number of active warps on a GPU core. This X
scheduling cycles will be used to overlap with other latencies
for the particular warp. Because the additional latencies are
hidden by the scheduler which overlaps the scheduling latency
with the execution of the other ready warps, these stall cycles
are shown in the scheduling latency category.
3. C HARACTERIZATION R ESULTS
Figure 1 shows the latency characterization results for
the Rodinia applications [4] with the RR scheduling policy.
The x-axis presents the latency breakdown of the GPGPU
applications sorted by the degree of the scheduler’s latencyhiding ability while the y-axis shows the ratio of latency stalls
attributed by the various factors.
We first focus on the Scheduling bars to investigate the
latency-hiding ability. The RR scheduler can hide the majority
of the warp stall cycles in applications such as backprop,
kmeans, and pathfinder. These applications are considered
as well-behaving GPGPU applications. On the other hand,
the latency-hiding ability of the RR scheduler is poor for
applications, e.g., myocyte and needle. This is because there
lacks warp-level parallelism in these two applications, i.e.,
there are not enough active warps in the warp pool.
We next look at the amount of latency stalls contributed
by the three classic types of pipeline hazards. Data hazard
stalls are caused by the particular instruction ordering within
an application. We observe that applications, e.g., myocyte,
needle, and particle_float, suffer from data hazard stalls
more than the other applications. Structural hazard stalls are

caused by the competition for pipeline resources, in this case,
the functional units in the pipeline. Applications, e.g. lavaMD,
spend a significant amount of time waiting for the structural
hazard to be resolved. This is because the warps in lavaMD
heavily compete for the load/store unit in the pipeline. For
all GPGPU applications, we do not see much stall penalty
caused by the control hazard. Compared with CPU applications, GPGPU applications contain less branch instructions [5].
Furthermore, since the branch resolution latency is relatively
small, it can often be easily hidden by the scheduler’s multithreading feature.
In addition to the structural hazard stall caused by the
unavailability of functional units, contention in the miss status
holding registers (MSHRs) can cause additional penalty. lud,
in particular, experiences a significant amount of delay by the
unavailability of the MSHRs. This is because warps in lud,
a memory-bandwidth intensive program [4, 6], often request
data from the memory in a bursty manner. As a result, the
performance of lud is significantly degraded by the MSHR
contention. Such contention happens to srad_2 and bfs as
well. Our studies show that when increasing the number of
MSHR entries from 32 to 64, the MSHR stall cycles are halved
for bfs.
Finally, the Sync. component in Figure 1 indicates the
amount of time warps spend waiting on barriers in the kernel.
b+tree and particle_float are two applications in which
warps spend a large amount of time waiting on barriers.
This significant synchronization time is caused by workload
imbalance between the warps.
4. C ONCLUSION
This paper demonstrates a latency profiling approach to
characterize and evaluate for the latency-hiding capability of
modern GPU architectures. We find that the fast contextswitching and massive multi-threading architecture can effectively hide much of the latency by swapping in and out warps.
However, for certain GPGPU applications, such as bfs, the
performance is limited by other factors. In future work, we
plan to use the latency profiling approach to further investigate
the limits of GPUs and seek for performance improvement
opportunities.
R EFERENCES
[1] E. Lindholm, J. Nickolls, S. Oberman, and J. Montrym, “NVIDIA Tesla:
A Unified Graphics and Computing Architecutre,” IEEE Micro, Mar 2008.
[2] A. Bakhoda, G. Yuan, W. W. L. Fung, H. Wong, and T. M. Aamodt,
“Analyzing CUDA Workloads Using a Detailed GPU Simulator,” in Proc.
of the 2009 International Symposium on Analysis of Systems and Software
(ISPASS-2009), 2009.
[3] Glaskowsky, Peter N., “NVIDIA’s Fermi: The First Complete GPU
Computing Architecture,” http://nvidia.com.
[4] S. Che, M. Boyer, J. Meng, D. Tarjan, J. W. Sheaffer, S.-H. Lee, and
K. Skadron, “Rodinia: A Benchmark Suite for Heterogeneous Computing,” in Proc. of the 2009 International Symposium on Workload
Characterization (IISWC-2009), 2009.
[5] A. Kerr, G. Diamos, and S. Yalamanchili, “A Characterization and
Analysis of PTX kernels,” in Proc. of the 2009 International Symposium
on Workload Characterization (IISWC-2009), 2009.
[6] A. Jog, O. Kayiran, N. C. Nachiappan, A. K. Mishra, M. T. Kandemir,
O. Mutlu, R. Iyer, and C. R. Das, “OWL: Cooperative Thread Array
Aware Scheduling Techniques for Improving GPGPU Performance,” in
Proc. of the 18th International Conference on Architectural Support for
Programming Languages and Operating Systems (ASPLOS-13), 2013.

146

Performance. Energy Characterizations and Architectural Implications of
An Emerging Mobile Platform Benchmark Suite - MobileBench
Dhinakaran Pandiyan, Shin-Ying Lee, and Carole-Jean Wu
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University
Tempe, Arizona 85281
Email: {dpandiya.lee.shin-ying.carole-jean. wu} @asu.edu

Abstract-In this paper, we explore key microarchitectural
features of mobile computing platforms that are crucial to the
performance of smart phone applications. We create and use
a selection of representative smart phone applications, which
we call MobileBench that aid in this analysis. We also evaluate
the effectiveness of current memory subsystem on the mobile
platforms. Furthermore, by instrumenting the Android frame­
work, we perform energy characterization for MobileBench on
an existing Samsung Galaxy S III smart phone.

roadmap for the smart phone platforms has largely followed its
desktop predecessors - cramming more and more architectural
features such as branch predictors or cache prefetchers, and
putting more and more cores on the same die, as exemplified
by the recent Qua1comm S4 Snapdragon and NVIDIA Tegra 3
quad-core smart phone processors. Mobile phone computing
has evolved in such a manner that it is time to re-evaluate
and re-think the needs of the applications that we run on
these mobile platforms. The processor cores in these modern
smart phones have grown more complex with a goal to deliver
high performance, since users expect feature-rich, responsive
and interactive applications on their smart phones. However,
there are some particular characteristics of the smart phone
applications that have not been fully examined and exploited.

Based on our energy analysis, we find that application cores
on modern smart phones consume significant amount of energy.
This motivates our detailed performance analysis centered at the
application cores. Based on our detailed performance studies,
we reach several key findings. (i) Using a more sophisticated
tournament branch predictor can improve the branch prediction
accuracy but this does not translate to observable performance
gain. (ii) Smart phone applications show distinct TLB capacity
needs. Larger TLBs can improve performance by an avg. of
14%. (iii) The current L2 cache on most smart phone platform
experiences poor utilization because of the fast-changing memory
requirements of smart phone applications. Using a more effective
cache management scheme improves the L2 cache utilization by
as much as 29.3% and by an avg. of 12%. (iv) Smart phone appli­
cations are prefetching-friendly. Using a simple stride prefetcher
can improve performance across MobileBench applications by an
avg. of 14%. (v) Lastly, the memory bandwidth requirements of
MobileBench applications are moderate and well under current
smart phone memory bandwidth capacity of 8.3 GB/s.

First, users typically do not use their smart phones for
Typical smart phone usage reveals
a pattern of a short-term use, e.g., texting, viewing pictures,
or searching for information about a restaurant, followed by
a long period of idle time during which the device is often
put into sleep mode. Second, the main computation of many
smart phone applications may be off-loaded onto the cloud,
whereas the smart phone device is primarily responsible for
displaying the content.Finally, smart phone applications are
generally interactive applications that require user input and
are expected to be responsive to handle a burst of data in a
short amount of time.
continuous computation.

With these insights into the smart phone application char­
acteristics, we hope to guide the design of future smart phone
platforms for lower power consumptions through simpler archi­
tecture while achieving high performance.

Index Terms-smart phone benchmarks; smart phone archi­
tectures; energy and performance characterization.

1.

In order to explore key characteristics critical to smart phone
applications, examine the effectiveness of modern architec­
tural features for these applications, and design architectural
features specifically for smart phone devices, this paper devel­
ops a mobile platform benchmark suite, MobileBench. Mo­
bileBench is a collection of representative smart phone appli­
cations, including general-purpose interactive web browsing,
education-oriented web browsing, photo browsing, and video
playback, that constitute the majority of activities performed
on today's smart phone platforms.

INTRODUCTION

In recent years, there has been an explosive growth in
the use of mobile phones - especially smart phones - for
our everyday computing needs. The global shipment of smart
phones has already surpassed the shipment of personal com­
puters in 2011.1n 2012, more main memory modules (DRAM)
are shipped to smart phones and tablet computers than to
traditional personal computers for the first time in history [1].
These trends indicate the increasing importance of studying
the perfonnance requirements of the applications that we run
on these mobile computing platforms. We need to better un­
derstand the architectural implications that these applications
have brought upon, in order to design architectures that can
be as powerful and responsive as need be and, at the same
time, be as efficient and energy-conserving.
From the performance perspective, the current architectural
978- 1 -4799-055-31 1 3/$3 1 .00 ©20 1 3 IEEE

This paper aims to answer the following questions:
1) What is the energy profile like for smart phone applica­
tions? What is the most energy-hungry processing element
on a modern smart phone platform?
2) What are the key performance-critical components on the
smart phone processors? E.g., how sophisticated does the
branch predictor need to be for smart phone applications?

133

detailed performance analysis and architectural insights, we
hope to guide the design of future smart phone platforms for
lower power consumptions through simpler architecture while
achieving high performance.
2. C O N S TRUCTING M OB ILE BENCH BENCHMARK SUITE:
A key aspect of this work is to run each MobileBench
application on a smart phone platform and to identify key
microarchitectural characteristics of these smart phone ap­
plications with real-system effects. The MobileBench suite
includes a diverse range of real-world smart phone applica­
tions. In addition to the publicly available BBench [2] that is
used to represent simple web browsing behavior, we include
four additional commonly-used benchmarks - realistic web
browsing, education-oriented web browsing, photo rendering,
and video playback. Next, we discuss each MobileBench
application in more detail.
General Web Browsing (GWB): One of the most impor­
tant smart phone applications is web browsing. In fact, the
web browser is one of the most commonly-used interactive
applications on smart phones, and many other smart phone
applications are also browser-based. To study the behavior of
general-purpose web browsing, Gutierrez et al. [2] constructed
BBench which is an offline, automated benchmark to assess
the performance of a web browser when rendering a collection
of 11 popular websites on the web, including Amazon, BBC,
CNN, Craiglist, eBay, ESPN, Google, MSN, Slashdot, Twitter,
and YouTube. BBench traverses the collection of the websites
repeatedly by loading the webpage and scrolling down to the
bottom of the webpage before proceeding to the next website.
In this paper, we refer to BBench as GWB since it is a benchmark
which focuses on simple general web browsing behavior.
Realistic General Web Browsing (RealisticGWB): The
always-scroll-down browsing pattern in GWB does not reflect
a realistic browsing pattern. In order to model a more real­
istic user web browsing behavior, we instrument index.html
for each webpage to include additional movement patterns.
Specifically, the RealisticGWB benchmark introduces verti­
cal up-and-down, horizontal right-and-left movements with
random delays between each movement. This models the
browsing pattern where users spend more time reading web
contents located on specific parts of a webpage and skim
through the rest of the page.
Education Web Browsing (EWB-Blackboard): As technol­
ogy advances, students today are able to use their smart phones
to read course announcements and get started with assignments
by accessing course websites on smart phone devices. These
educational websites, however, exhibit significantly different
types of contents than those included in general-purpose
web browsers. Unlike general-purpose websites where web
contents are more sophisticated, e.g., with images, audio/video
streams, or advertisement clips, web contents on these edu­
cational websites are mostly text or document formats, where
documents are often embedded in web links. To capture this set
of browsing behaviors in addition to RealisticGWB, we develop
a benchmark that focuses on browsing educational websites.
To model EWB-Blackboard, we use a popular education-

3) Is a large translation lookaside buffer needed to accelerate
virtual-to-physical address translation?
4) For memory-bound smart phone applications, what is the
appropriate cache memory hierarchy? How can we improve
the cache memory utilization for smart phone applications?
5) Are smart phone applications prefetching-friendly? Is it
needed to include a hardware prefetcher on the devices?
6) Is memory bandwidth a performance bottleneck due to the
smaller size of the on-chip caches?
On the other hand, from the perspective of energy-efficiency,
modern smart phones are limited by their energy usage, due
to the constrained battery capacity. To understand the energy
profile of smart phones, we implement EnergyUsageCollector,
a background service application within the Android Jelly
Bean framework. EnergyUsageCollector collects energy usage
for critical platform components, e.g., application cores, WiFi
and radio antenna, LCD screen, etc., for running applications.
When the brightness of the LCD screen is at 25%, Ener­
gyUsageCollector shows that application cores become the
most energy-hungry element, consuming more than 50% of the
total energy capacity. This motivates us to delve deeper into
understanding the architecture of the application cores with
detailed full-system simulation. To identify the performance­
critical components for the baseline architecture, we run
MobileBench on the Android ICS operating system on an
ARM-based processor and reach several important findings.
• With the detailed energy profile, we show that the appli­
cation cores, executing the main application, the library
functions, and the kernel source codes, dominate the energy
consumption of the smart phone device.
• Using a more sophisticated tournament branch predictor can
improve the branch prediction accuracy but this does not
translate to observable performance gain.
• Smart phone applications show distinct TLB capacity needs.
Although not all MobileBench applications see performance
benefit with a larger TLB, using a larger TLB improves
MobileBench performance by as much as 50% and by an
average of 14%.
• The current L2 cache on the smart phone platform generally
experiences poor utilization because of the fast-changing
application memory requirements. Using a more effective
cache management scheme mitigates the problem and can
increase the cache utilization by as much as 29.3%.
• Prefetching can also mitigate the long memory latency
between the processor and the memory. Overall, smart
phone applications are prefetching-friendly. With a simple
stride prefetcher, the application performance is improved
by an average of 14%.
• Lastly,
the memory bandwidth requirements of Mo­
bileBench applications are moderate and are well under the
current smart phone memory bandwidth capacity.
This work provides a benchmark suite of frequently-used
smart phone applications which is used to explore the best
suitable architectural designs for mobile platforms I. With the
I MobileBench is made available to the research community at http://Iab.
engineering.asu.edulmobilebench/

1 34

--------600 �---------------------------=�

oriented web platform commonly used in universltles to
host course materials and tools, Blackboard.Students access
Blackboard webpages for course information, announcements,
assignments, discussions, etc. In addition to the Blackboard
webpage browsing, EWB-Blackboard often involves viewing
assignment documents that are not directly displayed in a web
browser. For example, course assignments from Blackboard
webpages are often made accessible in Portable Document File
(PDF) format. This means that in-between several Blackboard
webpage browsing sessions, students often need to switch from
web browsing to document viewing.
To understand the interaction between Blackboard webpage
browsing and document viewing, we model the behavior of
first browsing through Blackboard webpages and opening
an assignment file embedded in a link on the Blackboard
webpage. To open the assignment file in PDF format, we use
a third-party PDF viewer, ezPDF Reader, on our target.
Photo Viewing (PhotoView): With the increasing number
of pixel counts for the camera on modern smart phones (as
high as 8 mega pixels), high resolution photos are prevalent
on these mobile platforms. To view high resolution photos
smoothly, modern smart phones must be capable of displaying
high resolution photos on the screen timely for a satisfactory
user experience. To represent this class of applications, we
automate high resolution photo rendering for the Android
platform using a picture viewing application: QuickPic. Our
PhotoView benchmark includes consecutive photo rendering
of five high resolution images, each with a resolution of
4912x3264 and is of size between 4 to 6 MB.
Video Playback (VideoPlayback): In addition to PhotoView,
an important class of applications for modern smart phones
is high definition video streaming. With popular video shar­
ing, users today frequently view video/movie clips on their
smart phones and expect high performance delivery in this
application class. We prepare VideoPlayback to evaluate only
the rendering performance for our target mobile platform,
excluding any network issues that might affect our results. We
use the same QuickPic application to play a high-definition
(nOp) MPEG-4 video of size 80 MB and 1 minute in length.
3. EXPERIMENTAL SETUP
This section introduces our experimental methodology for
both real-system measurements and full-system simulation.

«
.§.. 450
� 300

�

...

150

Fig. 1 . The amount of energy consumed by various hardware component on
Samsung Galaxy S III smart phone.

EnergyUsageCollector measures the amount of time an ap­
plication spends utilizing the different hardware components,
e.g., application CPU cores, Wifi, LCD screen. To calculate
the total energy consumed for each hardware component,
EnergyUsageCollector simply multiplies the amount of time
spent at each component with the energy constant from
powecprofile.xml. This approach is similar to how the An­
droid framework estimates for its remaining battery capacity
for the operating system level power management activities.
Because our EnergyUsageCollector implementation requires
changes to the default Android framework, we flash the
device ROM for the service application to take effect. For
MobileBench, we run and collect the application energy profile
for a duration of 30 minutes. Note, because power_profile.xml
is available in most of the modern smart phones, our Ener­
gyUsageCollector can be easily ported to other smart phone
devices to provide energy profile characterization.
To validate EnergyUsageCollector's measurement, we mea­
sure the device energy consumption with a Watt's Up RC
watt meter (part# BOOOX4MSVW) with which the current
measurement resolution is O.OlA and the energy measurement
resolution is 0.1Who The source of the power meter is con­
nected to a constant DC power source whereas the load is
connected to the smart phone device.
3.2. Full-System Performance Simulation

We evaluate the performance of MobileBench applications
with full-system simulation using gemS [3]. We model a smart
phone platform based on the ARMv7 processor architecture
running the Android Ice Cream Sandwich (ICS) operating
system. The browser-based applications, GWB, RealisticGWB,
and EWB-Blackboard, render webpages off-line and focus on
webpage rendering performance.

3.1. Real-Device Energy Measurement Infrastructure

To characterize the energy consumption for the various
important hardware components on a smart phone platform,
we implement a background service application, called En­
ergyUsageCollector, that runs within the Android framework
perioically to collect energy usage behavior. EnergyUsageCol­
lector calculates the energy consumption of a running appli­
cation based on two pieces of important information. First,
by reading the energy specification sheet (powecprofile.xml
in framework-res.apk) provided by smart phone vendors, En­
ergyUsageCollector obtains the energy consumption specific
to the various hardware components. Figure 1 shows the
amount of energy consumed by various hardware component
on our smart phone target, Samsung Galaxy S III. Then,

In order to identify the microarchitectural components that
are performance-critical for smart phone applications, we
model the processor pipeline with the influence of instruction
and data translation lookaside buffers (IIDTLBs), a tournament
branch predictor, L1 instruction and data caches as well as an
unified L2 cache, and a hardware prefetcher. The simulated
processor has a 64-entry ITLB, a 64-entry DTLB, and an
1KB page table walker cache. In addition, the core has 4-way
set-associative private L1 instruction and data caches. The L1
caches are both 64KB with 64B cache lines. There is also a
16-way set-associative unified L2 cache that is 1MB with 64B
cache lines. The configuration for the processor core and the

1 35

Table I
ARCHITECTURAL PARAMETERS OF THE SIMULATED SY STEM.
Operating System

Android Ice Cream Sandwich

Target Platform

ARMv7 processor architecture

Local

branch

pre-

Table II
DESCRIPTION AND CONFIGURATION FOR THE STATE-OF-THE-ART CACHE
MANAGEMENT TECHNIQUES UNDER STUDY. THE CONFIGURATION IS
SELECTED TO REPRESENT THE BEST PERFORMANCE IN EACH SCHEME.

2-level; level-one (Ll) predictor: 1024-entry
local branch history table;

dictor

level-two (L2) predictor: 1024-entry pattern
branch

predictor

predictor:

Ll

64-entry

lO-bit policy selector; 32 sets per set-dueling mon­

SHiP

ta­

branches; L2 64-entry pattern table of 3-bit
saturating counters;

c

0
"lj
...
'"
QI
...
III
QI

counters
Ll I1DTLB 64-entry, full-assoc.; L2 TLB
page table walker cache
64KB, 4-way, Private, 1 cycle

MSHR

Up to 32 outstanding misses

Ll Data Cache

64KB, 8-way, Private, 1 cycle

L2 Unified Cache

1MB, 16-way, Private, Inclusive, 10 cycles

L2 Cache Prefetcher

Stride-prefetcher with depth of 8;

Main Memory

32 outstanding requests, 200 cycles

D screen III! wifi D kernel !llllibraries

• main application D other

3 100%

Selector: 8192-entry table of 2-bit saturating

Ll Inst. Cache

16K-entry SHCT; 2-bit saturating counters; PC­
based signature; 2-bit RRIP counters.

ble indexed with the six most recent local

TLB

lO-bit policy selector; 32 sets per set-dueling mon­
itor; 2-bit RRIP counters.

bit saturating counters;
branch

DIP
DRRIP

Global branch predictor: 8192 entries of 2Local

Parameters
Random victim selection; 4-bit LRU counters.
itor; 4-bit LRU counters.

history table of 3-bit saturating counters.
Tournament

Policy
Random

40%

e.o

20%

QI
c
...

memory subsystem is roughly based on the ARM Cortex-A9
processor core [4]. Table I summarizes these parameters used
in our simulation infrastructure.

0%

- -

-

60%

12
e

0.
>

-

80%

-

-

- -

-

�I�I�I�I�I�I�I�I�I�I�I�I�I�I�I�I�I�I

3.3. Branch Predictors

To investigate the performance impact of branch predictors,
we include two different branch predictors in our study:
a local and a tournament branch predictor [5]. The local
branch predictor represents a simple predictor which uses local
information to predict branch outcomes, whereas the tourna­
ment branch predictor uses a more sophisticated algorithm by
combining local and global information to reach a prediction.
3.4. Memory Subsystem

Technology trends have shown that main memory speeds
significantly lag behind processor speeds. Therefore, the per­
formance of the memory subsystem often has a significant
impact on the overall system performance. While many smart
phone processors adopt the traditional multi-level memory
hierarchy for TLBs and caches, e.g., ARM Cortex-A9 or
Intel Atom, it is, however, unclear how well this memory
architecture performs for the commonly-used smart phone
applications in MobileBench.
To have an in-depth understanding of memory characteris­
tics for MobileBench applications, we compare the memory
subsystem performance using different TLB sizes, various
state-of-the-art cache insertion and replacement techniques,
an inclusive or an exclusive cache hierarchy, and a hardware
prefetcher. For the purpose of the cache performance charac­
terization, we modify the cache module implementation of a
publicly available simulation framework released in the First
HLP Workshop on Computer Architecture Competitions [6].
The modified cache simulator models a two-level cache hi­
erarchy with the same parameters listed in Table I. Next,
in order to create the input memory traces for the modified
cache simulator, we execute each MobileBench application
individually in gem5 (under the environment described earlier
in this section). Therefore, the input memory traces also

Fig. 2.
Energy profile breakdown for various smart phone components
running MobileBench applications.

include full-system effects.
To study the performance effects of more advanced cache
management techniques, we implement three high performing
cache insertion/replacement policies, i.e., DIP [7], DRRIP [8],
and SHiP [9], in addition to the default Least-Recently-Used
(LRU) and random cache replacement schemes. We give the
parameters of the cache management techniques in Table II.
Stride prefetcher: In addition to investigating the perfor­
mance impact of using more effective cache management
techniques, we are also interested in memory prefetching
characteristics of MobileBench applications. To do so, we
model a simple stride prefetcher for the L2 cache. The stride
prefetcher trains on L2 cache accesses for learning about the
stride lengths and can fetch up to 8 prefetch requests ahead.
4. PLATFORM E NERGY C H ARACTERIZATION WITH
E NERGYU S AGEC OLLECTOR

This section shows the MobileBench energy profile with
EnergyUsageCollector application. Figure 2 shows the Mo­
bileBench application's energy profile. When at the brightest
level (100%), the LCD screen is undoubtedly the energy hog
among all platform components. However, when the brightness
of the LCD screen is lower to 25%, the energy consumption
of the application cores starts dominating.
The second important observation based on the Mo­
bileBench energy profile is that, except for general web brows­
ing (GWB and RealisticGWB), commonly-used smart phone
applications spend a significant amount of energy at executing
library function calls (by as much as 36% for PhotoView at the
screen brightness of 25% and by an average of 21% for all

1 36

QlCO

..c:

MobileBench applications).

��1rJ
co

For media-content based applications, e.g., Video Playback
and PhotoView in MobileBench, the application cores consume
30% (at 100% LCD brightness) to 70% (at 25% LCD bright­
ness) of total energy. Given that users spend a significant
amount of time executing media-content based applications
and the auto-brightness setting runs on most of today's
smart phone platforms (by advanced power management), the
application core energy consumption becomes increasingly
dominant.

QI

�E
�

i·�

QI
QI
c..c:

1.75
1.5
1.25
1
0.75
0.5
0.25
0

-

• 32-entry
� 64-entry
• 128-entry
l1li 256-entry

Fig. 3. Performance comparison for using various TLB sizes from 32 to 256
entries. MobileBench applications show distinct TLB capacity needs. Note,
performance is normalized to the baseline 64-entry TLBs .

5.1. MobileBench applications exhibit distinct dynamic in­
struction type distributions

To identify the critical microarchitectural features that af­
fects the performance of an application, we need to first un­
derstand the composition of the applications [11]. On average,
61% of the instructions in the MobileBench applications are
integer instructions and only 1% of the instructions use the
floating point functional unit. 16% of instructions are control
instructions and about 25% of instructions are memory load
and store instructions. Compared to other MobileBench appli­
cations, VideoPlayback has the largest proportion of memory
loads and stores among all instructions it has executed. This is
because, over the program execution, VideoPlayback streams
over large amount of data residing in the high definition
frames continuously. As a result, it has a relatively larger
portion of memory load and store instructions in its program.
Furthermore, VideoPlayback has a relatively small number of
branch instructions in its program. This is because of its
inherent program behavior. Video playback usually contains
fewer number of control instructions in its execution [12].

Overall, when the screen display brightness is at a reasonble,
25% brightness level, the application cores executing the
main application, the library functions, and the kernal souce
codes, become the dominating energy-hungry component on
the Samsung Galaxy III platform. This motivates a deeper
understanding of the application core's architecture - what
are the performance-critical microarchitectural features on the
application cores? Next section presents our performance anal­
ysis with a detailed full-system simulation for MobileBench.
PERFORMANCE ANALY S I S FOR

C

0 ._ tl.O
zQj:E
"'u '"
Cl.
_co

Validation. As Section 3.1 presents, we validate the total
energy consumption obtained by EnergyUsageCollector with
the Watt's Up power meter measurement. EnergyUsageCol­
lector estimates the energy consumption of the device with a
minimum error of 3.6% and an average error of 14.5% for
VideoPlayback and the web browsing applications. However,
this error increases sharply by 3.6X for PhotoView. The energy
consumption estimate given by EnergyUsageCollector is much
higher than that by the power meter. This is because Ener­
gyUsageCollector does not consider the RGB components of
the pixels which constitute the photo images. Previous study
[10] has shown that the power consumption of a white pixel
in comparison to that of a black pixel can be as high as 5
times. This change in energy consumption based on the color
composition becomes particularly significant for PhotoView,
which spends the majority of the time displaying the images on
the screen. To improve the accuracy of EnergyUsageCollector,
both EnergyUsageCollector and the default battery estimation
application in the Android framework need to account for the
color profile of images being displayed on the screen.

5.

-'

"'1-"'2
0 >- QI

5.2. MobileBench shows distinct TLB capacity requirements

Since every memory reference requires virtual-to-physical
address translation, TLBs cache frequently accessed page table
entries to accelerate these translations. When there is a hit in
the TLB, the virtual-to-physical address translation can be used
directly; otherwise, page table walks are needed.
To characterize the performance impact of TLBs for Mo­
bileBench, we vary the size of the instruction and data TLBs
from 32 to 256 entries. Figure 3 shows the corresponding
performance changes in IPC with respect to the baseline 64entry TLBs. In general, increasing the TLB sizes helps to
improve MobileBench application performance. Figures 4 (a)
and (b) compare the number of page table walks needed to be
performed for resolving ITLB and DTLB misses respectively.
The performance of web browsing applications in Mo­
bileBench, GWB and RealisticGWB, is significantly affected by
the size of the ITLB. This is because web browsing applica­
tions involve more shared library functional calls referencing
a larger instruction address space. Figure 4 (a) shows that,
compared with the baseline 64-entry ITLB, using a larger
ITLB of 256 entries reduces the number of page table walks
required by the two applications by 87% and 86% respectively.
The miss rate reductions correspond to 13.3% and 8.2%
performance improvement for GWB and RealisticGWB.

M OB ILE B ENCH

In order to glean insights into the inherent properties of
these smart phone applications, we study in detail six char­
acteristics of each application in MobileBench: (1) instruction
type distribution (Section 5.1), (2) TLB capacity requirement
(Section 5.2), (3) average memory access time (Section 5.3),
(4) working set size (Section 5.4), (5) access pattern analysis
(Section 5.5), and (6) program phases of memory accesses
(Section 5.6). With the understanding of the applications'
behaviors, we discuss the performance results which pro­
vide insights into potential improvement for mobile plat­
form processor designs. Specifically, we look at the impacts
of the following critical components: (1) branch predictors
(Section 5.7), (2) state-of-the-art cache memory management
techniques (Section 5.8), (3) prefetchers (Section 5.9), and (4)
inclusive vs. exclusive cache hierarchy (Section 5.10).

1 37

.32-entry
� 64-entry
Ell 256-entry
D 128-entry
2.5 -,-----

�

.32-entry
D 128-entry
5

64-entry
Ell 256-entry

<u

�

[

a.

I
� ��

-.."'-

"",,-""",-

_

2.5

+-----...... -

�

2

�

-

-5

1.5

::l

-

�

1

�

50%
25%

-

� 0.5

<u

0%

E

:E

(a)

-,--,"""-,,""

-,-_______

75%

�

h.

100%

3

�
<u

-

Ell L 2 Cache Miss Access Time
� L2 Cache Hit Access Time
D Ll Data Cache Hit Access Time
• Lllnst. Cache Hit Access Time

-ro---r'--,

�
c

+--.-+11-.- __·"

-.. -

o +".,- ----,""-r ----,"--,

(b)

Fig. 4. Miss rate comparison for using various (a) ITLB and (b) DTLB sizes
from 32 to 256 entries.

Fig. 5. A large portion of average memory access time is spent at handling
L2 cache misses.

In addition to the web browsing applications, VideoPlayback
sees even more performance improvement when the DTLB
size is increased from 32 entries to 128 entries: a performance
gain of 75%. This is because VideoPlayback accesses a large
data working set spanning over multiple memory pages. The
number of virtual-to-physical address translations could not fit
well into either the 32-entry nor the baseline 64-entry DTLB.
Therefore, page table walks need to be performed frequently
to resolve the address translations that are not cached in the
DTLB. When the number of DTLB entries are increased
from the baseline 64 to 128 entries, the number of page
table walks performed are reduced significantly by 91% which
corresponds to the IPC performance improvement of 52%.
In contrast to the large TLB size requirement for
GWB, RealisticGWB, and VideoPlayback, EWB-Blackboard and
PhotoView while experiencing lower ITLB and DTLB miss
rates, do not see much performance gain with larger ITLB or
DTLB. Overall, the smart applications in MobileBench have
distinct TLB capacity requirements. This motivates design op­
timization for the TLB structures to maintain high performance
for applications that are sensitive to the TLB sizes.

libraries and device drivers. In addition to the fact that there are
more instruction references (one for every instruction issued)
than data references, the frequent function calls to the shared
libraries contribute to more instruction cache references as
well. As a result, while the L1 instruction cache hit rate
is slightly higher than that of L1 data cache, the total L1
instruction cache hit latency still dominates.
In contrast to the high temporal locality access pattern seen
in the L1 instruction and data caches, most references that miss
in the L1 caches are likely to miss in the L2 cache as well
(L2 Cache Hit Access Time of Figure 5 (a) is insignificant).
Consequently, a significant amount of overall memory access
latency, 41%, is spent at accessing the main memory. This
low utilization of the L2 cache causes not only performance
overhead but also increases bandwidth requirement.
Figure 5 (b) shows the off-chip memory bandwidth utilized
by the smart phone applications. PhotoView is the applica­
tion that has the highest memory bandwidth utilization in
MobileBench. The high bandwidth requirement is because
PhotoView needs to perform high definition photo rendering for
five distinct high resolution photos consecutively. On the other
hand, one might think that VideoPlayback should also show
a similar memory bandwidth requirement. However, because
most video encoding schemes only encode the differences
between frames for most video frames, VideoPlayback does
not have to fetch a lot of data when transitioning from one
frame to another. As a result, within the same time period, its
bandwidth requirement is not as much as that of PhotoView.

5.3. A large portion of average memory access time is spent
at handling L2 cache misses

In addition to the TLB structure, around half of the proces­
sor die area is dedicated to the memory subsystem. By includ­
ing the multi-level cache hierarchy on chip, frequently-used
data are kept close to the processor, enabling fast accesses.
Figure 5 (a) shows the breakdown for the average memory
access time of the Ll instruction and data caches, as well as
the unified L2 cache in the multi-level cache hierarchy. The
L1 caches retain the most frequently accessed instruction and
data working sets close to the processor so it can reduce the
processor stall time. While the Ll cache access latency is only
one cycle, it still contributes to over half of the total average
memory access time across all MobileBench applications (Ll
Inst. Cache Access Time and L1 Data Cache Hit Access Time
in Figure 5 (a)). This is because the L1 caches are also one
of the most frequently-accessed structures.
Furthermore, as Gutierrez et al. pointed out in their in­
teractive smart phone application studies [2], smart phone
applications often generate more function calls to the shared

Finally, today's smart phone chips are equipped with mem­
ory bandwidth capacity a couple times larger than what Mo­
bileBench applications need. For example, Apple's A6 chips
in the iPhone 5 released in 2012 utilizes a memory bandwidth
that has the peak capacity of 8,528 MB/s. Unless multiple
bandwidth-hungry smart phone applications are executing si­
multaneously, current memory bandwidth capacity should be
sufficient for fast data delivery. Although memory bandwidth
on today's smart phone chips is not a performance bottleneck,
we believe that the memory bandwidth requirement can be
significantly reduced via more effective L2 cache management,
so fewer data accesses go to the main memory.

138

100
_

80

2l

60

::l

40

�
/:.
:E

Baseline L2 Cache Size
---+--

1

···GWB
-Realistic GWB

-PhotoView

...
c
u

- VideoPlayback

20
o

.
....
.......

0.8

'F....-..:..,.
..
....
. =-��
...
�---- .... EWB-Blackboard

0.6
0.4
0.2

e·
512

2048

8192

Cache Size (KB)

0

32768

0

100

16-way cache set assac.

200

300

Reuse Distance

400

500

Fig. 7.
Reuse distance analysis for L2 cache accesses. MobileBench
applications ex.hibit heterogeneous cache access patterns.

Fig. 6. L2 miss rate comparison for using various cache sizes. The active
working set of MobileBench applications is much larger than the last-level
cache capacity.

stay in the cache and to receive future cache reuses. This is
because cache access outcomes (a cache hit or a cache miss)
depends not only on a reference's reuse distance but also on
the ordering of the memory references.
Through detailed cache access pattern analysis, we ob­
serve a frequent streaming and mixed access patterns2 in
MobileBench applications. With a more effective cache man­
agement technique, streaming cache references can be filtered
out from the cache more quickly, so data with relatively better
locality will be retained in the cache and continue to receive
cache reuses. Consequently, the poor cache utilization problem
caused by the streaming and mixed access patterns can be
much reduced. Section 5.8 will later show that, compared with
the baseline LRU replacement, EWB-Blackboard performs much
better under a more advanced cache management technique,
e.g., DIP, DRRIP, or SHiP, because references that will not
receive cache reuses are identified and quickly filtered out
from the cache. As a result, the L2 cache miss rate of
EWB-Blackboard is reduced significantly by 29.3%.

5.4. The active working set of MobileBench applications is
much larger than the last-level cache capacity

In addition to the memory bandwidth resource, a crucial
on-chip resource for achieving high performance is the multi­
level cache hierarchy, in particular the L2 cache. The last-level
L2 cache bridges the long latency gap between the processor
and the memory and, therefore, its efficiency can directly
impact an application's performance. This and the next two
sections focus on the study of the last-level L2 cache. We first
investigate the L2 cache memory requirement of MobileBench.
To do so, we measure the working set sizes.
Figure 6 shows the cache sensitivity study for MobileBench
applications by varying the cache sizes. With the baseline
1MB L2 cache, high cache miss rates are experienced for
MobileBench. This is because the active working sets of the
smart phone applications under study simply do not fit in the
1MB L2 cache. While increasing the cache capacity helps
retain an application's working set in the cache eventually, this
comes with the expense of additional area and power overhead.
An alternative solution is to deploy a more effective cache
management policy for the L2 cache. Section 5.8 will show
that using more advanced cache management techniques can
significantly reduce the L2 cache miss rate for MobileBench.

5.6. MobileBench shows fast-changing program phases

In addition to the prevalent mixed access pattern in Mo­
bileBench applications, memory access characteristics within
an application show fast phase changes as well. Over one
program phase, an application can mostly have a streaming
memory access pattern resulting in poor cache utilization.
During another phase, the application can start seeing a
locality-friendly memory access pattern.
To illustrate this fast changes in memory access charac­
teristics as programs run, we measure which insertion policy
- LRU-insertion or MRU-insertion - performs better for the
running application under DIP. Figure 8 shows the preferred
insertion policy as programs execute at runtime. The x-axis
plots time in the unit of L2 cache accesses and the y-axis plots
the policy selector value. The policy selector is a lO-bit saturat­
ing counter and its most significant bit (MSB) determines the
preferred cache insertion policy over the program execution.
If the MSB of the selector is 1, the L2 cache performs better
under the LRU-insertion policy so it will use the LRU-insertion
policy. On the other hand, if the MSB of the selector is 0, the
L2 cache follows the MRU-insertion policy.
All MobileBench applications show at least three phase
changes in cache memory access patterns. In particular, for

5.5. MobileBench exhibits heterogeneous cache accesses

To delve deeper into understanding the degree of data
locality in the L2 cache for MobileBench applications, we
analyze the reuse distance of all L2 cache references. The
reuse distance of a memory reference is defined as the number
of distinct interleaving memory references until its successive
reuse. Figure 7 illustrates the reuse distance characteristics
for MobileBench applications. The x-axis represents reuse
distances and the y-axis represents the cumulative density
function (CDF) of reuse distances. For all smart phone ap­
plications in MobileBench, only less than 20% of memory
references have reuse distances smaller than the L2 cache
set associativity. These are the memory references which are
guaranteed to be retained in the cache and be able to receive
future cache hit reuses. For memory references with reuse
distances larger than the cache set associativity, they will result
in misses under the baseline LRU replacement policy.
With a more effective cache management technique, mem­
ory references that will be reused but with reuse distances
larger than the cache set associativity have a higher chance to

2[n a mixed access pattern, frequently-accessed data are interleaved with
less-frequently-accessed data.

139

Dynamic Cache Insertion Policy for Mobile Applications

o Local branch predictor

�.�:.��-��t��:����-������\--- ll �
L_
7 Idobn : J1DL2�,,'w"'''<l22'== J;
_______

o

le+06

....

_______ __�' ___________

2e+06

3e+06

4e+06

VideoPlayback

RU-mSemon

5e+06

6e+06

PhotoView

�� :i- - - - -<t.�liy'J \-� - - - - -�- - -i
- - - -v--- - �--����������� -�:�::'!!;�
, -- �- 7

o '"
o

o

�

le+06

-

"
"

2e+06

,

1

!
\"''''J>I'£:

/!
4e+06

3e+06

'5
Q.

i

O d

0

11 1024

le+06

,,<

-- -

II < ,

---- ------

o

:
;1 ,
l ty , L
2e+06
3e+06

,

le+06

I

"""

-

,

,

5e+06

Ii,,,

---------- ----------- -----------

2e+06

3e+06

4e+06

5e+06

� � �:

_

6e+06

Fig. 8. Dynamic insertion policy decisions as programs run. MobileBench
applications show fast-changing program phases.

the educational web browsing application, EWB-Blackboard,
memory phase changes are the most frequent. For example, in
EWB-Blackboard, users focus on a small, specific range of web
contents or a specific course document for a period of time,
which often generates data accesses with good locality. But,
for another period of time, users quickly skim through lots of
different web contents that can generate data accesses exhibit­
ing a streaming access pattern. The different activities users
perform in EWB-Blackboard translate to more heterogeneous
cache memory access patterns seen by the L2 cache. Further­
more, we simulate different activities in EWB-Blackboard. In
addition to the education webpage browsing, we model the
scenario where users open a course assignment file in the PDF
format. This also contributes to the phase shifts in cache mem­
ory access patterns seen by the memory subsystem. Because
cache memory access patterns for smart phone applications are
more dynamic, a high performance cache management scheme
targeting smart phone chips must take this into account and
be able to adapt to the fast-changing memory phases.

I

I

I

I

I

I

I

I

.s
:5 1.01
�
Q.

I

�

'"

I

0

S

"t:S
(fJ
.!::!

E
0

80

85

90

95

100

Branch Predictor Accuracy (%)

(a )

1

u

'"
75

6e+06

.

,

GWB

Ru-msemon

/�

4e+06

"

\
;/,_

,

�

Realistic GWB

6e+06

i��� rj- -?��-:-�--�:!.--��--���------i--------��7-J ����_,'��:���:�l·��:-��:�������.
:1 � V- � V-- �
� : �� � �� �����;;��
>
u

EWB-Blackboard

RU-msertion

5e+06

1.02

• Tournament branch predictor

Z
U

!!:

0.99

co

3

\.9

co

3

\.9
U

.p

�

'"
Qi

cr::

n

"E

'"

0
.D
.><

u

'"

co
cO
3
LLJ

3:

.><

0
.r:
Q.

a::
0
Qi
"0

Qi

:>
.8

u

'"
'"

.D
>

:>

(b)

Fig. 9.
A simple branch predictor leaves room for further performance
improvement.

can proceed with speculative execution. If the branch predic­
tion is correct, the processor avoids the stall time caused by
branch instructions. However, if a branch misprediction occurs,
the processor needs to squash instructions in the pipeline.
To understand the complexity of the branch predictor needed
to achieve high performance for MobileBench applications,
we explore two different branch predictors: a local branch
predictor and a tournament branch predictor. The detail of each
predictor can be found in Section 3.3.
Figure 9 (a) shows the prediction accuracy for the two
branch predictors used in our studies. In general, the more
sophisticated tournament branch predictor performs better than
the simple local branch predictor. In particular, the tourna­
ment predictor outperforms the local branch predictor for
RealisticGWB and EWB-Blackboard by a larger margin. This is
because, in both RealisticGWB and EWB-Blackboard, realistic
web browsing patterns are simulated. The benchmarks model
various movement patterns by browsing webpages up-and­
down, left-and-right with some random delay. This means, in
both applications, there are more if-else statements for the
different movements than the simplistic always-scroll-down
browsing pattern in GWB. We believe this is the reason the
more sophisticated tournament branch predictor achieves much
better accuracy than the simple local branch predictor.
For other applications in MobileBench, the simple local
branch predictor is sufficient and can provide similar predic­
tion accuracy as the tournament branch predictor. Figure 9
(b) shows the corresponding IPC performance comparison
for using the two different branch predictors. The y-axis
plots the IPC performance of using the tournament branch
predictor normalized to the performance of using the local
branch predictor. For the two applications, RealisticGWB and
EWB-Blackboard, that benefit the most from using the tourna­
ment branch predictor, RealisticGWB receives 1% performance
improvement whereas EWB-Blackboard receives 2% perfor­
mance improvement. The speedup is not impressive due to
the fact that the pipeline stages for smart phone processors are
relatively short, as compared to their desktop counterpart. As
a result, the performance impact of using an accurate branch
predictor is not significant.

5.7. A simple branch predictor leaves room for further per­
formance improvement

After our detailed instruction and memory behavior charac­
terization for MobileBench from Section 5.1 to Section 5.6,
we next discuss microarchitectural optimization techniques we
apply to the mobile platform processor design and present the
corresponding performance impacts.
The branch predictor is one of the most performance-critical
component in today's pipelined processors. Without a branch
predictor, the processor has to wait until the branch instruction
is resolved (often at the execution stage in the pipeline)
before the next instruction can enter the processor pipeline. By
providing a prediction for the branch instruction, the processor

1 40

1.2
�'" !50 1
", ... 0.8
.- ..,
::2: '" 0.6
�v �'"
'" E 0.4
u ...
N 0 0.2
.... z
0

III
b.O _
C ''iii

"' ::>

::>

1II-

� 0.95
<U
o � 0.9
Z
B � 0.85
.., III
<U �
.�
0.8
- ....
'III <u
E � 0.75
0 ....
z <u :u 0.7

I-

• Random

11-

-

ril D R R I P
Ol S H i P

I

co

co

�
.-

19

19
u
'0
.�
!II
Q)

.!
<u
� �

Fig. 1 0 . L 2 miss rate comparison for using various state-of-the-art cache
management techniques. Using a more effective cache management technique
can significantly improve the L2 cache utilization.

-

•

.... �

� �

1.25

�

1.2

'- c.. :u

._

D DI P

'-

<u
�

s s

N
....

a::

5.S. There is a need for using an effective cache management

�
III
0
�

-'"
u

!II
co
cO

S
UJ

3
Q)
:>
0
..,
0
..r::
0..

Q)
III Ill)
!II
� ...
>- Q)
!II >
a:: «
0
Q)
-'"
u

"0

:>

1.15
:, �
.�
� 1.1
::l . .... ...
o c
Z III

B�

.., <u
<u �

� .99
1II l:
E -

o

Z
u

�

(a )

technique for the L2 cache

Fig. I I .

As previously presented in Sections 5.5 and 5.6, fast­
changing, heterogeneous cache access patterns are commonly
in smart phone applications. However, under the default LRU
replacement policy, the L2 cache cannot effectively retain the
more frequently-used data of smart phone applications and, as
a result, experiences poor utilization. To investigate the per­
formance impacts of using more advanced cache management
techniques for MobileBench applications, we implement sev­
eral state-of-the-art cache insertion and replacement policies
for the last-level L2 cache: random, DIP, DRRIP, and SHiP,
and compare the L2 cache miss rate reduction with respect to
the default LRU replacement scheme. The parameters of each
cache management techniques can be found in Table II.
Figure 10 illustrates the L2 miss rates for MobileBench
using the various cache management techniques. Among all
cache management schemes under study, SHiP performs the
best and can reduce the L2 miss rate by as much as 29.3%
for EWB-Blackboard and by an average of 12% for all Mo­
bileBench applications. Other state-of-the-art cache manage­
ment schemes perform relatively well also and can reduce the
L2 cache miss rate for MobileBench effectively.
As previously shown in Section 5.5, under the default LRU
replacement scheme, cache lines with temporal locality cannot
always be identified and retained in the cache effectively
because of other streaming memory references. With a more
effective cache management technique however, the streaming
and mixed access patterns in the smart phone applications can
be quickly detected. For example, DRRIP and SHiP cache
management schemes dynamically predict the reuse pattern for
an incoming cache line and adjust the cache line's insertion
position according to the reuse pattern prediction. When a
mixed cache access pattern is encountered, DRRIP and SHiP
can distinguish cache lines that are less likely to receive future
cache reuses from other lines exhibiting good data temporal
locality. By giving streaming references less time in the
cache, the interference between the streaming references and
cached data is reduced. Compared to DRRIP and SHiP cache
management techniques, DIP can only address the streaming
access pattern but cannot intelligently differentiate cache lines
that are likely to be reused from cache lines that are less
likely to be reused upon insertion. So, it generally performs

-

1.05
•

Q)
3 -'"
� Q)
u
!II Ill)
S S !II
!II
0 :> � ...
19 19 �
>- Q)
u
0 !II
>
..,
'0 -'"
0 a:: «
.� u
!II ..r::
!II co 0.. 0
Q)
Q)
"0
a:: cO
:>
S
co

co

UJ

(b)

MobileBench applications are prefetching-friendly.

less well. Overall, with a more effective cache management
scheme, the L2 cache utilization for MobileBench applications
can be significantly improved.

5.9. MobileBench applications are prefetching-friendly

In addition to using an effective cache management scheme
to bridge the memory latency gap, another popular approach
is to prefetch data into the cache hierarchy before the actual
demand reference. While prefetching can hide memory latency
and improve performance significantly, it can severely de­
grade performance in the event of untimely and/or inaccurate
prefetch requests. To investigate how the performance for
MobileBench applications can be improved (or degraded)
in the presence of hardware prefetching, we measure the
MobileBench performance in the presence of a simple stride
prefetcher and compare it against the performance in the
absence of the prefetcher. The prefetcher parameters are pre­
sented in Table I and its algorithm is described in Section 3.4.
In the presence of the stride prefetcher, the L2 miss rate
for MobileBench applications is significantly reduced by an
average of 15.1%. Figure 11 (a) shows the L2 miss rate when
using the L2 cache stride prefetcher, normalized to when not
using the prefetcher. Across all MobileBench applications, the
L2 miss rate reduction is consistently observed. Furthermore,
Figure 11 (b) illustrates the corresponding IPC performance
improvement for each of MobileBench applications. Among
all MobileBench applications, while VideoPlayback has the
most L2 miss rate reduction, its corresponding IPC perfor­
mance gain is only 2.8%. This is because, compared to other
MobileBench applications, VideoPlayback is also the applica­
tion that suffers the least from the low L2 cache utilization
(see Figure 5 (b» . Therefore, although the prefetcher can
reduce the L2 miss rate by 23.3%, this miss rate reduction
is not reflected in a similar degree of performance gain
for VideoPlayback. Overall, MobileBench applications benefit
significantly from hardware prefetchers. The simple prefetcher
increases MobileBench performance by an average of 14%.
5.10. MobileBench applications see performance gains from
using an exclusive cache hierarchy

Another important design issue regarding a multi-level
cache hierarchy is the property of cache inclusion versus

141

exclusion. In an inclusive cache hierarchy, data in the upper­
level caches (e.g., Ll caches) must also exist in all of its lower­
level caches (e.g., L2 caches). The advantage of having an
inclusive cache is that when a clean Ll cache line is evicted,
this line can simply be discarded. In contrast, for an exclusive
cache hierarchy where there is no duplicate in the hierarchy,
any line evicted from the Ll cache always incurs an exchange
of cache lines between the Ll and L2 caches. The Ll victim is
written back to the L2 cache and the requested line is inserted
in the Ll cache. While the procedure for handling Ll victims
is more complicated for an exclusive cache hierarchy than for
an inclusive one, the overall cache capacity using an exclusive
cache hierarchy is larger by the size of the Ll cache.
To compare the performance between the two different
cache hierarchies for MobileBench, we implement an ex­
clusive cache hierarchy using the same cache parameters
described in Table I. Across all applications, the L2 miss rate
is improved by 4.2% on average using the exclusive cache
hierarchy. Although smart phone applications benefit from the
larger effective cache capacity with the exclusive hierarchy, the
memory controller design for supporting the exclusive cache
hierarchy will become much more complicated as we move to
multicore chips with cache coherence.
6. RELATED WORK
The availability of representative smart phone benchmarks
enables the development of future smart phone processors.
Unfortunately, to date, there is a only limited number of
benchmarks for evaluating modern smart phone chip designs.
Existing smart phone benchmarks (e.g., The Embedded Micro­
processor Benchmark Consortium [13]) are either proprietary
or are not freely accessible. BBench [2], a recently released
benchmark, attempted to address the problem of the lack
of smart phone benchmark resources for researchers. With
BBench, we can now evaluate a web browser's performance
when it is rendering some of the most popular sites on the web.
In fact, it is the most related work to this paper. While BBench
provides access for evaluating a web browser's rendering
performance, its algorithm only focuses on the rendering
performance but does not take into account user browsing
patterns. In reality, users perform more heterogeneous brows­
ing patterns. To address this, we design RealisticGWB that
considers various additional movement patterns with random
delays. This simulates the browsing pattern where users spend
more time reading a specific section of a webpage and quickly
skim over the rest. In addition to the web browser based
benchmarks, MobileBench includes other workloads which
represent the commonly used smart phone applications. The
accessibility of MobileBench will enable mobile platform de­
signers to perform characterization and performance analysis
for a variety of applications.
In addition to benchmarks that model interactive user behav­
iors on smart phone platforms, there are several stressmarks
that allow smart phone users to evaluate the CPU or mem­
ory system performance, e.g., GeekBench2,Quadrant,or Vel­
lamo.These stressmarks typically assess the integer, floating­
point CPU performance, memory speed, or bandwidth per-

formance individually. However, the evaluation performed by
these stressmarks does not model realistic user behaviors on
smart phones and therefore does not directly translate to
user-visible performance. Similarly, several other stressmarks
aim to evaluate the graphics system performance of modern
mobile computing platforms, e.g., An3DBench, Basemark,or
CF-Bench.Again, these stressmarks do not model realistic user
behaviors and are used solely for the purpose of comparing the
graphics system performance between different smart phone
architectures.
7. C O N C L U S I O N
In summary, this paper presents detailed performance and
energy characterizations for MobileBench, a collection of
smart phone applications. MobileBench includes benchmarks
for realistic interactive web browsing, educational web brows­
ing, photo rendering, and video playback, representing a
diverse variety of applications commonly-run on mobile plat­
forms. With the performance and energy characterization, the
architectural insights provided by this paper, and the release
of MobileBench, we hope to inspire innovative designs that
lower power consumptions through simpler architecture while
maintaining high performance for smart phone devices.
ACKNOWLED GMENT

The authors would like to thank Lingjia Tang and the anony­
mous reviewers for their feedback. This work was partially
supported by the NSF IfUCRC Center for Embedded Systems,
and from NSF grant #0856090.
REFERENCES
[ I ] iSuppli, "PC Share of DRAM Market Dips Below 50 Percent for First
Time," http://goo.gVkrPRS .
[2] A. Gutierrez, R. G. Dreslinski, T . F . Wenisch, T . Mudge, A. Saidi,
C. Emmons, and N. Paver, "Full-System Analysis and Characterization
of Interactive Smartphone Applications," in Proc. of the International
Symposium on Workload Characterization, 20 1 1 .
[3] N . Binkert, B . Beckmann, G . Black, S . K. Reinhardt, A . Saidi, A . Basu,
J. Hestness, D . R. Hower, T. Krishna, S. Sardashti, R. Sen, K. Sewell,
M. Shoaib, N. Vaish, M. D. Hill, and D. A. Wood, "The gem5
Simulator," SIGARCH Comput. Archil. News, 201 1 .
[4] ARM, "Cortex A9 MPCore Technical Reference Manual," 2009 .
[5] S. McFarling, "Combining Branch Predictors," DigitaJ Western Research
Laboratory Technical Note TN-36, Tech. Rep . , June 1 99 3 .
[ 6 ] HLP Workshop o n Computer Architecture Competitions, www.jilp.orgl
jwac- lI.
[7] M. K. Qureshi, A. Jaleel, Y. N. Patl, S . C. Steely Jr., and J. Emer,
"Adaptive Insertion Policies for High Performance Caching," in Proc.
of the 35th International Symposium on Computer Architecture, 2007 .
[8] A. Jaleel, K. B. Theobald, S. C. Steely Jr., and J. Emer, "High Per­
formance Cache Replacement Using Re-Reference IntervaJ Prediction
(RRIP)," in Proc. of the 38th International Symposium on Computer
Architecture, 20 1 0 .
[ 9 ] c.-J. Wu, A. Jaleel, W. Hasenplaugh, M. Martonosi, S . C. Steely, Jr., and
J. Emer, "SHiP: Signature-Based Hit Predictor for High Performance
Caching," in Proc. of the 44th Annual International Symposium on
Microarchitecture, 201 1 .
[ I O] R . Murmuria, J . Medsger, A . Stavrou, and 1 . Voas, "Mobile application
and device power usage measurement," in Proc. of SERE, 2012.
[ I I ] G. M. Amdahl, "Validity of the Single Processor Approach to Achieving
Large ScaJe Computing Capabilities," in Proc. of the Spring Joint
Computer Conference, 1 967 .
[ 1 2] S. Bird, A. Phansalkar, L. K. John, A. Mericas, and R. Indukuru, "Per­
formance Characterization of SPEC CPU Benchmarks on Intel 's Core
Microarchitecture Based Processor," in Proc. of the SPEC Benchmark
Workshop, 2007 .
[ 1 3 ] The Embedded Microprocessor Benchmark Consortium, www.eembc.
org.

1 42

