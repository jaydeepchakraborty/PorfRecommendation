Investigating Human Factors in Image Forgery Detection
Parag Shridhar Chandakkar

Baoxin Li

Computer Science and Engineering
Arizona State University

Computer Science and Engineering
Arizona State University

pchandak@asu.edu

baoxin.li@asu.edu

ABSTRACT
In today’s age of internet and social media, one can find an
enormous volume of forged images on-line. These images
have been used in the past to convey falsified information
and achieve harmful intentions. The spread and the effect of
the social media only makes this problem more severe. While
creating forged images has become easier due to software
advancements, there is no automated algorithm which can
reliably detect forgery.
Image forgery detection can be seen as a subset of image understanding problem. Human performance is still the
gold-standard for these type of problems when compared to
existing state-of-art automated algorithms. We conduct a
subjective evaluation test with the aid of eye-tracker to investigate into human factors associated with this problem.
We compare the performance of an automated algorithm
and humans for forgery detection problem. We also develop
an algorithm which uses the data from the evaluation test to
predict the difficulty-level of an image1 . The experimental
results presented in this paper should facilitate development
of better algorithms in the future.

(a)

(b)

Figure 1: Examples of infamous tampered images.

1.

INTRODUCTION AND RELATED WORK

Forged images are in abundance in today’s age of social
media and internet. They can be used to spread false information through social media and thereby achieve harmful
intentions. They have been used in areas such as sports,
fashion, politics, professional photography etc. for different
motives. History of image forging dates back to 1800’s, then
mostly done for political reasons. We present two of the most
infamous cases of forgery to show the severity of the problem. First infamous incident occurred in 1950 when a forged
photo reportedly contributed to the electoral defeat of Senator Millard Tydings (right). The photo in Figure 1a shows
Millard Tydings having a conversation with Earl Browder
(left), who was a leader of American Communist Party. In
the second incident, Fig. 1b shows a photo of Osama Bin
Laden after his encounter with US forces on May 2nd , 2011.
Though the photo was reportedly published in many places,
it was later determined to be fake.
Image forgery can be categorized into two types: 1. Image
splicing 2. Image tampering. Splicing is the simplest form
of forgery where no post-processing is performed on the image. Tampering involves certain post-processing operations
such as blurring, resizing etc. They are performed on the
image to make it look as natural as possible. Most of the
images on web are tampered. Creating forged images has
become easier and detecting them is getting difficult due to
constant advancements in editing software. There exist two
common approaches to detect forgery, namely active and
passive. Watermarking is an example of an active approach.
It requires extra effort and most of the images on the web are
not watermarked. Passive approach determines the authenticity of an image by analysing the image itself. The forgery
detection process employed by humans is an example of a
passive approach.
Image forgery detection is a subset of image understanding
problems which include scene classification, object detection
etc. Human performance is still the gold-standard for most

Categories and Subject Descriptors
I.5.2 [Pattern Recognition]: Pattern analysis; I.4.9 [Image
Processing and Computer Vision]: Applications; I.4.m
[Image Processing and Computer Vision]: Miscellaneous

General Terms
Human Factors, Experimentation, Performance

Keywords
Image Forgery, Subjective Evaluation, Eye-tracking
1
The difficulty-level of an image here denotes how difficult
it is for humans to detect forgery in an image. Terms such
as “Easy/difficult image” will be used in the same context.

Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for profit or commercial advantage and that copies bear this notice
and the full citation on the first page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute
to lists, requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
HuEvent’14, November 7, 2014, Orlando, Florida, USA.
Copyright © 2014 ACM 978-1-4503-3120-3/14/11…$15.00.

http://dx.doi.org/10.1145/2660505.2660510 .

41

Human prediction performance

2.

Prediction Accuracy (in %)

of these problems. We investigate if the claim holds true for
forgery detection. We conduct a subjective evaluation test
with eye-tracking to quantify the human performance and
to understand the behavioural aspect of this problem. In
an attempt to relate human vision and the forgery detection
process, we examine the relation between the saliency of an
image and its effect on prediction performance. By using
the eye-tracking data and the performance statistics from
the evaluation test, we develop an algorithm to predict the
difficulty-level of an image. We also compare the human
performance against that of an automated algorithm. We
envision the development of better algorithms in the future
using these findings.
Recent years have seen an active research in this area.
Copy-and-move forgery (CMF) is one of the most common
methods of forgery in digital images. SURF-feature and
textural-descriptors were used to detect CMF [1,2]. Forgery
in JPEG images is detected by analysing the DCT coefficients as the forged image is most likely to be compressed
twice [5]. Another class of approaches uses high-level information in an image, such as, shadows [7], light environment [11] etc. Approaches solely depending on image
statistics are image-format independent and are more computationally complex. Hilbert-Huang transform and Markov
transition matrix of block DCT coefficients were proposed
in [4] and [12] respectively. We refer the reader to [3] and [9]
for an extensive review of forgery detection approaches.
This paper focuses on human performance evaluation of
forgery detection. To the best of our knowledge, this work is
first of its kind. Human performance evaluation studies have
increased performance of object detectors and annotation
predictors in [14]. Eye-tracking has also been used to study
the behavioral aspects of radiologist’s performance [8].
The rest of the paper is structured as follows. Section 2
describes the proposed approach. The results and discussions are presented in section 3. Section 4 concludes the
paper and lists potential future works.

1
0.8
0.6
0.4
0.2
0

Class 00

-0.2

Class 01

Class 10

Class 11 Authentic

Image Category
Image-level prediction
Region-level prediction

Figure 2: Human prediction performance grouped by category

presented with 50 images at a time. Subjects in third group
are presented with all 73 images. The images stay the same
within a group and those 50 images are sampled from 73
images. All the subjects sat at an approximate distance
of 25 inches from a 15.6 inch screen having resolution of
1920×1080. Natural lighting conditions are used for the test
and no time-limit is imposed to allow the subjects to scan the
image at their own pace. The distribution of authentic and
forged images is unknown to subjects. Before starting the
evaluation, five examples of each kind of images are shown
to each subject with the forged region being pointed out in
order to give them an idea about the type of forgery. They
are instructed to classify each image into one of two groups,
namely, authentic or forged and point out the forged region.

2.2

Analysis

The eye-tracker data and image-level statistics are used
to address several questions relating gaze and other factors
which may affect the difficulty-level of an image. The contribution of our analysis is three-fold. Firstly, we provide a
quantitative measure of human performance on our dataset.
We use the model of image saliency to predict the difficultylevel of an image. Third contribution aims at developing
an automated algorithm to predict the difficulty-level using
data from saliency model and gaze-points.

PROPOSED APPROACH

The aim of this paper is to examine human factors related to the forgery detection problem. The detection performance of many subjects on a set of forged images is recorded
to present a quantitative assessment of the human performance. We analyse the human gaze-points with the aid of
eye tracker to understand the behavioural aspect. It was
shown in [14] that there is a strong relation between the
human-level understanding of an image and the human gazepoints. The experimental set-up and protocol is designed to
establish a relationship between the pattern of human gazepoints and the difficulty-level of a forged image.

2.1

1.2

2.2.1

Relating image saliency with forgery

For every forged image in the database, we have their
authentic counterparts available for analysis. We propose to
model the change in saliency between the forged image and
its authentic counterpart. We compute saliency in spatial
[10] as well as frequency-domain [6]. Depending on the way
an image has been forged, we categorize an image into one
of four classes as follows:
• Class 01: The forged part is non-salient in the authentic image and becomes salient in the forged image.

Dataset and Experimental set-up

Class 00, 10 and 11 can be similarly described. We find
that change in saliency significantly affects the difficultylevel of an image. Fig. 2 shows the category-wise accuracy.
Quantitative results are given in section 3.

We present a quantitative measure of human performance
and study the relationships between the human gaze and
image content using images from two standard image forgery
datasets, CASIA v1.0 and v2.0 (from http://forensics.
idealtest.org). CASIA v1.0 is a splicing dataset whereas
its next version has tampered images. Our database has
73 images out of which 14 are spliced (taken from CASIA
v1.0), 44 are tampered (taken from CASIA v2.0) and 15 are
authentic images. The evaluation test is performed with 24
subjects over a span of one week. Subjects are divided into 3
groups of 9, 9 and 6. Each subject in the first two groups is

2.2.2

Relating gaze with forgery

Human gaze contains abundant information about the
task and human thought-process [13]. We analyse the information and come up with the following metrics to better
explain the human process of forgery detection.
1. For class 00 and 10, we compute the following metric:

42

39.74

759

70

4.5

24.92
1053

60
50
40

5

4

680
20.31

3.5

17.62

30

3

Class 00

Class 01
Class 10
Image Category

Mean Fixation Order

Class 11

3.1

Mean Fixation Timing

Figure 3: Effect of image category on fixations

Gaze-metric1 =

# fixations in forged region
# fixations in salient region

(1)

Salient region in an image can be detected by any standard saliency algorithm. We define Gaze-metric2 for
class 01 and 11. We count the number of fixations lying
elsewhere instead of in salient region since salient and
forged region intersect. This gives a measure about the
number of fixations used to detect forgery. Intuitively,
Gaze-metric1 (Gaze-metric2 ) should get higher values
for easy images. Higher values for both metrics imply
shorter fixation duration in the salient region (elsewhere) than in the forged region. Since human vision
is usually clustered in the salient part of an image, this
behaviour supports the claim of task-driven human vision [13].

3.2

Fixation analysis

Two gaze metrics proposed in the section 2.2.2 can be
successfully used to predict easy images. Values of both the
metrics are collected for all the images in respective classes
and are sorted. The values are binary thresholded at a point
where large change is observed and are grouped into 2 groups
per metric. Group 1 and 3 (2 and 4) contains metric values
above (below) threshold. The thresholds are determined to
be 4 and 2 respectively for two metrics. Accuracies of all the
images are also classified into 2 groups as per the obtained
thresholds. The mean accuracies of images (averaged over all
subjects) in the 4 groups are found to be 75%, 58.7%, 66.7%
and 47.8% respectively in accordance with our analysis.
Next, we analyse the effect of image category on fixation
duration and order. The data-labels plotted on top of each
data-point in Fig. 3 represent the maximum value. For example, 1410 denotes the maximum (over all subjects) number of fixations required to fixate a subject’s vision on forged
region in all the images in class 00. Similarly, 20.31 denotes
the maximum amount of time a subject fixated in forged
region. This plot shows that fixation duration and order are
both affected by the saliency. For class 01, subjects fixated
on the forged region earlier whereas most number of fixations were required in the class 00. It is interesting that in
spite of the contrast in the fixations, these two categories get
the top-2 prediction scores. In class 10, subjects fixated on
the forged region for the least amount of time. On the other

2. We analyse the effect of image category on duration of
fixations in the forged region. For each category, we
take average of the duration of fixations in the forged
regions. Note that we average over images in a particular category as well as over all the subjects. We also
record the order of fixations in the forged regions while
subjects scan the given image. The plot of mean order
of fixations versus image category is shown in Fig. 3.
The analysis of fixation statistics is given in section 3.
3. We study the effect of fixation duration over an entire
image on the prediction accuracy. We add-up duration
of fixations for all the images for each subject. Scanning an image for a longer duration can be related to
analysing image over multiple scales and regions. The
plot is shown in Fig. 4.
Finally, we use clustering to group the features from the
gaze-data and saliency model into two clusters. The images
in a cluster should correspond to the same category, i.e., easy
or difficult. Thus a large difference between the accuracies
of two clusters (averaged over all the subjects) is expected.

Analysis of fixation duration vs.
prediction accuracy
Fixation duration (in seconds)

3.

Saliency analysis

The Highest performance is obtained in class 01 as expected due to saliency of forged region. The lower performance associated with class 10 can be associated to the nonsaliency of the forged region after it undergoes editing. The
forgery operation in the 10 category usually involves removal
of the salient region in the authentic image and replacing it
with a non-salient texture, which is un-noticed by many subjects. However, lowest accuracy is achieved on the class 11,
indicating that saliency is not necessarily a factor which decides difficulty of a forged image. Images in the class 11
are skillfully forged by preserving the spatial and contextual
continuity after editing. This makes forgery detection difficult for superficial observers. Second-highest performance is
obtained on class 00 which further supports our claim about
saliency not being a deciding factor.

RESULTS AND DISCUSSIONS

In this section, we present the quantitative results and
discuss the findings for various metrics presented in section
2.2. Human prediction performance on authentic as well
as forged images is shown in Fig. 2. The green and red
bars represent prediction performance on an image-level for
authentic and forged images respectively. The orange bars
represent region-level prediction accuracy for forged images.

0.52

1410

80

The error bars represent the standard deviation in a particular category (calculated over all the subjects).

700
600

500
400
300

200

0.51
0.77
0.57
0.42
0.28
0.51
0.39
0.36
0.37
0.58
0.75
0.94
0.59
0.54
0.68
0.47
0.49
0.50
0.62
0.54
0.64
0.64
0.73

90

5.5

Mean duration of fixations in tampered region

Mean order of fixations in tampered region

Analysis of fixations vs. image category
100

100
0

1

3

5

7

9

11
13
15
17
19
21
Subject index
(In the order of increasing fixation duration)

Figure 4: Effect of fixation duration on prediction.

43

23

(a)

deciding factor. We use eye-tracker data and show the effect of image category on the duration of fixations and their
order. We successfully apply clustering on the generated
data to group the images into easy and difficult categories.
We compare the performance of automated algorithm and
humans to show that automated algorithm is better at detecting skilled forgery. In the future, we would like to closely
examine the dependency between underlying statistics in the
fixation pattern, image saliency and image forgery.
Acknowledgement: The work was supported in part
by a grant (1135616) from the National Science Foundation.
Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.

(b)

Figure 5: Fixation heatmap of (a) correct detection (b) missed
detection of forged images. Forged area is highlighted in yellow
(Best viewed in color).

5.

hand, they fixated for the most amount of time in class 11,
probably because the forged region also intersects with the
salient region. Yet, these two classes have two lowest prediction scores associated with them. Human fixations are thus
affected by the image saliency and the given task.
What would be the effect of fixating longer at an image?
Will it result in increased prediction accuracy? The plot in
Fig. 4 suggests otherwise. In the plot, data-label over each
data-point denotes the accuracy obtained by the subject.
The subjects are arranged in the increasing order of fixation
duration. The pattern suggests that fixation duration over
an entire image contributes little to the prediction process.

3.3

Comparison with automated algorithm

We implement a passive approach for image forgery detection proposed in [12]. On the subset of 50 images, best
human and computer performances are found to be 72%
and 62% respectively. For all 73 images, the same numbers
are 68.49% and 68.49% respectively. We also examine the
class-wise performance. Best Human performance on the 4
classes and on authentic set is 75%, 100%, 63.63%, 22.22%
and 60% respectively. The same set of numbers for computer performance is 75%, 78.57%, 45.45%, 77.78% and 60%
respectively. Thus humans are better at detecting unskilled
forgery but on the difficult images automated algorithm performs slightly better.

3.4

Prediction algorithm

We develop an automated algorithm to group the images
under two categories, namely, easy and difficult, by using
the data from saliency and gaze-analysis . We use K -means
where the feature vector contains the following quantities:
1. Class index 2. Gaze metrics 3. Order of fixation in forged
region 4. Duration of fixations in forged region.
After clustering the feature space into 2 groups, the accuracies for both the clusters obtained are 63.87% and 37.69%.
Note that the accuracies are averaged over all the images in
that cluster and over all the users. In spite of averaging
over all subjects and images, the collected features are able
to fairly predict the difficulty level of an image.

4.

REFERENCES

[1] E. Ardizzone, A. Bruno, and G. Mazzola. Copy-move
forgery detection via texture description. In
Proceedings of the 2nd ACM workshop on Multimedia
in forensics, security and intelligence, 2010.
[2] X. Bo, W. Junwen, L. Guangjie, and D. Yuewei.
Image copy-move forgery detection based on surf. In
Multimedia Information Networking and Security
(MINES), International Conference on, 2010.
[3] H. Farid. Image forgery detection. Signal Processing
Magazine, IEEE, 2009.
[4] D. Fu, Y. Q. Shi, and W. Su. Detection of image
splicing based on hilbert-huang transform and
moments of characteristic functions with wavelet
decomposition. In Digital Watermarking. Springer,
2006.
[5] J. He, Z. Lin, L. Wang, and X. Tang. Detecting
doctored jpeg images via dct coefficient analysis. In
ECCV. Springer, 2006.
[6] X. Hou, J. Harel, and C. Koch. Image signature:
Highlighting sparse salient regions. PAMI, IEEE
Transactions on, 2012.
[7] E. Kee, J. O’brien, and H. Farid. Exposing photo
manipulation with inconsistent shadows. ACM
Transactions on Graphics (TOG), 2013.
[8] E. A. Krupinski. Visual scanning patterns of
radiologists searching mammograms. Academic
radiology, 1996.
[9] B. Mahdian and S. Saic. A bibliography on blind
methods for identifying image forgery. Signal
Processing: Image Communication, 2010.
[10] R. Margolin, A. Tal, and L. Zelnik-Manor. What
makes a patch distinct? In CVPR, 2013 IEEE
Conference on, 2013.
[11] R. Ramamoorthi and P. Hanrahan. On the
relationship between radiance and irradiance:
determining the illumination from images of a convex
lambertian object. JOSA A, 2001.
[12] P. Sutthiwan, Y. Q. Shi, H. Zhao, T.-T. Ng, and
W. Su. Markovian rake transform for digital image
tampering detection. In Transactions on data hiding
and multimedia security VI. Springer, 2011.
[13] A. L. Yarbus, B. Haigh, and L. A. Rigss. Eye
movements and vision. Plenum press New York, 1967.
[14] K. Yun, Y. Peng, D. Samaras, G. J. Zelinsky, and
T. L. Berg. Studying relationships between human
gaze, description, and computer vision. In CVPR,
2013 IEEE Conference on. IEEE, 2013.

CONCLUSION AND FUTURE WORK

We investigate the human factors associated with the process of forgery detection by conducting a subjective evaluation test with the aid of eye-tracker. To analyze the effect of saliency on forgery detection, we group the images in
four classes and perform statistical analysis. We show that
though saliency affects the prediction, it is not necessarily a

44

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 19, NO. 4, APRIL 2009

533

Virtual View Specification and Synthesis
for Free Viewpoint Television
Wenfeng Li, Jin Zhou, Student Member, IEEE, Baoxin Li, Senior Member, IEEE,
and M. Ibrahim Sezan, Fellow, IEEE

Abstract— Free viewpoint television (FTV) is a new
concept that aims at giving viewers the flexibility to select a
novel viewpoint by employing multiple video streams as the input.
Current proposed solutions for FTV include those based on rayspace resampling which demand at least dozens of cameras and
large storage and transmission resources for those video streams.
Image-based rendering (IBR) methods that rely on dense depth
map estimation also face practical difficulties since accurate
depth map estimation remains a challenging problem. This paper
proposes a framework for FTV based on IBR that relieves the
need for an accurate depth map by introducing a hybrid virtual
view synthesis method. The framework also includes an intuitive
method for virtual view specification in uncalibrated views. We
present both simulation and real data experiments to validate
the proposed framework and the component algorithms.
Index Terms— 3-D TV, image-based rendering, view specification, view synthesis.

I. I NTRODUCTION

T

ELEVISION has been probably the most important visual
information system in the past decades, and it has indeed
become a commodity of modern human life. In conventional
TV, the viewer’s viewpoint is determined by that of the
acquisition camera. Recently, a new concept, which is called
free viewpoint television (FTV) [1], [2], was proposed, which
promises to bring a revolution to TV. The basic idea of FTV
is to provide the viewer the freedom of choosing his/her own
viewpoint by incorporating multiple video streams captured by
a set of cameras. In addition to home entertainment, the FTV
concept can also be applied to many other related domains,
such as gaming and education. Note that the user-chosen viewpoints do not need to coincide with those of the acquisition
cameras, and thus this is not a simple view change by switching cameras (as possible with some DVDs with a few preset
viewing angles). Apparently, FTV demands a wide spectrum
of efforts such as acquisition hardware, coding, bandwidth
management, and standardization, etc. In this paper, we focus
on one particular aspect of FTV, called virtual view synthesis,
including the related task of virtual view specification.

Manuscript received March 26, 2008; revised June 13, 2008 and September 11, 2008. First version published March 4, 2009; current version published
May 20, 2009. This paper was recommended by Associate Editor Xin Li.
W. Li, J. Zhou and B. Li are with the Department of Computer Science
and Engineering, Arizona State University, Tempe, AZ 85281, USA (e-mail:
baoxin.li@asu.edu).
M. I. Sezan is with Sharp Laboratories of America, Inc., Camas, WA 98607,
USA.
Digital Object Identifier 10.1109/TCSVT.2009.2014021

The problem of virtual view synthesis can be defined as
follows: given a set of images acquired from different viewpoints, construct a new image that appears to be acquired from
a new viewpoint. This problem is also known as image-based
rendering (IBR), which has been extensively studied in the
literature. For example, [3] and [4] are among the early papers,
and the more recent ones include [5]–[9]. While there exist
alternative approaches to virtual view synthesis in FTV (e.g.,
the ray-space sampling and interpolation approach in [2]),
some IBR methods have the potential of requiring fewer
cameras and loose constraints on the camera configuration
(both being desired features for the FTV application). Our
proposed approach belongs to this category.
For a realistic camera–scene configuration, the same physical point may have a very different color in the virtual view
than in any other given view, even without considering occlusion, depending on the viewing angles, the illumination and
reflection models, etc. Therefore, IBR approaches, while being
attractive, have a fundamental limitation: the virtual views
cannot be too far from the given views, otherwise unrealistic
color may ensue. This suggests that the cameras used in the
FTV application should be located strategically so that most
potentially interesting viewpoints should lie among the given
views. In addition, for the FTV application, the cameras are
those that were used in photographing the movie or TV program and thus it is impractical to assume the availability of full
camera calibration information. In other words, it is unrealistic
to expect that the movie/TV producer would painstakingly
calibrate the set of cameras and store the calibration data
whenever the cameras are moved during photographing.
With these considerations, this paper presents a complete
approach aimed at addressing the following practical tasks for
realistic FTV application: given a set of uncalibrated views
of the same scene, provide the viewer an intuitive way of
specifying a virtual viewpoint in relation to the given available
views and synthesize a virtual view using only the input views
without relying on accurate camera calibration information.
In Section II, we review some related work and briefly
discuss the difference between the proposed approach and the
existing work. In Section III, we first briefly present the basic
formulation of the problem and then describe our method for
intuitive specification of a virtual view in uncalibrated cameras. The proposed IBR-based synthesis algorithm is presented
in Section IV. Experimental results are given in Section V. We
conclude in Section VI with brief discussion on limitations of
the proposed approach and future research directions.

51-8215/$25.00 © 2009 IEEE

534

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 19, NO. 4, APRIL 2009

II. R ELATED W ORK
We first review a few relevant IBR approaches. Viewdependent texture mapping [10], which warps given images
to a virtual view, may provide a practical IBR solution for
FTV due to its fast rendering speed and the ability to retain
texture details. However, this method relies on pre-computed
geometry information, such as depth maps, whose accurate
estimation is a challenge. Even a small error in the depth map
may cause structural failure or “floaters” in the synthesized
image, to which human eyes are very sensitive. The inverse
methods [5], [11] fill each pixel in the virtual image without
explicit depth information. While such methods work well for
regions with little texture, they tend to lose texture details and
have to introduce high-order constraint [5]. Further, they are
typically computationally expensive due to the search over all
possible depths in synthesizing a pixel. It appears to be logical
to combine these two complementary methods, but this has not
drawn much attention. In this paper, we propose a hybrid approach to address this problem. While using any state-of-theart method to compute the depth map for a pair of images (to
be used in forward warping), we make the rendering algorithm
tolerant to inaccurate depth-map estimation by introducing a
probabilistic inverse method to complete the synthesis. As a
result, the quality of the synthesized image is more insensitive
to the parameter setting of any of the components than when
they are used separately. Further, the cost of the inverse method
is drastically reduced since it is based on the forward-warped
image.
A few papers have addressed the problem of virtual view
synthesis with uncalibrated views. In [12], the view is interpolated separately for the background and for the moving objects, which simplifies the correspondence problem but is not
generally applicable due to the need of a segmentation stage.
Pollefeys et al. [13] used a sequence of uncalibrated images to
reconstruct a 3-D structure and then project to new views. In
our approach, the projective reconstruction is limited to two
views that are close to the desired virtual view. Thus we do
not need to do bundle adjustment over multiple views, and potentially the focal lengths of the two cameras can be different.
Many existing IBR approaches either assume the availability
of camera calibration information (e.g., [6], [7]) or perform
camera calibration using the input data (e.g., [12], [13]). For
an application like FTV, it may be impossible to go back to
the scene to calibrate the cameras, and thus any calibration
has to be self-calibration, which generally utilizes costly optimization methods that often rely on a good initial guess of the
parameters and converge to only a local minimum. When there
are multiple views, the parameters that need to be optimized
increase drastically. If the cameras have different parameters,
it becomes more difficult to initialize these parameters. On the
other hand, weak calibration provides the epipolar constraint
on two views, which can be extended to three views and then
used for pixel mapping in view synthesis. This is the method
the proposed approach adopts.
There is a problem associated with using only weak calibration for virtual view synthesis: there is no natural method for
specifying the virtual view. The challenge of specifying virtual

viewpoints in uncalibrated setups is evidenced by the papers
that are dedicated to this issue. In [14] the new viewpoint
is specified by linearly interpolating two projection matrices.
Such simple interpolation is not valid for some geometry transforms such as rotations. Interpolating infinity homographies is
used in [15] for virtual view specification, where the infinity
homographies are obtained through an optimization stage. It
remains to be studied whether the optimization results (which
are in a sense equivalent to self-calibration), when applied to
view specification, can produce low enough projection error
(compared with relying on the matrix F). In addition, when the
motion between two cameras is pure translation, the infinity
homographies become identity matrices and thus the approach
cannot be applied (i.e., one may always get an identity matrix
through interpolation).
In this paper, we propose an intuitive method for specifying the virtual view in uncalibrated cameras. From weak
calibration of two views, two quasi-Euclidean camera models
are reconstructed based on a general camera model. View
interpolation is done by mapping the quasi-Euclidean reconstruction to a regularized projective reconstruction through a
skew matrix. As the point mapping always uses projective
reconstruction, in theory the method does not suffer from
reprojection error. In addition, the method is more flexible
in handling cameras with different focal lengths.
III. BASIC F ORMULATION AND V IRTUAL
V IEWPOINT S PECIFICATION
We assume that multiple synchronized video streams of the
same scene are captured by a set of uncalibrated cameras, and
therefore our focus will be on the frames having the same
timestamp, which are the multiple views to be considered. In
the following, to set the stage for further discussion, we first
introduce the basic multiview geometry used in this paper.
Then we outline the basic idea of the proposed method in
Section II-B. The method for specifying a virtual viewpoint
between two given views is described in Sections II-C and
II-D. The problem of possible degeneracy is discussed in
Section II-E, followed by an approach proposed in Section II-F
for automatically identifying two basis views close to a desired
virtual view, in uncalibrated cameras.
A. Multiview Geometry
For N given cameras used in acquisition, each can be represented with a projection matrix as
Pi = K i Ri I | − Ci

(1)

where R is a 3 × 3 rotation matrix and C is a 3 × 1 translation
vector. The camera intrinsic matrix K is an upper-triangular
matrix that has the form
⎡
⎤
f s px
K = ⎣ 0 rf p y ⎦
(2)
0 0 1
where f is the focal length, px , p y is the principal point which
is normally very close to the center of the image plane, s is
the skew, and r is the aspect ratio. For professional cameras

LI et al.: VIRTUAL VIEW SPECIFICATION AND SYNTHESIS FOR FREE VIEWPOINT TELEVISION

535

used in movie/TV production, we may assume that there is no
skew and the aspect ratio is 1.
The projection matrices project any 3-D point in the scene to
a set of corresponding points on the respective image planes by
xi ∼
= Pi X

xk

F1k

(3)

where both xi and X are homogenous coordinates. With
images taken from a scene, the process of recovering camera matrices and X is the well-known structure-from-motion
problem, which in general involves bundle adjustment [16] by
starting from some initial values and then iteratively adjusting
Pi and X to minimize the point reprojection error.
Estimating the camera matrices is difficult, as the parameters
are coupled in a complex form in the projection matrix. Weak
calibration, or computing the epipolar constraint, is much easier, as it does not extract camera parameters individually. The
epipolar constraint can be described as follows: given one
point xi on image i, its corresponding point x j on image j
must lie on the epipolar line Fij xi , written as
x Tj Fij xi = 0

Auxiliarv view k

Basis view 1

Basis view 2

F2k

x2

F12
x1

F10

Virtual view

F20

x0

Fig. 1. Illustration of the relation among the views through fundamental
matrices.

(4)

where Fij is the fundamental matrix between image i and j.
Weak calibration only requires a set of corresponding points
and the algorithm is efficient and robust to outliers.
Fundamental matrix can also be used to connect three views.
We denote two given views as “basis view 1” and “basis
view 2.” Given the third view k, if F1k and F2k can be
computed, then for any two corresponding points x1 and x2
the intersection of two epipolar lines F1k x1 and F2k x2 will
be the corresponding point xk . We then can use the relation
to incorporate more views, as illustrated in Fig. 1. For any
image point x1 , we only need to search along its epipolar line
on image two to find the corresponding point x2 , and then
transfer it to the virtual view to do the synthesis. To facilitate
the search, other views can be used as “auxiliary view” to
verify whether x1 and x2 are corresponding points. This is
done by checking the consistency of colors from pixels x1 , x2 ,
and xk ’s, which will be discussed further in the virtual view
synthesis part. In subsequent discussion, we call the two given
views that are closest to a virtual view the basis views, which
will be automatically determined. Being able to start with two
views that are assumed to be closer to the desired virtual
view than others contributes to the reduction in the number
of required views. These two closest views are used heavily
for synthesizing the new view in forward mapping while others
are used for improvement with the inverse method.
B. Basic Idea
To support relative viewpoint transition among uncalibrated
views, we have designed our system to continuously interpolate virtual views from one view to another, with interpolation
being controlled by two parameters α and β, which are used
to change the view point in the direction connecting the
two viewpoints and the perpendicular direction. We show in
Section II-F how to automatically select the second basis view
based on the current view and the arrow button pressed by the
user. This approach enables intuitive view specification and

Fig. 2.

Virtual view specification: A mockup illustrating the main idea.

control, e.g., using a typical TV remote. The basic idea is
illustrated in the mockup user interface in Fig. 2, where the
typical arrow buttons on the remote unit control the viewpoint
change and the resulting video is displayed immediately on the
screen as visual feedback to the viewer. Essentially, we provide
a viewer with the capability of varying a virtual viewpoint
gradually between two views that are automatically determined
based on the viewer’s action of pressing a directional button
on a TV remote.
C. Virtual Viewpoint Specification: Calibrated Case
Although our ultimate goal is to handle the uncalibrated
case, we first show how to specify virtual views during the
transition between two given views in the calibrated case, as
it is instructive. Suppose that we have two camera projection
matrices for the two given views
Pi = K i Ri I | −Ci , i = 1, 2.

(5)

Since we are only concerned with the relative relationship
between the two views, by applying the following homography

536

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 19, NO. 4, APRIL 2009

transform to each of the projection matrices we get


Pi

R1−1
0T

C1
where H =
1
canonical form as



= Pi H, i = 1, 2

given by
(6)

the projection matrices are converted to
Pi = K i Ri I | −Ci .

From (5) and (6), we have
R1 = I, R2 = R2 R1−1
C1 = 0, C2 = R1 (C2 − C1 )

(7)

i.e., the first camera’s center is now the origin, and camera
two is related to camera one by rotation R2 and translation C2 .
For the sake of simplicity in notation, we omit the apostrophe
hereafter for the canonical form of camera matrices.
Using zero as the index of the virtual view, based on the
canonical form, the camera matrix for the virtual view is
P0 = K 0 R0 I | − C0 .

(8)

We use α and β to parameterize the path between basis views
1 and 2. Then, (8) becomes
P0 (α) = K 0 (α)R0 (α)I | − C0 (α, β).

(9)

For the camera intrinsic matrix, the gradual change from
view one to view two may be viewed as camera one changing
its focal length and principal points gradually to those of
camera two (if the two cameras are identical, then this will
not have any effect, as desired). Therefore, we can interpolate
the intrinsic matrix and obtain K 0 (α) as
K 0 (α) = (1 − α)K 1 + α K 2 .

(10)

Now, we need to construct R0 (α) by interpolating R1
and R2 . Multiple methods exist for such interpolation in
virtual reality and 3-D gaming. The most frequently used are
the Euler angles formula [17], Rodrigues’ rotation [18], and
Quaternions [19]. We use the Euler angles formula by linearly
interpolating the rotation angles about the x, y, and z axes. The
advantage of using Euler angles is that it gives more intuitive
meaning on the geometric transform and we can easily relate
the parameters to camera tilting, panning, and yawing.
Finally, we construct the new camera center by
C0 (α, β) = (1 − α)C1 + αC2 + β(C2 − C1 )⊥

(11)

where (C2 − C1 )⊥ stands for the normal vector of C2 − C1 .
This equation moves the center point of the new camera first to
the interpolation point of two basis camera centers and then
shifts a small value in the perpendicular direction specified
by β. The parameter β provides the ability of extrapolation
beyond the line linking the two basis views. But to maintain
the quality of virtual view, we limit β to a small value (0.2 in
our experiments).
D. Virtual Viewpoint Specification: Uncalibrated Case
We now consider the uncalibrated case, i.e., how we can
achieve the same effect as in the previous section from the
fundamental matrix only. Given a fundamental matrix F12 ,
one way to obtain the two canonical camera matrices is ( [20])

P1 = I |0,

P2 = e2× F12 |λe2

(12)

where e2 is the epipole on image two which can be computed
T e = 0, and λ can be any nonzero scalar. With any two
with F12
2
matched points x1 and x2 , X can be obtained through triangulation [21] to meet the requirement of both (3) and (4). The
3-D structure obtained in this way is a projective reconstruction [22], which is determined only up to a projective transform with respect to the 3-D structure in the Euclidean space.
Thus the projective reconstruction is not unique. As shown in
(6), there is a family of projection matrices and 3-D points X
which conform to all constraints and can be related by a family
of nonsingular homography transform matrices {H }. As the
projective reconstruction in (12) comes from the fundamental
matrix, it inherits all advantages of using epipolar constraint: it
is fast to compute without using bundle-adjustment and there
is no need for initial guessing of unknown parameters; it is
accurate in terms of reprojection error (shown to be within
one pixel in the literature) and the algorithm is stable.
However, there are two practical problems of using projective reconstruction for interpolation. First, in the projective
reconstruction framework, the projection of a 3-D point on an
image is bounded only in the original two viewpoints but not in
a new one. Skew may occur in interpolated views. It is usually
small when the baseline is very small but there is no guarantee.
An extreme case is, given the Euclidean reconstruction in (5),
where one can replace the rotation matrix by another one
which is composed of complementary angles to the original
one. Interpolating such projection matrices will generate an
effect of rotating the camera in the opposite direction. In [14],
the views are rectified to minimize the skew, but not all views
can be rectified without large distortion. Another problem is
that interpolating the projection matrices lacks any geometric
meaning and thus is not intuitive to use.
In the following, we show how to solve the problems by regularizing the projective reconstruction with a quasi-Euclidean
reconstruction and generating a geometrically intuitive interpolation.
We first estimate the essential matrix, which has the form
E 12 = K 2T F12 K 1 .

(13)

As the camera intrinsic matrices K 1 and K 2 are unknown, we
approximate them by setting the skew to be zero, the principal
point to the center of the image, and the aspect ratio to 1. These
are very good approximations to the real values especially for
high-quality cameras. Now the only undetermined parameters
are the focal lengths of the two cameras. We set the focal
length based on the image width w and height h as
f = (w + h)/2.

(14)

This is from the observation that most digital cameras with
normal lenses have focal length 0.8–3 times the image
dimension. And we will demonstrate with experiments that the
final results (the quality of the rendered video) are relatively
insensitive to inaccuracy in such an approximation.
From [20], camera motion can be obtained from an essential
matrix by the following steps: Decompose E 12 by singular

LI et al.: VIRTUAL VIEW SPECIFICATION AND SYNTHESIS FOR FREE VIEWPOINT TELEVISION

537

where we write e2 = K 2−1 e2 as the new epipole after point
normalization. Following standard method [24], this is done by

value decomposition (SVD) as
E 12 = USV

(15)
M = V1V |V2V |V3V |V4V

where U and V are orthonormal matrices, and S is a diagonal
matrix. Then, we have
t × = UZU T
with

⎡

0
Z = ⎣−1
0

R = UWV T or R = UW T V T

1
0
0

⎤
⎡
0
0
0 ⎦ W = ⎣1
0
0

−1
0
0

⎤
0
0⎦
1
(16)

where R is exactly the rotation matrix. t can be obtained from
the last column of U . (See [20] for proof.) The projection
matrices in the canonical form can be written as
P1 = I | 0,

P2 = K 2 R K 1−1 | t.

(17)

There are multiple folds of ambiguities in (16) and the valid
solution is obtained by imposing the constraint that most of
the reconstructed 3-D points are in front of both cameras.
We call (17) the quasi-Euclidean reconstruction, as approximate camera matrices are used. The reprojection error may
be large with this reconstruction, as shown in our experiments
later, up to several pixels. Any methods trying to use (17) have
to minimize the reprojection error and that process amounts
to self-calibration, whose disadvantages have been discussed
previously.
To avoid the optimization procedure, we now show how to
use the projective reconstruction by reshaping it to approximate the quasi-Euclidean reconstruction like the technique
used in [23]. We first use the approximate camera intrinsic
matrices to normalize all corresponding points x1 and x2 by
x1 = K 1−1 x1 , x2 = K 2−1 x2 .

(18)

Then the quasi-Euclidean reconstruction is noted as
P1E = I |0,

P2E = R|K 2−1 t.

(19)

From [20], the family of projective reconstruction can be
written as a four-parameter family of camera pairs as
P1P = I |0,

P2P = e2× F12 + e2 v T |λe2 .

= I |0,
= K 2−1 e2× F12 K 1 + K 2−1 e2 v T |λK 2−1 e2

(21)

where v can be any 3 × 1 vector and λ is a nonzero scalar.
The goal is to find a set of values for v and λ so that
P2E ≈ P2P . Comparing (21) with (20), it can be proved that
K 2−1 t = μK 2−1 e2 , so λ is determined. To best approximate the
rotation matrix R, we make orthogonal projection of R onto
the space spanned by
V1 = K 2−1 e2× F12 K 1 , V2 = e2 1 0 0
V3 = e2 0 1 0, V4 = e2 0 0 1

(22)

(23)

where A V stands for the vector form of a matrix A. Now R p
only approximates R in the vector space but may vary drastically in the norm. This will raise a problem in viewpoint
interpolation since the one with a large norm may dominate
the interpolated result. To overcome this, the two matrices
can be normalized based on their three largest elements which
are the diagonal elements, given a rotation matrix with small
angles. Thus we do
R p = R p (trace(R))/(trace(R p )).

(24)

Now the regularized projective reconstruction is
P1P R = I |0,

P2P R = R p |λe2 .

(25)

Interpolating I and R p is still not easy, as R p is not a perfect
rotation matrix. Trying to adjust it into a rotation matrix using
approximation will increase the reprojection error. Per-element
linear interpolation as in [14] is problematic for rotation
matrices. We solve the problem by estimating the homography transform from the quasi-Euclidean reconstruction to
the regularized projective reconstruction, which we call the
skew matrix.
From PiPR = PiE Hs it follows that
	

I 0
Hs = T
(26)
d
c
and e2 c T = R p − R, d = μ/λ. Using the fact that (e2 )T e2 = 1,
c T can be obtained as
c T = (e2 )T (R p − R).

(27)

As the skew matrix is a mapping from the quasi-Euclidean
space to the projective space for any projection matrix, we can
obtain an interpolated projection matrix by using the method
from Section II-C in the quasi-Euclidean space, then by performing the homography transform to find its corresponding
projective reconstruction with

(20)

After point normalization, it becomes
P1P
P2P

R Vp = (M T M)−1 M T R V

P0PR = P0E Hs .

(28)

This approach has proven to be effective with multiple sets
of data that we have experimented with, as will be illustrated
in Section V, even when the approximation in (14) is used
without accurate knowledge of camera internal matrices.
E. Degeneracies
Degeneracies can occur in multiple cases when using the
fundamental matrix. The cases when the fundamental matrix
cannot be uniquely determined, such as when all feature points
are on a plane or there is no translation between two cameras,
are deemed as very rare in the FTV application, considering
that the multiple cameras are capturing a regular scene at the
same time. Thus we will not consider these cases.

538

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 19, NO. 4, APRIL 2009

The degeneracy that affects our algorithm is in the following
case. When accessing points in an auxiliary view, the intersection of two epipolar lines may not be uniquely or accurately
determined if the lines are nearly parallel to each other.
Fortunately, as our virtual view synthesis does not critically
depend on any auxiliary view, we can drop those pixels that
come from two epipolar lines that have intersecting angles less
than a certain threshold. In our experiment, we always pick the
three pixels that have large angle for the intersecting epipolar
lines. It is worth pointing out that a trifocal tensor [25], [26]
can be used to avoid the degeneracy problem.
F. Camera Position Autodetection
The view specification method described thus far assumes
two basis views. Since at any point in time the viewer sees only
one view on the TV screen, when she presses an arrow button,
the algorithm can use the current displayed view as the first
basis view but it needs to find the second basis view from the
given views. That is, we need to have a method of determining
relative positions of the uncalibrated views. Recording the
camera positions could be done with special hardware attached
to the video cameras. Here, we propose a fully automatic
method to solve the problem based on only the given video
streams. We show that when the focal length is the only
varying parameter, an estimated camera matrix can determine
the relative position of two cameras without ambiguity.
Recall that in (15), we find the SVD of the essential matrix
E. Calculating the SVD of E involves finding eigenvalues and
eigenvectors of EE T . The translation vector t corresponds to
the smallest eigenvalue of EE T ,which is 0, so
EE T t = 0.

E  has the form
⎤
⎦.
1

(31)

Since E  does not change the rank of E, and rank (E) =
rank(F) = 2, E  E T also has zero as its smallest eigenvalue
which relates to an eigenvector t  . This will be the translation
vector for the estimated camera relative position, i.e.,
E  E T t  = 0.
From the above, it is easy to prove that
⎤
⎡
1/a
1⎣ 1

⎦t
1/b1
t =
c
1

Given a set of cameras V
1. Remove two cameras A and B from V . Determine their
relative position as
dir ∈ {RIGHT , LEFT , UP, DOWN}
and link A and B with dir. Let current = prev = B
2. If V is empty, go to 5, else remove a camera C from V
3. Compute C with current, get relative position dir
4. Move on dir of current camera, get next camera,
if next = prev or next = NULL
insert C between current and next
prev = current
current = C
else
Go to step 2.
5. End
With relative positions of all the views determined, the
system can automatically select the second basis view for
subsequent view interpolation.

(29)

An estimated camera matrix, which is denoted as K  , is related
to the real camera matrix by
⎡
⎤
a
⎦K
b
(30)
K = ⎣
1
and an approximation of the essential matrix
⎡
⎤ ⎡
a1
a2
⎦E⎣
b1
b2
E = ⎣
1

to estimate the essential matrix. We can also conclude that the
value of the estimated t is not affected by the intrinsic matrix
of the second camera. Therefore, we can use this method to
estimate the relative position of any pair of cameras and, in
turn, to determine the relative positions of all the uncalibrated
views. In particular, we can determine the relative left–right
or up–down relationships between the views, which are then
used in conjunction with the arrow buttons in specification of
a virtual viewpoint. In our implementation, a link list is used
to manage all camera positions. The link list is constructed by
the following algorithm.

(32)

(33)

where c is a positive normalization factor such that ||t  || = 1.
The conclusion is that the sign of each element in the
translation vector t is not affected by the matrix K that is used

IV. IBR FOR FTV: T HE P ROPOSED A PPROACH
As stated previously, we have multiple views, with two that
are the closest to the user’s desired virtual viewpoint. While the
notion of closeness is not well defined in uncalibrated views,
in our problem statement the closest views are determined on
the basis of the virtual viewpoint specified by the user. Note
that the basis views will in general dynamically change based
on user’s choice of the virtual viewpoint and it is thus different
from the terminology used in [8] where the basis views (or
basis images) are defined by two specially positioned cameras.
Our proposed approach to IBR consists of the following
major steps:
1. pairwise weak calibration of all views to support any
virtual viewpoint selected by the viewer;
2. color-segmentation-based correspondence between the
two basis views, where other views are taken into consideration;
3. forward warping from the basis views to the virtual view
with verified disparity map;
4. backward search on all auxiliary views to find a dominant
and disparity consistent color for unfilled pixels;
5. probabilistic image completion with multiple views.
These steps and their interaction are summarized in Fig. 3.
We detail these steps in the following.

LI et al.: VIRTUAL VIEW SPECIFICATION AND SYNTHESIS FOR FREE VIEWPOINT TELEVISION

539

Video Streams

Data Preprocessing
Camera Estimation and Selection
Disparity Map Generation

(a)

(b)

(c)

(d)

(e)

(f)

Video Rendering
Forward Warping
Control
Backward Search and Propagation
Image completion with Multiple Views

Fig. 3.

Viewer
Display

Flowchart of the proposed system.

A. Segmentation-Based Correspondence
In the two basis views, to establish the correspondence
between x and x  , we first use graph-cut-based segmentation [26] to segment each of the basis views. For all segments,
we assume that pixels within the same segment have the
same disparity. It is obvious that over-segmentation is favored
for more accurate modeling. In a way similar to [6], we limit
each segment to be no wider and taller than 15 pixels, which
is a reasonable value for a traditional NTSC TV frame with
resolution of 720 × 480 pixels.
Each segment is warped to another image by the epipolar
constraint described earlier (also in Fig. 1). Instead of using
the common sum of squared difference, or sum of absolute
difference criteria as matching scores, we count the number
of corresponding pixel pairs whose relative absolute-value
difference is less than 0.2 (i.e., |I1 − I2 |/(I1 + ε) < 0.2, where
ε is a small value to prevent the denominator being zero).
The number of corresponding pixels satisfying this criterion,
normalized by the number of pixels in the segment, is used as
the matching score and is denoted as m ijk (d) for any possible d
and for the jth segment in the basis image i matching on kth
basis or auxiliary images. In our experiments, this measure was
found to be most robust to changes in lighting conditions.
In addition to using the matching score from the two basis
images only, we can incorporate all the auxiliary images and
thus compute a final matching score for the jth segment in
the basis image i with disparity d as
m ij (d) = max{m ijk (d)}.
k

(34)

Note that the search in other auxiliary views is aimed at finding
the d that will give rise to the largest color consistency among
the views that are related as in Fig. 1.
Furthermore, instead of picking the maximum from the
matching score vector, which is very noisy, we now update
that score in the following iterative optimization procedure. It
is similar to the cooperative algorithm [28]. Basically, the idea
is to smooth the matching score of each color segment based
on its neighboring segments of similar color and neighboring
disparities in order to enforce disparity smoothness.

Fig. 4. Color segmentation and disparity maps of the monkey scene and the
snoopy scene. (a–b) Original images. (c–d) Color-based segmentation results
shown as pseudo colors. (e–f) Computed disparity maps.

sij0 (d) = m ij (d)




rijt (d) =

sijt (d)

φ d0 ∈d−,d+

⎛

sijt+1 (d) = sij0 (d) ⎝rijt (d)






(35)
⎞γ
rijt (d)⎠

d∈dmin ,dmax

where φ is the set of neighboring segments with similar color
(with a Euclidian color distance smaller that a predetermined
threshold, which is set to 150 in our experiments),  is the
disparity smoothness window (set to two), γ is the inhibition
constant (set to two for computational simplicity) controlling
the convergence speed, and t the iteration index. For the
stopping criteria, we use the following: At any iteration t, if
for any d, sij exceeds the threshold 0.8, the updating process
for this segment will stop at the next iteration and the entire
procedure will terminate until all segments converge (i.e., no
segments need to be updated). Our experiments show that the
algorithm converges typically after 10 iterations and we thus
fix the number of iterations to 10.
If each segment is viewed as a node in a Markov random field (MRF), many other algorithms can be used for
optimization, such as graph-cut and belief propagation [29].
However, as we remove those outliers from the depth map, the
improvement on the synthesized image from using different
depth map estimation methods turns out to be marginal. Thus
we use the modified cooperative algorithm, as it does not need
sophisticated parameter tuning for attempting to obtain perfect
depth maps.

540

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 19, NO. 4, APRIL 2009

(a)

(b)

Fig. 5. (a) Sample virtual view from forward warping and (b) the entire
process of Fig. 3.

The optimization procedure is performed for both basis
views, and the disparity map is further verified by a left–
right consistency check [30], and only those segments with
consistent results are used for synthesizing the virtual view
(thus some segments may not be used, resulting in “incomplete
disparity map”). In Fig. 4, we show two examples of the colorsegmentation results together with the resultant disparity map.
B. Forward Warping
Using the verified disparity map and the two basis views,
an initial estimate of the virtual view can be synthesized by
forward warping. For a pixel x in basis view one and x  in
basis view two, the color of their corresponding pixel x" in
the virtual view is computed using simple linear interpolation
I (x") = (1 − α)I (x) + α I (x  )

(36)

with 0 ≤ α ≤ 1 controlling the contribution of the basis views,
which is set to the same α as is defined in Section III. The
notation in (36) implies that the R, G, and B color values of
the resulting pixel are computed in the same fashion as the
linear combination of respective values of x and x  .
Forward warping preserves well the texture details and
can be easily implemented in hardware, making real-time
rendering possible. Fig. 5(a) shows an intermediate image
obtained after forward warping.
C. Backward Search and Propagation
In the initial virtual view resulting from forward warping, it
is not uncommon to see many uncovered pixels, called “black
holes,” due to an incomplete disparity map. Among others,
occlusion is one reason for the problem. For each black-hole
pixel, we check its neighbor for a pixel that has been assigned
a color value from the initial synthesis. The disparity of that
pixel is then used for backward search on two basis views.
Unlike other similar disparity search algorithms that do an
exhaustive search on the whole disparity space, we search only
a small range within the disparity of the “valid” neighbors
(those with assigned colors). The search objective function is
defined as
 


2
λ
Dc ( pdn , pi ) + (1 − λ)Dd (dn , d)
F(d) =
min
d∈dn −,dn +

i=1

(37)
where dn is the disparity of a valid neighbor pixel and pdn
is its color, pi are colors from two basis views corresponding

to d, Dd and Dc are two distance functions defined on disparity and color, and λ is a weight coefficient. The combination
of the differences of color and the disparity is intended for
the smoothness of both texture (color) and depth. In reality,
F(d) is set to the minimum value obtained from all the valid
neighbor pixels. A new disparity will be accepted only when
the resulting F(d) is below a predetermined value. If the
search fails after all possible d values are tested on all valid
neighbors, the corresponding pixel is left empty until one of
its invalid neighbors is filled and a new d is propagated and
tested again. Otherwise, it is assigned a color value that is
based on the blending method of (36) and is labeled as valid.
A new search then continues for other black-hole pixels. In our
experiments, this algorithm proved to be fast and preserved
color smoothness and texture details.
D. Probabilistic Image Completion with Multiple Views
Even after the backward search and propagation processes,
there may still be “black holes” left when the points cannot
be seen by either of the basis cameras or cannot be reliably
determined from the two basis views. To address this, we
use a probabilistic method to complete the image, using, as
before, all the available views. The problem can be described
as follows: Given N 2-D input images I1 , . . . , I N , find the
most likely view from a virtual viewpoint. Let x be a 2-vector
[u, v], which is the coordinate of a pixel in an image. Ii (x)
represents the color of the pixel located at x in the ith image,
which is a 3-vector in the RGB color space. I0 is the image
we want to complete. Let d be the configuration of disparity
for each x in I1 , which is unknown. We can maximize a
posteriori probability density of I0 and d, given the observed
data I1 , . . . , I N , which is
p(I0 , d|I1 , . . . , I N ) =

p(I1 , . . . , I N |I0 , d) · p(I0 , d)
. (38)
p(I1 , . . . , I N )

There are many approaches to this optimization problem. In
our work, we consider only the likelihood part of the equation.
Other methods involving the prior components can be found
in [5] and [31]. As in the case of two views, a mean value of
pixels of all views with the minimum variance is the maximum
likelihood estimation under the Gaussian distribution
assumption. However, this simple model does not consider
the occlusion problem, which is the key culprit causing the
black holes. When outliers due to occlusion are present, a
mean value cannot represent the actual color, especially when
the number of input views is not large enough (which is the
case in our application). We illustrate this with the following
simulation based on Persistence of Vision Raytracer (POVRay) [32]. A scene is simulated as in Fig. 6(a); 100 images are
obtained by projecting the scene with different camera settings
as if 100 cameras are used to capture the image. In the scene,
one point on the red wooden ball (marked with a yellow
circle) is not visible in all cameras. It is occluded by the cone
in front of it at some viewpoints, which we call outliers as
they do not agree with others. Fig. 7 plots the R histogram of
the pixel under consideration. Apparently, a simple Gaussian
distribution is not a good model in this case, although we do

LI et al.: VIRTUAL VIEW SPECIFICATION AND SYNTHESIS FOR FREE VIEWPOINT TELEVISION

541

(a)
(a)

(b)

(c)

Fig. 8. Synthesized view using 13 input images: a close look at different
methods: (a) simple Gaussian, (b) clustering, and (c) the proposed method.

(b)

(c)
outliers

(d)
Fig. 6. Illustrating the occlusion problem. (a) A simulated scene. (b), (c) Two
sample views of the scene. (d) Colors of corresponding points from different
views. The outliers do not correspond to the same scene point as the other
pixels do.

that all pixels can be covered by the above procedure. For
example, the view to be synthesized could be covered by an
insufficient number of cameras so that no reliable color can
be obtained from given data. A simple linear interpolation or
extrapolation can handle this situation. It can also be alleviated
by constraining the free viewpoint range, which is already part
of our assumption (i.e., the virtual view is always between two
views, and that the cameras are strategically positioned).
This final step completes the entire synthesis algorithm of
Fig. 3. Fig. 5(b) illustrates a sample virtual view from all the
steps of the algorithm [comparing to Fig. 5(a)].
V. E XPERIMENTS AND E VALUATIONS

Fig. 7.

Histogram of the chosen point in Fig. 6 (R component only).

see a Gaussian component as indicated by dotted lines. Based
on the observation from Fig. 7, we propose a mixture model
with a Gaussian component and a uniform component. As
the portion of outliers and Gaussian distribution is unknown,
an EM algorithm is used to iteratively estimate all parameters
as we have reported in our previous paper [33].
Note that the above mixture-model based approach can be
applied to estimate the entire virtual view, as in [33]. Fig. 8
illustrates such an example. In our current study, this step
is used only for filling the final black holes and thus the
computational burden of the method is not an issue. Some
quantitative evaluation results of this step are presented in
Section V. It should also be noted that there is no guarantee

In this section, we present experimental results to evaluate
the proposed system. We first present experiments validating
some of the key components of the proposed approach and
then demonstrate the performance of the overall system using
a set of test sequences.
First, we verify our viewpoint specification method by
showing a simulated viewpoint moving path using the data
from [5]. In this case, the cameras are calibrated and the
associated path has Euclidian meaning in the real world. Fig. 9
shows two paths, with the viewpoint moving from camera 67
to 74 over a parabola and continuing to camera 80 following
a piecewise linear curve. These effects of different camera
motion curves are achieved by using a combination of the
viewpoint control parameters as proposed in Section III-C.
To evaluate the proposed virtual view specification with
uncalibrated views, we captured three sequences of images
by randomly moving the camera to different viewpoints for
the same scene. The first two sequences are called “snoopy”
and “kitchen,” in which we use the same focal length but
turn on the autofocus function; thus the focal length of each
view is almost the same with very small variances. The third
sequence has pairs of images with significantly different focal
lengths, called “kitchen mix.” Each sequence has 7–10 images,
therefore 21 to 45 pairs. We use the SIFT [34] algorithm to find
corresponding feature points and the RANSAC algorithm from
OpenCV [35] to compute the fundamental matrices. These are
all publicly available routines and the parameters are set as
default. The reprojection error of epipolar constraint is defined
as the distance of a feature point to its corresponding eipolar
line. The reprojection error of 3-D reconstruction is defined
as the distance of a reprojected point to its corresponding

542

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 19, NO. 4, APRIL 2009

z

51

67

50

74

49
6.5
20

80
7

(b)

(a)

22
24

7.5
26
8

y
Fig. 9.

28
8.5

30

x

Samples of simulated virtual viewpoint moving path.
TABLE I
C OMPARISON OF R EPROJECTION E RRORS

XXX
X

Sequence
XXX
Error
XXX
(min/max/mean)

Snoopy

Kitchen

Kitchen mix

Epipolar
Constraint

0.24/0.39/0.29 0.20/0.45/0.29 0.24/0.47/0.32

Quasi-Euclidean
Reconstruction

0.24/3.15/1.28 0.16/2.09/0.83 0.45/4.96/3.02

Regularized Projective
Reconstruction

0.15/0.45/0.24 0.09/0.49/0.20 0.13/0.72/0.29

feature point. The mean error of all corresponding feature
points is computed for each image pair, and the minimum,
maximum, and mean values for epipolar constraint, quasiEuclidean reconstruction, projective reconstruction, and regularized projective reconstruction are recorded in Table I.
The results in the table show that the proposed regularized
projective reconstruction has similar reprojection errors as the
epipolar constraint. It could be easily explained as due to the
fact the projective reconstruction is based on the fundamental
matrix. The reprojection errors for quasi-Euclidean reconstruction are larger than those from projective reconstruction since
it uses inaccurate camera matrices. Interestingly, the minimum
error of quasi-Euclidean reconstruction is also very small. It is
for this reason that quasi-Euclidean reconstruction can be used
for virtual view synthesis sometimes, but it also shows that it is
not reliable as the maximum error may be up to several pixels.
It also implies that accurately recovering camera parameters
is difficult, as even when wrong parameters are used in the
reconstruction, the error can be so small that an optimization
algorithm will easily get stuck at a local minimum. The results
with mixed focal lengths show that when the camera focal
lengths are the same, the error can be small, but when the
cameras have much different focal lengths, it is hard for
quasi-Euclidean reconstruction to get accurate results, while
the proposed approach maintains stably the same level of
reprojection error. This provides a comparison between our
approach and those approaches that rely on self-calibration.
In Fig. 10, we show the motion of each feature point when
virtually interpolating two views. By using the 3-D reconstruction, the 3-D points are reprojected to the image plane.

(c)
Fig. 10. Motion of corresponding points in the interpolated views using the
proposed approach. Two basis views are marked with corresponding feature
points on the top row.

Ideally, they should move from the locations of the set of
feature points in basis view 1 to those in basis view 2. From the
figure, the results of regularized projective reconstruction are
accurate in terms of reprojection error and without distortion.
In the FTV application, the cameras should not use such
drastically different focal lengths as in this experiment, since
the quality of synthesized image cannot be easily maintained
(e.g., purely due to the different spatial sampling rate of
the two views). But this experiment is intended to show the
generality of our proposed model. In this experiment, although
the camera parameters are unknown, we use the same general
camera model. The viewpoint changing effect can still be
accomplished. This is because the change in focal length has
a similar effect on the projected image as the camera moving
forward or backward. From (33), the distortion between our
general camera model and the real camera model will be
carried to the camera translation vector, whose effect has been
analyzed in (33).
To further test the robustness of the general camera model,
we use synthetic data to run the uncalibrated view specification
and synthesis scheme with various configurations. In Fig. 15,
two basis views with focal length f = 600 are used. The
ground truth of depth map is known, and the virtual view
is synthesized by warping pixels from two basis views with
the depth maps. The experiment shows that the focal length
used in the general camera model can be in a wide range
without causing distortion. As shown in the figure, f = 500
and f = 2500 produced similar results without noticeable

LI et al.: VIRTUAL VIEW SPECIFICATION AND SYNTHESIS FOR FREE VIEWPOINT TELEVISION

543

27
26

PSNR

25
24
23
Simple Gaussian
Clustering
Mixture Model

22

Y
21

Z

5

10
15
Number of input images

20

X
Fig. 12.

(a)

Y
Z

Comparison of three probabilistic IBR methods.

TABLE II
R ESULTS OF L EAVE -O NE -O UT T EST W ITH P LANT AND T OY DATA
Image

1

2

3

4

5

6

7

8

PSNR

22.9

26.9

28.1

27.8

27.6

27.7

27.2

24.9

X

(b)
Fig. 11. Relative camera position estimation from uncalibrated views: (a) The
tiger sequence. (b) The house sequence.

distortion; while with f = 250, the 3-D reconstruction is
so distorted from the reality that some points have wrong
depth and occlude other points that should be in front of
them. With such experiments we found that this occurs when
the focal length is chosen as small as half of the image
width. Focal length in this range would correspond to a very
wide-angle lens, which should always be used with caution
in most IBR applications due to many other reasons such as
radial distortion and violation of the pin-hole camera model.
In conclusion, we believe that for typical cameras used in
movie production, the deviation of the approximated focal
length from the true value will have negligible impact on the
quality of the synthesized video.
We also tested our approach for automatic estimation of
relative camera positions as follows: two sets of frames were
captured with a handheld consumer video camera and recorded
the relative positions of the frames as the ground truth. The
frames were then used as the input to the algorithm described
in Section III-D. Fig. 11 shows the results of automatic reordering of input images. The cameras (represented by the
image taken from that camera) are plotted in their pseudo 3-D
positions, which are obtained by stitching the pairwise relative
positions. These were found to be exactly the same as the
ground truth in terms of the relative left–right and up–down
positions of the views.
Using the simulation experiments of Fig. 6 (which provides
us with access to actual views even from a virtual viewpoint),

Fig. 13. One sample of the synthesized image using the mixture-model-based
approach.

we show the performance gain with the proposed mixturemodel-based approach over both a simple clustering-based
estimation and a Gaussian model-based estimation. For each
method, different numbers of input images are used for the
estimation. The input images are selected randomly from the
total 100 images, and each method is run 30 times to get
a mean value of the PSNR. Since for most parts of the
synthesized image the three methods provide almost the same
results, we restricted PSNR computation to the sub-image
of Fig. 8 for better comparison. Sample results from such
a comparison are given in Fig. 12, which shows that the
proposed method always outperforms the other two methods.
Note that the PSNR does not always increase with the number
of input images (although the trend is obvious), as the input
images are randomly chosen and the virtual view depends
on the similarity of the viewpoints as well as the number of
reference views.

544

Fig. 14.

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 19, NO. 4, APRIL 2009

(a)

(e)

(b)

(f)

(c)

(g)

(d)

(h)

Sample results from the Monkey and Snoopy sequences.

To illustrate the performance of the mixture-model-based
approach on real data, we also tested it on the plant and toy
images from [36]. Eight input images are used. The leaveone-out test is done with each one of the images while using
the rest as reference views. The PSNR of each synthesized
images is listed in Table II. Fig. 13 shows one synthesized
image. To our knowledge, direct comparison of IBR works is
still difficult as there are many parameters in each approach
in the literature and we are not aware of any group that has
made their system freely available for such evaluation. Hence
we report the PSNR value to provide a more objective metric
for a reader who is interested in comparing this with another
approach.
As examples of final results of the proposed view synthesis
approach, we present results obtained from the following
two data sets: the “monkey scene” from [5] and the “snoopy

scene” captured by a handheld camera in our lab. For the
monkey scene, there are 89 views in this data set and we
picked views 67 and 74, which have a proper distance, as the
basis views. We randomly picked six other views as auxiliary
views. One hundred virtual views were synthesized with the
viewpoint moving gradually between the two basis views.
Fig. 14(a)–(d) shows the two basis views and two examples
of synthesized images.
For the “snoopy scene,” the camera parameters were not
known. We relied on our proposed algorithms to solve the
entire problem from purely uncalibrated views. The results are
shown in Fig. 14(e)–(h). During acquisition, the camera was
set to auto-focus and hence the internal matrices might have
varied in the images. However, with the same general camera
model (for both the monkey scene and the snoopy scene), the
results are reasonable, indicating that our method works well.

LI et al.: VIRTUAL VIEW SPECIFICATION AND SYNTHESIS FOR FREE VIEWPOINT TELEVISION

(b)

(c)

(d)

545

(e)

(a)

(f)

(h)

(i)

(j)

(k)

(l)

(g)

(m)

(n)

(o)

(p)

Fig. 15. View interpolation using different focal length. The virtual viewpoint is obtained with the proposed approach, and the virtual view is synthesized
by warping two basis views with their depth maps. Some pixels are left blank as they are not visible in the basis views. (a, f) Two basis views. (g, l) Depth
map of two basis views. (b–e, h–k, m–p) Interpolated views with focal length 500, 2500, and 250, respectively.

VI. C ONCLUSION AND D ISCUSSION
We have presented a complete view specification and synthesis approach for FTV application. Experimental results
using multiple data sets have proved the correctness and
the feasibility of the proposed approach. The overall visual
quality of the synthesized virtual view using the proposed
system is comparable to that from typical approaches in
the literature, while our proposed system does not assume
the availability of key camera calibration information (albeit
simplification about the camera model such as no skew and
unitary aspect ratio has been made) and uses fewer views
than most existing approaches do. In our approach, a simple
interface for controlling view transition is possible based on a
typical TV remote. Thus, the proposed approach may provide
a practical scheme for IBR-based FTV. Nevertheless, as is
evident from the examples included, the synthesized images
may still depict defects (as is the case with any existing IBR
method), and thus further improvement is needed before such
an approach can by deployed in reality.
Since the weak calibration (upon which the whole system is
based) could be done for each frame, in this sense the system
can handle free movement of the camera. However, the computation cost could be a problem. A practical tradeoff could
be to perform the weak calibration only for each of the shots
of a video with the assumption that at least within the same
shot the camera is still (this is of course not always satisfied
and thus can only be a suboptimal tradeoff if the computation
must be saved). In addition, since the proposed approach
relies on weak-calibration (the computation of the F matrix), it
will encounter difficulty whenever the weak calibration is not
possible. Other delegacies were also analyzed in Section III.
Our current implementation can render an image in under
1 s in most cases, but may take a few seconds if there are

too many textureless pixels. Real-time implementation can be
pursued with hardware acceleration. A multiscale scheme may
also be employed to alleviate the problem. Other future targets
include analysis of the impact of various steps of the proposed
scheme on the overall quality of the virtual view. Future work
will also include introducing temporal constraints in video to
improve some of the algorithm steps such as feature detection
and disparity estimation.
R EFERENCES
[1] N. Bangchang, T. Fujii, and M. Tanimoto, “Experimental system of
free viewpoint TeleVision,” in Proc. IST/SPIE Symp. Electron. Imaging,
vol. 5006, no. 66, Jan. 2003, pp. 554–563.
[2] M. Tanimoto and T. Fujii, “FTV: Achievements and Challenges,”
ISO/IEC JTC1/SC29/WG11 M11259, Oct. 2004.
[3] S. Laveau and O. Faugeras, “3-D scene representation as a collection
of images,” in Proc. Int. Conf. Pattern Recognition, Jerusalem, Israel,
1994, pp. 689–691.
[4] D. Scharstein, “Stereo vision for view synthesis,” in Proc. Int.
Conf. Comput. Vision Pattern Recognition, 1996, San Francisco, CA,
pp. 343–350.
[5] A. W. Fitzgibbon, Y. Wexler, and A. Zisserman, “Image-based rendering
using image-based priors,” Int. J. Comput. Vis., vol. 63, pp. 141–151,
Jul. 2005.
[6] L. Zitnick, S.B. Kang, M. Uyttendaele, S. Winder, and R. Szeliski,
“High-quality video view interpolation using a layered representation,”
ACM Trans. Graph., vol. 23, no. 3, pp. 600–608, 2004.
[7] K.R. Connor and I.D. Reid, “Novel view specification and synthesis,”
in Proc. Brit. Mach. Vision Conf., 2002, Cardiff, England, pp. 243–252.
[8] Y. Ito and H. Saito, “Free-viewpoint image synthesis from multipleview images taken with uncalibrated moving cameras,” in Proc. Int.
Conf. Image Processing, vol. 3, 2005, Genoa, Italy, pp. 29–32.
[9] C. Buehler, M. Bosse, L. McMillan, S. Gortler, and M. Cohen, “Unstructured lumigraph rendering,” SIGGRAPH 2001, Los Angeles, CA,
pp. 425–432.
[10] P. E. Debevec, G. Borshukov, and Y. Yu, “Efficient view-dependent
image-based rendering with projective texture-mapping,” in 9th Eurographics Rendering Workshop, Vienna, Austria, Jun. 1998.
[11] M. Irani, T. Hassner, and P. Anandan, “What does the scene look
like from a scene point?” in Proc. Eur. Conf. Comput. Vision, 2002,
Copenhagen, Denmark, pp. 883–897.

546

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 19, NO. 4, APRIL 2009

[12] N. Inamoto and H. Saito, “Virtual viewpoint replay for a soccer match
by view interpolation from multiple cameras,” IEEE Trans. Multimedia,
vol. 9, no. 6, pp. 1155–1166, Oct. 2007.
[13] M. Pollefeys, L. Van Gool, M. Vergauwen, F. Verbiest, K. Cornelis,
J. Tops, and R. Koch, “Visual modeling with a hand-held camera,” Int.
J. Comput. Vis., vol. 59, no. 3, pp. 207–232, Sep.–Oct. 2004.
[14] S. M. Seitz and C. R. Dyer, “View Morphing,” in Proc. SIGGRAPH
1996, vol. 1, Vienna, Austria, 1996, pp. 84–89.
[15] A. Fusiello, “Specifying virtual cameras in uncalibrated view synthesis,”
IEEE Trans. Circuits Syst. Video Technol., vol. 17, no. 5, pp. 604–611,
May 2007.
[16] R. Szeliski and P. H. S. Torr, “Geometrically constrained structure from
motion: Points on planes in 3-D structure from multiple images of largescale environments,” in Proc. Eur. Workshop SMILE ’98, Lecture Notes
Comput. Sci., vol. 1506, Freiburg, Germany, pp. 171–186.
[17] G. Arfken, Mathematical Methods for Physicists. 3rd ed. Orlando, FL:
Academic Press, 1985.
[18] R. W. Brockett, “Robotic manipulators and the product of exponentials
formula,” in Proc. Int. Symp. Math. Theory Networks Syst., 1983,
Blacksburg, VA, pp. 120–127.
[19] J. B. Kuipers, Quaternions and Rotation Sequences: A Primer with
Applications to Orbits, Aerospace and Virtual Reality. Princeton, NJ:
Princeton Univ. Press, 2002.
[20] R. Hartley and A. Zisserman, Multiple View geometry Comput. Vision.
Cambridge, U.K.: Cambridge Univ. Press, 2000.
[21] R. Hartley and P. Sturm, “Triangulation,” Comput. Vision Image Understanding, vol. 68, no. 2, 1997, pp. 146–157.
[22] C. Rothwell, G. Csurka, and O. Faugeras, “A comparison of projective
reconstruction methods for pairs of views,” in Proc. Int. Conf. Comput.
Vision, Cambridge, MA, 1995, pp. 932–937.
[23] P. Beardsley, A. Zisserman, and D. Murray, “Sequential updating of
projective and affine structure from motion,” Int. J. Comput. Vis., vol. 23,
no. 3, pp. 235–259, Jun.–Jul. 1997.
[24] C. D. Meyer, Matrix Analysis and Applied Linear Algebra, Society for
Industrial and Applied Mathematics, 2000, Philadelphia, PA, chap. 4.
[25] A. Shashua and M. Werman, “Trilinearity of three perspective views and
its associated tensor,” in Proc. Int. Conf. Comput. Vision, Cambridge,
MA, 1995, pp. 920–925.
[26] H. Li and R. Hartley, “Inverse tensor transfer for novel view synthesis,”
in Proc. Int. Conf. Image Processing, vol. 2, no. 2, 2005, Genoa, Italy,
pp. 97–100.
[27] P.F. Felzenszwalb and D.P. Huttenlocher, “Efficient graph-based image
segmentation,” Int. J. Comput. Vis., vol. 59, no. 2, pp. 167–181, Sep.
2004.
[28] C.L. Zitnick and L. Kanade, “A cooperative algorithm for stereo matching and occlusion detection,” IEEE Trans. Pattern Anal. Mach. Intell.,
vol. 22, no. 7, pp. 675–684, Jul. 2000.
[29] A. Klaus, M. Sormann, and K. Karner, “Segment-based stereo matching
using belief propagation and a self-adapting dissimilarity measure,” in
Proc. Int. Conf. Pattern Recognition, vol. 3, Hong Kong, China, 2006,
pp. 15–18.
[30] P. Fua, “A parallel stereo algorithm that produces dense depth maps and
preserves image features,” Mach. Vis. Appl., vol. 6, no. 1, pp. 35–49,
Dec. 1993.
[31] S. Roth and M. J. Black, “Fields of experts: A framework for learning
image priors,” in Proc. Int. Conf. Comput. Vision Pattern Recognition,
vol. 2, San Diego, CA, 2005, pp. 860–867.
[32] The Persistence of Vision Raytracer. [Online]. Available:
http://www.povray.org
[33] W. Li and B. Li, “Probabilistic image-based rendering with Gaussian
mixture model,” in Proc. Int. Conf. Pattern Recognition, vol. 1, Hong
Kong, China, 2006, pp. 179–182.
[34] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
Int. J. Comput. Vis., vol. 60, no. 2, pp. 91–110, Nov. 2004.
[35] Open Source Computer Vision Library (OpenCV). Available:
http://www.intel.com/technology/computing/opencv/
[36] O. Woodford and A. Fitzgibbon, “Fast image-based rendering using
hierarchical image-based priors,” in Proc. Brit. Mach. Vision Conf.,
Oxford, U.K., Sep. 2005.

Wenfeng Li (S’04) received the B.E. degree in automation from Tsinghua University, Beijing, China
in 2000 and the M.S. degree in computer science
from Southern Illinois University, Carbondale, in
2003. He is currently working toward the Ph.D.
degree at the Department of Computer Science and
Engineering, Arizona State University, Tempe.
His research interests include computer vision,
image and video processing and statistic learning.

Jin Zhou (S’04) received the B.E. degree in automation in 2001 and the M.S. degree in pattern recognition and intelligent systems in 2004, both from University of Science and Technology of China. He is
currently working toward the Ph.D. degree at the
Department of Computer Science and Engineering,
Arizona State University, Tempe.
His research interests include computer vision,
robotics, and image and video processing.

Baoxin Li (S’97–M’00–SM’04) received a Ph.D.
degree in electrical engineering from the University
of Maryland, College Park, in 2000.
He is currently an Assistant Professor in the
Department of Computer Science and Engineering,
Arizona State University, Tempe. He was previously
a Senior Researcher with Sharp Laboratories of
America, Camas, Washington, where he was the
Technical Lead in developing SHARPs Hi-Impact
Sports technologies. He was also an Adjunct Faculty
Member with Portland State University, OR, from
2003 to 2004.
Dr. Li research interests include pattern recognition, computer vision,
multimedia processing, and statistical methods in visual computing.

M. Ibrahim Sezan (F’04) received the B.S. degree in electrical engineering and mathematics from
Bogazici University, Istanbul, Turkey, in 1980, the
M.S degree in applied physics from Stevens Institute
of Technology, Hoboken, NJ, in 1982, and the Ph.D.
degree in electrical, computer and systems engineering from Rensselaer Polytechnic Institute, Troy, NY,
in 1984.
He is currently the Chief Technology Officer of
Sharp Laboratories of America, Inc., Camas, WA,
where he also directs the research in advanced video
and display technologies. From 1984 to 1996, he was with Eastman Kodak
Company, Rochester, NY, where he founded and headed the Video and
Motion Technology Area in the Imaging and Advanced Research Development
Laboratories during 1992 and 1996. From 1987 to 1996, he also held Adjunct
Professor positions at the University of Rochester, Rochester, NY, Rochester
Institute of Technology, Syracuse University, Syracuse, and Rensselaer Polytechnic Institute, where he developed and taught graduate courses in electrical
engineering and mathematics and co-advised doctoral theses. He has edited
and contributed to a number of technical books and research monographs
on digital image and video processing. He has extensively published in the
areas of digital video enhancement, smart algorithms for visual analysis
and understanding, and image retrieval and search. He has chaired technical
committees and assumed leadership positions in MPEG-2, MPEG-4, MPEG7, and MPEG-7 as well as TV-Anytime Standardization. He has also published
papers in the areas of image and video restoration and enhancement, analysis,
compression, networked video, semantic video event modeling and detection,
user preference modeling, and recommendation engines. He is the holder or
co-holder of 39 U.S. patents.

arXiv:1704.01262v1 [cs.CV] 5 Apr 2017

Investigating Human Factors in Image Forgery Detection
Parag Shridhar Chandakkar

Baoxin Li

Computer Science and Engineering
Arizona State University

Computer Science and Engineering
Arizona State University

pchandak@asu.edu

baoxin.li@asu.edu

ABSTRACT
In today’s age of internet and social media, one can find an
enormous volume of forged images on-line. These images
have been used in the past to convey falsified information
and achieve harmful intentions. The spread and the effect of
the social media only makes this problem more severe. While
creating forged images has become easier due to software
advancements, there is no automated algorithm which can
reliably detect forgery.
Image forgery detection can be seen as a subset of image understanding problem. Human performance is still the
gold-standard for these type of problems when compared to
existing state-of-art automated algorithms. We conduct a
subjective evaluation test with the aid of eye-tracker to investigate into human factors associated with this problem.
We compare the performance of an automated algorithm
and humans for forgery detection problem. We also develop
an algorithm which uses the data from the evaluation test to
predict the difficulty-level of an image1 . The experimental
results presented in this paper should facilitate development
of better algorithms in the future.

Categories and Subject Descriptors
I.5.2 [Pattern Recognition]: Pattern analysis; I.4.9 [Image
Processing and Computer Vision]: Applications; I.4.m
[Image Processing and Computer Vision]: Miscellaneous

General Terms
Human Factors, Experimentation, Performance

Keywords
Image Forgery, Subjective Evaluation, Eye-tracking
1
The difficulty-level of an image here denotes how difficult
it is for humans to detect forgery in an image. Terms such
as “Easy/difficult image” will be used in the same context.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from Permissions@acm.org.
HuEvent’14 November 07 2014, Orlando, FL, USA
Copyright 2014 ACM 978-1-4503-3120-3/14/11 ...$15.00
http://dx.doi.org/10.1145/2660505.2660510 .

(a)

(b)

Figure 1: Examples of infamous tampered images.

1.

INTRODUCTION AND RELATED WORK

Forged images are in abundance in today’s age of social
media and internet. They can be used to spread false information through social media and thereby achieve harmful
intentions. They have been used in areas such as sports,
fashion, politics, professional photography etc. for different
motives. History of image forging dates back to 1800’s, then
mostly done for political reasons. We present two of the most
infamous cases of forgery to show the severity of the problem. First infamous incident occurred in 1950 when a forged
photo reportedly contributed to the electoral defeat of Senator Millard Tydings (right). The photo in Figure 1a shows
Millard Tydings having a conversation with Earl Browder
(left), who was a leader of American Communist Party. In
the second incident, Fig. 1b shows a photo of Osama Bin
Laden after his encounter with US forces on May 2nd , 2011.
Though the photo was reportedly published in many places,
it was later determined to be fake.
Image forgery can be categorized into two types: 1. Image
splicing 2. Image tampering. Splicing is the simplest form
of forgery where no post-processing is performed on the image. Tampering involves certain post-processing operations
such as blurring, resizing etc. They are performed on the
image to make it look as natural as possible. Most of the
images on web are tampered. Creating forged images has
become easier and detecting them is getting difficult due to
constant advancements in editing software. There exist two
common approaches to detect forgery, namely active and
passive. Watermarking is an example of an active approach.
It requires extra effort and most of the images on the web are
not watermarked. Passive approach determines the authenticity of an image by analysing the image itself. The forgery
detection process employed by humans is an example of a
passive approach.
Image forgery detection is a subset of image understanding
problems which include scene classification, object detection
etc. Human performance is still the gold-standard for most

of these problems. We investigate if the claim holds true for
forgery detection. We conduct a subjective evaluation test
with eye-tracking to quantify the human performance and
to understand the behavioural aspect of this problem. In
an attempt to relate human vision and the forgery detection
process, we examine the relation between the saliency of an
image and its effect on prediction performance. By using
the eye-tracking data and the performance statistics from
the evaluation test, we develop an algorithm to predict the
difficulty-level of an image. We also compare the human
performance against that of an automated algorithm. We
envision the development of better algorithms in the future
using these findings.
Recent years have seen an active research in this area.
Copy-and-move forgery (CMF) is one of the most common
methods of forgery in digital images. SURF-feature and
textural-descriptors were used to detect CMF [1,2]. Forgery
in JPEG images is detected by analysing the DCT coefficients as the forged image is most likely to be compressed
twice [5]. Another class of approaches uses high-level information in an image, such as, shadows [7], light environment [11] etc. Approaches solely depending on image
statistics are image-format independent and are more computationally complex. Hilbert-Huang transform and Markov
transition matrix of block DCT coefficients were proposed
in [4] and [12] respectively. We refer the reader to [3] and [9]
for an extensive review of forgery detection approaches.
This paper focuses on human performance evaluation of
forgery detection. To the best of our knowledge, this work is
first of its kind. Human performance evaluation studies have
increased performance of object detectors and annotation
predictors in [14]. Eye-tracking has also been used to study
the behavioral aspects of radiologist’s performance [8].
The rest of the paper is structured as follows. Section 2
describes the proposed approach. The results and discussions are presented in section 3. Section 4 concludes the
paper and lists potential future works.

2.

PROPOSED APPROACH

The aim of this paper is to examine human factors related to the forgery detection problem. The detection performance of many subjects on a set of forged images is recorded
to present a quantitative assessment of the human performance. We analyse the human gaze-points with the aid of
eye tracker to understand the behavioural aspect. It was
shown in [14] that there is a strong relation between the
human-level understanding of an image and the human gazepoints. The experimental set-up and protocol is designed to
establish a relationship between the pattern of human gazepoints and the difficulty-level of a forged image.

2.1

Dataset and Experimental set-up

We present a quantitative measure of human performance
and study the relationships between the human gaze and
image content using images from two standard image forgery
datasets, CASIA v1.0 and v2.0 (from http://forensics.
idealtest.org). CASIA v1.0 is a splicing dataset whereas
its next version has tampered images. Our database has
73 images out of which 14 are spliced (taken from CASIA
v1.0), 44 are tampered (taken from CASIA v2.0) and 15 are
authentic images. The evaluation test is performed with 24
subjects over a span of one week. Subjects are divided into 3
groups of 9, 9 and 6. Each subject in the first two groups is

Figure 2: Human prediction performance grouped by category

presented with 50 images at a time. Subjects in third group
are presented with all 73 images. The images stay the same
within a group and those 50 images are sampled from 73
images. All the subjects sat at an approximate distance
of 25 inches from a 15.6 inch screen having resolution of
1920×1080. Natural lighting conditions are used for the test
and no time-limit is imposed to allow the subjects to scan the
image at their own pace. The distribution of authentic and
forged images is unknown to subjects. Before starting the
evaluation, five examples of each kind of images are shown
to each subject with the forged region being pointed out in
order to give them an idea about the type of forgery. They
are instructed to classify each image into one of two groups,
namely, authentic or forged and point out the forged region.

2.2

Analysis

The eye-tracker data and image-level statistics are used
to address several questions relating gaze and other factors
which may affect the difficulty-level of an image. The contribution of our analysis is three-fold. Firstly, we provide a
quantitative measure of human performance on our dataset.
We use the model of image saliency to predict the difficultylevel of an image. Third contribution aims at developing
an automated algorithm to predict the difficulty-level using
data from saliency model and gaze-points.

2.2.1

Relating image saliency with forgery

For every forged image in the database, we have their
authentic counterparts available for analysis. We propose to
model the change in saliency between the forged image and
its authentic counterpart. We compute saliency in spatial
[10] as well as frequency-domain [6]. Depending on the way
an image has been forged, we categorize an image into one
of four classes as follows:
• Class 01: The forged part is non-salient in the authentic image and becomes salient in the forged image.
Class 00, 10 and 11 can be similarly described. We find
that change in saliency significantly affects the difficultylevel of an image. Fig. 2 shows the category-wise accuracy.
Quantitative results are given in section 3.

2.2.2

Relating gaze with forgery

Human gaze contains abundant information about the
task and human thought-process [13]. We analyse the information and come up with the following metrics to better
explain the human process of forgery detection.
1. For class 00 and 10, we compute the following metric:

The error bars represent the standard deviation in a particular category (calculated over all the subjects).

3.1

Figure 3: Effect of image category on fixations

Gaze-metric1 =

# fixations in forged region
# fixations in salient region

(1)

Salient region in an image can be detected by any standard saliency algorithm. We define Gaze-metric2 for
class 01 and 11. We count the number of fixations lying
elsewhere instead of in salient region since salient and
forged region intersect. This gives a measure about the
number of fixations used to detect forgery. Intuitively,
Gaze-metric1 (Gaze-metric2 ) should get higher values
for easy images. Higher values for both metrics imply
shorter fixation duration in the salient region (elsewhere) than in the forged region. Since human vision
is usually clustered in the salient part of an image, this
behaviour supports the claim of task-driven human vision [13].
2. We analyse the effect of image category on duration of
fixations in the forged region. For each category, we
take average of the duration of fixations in the forged
regions. Note that we average over images in a particular category as well as over all the subjects. We also
record the order of fixations in the forged regions while
subjects scan the given image. The plot of mean order
of fixations versus image category is shown in Fig. 3.
The analysis of fixation statistics is given in section 3.
3. We study the effect of fixation duration over an entire
image on the prediction accuracy. We add-up duration
of fixations for all the images for each subject. Scanning an image for a longer duration can be related to
analysing image over multiple scales and regions. The
plot is shown in Fig. 4.

Saliency analysis

The Highest performance is obtained in class 01 as expected due to saliency of forged region. The lower performance associated with class 10 can be associated to the nonsaliency of the forged region after it undergoes editing. The
forgery operation in the 10 category usually involves removal
of the salient region in the authentic image and replacing it
with a non-salient texture, which is un-noticed by many subjects. However, lowest accuracy is achieved on the class 11,
indicating that saliency is not necessarily a factor which decides difficulty of a forged image. Images in the class 11
are skillfully forged by preserving the spatial and contextual
continuity after editing. This makes forgery detection difficult for superficial observers. Second-highest performance is
obtained on class 00 which further supports our claim about
saliency not being a deciding factor.

3.2

Fixation analysis

Two gaze metrics proposed in the section 2.2.2 can be
successfully used to predict easy images. Values of both the
metrics are collected for all the images in respective classes
and are sorted. The values are binary thresholded at a point
where large change is observed and are grouped into 2 groups
per metric. Group 1 and 3 (2 and 4) contains metric values
above (below) threshold. The thresholds are determined to
be 4 and 2 respectively for two metrics. Accuracies of all the
images are also classified into 2 groups as per the obtained
thresholds. The mean accuracies of images (averaged over all
subjects) in the 4 groups are found to be 75%, 58.7%, 66.7%
and 47.8% respectively in accordance with our analysis.
Next, we analyse the effect of image category on fixation
duration and order. The data-labels plotted on top of each
data-point in Fig. 3 represent the maximum value. For example, 1410 denotes the maximum (over all subjects) number of fixations required to fixate a subject’s vision on forged
region in all the images in class 00. Similarly, 20.31 denotes
the maximum amount of time a subject fixated in forged
region. This plot shows that fixation duration and order are
both affected by the saliency. For class 01, subjects fixated
on the forged region earlier whereas most number of fixations were required in the class 00. It is interesting that in
spite of the contrast in the fixations, these two categories get
the top-2 prediction scores. In class 10, subjects fixated on
the forged region for the least amount of time. On the other

Finally, we use clustering to group the features from the
gaze-data and saliency model into two clusters. The images
in a cluster should correspond to the same category, i.e., easy
or difficult. Thus a large difference between the accuracies
of two clusters (averaged over all the subjects) is expected.

3.

RESULTS AND DISCUSSIONS

In this section, we present the quantitative results and
discuss the findings for various metrics presented in section
2.2. Human prediction performance on authentic as well
as forged images is shown in Fig. 2. The green and red
bars represent prediction performance on an image-level for
authentic and forged images respectively. The orange bars
represent region-level prediction accuracy for forged images.

Figure 4: Effect of fixation duration on prediction.

(a)

(b)

Figure 5: Fixation heatmap of (a) correct detection (b) missed
detection of forged images. Forged area is highlighted in yellow
(Best viewed in color).

hand, they fixated for the most amount of time in class 11,
probably because the forged region also intersects with the
salient region. Yet, these two classes have two lowest prediction scores associated with them. Human fixations are thus
affected by the image saliency and the given task.
What would be the effect of fixating longer at an image?
Will it result in increased prediction accuracy? The plot in
Fig. 4 suggests otherwise. In the plot, data-label over each
data-point denotes the accuracy obtained by the subject.
The subjects are arranged in the increasing order of fixation
duration. The pattern suggests that fixation duration over
an entire image contributes little to the prediction process.

3.3

Comparison with automated algorithm

We implement a passive approach for image forgery detection proposed in [12]. On the subset of 50 images, best
human and computer performances are found to be 72%
and 62% respectively. For all 73 images, the same numbers
are 68.49% and 68.49% respectively. We also examine the
class-wise performance. Best Human performance on the 4
classes and on authentic set is 75%, 100%, 63.63%, 22.22%
and 60% respectively. The same set of numbers for computer performance is 75%, 78.57%, 45.45%, 77.78% and 60%
respectively. Thus humans are better at detecting unskilled
forgery but on the difficult images automated algorithm performs slightly better.

3.4

Prediction algorithm

We develop an automated algorithm to group the images
under two categories, namely, easy and difficult, by using
the data from saliency and gaze-analysis . We use K -means
where the feature vector contains the following quantities:
1. Class index 2. Gaze metrics 3. Order of fixation in forged
region 4. Duration of fixations in forged region.
After clustering the feature space into 2 groups, the accuracies for both the clusters obtained are 63.87% and 37.69%.
Note that the accuracies are averaged over all the images in
that cluster and over all the users. In spite of averaging
over all subjects and images, the collected features are able
to fairly predict the difficulty level of an image.

4.

CONCLUSION AND FUTURE WORK

We investigate the human factors associated with the process of forgery detection by conducting a subjective evaluation test with the aid of eye-tracker. To analyze the effect of saliency on forgery detection, we group the images in
four classes and perform statistical analysis. We show that
though saliency affects the prediction, it is not necessarily a

deciding factor. We use eye-tracker data and show the effect of image category on the duration of fixations and their
order. We successfully apply clustering on the generated
data to group the images into easy and difficult categories.
We compare the performance of automated algorithm and
humans to show that automated algorithm is better at detecting skilled forgery. In the future, we would like to closely
examine the dependency between underlying statistics in the
fixation pattern, image saliency and image forgery.
Acknowledgement: The work was supported in part by
a grant (#1135616) from the National Science Foundation.
Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.

5.

REFERENCES

[1] E. Ardizzone, A. Bruno, and G. Mazzola. Copy-move
forgery detection via texture description. In
Proceedings of the 2nd ACM workshop on Multimedia
in forensics, security and intelligence, 2010.
[2] X. Bo, W. Junwen, L. Guangjie, and D. Yuewei.
Image copy-move forgery detection based on surf. In
Multimedia Information Networking and Security
(MINES), International Conference on, 2010.
[3] H. Farid. Image forgery detection. Signal Processing
Magazine, IEEE, 2009.
[4] D. Fu, Y. Q. Shi, and W. Su. Detection of image
splicing based on hilbert-huang transform and
moments of characteristic functions with wavelet
decomposition. In Digital Watermarking. Springer,
2006.
[5] J. He, Z. Lin, L. Wang, and X. Tang. Detecting
doctored jpeg images via dct coefficient analysis. In
ECCV. Springer, 2006.
[6] X. Hou, J. Harel, and C. Koch. Image signature:
Highlighting sparse salient regions. PAMI, IEEE
Transactions on, 2012.
[7] E. Kee, J. O’brien, and H. Farid. Exposing photo
manipulation with inconsistent shadows. ACM
Transactions on Graphics (TOG), 2013.
[8] E. A. Krupinski. Visual scanning patterns of
radiologists searching mammograms. Academic
radiology, 1996.
[9] B. Mahdian and S. Saic. A bibliography on blind
methods for identifying image forgery. Signal
Processing: Image Communication, 2010.
[10] R. Margolin, A. Tal, and L. Zelnik-Manor. What
makes a patch distinct? In CVPR, 2013 IEEE
Conference on, 2013.
[11] R. Ramamoorthi and P. Hanrahan. On the
relationship between radiance and irradiance:
determining the illumination from images of a convex
lambertian object. JOSA A, 2001.
[12] P. Sutthiwan, Y. Q. Shi, H. Zhao, T.-T. Ng, and
W. Su. Markovian rake transform for digital image
tampering detection. In Transactions on data hiding
and multimedia security VI. Springer, 2011.
[13] A. L. Yarbus, B. Haigh, and L. A. Rigss. Eye
movements and vision. Plenum press New York, 1967.
[14] K. Yun, Y. Peng, D. Samaras, G. J. Zelinsky, and
T. L. Berg. Studying relationships between human
gaze, description, and computer vision. In CVPR,
2013 IEEE Conference on. IEEE, 2013.

ACTIVE LEARNING FOR TAG RECOMMENDATION UTILIZING ON-LINE PHOTOS
LACKING TAGS
Yajun Gao

Baoxin Li

College of Computing
Georgia Institute of Technology

Computer Science and Engineering
Arizona State University

ABSTRACT
Recommending text tags for on-line photos is useful for Internet
photo services. Typical solutions to this problem require analysis
of the correlation among different attributes of the photos, including
the correlation between the textual features and visual features computed from a photo. However, most on-line photos have very few
tags or even no tags, and thus they contribute little or none to the
analysis of tag-photo correlation, which is a key component in those
schemes that rely on such analysis for tag recommendation. To address this practical challenge, we propose an active learning method
for incorporating photos with no or few tags so as to enhance the correlation analysis for improved performance in tag recommendation.
We demonstrate the effectiveness of the proposed approach using a
dataset of more than 33, 000 photos collected from Flickr.
Index Terms— Tag recommendation, active learning
1. INTRODUCTION
Tagging allows annotations to be associated with various media objects like images or videos, hence enabling the application of the
well-developed, text-based retrieval techniques on non-textual multimedia data. Recent years have witnessed signiﬁcant development
on tag recommendation systems, which assist a user in the tagging
process by suggesting a set of relevant tags for a given object. For
example, photo tag recommendation has attracted a lot of attention
due to the popularity of on-line photo sharing services like Yahoo
Flickr and Google Picasa. Efforts in this regard range from those
that are based on a small number of existing tags for recommending
more (e.g., [1]) to those that utilize purely the images (e.g., [2]). Our
study in this paper deals with the latter situation, which is a more
practical scenario of tag recommendation for on-line photo services.
Photo tagging based on only images is closely related to image
understanding, which has seen many years of development. For example, Hironobu et al. in [3] used co-occurrence models to predicate
annotated words for images, and Duygulu et al. in [4] modeled annotation as machine translation. In both cases, images are divided
into sub-regions and a mapping between keywords and sub-regions
are learned from a training set. The mapping can then be applied
to suggest annotations for new images. This and similar approaches
may be useful for images that can be characterized by some key
sub-regions, but less effective in generating tags that may be associated with more global visual content. In [2], an approach was
proposed for learning the correlation between visual features (both
global and local) of an image and its associated tags and then using
the learned model for tag recommendation. One potential practical challenge of such an approach is that, in order to learn a useful
model for a large vocabulary of tag words and a (generally speaking)
high-dimensional visual feature, a good training set with well-tagged

978-1-4673-2533-2/12/$26.00 ©2012 IEEE

2869

images would be required. (The study of [2] limits the scope of tagging to only a small set of words to alleviate this problem.) This is
in fact also an issue with the preceding methods of [3] and [4]. Unfortunately, most on-line photos have very few or no tags. For example, it was estimated that more than 84% of public photos on Flickr
have only 0-3 tags [5]. This poses as a challenge for the analysis
of the semantic relationship between tags and photos. Other related
work includes latent semantic analysis with application in the concept space [6, 7], and the collaborative ﬁltering methods [8, 9]. Such
methods in general do not readily extend to the correlation analysis
of images and keywords for photo tagging.
In this paper, we address the above challenge by formulating the
following active learning problem: given a dataset that contains a
high percentage of images that are under-tagged (with 3 or fewer
tags, including none), to automatically select a subset so as to learn
optimal correlation between the images and the tags for improved
recommendation. This is an active learning problem especially since
the selected subset should include images that may have no tags at
all in order to maximize the utilization of information in the overall dataset. In our approach, we ﬁrst employ Canonical Correlation Analysis (CCA) for obtaining a baseline image-tag correlation
model from all images with at least one tag, and then formulate an
optimization problem for improving the baseline by incorporating a
subset of the untagged images. To obtain an efﬁcient solution, we
propose a two-stages process, where images with few (but not zero)
tags and images without any tag are handled slightly differently, for
achieving efﬁciency while maintaining stability with respect to the
baseline model. Clearly, this approach would enable the incorporation of the newly added, untagged images into the dataset for improving an existing model. To evaluate the effectiveness of the proposed approach, we tested with a dataset of 33, 064 Flickr images
(of which 11, 336 are without any tags) based on the accuracy of tag
recommendation for a testing set, using the original, user-generated
tags as the ground truth.
2. PROPOSED APPROACH
For completeness and clarity, we ﬁrst brieﬂy review the method of
using Canonical Correlation Analysis (CCA) for tag-image correlation analysis. CCA attempts to ﬁnd basis vectors for two sets of
variables such that the correlation between the projections of the
variables onto these basis vectors is mutually maximized [10]. The
canonical correlation between the two data sets is deﬁned as
Wx XY Y WyT
Wx ,Wy Wx X2 Wy Y 2

ρ = max

(1)

where X and Y are the two sets of variables and Wx and Wy are the
basis vectors onto which X and Y are projected, respectively.

ICIP 2012

In principle, a baseline tag recommendation algorithm can be
designed as follows after a CCA model has been learned from a
well-tagged training set. We ﬁrst compute a similarity measure between a given test photo and the training images in the CCA space,
obtaining a ranked list of tags from those most similar images in
the training set. Then tags for the test photo may be recommended
from this list while considering other constraints (e.g., how many
tags to keep). In such a scheme, the quality of the learned correlation model is strongly linked to the quantity of the well-annotated
training data - having access to more well-tagged images would in
general improve the tag-photo correlation analysis and thus improve
the tag recommendation performance. Unfortunately, as discussed
previously, most on-line photos are under-tagged. In the following,
we present our approach for employing active learning to maximally
utilize under-tagged images in a given dataset so as to improve the
learning of the correlation model for tag recommendation.

where qj = 1 denotes that image xj ∈ S, and qj = 0 denotes
that it is not. Now we can write the expanded training set as L =
∗
∗
{q1 (x1 , y1 ), ..., ql (xl , yl ), ql+1 (xl+1 , yl+1
), ..., ql+u (xl+u , yl+u
)}.
A properly-expanded training set L should be one that yields the
minimal difference in terms of the correlation model (compared to
that from the original tagged set) while keeping all the data in the
original training set (i.e., L ∈ L ). The latter constraint can be
conveniently expressed via indicator variables q : qj = 1 for all the
photos in the original tagged dataset L. By measuring the difference
between two correlation models via the difference between their
respective correlation coefﬁcients and the angles of the principal
axes, we can deﬁne the following optimization problem as the goal
of active learning:
L∗
s.t.

2.1. An Active-Learning Formulation
Active learning is a machine learning technique that selects unlabeled samples from an existing pool for labeling according to certain criteria. Ideally, the selected samples plus their newly-annotated
labels will enrich the original (smaller) training set for improving
an analysis task. In this work, by extending the concept of “labeling” to tagging, we deﬁne an active-learning problem of automatically selecting a batch of untagged pictures to expand a given
training set. Formally, let L = {(x1 , y1 ), ..., (xl , yl )} denote a set
of l initially tagged pictures, where xi represents the visual content and yi represents the textual tags of the ith photo, and let U =
{xl+1 , xl+2 , ..., xl+u } denote an untagged set containing u images
with only visual features. Then the active-learning problem is to
choose the best subset of images from the untagged dataset so as to
improve the correlation model for tag recommendation. That is, an
optimal subset of pictures S = {xk1 , ..., xkn } ∈ U is to be selected
for augmenting the training set, so as to obtain the most performance
gain in tag recommendation.
With the scale of on-line photos, it is impractical to consider
manual labeling, and thus it is necessary to employ an automatic tagging strategy in the process of selecting the untagged subset. With
the assumed availability of a given, tagged training set, it is natural
to consider an automatic tagging strategy based on the model learned
from the given training set. In this work, we predict tags for those
unlabeled examples in the process of selecting the subset, using the
CCA model learned from the labeled set L. Accordingly, both the
candidate subset with predicted labels and the existing labeled data
can simultaneously inﬂuence the result of sample selection. It is desired to ensure that the tags assigned to the photos in a candidate set
S have a tag-photo correlation similar to the one learned from the
given training set, since the model learned from the training set is
assumed to have captured the proper semantic relationship between
the tags and the photos. Meanwhile, a prediction scheme will invariably introduce changes to the correlation model after the new data
have been added to the training set. We would want to avoid incurring dramatic change to the original model since it was learned from
groundtruth labeling (i.e., the original tags). These properties may
be at least partially measured by considering both the correlation coefﬁcients and the CCA subspaces, before and after incorporating the
new data into the original training set.
With the preparation of the above discussion, we can now de∗
ﬁne the active learning task more formally. Let yl+i
be the set
of predicted labels associated with the photos from the unlabeled
set. We introduce a vector of indicator variables q ∈ [0, 1]l+u ,

2870

=



argminρ(L ) − ρ(L) − λWxT Wx

L =S∪L

(2)

qj = 1 f or 1 ≤ j ≤ l
qj ∈ {0, 1} f or l + 1 ≤ j ≤ l + u

where L∗ denotes the labeled set expanded with the optimal selection from an unlabeled set, ρ the correlation coefﬁcient, Wx the projection of the visual features, and λ a normalization factor balancing
the contributions of the two terms in the objective function.
The ﬁrst term of the objective function in Eqn. 2 measures the
similarity of semantic correlation between the expanded training set
and the original training set, which in a sense indicates the accuracy
of the tag prediction step in selecting the candidate set S. On the
other hand, the second term measures the difference between the
projection directions before and after the addition of the selected
data. Minimizing this term will ensure that the new model will not
deviate too much from the original model (which is trusted since
it is from a tagged set). This optimization problem is an integer
programming problem that involves canonical correlation objective
function, which is difﬁcult to solve analytically. We will propose
below an algorithm to obtain an approximate solution.
2.2. Proposed Algorithm
We now present the proposed approach for approximately achieving the goal of the above active learning task. We ﬁrst propose a
helpful preprocessing step: tag augmentation. For a tagged training
set collected from actual on-line photos, the majority of them still
have only fewer than 3 tags. This will render the learning of a baseline correlation model difﬁcult due to the lack of sufﬁcient textual
information. To alleviate this issue, we ﬁrst propose a novel preprocessing procedure, termed ”tag augmentation”, which enriches the
tags for the tag-lacking photos in the training set.
In this procedure, for a given set of training example L, we ﬁlter
out those photos with fewer than M tags. For these photos, we infer
additional tags based on the rest of photos in the tagged set, hence
augmenting their tags by keeping the original plus the newly-inferred
ones together. These additional tags are inferred by ﬁnding photos
with similar visual features in the CCA subspace (learned from the
given training set). Since visual features and tags are maximally
correlated in the CCA subspace, similar visual features in this space
correspond to photos that should be tagged similarly. Thus, for a
given under-tagged photo, the most commonly shared tags among its
similar photos are used for augmenting its tag set. We use Euclidean
distance and K-nearest neighbor algorithm to ﬁnd similar photos in
the CCA subspace, as summarized in Algorithm 1.
With the tag-augmented training set, we learn a new CCA model
and project the entire unlabeled set onto this space for selecting a

learnt CCA subspace. This contributes to minimizing the ﬁrst term
in Eqn. 2 as long as one can reasonably assume that visually-similar
images should have similar tags.
Algorithm 2 Expanding the Training Set
Input: L, U and C
1: Compute distance d(xl , xu ) ∀xl ∈ L and ∀xu ∈ U ;
2: Select xu , if mind(x, xu ) < C;
x∈L

3: Predict tags for xu in CCA subspace;
4: Add xu with their predicted tags to L and form the expanded

training set L
Output: L

3. EXPERIMENTS

Fig. 1. An overview of our proposed approach.

subset in active learning, resulting in an expanded training set. Finally we learn the CCA model from this expanded training set and
use it for improved tag recommendation. The schematic ﬂow of the
entire approach is shown in Fig. 1. We elaborate below the procedure
used for selecting the subset in active learning.
Algorithm 1 Tag Augmentation
1: Perform CCA on the training set L and project visual feature X
onto the subspace;
2: Find the K nearest neighbors of each under-tagged photo in the
training set according to the Euclidean distance in the CCA subspace;
3: Rank the frequency of tags from the K nearest neighbors;
4: Choose top N tags to augment the tag set of the photo.
Unlabeled photos that are similar to the training set L are selected based on the Euclidean distance of visual features between an
unlabeled photo and the training photos. By setting a parameter C
as the threshold, we can obtain a set of photos from the unlabeled
dataset as the candidate selection. Then we infer tags for the photos
in the selection following the same procedure as we used for tag augmentation (except now the photos have no tags to begin with). We
add these selected photos with their augmented tags to the labeled set
and form a new training set for computing an improved CCA model.
This is summarized in Algorithm 2 (cf. ”Selection” in Fig. 1).
In this algorithm, the input is an initial training set with no limitation on the number of tags for each photo. We learn the initial
correlation model via CCA on the training set. For labeled data with
too few tags, we augment their tags based on their similarity to the
rest of the training photos in the CCA subspace. Then we choose a
set of unlabeled examples based on their similarity to the data in the
training set in the CCA subspace and then add them to the training
set with their corresponding augmented tags.
The above method, while supplying only an approximate solution to the general problem of Sect. 2.1, can ﬁnd intuitive interpretations as to how it approximates Eqn. 2 in some sense. Recall that
part of the effort in solving Eqn. 2 is to minimize the ﬁrst term when
selecting new unlabeled images. In the above approximate solution,
images are selected only when their visual features are similar in the

2871

We now present our experiments for validating the effectiveness of
the proposed approach. Our evaluation is largely based on the comparison of the performance of tag recommendation before and after
introducing the proposed method. We will ﬁrst describe the experimental setup and some speciﬁcs about feature extraction (which has
been deferred to this section since the general approach of Section 2
does not depend on the speciﬁc way of feature extraction).
3.1. Experiment Setup and Feature Extraction
We collected images on Flickr by randomly picking user IDs and
then downloading pictures under the IDs. To avoid too many pictures from the same ID, no more than 200 images were saved for the
same ID. In total, we collected more than 33 thousand photos, among
which the number of images with tags is 21728. From these tagged
images, a list of 1254 tags were extracted by ﬁnding the most frequent words (after stemming and removing stop words). The textual
features of the photos are deﬁned by a vector indicating the presence
of the extracted list of tags. It is worth noting that the dataset consists of 9653 photos with only a single tag and 2568 photos having
2 tags. Together these already cover 56.24% of the labeled photos.
The proposed approach leads to a procedure for obtaining a
ranked list of tags for a test photo. In our evaluation, the system
returns the tags in decreasing scores that are computed from the
distance in the CCA subspace. We randomly selected half of the
labeled photos as the training set and the rest for testing. We consider the photos with no more than two tags as being lacking tags
and the top four tags in the ranked list will be selected to assign to
each photo. In the evaluation of the recommendation performance,
we kept top 20 tags in the ranked list as the ﬁnal recommendation.
For image feature, we used spatial-pyramid-based HSV histograms plus Gabor gradients for capturing both global and local
color and texture information of images, as in [2].
3.2. Evaluation Metrics
As mentioned above, the evaluation of the proposed method was
largely based on comparison of tag recommendation performance
before and after introducing our method, using the real dataset described above. We used the following two metrics in measuring the
accuracy of tag recommendation.
1. k-HitRate: we compared the recommended tags generated
by our approach to the tags provided by the original owners of the
photos. If one of the user tags is among the recommended tag list,

we call it a hit. And we use ≥k-HitRate for showing the performance, which gives the percentage of images out of all test images
that achieve k or more hits.
2. Tag-Recall: this is computed as the percentage of correctly
recommended tags (the number of hits for a given picture/ the number of original tags of the picture) in the testing set.
We emphasize that these metrics are in fact overly strict for evaluating tag recommendation since many recommended tags are actually deemed by humans as useful descriptors for the underlying
pictures, although the original users did not use them. That is, a
subjective evaluation would show the proposed approach with even
more gain. Nevertheless, subjective evaluation typically suffers from
limited maximal number of images that can be tested. Hence, for
simplicity and for performing a large-scale comparison, we adopted
the above objective metrics in our study.
3.3. Results and Analysis
We ran 10 trials with random split of the training and test sets in order
to avoid data selection bias. All experiments were based on these ten
trials from the dataset. Recommendation results based on the metrics
deﬁned above were obtained for the baseline CCA model and our active learning model. We also include the results from an intermediate
step using only tag augmentation, to demonstrate the effectiveness of
this strategy. Table 1 summarizes the average ≥k-HitRate for k up
to 3, for the above three cases. Due to the stringent metrics used,
the performance number degrades rapidly with increased k. However, the important observation here is the improved performance of
the proposed method over the baseline. Overall, the proposed active
learning approach achieved an improvement of 34.8% over the original in terms of the hit rate (11.5/33.1). Table 2 further shows the
average Tag Recall with an improvement of 28.9% over the original
in accuracy. Both results demonstrate that our proposed approach
improves the correlation model for tag recommendation.
Finally, to demonstrate the effectiveness of the proposed method
in incorporating unlabeled images for improving tag recommendation, we also tested with a variable percentage of the untagged data in
our experiments. The resultant performance metrics as a function of
the percentage are given in Fig. 2. From these plot, one can conclude
that, in general, incorporating untagged photos can indeed improve
the performance. The plots also indicate that there may be an optimal point at which the untagged photos will be maximally helpful.
Although exploring this issue is beyond the scope of the current paper, the observation is intuitively understandable in the sense that, if
too much incoherent data are included, the learned semantic model
can in fact suffer as it has to accommodate relationships among the
data that is not necessarily meaningful for tag recommendation.
Table 1. Average Tag hit rate on test sets
≥k-HitRate
k=1 k=2 k=3
Baseline CCA Model
33.1% 14.1%
7.8%
Model with Tag Augmentation 40.3% 16.7%
8.6%
Model with Active Learning
44.6% 18.1%
9.0%
Table 2. Average Tag Recall on test sets
T agRecall
P ercentage
Baseline CCA Model
20.1%
Model with Tag Augmentation
23.9%
Model with Active Learning
25.9%
4. CONCLUSION AND FUTURE WORK
In this work we formulated the problem of active learning for tag
recommendation, aiming at alleviating the practical challenge of in-

2872

Fig. 2. Result with varying percentage of the entire unlabeled set.
sufﬁcient annotation in on-line photos, and proposed a method to
select untagged photos to expand a given training dataset so as to
support the learning of semantic relationship between photos and
text tags via canonical correlation analysis. Experiments were performed using actual photos collected on-line and the results suggest
that the proposed approach is effective and promising.
Our future work will expand the current evaluation to include a
component utilizing subjective assessment of the usefulness of the
tags. We will also explore more comprehensive solutions to the optimization problem in Sect. 2.2, plus additional ways of measuring
the difference between two CCA spaces in active learning.
5. REFERENCES
[1] B. Sigurbjörnsson and R. Van Zwol, “Flickr tag recommendation based on collective knowledge,” in Proc. of the 17th ACM
Intl. Conf. on World Wide Web, 2008.
[2] Z. Wang and B. Li, “Learning to recommend tags for On-line
photos,” in Proc. 1st Intl. Workshop on Social Computing &
Behavioral Modeling, 2009.
[3] Y.M. Hironobu, H. Takahashi, and R. Oka, “Image-to-word
transformation based on dividing and vector quantizing images
with words,” First Intl. Workshop on Multimedia Intelligent
Storage and Retrieval Management, 1999.
[4] P. Duygulu, K. Barnard, J. De Freitas, and D. Forsyth, “Object
recognition as machine translation: Learning a lexicon for a
ﬁxed image vocabulary,” in Proc. of ECCV, 2002.
[5] N. Garg and I. Weber, “Personalized, interactive tag recommendation for ﬂickr,” in Proc.ACM conf. on Recommender
systems, 2008.
[6] S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, and
R. Harshman, “Indexing by latent semantic analysis,” J. of
American Society for Info. Sci., 41(6), 1990.
[7] T.K. Landauer,
Handbook of latent semantic analysis,
Lawrence Erlbaum, 2007.
[8] D.M. Nichols, “Implicit rating and ﬁltering,” in Proc. of 5th
Workshop on Filtering and Collaborative Filtering, 1997.
[9] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and J. Riedl,
“GroupLens: an open architecture for collaborative ﬁltering of
netnews,” in Proc. ACM Conf. on Computer Supported Cooperative Work, 1994.
[10] D.R. Hardoon, S. Szedmak, and J. Shawe-Taylor, “Canonical
correlation analysis: An overview with application to learning
methods,” Neural Computation, 16(12), 2004.

MULTI-TARGET TRACKING BASED ON KLD MIXTURE PARTICLE FILTER WITH
RADIAL BASIS FUNCTION SUPPORT
Jayanth Madapura †, Baoxin Li ‡

†

‡

Dept. of Electrical Engineering, Arizona State University, Tempe, AZ, USA
Dept. of Computer Science & Engineering, Arizona State University, Tempe, AZ, USA
{jayanth.madapura, baoxin.li}@asu.edu
ABSTRACT

The two major difficulties associated with practical real
time multi-target tracking are accuracy and speed. A new
technique is proposed for multi-target tracking based on
multi-modal particle filter with fast tracking capability and
improved accuracy. The speed in tracking is achieved by a
KLD sampling stage while the accuracy is improved by an
additional stage that uses radial basis functions (RBF) for
interpolating the sparse particles. Test results of the
proposed multi-target tracking approach on both synthetic
and real video data demonstrate the improved performance.
Index Terms— Mixture Particle Filter, Kullback-Leibler
Divergence, KLD Sampling, Radial Basis Functions.
1. INTRODUCTION
Accurate and fast multi-target tracking is amongst the
fundamental challenges in visual computing. Although
particle filters [1] are very successful and accurate in
tracking a wide range of challenging non-linear and nonGaussian models, they are poor in consistently maintaining
multiple modes in the target distributions that arises due to
presence of multiple objects. To address this issue of
maintaining the multi-modality of the target distribution, a
technique called mixture particle filter [2] was proposed
which models the target distribution as a non-parametric
mixture model by distinguishing each target distribution and
evolving each of them individually.
The number of particles used to estimate the state is a
deciding factor in the accuracy and speed of the particle
filter. The particle filter used in tracking the dynamic states
become numerically intensive as the number of particles
increases, although in general the accuracy of estimation
increases with the number of particles used. Issue of
improving accuracy of estimate without increasing number
of particles needs to be addressed. Finding a solution to this
problem becomes more imminent especially in the case of
mixture particle filter where multiple modes of the density
due to the underlying targets would naturally demand a
larger number of particles and hence lending mixture
particle filter computationally intensive and sluggish.

1-4244-1484-9/08/$25.00 ©2008 IEEE

725

This paper introduces a strategy by adopting KullbackLeibler divergence (KLD) sampling [5, 6] which makes the
mixture particle filter less intensive. Further, the accuracy is
preserved or improved through re-sampling the particles
based on radial basis functions (RBF) [4]. Synthetic data
and real video experiments are presented to demonstrate the
improved performance of the proposed technique.
The basic idea of the KLD based mixture particle filter is
presented in Section 2. Section 3 explains accuracy
improvement of the estimate with radial basis function
support. Then the proposed tracking approach is given in
Section 4. Results for synthetic data and real video data are
presented in Section 5. Section 6 concludes the paper.
2. KLD BASED MIXTURE PARTICLE FILTER
The basic idea behind the KLD based mixture particle filter
is invoking KLD sampling to adapt the cardinality of the
particle set during tracking.
In the mixture particle filter, individual modes in the
posterior distribution representing multiple targets are
clustered out using standard clustering techniques. At every
time instant the overall prediction distribution is obtained by
computing prediction distribution for each of the
components individually. Overall weight set is obtained by
normalizing over all weight sets of individual components.
To capture multi-modality the posterior is formulated as
an M-component mixture model as follows:
M
(1)
P x | z 
S P x | z 
t

¦

t

m ,t

m

t

t

m 1

where

Pm  x t | z t 

S m ,t

P  z t | x t  Pm  x t | z t  1 

³ P z

t

| x t , z t  1  Pm  x t | z t  1 d x t

S m , t  1 Pm  z t | z t  1 

M

¦S

n ,t 1

Pn  z t | z t  1 

(2)
(3)

n 1

To improve the speed performance by regulating the
number of particles in each time instant, KLD sampling
[5,6] is used so that the number of particles is made adaptive
such that the divergence between real posterior and
estimated posterior is bounded by a specified limit. The
particle subsets associated with each of the individual
targets are subjected to KLD sampling. This enhances the
speed performance of mixture particle filter as only minimal

ICASSP 2008

and optimum number of particles will be kept to ensure
desired accuracy.
3. MEAN ESTIMATION USING RADIAL BASIS
FUNCTIONS
A typical particle filter provides a way of performing
recursive Bayesian estimation using a set of particles
associated with the underlying system. The state set and its
corresponding weight set can be used to estimate the mean
state. In the proposed approach, at this stage a new set of
states within the range of existing state set of the particle set
is generated from the same proposal distribution used in
particle filter. Also, new states are chosen such that they are
concentrated around particles with large weights. The
cardinality of new state set is larger than the existing set.
The corresponding weights associated with the new state set
including the old state set can be obtained by interpolation
of existing particle set using radial basis functions [4] with
an appropriate basis function and smoothness. The
underlying posterior distribution s(x) is estimated from
particle weights W at distinct states given by particle states
X of the existing particle set.
Suppose g(x) is a real valued function, we try to
approximate g(x) by s(x) given values of g = {g1, g2,…, gN}
at discrete X = {x1, x2,…, xN}. The underlying function s(x)
is formulated as a radial basis function of the form,
N
(4)
s x
p x 
O\ x  x , x  \ d
i 1

i



i

N

i

It can be show that,

§ 
·
§ 
·
Prob ¨ X : E N  f ; X  EP  f  x   !G min ¸ ! Prob ¨ X : E M  f ; X  EP  f  x   !G min ¸
¸
¨
¸
¨
©
¹
©
¹

1

726

(9)

Bounds [7, 8] exits such that,

§ 
·
Prob¨ X : E N  f ; X   E P  f  x   !G min ¸ d q N ,G min
¨
¸
©§
¹ ·

Similarly, Prob ¨¨ X : E M  f ; X  E P  f  x   !G min ¸¸ d q M ,G min
©
¹



(10)







(11)

The bounds are given by,

q

1
­
°
2
G
4
D
°
m in
°
2
°°
2 DG m
in
®2e
°
° var f 
°
2
° DG m
in
°̄

 D ,G m i n 

B e r n o u lli B o u n d

C h e r n o ff

Bound

(12)

C hebychev B ound

Since M>>N we have,



where p is a polynomial of degree at most k, O i is a realvalued weight, |.| is the Euclidean norm,\ is an appropriate
basis function. Given the interpolant values g the weights
and the coefficients that give p in terms of the basis are
found such that s(x) satisfies,
(5)
s  x i  g i i 1, 2 , . . . , N
This gives a closed form representation of underlying
function s(x). In a similar fashion we try to formulate the
underlying posterior density given the weights and the
states, forming the particle set, representing g and X
respectively by s(x). Now from the closed form
representation of the posterior density s(x), weights
associated with the new state set, are generated as discussed
earlier, is obtained. Estimate of the mean state using the
new, larger particle set is better than that estimated with the
original particle set.
This can be shown with an example of a simple problem
of integration. Let P be the probability measure defined
over variable space x. We are interested in estimating
expectation of a function of interest f over x with respect to
the probability density P,
E p  f  x   ³ X f  x  P  x dx
(6)
Since the integral cannot be evaluated analytically, Monte
Carlo methods are used. An estimate is obtained by
generating N samples according to P. The empirical mean of
f is given as,
1 N
E N  f ; X :
¦ f ( x i ) w h e r e x ~ P  X  (7)

i 1







q M ,G min  q N ,G min



(13)

Then we have,

§
·
§
·
¨ 
¸
¨ 
¸
Prob ¨ X : E N  f ; X  EP  f  x   !G min ¸ !! Prob ¨ X : E M  f ; X   EP  f  x   !G min ¸
¨
¸
¨
¸
©
¹
©
¹

(14)
Thus, consistently the probability that the difference
between true mean and the mean estimated with N particles
exceeds a bound, is far greater than the probability that the
difference between true mean and the mean estimated with
M particles, obtained by RBF interpolation, exceeds the
same bound. From this we can infer that the mean estimated
by RBF interpolation is more accurate.
14

x 10

-3

Weights vs. States Plot

14

12

12

10

10

W eight

  ¦

i

M

Weight

 

Now a new set of variable samples is generated, distributed
according to P, with the same range as the existing set. The
cardinality of the new sample set along with the existing set
is M>>N. RBF interpolation is used on f to obtain the
underlying function s(x) of f from which the function values
are calculated for the new state set. The new empirical mean
of f obtained by using the values of the function f for the
new sample set along with that of the old is obtained as,

1 M
E M  f ; X :
¦ f ( x ) w h e r e M ! ! N (8)

8
6
4
2
-0.4

x 10

-3

RBF Interpolation of Weights

8
6
4

-0.2

0

State

0.2

0.4

2
-0.4

-0.2

0

State

0.2

0.4

Fig. 1. Weights vs. states plot of Initial Particle set (left) and
overall Particle set after RBF interpolation.
An illustration of RBF interpolation is shown in Fig 1.
The plot on left in Fig 1. shows the weights vs. states plot
with N elements of dual-object scenario and that on right
shows weights vs. states plot with M>>N elements obtained

by RBF interpolation with appropriate basis function and
smoothness from a simulation run of regular particle filter.
4.

PROPOSED

MULTI-TARGET

TRACKING

Summarizing the ideas put forth in Sections 2 and 3, we
propose a multi-tracking approach using Mixture Particle
filter which incorporates KLD sampling and RBF
interpolation technique. The proposed KLD mixture particle
filter with RBF support is presented in Table 1, with steps
explained in detail in the following paragraphs.
Step 1: KLD Sampling: The state space is downsized to
the calculated cardinality n F and samples are generated from
suitably chosen proposal distribution q which depends on
the previous state and present measurement.
Table 1. Proposed KLD-MPF with RBF support algorithm.
i
i
Inputs: S t 1
x t 1 , wt 1 | i 1, ..., N , observation zt ,

^

`

bounds e, G , minimum number of samples n Fmin
St

0, n

0, n F

0, k

0

do
n
n
1. Sampling the states xt  q xt | xt 1 , zt
2. Re-weighting using likelihood function



a.

w tn

n
wt

b. S m , t

n
wt 1 P

n

, w t

j
¦ w t
jI m
S m , t  1 w m , t

n
t

,
M
¦ S n , t  1 w n , t
n 1

3. Update particle size

nF

n
t

n
t 1


n
t

n
t 1

t

using KLD transformation

n
if xt falls into empty bin b then
k k 1

b = mark bin as non-empty
if n t n F
then
nF

end if
end if

min
k 1
2e

­
2
®1 
¯ 9( k  1)



½
z1G ¾
9( k  1)
¿

3

while ( n  n F and n  n F
)
min
4. Normalize the weights and form the state-space
w
t

w
t
n i and
¦ wt
i 1

xt

5. Estimation using radial basis function interpolation

^ xti , wti

RBF interpolated particle set St
xt

M i i
¦ wt x t
i 1

return St

St

|i

1, ..., M

`

where M>>N and wt normalized

6. Formulate the particle set

^

i

i

xt , wt

| i

1 , ..., N

(15)
KLD is always positive and zero if the distributions are
identical. Equation (16) gives the number of particles n that
guarantees with probability 1-į that KLD is less than e.
k 1 ­
2

®1 
2e ¯ 9( k  1)

3

½
z1G ¾
9( k  1)
¿
2

5. RESULTS

2

n 1

n

§ p( x) ·
¸
© q( x) ¹

¦ x p ( x ) log ¨

(16)
where z1-į is the upper 1-į quantile of the standard normal
distribution and k is the number of bins of the MC posterior
density estimate with support.
Step 4: The weights of the particles are then normalized.
Step 5: RBF interpolation of the particle set according to
Section 3.
Step 6: The particle set is formulated. Mean state can be
computed from this particle set with size M>>N using,
¦ iM 1 w ti X ti
Xt
(17)
Only initial N samples prior to RBF interpolation are
propagated in to the next iteration of mixture particle filter.

m

¦ w i
m ,t
i I m

w m , t

KLD ( p , q )

n

nI

z | x Px | x 
qx | x , z 
t

the accuracy and speed of the particle filter. Relation
between the number of particles used and the accuracy of
the estimates is determined by a metric called KullbackLeibler divergence which measures how best the MC
estimate of the posterior density can match the true
posterior density. The smaller the divergence value, the
better is the match. Thus, the problem of achieving an
optimal tradeoff between accuracy and speed reduces to the
problem of determining number of particles at each
iteration of particle filter such that, with a probability the
error between true posterior and posterior estimate is
minimum. Suppose that we have two distributions p and q,
KLD [5] is defined as,

`

Step 2: Re-weighting: The particle weights wt and the
M
mixture component weights 3 t ­®¯ S m , t ½¾¿
are calculated
m 1
using steps 2a. and 2b. The likelihood
is
P  zt | xt 
calculated using a suitable likelihood function )  xt , z t  .
Step 3: Particle Set Cardinality Calculation: The number
of particles used to estimate the state is a deciding factor in

727

The proposed approach is systematically evaluated and
compared using simulated data. The performance is also
evaluated on real video sequences.
Consider a point moving in 1-D. A true path of the point
is generated based on a non-linear motion model, and nonGaussian noise is then added to the actual path to simulate a
noisy measurement of the actual path. Then we evaluate our
KLD-MPF with RBF support algorithm and compare
against regular MPF [3] and KLD-MPF to track the true
path using the noisy path as measurement for two targets.
Fig. 2. illustrates this comparison. The speed and accuracy
performance from this comparison is tabulated in Table 2
which shows that KLD-MPF with RBF support outperforms
regular MPF with sufficient accuracy and speed
performance.
Next, we apply KLD-MPF with RBF support on real
video sequence. This is intended to verify the claim of gain
in speed of tracking while maintaining accuracy of multiple
tracked objects.

Regular Mixture Particle filter

KLD Mixture Particle filter with RBF
3

2.5

S ta te v a lu e

2.5

S ta te v a lu e

3

Actual value 1
Observation 1
Estimated value 1
Actual value 2
Observation 2
Estimated value 2

2

6. CONCLUSIONS

Actual value 1
Observation 1
Estimated value 1
Actual value 2
Observation 2
Estimated value 2

2

1.5

1.5

1

1

0.5
0

50

100

Time in sec

150

200

0.5
0

50

100

Time in sec

150

200

Fig. 2. Left: KLD-MPF with RBF support; Right: MPF.
Table 2. Time elapsed and MSE accuracy with KLD-MPF
with RBF support, KLD-MPF and regular MPF.
KLD-MPF with RBF
KLD-MPF
Regular MPF
Track1 Track2
Track1 Track2
Track1 Track2
Time
Time
Time
MSE MSE
MSE MSE
MSE MSE
(secs)
(secs)
(secs)
x10-4 x10-4
x10-4 x10-4
x10-4 x10-4
5.482 17
30 2.765 25
33 30.05 26
37
5.125 7.792 17 2.735 17
29 29.66 6.715
14
4.640
9
40 2.703 16
64 30.09 11
40
5.453 18
21 2.703 21
27 29.83 19
25
5.531 19
20 2.734 23
41 29.87 21
21
4.619 29
18 2.781 49
20 29.97 27
18
5.720 26
12 2.750 28
12 29.53 27
14

The performance of the algorithm on real video shows that
with an initial particle size of 2000 per object and 8 objects,
KLD-MPF with RBF support algorithm is consistently 5
times faster than regular MPF without loss in accuracy as
seen from Fig. 3. The elapsed time performance over 24
frame (1 sec) sequence is in Table 3. Tracking runs on real
hockey video and a dual-object surveillance video with is
shown in Fig. 3. Experiments were performed on Matlab on
a non-dedicated 1.7GHz Pentium processor with 1GB
RAM.
Table 3. Time elapsed for KLD-MPF with RBF support and
regular MPF on multi-object hockey sequence.
KLD-MPF RBF
Time (secs)
49.8270
49.7170
50.0640
52.0470

Regular MPF
Time (secs)
258.6860
260.9200
257.0480
258.1420

Speed Improvement
5.1968
5.2481
5.1343
5.0569

In conclusion, along with a tracking speed improvement the
KLD-MPF with RBF support outperforms regular MPF
with preserved or improved accuracy promising an efficient
real time tracking system. KLD sampling is introduced in to
the mixture particle filter which improves speed
performance tremendously. Further, radial basis function
tweak is used to improve the accuracy of the estimate
without any overhead in the propagation iterations of
mixture particle filter. Since the number of objects tracked
is many, the KLD-MPF with RBF support provides a
practical tracking solution with very good accuracy with
limited number of particles per object.
7. REFERENCES
[1] S. Arulampalam, S. Maskell, N. J. Gordon, and T.
Clapp, “A Tutorial on Particle filters for On-line Nonlinear/Non-Gaussian
Bayesian
Tracking”,
IEEE
Transactions on Signal Processing, Feb. 2002.
[2] J. Vermaak, A. Doucet, and P. Perez, “Maintaining
multi-modality through mixture tracking”, IEEE ICCV,
Nice, Oct. 2003.
[3] K. Okuma, A. Teleghani, N. de Freitas, J. Little and D.
Lowe, “A boosted particle filter: Multitarget detection
and tracking”, ECCV, 2004.
[4] Buhmann, M.D., “Multivariate cardinal interpolation
with radial basis functions”, Constr. Approx. 6, 1990.
[5] D. Fox, “Adapting the sample size in Particle filters
through KLD-sampling”, IJRR, 22, 2003.
[6] J. Madapura, B. Li, “3D Articulated Human Body
Tracking using KLD-Annealed Rao-Blackwellised
Particle Filter”, IEEE ICME, pages 1950-1953, July 2007.
[7] Y. Boers, “On the number of samples to be drawn in
particle filtering”, IEE Colloquium on Target Tracking:
Algorithms and Applications, 1999.
[8] M. Vidyasagar, “Statistical learning theory and
randomized algorithms for control”, IEE Control
Systems Magazine, Vol. 18, Issue 6, 1998.

Fig. 3. KLD-MPF with RBF support tracking on dual-object video (top) & multi-object hockey sequence (bottom).

728

SUPPORTING NAVIGATION OF OUTDOOR SHOPPING COMPLEXES FOR VISUALLYIMPAIRED USERS THROUGH MULTI-MODAL DATA FUSION
Archana Paladugu, Parag S. Chandakkar, Peng Zhang, Baoxin Li
Computer Science and Engineering
Arizona State University
{apaladug,pchandak,pzhang41,baoxin.li}@asu.edu
ABSTRACT
Outdoor shopping complexes (OSC) are extremely difficult
for people with visual impairment to navigate. Existing GPS
devices are mostly designed for roadside navigation and
seldom transition well into an OSC-like setting. We report
our study on the challenges faced by a blind person in
navigating OSC through developing a new mobile
application named iExplore. We first report an exploratory
study aiming at deriving specific design principles for
building this system by learning the unique challenges of the
problem. Then we present a methodology that can be used to
derive the necessary information for the development of
iExplore, followed by experimental validation of the
technology by a group of visually impaired users in a local
outdoor shopping center. User feedback and other
performance metrics collected from the experiments suggest
that iExplore, while at its very initial phase, has the potential
of filling a practical gap in existing assistive technologies for
the visually impaired.
Index Terms — Visual Impairment, Outdoor
navigation, Touch Interface, GPS, Design, User Study.
1. INTRODUCTION
Navigating an unfamiliar outdoor space is a challenging task
for visually impaired (VI) people. The use of technological
devices in conjunction with specialized training and
accessible architecture of buildings and roads has made this
task possible. However, there are still many factors that
make this task stressful and sometimes dangerous for VI
users. The situation is worsened especially for outdoor
shopping complexes (OSC), where the building layouts are
often cluttered with haphazard placement of stores, parking
spaces, walkways and driving roads, etc. Existing GPS
devices seldom provide useful navigation information for a
VI user in such environments.
The first problem towards building a system for OSC
navigation is the lack of awareness among developers
regarding the inadequacy of the current systems and the lack
of studies stating ways to overcome the existing information

deficit. This paper reports our study on the challenges faced
by VI users when navigating OSC and discusses the
necessary information required to bridge this gap. Secondly,
considering that the required information may not be readily
available from a single source. We propose a solution to
fuse information from multiple sources. Thirdly, once the
necessary information is in place, our application catering to
VI users presents a voice-over for visual data. We evaluate
these techniques and propose a system design based on our
user studies. This paper delves into these three aspects that
are required for the development of this system.
The focus on OSC was motivated both by their direct
relevance to the life of VI users and by the fact that
conventional map services do not support navigation in an
OSC. Typical OSC are fairly complex in layout and thus
serve well as representatives of general outdoor building
complexes. Besides building a functional iPhone app that is
freely available to our participants, the work contributes to
developing general principles and guidelines in designing
interface schemes and information representation paradigms
for supporting mobile-device-based navigation assistance for
people with visual impairment. The paper is organized as
follows. The literature is presented in Section 2. We present
the user survey on the current deficit and present our
assessment of required information in Section 3. We present
our proposed approach covering details of data fusion,
design of the interface and schema used for data presentation
in Section 4. Our evaluation is presented in Section 5. The
conclusion and discussion is presented in Section 6.
2. RELATED WORK
Navigation assistance for visually impaired users includes
accessible infrastructure, specialized orientation and
mobility training [1], and technological aids [2].
Technological aids, which are the focus of this work, may be
devices that give live help on site or tools that help the user
to prepare for the journey ahead of time. A review of
dedicated devices for on-site navigation help using GPS,
sonar, laser, RFID, etc. can be found in [3]. Recent years
have seen new systems that are built upon general-purpose

touchscreen mobile devices, moving away from the
requirement of dedicated hardware. The most dominant
players of this type are these three iPhone apps: Ariadne
GPS
(www.ariadnegps.eu),
MotionX
GPS
(gps.motionx.com)
and
Lookaround
GPS
(www.senderogroup.com). Google’s Intersection Explorer
allows touch-based walkable path exploration. Recent years
have seen add-on techniques to make the underlying maps
more helpful by using audio, haptic [8] and spatial tactile
feedback [7]. There are relatively fewer methods/systems for
supporting exploration and spatial learning of locations (to
help a user’s planning of a trip ahead of time). Examples
include tactile map systems [4] and verbal description
generation methods [5].
Despite the existence of the aforementioned work, the
reality remains that none of them can support navigation in
an OSC setting largely due to the lack of adequate mapping
information [6], even with the newer type of systems like
Ariadne GPS. A solution to this deficit is to obtain
information for various sources and fuse them before
integrating them into the application. Some prior work on
map registration and geo-spatial data conflation addresses
the problem of combining data from different sources to
obtain the information for the applications that require
additional geo spatial information. A method was proposed
in [9] to align successive images taken aerially with an
overall map of the region using feature-based registration
and mosaicking techniques. Linear features like active
contours to register images were proposed in [13]. A
technique that uses feature matching using RMS estimation
on affine transform was proposed in [14]. Street data were
used as control points and then cross correlation was
employed to obtain matching in [15]. None of these methods
are directly applicable for the following reasons. Firstly,
these methods have prior knowledge of the structure of their
maps and available information in their maps. Our data
fusion uses information obtained from standard maps as well
as shopping directories crawled from the web which can be
viewed as pseudo-maps. Secondly, detecting control points
now becomes a different task due to the dissimilarity
between two images on a lower-level. The data fusion for
our application needs to be flexible enough to work with a
reasonable accuracy for images obtained online with large
variation in their structure. Further, current design of some
of those systems is essentially based on the concept of
making the underlying information (targeted at sighted
users) more accessible to VI users, without taking into the
real needs of the VI user in navigating a place.
The importance of having an integrated mapping system
is evidenced by the fact that Google and Bing maps are
working on integrating floor plans of malls into their map
services. Google Maps has a user interface [17] published
for a user to identify three control points and scale the map
on top of their street view map for user input. Bing maps has

also handled similar problem [18] without the need for
human intervention on popular shopping malls. Still, these
map services are unable to provide navigation assistance
inside OSC, and there is a need for a framework with data
fusion to truly build an application for a VI user in
navigating OSC, which is the focus of our study.
3. USER SURVEY ON CHALLENGES FACED
We started an exploratory study with a group of visuallyimpaired smart-phone users in order to understand the
challenges and identify deficits of existing solutions.
Feedback from our participants and online surveys of
visually-impaired communities helped us to conclude that
that the iPhone appears to be the most used and preferred
smart phone among users with visual impairment. Most of
the users are familiar with the voice-over feature on the
phone. Three of the five users we interviewed reported that
they collect information from the store before planning their
visit. The information asked for includes directions from
transit stop to the store entrance, landmarks that can identify
the store, etc. The users reported a significant ease in
navigation using GPS devices when walking on mainstream
streets as opposed to walking inside an OSC. All our
participants reported that seeking help from nearby humans
would be their final resort. A summary of the key findings in
the case study is given below:
•
•

•

•
•
•

•
•

An on-demand description of their surroundings is
always helpful for them to orient themselves.
Description of the surroundings can be effectively done
in terms of egocentric or allocentric methods. The
usability and preference of this description varies
widely from user to user and by location.
Some users prefer to have the description of distances
given in steps, as opposed to feet; low-vision users may
still enjoy the availability of a zoomed map.
Tactile landmarks are preferred to assert the location.
Using angles for direction is not usable. But the users
are familiar with terms like “diagonally right/left”.
It is not a good idea to direct them to walk through
parking lots. It is often unsafe to do so and would
involve hitting the cars with the cane.
Longer but safer routes are preferred over shorter routes
with parking lots or obstacles.
Extra information: Restroom location, traffic conditions
on streets encountered, user created markers for future
reference, etc. would be great add-ons.

Part of the study involved asking our participants to
navigate a chosen local shopping complex with the help of
the two aforementioned iPhone apps. It is to be noted that
these apps helped only for navigating the major streets
bordering the shopping complex but failed to provide much
assistance for navigating the complex itself.

3.1. Design Guidelines for Building Apps for OSC
Based on our case study, we established a set of guidelines
for developing an effective iPhone app in addressing the
deficits of existing solutions. We summarize them below:
Avoid additional screens as much as possible: A VI user
would face frustration in using finger gestures to get back
and forth in a multi-layer menu and thus a flat structure
should be used as much as possible in the interface.
Less is more: Our users disliked navigating through a page
filled with too many buttons. They asked for a few
functionalities that convey a lot of information, during our
case study questionnaire.
Layered information delivery: Users with varying abilities
will need different amount of information. Having
information interlaced with gestures is preferred, so
additional information is presented only on-demand.
Orientation and Mobility training: Most of the blind users
have undergone the O&M training. The app needs to be
consistent with the protocols in presenting the information.
Supporting user notes: Every user from our case study
picked up some different cues about his/her surroundings. A
mechanism for the user to record his/her own markings
would enhance the usefulness of the application.
Supporting user customization: Low-vision and completelyblind users have different requirements. Users may prefer
measuring distance in different ways. The app should allow
some user-level customization to support such features.
4. PROPOSED DESIGN
We now present the design of a novel iPhone app that aims
at addressing the challenges faced by VI users in navigating
OSC. Largely based upon the design principles which were
derived from the exploratory study, the proposed design and
implementation attempts to overcome the challenges from
the following four aspects: an information fusion technique,
an information representation structure that is appropriate
for mapping-related tasks in an OSC setting, an intuitive
interface and interaction scheme, and the support for usercustomization to cater to individual needs of the users.
These are illustrated in Figure 1.
4.1. Tiered Information Creation
The major part of the problems faced when using map-based
technologies for navigation in OSC is the lack of required
information. Publicly available map services such as Google
maps do not have the desired level of details for typical
OSC. However, most shopping centers maintain and publish
maps with rich annotations. Also, a sighted volunteer may be
able to label a satellite image of a shopping complex as to
where are the parking lots etc. Finally, a user while using the

Fig. 1. The tiered information representation scheme used to
support the application.

app, may want to insert his/her notes to a location.
Considering all these possibilities, we adopt a tiered
representation for all the mapping information. Figure 1
illustrates how this is currently implemented. In Figure 1, the
base layer corresponds to the map that is typically available
from a GPS system, the second layer is the layout map given
by the shopping center, the third layer illustrates an image of
the same locale with additional labels, and the fourth layer is
used to store user notes.
4.1.1. Base Layer Information
As seen in Figure 1, the base layer contains information
available in the typical mobile applications using
Google/Bing/Apple/other maps. This information is
sometimes sparse, and from our studies, often inaccurate in
an OSC setting. The locations of individual stores and bus
stations are, more often than not, inaccurate and haphazardly
placed. Using this information for VI users is often
dangerous for this reason. In our scheme, we employ
additional sources to obtain this kind of information with
reasonable accuracy and map it on top of the base layer.
4.1.2. Second Layer: Information from the Web
The most vital information when navigating an OSC is the
location of the stores inside the mall. This is the kind of
information that is not available on Google maps. One way
to obtain this sort of information is to devise a method to
integrate Google maps with the store directories available on
the website of OSC. This can be viewed as a map-to-storemap registration problem. We try to register the shopping
mall directory to its corresponding Google map. The
purpose is two-fold. Shopping directory has much more
information as compared to maps and by overlaying
shopping directory onto Google maps, we can still retain the
GPS coordinate information.

Control point detection is a major step in any
registration problem. We propose the use of shop centers as
the control points as the shapes of stores in both Google
maps and shopping directory have high-level similarity. We
also employ road and parking lot detection but are not used
for extracting control points since shopping directory may
not have them. Firstly, we detect yellow-colored major roads
and orange-colored freeways from Google maps by simple
color segmentation (Fig. 2). Black-colored text labels on
Google maps are detected by using the same principle.
Roads in Google Maps always have text labels on them and
they have a lower-limit on their width. We use this fact to
distinguish parking lots from roads. The roads are detected
by using region-growing image segmentation technique in
combination with distance transform. Distance transform
allows us to monitor the width of the road. If it falls below
the predefined minimum width or if there is no “white” road,
then the region-growing stops. Text labels act as seed points
while performing region-growing.

(a)

(b)

(c)
Fig. 3. Control point detection in (a) Google maps and (b)
shopping mall directory (c) Registration of Google map and
shopping directory.

Our problem inherently has lot of outliers because of “extra”
information present in the shopping directory and thus we
need an algorithm, which can perform better in presence of
noise and outliers. Therefore we propose the use of
Coherent Point Drift (CPD) algorithm proposed in [16]. It
considers the registration problem as probability density
estimation problem. It represents the first data point set by
Gaussian Mixture Model (GMM) centroids and tries to fit
the second data point set by maximizing the likelihood. The
results obtained are indeed promising and they are shown
below (Fig. 4):

Fig. 2. Robust street detection.

The next procedure involves detecting stores in Google
maps and the corresponding shopping directory. In Google
maps, each shop has a label associated with it and it also has
a border. Thus we can detect stores by extracting the entire
portion which has the same color as the background color of
the text label. The convex hull of the portion is calculated
and those having arbitrary shape (not resembling to
rectangle) are discarded. This is done based on the
assumption that each shop has a regular shape. To detect
stores from shopping directory, we ask user to select points
which represent colors of the stores in the shopping
directory. After this, the stores are detected by simple color
segmentation. The control points in the two binary images
are the centroids of each disconnected component (store).
The centers are overlaid onto the original image for
visualization purpose in Fig. 3.

Fig. 4. Registration of two sets of control points using CPD.

4.1.3. Additional Annotations
According to our user consensus, they would like to know
the location and the extent of parking lots, walkways and bus
route information. This kind of metadata is not available on
any mapping service. But the imagery of the maps shows a
clear distinction that can be utilized to demarcate these
areas. In our scheme, we do parking lot and walkway

detection. The parking lot detection is carried out as
described in the previous section. The walkways in front of
stores can be detected by using the grey color gradient used
consistently at this scale. We add bus route information
obtained from the metadata to obtain all the additional data
requested in the user survey.
4.1.4. User Defined Tags
In spite of providing additional user friendly information,
there were some elements of each location that the visually
impaired users highlighted as useful in navigation. This
information ranged from the presence of water fountains,
notes on location of shrubs on the walkways, narrowness of
some passages, etc. Such information could only be gathered
from the users themselves. We collected this information
from our participants, orientation and mobility instructors
and spouses of VI participants and geo tagged this
information as a layer.
4.2. Supporting User Customization
The necessity of supporting user customization has been
concluded earlier. In our current implementation, we support
the following three types of customization. The first
customization provided is the user preference on distance
metric. Some users preferred the usage of steps and blocks
to usage of absolute distance in terms of feet. We introduce
a user-based calibration feature that calculates the step-tofeet ratio. The user is asked to walk 20 steps prior to the visit
in a familiar environment to allow for calibration. The
second customization is the method used to convey
direction. Users prefer egocentric or allocentric method of
description, depending on the location and complexity of the
surroundings. The third customization provided is based on
the level of vision of the user. The totally-blind users
interact with the system using the predefined gestures and
through voice-over output. The low-vision users can have
the additional freedom of interacting with a spatial zoomable map and larger font sizes.

Homepage: The homepage of the app consists of an entry
into all the possible functions of the app. We adhered to the
design paradigm inferred from the user study, stating the
user preference of not having too many screens to navigate
and having a few necessary functions on the screen. We
have designed our homepage in such a way that the user can
obtain all the important information by staying on the
homepage. We interlaced various gestures to help access
additional information. Figure 5 shows a screenshot of our
homepage and an overview of all the functions.
Where am I? “Where am I” is a popular functionality
provided by most of the existing GPS applications. A sample
result for this function can be illustrated as follows: “Facing
North near 661 Meadow Avenue”. Once inside a shopping
space, a description in terms of an address is no longer
relevant information. We modify this functionality to make it
more informative and convey necessary information in a
tiered manner. When a user double taps on the “Where am
I?” button, our application reads out the orientation of the
user, the closest landmark and the nearest landmarks in
either directions as shown in Figure 5. He repeats this
gesture to listen to this information again. The user has an
option to ask for more information after he listens to this
information. A pre-assigned gesture (3 taps on the iPhone)
provides information about the nearest streets and any user
tagged notes if they exist around this location. He repeats
this gesture to listen to this information again.

4.3. An Intuitive Interface for Supporting Necessary
Functionalities
Based on our case study, user input and our understanding of
the existing GPS devices, we propose the following interface
to help users navigating OSC. Our application uses iPhones
accessibility mode and uses the standard voice-over
gestures. A user can scroll through the buttons without
knowing their spatial locations, using one finger swipe. A
double tap anywhere on the screen selects an item. We
refrain from introducing too many additional gestures to
make the app as simple to use as possible. The interface is
elaborated below.

Fig. 5. An overview of iExplore’s homepage and a sample
output for each of the proposed functions can be seen above.

Points of Interest: Most applications and GPS devices
support points of interest, where a predefined number of
landmarks around the current location are listed out to the
user in terms of distance and direction. We include this
feature in our application, except that the information
associated with this button is according to the tiered
representation discussed above.

Where is my destination?: The directions provided by most
applications are hard to use for VI users. We propose a
scheme to provide blind-friendly directions. Once inside an
OSC, the user can be standing inside a parking lot, a store or
at a major landmark. Our description of the destination
location takes into account that users do not like walking
through the parking lots, considers the safety quotient of
streets, includes description of places in-between in terms of
stores lining the route and major streets on the way. The user
inputs the name of the store or landmark he is interested in
using the speech input feature provided. A verbal description
(as seen in Figure 5) is then generated tailored to the user
preference of egocentric or allocentric methodology, in
terms of their distance preference. We use Dijkstra's
algorithm to compute the shortest path among the walkable
options. Using this path, our description takes into account
relative positions of the landmarks and additional tagged
information about the surroundings
5. EVALUATION
The proposed design has been implemented as an iPhone
app called iExplore, which was tested by four participants
for navigating a local outdoor shopping complex. We
summarize some major outcomes of the tests below.
Although this method is applicable to any outdoor shopping
complex, for the sake of user studies, we picked one local
mall and conducted our experiments at this location. The
statistics of this experiment are presented below. The dataset
consists of 6 outdoor shopping complexes. We used the
additional features of the API to obtain street map with and
without labels. We tested the accuracy of map registration
and parking lot detection on this dataset. The dataset was
chosen on the basis of availability of store map directories
on their websites.
5.1. Accuracy of the Map Registration Technique
After registering Google maps to the corresponding
shopping directory, we develop an evaluation scheme to
validate the proposed method. It is described as follows: We
detect stores in Google maps as well as in the registered
image. Now, we estimate the percentage of pixels of the
Google map which overlap with the pixels of registered
image. This also validates our shop detection algorithm.
Figure 6(a) and 6(b) illustrate the result of shop detection
onto the Google map and the shopping directory. There is a
66.45% overlap in the area marked by our approach when
compared to the original store directory and the ground
truth. The parking lot detection was done nearly perfectly.
5.2. User Feedback on Overall System Design
Upon loading the app on the user’s iPhone, we asked them
to take time and get familiar with the buttons and tabs on the

(a)

(b)

Fig. 6. Store detection in (a) Google map and (b) Registered
image

home screen. The users were asked to think aloud (make
explicit comments) while using the app. They were not
provided any additional training on the usage. After the
users were familiar with all the functionalities, they were
asked questions about the interface, its usability, the
intuitiveness of each button, easiness of navigating the
buttons, etc. The users were encouraged to ask us questions
to clarify any aspect they found confusing. One key
observation every participant made was that the interface has
very few selectable items on the screen. They were able to
summarize the key components upon closing the app. They
also reported to have received more useful information than
they would have hoped for (compared with other systems)
from each functionally buttons.
5.3. Testing the Proposed Functionalities
The users were taken back to the testing site, where we
conducted our initial case studies. We started at the bus
station for uniformity and to simulate a real-time scenario.
The users were asked to test each of the functionalities and
were asked to walk around. We tagged them with a
volunteer for safety reasons and the volunteer collected
feedback from the users. The user was asked to change the
settings to his preference of distance, terminology for
directions, etc. Once done, the user is asked to go back to
the home screen. He is asked to find the “Where am I”
button and access the information. We then asked the user to
point out the direction using their hand, towards one of the
landmarks mentioned. This test was aimed at testing the
accuracy of information conveyed as well as user
understanding of the verbal descriptions provided. The users
got their directions correctly all the time in the test.
5.4. Evaluating Navigation Support
To evaluate the support provided by iExplore for navigation,
we defined preset routes with 2 turns and asked the users to
use the app to find the destination. If the participant took a
wrong turn, we recorded a miss for that segment of the
journey and let the user turn around. The user was asked to
use the app to gain a better understanding of his
surroundings. The system failed to orient the user in the right
direction once and one out of the four users asked for

assistance once during the experiment. At the end of the
stretch, the user was asked to describe his surroundings and
the relative positions of the store he walked by. This test
aimed at validating our assumption that the users liked
exploring and gaining information about their surroundings.
We did not intend this test to be a memory game, but most
users were able to figure out the relative positions of the
stores they walked by nearly perfectly.
6. CONCLUSIONS AND DISCUSSION
We developed an iPhone application, iExplore, which
provides assistance in navigating an outdoor shopping
complex, for VI users. This application has the potential to
turn highly inaccessible locations (like OSC) into blindfriendly and navigable locations. The current version is not
yet a perfect solution that solves every issue, but the users
from our case study indicated high confidence in exploring
the area with the help of this application and relying less on
asking for help. We also put forth a summary of some design
principles that we learnt through our studies. We then
presented a way to obtain the relevant information from
multiple sources and use a tiered presentation to effectively
present the information. Our study was conducted and the
proposed app was tested in one outdoor shopping location.
Our framework has the following limitations. Firstly, map
registration methods on unseen, non-standardized images
provide a relatively low accuracy. But using non-rigid
registration helps get an approximate location for each store
which can be utilized to give the users a proximate location.
Secondly, the store directories available online for download
often contain noisy information and legend information that
is hard to work with. We worked around this problem by
picking our dataset to have moderately complex maps. Our
future work will include a detailed study for various layouts
and locations. Thirdly, there is a huge element of user
subjectivity in our design. We plan to overcome this by
increasing the number of subjects in our experiments and
testing the application in different OSC locations.
Acknowledgement: The work was supported in part by a
grant (#1135616) from the National Science Foundation.
Any opinions expressed in this material are those of the
authors and do not necessarily reflect the views of the NSF.
7. REFERENCES
[1] Blasch,B., Wiener, W., Welsh, R. Foundations of
orientation and mobility, 2nd Ed. AFB Press, New
York, NY, USA, 1997.
[2] Giudice, N.A., & Legge, G.E. (2008). Blind navigation
and the role of technology. In A. Helal, et al (Eds.),
Engineering handbook of smart technology for aging,
disability, and independence (pp.479-500): John Wiley
& Sons., 2nd Ed.

[3] N. Fallah, L. Apostolopoulos, K. Bekris, E. Folmer, The
user as a sensor: Navigating users with visual
impairments in indoor spaces using tactile landmarks.
Proc. CHI 2012.
[4] Z. Wang, B. Li, T. Hedgpeth, T. Haven. Instant tactileaudio map: enabling access to digital maps for people
with visual impairment. ACM SIG ASSETS 2009.
[5] Kalia, A.A., Legge, G.E., Roy, R., & Ogale, A. (2010).
Assessment of Indoor Route finding Technology for
People who are Visually Impaired. Journal of Visual
Impairment & Blindness, 104(3), 135-147.
[6] Kitchin, R. M., Jacobson, R. D. Techniques to collect
and analyze the cognitive map knowledge of persons
with visual impairment or blindness: issues of validity.
J. of Visual Impairment & Blindness (1997), 360-376.
[7] Yatani, K., Banovic, N., Truong, K. N.. SpaceSence:
Representing Geographical Information to Visually
Impaired People Using Spatial Tactile Feedback. Proc.
CHI 2012, ACM (2012),415-424.
[8] Yang, R., Park, S., Mishra, S. R., Hong, Z., Newsom,
C., Joo, H., Hofer, E., Newman, M. W. Supporting
spatial awareness and independent wayfinding for
pedestrians with visual impairments. Proc. ASSETS
2011, ACM Press(2011), 27-34.
[9] Lin, Y., Place, W., & Angeles, L. (2007). MapEnhanced UAV Image Sequence Registration and
Synchronization of Multiple Image Sequences.
[10] Wang, C., Stefanidis, A., Croitoru, A., & Agouris, P.
(2008). Map Registration of Image Sequences Using
Linear Features. Photogrammetric Engineering &
Remote Sensing, 74(1), 25–38.
[11] Zitová, B., & Flusser, J. (2003). Image registration
methods: a survey. Image and Vision Computing,
21(11), 977–1000.
[12] Chen, C.-C., Knoblock, C. a., & Shahabi, C. (2007).
Automatically Conflating Road Vector Data with
Orthoimagery. GeoInformatica, 10(4), 495–530.
doi:10.1007/s10707-006-0344-6
[13] Wang, C., Stefanidis, A., Croitoru, A., & Agouris, P.
(2008). Map Registration of Image Sequences Using
Linear Features. Photogrammetric Engineering &
Remote Sensing, 74(1), 25–38.
[14] Roux, M., Nationale, E., Ima, D., & Cedex, F. P.
(1996). Automatic registration of SPOT images and
digitized maps, Ima, 46, 8–11.
[15] Chen, C.-C., Knoblock, C. a., & Shahabi, C. (2007).
Automatically Conflating Road Vector Data with
Orthoimagery. GeoInformatica, 10(4), 495–530.
[16] Jian, Bing, and Baba C. Vemuri. "Robust point set
registration using gaussian mixture models." Pattern
Analysis and Machine Intelligence, IEEE Transactions
on 33.8 (2011): 1633-1645.

ON THE GENERALITY OF NEURAL IMAGE FEATURES
Ragav Venkatesan, Vijetha Gatupalli, Baoxin Li
School of Computing Informatics and Decision Systems Engineering,
Arizona State University, Tempe, AZ, USA.
ABSTRACT
Often the ﬁlters learned by Convolutional Neural Networks (CNNs) from different image datasets appear similar.
This similarity of ﬁlters is often exploited for the purposes of
transfer learning. This is also being used as an initialization
technique for different tasks in the same dataset or for the
same task in similar datasets. Off-the-shelf CNN features
have capitalized on this idea to promote their networks as
best transferable and most general and are used in a cavalier
manner in day-to-day computer vision tasks.
While the ﬁlters learned by these CNNs are related to the
atomic structures of the images from which they are learnt,
all datasets learn similar looking low-level ﬁlters. With the
understanding that a dataset that contains many such atomic
structures learn general ﬁlters and are therefore useful to initialize other networks with, we propose a way to analyse and
quantify generality. We applied this metric on several popular character recognition, natural image and a medical image
dataset, and arrive at some interesting conclusions. On further
experimentation we also discovered that particular classes in
a dataset themselves are more general than others.

ܵ
‫ܦ‬ସ

‫ܦ‬ଷ

‫ܦ‬ହ

Fig. 1. Thought experiment to describe the dataset generality.
S is the space of all possible atomic structures, D1 − D5 are
the atomic structures present in respective datasets.

for some atomic structure in the data itself. Each layer is a
mapping form the previous layer to the next layer that is constructed using combinations of these atomic structures in the
ﬁrst layer in order to minimize a cost.

1. INTRODUCTION

Let us ﬁrst deﬁne atomic structures to be the forms that
CNN ﬁlters take by virtue of the entropy of the dataset.
At the ﬁrst layer of a CNN, these might be the edge and
blob detectors. Consider the following thought experiment:
Let’s assume that all possible atomic structures reside in
an universe S. Suppose we have a set of three datasets
D = {D1 , D2 , D3 } and D ∈ S. Consider the system in
ﬁgure 1. One would now recognize that D1 is a more general
dataset with respect to D2 and D3 . It is so because, while D1
contains most of the atomic structures of D2 and D3 , the latter does not contain as many atomic structures of D1 . While
this analysis is simpliﬁed for one layer, in typical CNNs, coadaptation plays a major role in the learning of these atomic
structures. Therefore, generality as deﬁned by the overlap of
areas in a layer-wise Venn diagram is impractical to obtain.

Neural networks, particularly CNNs have broken all records
recently in the computer vision research area. Large networks
are often trained with large number of data samples to achieve
good accuracies [1, 2]. While using CNNs as feature detectors has long been studied (e.g.,[3]), recent years large networks trained with large-scale datasets (e.g.,[4]) have started
being used as “off-the-shelf” tools for feature extraction. Still,
some studies show that a few (< 1%) nodes are all that are actively contributing to classiﬁcation [5]. While it is reasonable
to expect edge detectors and Gabor-like features in the lowerlevel ﬁlters and more sophisticated concepts at the higher levels, it is not clear as to why these ﬁlters adapt themselves
in this manner. What is fairly clear though is that different
datasets result in different sets of ﬁlters that are similar if the
datasets are similar. It is only natural to ask, what role does
the images themselves play in such ﬁlters being learnt and
how they compare with ﬁlters learnt from another dataset. In
this paper we take the view that the ﬁlters learnt by networks
when trained using a particular dataset represent the detectors

,(((

‫ܦ‬ଵ

‫ܦ‬ଶ

In this paper we postulate that, the generalization performances of CNNs on one dataset re-trained on a network initialized by the weights of another network trained using another dataset, could be used to derive generality between the
said datasets. We call this process of pre-trained initialization



,&,3

as prejudicing. By prejudicing on the ﬁrst dataset, we froze1
and unfroze layers and retrained the networks on the second
dataset. When the prejudicing dataset is more general than
the re-train dataset, the classiﬁer generalizes better.
We developed a generality metric by comparing the gain
in performances of networks of various obstination. Using
a generality such as the one proposed, it becomes clear as
to what kind of datasets are to be used to prejudice CNNs
with, during transfer learning. We also discovered that samples with particular labels within a dataset alone are general
enough that if we begin by training the network on only those
and then moved on to the rest of the classes, we were able
to learn the rest of the dataset with considerably less training samples while achieving comparable generalization performances. This study led us to two major research insights:
1.If one has very few data to learn from, which other dataset
is better to prejudice the network with? 2. Among the various
classes during the training procedure, if we prejudice with a
certain general set of classes ﬁrst and then move on to others later, generalization to all classes, even for those with few
samples is better.
The rest of the paper is organized as follows: section 2
discusses related works, section 3 presents the design of
our experiments, section 4 shows some results on the coreexperiment and section 5 provides concluding remarks.

trained network and uses this as prejudice, no information is
actually being transferred in terms of actual ﬁlters. Ergo, this
work, while interesting, also doesn’t help in understanding
generality of the data itself in a more direct manner. Some
of the claims made by this article though were indirectly and
independently veriﬁed by us through our generality results.
The basic claim of their work is that among only a handful
of classes, there is enough knowledge to generalize to other
classes. Unless there exists some generality between classes,
training on particular classes will not have been representational enough for the other classes. We directly verify this by
showing that some classes alone have a high generalization
to the rest of the dataset and make a similar conclusion from
an entirely independent direction of research.
3. DESIGN OF EXPERIMENTS
We designed these experiments across three broad categories
of datasets:1. Character datasets that included MNIST [9],
MNIST-rotated [10], MNIST-random-background [10], MNISTrotated-background [10], Google street view house numbers [11], Char 74k English [12] and Char 74k Kannada [12]
2. Natural image datasets that includes Cifar 10 and Caltech
101 [13, 14] and 3. Natural images against medical images
that included in addition to Caltech 101 a Colonoscopy video
quality dataset. We leave it to the reader to ﬁnd for themselves details about the datasets from the original articles.
Details of the datasets used, network architecture and other
logistical details are discussed in the supplementary. The
experiments were designed using Theano, and the code is
available here [15].
Among the various datasets used, it is natural to expect
any network trained on MNIST to contain simpler ﬁlters
than MNIST-rotated. This is because, while MNIST-rotated
contains many structures from MNIST, due to the rotations,
MNIST-rotated will contain additional structures that require
the learning of more complicated ﬁlters. A network trained
on MNIST-rotated on its ﬁrst layers will be expected to additionally have ﬁlters for detecting sophisticated oriented edges
than for MNIST. This would mean that prejudicing a network
with MNIST to then re-train MNIST-rotated is much less
helpful than vice versa. A network prejudiced with a general enough dataset is better to be retrained for it generalizes
easily. A prejudice must come from a more general dataset
if a prejudice transfers positive knowledge as shown in their
generalization performances. We use this simple intuition
to argue that MNIST-rotated is a more general dataset with
respect to MNIST.
Our basic experiment was conducted between pairs of
datasets Di and Dj . Firstly, we train (prejudice) a randomly
initialized network with dataset Di . We call this network
n(Di |r) or the base network (r implies random initialization). We then proceed to retrain n(Di |r) as per any of the
setup shown in ﬁgure 2. nk (Dj |Di ) would imply that there

2. RELATED WORK
One prior art closely related to this article is [6]. In that article, the authors considered two tasks A and B that were
essentially 500 classes each from the Imagenet dataset [7].
They experimented by obstination and prejudice, the speciﬁcity of each layer and their contributions to the overall performance. They also showed that networks working on similar tasks had a high memorability and that co-adaptation of
layers increased the generalization performance. While this
analysis is interesting, it was performed on only one dataset:
Imagenet. By design, the networks were forced to learn very
general ﬁlters, so as to be best transferable. Also, the paper
analysed the transferability of the feature extractors from the
perspective of the networks in terms of their fall in generalization performance. This analysis was not catered to the
dataset’s perspective, which is that the ﬁlters learned are a
property of the dataset being trained on.
Another closely related work is the work by Hinton et
al, on the transfering of softmax layers [8] . Here the authors suggest that among the various classes in a dataset,
there exists some amount of generalization knowledge that
could be transferred. They showed that the network learns the
relationship between the classes even though not explicitly
trained [8]. Although the author retrains an entire network
that is randomly initialized using the softmax outputs from a
1 Obstinate layer or freezing implies that the weights were not changed
during backprop. The layer remains prejudiced.



0

0

0

0

1

1

1

1

9

9

9

9

Fig. 2. Protocol of obstination: From left to right, all layers frozen, one, two and three layers unfrozen. Green represent
unfrozen and red represent frozen. Note that the layers are always unfrozen from the end and that the softmax layer is always
unfrozen and randomly initialized. This should be generalized similarly for more than three layers also.
are k degrees of freedom, or to be precise, k layers of ﬁlters
that are allowed to learn by dataset Dj that is prejudiced by
the ﬁlters of n(Di |r). nk (Dj |Di ) has N − k obstinate layers
that carries the prejudice of dataset Di , where N is the total
number of layers. Note that more degrees of freedom implies
that the network is less obstinate to learn. Also note that these
layers can be both convolutional or fully connected neural
layers. Any idea expressed here can be extended to any type
of parametrized layers. In fact while we perform operations
such as batch normalizations, we even freeze and unfreeze
the α and β of batch norm [16].
Suppose the generalization performance of n(Dj |r) is
Ψ(Dj |r) and the generalization performance of nk (Dj |Di )
is Ψk (Dj |Di ). Dataset generality of Di with respect to Dj at
the layer k is given by,

Retrainedmnist-rotated-bg for different bases

1.1
1

Generality

0.9
0.8
0.7
mnist-rotated-bg
MNIST random background
MNIST
MNIST rotated background
NIST Special Dataset-19
Google Street View House Numbers
Char 74k English
Char 74k Kannada

0.6
0.5
0.4
0.3

0

1

2

3

Number of Layers Unfrozen
Caltech 101 vs. Colonoscopy

1.6
1.4

Generalities

1.2
1
0.8
0.6

Ψk (Dj |Di )
gk (Di , Dj ) =
Ψ(Dj |r)

caltech 101 on caltech 101
caltech 101 on colonoscopy
colonsoopy on colonoscopy
colonoscopy on caltech101

0.4

(1)

0.2

This indicates the level of performance that is achieved by
Dj using N − k layers worth of prejudice from Di and k layers worth of features from Di combined with k layers of novel
knowledge from Dj together. gk (Di , Dj ) > gk (Di , Dl ) indicates that at k layers, Di provides more general features to Dj
than to Dl . Conversely, when initialized by n(Di |r), Dj has
an advantage in learning than Dl . Note that, gk (Di , Di ) ≥
1 ∀k. gk (Di , Dj ) for i = j might or might not be greater
than 1.
Di and Dj need not be entire datasets but can also be just
disjoint class instances of the same dataset that is split in two.
For instance, we divided the MNIST dataset into two parts.
The ﬁrst part contained the classes [4, 5, 8], the rest were contained by the second part. We performed the generality experiments with MNIST[4, 5, 8] as base. We re-trained this prejudiced network using the second part with the same experiment
design as above. We repeated this experiment several times
with decreasing number of training samples per-class in the
retrain dataset of MNIST [0, 1, 2, 3, 6, 7, 9]. The testing set
remained the same size. We created seven such datasets with
7p, p ∈ [1, 3, 5, 10, 20, 30, 50] samples each. We found that
initializing a network that was trained on only a small sub-set
of well-chosen classes can signiﬁcantly improve generaliza-

0

1

2

3

4

5

6

Fig. 3. Generalities. The dark line represents the accuracy
of n(D|r). Please zoom on a computer monitor for closer
inspection. More plots for other combinations of datasets are
included in the supplementary.
tion performance on all classes, even if trained with arbitrarily
few samples, even at the extreme case of one-shot learning.
4. RESULTS AND OBSERVATIONS
Figure 3 shows the generalities of MNIST-rotated-bg and
Kannada prejudiced by all other the character datasets. For
reference each plot also shows the generalization performance of a randomly initialized base convolutional network.
The following are some observations of interest:
While no dataset is qualitatively the most general, it is
quite clear that MNIST dataset is the most speciﬁc. Rather,
MNIST dataset is one that is generalized by all datasets very
highly at all layers. Surprisingly, MNIST dataset actually
gives better accuracy when prejudiced with other datasets,



p
1
3
5
10
20
30
50

base
Random
MNIST[458]
Random
MNIST[458]
Random
MNIST[458]
Random
MNIST[458]
Random
MNIST[458]
Random
MNIST[458]
Random
MNIST[458]

k=0
73.07
83.61
90.98
91.55
95.52
96.5
96.38

k=1
73.91
87.2
92.98
93.71
95.52
97.34
97.40

k=2
76.37
85.7
92.6
93.82
97.07
97.35
97.71

k=3
55.61
77.52
73.34
87.6
83.32
92.07
81.31
95.08
87.77
96.78
88.62
97.45
90.78
97.38

The colonoscopy dataset’s labels identify if a image is
deemed to be of a quality that is good enough so as to make
a diagnosis on the pathology of that particular image. Most
often the video quality in colonoscopy is affected because of
saturation when too much light is thrown at a scene. The quality is also affected due to light reﬂection from bodily ﬂuids
that is also noticeable in the activations. Most of the ﬁlter colors are yellowish or blueish. On an colonoscopy video most
often the video is also labelled poor quality when these colors
are present, as these colors are often present mostly because
of scattering and reﬂections. Having made these observations
one would arrive at the obvious conclusion that neither dataset
generalizes the other. This was indeed the result observed
from ﬁgure 3. Although, Caltech 101 seem to generalize a
bit better for even though it predominantly learns shapes, it
learns some color features also.
From all these results and observations, we could summarize that one should prefer to initialize with a general dataset
that might have a lot of variability or rather generality in data,
when attempting to train with very few number of samples.
Whenever possible one must initialize the network trained by
a general dataset as this always boosts generalization performance. When there are biased datasets with large number
of samples in some classes and fewer in others, one should
train the most general classes ﬁrst. Once the network is wellprejudiced one should start introducing the classes with fewer
number of and less general samples, provided the general
class is general enough.

Table 1. Sub-sample experiment and its generalization accuracies for different layers of freezing. The re-train network
was MNIST[0, 1, 2, 3, 6, 7, 9]. For obvious reasons random
initializations are trained only with all layers unfrozen, hence
the missing values.

rather than when initialized with random, if all layers were
allowed to learn. This is a strong indicator that all datasets
contain all atomic structures of MNIST.
While initially one would have assumed that Kannada
would be a general dataset, we observed the contrary. SVHN,
Char74-English and Nist generalizes better to Kannada than
even Kannada itself does. English characters seem to be a
more general set than Kananda. While counter-intuitive, this
result is immediately obvious when one pays close attention
to the ﬁlters that are learnt and the dataset itself. Kannada
is dominated by predominantly curved edges only, whereas
even MNIST has a multitude of unique atomic structures.
For the intra-class experiment described above, table 1
shows the accuracies. From the table one can observe that
even with one-sample per class, a 7-way classiﬁer could
achieve 22% more accuracy than a randomly initialized network. It is note worthy that the last row of table 1 still has 100
times less data than the full dataset and it already achieves
close to state-of-the-art accuracy even when no layer is allowed to change. This is a remarkably strong indicator that
the classes [4, 5, 8] generalizes the entire dataset. We also
observed that once initialized with a general enough subset of
classes from within the same dataset, the generalities didn’t
vary among the layers like it did when we initialized with data
from outside the mother dataset. We also observed that the
more the data we used, more stable the generalities remained.
Point of take away from this experiment is that if the classes
are general enough, one may initialize the network with only
those classes and then learn the rest of the dataset even with
very small number of samples.

5. CONCLUSIONS
In this paper, we used the performance of CNNs on a dataset
when initialized with the ﬁlters from other datasets as a tool
to measure generality. We proposed a generality metric using these generalization performances. We used the proposed
metric to compare popular character recognition datasets and
found some interesting patterns and generality assumptions
that add to the knowledge-base of these datasets. In particular, we noticed that MNIST data is one of the most speciﬁc dataset. We also found that Char74k Kannada is less
general than English datasets. We also calculated generality
on class-level within a dataset and conclude that a few wellchosen classes used as pre-training could build a network that
is well-initialized that even with 100 times less samples, we
could learn the other classes. We also provided some practical guidelines for a CNN engineer to adopt. After performing
similar experiments on popular imaging datasets and medical
datasets, we made similar serendipitous observations.
Acknowledgments: This work was supported in part by
ARO grant W911NF1410371. Any opinions expressed in this
material are those of the authors and do not necessarily reﬂect
the views of ARO.



6. REFERENCES

[13] Alex Krizhevsky and Geoffrey Hinton, “Learning multiple layers of features from tiny images,” 2009. 2

[1] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich, “Going deeper with convolutions,” arXiv preprint arXiv:1409.4842, 2014. 1

[14] Li Fei-Fei, Rob Fergus, and Pietro Perona, “Learning generative visual models from few training examples: An incremental
bayesian approach tested on 101 object categories,” Computer
Vision and Image Understanding, vol. 106, no. 1, pp. 59–70,
2007. 2

[2] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, “Imagenet classiﬁcation with deep convolutional neural networks,”
in Advances in neural information processing systems, 2012,
pp. 1097–1105. 1

[15] Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, James
Bergstra, Ian J. Goodfellow, Arnaud Bergeron, Nicolas
Bouchard, and Yoshua Bengio, “Theano: new features and
speed improvements,” Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012. 2

[3] Bao-Qing Li and Baoxin Li, “Building pattern classiﬁers using convolutional neural networks,” in Neural Networks, 1999.
IJCNN’99. International Joint Conference on. IEEE, 1999,
vol. 5, pp. 3081–3085. 1

[16] Sergey Ioffe and Christian Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate
shift,” arXiv preprint arXiv:1502.03167, 2015. 3

[4] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik, “Rich feature hierarchies for accurate object detection and
semantic segmentation,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2014, pp.
580–587. 1
[5] Victor Escorcia, Juan Carlos Niebles, and Bernard Ghanem,
“On the relationship between visual attributes and convolutional networks,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2015, pp. 1256–
1264. 1
[6] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson,
“How transferable are features in deep neural networks?,” in
Advances in Neural Information Processing Systems, 2014, pp.
3320–3328. 2
[7] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
Li Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,” International Journal of Computer Vision (IJCV), pp.
1–42, April 2015. 2
[8] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean, “Distilling the knowledge in a neural network,” arXiv preprint
arXiv:1503.02531, 2015. 2
[9] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick
Haffner, “Gradient-based learning applied to document recognition,” Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–
2324, 1998. 2
[10] Hugo Larochelle, Dumitru Erhan, Aaron Courville, James
Bergstra, and Yoshua Bengio, “An empirical evaluation of deep
architectures on problems with many factors of variation,” in
Proceedings of the 24th international conference on Machine
learning. ACM, 2007, pp. 473–480. 2
[11] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco,
Bo Wu, and Andrew Y Ng, “Reading digits in natural images with unsupervised feature learning,” in NIPS workshop
on deep learning and unsupervised feature learning. Granada,
Spain, 2011, vol. 2011, p. 5. 2
[12] T. E. de Campos, B. R. Babu, and M. Varma, “Character recognition in natural images,” in Proceedings of the International
Conference on Computer Vision Theory and Applications, Lisbon, Portugal, February 2009. 2



Virtual View Specification and Synthesis in Free Viewpoint
Television Application
Wenfeng Li1, Jin Zhou1, Baoxin Li1, M. Ibrahim Sezan2
1

Dept. of Computer Science & Engineering
Arizona State University, Tempe, AZ 85287
{wenfeng.li,jin.zhou,baoxin.li}@asu.edu

Abstract
In conventional TV, users can get only a single view of a
real 3D world. The view is determined not by a user but by
the camera position during acquisition. Free viewpoint
television (FTV) is a new concept which aims at giving
viewers the flexibility to select a novel viewpoint by
employing multiple video streams as the input. Current
proposed solutions for FTV include those based on
ray-space sampling and interpolation. Most existing work
makes the assumption that the cameras are calibrated.
Moreover, virtual view specification is done in ways that
would not be practical for actual use (e.g., some methods
specify the virtual view by a rotation matrix and a
translation vector). This paper proposes a framework for
FTV based on image-based rendering, complete with key
algorithms
for
color-based
segmentation
and
correspondence, and multiple-image-based virtual view
synthesis from uncalibrated cameras. Moreover, we
propose an intuitive approach to specifying the virtual
viewpoint, based on any two views chosen by the user. This
makes the specification of the virtual view very intuitive
and thus practical for the FTV application. Experiments
with real video streams were performed to validate the
proposed approaches.

1. Introduction
Television is probably the most important visual
information system in past decades, and it has indeed
become a commodity of modern human life. In
conventional TV, the viewer’s viewpoint is determined by
that of the acquisition camera. Recently, a new concept,
free viewpoint television (FTV) [8-9], has been proposed,
which promises to bring a revolution to TV. The basic idea
of FTV is to provide the viewer the freedom of choosing
his/her own viewpoint, by incorporating multiple video
streams captured by a set of cameras. In addition to home
entertainment, the FTV concept can certainly apply to other
related domains such as gaming and education. Note that,
the user-chosen viewpoints do not need to coincide with
those of the acquisition cameras, and thus this is not a
simple view change by switching cameras (as possible with
some DVD for a couple of preset views). Apparently, this

Proceedings of the Third International Symposium on
3D Data Processing, Visualization, and Transmission (3DPVT'06)
0-7695-2825-2/06 $20.00 © 2006

2

SHARP Laboratories of America
5759 NW Pacific Rim, Camas, WA 98607
sezan@sharplabs.com
FTV revolution demands a whole spectrum of efforts
ranging from acquisition hardware, coding, bandwidth
management, and standardization, etc. In this paper, we
focus on one particular aspects of FTV: virtual view
synthesis.
The problem of virtual view synthesis can be defined as
follows: Given a set of images acquired from different
viewpoints, construct a new image that appear to be
acquired from a novel viewpoint. This problem is also
known as image-based rendering (IBR), which has been
extensively studied in the literature. For example, [10] and
[11] are among the early papers, and more recent ones
include [1,3,6,12,14]. While there exist alternative
approaches to virtual view synthesis in FTV (e.g., the
ray-space sampling and interpolation approach in [9]), IBR
has the potential of requiring fewer cameras and loose
constraints on the camera configuration. Our proposed
approach belongs to this category.
In the FTV application, it is unlikely that the camera
calibration information can be made available (e.g.,
imagine shooting a movie with multiple cameras which
need to be calibrated each time they are moved). This
immediately renders those IBR methods requiring full
camera calibration inapplicable. Moreover, before virtual
view synthesis, the virtual view needs to be specified.
Existing IBR techniques use a variety of ways for this
purpose. For example, in [3], the virtual view specification
is trivial as the entire setup is fully calibrated; in [10], a
method was given based on the user’s manual pick of some
points including the projection of the virtual camera center;
in [6], this is determined by a rotation matrix and a
translation vector with respect to a known camera.
Apparently, none of these approaches can be easily adopted
by the FTV application with uncalibrated cameras, where
an ordinary user needs an intuitive way of specifying some
desired (virtual) viewpoints.
In this paper, we propose a framework for the rendering
problem in FTV based on IBR. Our approach uses multiple
images from uncalibrated cameras as the input. Further,
while a virtual view is synthesized mainly from two
principal views chosen by a viewer, other views are also
employed to improve the quality. Being able to start with
two user-chosen views (which are assumed to be closer to

the desired virtual views than others) contributes to the
reduction in the number of required views, as these two
views can be used heavily for synthesizing the new view
while others are used for only improvement. These aspects
of the proposed approach sets it apart from exist methods
such as in [3,6,11-13]. Moreover, as the second major
contribution of the paper, we propose an intuitive method
for specifying the virtual view in uncalibrated cameras, and
thus providing a practical solution to view specification in
the FTV application without requiring either full camera
calibration (as in [3]) or complicated user interaction (as in
[6,10]), which are all impractical for FTV.

2. Problem Definition
In this section, we formally define the problem to be
solved. The problem definition also reflects our view on
how potentially the FTV application should configure the
entire system including how (ideally) cameras should be
positioned and how a user may interact with the system, if
the proposed approach is used for rendering.
We assume that multiple synchronized views of the same
scene are captured by a set of fixed but otherwise
uncalibrated cameras1. We assume that the multiple video
streams are available to a viewer. The viewer is to specify a
virtual viewpoint and requests the system to generate a
virtual video corresponding to that viewpoint.
In a typical IBR approach, since no explicit 3D
reconstruction and re-projection is performed, in general
the same physical point may have a different color in the
virtual view than from any of the given views, even without
considering occlusion. The differences among different
views can range from little to dramatic, depending on the
viewing angles, the illumination and reflection models etc.
Therefore, IBR approaches, while being attractive, have a
fundamental limitation: the virtual views cannot be too far
from the given views, otherwise unrealistic color may
entail.
With this consideration, we further assume that the
cameras used in a FTV program are located strategically so
that most potentially interesting viewpoint should lie
among the given views. For the convenience of a viewer,
this can be simplified to the following: the virtual view is
defined as one between any two user-chosen views from
the given multiple ones. The choice of the two views can be
quite intuitive and transparent in practice: for example, a
viewer may feel that view 1 is too to-the-left than the
desired, while view 2 is too to-the right; then the desired
virtual view should in somewhere between view 1 and view
2. (An implementation scheme will be presented in Sect. 4.)
Thus, our system should solve the following two
1
In practice, moving cameras pose no theoretical problem if the weak
calibration is done for very frame. Practically, it may be assumed that the
cameras are fixed at least for a video shot and thus the weak calibration is
needed only for each shot.

Proceedings of the Third International Symposium on
3D Data Processing, Visualization, and Transmission (3DPVT'06)
0-7695-2825-2/06 $20.00 © 2006

problems to support the FTV application:
(1) Given the multiple video streams from uncalibrated
cameras and any two user-chosen views, synthesize a
virtual view in between the two views; and
(2) Provide the viewer an intuitive way of specifying the
virtual viewpoint in relation to the given available views.

3. IBR for FTV: Proposed Approach
As defined above, we have a set of video streams with
two that are the closest to the user’s desired viewpoint
(while the notion of closeness is not well-defined in
uncalibrated views, in our problem this is given by the
user’s pick of the views. We will defer the specification of
the virtual viewpoint to Sect. 4). Naturally, from the
discussion in Sect. 2, we want to make maximum use of the
two specified views although other views can be potentially
helpful. In this sense, we will call the two user-chosen
views as the basis images. Note that the basis images are
dynamic based on the user’s choice and this is different
from, for example [12], where the basis images are defined
by two specially-positioned cameras.
Our proposed approach to virtual view synthesis consists
of the following major steps:
1. Pair-wise weak calibration of all views to support
potentially any pair that a viewer may choose.2
2. Color-segmentation-based correspondence between
the two basis views, where other views are taken into
consideration.
3. Forward warping from basis views to the virtual view
with verified disparity map.
4. For unfilled pixels, use an algorithm to do backward
search on all auxiliary views to find a dominant and
disparity consistent color.
We detail these steps in the following subsections.

3.1. Virtual view synthesis via weak calibration
We assume that n cameras are used in the system (n=8 in
our experiments). We denote the basis views as basis
camera 1 and basis camera 2. The remaining views are
called auxiliary cameras 3 to n. Fundamental matrices
between the basis and the auxiliary cameras are calculated
with feature detector and the RANSAC algorithm and
denoted as F13, F23, …F1n, F2n. The fundamental matrix
between the basis cameras is F12. Computation of
fundamental matrices needs to be done once unless the
cameras are moved. The fundamental matrices between the
basis and the virtual views are denoted as F10 and F20,
respectively (which is obtained in Sect. 4, as we cannot use
the same feature-detection-based method since the virtual
view does not exist yet).
With fundamental matrices determined, for any point x
2
In practice, pair-wise processing may not be needed if, for example,
one view lies on the straight line between another two views.

in camera 1, its corresponding point in camera 2, x’, is
constrained via the fundamental matrix by x’TF12x=0,
which can be used to facilitate the search for the disparity d.
A third corresponding point in an auxiliary camera k is
denoted by xk which is determined from xkTF1kx=0 and
xkTF2kx’=0. Once the correspondence between x and x’ is
determined, a virtual view pixel x” can be determined by
forward mapping, where x” satisfies both x”TF10x=0 and
x”TF20x’=0. These relationships are illustrated in Fig. 1.
Auxiliary camera k
X

xk

F1k

Basis camera 1

F2k

Basis camera 2

F12

x’

x
Virtual camera
F10

computing the final matching score for a segment S j in
basis image i (denoted as S ij ) with disparity d as

mij (d ) = max k {mijk (d )}

(1)

where mijk (d ) is the matching score of segment S ij in any
other basis or auxiliary camera k. (Note that, the d is always
for the basis views, and searching in other auxiliary views
is equivalent to checking which d is able to give arise to the
most color consistency among the views whose relation is
given in Fig. 1).
Furthermore, instead of deciding on a single d based on
the above matching score, we now use that score in the
following iterative optimization procedure, which is similar
to cooperative algorithm [2]. The basic idea is to update the
matching score of each color segment based on its
neighboring segments of similar color in order to enforce
disparity smoothness:
Sij 0 ( d ) = mij ( d ), rij k (d ) = ∑
∑ Sij k (d0 )
f d 0 ˛( d -D ,d +D )

F20

x”

Figure 1. Illustration of the relationship among the views
through fundamental matrices.

3.2. Segmentation-Based Correspondence
In Sect. 3.1, even with the epipolar constraint, we still
need to search along an epipolar line for the disparity for a
given point x. To establish the correspondence between x
and x’, we first use graph-cut-based segmentation [4] to
segment each of the basis views. For all pixels within each
segment, we assume that they have the same disparity, i.e.
on the same front parallel plane. It is obvious that
over-segmentation is favored for more accurate modeling,
and similar to [3] we limit each segment to be no wider and
higher than 15 pixels, which is a reasonable value for a
traditional NTSC TV frame with pixel resolution of 720·
480.
Each segment is warped to another image by the epipolar
constraint described above (also see Fig. 1). Instead of
using the common sum-of-squared-difference (SSD), or
sum-of-absolute-difference (SAD) criteria as matching
scores, we simply count the number of corresponding pixel
pairs whose relative difference (with respect to the absolute
value) is less than 0.2 (i.e.|R1-R2|/R1<0.2, similar for G and
B), and this number, normalized by the number of pixels in
the segment, is used as the matching score, denoted mij(d)
for any possible d and for j-th segment in basis image i.
This measure was found to be more robust to lighting
condition changes in our experiments.
In addition to using the matching score from the other
basis image, we incorporate all the auxiliary images by

Proceedings of the Third International Symposium on
3D Data Processing, Visualization, and Transmission (3DPVT'06)
0-7695-2825-2/06 $20.00 © 2006

b
(2)


k
r
(
d
)


ij
0
k +1
Sij (d ) = Sij (d ) 

k
 d˛( d∑,d ) rij (d ) 
 min max

where f is the set of neighbor segments with similar color
(defined by Euclidian color distance under a
pre-determined threshold), is the inhibition constant (set
to 2 for computational simplicity) controlling the
convergence speed, and k the iteration index. We use the
following stopping criteria: at any iteration k, if for any d,
Sij exceeds the threshold, the updating process for this
segment will stop at next iteration; the entire procedure will
terminate until it converges (i.e., no segments need to be
updated). Our experiments show that the algorithm
converges typically after 10 iterations and thus we fix the
number of iteration to 10.
The above procedure is performed for both basis views,
and the disparity map is further verified by left-right
consistency check, and only those segments with consistent
results are used for synthesizing the virtual view (thus some
segments may not be used, resulting in an incomplete
disparity map). In Fig. 2, we show two examples of the
color-segmentation results together with the resultant
disparity map.
β

3.3. Forward Warping
Using the verified disparity map and the two basis views,
an initial estimate of the virtual view can be synthesized by
forward warping. For a pixel x in basis view 1 and x’ in
basis view 2, their corresponding pixel on the virtual view
will be x” whose color is computed as
(3)
RGB ( x ") = (1 - a ) RGB ( x) + a RGB( x ')
with a being a coefficient controlling the contribution of
the basis views (which may be set to the same a to be

defined in Sect. 4).
Forward warping can preserve well texture details and it
can easily be implemented in hardware, making real-time
rendering possible. Figure 3 shows an intermediate image
obtained after forward warping.

Figure 2. Color segmentation and disparity maps of the
monkey scene and the snoopy scene. Top row: original
images. Center row: color-based segmentation results
shown as pseudo colors. Bottom row: computed disparity
maps.

functions defined on disparity and color; and l is a weight
coefficient. The combination of the differences of color and
the disparity is intended for the smoothness of both texture
(color) and depth. In reality, F(d) is set as the minimum one
obtained from all the valid neighbor pixels. A new disparity
will be accepted only when the resulting F(d) is below a
predetermined value. If the search fails after all possible d
is tested on all valid neighbors, the corresponding pixel is
left empty until propagation is reached from other pixels.
Otherwise it is assigned a color based on the blending
method of Eqn. (3) and is labeled as valid. New search then
continues for other black-hole pixels.

Figure 3. Virtual view after forward warping with original two
basis views on the top row (the basis views are shown in
reduced size to save space).

3.4. Backward Searching and Propagation
In the initial virtual view given by forward warping, it is
not uncommon to see many uncovered pixels, which we
name as “black holes”. These black holes are due to
incomplete disparity map (see Sect. 3.2) (among others,
occlusion is one reason for this). For each black-hole pixel,
we check its neighbor for a pixel that has been assigned a
color value from the initial synthesis. The disparity of that
pixel is then used for backward search on all the images.
Unlike other similar disparity or depth searching
algorithms that do exhaustive search on the entire disparity
space, we search for a small range within the disparity of
the “valid” neighbors (those with assigned color). The
search objective function is defined as:
F (d ) = min {l Dist color ( p dn , p )
(4)
d ˛[ d n - D , d n + D ]
+ (1 - l ) Dist disp (d n , d )}
where dn is the disparity of a valid neighbor pixel and pdn is
its color; p = { p1 , p2 } are colors from two basis views

corresponding to d; Distdisp and Distcolor are two distance

Proceedings of the Third International Symposium on
3D Data Processing, Visualization, and Transmission (3DPVT'06)
0-7695-2825-2/06 $20.00 © 2006

Figure 4. Complete virtual view after the entire process.

3.5. Using Multiple Views
Even after the search and propagation processes, there
may still be “black holes” left when the points cannot be
seen in both basis cameras. To address this, the same search
and propagation method as in the above is used but
with p = { pi }, i „ 1, 2 . This means that we assume that the
pixel may be (for example) occluded in either or both of
views and thus both of them are excluded. But we may be
able to obtain the information from other views. Since there

is no information for any preference for any of the auxiliary
views, a dominant color found from the views is taken to
fill the black holes. While it may appear to be
computationally expensive to search in multiple images if
the number of views n is large, considering that the number
of uncovered pixels is relatively small after the previous
steps, this search is quite fast in practice.
It should be noted that there is no guarantee that all pixels
can be covered by the above procedure. For example, the
problem may be caused by a few isolated noisy pixels, or
maybe the scene is not covered by all the cameras. A simple
linear interpolation can handle the former situation while
the latter situation can be alleviated by constraining the free
viewpoint range, which is already part of out assumption
(i.e., the virtual view is always between two views, and the
cameras are strategically positioned).
A complete virtual view obtained by following the entire
process is shown in Figure 4.

4. Viewpoint Specification
In this section, we propose an intuitive way for virtual view
specification based on only uncalibrated views. Essentially,
we provide a viewer with the capability of varying a virtual
view gradually between any two chosen views. The virtual
view can thus be determined by, for example, conveniently
pushing a +/- button (or similar) until the desired viewpoint
is shown, similar to controlling color or contrast of a TV
picture via a remote control button (similarly, a joystick on
remote or a game console can be used to implement the
idea).

4.1. Basic Idea
A viewpoint can be specified by a translation vector and
a rotation matrix with respect to any given view to
determine its position and direction. But it is unrealistic to
ask a TV viewer to do this. A practical method is to start
with a real view and let the viewer move to a desired
viewpoint in reference to that view. This relative viewpoint
moving, in an interactive manner, is much more convenient
for the user. Thus we have designed our system to be able to
interpolate continuous virtual views from one view to
another. The interpolation can be controlled by a single
parameter . When =0, the basis view 1 is the current
view; and with increasing to 1, the viewpoint changes
gradually to another view 2, which is determined by the
button pushed. A mockup user interface is illustrated in
Figure 5 for an illustration of this idea, where the left-right
arrow buttons control the viewpoint change from two
underlying basis views, and the result is shown
immediately on the screen as visual feedback to the viewer.
(In reality, we may display the two basis views on the
screen as well.) The up-down arrow buttons can add
variability of the views along a path between the two basis
views (as explained in detail in the below).
α

α

α

Proceedings of the Third International Symposium on
3D Data Processing, Visualization, and Transmission (3DPVT'06)
0-7695-2825-2/06 $20.00 © 2006

Figure 5. Virtual view specification: A mockup illustrating
the main idea. Pushing the arrows would lead to a virtual
view that “moves away” from the current view.

4.2. Viewpoint Interpolation: Calibrated Case
We begin with the calibrated case as it is instructive,
although our ultimate goal is to deal with the uncalibrated
case. Note that in this case, potentially one can use either of
the methods described in [1] and [6]. However, as
discussed previously, these methods are not practical to use
(e.g., it is unlikely that we can ask a TV viewer to input a
rotation matrix and a translation vector). We now consider
how to use an interface such as that of Fig. 5 to support
intuitive virtual view specification. Suppose we have two
camera matrices for the two basis views respectively:
(5)
P1 = K1 R1[ I | -C1 ], P2 = K 2 R2 [ I | -C2 ]
Since we will handle uncalibrated case later, we are
concerned with only relative relationship between the two
views. By applying the following homography transform to
each of the projection matrices,
Pi ' = Pi H
where
I
H = Hc H R , Hc =  T
0

C1 
 R -1
, H R =  1T

1
0

0

1

(6)

we convert the cameras to canonical form as:
-1
 P1 ' = K1 R1 '[ I | -C1 '] = K1[ I | 0]
 R2 ' = R2 R1
with 

P2 ' = K 2 R2 '[ I | -C2 ']
C2 ' = R1 (C2 - C1 )


(7)

i.e., the first camera’s center is the origin, and camera 2 is
related to camera 1 by rotation R2 and translation C 2 ' .
We can specify the virtual view based on the canonical
form. Suppose the camera matrix for the virtual view is:
(8)
P0 ' = K 0 ' R0 ' [ I | -C0 ' ]
We use to parameterize the path between basis views 1
and 2. Eq. (8) becomes
(9)
P0 ' (a ) = K 0 ' (a ) R0 ' (a )[ I | -C 0 ' (a )]
α

For the camera intrinsic matrix, the gradual change from
view 1 to view 2 may be viewed as camera 1 changing its
focus and principal points gradually to those of camera 2 (if
the two cameras are identical, then this will not have any
effect, as desired). Thus, we can interpolate the intrinsic

matrix and obtain K v '(a ) as:

K 0 ' (a ) = (1 - a ) K1 + aK 2

(10)

For R0 '(a ) , suppose
Ri ' = [ ri , si , ti ]

T

(11)

where ri , si and ti represent the x-axis, y-axis and z-axis,
respectively. We construct R0 ' (a ) = [ r0 (a ), s0 (a ), t 0 (a )]
as follows:
t 0 (a ) = ((1 - a )t1 + at 2 ) / || (1 - a )t1 + at 2 ||

s ' = (1 - a ) s1 + as 2

(12)

r0 (a ) = ( s '·t 0 (a )) / || s '·t 0 (a ) ||
s0 (a ) = t 0 (a ) · r0 (a )
The first step in Eqn. (12) means that we construct the
new z-axis as the interpolation of two original z axes. Then
we interpolate a temporary y-axis as s ' . Note that s ' may
not be perpendicular to the new z-axis. But with it, we can
construct a new x-axis ( r0 (a ) ) with the new z-axis and a
temporary y-axis. Finally, we construct the new y-axis as
the cross product of the new z-axis and x-axis.
Finally, we can construct the new camera center using
linear interpolation:
(13)
C0 ' (a ) = (1 - a )C1 '+aC2 '
From Eqn. (13), the new camera center is on the line
connecting the two camera centers, resulting in degeneracy
for the epipolar constraint and thus we cannot use it for
virtual view synthesis (see Fig. 1). While methods exist to
handle the degeneracy (e.g., [7]) without using the epipolar
constraint, we are interested in keeping the benefits derived
from the constraint and thus want to avoid the degeneracy
so that the fundamental matrix based method is still
applicable. Thus we need to move the path away from the
exact line between the two views. This can be achieved by
simply increase slightly the y components of the virtual
camera center computed from Eqn. (13).
In
implementation, by increasing/decreasing the y component,
we can further achieve the effect of changing the viewpoint
perpendicular to the first direction. Suppose that
Cv( )=[xv,yv,zv], we get a new Cv’( ) as
Cv '(a ) = [ xv , yv + g , zv ] .
α

α

This entire process is illustrated in Fig. 6, where the
Scene object
Basis view 2
Virtual view
Basis view 1

γ

1α

α

Figure 6. The virtual view as a function of the basis views
through two parameter and , which can be controlled by
the left-right and up-down arrows of Fig. 5 respectively.
α

γ

Proceedings of the Third International Symposium on
3D Data Processing, Visualization, and Transmission (3DPVT'06)
0-7695-2825-2/06 $20.00 © 2006

With the interpolated P0, the corresponding fundamental
matrices can be calculated easily (see, for example, [5]) and
then used for virtual view synthesis.

4.3. Viewpoint Interpolation: Uncalibrated Case
Now we consider the uncalibrated case, i.e., how we can
achieve similar results of Sect. 4.2 from only the
fundamental matrices. Given a fundamental matrix F12 , the
corresponding canonical camera matrices are:
(14)
P1 = [ I | 0], P2 = [[e ']· F12 + e ' vT | l e ']
where e’ is the epipole on image 2 with F12 T e ' = 0 , v can be
any 3-vector, and l is a non-zero scalar. Note that the
reconstructed P2 is up to a projective transformation.
Apparently, a randomly-chosen v cannot be expected to
give us a reasonable virtual view if the fundamental matrix
is based on a P2 defined by such a v. Here we present an
approach to obtaining the P’s from an approximately
estimated essential matrix. We first estimate the essential
matrix by a simple approximation scheme. The essential
matrix has the form:
-1
(15)
E12 = K 2 F12 K1
For unknown camera matrices K, although
auto-calibration can recover the focal length at the expense
of tedious computation, it is not a practical option for the
FTV application (unless the information is obtained at the
acquisition stage). As an approximation, we set the
parameters of the camera matrix based on the image width
w and height h:
(16)
f = ( w + h) / 2, px = w / 2, p y = h / 2
So K becomes:
f
K =  0
 0

0
f
0

px 
p y 
1 

(17)

Further, we assume that both cameras have similar
configuration and use the same K to get the essential
matrix E12 . An essential matrix can be decomposed into a
skew-symmetric matrix and rotation matrix as:
(18)
E12 = [t ]· R
where R and t can be viewed as the relative rotation and
translation matrix of camera 2 relative to 1. Now we have
(19)
P1 = K [ I | 0], P2 = K [ R | t ]
and thus the corresponding fundamental matrices can be
recovered (see, for example, [5]). This approach proved to
be effective with multiple sets of data that we have
experimented with, as will be illustrated in Sect. 5, even if
we base on only an estimate in Eqn. (16) without knowing
the actual camera internal matrices.
We would like to point out that, although it seems that
we are going back to the calibrated case by estimating the
essential matrix, the scheme is totally different from true
full calibration. This is because one cannot expect to use the

approximation of Eqn. (16) for estimating the true rotation
and translation that are needed for specifying the virtual as
in the calibrated case (e.g., as in [3,6]). However, it is
reasonable to use the approximation in the interpolation
scheme in Sect. 4.3 (Eqns. (12) and (13)).

5. Experimental Results
We first verified our viewpoint specification method by
showing a simulated free viewpoint moving path by using
data that is made available on the Internet by the authors of
[1] since the cameras are calibrated and the associated path
has Euclidian meaning in real world. Fig. 7 shows two
paths with viewpoint moving from camera 67 to 74 over a
parabola and continuing to camera 80 following a
piecewise linear curve (the latter is simply the result of
Sect. 4.2 while the parabola illustrates the possibility of
using other schemes such as a quadratic function for
interpolation between the two views).

Figure 7. Simulated virtual viewpoint moving paths. The red
dots are the given camera positions.

As examples of our experiments, we present results
obtained from the following two data sets: the “monkey
scene” from [1] and a “snoopy scene” captured by us. For
the monkey scene, there are 89 views in this data set and we
picked views 67 and 74, which have a proper distance, as
basis views. We randomly picked 6 other views as auxiliary
views. One hundred virtual views are synthesized with
viewpoint moving gradually between the two basis views.
Fig. 8 (left) shows the two basis views and three examples
of synthesized images. In this test, we use the camera
calibration information provided with the data to compute
the fundamental matrices, and thus the results are pretty
good, illustrating that the remaining procedures (other than
the weak calibration stage for computing the fundamental
matrices) are performing correctly.
For the “snoopy scene”, the camera parameters are not
known, and we relied on our proposed algorithms to solve
the entire problem from purely the uncalibrated views. The
results are shown in Fig. 8 (right). During acquisition, the
camera was set to auto-focus and thus the internal matrices
may vary in the images. However, with fixed parameters
(for both the monkey scene and the snoopy scene), the
results are reasonable, indicating that our method in Sect.

Proceedings of the Third International Symposium on
3D Data Processing, Visualization, and Transmission (3DPVT'06)
0-7695-2825-2/06 $20.00 © 2006

4.3 is working even with the assumption of Eqn. (16).
The results in Fig. 8 (right) is slightly worse than the
monkey scene (left). For example, view 30 still contains
some problematic regions. Among others, we attribute this
to the potential inaccuracy in feature-detection-based
procedure for estimating the fundamental matrices.
It needs to pointed out that the basis views in Fig. 8 are
not very close although they appear to be so. The viewing
angle difference between the basis views are in fact roughly
30 degrees. This can be seen more clearly from the
background of the views (e.g. looking at the snoopy’s nose
in relation to the computer keyboard in the background).

6. Conclusion and Discussion
We have presented a new virtual view specification and
synthesis approach for the FTV application. Our
experimental results using multiple data sets have proved
the correctness and the feasibility of the proposed
approach. The proposed approach may therefore provide a
practical scheme for realizing IBR-based FTV.
In terms of the visual quality of the generated virtual
views, our current experiments do not indicate that our
results are better than the leading approaches reported in
the literature. However, our approach is capable of purely
working from uncalibrated views without using any
pre-calibration information, rendering it as a viable
approach for practical FTV. Further visual quality
improvements are expected as we further improve the
components, especially the weak-calibration stage.
Future work includes analyzing the impact of the various
steps of the proposed schemed on the overall quality of the
virtual view. The current approach is assumed to be applied
on a frame-by-frame basis, and we need to study the how
temporal information in video may impact on some of the
stages such as feature detection (e.g., feature tracking could
be used) and visual quality related issues (e.g.,
spatio-temporal artifacts). Also, our development thus far
has been focused on feasibility study of the key algorithms,
and the time complexity of an integrated system has yet to
be studied.

References
[1] A. W. Fitzgibbon, Y. Wexler and A. Zisserman,
“Image-based rendering using image-based priors”, ICCV
2003: 1176-1183.
[2] C.L. Zitnick and L. Kanade, “A cooperative algorithm for
stereo matching and occlusion detection”, PAMI 22(7):
675-684, 2000.
[3] L. Zitnick, S.B. Kang, M. Uyttendaele, S. Winder, and R.
Szeliski, “High-quality video view interpolation using a
layered representation”, SIGGRAPH 2004: 600-608.
[4] P.F. Felzenszwalb and D.P. Huttenlocher, “Efficient
graph-based image segmentation”, IJCV 59(2): 167-181,
2004.

[5] R. Hartley and A. Zisserman, Multiple View Geometry in
Computer Vision, Cambridge University Press, Cambridge,
UK, 2000.
[6] K.R. Connor and I.D. Reid, “Novel view specification and
synthesis”, Proc. British Machine Vision Conf., 2002.
[7] A. Shashua and M. Werman, “Trilinearity of three
perspective views and its associated tensor”, ICCV 1995:
920-925.
[8] N. Bangchang, T. Fujii, M. Tanimoto, “Experimental system
of free viewpoint Television", Proc. IST/ SPIE Symposium
on Electronic Imaging, Vol. 5006-66, pp. 554-563, Jan 2003.
[9] M. Tanimoto, T. Fujii, “FTV: Achievements and
Challenges”, ISO/IEC JTC1/SC29/WG11 M11259, Oct.
2004.

[10] S. Laveau and O. Faugeras, “3-D Scene Representation as a
Collection of Images”, Proc. of Int. Conf. on Pattern
Recognition, Oct. 1994. (INRIA Technical Report 2205).
[11] D. Scharstein, “Stereo vision for view synthesis”, Proc. IEEE
CVPR, 1996.
[12] Y. Ito and H. Saito, “Free-viewpoint image synthesis from
multiple-view images taken with uncalibrated moving
cameras”, Proc. IEEE ICIP 2005.
[13] C. Buehler, M. Bosse, L. McMillan, S. Gortler, M. Cohen,
“Unstructured Lumigraph Rendering”, SIGGRAPH 2001.
[14] S. Wurmlin, E. Lamboray, M. Washchbusch, P. Kaufmann,
A. Smolic, M. Gross, “Image-space Free-viewpoint Video”,
Proc. Vision, Modeling, and Visualization, 2005.

Basis View 1

Viewpoint 30

Viewpoint 90

Basis View 2

Figure 8. Synthesized and basis views. Left column: monkey scene. Right column: snoopy scene

Proceedings of the Third International Symposium on
3D Data Processing, Visualization, and Transmission (3DPVT'06)
0-7695-2825-2/06 $20.00 © 2006

Probabilistic Image-Based Rendering with Gaussian Mixture Model
Wenfeng Li and Baoxin Li
Department of Computer Science & Engineering
Arizona State University, Tempe, AZ 85287, USA
E-mail: {wenfeng.li, baoxin.li}@asu.edu
Abstract
One major challenge in traditional image-based
rendering is 3D scene reconstruction by estimating
accurate dense depth map, which suffers from the
ambiguities in textureless or periodically textured
regions. Alternatively, statistical methods may be used
to estimate a most likely color for each pixel for
photorealistic rendering from multiple views of the
same scene. Such statistical methods normally require
a relatively large number of input images to achieve
reasonable quality for the synthesized image, if the
estimation is purely nonparametric. In this paper,
based on some reasonable assumptions on the
configuration of the multiple views, we propose to use
a two-component Gaussian mixture model for the
appearance of a given pixel in all the views so that
both the problem of occlusion and the problem of noise
can be considered simultaneously. Then we use the
Expectation-Maximization algorithm to estimate the
model parameters. The virtual pixel is given as a
maximum likelihood estimate for one of the mixture
components. Experiments shows that reasonable
performance can be obtained even with only a few
input images.

1. Introduction
Image-based rendering (IBR), which generally
refers to techniques that generate new images from
other given images, has been extensively researched
for more than a decade. A typical IBR task is to
synthesize a new image for a (virtual) viewpoint, given
some captured images from different viewpoints. A
natural solution followed by many studies is to
reconstruct the 3D geometry of the scene from the
input images and then to use per-pixel geometry and
color information to synthesize the new image.
Typically, stereo matching [4] is used to establish perpixel correspondence based on a pair of input images.
When multiple input images are available, full 3D
details can be reconstructed by space carving
techniques (e.g., [5,6]). While being conceptually
intuitive, dense depth estimation remains to be a
challenging task due to reasons such as the ambiguity

Proceedings of the 18th International Conference on Pattern Recognition (ICPR'06)
0-7695-2521-0/06 $20.00 © 2006

of correspondence in textureless or periodicallytextured regions, occlusion, imaging noises, and
lighting variations, etc.
When a large number of views are available, an
alternative approach is to use probabilistic inference
methods for estimating a most likely pixel from the set
of pixels in all the views that are determined by, for
example, the camera calibration matrices. For instance,
a simple average of all pixels in the set can be used as
an estimate for the virtual pixel; or, the color
corresponding to the peak in the histogram for the set
of pixels can be used instead. These simple techniques,
together with other existing work (e.g. [1,7], where
slightly more sophisticated methods were used) treat
each pixel in the set equally, although in reality the
pixels may correspond to different scene points.
Consequently, these techniques tend to blend the colors
of different scene objects (e.g., foreground objects and
the background) and thus degrade the synthesized
view. This is even more serious with a small number of
views. (With a large number of input images, one
might hope that the correct color will beat other
distractions.)
In this paper, based on some reasonable
assumptions on the configuration of the multiple
views, we propose to use a two-component Gaussian
mixture model for the appearance of the set of pixels
corresponding to a virtual pixel location. Then we use
the Expectation-Maximization algorithm to estimate
the model parameters. The color of the virtual pixel is
given as a maximum likelihood estimate for one of the
components. By using such a parametric approach, we
aim at achieving reasonable performance even with
only a relatively small number of views.

2. Problem
Method

Statement

and

Proposed

Given N 2-D input images I1,…, IN. x is a 2-vector
[u, v] which is the coordinate of a pixel in an image.
Ii (x) represents the color of the pixel located at x in the
ith image, which is a 3-vector in the RGB color space.
I0 is the image we want to synthesize, also called the
virtual view. If all cameras for capturing the input

images are fully calibrated, and if the projection matrix
of the virtual view is known, we have the 3×4
projection matrices P1, … PN, and P0, which project a
scene point with homogeneous 3D coordinate X to
image points xi, written as
(1)
xˆ i = Pi X , i = 0,..., N , xˆ i = s[ui , vi ,1]

Since we want to know the color I0(x) for each pixel
x=[u, v] in I0, we trace the ray originating from the
camera optical center through the image plane point x
to the scene point X. If the z value in X is known,
which is called depth in this paper, X is determined
uniquely by x and P0. Using (1) to map X onto images
1 to N, we get corresponding pixels Ii (xi) and write the
mapping function as xi=li(x). Let D be the
configuration of depth z for each x in I0, which is
unknown. We want to maximize the a posteriori of I0
and D given the observed data I1, … , IN, which is
p ( I1 ,...I N | I 0 , D ) ⋅ p ( I 0 , D ) (2)
p( I 0 , D | I1 ,...I N ) =
p( I1 ,...I N )
In our method, D is used as only a proxy in
calculation, and our ultimate goal is to find a good
estimation of the colors. Suppose that we do not have
any preference over the given views and that there are
no special constraints on I0 and D, then the prior and
the denominator can be left out. If we assume that the
pixels are i.i.d, we can further write
p( I 0 , D | I1 ,...I N ) ∝ ∏ p( I1 ( x1 ),...I N ( xN ) | I 0 ( x), D( x))
x

N

= ∏∏ p ( I i ( xi ) | I 0 ( x), D ( x))
x

(3)

i =1

2.1 A 2-Component Gaussian Mixture Model
Due to various reasons including lighting condition
changes and noise, the projected color of the same
scene point X on all input images may not be the same.
We model this variation by a Gaussian distribution
N(Ii (xi);I0(x), Ȉ), i.e., we assume that the desired virtual
pixel is the mean of this distribution. (This may require
that the virtual view is among the given views, and we
believe that this is a reasonable requirement or
assumption if realistic rendering is the priority.) With
this simple distribution, by taking the logarithm of (3),
we can find the maximum likelihood estimation (MLE)
by minimizing the mean squared error, and the optimal
solution for I0 is the mean of all Ii. This can be viewed
as a more general presentation of the strategy of simple
average. Experiments show that this simple model
works well if X is visible in all input images. However,
occlusion and noise will greatly degrade the
performance of this simple model, and the resultant
mean value I0(x) may be distorted severely. We will
use the term contamination to denote the problem due
to either occlusion or noise. This problem is difficult to

Proceedings of the 18th International Conference on Pattern Recognition (ICPR'06)
0-7695-2521-0/06 $20.00 © 2006

address by existing techniques such as using image
priors [1]. In the following, we analyze this problem
and propose a Gaussian mixture model.

Figure 1. Colors of corresponding points from different
views. The outliers do not correspond to the same
scene point as the other pixels do.

Figure 2. Histograms of color distribution for two pixel
locations respectively. The left one may be modeled
by two Gaussian components, while the right one is
better approximated by a mixture of Gaussian and
uniform distributions.

Fig. 1 shows 89 sampling pixels for the same scene
point from different views with outliers pointed out.
Fig. 2 illustrates the histogram of colors from all
corresponding points determined by the projection
matrices. Without loss of generality, only the R
component is drawn for. Ideally, most of these colors
should be the same. However, it is obvious from Fig. 2
that other clusters exist, corresponding to different
colors. These pixels of different colors are from
outliers. If the outliers are (mostly) from the same
scene object (e.g., a relatively homogeneous
background), they may form a Gaussian peak as
illustrated in Fig. 2(left). On the other hand, if the
outliers are from different scenes points, they may
result in the (approximately) uniformly-distributed
cluster as in Fig. 2(right). Thus the practical problems
are to determine whether the set of pixel values
corresponding to a virtual pixel are contaminated, and
if yes, what the likely model for the contamination may
be. To address these problems, we introduce a binary
hidden variable V: if X is not contaminated in the ith
image then Vi =1, otherwise Vi =0. In implementation, a
weighting factor Į is used to allow components to be
probabilistically associated with each of the classes.
Furthermore, to consider the two situations illustrated
in Fig. 2 in the same framework, we use another
parameter ȕ to control the switch between a uniform
distribution and a Gaussian distribution, for modeling
the contamination. Thus if we believe a point X is not

contaminated with probability Į, the per-pixel mixture
likelihood becomes
p ( I i ( xi ) | I 0 ( x ), D ( x )) =

(4)
α ⋅ ȃ ( I i ( xi ); I 0 ( x), Σ ) +
(1 − α ) ⋅ {β ⋅ N ( I i ( xi ); I ' ( x ), Σ ' ) + (1 − β ) ⋅ U }
where ȕ is a binary switching variable determined by
the clustering results (i.e., choose 1 or 0 depending on
which value maximizes the likelihood). The task is
then to find the MLE maximum likelihood estimate
under this 2-component model.

2.2 An EM Solution
For finding the MLE based on Eqn. (4), we use the
EM algorithm [9]. In [2] and [3], the EM algorithm is
used to estimate the depth from multiple views. There
are some possible disadvantages in doing this. For
instance, the likelihood function with respect to D is
full of local minima but the EM algorithm converges to
only a local minimum. Also, there is no analytical
dependence between the colors and the depth for a
general scene. In our approach, we pursue the
estimation for the best color on each depth D*. Let
Θ=(I0, D*), the EM algorithm is as follows.
E-step: Based on the current estimate for Θ*, the
expectation of complete data log-likelihood is
E p (V |Θ* ) [log L] =
N

¦¦ Pr ob(V

i

x

i =1

= 1 | Θ* ) log(α ⋅ ȃ ( I i ( xi ) | I 0 ( x), Σ) (5)

N

+ ¦¦ Pr ob(Vi = 0 | Θ* ) log((1 − α ) ⋅
x

i =1

{β ⋅ N ( I i ( xi ); I ' ( x), Σ ' ) + (1 − β ) ⋅ U })

where
Pr ob(Vi = 1 | Θ* ) =

α ⋅ ȃ ( I i ( xi ) | I 0 ( x), Σ)
α ⋅ ȃ ( I i ( xi ) | I 0 ( x), Σ) +
(1 − α ) ⋅ {β ⋅ N ( I i ( xi ); I ' ( x), Σ' ) + (1 − β ) ⋅ U })
Pr ob(Vi = 0 | Θ* ) = 1 − Pr ob(Vi = 1 | Θ * )
M-step: To maximize (5), take derivative with

respect to variables to be estimated, we can get
N

¦ Pr ob(V

i

I 0 ( x) =

= 1 | Θ* ) I i ( xi )

i=1

N

¦ Pr ob(V

i

Σ=

¦ Pr ob(V

i

= 1 | Θ* )( I i ( xi ) − I 0 ( x ))(I i ( xi ) − I 0 ( x))T

i =1

N

¦ Pr ob(V

i

U=

1
max{ I i ( x ) − I j ( x ) + 1}

The procedure will stop (converge) when all
variables do not change (or change little).

2.3 Weakly Calibrated Views
If we have a large number of views, full calibration
of all views will be expensive and also less practical.
However it is relatively much easier to compute only
the fundamental matrix between any pair of views. We
now describe how the proposed method can work with
only the epipolar constraint (weak calibration).
For any point x=[u v] in I0, its corresponding point
x1=[u1 v1] in image I1 is constrained by an epipolar line
which can be computed with the fundamental matrix
F01. If we know the horizontal disparity between x and
x1 which is d=u1-u, x1 is determined. With the
correspondence between x and x1, a third point x2 can
be located in image I2 by the intersection of two
epipolar lines constrained by x , x1 and the fundamental
matrix F02, F12. Now I1 and I2 work as two anchors to
locate other corresponding points in all input images.
Thus the 3D depth z of the scene is mapped into a
disparity d in an anchor view, and we can use the
method described in Sect. 2.2.
One difficulty is to find the fundamental matrix
between the virtual view and the two anchor views
without camera calibration data. We have developed an
approach to this problem [11]. Another problem with
epipolar constraints is the degeneracy when two
epipolar lines cannot determine a reliable intersection,
such as the case when two epipolar lines are nearly
parallel. Our experiments proved that pixels from such
views will be classified as outliers and will not affect
the final result. Our experiments reported in the next
section are based on only weak calibration.

3. Experiments

= 1 | Θ* )

i =1

N

Thus, the mean is estimates by a weighted mean of
all observed data, and the weight of each component is
proportional to the probability with which it belongs to
the uncontaminated class. The above is for the situation
when the second component is Gaussian. When the
second component is uniform instead, in M-step we
computer U as

= 1 | Θ* )

i =1

N

α = ¦ Pr ob(Vi = 1 | Θ* ) / N
i =1

Proceedings of the 18th International Conference on Pattern Recognition (ICPR'06)
0-7695-2521-0/06 $20.00 © 2006

Sample results based on experiments with the
“monkey scene” data set from [1] are presented in this
section to illustrate the performance of the proposed
method. In the dataset there are 89 images from
different viewpoints each with camera calibration
information. We randomly pick the input views,
compute their fundamental matrices. Different
numbers of input images are tested in our algorithm.

With only 8 to 10 inputs, we can achieve reasonable
results. Fig. 3(b) is one sample result synthesized with
8 images. For comparison we also implement an
algorithm based on a simple Gaussian model (Fig.
3(a)) with the same number of inputs, where artifacts
are obvious in regions such as those marked by red
circles. Fig. 4 supplies another example from different
virtual viewpoint.

4. Conclusion and Future Work
While having some attractive advantages,
probabilistic IBR typically requires a large number of
views. To alleviate this problem, we proposed an
approach based on a 2-component Gaussian mixture
model for the pixels linked to a virtual pixel.
Experiments show the effectiveness of the method in
face of only a small number of views. It was noticed in
our experiments that some details are not kept well
when these details are surrounded by textureless
background. This can be explained by the consistency
of the background pixels, which causes them to be
classified as the true object pixels. One potential
remedy is to use the texture synthesis technique such as
the image prior used in [1] and [7].

(a)

(c)

(b)

(d)

[3] C. Strecha, R. Fransens, and L. V. Gool, “Wide-Baseline
Stereo from Multiple Views: A Probabilistic Account”,
CVPR, pp. 552-559, 2004.
[4] D. Scharstein and R. Szeliski, “A Taxonomy and
Evaluation of Dense Two-frame Stereo Correspondence
Algorithms”, IJCV, 47(1): pp. 7–42, 2002.
[5] A. Broadhurst and R. Cipolla, “A Statistical Consistency
Check for the Space Carving Algorithm”, ICCV, pp. 388393, 2001.
[6] S. M. Seitz and C. R. Dyer, “Photorealistic Scene
Reconstruction by Voxel Coloring”,CVPR, 1997.
[7] O. Woodford and A. Fitzgibbon, “Fast Image-based
Rendering using Hierarchical Image-based Priors”, BMVC,
2005.
[8] D.J. Miller and J. Browning, “A Mixture Model and EMBased Algorithm for Class Discovery, Robust Classification,
and Outlier Rejection in Mixed Labeled/Unlabeled Data
Sets”, IEEE Trans. Pattern Analysis and Machine
Intelligence, 25(11), pp. 1468-1483, 2003.
[9] A. Dempster, N. Laird, and D. Rubin, “MaximumLikelihood from Incomplete Data via the EM Algorithm”, J.
Royal Statistical Soc. B, 39, pp. 1-38, 1977.
[10] P.H.S. Torr, A. Zisserman, and S.J. Maybank, “Robust
Detection of Degenerate Configurations for the Fundamental
Matrix”, ICCV, p. 1037, 1995.
[11] W. Li, J. Zhou, B. Li and M. I. Sezan, “Virtual View
Specification and Synthesis in Free Viewpoint Television
Application”, Third International Symposium on 3D Data
Processing, Visualization and Transmission,2006.

Figure 3. (a)Synthesized view with a simple Gaussian
model; (b) Synthesized view with the proposed
method; (c)-(d) A close look at the two encircled
regions and comparisons (the right hand side images
are from (b)).

References
[1] A. Fitzgibbon, Y. Wexler, and A. Zisserman, “ImageBased Rendering Using Image-Based Priors”, ICCV, pp.
1176-1183, 2003.
[2] P. Gargallo, P.F. Sturm, “Bayesian 3D Modeling from
Images Using Multiple Depth Maps”, CVPR, pp. 885-891,
2005.

Proceedings of the 18th International Conference on Pattern Recognition (ICPR'06)
0-7695-2521-0/06 $20.00 © 2006

Figure 4. Synthesized view from another viewpoint.
Top: Based on a simple Gaussian model; Bottom:
From the proposed method.

Journal of Biomedical Informatics 59 (2016) 102–114

Contents lists available at ScienceDirect

Journal of Biomedical Informatics
journal homepage: www.elsevier.com/locate/yjbin

Affordable, web-based surgical skill training and evaluation tool q
Gazi Islam a,⇑, Kanav Kahol b, Baoxin Li c, Marshall Smith d, Vimla L. Patel a,e
a

Department of Biomedical Informatics, Arizona State University, 13212 E Shea Blvd, Scottsdale, AZ 85259, United States
Public Health Foundation of India, New Delhi, India
c
Department of Computer Science, Arizona State University, 699 S Mill Ave, Tempe, AZ 85281, United States
d
Banner Good Samaritan Medical Center, 1111 E. McDowell Road, Phoenix, AZ 85006, United States
e
The New York Academy of Medicine, 1216 Fifth Ave, New York, NY, United States
b

a r t i c l e

i n f o

Article history:
Received 2 December 2014
Revised 1 November 2015
Accepted 3 November 2015
Available online 10 November 2015
Keywords:
Skill assessment
Feature detection
Computer vision
Web-based evaluation tool
Surgery

a b s t r a c t
Advances in the medical field have increased the need to incorporate modern techniques into surgical
resident training and surgical skills learning. To facilitate this integration, one approach that has gained
credibility is the incorporation of simulator based training to supplement traditional training programs.
However, existing implementations of these training methods still require the constant presence of a
competent surgeon to assess the surgical dexterity of the trainee, which limits the evaluation methods
and relies on subjective evaluation. This research proposes an efficient, effective, and economic videobased skill assessment technique for minimally invasive surgery (MIS). It analyzes a surgeon’s hand
and surgical tool movements and detects features like smoothness, efficiency, and preciseness. The system is capable of providing both real time on-screen feedback and a performance score at the end of the
surgery. Finally, we present a web-based tool where surgeons can securely upload MIS training videos
and receive evaluation scores and an analysis of trainees’ performance trends over time.
Ó 2015 Elsevier Inc. All rights reserved.

1. Introduction
Surgical skill is dependent on both psychomotor and cognitive
proficiency [1]. Surgeons have to learn the art of executing fine
motor movements while maintaining tissue integrity within the
human body. This is a challenging task, one that requires a significant time investment both from senior surgeons mentoring the
residents and the residents themselves. However, this traditional
one-to-one apprenticeship model has significant limitations. First,
there is a dearth of trained surgeons, especially in the developing
world, who qualify as evaluators. Secondly it requires dedicated
time of the surgeon. Hence, it is almost impossible to immediately
evaluate every surgical training exercise. The trainees are only
assessed before moving to the next stage of training. As a result
feedback becomes summative and this lack of granularity and
proximity in the assessment fails to provide meaningful guidance
on how to improve the subtle aspects of the individual’s clinical
skills [2].

q
We acknowledge the support of NSF (Grant No. 0904778). We thank Akshay
Vankipuram for his help in editing the manuscript.
⇑ Corresponding author. Tel.: +1 480 239 9709.
E-mail addresses: gislam@asu.edu (G. Islam), kanav.kahol@phfi.org (K. Kahol),
Baoxin.li@asu.edu
(B.
Li),
mark.smith@bannerhealth.com
(M.
Smith),
vimla.patel@asu.edu, vpatel@nyam.org (V.L. Patel).

http://dx.doi.org/10.1016/j.jbi.2015.11.002
1532-0464/Ó 2015 Elsevier Inc. All rights reserved.

Due to the demand for greater accountability and patient safety
in health care delivery today; effective surgical performance measurement is becoming an absolute necessity [3]. To ensure the best
surgical performance, systematic simulator training programs are
being developed and integrated into traditional training in hospitals [4]. It is a new and progressive way to improve surgical resident training and surgical skills learning. The simulation systems
allow multiple practice sessions, objective measurement of skills
and the ability to deliver remote training. While simulation training is emerging as an effective means of practicing in a consequent
free environment, the evaluation of surgical proficiency in simulation also requires the constant presence of a competent surgeon.
This means that evaluating surgical dexterity still remains highly
subjective and does not yield quantitative data [5]. High-end virtual reality (VR) simulators use sensors that map movements into
virtual space where they are analyzed by algorithms [6–8]. While
this method does offer a degree of objectivity it still has practical
limitations. One such confine is that the introduction of sensor
mechanisms both interfere with the surgeons’ movements [9]
and add adaptations that are not seen in real surgical environments. There are also human constraints, such as thinking of a surgical simulator as a videogame, which has no didactic value [10].
Prior experience with videogames has also been shown to be a
handicap when using simulators [11]. And yet another concern is
that such systems will create a false sense of security, built on

G. Islam et al. / Journal of Biomedical Informatics 59 (2016) 102–114

the development of incorrect habits while getting used to a virtual
environment [10]. Thus the solution lies in developing an objective
proficiency measurement system for surgery that will be (a) reliable, (b) replicable in real surgery, and (c) affordable.
Use of a video camera in surgical skill training has become very
valuable as this video data can be used for future assessment. The
video data can be further analyzed with computer vision analysis
without the use of any external sensors. Computer vision analysis
(herein referred to simply as ‘‘computer vision”) has produced
many technological breakthroughs in the last few decades. It has
been successfully used for object detection, tracking, motion detection and analysis. A variety of computer vision applications that
have been invented in the past few years can be applied in clinical
as well as other domains of biomedical informatics to solve many
problems [12,13]. In this research, open source computer vision
[12] was used to track physicians’ hand and surgical tool motion
from captured video and with the goal of assessing surgical dexterity from the analysis. The tracking of a surgeon’s hand and surgical
tool movements can be one of the most important features in
assessing surgical performance. This project addresses the issues
of cost, replicability in a real environment, and the measurement
of skills without interfering in surgeons’ psychomotor movements
by adding bulky sensors.
The system was used in minimally invasive surgical (MIS) techniques. Unlike open surgery, MIS uses tiny incisions to operate in
constrained environments [14] which requires high levels of psychomotor proficiency and significant training. The specific psychomotor skills and hand-eye coordination needed for this type
of surgery were reinforced through box-trainers and computerenhanced simulation trainers, and the movements of instruments
were captured by cameras by which the videos could later be analyzed by a computer vision system.
The proposed system used an optical flow algorithm to analyze
the surgical field video in real time and provide dynamic feedback
scores in order to assess surgical training performance. We
assessed surgical proficiency as a multidimensional vector composed of motion smoothness, surgical gesture proficiency, and
number of errors. Surgical proficiency is multidimensional by its
nature and it was important to develop a system that captured
each of the dimensions. This required a different algorithm for each
of the proficiency measures. We then tested the hypothesis that
this system could accurately capture surgical proficiency by differentiating between the performances of experts, intermediates, and
novices.
Our system is capable of providing trainees both real time and
summative feedback. Real time feedback was employed to provide
measures to surgeons during the training which helped them
dynamically control the learning experience. Summative feedback
helped summarize their performance to gain knowledge about
trends. We also developed an Internet based tool where users
could upload pre-recorded surgical exercise videos and receive
an immediate proficiency score.

2. Background
Laparoscopic surgery requires many hours of systematic practice on a simulator to acquire psychomotor skills. One simulator
box that has commonly been utilized is the Fundamentals of
Laparoscopic Surgery (FLS), and it has become one of the most
widely used simulators for training. It is endorsed by the American
College of Surgeons (ACS) and establishes a standard set of didactic
information and manual skills serving as a basic curriculum to
guide surgical residents, fellows, and practicing surgeons in the
performance of basic laparoscopic surgery [15]. The training is
composed of a laparoscopic trainer box which consists of a number

103

of non-procedure specific simulation exercises incorporating most
of the psychomotor skills necessary for basic laparoscopic surgery.
We employed exercises from the FLS simulator to develop our
system.
The FLS box offers a number of exercises. However for this
study, three exercises were considered focusing on hand-eye coordination, ambidexterity, and depth perception (Fig. 1) [8]:
 Peg transfer: The peg transfer exercise requires the trainee to
lift six objects with a grasper/dissector with the nondominant hand, transfer the object midair to the dominant
hand and then place each object on a peg on the opposite side
of the board. There is no importance placed on the color of
the objects or the order in which they are placed or where. Once
all six objects have been transferred, the process is reversed.
The exercise is timed and a penalty is imposed for any peg
dropped out of the reach of the tool.
 Intracorporeal suture: This suturing task involves the placement of a suture precisely through two marks on a Penrose
drain that has been slit along its long axis, and then tying an
intracorporeal knot in the suture. The knot must have one double throw and 2 single throws. A penalty is imposed if the drain
is avulsed from the block to which it is secured by double-sided
adhesive tape.
 Shape cutting: This cutting exercise requires cutting out a circle
from a square piece of gauze. One hand should be used to provide traction on the gauze using the grasper and to place the
gauze at the best possible angle to the cutting hand. A penalty
is imposed for any deviation from the line demarcating the
circle.
In iterative practice sessions on the FLS, a user receives only
summative feedback after a certain number of repetitions. Currently there are two feedback systems available. One requires the
presence of an observer to monitor exercise completion time and
committed errors which are sent to the FLS to score. The FLS typically returns the result (pass or fail) in 4–6 weeks. The second system requires post-hoc analysis on a recorded surgery of a
competent surgeon or trainer to subjectively assess the surgical
dexterity of the trainee by providing a composite score, which
lacks inter-rater reliability and is an expensive process. However,
both these methods remain primarily subjective in nature.
Several video and sensor-based systems have been developed to
capture a user’s motion and other important features which can be
later analyzed objectively and quantitatively and correlated with
the skill level [16]. However, most of the quantitative skill assessment systems available are sensor-based, i.e., sensors are integrated into surgical tools or surgeon’s gloves to track different
movement features. Although these sensory systems capture
motion features quite well, there has not been enough work done
in combining tool and hand movement data together to assess the
surgical proficiency. Other researchers compelled surgeons to wear
sensors to monitor certain features or body’s center of pressure
value and observed the correlation with the skill execution, but
unfortunately these kinds of data are not sufficiently comprehensive to successfully assess the skill level. Moreover, skill assessments are usually performed only in simulated training
environment since the integration of wearable sensors in live surgery interferes with proper surgical skill execution [16]. And even
then the sensors have to be sterilized to be used in real surgery
which increases the cost of the entire surgical procedure [17].
Unfortunately, none of these techniques described accurately
assess actual surgical competence.
These drawbacks of sensor integration caused a shift in focus
toward video based systems. Some video-based systems use only
external cameras to capture hand movement while performing sur-

104

G. Islam et al. / Journal of Biomedical Informatics 59 (2016) 102–114

Fig. 1. Exercises for fundamentals of laparoscopic surgery.

gical exercise. This approach requires the camera to have an unobstructed view of the hands and is heavily reliant on the consistency
of the ambient lighting, a lack of background noise, and the resolution of the video [16]. And due to the use of long instruments in MIS,
it is almost impossible to derive tool movement data from a hand
movement video. Other video-based approaches track surgical
tool-tip; however, this analysis is solely dependent on the
travel-length value of the tool-tip. At this point no work has been
done that combines analyses from both hand and surgical tool
movement. This is critical in surgery where surgeons’ hand and
tool movements are important components of ensuring patient
safety.
Thus there exists a need for a complete performance evaluation
system with automatic tracking. In the present study, we aimed to
develop a video-based assessment tool for laparoscopic surgery
training to assess the generic and specific technical aspects of surgical features. This method of assessment has previously been proven highly effective in improving technical skills acquisition and
self-assessment [18]. The recorded videos can be shared securely
via online for quicker distribution to experts for assessment [19].
Thus an Internet based video sharing tool was developed where
recorded videos could be uploaded and assessed automatically by
the developed algorithm.
3. Methodology
The proposed system in this research was designed to work
with the Fundamentals of Laparoscopic Surgery trainer box. Video
recordings rendered from inside the box and two external cameras
were analyzed to provide formative feedback scores on the screen
upon the completion of each exercise (Fig. 2). The idea was to
assess both hand and tool data and calculate a score which is fed
back to the user.

H

8
if V ¼ R
>
< 60ðG  BÞ=S
120 þ 60ðB  RÞ=S if V ¼ G
>
:
240 þ 60ðR  GÞ=S if V ¼ B

If H < 0 then H
H þ 360.
The HSV image of the video was split into a single channel Hue
image. To isolate marker of the tool and hands, a histogram of the
Hue channel was calculated. Color of the tool-marker was red and
the gloves were purple whose corresponding histogram values
were found and binary thresholding applied to detect the tool
and the hand respectively. In this case, if the pixel value in the
hue channel matched the histogram color value, it was changed
to 255 i.e., white. The rest of the pixels were set to 0 i.e., black.


dstðx; yÞ ¼

maxValue if srcðx; yÞ > threshold
0
otherwise

Finally to maximize the elimination of noise, advanced morphological transformation was used. Advanced morphological transformation performed as a noise filter by using erosion and
dilation as basic operations. Erode and dilate functions use the
specified structuring element that determines the shape of a pixel
neighborhood over which the minimum and maximum (respectively) is taken [12].

dst ¼ openðsrc; elementÞ ¼ dilateðerodeðsrc; elementÞ; elementÞ
where

erodeðx; yÞ ¼
dilateðx; yÞ ¼

min

ðx0 ;y0 Þ2kemel

srcðx þ x0 ; y þ y0 Þ

max srcðx þ x0 ; y þ y0 Þ

ðx0 ;y0 Þ2kemel

Figs. 3 and 4 show the noise-free detection tools and hands
respectively.

3.1. Object segmentation
The exercises in the FLS require surgeons to move certain
objects. Analysis of the movements, jerkiness and placement of
the objects can help provide assessments of the surgeons’ level of
proficiency. For example, a piece of gauze that is cut by the surgeon
provides valuable information about the surgeons’ proficiency,
since the shape of the cut, its size and duration to execute that
cut are all important cues. The system therefore required an efficient object segmentation algorithm. We tested a number of computer vision algorithms to analyze motion.
The color image was converted from a Red–Green–Blue (RGB)
color space to Hue–Saturation–Value (HSV) color space. In the
RGB model, images are represented by three components, one for
each of the primary colors – red, green and blue. However, the
HSV color space can capture the distinct image features better than
RGB color space [20,21]. The HSV image contains 3 channels: hue
(H), saturation (S) and value (V) where Hue is a color attribute
and represents a dominant color [22]:

3.2. Motion detection
Measuring motion and its features as mentioned before is critical to assessing surgical proficiency. In the video, the sequence of
images are taken at a fixed time interval. The motion of objects in
3-D induces 2-D motion in the image plane. The motion is called
optical flow. There are several methods for calculating optical flow.
By applying a frame-to-frame differencing technique to find the
object silhouette, and motion history images (MHI), the dynamic
part of the scene was captured. Frame-to-frame differentiating
function calculates absolute difference between two arrays of consecutive frames [12].

dst ¼ jsrc1  src2j
The function extracts templates by thresholding frame differences and then updates the motion history image by passing the
resulting silhouette.

G. Islam et al. / Journal of Biomedical Informatics 59 (2016) 102–114

105

Fig. 2. Proposed system with integrated hand/tool video and a feedback system.

Fig. 3. Noise-free tool detection applying advanced morphological transformation.

8
>
< timestamp if silhouetteðx; yÞ – 0
mhiðx; yÞ ¼ 0
if silhouetteðx; yÞ ¼ 0 and mhi < ðtimestamp  durationÞ
>
:
mhiðx; yÞ
otherwise

That is, MHI pixels where motion occurs are set to the current
timestamp, while the pixels where motion happened before specified duration are cleared (Fig. 5) [23].
The gradients of the resulting motion history image were taken
to produce a mask of valid gradients. First the derivatives Dx and
Dy of MHI are calculated to find the gradient orientation as:

orientationðx; yÞ ¼ TAN1

Dyðx; yÞ
Dxðx; yÞ

After that, the mask was filled to indicate where the orientation
was valid. For the local motion segments, small segmentation areas
were first rejected and then the orientation was calculated using

regions of interest (ROIs) that bound the local motions; then the
areas of valid motion within the local ROIs were calculated. Any
such motion area that was too small was rejected. Fig. 6 shows
the detected motion in both hand and tool videos. The algorithm
recorded the coordinates of the center point of motion in pixel
and the gradient of movement in degrees.
3.3. Feature extraction
Two arrays of data were extracted from both left and right hand
and tool positions (pixel) and gradients (degree). Coordinates of the
tool were analyzed to extract smoothness, extra movement, and

106

G. Islam et al. / Journal of Biomedical Informatics 59 (2016) 102–114

Fig. 4. Noise-free hand detection applying advanced morphological transformation.

Fig. 5. Example of motion history image (MHI) of hand movement.

movement efficiency. Tremor was calculated by analyzing both the
position and direction of movement. Angular values of the direction
of hand movement were interpreted into 4 gestures – left, right, up,
down and stationary for further extraction of features like perception of depth and hand movement efficiency.
3.3.1. Motion smoothness (tool video)
Smooth steady motion is one of the most important features in
assessing surgical skill [24]. From the coordinates of the tool position, Euclidean distance between every frame was calculated. If the
hand movement is x, then hand motion, acceleration and jerkiness
could be found from the following equations:

Hand motion ¼

dx
dt

3.3.2. Snaps (tool video)
Extra movement such as snaps is another important factor that
may be used to determine surgical proficiency. It was calculated by
the 4th derivative of hand movement [25]. Total value of positive
snaps was calculated to find this movement feature in every
exercise.
4

Snaps ¼
2

Hand motion acceleration ¼

d x
dt

d x
dt

d x
dt

4

2

3

hand motion jerkiness ¼

for movement of 5 frames respectively. The figure also shows that
deceleration, negative jerk, and snaps features were derived from
it, but deceleration and its higher derivatives were ignored since
they have no association with smooth and steady motion. Positive
jerk value for the entire duration of exercise was calculated to find
the counter feature i.e., motion smoothness [25].

3

Tool movement for 5 consecutive frames is summarized in
Fig. 7, which shows velocity, acceleration, jerkiness and snap of
the tool movement which are the 1st, 2nd, 3rd and 4th derivatives

3.3.3. Movement efficiency (tool video)
On surgical simulators measures of economy of tool movement
have been shown to be reliable, valid, and objective measures of
technical competence [26]. Once the Euclidean distance between
tool positions and the starting frame tool position was calculated
for each frame, the values were averaged to find the tool movement efficiency. Average movements of both left and right hands

G. Islam et al. / Journal of Biomedical Informatics 59 (2016) 102–114

107

Fig. 6. Detected motion in hand and tool videos.

3.3.4. Tremor (tool video)
Tremor is an involuntary, roughly sinusoidal component inherent in a normal human hand motion. It has been found to consist of
a ‘‘mechanical–reflex” component which is thought to originate
from the central nervous system and has a frequency range of 8–
12 Hz [24]. Imprecision in laparoscopic surgery due to tremor
has long been a concern. Surgical tool tremor was calculated by
the motions that change directions in 2 frames and had a value
of no more than 5 pixels. It was characterized by the following
function where numbers in the parentheses refer to the frame
numbers:

Fig. 7. Different order derivatives of tool movement across five frames.

Fig. 8. Trajectory of tool movement of left and right hands of novice and expert
surgeons.

of novices and experts are shown in Fig. 8. It is clear that novices
expended much more movement than the experts for the same
exercises.

If ((direction[6] != direction[4]) && (movement[6] 
movement[4] <= 5)) {
If ((direction[4] != direction[2]) && (movement[4] 
movement[2] <= 5)) {
If ((direction[2] != direction[0]) && (movement[2] 
movement[0] <= 5)) {
tremor = true;
}
Else
tremor = false;
}
}

3.3.5. Depth perception (hand video)
Repetitive motion toward the direction of the tool was recorded
as up-down motion and accounted for the perception of depth.
Table 1 categorized up-down movements with a range of angular
movements. This repetitive hand movement is observed mostly
in the inexperienced residents due to, (1) translation of the
2-dimentional image of the operating field from the video screen
into a 3-dimentional mental image [27], (2) learning to operate
using long instruments, and (3) mastering ambidexterity and
eye-hand coordination. This motion was calculated by the total
number of hand motions both in up and down directions.

108

G. Islam et al. / Journal of Biomedical Informatics 59 (2016) 102–114

Table 1
Segmentation of movement from angular values.
Angle

Direction

45°<, 6 135°
135°<, 6 225°
225°<, 6 315°
45°P, > 315°
0

Up
Left
Down
Right
No movement

3.3.6. Bimanual dexterity (hand video)
Bimanual dexterity is a measurement of how well a resident is
able to optimize the use of both hands [28,29]. If the resident
ignores the non-dominant hand, then he/she probably has not
mastered the bimanual dexterity. Bimanual dexterity was calculated by comparing percentage of idle time for each of the hands
for the total time to complete the exercise.
3.4. Detecting error
Errors vary from task-to-task. Three different algorithms were
developed to detect task-specific errors from the tool videos and
record separately in peg transfer, intracorporeal suturing, and
shape cutting exercises.
3.4.1. Peg transfer
The developed algorithm detected the colorful triangular
objects and computed the total number by counting all of the pixels inside the blue rectangle (Fig. 9). If at the end of the exercise all
the objects were not inside the rectangle, then the missing number
of objects was recorded as errors.
3.4.2. Intracorporeal suturing
During intracorporeal suturing, if the Penrose drain was pulled
excessively from its original location, then it was considered to be
tissue damage [30]. The program detected the Penrose drain and
automatically calculated the deviation from its original position
during the exercise (Fig. 10).
3.4.3. Shape cutting
In the shape cutting exercise, the user excised the required
piece of gauze and then placed that piece under the camera, and
upon pressing ‘spacebar’, the program automatically detected the
shape of both the inside and outside cuts. The program saved an
image (snapshot) and converted it from an RGB to grayscale image.
Then it employed a Gaussian blur filter to smooth the image and
also a Canny’s filter to find edges. Finally the Hough transformation
found the actual circle and two binary images were produced for

Fig. 10. Detection of tissue damage during exercise.

the contents outside and inside the circle respectively (Fig. 11).
By counting the number of white pixels, the program automatically
found the outside and inside imperfection and provided a combined precision of cut score.
3.5. Multivariable linear regression
The computer vision algorithm extracted six features from each
of the laparoscopic cholecystectomy videos along with their corresponding hand movements. Each of the videos was rated by expert
surgeons in three categories: smoothness, efficiency, and precision.
Sets of three of the extracted features were found to be associated
with each of the rated features by the experts. For example, the
smoothness score was associated with the jerkiness, snaps and tremor; and the efficiency score was associated with movement efficiency, depth perception and bimanual dexterity. Three of the
extracted features were grouped into a single feature which the
experts rated. Multivariate linear regression models were built
using the tool ‘‘Weka” to find the relationship between extracted
features and experts’ scores.

ha ðsmoothÞ ¼ a0 þ a1 ðjerkinesssÞ þ a2 ðsnapsÞ þ a3 ðtremorÞ
hb ðefficiencyÞ ¼ b0 þ b1 ðmovement efficiencyÞ
þ b2 ðdepth perceptionÞ þ b3 ðbimanual dexterityÞ
The precision score was solely associated with the number of
error committed and the time taken to complete the exercise. After
discussions with expert surgeons, the equation for measuring error
for each of the surgery types was created and is given below:

errorPeg transfer ¼

1
 ððcompletion timeðif > 48Þ  48Þ s
5
þ ðNo: of drops  25Þ sÞ

errorIntracorporeal suture ¼

errorShape cutting ¼

Fig. 9. Error recording in peg transfer exercise by automatic detection of an object
outside the test area.

1
 ððcompletion timeðif > 112Þ  112Þ s
12
þ tissue damageÞ

1
 ððcompletion timeðif > 98Þ  98Þ s
10
þ cut imperfectionÞ

48 s, 112 s and 98 s are the average time for experts to complete peg
transfer, intracorporeal suture and shape cutting exercise
respectively.

G. Islam et al. / Journal of Biomedical Informatics 59 (2016) 102–114

109

Fig. 11. Automatic measuring of precision in shape cutting task.

3.6. Final scores
Once the exercise was completed, the system would calculate
scores for each feature and display it on the screen along with a
composite score. Features from hand and tool videos were normalized to 100. Finally, the following scoring scheme was employed
for each exercise to calculate the composite score:

scorePeg transfer ¼

ha ðsmoothÞ þ hb ðefficiencyÞ
 errorPeg transfer
2

scoreIntracorporeal suturing ¼

scoreShape cutting ¼

ha ðsmoothÞ þ hb ðefficiencyÞ
2
 errorIntracorporeal suture

ha ðsmoothÞ þ hb ðefficiencyÞ
 errorShape cutting
2

3.7. Training efficacy
A control group and a test group of subjects were used to test
the effectiveness of the feedback system. No feedback was provided to the control group, but the test group was provided
real-time feedback on motion smoothness, movement efficiency,
number of committed errors and elapsed time. Each subject from
both of the groups repeated a single surgical exercise after three
weeks. For each of the subjects, a performance score for each
parameter was calculated and added to form a composite score.
A learning curve was created by plotting the composite score over
time using the Matlab program. We expected the learning curve for
the test group to be steeper than that of the control group
(see Fig. 12).

Fig. 12. Internet based system for training of the algorithms.

expert surgeons could login into the system and rate the deidentified videos in smoothness, efficiency, and precision. These
ratings were then used to train the neural network to improve
the accuracy of the evaluations.

3.8. Webtool integration
We also designed and implemented an Internet based system
which could automatically evaluate a FLS exercise video. The website was hosted on a secured server where users could login and
upload videos and receive instant evaluation scores on different
performance features. Both computer vision and neural network
algorithms were run to provide the assessment score. Users could
also access their previous assessment scores and observe the progress. All these videos were de-identified and stored in a secured
repository. The website also contained the rater’s account where

4. Results
Video data of medical students and surgical residents (system
users) from various post graduate years (PGY) were collected while
practicing peg transfer, intra corporeal suturing and shape cutting
exercises using the FLS trainer. Each subject conducted the exercise
at two consecutive times, and videos of the second trial were analyzed. Each exercise data was captured with three synchronous
cameras; two cameras for capturing each of the hand movements

110

G. Islam et al. / Journal of Biomedical Informatics 59 (2016) 102–114

Table 2
Number and expertise levels of subjects.
Group

Experience level

No. of subjects

Expert
Intermediate
Novice

PGY5–PGY4
PGY1–PGY3
Medical students

8
12
32

and one camera for capturing the tool movement. A total of 52
sample videos were collected and grouped as shown in Table 2.

4.1. Inter-rater reliability
Fifty-two de-identified FLS exercise videos were sent to three
expert surgeons for evaluation. Each video was analyzed and rated
in terms of smoothness, efficiency and precision. To find the interrater reliability, Fleiss’ Kappa coefficient between experts’ score
was calculated. Fleiss’ kappa is a statistical measure for assessing
the reliability of agreement between a fixed numbers of raters
when assigning categorical ratings to a number of items or classifying items. Table 3 shows fair to moderate agreement between
the raters with significant p-value.

Percentage of time the non-dominant hand moved compared to
the dominant hand was calculated to assess bimanual dexterity.
Both intermediates and novices showed almost 40% less activity
in the non-dominant hand as compared to the experts (Fig. 15).
Table 4 presents the results of ANOVA among the different skill
levels which was found to be very significant for all six features.
p < 0.05 was taken as statistically significant.
Random Forest classifier was used for the classification of each
gesture data. Sixty-six percent of the data for both hand and tool
gestures was used as training data. The remainder 34% of the data
was unlabeled and used for testing of each gesture. Each gesture
shows a significantly high true positive rate of detected features
(Table 5).
Motion smoothness, motion redundancy and tremor value for
both hand and tool gestures were normalized and Linear Discriminant Analysis (LDA) was performed. Fig. 16 shows the result of the
LDA analysis for both hand and tool gestures where the data
roughly conform to 3 distinct regions in a 2-dimensional projection
space. These initial experiments validated the hypothesis that LDA
could be used to simplify the original data into a simpler, lowdimensional data set. In addition the features from the tool movements were more accurate at different levels of expertise than
those of the hand movements.
4.3. Smoothness and efficiency score

4.2. Stratification of expertise
All of the frames of the surgical tool and hand videos were analyzed. Arrays of position and movement gradients were captured
from each frame where each sample consisted of both left and
right tools/hands. The angular value of the movement gradient
was converted to four motions: left, up, right and down. Several
algorithms analyzed the displacement and direction of movements
and prepared an occurrence matrix for each of the gestures.
Fig. 13 shows the jerkiness, snaps and tremor scores for both
the left and right surgical tools of participants from three levels
of expertise. The histogram clearly shows that experts had lower
jerkiness scores, hence their movements were smoother than those
of intermediates and novices. The total numbers of snaps were also
calculated and average snaps among the different expertise groups
were displayed in Fig. 13. Tremors were difficult to assess subjectively, however the motion detection algorithm was able to detect
very subtle movement and the system calculated tremors for both
hands and tools. The figure shows a decrease in tremor features
with the increasing experience level of the examiners. Due to the
use of longer instruments, very subtle hand movement could cause
greater tool movement and thus a greater number of tremors were
noticeable in tool analysis.
Motion redundancy for both hand movements was analyzed.
More than 65% of the redundant motion was observed in updown direction, which results from extra motion in perception of
depth. Fig. 14 shows motion redundancy in up-down direction. It
appears that as the level of expertise increases the perception of
depth also increases, producing a reduction in vertical redundant
motion. In addition, the average traveling distances for the tools
were also calculated and it showed that experts required on average 40% less movement than novices to complete the same
exercise.

Training dataset was used to find the regression parameters.
Once all the parameters were determined, the test data set was
run to find the correlation coefficient between raters’ average score
and predicted score. Both equations showed more than 90% correlation with significant p-value (Table 6).
4.4. Error detection
For error detection, the automatically generated error score was
compared to the observers score for each of the videos. The system
was able to detect drops of triangular objects in peg transfer exercise very accurately. The overall sensitivity of the system was
found to be 87% (Table 7).
4.5. Efficient skill learning
The proposed video-based surgical skill assessment technique
could provide immediate feedback, hence it was also tested as a
tool for the efficient skill learning technique. Thirty-two medical
students (novices) were used as the control group where noscreen feedback or assessment score was provided. Twenty-two
medical students were used as the experimental group where they
were provided real-time on-screen feedback and assessment score
at the end of every trial. All participants in each group performed
the peg transfer and shape cutting exercise at least 16 times. The
average score for each of the two groups as a function of the number of trails is shown in Fig. 17, where an improved learning curve
was observed for the experimental group. Paired t-test was performed showing a significant difference in performance between
the two groups (p < 0.0001).
5. Discussion

Table 3
Inter-rater reliability: Fleiss’ Kappa coefficient between raters.

Smoothness
Efficiency
Precision

Fleiss’ kappa

P

Agreement

0.4349
0.3581
0.4393

<0.0001
0.0001
<0.0001

Moderate agreement
Fair agreement
Moderate agreement

In the present study we developed and evaluated a video-based
assessment tool for laparoscopic surgery training to assess the generic and specific technical aspects of surgical skills. Fifty-four FLS
training exercise videos performed by medical students and residents from various postgraduate years of training were analyzed.
Several functions were developed that combined a series of com-

111

G. Islam et al. / Journal of Biomedical Informatics 59 (2016) 102–114

Fig. 13. Jerkiness, snaps and tremor scores as a function of expertise.

Fig. 14. Perception of depth and movement efficiency as a function of expertise.

Table 5
Random forest analysis of training and testing data sets by tool and hand gestures.
Gestures

True positive (%)

False positive (%)

Tool

Smoothness
Snaps
Movement efficiency
Tremor

72.2
66.7
60.0
80.0

17.7
34.8
29.7
4.5

Hand

Depth perception
Bimanual dexterity

72.2
94.4

16.0
1.6

Fig. 15. Comparison of bimanual dexterity as a function of expertise.

Table 4
Comparison of skill levels by expertise, using the tool and hand gestures (ANOVA
analysis).
Gestures

Tool

Smoothness
Snaps
Movement
efficiency
Tremor

Hand

Depth
perception
Bimanual
dexterity

Expert vs
intermediate
p-value
0.0032
0.0425
0.0295

Intermediate vs
novice p-value

Expert vs
novice p-value

<0.0001
0.0325
0.0002

<0.0001
0.00345
<0.0001

0.0002

0.0045

<0.0001

<0.0001

<0.0001

0.0001

<0.0001

<0.0001

<0.0001

puter vision algorithms to accurately track the surgical tool-tips,
the surgeon’s hands and the objects in the surgical scene. The
motion detection function tracked the position and direction of
the surgical tools and the surgeon’s hand movements. It then used
this information to extract a number of psychomotor skill assess-

ment features which included jerkiness, snaps, tremor, movement
efficiency, perception of depth and bimanual dexterity. For each of
the features, ANOVA analysis showed a statistically significant difference in variance between the consecutive expertise groups, i.e.
novices-vs-intermediates and intermediates-vs-experts. Also when
classified into 3 groups for each of these features (using Random
Forest classifier), it showed an average of 74% correctly classified
groups according to the expertise level. Each of these videos were
de-identified and sent to three experts to be rated in three categories: smoothness, efficiency, and precision. Only scores with high
inter-rater reliability were utilized in the development of the scoring algorithms. Sets of three of the extracted features were found
to be associated with one of the rated features by the experts.
For example, the smoothness score was associated with the jerkiness, snaps and tremor; the efficiency score is associated with
movement efficiency, depth perception, and bimanual dexterity;
and precision was associated with the number of error committed
and the time taken to complete the exercise. A multivariate linear
regression model was built for each of the categories and regression parameters were found using the tool ‘‘Weka”. The entire
dataset was split into a 60% training set, 10% cross-validation set
and 30% testing set to find the parameters and test the regression
models. Analysis results showed a very high correlation (91% aver-

112

G. Islam et al. / Journal of Biomedical Informatics 59 (2016) 102–114

Fig. 16. Linear discriminant analysis of hand and tool gestures at different levels of expertise.

Table 6
Correlation between raters’ average and predicted scores.
Features
Smoothness
Efficiency

4.86  0.74  (jerk)  0.29
 (snap)  0.92  (tremor)
1.67  0.41  (movement
efficiency) + 0.64  (bimanual
dexterity)
 0.94  (depth perception)

Correlation
coefficient

p-value

0.9241

<0.0001

0.9028

<0.0001

Table 7
Correlation between detected and actual errors on three tasks.

Peg transfer
Intracorporeal suturing
Shape cutting

Drop count
Tissue damage
Precision of cut

Correlation coefficient

p-value

0.98
0.77
0.86

<0.0001
0.0008
<0.0001

Fig. 17. Learning curve over the number of trials with and without feedback.

age) of experts’ ratings with significant p-values. The system automatically detected the number of errors committed during each
exercise. Errors in this situation were very task specific and identified by continuous observations by the expert observers. Overall
the system successfully detected 87% of the errors automatically,
thus supporting the hypothesis that the skill assessment provided
by the proposed automatic quantitative scoring system is equivalent to that of the gold standard.
Although our current findings contrasted an expert group with
a novice group, expertise is best viewed as a continuum with a
number of levels that result in unique performance characteristics.
The development of expertise is marked by specific transitions corresponding to reorganizations of knowledge and non-monotonic
(not linear) increases in the learning curve [31]. To observe the

effect of the feedback system on the process of development of
expertise, we conducted an experiment with 54 medical students
where each performed 2 specific FLS box exercises 16 times. None
of the subjects had any prior experience with the task. Thirty-two
of the novices were used as the control group where no performance feedback was provided during the learning period. They
simply followed the provided guidelines for the tasks. For the other
22 novices (experimental group), on-screen real-time feedback and
performance score were provided subsequent to each exercise.
The performance curve for the control group of novices who
were not provided any feedback during the course of the 16 exercise session showed an ‘‘intermediate effect” in their learning
curve. Intermediate effect is defined by Patel et al. [32,33] as ‘‘a
temporary decline in performance as knowledge is acquired and
organized, when a linear increase in performance with the length
of training or time on task would be expected”. On the other hand,
the performance curve of the experimental group exhibited a more
steady performance throughout the learning session. Although the
rate of learning became saturated after a certain number of trials,
the learning curve did not show the intermediate effect in the
experimental group. Paired t-test showed significant improvement
in the performance of the experimental group over the control
group. The average group score showed an 11% better performance
for the experimental group over the control group, thus supporting
the second hypothesis that the immediate feedback system would
increase training effectiveness by reducing the time it takes to
attain a desired level of proficiency.
Finally, we developed an Internet based tool where a user could
upload FLS exercise videos and receive an immediate assessment
score. Users could also track their progress by observing their past
scores. All these videos were de-identified and added to an online
surgical video repository. Expert raters could observe these videos
and provide performance score for smoothness, precision and efficiency features. Once a video rating by a minimum of three expert
was obtained, the score was used to retrain the neural network
(see Fig. 18).
The main limitation of our study was the dearth of surgical
videos. Although videos of the surgical procedures were readily
available; capturing hand video required placing an additional
camera on the system. Moreover, each tool and hand video was
rated by three expert surgeons in three different categories, and
only videos with high inter-rater reliability were included in the
study. This constraint reduced the total number of usable FLS
videos. Despite these shortcomings, there was significant statistical significance in the reliability and validity of the assessment
tool. For future studies, we aim to increase this number by capturing more surgical videos. The web-based assessment application is
expected be a useful tool for acquiring a large number of video ratings from experts.

G. Islam et al. / Journal of Biomedical Informatics 59 (2016) 102–114

113

Fig. 18. Internet based video assessment tool for efficiency, smoothness and precision.

6. Conclusion
This research presents a video-based surgical skill assessment
technique. We used a computer vision algorithm to analyze the
video of a surgeon’s hand and surgical tool movement, and extract
features like surgical tool-movement smoothness, movement efficiency, individual gesture proficiency, and task specific errors. Data
from different surgical residents at various level of training and
expert surgeons were collected to train and test the algorithm.
Since the research analyzes video data of the surgery rather than
any wearable sensors, it is cost effective and also overcomes the
drawbacks of most of surgical skill assessment techniques presently available.
The proposed video-based surgical skill assessment technique
can provide real-time on-screen feedback, so it is also being tested
as a tool for an efficient skill learning technique. After analyzing
data from the experiment with an on-screen feedback system,
the results showed steeper learning curve than the system without
the feedback. More data is being collected for analysis to help further strengthen this hypothesis.
Objective evaluation remains the holy grail of this line of experimentation, and to that effect the ultimate achievement would be
an acceptance of this tool by the various Boards of Surgical Specialties as a validated tool. While there are several challenges that

need to be addressed to achieve this, such as large scale multisite
trials and repeatability, the work presented here lays the foundation for such experimentation. A huge opportunity lies in addressing the need for objective evaluation of both cognitive and
psychomotor surgical skills concurrently. We also need to understand how surgical errors evolve [34], and if such a system can help
predict an error before it occurs. For example, could increased snap
values potentially predict an impending mistake? While these
remain important questions, the work here takes the first step at
developing a comprehensive, affordable and scalable approach to
surgical proficiency determination. In addition to the developed
informatics tool presenting a practical solution, it hopefully will
also encourage research and investigation in this area.
Appendix A. Supplementary material
Supplementary data associated with this article can be found, in
the online version, at http://dx.doi.org/10.1016/j.jbi.2015.11.002.
References
[1] F. Treves, A Manual of Operative Surgery, Cassell and Company, London, 1891.
[2] J. Ende, Feedback in clinical medical education, JAMA 250 (6) (1983) 777–781.
[3] L.T. Kohn, J.M. Corrigan, M.S. Donaldson, To. Err, To Err is Human: Building a
Safer Health System, National Academy Press, Washington, DC, 2000.

114

G. Islam et al. / Journal of Biomedical Informatics 59 (2016) 102–114

[4] L.M. Sutherland, P.F. Middleton, A. Anthony, J. Hamdorf, P. Cregan, D. Scott, G.J.
Maddern, Surgical simulation: a systematic review, Ann. Surg. 243 (3) (2006)
291–300.
[5] A. Dosis, F. Bello, T. Rockall, Y. Munz, K. Moorthy, S. Martin, A. Darzi, ROVIMAS:
a software package for assessing surgical skills using the da Vinci
telemanipulator system, in: 4th International IEEE EMBS Special Topic
Conference on Information Technology Applications in Biomedicine, 2003.
[6] A. Dosis, R. Aggarwal, F. Bello, Synchronized video and motion analysis for the
assessment of procedures in the operating theater, Arch. Surg. 140 (3) (2005)
293–299.
[7] R. Aggarwal, T. Grantcharov, K. Moorthy, T. Milland, A. Darzi, Toward feasible,
valid, and reliable video-based assessments of technical surgical skills in the
operating room, Ann. Surg. 247 (2) (2008) 372–379.
[8] R. Aggarwal, T. Grantcharov, K. Moorthy, T. Milland, P. Papasavas, A. Dosis, F.
Bello, A. Darzi, An evaluation of the feasibility, validity, and reliability of
laparoscopic skills assessment in the operating room, Ann. Surg. 245 (6) (2007)
992–999.
[9] K. Kahol, M. Vankipuram, Hand motion expertise analysis using dynamic
hierarchical activity modeling and isomap, in: 19th International Conference
on Pattern Recognition, 2008, ICPR 2008, 2008.
[10] I. Oropesa, P. Sánchez-González, P. Lamata, M.K. Chmarra, J.B. Pagador, J.A.
Sánchez-Margallo, F.M. Sánchez-Margallo, E.J. Gómez, Methods and tools for
objective assessment of psychomotor skills in laparoscopic surgery, J. Surg.
Res. 171 (1) (2011) e81–e95.
[11] J. Lynch, P. Aughwane, T.M. Hammond, Video games and surgical ability: a
literature review, J. Surg. Educ. 67 (3) (2010) 184–189.
[12] G. Bradski, A. Kaehler, Learning OpenCV: Computer Vision with the OpenCV
Library, O”Reilly Media Inc., Sebastopol, CA, 2008.
[13] L.G. Shapiro, G.C. Stockman, Computer Vision, Prentice-Hall Inc., USA, 2001.
[14] R. Aggarwal, T. Grantcharov, K. Moorthy, J. Hance, A. Darzi, A competencybased virtual reality training curriculum for the acquisition of laparoscopic
psychomotor skill, Am. J. Surg. 191 (1) (2006) 128–133.
[15] Fundamentals of Laparoscopic Surgery. . . the definitive laparoscopic skills
enhancement and assessment module. <http://www.flsprogram.org/>.
[16] J. Chen, M. Yeasin, R. Sharma, Visual modelling and evaluation of surgical skill,
Pattern Anal. Appl. 6 (2003) 1–11.
[17] M. Aizuddin, N. Oshima, R. Midorikawa, A. Takanishi, Development of sensor
system for effective evaluation of surgical skill, in: The First IEEE/RAS-EMBS
International Conference on Biomedical Robotics and Biomechatronics, 2006,
BioRob 2006, 2006.
[18] P.J. Driscoll, A.M. Paisley, S. Paterson-Brown, Video assessment of basic
surgical trainees’ operative skills, Am. J. Surg. 196 (2) (2008) 265–272.

[19] R. Gonzalez, J. Martinez, E. Menzo, A. Iglesias, C. Ro, A. Madan, Consumer-based
technology for distribution of surgical videos for objective evaluation, Surg.
Endosc. 26 (8) (2012) 2179–2182.
[20] C. Wen, Y.Q. Shi, X. Guorong, Identifying computer graphics using HSV color
model and statistical moments of characteristic functions, in: 2007 IEEE
International Conference on Multimedia and Expo, 2007.
[21] A.R. Smith, Color gamut transform pairs, SIGGRAPH Comput. Graph. 12 (3)
(1978) 12–19.
[22] S. Sural, Q. Gang, S. Pramanik, Segmentation and histogram generation using
the HSV color space for image retrieval, in: Proceedings of the 2002
International Conference on Image Processing, 2002.
[23] M. Shah, Fundamentals of Computer Vision, University of Central Florida,
Orlando, FL, 1997.
[24] C.N. Riviere, R.S. Rader, N.V. Thakor, Adaptive canceling of physiological
tremor for improved precision in microsurgery, Biomed. Eng. (1998).
[25] M.J. Richardson, T. Flash, Comparing smooth arm movements with the twothirds power law and the related segmented-control hypothesis, J. Neurosci.
22 (18) (2002) 8201–8211.
[26] E.D. Grober, M. Roberts, E. Shin, M. Mahdi, V. Bacal, Intraoperative assessment
of technical skills on live patients using economy of hand motion: establishing
learning curves of surgical competence, Am. J. Surg. 199 (1) (2010) 81–85.
[27] B. Pamela, E. Andreatta, M. Derek, T. Woodrum, M. Rebecca, M. Minter,
Laparoscopic skills are improved with lapmentorTM training, Ann. Surg. 243 (6)
(2006) 854–863.
[28] A.A. Gumbs, N.J. Hogle, D.L. Fowler, Evaluation of resident laparoscopic
performance using global operative assessment of laparoscopic skills, J. Am.
Coll. Surg. 204 (2) (2007) 308–313.
[29] M.C. Vassiliou, L.S. Feldman, C.G. Andrew, S. Bergman, K. Leffondré, D.
Stanbridge, G.M. Fried, A global assessment tool for evaluation of
intraoperative laparoscopic skills, Am. J. Surg. 190 (1) (2005) 107–113.
[30] S.P. Rodrigues, T. Horeman, J. Dankelman, J.J. van den Dobbelsteen, F.W. Jansen,
Suturing intraabdominal organs: When do we cause tissue damage?, Surg
Endosc. 26 (4) (2012) 1005–1009.
[31] V.L. Patel, G.J. Groen, Developmental accounts of the transition from medical
student to doctor: some problems and suggestions, Med. Educ. 25 (6) (1991)
527–535.
[32] V.L. Patel, J.F. Arocha, D.R. Kaufman, A primer on aspects of cognition for
medical informatics, J. Am. Med. Inform. Assoc. 8 (4) (2001) 324–343.
[33] V.L. Patel, D.R. Kaufman, J.F. Arocha, Emerging paradigms of cognition in
medical decision-making, J. Biomed. Inform. 35 (1) (2002) 52–75.
[34] V.L. Patel, T. Cohen, New perspectives on error in critical care, Curr. Opin. Crit.
Care 14 (4) (2008) 456–459.

Learning Motion Correlation for Tracking Articulated Human Body with a
Rao-Blackwellised Particle Filter
Xinyu Xu, Baoxin Li
Department of Computer Science and Engineering
Arizona State University
{xinyu.xu, baoxin.li@asu.edu}

Abstract
Inference in 3D articulated human body tracking is
challenging due to the high dimensionality and
nonlinearity of the parameter-space. We propose a
particle filter with Rao-Blackwellisation which
marginalizes part of the state variables by exploiting the
correlation between the right-side and the left-side joint
Euler angles. The correlation is naturally induced by the
symmetric and repetitive patterns in specific human
activities. A novel algorithm is proposed to learn the
correlation from the training data using Partial Least
Square regression. The learned correlation is then used as
motion prior in designing the Rao-Blackwellised particle
filter, which estimates only one group of state variables
using the Monte Carlo method, leaving the other group
being exactly computed through an analytical filter that
utilizes the learned motion correlation. We evaluate the
effectiveness of the motion correlation for 3D articulated
human body tracking. The accuracy of the proposed 3D
tracker is quantitatively assessed based on the distance
between the true and the estimated marker positions.
Extensive experiments with multi-camera walking
sequences from the HumanEva-I/II data set show that (i)
the proposed tracker achieves significantly lower
estimation error than both the Annealed Particle Filter
and the standard Particle Filter; and (ii) the learned
motion correlation generalizes well to motion performed
by subjects other than the training subject.

1. Introduction
In 3D articulated human body tracking, the highdimensional, multi-modal and highly nonlinear parameterspace often renders inadequate many otherwise effective
approaches including the popular particle filtering (PF).
We observe that strong correlation exists between the
right-side and the left-side body motion due to the
symmetric kinematics and repetitive temporal patterns in
specific human activities like walking. The correlation
should be able to provide motion prior that can be used in
a statistical inference framework to reduce the sampling
burden of the traditional Sequential Monte Carlo method
in tracking, mitigating problems caused by model
ambiguities, self-occlusion and poor image observations.

978-1-4244-1631-8/07/$25.00 ©2007 IEEE

Denote the left-side poses as L (response variables) and
the right-side poses as R (predictor variables). This paper
proposes to learn the correlation between L and R using
Partial Least Square (PLS) regression [1, 2]. The
correlation is naturally induced by the highly symmetric
and repetitive temporal patterns in specific human
activities like walking and jogging. By projecting the
training vectors of R and L into their latent structure, PLS
regression generates deterministic, predictive coefficients
that capture the latent relationship between R and L. These
coefficients also generalize well for future prediction of L
from R. We then utilize the learned correlation model in
designing a new Rao-Blackwellised particle filter (RBPF)
for 3D articulated people tracking, which samples only a
subspace of the state variables with the Monte Carlo
method, with the remaining variables being estimated with
an analytical filter based on the learned correlation. The
proposed RBPF-PLS algorithm combines the benefits of
the representation power of PF that can model non-linear
non-Gaussian multi-modal densities, the efficiency of the
exact filter (Kalman filter), and the effectiveness of
motion correlations in constraining search into the most
likely region of the parameter space. The resultant tracker
is shown to outperform state-of-the-art approaches
including Annealed Particle Filtering [3] for 3D people
tracking.
Unlike previous work [13, 14, 18] that learn a lowdimensional embedding of the original full-state space,
our method tries to build a good predictive model by
capturing the intrinsic correlation among state variables.
This enables us to use only a small amount of training data
to capture the complexities of human motion. In addition,
one major advantage of our method is “smart sampling”:
the correlation model enables us to generate only plausible
samples that conform to the certain prior correlation
among state variables, thus constraining the sampling to
portions of the parameter space that are more likely to
correspond to human motions.

2. Related Work
3D articulated human tracking remains challenging due
to the high-dimensional state-space, noisy/ambiguous
image data and weak dynamics models. One category of
solution uses strong motion prior to constrain the search
into the most likely region of the parameter space. Hybrid

and switching linear dynamics provide rich classes of
temporal behaviors [21]. However, they are
computationally costly in learning, and require large
amounts of training data [14]. One way to cope with the
high-dimensional state-space is to learn low-dimensional
latent variable models. The simplest case involves a linear
subspace projection using PCA [8, 10, 12], and the
tracking problem is formulated as either minimizing
differentiable deterministic objective function [12] or
multi-hypothesis optimization in a Bayesian framework
[8, 10]. Recent work [13, 14] has seen some success in
learning probabilistic models using nonlinear Gaussian
Process Dynamic Models with small amount of training
data. Nonlinear dimensionality reduction has also been
used in providing richer motion model [18, 19]. However
learning a general probabilistic motion prior in the original
full-space is intrinsically difficult given the highdimensionality and the limited availability of huge
amounts of training data to account for motion variations,
correlations among joint angles, and the correlation in
motion over time [8]. Even if this model can be learned,
the search over a high-dimensional space in a probabilistic
framework remains challenging.
RBPF has been studied in 2D tracking [5, 9, 11], fault
diagnosis for robot [25], and integrated aircraft navigation
[20]. For example, in [5], the authors utilize a dependency
relationship between object location and its scale that is
imposed by the camera-scene configuration. However,
that dependency is too restrictive and cannot be easily
extended to other tracking problems including 3D
tracking. Likewise, there is no direct way of extending the
methods in [9, 11, 20, 25] for 3D human tracking. In this
work, we design an RBPF for articulated 3D human
tracking where the dimensionality of the state-space is
much higher than that of 2D tracking, and hence the
incorporation of the learned motion correlation becomes
even more important.

3. Learning Motion Correlation Using PLS
Regression
We model human body as 10 cylinders connected at
revolute joints as in [8]. The state-space is 25dimensional, including 6D for the global position and
orientation, 3D for the head orientation, and 8D for the
left-side and the right-side joint angles respectively.
Motivated by the observation that there exists certain
correlation between the left-side and the right-side body
motion in specific human activities like walking and
jogging, we design a method based on PLS regression to
capture this correlation. Clearly the global position and
orientation and the head orientation should not be included
in this correlation analysis.
Specifically, we arrange n observations of the right-side
and the left-side joint angles into two matrices, R and L,

respectively n×N and n×M in size (N=M=8), where the
columns correspond to the joint angles (variables) and the
rows to the observations at discrete time. For a single
column vector li in L, it is approximated by a linear
combination of the column vectors of R.
N

li = ∑ rk bik + ε = Rbi + ε , i = 1

M

(1)

k =1

where L=(l1 l2 ... lM) and R=(r1 r2 ... rN), and bik is a scalar
denoting the regression coefficient, and ε the residual
error. For multiple-valued left-side pose matrix L, we
gather {bi |i=1...M} into a coefficient matrix B. Then Eqn.
(1) becomes
(2)
L = RB + Ω
Given training data L (responses) and R (predictors), we
will use PLS regression to estimate B, which captures the
motion correlation.
3.1. Learning Motion Correlation: the Algorithm
In 3D articulated human tracking, although the number
of manifest variables is large, there may be only a few
underlying or latent variables that account for most of the
variations in both the predictors and the responses [19].
Our idea is to use PLS regression to extract these latent
variables that account for as much of the manifest
predictor variation as possible while modeling the
responses well. Thus, only strong systematic variation in R
with good correlation to L should be included in the
prediction model. Hence, the key of PLS is to maximize
the covariance (interpretation) and the correlation
(prediction) at the same time. PLS regression achieves this
by maximizing the sample covariance between a
projection of R and a projection of L [1]:

( Rr ) ( Ls ) = arg max
'

{r , s} = arg max

'

'

r ' S RL s
'

(3)
'

rr ss
rr ss
Here SRL is the covariance matrix between R and L. Eqn.
(3) is equivalent to:
{r , s} = arg max[cov( Rr , Ls )]2
|r | =| s| =1
(4)
2
= arg max var( Rr )[corr ( Rr , Ls )] var( Ls )
r,s

|r | =| s| =1

In Eqn. (4), the maximal correlation is balanced with the
requirement to explain as much variance as possible in
both the R-space and the L-space.
We use NIPALS [2] to implement the PLS regression.
It decomposes the mean-centered n×N matrix R and the
mean-centered n×M matrix L into the form:
(5)
R=TPT+E
(6)
L=UQT+F
where the T, U are n×p matrices of the p extracted latent
vectors (LVs), the N×p matrix P and the M×p matrix Q
represent matrices of loadings, and the n×N matrix E and
the n×M matrix F are the matrices of residuals. The

(a)

(b)

(c)

(d)

(e)
(f)
(g)
(h)
Figure 1. The correlation model created by PLS regression. (a) The cumulative variance captured by PLS regression for the S2 R and L
matrix. (b) Project each training point onto the 2D latent space for S2, the first 2 extracted LVs are used for the projection. (c, d) Project
each training point onto the 3D latent space for S2 (c) and S3 (d). The first 3 LVs are used for the projection. (e) Correlation map for S2
grouped by similarity. (f) The predicted left elbow versus the true left elbow for S2 trial 1 validation sequence. (g, h) Motion correlation
is learned from S1 (g) and S2 (h) trial 1 training sequence, and we do prediction for S1/S2/S3/S5 trial 1 validation sequences.

The regression model B is then computed by ([1]):
(7)
B = RT U (T T RRT U ) −1T T L
In our work, for the training right-side poses R, the leftside poses L̂ estimated by PLS regression is L̂ = RB . And
for new testing right-side poses Rt, the predicted left-side
poses is Lˆt = Rt B .
This PLS-regression-based learning method has several
advantages. First, it is superior to Principle Components
Regression (PCR) [15] in constructing a good predictive
model, in that PCR uses PCA [8, 10, 12] to extract latent
vectors, this yields informative directions in the predictor
(R) space, but they may not be associated with the shape
of the response (L) surface. By contrast, PLS regression
finds components from R that are also relevant for L.
Second, our method needs a small amount of training
points to learn a reliable model. Third, the training points
can be in any random order, eliminating the need of
identifying the motion cycles. Lastly, motion variation
among different subjects can be accounted for by learning
from multi-subject motion data simultaneously. For doing
so, we only need to stack the training right-side (left-side)
poses of multiple subjects into a single R (L), and then we
follow the same regression procedures.

3.2. Learning Motion Correlation: Results and
Analysis
We use primarily the HumanEva-I data set [17] in
evaluating our motion correlation learning method, for
human walking activity. Motion capture data from four
subjects, S1, S2, S3 and S5 are used for the training. In
each case, before PLS regression, both R and L are made
mean-centered, and the number of training points is
roughly 450, corresponding to poses obtained from 450
frames. In order to get a reliable predictive model, we

randomly partition the data into 10-fold and the model is
obtained by 10-fold cross validation. Since our objective is
to construct good prediction model instead of
dimensionality reduction, we used all the 8 LVs.
Variance captured and analysis of the latent space: Fig.
1(a) shows the cumulative variance captured by PLS
regression for R and L of S2. We can see that 8 (p=8) LVs
explain 100% of the variance of R, and 81.94% of the
variance of L. It is often true that the higher the variance
captured, the lower the prediction error. However,
increasing the number of training points may not
necessarily increase the percentage of the variance
captured by the model, indicating that even with a small
amount of training data the method should still be
effective. In fig. 1(b), we project each training point of S2
(8D) onto the 2D latent space. And In fig. 1(c) and (d), we
project each training point of S2 and S3 onto the 3D latent
space. PLS produces smooth, clustered latent trajectories,
and hence reliable dynamic predictive model. In (c) and
(d), the 3D latent trajectories for S2 and S3 are very
similar, indicating that people walk in a similar way
intrinsically (from the perspective of the latent space).
Correlation map: Fig. 1(e) illustrates the correlation map
grouped by similarity among each variable of the rightside poses (vertically) and each of the left-side poses
(horizontally). It shows that each variable of the right-side
poses is very similar to the corresponding variable of the
left-side poses. Note also, the motion along right hip z-axis
has strong correlation with the motion of left-side elbow;
this makes sense because due to the motion symmetry,
when our right leg lags behind our torso, our left arm
usually moves in front of our torso.
Prediction error: Fig. 1(f) plots the evolution of the
predicted and the corresponding ground truth of the left

elbow in S2 trial 1 validation sequence. The prediction is
quite accurate. This is also true for other variables.
How reliable the learned motion correlation is? The
reliability of the motion model is evaluated based on the
prediction RMSEs and the tracking errors obtained by
cross validation among different subjects. That is, we
learn the motion correlation from one subject, and we
apply it to predict/track the left-side poses of the other
subjects, and we plot the prediction RMSEs and tracking
errors. For example, in Fig. 1(g), motion prior is learned
from S1 trial 1 training sequence, and we do prediction for
S1/S2/S3/S5 trial 1 validation sequences. It can be seen
from Fig. 1(g, h) that the prediction error is the lowest
when the training and testing subject is the same, this
conforms to our intuition. And except for left knee, the
errors for all the other variables are very small, indicating
that the correlation model is reliable. The relatively large
error in left knee may be due to the large motion variations
of the knee joint among different persons caused by the
more freely motion in the knee. Fig. 1(g, h) also tells us
that the walking style of S5 is quite different from S1, S2
and S3 because of the large error gap among them, and S2
walks in a similar fashion to S3. These have been verified
by our visual inspection of the image data and by the
latent space projection.

4. 3D Human Tracking with the Learned
Motion Correlation
We formulate the tracking problem as one of estimating
the posterior probability distribution over the state-space
given a sequence of image observations Z1:t. One of the
non-parametric approximation methods, Particle Filtering
(PF) [22], represents a distribution by a set of random
particles with associated weights. While having many
good properties, this method becomes impractical when
the dimensionality of the state-space becomes large. In
this work, we marginalize the left-side pose variables by
utilizing the learned motion correlation (through the
matrix B in Eqn.(7)). The marginalization is realized by a
variance-reduction technique, Rao-Blackwellisation [4].
The resultant method is often called Rao-Blackwellised
Particle Filtering (RBPF). The key idea of RBPF is to
partition the original state-space into two parts, Xt (root
variables) and Lt (leaf variables), such that p(Lt|Xt,Z1:t) is a
distribution that can be computed exactly conditional on
Xt, and the distribution p(Xt|Z1:t) is estimated using Monte
Carlo methods like particle filtering [7]. The justification
for this decomposition follows from the factorization of
the posterior probability [7]:
p(Xt,Lt|Z1 :t)=p(Lt|Xt,Z1 :t)p(Xt|Z1 :t )
(8)
In our work, Xt includes the global position and orientation
Gt, head orientation Jt, plus the right-side poses Rt, i.e. Xt
=(Gt, Jt, Rt); Lt consists of the left-side poses. If the same
number of particles is used in a standard PF and a RBPF,
intuitively the latter will provide better estimates for two

reasons: first, the dimensionality of p(Xt|Z1:t) is smaller
than p(Xt,Lt|Z1:t); second, optimal algorithm may be used
to estimate the leaf variables [7].
Given the learned motion correlation B, and the image
measurements Z1:t, we rewrite Eqn. (8) into Eqn. (9):
p( X t , Lt | Z1:t , B) = p( Lt | X t , Z1:t , B) p( X t | Z1:t )
∝ p( Lt | X t , Z1:t , B) p( Z t | X t ) ∫ p ( X t | X t −1 ) p ( X t −1 | Z1:t −1 ) dX t −1
Standard

∝ p(Zt | Xt , Lt ) p(Lt | Xt , Z1:t , B)∫ p( X t | Xt −1) p( Xt −1 | Z1:t −1)dXt −1

step 3
step 2
step 1
(9)
In our RBPF 3D tracking algorithm, at the first step (§
4.1), we propagate the root variables Xt according to a
temporal dynamic model. At the second step (§4.2), we
predict the leaf variables analytically using Kalman
Filtering prediction, assuming a linear-Gaussian substructure between the right-side (Rt in Xt) and the left-side
(Lt) poses conditional on the learned correlation B. At the
third step (§4.3), the predicted root and leaf variables are
weighted by the image likelihood p(Zt|Xt,Lt). At the fourth
step (§4.4), we use resampling with replacement to select
particles with large weights and discard particles with
small weights. And in the last step (§4.4), we update the
leaf variables using auxiliary measurements.
Similar to PF, RBPF represents posterior PDF by a set
of weighted particles. But unlike PF, each particle now
maintains not just a sample from p(Xt|Zt), which we denote
as Xti, but also a parametric representation of the
distribution p(Lt|Xti,Zt), which consists of the mean vector
of the leaf variables, µti =E[Lt], and the estimation error
covariance for the leaves σti =E[(Lti-µti)(Lti-µti)T]. So each
particle is represented by a triplet sti=<Xti, µti, σti >.

4.1. Temporal Dynamics
We use a simple first-order Gauss-Markov dynamics
model:
X ti − ∼ p( X ti | X ti−1 ) = N ( X ti−1 , Σ)

(10)

The noise is drawn from a normal distribution with
diagonal covariance where the standard deviation of each
body angle equal to the maximum absolute inter-frame
angular difference, as in [3]. As in [16], we can enforce a
hard prior that eliminates any particle corresponding to
implausible body poses to reduce the search space. For
example, we can discard particles whose angles exceed
anatomical joint limits and who produces inter-penetrating
limbs.

4.2. Kalman Prediction for the Leaf Variables
Once the root variables are propagated, the leaf
variables can be analytically computed by making use of
the motion correlation B using a Kalman Filter.
Conditional on the root variables and the correlation

model, the leaf variable L is estimated using a linearGaussian sub-structure specified by Eqn. (11)
Lt = ALt −1 + B ( Rt− − Rˆ t −1 ) + τ t −1 , p (τ ) ∼ N (0, Θ) (11a)
(11b)
Ot = HLt + ς t , p(ς ) ∼ N (0, Φ )
Eqn. (11a) relates the leaf state at time t-1 to the leaf state
at time t. Eqn. (11b) relates the leaf state at time t to the
auxiliary measurement Ot, which is simply the leaf state
estimates in the previous time step (see §4.4 for details).
The random variables τt and ςt represent the process and
measurement noise. Matrix A and H are assumed to be
identity matrices.
The leaf variables are predicted by Eqn. (12)

µ = Aµ + B ( R − R )
σ ti − = Aσ ti−1 AT + Θ
i−
t

i
t −1

i−
t

i
t −1

(12a)

a number of sample points inside the projection of the
limbs. Due to the ambiguous and complex background in
the Human-I/II data set, silhouette segmentation is
challenging. The silhouette maps used in tracking S1, S2
S3 and S4 contain missing limbs and are very noisy (Fig.
2, left). This is one of the reasons that the tracking error
for S1, S2 and S3 is relatively higher than that for S5.

Figure 2. Left: Four image silhouettes. The image observations
are incomplete in that some limbs are missing and the data is
noisy. Right: the edge map.

(12b)

Eqn. (12a) predicts the leaf mean one step ahead, where
the matrix B denotes the learned motion correlation. Eqn.
(12a) means that each time as the right-side joint angles
increase ∆= Rti − − Rti−1 from t-1 to t, the left side joint
angles will be increased B( Rti − − Rti−1 ) . Essentially, Eqn.
(12a) implies that the predicted left-side angular velocity
can be approximated by a linear combination of the rightside angular velocity through the correlation model B.
Eqn. (12b) performs leaf covariance prediction. One may
propose to learn the regression model B from the angular
velocity data blocks (instead of the joint angles). However,
we found that regressing on joint angles is more effective
and reliable for tracking than regressing on joint angular
velocity.

4.3. Image Likelihood
We use edges and silhouettes to compute the likelihood,
as in [3, 16]. For edge-based likelihood, we first use
gradients that have been thresholded to obtain binary edge
maps, this map is then convolved with a Gaussian kernel
to yield a edge distance map which indicates the proximity
of a pixel to an edge (Fig. 2, right). The negative loglikelihood is then estimated by projecting onto the image
sparse points along the edges of all cylinders of the model
and computing the mean square error (MSE) of the edge
map responses, as in [3]:
1
2
(13)
− log p( St | Z t ) ∝
(1 − Ψ(ξ ) )
∑
ξ
{ξ }
where St =(Xt, Lt) is the full state vector, {ξ} is the set of
projected points and Ψ is the edge distance map. For
silhouette-based likelihood, silhouette maps are generated
by learning a mixture of Gaussian model for each pixel
over 1000 background images using EM and comparing
the background pixel probability to that of an uniform
foreground model [17]. We then compute the MSE
between predicted and the observed silhouette values over

4.4. Resampling and Kalman Update
After we weight each root and leaf particle by the
likelihood, we conduct resampling with replacement to
select particles with large weights. Following that, Kalman
update Eqn. (14a)~(14c) is performed to update the leaf
variables. In Eqn. (14a), Kti is the Kalman gain which aims
at minimizing the posterior error covariance. Eqn. (14b)
incorporates a measurement Ot-1 into the a priori leaf state
estimate to obtain an improved a posteriori leaf state
estimate. Here, Ot-1 is simply the leaf state estimates in the
previous time step. One may question that the previous
leaf variables Ot-1 cannot serve as observations since they
are not the true image measurements. This is practically
not an issue because from previous steps they have already
incorporated the information from the true measurements
(edges and silhouettes). Essentially the auxiliary
observation can be viewed as being indirectly linked to the
root variables and the true measurements through a nonGaussian nonlinear function Ot = φ(Xt, Zt). Eqn. (14c)
updates the posterior error covariance σ ti
(14a)
K ti = σ ti − H T ( H σ ti − H T + Φ )−1
µti = µti − + K ti (Ot −1 − H µti − )

(14b)

σ = σ − K Hσ

(14c)

i
t

i−
t

i
t

i−
t

5. Tracking Results
We have done extensive experiments to track circular
walking activity performed by various subjects using the
HumanEva-I/II data set [17]. We use the ground truth
motion capture data in HumanEva-I to estimate the motion
correlation. In all the experiments, the first frame is
initialized by the ground truth. The training images and
the testing images are always disjoint (either from
different subjects and/or from different parts of a video).
All the subjects wear natural clothes and the walking is
performed in an indoor environment. For this dataset,

1
15
30
45
60
75
90
105
120
135
150
Error graph
Figure 3. Track subject S5 using correlation model learned from S5 training (top), S1 trial 1 training (middle) and S1S2S3 trial 1 training
data (bottom). The right-most column shows the error graph for each case. Note that, in the bottom row, we learn correlation from 3
subjects (S1, S2 and S3) simultaneously. We use 4 camera views and 1000 particles for all the three sequences, every 15th frame from
camera 2 are shown here.

tracking is challenging especially due to the incomplete
and noisy image observations and severe self-occlusions.
We have quantitatively evaluated the performance of
our method and compared it with Annealed Particle
Filtering (APF) [3] and standard PF [22] using the error
measure proposed in [23, 16]. First, we compute a 3D
error for each particle in each frame based on 15 virtual
markers that corresponds to the locations of joints and
limb endpoints: for each particle sti , the full pose error
δ ( sti , γ t ) is computed as the average distance in
millimeters of all virtual markers m ∈ Γ with respect to the
true pose γ t

δ ( sti , γ t ) =

∑

m∈Γ

m ( sti ) − m (γ t )
Γ

(15)

where m(y) returns the 3D location of marker m for the
body model y. Then we use the same optimistic error
measure as [16] to compute the posterior distribution error
for each frame based on randomly-sampled 100 particles.
For a sequence of T frames, we can compute the average
distance as the mean error over T frames.
In all the comparative experiments, our method
performs slightly faster than APF and much faster than PF.

5.1. Generalization ability of the correlation
Our experiments show that the learned correlation
model is insensitive to the choice of the training subject.
For example, if we learn the correlation model from
subject S1, and apply it to track S5, the tracking

performance does not degrade too much comparing to
learning correlation from S5 and applying it to track S5. In
fact, we can learn a correlation model from multi-subject
motion data, and apply it to track various other subjects
that are not in the training set. In Fig. 3, we track S5 using
the correlation learned from S5 (top row), S1 (middle) and
S1S2S3 (bottom) respectively, 4 camera views and 1000
particles per frame are used for the three sequences. The
mean errors over 150 frames are 48.98, 51.66 and 55.30,
all in millimeters, respectively, as are shown in the rightmost error graphs. The error difference between these
three is marginal, indicating that the correlation model is
insensitive to the choice of training subject.

5.2. Tracking performance on HumanEva-I/II
Fig. 4 shows the results on tracking S1, S3 in
HumanEva-I and S4 in HumanEva-II. The three sequences
use the same correlation model which is learned from
S1S2S3 trial 1 training data (learn correlation from multisubjects simultaneously). Although the training data does
not include any information from S4, the performance of
tracking S4 is still very good. This demonstrates that the
learned motion correlation is able to generalize to different
walking styles and to motions outside of the training data.
Note also, the image data for the S1 and S3 sequences are
very noisy, and in some frames, some limbs are missing in
almost all the camera views, in this case the learned
motion correlation plays a critical role in dealing with the
missing data. Fig. 5 shows the tracking error on
HumanEva-I S1, S2 and S3 sequences. Note that part of

Figure 4. Track S1 (top), S3 (middle) in HumanEva-I and S4 (bottom) in HumanEva-II. The three sequences use the same correlation
model which is learned from S1S2S3 trial 1 training data. The number of cameras used for tracking is 7 (top), 5 (middle) and 4 (bottom)
respectively. 1000 particles are used to track S1 and S4 and 600 particles for S3. We did not apply hard prior for these three sequences.
Due to the limited space, every 15th frame from one view is shown here.

Figure 5. Error on HumanEva-I S1, S2 and S3 trial 1 validation
sequences.

the tracking errors are due to the inaccurate static body
measurement like the limb lengths provided in the training
data.

5.3. Comparison with APF and PF
We compared our method with APF [3] and standard
PF [22] systematically. Fig. 6(a) shows the tracking errors
on S5 obtained by RBPF-PLS (our method), APF and PF,
respectively. In order to fairly compare these three
methods using the S5 sequence, we use the same
configurations for the three methods: 1000 particles per
frame (for APF, it is equal to 5 layers annealing with 200
particles per layer), a likelihood based on silhouettes and
edges, first-order temporal dynamics, applying a hard
prior that discard particles corresponding to infeasible
body poses, and 4 camera views. For RBPF-PLS, the
motion prior is learned from S1 trial 1 training sequence.
It is obvious that the error obtained by our method is lower
than both APF and PF.
We also compare our method with APF and PF using
even challenging HumanEva-I/II data sets. Fig. 6 plots the
tracking error on S1, S2, S3 trial 1 validation sequences.

The mean error over three subjects obtained by RBPFPLS is 148.67mm, which is significantly lower than those
obtained by APF (171.09mm) and PF (165.22mm). We
also notice that our method performs significantly better
than the approach proposed in [26], the error of which is
around 200mm in tracking S1. It is worth noting that, the
error obtained by our method is slightly higher than what
was reported for APF in [24]. We posit that this is due to
the following reasons: (i) the APF error in [24] was
obtained using 1250 particles while RBPF-PLS uses only
1000 particles; (ii) we did not apply hard prior to RBPFPLS in tracking S1,S2,S3 while [24] enforced hard priors
in APF. As mentioned in §4.1, applying hard constraints
may significantly improve tracking performance. One
problem with annealed approach emerges in the
experiments: when silhouette data was ambiguous or
noisy, the APF sometimes got “stuck” in the wrong
interpretation and the annealing forced the method to
represent one of the modes in the data. Similar findings
were reported by Balan et al [16] as well. This may
explain the poorer performance of APF than PF for some
of the HumanEva-I subjects.

(a)

(b)

Figure 6. Tracking errors on S5 (a) and S1, S2, S3 (b) obtained
by RBPF-PLS, APF and PF.

6. Conclusions and discussions
We proposed a tracking approach which marginalizes
part of state variables using the motion correlation learned
by PLS regression through an RBPF. The motion
correlation is justified by the symmetry between the rightside and the left-side poses in human activities like
walking and jogging. We show that the motion correlation
can be learned from modest amount of training data, and it
is effective for tracking a range of human walking styles.
Incorporating the correlation model into the statistical
RBPF tracking framework enhances its capability in
dealing with partially visible silhouettes. Also, the
sequential importance sampling is done in a reduced statespace. As a result, both the accuracy and the efficiency of
tracking are improved.
In this work, tracking was accomplished with rather
simple first-order dynamics and image observations that
are noisy and incomplete. The quality of our results with
such simple dynamics, a lower-dimensional body model,
and appearance models demonstrates the predictive ability
of the correlation model and the inference power of the
RBPF. More sophisticated likelihood and dynamic models
should produce even better results. We also expect that the
tracking performance will be further improved by using
kernel PLS regression to find out the non-linearity in the
motion data.

References
[1] R. Rosipal and N. Kramer. Overview and recent advances
in Partial Least Squares. Subspace, Latent Structure and
Feature Selection, Statistical and Optimization,
Perspectives Workshop, SLSFS 2005.
[2] H. Wold. Path models with latent variables: the NIPALS
approach. in: H. M. Blalock (Ed.), Quantitative Sociology:
International Perspectives on Mathematical and Statistical
Model Building. Academic Press, New York, 1975.
[3] J. Deutscher and I. Reid. Articulated body motion capture
by stochastic search. IJCV, 61(2):185-205, 2004.
[4] G. Casella, C. P. Robert. Rao Blackwellisation of sampling
schemes. Biometrika, 83(1):81–94, 1996.
[5] X. Xu, B. Li. Adaptive Rao-Blackwellised Particle Filter
and its evaluation for tracking in surveillance. IEEE Trans.
Image Processing, 16(3):838-849, 2007.
[6] T. Schon, F. Gustafsson & P.-J. Nordlund. Marginalized
particle filters for mixed linear/nonlinear state-space
models. IEEE Trans. Signal Processing, 53(7):2279–2289,
2005.
[7] K. Murphy and S. Russell. Rao-Blackwellised particle
filtering for dynamic Bayesian networks. in Sequential
Monte Carlo Methods in Practice, A. Doucet, et al Eds.
New York: Springer-Verlag, 2001, ch. 24.
[8] H. Sidenbladh, M. J. Black, L. Sigal. Implicit probabilistic
models of human motion for synthesis and tracking. Proc.
ECCV (1), pp. 784-800, 2002.

[9] Z. Khan, T. Balch, and F. Dellaert. A Rao–Blackwellized
particle filter for Eigen tracking. Proc. CVPR (2), pp. 980986, 2004.
[10] H. Sidenbladh, M. J. Black, D. J. Fleet. Stochastic Tracking
of 3D Human Figures Using 2D Image Motion. Proc.
ECCV (2), pp. 702-718, 2000.
[11] D. Schulz, D. Fox, and J. Hightower. People tracking with
anonymous and id-sensors using Rao–Blackwellised
particle filters. Int. Joint Conf. Artificial Intelligence,
(IJCAI), 2003.
[12] R. Urtasun, D. Fleet and P. Fua. Monocular 3D tracking of
the golf swing. Proc. CVPR (2), pp. 932-938, 2005.
[13] R. Urtasun, D.J. Fleet, A. Hertzmann and P. Fua. Priors for
people tracking from small training sets. Proc. ICCV (1),
pp. 403-410, 2005.
[14] R. Urtasun, D. J. Fleet, P. Fua. 3D people tracking with
Gaussian process dynamical models. Proc. CVPR (1), pp.
238-245, 2006.
[15] W. F. Massy. Principal components regression in
exploratory statistical research. J. of American Statistical
Association, 60:234-256, 1965.
[16] A. Balan, L. Sigal and M. Black. A quantitative evaluation
of video-based 3D person tracking. IEEE Workshop on VSPETS, pp. 349-356, 2005.
[17] L. Sigal and M. J. Black. HumanEva: Synchronized video
and motion capture dataset for evaluation of articulated
human motion. TR CS-06-08, Brown University, 2006.
[18] A. Elgammal, C. S. Lee. Inferring 3D Body Pose from
Silhouettes using Activity Manifold Learning. Proc. CVPR
(2), pp. 681- 688, 2004.
[19] C. Sminchisescu, A. Jepson. Generative modeling for
continuous nonlinearly embedded visual inference. Proc.
ICML, pp. 140-147, 2004.
[20] P.-J. Nordlund and F. Gustafsson. Sequential Monte Carlo
filtering techniques applied to integrated navigation systems.
Proc. Amer. Control Conf., vol. 6, pp. 4375-4380, 2001.
[21] B. North, A. Blake, M. Isard, and J. Rittscher. Learning and
classification of complex dynamics. IEEE Trans. PAMI,
25(9):1016-1034, 2000.
[22] N. J. Gordon, D. J. Salmond, A. F. M. Smith. Novel
approach to nonlinear/non-Gaussian Bayesian state
estimation. IEE Proceedings-F, 140(2):107–113, 1993.
[23] L. Sigal, S. Bhatia, S. Roth, M. J. Black, and M. Isard.
Tracking loose-limbed people. Proc. CVPR (1), pp. 421428, 2004.
[24] L. Sigal. HumanEva-I dataset and evaluation metrics. Talks
in: EHuM workshop, in conjunction with NIPS 2006.
[25] N. de Freitas, et al. Diagnosis by a waiter and a Mars
explorer. Proc. IEEE, special issue on sequential state
estimation, 92(3):455-468, 2004.
[26] R. Li, M-H. Yang, S. Sclaroff, T-P. Tian. Evaluation of 3D
Human Motion Tracking with a Coordinated Mixture of
Factor Analyzers. Proc. EHuM workshop, NIPS 2006.

A Structured Approach to Predicting Image Enhancement Parameters
Parag Shridhar Chandakkar
Baoxin Li
School of Computing, Informatics and Decision Systems Engineering, Arizona State University

arXiv:1704.01249v1 [cs.CV] 5 Apr 2017

{pchandak,baoxin.li}@asu.edu

Abstract
Social networking on mobile devices has become a commonplace of everyday life. In addition, photo capturing process has become trivial due to the advances in mobile imaging. Hence people capture a lot of photos everyday and they
want them to be visually-attractive. This has given rise to
automated, one-touch enhancement tools. However, the inability of those tools to provide personalized and contentadaptive enhancement has paved way for machine-learned
methods to do the same. The existing typical machinelearned methods heuristically (e.g. kNN-search) predict the
enhancement parameters for a new image by relating the
image to a set of similar training images. These heuristic methods need constant interaction with the training images which makes the parameter prediction sub-optimal
and computationally expensive at test time which is undesired. This paper presents a novel approach to predicting
the enhancement parameters given a new image using only
its features, without using any training images. We propose
to model the interaction between the image features and
its corresponding enhancement parameters using the matrix factorization (MF) principles. We also propose a way
to integrate the image features in the MF formulation. We
show that our approach outperforms heuristic approaches
as well as recent approaches in MF and structured prediction on synthetic as well as real-world data of image enhancement.

1. Introduction
The growth of social networking websites such as Facebook, Google+, Instagram etc. along with the ubiquitous
mobile devices has enabled people to generate multimedia content at an exponentially increasing rate. Due to the
easy-to-use photo-capturing process of mobile devices, people are sharing close to two billion photos per day on the
social networking sites 1 . People want their photos to be
visually-attractive which has given rise to automated, one1 http://www.kpcb.com/internet-trends

touch enhancement tools. However, most of these tools
are pre-defined image filters which lack the ability of doing
content-adaptive or personalized enhancement. This has fueled the development of machine-learning based image enhancement algorithms.
Many of the existing machine-learned image enhancement approaches first learn a model to predict a score quantifying the aesthetics of an image. Then given a new lowquality image2 , a widely-followed strategy to generate its
enhanced version is as follows:
• Generate a large number of candidate enhancement parameters3 by densely sampling the entire range of image parameters. Computational complexity may be reduced by applying heuristic criteria such as, densely
sampling only near the parameter space of most similar training images.
• Apply these candidate parameters to the original lowquality image to create a set of candidate images.
• Perform feature extraction on every candidate image
and then compute its aesthetic score by using the
learned model.
• Present the highest-scoring image to the user.
There are two obvious drawbacks for the above strategy. First, generating and applying a large number of candidate parameters to create candidate images may be computationally prohibitive even for low-dimensional parameters. For example, a space of three parameters where each
parameter ∈ {0, ..., 9} produces 103 combinations. Second,
even if creating candidate images is efficient, extracting features from them is always computationally intensive and
is the bottleneck. Also, such heuristic methods need constant interaction with the training database (which might be
2 We call the images before enhancement as low-quality and those after enhancement as high-quality in the rest of this article. The process of
enhancing a new image is called “the testing stage”.
3 The brightness, saturation and contrast are referred to as “parameters”
of an image in this article.

c

2016
IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers
or lists, or reuse of any copyrighted component of this work in other works. DOI: 10.1109/WACV.2016.7477571

stored on a server) that makes the parameter prediction suboptimal. All these factors contribute to making the testing
stage inefficient.
Our approach assumes that a model quantifying image
aesthetics has already been learned and instead focuses on
finding a structured approach to enhancement parameter
prediction. During training, we learn the inter-relationship
between the low-quality images, its features, its parameters
and the high-quality enhancement parameters. During the
testing stage, we only have access to a new low-quality image, its features, parameters and the learned model and we
have to predict the enhancement parameters. Using these
enhancement parameters, we can generate the candidate images and select the best one using the learned model. The
stringent requirement of not accessing the training images
arises from real-world requirements. For example, to enhance a single image, it would be inefficient to establish a
connection with the training database, generate hundreds of
candidate images, perform feature extraction on them and
then find the best image.
The search space spanned by the parameters is huge.
However, the enhancement parameters are not randomly
scattered. Instead they depend on the parameters and features of the original low-quality image. Thus we hypothesize that the enhancement parameters should have a lowdimensional structure in another latent space. We employ an MF-based approach because it allows us express
the enhancement parameters in terms of three latent variables, which model the interaction across: 1. the lowquality images 2. their corresponding enhancement parameters 3. the low-quality parameters. The latent factors are
learned during inference by Gibbs sampling. Additionally,
we need to incorporate the low-quality image features since
the enhancement parameters also depend on the color composition of the image, which can be characterized by the
features. The feature incorporation in this framework is
achieved by representing the latent variable which models
the interaction across these images as a linear combination
of their features, by solving a convex `2,1 -norm problem.
We review the related work on MF as well as image enhancement in the following section.

2. Related Work
Development of machine-learned image enhancement
systems has recently been an active research area of immense practical significance. Various approaches have been
put forward for this task. We review those works which
improve the visual appearance of an image using automated techniques. To encourage research in this field, a
database named MIT-Adobe FiveK containing corresponding low and high-quality images was proposed in [4]. The
authors also proposed an algorithm to solve the problem of
global tonal adjustment. The tone adjustment problem only

manipulates the luminance channel, where we manipulate
saturation, brightness and contrast of an image.
Content-based enhancement approaches have been developed in the past which try to improve a particular image region [2, 9]. These approaches require segmented regions which are to be enhanced. This itself may prove to
be difficult. Approaches which work on pixels have also
been developed using local scene descriptors. Firstly, similar images from the training set are retrieved. Then for
each pixel in the input, similar pixels were retrieved from
the training set, which were then used to improve the input
pixel. Finally, Gaussian random fields maintain the spatial smoothness in the enhanced image. This approach does
not consider the global information provided by the image
and hence the enhancements may not be visually-appealing
when viewed globally. In [8], a small number of image enhancements were collected from the users which were then
used along with the additional training data.
Two recent works involving training a ranking model
from low and high-quality images are presented in [5, 25].
The authors of [25] create a data-set of 1300 corresponding
low and high-quality image pairs along with a record of the
intermediate enhancement steps. A ranking model trained
on this type of data can quantify the aesthetics of an image.
In [5], non-corresponding low and high-quality image pairs
extracted from the Web are used to train a ranking model.
Both of these approaches use kNN-search during the testing
stage to create candidate images. After extracting features
and ranking them, the best image is presented to the user.
The task of enhancement parameter prediction could be
related to the attribute prediction [17, 18, 11, 7]. However,
the goal of the work on attribute prediction has been to
predict relative strength of an attribute in the data sample
(or image). We are not aware of any work which predicts
parameters of an enhanced version of a low-quality image
given only the parameters and features of that image. Since
our approach is based on MF principles, we review the related recent work on MF.
MF [19, 15, 20, 10, 24] is extensively used in recommender systems [12, 1, 13, 23, 14, 22, 21]. These systems
predict the rating of an item for a user given his/her existing
ratings for other items. For example, in Netflix problem,
the task is to predict favorite movies based on user’s existing ratings. MF-based solutions exploit following two key
properties of such user-item rating matrix data. First, the
preferred items by a user have some similarity to the other
items preferred by that user (or by other similar users, if we
have sufficient knowledge to build a similarity list of users).
Second, though this matrix is very high-dimensional, the
patterns in that that matrix are structured and hence they
must lie on a low-dimensional manifold. For example, there
are 17, 770 movies in Netflix data and ratings range from
1 − 5. Thus, there are 517770 rating combinations possible

per user and there are 480, 189 users. Therefore, the number of actual variations in the rating matrix should be a lot
smaller than the number of all possible rating combinations.
These variations could be modeled by latent variables lying
near a low-dimensional manifold. This principle is formalized in [15] with probabilistic matrix factorization (PMF).
It hypothesizes that the rating matrix can be decomposed
into two latent matrices corresponding to user and movies.
Their dot product should give the user-ratings. This works
fairly well on a large-scale data-set such as Netflix. However, a lot of parameters have to be tuned. This requirement
is alleviated in [20] by developing a Bayesian approach to
MF (BPMF). BPMF has been extended for temporal data
(BPTF) in [24]. MF is used in other domains such as computer vision to predict feature vectors of another viewpoint
of a person given a feature for one viewpoint [6]. We adopt
and modify BPTF since it allows us to model joint interaction across low-quality images, corresponding enhancement
parameters and the low-quality parameters. In the next section, we detail our problem formulation and proposed approach.

3. Problem Formulation and Proposed Approach
We have a training set consisting of N images
{S1 , . . . , SN }4 . Parameters of all images are represented as
A = {A1 , . . . , AN } where Ai ∈ RK×1 ∀ i ∈ {1, . . . , N }.
Each image has M enhanced versions and each version has
the same size as that of its corresponding low-quality image. All versions corresponding to the ith image are represented as {W1i , . . . , WM
i }. All versions are of higher quality as compared to its corresponding image. Parameters of
all M versions of the ith image (also called as candidate pa1
M
rameters) are represented as A0 = {A0i , . . . , A0i }, where
0j
K×1
Ai ∈R
∀ i, j. Features of all low-quality images are
represented as F = {F1 , . . . , FN } where Fi ∈ RL×1 ∀ i.
In practice, we observe that M  N, K < M . Our goal
is to be able to predict the candidate parameters for all the
versions of the ith image by only using the information provided by Ai and Fi . To the best of our knowledge, this is
a novel problem of real significance that has not been addressed in the literature. We now explain our proposed approach.
As mentioned before, our task is to predict the candidate
parameters for all the enhanced versions of a low-quality
image with the help of its parameters and features. The values for all the K parameters corresponding to N images
and their N · M versions (total N + N · M ) can be stored
4 We use bold letters to denote matrices. Non-bold letters denote
scalars/vectors which will either be clear from the context or will be mentioned. X i , Xi , XT , Xij and ||X||p denote row, column, transpose, entry
at row i and column j of a matrix X and pth norm of matrix X respectively.

in three-dimensional matrix R ∈ RN ×(M +1)×K . We need
k
k
k
to predict R̂ij
= Rik + ∆Rij
or in turn just ∆Rij
. Rik deth
notes the k parameter value (k ∈ {1, . . . , K}) of the ith
k
low-quality image and R̂ij
is the k th parameter value of j th
version of the ith image. Given a new nth low-quality imk
age, we only need to predict ∆Rnj
∀ j = {1, . . . , M }, ∀ k.
k
During training, we can compute ∆Rij
from available
k
k
Rij and R̂ij . Following MF principles, we express ∆R as
an inner product of three latent factors, U ∈ RD×N , V ∈
RD×M and T ∈ RD×K [20, 24]. D is the latent factor dimension. These latent factors should presumably model the
underlying low-dimensional subspace corresponding to the
low-quality images, its enhanced versions and its parameters. This can be formulated as:
k
∆Rij
=< Ui , Vj , Tk >≡

D
X

Udi Vdj Tdk ,

(1)

d=1

where Udi denotes the dth feature of the ith column of
U. Presumably, as we increase D, the approximation erk
ror ∆Rij
− < Ui , Vj , Tk > should decrease (or stay constant) if the prior parameters for latent factors U, V and
T are chosen correctly. Following [20], we choose normal distribution (with precision α) for: 1. the conditional distribution ∆R|(U, V, T) and 2. for prior distributions - p(U|ΘU ), p(V|ΘV ) and p(T|ΘT ), where ΘU =
−1
−1
(µU , Λ−1
U ), ΘV = (µV , ΛV ), ΘT = (µT , ΛT ). ΘU , ΘV
and ΘT are hyper-parameters, and µ and Λ are the multivariate precision matrix and the mean respectively. Since
the Wishart distribution is a conjugate prior for multivariate normal distribution (with precision matrix), we put
Gaussian-Wishart priors on all hyper-parameters5 . We
could find the latent factors U, V and T by doing inference
through Gibbs sampling. It will sample each latent variable
from its distribution, conditional on the values of other varik
ables. The predictive distribution for ∆Rij
can be found by
using Monte-Carlo approximation (explained later).
However, it is important to note the following major differences in our problem when compared with the previous
work on MF [20, 24]. In product or movie rating prediction problems, an average (non-personalized) recommendation may be provided to a user who has not provided any
preferences (not necessarily constant for all users). In our
case, each image may require a different kind of parameter
adjustment to create its enhanced version and thus no “average” adjustment exists. As explained before, the adjustment
should depend on the image’s features, which characterize
that image (e.g. bright vs. dull, muted vs. vibrant). In our
problem, it is particularly difficult to get a good generalizing
performance on the testing set as we shall see later. The loss
in performance of existing approaches on the testing set can
5 For

details, see supplementary material on author’s website.

be attributed to the different requirements for parameter adjustments for each image. Thus it becomes necessary to include the information obtained from image features into the
formulation. We show that simply concatenating the parameters and features and applying MF techniques presented in
[20, 24] does not provide good performance, possibly because they lie in different regions of the feature space.
To overcome this problem, we observe that the conditional distribution of each Ui factorizes with respect to the
individual samples. We propose to express U as a linear
function of F by using a convex optimization scheme. We
then integrate it into the inference algorithm to find out the
latent factors. The linear transformation can be expressed
as,
Ui = FiT P + Q, ∀ i ∈ {1, . . . , N },
L×1

D×1

(2)

D×D

where Fi ∈ R
, Ui ∈ R
,P ∈ R
and Q ∈
R1×D . Note that to carry out this decomposition, we have to
set D = L. This is not a severe limitation since L is usually
large (∼ 1000) and as we have mentioned before, increasing D should decrease the approximation error at the cost
of increased computation. Henceforth we assume that our
feature extraction process generates Fi ∈ RD×1 . Also, note
that large L does not mean that the latent space is no longer
low-dimensional, because L is still smaller as compared to
all the possible combinations of parameters (e.g. 517770 ).
We propose an iterative convex optimization process to
determine coefficients P and Q of Equation 2. We propose
the following objective function to determine them: q
min
P,Q

N
X

and Q could be (trivially) obtained by just setting each entry
of P to a very small value and letting a column of Q ≈ Ui
(which makes Fi redundant). Secondly, while testing for a
new image, we would have to devise a strategy to determine
the suitable value for Q. For example, we could take the
column of Q that corresponds to the nearest training image.
This adds unnecessary complexity and reduces generalization. By making Q a row vector, we consider that it may
be possible to arrive to the space of enhancement parameters by linearly transforming the low-quality image features
with a constant offset. In other words, we want P to transform the features into a region in the latent space where all
the other high-quality images lie and Q provides an offset to
avoid over-fitting. This is a joint `2,1 -norm problem which
can be solved efficiently by reformulating it as convex. We
thus reformulate Equation 3 as follows, inspired by [16]:

min
P,Q

The `2,1 -Norm of a matrix X ∈ RM ×N is defined as,
M
P
`2,1 (X) =
||Xi ||2 . Also, for a row vector Q, we have
i=1

||Q||2 = ||Q||2,1 . Thus Equation 4 can be further written
as:

min
P,Q

||FiT P + Q − UiT ||2 + β||P||2,1 + γ||Q||2

N
γ
1X
||F T P + Q − UiT ||2 + ||P||2,1 + ||Q||2 (4)
β i=1 i
β

1 T
||F P+1N Q−UT ||2,1 +||P||2,1 +δ||Q||2,1 , (5)
β

(3)

i=1

The objective function tries to reconstruct U using P, Q
and F while controlling the complexity of coefficients.
Let’s concentrate on the structure of P (by neglecting the
effect of Q momentarily). The columns of P act as coefficients for Fi . Ideally, we would want the elements of Ui
to be determined by a sparse set of features, which implies
sparsity in the columns of P. To this end, we impose `2,1 norm on P, which gives us a block-row structure for P.
Let us consider the structure of Q along with P. Equation 2 shows that different columns of Ui depend on different image features Fi . Also, we expect that a different set
of columns of P will get activated (take on large values) for
different Fi . We add an offset Q ∈ R1×D for regularization.
Thus the offset introduced by Q remains constant across all
the images but changes for each Fi,j . Making Q to be a
row vector also forces P to play a major role in Equation
3. This in turn increases the dependence of Ui on Fi . If we
were to define Q as the same size of U (which would mean
different offsets for each image as well as its features), it
would pose two potential disadvantages. Firstly, optimal P

where δ = βγ and 1N is a column vector of ones ∈ RN .
Now, put FT P + 1N Q − βE = UT . Thus Equation 5
becomes:

min ||E||2,1 + ||P||2,1 + δ||Q||2,1 ,

E,P,Q

s.t. FT P + 1N Q − βE = UT ,
 
 
(6)
 E 
E




T
−1
N
min  P  s.t. −βIN F δ 1  P  = UT
E,P,Q 
δQ 
δQ
2,1

Equation 6 is now in the form of: min ||X||2,1 s.t. ZX =
X

B and thus convex. It can be iteratively solved by an efficient algorithm mentioned in [16]. We set β = 0.1 and
δ = 3. Once we have expressed U as a function of F, we
use Gibbs Sampling to determine the latent factors P, Q, V
and T [20]. As mentioned before, the predictive distribution
k
for a new parameter value ∆R̂ij
is given by a multidimensional integral as:

Algorithm 1 Gibbs Sampling for Latent Factor Estimation
Initialize model parameters {P(1) , Q(1) , V(1) , T(1) }.
T
Obtain U(1) = FT P(1) + Q(1)
For y = 1, 2, . . . , Y
• Sample the hyper-parameters according to the
derivations 6 :
α(y) ∼ p(α(y) |U(y) , V(y) , T(y) , ∆R),
(y)
(y)
(y)
(y)
ΘU ∼ p(ΘU |U(y) ), ΘV ∼ p(ΘV |V(y) ),
(y)
(y)
(y)
ΘT ∼ p(ΘT |T )
• For i = 1, ..., N , sample the latent features of an
image (in parallel):
(y+1)
(y)
Ui
∼ p(Ui |V(y) , T(y) , ΘU , α(y) , ∆R)
Determine P(y+1) and Q(y+1) using the iterative
T
optimization by substituting B = U(y+1) .

T
Reconstruct U(y+1) : Û(y+1)
= FT P(y+1) +Q(y+1)
• For j = 1, ..., M , sample the latent features of the
enhanced versions (in parallel):
(y+1)

Vj

(y)

∼ p(Vj |Û(y+1) , T(y) , ΘV , α(y) , ∆R)

• For k = 1, ..., K, sample the latent features of parameter (in parallel):
(y+1)

Tk

(y)

∼ p(Tk |Û(y+1) , V(y+1) , ΘT , α(y) , ∆R)

k
p(∆R̂ij
|∆R) =

Z

k
p(∆R̂ij
|Ui , Vj , Tk , α)·

p(U, V, T, α, ΘU , ΘV , ΘT |∆R)·

(7)

d(U, V, T, α, ΘU , ΘV , ΘT ).
We resort to numerical approximation techniques to
solve the above integral. To sample from the posterior, we
use Markov Chain Monte Carlo (MCMC) sampling. We
use the Gibbs sampling as our MCMC algorithm. We can
approximate the integral by,

k
p(∆R̂ij
|∆R) ≈

Y


X
(y)
(y)
(y)
k
p ∆R̂ij
|Ui , Vj , Tk , α(y)
y=1

(8)
Here we draw Y samples and the value of Y is set by observing the validation error. The sampling from U, V and
T is simple since we use conjugate priors for the hyperparameters. Also, a random variable can be sampled in
parallel while fixing others which reduces the computational complexity. Algorithm 1 shows how to iteratively
6 See supplementary material on author’s website for detailed derivations.

sample U, V, T and obtain P and Q. Note that it is required in the algorithm to reconstruct U(y+1) at every iteration since there will always be a small reconstruction
error ||Û(y+1) − U(y+1) ||. The error occurs because we
force Q to be a row vector, which makes the exact recovery of U(y+1) difficult. The reconstructed error causes
adjustment of V and T. Once we obtain the four latent
factors, our task is to predict the parameter values for M
enhanced versions having K parameters each. Suppose
Ft is the feature vector of a new image, then the paramk
can be simply obtained by computing,
eter values ∆R̂tj
k
∆R̂tj
=< FtT P + Q, Vj , Tk > ∀ j ∈ {1, . . . , M } and
k ∈ {1, . . . , K}. If the parameter value predictions lie beyond a certain range then a thresholding scheme can be used
based on the prior knowledge. For example, to constrain the
predictions between [0, 1], a logistic function may be used.

4. Experiments
We conduct two experiments to show the effectiveness
of our approach. We did the first one on a synthetic data
and compared it with: 1. BPMF 2. our own discrete version of BPTF, called D-BPTF. 3. multivariate linear regression (MLR) 4. twin Gaussian processes (TGP) [3] 5.
Weighted kNN regression (WKNN). For D-BPTF, we make
minor modifications in the original BPTF approach [24] by
removing the temporal constraints on their temporal variable, since there are no temporal constraints in our case.
The inference for their temporal variable is then done in
the exactly same manner as the other non-temporal variables. This gave us a marginal boost in the performance.
For MLR, We use a standard multivariate regression by
maximum likelihood estimation method. Specifically, we
use MATLAB’s mvregress command. TGP is a generic
structured prediction method. It accounts correlation between both input and output resulting in improved performance as compared to MLR or WKNN. The WKNN approach predicts the test sample as a weighted combination
of the k-nearest inputs. The first two algorithms do not allow features inclusion. For MLR, TGP and WKNN, we
j
concatenate Ai and Fi , and use it to predict A0i . Even
for our approach, we concatenate Ai and sample feature to
form Fi . The intuition behind this concatenation is that the
enhancement parameters should be a function of input parameters as well along with the features. We did observe
performance boost after concatenating the features and parameters.
The second experiment demonstrates the usefulness of
this approach in a real-world setting where we have to predict paramters of the enhanced versions of an image (then
generate those versions by applying predicted parameters
to the input low-quality image) without using any information about the versions. We compare our approach with the

Training RMSE for simulation

Testing RMSE for simulation

0.6

0.6

0.5

0.55
0.5

0.4

0.45

0.3

0.4

0.2

0.35
1

2

3

4

5

PMF_Simulation

6

7

8

9

10 11 12 13 14 15 16

D-BPTF_Simulation

1

Proposed_Simulation

2

3

4

Training RMSE for image enhancement

0.18

0.15

0.16

0.12

0.14

0.09

0.12

0.06

0.1

0.03

0.08
1

2

3

4

5

6

7

8

9

10 11 12 13 14 15 16

D-BPTF_Enhancement

6

7

8

9

10 11 12 13 14 15 16

D-BPTF_Simulation

Proposed_Simulation

Testing RMSE for image enhancement

0.18

PMF_Enhancement

5

PMF_Simulation

Proposed_Enhancement

1

2

3

4

5

PMF_Enhancement

6

7

8

9

10 11 12 13 14 15 16

D-BPTF_Enhancement

Proposed_Enhancement

260
195
130
65
0
Image 1 Image 2 Image 3 Image 4 Image 5

KNN

Best
image

Proposed Approach

Figure 1. Top plots: train and test RMSEs for both the experiments. Bottom plot: First 5 sets of bars show votes for version 1 to 5 of
kNN vs. the best image of our approach. The last set of bars shows votes for the best image of both approaches. Please zoom in for better
viewing. See in color.

competing 5 algorithms in addition to kNN-search as it is
also used in [26, 5]. We also analyzed the effect of Q in our
solution by: removing Q i.e. U = FT P.

4.1. Data set description and experiment protocol
The synthetic data is carefully constructed by keeping
the following task in mind. We are given a training set
consisting of: 1. F ∈ RD×N ; 2. A ∈ RK×N ; and
3. only parameters of M versions for each input sample A0 ∈ RK×N ×M . Our aim is to predict parameters for a set
of M versions given a new Fi and Ai . In real-world problems, A and F are interdependent. The parameters of M
versions are dependent on both A, F. Hence we construct
the synthetic data as follows.
Firstly, we generate a set of 3-D input parameters - A drawn from a uniform distribution [0, 1]. Then we generate
a 50-D feature set F, where each element of Fi is related
to all Ak,i ∀ i = {1, . . . , 103 }, k = {1, 2, 3} by a nonlinA
ear function. For example, Fj,i = r1 1,i + 1+e−r12 A2,i +
3
Ar3,i
, ∀ j ∈ {1, . . . , 50} and r1 , r2 , r3 are random numbers.
The parameters of enhanced versions, A0k,i,m , are also non-

linearly related to Ak,i ∀ k, ∀ m ∈ {1, . . . , 4} and Fi . For
A

example, A0k,i,m = η r1 1,i +

1
1+e−r2 A2,i

3
+ Ar3,i
+ (1 −

η) · ||Fi ||2 . The contribution of Fi is decided by η. We perform 3-fold cross-validation. We predict the values of A0
in the test set (disjoint from training) using corresponding
A and F. RMSE is computed between the predicted and
actual A0 .
The MIT-Adobe FiveK data-set contains 5000 highquality photographs taken with SLR cameras. Each photo
is then enhanced by five experts to produce 5 enhanced versions. We extract average saturation, brightness and contrast for every image, which are parameters ∈ A. We also
extract 1274-D color histogram with 26 bins for hue, 7 bins
each for saturation and value. We also calculate localized
features of 144-D each for contrast, brightness and saturation. Finally, we append average saturation, brightness and
contrast of the input low-quality image, which are its parameters. Thus we get a 1709-D (= 1274 + 3 × 144 + 3)
representation for every image ∈ F. We train using 4000
images and use 500 images each for validation and testing.
We predict parameters for 5 versions in a 3 × 5 matrix for

Figure 2. Left: Original image, Middle: enhanced image by kNN and Right: proposed approach 7 . See in color.

each image in the testing set. An entry A0i,j denotes the
value for ith parameter of j th enhanced version. To enable comparison with the expert-enhanced images of the
data-set, we also compute parameters for 5 enhanced versions for each image, which we treat as ground-truth. We
evaluate this experiment in two ways. Firstly, we calculate
RMSE between the parameters of 5 expert-enhanced photos and the parameters of the predicted versions using five
aforementioned algorithms. Secondly, we conduct a subjective test under standard test settings (constant lighting, position, distance from the screen). In this case, we compare
our approach with the popular kNN-search-based approach.
It first finds the nearest original image in the training set to
the testing image - im - and then applies the same parameter
transformation to im to generate 5 version. In our approach,
we predict the parameters for enhanced versions using the
proposed formulation. We threshold the parameter values
as:
A0k,i,m = min(A0k,i,m , Ak,i + λk Ak,i ),
A0k,i,m = max(A0k,i,m , Ak,i − ζk Ak,i ),

(9)

where λ and ζ are multipliers for the k th parameter. In
our case, the multipliers for saturation, brightness and con-

trast are: λ = {0.4, 0.4, 0.05}, ζ = {0.3, 0.3, 0.01}. As
mentioned before, the clipping scheme in our formulation
should be set using prior knowledge. Here, we know that
the enhanced images usually have a larger increase (as compared to decrease) associated with their parameters. Also,
changing contrast by a very small amount affects the image
greatly.
The predicted parameters are applied to the input image
to obtain its enhanced versions. The procedure is the same
for both the approaches and is as follows. First we change
contrast till the difference between the updated and the predicted contrast is marginal. We update contrast first since
changing it updates both brightness and saturation. We then
update brightness and saturation till they come significantly
closer to their corresponding predicted values. This gives
us 5 versions for both approaches. To allow comparisons
within a reasonable amount of time, we use a pre-trained
ranking weight vector w (from [5]) to select the best image of our approach (im-proposed) and kNN-approach (imkNN). For the subjective test, people are told to compare improposed with the 5 enhanced versions of kNN-approach as
well as with im-kNN. Thus for every input image, people
7 See supplementary on author’s website for additional full-resolution
results.

perform 6 comparisons. The image order was randomized.
We conducted the test with 11 people and 35 input images.
Thus every person compared 210 pairs of images. They
were told to choose a visually-appealing image. The third
option of simultaneously preferring both images was also
provided. This option has no effect on cumulative votes.

4.2. Results
The parameters for the synthetic data were more accurately predicted by our approach than BPMF, D-BPTF,
MLR, TGP and WKNN. It is worth noting that though
the training error continues to decrease for our approach,
BPMF and D-BPTF, the testing error starts increasing after
only 5 and 8 iterations for BPMF and D-BPTF, respectively.
However, testing error in our approach decreases rapidly for
4 iterations and then it decreases very slowly for the next
12, as shown in Fig. 1. The RMSE on test set for BPMF,
D-BPTF, MLR, TGP, WKNN and the proposed approach
is 0.4933, 0.4865, 0.6293, 0.4947, 0.8014 and 0.3644. The
numbers show that our approach is able to effectively use
the additional information provided by features and the interaction between A, F and all versions to provide better
prediction. On the other hand, BPMF and D-BPTF start
over-fitting quickly due to lack of sample feature information while MLR and WKNN fail to model the complex interaction between variables. TGP performs better because
of its ability to capture correlations between input and output. However, TGP still treats each version independently
and thus its performance still falls short of our approach.
In the second experiment, the RMSE for BPMF,
D-BPTF, MLR, TGP, WKNN and our approach is
0.1251, 0.1328, 1.2420, 0.1268, 0.1518 and 0.0820 respectively. The testing error starts increasing after only 3 and
5 iterations for BPMF and D-BPTF, respectively. It is important to note that we do not use the clipping scheme mentioned in Equation 9 in order to do a fair comparison of
RMSEs between all the five approaches and the proposed
appraoch. For the subjective evaluation, Fig. 1 shows cumulative votes obtained for ours and the kNN-based approach for comparison between 5 images chosen by kNN
and the best image chosen by our approach. Fig. 1 also
shows votes obtained for the best images chosen by both
approaches. Fig. 2 shows two input images enhanced by
both the approaches. The top row of Fig. 2 shows that kNN
reduces the saturation while increasing the brightness. Our
approach balances both of them to obtain a more appealing
image. In the bottom row, however, both approaches fail to
produce aesthetic images as images become too bright. It
is probably due to the portion of the sky in the input image.
For both the images, most people prefer images enhanced
by our approach. Computationally, our approach is superior than kNN. Complexity of our approach is independent
of data-set size at testing time whereas kNN searches the

Table 1. Effect of varying β and δ

Parameter setting
β = 0.001, γ = 6
β = 0.01, γ = 6
β = 0.02, γ = 0.1
β = 0.2, γ = 0.05
β = 0.8, γ = 0.05
β = 0.1, γ = 0.3
β = 0.1, γ = 0.8
β = 0.1, γ = 2

RMSE (lower the better)
0.3162
0.0962
0.0907
0.0930
0.0872
0.0820
0.0821
0.0820

entire data-set for the closet image and then applies its parameters.
We reconstructed U = FT P and observed performance
drop as it overfits. We get RMSE of 0.9305 and 0.3762
on enhancement and simulation data, respectively. We believe the real-world enhancement data has correlations naturally embedded in it unlike in synthetic data. Thus the performance drop is drastic in case of enhancement since the
problem of recovering P only from U and F is ill-posed.
We also analyzed the effect of varying β and δ. Since our
approach uses Bayesian probabilistic inference, small variations in β and δ do not significantly affect the performance.
Table 1 lists the various parameter settings and its effect on
the performance of the second experiment (i.e. image enhancement):

5. Conclusion
In this paper, we introduced a novel problem of predicting parameters of enhanced versions for a low-quality image by using its parameters and features. We developed an
MF-inspired approach to solve this problem. We showed
that by modeling the interactions across low-quality images, its parameters and its versions, we can outperform
five state-of-art models in structured prediction and MF. We
proposed inclusion of feature information into our formulation through a convex `2,1 -norm minimization, which works
in an iterative fashion and is efficient. Thus our approach
utilizes information which helps characterize input image.
This leads to better generalization and prediction performance. Since other approaches do not model interdependence between image features and parameters of their corresponding enhanced versions, they start over-fitting quickly
and produce an inferior prediction performance on the test
set. Experiments on synthetic and real data demonstrated
superiority of our approach over other state-of-art methods.
Acknowledgement: The work was supported in part
by an ARO grant (#W911NF1410371) and an ONR grant
(#N00014-15-1-2344). Any opinions expressed in this material are those of the authors and do not necessarily reflect
the views of ARO or ONR.

References
[1] L. Baltrunas, B. Ludwig, and F. Ricci. Matrix factorization
techniques for context aware recommendation. In Proceedings of the fifth ACM conference on Recommender systems,
pages 301–304. ACM, 2011.
[2] F. Berthouzoz, W. Li, M. Dontcheva, and M. Agrawala. A
framework for content-adaptive photo manipulation macros:
Application to face, landscape, and global manipulations.
ACM Trans. Graph., 30(5):120, 2011.
[3] L. Bo and C. Sminchisescu. Twin gaussian processes for
structured prediction. International Journal of Computer Vision, 87(1-2):28–52, 2010.
[4] V. Bychkovsky, S. Paris, E. Chan, and F. Durand. Learning photographic global tonal adjustment with a database of
input/output image pairs. In Computer Vision and Pattern
Recognition (CVPR), 2011 IEEE Conference on, pages 97–
104. IEEE, 2011.
[5] P. S. Chandakkar, Q. Tian, and B. Li. Relative learning from
web images for content-adaptive enhancement. In Multimedia and Expo (ICME), 2015 IEEE International Conference
on, pages 1–6. IEEE, 2015.
[6] C.-Y. Chen and K. Grauman. Inferring unseen views of people. In Computer Vision and Pattern Recognition (CVPR),
2014 IEEE Conference on, pages 2011–2018. IEEE, 2014.
[7] L. Chen, Q. Zhang, and B. Li. Predicting multiple attributes
via relative multi-task learning. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages
1027–1034. IEEE, 2014.
[8] S. B. Kang, A. Kapoor, and D. Lischinski. Personalization of
image enhancement. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 1799–1806.
IEEE, 2010.
[9] L. Kaufman, D. Lischinski, and M. Werman. Content-aware
automatic photo enhancement. In Computer Graphics Forum, volume 31, pages 2528–2540. Wiley Online Library,
2012.
[10] N. D. Lawrence and R. Urtasun. Non-linear matrix factorization with gaussian processes. In Proceedings of the 26th Annual International Conference on Machine Learning, pages
601–608. ACM, 2009.
[11] S. Li, S. Shan, and X. Chen. Relative forest for attribute
prediction. In Computer Vision–ACCV 2012, pages 316–327.
Springer, 2013.
[12] H. Ma, H. Yang, M. R. Lyu, and I. King. Sorec: social
recommendation using probabilistic matrix factorization. In
Proceedings of the 17th ACM conference on Information and
knowledge management, pages 931–940. ACM, 2008.
[13] H. Ma, D. Zhou, C. Liu, M. R. Lyu, and I. King. Recommender systems with social regularization. In Proceedings
of the fourth ACM international conference on Web search
and data mining, pages 287–296. ACM, 2011.
[14] B. Marlin, R. S. Zemel, S. Roweis, and M. Slaney. Collaborative filtering and the missing at random assumption. arXiv
preprint arXiv:1206.5267, 2012.
[15] A. Mnih and R. Salakhutdinov. Probabilistic matrix factorization. In Advances in neural information processing systems, pages 1257–1264, 2007.

[16] F. Nie, H. Huang, X. Cai, and C. H. Ding. Efficient and
robust feature selection via joint `2,1 -norms minimization.
In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and
A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 1813–1821. Curran Associates, Inc.,
2010.
[17] D. Parikh and K. Grauman. Relative attributes. In Computer Vision (ICCV), 2011 IEEE International Conference
on, pages 503–510. IEEE, 2011.
[18] D. Parikh, A. Kovashka, A. Parkash, and K. Grauman. Relative attributes for enhanced human-machine communication.
In AAAI, 2012.
[19] J. D. Rennie and N. Srebro. Fast maximum margin matrix
factorization for collaborative prediction. In Proceedings
of the 22nd international conference on Machine learning,
pages 713–719. ACM, 2005.
[20] R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using markov chain monte carlo. In Proceedings of the 25th international conference on Machine
learning, pages 880–887. ACM, 2008.
[21] Y. Shi, M. Larson, and A. Hanjalic. Collaborative filtering
beyond the user-item matrix: A survey of the state of the
art and future challenges. ACM Computing Surveys (CSUR),
47(1):3, 2014.
[22] Q. Song, J. Cheng, and H. Lu. Incremental matrix factorization via feature space re-learning for recommender system.
In Proceedings of the 9th ACM Conference on Recommender
Systems, pages 277–280. ACM, 2015.
[23] S. Wang, J. Tang, Y. Wang, and H. Liu. Exploring implicit
hierarchical structures for recommender systems. In International Joint Conference on Artificial Intelligence (IJCAI).
IJCAI, 2015.
[24] L. Xiong, X. Chen, T.-K. Huang, J. G. Schneider, and J. G.
Carbonell. Temporal collaborative filtering with bayesian
probabilistic tensor factorization. In SDM, volume 10, pages
211–222. SIAM, 2010.
[25] J. Yan, S. Lin, S. B. Kang, and X. Tang. A learning-to-rank
approach for image color enhancement. In Computer Vision
and Pattern Recognition (CVPR), 2014 IEEE Conference on,
pages 2987–2994. IEEE, 2014.
[26] J. Yan, S. Lin, S. B. Kang, and X. Tang. A learning-to-rank
approach for image color enhancement. In Computer Vision
and Pattern Recognition (CVPR), 2014 IEEE Conference on,
pages 2987–2994. IEEE, 2014.

6. Supplementary
The notation style is the same as that of the main paper.
A different section is created to address each foot-note.

7. Prior Distributions

We now derive the desired conditional distribution by
substituting all the model components previously described.
Hyper-parameters: We use the conjugate prior for
the parameter value precision α, we have that the conditional distribution of α given ∆R, U, V and T follows the
Wishart distribution:

The prior distributions on U, V and T are chosen as normal distributions. We also consider a normal distribution
to model the randomness in the attribute difference values
∆R. The details are as follows:

p(α|∆R, U, V, T) = W(α|W0∗ , ν0∗ ),
ν0∗ = ν̃0 +

p(∆R|U, V, T, α) = N1 (< Ui , Vj , Tk >, α−1 )
2
Ui ∼ ND (0, σU
ID ), ∀ i = {1, . . . , N }

Vj ∼ ND (0, σV2 ID ), ∀ j = {1, . . . , M }
Tk ∼

ND (0, σT2 ID ),

N X
M X
K
X

k
Iij
,

i=1 j=1 k=1

(W̃0∗ )−1
(10)

N X
M X
K
X

∀ k = {1, . . . , K},

where α is precision, ID is a D × D identity matrix,
NZ (µ, Λ) is a Z-dimensional multivariate Gaussian distribution with Z-dimensional mean vector µ and a Z × Z covariance matrix Λ. For both simulation and enhancement
2
experiment, we use α = 2, σU
= σV2 = σT2 = 0.01.
We now choose prior distributions for the hyper-priors.

=

W̃0−1 +
k
(∆Rij
− < FiT P∗ + Q∗ , Vj∗ , Tk∗ >)2

i=1 j=1 k=1

(13)
k
k
where Iij
= 1 if an attribute value ∆Rij
is present (not
k
∗
missing), otherwise Iij = 0. Also, U = FiT P∗ + Q∗ .
For ΘU = {µU , ΛU }, we can integrate out all the random variables given in Equation 12 except U and obtain
the Gaussian-Wishart distribution:

p(α) = W(α|W̃0 , ν̃0 ),
p(ΘU ) = p(µU |ΛU ) · p(ΛU ) · N (µ0 , (β0 ΛU )−1 )·
W(ΛU |W0 , ν0 ),
p(ΘV ) = p(µV |ΛV ) · p(ΛV ) · N (µ0 , (β0 ΛV )−1 )· (11)
W(ΛV |W0 , ν0 ),
p(ΘT ) = p(µT |ΛT ) · p(ΛT ) · N (µ0 , (β0 ΛT )−1 )·
W(ΛT |W0 , ν0 ),
Here, W is the Wishart distribution of a D × D random matrix Λ with ν0 degrees of freedom and a D × D
scale matrix W0 . The parameters in the hyper-priors:
µ0 , β0 , W0 , ν0 , W̃0 and ν˜0 are treated as constants during
training. They are set using prior knowledge of the application. For both experiments, we use: µ0 = 0, β0 = 1, W0 =
ID , ν0 = D, W̃0 = 1, ν̃0 = 1. The Bayesian formulation of
the factorization adjusts the parameters within a reasonable
range.

8. Sampling of Hyper-parameters
Conditional distributions in Gibbs Sampling: The
joint posterior distribution can be factorized as:
p(U, V, T, α, ΘU , ΘV , ΘT |∆R) ∝ p(∆R|U, V, T, α)·
p(U|ΘU ) · p(V|ΘV )·
p(T|ΘT ) · p(ΘU )·
p(ΘV ) · p(ΘT ) · p(α)
(12)

p(ΘU |U) = N (µU |µ∗0 , (β0∗ ΛU )−1 ) · W(ΛU |W0∗ , ν0∗ ),
β0 µ0 + N Ū ∗
, β0 = β0 + N, ν0∗ = ν0 + N ;
β0 + N
β0 N
· (µ0 − Ū )(µ0 − Ū )T ,
(W0∗ )−1 = W0−1 + N S̄ +
β0 + N
N
N
1 X
1 X
Ui , S̄ =
(Ui − Ū )(Ui − Ū )T .
where, Ū =
N i=1
N i=1
(14)
similarly, ΘV = {µV , ΛV } is conditionally independent
of all other parameters given V, and its conditional distribution has the form:
µ∗0 =

p(ΘV |V) = N (µV |µ∗0 , (β0∗ ΛV )−1 ) · W(ΛV |W0∗ , ν0∗ ),
β0 µ0 + N V̄ ∗
, β0 = β0 + N, ν0∗ = ν0 + N ;
β0 + N
β0 N
(W0∗ )−1 = W0−1 + N S̄ +
· (µ0 − V̄ )(µ0 − V̄ )T ,
β0 + N
N
N
1 X
1 X
V̄ =
Vi , S̄ =
(Vi − V̄ )(Vi − V̄ )T .
N i=1
N i=1
µ∗0 =

(15)
similarly, ΘT = {µT , ΛT } is conditionally independent
of all other parameters given T, and its conditional distribution has the form:

where Yij ≡ (FiT P + Q) · Vj
p(ΘT |T) = N (µT |µ∗0 , (β0∗ ΛT )−1 ) · W(ΛT |W0∗ , ν0∗ ),
β0 µ0 + N T̄ ∗
, β0 = β0 + N, ν0∗ = ν0 + N ;
β0 + N
β0 N
(W0∗ )−1 = W0−1 + N S̄ +
· (µ0 − T̄ )(µ0 − T̄ )T ,
β0 + N
N
N
1 X
1 X
T̄ =
Ti , S̄ =
(Ti − T̄ )(Ti − T̄ )T .
N i=1
N i=1
µ∗0 =

(16)
Model Parameters: We first consider the latent example (data sample) features U. Since its columns affect the
example features independently, its conditional distribution
factorizes w.r.t. individual Ui .

p(U|∆R, V, T, α, Θ) =

N
Y

p(Ui |∆R, V, T, α, ΘU ).

i=1

(17)
Then for each latent example feature vector Ui ,
p(Ui |∆R, V, T, α, ΘU ) = N (Ui |µ∗i , (Λ∗i )−1 ),
M X
K
X

µ∗i ≡ (Λ∗i )−1 (ΛU µU + α

k k
Iij
Rij Yjk )

j=1 k=1

Λ∗i ≡ ΛU + α

M
K X
X

(18)

k
T
Iij
Yjk Yjk
.

k=1 j=1

where Yjk ≡ Vj ·Tk , which represents element-wise product
between Vj and Tk .
Similarly, for each latent modified version feature Vj , we
have:
p(Vj |∆R, U, T, α, ΘV ) = N (Vj |µ∗j , (Λ∗j )−1 ),
µ∗j ≡ (Λ∗j )−1 (ΛV µV + α

N X
K
X

k k
Iij
Rij Yik )

i=1 k=1

Λ∗j ≡ ΛV + α

K X
M
X

(19)

k
Iij
Yik YikT .

k=1 j=1

(FiT P

where Yik ≡
+ Q) · Tk
For each latent attribute feature Tk , we have:
p(Tk |∆R, U, V, α, ΘT ) = N (Tk |µ∗k , (Λ∗k )−1 ),
µ∗k ≡ (Λ∗k )−1 (ΛT µT + α

N X
M
X
i=1 j=1

Λ∗k ≡ ΛT + α

K X
M
X
k=1 j=1

k
Iij
Yij YijT .

k k
Iij
Rij Yij )

(20)

NON-LINEAR IMAGE ENHANCEMENT FOR DIGITAL TV APPLICATIONS
USING GABOR FILTERS
Yue Yang and Baoxin Li

Centerfor Cognitive Ubiquitous Computing

Department o Computer Science & Engineering
Arizona State University, Tempe, AZ, USA

tyueyang,

baoxin.

ABSTRACT
We propose a non-linear image enhancement method based on
Gabor filters, which allows selective enhancement based on the
contrast sensitivity function of the human visual system. We
also propose an evaluation method for measuring the
performance of the algorithm and for comparing it with
existing approaches. The selective enhancement of the
proposed approach is especially suitable for digital television
applications to improve the perceived visual quality of the
images when the source image contains less satisfactory
amount of high frequencies due to various reasons, including
interpolation that is used to convert standard definition sources
into high-definition images.

1. INTRODUCTION

Enhancing the perceptual sharpness of an image is a wellstudied topic that has found many applications. A typical image
enhancement scheme can be illustrated by the diagram in Fig.
1, where the enhancement is achieved by adding back to the
original image a high-pass image derived from the original
image, after proper post-processing (including linear or nonlinear operations). This is the basic principle behind unsharp
masking and high-boostfiltering [1]. The methods proposed in
[2] and [3] follow similar strategy except that the high-pass
image is post-processed by different non-linear operations.
Non-linear processing can presumably generate new frequency
components and thus it is atttite in some applications. The
method of [2] is a global approach, which uses the Laplacian
pyramid representation of an image to extract the highfrequency components of the original image. After proper
nonlinear mapping, those components are then added back to
the original image to achieve the enhancement. The major
nonlinear step involves clipping and scaling the extracted
components. The method of [3] can be viewed as a local
approach, where the best result is achieve by first detecting the
orientation of the edges in the image and then applying a
similar non-linear processing along the perpendicular
directions of the edges. A global approach has some
advantages such as easier implementation of the algorithm and
simpler control of the algorithmic parameters.
Output

Input

image

L4~ ~ ~
r1r

High-pass

Post-processing

image

filtering

Figure 1. A typical scheme for image enhancement,
where Post-processing could be simple scaling (linear) or
other complex non-linear operations.

0- 7803-9332-5/05/$20. 00 C2005 IEEE

ii}(@asu.edu

In this paper, we propose a new global technique for non-linear
image enhancement by using Gabor filters. In particular, we
show that the proposed method allows orientation-selective
enhancement based on the contract sensitivity function (CSF)
of the human visual system, so that we can sharpen the image
while keeping the subjective ringing effect to a minimum,
making the technique especially suitable for digital television
(DTV) applications. Furthermore, we systematically evaluate
the proposed method and compare it with the method proposed
in [2].

2. PROPOSED ENHANCEMENT METHOD
2.1 Basic Strategy

The basic strategy of the proposed approach shares the same
principle of the methods in [2,3] and the structure illustrated in
Fig. 1. That is, assuming that the input image is denoted by I,
then the enhanced image 0 is obtained by the following
processing
O = I NL (HP( I))
where HP( stands for high-pass filtering and NL( is a
nonlinear operator. As will become clear in subsequent
sections, the non-linear processing includes a scale step and a
clipping step, similar to [2] and [3]. The HP( step is based on a
set of Gabor filters. (While Gabor filters are typically viewed
as band-pass filters, in this work, due to the particular selection
of the parameters, the corresponding Gabor filters are
effectively of high-pass nature).
2.2. Gabor Filters

Gabor filters have found applications in the enhancement and
processing of fingerprint images and texture images, object and
face recognition, etc. (e.g., see [4-7]). The frequency-selective
and orientation-selective properties combined with the optimal
joint resolution in both spatial and frequency domains make
Gabor filters a good choice for image enhancement. The Gabor
filter can be defined as a form of a plane wave restricted by a
Gaussian envelope. Following ttwotations of [5], we define
the Gabor filter as

0Q

(X)

K2

K2 -22
)[exp(iki) -exp(-( )]A
2
kf exp(-2
2~~~~~~

where x and k are the spatial and frequency vectors,
respectively. The Fourier transform of the above filter is give
by
F 0k- (X) } (ko)
=2 ex(2
u72(k02 + kk2)

~exp(_ 2k2 )-kexp(-

From the Fourier transform of the filter we can see that the first
term of the function is a band pass Gaussian filter centered at
the frequency given by ko.
2.3. Non-linear Enhancement Using Gabor Filters

The first step of the proposed method is to use Gabor filters to
extract directional high frequency components from the
original image. We choose four directions for the enhancement:
vertical, horizontal, diagonal at 450 and 1350 respectively. Let
Hi denote the output of Gabor filter Gi, at the above four
orientations respectively, we have

Hi =G1(I)

i=1,2,3,4

These four different high frequency components are then
clipped to obtain four new images Ci, i=1,...,4:

Ci= clip(Hi)

where the clipping function clip() is defined as
x 2T
pT
-T<x<T
(1)
clip (x) x
- T
x < -T
with T being a threshold defined according to the maximum
value of corresponding Hi. The clipping is the source of
nonlinearity.
The second step is to scale these clipped high frequency
components by a constant greater than 1, and then we add them
back to the original image:

O = I + siCi +

S2C2 + S3C3 + S4C4

(2)

wheres i, i=1,...,4, are the scale factors.

2.4. Selective Boosting Based on Human CSF

Adding high-frequency components to the original image may
creates noticeable ringing effects, especially when the highfrequency components are harmonics of some low frequencies
of the original image. Therefore, on one hand, we may want to
use large values for si to get a sharper visual effect; on the other
hand, the values of si cannot be too high in order to avoid
significant ringing effects. Thus the choice of is si critical, as is
the case in [2]. From Eqn. (2), we can easily control the
contribution of the directional high-frequency component
i.
This is an advantage that the existing methods such as [2] do
not have. In applications such as DTV, where the relative
orientation between the shown image and the viewer is in
general fixed, we can exploit the difference contrast sensitivity
of human visual system at different spatial orientations to
determine the best set of scaling parameters in Eqn. (2). In this
paper, according to the CSF of the human visual system (e.g.,
see [8]), we propose to defined the scale parameters as follows,
Si = S, i = 1,2 and si = 1.5 x s, i = 3,4
That is, we enhance the two diagonal orientations 1.5 times
more than the horizontal/vertical orientations. The rationale is
that, since human CSF is smaller along the diagonals, we may
be able to sharpen the image more along those directions
without creating too much noticeable ringing artifacts.
This strategy of selective boosting has also another potential
advantage in the DTV applications where the source image is
compressed by block-based methods, since the relatively less
strong enhancement along the vertical and horizontal directions
will cause less ringing artifacts along the block boundaries.

3. PROPOSED EVALUATION METHOD
The performance of a perceptual image enhancement algorithm
is typically judged through a subjective test. In most current
work in the literature, such as [2,3], this subjective test is
simplified to simply showing an enhancement image along
with the original to a viewer. While a viewer may report that a
blurry image is indeed enhanced, this approach does not allow
systematic comparison between tow competing methods.
Furthermore, since the ideal goal of enhancement is to make up
the high-frequency components that are lost in the imaging or
other processes, it would bdesired to show whether an
enhancement algorithm indeed generates the desired highfrequency components. The tests in [2,3] do not answer this
question. (Note that, although showing the Fourier transform
of the enhanced image may illustrate whether high-frequency
components are added, as in [2], this is not an accurate
evaluation of a method, due to the fact that the Fourier
transform provides only a global measure of the signal
spectrum. For example, disturbing ringing artifacts may appear
as false high-frequency components in the Fourier transform.)
In this paper, we propose to use a new evaluation method,
combining peak-signal-to-noise-ratio (PSNR) based evaluation
with well-designed psychophysiclests, to systematically
compare two competing approaches.

3.1 PSNR-based Evaluation

The first step of the proposed evaluation consists of PSNRbased comparison. We use high-resolution images with rich
details as the test images. We blur the test images (ground
truth) with a low-pass filter to simulate the blurring
degradation. We then enhance the artificially-blurred image
with the proposed method and a competing method, and then
compare the enhanced image against the ground truth, in terms
of PSNR. (In addition, the resultant images can be tested via
the subjective tests described in Sect. 3.2.)

Although PSNR is not deemed as a good measure for
subjective quality, in the designed experiments, since we have
high resolution images, we can use PSNR to measure how
close the enhanced image is to the original one, and thus PSNR
in this case provides a metric for comparing two competing
methods by evaluating how much real high-frequency
components (instead of false "details" due to noise) are added
by a given method, through comparing against a true highfrequency image. This process is illustrated in Fig. 2.
Input image
(high resolution)

-

Error image

Figure 2. Evaluation based on PSNR.
3.2 Comparative Subjective Evaluation
In the second step of the proposed evaluation, psychophysical
tests are adopted to perform comparative, subjective
evaluation. Viewers are asked to compare the outputs of

different methods. The enhanced images from competing
methods are presented to a viewer at random order (so that only

the operator but not the viewer knows which method was used
to obtain the shown image). The viewers also know nothing
about the enhancement methods. They are only asked to vote
for the better image when comparing two images. Further, the
images are presented at the same location on the screen at
alternating order (i.e., the viewer only sees one image at a time,
but he/she can switch between the two images being
compared). Since the compared images are display in the same
location on the screen, this enables the comparison of even a
tiny difference. We believe that this is a much better and more
accurate way of evaluating two methods, compared with
simply presenting the results side-by-side, as is normally done
in the literature.
4. EXPERIMENTAL RESULTS

It is interesting to note that, when performing subjective tests
with these two images, although Method 0 gets three out of
total six votes from the three viewers initially, after presented
with the original high-resolution image as a reference, the
viewers inevitably voted for the proposed method II.

Figure 3. Partial view of the two high-resolution images.
The original sizes of images are 720x1280 and 1218x975,
respectively.

Based on the evaluation methodsciissed in Sect. 3, we can
perform two kinds of experiments. In the first kind of test, we
begin with a high resolution image, blur it with low pass filter,
and then enhance the blurred image. PSNR is then computed
based on the ground truth and the enhanced image. In the
following, results from two images are presented. The images
are named "Calendar" and "Lady", respectively..
In our comparative experiments, we have chosen the method of
[2] as the competing approach, since it is similarly a global
approach. The best set of parameters recommended in [2] are
used (S=5, C=0.4). For our method, we also choose a set of
fixed parameters with s =1.2. The variance of the Gabor filter
is 0.7. To save computation, the clipping is achieved by simply
limiting the enhanced pixels within [0,255], without invoking
Eqn. (1).
In the following, Method 0 refers to the method of [2] with the
recommended parameter setting given by the authors in the
paper. Proposed method I is used to stand for the basic
algorithm described in Sect. 2.3, while Proposed method II
refers to the variant of the basic algorithm with selective
boosting, as described in Sect. 2.4.

(b)

X

N
) \
V X \ (c)
(d)
Figure 4. A close look (at the native resolution) at the
enhanced images: (a) by Proposed method I; (b) by
Proposed method II; (c) by Method 0; (d) original image.

4.1 The Tests Results Based on PSNR

The two high-resolution images are illustrated in Fig. 3, and the
PSNR results from the three methods are given in Table 1.
From the table, it is clear that the proposed methods outperform
Method 0. Also, in terms of PSNR, the selective boosting
method (Proposed method II) only lags the proposed method I
slightly, but it gives a sharper image than the latter.
Table 1. PSNR for the three methods
Calendar
Lady Image
image
PSNR(dB)
PSNR(dB)
Method 0
23.5843
28.8002
26.3994
30.2453
Proposed method I
30.0346
Proposed method II 25.9031
In Figures 4 and 5, close-up views of the enhanced images are
given, along with the original image (all at 100% of the
original zoom factor). It is noticed that, in Fig. 4, while (c)

looks sharper, it contains noticeable artifacts (e.g., the color of
the digits is distorted). On the other hand, in Fig. 5, the result in
(c) shows too much ringing effects, resulting wrinkles and
black spots that do not exist in the original image.

(a)

(b)

(d

Figure 5. A close look (at the native resolution) at the enhanced
images: (a) by Proposed method I; (b) by Proposed method II;
(c) by Method 0; (d) original image. The strong artifacts in (c)
are obvious, especially when the original (d) is given as a
reference.
4.2 Results of Comparative Subjective Tests
In the experiments based on the psychophysical test described
in Sect. 3.2, three viewers were asked to evaluate the results

from the three methods for four test images, which contain
different level of details, as illustrated in Fig. 6. These are
JPEG-compressed images. We intentionally select JPEGcompressed images in this case, since in the TV application,
many types of sources may subject to similar DCT-based
compression (e.g., MPEG video from a DVD).

Figure 6. Four JPEG-encoded test images with different
levels of details (the original resolution: -768x512).
The final votes for these four images are given in Table 2.
From the voting results, we can see that the technique of the
selective boosting method is on average better than the other
two techniques. Also, the only case where Method 0 seems to
outperform the proposed methods is when the image is highlytextured (the lower right image in Fig. 6), where the ringing
effects of Method 0 invoke the false feeling of richer details. In
the close look of the results in Fig. 7, it is easy to notice the
excessive rings effects introduced by Method 0, while the
proposed methods produce only mild artifacts. Although this
initial test was based on only three subjects, the decisive results
show the potential of the proposed methods.

Imagel

Image2
Image3
Image4

Table2. The votes for four test images
Method 0
Proposed I
Proposed II
0
3
0
0
0
3
1
2
0
2
1
0

plate image. Parts of the processed images are listed in Fig.
8. It is found that the proposed methods do not introduce
anisotropic visual effects. Also, the proposed method II
(selective boosting) produces the most desirable results the
voting-based subjective test.

roo =(RI

c)

J_

(d1)
-1

Figure 8. A close look (at the native resolution) at the
enhanced images: (a) original image; (b) by proposed
method I; (c) by Method 0; (d) by method II (with
selective boosting).

5. SUMMARY AND CONCLUSION
We have proposed an image enhancement method using Gabor
filters. The proposed method allows orientation-selective
enhancement based on human CSF. We also introduced an
evaluation method for measuring the performance of the
proposed method and for comparing it against an existing
method. Results show that the proposed method is promising.

6. REFERENCES

Figure 7. A close look (at the native resolution) at the
enhanced images. Top row: by Proposed method I.
Center: by Proposed method II. Bottom: by Method 0.

4.3 Testing on Circular Patterns
To test if the directional enhancement causes anisotropic
visual effects, we processed a blurred high-resolution zone

[1] R. C. Gonzalez and R. E. Woods, Digital Image
Processing,2 nd Edition, Prentice Hall, 2001.
[2] H. Greenspan, C. H. Anderson, and S. Akber, "Image
Enhancement By Nonlinear Extrapolation in Frequency Space",
IEEE Tran. on Image Processing, Vol. 9, No. 6, 2000.
[3] J.A.P.Tegenbosch, P.M. H7man, M. K.Bosma "Improving
Non-linear Up-scaling by Adapting to the Local Edge
Orientation", Proc. Visual communications and image
processing, 2004.
[4] L. Hong, Y. Wan, and A. Jain, "Fingerprint image
enhancement algorithm and performance evaluation," IEEE
Trans. Pattern analysis and machine intelligence, Vol. 20, No.
8, pp.777-789, 1998.
[5] M.Lades, J.C. Vorbrulggen, J. Buhmann, J. Lange, C. v.d.
Malsburg, R.P. Wulrtz, W. Konen "Distortion Invariant Object
Recognition in the Dynamic Link Architecture", IEEE
Transactions on Computers ,1993.
[6]. J. Yang, L. Liu, T. Jiang' and Y. Fan "A modified Gabor
filter design method for fingerprint image enhancement",
Pattern Recognition Letters, Vol. 24, No. 12, pp. 1805-1817,
2003.
[7] E. Peli, "Adaptive Ennhancement Based on a Visual
Model", Optical Engineering, 1987.
[8] E. Peli, "Contrast Sensitivity Function and Image
Discrimination", Journal ofOptical Society ofAmerica, A, Vol.
18, Nov. 2, pp. 283-293, 2002.

Proceedings of the Seventh International AAAI Conference on Weblogs and Social Media

Towards Predicting the Best Answers
in Community-Based Question-Answering Services
Qiongjie Tian and Peng Zhang and Baoxin Li
Computer Science and Engineering
Arizona State University
Tempe, Arizona, 85281
{qtian5,pzhang41,baoxin.li}@asu.edu

Abstract

answer. The study on best answer prediction can also contribute to the understanding of answer quality and help users
improve their answers.
For a candidate answer Ac to be considered as the best
answer, in general three factors need to be assessed: (1)
the quality of the answer content (e.g., its readability); (2)
whether the answer contributes to solving the given question
Q; and (3) how it competes with other answers Ai . These are
schematically illustrated in Figure 1). We call the third factor
contextual information since it is relative in nature. While
there have been some reported studies ((Adamic et al. 2008;
Shah and Pomerantz 2010; Blooma, Chua, and Goh 2010),
to be detailed in the next section) on predicting the best answer, it remains to be fully explored to consider all these factors coherently and to evaluate the importance of the contextual information in solving the problem. This is the objective
of this study.

Community-based question-answering (CQA) services
contribute to solving many difficult questions we have.
For each question in such services, one best answer can
be designated, among all answers, often by the asker.
However, many questions on typical CQA sites are left
without a best answer even if when good candidates are
available. In this paper, we attempt to address the problem of predicting if an answer may be selected as the
best answer, based on learning from labeled data. The
key tasks include designing features measuring important aspects of an answer and identifying the most importance features. Experiments with a Stack Overflow
dataset show that the contextual information among the
answers should be the most important factor to consider.

Introduction
Community-based question-answering (CQA) services help
people solve many difficult questions. The importance and
huge societal impact of such services are evidenced by
the heavy traffic observed on popular CQA sites like Yahoo Answers (answers.yahoo.com), Baidu Zhidao (zhidao.baidu.com), and Stack Overflow (stackoverflow.com).
On a CQA site, a person (the asker) posts a question and
waits for answers from other users (the answerers). If multiple answers are provided, the asker can select the most suitable one, which is called the accepted answer or the best
answer. Questions that do not have a designated best answer are stamped as ”not-answered”. Not every asker always
selects the best answer for his/her question. This could be
simply due to lack of action, or due to the difficulties in deciding on the best answer. As a result, many questions are
left as ”not-answered” (e.g., see (Yang et al. 2011)). Notanswered questions do not facilitate knowledge exchange, as
other users would hesitate to rely on them for information,
given their ”not-answered” labels, even if in reality there
may be many good candidate answers posted. Some sites
also delete such not-answered questions after certain time of
their posting, resulting in lost knowledge if there is indeed
a suitable answer posted already. Towards addressing these
problems, this paper focuses on learning from labeled data
to predict whether an answer should be selected as the best


	


Figure 1: It illustrates three factors in assessing the likelihood of an answer Ac under consideration as the best answer: the dash-lined rectangle indicates the answer set to the
question Q. fA↔Q is the set of features measuring relevance
of Ac to Q, fA is the set of features measuring the inherent
quality of Ac , and fA↔A is the set of features measuring the
competition between Ac and the other answers A0 , · · · , AN .

The major contribution of the work is twofold. Firstly,
based on the analysis of a large CQA dataset, we designed
features to measure the three key factors in selecting the
best answer, especially contextual information. Secondly,
through designing and evaluating a learning approach using
these features to predict whether an answer may be selected
as the best answer, we studied the importance of the factors
based on their contribution to making the correct prediction.

c 2013, Association for the Advancement of Artificial
Copyright �
Intelligence (www.aaai.org). All rights reserved.

725

Related Work
There are a few related studies in the literature. Liu et al.
worked on predicting the asker’s satisfaction with the answers (Liu, Bian, and Agichtein 2008). The features used
do not measure contextual information among the answers.
Harper et al. studied answer quality by answering two research questions: how the answer quality in different CQA
sites is different from each other and how askers receive
better answers (Harper et al. 2008). They found that feebased CQA sites are more likely to receive high quality answers. Jeon et al. continued to work on the further effect
of price on answer quality in fee-based CQA sites (Jeon,
Kim, and Chen 2010). For the answer quality in different
CQA sites, Fichman also made a detailed comparison (Fichman 2011). Shah et al. worked on the best answer prediction
(Shah and Pomerantz 2010). In their work, they extracted
features which contain information from the questions, the
answers, and the users. But there is no consideration on the
relationship between the answers and the questions, or relationship among the answers. This is the same case with the
work in (Blooma, Chua, and Goh 2010). Yang et al. worked
on predicting whether a question will receive the best answer and analyzed which factors contribute to solving the
problem (Yang et al. 2011). Adamic et al. studied activity
characteristics and mentioned how to predict whether one
answer is the best answer given the question with its answers (Adamic et al. 2008), using content feature proposed
in (Agichtein et al. 2008). In both cases, not all the factors
were considered and especially the contextual information
among the answers was not explicitly employed.

Figure 2: This is a sample to show the questions and answers
on Stack Overflow site.
lect questions posted in June 2011 and then track all the answers or comments until August 2012. That is, each question
was posted for more than one full years before the answers
were collected. In this way, we may assume that all the questions were given enough time to gather good answers. This
resulted in a subset of 103,793 questions and 196,145 answers, on which the later experimental results were based.

Features Description
As described above, our goal is to predict whether an answer will be selected as the best answer. We now design
features for a given answer (with its corresponding question and other answers). The questions and answers are first
preprocessed via standard procedures as illustrated in Figure 3, where the original text streams (sentences) are represented by the vector-space unigram model with TF-IDF
weights (Shtok et al. 2012). In subsequent discussion, this
pre-process result will contribute to the extraction of the following features (Table 1), corresponding to the three factors
(Figure 1) discussed previously.

Stack Overflow Description
This study is based on Stack Overflow, a CQA site for computer programming, which was selected for its good quality
control on the questions (and accordingly the answers) since
any post unrelated to programming will be deleted automatically or via voting by senior users. Each question has three
main parts: title, body and tags. In the body part, askers can
describe their problems in detail. They may use figures or
URL links etc. For tags, they may choose at most five existing terms that are most related to the question, or they
can create new tags. Each question may receive multiple answers. For each question or answer, users can add comments
to further discuss it. If one comment is good for solving the
problem, it will be awarded with a score which shows in
front of the comment. For each post (a question or an answer), it will have upvotes or downvotes from senior users
and the corresponding askers or answerers will earn or lose
reputation correspondingly. For a question, after it receives
multiple answers, the asker can select one which in his or
her opinion is most suitable for his or her question. The selected answer is called Accepted Answer, which is used in
this study interchangeably as the best answer. Figure 2 illustrates one sample on Stack Overflow.
The dataset we used in this paper was downloaded from
Stack Overflow for questions and answers posted before August 2012. The original dataset has contains 3,453,742 questions and 6,858,133 answers. In our experiment, we first se-

Answer Context: fA↔A
To describe the context information, we use three features:
similarity between the answer Ac under consideration and
other answers Ai to the same question, the number of Ai ,
and the order Ac was created ans index (e.g. by sorting the
creation time, we know that Ac is the 4th answer to its question). The similarity feature has three dimensions: average,
minimum and maximum similarity between Ac and Ai as
defined below:
�
sim(Ac , Ai )
ave Ans sim =

i�=c

(2)

max Ans sim = max sim(Ac , Ai )

(3)

i�=c

i�=c

726

(1)

num(Ai�=c )
min Ans sim = min sim(Ac , Ai )

Table 1 summarizes the above three types of features. Together, we compute a 16-dimensional feature vector for a
candidate answer under consideration.

Prediction via Classification
With the features extracted for a candidate answer, we predict if it may be selected as the best answer through learning a classifier using labelled data: feature vectors corresponding to best answers and non-best-answers according
to the ground-truth are used to learn a 2-class classifier.
The classifier we used is based on the random forest algorithm(Breiman 2001). Random forest is an efficient algorithm to classify large dataset. It also provides an efficient
approach to computing feature importance, which is useful
for us to analyze the importance of each feature Table 1.

Figure 3: This figure shows the process to compute the similarity between two sentences. Part A is the pre-process module which is used in Part B. Part B is the flow chart to show
how to compute the similarity.

Table 1: Features designed for an answer Ac to a question
Q. Ai are other answers to Q.
group index symbol
feature description
0,1
ave comment,
they are the average and
var comment
variance of the scores of
the comments to Ac .
2
comment num
Ac ’s comments number.
fA
3, 4, URL tag, pic, they show whether Ac
has a URL tag, illustra5
code
tion figures, or codes.
6
ans len
it is the length of Ac .
7, 8 readability
they show whether Ac is
easy to read, see Eq.6
9
QA sim
the similarity between
Ac and Q. (Figure 3).
fA↔Q 10
timeSlot
the difference between
Ac ’s creation time and
Q’s.
11,
ave Ans sim,
the average, minimum,
12,
min Ans sim,
maximum of similarities
max Ans sim
13
between Ac and Ai .
fA↔A 14
competitor num the number of Ai .
15
ans index
the order that Ac was
created. E.g. it is the 2nd
answer to the question.

where sim(·, ·) is the cosine similarity as in Figure 3 and
num(Ai�=c ) is the total number of other answers Ai .

Question-Answer relationship: fA↔Q
This group of features are based on the similarity between
Ac and Q, which is sim(Ac , Q), and also the time lag between the postings of the question and the answer, which
is timeSlot(Ac , Q). Since each question consists of a title
and a body, to compute the similarity, we combine the title
and the body before calculating the cosine similarity. Because the question can receive an answer at any time if it is
not locked or closed, the time lapse between question and
answer varies dramatically (e.g., from a few seconds to one
year in our data). Thus, we represent this lag using logarithm
scale.
QA sim = sim(Ac , Q)
(4)
timeSlot = timeSlot(Ac , Q)
(5)

Answer content: fA
To describe the content quality of an answer, multiple features are defined below:
• Features from the answer body: the length of answer
body, whether it has illustration pictures/codes, whether
it refers to other web pages using URL, etc. Moreover,
if one answer has a clear paragraph structure instead of
messing everything up into one paragraph, it will be easy
to read and then likely to be selected as a best answer.
Thus, the readability of the answer also affects whether
the answer will be selected as best answer and we define
it as features related with paragraph length (Eq.6).
readability = [max(Li ),
i

M
1 �
Li ]
M i=1

Experimental Results
The experiments were based on the Stack Overflow dataset
described earlier. Among the 103,793 questions and 196,145
answers used, there are 4,950 questions that do not have
any answer and 45,715 questions with only one answers.
For questions with only one, 16,986 of them have no best
answers while 28,729 having the best answers. We used all
196,145 answers in our experiment, with the best answers as
positive samples and the negative samples being the answers
that are not best answers.
We use random forest classifier to do classification and
twofold cross-validation. The average accuracy is shown in
Table 2. We emphasize that the focus of this study is on analyzing only features extracted from the questions and answers without using user-specific information. User-specific

(6)

where Li is the length of ith paragraph of the answer and
M is the total number of paragraphs.
• Features from an answer’s comments: The features are the
number and average score of the comments and the variance of the scores.

727

large dataset have shown that some features, and in particular those reflecting the contextual information among the
answers, are more important for the task. The results also
suggest that the features designed in the paper appear to be
able to do the job reasonably well. In the future, we plan to
study the importance of user-centric information (e.g., usage
history, location etc.) for the prediction problem.

Acknowledgments

This work was supported in part by a grant (#1135616)
from the National Science Foundation. Any opinions expressed in this material are those of the authors and do not
necessarily reflect the views of the NSF.

Figure 4: The distribution of feature importances. The bars
correspond to 16 features defined in Table 1, respectively.

References

Adamic, L.; Zhang, J.; Bakshy, E.; and Ackerman, M. 2008.
Knowledge sharing and yahoo answers: everyone knows
something. In Proceedings of the 17th international conference on World Wide Web, 665–674. ACM.
Agichtein, E.; Castillo, C.; Donato, D.; Gionis, A.; and
Mishne, G. 2008. Finding high-quality content in social
media. In Proceedings of the international conference on
Web search and web data mining, 183–194. ACM.
Blooma, M. J.; Chua, A.-K.; and Goh, D.-L. 2010. Selection
of the best answer in cqa services. In Information Technology: New Generations (ITNG), 2010 Seventh International
Conference on, 534–539. IEEE.
Breiman, L. 2001. Random forests. Machine learning
45(1):5–32.
Fichman, P. 2011. A comparative assessment of answer
quality on four question answering sites. Journal of Information Science 37(5):476–486.
Harper, F. M.; Raban, D.; Rafaeli, S.; and Konstan, J. A.
2008. Predictors of answer quality in online q&a sites. In
Proceeding of the twenty-sixth annual SIGCHI conference
on Human factors in computing systems, 865–874. ACM.
Jeon, G. Y.; Kim, Y.-M.; and Chen, Y. 2010. Re-examining
price as a predictor of answer quality in an online q&a site.
In Proceedings of the 28th international conference on Human factors in computing systems, 325–328. ACM.
Liu, Y.; Bian, J.; and Agichtein, E. 2008. Predicting information seeker satisfaction in community question answering. In Proceedings of the 31st annual international ACM
SIGIR conference on Research and development in information retrieval, 483–490. ACM.
Shah, C., and Pomerantz, J. 2010. Evaluating and predicting answer quality in community qa. In Proceeding of the
33rd international ACM SIGIR conference on Research and
development in information retrieval, 411–418. Citeseer.
Shtok, A.; Dror, G.; Maarek, Y.; and Szpektor, I. 2012.
Learning from the past: answering new questions with past
answers. In Proceedings of the 21st international conference
on World Wide Web, 759–768. ACM.
Yang, L.; Bao, S.; Lin, Q.; Wu, X.; Han, D.; Su, Z.; and Yu,
Y. 2011. Analyzing and predicting not-answered questions
in community-based question answering services. In Proceedings of AAAI, 1273–1278.

information, when available, can be used to further improve
the performance as done in (Yang et al. 2011).
The distribution of the feature importance is shown in Figure 4. Both Figure 4 and Table 2 indicate that features from
the answer context fA↔A contribute the most. We also compute the average feature importance from the three groups
of features. For features from the answer context, the average feature importance is 0.1202. For the features from
the question-answer relationship, the average feature importance is 0.05871. For the features from the answer content,
the average feature importance is 0.03128. This also shows
the importance of fA↔A . In the following, we discuss feature importances based on Figure 4, respectively.
Table 2: Prediction accuracy for different feature groups.
fA↔A , fA↔Q , fA are three groups of features we described
in the previous sections.
Features
fA↔A
fA↔Q
fA
all
Accuracy 70.71% 60.27% 65.59% 72.27%
In the group fA↔A , the most important feature is competitor num. This suggests that the more competitors the answer
Ac has, the less likely is may be selected as the best answer.
The feature min Ans sim has slightly less but comparable
importance as competitor num. This shows that the best answer is usually most different from the others. However it
does not mean the best answer and the competitors should
be totally different. Since all the answers aim at answering
the same questions, they also should have similarity. We can
see this from the importance of ave Ans sim.
In the group fA↔Q , the feature timeSlot contributes
more than the feature QA sim. This shows that earlier answers have a higher chance to be selected as the best answer.
Within the group fA , comment num and ans len contribute more than the others. This suggests that the best answer is usually the one with more details and comments.
This is reasonable and intuitive. The readability feature
also contributes significantly, suggesting that answers that
are easy to read are likely to be selected.

Conclusion and Future work
We studied the problem of predicting the best answer on
CQA sites. Our experiments and analysis with a reasonably

728

VIRTUAL VIEW SYNTHESIS WITH HEURISTIC SPATIAL MOTION
Wenfeng Li and Baoxin Li
{wenfeng.li, baoxin.li}@asu.edu

Department of Computer Science and Engineering, Arizona State University
ABSTRACT
Probabilistic methods have been used in image-based rendering
for solving the virtual view synthesis problem with Bayesian
inference. To work well, the inference process requires the
input views to be consistent to yield reasonable result, which in
turn constrains the cameras to be very close to each other.
Many approaches to relieving such constraint focus on the prior
model. In this paper, we present a method which treats the
virtual view as the outcome of a spatial motion from one real
view. A sequence of images is generated heuristically to
preserve textures with the aid of steerable filters. Interim results
are further refined with texture-based Markov random field
prior model. Experiments show that the synthesized view can
have satisfactory image quality with only a few input images
from wide baseline cameras.

Index Terms— Virtual view synthesis, image-based
rendering, Bayesian inference, motion
1. INTRODUCTION
Synthesizing a new view other than the input images
captured from different viewpoints of a 3D scene is a main
goal of image-based rendering (IBR) [1, 2]. Typical
methods use 3D data to render a new view by projecting
pixels, such as view-dependent texture-mapping [4], 3D
warping [5], and layered-depth images [5]. Image-based
modeling approaches, such as stereo matching [3], can
compute the 3D data using only the input images. It remains
to be a challenge if an accurate dense depth map is desired
as the quality of a synthesized view is fragile to the depth
map. Without computing the depth map, light field
rendering [7] and plenoptic sampling [8] use a large number
of cameras to capture many views and generate new views
by re-sampling the rays. Recently, probabilistic methods
have been used in image-based rendering [9 - 11] which
model the virtual view synthesis problem as a Bayesian
inference and infer a new image with multiple views as
input data. Those methods can be generally considered as a
maximum a posterior (MAP) approach. Obtaining the
likelihood term is easily done by computing image
similarity among multiple views with the aid of camera
geometry (projection matrix or epipolar constraint). The
advantage of such methods is that photorealistic images can
still be reconstructed even if the depth map is not accurate
and therefore they can tolerate certain degree of ambiguity
due to lack of texture. However, the results by using simple
maximum likelihood estimation (MLE) are noisy as when

978-1-4244-1764-3/08/$25.00 ©2008 IEEE

1508

the viewpoints of input images are widely separated, more
ambiguity in pixel correspondences pose as a challenge
again. To work well, Bayesian inference favors consistent
input data to yield reasonable results, which strictly
constrains the reference views to be close to each other and
requires more cameras to cover a scene.
To reduce the ambiguities, the spatial dependences
among pixels should be taken into account. Markov random
field (MRF) prior is introduced into Bayesian inference for
modeling pixel interactions, such as a texture dictionary
used in the work of Fitzgibbon et al.[9]. Woodford et al. [10,
11] extended this work by using different MRF prior with
field of experts and pairwise dictionaries. Most of the MRF
prior models depend on the likelihood term, which is
vulnerable if the latter fails.
Observing the fact that in real virtual view synthesis
applications, the virtual view is always close to at least one
reference view, we present a framework which treats the
virtual view as an outcome of a spatial motion from one real
view. A sequence of images is generated consequently
starting with the real view. Each image works as a reference
frame to heuristically synthesize the next frame with
Bayesian inference. Steerable filters are used to predict
accurate motion of textures. Small errors still occur in
interim images, but will be fixed with texture-based MRF
prior model.
2. PROBLEM DESCRIPTION
The virtual view synthesis problem can be described as:
given a set of input images X = { X j | j = 1 , … , N } of a 3D
scene as reference views, compute a virtual view Y, where
X j = { x j ( r , c ) } and Y = { y(r, c) }, with x and y
representing pixels. For simplicity, X j and Y are always
treated as 2D matrices. We assume known camera geometry
in the form of the projection matrices for all reference view
X j ’s and the virtual view Y. However, this is not required as
any approach that can connect corresponding pixels will
suffice. For each pixel y(r, c) in the virtual view, it can be
back-projected to a 3D ray which connects the virtual
camera optical center and the image point. There is still one
unknown value, the depth z. The configuration of all z is
noted as D={z(r, c)}. Following Bayes’ rule:
P( X | Y , D) P(Y , D)
(1)
P(Y , D | X )
P( X )

ICIP 2008

For given observed data X, (1) can be written as:
(2)
P(Y , D | X ) v P( X | Y , D) P (Y , D)
Although D is in the posterior term, the goal of virtual
view synthesis is different from stereo matching. Here Y is
the target to obtain. As long as synthesized view Y is desired,
D can be different from the ground truth. For example, for
regions of an image with little texture, accurate per-pixel
depth may be difficult to obtain, but the synthesized view
could still look realistic.
If considering the pixels as i.i.d. random variables, the
posterior can be written as the product of the per-pixel
posterior,
N

 P( x

j

| y, z ) P( y, z )

(3)

( r ,c ) j 1

With any likelihood model, for example, the minimum
variance on colors, one can find an MLE solution by
focusing on that term only. To improve upon MLE, the
prior term needs to be considered, which may be modeled
through a potential function in random fields such as in [9]:
(4)
P( y, z )  P( y v , z )
V

where V is defined as a set of neighbor indices on site (r, c),
and 4-neighbor model is commonly used.
To compute the likelihood, y and z has to be enumerated
in the color space and the depth space. In practice, z is
considered to follow a uniform distribution within certain
range and can be discretized evenly into a finite set of
sampling points. While enumerating z alone is tractable,
enumerating both z and y for each pixel is impractical.
Fortunately, we show in the below that this is not necessary.
For one point (r, c) with z, the pixels sampled from the
reference views are determined by the camera geometry. If
the negative logarithm of probability is used as an energy
function, the energy that needs to be minimized is:

Ecolor

N

¦ || y  x

j

||2

(5)

j 1

where the Euclidian distance in the RGB color space is used
to measure the similarity of pixels. As the function has a
quadratic form, the best color that minimizes (5) is the mean
value of all x j .
To be robust to noise and scene occlusions, an energy
function with robust kernels can be used to replace (5). The
best color can be obtained with iterated methods such as
mean-shift [11] or EM algorithm.
3. PROPOSED METHOD
In existing methods, all input images make equal
contribution to the new view, which overlooks the fact that
some viewpoints are closer to the virtual viewpoint and
therefore the images from those viewpoints should have
more in common with the virtual view. We simplify the
problem by assuming that the image of the virtual view can
be obtained by going through a spatial motion from one

1509

reference image whose viewpoint is closest to the virtual
view. In real applications, the virtual view is often defined
as an interpolation of two views, or an extrapolation from
one view. In the first case, one of the reference views,
which can be chosen by the interpolation coefficient, could
be set as the base view. For instance, if it is a linear
interpolation, 0.5 will be the threshold to choose which
view to use. In the second case, the base view is naturally
chosen as the view to extrapolate and the spatial motion will
be the real motion for the extrapolation. The viewpoints of
those interim images can be generated with the camera
matrix operations, such as applying small camera rotation
and translation.
We describe our approach as follows
Step 1: Warping to the next view.
The straightforward way to use one reference image to
generate the next frame is to warp the reference image to the
target position by some spatial transform. This requires a
depth map but if the motion is very small, the error caused
by failure of detecting correct depth will also be very small.
As a matter of fact, most of the depth map failures
happening on textureless regions can be ignored in small
motion warping. For each pixel, the following probability
function is used to find a best depth:

p

exp(

¦ min(|| x
j zi

i

 x j ||,W )

V  ( N  1)

)

(6)

where i is the index of base view, xi is the pixel value, xj’s
are the projected pixel values on all other reference views, Ĳ
is a cutoff value working as a robust kernel and ı is a
scaling factor. For those pixels near edges, their colors can
be modeled as a linear combination of two colors on both
sides and using (6) does not in general give correct result as
it is very sensitive to the coefficient of linear combination,
which could be drastically different when the views are
widely separated. We use steerable filters to find the
correspondence of those pixels in areas with color gradients.
The 7×7 spatial filters are shown in Figure 1, which have
the following mathematical expression:
(7)
H ( x, y ) G1 ( x) * G0 ( y )
where Gn(u) denotes the n-th order Gaussian derivative
filter on direction u. The steerability feature [12] of these
filters provides an efficient way to handle the rotation of the
image patches across multiple views. The filter response at
each point is steered to the orientation of its maximum
intensity gradient by:
(8)
RT ( x, y ) R1 cos T  R2 sin T
where R1 and R2 are the filter responses with two bases and
ș = tan-1(R1/R2). A new filtered image is illustrated in Figure
1.
The probability given by (6) can also be applied to the
filtered images. By combining the probability from both the
original and the filtered images, we have

˄9˅
p p c  wp r
where pc and pr have the form of (6) but apply to original
and the filtered images respectively. w is a weighting
coefficient which is proportional to the strength of the filter
response. Figure 2 compares the depth map computed with
and without using filters, it shows that the depths on edges
are correctly computed with filters. We do not claim that
our approach is better than many others to find correct depth
map, but we have been able to predict the motion of edges
with given multiple views. Using this depth map, pixels are
warped to the next frame and an initial image is obtained. If
there is duplication when multiple pixels are mapped to the
same point, the one that has greater filter response wins. It
implies textured regions are favored over textureless regions.

a
b

c

based MRF prior similar as in [9] by writing the energy
function as
E E color  Etexture
(10)
N
j 2
min(||
y

x
||
,
W
)

O
||
V

V
||
¦
s
t
j 1

where Vs is a small patch on the target view and Vt is a small
patch on the base view around the corresponding point
given depth z. Ȝ is a weighting coefficient. Computing the
texture energy for each point is computationally prohibitive.
To improve efficiency, we modify the MRF model as
follows: Divide the target view into 5×5 blocks, for each
block, use the center point to compute the corresponding
point on the base view and then the texture energy in sumof-absolute-differences. This energy will be used for all
points that fall into that block. Iterated conditional modes
(ICM) is used to optimize the MRF. In our experiments, 3
iterations of ICM can produce good result.
When a new frame is synthesized, it works as the base
view in the next round and above procedures are repeated
until the desired virtual view is produced.
4. EXPERIMENTS

(a) Steerable filters
(b,c) Filtered images
(d) Steered images

d
Figure 1. Steerable filters and filtered images. After
being steered to strongest gradient orientation, both
horizontal and vertical edges are emphasized.

Figure 2. Depth map computed with (right) and without
(middle) filters for part of image (left). Errors on edges
of eyes and nose are fixed when using steerable filters.
Step 2: Heuristic inference
As step 1 is done by per-pixel forward mapping, there are
many points on the next frame that are not filled. We use
MLE approach by minimizing the energy in (5) to find best
colors for such points. To use the information from the base
image, only those depths from neighbors within an 8×8
block will be searched. If there are less than 2/3 points in
such block are filled in step 1, all depths will be searched.
Step 3: Texture-based MRF prior
Since in our assumption, the next frame undergoes only a
small motion, the two consecutive images have high
similarity. The texture patterns in one frame should be able
to find a good match on previous one. We add a texture-

1510

We use multiple sets of data in our experiments, which are
provided by [9-11]. For the monkey data, there are 89
images from different viewpoints extracted from a video
sequence (Figure 3). These views are densely positioned but
when the distance between indices increases the views are
becoming drastically different to be used for image-based
rendering. 6 evenly separated views are chosen as reference
images which are as far from each other as possible. We use
the camera matrices of those views that are not chosen to
reconstruct images and compare them with the ground truth.
We are able to synthesize those views with proposed
approach with reasonably good quality as shown in Figure 3.
To the authors’ knowledge, synthesizing a virtual view from
such widely separated views cannot be addressed well by
other probabilistic approaches which typically use only
views very close to the virtual view.
The quantitative analysis with root-mean-square (RMS)
error in one of our interpolation test is in Figure 4. Images
between reference view 30 and 45 are reconstructed and
compared with ground truth. While the RMS error increases
as the viewpoint moves away from one reference view, at
the maximum point, which in most cases is roughly the
middle point between two reference views, the synthesized
image can still keep good quality. For comparison, we also
synthesized the views with MLE method using a robust
kernel energy function and mean-shift algorithm, the quality
is worse and it has no obvious relation with the distance
between the virtual view and a reference view. This could
be explained as no prior information about the view
positions is used in the synthesis.
Other than interpolation, the new view does not have to

be limited between two existing views. Figure 5 shows an
example of extrapolation with plant and toy dataset. A new
viewpoint other than all known views is generated by
applying camera rotation and translation to a reference view.

efficiency is compromised as many interim frames have to
be synthesized. We believe this will not be the main
obstacle for this approach to be deployed as there is much
room left in the algorithm to be optimized. One piece of
future work could be how to find the optimal base view and
motion speed.

Figure 5: Extrapolation: using proposed method to
synthesize a new view with plant & toy dataset.
Textures and details are preserved even on large textureless background
6. REFERENCES

Figure 3. Interpolation: proposed approach use only 6
widely separated views from 89 views to reconstruct
others. Subjective assessment: one reconstructed image
with MLE (left) and proposed approach (right).

Figure 4: RMS error of reconstructed views between
reference view 30 and 45 in monkey dataset.
5. CONCLUSION AND DISCUSSION
A new virtual view synthesis approach has been proposed
by generating a spatial motion from one reference views.
Experimental results demonstrated the feasibility and
effectiveness of proposed approach. One drawback is that

1511

[1] H. Y. Shum and S. B. Kang, A review of image-based
rendering techniques, IEEE/SPIE Visual Communications and
Image Processing (VCIP), pp. 2-13, 2000.
[2] C. Zhang and T. Chen, A survey on image-based rendering:
representation, sampling and compression, Signal Processing:
Image Communication, vol. 19(1), pp. 1-28, January 2004.
[3] D. Scharstein, Stereo vision for view synthesis, Proc. IEEE
CVPR, 1996.
[4] P. E. Debevec, G. Borshukov, and Y. Yu. Efficient viewdependent image-based rendering with projective texturemapping. In 9th Eurographics Rendering Workshop, Vienna,
Austria, June 1998.
[5] W. Mark, L. McMillan, and G. Bishop. Post-rendering 3d
warping. In Proc. Symposium on I3D Graphics, pp. 7–16,
1997.
[6] J. Shade, S. Gortler, L.-W. He, and R. Szeliski, Layered depth
images, In Computer Graphics (SIGGRAPH’98) Proceedings,
pp. 231–242, 1998.
[7] M. Levoy and P. Hanrahan. Light field rendering, In
Computer Graphics (SIGGRAPH’96) Proceedings, pp. 31–42,
1996.
[8] S. J. Gortler, R. Grzeszczuk, R. Szeliski, and M. F. Cohen,
The lumigraph, In Computer Graphics (SIGGRAPH’96)
Proceedings, pp. 43--54, 1996.
[9] A. W. Fitzgibbon, Y. Wexler and A. Zisserman, Image-based
rendering using image-based priors, ICCV 2003, pp. 11761183.
[10] O. Woodford, I. Reid, P. Torr and A. Fitzgibbon, Fields of
experts for image-based rendering, BMVC 2006.
[11] O. Woodford, I. Reid, and A. Fitzgibbon, Efficient new-view
synthesis using pairwise dictionary priors, CVPR 2007.
[12] W. T. Freeman and E. H. Adelson, The design and use of
steerable filters, IEEE Trans. on Pattern Analysis and
Machine Intelligence, 13(9), pp. 891-906, 1991.

AN ENHANCED RATE CONTROL SCHEME WITH MOTION ASSISTED
SLICE GROUPING FOR LOW BIT RATE CODING IN H.264
Avin Kumar Kannur and Baoxin Li
Department of Compute Science and Engineering
Arizona State University, Tempe, Arizona, 85281
Email: {akannur, baoxin.li} @asu.edu
FMO [4] is a unique tool set available in H.264 and is
primarily used for error resilience and error concealment video
coding [5]. Not limiting to this, it is also shown to be a handy
feature for coding arbitrary-shaped Region Of Interest (ROI)
[6] [7]. In such methods, the picture is divided into a
foreground region with multiple or single slice groups and a
background region (often static or with no or less motion
contents).
In this paper we present a rate control scheme for low bit rate
applications based on an existing frame layer bit allocation
scheme [1] and a novel ROI classification technique for
grouping macro-blocks (MB) into slice groups. The grouping
is based on the motion analysis using MV-based classification,
which is further coupled with a spatial object mask derived
from change detection. This results in two rate control schemes
for H.264, which are evaluated and compared with a set of
standard test videos.
The proposed rate control schemes are presented in Section
2. Experimental results are documented in Section 3, and we
draw conclusions from our work in Section 4.

ABSTRACT
This paper presents an enhanced rate control scheme for H.264
using motion detection and motion analysis. PSNR based
measure is used for determining motion complexity and
detecting scene changes. Motion Vectors and Rate Distortion
measures are further used to classify regions with motion
inside a frame and Macro-blocks are then grouped into slices
using the Flexible Macro-block Ordering feature of H.264. We
present a filtered slice grouping technique to maintain a stable
region of motion. The proposed method provides smoother
quality videos with better average PSNR. The bit rates are
within bound and fewer frames are skipped in sequences with
scene changes at low bit rates.
Index Terms – Flexible Macro-block Ordering (FMO),
Filtered Slice Grouping, Scene change, Frame Skipping.

1. INTRODUCTION
H.264/AVC is the latest video coding standard proposed by
ITU-T Video Coding Experts Group and the ISO/IEC Moving
Picture Experts Group (MPEG) and supports a wide variety of
video applications with varying bit rate constraints. While
there are many advanced rate control schemes for H.264/AVC,
there is still a need for efficient rate control schemes which can
handle exceptions such as scene changes (regions where
conventional flow of rate control is broken) in low bit rate
applications in order to maintain the smoothness of quality in
delivering the video. The rate control scheme of [1], which is
based on PSNR and MAD measures for rate control, is along
such a direction.
In addition to handling the irregularity of contents along the
temporal axis as in the case of scene change, there is also a
possibility of allocating different number of bits within a frame
to regions of different properties (such as treating static regions
differently from regions of moving objects). These types of
techniques typically rely on a segmentation step as
preprocessing, which may be done efficiently in the
compressed domain (e.g., [2]). A motion assisted rate control
based on pre-filtered and global motion compensated Motion
Vectors (MV) is presented in [3] which is found to enhance
the subjective quality of regions with high visual sensitivity.

978-1-4244-1764-3/08/$25.00 ©2008 IEEE

2. MV ASSISTED SLICE GROUPING FOR FMO
BASED RATE CONTROL
Conventional rate control algorithms are not effective in
addressing scene changes, which result in abrupt variation of
signal statistics along the temporal axis. Schemes without
considering such abrupt variation often result in buffer
overflow especially at low bit-rates, due to the fact that most
MB in a scene change are typically intra-coded. In these cases
the computed QP value is no longer suitable and it must be
adjusted so that the coming frames are not penalized during bit
allocation. An enhanced rate control scheme using PSNR
based frame complexity measure was presented in [1] which
we incorporate in our rate control. In this approach, scene
changes are detected based on the derived PSNR ratios and are
used to serve as signaling function for bit allocation at scene
changes. Bits are allocated in the frame layer based on the
frame complexity, taking into account the current buffer status
and bits remaining for encoding the remaining frames of the
Group of Picture (GOP).
Furthermore, since MV represents the motion dynamics
within a frame, we group the MBs within a frame into motion

2100

ICIP 2008

regions based on the MVs: those corresponding to moving
regions and those corresponding to static regions. The moving
regions may be further classified into multiple regions
depending on the motion magnitude. Upon the categorization
of the MBs, we utilize the “explicit slice group ordering”
option in FMO to define slice groups (SG), with different
slices corresponding to groups of MBs with different motion
properties. With such slice grouping, we can code the slice
groups with different quality. For example, in applications for
which we believe that moving regions correspond to objects of
higher significance, we can code the slice groups with high
motion MB finely while coding others with low motion MB
relatively coarsely. In the following subsections, we detail our
algorithmic components for implementing the idea.

might have moved before the next update of its slice grouping
label, we also label the neighboring MBs of a high-motion MV
to be SG1. This helps to preserve motion in a small region in
between the two update points. Furthermore, in order to ensure
that regions with large distortion but small motion (such as
textured regions) are allocated sufficient bits, we also
incorporate the distortion measure in the slice grouping
scheme as follows.
We first calculate the mean distortion E[D] using ,
N

E [D ] =

MVmax (i, y ) = max( MV (i, j , y ))

[

]

E MVi , x =

[

]

E MVi , y =

max

(k , x )

k

N int er

¦ MV

max

k

(k , y )

(k )

­ § ( MVi, x > E[ MVi, x ])or ( MVi, y > E[ MVi, y ]) ·
¸
°1, if ¨
¸
X i = ® ¨ or ( RD cos t
min (i ) > E[ D ])
¹
©
°
0, Otherwise
¯

­α ∗ X i + (1 − α 0 ) ∗ Yi −1 ; if ( X i = 1)
Yi = ® 0
otherwise
¯ β 0 ∗ Y i −1 ;

(5)

(6)

(7)

where Xi is the filter input, Yi is the filter output for the ith MB,
Į0 and ȕ0 are filter coefficients with values 0.15 and 0.95
respectively. Į0 is a measure of how fast the system adapts to a
change in ROM and ȕ0 is a measure of how long it retains an
updated ROM.
We then use the updated filter output value to compute the
slice group mapping.

(1)

where MVmax(i,x) and MVmax(i,y) are the maximum MV for the
ith MB in x and y directions respectively and j is a MB sub
partition index. We calculate the mean MV in the
neighborhood of the ith MB using

¦ MV

min

k =1

N
where RDcostmin is the rate distortion cost equation used by the
encoder to choose the optimal mode for encoding an MB with
the least cost, N is the total number of MB coded in inter
mode. Then we compute slice grouping label using the
following filter

2.1. Slice Group Ordering
We assume that neighboring frames differ not significantly
(smooth video) and hence the MV’s of the previous encoded
frame are used to determine the regions of motion in the next
frame. This enables our algorithm to be implemented within a
single pass. We use the magnitudes of the x and y components
of the MV’s in our analysis. The MV can be preprocessed to
account for global motion [2,3]. In an MB that is coded with
multiple coding sub-blocks, we take the maximum value of the
MV components to determine the component values of MB, as
shown below.

MVmax (i, x ) = max( MV (i, j , x))

¦ RD cos t

­ SG1, if (Yi > Ythreshold )
Li = ®
¯ SG 0, otherwise

k ∈ Neighborhood of i (2)

(8)

Ythreshold is set to 0.5 in our experiments. One sample result of
this slice grouping algorithm is shown in Fig 2.1. We see that
this scheme may result in fragmented object segmentation. An
alternative approach is discussed next to alleviate this issue.

k ∈ Neighborhood of i (3)

N int er

where Ninter is the number of MBs coded in inter prediction
mode in the neighborhood region of ith MB. The slice grouping
label for the ith MB is determined as (for a two-category
example)

­° SG1, if ( MVi,x > E[MVi,x ]) or ( MVi, y > E[MVi, y ])
Li = ®
°̄SG0, otherwise

(a)
(b)
(c)
Fig 2.1. (a) Frame 19, (b) Frame 20 of Foreman Sequence, (c)
Slice Grouping – SG 0, SG 1.

(4)

This basic technique, while being intuitive, is not effective in
practice, and thus two schemes are proposed below to enhance
this basic idea.

2.1.2 Scheme 2: Filtered Slice Group Ordering with Object
Mask
MV based slice grouping can lead to fragmented MB slicing
and may not be able to capture the entire object of interest, but
only regions of motion within it. We need an object mask
which can track the entire object (especially in sequences like
video conferencing where motion can be within an object
which occupies significant portion of the frame spatially). We
incorporate a spatial domain object mask generation using the
Change Detection Mask presented in [8]. We then apply our

2.1.1. Scheme 1: Filtered Slice Group Ordering
We found through experiments that, updating the slice group
mapping of the MB for each frame would result in rapid
changes in Region of Motion (ROM). To ensure a relatively
stable ROM, we update the slice groups only periodically at a
predefined rate (e.g., every 4th frame). Considering that an MB

2101

after the safety margin has reached, we proactively skip very
low complexity frames (details in next section) which are very
effectively similar to the previously encoded frames. This
proactive skipping of frames provides noticeable benefits as
will be seen in results listed in the next section.

Scheme 1 to further enhance the motion details within these
objects. We now group the MB into 3 slice groups, SG0 for
static background, SG1 for static parts within the object and
SG 2 for motion parts within the object. Some sample slice
grouping results are shown in Fig 2.2.

PSNR Variation Plot

60

Bit Consumption Plot
30000

JM 12.4
25000

40

Ours

20000

30
20
JM 12.4

10

Bits Used

P SN R in d B

50

15000
10000
5000

Ours
0

0
1

14 27 40 53 66 79 92 105 118 131 144
Frame Number

1

10 19 28 37 46 55 64 73 82 91 100 109 118 127 136 145

-5000

Frame Number

(a)
(b)
Fig 3.1 (a) PSNR Variation and (b) Bit Consumption, for
Akiyo sequence coded at 15fps, 84kbps. (JM , Proposed)

(a)
(b)
(c)
Fig 2.2. (a) Change Detection Masks, (b) Object Masks
generated for Akiyo and News sequences, (c) Slice Grouping,
SG 0, SG 1, SG 2.
2.2. Selection of QP values for the Slice Groups
We choose different QP values for the slice groups so as to
assign more bits to regions with high motion and distortion
contents (SG2 and SG1) and fewer bits to low motion region
(SG0). A simple way of achieving this is
QPi − 2, SG(2)
­
°
QPi,SG( j) = ®
QPi − 1, SG(1)
°min(QP + floor(QP / α) + β ,51), SG(0)
i
i
¯

(9)

where QPi is the quantization parameter for the ith frame, and
QPi,SG(j) is the refined QP for the ith frame and jth slice group. Į
and ȕ are constants with values 8 and 2 respectively.

(a)

(b)

Fig 3.2. Subjective video quality (a) Proposed; (b) JM 12.4.

3. EXPERIMENTAL RESULTS

It was found that the selection of QP for the first I frame of
the sequence is very critical and an improper QP will break the
rate control scheme. With our experiments we found that an
initial QP of 28 works well for most sequences. We can further
refine this by taking into account the GOP size and number of
bits available per picture.

We have implemented the proposed rate control algorithms
based on the JM 12.4 H.264 video codec. For our tests we
used the encoder in Base Profile, RDO enabled, search range
for motion estimation is set to 16, number of reference frame
as 1, entropy coding technique to be Context Adaptive
Variable Length Coding (CAVLC). Standard test sequences in
QCIF 4:2:0 formats were taken for testing our rate control
schemes. GOP size was set to 60 and frame rate to be 15fps
(except for the Claire and the Carphone sequences for Scheme
2, where the frame rate is 20 and 10 fps respectively). We
compare our proposed rate control schemes with JM 12.4
quadratic rate control scheme.
Fig. 3.1 gives one sample result for the PSNR variations and
bit allocations for the Akiyo sequence. It was found that the
PSNR variation is much smoother in the proposed method and
better visual quality is achieved. The bit allocation is done on
the frame complexity basis and we see in 3.1(b) that we have
better bit allocation for frames at the end of GOP (the sequence
contains 3 GOPS). More results are tabulated in Table 3.1,
where for comparative convenience of both the schemes, we
have included a few identical test cases (e.g., Akiyo at about
84kbps). The table also contains a couple of sequences that
consist of multiple concatenated sequences (e.g., AkiyoCarphone) which were intended to simulate scene changes. In

2.3. Scene Change Handling and Frame Skipping
When a scene change is detected, we cannot use the above
filtered slice grouping scheme as all the MB are new. In this
case, we update the slice group mapping on scene changes
since we cannot use the slice group map of the previous
iteration. We use a default rectangular window at the center for
slice grouping, which is a reasonable approach since for most
deliberately captured video; the region of interest is typically
located at the center of a frame. In subsequent iterations the
slice grouping will be updated as described above.
A common problem noticed in low bit rate applications with
scene changes is frame skipping as the encoder will skip
frames if the buffer has reached the safety margin. Frames will
be skipped irrespective of their importance and thus it is very
likely that frames with high motion (which may be significant
in terms of their semantics) are skipped, resulting in a
perceptual loss of video quality. Instead of skipping frames

2102

these concatenated sequences, we repeat the component
sequences to introduce four scene changes.
While Table 3.1 presents the comparison in terms of PSNR,
Fig 3.2 illustrates the subjective video quality of the proposed
approach in comparison with that of the JM 12.4 version. It is
obvious that we gain subjective video quality by allocating
more bits for regions with motion. Fig 3.3 shows the RD
performance curves for two test sequences (Akiyo and News),
which show that for a range of low bitrates (above 32kbps), the
proposed schemes outperform the JM coder. (For even lower
rates, the overhead of our scheme due to slice mapping etc will
offset its benefit and thus the performance gain diminishes.)
We also found that the proposed method of skipping frames
on a proactive basis achieves better result than JM. For our
experiments we set 80% as the buffer fullness limit for JM. In
our method we skip frames when the buffer fullness crosses
65% limit and the PSNR drop is less than 0.45 dB. As a result
we saw our method skips only 5 frames when compared to 12
frames by JM in the salesman sequence. In case of Akiyocarphone sequence the frames dropped are 3 and 19 in the
proposed and the JM versions respectively.

5. REFERENCES
[1] Minqiang Jiang and Nam Ling, “On Enhancing
H.264/AVC Video Rate Control by PSNR-Based Frame
Complexity Estimation”, IEEE Transactions on Consumer
Electronics, 284 Vol. 51, No. 1, February 2005.
[2] Zhi Liu, Yu Lu, and Zhaoyang Zhang, “Real-time
spatiotemporal segmentation of video objects in the H.264
compressed domain”, Journal of Visual Communication and
Image Representation, v 18, n 3, 275-90, June 2007.
[3] Gounyoung Kim, Eleftheriadis, A.; “Motion Vector Field
Analysis in Motion-Assisted Rate Control for H.264”, ICIP,
Page(s):61-64, 8-11 Oct. 2006.
[4] Y. Dhondt and P. Lambert, “Flexible Macroblock
Ordering: an error resilience tool in H.264/AVC,” in Fifth
FTW PhD Symposium, no. 106, Dec. 2004.
[5] Sood, Amit; Chilamkurti, Naveen K; and Soh, Ben, “Study
and analysis of an error resilient technique in H.264 video
using flexible macroblock grouping,” WiCOM 2006, 2007, p
4149485.
[6] Lambert, Peter; De Schrijver, Davy; Van Deursen, Davy;
De Neve, Wesley; Dhondt, Yves; Van De Walle, , “A real-time
content adaptation framework for exploiting ROI scalability in
H.264/AVC” ACIVS Proceedings, p 442-453, 2006.
[7] P. Sivanantharasa’, W.A.C. Fernando, SMIEEE, H.
Kodikara Arachchi, “Region of Interest Video Coding with
Flexible Macroblock Ordering”, First International Conference
on Industrial and Information Systems, ICIIS 2006, 8 – 11
August 2006.
[8] J. Y. Zhou, Ee Ping Ong, Chi Chung Ko, “Video Object
Segmentation and Tracking for Content-Based Video Coding”,
ICME (III), 1555-1558, August 2000.

4. CONCLUSION

We have presented a rate control method based on slice
grouping using motion-analysis-based MB categorization.
When coupled with the frame layer bit allocation scheme, this
leads to an efficient rate control algorithm for low bit-rate
applications of H.264. We evaluated the proposed approach
and compared with the quadratic rate control scheme of JM
12.4, using a set of standard test videos. Results have shown
the effectiveness and advantages of the proposed method.
Table 3.1. PSNR and Bit Rate comparative measures.
Test Sequence
(QCIF, 4:2:0 )

Fig 3.3. Performance Curves (PSNR v/s Bit Rate)

Average PSNR (dB)

Bit Rates (kbps)

JM

Scheme 1

Gain

JM

Scheme 1

Akiyo

42.28

43.01

0.73

85.35

84.07

Salesman

37.20

37.38

0.18

56.54

56.07

Foreman

35.45

35.57

0.12

64.26

64.06

Akiyo-Carphone

37.29

37.94

0.65

64.08

63.91

Foreman - News
Container

38.92
38.86

39.41
39.1

0.49
0.24

96.7
48.01

96.37
48.23

JM

Scheme 2

Gain

JM

Scheme 2

Akiyo

42.28

44.01

1.73

85.35

84.27

Salesman

37.20

37.53

0.33

56.54

56.62

Foreman

35.45

35.84

0.39

64.26

64.16

News

39.07

39.77

0.7

77.35

76.33

Claire

42.45

43.51

1.06

78.24

76.45

Carphone

40.79

40.96

0.17

84.18

84.05

2103

Retrieving Unfamiliar Faces: Towards Understanding
Human Performance
Xu Zhou

Baoxin Li

Arizona State University
Tempe, Arizona, USA

Arizona State University
Tempe, Arizona, USA

xzhou50@asu.edu

baoxin.li@asu.edu

ABSTRACT

selection for face retrieval [3]. In general, existing automatic
algorithms are either still under-performing or yet to be evaluated
on more challenging datasets with images from uncontrolled
imaging conditions, indicating much more room for improvement
on developing face retrieval approaches.

Face image retrieval is to find from a dataset all images containing
the same person in the query image. Automatic face retrieval has
seen fast development in recent years, although humans still
appear to be the better performer on this task. This paper reports a
study towards understanding human performance on retrieving
unfamiliar faces. Wild Web face images are utilized in the study,
and two experiments are designed to assess human performance
and behavior on the retrieval task. The experiments help to
identify a set of important features and also to understand how
human behaved when facing the task of retrieving unfamiliar
faces. Such observations/conclusions may provide guidelines for
improving existing automated algorithms.

In the meantime, there have been efforts reported in the literature,
attempting to understand human performance on face retrieval or
recognition, as it appears human does a better job on such tasks.
For example, in [4] and [6], human performance on face
recognition was compared to automatic algorithms, both drawing
the conclusion that human is more accurate than existing
automatic algorithms. The study of [10] showed that faces of the
same race are easier to recognize by human; and sex effect is
studied in [9], with the conclusion that women are better at
recognizing female faces. Nevertheless, little was revealed on how
and why people obtained better performance, and thus it is
difficult to derive principles for improving automated approaches.

Categories and Subject Descriptors

H.1.2 [Information Systems]: User/Machine Systems –Human
factors, Human information processing.

General Terms

In this paper, we set out to explore human performance on the
task of retrieving unfamiliar faces. We consider only the task of
retrieval of unfamiliar faces (as opposed to face recognition),
since such a task would require a subject to derive all the
necessary information from a given query image. This would
presumably help us to avoid the complicating factors such as
memorized identity in the task of recognizing a known person.
Hence the study would potentially reveal the types of information
that may be employed by an automated system, which would in
general perform the retrieval task with information derived only
from the query image. To the best of our knowledge, this is a
novel problem that has received little attention in the literature.
In our study design, to make the retrieval task even more
challenging, we employ wild Web face images (as opposed to
standard datasets that were used in most existing work). We also
design two types of experiments, intended to assess the conscious
and unconscious thinking process of the subjects respectively in
performing the retrieval task.

Human information processing.

Key words

Face image retrieval; Human performance

1. INTRODUCTION

Face retrieval is defined as given a query face image, to find the
images containing the same person as in the query image from a
large image dataset. The task is in general different from, although
related to, face recognition in that it is not necessary to assume a
finite set of identities (i.e., subjects) for a given dataset, and the
retrieval is done only with respect to the given query image.
In recent years, many face retrieval algorithms have been
proposed. Wu and Ke proposed a face image retrieval system [8]
using a scalable face representation, where a multi-reference
distance approach related to Pseudo-Relevance Feedback (PRF)
[11] was used to rank the candidate images using the Hamming
signature. In [1], a method using soft biometrics was described,
which employs demographic information and facial marks for
improving face image matching and retrieval. In [2], SIFT
features were used for initial retrieval, followed by a relevance
feedback strategy. There are also efforts dealing with feature

2. METHOD

Recognizing that a subject may or may not be able to explicitly
describe how he/she actually performed the task for a given query
image, our basic strategy is to rely on two types of experiments:
one mainly based on questionnaire to the subjects to collect their
self-reported conscious thinking process during making the query
(Experiment 1), and one mainly based on an eye-tracking system
for capturing their unconscious search of the visual field of view
during making the query (Experiment 2). We elaborate below the
experimental protocols and the images used in the experiments.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or
distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by
others than ACM must be honored. Abstracting with credit is permitted. To copy
otherwise, or republish, to post on servers or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from Permissions@acm.org.
MM'15, October 26–30, 2015, Brisbane, Australia.
© 2015 ACM. ISBN 978-1-4503-3459-4/15/10…$15.00.
DOI: http://dx.doi.org/10.1145/2733373.2806381

2.1 Dataset Design

The face images in our experiments were selected from the Face
In the Wild Database [5], a database of face photographs designed
for studying the problem of unconstrained face recognition, which

.

1287

contains more than 13,000 images of faces collected from the
web. These faces have been automatically labeled using the
system described in [7]. Since the dataset has 1680 people who
have two or more images, we were able to select the images that
suit our needs in this study. In particular, we avoid images of
well-known celebrities since our goal is to study human
performance on retrieving “unfamiliar faces”. Twenty query
images of 20 different people, 10 female and 10 male respectively,
were selected for the first experiment; and 40 query images (20
female and 20 male) were used for the second experiment. All
selected images were later examined to see if it is unfamiliar
before it is used in the analysis (see Section 2.2).

Question 2: “What kind of features do you use or rely on when
you retrieve a face?”
Question 3: “Rank the importance of the following features in
your retrieval: a. Hair style; b. Skin tone; c. Facial features; d.
Clothing style; e. Race; f. Age”
In Experiment 2, an eye-tracking system was deployed, which
records the eye fixation points of a subject doing the retrieval task.
The eye-tracking data were time-stamped and thus were later
synchronized with the sequence of queries made and thus we can
analyze a subject’s unconscious search of the visual field for any
given query image.

Additional care was taken in selecting the candidate images for
any query image, so as to make the task more challenging. First,
people appearing in these images belong to the same gender of the
query. Second, we made sure that the candidate images do not
have similar clothing style or background to the query image. On
the other hand, to ensure the task is better-defined and to support
better eye-tacking-based analysis, we also ensured that most of the
images have only one dominant face in it.

2.3 Experimental Procedure

We recruited 11 and 9 subjects for Experiment 1 and Experiment
2 respectively. The age range is from 24 to 31, 2 of them are
female and the remaining are male. None of them had prior
knowledge about what the query experiments were about and they
were trained to perform the task right before the experiments.
Each subject was asked to finish 80 sessions of face retrieval in
the first experiment. Each query image was shown 4 times in
random order, each time with a different candidate set. The 4
candidate sets are: 1. Color images with one and only one frontal
face image (target image) belonging to the same person as the
query image; 2. Color images with one and only one off-frontal
face image belongs to the same person as the query image; 3.Grey
scale version of set 1; 4. Grey scale version of set 2.
The sequence of these 80 sessions is completely random and
within each session, the position of the 25 candidate images is
randomized too. Additional care was taken to avoid positioning
the target image on some special spots (e.g., the top row, where it
may be too obvious, or the right-most row, where it is too close to
the equerry image on the interface).
In Experiment 2, considering the resolution of the eye tracker,
instead of playing 25 images at a time, we reduced the number to
16. After the observations from the first experiment that color
images are more supportive to the retrieval task, in the second
experiment we used only color images. Hence a total of 40
sessions of face retrieval tasks were done by each of the 9 subjects.

Figure 1. Illustrating the interface: the query image shown on
the right and the candidate images displayed on the left.

2.2 Design of the Experiments

A software utility was built for the experiments. Each subject who
participated in the experiments was asked to log into the study via
an interface. After that, the subject can click a menu button to
start the experiment, then a query image is shown along with an
array of N x N candidate images, as illustrated in Figure 1. N is
either 4 or 5 in our experiment. The subject then needs to find out
which face image (i.e., the “target image”) in the array belongs to
the same person in the query image (making the selection is a
simply click of mouse, which takes little time). After that, the
subject can click a button to move on to the next query image.

3. RESULTS AND ANALYSIS

We now report the experimental results, which are presented in
three different groups: objective performance metrics (Sect. 3.1),
subjective/qualitative responses from the participants (Sect. 3.2),
and eye-tracking-based experiment and analysis (Sect. 3.3).

3.1 Objective performance metrics in Experiment 1

For each subject, we computed the hit rate and the average time
spent on dealing with different types of images, as elaborated
below. (For computing the average time spent on certain type of
images, we only use results when the query is correctly done.)

In Experiment 1, after recording the subject’s query results
(his/her response to each query image, including the time spent on
each query image), we asked each subject a set of pre-defined
questions and recorded their answers. The questionnaire contains
two parts. Part 1 is to confirm the unfamiliarity of the query image.
If a subject feels any of the query images have “familiar faces”,
then all the results related to these query images are not
considered when we evaluate his/her performance metrics.

Overall hit rate: The overall hit rate indicates that the subjects'
performances are not ideal. The hit rates fall in to a range from
57.5%~97.5%, with the average of 78.64%.
Frontal vs. off-frontal faces: The majority (8 out of 11) spent
longer time when dealing with off-frontal faces. The average time
on frontal faces is 15.2s with STD of 5.2s while it is 17.4s on offfrontal faces with STD of 5.3s. This indicates that pose is an
influential factor in face retrieval. An automated algorithm may
need to explicitly handle the pose for improved performance.

In Part 2, the following three questions are presented to each
subject after the retrieval task is completed:
Question 1: “Do you think color images give more information
than grey scale images when you do the retrieval task?”

1288

clothing style, e. race, f. age”, ranking scores are from 1-6 while
1 indicates most important and 5 indicates least important. Table
1 shows the average ranking scores of these features, lower scores
indicate higher priority. Based on these ranking scores, the
ranking of the 6 features are: facial features, skin tone, race, hair
style, age, and clothing style.

Color vs. greyscale faces: The average time spent on color image
is 16.0s with STD of 5.6s, while on greyscale image it is 17.4s
with STD of 5.5s. 6 out of 11 subjects spent shorter time on color
images. This indicates that color images are easier to retrieve than
greyscale images. We note that most existing face recognition
system actually use only greyscale images.
Male vs. female faces: The average time spent on male faces is
15.1s with STD of 5.0s, and 19.1s on female faces with STD of
6.6s. 26% more time is spent when dealing with female faces and
all the subjects except one spent longer time on female faces than
male faces. This indicates that female faces are harder to retrieve.
One possible reason is the appearance of a female may change
dramatically (e.g., due to new hair style or make-up), as illustrated
in Figure 2. This suggests that, features that are invariant to
common appearance changes in female images should be
considered for improving an automated approach.

Figure 3. Relative frequency of features mentioned by the
subjects, from left to right: hair, jaw/chin, eyebrows, mustache,
mouth, glasses, eye, face shape, skin tone, nose.

(a)

(b)

Table 1. Average ranking scores.
b
c
d
e

Feature

a

Score

3.36

3.3

2.91

2.73

5.55

3.00

f
3.45

Experiment with eye-tracking

Experiment 1 provided some holistic understanding of human
performance in retrieving unfamiliar faces, mostly in terms of
accuracy (hit rate), time spent, and self-reported important factors.
Recognizing the fact that a subject may employ some unconscious
process that is not realized by the subject him/herself, we
employed eye-tracking in Experiment 2 in order to learn the
search behavior of the subjects in retrieving the images. Such
experiment may also provide additional data for verifying the selfreported behaviors. In this experiment, an eye tracker tracks the
eye fixation and gaze points during the retrieving actions of a
subject. The recorded data include: eye gaze points, eye fixation
points (duration >=0.1s), duration of each gaze or fixation points,
and special events (when the subject advances to the next query).

(c)
(d)
Figure 2. Drastic appearance changes in female face images. (a)
and (b) belong to the same person, so as (c) and (d).
Hit rate vs. time spent: As shown above, the hit rate is only
78.64%. Intuitively we may think that the more time people spend
on looking at the images, the higher chance that they can select
the correct target image. However, interestingly, plotting the
average time spent vs the hit rate reveals a wide scatter without
any obvious positive correlation among these two metrics.

3.2 Subjective responses

The raw data from the eye tracker is the gaze point positions. The
gaze points are sampled at a frequency of 100HZ which is dense
enough to record accurate eye movement. This high sampling rate
also caused crowded gaze points at some positions and thus care
needs to be taken to make use of the noisy data. So based on the
raw data, fixation points are generated in the following way: in a
small radius if the difference of the time stamp of the first gaze
point and the last point is more than 0.1s, then the mean position
of these points is recorded as a fixation point.

The subjective responses to the questionnaire reveal additional
information that either confirms or deepens the understanding of
what observed in Sect. 3.1. For Question 1: “Do you think color
images give more information than grey scale images when you
do retrieval?”, 10 out of 11 subjects responded “yes”, only one
responded “no” with the explanation that he only looked at facial
features. This correlates well with the results of Sect. 3.1.
In asking Question 2: “What features did you use or rely on when
retrieving a face?” we intended to learn specific features that
human uses on retrieval. Figure 3 depicts all features mentioned
by subjects and the corresponding number of times they are
mentioned. Synonyms are merged. Frequently-mentioned features
are: hair, mustache, eye glasses, face shape, nose and mouth. This
open question gives us some important hints. For example, most
existing automatic face retrieval algorithms do not use hair or
mustache as one of the features, while more attention is given to
facial components like eyes, nose, and mouths.

We use both gaze points and fixation points for objective analysis.
The candidates that received the least numbers of gaze points are
those that were quickly discounted by the subjects. By analyzing
such images, we came to the observation that hair color, skin tone
and eye glasses contributed to such quick elimination. More
specifically, for the 40 sessions, choosing the 5 images with the
least gaze points, a total of 5*40=200 images were identified (100
from female candidates and 100 from male candidates). Among
these, hair color is the most distinguishing: 64% (female) and
61% (male) rejected candidates having a different hair color than
the query image. For female sessions, skin tone is also distinctive:

For Question 3: “Rank the importance of the following features in
your retrieval: a. hair style, b. skin tone, c. facial features, d.

1289

63% of the candidates have a different skin tone than the query.
We also observed 22% of the female candidates are persons
wearing eye glasses while the query image does not (or vice versa).
For male sessions, the percentages are 41% for the skin tone
difference and 45% for with/without eye glasses, respectively.

what are important in human’s retrieving action, providing
insights into new ways of improving automated approaches.

We can plot the “heat map”, which shows where the subject
looked at during performing the task. Figure 4 is a sample heat
map for one session, in which we observe that the searching is
focused on only a few candidates. The subjects also performed
intensive comparisons between query image and candidates. For
all the 40 query images, we got 719 fixation points in total, among
which 107 points are located around the eyes, 226 around the
nose and 386 around the mouth. Surprisingly, we see that eyes are
less important than nose and mouth in this searching stage.
Figure 5. A difficult case: the busy gaze trajectories suggest
intensive comparisons between the query image and candidates
were performed. Easy cases have much simpler trajectories.
Acknowledgement: The work was supported in part by a grant
(#1135616) from the National Science Foundation. Any opinions
expressed in this material are those of the authors and do not
necessarily reflect the views of NSF.

6. REFERENCES
[1] Unsang Park; Jain, A.K. 2010. Face Matching and Retrieval
Using Soft Biometrics, Information Forensics and Security,
IEEE Transactions on , vol.5, no.3, pp.406-415
[2] B. Hu; Bijuan Weng; Shumin Ruan. 2012. Face recognition
and retrieval based on feedback log information, Computer
Science and Automation Engineering (CSAE), IEEE
International Conference on , vol.1, no., pp.578-584, 25-27
[3] W. Dai; Y. Fang; B. Hu, Feature selection in interactive face
retrieval, Image and Signal Processing (CISP), 2011 4th
International Congress on , vol.3, no., pp.1358-1362
[4] Hancock, P. J.B.; Bruce, V.; Burton, A. M. 2000.
Recognition of unfamiliar faces, Trends in cognitive sciences,
volume 4 issue 9 pp.330 – 337.
[5] Gary B. Huang; Manu Ramesh; Tamara Berg; and Erik
Learned-Miller. 2007. Labeled Faces in the Wild: A
Database for Studying Face Recognition in Unconstrained
Environments, Univ. Mass., Amherst, Tech. Report 07-49
[6] Adler, A.; Schuckers, M.E. 2007. Comparing Human and
Automatic Face Recognition Performance, Systems, Man,
and Cybernetics, Part B: Cybernetics, IEEE Tran. on,
vol.37, no.5, pp.1248-1255
[7] Tamara L. Berg; Alexander C. Berg; Jaety Edwards; David A.
Forsyth. 2004. Whos's in the Picture, Neural Information
Processing Systems (NIPS)
[8] Z. Wu, Q. Ke, J. Sun, H. Shum, Scalable Face Image
Retrieval with Identity-Based Quantization and
Multireference Reranking, Pattern Analysis and Machine
Intelligence, IEEE Trans. on, vol.33, no.10, pp.1991-2001
[9] C. L. Herlitz. 2002. Sex differences in face recognition—
Women’s faces make the difference, Brain Cogn., vol. 50, no.
1, pp. 121–128
[10] N. Furl, A. J. O’Toole, and P. J. Phillips. 2002. Face
recognition algorithms as models of the other race effect,
Cogn. Sci., vol. 96, pp. 1–19
[11] C.D. Manning, P. Raghavan, and H. Schu¨ tze. 2008.
Relevance Feedback and Query Expansion, Introduction to
Information Retrieval, pp. 177-194, Cambridge Univ. Press.

Figure 4. Sample heat map of one query image. The fixations of
one subject are visualized: red/yellow/green regions respectively
receive many/fewer/least fixations.
Considering both the hit rate and the average time spent on a
particular session, we may further define “easy cases” and
“difficult cases” by thresholding these metrics. With this, we also
looked into the "comparison behavior" of a subject. This behavior
is defined as sweeping between the query and candidate images.
We found that there is a correlation between the number of
sweeps in one session and the difficulty level of the session. The
average number of sweeps performed in one session is 56.72. For
easy/difficult cases, the average numbers of sweeps are 33.6 and
98 respectively. It is obvious that in the difficult cases, the
subjects needed to do more localized checking more often. Figure
5 shows a sample from the difficult cases.
Finally, we noticed that not all the candidates in one session
received equal comparisons, and the variation can be dramatic.
This suggests that most candidates are eliminated quickly by some
semi-global features; then intensive comparison is done on a few
candidates using more localized feature, and in this stage the nose
and mouth are dominant factors that influence the final decision.
Such a finding may lead to a staged design of new automated
retrieval approaches: semi-global-feature-based elimination
followed by localized-feature-directed refined comparison.

4. CONCLUSION

We carried out experiments to understand human performance in
retrieving unfamiliar faces. In addition to measuring the hit rate
while documenting other performance metrics, we employed eyetracking to capture the search patterns of the subjects. The results,
while confirming some intuitions, also revealed new clues as to

1290

Simulation of Diabetic Retinopathy Neovascularization in
Color Digital Fundus Images
Xinyu Xu1, Baoxin Li1, Jose F. Florez2,3,4, and Helen K. Li2,3,4
1

Dept. of Computer Science and Engineering, Arizona State University, Tempe, AZ, U.S.A
Dept. of Ophthalmology and Visual Sciences, The University of Texas Medical Branch,
Galveston, TX, U.S.A
3
School of Health Information Sciences, University of Texas Health Science Center,
Houston, TX, U.S.A
4
Universidad De Antioquia, Medellin, Colombia, U.S.A

2

Abstract. Diabetic retinopathy (DR) has been identified as a leading cause of
blindness. One type of lesion, neovascularization (NV), indicates that the
disease has entered a vision-threatening phase. Early detection of NV is thus
clinically significant. Efforts have been devoted to use computer-aided analyses
of digital retina images to detect DR. However, developing reliable NV
detection algorithms requires large numbers of digital retinal images to test and
refine approaches. Computer simulation of NV offers the potential of
developing lesion detection algorithms without the need for large image
databases of real pathology. In this paper, we propose a systematic approach to
simulating NV. Specifically, we propose two algorithms based on fractal
models to simulate the main structure of NV and an adaptive color generation
method to assign photorealistic pixel values to the structure. Moreover, we
develop an interactive system that provides instant visual feedback to support
NV simulation guided by an ophthalmologist. This enables us to combine the
low level algorithms with high-level human feedback to simulate realistic
lesions. Experiments suggest that our method is able to produce simulated NVs
that are indistinguishable from real lesions.

1 Introduction
Diabetic retinopathy (DR) has been identified as a leading cause of blindness [1].
Studies have shown that early detection and treatment significantly reduces the risk of
severe vision loss [2]. Diabetic retinopathy evaluation programs typically rely on
experts to review large numbers of retinal images from diabetic patients. Researches
are underway to use computers to assist or automatically detect/diagnose DR by
analyzing color digital fundus images [3-7]. While computer-assisted approaches
offer the possibility of more cost-effective or timelier evaluation, they also present
new challenges. For example, how algorithms are affected by differences in digital
imaging factors such as resolution, contrast or color is not well understood. Testing
detection/diagnostic algorithms also requires what may be prohibitively large image
databases of real DR lesions. Computer-generated lesions offer the possibility of
testing DR detection/diagnostic algorithms on large simulated datasets and tuning
approaches to account for differences in digital imaging factors.
G. Bebis et al. (Eds.): ISVC 2006, LNCS 4291, pp. 421 – 433, 2006.
© Springer-Verlag Berlin Heidelberg 2006

422

X. Xu et al.

In this paper, we develop techniques to simulate neovascularization (NV), a
common DR lesion that signifies the disease has reached a vision-threatening phase.
NV is a growth of new blood vessels on the surface of the retina [8] (Fig. 2 left). A
review of literature indicates that little work has been done in simulating human tissue
in digital imagery. The few examples include the work done by Landini for
simulating corneal neovascularization [9] and by Hoe for liver lesion simulation [10].
In our work, we propose a systematic approach to simulating NV. Two algorithms
based on local fractal growth models are proposed to simulate the geometrical
structure of an NV lesion. A color generation method, which is adaptive to the region
to which the simulated NV is inserted, is then proposed for assigning photorealistic
pixel values to the structure. Moreover, an interactive system is developed for
providing instant visual feedback to support NV simulation guided by an
ophthalmologist. This enables us to combine the low level simulation algorithms with
high-level human feedback to generate realistic NVs. Our current experiments on
non-proliferative DR images have generated NVs that are deemed realistic by
ophthalmologists. The complete system is under deployment for ophthalmologists’
formal evaluation of its performance including the acceptance rate (see Sect. 3).

2 Proposed Method
2.1 Methodology Overview
Fractal geometry is commonly encountered in nature, e.g., branching patterns in trees,
blood vessels patterns and shape of tumors studied in pathology. Fractals are based on
the concept of self-similarity of spatial geometrical patterns despite a change in scale
or magnification so that small parts of the pattern exhibit the pattern’s overall
structure [17]. The concept of fractals as mathematical entities to describe complex
natural branching patterns was first considered by Mandelbrot [12]. The fractal
dimension (D), typically a non-integer value between 1 and 2, describes how
thoroughly the pattern fills two-dimensional spaces [11].
The applications of fractals to biology and medicine cover a wide range of scale:
molecules, cells, tissues, and organs [18]. Masters and Platt [19] and Family et al. [20]
were the first to introduce the use of fractal analysis to retinal vascular branching
patterns. One common goal of these studies is to determine the fractal dimension of
those structures and then to use this number as an index to discriminate the class of
normal structures from abnormal and pathological structures [13].
Inspired by the work on fractal-based analysis, we propose to do fractal-based
synthesis, NV simulation. Our objective is to create by simulation NVs that conform
to bio-physical growth mechanism of real NVs and are consistent with the observed
appearance of real NVs. This is a challenging task. Some of the key challenges that
affect the morphology and appearance of NV and our corresponding strategies are
discussed in the following:
1. NV could present various patterns, which are mostly like random winding vessels
and some may be like flowers, sea coral or other complicated structures. While
there is no proven optimal way of simulating these patterns, inspired by the success
of fractal-based analysis of retina vessels, we employ three fractal models to create

Simulation of DR Neovascularization in Color Digital Fundus Images

423

the structure of NV: random walk [21] (self-avoiding and self-intersecting),
invasion percolation (IP) [14] and spreading percolation (SP) [14]. The random
walk models are chosen with the consideration that the NVs are random in nature
and thus no models with strong structural constraint should be used. We will
present in detail the random walk fractal model.
2. The caliber of NVs is much smaller than the natural retinal veins from which NVs
originate. In our simulation, the caliber of NV is a function of the width of the
retinal vein branch.
3. The colors of NV in most cases are reddish varying with different degree of
saturation and brightness. We generate colors by sampling the empirical
hue/saturation/value (HSV) color density defined by local normal vessel segments.
4. NVs are usually located at the connection of branches of natural retinal veins or at
arterial-venous crossing sites. In our simulation, an ophthalmologist specifies
optimal locations where simulated NV should be inserted using a graphic user
interface. This interface also allows other parameters, such as coverage and
complexity of the simulated NV, to be configured.
The key steps of the approaches are illustrated in Fig. 1, with the details of the
algorithms presented in subsequent sections.
Extract normal vessel segment
Compute HSV joint distribution

1. Extract
normal vessel

Generate NV structure
Path smoothing

2. Generate
fractal NV

Caliber enlargement
Generate color
Blend NV with background

3. Generate
Color
4. Blend with
background

Fig. 1. DR NV simulation architecture

2.2 NV Shape Generation
Generate Binary NV Structure Using Fractals. Our first algorithm for NV
structure simulation is based on the following observation: NVs are thin, long,
connected, winding vessels with variable degrees of random curvature. To
simulate these vessels, we designed an algorithm whose core is a self-intersecting
random walk.
ALGORITHM 1:
1. Create a square lattice with side length 2L+1 and spacing 1, initialize the center of
the lattice, O, to be occupied by a particle.

424

X. Xu et al.

2. The first position the particle jumps to (x1) is randomly chosen on a circle with
radius r and centered at O. Suppose the angle at position x1 is θ1, then θ1=
2*pi*rand.
Loop for t =2:TIMES
3. Along the direction pointed out by θt-1, we create a circular sector centered at xt-1
with the central angle 2ε and radius r (ε denotes the value of half central angle). The
position of the walking particle at time t, xt, is randomly chosen on this circular
sector given by

θ t = 2ε i rand + (θ t −1 − ε )
x th = x th−1 + r icos(θ t )
x =x
v
t

v
t −1

.

(1)

+ r isin(θ t )

where x th and x tv denote the respective horizontal and vertical coordinate of the
walking particle at time t.
4. Set lattice position xt to 1indicating that this site has been occupied.
5. Record the path and the order.
End loop

ε
θ
1

Fig. 2. Left: A DR image with natural NV (thin vague vessels). Right: NV structure generation
using ALGORITHM 1. The red (dark grey) circle indicates the seed particle at the center of
lattice. The green (light grey) circles denote the position of the walking particle at different
times. The solid line linking the circles illustrates the growth path of the particle. The dotted
lines illustrate the restriction of the growth direction within [θt-1+ε θt-1+ε].

Fig. 2(right) graphically illustrates ALGORITHM 1. The entire process is a
Markovian self-intersecting random walk [21] since the position at time t only
depends on the position at time t-1. The path may intersect with itself, which entails
some complex patterns. The curvature of the path is controlled by the central angle 2ε
of the circular sector. The greater the central angle, the more likely the path is
convoluted. Because the pixels of real NV are connected and continuous, the jump
distance at each time instance, r, is set to 1 to prevent holes or discontinuity in the
generated vessel. In the algorithm, the parameter TIMES and the lattice side length L
control how much area the NV will cover. Two NV paths generated by this algorithm
are shown in Fig. 3 (a).

Simulation of DR Neovascularization in Color Digital Fundus Images

425

Our second algorithm is based on the observation that some NVs appear to have the
following pattern: (1) from a single seed point located on retinal veins grows one or
multiple vessels (we call the major vessel the first level); (2) from major vessels grow
one or multiple ramifications that could intersect with each other (we call these
ramifications the second level; there could be additional levels); (3) each vessel
branch could be a simple curve or very convoluted.
To generate NVs with the above characteristics, we proposed the following selfavoiding random walk algorithms.
ALGORITHM 2:
1. Initialize the center of a lattice (side length 2L+1) to be occupied by a seed particle.
2. The 2-D area surrounding the seed particle is divided into 8 sectors, each
representing a possible area the walking particle will grow into (from (π/4)*(j-1) to
(π/4)*j, j = 1, 2, 3,…8).
3. An 8 dimensional probability vector p={p1, p2, p3, …, p8} is generated where pi
denotes the probability of growing into sector i. p is calculated to have one
dominant entry pj which is larger than other probability members so that the particle
will more likely grow into area j.
Loop for t = 1 : TIMES
4. By sampling the cumulative probability of vector p, an angle θt is calculated to
determine the position of the walking particle at time t. θt is uniformly distributed
within [π*(j-1)/4 π*j/4].
Direction (t) = the index of pie by sampling p .

(2)

θt = ( π 4)irand + (π 4)iDirection(t );

5. The position of the walking particle at time t, xt, is given by:
x th = x th− 1 + r i c o s ( θ t )
x tv = x tv − 1 + r i s i n ( θ t )

.

(3)

End loop

(a

(a)

(a)

(b)

(b)

(b)

Fig. 3. (a) NVs generated by self-intersecting random walk (ALGORITHM 1). (b) NVs
generated by multi-level self-avoiding random walk (ALGORITHM 2).

Since one entry of the probability vector p, pj, is larger than other entries, the vessel
usually grows toward one dominant direction with certain local randomness and

426

X. Xu et al.

intersections along the path. Therefore globally this single vessel branch is a selfavoiding random walk. Note that the above steps can only generate one branch. To
produce NV with multiple levels and branches that intersect with one another, the
algorithm is recursively called with a different p for different branches such that the
orientations of branches are randomly distributed. Examples of NV structures
generated by recursively executing ALGORITHM 2 are illustrated in Fig. 3 (b).
Path Smoothing. The vessels in Fig. 3 (b) may be too jagged, and thus a smoothing
filter is applied to make the path more natural. This is achieved by a moving average
along individual paths. Fig. 4 shows results corresponding to Fig. 3 (b) after path
smoothing.

Fig. 4. Random walk fractals after path smoothing

Fig. 5. Binary NV structure after caliber enlargement

Caliber Enlargement. The path generated from the above algorithms may be too thin
as the width of path is only one-pixel. To obtain vessels of different calibers, we
perform caliber enlargement. The first step is to decide the enlargement scale for each
pixel. In the real NV, we found that the wider the normal retina vessel where NV
sprouts out, the wider the newly grown NV vessel. Therefore, the enlargement scale is
determined by measuring the width of normal vessels where simulated NV will be
inserted. Based on the measured width of normal vessel, empirical rules are set to
define the enlargement scale of simulated NV vessels: if the width of normal vessel is
larger than 10 pixels, the enlargement scale is set to 2 or 3; if the width of normal
vessel is larger than 5 but less than 10, the enlargement scale is set to 1 or 2; in other
cases, the enlargement scale is 1. Only three scales are used as real NV vessels are
usually very thin. Next, for each pixel on a branch, a “disk” is created with the center

Simulation of DR Neovascularization in Color Digital Fundus Images

427

at the current pixel and a radius equal to the enlargement scale wi. The values of all
pixels within this disk are set to 1, denoting that the pixels have been added to the
original NV structure. The disk is then added to the original binary NV using logic
operation OR. Using a disk gives us smooth vessels. Fig. 5 illustrates the results of
caliber enlargement.
2.3 Photorealistic Color Generation
The binary NV structure such as those in Fig. 5 needs to be ‘painted’ with appropriate
colors based on the context of the background. As colors of real NV are similar to the
colors of nearby normal vessels, the colors of the simulated NV are generated
adaptively by sampling color densities of nearby normal vessel segments.

Fig. 6. NV with color overlaid on the background image

Extract Normal Vessel. The extraction algorithm starts from a pixel on a normal
vessel where the simulated NV is to be inserted. This pixel may be randomly selected
after automatic vessel detection, or more preferably, specified by a user (see Section
3). Then through breadth-first search we find all the pixels with colors similar to the
chosen pixel within a square region centered at the chosen pixel. This yields normal
vessel pixels in the local region. After extraction, the width of normal vessels is
calculated by scanning the width in four directions (horizontal, vertical, two
diagonals) and setting the width to the minimum of the four values. A joint histogram
of HSV is computed from the detected normal vessel pixels and used as the desired
color density for the specified region. Because the colors of most extracted vessel
pixels are reddish, this joint density can be further restricted to a sub-region of the
original color space.
Generate NV Color. The color of simulated NV pixels is generated one by one by
sampling the HSV color distribution of normal vessel segment. Theoretically, this
may create inhomogeneity of color on the NV since we do not consider the spatial
correlation of the pixels. This is not a practical concern as the density is highly peaked
and thus colors are mostly similar. Fig. 6 shows the appearance of simulated NVs
with sampled colors.
Blending NV with Background. Fig. 6 shows that the simulated NVs are too salient
to be natural due to the clear-cut boundaries. True NVs have the following important
appearance characteristics which have not been considered yet: real NVs look well

428

X. Xu et al.

“blended” with surroundings; real NV may become progressively invisible as the
vessel extends; also, the color of central vessel pixels are typically more visible than
that of the NV boundary pixels; color is not uniform along the branch.
We developed the following strategies to simulate these characteristics:
1. The color of NV pixels (for H, S and V) belonging to the first 70% of one branch is
computed as the weighted average of sampling color of this pixel and the color of
corresponding pixel underneath, as in Eq. 4, where α is a weight. The larger the
weight, the more visible the simulated color with respect to the background color.
NV _ Color = α iSampling _ Color + (1 − α )* BG _ Color

(4)

2. To make the color of the central vessel pixels more visible than that of the NV
boundary pixels, we create a bending matrix whose entry corresponds to the weight
α that controls how much the sampled NV color contributes to the final color. And
more importantly the weights of interior matrix elements decrease linearly to the
weights of periphery elements. Note that the color of those background pixels not
located on the binary NV structure should not be modified by the blending matrix,
so we make the size of the blending matrix be equal to that of the enlargement disk
discussed in caliber enlargement. A blending matrix with radius 2 is illustrated
below. The weights decrease from inner (maxbld =0.6) to outer (minbld =0.2).
0.2
⎛ 0
⎜
0.2
0.4667
⎜
⎜ 0.3333
0.6
⎜
0.4667
⎜ 0.2
⎜ 0
0.2
⎝

0.3333

0.2

0.6
0.6

0.4667
0.6

0.6

0.4667

0.3333

0.2

0 ⎞
⎟
0.2 ⎟
0.3333 ⎟
⎟
0.2 ⎟
0 ⎟⎠

.

(5)

3. For the last 30% pixels on a branch, a blending matrix is also created using the
above method but with varying minbld and maxbld for different pixels: they both
decrease linearly to 0 as the path reaches its end.
This approach leads to results shown in Fig. 7 where considerable improvement
comparing to Fig. 6 can be observed.

Fig. 7. Neovascularization after color blending

3 An Interactive GUI
If positions to insert simulated NVs were determined by randomly selecting pixels
from normal vessels which are automatically detected (e.g., by methods of [15, 16]),

Simulation of DR Neovascularization in Color Digital Fundus Images

429

our simulation algorithms would have formed a fully automatic system. However,
fully automatic approaches have two drawbacks: (1) Point randomly picked on the
detected vessel may not be the location preferred by ophthalmologists; (2) More
severely, existing vessel detection approaches are not perfect, possibly entailing
clinically meaningless results if the point is located on false detections. To address
these issues, we propose an interactive approach: normal lesion detection is initialized
by an ophthalmologist selecting a point on the vessel (as in Extract Normal Vessel).
This greatly improves the performance of vessel detection. Moreover, the approach
provides instant visual feedback to allow immediately rejection of the unrealistic
simulations. Implementing the approach as an interactive GUI also enables the user to
adaptively configure many of the algorithmic parameters if the default setting does
not generate satisfactory results. A screen shot of the current GUI is shown in Fig. 8.

Fig. 8. An interactive GUI for NV simulations

There are three parameter panels in the GUI: Complexity, Visibility and Coverage.
Complexity ranges from 1 to 5. This parameter allows a user to create simple to
complex NVs. NV may be sinuous and random in shape (Complexity = 5) or simple
(Complexity = 1).
Visibility: Two parameters, minimum blending factor and maximum blending factor,
are associated with visibility. Minimum and maximum blending factors correspond
respectively to the minimum and maximum value in the blending matrix.
Coverage: Two parameters, length of vessel branch and size of NV square lattice, are
associated with NV coverage. The length of the NV branch is measured in terms of
the number of random walk steps. The size of NV square lattice gives the area that an
NV covers.
In a typical run, once all the parameters are set, the user clicks the button “Run
Experiments” to start NV simulation for a set of input images. The system reads one
background image and displays it on the screen. Then the user selects one or more

430

X. Xu et al.

locations to add NV. Next the system performs the NV simulation algorithm listed in
Fig. 1 and displays simulated NVs on the screen. At this time, the user can examine
the simulated NV to see if it is realistic. If unsatisfactory, the user can adjust the
blending slider to improve the blending effect. The image with satisfying simulated
NVs is stored by clicking the “Save Image” button. This can be repeated for different
background images.
It must be noted that, although the system is interactive, a large portion of the
process is automatic, and ideally, the user will only need to select the points without
further adjusting. Thus a performance factor of the system is the acceptance rate, i.e.,
the percentage of the simulated NVs that do not need further adjustment.

4 Experiments and Evaluation
We have tested the system on natural human retina images. Since NV occurs during
the stage of proliferative DR, some non-proliferative DR lesions such as
microaneurysm or hemorrhages are usually present at that time. So images with nonproliferative lesions are selected from a database to serve as the background images
into which the simulated NVs are inserted. Some simulated NVs are shown in Fig. 9,
where background images with different pigments are used, illustrating that the
proposed approach is able to adapt to the appearance of background images. In Fig.
10, images with simulated NVs are intentionally mixed with real NV images,
illustrating that the system is able to generate NVs that are indistinguishable from the
real ones. Since the image of the entire retina is too large (1024*768), only the area of
retina with simulated NV is shown here.

Fig. 9. Examples of photorealistic simulated NV. Here, the width of the simulated vessel varies
along the path. Color also presents various degree of saturation.

Simulation of DR Neovascularization in Color Digital Fundus Images

(a)

(b)

(d)

(e)

431

(c)

(f)

Fig. 10. Images with either simulated or real NVs (but not both). Would you be able to tell the
difference? See1 for answer.

The complete system is under deployment for ophthalmologists’ formal evaluation
of its performance, including the acceptance rate, in a clinic setting. The initial results
suggest that this is a promising method.

5 Conclusion and Future Work
In this paper we present a diabetic retinopathy neovascularization simulation system.
The shape of NV is generated by self-avoiding random walk and self-intersecting
random walk (spreading percolation and invasion percolation have also been tested,
but not discussed here). The color of NV is generated by sampling the color
distribution of normal retina vessel. In addition we have developed an interactive
system to provide user feedback for optimal performance of the automated
algorithms. Experiments on images with non-proliferative DR lesions show that the
system is able to simulate NVs indistinguishable to the real ones.
There exist known limitations in the current system, i.e., it cannot create some
particular NV patterns such as flower-like NVs, which will be addressed in our future
work. Another challenging problem is, currently the performance of the system relies
on subjective judgment of ophthalmologists, and thus it is difficult to obtain statistics
from a large pool of ophthalmologists to study possible bias/variance of the subjects.
1

In Fig. 10, (a), (d) and (e) are real NVs, others are simulated.

432

X. Xu et al.

It is likely that the fractal dimension might be the quantitative index of measuring the
similarity between simulated NVs and real NVs, which is one possible working
direction for our future research.

Acknowledgment
This project was supported by grant #1-2002-212 from the Juvenile Diabetes
Research Foundation International.

References
1. Javitt, J. C., Aiello, L. P., Chiang, Y., Ferris, F. L., Canner, J. K. S., Greenfield: Preventive
eye care in people with diabetes is cost saving to the federal government. Diabetes Care.
17(8): 909-917 (1994)
2. ETDRS Research Group: Early photocoagulation for diabetic retinopathy: Early treatment
diabetic retinopathy study report number 9. Ophthalmology. (1998) 766–785
3. Lee, S., et al: Comparison of diagnosis of early retinal lesions of diabetic retinopathy
between a computer and human experts. Arch Ophthalmol. 119: 509–515 (2001)
4. Usher, D., et al: Automated detection of diabetic retinopathy in digital retinal images: a
tool for diabetic retinopathy screening. Diabet Med. Jan. 21(1):84-90 (2004)
5. Hipwell, J.H., et al: Automated detection of microaneurysms in digital red-free
photographs: a diabetic retinopathy screening tool. Diabet Med. 17(8):588-94 (2000)
6. Sinthanayothin, C., et al: Automated detection of diabetic retinopathy on digital fundus
images. Diabet. Med. 19(2):105-12 (2002)
7. Teng, T., et al: Progress towards automated diabetic ocular screening: a review of image
analysis and intelligent systems for diabetic retinopathy. Medical & Biological
Engineering & Computing. 40(1):2-13 (2002)
8. Early Treatment Diabetic Retinopathy Study Research Group: Grading diabetic
retinopathy from stereoscopic color fundus photographs: An extension of the modified
Airlie House classification. ETDRS Report Number 10. Ophthalmology 98: 786-806
(1991)
9. Landini, G., Misson, G.: Simulation of corneal neovascularization by inverted diffusion
limited aggregation. Invest Ophthalmol Vis Sci. 34(5):1872-5 (1993)
10. Hoe, C.L., Samei, E., Frush, D.P., Delong, D.M.: Simulation of liver lesions for pediatric
CT. Radiology. 238(2):699-705 (2006)
11. Cross, S. S.: Fractal in pathology. Journal of Pathology. 182:1–8 (1997)
12. Mandelbrot, B.: The Fractal Geometry of Nature. San Francisco: WHFreeman. (1982) 460
13. Masters, B. R.: Fractal analysis of the vascular tree in the human retina. Annu. Rev.
Biomed. Eng. 6:427–52 (2004)
14. Vicsek, T.: Fractal Growth Phenomena. World Scientific Pub Co Inc, 2nd edn. (1992) 105111 and 111-114
15. Can, A., Stewart, C.V., Roysam, B., and Tanenbaum, H.L.: A Feature-based, robust,
hierarchical algorithm for registering pairs of images of the curved human retina. IEEE
Transactions on PAMI. 24:347-364 (2002)
16. Lalibert, F., Gagnon, L., and Sheng, Y.: Registration and Fusion of Retinal Images-An
Evaluation Study. IEEE Transactions on Medical Imaging. 22:661–673 (2003)

Simulation of DR Neovascularization in Color Digital Fundus Images

433

17. Patton, N., Aslam, T.M., et al: Retinal image analysis: concepts, applications and potential.
Progress in Retinal and Eye Research. 25(1):99-127 (2006)
18. Stanley, H.E., Amaral, L.A.N., Buldyrev, S.V., Goldberger, A.L., Havlin, S., et al: Scaling
and universality in living systems. Fractals. 4:427-51 (1996)
19. Masters, B., Platt, D.: Development of human retinal vessels: a fractal analysis. Invest.
Ophthalmol. Vis. Sci. 30 (Suppl.):391 (1989)
20. Family, F., Masters, B., Platt, D.: Fractal pattern formation in human retinal vessels.
Physica D 38:98-103 (1989)
21. Vicsek, T.: Fractal Growth Phenomena. World Scientific Pub Co Inc, 2nd edn. (1992)
119-135

Computer Vision and Image Understanding 115 (2011) 375–389

Contents lists available at ScienceDirect

Computer Vision and Image Understanding
journal homepage: www.elsevier.com/locate/cviu

Multifactor feature extraction for human movement recognition
Bo Peng a,b, Gang Qian a,b,⇑, Yunqian Ma d, Baoxin Li c
a

School of Arts, Media and Engineering, Arizona State University Tempe, AZ 85287, USA
School of Electrical, Computer and Energy Engineering, Arizona State University Tempe, AZ 85287, USA
c
School of Computing, Informatics, and Decision Systems Engineering, Arizona State University Tempe, AZ 85287, USA
d
Honeywell Labs, 1985 Douglas Drive North, Golden Valley, MN 55422, USA
b

a r t i c l e

i n f o

Article history:
Received 1 March 2010
Accepted 1 November 2010
Available online 12 November 2010
Keywords:
Feature extraction
View-invariance
Multifactor analysis
Pose recognition
Gesture recognition

a b s t r a c t
In this paper, we systematically examine multifactor approaches to human pose feature extraction and
compare their performances in movement recognition. Two multifactor approaches have been used in
pose feature extraction, including a deterministic multilinear approach and a probabilistic approach
based on multifactor Gaussian process. These two approaches are compared in terms of the degrees of
view-invariance, reconstruction capacity, performances in human pose and gesture recognition using real
movement datasets. The experimental results show that the deterministic multilinear approach outperforms the probabilistic-based approach in movement recognition.
Ó 2010 Elsevier Inc. All rights reserved.

1. Introduction
3D human movement sensing and analysis research, including
articulated motion tracking and movement (e.g., pose, gesture
and action) recognition, enables machines to read and understand
human movement, which is critical to developing embodied
human–machine interaction systems and allows users to communicate with computers through actions and gestures in a much
more intuitive and natural manner than traditional human–
computer interfaces through mouse clicks and keystrokes. Embodied human–machine interfaces have found many important
applications, including immersive virtual reality systems such as
CAVE [1], industrial control [2], embodied gestural control of
media [3], healthcare [4], automatic sign language analysis and
interpretation [5–8], mediated and embodied learning [9], computer games [10], human–robot interaction [11] and interactive
dance works [12–15]. The sensing and analysis of 3D human
motion is also an enabling component of intelligent systems that
need to understand human actions or activities (e.g., video-based
surveillance, motion-based expertise analysis as in sports, etc.).
Due to its nonintrusive nature, in the past decade video-based
3D human movement sensing and analysis has received signiﬁcant
attention, evidenced by numerous quality research papers in
prestigious computer vision and pattern analysis journals and
⇑ Corresponding author at: School of Arts, Media and Engineering, Arizona State
University Tempe, AZ 85287, USA. Fax: +1 480 965 0961.
E-mail addresses: bo.peng@asu.edu (B. Peng), gang.qian@asu.edu (G. Qian),
Yunqian.Ma@honeywell.com (Y. Ma), baoxin.li@asu.edu (B. Li).
1077-3142/$ - see front matter Ó 2010 Elsevier Inc. All rights reserved.
doi:10.1016/j.cviu.2010.11.001

conferences. For example, vision-based pose recognition has been
extensively studied in the literature [16]. Numerous methods exist
for recognizing hand [17–19], arm [20], and full-body [21–23]
poses. Existing methods can also be categorized according to the
number of cameras used, i.e., single-view [18,21] versus multiview [23], and types of features extracted e.g., 2D silhouette
[21,22], and 3D volumetric reconstruction [23]. Likewise, numerous video-based methods have been developed for hand [24–26]
arm [27,28] and full-body [29–32] gesture recognition. Recent literature surveys on gesture recognition can be found in [33]. According
to the system methodology, video-based gesture recognition systems can be classiﬁed as kinematic-based [10,25,29,34–37] and
template-based approaches [28,30–32,38–40].
In practice, it is often necessary for a 3D human movement sensing and analysis system to have a high degree of robustness with
respect to adversary or uncooperative acquisition conditions including the change of camera view-angles. For example, view-invariance
is important for video-based gesture recognition in many HCI applications. Many existing movement recognition methods, especially
the monocular approaches, are view-dependent [41,42,8,43–45],
i.e., assuming that the relative torso orientation with respect to the
cameras is known. While this may be a valid assumption in some
scenarios, such as automatic sign language interpretation, having
to know the body orientation with respect to the camera presents
an undesirable constraint that hampers the ﬂexibility, and sometimes, the usability of an HCI system in many applications such as
interactive dance [15] and embodied learning [46]. In these applications, a truly practical solution needs to be able to recognize a gesture from any view-point so that the subject can freely move and

376

B. Peng et al. / Computer Vision and Image Understanding 115 (2011) 375–389

orient in the space. Therefore, the reliable extraction of invariant human pose descriptors from video has become one of the most fundamental research problems in video-based 3D human movement
sensing and analysis. In this paper, we systematically examine two
view-point invariant pose feature extraction methods based on multifactor analysis and compare these two feature extraction methods
in terms of the quality of the extracted features, and their performances in human pose and gesture recognition.
The common practice to extract view-invariant feature is to ﬁrst
represent the visual hull data or its transformations (e.g., FFT or 3D
shape context) in a body-centered spherical or cylindrical coordinate system and then suppress the angular dimension to achieve
some level of view-invariance. Angular suppression inevitably confuses different poses in some cases and introduces ambiguity in
pose and gesture recognition. For example, two poses in the anatomically neutral position with the right arm fully extended to
the front and to the right respectively will result in the same feature using the angular suppression approach. Therefore, robust
extraction of pose descriptors that are invariant to view changes
and at the same time can preserve differences caused by actual
body conﬁguration is a practical challenge the existing work cannot solve. In addition, such view-invariant features are designed
for pose and gesture recognition, and it remains yet another challenge to establish invariant pose descriptor for 3D articulated body
tracking. To summarize, reliable invariant pose descriptor extraction for 3D human movement sensing and analysis is a largely
underdeveloped research area with a lot of open problems.
In the below we brieﬂy review existing approaches to extracting
view-invariant pose features from 3D visual hull data, which is the
dominant method in the literature. In such approaches, to extract
view-invariant features, the volumetric data is ﬁrst transformed
into certain alternative representations such as the 3D shape context (e.g., in [27,47]) and the 3D motion history volume (MHV)
(e.g., in [30]). Data points in these alternative 3D representations
can be indexed in a body-centered cylindrical coordinate system
using (h, r, h) coordinates, which respectively correspond to the
height, radius and angular location of the data point. The h-axis
of the body-centered coordinate system coincides with the vertical central axis of the subject. To further obtain view-invariant
features, the angular dimension h is suppressed in the feature
extraction process so that the ﬁnal extracted feature is independent of the data point distribution in h. To achieve this goal, in
[27,30,47], data points are ﬁrst grouped into rings centered at
and orthogonal to the h-axis so that data points on the same ring
correspond to different h, while sharing the same h and r. Then a
ring-based feature is extracted from each ring and these ringbased features of all the rings constitute the ﬁnal h-independent
feature vector of the input volumetric data. A number of methods
have been used to obtain such ring-based features. In the case of
3D shape context [27,47], the sum of the bin values on a ring is
taken as the corresponding ring-based feature. Similarly, in the
case of 3D MHV [30], the Fourier transform of the data points
along a ring is ﬁrst computed and then the sum of their Fourier
magnitudes is taken as the ring-based feature. Pose features extracted using these methods are view-invariant since the orientation of the subject no longer affects the extracted features.
However, as we mentioned earlier, suppression of the angular
dimension may cause information loss and introduce ambiguity
in gesture recognition. In [23,48], another invariant human pose
descriptor based on the 3D shape context has also been introduced and used for pose classiﬁcation [23] and action clustering
[48]. This pose descriptor is obtained as the weighted average of
a number of local 3D shape context features centered at sample
points from a reference visual hull. This pose descriptor has good
invariance property for translation and scaling, but not for view
changes.

Multifactor analysis has been a popular computational tool to
decompose ensembles of static data into perceptually independent
sources of variations. Previous successful applications include multifactor face image representation using tensorface [49], modeling
of 3D face geometry [50], texture and reﬂectance [51], and image
synthesis for tracking [52]. Multilinear analysis has also been used
to establish a generative model in a manifold learning framework
capturing the separate contributions of poses and body shapes
[53] to the image observations. To handle potential non-multilinearity, various kernelized multilinear analysis methods have also
been developed [54]. Moreover, probabilistic multifactor analysis
in the Gaussian process framework has also been proposed [55].
Multilinear analysis has also been used in extracting viewinvariant features. We have developed the Full-Body TensorPose
approach for view-invariant pose descriptor extraction [40,56–61]
from video using multilinear analysis. The observation vector in
the data tensor can be formed by using either raw image observations such as silhouettes or 3D volumetric reconstruction such as visual hulls. In both cases, the observation of a human pose is affected
by three main factors: joint angle conﬁguration of the subject (different poses), the subject’s body orientation with respect to the camera
system, and the body shape of the subject. The Full-Body TensorPose
framework is able to extract view-invariant pose descriptors using
multilinear analysis. It has been successfully applied to view-invariant static and dynamic gesture recognition [40,56–61]. Encouraging
gesture recognition results have been obtained. Using these pose
coefﬁcients as feature vectors, support vector machine (SVM) [62]
was used for static gesture recognition and the hidden Markov Model (HMM) for dynamic gesture recognition [40,56].
It is worth mentioning that the term Tensorposes has been used
in [63,64] to describe a multilinear approach to locating nose-tips
and head orientation estimation from face images. In [63,64], a
tensor model is used to characterize the appearance variations
caused by different subjects and head pose angles. To avoid confusion, we have adopted the term Full-Body TensorPose to refer to the
approach we introduce in this paper for full-body pose feature
extraction.
Multilinear analysis is a deterministic approach based on highorder singular value decomposition. Recently, probabilistic multifactor analysis in the Gaussian process framework has also been
proposed [55] to factorize different contributing factors in human
movement. In this paper, we extend our research on robust extraction of invariant human pose descriptors from video data by
systematically investigating multilinear, non-multilinear (kernelbased), deterministic and probabilistic multifactor analysis techniques for view-invariant human pose feature extraction. The
two different multifactor analysis approaches to pose feature
extraction have been evaluated in terms of the following key performance metrics: degree of invariance to view changes, representation capacity, and performances in video-based pose and gesture
recognition. The key contribution of this paper is a systematic
comparative study of two different multifactor approaches to
view-invariant human pose extraction and their performances in
movement recognition. The experimental results have indicated
that the view-invariant pose features obtained using the multilinear analysis are more discriminative and lead to better movement
recognition performances than the MGP features. Another important contribution of this paper is to use MGP for view-invariant
pose feature extraction, which has not been done in previous work,
e.g., [55]. Our experimental results have indicated that the MGPbased method is slightly superior to the multilinear approaches
in terms of the degree of view-invariant and the reconstruction
quality. Such properties are valuable to movement estimation
and tracking.
The organization of the remaining of the paper is as follows. In
Section 2, we brieﬂy review key elements in deterministic

377

B. Peng et al. / Computer Vision and Image Understanding 115 (2011) 375–389

multilinear analysis and probabilistic kernel-based multifactor
analysis in the form of Gaussian processes. We then introduce
our approaches to view-invariant pose feature extraction using
both multifactor analysis techniques. In Section 4, our methods
for static pose and dynamic gesture recognition are introduced.
Experimental results and performance analysis of both multifactor
analysis techniques in pose feature extraction and movement recognition are presented in detail in Section 5. Finally in Section 6,
we present our conclusions and future research directions.

Aj 2 RNj ðN1 ...Nj1 Njþ1 ...Nn Þ . An illustration of the unfolding of a 3-mode
tensor is shown in Fig. 1a and b. As an analog to matrix–matrix
multiplication, an n-mode tensor can be multiplied by compatible
matrices in each mode. The mode-j multiplication of A with a matrix B 2 RMNj is C = A  jB, and C 2 RN1 ...Nj1 MNjþ1 ...Nn . The entries of C are given by

Cði1 ; . . . ; ij1 ; i; ijþ1 ; . . . ; in Þ ¼

Nj
X

Bði; kÞAði1 ; . . . ; ij1 ; k; ijþ1 ; . . . ; in Þ;

k¼1

i ¼ 1; . . . M
2. Background on multifactor analysis
2.1. Deterministic multilinear analysis
Multifactor analysis can be performed in both multilinear and
non-multilinear fashions. Linear multifactor analysis, only referred
to as multilinear analysis, assumes a multilinear relationship between the contributing factors and observation data. That is, when
all the factors are known except one, the observation becomes a
linear function of the remaining unknown factor. In multilinear
analysis, a tensor representation of the data is often used. A tensor,
also known as n-way array or multidimensional matrix or n-mode
matrix, is a higher order generalization of a vector (1-mode tensor)
and a matrix (2-mode tensor). High-order tensors can represent a
collection of data in a more complicated way. When a data vector
is determined by a combination of m factors, the collection of the
data vectors can be represented as an (m + 1)-mode tensor
T 2 RNv N1 N2 ...Nm , in which Nv is the length of the data vector
and Ni, (i = 1, 2,. . .,m) is the number of possible values of the ith
contributing factor, e.g., the number of people, view-points, or
types of illumination in the case of tensorface.
A tensor can be unfolded into a matrix along each mode. The
mode-j unfolding matrix of a tensor A 2 RN1 N2 ...Nn is denoted as

ð1Þ

For example, the multiplication of a 3-mode tensor and a matrix in
each mode is illustrated in Fig. 1c. When a 3-mode tensor is multiplied by a compatible row vector, it degenerates into a matrix
(Fig. 1d).
As a generalization of singular value decomposition (SVD) on
matrices, we can also perform high-order singular value decomposition (HOSVD) [49] on tensors. A tensor A 2 RN1 N2 ...Nn can be
decomposed into

A ¼ S1 U1 2 U2 . . . n Un ;
N j N 0j

ð2Þ

; N 0j

where Uj 2 R
6 Nj ; j ¼ 1; . . . ; n are mode matrices containing
orthonormal column vectors which are analogous to the left and
0
0
0
right matrices in SVD. The tensor S 2 RN1 N2 Nn is called the core
tensor which is analogous to the diagonal matrix in SVD. In order
to calculate the mode matrices Uj (j = 1,. . .,n), we can ﬁrst calculate
the SVD of the unfolding matrix A(j). Then Uj can be obtained by taking the columns of the left matrix of the SVD of A(j) of corresponding
to the N0j largest singular values. Then, the core tensor S can be calculated as follows.

S ¼ A1 UT1 2 UT2    n UTn :

ð3Þ

Let uj,k be the kth row vector of matrix Uj, the element of A at location (i1,i2,. . .,in) is essentially a multilinear function of the u vectors:

Fig. 1. Basic tensor operations. (a) and (b) Unfolding a 3-mode tensor, (c) and (d) multiplication of a 3-mode tensor with a matrix and a vector in each mode.

378

B. Peng et al. / Computer Vision and Image Understanding 115 (2011) 375–389

Aði1 ; i2 ; . . . ; in Þ ¼ S1 u1;i1 2 u2;i2    n un;in

ð4Þ

Let A(i1, , ij1, :,ij+1, , in) be the column vector containing a series of
elements A(i1, , ij1, ij, ij+1, , in), ij = 1, , Nj. It can also be represented
in a multilinear form:

Aði1 ; . . . ; ij1 ; :; ijþ1 ; . . . in Þ
¼ Sj Uj 1 u1;i1 2 u2;i2 . . . j1 uj1;ij1 jþ1 ujþ1;ijþ1 . . . n un;in

ð5Þ

2.2.2. Multifactor Gaussian process
MGP is a special case of Gaussian process with a special design
of kernel functions. MGP models the effect of multiple independent
factors on the output. Therefore, the input space is divided into
multiple factor spaces. An input vector x = {x(1),. . .,x(M)} is made
up of components from a few factor spaces. The kernel function
of MGP is then given by

kðx; x0 Þ ¼

M
Y

ki ðxðiÞ ; x0ðiÞ Þ þ b1 dðx; x0 Þ;

ð8Þ

Consider the data tensor T 2 RNv N1 N2 ...Nm , where the ﬁrst mode
corresponds to the dimension of the data vector. Thus, a data vector
T(:,i1, , ij, , im) can be represented in the above multilinear form, in
which the coefﬁcients uk;ik in each mode are independent factors
contributing to this data vector. The interaction of these factors is
governed by the tensor S  1U1. In the case that no decomposition
is done along the data mode, U1 becomes an identity matrix and
S  1U1 = S. When a new data vector is available, the corresponding
coefﬁcient vectors can be found by solving a multilinear equation
using the alternating least squares method.
This multilinear analysis framework has been widely used for
factorization of individual contributing factors in a few applications, including face image representation using tensorface [49],
modeling of 3D face geometry [50], texture and reﬂectance [51],
and image synthesis for tracking [52].

in which d(x, x0 ) is 1 when x = x0 and zero otherwise, and b is the
output noise factor. The kernel function of each factor ki(x(i), x0 (i))
can be independently deﬁned. Therefore, the inﬂuence of each factor on the output can be deﬁned separately, as to be discussed in
Section 3.2.
MGP can be applied to model the mapping from a latent space
consisting of multiple factor spaces to a high-dimensional observation space. In this case, each dimension of the observation vector is
an MGP, and kernel functions for all dimensions are the same or at
most differed by a linear scaling factor. If given an observation
point set, the latent points are unknown, the model can be viewed
as a special case of Gaussian process latent variable model
(GPLVM) [66].

2.2. Probabilistic multifactor analysis

3. Multifactor pose feature extraction

Recently, the multifactor Gaussian process (MGP) model has
been proposed [55] as a probabilistic kernel-based multifactor
analysis framework for separation of style and content of movement data and movement synthesis. MGP was developed based
on the Gaussian process (GP) method [65] and the Gaussian process latent variable model [66] by inducing kernel functions based
on the multilinear model. Because of the use of kernel functions,
MGP is able to represent more general multifactor models beyond
multilinear relationship. In addition, since MGP is rooted on GP, it
is inherently a probabilistic framework.

In our proposed method, given a 3D human movement sensing
and analysis task, training data are collected from typical movement related to the speciﬁc application and then a mapping function from the video observation (e.g., visual hull data) to the pose
descriptor will be established through learning. Based on this mapping function, invariant pose descriptors can then be directly extracted from the input video. Such an intermediate pose
representation of human poses can be used in a wide range of
applications involving video-based 3D human movement sensing
and analysis, such as articulated movement tracking and gesture
recognition.
In our research, we have examined two multifactor analysis
techniques, including the multilinear approach and the multifactor
Gaussian process approach, in developing a new human pose
descriptor extraction scheme that achieves the desired invariance
to changes in orientation of the subject. In this section, we present
our proposed approaches to invariant pose descriptor extraction
using the multilinear and the multifactor Gaussian process
techniques.

2.2.1. Gaussian processes
Let f be a zero-mean Gaussian process, deﬁned as a function of
X = {x1, x2. . .xN} and the function values f = {f(xn)|n = 1, 2,. . .,N} satisfy a multivariate Gaussian distribution

pðf; KÞ ¼ Nðf; 0; KÞ

ð6Þ

The elements of the covariance matrix K are deﬁned based on the
covariance function or kernel function Ki,j = k(xi, xj). Two commonly
applied kernel functions are the linear kernel k(x, x0 ) = xx0 , and the
RBF kernel k(x, x0 ) = exp(c||x  x0 ||2).
Gaussian processes can be applied to predict the function value
of unknown point. If a set of input variables X and corresponding
function values f are known, the conditional distribution of function value f* at a new input point x* is [65]:
T

pðf jfÞ ¼ Nðf ; k K1 f; k  k K1 k Þ;

ð7Þ

where k** = k(x*, x*) is the unconditional variance of f*; k* is the
cross-covariance vector of f* and f, i.e., k*(i) = k(x*, xi); and K is the
covariance matrix of f determined by X and the kernel function as
deﬁned above. This conditional distribution of f* given f is still
Gaussian since they are jointly Gaussian. The conditional mean of
f* is a linear combination of sample function values f. The conditional variance of f* is smaller than the unconditional variance
due to the knowledge of the sample points. In addition, this conditional distribution of f* is completely determined by the kernel function and the input point x* when the sample input and output points
are given.

i¼1

3.1. Invariant pose descriptor extraction using multilinear analysis
We propose to extract invariant pose descriptors using multilinear analysis. The observation vector in the data tensor can be
formed by 3D volumetric reconstruction such as visual hull data.
Visual hull observation of a human pose is affected by three main
factors: joint angle conﬁguration of the subject (different poses),
the subject’s body orientation about the camera system, and physical body variations of the subject (e.g., height and weight). The basic idea of the proposed framework for invariant pose feature
extraction is that given a set of key body poses identiﬁed either
manually or automatically, we construct a multilinear analysis
framework to obtain invariant descriptors for these key poses. In
the application of gesture recognition, the key poses can be landmarks representing the dynamic gesture.
3.1.1. Model learning and pose descriptor extraction
To extract invariant pose descriptors using multilinear analysis,
a tensor including visual hull data of a number of key poses in

B. Peng et al. / Computer Vision and Image Understanding 115 (2011) 375–389

different body orientations need to be constructed. For example,
Fig. 2 illustrates a visual hull tensor with three modes, voxel, body
orientation, and pose, which can be used to obtain view-invariant
pose descriptor. For a speciﬁc application, a set of key poses needs
to be selected. In our proposed approach, the key poses are selected
automatically based on the motion energy of the movement data
[30]. Once the set of key poses are selected, each sample visual hull
of the key pose can be rotated about the axis perpendicular to the
ground plane in order to generate the training visual hulls of the
pose in each designated orientation. In our approach, the designated orientation angles are evenly distributed between 0° and
360°. To remove the effect of translation and scaling to the visual
hull, a simple normalization procedure [58] can be applied to the
visual hull data. The normalized visual hull then is vectorized
and concatenated to form a complete input vector. As shown in
Fig. 2, the input observation vectors are spanned along the orientation mode and the pose mode to form the training tensor D, which
can be decomposed as

D ¼ C1 U Voxel 2 U Orientation 3 U Pose

ð9Þ

by using high-order singular value decomposition (HOSVD) [49] to
extract the core tensor C and the basis U matrices. The rows of UPose
correspond to the pose features of the training key poses used to
establish the pose tensor, and they are orthonormal vectors. To keep
these key pose feature vectors orthogonal to each other, no dimension reduction is performed at this step. Orthogonality implies large
distance in the feature space, and such discriminative property is
beneﬁcial to movement recognition.
Given any input visual hull, the corresponding pose and body
orientation coefﬁcient vectors can be computed using the Tucker-alternating least squares methods (ALS) [67–70] based on the
core tensor C. Given input visual hull of the same pose from different views, the extracted pose coefﬁcient vectors will be similar to
each other. Thus the pose coefﬁcient vector can be used as viewinvariant pose descriptors.
3.2. Invariant pose descriptor extraction using multifactor Gaussian
process model
In our proposed method, we have used the kernel-based nonlinear multifactor Gaussian process-based [55] approach to tackle the
challenge of achieving invariance in extracting pose descriptors.
Recently, kernel-based multifactor analysis has been developed

Fig. 2. Formation of pose tensor using visual hulls.

379

to separate content and style from motion capture data. Such kernel-based approach has been found to be effective to represent and
model potential nonlinearity involved in the contributing factors
such as pose and orientation to the visual hull observation.
Although the kernel-based multifactor analysis [55] has been proposed, it has been mainly used in modeling and synthesising
movement data. It has not been exploited in view/body-invariant
pose feature extraction. Our experimental results have shown
additional advances made in view-invariant pose descriptor
extraction using kernel-based multifactor analysis complimentary
to those made using the multilinear approach.
3.2.1. Model learning
In our proposed approach, the observed visual hull is modeled
to be affected by two factors: the body pose and the body orientation. Therefore, a latent point can be represented as x = {x(p), x(o)} in
which x(p) is the pose descriptor vector and x(o) is the orientation
vector. Training data can be obtained in the same way as the multilinear approach. Let the training samples be a matrix Yt in which
each row is a vectorized volumetric reconstruction. Since zeromean MGPs are applied, a mean vector needs to be subtracted from
each row of Yt. After centering the training data, we also normalize
the training data by dividing elements in each dimension by the
standard deviation of the dimension. The resulting training observations are denoted as Y. Let Np be the number of key poses and No
the number of designated orientations. Denoting N = Np  No to be
the number of training samples and D to be the dimensionality of
the observation vector, the dimensionality of Y is N  D.
The body orientation variable has one degree of freedom. In order to model the periodicity of rotation, we have used a 2D orientation vector x(o) to represent the body orientation, which can be
intuitively considered to be the corresponding location of the body
orientation angle on the unit circle. On the other hand, Dp the
dimensionality of the pose feature vector x(p) can be determined
according to the distribution of the latent points. Initially, Dp is
set to be equal to Np and the latent points corresponding to Y are
obtained according to the training procedure presented in the following paragraph. These latent points contain redundant information, and dimension reduction can be performed to improve
computational efﬁciency in pose feature extraction. Similar to
dimension reduction in the principal component analysis (PCA),
Dp is found based on the eigen-analysis of the covariance matrix
of the laten points. To reduce Dp, in our research we ﬁrst compute
M the covariance matrix of the learned pose vectors and ﬁnd the
eigenvalues of M, which correspond to the energy of the covariance
matrix. We then ﬁnd the smallest set of eigenvalues so that a certain percentage of the energy can still be preserved when the
remaining eigenvalues are dropped. Dp is then selected to be the
cardinality of the smallest set of eigenvalues satisfying the energy
preservation constraint. This eigenvalue set usually contains the Dp
largest eigenvalues. In our research, 99.5% of the energy is preserved and Dp is found to be 12.
Given the dimensions of the latent space, the following training
procedure can be taken to learn the latent points. In addition, during the training process, we constrain that the latent points of samples belonging to the same pose remain the same. Likewise, the
orientation coefﬁcients of samples belonging to the same orientation will maintain to be the same. In this training process, the orientation coefﬁcients of the designated orientations are initialized
to be 2D points evenly distributed on the unit circle. The latent
points of the key poses are initialized to be nonnegative Dp dimensional vectors. The pair-wise distances of these initial pose latent
points are also made similar to each other. With such constraints
and initial latent points, given training observations Y, the corresponding set of latent points X ¼ fxn gNn¼1 and parameters of the
kernel function C ¼ fcp ; co ; b g are optimized by maximizing the

380

B. Peng et al. / Computer Vision and Image Understanding 115 (2011) 375–389

following log likelihood function with respect to X ¼ fxn gNn¼1 and
C = {cp, co, b}:

LðX; CÞ ¼ log pðYjX; CÞ ¼ log

D
Y

!

In this section, we present two exemplar applications showing
how the invariant human pose descriptors can be applied in movement recognition.

pðyi jKðX; CÞÞ

i¼1
D
ND
1X
¼
logð2pjKðX; CÞjÞ 
yT KðX; CÞ1 yi
2
2 i¼1 i

ð10Þ

where yi is the ith column vector of Y. The covariance matrix K(X,
C) is speciﬁed as

Ki;j ðX; CÞ ¼ kðxi ; xj ; CÞ:

ð11Þ

where the kernel function is deﬁned as



 c

cp
kðx; x0 ; CÞ ¼ exp  jjxðpÞ  x0ðpÞ jj2 exp  o jjxðoÞ  x0ðoÞ jj2
2
2
þ b1 dðx; x0 Þ
ð12Þ
The quasi-Newton [71] method is applied for optimization. At each
optimization step, the latent points and kernel parameters are optimized together. In this case, the RBF kernel is applied for pose and
orientation factors. In our future research, we will also explore
other types of kernel functions and their combinations.
After the latent points X* and model parameters C* are learned
from the training data Y, the conditional distribution of the obserðoÞ
vation y* corresponding to a new latent point x ¼ fxðpÞ
 ; x g can be
found as a normal distribution according to the Gaussian process
theory. Deﬁne k* = k(x*, X*, C*) to be the cross-covariance vector
with x* and X. The ith element of k(x, X, C) is given by

ki ðx; X; CÞ ¼ kðx; xi ; CÞ;

i ¼ 1; . . . d

ð13Þ
K*

where d is the dimension of the latent space. Deﬁne
=
C*)
and k* = k(x*, x*, C*). The conditional distribution of y* is given by

pðy jx ; Y; X ; C Þ ¼

D
Y

4. Movement recognition using invariant-features

T

1
NðyðiÞ
yi ; k  k K1 k Þ
 ; k K

K(X*,

ð14Þ

4.1. Pose recognition
The view-invariant pose features can be directly used for
view-invariant 3D pose recognition. In our research, pose recognition is achieved by using the support vector machine (SVM)
classiﬁers in a one-versus-the-rest manner [72]. For each pose,
we train a binary classiﬁer to identify whether the input pose
feature ‘‘is’’ or ‘‘is not’’ the target pose. The training data of the
classiﬁer for a speciﬁc pose consists of positive samples of the
corresponding pose and negative samples obtained from other
poses. When all the classiﬁers are trained, pose recognition can
be achieved by a traversal of all the classiﬁers. Each classiﬁer will
return a binary label indicating whether it accepts the input as
the corresponding pose as well as a value of the discriminative
function.
In our pose recognition experiment, the kernel types and kernel
parameters are set to be the same for all binary SVM classiﬁers.
Therefore, all the SVM classiﬁers work in the same feature space.
It is possible that there are multiple SVM classiﬁers which return
positive results given a testing frame. In this case, the signed distances from the testing point to the dividing hyperplanes of these
SVMs are used to select the ﬁnal class. Basically, the pose class corresponding to the SVM classiﬁer yielding the maximum signed distance to the dividing hyperplane is selected to the recognized pose.
For an SVM classiﬁer, this distance from a testing frame to its
dividing hyperplane can be easily found by dividing the corresponding value of the discriminative function by a normalization
term corresponding to the length of the linear coefﬁcient vector
of the dividing hyperplane. Details of SVM classiﬁcation can be
found in [73].

i¼1

where yðiÞ
 is the ith element of y* and yi is the ith column vector of Y.

4.2. Gesture recognition

3.2.2. Invariant pose descriptor extraction
To extract invariant pose descriptors, we need to infer the latent
ðoÞ
point x ¼ fxðpÞ
 ; x g from a new observation y*. In our proposed
framework, this is done by solving the following optimization
problem

A dynamic gesture can be viewed as a time series of static body
poses. Once invariant pose descriptors can be extracted from the
input video data of the static poses, a gesture can be represented
by a sequence of invariant pose descriptors. Using these pose
descriptors as the observation vectors, gestures can be recognized
from a continuous movement stream by using the hidden Markov
Models (HMM) as shown in Fig. 3. These movement HMMs are
learned from training data to represent the speciﬁc non-gesture
movement patterns. Because of the use of the invariant pose
descriptor, the resulting gesture recognition system is also invariant to changes in orientation.

x ¼ arg max log pðy jx; Y; X ; C Þ
x

ð15Þ

in which the distribution p(y*|x, Y, X*,C) is deﬁned above. To solve
this optimization problem, the quasi-Newton method is applied.
Since the quasi-Newton method can only ﬁnd the local optimal
points, it is important to ﬁnd a good initial point for the optimalization process. Furthermore, it is often plausible to use multiple
initial points to improve the optimization result. In our research,
we ﬁrst select m observations in Y that are closest to y* and then
take their corresponding latent points as the initial points. After
optimization, m local optimal latent points can be obtained, and
the latent point yielding the largest likelihood of y* is chosen to
be the solution for x*. In practice, there is a tradeoff in selecting
m between precision (requiring a larger m) and computational efﬁciency (requiring a smaller m). In our research in pose feature
extraction using MGP, we have used m = 10 initial points. By inferring the latent point x*, the pose coefﬁcient xðpÞ and the orientation
coefﬁcient xðoÞ can be obtained. Then pose component xðpÞ can be
taken as the invariant pose descriptor describing the corresponding pose.

5. Experimental results and performance comparison
Due to their different nature as discussed above, the two proposed approaches may perform differently for the same application. It is important to identify their applicability. To provide
concrete insights to the applicability of the potentially complementary approaches for practical applications, we have systematically evaluated and compared these two invariant pose descriptors.
Speciﬁcally, we have systematically evaluated the invariant pose
descriptor extraction algorithms in terms of the following key performance metrics: degree of invariance to view changes, representation capacity, and performances in video-based movement
recognition.

B. Peng et al. / Computer Vision and Image Understanding 115 (2011) 375–389

381

Fig. 3. Using the invariant pose descriptor in gesture recogntion.

5.1. Exemplar view-invariant features
Using the multilinear and the multifactor Gaussian process approaches, We have obtained two different view-invariant pose
descriptor results using the 3-mode (voxel, pose, and view angle)
pose tensor with 25 key poses and 16 views. The ﬁrst row of
Fig. 4 shows 3D visual hull data (new testing data outside of the
training set) of a key pose from 16 views (Fig. 4a), the 25 dimen-

sional pose descriptors extracted using the multilinear approach
(Fig. 4b), and the 12 dimensional pose descriptors extracted using
the multifactor Gaussian approaches (Fig. 4c). The ﬁrst row of Fig. 5
presents visual hulls and the corresponding pose features of a nonkey pose. From these ﬁgures it can be seen that the pose descriptors of the same pose extracted from visual hull data of the same
pose viewed from different view-angles are indeed close to each
other.

Fig. 4. Visual hull data of a key pose in 16 body orientations (a) and the corresponding multilinear (b) and MGP (c) pose vectors. Subplots (d–f) show sample data (d) of the
same pose corrupted by protrusion errors (circled) and the corresponding multilinear (e) and MGP (f) pose vectors. Subplots (g–i) show sample data (g) of the pose with
occlusion errors and the corresponding multilinear (h) and MGP (i) pose vectors.

382

B. Peng et al. / Computer Vision and Image Understanding 115 (2011) 375–389

Fig. 5. Visual hull data of a non-key pose in 16 body orientations (a) and the corresponding multilinear (b) and MGP (c) pose vectors. Subplots (d–f) show sample data (d) of
the same pose corrupted by protrusion errors (circled) and the corresponding multilinear (e) and MGP (f) pose vectors. Subplots (g–i) show sample data (g) of the pose with
occlusion errors and the corresponding multilinear (h) and MGP (i) pose vectors.

In practice, visual hull data are often noisy due to 3D reconstruction errors. Visual hull errors often exhibit as large blocks of
uncarved background voxels and large blocks of miscarved foreground voxels. In our research, the ﬁrst type of errors is referred
to as protrusion errors and the second type partial occlusion
errors. To verify and compare the robustness of the two proposed
feature extraction approaches in the presence of such errors, we
have examined pose features extracted from noisy visual hull data.
In our experiment, noisy visual hull data were obtained by adding
either the protrusion or partial occlusion errors to the original data
used in the previous analysis (upper left subﬁgures in Figs. 4 and
5). To add a protrusion error to a visual hull, we ﬁrst select a protrusion sphere with a random center in the background voxels and
a diameter of three voxels (a 10th of the side length of the volumetric reconstruction). To realistically synthesize a protrusion
error, this random sphere has to overlap with the visual hull with
overlapping volume less than half of the sphere. Otherwise,
another random sphere will be selected and tested, until a valid

protrusion error is synthesized. Once a protrusion sphere is found,
all the voxels inside the sphere are considered protrusion voxels
and their values set to 1. Likewise, to add a partial occlusion error,
a partial occlusion sphere is ﬁrst generated with a random center
in the foreground voxels and a diameter of three voxels. Then all
the voxels within this sphere are considered occluded and their
values set to 0. Examples of such noisy visual hull data are shown
in Fig. 6.
The noisy visual hull data for a key pose and the corresponding
pose vectors extracted using the multilinear and the MGP approaches are presented in the middle (protrusion error) and bottom (partial occlusion) rows of Fig. 4. The noisy data and the
corresponding pose vectors of the non-key pose are given in
Fig. 5. In total, 12 sets of pose features have been extracted and
shown in Figs. 4 and 5, corresponding to 12 different scenarios indexed according to the dataset (key pose versus non-key pose),
error type (no error, protrusion error, and partial occlusion error),
and the feature extraction method (multilinear versus MGP). It

B. Peng et al. / Computer Vision and Image Understanding 115 (2011) 375–389

383

Fig. 6. Examples of two types of visual hull errors: (a) original visual hull, (b) noisy data with a protrusion error (circled), (c) noisy data with a partial occlusion error (circled).

can be clearly seen from these ﬁgures that both the multilinear and
the MGP approaches are robust to visual hull errors.
To obtain a quantitative measure of the error-resilience property of both approaches, we have further computed the pair-wise
Euclidean inter-orientation distances between the pose vectors
for each scenario. For each dataset and each feature extraction
method, the average norm of the corresponding pose features extracted from the error-free (no additional error added) data is used

Table 1
Normalized/relative average inter-orientation distances for the 12 scenarios.
Method and noise type

Key pose

Non-key pose

Multilinear, original data
Multilinear, protrusion
Multilinear, partial occlusion
MGP, original data
MGP, protrusion
MGP, partial occlusion

0.055/1
0.072/1.31
0.085/1.55
0.074/1
0.074/1
0.083/1.12

0.112/1
0.159/1.42
0.177/1.58
0.025/1
0.027/1.08
0.028/1.12

as the normalization constant for the corresponding dataset and
feature extraction method. The inter-orientation distances computed for each scenario are then normalized using the corresponding normalization constant according to the dataset and the feature
extraction method. The average distances for all the scenarios are
shown in Table 1. According to Table 1, we can see that both the
multilinear and the MGP approaches are robust to visual hull
errors in the sense that the average inter-orientation distances
for the noisy cases are all comparable to the average distance in
the corresponding error-free case.
In Table 1, we have also given as the second number in each cell
the relative average inter-orientation distances for each scenario
to examine the relative amount the average distance has increased
due to the added noise for a particular dataset, noise type, and pose
feature extraction method. It can be seen that when the MGP
method is used, the corresponding incremental percentages in
the average distances caused by noise are much less than those
obtained using the multilinear method. Hence, the MGP method
is much more robust to visual hull errors than the multilinear
approach, most likely due to its probabilistic nature.

Fig. 7. Examples of noisy visual hull data in the IXMAS dataset. Typical errors (in the circles) include remaining uncarved blocks in the background (a and b) and missing
(wrongly carved) blocks in the foreground (c–f).

384

B. Peng et al. / Computer Vision and Image Understanding 115 (2011) 375–389

5.2. Degree of invariance
It is critical to produce a measure of invariance for the proposed
pose descriptors obtained using different algorithms. In our research, we have systematically studied and evaluated the invariance of the proposed pose descriptors to view changes.
To evaluate the view-invariance, we ﬁrst randomly select a
number of poses and their visual hull data from the IXMAS dataset.
The selected data include poses close to the key poses as well as
those very much different from the key poses. Many of the visual
hull data in the IXMAS dataset suffer from the protrusion errors
and the partial occlusion errors introduced before. Examples of
such visual hull construction errors are shown in Fig. 7. Therefore,
the results reported in this section also reﬂect the performance of
the proposed features using noisy visual hull data.
Using the selected visual hull data, a view-invariance evaluation
test dataset is then synthesized by rotating each of the testing
poses to different (e.g., 16) facing directions. Once the test dataset
is constructed, for each pose, we can extract the corresponding
pose descriptors from these visual hulls and obtain their pair-wise
Euclidean distance. The maximum of these distances, deﬁned as
maximum inter-orientation distances (MIOD), are applied as a
measure of View-invariance of pose vectors obtained at this
frame. The histogram of the MIOD for all the testing poses presents
a big picture of the view-invariance of the corresponding pose
descriptor.
We have obtained results in evaluating the view-invariance
property of the pose descriptor extracted using the 3-mode tensor
analysis and the pose descriptor extracted using both the multilinear analysis and MGP. In our study, we randomly selected 10
frames of visual hull data from each of 10 subjects and totally
100 frames from the 10 subjects were used for view-invariance
evaluation. Among these 100 frames, 18 frames are close to one
of the key poses. To put such a view-invariance difference measure

into the proper context, we also computed the pair-wise interframe distances using the 100 original visual hull frames. The histogram of the MIOD of both the key pose frames and the non-key
pose frames corresponding to the pose features obtained using
multilinear analysis are shown in Fig. 8a and b. The histogram of
overall inter-frame distance is shown in Fig. 8c. Using the same
dataset, pose features were also extracted using the MGP method
and the corresponding histograms are shown in Fig. 9.
Please note that the normal distance ranges of the multilinear
and MGP features are different, 0–2 for the multilinear feature
and 0–6 for the MGP feature (see Figs. 8 and 9). To obtain a normalized view for the distance distributions, all the histograms are set
to 10 bins, and the size of each bin is a 10th of the maximum overall inter-frame distance (MOIFD) of corresponding type of pose
descriptor. From Figs. 8 and 9 it can be seen that the MIOD values
of nearly all the key pose frames (17 out 18 for multilinear pose
descriptors and all the 18 for MGP pose descriptors) and the majority (73 out 82 for multilinear pose descriptors and 78 out of 82 for
MGP pose descriptors) of non-key pose frames are less than a 10th
of MOIFD. Therefore, only a small percentage (10% for multilinear
pose descriptors and 4% for MGP pose descriptors) of the testing
frames has high MIOD values. Hence, we experimentally veriﬁed
that the proposed pose feature extraction method using both multilinear analysis and MGP can effectively extract view-invariant
features from visual hull data. Moreover, between the two different
view-invariant feature extraction approaches, the MGP-based
approach exhibits slightly stronger view-invariance property that
the multilinear analysis-based approach.
5.3. Representation capacity
For a pose descriptor, it is important to assess how capable it can
represent different body poses. To this end, we have examined its
representation capacity by looking at the visual hull reconstruction

Fig. 8. Distance distributions of pose vectors obtained by multilinear analysis. (a) Inter-orientation distances of pose vectors obtained from key pose frames. (b) Interorientation distances of pose vectors obtained from non-key pose frames. (c) Inter-frame distances between pose vectors obtained from 100 frames.

Fig. 9. Distance distributions of pose vectors obtained using MGP. (a) Inter-orientation distances of pose vectors obtained from key pose frames. (b) Inter-orientation
distances of pose vectors obtained from non-key pose frames. (c) Inter-frame distances between pose vectors obtained from 100 frames.

B. Peng et al. / Computer Vision and Image Understanding 115 (2011) 375–389

385

Fig. 10. Illustration of visual hull reconstruction: (a) the original visual hull, (b) reconstruction from the multilinear method, (c) reconstruction from the MGP method.

quality for both key poses and non-key poses. Since both the
multilinear method and the MGP method are essentially generative
models, given a testing visual data, y*, it is straightforward to ﬁnd the
reconstruction y*0 from the inferred latent point. The distance
between y* and y*0 can measure how well the pose descriptor represents y*. Finding such reconstruction error for a number of testing
poses provides a picture of the representation capacity of the pose
descriptor. Fig. 10 shows a testing visual hull and reconstructions
obtained from the pose descriptors computed using the multilinear
(middle) and the MGP methods. It can be seen that both approaches
can reconstruct the original input visual data to a certain extent and
the MGP-based method achieves a better reconstruction results
than the multilinear analysis-based method.
5.4. Results on pose recognition
We have applied the view-invariant pose descriptors obtained
using multilinear analysis and those obtained using MGP for pose
recognition. We performed tests on two datasets. One dataset is
composed of 20 dance poses. The other data set contains 20 poses
selected from the IXMAS action dataset [30].
5.4.1. Recognition of dance poses
We performed pose recognition on a data set containing 20
dance poses choreographed by a professional dancer. These poses
are shown in Fig. 11a. This data set also contains 20 trick poses.

This trick poses are outliers but are similar to one of the 20 standard poses. The trick poses are shown in Fig. 11b.
For this dataset, we applied a pair of 2D silhouettes obtained
from orthogonal views as the input data of a pose. The viewinvariant pose extraction algorithms using the multilinear analysis
and the MGP method introduced in Section 3 can also be used. In
this case, an input data sample is obtained by concatenating the
two vectorized body silhouettes of the subject captured by the
two cameras.
In our research, SVM with the radial basis function (RBF) kernels
have been used for pose recognition in a one-versus-the-rest manner. In order to train the SVM classiﬁers, we applied images synthesized using motion capture data and animation software as
well as real images captured by two video cameras. For testing,
we applied additional real images which are different from those
in the training set. For each pose, there are 192 synthetic training
samples and eight real training samples, and 16 real image pairs
are used as testing samples. For trick poses, we used 320 real image
pairs, 160 of them applied as training samples, and the remaining
160 applied as testing samples.
In this test, a testing data sample such as a sample from a trick
pose could belong to none of the poses in the pose vocabulary (i.e.,
an outlier). In this case, if none of the classiﬁers accepts a testing
sample, it is then identiﬁed as an outlier. Otherwise, the testing
sample will be recognized as the pose corresponding to the classiﬁer yielding the maximum signed distance from the testing point

Fig. 11. (a) The 20 dance poses. (b) The 20 trick poses.

386

B. Peng et al. / Computer Vision and Image Understanding 115 (2011) 375–389

Table 2
Recognition results of 20 dance poses.
Feature extraction
method

SVM
parameters

Recognition
rate (%)

False detection
rate (%)

Multilinear analysis
Multilinear analysis
Multilinear analysis
MGP
MGP
MGP

r = 0.7, C = 5
r = 1.2, C = 6
r = 1, C = 2
r = 0.2, C = 6
r = 0.3, C = 4
r = 0.4, C = 9

87.81
88.75
89.06
62.81
68.43
74.38

5
8.75
10.63
5
8.75
10.63

to the SVM dividing hyperplane in the feature space among all the
classiﬁers accepting the testing sample. Therefore, each testing
sample is recognized as one of the 20 poses or as an outlier. We
evaluate the recognition results using the recognition rate (RR)
and the false detection rate (FDR). The recognition rate is the percentage of testing samples of standard poses that are correctly recognized as their corresponding poses. The false detection rate is the
percentage of testing samples of trick poses that are wrongly recognized as one of the standard poses.
The RR and FDR of pose recognition using both multilinear analysis based features and MGP based features are shown in Table 2. It
can be seen that using features obtained from multilinear analysis
has led to better pose recognition results than those obtained using
features obtained from the MGP method.
5.4.2. Recognition of poses in IXMAS dataset
We also performed recognition of 20 poses in IXMAS dataset
[30]. The IXMAS dataset contains multi-view image and visual hull
data of a number of daily gestures performed by multiple subjects.
In our experiments, we have used the 3D visual hull data as movement observation for feature extraction and pose recognition. We
applied the key pose selection mechanism based on the motion
energy [58] to select 25 key pose form one of the movement
pieces. Then, we labeled the corresponding poses in the movement
pieces performed by the ﬁrst 10 subjects in the dataset. We discarded ﬁve poses that are not commonly exist in movements of
all subjects, and chose the remaining 20 pose as the set of poses
for recognition. The 20 pose are shown in Fig. 12. In average, there
are 23 samples of each pose performed by all subjects for training
and testing.
For this data set, training and testing are performed in a crossvalidation manner. At each cycle, poses performed by one subject
is applied as testing data and the remaining are applied as training
data. This process is repeated for all the 10 subjects.
In this experiment, all the testing samples are from the 20 target poses. Therefore, during pose recognition, a testing sample will
be classiﬁed into one of the poses in the pose vocabulary. In this

case, we simply assign the sample to the pose corresponding to
the classiﬁer yielding the maximum signed distance to the SVM
dividing hyperplane in the feature space and every testing sample
is assigned to one and only one pose class. To evaluate the performance of pose recognition, we have computed the recognition rate
(RR) and the false alarm rate (FAR). For a particular pose p, the corresponding RR is computed as the percentage of correctly recognized in-class pose samples, and the corresponding FAR is the
ratio of the number of testing samples misrecognized as pose p
to the total number of out-class samples for pose p (i.e., testing
samples of the other poses).
In our research, we have obtained pose recognition results from
the multilinear and the MGP features as well as from raw visual
hull data using the SVM and the K-nearest neighbors (K-NN) classiﬁers. The Euclidean distances have been used in all cases. In the
case of SVM, the RBF kernel has been used for the multilinear
and MGP features. When the raw visual hull data is used in SVM,
the linear kernel has been adopted, due to the high dimensionality
of the visual hull data [74]. For each classiﬁer-feature (or raw data)
scenario, a grid search has been carried out in the corresponding
parameter space to identify the best parameters. The pose recognition results obtained using the optimal classiﬁcation parameters
for each case are given in Table 3. It can be seen from Table 3 that
the multilinear feature has led to the best pose recognition results
using either SVM or K-NN. The result obtained from MGP features
using SVM is better than those directly from the raw visual hull
data using SVM or K-NN.
5.5. Results on gesture recognition
Using the view-invariant descriptors from the multilinear and
MGP approaches, we have obtained results for gesture recognition
using the pre-segmented gesture data from the IXMAS dataset for
gesture training and testing. Similar to the previous case in pose
recognition, the 3D visual hull data has been used for extracting
the pose descriptors. In our experiment, each gesture is modeled

Table 3
Recognition results of 20 poses in IXMAS dataset.
Classiﬁer

Feature extraction
method

Recognition rate
(%)

False alarm rate
(%)

SVM

Multilinear analysis
MGP
Raw visual hull data

74.40
68.09
66.42

1.35
1.68
1.77

K-NN

Multilinear analysis, K = 13
MGP, K = 9
Raw visual hull data, K = 1

71.43
64.38
65.49

1.51
1.88
1.81

Fig. 12. Twenty poses selected from the IXMAS data set.

B. Peng et al. / Computer Vision and Image Understanding 115 (2011) 375–389
Table 4
Gesture recognition results using pre-segmented data.

387

6. Conclusions and future work

Method

Recognition
rate (%)

False alarm
rate (%)

Weinland 3D [31]
Weinland 2D [30]
Proposed method using multilinear descriptors
Proposed method using MGP descriptors

93.33
81.3
94.59
87.90

0.67
1.97
0.54
1.17

as a 12-state left-to-right HMM. Following [31,30], we applied the
data obtained from the ﬁrst 10 subjects for training and testing. We
applied the same cross-validation strategy as described in the previous section for training and testing, which also concurred with
the strategy applied in [31,30].
The comparison of gesture recognition results is shown in Table
4. It can be seen that using pose descriptors obtained by multilinear analysis, results of gesture recognition are better than those obtained using the MGP-based method, and those obtained using the
state-of-the-art algorithms using the same training and testing
datasets based on the IXMAS dataset [30].
5.6. Discussions
We have presented our approaches and experimental results for
extracting invariant pose descriptors using both multilinear and
the multifactor Gaussian process-based methods and their performances in movement recognition. It is clear from the experimental
results that the in terms of the degree of view-invariance, the
reconstruction quality, the MGP-based method is slightly better
than the multilinear-based method. On the other hand, multilinear-based pose feature extraction method performs much better
than the MGP-based method in movement recognition, including
both static pose and dynamic gesture recognition. In the following,
we attempt to provide insights and explanation to such experimental results.
Both the multilinear and MGP methods conduct multifactor
analysis, but in different manners. The multilinear approach is a
deterministic. Once trained, the core tensor is ﬁxed and it encodes
how different factors interact to generate the observed visual hull
data. For this reason, pose descriptors extracted from the multilinear approach might be more sensitive to changes in these
additional factors (e.g., the body shape factor). In contrast, the
MGP-based approach is based on a probabilistic generative model
(the Gaussian process). Essentially, it assumes that core tensor follows a zero-mean normal distribution. In model learning, instead
of ﬁnding the core tensor as in the case of the multilinear analysis,
the kernel parameters are trained from the training data. The beneﬁt of such a probabilistic model is that it is likely that such model
is more ﬂexible to changes in the other non-modeled contributing
factors and it is also more resilient to observation noise present in
the visual hull reconstruction. For this reason, the MGP-based
method presents slightly better view-invariance property and
reconstruction quality than the multilinear-based method.
On the other hand, the multilinear-based method is superior to
the MGP-based method in movement recognition. This is largely
due to the inherent discriminative capacity of the multilinear analysis framework in pose feature extraction. In this approach, the key
poses are mapped to orthogonal pose descriptors during the training. This is a huge beneﬁt for discriminative analysis, such as pose
and gesture recognition. In contrast, the pose descriptors obtained
using the MGP approach do not have the orthogonal pose descriptor structure for the key poses and the general assumption of zeromean normal distributed core tensor elements in the MGP
approach may not effectively capture the underlying structure of
the pose descriptor space, leading to inferior classiﬁcation.

It is clear in our research that both multilinear analysis and
multifactor Gaussian process are effective view-invariant pose
extraction approaches. Experimental results show that the MGPbased method is slightly superior to the multilinear approaches
in terms of the degree of view-invariant and the reconstruction
quality. On the other hand, the multilinear-based approach greatly
outperforms the MGP-based method in movement recognition due
to the inherent discriminative power of the multilinear analysis
framework.
In our future work, we will further our research on a number of
fronts. First of all, we will investigate the potential of integrating
multilinear and MGP-based approaches. For example, the core tensors obtained using multilinear analysis can be used as the mean
tensor in the MGP approach. In doing so, we can maintain an arching structure for the learning of the latent points X while persevering the representation ﬂexibility provided by the probability
framework. When nonlinear kernel functions are used in the
MGP approach, the kernelized multilinear analysis [54] method
will then be used to provide the mean core tensor for the MGP
training. Fully exploring these issues will be one task in the future
research.
Another important area of our future research is to include
other contributing factors in pose feature extraction. People in different gender, age, and weight groups tend to have different body
shape. In addition, loose clothes of the subject can also introduce
extra variations in body shape. Pose features extracted using the
above multilinear analysis of the 3-mode (voxel, pose, and orientation) pose tensor are not invariant to body shapes. In other words,
the features extracted from the same pose of different subjects
with different body shapes (e.g., overweighed versus slim) can be
different. This is a very critical problem for practical action recognition systems. To address this challenge, in our future research we
will further introduce an additional mode in the pose tensor to reﬂect the changes in body shape, so that the resulting pose descriptor will be both view-invariant and body-shape-invariant. In this
case, we need to establish a training set of visual hull data of the
key poses in different body shapes. To this end, we will both collect
real data and generate synthetic data using animation software.
In our future research, we will also explore kernelized multilinear analysis [54] in extracting invariant pose descriptors and compare its performance against those discussed in this paper.
We will also examine the application of the invariant pose features in video-based articulated movement tracking. The basic idea
is that once the view-invariant descriptors are extracted, a bidirectional mapping between the invariant descriptors and body kinematics (e.g., joint angles) can be established in a shared-GPLVM
framework [75]. The backward mapping from the descriptors to
the kinematics can provide initialization in tracking and recover
the tracker from tracking failures. During the tracking, the forward
mapping from kinematics from descriptors can also be used to
evaluate likelihood in a Kalman or particle ﬁlter tracking framework. In addition in a particle ﬁltering tracking framework, given
an input visual hull data, samples can be drawn surrounding the
backward mapping results to further exploit the local kinematic
space.
Acknowledgments
The authors are grateful to the anonymous referees for this
paper for their insightful comments. This work was supported in
part by the US National Science Foundation (NSF) Grants RI-0403428 and DGE-05-04647. Any opinions, ﬁndings and conclusions
or recommendations expressed in this material are those of the
authors and do not necessarily reﬂect the views of the US NSF.

388

B. Peng et al. / Computer Vision and Image Understanding 115 (2011) 375–389

References
[1] C. Cruz-Neira, D.J. Sandin, T.A. DeFanti, R.V. Kenyon, J.C. Hart, The CAVE: audio
visual experience automatic virtual environment, Communications of the ACM
35 (1992) 64–72.
[2] T.E. Starner, Visual recognition of american sign language using hidden markov
models, in: Media Arts and Sciences, vol. Master: MIT, 1995, pp. 52.
[3] C. Sul, K. Lee, K. Wohn, Virtual stage: a location-based Karaoke system, IEEE
Multimedia 05 (1998) 42–52.
[4] C. Keskin, K. Balci, O. Aran, B. Sankur, L. Akarun, A multimodal 3D healthcare
communication system, in: Proceedings of the 3DTV Conference, 2007,
pp. 1–4.
[5] S.C.W. Ong, S. Ranganath, Automatic sign language analysis: a survey and the
future beyond lexical meaning, IEEE Transactions on Pattern Analysis and
Machine Intelligence 27 (2005) 873–891.
[6] T. Starner, J. Weaver, A. Pentland, Real-time American sign language
recognition using desk and wearable computer based video, IEEE
Transactions on Pattern Analysis and Machine Intelligence 20 (1998) 1371–
1375.
[7] J.F. Lichtenauer, E.A. Hendriks, M.J. Reinders, Sign language recognition by
combining statistical DTW and independent classiﬁcation, IEEE Transactions
on Pattern Analysis and Machine Intelligence 30 (2008) 2040–2046.
[8] H.-D. Yang, S. Sclaroff, S.-W. Lee, Sign language spotting with a threshold
model based on conditional random ﬁelds, IEEE Transactions on Pattern
Analysis and Machine Intelligence 31 (2009) 1264–1277.
[9] D. Birchﬁeld, T. Ciufo, G. Minyard, G. Qian, W. Savenye, H. Sundaram,
H. Thornburg, C. Todd, SMALLab: a mediated platform for education, in:
Proceedings of ACM SIGGRAPH, 2006.
[10] H.S. Park, D.J. Jung, H.J. Kim, Vision-based game interface using human gesture,
in Advances in Image and Video Technology. Berlin/Heidelberg: Springer,
2006, pp. 662–671.
[11] S.-W. Lee, Automatic gesture recognition for intelligent human–robot
interaction, Proceedings of the FGR (2006) 645–650.
[12] A. Camurri, B. Mazzarino, M. Ricchetti, R. Timmers, G. Volpe, Multimodal
analysis of expressive gesture in music and dance performances, in: G.V.A.
Camurri (Ed.), Gesture-based Communication in Human–Computer
Interaction, Springer-Verlag, 2004.
[13] A. Camurri, S. Hashimoto, M. Ricchetti, A. Ricci, K. Suzuki, R. Trocca, G. Volpe,
EyesWeb: towards gesture and affect recognition in dance/music interactive
systems, Computer Music Journal 24 (2000) 57–69.
[14] A. Camurri, R. Trocca, Analysis of expressivity in movement and dance, in:
presented at Colloquium on Musical Informatics, L’Aquila, Italy, 2000.
[15] G. Qian, F. Guo, T. Ingalls, L. Olson, J. James, T. Rikakis, A Gesture-Driven
Multimodal Interactive Dance System, in: Presented at IEEE International
Conference on Multimedia and Expo, 2004.
[16] Y. Wu, T.S. Huang, Vision-based gesture recognition: a review, in: Lecture
Notes in Artiﬁcial Intelligence 1739, Gesture-Based Communication in
Human–Computer Interaction, (International Gesture Workshop, GW’99),
1999.
[17] Y. Cui, D. L. Swets, J. Weng, Learning-based hand sign recognition using
SHOSLIF-M, in: Proceedings of the IEEE International Conference on Computer
Vision, 1995, pp. 631–636.
[18] Y. Wu and T. Huang, View-independent recognition of hand postures, In:
Proceedings of the IEEE International Conference On Computer Vision And
Pattern Recognition, 2000, pp. 88–94.
[19] A. Imai, N. Shimada, Y. Shirai, 3-D hand posture recognition by training
contour variation, AFGR (2004) 895–900.
[20] M. Singh, M. Mandal, A. Basu, Pose recognition using the Radon transform,
Circuits and Systems, 2005. 48th Midwest Symposium on, vol. 2, 2005, pp.
1091–1094.
[21] I. Haritaoglu, D. Harwood, L. Davis, Ghost: A human body part labeling system
using silhouettes, in: Proceedings of the International Conference on Pattern
Recognition, 1998.
[22] G.R. Bradski, J.W. Davis, Motion segmentation and pose recognition with
motion history gradients, Machine Vision and Applications 13 (2002) 174–
184.
[23] I. Cohen, H. Li, Inference of human postures by classiﬁcation of 3D human body
shape, in: Proceedings of the IEEE International Workshop on Analysis and
Modeling of Faces and Gestures, 2003, p. 74.
[24] H. Francke, J. Ruiz-del-Solar, a. R. Verschae, Real-time hand gesture detection
and recognition using boosted classiﬁers and active learning, in: Advances in
Image and Video Technology, Springer, Berlin/Heidelberg, 2007, pp. 533–547.
[25] G. Ye, J.J. Corso, D. Burschka, G.D. Hager, VICs: a modular HCI framework using
spatiotemporal dynamics, Machine Vision and Applications 16 (2004)
13–20.
[26] G. Ye, J.J. Corso, G.D. Hager, Gesture recognition using 3D appearance and
motion features, in: Proceedings of the CVPR Workshops, 2004, pp. 160–166.
[27] M.B. Holte, T.B. Moeslund, View invariant gesture recognition using 3D motion
primitives, in: Proceedings ICASSP, 2008, pp. 797–800.
[28] T. Kirishima, K. Sato, K. Chihara, Real-time gesture recognition by learning and
selective control of visual interest points, Pattern Analysis and Machine
Intelligence 27 (2005) 351–364.
[29] C. Lee, Y. Xu, Online, Interactive learning of gestures for human/robot
interfaces, in: 1996 IEEE International Conference on Robotics and
Automation, vol. 4, 1996, pp. 2982–2987.

[30] D. Weinland, R. Ronfard, E. Boyer, Free viewpoint action recognition using
motion history volumes, Computer Vision and Image Understanding 104
(2006) 249–257.
[31] D. Weinland, E. Boyer, R. Ronfard, Action Recognition from Arbitrary Views
using 3D Exemplars, in: Proceedings of the IEEE International Conference on
Computer Vision, 2007, pp. 1–7.
[32] F. Lv, R. Nevatia, Single view human action recognition using key pose
matching and Viterbi path searching, in: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2007, pp. 1–8.
[33] S. Mitra, T. Acharya, Gesture recognition: a survey, systems, man, and
cybernetics, IEEE Transactions on Part C: Applications and Reviews 37
(2007) 311–324.
[34] A.F. Bobick, Y.A. Ivanov, Action recognition using probabilistic parsing, in:
Presented at Computer Vision and Pattern Recognition, 1998. Proceedings.
1998 IEEE Computer Society Conference on, 1998.
[35] A. Yilmaz, Recognizing human actions in videos acquired by uncalibrated
moving cameras, in: Proceedings of IEEE International Conference on
Computer Vision, 2005, pp. 150–157.
[36] Y. Shen, H. Foroosh, View-invariant action recognition from point triplets, IEEE
Transactions on Pattern Analysis and Machine Intelligence 31 (2009) 1898–
1905.
[37] V. Parameswaran, R. Chellappa, View invariance for human action recognition,
International Journal of Computer Vision 66 (2006) 83–101.
[38] L. Gorelick, M. Blank, E. Shechtman, M. Irani, R. Basri, Actions as space-time
shapes, IEEE Transactions on Pattern Analysis and Machine Intelligence 29
(2007) 2247–2253.
[39] A.F. Bobick, J.W. Davis, The recognition of human movement using temporal
templates, IEEE Transactions on Pattern Analysis and Machine Intelligence 23
(2001) 257–267.
[40] B. Peng, G. Qian, S. Rajko, View-invariant full-body gesture recognition via
multilinear analysis of voxel data, in: Proceedings of International Conference
on Distributed Smart Cameras, 2009, pp. 1–8.
[41] S. Eickeler, A. Kosmala, G. Rigoll, Hidden Markov model based continuous
online gesture recognition, in: Proceedings of the International Conference on
Pattern Recognition, 1998, pp. 1206–1208.
[42] J. Alon, V. Athitsos, Q. Yuan, S. Sclaroff, A uniﬁed framework for gesture
recognition and spatiotemporal gesture segmentation, IEEE Transactions on
Pattern Analysis and Machine Intelligence 31 (2009) 1685–1699.
[43] Y.X. Zhu, G.Y. Xu, D.J. Kriegman, A real-time approach to the spotting,
representation, and recognition of hand gestures for human–computer
interaction, Computer Vision and Image Understanding (2002) 189–208.
[44] H.-K. Lee, J.H. Kim, An HMM-based threshold model approach for gesture
recognition, IEEE Transactions on Pattern Analysis and Machine Intelligence 21
(1999) 961–973.
[45] H.-D. Yang, A.-Y. Park, S.-W. Lee, Gesture spotting and recognition for human–
robot interaction, IEEE Transactions on Robotics 23 (2007) 256–270.
[46] D. Birchﬁeld, T. Ciufo, G. Minyard, SMALLab: a mediated platform for
education, in: Proceedings of the 33rd International Conference and
Exhibition on Computer Graphics and Interactive Techniques in Conjunction
with SIGGRAPH, 2006.
[47] C. Chu, I. Cohen, Pose and Gesture Recognition using 3D Body Shapes
Decomposition, in: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2005, pp. 69–78.
[48] M. Pierobon, M. Marcon, A. Sarti, S. Tubaro, Clustering of human actions using
invariant body shape descriptor and dynamic time warping, in: IEEE
Conference on Advanced Video and Signal Based Surveillance, 2005, pp. 22–27.
[49] M.A.O. Vasilescu, D. Terzopoulos, Multilinear analysis of image ensembles:
TensorFaces, ECCV (1) (2002) 447–460.
[50] D. Vlasic, M. Brand, H. Pﬁster, J. Popovi, Face transfer with multilinear models,
in: ACM SIGGRAPH 2005 Papers, ACM Press, Los Angeles, California, 2005.
[51] M.A.O. Vasilescu, D. Terzopoulos, Tensortextures: multilinear image-based
rendering, ACM Transactions on Graphics 23 (2004) 334–340.
[52] C.-S. Lee, A. Elgammal, Modeling view and posture manifolds for tracking, in:
Presented at Proceedings of International Conference on Computer Vision, Rio
de Janeiro, Brazil, 2007.
[53] A. Elgammal, C.-S. Lee, Separating style and content on a nonlinear manifold,
Presented at Proceedings of The IEEE International Conference on Computer
Vision and Pattern Recognition, 2004.
[54] Y. Li, Y. Du, X. Lin, Kernel-based multifactor analysis for image synthesis and
recognition, in: Presented at Computer Vision, 2005. ICCV 2005. Tenth IEEE
International Conference on, 2005.
[55] J.M. Wang, D.J. Fleet, A. Hertzmann, Multifactor Gaussian process models for
style-content separation, in Proceedings of the 24th International Conference
on MACHINE LEARNING, ACM Press, Corvalis, Oregon, 2007.
[56] B. Peng, G. Qian, S. Rajko, A view-invariant video based full body gesture
recognition system, in: Presented at International Conference on Pattern
Recognition, Tampa, FL, 2008.
[57] B. Peng, G. Qian, Binocular dance pose recognition and body orientation estimation via multilinear analysis, in: S. Aja-Fernández, R. de Luis García, D. Tao, X. Li
(Eds.), Tensors in Image Processing and Computer Vision, Springer-Verlag, 2009.
[58] B. Peng, G. Qian, S. Rajko, View-invariant full-body gesture recognition via
multilinear analysis of voxel data, in: Presented at ACM/IEEE International
Conference on Distributed Smart Cameras Como, Italy, 2009.
[59] B. Peng, G. Qian, Y. Ma, Recognizing body poses using multilinear analysis and
semi-supervised learning, Pattern Recognition Letters 30 (14) (2009) 1289–
1294.

B. Peng et al. / Computer Vision and Image Understanding 115 (2011) 375–389
[60] B. Peng, G. Qian, Binocular dance pose recognition and body orientation
estimation via multilinear analysis, in: Proceedings of the Workshop on
Tensors in Image Processing and Computer Vision in Conjunction with the
IEEE Conference on Computer Vision and Pattern Recognition, 2008,
pp. 1–8.
[61] B. Peng, G. Qian, S. Rajko, View-Invariant Full-Body Gesture Recognition from
Video, in: Proceedings of the International Conference on Pattern Recognition,
2008, pp. 1–5.
[62] V.N. Vapnik, The Nature of Statistical Learning Theory, Springer, 1995.
[63] J. Tu, Y. Fu, T.S. Huang, Locating nose-tips and estimating head poses in images
by tensorposes, IEEE Transactions on Circuits and Systems for Video
Technology 19 (2009) 90–102.
[64] J. Tu, T. Huang, Locating nosetips and estimating head pose in images by
tensorposes, in: Proceedings of IEEE International Conference on Image
Processing IV, 2007, pp. 513–516.
[65] C.E. Rasmussen, Gaussian Processes for Machine Learning, MIT Press, 2006.
[66] N. Lawrence, Probabilistic non-linear principal component analysis with
gaussian process latent variable models, Journal of Machine Learning
Research 6 (2005) 1783–1816.

389

[67] P. Kroonenberg, J. de Leeuw, Principal component analysis of three-mode data
by means of alternating least squares algorithms, Psychometrika 45 (1980)
69–97.
[68] J. ten Berge, J. de Leeuw, P. Kroonenberg, Some additional results on principal
components analysis of three-mode data by means of alternating least squares
algorithms, Psychometrika 52 (1987) 183–191.
[69] R. Sands, F. Young, Component models for three-way data: an alternating least
squares algorithm with optimal scaling features, Psychometrika 45 (1980)
39–67.
[70] M.A.O. Vasilescu, D. Terzopoulos, Multilinear subspace analysis of image
ensembles, Computer Vision and Pattern Recognition (2) (2003) 93–99.
[71] J. Nocedal, S.J. Wright, Numerical Optimization, Springer-Verlag, 1999.
[72] V.N. Vapnik, Statistical Learning Theory, Wiley, New York, 1998.
[73] C.M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006.
[74] C.-W. Hsu, C.-C. Chang, C.-J. Lin, A Practical Guide to Support Vector
Classiﬁcation, Technical Report, Department of Computer Science, National
Taiwan University, 2003.
[75] C.H. Ek, J. Rihan, P.H.S. Torr, G. Rogez, N.D. Lawrence, Ambiguity modeling in
latent spaces, Machine Learning for Multimodal Interaction (2008) 62–73.

COMPRESSIVE IMAGING OF COLOR IMAGES
Pradeep Nagesh and Baoxin Li
Dept. of Computer Science & Engineering
Arizona State University, Tempe, AZ 85287, USA
ABSTRACT
In this paper, we propose a novel compressive imaging framework
for color images. We first introduce an imaging architecture based
on combining the existing single-pixel Compressive Sensing (CS)
camera with a Bayer color filter, thereby enabling acquisition of
compressive color measurements. Then we propose a novel CS
reconstruction algorithm that employs joint sparsity models in
simultaneously recovering the R, G, B channels from the
compressive measurements. Experiments simulating the imaging
and reconstruction procedures demonstrate the feasibility of the
proposed idea and the superior quality in reconstruction.

Index Terms— Compressive Sensing, Bayer Color Filter,
Joint Sparsity Models, l1-Minimization.
1. INTRODUCTION
Modern digital cameras acquire an image in the form of millions of
“pixels” and subsequently use transform coders like a JPEG or
JPEG-2000 to reduce the data-rate for transmission or storage. In
recent years a new signal acquisition theory known as Compressive
Sensing (CS) has emerged, which provides a method for acquiring
and compressing data simultaneously. According to CS, only ‫ܯ‬
(with ‫ ܯ‬൏൏ ܰ) non-adaptive linear measurements of a ‫ܭ‬-sparse
signal of ܰ samples contain sufficient information for perfect
reconstruction using non-linear optimization methods, provided
that some conditions are satisfied [1,2]. Formally, let ࢞ ‫ א‬Թ୒ be a
real-valued signal that has sparse representation in a basis ࢸ ‫א‬
Թ୒ൈ୒ (assume orthonormal for convenience). Suppose that we
have a sensing systemࢶ ‫  א‬Թ୑ൈ୒, with ‫ ܯ‬൏ ܰ such that ࢶ and
ࢸ are incoherent, then from a measurement of ࢞ defined as,
࢟ ൌ ࢶ࢞ǡ࢟ ‫  א‬Թ୑

(1)

one can recover ࢞ from ࢟ as long as the number of measurements
‫ ܯ‬is of the order ofܱሺ‫ܭ‬ሺܰሻሻ, where ‫ܭ‬is the sparsity of ࢞ in
the basisࢸ (i.e, ‫ ܭ‬is the number of non-zero values inࣂ, ࣂ ൌ
ࢸࢀ ࢞ǡ ࣂ ‫  א‬Թ୒ ). The reconstruction is based on l1 optimization:
෡ ൌ ܽ‫݊݅݉݃ݎ‬ԡࣂԡଵ ‫ݏ‬Ǥ ‫ݐ‬Ǥ࢟ ൌ ࢶࢸࣂ
ሺ݈ଵ ሻࣂ

(2)

In the case of color imaging, one naïve approach would be to
measure and reconstruct the R,G and B planes separately. This
approach nevertheless cannot adequately exploit strong correlation
among the color channels. Considering this fact and the reality of
the wide-spread use of the Bayer filter in color imaging, in this
paper we first introduce an imaging architecture for the acquisition
of compressive color measurements based on combining the
“single-pixel camera” [3] with a Bayer color filter. Then we
propose a novel CS reconstruction algorithm that employs joint

978-1-4244-2354-5/09/$25.00 ©2009 IEEE

1261

sparsity models in simultaneously recovering the R, G, B channels
from the compressive measurements. We design experiments to
evaluate the performance of the proposed algorithm. The results
show that our method is able to produce significant improvement
in reconstruction, compared with the naïve approach of processing
the R, G and G channels separately.
The remaining of the paper is organized as follows. Section 2
introduces the architecture for compressive color imaging based on
the single-pixel camera of [3]. Section 3 presents the proposed
reconstruction algorithm. Section 4 provides our experimental
results. We conclude with discussion on future work in Section 5.

2. A CS COLOR CAMERA ARCHITECTURE
For clarity and completeness, we first briefly review the working
principle of the CS-based single-pixel-camera of [3] (more details
can be also found in [4,5]). Refer to Fig. 1, ignoring the rotating
color filter for the moment. Lens 1 captures an image of the scene
on the digital micro mirror (DMD) array. With the help of pseudorandom number generator (RNG), randomly selected mirrors of
DMD are oriented in a direction towards lens 2 (a “1”), while the
rest are oriented in a different direction (a “0”). The net-effect is
that the photodiode captures the summation of the light photons.
This process can be interpreted as obtaining ݉௧௛ measurement
‫ܡ‬ሺ݉ሻof Eqn. (1) (݉ ൌ ͳǡʹ ǥ ‫)ܯ‬, as an inner product between the
random vector ߶௠ ‫  א‬Թே (RNG configuration of 1s and 0s) and
actual image࢞ ‫  א‬Թே . Here ߶௠ can be considered as rows to form
the random measurement matrix (ࢶ ൌ  ሾ߶ଵ ߶ଶ ǥ߶ெ ሿ் ‫  א‬Թெൈே ).
By simple calibrations, ࢶ can be changed to other random
structures of like -1/+1 etc [5].
We now introduce a simple way of extending the above singlepixel camera with a virtual Bayer filter for compressive acquisition
of a color image. The basic idea is illustrated in Fig. 1. Fig. 1(a)
shows the mosaic structure of the Bayer filter. In Fig. 1(b), the
mirrors and the rotating color filter (RCF) are synchronized to
allow the acquisition of the R, G and B pixels with a pattern
corresponding to that of the actual Bayer filter. The RNG further
allows random measurements of the R,G and B planes and can be
configured seperately. Note that the G mirrors are explicitly
labeled as G1 and G2, and we capture two separate measurements
for G plane. Specifically, referring to Fig. 1(b), we explain the
steps to capture the R plane (G1, G2 and B are similar to this).
First, the Rotation Control ensures that the red portion of RCF is
positioned between lens 1 and the DMD array and that only the R
mirrors operate, while G1, G2 and B mirrors are turned off.
Second, the RNG randomly chooses some R mirrors to point
towards lens 2 and the rest away from it (this is the vector߶௠ ‫א‬
ԹேȀସ ). After ‫ܯ‬ோ repetitions we get, the measurement vector ࢟ࡾ as,

ICASSP 2009

R

G1

R

G1

G2

B

G2

B

R

G1

R

G1

Joint Color CS
Reconstruction

Photodiode
A/D

G2

B

Rearrange to
Mosiac Structure

Lens 1
Lens 2

Fig 1(b)

B

G2

DMD Array

Rotating
Color Filter

Demosaicing

Color
Image

RNG + Rotation Control

Fig 1(a)
Fig 1: (a) Shows “Virtual Bayer Filter” structure on the DMD array. There is no real Bayer filter, but each micro mirror is virtually
“labeled” so that mosaic structure of a Bayer filter is formed. (b) Proposed color CS camera architecture. This has a Rotating Color Filter
(RCF) and a Rotation Control unit (RC) as new components in the Camera of [3]. It captures R, G and B measurements directly on Bayer
planes, (thereby reducing the overall measurements) and uses joint R-G-B reconstruction scheme to produce better quality color image.
࢟ࡾ ൌ  ࢶࡾ ࢞ࡾ ǡ࢟ࡾ  ‫ א‬Թெೃ ൈଵ
்

(3)

ெೃ ൈேȀସ

where, ࢶࡾ ൌ ሾ߶ଵ ߶ଶ ǥ߶ெ ሿ ‫  א‬Թ
 is the random
measurement matrix and ࢞ࡾ is sub-sampled image ࢞, with only
samples retained from the alternate locations corresponding to the
mosaic cell “R” of the Bayer filter. Mathematically we denote such
a mapping function as ࡮ࡾ ǣ ࢞ ՜ ࢞ࡾ  or ࢞ࡾ ൌ  ࡮ࡾ ሺ࢞ሻ , with ࢞ ‫א‬
Թே and ࢞ࡾ ‫ א‬ԹேȀସ . We can define similar mapping functions
࡮࡮ ǡ ࡮ࡳ૚ and ࡮ࡳ૛ for B, G1 and G2 respectively. We call this “Bayer
Measurement” and the process can be described by the following
set of equations,
࢟ࡾ  ൌ  ઴‫ ࡾ࡮ ܀‬ሺ࢞ሻǡ࢟ࡾ  ‫ א‬Թெೃ ൈଵ
࢟ࡳ૚ ൌ  ઴۵૚ ࡮ࡳ૚ ሺ࢞ሻǡ࢟ࡳ૚ ‫ א‬Թெಸ ൈଵ
࢟ࡳ૛ ൌ  ઴۵૛ ࡮ࡳ૛ ሺ࢞ሻǡ࢟ࡳ૛ ‫ א‬Թெಸ ൈଵ
࢟ࡾ  ൌ  ઴۰ ࡮࡮ ሺ࢞ሻǡ࢟࡮  ‫ א‬Թெಳ ൈଵ

(4)

3.1. Joint R-G-B Reconstruction: A Baseline Algorithm
Let ࢘ǡ ࢍ and࢈ ‫  א‬Թே be the raw R, G and B images, and ࣂ࢘ ǡ ࣂࢍ
andࣂ࢈ ‫  א‬Թே be their transform coefficients (in a basisࢸ). Similar
to [6], we assume the following simple additive model,
ࣂ࢘ ൌ  ࣂࢉ  ൅  ࣂ࢏࢘ ൌ  ࢸࢀ ሺ࢘ࢉ  ൅  ࢘࢏ ሻ  ൌ  ࢸࢀ ࢘
ࣂࢍ ൌ  ࣂࢉ  ൅  ࣂ࢏ࢍ ൌ ࢸࢀ ൫ࢍࢉ  ൅  ࢍ࢏ ൯ ൌ  ࢸࢀ ࢍ
ࣂ࢈ ൌ  ࣂࢉ  ൅  ࣂ࢏࢈ ൌ  ࢸࢀ ൫࢈ࢉ  ൅  ࢈࢏ ൯ ൌ ࢸࢀ ࢈

(5)

where, ࣂࢉ ‫  א‬Թே is a sparse component, common to ࣂ࢘ ǡ ࣂࢍ
and ࣂ࢈ ǡ derived from a common support ષ (of non-zero
coefficients), with cardinality ‫ ܭ‬௖ . Further, ࣂ࢏࢘ , ࣂ࢏ࢍ and ࣂ࢏࢈  are the
sparse innovation components that are unique to each image. If we
let the sparsities of these components as ฮࣂ࢏࢘ ฮ଴ ൌ ‫ܭ‬௥௜ ǡฮࣂ࢏ࢍ ฮ ൌ
଴

For color imaging, the Bayer filter compressive measurements are
more efficient, since they reduce (i) the data-rate, exploiting the
correlation between R-G-B pixels and (ii) acquisition time per
color image. Note that this architecture is just a conceptual design
and there may be more efficient ways of implementing a Bayer
filter, depending on practical feasibility and costs. In addition, if
the acquisition data rate or time is not a constraint, one could also
obtain full R, G, B measurements using the RCF (i.e., forgoing the
Bayer mosaic and capturing full R-G-B measurements). However,
our main focus in this paper is to present an efficient color-image
CS reconstruction scheme (Section 3), operating on reduced
measurements captured with a Bayer filter, and thus we will
assume that the data are acquired as discussed above.

3. PROPOSED RECONSTRUCTION ALGORITHM
With the compressive color measurements taken as discussed
above, we now propose a CS approach for the reconstruction of the
original color image, exploiting the inter-correlations among the R,
G and B planes. The objective is to achieve better quality for the
reconstructed images, compared to individually reconstructing the
R, G, B images. To this end, we first present a baseline algorithm
(Section 3.1) based on simple extension of the joint sparsity
models (JSM) that has been proposed in the context of distributed
compressive sensing of 1-D multi-sensor signals [6]. To fully
utilize the specific inter-pixel relationship arising from the Bayer
mosaic, in Section 3.2, we propose an extended JSM that aims at
accounting for the spatial shift of the R, G, and B pixels.

1262

‫ܭ‬௚௜ ǡ ฮࣂ࢏࢈ ฮ଴ ൌ ‫ܭ‬௕௜ and that of original images as, ԡࣂ࢘ ԡ଴ ൌ
‫ܭ‬௥ ǡฮࣂࢍ ฮ ൌ ‫ܭ‬௚ ǡԡࣂ࢈ ԡ଴ ൌ ‫ܭ‬௕ , then, the joint representation of
଴

்

࢘ǡ ࢍ and ࢈ , denoted as ࡿ ൌ ൣࣂࢉ ࣂ࢏࢘ ࣂ࢏ࢍ ࣂ࢏࢈ ൧ has sparsity ‫ ܭ‬ൌ
‫ ܭ‬௖ ൅‫ܭ‬௥௜ ൅ ‫ܭ‬௚௜ ൅ ‫ܭ‬௕௜ , while the independent representation has a
෩ ൌ ‫ܭ‬௥ ൅ ‫ܭ‬௚ ൅ ‫ܭ‬௕ . Since ࢘ǡ ࢍ and ࢈ are
total sparsity of ‫ܭ‬
sufficiently correlated, many coefficients in ࣂ࢘ ǡ ࣂࢍ and ࣂ࢈ from
support ષ would be equal with high probability (or bounded within
a small value ߝ in a practical case); hence “sparsity reduction” can
෩ , as discussed in [6]. This is a process
be applied such that‫ ܭ‬൏  ‫ܭ‬
which involves extractingࣂࢉ from support ષsuch that the sum of
sparsities of innovations ‫ܭ‬௥௜ ൅ ‫ܭ‬௚௜ ൅ ‫ܭ‬௕௜ is least. Note here that
evenࣂ࢏ ’s can be intelligently chosen to help minimize‫ܭ‬௥௜ ൅ ‫ܭ‬௚௜ ൅
‫ܭ‬௕௜ , if any two are equal. However, such a situation is taken care by
the choice ofࣂࢉ . This joint representation, ࡿ, with least possible
sparsity is called “reduced sparsity representation” and can be
recovered back by JSM reconstruction method as given below.
෡ ൌ ܽ‫ ݊݅݉ ݃ݎ‬ԡࡿԡ૚ ࢙Ǥ ࢚Ǥ࢟ ൌ  ࢶ
෩ࢸ
෩ࡿ
ࡿ

(6)

்

where ࢟ ൌ  ൣ࢟࢘ ࢟ࢍ ࢟࢈ ൧ ‫  א‬Թ൫ெೝାெ೒ାெ್ ൯  is a vector formed by
෩ ൌ ‫ ܏܉ܑ܌‬ቀൣࢶ࢘ ࢶࢍ ࢶ࢘ ൧் ቁ is
individual ࢘ǡ ࢍ and࢈measurements, ࢶ
a matrix whose diagonal elements are the individual measurement
matrices ࢶ࢘ ‫ א‬Թெೝൈே ǡ ࢶࢍ ‫ א‬Թெ೒ൈே and ࢶࢍ ‫  א‬Թெ್ ൈே . Further,
෩ ൌ ሾ࡭࡮ሿ , where ࡭ ൌ ሾࢸࢸࢸሿ் and ࡮ ൌ ‫܏܉ܑ܌‬ሺሾࢸࢸࢸሿ் ሻ .
ࢸ
෩), the ROwing to reduced sparsity of joint representation (‫ ܭ‬൏  ‫ܭ‬
G-B joint recovery is advantageous over the naïve approach since

(i) the minimum number of measurements required for faithful
reconstruction is reduced; and (ii) conversely, for a fixed
෩ , ܿ ൎ4), the fidelity of
measurement (not sufficiently higher than c‫ܭ‬
reconstructed image would be superior. This approach can also be
extended for reconstructing the Bayer images from Bayer
measurements, which is the reason why we split G plane into two
separate G1 and G2 planes in Eqn. (4) so as to support an additive
model similar to Eqn. (5). However, we may note that unlike the
original ࢘ǡ ࢍ and ࢈ images,
the
Bayer
images
࢞ࡾ ǡ ࢞ࡳ૚ ǡ ࢞ࡳ૛ and࢞࡮ (from Bayer planes of Fig 1(a)) are not from
aligned pixels positions, which means the inter-correlation is
reduced, diminishing the efficacy of the baseline algorithm. This
necessitates a better correlation model and recovery algorithm for
reconstruction of Bayer images, which is presented next.
3.2. Extended Joint R-G-B Reconstruction (E-JSM)
Before we present the idea of E-JSM model, we would first like to
discuss some key aspects of on Bayer images w.r.t JSM model.
Under JSM of Eqn. (5) the “reduced sparsity representation” of the
Bayer images (࢞ࡾ ǡ ࢞ࡳ૚ ǡ ࢞ࡳ૛ and࢞࡮ ‫ א‬ԹேȀସ ) can be written as,
ࡿൌ

்
ൣࣂࢉ ࣂ࢏ࡾ ࣂ࢏ࡳ૚ ࣂ࢏ࡳ૛ ࣂ࢏࡮ ൧

‫ א‬ԹହேȀସ

(8)

ࣂࢉ

where,
is the common component extracted from a common
support ષ۰ of cardinality ‫ܭ‬஻௖ and ࣂ࢏ Ԣ‫ ݏ‬are innovation components.
While this seems reasonable, noting that Bayer images exhibit
reduced inter-correlation, we make the following observations,
(i) The common support ષ۰ of Bayer images is a smaller subset
compared to ષ original image i.e., Ͷ‫ܭ‬஻௖ Ȁܰ ൑ ‫ ܭ‬௖ Ȁܰ.
෩ (ષ
෩  ‫ ؿ‬ષ,
(ii) For the ࢘ǡ ࢍ and࢈ images, if we define the subset ષ
෩ ௖ )  such that ࣂ࢘ ሺ݊ሻ ൌ ࣂࢍ ሺ݊ሻ ൌ ࣂ࢈ ሺ݊ሻ or ࣂ࢘ ሺ݊ሻ ൎ
cardinality ‫ܭ‬
ࣂࢍ ሺ݊ሻ ൎ ࣂ࢈ ሺ݊ሻǡ ‫ א ݊׊‬ષ (or bounded within a small value ࢿ for a
෩ ۰‫ ܀‬for the Bayer
practical case), and an equivalent version ષ
௖
௖
෩஻ோ
෩஻ோ
෩ ௖ Ȁܰ.
), we may write, Ͷ‫ܭ‬
Ȁܰ ൑ ‫ܭ‬
images (cardinality ‫ܭ‬
Observations (i) and (ii) suggest that, even after sparsity swapping,
reduction in sparsity in the joint representation of Eqn.(8) is much
smaller than the case of original࢘ǡ ࢍ and࢈ images. However, since
the Bayer R, G, B are neighboring pixels, they exhibit sufficient
pair-wise correlation. So, in this sense, there is still redundancy in
innovation components ࣂ࢏ࡾ ǡ ࣂ࢏ࡳ૚ ǡ ࣂ࢏ࡳ૛ and ࣂ࢏࡮ , say if we consider
them, pair-wise. As an example, consider ࣂ࢏ࡾ andࣂ࢏ࡳ૚ ‫ א‬ԹேȀସ . For
these two innovation images, we may still identify a common
௜
) and its
support of non-zero coefficients ષ௜ୖୋభ (ฮષ௜ୖୋభ ฮ ൌ ‫ୖܭ‬ୋ
భ
଴
௜
௜
௜
࢏
෩ ୖୋ ( ฮષ
෩ ୖୋ ฮ ൌ ‫ܭ‬
෩ୖୋ ) such that ࣂࡾ ሺ݊ሻ ൌ ࣂ࢏ࡳ ሺ݊ሻ
subset ષ
భ

భ

଴

భ

૚

෩ ௜ୖୋ . Thus, we can find a common
(orࣂ࢏ࡾ ሺ݊ሻ ൎ ࣂ࢏ࡳ૚ ሺ݊ሻ)‫ א ݊׊‬ષ
భ
௖
component ࣂோீଵ by applying “pair-wise sparsity reduction” again,
rather than letting the burden on the joint choice of ࣂࢉ (and also
corresponding ࣂ࢏ ’s of Eqn.(8)). We may now define a new E-JSM
additive model considering a global common and all six pair-wise
common components as follows,
ሖ ࢏ࡾ
ࣂࡾ  ൌ  ࣂࢉ  ൅  ࣂ௖ࡾࡳ૚ ൅  ࣂ࢏ࡾࡳ૛  ൅  ࣂ௖ࡾ࡮  ൅  ࣂ
ሖ ࢏ࡳ
ࣂࡳ૚ ൌ  ࣂࢉ  ൅  ࣂ௖ࡾࡳ૚ ൅  ࣂ࢏ࡳ૚ ࡳ૛  ൅  ࣂࡳ௖ ૚࡮  ൅  ࣂ
૚
ሖ ࢏ࡳ
ࣂࡳ૛ ൌ  ࣂࢉ  ൅  ࣂ௖ࡾࡳ૛ ൅  ࣂ࢏ࡳ૚ࡳ૛  ൅  ࣂ௖ࡳ૛ ࡮  ൅  ࣂ
૛
ሖ ࢏࡮
ࣂ࡮  ൌ  ࣂࢉ  ൅  ࣂ௖ࡾ࡮  ൅  ࣂࡳ࢏ ૚࡮  ൅  ࣂ௖ࡳ૛ ࡮  ൅  ࣂ

(9)

Under this mode we have a “sparser” joint representation as,

1263

்

ࡿࡱ ൌ ൣࣂࢉ ࡿ௖ா ࡿ௜ா ൧ ‫ א‬ԹଵଵேȀସ

(10)

where, ࡿ௖ா ൌ ሾࣂ௖ோீଵ ࣂ௖ோீଶ ࣂ௖ோ஻ ࣂ௖ீଵீଶ ࣂ௖ீଵ஻ ࣂ௖ீଶ஻ ሿ  ‫ א‬Թ଺ேȀସ is the
vector
of
pair-wise
common
components
and
ሖ ࢏ࡾ ࣂ
ሖ ࢏ࡳ ࣂ
ሖ ࢏ࡳ ࣂ
ሖ ࢏࡮ ൧ ‫ א‬ԹସேȀସ is a vector of new innovation
ࡿ௜ா ൌ ൣࣂ
૚
૛
components. The recovery of ࡿࡱ given the measurements, ࢟࡮ ൌ
்

ൣ࢟ࡾ ࢟ࡳ૚ ࢟ࡳ૛ ࢟ࡾ ൧ , is achieved by the following l1 minimization,
෡ࡱ ൌ ܽ‫ ݊݅݉ ݃ݎ‬ԡࡿࡱ ԡ૚ ࢙Ǥ ࢚࢟࡮ ൌ  ࢶ
෩ ࡮ࢸ
෩ ࡮ ࡿࡱ
ࡿ

(11)

෩ matrix is given by ࢸ
෩ ࡮ ൌ ሾ࡭࡮࡯ሿ  ‫ א‬ԹସேൈଵଵேȀସ , where
Here ࢸ
࡭ ൌ  ሾࢸࢸࢸࢸሿ் corresponds to global common, ࡯ ൌ ‫܏܉ܑ܌‬ሺ࡭ሻ
for the innovation and the ࡮ matrix is a matrix whose columns
have inverse basis ࢸ at two locations corresponding to pair-wise
෩ ࡮ ൌ ࢊ࢏ࢇࢍሺൣࢶࡾ ࢶࡳ ࢶࡳ ࢶ࡮ ൧் ሻ .
common components and ࢶ
૚
૛
Having recovered ෡
ࡿࡱ as above, we can form the estimates of spatial
ෝࡳ૚ ǡ ࢞
ෝࡳ૛ and ࢞
ෝ࡮ ‫ א‬ԹேȀସ using an inverse
ෝࡾ ǡ ࢞
Bayer images ࢞
transform. The full r, g and b images can then be obtained using
any existing demosiacing techniques.

4. EXPERIMENTAL RESULTS
We designed experiments to simulate the imaging process as
illustrated in Fig. 1 on MATLAB platform. The measurement
matrices used were random sequences of -1/+1 (sampled from
uniform distribution) and also real numbers in the range [-1, 1]
with Gaussian distribution). We used the TV minimization
algorithm [8] in reconstruction and performed all our experiments
using 2D DCT basis. Given a high resolution color image, Bayer
color filtering was simulated by taking the proper R, G1,G2 and B
components. Then CS measurements through random matrices
were obtained. We tested our approaches (both E-JSM and JSM)
with various popular test images of different color and visual
content, and compared the performances with the approach of
independent CS reconstruction. Some sample test images are
shown in the Fig. 2. The performance results (Table 1, overall
PSNR) is evaluated for 30%, 25% Bayer measurements, with the
reference image being a Bayer filtered and demosaiced (through
bilinear interpolation) version of the raw test image. We note that
both E-JSM and JSM outperform the independent reconstruction in
all cases. The E-JSM outperformed JSM for most of the times (the
only exception is for the Goldhill). Furthermore, visually and
perceptually, the E-JSM approach was always found superior, with
better preserved image details and fewer color artifacts. This is
illustrated in Fig. 3 where in a cropped portion of some sample
images are shown in full resolution for visual comparison.

5. CONCLUSION AND FUTURE WORK
We introduced a new scheme for color image acquisition by a CS
camera, where we capture R, G and B measurements of alternate
pixels according to Bayer mosaic filter structure. In the process the
total measurements are reduced (Bayer measurements). Further, we
introduced a new joint reconstruction scheme that works on the
Bayer measurements and exploits the correlation between R, G and
B to produce better quality for the reconstructed color image.
Experiments demonstrated the advantage of the proposed method.
We are working on further improving the approach by explicitly
modeling the spatial shift of the R,G,B pixels in a Bayer mosaic.

6. REFERENCES
[1] E J. Candès and M B. Wakin “An Introduction to Compressive
Sampling”, IEEE Signal Processing Magazine, March 2008.
[2] D. L. Donoho, “Compressed sensing,” IEEE Trans. Inform.
Theory, vol. 52, pp. 1289–1306, July 2006.
[3] M.B. Wakin, J.N. Laska,
M.F. Duarte,
D. Baron, S.
Sarvotham. D. Takhar
K.F Kelly. R.G Baraniuk. “An
Architecture for Compressive Imaging” IEEE Image. Proc, pp1273-1276, Oct 2006.
[5] M. F. Duarte, M. A. Davenport, D. Takhar, J. N. Laska, T.
Sun, K.F. Kelly, R.G. Baraniuk “Single Pixel Imaging Via
Compressive Sensing” IEEE Sig. Proc. Magazine, Spl. Issue on
CS, March 2008.
[4] D. Takhar, J. N. Laska, M. B. Wakin, M. F. Duarte, D. Baron,
S. Sarvotham, K. F. Kelly, and R. G. Baraniuk, “A new
compressive imaging camera architecture using optical-domain

compression,” in Computational Imaging IV, vol. 6065, (San Jose,
CA), pp. 43–52, Jan. 2006.
[6] D. Baron, M. Wakin, M. Duarte, S. Sarvotham, and R.
Baraniuk, “Distributed Compressed Sensing”, Preprint, 2005,
Available online at http://www.dsp.ece.rice.edu/cs/
[7] J. Romberg “Imaging via Compressive Sampling”, IEEE Sig.
Proc Magazine, Special Issue on Compressive Sampling, March
2008.
[8] E. Candes and J. Romberg, “Practical signal recovery from
random projections,” 2005, preprint. [Online]. Available:
http://www.dsp.ece.rice.edu/cs/

. Table 1: Performance results on sample images of Figure 2.
CS PSNR**
Name
CS PSNR**
BM*
EJSM
JSM
BM*
EJSM
JSM
( %) PSNR** (dB) PSNR** (dB)
(dB)
( %) PSNR** (dB) PSNR**(dB)
(dB)
Lena
30
29.093
29.172
Lady
30
27.305
26.291
30.034
27.613
25
26.777
28.15
25
26.551
25.631
28.16
26.82
Pepper 30
28.395
28.077
Gold
30
29.689
27.838
29.714
29.838
Hill
25
25
26.947
26.628
28.639
26.927
28.027
28.891
Light
30
26.692
25.189
Girl
30
28.278
27.555
26.991
28.821
House
25
25.732
24.200
25
27.080
26.405
26.105
27.714
*BM: Bayer Measurements (section 2). An X% Bayer measurement means X/4% measurement data each on R & B planes, X/2% on G.
** PSNR here is measured between reconstructed image (CS/EJSM/JSM) and Bayer filtered, demosaiced (bilinear interploation) image.
Name

Fig. 2: Shows some sample
high
resolution
color
images used for testing.
From top right, Lena,
Peppers,
Light-House,
Lady, Gold-Hill and Girl.
The performance results on
these images are tabulated
in Table 1, for 30%, 25%
measurements on the Bayer
R, G1, G2 and B planes.
Fig. 3: Shows image sections & corresponding results. Column-wise: Col 1 is the Bayer sampled
and demoisaiced reference image (bilinear interpolation), Col 2,3 are joint R-G-B E-JSM, JSM
reconstructed images respectively. Col 4 is R,G,B independently reconstructed images. Row-wise:
cropped sections of original images, Lady, Light-House and Pepper in Rows 1,2 and 3 respectively.

1264

TactileFace: A System for Enabling Access to Face Photos
by Visually-impaired People
Nan Li
Zheshen Wang
Jesus Yuriar
Baoxin Li
Computer Science and Engineering, Arizona State University
{nanli2, zheshen.wang, jesus.yuriar, baoxin.li}@asu.edu
ABSTRACT

creating tactile facial images for visually-impaired people.
In this demonstration, we further improve the efficiency of
the original algorithm and build a prototype system
supporting instant capture, conversion and reproduction of
human face photos. While being prototypical in its current
form, by design the system serves to provide a novel and
intelligent interface that enables people who are blind to
have direct and independent access to an important media
type: human face images.

Face photos/Portraits play an important role in people's
social and emotional life. Unfortunately, this type of media
is inaccessible to people with visual impairment. We
propose a novel, prototypical system that was designed to
demonstrate the feasibility of bridging this accessibility gap
through automatic and realtime conversion of face images
into their tactile counterparts. Unlike conventional and
existing tactile conversion efforts, which are largely done
by human specialists, the proposed system serves to provide
an intelligent interface between blind computer users and
this important type of media, human face images.

SYSTEM AND DESIGN
Design Considerations

As mentioned previously, we aim at enabling independent
access to facial images for visually-impaired people via
automated creation of corresponding tactile graphics. There
are a few features which a desired system should have in
order to serve the needs of the target users.

Author Keywords

Human face/portrait, Tactile graphics, Interface.
ACM Classification Keywords

H.5.2
Information
Miscellaneous.

Interfaces

and

Presentation:

Supporting independent access: Different from existing
techniques which were proposed for assisting tactile
specialist in their manual conversion, the system needs to
be specifically designed for the end users who are blind.
Thus, it requires the conversion to be end-to-end and fully
automatic without requiring assistance of a sighted person.

General Terms

Design, Experimentation.
INTRODUCTION

Digital graphical content, such as digital images, has
become prevalent in the information era. Due to the
unfortunate loss of vision, people who are visually-impaired
are partially or completely deprived of the access to visual
contents. They have to rely on indirect means such as
textual descriptions to get some information from an
inherently visual media source. Tactile graphics are also
widely employed to render shapes, maps, and etc. However,
existing approaches to creation of tactile graphics are
mainly manual or semi-manual in nature and are in general
intended for sighted professionals to use. The process is
typically tedious and involving [2]. While face images are
important media that are widely used in applications such
as identification and social communication, they are
unlikely to be well reproduced by a typical manual process
of a tactile specialist. In recent years, efforts have been
devoted to automated approaches for visual-to-tactile
conversion. Some existing work can only handle simple
line-drawing graphics [1], and the system developed in [3]
relies on Photoshop for image simplification which still
requires non-trivial assistance from sighted people. In our
recent work [4-5], we proposed an automatic approach for

Supporting realtime access: For the system to be practically
useful, it needs to be able to create the end results in
realtime, just as what the existing screen-reading software
can do for a blind computer user. To this end, we aim at
building a realtime system that is the most involving: it
starts with images captured from a live camera and
produces the final tactile face images. If we make the
system work in realtime for this complex scenario, other
applications (e.g., with images given from a Web browser)
can be easily accommodated.
System Overview

Figure 1 illustrates a practical usage scenario of the
proposed interface with the real system setup in our lab.
Sample face images and the generated results are illustrated
in Figure 2 for showing the process of conversion. The
processing procedure is briefly introduced in the below.
Image Capture: As illustrated in Figure 1, assuming that a
user with vision loss gets to know a new friend, and he
wants to know how she looks like. A portrait image of the
friend can be instantly taken by using a low-end webcam
and sent for further processing. Alternatively, if the user has

Copyright is held by the author/owner(s).
IUI 2011, February 13–16, 2011, Palo Alto, California, USA.
ACM 978-1-4503-0419-1/11/02.

445

Figure 1. System overview.

a pre-stored portrait image of a person, he can simply send
it to the system for tactile conversion and reproduction (our
current system can only handle portrait images with clean
background).

users with visual impairment through haptic exploration.
Figure 2 -- third row illustrates the tactile printouts
generated from our demo system.
Evaluation: Both objective evaluation of face alignment
and subjective evaluation of the produced tactile faces with
both blind and blind-folded users were reported in [4-5].
Subjective evaluation showed that by touching the tactile
printouts from the system, most users were able to locate
the face components (e.g., eyes, nose, mouth), identify the
pose of the portrait and the gender of the people and
associate two pictures of the same person with different
poses or expressions among several tactile portraits.

Visual-to-tactile Conversion: As the core component of the
system, the visual-to-tactile face conversion unit takes a
visual portrait image (see Figure 2 -- first row) as input and
automatically generate a binary image (Figure 2 -- second
row) which is ready for tactile reproduction. More
specifically, the face region in the image is first detected by
a face detector. Then the face is modeled by an anchorpoint based deformable shape model, which is enriched by
local appearance patterns in terms of gradient profiles along
the shape. The generic face model learnt from a set of
training images is updated through Bayesian inference for
representing a new face image. Furthermore, to compensate
for the simplicity of the face model, edge segments of a
given image are used to enrich the basic face model in
generating the final binary image for tactile reproduction.
Details of the approach were described in [4-5].

SUMMARY

In this work, we propose to demonstrate an end-to-end
system for bridging the accessibility gap between visually
impaired people and human face images/portraits. The
prototype system was built upon our recent work, a novel
solution to visual-to-tactile conversion of human face
images. The system is fully automatic and the reproduction
procedure is done in realtime. The system can be potentially
set up in a disability resource center for providing an instant
service to local residents who are blind or integrated as a
portable device for personal/home usages of visuallyimpaired people.
ACKNOWLEDGEMENT

This work is supported in part by NSF grant #0845469.
REFERENCES
[1] Tactile graphics project:
http://tactilegraphics.cs. washington.edu, University of Washinton.
[2] P. Edman. Tactile Graphics. American Foundation for the
Blind, 1992.
[3] R. Ladner, M. Ivory, R. Rao, and S. Burgstahler. Automating
tactile graphics translation. International ACM SIGACCESS
Conference on Computers and Accessibility, 2005.
[4] Z. Wang and B. Li. A Bayesian Approach to Automated
Creation of Tactile Facial Images. IEEE Transactions on
Multimedia, 12(4):233–246, 2010.
[5] Z. Wang, X. Xu, and B. Li. Bayesian tactile face. IEEE
International Conference on Computer Vision and Pattern
Recognition, 2008.

Figure 2. Tactile face conversion: 1st row--original images; 2nd
row--generated binary images ready for tactile reproduction;
3rd row--tactile printouts from our demo system.

Tactile Reproduction: Tactile printout can be produced
through a tactile embosser (e.g. ViewPlus Cub Jr.
Embosser) or alternatively by using a regular printer plus a
thermal enhancer with swell-paper (Swell-paper, known as
Minolta or Microcapsule paper, is regular paper with a
special coating of heat sensitive chemicals. When exposed
to heat, embedded microcapsules of alcohol in the regions
with black ink burst and make the surface of the paper swell
up). Reproduced tactile printouts can be directly used by

446

DETECTING TEXT IN FLOOR MAPS USING HISTOGRAM OF ORIENTED GRADIENTS
Hima Bindu Maguluri, Qiongjie Tian and Baoxin Li
Computer Science and Engineering, Arizona State University, Tempe, USA
ABSTRACT
Automatic detection of text labels in maps is essential for
applications requiring automatic map understanding. This
task is challenging due to factors such as varying font size
and style, slanted words/phrases, and interfering graphics that
are similar to text. This paper presents an approach for text
detection in indoor floor maps. We exploit the difference
in spatial frequency of edge orientations between text and
non-text regions through Histogram of Oriented Gradients
(HOG) features, and design a gradient-filtered Support Vector Machine (SVM) classifier based on such features. Special
care was taken in conditioning the data for proper training
of the classifier. The proposed approach was evaluated on a
data set that had been collected and manually labeled. Experimental results show that the proposed method attained
improved performance, outperforming a couple of reference
methods/systems.
Index Terms— Text Detection, Histogram of Oriented
Gradients, Support Vector Machine.
1. INTRODUCTION
There are many applications that require automatic map understanding, a key task of which is accurate text detection
from maps. Examples include querying map images by keywords and making maps accessible to the visually impaired.
In the field of computer vision and image understanding,
many methods have been proposed for text detection in images (e.g., [1] [2] [3]). While advances have been made,
typical existing approaches do not deliver the desired accuracy demanded by practical applications. Further, many
approaches were reported in different contexts and were evaluated with different data sets, making it difficult to perform
comparative assessment in understanding the effectiveness of
the methods.
In this work, we focus on the task of detecting text from
floor maps, with an ultimate goal of assisting map understanding (and thus high accuracy in detection is the key).
Factors such as varying input resolution, diverse font size and
style, and slanted text are typical of floor maps from different
The work was supported in part by a grant (#0845469) from the National
Science Foundation. Any opinions expressed in this material are those of the
authors and do not necessarily reflect the views of the NSF.

978-1-4799-0356-6/13/$31.00 ©2013 IEEE

sources, and they often baffle existing approaches that rely
on a small training set or a set of fixed rules. We explore the
spatial frequency of edge orientations using HOG features,
and design a gradient-filtered SVM classifier based on such
features. Training the classifier was facilitated by conditioning the data to provide better labels. Further, recognizing that
there is no standardized data set for comparing the performance of different approaches, we collected and manually
labeled a data set for our experiments. The experimental
results show that the proposed method attained improved
performance, outperforming a couple of reference methods/systems. The data set is available for other researchers to
evaluate and compare their method.
2. RELATED WORK
Most of the recent work on text detection has been focused on
natural images [3] [4] [5] [6] [7] [8] [9]. Such algorithms are
in general not tuned for text detection in floor maps, which
exhibit different properties from natural images. For example, in a floor map, there are often graphics that are in many
ways similar to texts. There are also some methods for text
detection in graphic background. In [10], extraction of text
regions in multiple colors and complex backgrounds was discussed. In [11], an approach to separate text from the graphics
was presented, focusing on recovering text that has been overlapped by graphics. In [12], a method was proposed to detect
text regions by thresholding local frequency features. Unfortunately, while these methods have their potential strength and
weakness, there is no systematic comparison of their accuracy
based on a common test set, and thus little can be concluded
on their performance in general. For example, it is difficult to
expect that the technique of thresholding frequency features
can differentiate text from other text-like graphics, which occur frequently in floor maps.
We propose to use edge orientation information as the key
feature since texts have distinct shapes when compared with
general graphics. This is a proper feature especially since the
floor maps are largely binary in nature. This is achieved by
computing the HOG features ([13]). Further, to avoid setting
any hard threshold, which is difficult to do, we learn a SVM
classifier based on the features. Also recognizing the potential
of the gradient distribution in distinguishing texts from their
surroundings, the SVM classifier is modulated by a gradient-

1932

ICASSP 2013

based filter. To ensure proper training, we also propose a few
data conditioning techniques.
3. THE PROPOSED METHOD
The proposed method consists of three major processing
steps, which are detailed in the following sub-sections.
3.1. Extraction of HOG features
As a preprocessing step, a floor map image is first converted
into a grayscale version and then smoothened by using a
Gaussian filter to avoid excessive noise in gradient computation. A small Gaussian kernel is used to preserve major
edges. The magnitude and orientation of the gradient at
each pixel are obtained by convolving the smoothened image
with derivative filters along the horizontal and vertical directions. In theory the gradient magnitudes in homogeneous
regions of the image are zero. In practice, due to factors like
compression artifacts, even pixels of visually homogeneous
regions may have small non-zero gradient magnitude, and
the corresponding gradient orientation is not useful for text
detection. To alleviate this effect, a constant threshold k has
been applied to the gradient magnitude, as shown in Equation
1, where M is the gradient map (magnitude) of the given
image. Also, the text regions become more obvious after this
thresholding step as shown in Figure 1.
(
0
, if Mi,j ≤ k
Mi,j =
, ∀i, j
(1)
Mi,j , if Mi,j > k
For each pixel, a n × n window centred at the given pixel
is considered to construct a d dimensional HOG feature. The
height of each bin in the histogram is calculated as the number of pixels from the n × n window with non-zero gradient magnitude and with orientations that fall in the range of
the particular bin. The number of bins d has been chosen in
such a way to capture the orientation information in terms of
smaller ranges because text regions have wide variety of angles. This makes the histograms suitable for differentiation
between text and non-text regions. Figure 2 illustrates examples of the histograms corresponding to text and non-text
regions respectively.
3.2. Data conditioning

Fig. 1. Visualization of magnitude of gradients before (left)
and after (right) thresholding.

Fig. 2. Examples of HOG features corresponding to text (left)
and non-text regions (right).
train a subsequent classifier, we introduce a data conditioning
step to identify and relabel such pixels, thus providing better
labels for training. This is formally done by Equation 2,
(
0 , if Mp (i, j) = 0
Xi,j =
, ∀i, j
1 , if Mp (i, j) > 0
Np =

n X
n
X

Xi,j

(2)

i=1 j=1

(
Cl =

−1 , if Np < t
l
, if Np ≥ t

where Mp represents thresholded gradient magnitude values
of a n × n patch centered at the given pixel, Np represents the
total number of pixels with non-zero gradient values in the
patch, and t is a threshold. l and Cl are ground truth and conditioned labels respectively, as visualized in Figure 3, where
it can be seen that the conditioned labels represent text pixels
more accurately than the initial ground truth.

The histogram features corresponding to text and non-text
pixels are labeled as +1 and −1 respectively by selecting
rectangular boundaries that surround the text. An example
of ground truth is shown in Figure 3. Due to variations in
height and shape of the text characters, sometimes pixels that
are labeled as +1 and located inside the rectangle surrounding some texts can have very small number of edge orientations in its neighborhood and thus have features similar to
non-text regions. Examples include pixels lying in the borders of rectangles or pixels in between letters. To accurately

1933

Fig. 3. Examples of ground truth and conditioned labels.

3.3. Gradient-filtered SVM classification
With the labeled features, an SVM classifier is trained and
used for predicting text regions in test images. Example of
the SVM-predicted result can be seen in Figure 4. It is observed that the detected regions are not clean enough and they
include many surrounding non-text pixels. This gives rise to
many false detections and poor localization with each of output rectangle including too many words along with surrounding noise. As the gradient distribution has the potential to
detect accurate boundaries, the SVM-predicted output is filtered with non-zero gradient magnitude values as shown in
Equation 3,
(
0 , if Mi,j = 0
Hi,j =
, ∀i, j
1 , if M i, j > 0
(3)
Yi,j = Pi,j ∗ Hi,j , ∀i, j
where M represents the thresholded gradient image from
Equation 1, P the SVM-predicted output, H the gradientbased filter and Y the filtered output. This eliminates
many false detections and enhances localization by providing
tighter boxes surrounding texts.
To further eliminate any false positives or noise resulting from lines or large connected components, the output Y
is post-processed with filters obtained from a line detection
module and detection of large connected components.

Resultsf rom :

Precision

Recall

F

Our algorithm
Algorithm
of
[12]

85.8%
45.7%

57.9%
90.6%

69.2%
60.8%

Table 1. Comparison of pixel level accuracies from our algorithm and from [12].
The proposed method has been evaluated on our data set
and compared with results from the text detection method
used in [12]. We chose the method from [12] for comparison since it was designed for a very similar problem (text
detection from on-line maps). Out of the 30 images, 19 images were used for training the SVM and the remaining 11
were used for testing. Learning SVM model and classification have been achieved using LIBSVM tool [14]. Window
size n was chosen as 9, number of bins d as 16 and the constants k and t as 20 and 15 respectively. Experimental results
are presented at two levels: pixel level and word level.
4.1. Pixel-level evaluation
For each image, the precision and recall values were calculated in terms of number of pixels and then, averages of the
precision and recall values over all the 11 images were calculated. Comparison of the values of precision, recall and
the standard F1 score, f from the proposed method and the
method of [12] is shown in Table 1. It can be seen that our
algorithm has higher precision value. Though the numerical
value of recall is low for our algorithm, Figure 5 shows that
the output obtained by our algorithm is cleaner and more accurate (both in terms of detecting text regions and eliminating
false detections).

Fig. 4. Example of SVM-predicted result and gradientfiltered result.
4. EXPERIMENT DESIGN AND RESULTS
There is strong need for standardized data set of maps to ensure appropriate comparison of various methods on a common test set. We created a data set, by collecting floor maps
of 30 libraries and manually marking the ground truth. The
data set has been created in such a way that it includes diverse images with different variations in structure of the building, image resolution, average text height etc. This dataset is
available at http://www.public.asu.edu/˜bli24/
icassp2013.html for any interested researcher to use.
The current version include only library floor maps and the
dataset may be updated with other types of floor maps in the
future.

Fig. 5. Input (top left), ground truth (top right), detected result
from [12] (bottom left) and our algorithm (bottom right).
The reason for the recall value being low is that the ground
truth is a solid rectangle with its inside filled but we have refined our output by gradient filtering to remove unwanted pixels as explained in Section 3.3. The decrease in recall value
when calculated in terms of number of pixels is due to the

1934

Resultsf rom :

Precision

Recall

Our algorithm
Algorithm used in [12]

67.4%
50.0%

88.8%
61.0%

Table 2. Comparison of word level accuracies from our algorithm and [12].
difference in the nature of the output mask we obtained (Instead of solid filled output, we obtained clean output which
enhances localization) and results in Section 4.2 show that
this difference in nature of the mask achieves better localization and thus higher word level accuracy. The nature of our
output mask is shown as Filtered result in Figure 4 and it can
be noticed that the filtered result has clear boundaries. This
demonstrates that the low recall value in Table 1 actually indicates that our algorithm performs better than [12] by generating a cleaner output for better localization. We also plotted the
ROC curve and observed that the AUC (Area Under Curve)
was 0.942.
4.2. Word-level evaluation
To obtain meaningful recognition results on detected text regions, accurate localization of text is essential. The detected
boxes should be tight enough, so that they do not include surrounding graphics. For this purpose, we calculate the wordlevel accuracy to evaluate the detection and localization. We
use coordinates of the ground truth rectangles to evaluate the
bounding boxes obtained by our algorithm and we compare
with the results from [12].
For each ground truth text box, a recall value is calculated
as the ratio of the overlapping area between the ground truth
box and the detected text boxes to the area of the ground truth
text box. For each detected text box, a precision value is calculated as the ratio of overlapping area between the detected
box and the ground truth boxes to the area of the detected text
box. The average of the precision and recall values over all the
text boxes from the 11 test images were calculated. Comparison of the precision and recall values at word level is shown
in Table 2 and it can be seen that our method has significantly
higher precision and recall values.
These results support our explanation in Section 4.1 that
lower recall value in Table 1 is due to cleaner mask obtained
by removing unwanted pixels and thus our algorithm enhances localization and achieves higher word level accuracy.
Figure 6 shows an example of the detected text boxes from
[12] and from our algorithm.
4.3. Comparison with OMNIPAGE
We now present results from a commercial OCR software,
OMNIPAGE 2007. We observed the results from OMNIPAGE on all the test images and in most of the cases, the

Fig. 6. Localized text boxes from [12] (left) and our algorithm
(right).
software failed to give reasonable performance. As an example, Figure 7 shows the screen shot of results obtained when
a floor map is given as input to OMNIPAGE. It can be seen
that the output text file has very few meaningful words. The
brown polygonal regions marked on the input image show
the detected regions. The software could not detect accurate
boundaries of text regions from the input image and even in
the detected regions, it could not return meaningful words due
to the presence of many interfering lines and graphics. It was
also observed that when the cropped regions of detected text
boxes obtained from our algorithm were given as input, the
number of meaningful words were much higher. This shows
the usefulness of an accurate text detection and localization
method in recognizing texts for automatic map understanding.

Fig. 7. OMNIPAGE results for an input floor map.

5. CONCLUSIONS AND FUTURE WORK
In this paper, we reported an approach to text detection in
floor maps. We analyzed the challenges involved and the deficiencies of typical existing approaches. Then we presented
our method using edge orientation information in the form
of HOG features and a gradient-filtered SVM classifier. Experimental results demonstrate the usefulness of selected features and robustness of the proposed method even in handling
slanted text in low-resolution images. For future work, we
plan to extend the test dataset and also incorporate OCR feed
back into our system to eliminate false detection and recover
missing regions of partially detected text boxes.

1935

6. REFERENCES
[1] Chen, D. and Bourlard, H. and Thiran, J.P., “Text identification in complex background using svm,” in Computer
Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference
on. IEEE, vol., pp., II–621.
[2] Pan, Y.F. and Hou, X. and Liu, C.L., “A hybrid approach
to detect and localize texts in natural scene images,” Image
Processing, IEEE Transactions on, vol. 20, no. 3, pp. 800–
813, 2011.
[3] Minetto, R. and Thome, N. and Cord, M. and Stolfi, J.
and Précioso, F. and Guyomard, J. and Leite, NJ, “Text
detection and recognition in urban scenes,” in Computer
Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on. IEEE, pp., 227–234.

[12] Wang, Z. and Li, B. and Hedgpeth, T. and Haven, T.,
“Instant tactile-audio map: enabling access to digital maps
for people with visual impairment,” in Proceedings of the
11th international ACM SIGACCESS conference on Computers and accessibility. ACM, pp., 43–50.
[13] Dalal, N. and Triggs, B., “Histograms of oriented gradients for human detection,” in Computer Vision and Pattern
Recognition, 2005. CVPR 2005. IEEE Computer Society
Conference on. IEEE, vol., pp., 886–893.
[14] Chang, C.C. and Lin, C.J., “Libsvm: a library for support vector machines,” ACM Transactions on Intelligent
Systems and Technology (TIST), vol. 2, no. 3, pp. 27, 2011.

[4] Escalera, S. and Baró, X. and Vitrià, J. and Radeva, P.,
“Text detection in urban scenes,” in Proc. Conf. on Artificial Intelligence Research and Development, pp., 35–44.
[5] Neumann, L. and Matas, J., “Real-time scene text localization and recognition,” in Computer Vision and Pattern
Recognition (CVPR), 2012 IEEE Conference on. IEEE,
pp., 3538–3545.
[6] Epshtein, B. and Ofek, E. and Wexler, Y., “Detecting text
in natural scenes with stroke width transform,” in Computer Vision and Pattern Recognition (CVPR), 2010 IEEE
Conference on. IEEE, pp., 2963–2970.
[7] Chen, X. and Yuille, A.L., “Detecting and reading text
in natural scenes,” in Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE
Computer Society Conference on. IEEE, vol., pp., II–366.
[8] Ezaki, N. and Bulacu, M. and Schomaker, L., “Text detection from natural scene images: towards a system for
visually impaired persons,” in Pattern Recognition, 2004.
ICPR 2004. Proceedings of the 17th International Conference on. IEEE, vol., pp., 683–686.
[9] Ye, Q. and Jiao, J. and Huang, J. and Yu, H., “Text detection and restoration in natural scene images,” Journal of
Visual Communication and Image Representation, vol. 18,
no. 6, pp. 504–513, 2007.
[10] Tian, Y.L. and Yi, C. and Arditi, A., “Improving computer vision-based indoor wayfinding for blind persons
with context information,” Computers Helping People with
Special Needs, vol. 18, no. 6, pp. 255–262, 2010.
[11] Cao, R. and Tan, C., “Text/graphics separation in maps,”
Graphics Recognition Algorithms and Applications, vol.
18, no. 6, pp. 167–177, 2002.

1936

2013 IEEE Conference on Computer Vision and Pattern Recognition

Relative Hidden Markov Models for Evaluating Motion Skills
Qiang Zhang and Baoxin Li
Computer Science and Engineering
Arizona State Univerisity, Tempe, AZ 85281
qzhang53,baoxin.li@asu.edu

Abstract

uation and training. For example, [4] utilized control trajectories and motion capture data for human skill analysis,
[20] reported motion skill analysis in sports using data from
motion sensors, [18] studied computational skill rating in
manipulating robots, and [15] considered hand movement
analysis for skill evaluation in console operation.

This paper is concerned with a novel problem: learning temporal models using only relative information. Such
a problem arises naturally in many applications involving
motion or video data. Our focus in this paper is on videobased surgical training, in which a key task is to rate the
performance of a trainee based on a video capturing his
motion. Compared with the conventional method of relying
on ratings from senior surgeons, an automatic approach to
this problem is desirable for its potential lower cost, better objectiveness, and real-time availability. To this end,
we propose a novel formulation termed Relative Hidden
Markov Model and develop an algorithm for obtaining a
solution under this model. The proposed method utilizes
only a relative ranking (based on an attribute of interest)
between pairs of the inputs, which is easier to obtain and
often more consistent, especially for the chosen application
domain. The proposed algorithm effectively learns a model
from the training data so that the attribute under consideration is linked to the likelihood of the inputs under the
learned model. Hence the model can be used to compare
new sequences. Synthetic data is ﬁrst used to systematically
evaluate the model and the algorithm, and then we experiment with real data from a surgical training system. The experimental results suggest that the proposed approach provides a promising solution to the real-world problem of motion skill evaluation from video.

Among those ﬁelds, surgery is one domain where motion expertise is of the primary concern. Often a surgeon has to go through lengthy training programs that
aim at improving his/her motion skills. As a result,
simulation-based training platforms have been developed
and widely adopted in surgical education. For example, the Fundamentals of Laparoscopic Surgery (FLS) Box
(www.flsprogram.org) has practically become a standard training platform for minimally-invasive surgery. Accordingly, computational approaches have been developed
for motion skill analysis on such training platforms. Recognizing the sequential nature of motion data, many analysis
approaches utilize state-transition models, such as the Hidden Markov Model (HMM). For example, [14] provided an
HMM-based method to evaluate surgical residents’ learning curve. The method ﬁrst constructs different HMMs for
each different levels of expertise, and then calculates a probability distance between the expert and a novice resident.
The magnitude of the probability distance is used to rate
the level of the novice resident. HMM was also adopted
in [7] to measure motion skills in surgical tasks, where the
video is ﬁrst segmented into basic gestures based on velocity and angle of movement, with segments of the gestures
corresponding to the states of an HMM.

1. Introduction

One practical difﬁculty in these approaches is that they
require the skill labels for the training data since the HMMs
are typically learned from data of each skill level. Labeling
the skill of a trainee is currently done by senior surgeons,
which is not only a costly practice but also one that is subjective and less quantiﬁable. Thus it is difﬁcult, if not impossible, to obtain sufﬁcient and consistent skill labels for a
large amount of data for reliable HMM training. This problem has also been encountered in other ﬁelds such as image
classiﬁcation. For example, in [12], it was argued that using

Understanding human motion is an important task in
many ﬁelds including sports, rehabilitation, surgery, computer animation and dance. One key problem in such applications is the analysis of skills associated with body motion.
In domains such as dance, sports and surgery, the motion of
experts differs considerably from that of novices. Sensory
data that capture such motion may be analyzed to provide
a computational understanding of such differences, which
may in turn be used to facilitate tasks such as skill eval1063-6919/13 $26.00 © 2013 IEEE
DOI 10.1109/CVPR.2013.77

546
548

binary label to describe the image is not only too restrictive
but also unnatural and thus relative visual attributes were
used and classiﬁers were trained based on such features.
Relative information has also been used in other applications, e.g., distance metric learning [16], face veriﬁcation
[9], and human-machine interaction [13].
In this paper, we propose a novel formulation termed
Relative Hidden Markov Model and develop an algorithm
for obtaining a solution under this model. The proposed
method utilizes only a relative ranking (based on an attribute
of interest, or motion skill in the surgical training application) between pairs of the inputs, which is easier to obtain
and often more consistent. This is especially useful for the
applications like video-based surgical training, where the
trainees go through a series of training sessions with their
skills get improved over time, and thus the time of the sessions would already provide a natural relative ranking of the
skills at the corresponding time. The proposed algorithm
effectively learns a model from the training data so that the
attribute under consideration (i.e., the motion skill in our
application) is linked to the likelihood of the inputs under
the learned model. The learned model can then be used to
compare new data pairs. For evaluation, we ﬁrst design synthetic experiments to systematically evaluate the model and
the algorithm, and then experiment with real data captured
on a commonly-used surgical training platform. The experimental results suggest that the proposed approach provides
a promising solution to the real-world problem of motion
skill evaluation from video.
The key contribution of the work lies in the novel formulation of learning temporal models using only relative information and the proposed algorithm for obtaining solutions
under the formulation. Additional contributions include the
speciﬁc application of the proposed method to the problem
of video-based motion skill evaluation in surgical training,
which has seen increasing importance in recent years.

combines SVM with HMM to improve the discrimination
power of the learned model. These methods are “supervised” in nature, and thus the labeling of the state sequence
is required for the training data, which limits their practical
use. In [17], another discriminative learning method for
HMM was proposed, which only requires the labels of
the training sequences. The method initializes the HMMs
with maximum-likelihood method and then updates the
models with SVM. One drawback is that, the updated
models do not always lead to valid HMMs, which could be
problematic for a physics-driven problem where the model
states have real meanings (like the gesture elements in [7]).
Our proposed method requires neither the labeling of the
states nor the class label for the training sequences, which
are difﬁcult to obtain or even not accessible in applications.
Instead, only a relative ranking of the training data is used,
and the resultant model is a valid HMM.
Learning with relative information: Several methods
for learning with relative information have been proposed
recently. In [16], a distance metric is learned from relative
comparisons. Considering the limited training examples
for object recognition, [19] proposes an approach based on
comparative objective similarities, where the learned model
scores high for objects of similar categories and low for objects of dissimilar categories. In [9], comparative facial attributes were learned for face veriﬁcation. The method of
[12] learns relative attributes for image classiﬁcation and
the problem is formulated as a variation of SVM. Similar
idea was also been used in [13] for the purpose of humanmachine interaction. In [8], relative attributes feedback,
e.g., “Shoe images like these, but sportier”, is used to improve the performance of image search. Relative information between scene categories has also been used to enhance
the performances of scene categorization in [6]. These approaches are mostly for image-based attributes, whereas our
current task is on modeling sequential data, for which it is
natural to assume that the most relevant attributes (e.g., motion skills) are embedded in a temporal structure. This is
what our proposed method attempts to address.

2. Related Work
In this section, we review two categories of existing
work, discriminative learning for hidden Markov models
and learning based on relative information, which are most
related to our effort. Distinction between our proposed
method and the reviewed work will be brieﬂy stated.
Discriminative learning for HMM: Maximumlikelihood methods for learning HMM (e.g., the forwardbackward algorithm) in general do not guarantee the
discrimination ability of the learned models. To this end,
several discriminative learning methods for HMM have
been proposed. In [3], a discriminative training method
for HMM was proposed based on perceptron algorithms.
The methods iterates between the Viterbi algorithm and the
additive update of the models. Hidden Markov Support
Vector Machine (HM-SVM) was proposed in [1], which

3. Basic Notations of HMM
In this section, we brieﬂy describe HMM and introduce
some basic notations that will be used later. An HMM
can be deﬁned by a set of parameters: the initial transition
probabilities π ∈ RK×1 , the state transition probabilities
A ∈ RK×K and the observation model {φk }K
k=1 , where K
is the number of states. There are two central problems in
HMM: 1) learning a model from the given training data; and
2) evaluating the probability of a sequence under a given
model, i.e., the decoding problem.
In the learning problem, one learns the model (θ) by

549
547

maximizing the likelihood of the training data (X):


p(Xi |θ) ∼ max
log p(Xi |θ)
θ∗ : max
θ

θ

Xi ∈X

models is trained using the training data of each category
independently. That is, explicit class labels are required for
each training sequence. The proposed model has the following unique features:

(1)

Xi ∈X

• The model does not require explicit class labels. What
needed is only a relative ranking.
• The model explicitly considers the ranking constraint
between given data pairs, whereas independentlytrained HMMs in existing methods can’t guarantee it.
• Only one model is learned for the entire set of data.
There are two beneﬁts: more data for training and less
computation during testing.

where X is the set of i.i.d. training sequences.
One efﬁcient solution to the above problem is the
well-known Baum-Welch algorithm [2]. Another scheme,
namely the segmental K-means algorithm [5], may also be
used to seek a solution, and it has been shown that the likelihoods under models estimated by either of the two algorithms are very close [5]. When the training data include
sequences of multiple categories, multiple models would be
learned and each model will be learned from data of each
category independently.
In the decoding problem, given a hidden Markov model,
one needs to determine the probability of a given sequence
X being generated by the model. Generally we are more interested in the probability associated with the optimal state
sequence (z∗ ), i.e., p(X, z∗ |θ) = maxz p(X, z|θ). The optimal state path can be found via the Viterbi algorithm. To
use HMM in classiﬁcation, we ﬁrst compute the probability of the given sequence drawn from each model, then we
choose the model yielding the maximal probability.

Our method is also different from the existing work on
learning with relative attributes in that it models sequential
data and the relative ranking information is capsulated in a
temporal dynamic model of HMM (albeit new algorithms
are thus called for), which has demonstrated performance
in modeling physical phenomena like human movements.
In the following subsections, we present two instantiations of the general model expressed in Eqn. (2), and develop the corresponding algorithms in each case. It will
become clear that the ﬁrst model (Sec. 4.1), while being
intuitive, has some practical difﬁculties, which motivated
us to develop the improved model of Sec. 4.2. Both models/algorithms are presented (and evaluated later in Sec. 5)
for the progressive nature of the methods and for facilitating
the understanding of the improved model and algorithm of
Sec. 4.2, which is the recommended solution.

4. Proposed Method
Based on the previous discussion, we are concerned with
a new problem of learning temporal models using only relative information. This is a problem arising naturally in
many applications involving motion or video data. In the
case of video-based surgical training, the focus is on learning to rate/compare the performance of the trainees from
recorded videos capturing their motion. To this end, in
recognition of some fruitful trials of HMMs in this application domain, we propose to formulate the task as one of
learning a Relative Hidden Markov Model, which not only
maximizes the likelihood of the training data, but also maintains the given relative rankings of the input pairs. In its
most basic form, the proposed model can be formally expressed as (following the notations deﬁned in Eqn. (1))

p(Xi |θ)
(2)
θ : max
θ

4.1. The Baseline Model
One intuitive choice of the score function in Eqn. (2) is
the data likelihood, i.e., F (Xi , θ) = p(Xi |θ). With this, the
formulation in Eqn. (2) can be rewritten as

θ : max
p(Xi |θ)
(3)
θ

Xi ∈X

s.t. : p(X |θ) > p(Xj |θ), ∀(i, j) ∈ E
i

It has been proved in [11] that, the marginal likelihood
is dominated by the likelihood with the optimal path and
their difference decreases exponentially with regarding to
the length (number of frames) of sequence. This idea was
used in segmental K-means algorithm and similarly we can
approximate the marginal data likelihood p(X|θ) by the
likelihood with optimal path p(X, z∗ |θ) (when there is no
ambiguity, we will use z for z∗ ), which can be written as:

Xi ∈X
i

s.t. : F (X , θ) > F (Xj , θ), ∀(i, j) ∈ E
where F (X, θ) is a score function for data X given by model
θ, which is introduced to maintain the relative ranking of
the pair Xi and Xj , and E is the set of given pairs with
prior ranking constraint. Different score functions may be
deﬁned, as described in the following subsections.
From this formulation, the difference between the proposed method and any of the existing HMM-based methods
is obvious. In an existing HMM-based method, a set of

log p(X, z|θ)

=
+

log p(X1 |φz1 ) + log π(z1 )
T


[log p(Xt |φzt ) + log A(zt |zt−1 )]
(4)

t=2

If we assume
D a multinomial observation model, i.e.,
p(Xt |φzt ) = d=1 φzt (l)Xt (l) , where D is the dimension

550
548

of each frame, Xt (l) is the lth dimension of Xt and φzt is
the parameters of observation model with State zt . We further deﬁne the following variables for each sequence Xi :
ni ∈ RK×1

:

Oi ∈ RK×D

:

describe the proposed learning algorithm:
The Baseline Algorithm
Input: X, E, ρ, γ
Output: θ
Initialization: Initialize θ (and ψ) via ordinary HMM
learning algorithm;
while NOT terminated
Compute the optimal path z for each sequence;
Update the model ψ according to Eqn. 8;
end
Convert ψ to θ;

ni (k) = δ(zi1 = k)

Oi (k, d) =
Xit (d)
t:zt =k

Mi ∈ RK×K

:

Mi (k, l) =

T


δ(zit−1 = k)δ(zit = l)

t=2

where δ(·) is Dirac Delta function. Then the log likelihood
with the optimal path can be written as:
log p(Xi , zi |θ)

=



ni (l) log π(l) +

l

+





After the model is learned, it can be used to a testing pair:
For each sequence we evaluate the data likelihood via
the Viterbi algorithm and use the logarithm of the data
likelihood as the score of the data. By deﬁnition, the
obtained scores can be used to compare the pair.

Mi (k, l) log A(k, l)

k,l

Oi (k, d) log φk (d)

k,d

=

ψ T yi

4.2. The Improved Model

(5)

In the model described in Eqn. 8, we compare the logarithm of the data likelihood, which is, according to Eqn.
4, roughly proportional to the length of the data. Thus a
shorter sequence is likely to have a larger score. This means
that the learned model would be biased towards the shorter
sequences. If the observation describes a long, periodic
event, e.g., repeating an action multiple times within a sequence, we may consider normalizing the logarithm of the
data likelihood by the number of frames of the observation.
However, this cannot be applied directly for non-periodic
observations.
To overcome the above practical problem, we consider
an improved version. Recall that in HMM, we classify a
sequence based on the model with which the sequence gets
the maximal likelihood, i.e., it is the ratio of data likelihood with different models that decides the label of the
1)
data. For example, if log p(X,ẑ|θ
p(X,z̃|θ2 ) > 0, then we assign
X to Model θ1 . Thus we propose to use the ratio of the
data likelihoods of two HMMs as the score function, i.e.,
1)
F (X, θ) = log p(X,ẑ|θ
p(X,z̃|θ2 ) , where we “partition” the original
model into two models (or, effectively, we train a pair of
HMMs simultaneously). This results in the following improved model:


log p(Xi , ẑi |θ1 ) +
log p(Xj , z̃j |θ2 )
θ1 , θ2 : max

where ψ = [log π; vec(log A); vec(log φ)], yi =
[ni ; vec(Mi ); vec(Oi )] and vec converts matrix to vector.
With these, Eqn. 3 can be ﬁnally written as

yi
(6)
ψ : max ψ T
ψ∈Ω

i:Xi ∈X
T j

ψ y ≥ ψ y + ρ ∀(i, j) ∈ E
T i

s.t.

where ρ ≥ 0 deﬁnes the required margin between the logarithms of likelihood for a pair of data and Ω deﬁnes the set
of valid parameters for the hidden Markov model, i.e.:

eψ(i) = 1 (7)
ψ(i) ≤ 0 ;
i:ψ(i)∈log(π)



eψ(i) = 1



;

i:ψ(i)∈log(Aj )

eψ(i) = 1

i:ψ(i)∈log(φj )

where i : ψ(i) ∈ log(Aj ) is the set of the indexes which
corresponds to the jth row of matrix A.
For the model in Eqn. 3, we assumed that every pairwise ranking constraint provided in the data is correct (or
valid). However, in real data, there may be outliers in such
training pairs. To handle this, we further introduce some
slack variables , and relax the pair-wise ranking constraint
as ψ t y i + ij ≥ ψ t y j + ρ, ∀(i, j) ∈ E. Accordingly Eqn. 6
can be written as following:


yi − γ
ij
(8)
ψ : max ψ T
ψ∈Ω

Xi ∈X

θ1 ,θ2

−γ



j∈Ξ2

ij

(i,j)∈E

(i,j)∈E

s.t. :

ψ y + ij ≥ ψ y + ρ ∀(i, j) ∈ E
ij ≥ 0 ∀(i, j) ∈ E

where γ is the weight for the penalty term (i,j)∈E ij .
For initialization, we can set ij = 0. Now, we are ready to
s.t.

T i

i∈Ξ1

t j

p(Xi , ẑi |θ1 )
p(Xj , ẑj |θ1 )
+ ij ≥ log
+ρ
j j
p(X , z̃ |θ2 )
p(Xj , z̃j |θ2 )
(9)
ij ≥ 0 ∀ (i, j) ∈ E

log

where Ξ1 is the set of data associated with Model θ1
(Ξ2 for Model θ2 ), ẑi is the optimal path for sequence

551
549

xi with Model θ1 and z̃i for optimal path with Model
θ2 . The model in Eqn. 9 can also be written as the
standard form in Eqn. 8 with similar technique as described in Sec. 4.1, and thus the details are omitted.
The corresponding improved algorithm is given below:

nonlinear equality constraints for the baseline model (or the
improved model). However, the Hessian (H) of the problem
is a diagonal matrix and can be computed as H = Λ(ex ·
(Cλ)), where Λ(·) converts a vector to a diagonal matrix,
· is element-wise product and λ the Lagrange multipliers
for the nonlinear constraints. Thus the problem can still be
solved quickly.
The algorithm is terminated when at least one of the
following condition satisﬁed: the maximal number of iterations is achieved; all of the training pair get correctly
ranked; the model (i.e., the value of objective function)
doesn’t change.
The problem in Eqn. 10 (i.e., Eqn. 8 and 9) is not convex, due to the nonlinear equality constraint. Thus we can
only found local optimal solutions. While there is no guarantee on the convergence, empirically it was found that after a certain number of iterations the learned model starts to
deliver reasonable results (in terms of the percentage of the
training pairs getting correctly-maintained ranking).

Improved Algorithm
Input: X, E, ρ, γ, Ξ1 , Ξ2
Output: θ1 and θ2
Initialization: Initialize θ1 and θ2 via ordinary HMM
learning algorithm with data from Ξ1 and Ξ2 accordingly;
while NOT terminated
Compute the optimal path ẑ and z̃ for each sequence
with θ1 and θ2 ;
Update the model θ1 and θ2 according to Eqn. 9;
end
After we learn the model with the improved algorithm,
we can apply it to a given pair by ﬁrst computing their
likelihoods with respect to the ”sub-models” given by θ1
and θ2 (with the Viterbi algorithm), and then we use the
logarithm of the ratio of the data likelihoods as the score to
rank/compare the pair.
The learned models θ1 and θ2 serve as a uniﬁed model
to rank the data. We may view them as the centers of two
clusters, where the distances of the data to those two centers
can be related to the ranking score.
It needs to be emphasized that the improved model is
not equivalent to a supervised HMM with two classes. In
a 2-class HMM setting, two models will be independently
trained with their respective training sets. Here, the proposed model trains two ”sub-models” jointly with only relative ranking constraints. Speciﬁcally, if there is no further
information for Ξ, we could assume that Ξ1 = {i|(i, j) ∈
E, ∀j} and Ξ2 = {j|(i, j) ∈ E, ∀i}, and thus there could
be overlaps between Ξ1 and Ξ2 (which will become clear in
the experiment with synthetic data in Sec. 5). This situation
not even allowed by a supervised HMM setting. We don’t
require any extra properties for Ξ1 and Ξ2 , e.g., balances.

5. Experiments
In this section, we evaluate the proposed methods, including the baseline method and the improved method, using both synthetic data (Sec. 5.1) and realistic data collected
from the surgical training platform FLS box (Sec. 5.2). The
performance of the proposed methods is compared with a
supervised 2-class HMM. (Lacking a comparative approach
in the literature that is both unsupervised and works with
only relative rankings, this is believed to be a reasonable
way of a reference point to assess the proposed methods.)

5.1. Evaluation with Synthetic Data
To evaluate the proposed method, we generate synthetic
data: we ﬁrst generate six different HMMs (θ1 to θ6 , which
are referred as data-generating models), from each of which
we draw 200 sequences, with the length being uniformly
distributed between 80 to 120. Each data-generating model
has ﬁve states. For the sequences from each data-generating
model, we randomly assign 50 of them to the training set
and the remaining to the testing set. We assume there exists a score function such that F (Xi ) > F (Xj ) if and
only if Xi ∼ θk , Xj ∼ θl and k < l. That is, the sequences from a data-generating model with a lower index
are viewed to have a higher score (or ranking) than those
from a data-generating model with a higher index. A set
of pairs {(i, j)|Xi ∼ θk , Xj ∼ θk+1 , k = 1, · · · , 5} are
then formed accordingly, some of which are then randomly
selected as the training pairs E.
For all three methods, we assume that the maximal
number of states is ten. For the HMM algorithm and
the improved method, we initialize the two sets as Ξ1 =
{i|(i, j) ∈ E, ∀j} and Ξ2 = {j|(i, j) ∈ E, ∀i}. Note, the
data generated from data-generating Models θ2 ∼ θ5 could

4.3. Discussion
Eqn. 8 (similarly for Eqn. 9) can be written in a more
standard form:
x :

min fT x

(10)

x

s.t. : Ax ≤ b; x <= 0; Ce = 1
x

Eqn. 10 is a nonlinear programming problem (due to the
nonlinear equality constraint). To solve this problem, we
use primal-dual interior point method. The dimension of
this problem is K(1+K+D)+|E| (or 2K(1+K+D)+|E|)
with 2|E|+K(1+K +D) (or 2|E|+2K(1+K +D)) linear
inequality constraints and 1 + K + D (or 2(1 + K + D))

552
550

Performances with Different # Training Pairs
1

0.8

Accuracy

be included in both Ξ1 and Ξ2 . Thus existing discriminative
learning methods for HMM could not be applied here.
The learned models are then used to evaluate the testing set, i.e., how many testing pairs that they rank the same
as ground truth. The result of the methods with different
number of training pairs is summarized in Fig. 1, where
due to the computational time it takes, we don’t have the results for the baseline method when there are more than 3750
training pairs.. From Fig. 1, we can ﬁnd that the improved
method achieves the best results on both the training set and
the testing set; and the HMM method gives the worse result. In addition, the performance of both of the proposed
methods stabilized after certain number of training pairs.
However the performance of the HMM method drops dramatically when the number of training pairs reaches about
6250. It can be explained by that the two HMMs share a lot
of common data (for those generated by θ2 ∼ θ5 ) and the
models are trained independently without consideration of
their discrimination ability. Normalizing the logarithm of
data likelihood does not improve the performance of baseline method, which could be explained by that, all the sequences have roughly the same length, i.e., 80 ∼ 120. Fig.
2 shows the logarithm of the data likelihood ratio with the
models learned by the improved method, when about 1250
training pairs are provided. This clearly demonstrates that,
although we formed the training pairs only with data from
data-generating models of adjacent indices (i.e., i and i+1),
the learned model is able to recover the strict ranking of the
original data.
For empirically understanding the convergence behavior of the improved method, we plot in Fig. 3 the objective value in the model as a function of the number of iterations. We can ﬁnd that the improved method converges
fairly quickly (within about 14 iterations) and the value of
the objective function monotonically increases. The time
complexity for the improved algorithm is roughly O(|E|2 )
(e.g., about 60 seconds for about 600 constraints in Matlab
on a quad-core PC platform).
It is obvious from this experiment that the sequences are
different from (or similar to) each other only because they
are from different (or the same) data-generating models,
whereas their relative ranking can be arbitrarily deﬁned. In
the end, the proposed methods will learn a temporal model
to reﬂect the deﬁned rankings. This suggests that, as long as
we can assume there are some data-generating models for
the given sequential data, we can use the proposed methods to learn a relative HMM. This is the basis for applying
the approach to the surgical training data in the following
sub-section, where it is reasonable to assume that movement patterns of subjects with different skill levels may be
modeled by different underlying HMMs while the ranking
can be based on the time of training, which reﬂects the skill
level of the subject at the time.

0.6

0.4
HMM (Train)
Base (Train)
Base Normalized (Train)
Improved (Train)
HMM (Test)
Base (Test)
Base Normalized (Test)
Improved (Test)

0.2

0
156

312

625

1250

2500

3750

5000

6250

Number of Training Pairs

Figure 1. The results of four methods on training set (dashed
curve) and testing set (solid curve) with different numbers of training pairs.
ln(p(x,z|θ )/p(x,z|θ )) of Testing Data
1

2

100
50
0
−50
−100

0

500

1000

1500

ln(p(x,z|θ )/p(x,z|θ )) of Training Data
1

2

100
50
0
−50
−100

0

50

100

150

200

250

300

Figure 2. The logarithm of the data likelihood ratio with the models learned by the improved method. Top: the result for the testing
set. Bottom: the result for the training set. The data are grouped
(as the section partitioned by the red lines) according to the data
generation model from which they are synthesized.

5.2. Skill Evaluation Using Surgical Training Video
We now evaluate the proposed method using real videos
captured from the FLS trainer box, which has been widely
used in surgical training. The data set contains 546 videos
captured from 18 subjects performing the “peg transfer” operation, which is one of the standard training tasks a resident surgeon needs to perform and pass. The number of
frames in each video varies from 1000 to 6000 (depending
on the trainees’ speed in completing a training session). In
the training, the subject needs to lift six objects (one by one)
with a grasper by the non-dominant hand, transfer the object
midair to the dominant hand, and then place the object on a
peg on the other side of the board. Once all six objects are
transferred, the process is reversed, and the objects are to be
transferred back to the original side of the board. The videos
capture the entire process inside the trainer box, showing

553
551

5

x 10

1300

−5.95

1200

−6

1100

−6.05

1000

−6.1

900

−6.15

800

−6.2

700

−6.25

600

−6.3

2

4

6

8

10

12

Method
# Pairs
Accuracy

# Constraints Satisfied

Objective Value

−5.9

HMM
6363
79.39%

Baseline
6215
77.54%

Improved
6993
87.25%

Table 1. The result for experiment on evaluating surgical skills.
There are 8015 pairs in total (only 300 for training), excluding the
comparisons among data of different subjects.

ing data (within each subject) and compute the percentage
of correctly labeled pairs (recall that, we use their time of
recording as ground truth). The result is summarized in Tab.
1, where the improved method obtained a signiﬁcantly better result than the other approaches. Surprisingly, the baseline method even performed slightly worse than the HMM
method. This is largely due to the wide range of variations of the length of the input sequences. Fig. 4 shows
the computed scores with the learned models, where for
better illustration purpose we group them by their subject
ID and within each subjects’ corpus we sort the videos by
their recording time. From the ﬁgure, we can ﬁnd that the
improved method (bottom) reveals a more clear trend for
the data than both the HMM method (top) and the baseline
method (middle), i.e., the scores of the data increase over
times (X-axis) for each subject (segments within the red
lines). It is worth emphasizing that only one joint model is
learned from ranked pairs of subjects with potentially varying skill levels. Still the learned model is able to recover the
improving trend, independent of the underlying skill levels.
It is also interesting to look at what the jointly-learned
models look like in the proposed approach. Fig. 5 depicts
the two models learned by the improved method in this realdata based experiment. From the ﬁgure, we can see that the
two models have different transition patterns. For example,
the transition from State 8 to States 2 and 5 are only observed in Model 1. This may be linked to different motion
patterns for data of different surgical skills.

500
14

Iteration
Figure 3. The convergence behavior of the improved method,
where around 1250 training pairs were used. The blue curve/axis
shows the value of the objective function, and the green curve/axis
shows the number of constraints satisﬁed.

how the tools and objects are moved by the subject. In the
existing practice, senior surgeons rate the performance of
the trainees based on such videos. Our goal is to perform
the rating automatically with the proposed model. The data
set covers a training period of four weeks, with every trainee
performing three sessions each week.
The time of recording is used to rank the recorded videos
within each subjects’ corpus (i.e., a later video is associated with a better skill) based on the reasonable assumption
that the trainees improve their skills over time (which is the
whole point of having the resident surgeons going through
the training before taking the exam). Other than this relative ranking, there are no other labels assumed for the video,
e.g., there is no rank information between videos of different subjects (which would be hard to obtain anyway, since
there is no clearly-deﬁned skill levels for a group of trainees
with diverse background). Based on this, we randomly pick
300 pairs as the training pairs, similarly as in the experiment
using synthetic data.
We use the “bag of words” approach for feature extraction from the videos as follows. The spatiotemporal interest
point detector [10] is applied to obtain the histogram-ofgradient (HoG) features. K-means (k = 100) is then used
to build a codebook for the descriptors of the interest points.
Finally, the codebook is used to obtain a histogram of interest points for each frame, and thus each video is represented
as a sequence of histograms. This representation, compared
with the existing way of using bag of words in action recognition, i.e., transforming each video into a single histogram,
can better capture the temporal information of the data.
After learning the models from the training data, we
compute the score of the test data as the logarithm of data
likelihood (for the baseline method) or the logarithm of
the data likelihood ratio (for the improved method and the
HMM). We compare these scores for each pair of the test-

6. Discussions and Conclusions
In this paper, we presented a new formulation for the
problem of learning temporal models using only relative information. Algorithms were developed under the formulation, and experiments using both synthetic and real data
were performed to verify the performance of the proposed
method. In essence, the proposed method attempts to learn
an HMM with relative constraints. Such a setting is useful for many practical applications where relative attributes
are easier to obtain while explicit labeling is difﬁcult to get.
The application of video-based surgical training was the focus of this study, and the evaluation results using realistic
data suggests that the proposed method provides a promising solution to the problem of motion skill evaluation from
videos. For future work, we plan to extend the proposed
method to cover different observation models so that other
types of applications may be handled.
Acknowledgement: The work was supported in part by a

554
552

[4] F. Duan, Y. Zhang, N. Pongthanya, K. Watanabe, H. Yokoi,
and T. Arai. Analyzing human skill through control trajectories and motion capture data. In Automation Science and Engineering, 2008. IEEE International Conference on, pages
454–459, aug. 2008.
[5] B. Juang and L. Rabiner. The segmental¡ e1¿ k¡/e1¿-means
algorithm for estimating parameters of hidden markov models. Acoustics, Speech and Signal Processing, IEEE Transactions on, 38(9):1639–1641, 1990.
[6] I. Kadar and O. Ben-Shahar. Small sample scene categorization from perceptual relations. In CVPR 2012, pages 2711
–2718, june 2012.
[7] K. Kahol, N. C. Krishnan, V. N. Balasubramanian, S. Panchanathan, M. Smith, and J. Ferrara. Measuring movement
expertise in surgical tasks. In ACM Multimedia, pages 719–
722, New York, NY, USA, 2006. ACM.
[8] A. Kovashka, D. Parikh, and K. Grauman. Whittlesearch:
Image search with relative attribute feedback. In CVPR 2012,
pages 2973 –2980, june 2012.
[9] N. Kumar, A. Berg, P. Belhumeur, and S. Nayar. Attribute
and simile classiﬁers for face veriﬁcation. In ICCV 2009,
pages 365 –372, 29 2009-oct. 2 2009.
[10] I. Laptev. On space-time interest points. IJCV, 64(2):107–
123, 2005.
[11] N. Merhav and Y. Ephraim. Maximum likelihood hidden
markov modeling using a dominant sequence of states. Signal Processing, IEEE Transactions on, 39(9):2111 –2115,
sep 1991.

HMM

0
−20
−40
−60
1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16 17 18

10

11

12

13

14

15

16 17 18

10

11

12

13

14

15

16 17 18

Baseline Method

−4e3
−6e3
−8e3
−1e4
−1.2e4
1

2

3

4

5

6

7

8

9

Improved Method

0

−50

−100
1

2

3

4

5

6

7

8

9

Figure 4. Top: the logarithm of the data likelihood ratio from two
models learned by HMM. Middle: the logarithm of data likelihood with the model learned by the baseline method. Bottom: the
logarithm of the data likelihood ratio with the models learned by
the improved method. The red vertical lines separate the data of
different subjects, where X-axis is the corresponding subject ID.
Within each subjects’ corpus, the videos are sorted according to
their time of recording.
Model 2

Model 1

0.13

9
0.31/0.04

0.33/0.11

0.13

9

8

8

10

0.26/0.25

10

0.05/0.03
0.16
0.02/0.01

0.04

0.85

0.72

0.23

0.29

6

6
0.46/0.02

0.40/0.05
0.01

1
0.54

1

0.59

0.20/0.10

0.18/0.10

0.51

0.02
0.02

[12] D. Parikh and K. Grauman. Relative attributes. In ICCV
2011, pages 503 –510, nov. 2011.
[13] D. Parikh, A. Kovashka, A. Parkash, and K. Grauman. Relative attributes for enhanced human-machine communication.
In AAAI, 2012.
[14] J. Rosen, M. Solazzo, B. Hannaford, and M. Sinanan. Task
decomposition of laparoscopic surgery for objective evaluation of surgical residents’ learning curve using hidden
markov model. Computer Aided Surgery, 7:49–61, 2002.
[15] S. Satoshi and H. Fumio. Skill evaluation from observation
of discrete hand movements during console operation. Journal of Robotics, 2010, 2010.
[16] M. Schultz and T. Joachims. Learning a distance metric from
relative comparisons. NIPS, page 41, 2004.
[17] A. Sloin and D. Burshtein. Support vector machine training
for improved hidden markov modeling. Signal Processing,
IEEE Transactions on, 56(1):172 –188, jan. 2008.
[18] S. Suzuki, N. Tomomatsu, F. Harashima, and K. Furuta. Skill
evaluation based on state-transition model for human adaptive mechatronics (ham). In Industrial Electronics Society,
2004. IECON 2004. 30th Annual Conference of IEEE, volume 1, pages 641–646. IEEE, 2004.
[19] G. Wang, D. Forsyth, and D. Hoiem. Comparative object
similarity for improved recognition with few or no examples.
In CVPR 2010, pages 3525–3532. IEEE, 2010.
[20] K. Watanabe and M. Hokari. Kinematical analysis and
measurement of sports form. Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on,
36(3):549–557, 2006.

0.12/0.44
0.14

0.19

2

4

0.68

3

2
0.02

0.01

0.01

0.38

0.03/0.880.44/0.10

5

7

0.46

3

0.45
0.02

0.02

4

0.02

5

1.00

7

Figure 5. The two component models (Model 1 for Ξ1 and Model
2 for Ξ2 ) learned by the improved method, where we only draw
the edges with a transition probability larger than 0.01 and ignore
self transitions. The number attached to each edge indicates the
transition probability.

grant (#0904778) from the National Science Foundation
(NSF). Any opinions expressed in this material are those of
the authors and do not necessarily reﬂect the views of the
NSF.

References
[1] Y. Altun, I. Tsochantaridis, T. Hofmann, et al. Hidden
markov support vector machines. In ICML, volume 20,
page 3, 2003.
[2] L. E. Baum, T. Petrie, G. Soules, and N. Weiss. A maximization technique occurring in the statistical analysis of probabilistic functions of markov chains. The Annals of Mathematical Statistics, 41(1):pp. 164–171, 1970.
[3] M. Collins. Discriminative training methods for hidden
markov models: Theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing, pages 1–8.
Association for Computational Linguistics, 2002.

555
553

Proceedings of the 2006 IEEE International Conference on Robotics and Automation
Orlando, Florida - May 2006

Homography-based Ground Detection for A Mobile Robot
Platform Using a Single Camera
Jin Zhou and Baoxin Li
Department of Computer Science & Engineering
Arizona State University, Tempe, AZ 85287
{JinZhou, Baoxin.Li@asu.edu}
Abstract -This paper presents a practical approach to ground
detection in mobile robot applications based on a monocular
sequence captured by an on-board camera. We formulate the
problem of ground plane detection as one of estimating the
dominant homography between two frames taken from the
sequence, and then design an efficient algorithm for the
estimation. In particular, we analyze a problem inherent to any
homography-based approach to the given task, and show how the
proposed approach can address this problem to a large degree.
Although not explicitly discussed, the proposed method can be
used to guide the maneuver of the robot, as the detected ground
plane can in turn be used in obstacle avoidance.
Index Terms – mobile robot, obstacle detection.

I. INTRODUCTION
Vision-guided robot navigation has seen tremendous
developments in recent years, largely due to the availability of
low-cost imaging sensors and compact yet high-performance
processors that support image-analysis-based processing. For a
mobile robot, obstacle detection and avoidance is one of the
most important issues. In many applications, the mobile robot
may be considered to maneuver always on a planar surface.
This is naturally true for indoor environments, and for outdoor
applications, this may also be a good approximation in the
immediate small neighbourhood of the robot. Therefore,
obstacle detection may often be reduced to the problem of
ground plane detection: With the ground plane detected, other
objects can be viewed as obstacles if they are on the direction
of the movement and outside of the ground plane.
Various approaches have been proposed to address the
problem of ground detection. For example, simple approaches
identify the ground floor using color information [2,4,9].
While being simple to implement, these approaches are
suitable only for very specific environment. To handle general
environments, a few systems attempt to recover the structure
of the scene. To this end, different techniques have been used.
For example, the work in [3,8] uses stereo vision for this
purpose. Presently, monocular vision based approaches are
more popular and attractive since they are typically more costeffective and also there is no stringent requirement on camera
calibration (as in stereoscopic approaches). In [6, 10], optical
flow is used for obstacle avoidance. In [11], optical flow is
used in computing the surface normal for different image
patches, which are then grouped to detect the ground floor.
The optical-flow-based approaches tend to be computationally
costly. In addition, images from a practical mobile platform
typically suffer from unpredictable motion, which may have

0-7803-9505-0/06/$20.00 ©2006 IEEE

4100

dramatically different magnitude at different times. Also,
often, an image may contain large homogenous regions,
providing little information for reliable motion estimation for
these regions. All these pose difficulties for most motiontracking-based approaches, which normally assume a motion
model and rely on good motion estimation.
In this paper, aiming at designing a practical system to
detect ground with monocular vision, we propose a method
that makes use of only a sparse set of features detected from a
corner detector, assuming that motion estimation from these
features are better than that from other ordinary image pixels.
The motion analysis of these features is formulated as a
homography estimation problem for detecting the ground
plane (and in turn facilitating the detection of obstacles outside
of the ground plane). In addition to designing an efficient
clustering algorithm named H-clustering, we identify a
common issue in any homography-based approach - the
problem of the virtual plane, and propose a solution to
overcome it. Consequently, compared with prior work on the
same task using plane homography (such as [7]), the proposed
method performs better in finding the real ground plane.
This paper is organized as follows: Section 2 describes
feature detection and matching. Section 3 presents the
homography-based ground detection scheme and algorithm.
Experimental results are given in Section 4.
II. FEATURE DETECTION AND MATCHING
The basic idea behind the proposed approach is to exploit
the fact that the pixels corresponding to the ground plane have
coherent motion patterns that are different from the motion of
other pixels in the image, when the acquisition platform (the
robot) is moving. Given the unpredictable nature of the
platform motion, we propose to use a feature-correspondencebased approach to motion analysis, where the features are first
independently detected in two given frames and then
correspondence is established. This is due to the consideration
that, for example, optical-flow-based or feature-tracking-based
approaches would typically assume that the motion is small
and certain motion model may be needed.
For feature detection, we use the well-known PlesseyHarris corner detector [1]. Since the features are detected
independently from two frames, it is very likely that the two
sets of features do not have a one-to-one correspondence.
Further, even for the matched features, they may not
correspond to the same physical points. To solve the

correspondence problem with the above consideration, the
following method is used.
For two feature points p = ( x, y ) and p ' = ( x ', y ') from
different views, we define their dissimilarity as:
D ( p , p ') =

∑

| I 2 ( x + i , y + j ) − I1 ( x '+ i , y '+ j ) |

(1)

− w≤i , j ≤ w

where I n (i, j ) represents the intensity value of point (i, j ) in
the n-th image, w represent half of the matching window size.
With this definition, we search the most similar point in
the second view for each feature point in the first view. The
same process is applied in reverse direction from the second to
the first view. A match is declared only if both the search on
both directions results in the same pair.
To account for possible variability in corner detection in
two views as mentioned above, after a pair is matched, we
further perform a local search in the second view:

p ' = arg min p '' {D( p, p ' ' ), ∀p ' '∈ neighbor ( p ' )}

(2)

From (2), the point with minimal dissimilarity in a small
neighbourhood of the initial match p’ is used as the final
corresponding point for p.
III. HOMOGRAPHY BASED GROUND DETECTION
After we have the point correspondence, we can use them
to do ground detection. We do not assume any motion model
of the camera, and use only the point correspondence as the
basis for subsequent processing.
A. Homography
In theory, points on the same plane share a homography
transformation in two views. That is, for a set of point
correspondences {x i ↔ x i '} in two images, if all of the points
are coplanar, then there is a homography matrix such that
x i ' = Hx i

(3)

where x represents a homogeneous image coordinate
( x, y, w)T , and H is a 3 by 3 matrix. Since x is represented by
homogeneous coordinates, Eqn. (3) stands up to a scale factor.
Thus H has only eight degrees of freedom. To determine such
an H, four non-degenerated point correspondences are
required since each point correspondence provides two
independent constraints, although in practice typically more
points are used to improve the accuracy.

the camera in relation to the physical ground plane) may tell
which part of the image will be likely the ground, the above
homography-based analysis can be applied to the region of the
image corresponding to the ground (e.g., the lower half of the
image). Then the problem becomes the detection of a
dominant homography which accounts for the most feature
points in two views. This homography can be assumed as the
ground plane homography and can be used to distinguish
ground points and obstacle points.
In reality, Eqn. (3) is satisfied only approximately (e.g., in
least-square sense). We use the following distance
measurement together with a threshold to classify feature pairs
as inliers or outliers to a given homography H,
d (x, x', H) = ||x' − Hx ||

In short, we formulate the ground detection problem as:
Given a set of point correspondences {x i ↔ x i '} , search
for the homography matrix H that has most inliers.
C. Searching for the dominant homography

While one may use the classical RANSAC [5] robust
estimation algorithm for searching the dominant homography
from the given correspondence, one disadvantage is the poor
and unpredictable speed performance due to the random
sampling, which renders it inappropriate for a real-time
application. Ref. [7] proposed a bootstrap scheme to augment
this process. Briefly, it imposes the constraint that the points
should not be too distant or too close or collinear, and it forms
groups from neighbouring points and remove groups that are
not invariant over the two views; after that, it uses four points
from each group (with minimum disparity point removed) to
initialize the H computation. For the H computation, it
repetitively calculates inliers and uses all of the inliers to recompute H until the number of inliers stabilizes. Through this
kind of bootstrap, the sample for computing H is more likely
to be a good sample. However, this process cannot guarantee
to find the most dominant homography due to the following
two reasons: 1) Four points are only a minimal number to
determine H, which can easily result in an over-fitted H; 2)
Even with repetitive H computation, it may still converge only
to a local minimum. To overcome these problems, we propose
the following algorithm, which we name as H-clustering:
1.

Initialize the outlier set to be all of the feature points.

2.

Randomly choose a point in the outlier set. Find its
four closest points. Starting from these five points to
repetitively compute H until the number of inliers
converges. If the number of the inliers is less than 5,
discard this H. Otherwise, create a cluster whose
members are those inliers and remove these inliers
from outliers set.

3.

Repeat step 2 until the number of points in outliers
set is less than 5.

B. Ground detection based on dominant homography matrix

Equation (3) suggests a way to group the detected feature
points into coplanar sets. Each coplanar set shares the same
homography, which is different from that of another coplanar
set. In the target application, where the hardware configuration
of the system (e.g., the constraint on the viewing direction of

4101

(4)

4.

Merge the clusters which have overlapping members.
For each merging, use all of the points in both
clusters as initial points set to repetitively compute H
which is used to form a new cluster.

The first 3 steps effectively cluster the data set. Points in
each cluster share the same homography and the clusters may
overlap. Step 4 attempts to form bigger clusters based on the
initial clustering results.
The proposed H-clustering approach is fast to implement
and can find a better homography (i.e. the number of inliers
are larger). It was found in our experiments that the typical
number of repetitive H computations is less than 20 times.
Nevertheless, due to the approximation nature of any
homography based approach (e.g., the least-square fit of more
than 4 points), it is likely that the obtained “best” homography
contain some points that are not on the ground plane in the
physical world. We call this problem as “virtual plane”
problem. That is, we may find a virtual plane such that it
contains both the ground plane points and some other points
(e.g. see Fig. 1). If such extra points are in fact obstacles
outside of the ground plane and are on the direction of the
movement of the robot, they may pose as potential dangers.
We discuss this problem and propose our solution in the next
section.
D. The Virtual Plane Problem and the Proposed Solution

Although the above-described idea of using homography
in detecting the ground plane is theoretically attractive, in
practice, for a pair of real images, feature points sharing the
same homography may not really lie on the same physical
plane, as illustrated in Fig. 1, where all the feature points
(green crosses) and their correspondence (the endpoints of the
red line segments) fit well by a single homography. We use
the term “virtual plane” to describe the nonexistent plane
corresponding to this homography. This is a serious problem
for any homography-based approach, although we have seen
little discussion in the prior work.

Fig. 1 The virtual plane problem. The ground points and some obstacle points
share the same homography

We show here that the virtual plane problem is inherent to
any homography-based approach for the target application and
thus it must be addressed. We examine the following
examples.
In the simplest case, for two almost exactly the same
images arising from no or little motion of the robot platform,

4102

there is no or little displacement for the feature points, and
thus the homography matrix is the identity matrix which
includes all the features as inliers.
Pure rotation of the camera can also lead to a virtual plane
for the entire image points, as proved in the below. Suppose
that the first camera matrix is P = K [ I | 0 ] , and the second
camera matrix is P' = K' [ R | t ] , where R represent rotation
matrix. Then for a plane π = (n T , d )T so that points on the
 + d = 0 , we can compute an expression for any
plane n T X
homography induced by the plane:
H = K' ( R − tn T / d ) K −1

(5)

Since t = − RC , where C represents the displacement of
the second camera center, we can get
H = K' R (I + Cn T / d ) K −1

(6)

If C = 0, H will be the same for every plane, as (6) will
have no dependency on the plane parameters any more. This
proves that, for pure rotation, all points lie on one single
virtual plane. In reality, if C is very small, then H will be very
similar for different planes, which also leads to the virtual
plane problem.
Note that all the above situations can arise in the target
application. In practice, we may alleviate those problems by
selecting two views that have large camera center
displacement. However, the virtual plane problem still exists,
if two physical planes are far away from the camera: if d is
very large, then n T / d ≈ 0 . Whether the physical plane is
relatively far from the camera depends on the hardware setup
and also where the feature detector finds features on the
ground plane. In conjunction with the correspondence error
due to typically noisy input in the target application domain,
we conclude that the virtual plane problem is unavoidable. We
employ the following two techniques to address this problem:
1) Initial classification based on color

We first segment the image into regions based on color. In
the experimental platform, due to the hardware configuration,
the ground plane always occupies the lower part of the image.
Using this information, we can find potential regions
corresponding to the ground plane (by simply assuming that
the lower center region of the image must be the ground).
Then we just use the points from the potential ground regions
to estimate the dominant H.
The rationale behind this technique is that more obstacle
points will more easily lead to virtual plane. Note that the
initial color-based classification need not be accurate, as long
as it includes much more ground points than other points. This
technique assumes that we can find a good color metric to
exclude obstacle while including more ground pixels. This
assumption is not always satisfied. Therefore, we propose the
second technique.
2) Exclusively grouping and homography refinement

The color-based initial segmentation may not work if the
ground plane has a non-uniform appearance such as in Figure
1. The following technique can be used in this case.
After H-clustering, we have a set of clusters with each
cluster’s points share the same homography. There may be
many overlapped clusters and some clusters correspond to
virtual planes. We perform the following steps to find out the
real plane while avoiding a potential virtual plane.
Step 1. Exclusive Grouping. Merge two different clusters
to form a cluster group. Compute the number of the points in
the group with overlapped points is just counted only once.
Then we find a largest two-cluster-based group which contains
the most number of points.
Step 2. Homography Refinement. For the group found
above, we divide the points into three sets: (i) points in the
first cluster but not in the second cluster; (ii) points in the
second cluster but not in the first cluster; and (iii) points in
both clusters. Now we divide set 3 to two sets: based on Eqn.
(4), if the error from the first cluster is less than that from the
second cluster, put this point to set 1; else put it to set 2. Use
the updated set 1 and set 2 to compute H respectively, so that
we get two new clusters. The new clusters are assumed to
correspond to real planes.
The first step encourages two clusters that are big and also
complement to each other. If two clusters have significant
overlap, the grouping will not increase too much the size of
the group. The rationale behind the second step is that the
fitting error for the points on the virtual plane should be larger
than that of the real plane. If the virtual plane contains two real
planes, then if we use the real planes for computing the
homography separately, we should get smaller error compared
to using the virtual plane.
Since we have two different techniques, it is natural to ask
which one should be used. This depends on the operational
environment of the robot. If the first approach’s assumption is
satisfied, we should use it since it is fast and reliable. The
second approach does not rely on color or other heuristics.
However, it may fail when the homography of ground points
and obstacle points are very similar.

Fig. 2: Robot platform

A. Local Search Augmented Mutual Matching
We compared our local-search-augmented mutual
matching algorithm designed in Sect. II with traditional one
(i.e. no local search scheme). We tested on ten different image
sets, using Equation (1) to measure the dissimilarity between
the matched corners. Table I shows the results. From table I
we can find that the proposed method improves the matching
accuracy significantly. While one may argue that the
difference computed from Eq. (1) may not be a good metric
for measuring the goodness of matching, by visually
inspecting the results, we have found that, typically, the
optimal matching point departs from original detected corners
by 1 or 2 pixels, which will be accounted for by the proposed
method.
B. Analysis of the Virtual Plane Problem – Simulated Data
To demonstrate that the virtual plane problem often
happens in practice, we designed the following simulated
experiments. We first create a 3-D point set Xs with ten points
lie on the “ground” and nine points lie outside the plane (or on
some imaginary obstacles):
Xs = Xs_Ground ∪ Xs_Obstacle
Xs_Ground = {(2i , 2 j , 0) | i ∈ [3, 4], j ∈ [ −2, 2]}
Xs_Obstacle = {(3, 2i, j) | i ∈ [ −1,1], j ∈ [1, 3]}

Two cameras are given as
Cam1:{o = (14, 0, 4), l = (−1, 0, 0), u = (0, 0,1), f = 1}
Cam2 :{o = (13.5,1, 4), l = (−1, 0, 0), u = (0, 0,1), f = 1}

IV. EXPERIMENTAL RESULTS
In this section, we present the experimental results
obtained from various tests designed to measure the
performance of the proposed techniques. The real sequences
used in Figures 6~7 were captured by a robot car platform
illustrated in Fig. 2.

where f represents the focal length, o the camera center, l the
camera’s view direction, and u the viewing up direction which
defines the pose of the camera, as shown in Fig. 3.

Fig 3: Camera System.o represent camera center, l represent view direction
and u represent up direction.

4103

TABLE I: Local Search Augmented vs. Original Window Matching
1
2
3
4
5
6
7
8
9
10
LSA
7.8
11.3
7.1
8.9
5.5
5.5
8.8
5.0
7.2
9.4
WM
10.8
15.8
9.7
14.2
7.9
8.8
12.6
7.4
10.3
16.5
LSA represent local search augmented algorithm and WM represent original window matching algorithm. The data represent average intensity difference between
matched features.

With the simulated cameras, we obtained the “image”
containing the given 3-D points. To simulate the real world
situation, we add small noise to the image points. We use
Gaussian model to simulate the noise. With two cameras, we
can get two views of the same 3D points as well as the
correspondences. Fig. 4a shows the resultant image. Although
a human might be able to guess from Fig. 4a which points
form the ground and which points may be the obstacle, a
homography-based analysis by a computer may find many
other possible virtual planes, as illustrated in Fig. 4b~d. All of
Fig. 4c~d have 11 inliers which is larger than the ground plane
group or the obstacle plane group. Therefore, if we only use
the dominant homography, it is very likely that the result will
correspond to a virtual plane. With the exclusive grouping
introduced in Section III, the group from the clusters in Fig. 4e
and Fig. 4f will have the largest size, i.e. 19.
C. Tests with Real Image Data
We tested our algorithms on real image data. We carry out
two different kinds of experiments for evaluating the two
techniques for avoiding virtual planes respectively.
1) Exclusively Grouping and Homography Refinement
Fig. 5 shows the experiment results. Fig. 5a and Fig. 5b
are two images chosen for motion computation. Fig. 5b also
shows the point correspondences. Fig. 5c shows the largest
cluster. We can find that this cluster contains ground points as
well as obstacle points. Fig. 5d and Fig. 5e are the two new
clusters based on exclusive grouping and homography
refinement. Fig. 5d contains all of the ground points and none
of the obstacle points. Fig. 5e contains all of the obstacle
points and also contains some ground points. Actually, the
points lying on the intersection line of the ground plane and
the obstacle plane should be included. However, some ground
points not on the intersection line are also included. This is
due to the small homography distance for those points and the
obstacle plane homography.
2) Initial Classification based on Color
We use some images taken from the mobile robot. When
the camera’s view direction is horizontal, i.e. parallel to the
ground plane, we just need to detect the ground plane in the
lower half of the image. Fig. 6 shows the experimental results.
Fig. 6a shows the point correspondences. Fig. 6b shows a
virtual plane in this case. Fig. 6c shows the initial
classification based on color. The white area stands for the
obstacle and black areas represent ground. The result is not
very accurate, but it does not affect the final result. Fig. 6d
shows the ground points based on the initial classification.
With this initial ground points, we can find dominant H and
use this H to classify all of the points. Fig. 6e~f shows the
final results. The final results correct some initial misclassified
points. In this case, the virtual plane problem is avoided.

4104

D. Performance Statistics and Analysis
In another test with a 100-frame sequence captured from
the platform of Fig. 2, we chose two frames that are 20 frames
apart for detecting the ground plane. Fig. 7 shows some
intermediate results. In the start and middle phase, the system
performs well. There are also some misclassified ground
points on (1) and (2). This is due to the noise in the point
correspondences. In the latter parts of the sequence, errors
happen more frequently. The reason is that the number of
initial ground points is not big enough (about 10) and they
appear mostly on the left side of the image, leading to an overfit homography. This case happens when the obstacle is very
close and thus occupies a large portion of the image. In a real
application, we may not need to do this homography-based
recognition when the obstacle is at a close range. For example,
tracking may be employed to maintain the detect obstacles in
the sequence. Nevertheless, the experiment shows that our
method is reasonably robust.
V. CONCLUSIONS AND FUTURE WORK

In this paper, we presented a homography-based ground
detection approach. A simple local search scheme is employed
in the feature matching step to account for the variability of
the feature detector. A fast H-clustering algorithm was
proposed for estimating the dominant homography from a set
of point correspondences. In addition, we analysed the socalled virtual plane problem and proposed effective techniques
to address it. Experiments using both simulated data and real
data have shown that the proposed algorithms are promising
for practical deployment. We are currently integrating the
proposed approach with the control module of the robot
platform so that the detection results presented here can be
evaluated in the context of guiding a robot through a ground
full of obstacles.
REFERENCES
[1] C. J. Harris and M. Stephens, “A combined corner and edge
detector,” In 4th Alvey Vision Conference Manchester, pp 147-151,
1988.
[2] J. Hoffmann, M. Jungel and M. Lotzsch, “Vision Based System
for Goal-Directed Obstacle Avoidance, “ In 8th Int. Workshop on
RoboCup 2004.
[3] K. Sabe, M. Fukuchi, J.-S. Gutmann, T. Ohashi, K. Kawamoto,
and T. Yoshigahara, “Obstacle Avoidance and Path Planning for
Humanoid Robots using Stereo Vision,” In:Proc. of the Int. Conf. on
Robotics and Automation (ICRA'04), New Orleans, pp. 592 – 597,
Vol.1 April 2004.
[4] L. M. Lorigo, R. A. Brooks, and W. E. L.Grimsou, “Visuallyguided obstacle avoidance in unstructured environments,” in Proc.
IEEE/RSJ Int. Conf. Intelligent Robot and Systems Grenoble, France,
vol. 1, pp. 373-379, Sept. 1997.
[5] M. A. Fischler and R. C. Bolles. Random sample consensus: A
paradigm for model fitting with applications to image analysis and

automated cartography. Comm. Assoc. Comp. Mach., Vol. 24, pp381395, 1981.
[6] N. O. Stoffler, T. Burkert and G. Farber, “Real-time Obstacle
Avoidance Using an MPEG-Processor-based Optic Flow Sensor,”
Proc. of the 15th Int. Conf. on Pattern Recognition, vol. 4, pp. 161166, 2000.
[7] N. Pears and B. Liang, “Ground plane segmentation for mobile
robot visual navigation,” In IROS 2001, vol. 3, pp. 1513–1518, 2001.
[8] R. Mandelbaum, L. McDowell, L. Bogoni, B. Beich, and M.
Hansen, “Real-time stereo processing, obstacle detection, and terrain
estimation from vehicle-mounted stereo cameras,” In Proc. 4th IEEE

Workshop on Applications of Computer Vision, Princeton NJ, pp.
288-289, 1998.
[9] S. Lenser and M. Veloso, “Visual sonar fast obstacle avoidance
using monocular vision,” In Proc. of the IEEE/RSJ IROS 2003.
[10] T. Camus, D. Coombs, M. Herman, and T.-H. Hong, “Real-time
Single-Workstation Obstacle Avoidance Using Only Wide-Field
Flow Divergence,” In Proc. of the 13th Int. Conf. on Pattern
Recognition, vol.3, pp. 323-30, 1996.
[11] Y. Kim and H. Kim, “Layered ground floor detection for visionbased mobile robot navigation,” Proc. IEEE Int. Conf. on Robotics
and Automation, vol. 1, pp. 13-18, 2004.

(a)
(b)
(c)
(d)
(e)
(f)
Fig. 4: Simulated data. (a) point correspondences. (b) ~(d) virtual planes. (e) cluster correspond to real obstacle plane. (f) cluster correspond to real ground plane.

(a)
(b)
(c)
(d)
(e)
Fig. 5: Exclusive grouping and homography refinement. (a)first view; (b) second view and point correspondences; (c) virtual plane; (d) (e) refined results.

(a)

(b)

(c)

(d)
(e)
(f)
Fig. 6: Initial classification based on color. (a) point correspondences. (b) virtual plane. (c) initial classification based on color. (d) initial ground points. (e) final
ground points classified by H from (d). (f) final obstacle points.

(1)

(2)

(3)

(4)

(5)

(6)

(7)

(8)

(9)

(10)
(11)
Fig. 7: Intermediate results from a 100-frame sequence.

4105

(12)

HEAD TRACKING USING PARTICLE FILTER WITH INTENSITY GRADIENT AND
COLOR HISTOGRAM
Xinyu Xu and Baoxin Li
Center for Cognitive Ubiquitous Computing
Dept. of Computer Science & Engineering
Arizona Statue University
Tempe, AZ, U.S.A.
{xinyu.xu, baoxin.li}@asu.edu
ABSTRACT
This paper presents a method for tracking human head
using a particle filter to naturally integrate two
complementary cues: intensity gradient and color
histogram. The shape of the head is modeled as an
ellipse, along which an intensity gradient is estimated,
while the interior appearance is modeled using a color
histogram. These two cues play complementary roles in
tracking a human head with free rotation on a cluttered
background. To evaluate the tracker performance, we test
with both synthetic image sequences and real sequences.
Experiments show that the tracker is robust to 360degree rotation of the head on a cluttered background.

no assumption on linearity and Gaussian density are
required to make about the problem.
In addition to using a particle filter framework, the
proposed method is different from the approach of [5] in
that: (1) the gradient is not simply estimated in the
normal direction of an ellipse model; instead, a local
search is incorporated for accounting for the inaccuracy
of the ellipse model; (2) unlike in [5], the color model is
adaptive in this paper.
In Sect. 2, we describe the proposed method, and then we
present comparative experiments in Sect. 3, with both
synthetic and real sequences. We conclude with Sect. 4.

1. INTRODUCTION

2. PROPOSED METHOD

Head tracking can find a lot of applications including
automated surveillance, video conference, etc. However,
developing a head tracking algorithm that is robust to a
wide variety of conditions remains to be a challenging
problem. Part of the challenge comes from the lack of
reliable cues for tracking in all applications. For
example, color-based trackers may be distracted by other
targets of similar color, while edge-based trackers can be
misled by clutters in the background[1]. In addition,
compared with face tracking (e.g.,[7]), typically a head
tracker must handle a larger degree of head rotation,
which makes the color cue fragile under large head
motion, as the color of the front of head (face) is
typically totally different from that of the back of the
head. Thus, combining multiple cues may be the only
option.

We first describe the general framework of the proposed
tracker in Sect. 2.1, and then we present the details of the
algorithmic components in Sect.2.2 through Sect.2.4.

Recent work on tracking human head includes those
employing 3-D model for head pose estimation (e.g.
[2,3]) and those using both color and shape cues (e.g.
[5]). In this paper, we propose a method that uses the
particle filter to integrate both color (in terms of color
histogram) and shape (in terms of intensity gradient
along the head contour) information in order to handle
the above challenges. Both of the cues are based on 2-D
models, hence allowing efficient implementation.
Furthermore, using a particle-filter-based tracking
framework provides desired robustness and accuracy as

0-7803-9332-5/05/$20.00 ©2005 IEEE

2.1. A Head Tracking Framework Based on Particle
Filter
Particle filter has been used in various tracking problems
(e.g., [1][4]). In this work, we adopt a framework similar
to that used in [4]. Specifically, the head is modeled as
an ellipse centered at (x,y) and with size (Hx, Hy). At the
first frame, we use the face detection algorithm proposed
by [6] to detect the location and size of the head ellipse
(if the face is frontal; otherwise, a manual initialization is
performed by clicking on the head). The dynamics of the
(moving) head at time t are described by a state vector St
consisting of the following eight components
{x, y, Xv, Yv, Hx, Hy, Hvx, Hvy}
(2)
where (x, y) represent the center location of the head
ellipse, (Xv,Yv) represent the motion velocity, (Hx,Hy) are
the lengths of the half axes, and (Hvx,Hvy) are the
corresponding scale changes on the axes.
The key idea of a particle filter is to use a set of
properly weighted random samples, {st(n),ηt(n) |n =
1…N}, to approximate the posterior density function
P(St|Zt), where st(n) is a sample of the state vector, ηt(n)
the corresponding weight, and Zt the observation vector.

The proposed tracker performs the following three steps
in tracking the head: Sample Selection, Prediction and
Updating.
Sample Selection. Based on the weight associated with
each sample (particle), N particles are drawn from the set
{st-1(i)}i=1,2,3…N } at time t-1 with replacement to produce
the sample set at time t. The larger the weight, the more
likely a sample will be chosen to appear in the new
sample set.
Prediction. Each sample is propagated according to the
following system model:
St = ASt-1+ Nt-1
(3)
Nt-1 is a random vector drawn from the noise distribution
of the system. In this work, we use a simple system
model, where the head motion is assumed to be constant
velocity in translation and scaling, as in [4].
Update. Two new weights will be computed separately
for each sample based on the new observations: one is
based on the intensity gradient of the sample region;
another is based on the color histogram difference
between the measured color distribution and the model
color distribution (computed at the previous time
instant). Then the mean state, which specifies the tracked
head, is estimated by

E[S ] =

N

∑

η

(n)

s (n)

n =1

η

(n)

is the final weight obtained by averaging
In which
the gradient weight and color histogram weight by

η ( n ) = αη g( n ) + (1 − α )ηc( n )

(1)
Currently we simply average the gradient weight and
color histogram weight, so α is 0.5.
2.2. Gradient Estimation Based on the Head Shape
If we have obtained the contour of the head, then
computing the gradient along the head contour provides
a good measure for distinguishing the head from the
background. Assuming that the ellipse in the model
matches the head contour, then, for a particular state
sample specified by s, the normalized sum of the
gradient magnitude around the ellipse boundary is
computed as:

ψ

g

(s) =

1
Ns

N

(H x ,H y )

∑

g ( xi , yi )

i =1

where g(xi,yi) is the intensity gradient of pixel (xi,yi)
located at the boundary of ellipse specified by s. And Ns
is the number of pixels on the perimeter of the ellipse.
Since an ellipse does not accurately describe the contour
of the head, to make the above gradient estimate more
useful in case of the inaccurate modeling, the gradient at
pixel (xi,yi) is established as the maximum gradient by a
local search along the normal direction:

g ( xi , yi ) = max { g ( x n , y n )}
( x n , y n )∈ Ln

where Ln represents the normal line, (xn, yn) is the
coordinate of the points that are located on the normal
line. (xn, yn)must satisfy the following criterion :
( xn − xi )2 + ( y n − yi ) 2 < S ea ch R a n g e
yn =

( yi − C y ) * H
( xi − C x ) * H

2
x
2

* ( xn − xi ) + yi

y

The first formula specifies that the distance in the normal
direction between point (xn, yn) and (xi, yi) must be within
a certain search range. This will help to restrict our
search around the head contour and avoid hitting other
distracting points on the background (or within the face)
which have big intensity gradients. The second one is the
normal line equation at point (xi, yi). (Cx ,Cy) denotes the
ellipse center.
A simple operator is used to compute the gradient in xdirection
and
y-direction
for
pixel
(xn,yn):

gx(xn, yn) =I(xn −2, yn)+2*I(xn −1, yn)−2*I(xn +1, yn)−I(xn +2, yn)
gy(xn, yn) =I(xn, yn −2)+2*I(xn, yn −1)−2*I(xn, yn +1)−I(xn, yn +2)

And finally the gradient at point (xn, yn) is computed as

g ( xn , y n ) =

g x 2 ( xn , y n ) + g y 2 ( xn , y n )

The above local search provides a potentially significant
improvement over the simple gradient method in [5].
This step is especially helpful in the particle filter setting,
since some of state samples may deviate from the face
even if the head is a perfect ellipse.
2.3. Color Distribution Model
Following [4], we use a histogram-based color model.
The color distribution is represented by a color histogram
calculated in the RGB space using 8*8*8 bins. Pixels
which are closer to the region center are given higher
weights specified by:
⎧ 1 − r 2 : r < 1⎫
k (r ) = ⎨
⎬
⎩ 0 : o th e r w is e ⎭
This helps to put more weight on the center pixels than
on the boundary pixels, since the boundary pixels may
not correspond so well to the actual head.
The color distribution

py = {p(yu) }u=1,2...m of a region R

at location y is calculated as (see [4] for details):
p (yu ) = f

where

δ

⎛ || y − xi || ⎞
k⎜
⎟δ [ h ( xi ) − u ]
a
⎝
⎠
X i ∈R

∑

is the Kronecker delta function and h ( xi )

measures the color of the pixel at location xi . To
measure the similarity between the newly observed
image and the target model, Bhattacharyya coefficient is
computed between these two distributions:
m

ρ [ p, q ] = ∑ p ( u ) q ( u )
u =1

p stands for the color histogram of a sample hypothesis
in the newly observed image, and q represents the color

histogram of the target model. The larger ρ is, the more
similar the distributions are.
2.4. Determine the Sample Weights
Under the assumption that the true contour of the face
would typically generate a larger gradient value than a
contour that is randomly overlaid on the face, we employ
the following simple method to relate the computed
gradient to the sample weight
G

where σ

(n)
g

2
g

1

=

2π σ

−

e

( 1

g

2σ

is distracted a little bit by the clutters in the background.
In this case, the combined model with both color and
gradient information gives the best performance. (For the
given synthetic sequence, since the background is almost
colorless, using only the color model can also achieve
good tracking results, which are hence not shown here).

)2

(13)

2
g

g

is the variance controlling the model, and g

is calculated in Sect. 2.2. This Gaussian distribution
matches to the intuitive requirement that a larger g
should produce a larger weight. Although other model
can achieve similar objective, the Gaussian model is
found to be effective in this work.
For the color histogram model, we first calculate color
histogram inside the entire sample ellipse, and then the
Bhattacharyya coefficient is computed, which is then
again fed into a Gaussian distribution to obtain color
weight as follows:

G

(n)
g

=

1

−

e

d2

2σ c2

=

1

−

e

(1− ρ [ ps ( n ) , q ])

Figure 1. Tracker performance under different modes of
operations, on the synthetic sequence. Top row: with the
gradient model alone. Bottom row: with both the color
and the gradient models. In this example, since the
background is relatively uniform, there is no big
difference between the two modes of operations.

2σ c2

2π σ c
2π σ c
The above two weights are averaged to obtain a final
weight as in Eq. (1).
3. EXPERIMENTS AND EVALUATIONS
To evaluate the performance of the proposed tracker, we
test with both synthetic and real sequences. We evaluate
the method by invoking either or both of the cues in
different experiments. That is, the tracker has three
modes of operations: one based on the gradient model
alone, one based on the color model alone (effectively
the method of [4]), and one based on both models (the
proposed method). The experiments are performed on a
2.8GHz PC under Windows XP, the 24-bit RGB
sequences are of 320*240 resolution.
3.1. Experiments on Synthetic Sequences
A synthetic sequence is created by overlaying a moving
head (62*41 pixels) with constant velocity in translation
on a static image background. To show the tracking
results, for each frame, a deformed contour is drawn
around the ellipse boundary specified by the tracked
mean state. The deformed contour (plotted in green)
consists of pixels with maximum gradient along normal
direction of the mean state ellipse.
Figures 1 and 2 illustrate that, when the background is
relatively uniform, the gradient model alone performs as
good as the other two; but when the environment is
cluttered and has complex structures, the gradient tracker

Figure 2. Tracker performance under relatively cluttered
background. Top row: with the gradient model alone.
Bottom row: with both the color and the gradient models.
In this example, employing the color model helps greatly
to achieve more accurate tracking.

To further examine the tracker’s accuracy quantitatively,
we compare the tracked data (i.e., the ellipse center (x, y)
and size (Hx, Hy)) from the three modes (color only,
gradient only, color and gradient) against the
corresponding ground truth. Sample results are shown in
Fig. 3, where the result from the color model alone is
omitted since it is similar to the result from the combined
model, for reasons explained earlier. We found that, due
to the cluttered background, the gradient model alone
exhibits a larger tracking error than the combined model.
(Note that, in the synthetic data, there is no scale change,
so ideally the tracked Hy should be a constant. In Fig. 3,

the better performance of the combined model manifests
as the relatively smaller variations of the Hy values.)

[2] A. Colmenarez, R. Lopez and T. Huang, “3D Model-based
Head Tracking”, Visual Communication and Image Processing,
San Jose, CA, 1997.
[3] Z.Zivkovic,F.van der Heijden, "A stabilized adaptive
appearance changes model for 3D head tracking", Second
International Workshop on Recognition, Analysis and Tracking
of Faces and Gestures in Real-time Systems, Vancouver,
Canada, July 2001.
[4] K.Nummiaro, E. Koller-Meier, and L.V.Gool, “Object
Tracking with an Adaptive Color-Based Particle Filter”, Proc.
Symposium for Pattern Recognition of the DAGM, pp. 591-599,
2003
[5]S. Birchfield, “Elliptical Head Tracking Using Intensity
Gradients and Color Histogram”, IEEE Conference on
Computer Vision and Pattern Recognition, Santa Barbara, CA,
pp. 232-237 ,June 1998.

Figure 3. Quantitative comparison between the gradient
model and the combined model. Left column: tracked ycoordinates (dashed and red) and the ground truth (solid
and cyan). Right column: tracked Hy values. Top row: from
the gradient model alone. Bottom row: from the combined
model.

[6] R. Lienhart and J. Maydt, “An Extended Set of Haar-like
Features for Rapid Object Detection”, IEEE International
Conference on Image Processing (ICIP 2002), Rochester, USA,
September, 2002.
[7] J. Yang and A. Waibel, “A Real-time Face Tracker”, Proc.
of WACV 1996, pp. 142-147.

3.2. Experiments on real sequences
To demonstrate the tracker’s robustness, we conducted
extensive experiments on real image sequences. Figure 4
shows a sample result, with comparison with the color
histogram tracker of [4]. The result from using only the
gradient model is also given, as comparison. It is obvious
that the proposed method handles much better the
challenging situations including full 360-degree rotation
and camera zooming. This and other experimental results
show that, the color model contributes to the tracker by
ignoring cluttered objects background, while the gradient
model help to compensate for the dramatic color change
due to head rotation. Thus the combined model produces
on average the best results.
4. CONCLUSION AND FUTURE WORK
We proposed a head tracker using both the color and the
gradient information in a particle-filter-based framework.
Experiments show that these two complementary cues
contribute to a more robust tracker, which handles
difficult situations that a tracker based on either of the
cues alone cannot handle well. Future work along this
direction includes a complete investigation of the
tracker’s performance under multiple human subjects
with occlusion.
REFERENCES
[1] C.Shen, A.van den Hengel, and A. Dick, “Probabilistic
Multiple Cue Integration for Particle Filter Based Tracking”,
Proc. VIIth Digital Image Computing: Techniques and
Applications, Sydney, Australia Dec.2003.

Figure 4. Sample results from a real sequence. Left
column: Color histogram tracker of [4]. Center column:
the proposed method with only the gradient component.
Right column: the proposed method with both color and
gradient components. Note that, the first two frames is an
example of camera zooming, while the other frames
depict full 360-degree rotation.

STRUCTURE-PRESERVING IMAGE QUALITY ASSESSMENT
Yilin Wang1 Qiang Zhang2 Baoxin Li1
1

Department of Computer Science, Arizona State University, Tempe AZ
2
Advanced Image Research Lab, Samsung Electronic, Pasadena CA
{ywang370,Baoxin.Li}@asu.edu q.zhang1@samsung.com
ABSTRACT

Perceptual Image Quality Assessment (IQA) has many applications. Existing IQA approaches typically work only
for one of three scenarios: full-reference, non-reference, or
reduced-reference. Techniques that attempt to incorporate
image structure information often rely on hand-crafted features, making them difficult to be extended to handle different scenarios. On the other hand, objective metrics like
Mean Square Error (MSE), while being easy to compute, are
often deemed ineffective for measuring perceptual quality.
This paper presents a novel approach to perceptual quality
assessment by developing an MSE-like metric, which enjoys
the benefit of MSE in terms of inexpensive computation and
universal applicability while allowing structural information
of an image being taken into consideration. The latter was
achieved through introducing structure-preserving kernelization into a MSE-like formulation. We show that the method
can lead to competitive FR-IQA results. Further, by developing a feature coding scheme based on this formulation, we extend the model to improve the performance of NR-IQA methods. We report extensive experiments illustrating the results
from both our FR-IQA and NR-IQA algorithms with comparison to existing state-of-the-art methods.
Index Terms— Mean Square Error, Image Quality Assessment, kernel method.
1. INTRODUCTION
Perceptual image quality assessment (IQA) has many multimedia applications such as image denoising [1] and image
transmission. Based on the degree of reliance on a reference image, IQA models can be divided into three categories:
Full Reference IQA (FR-IQA), Reduced Reference IQA (RRIQA) and Non-Reference IQA (NR-IQA). FR-IQA needs a
reference image for estimating the distortion of a target image. Numerous FR-IQA models have been proposed, including those that incorporate image structure information [2],
mutual information [3, 4], and wavelet information [5], etc.
For RR-IQA, only partial information of a reference image
is needed, while NR-IQA models predict image quality without any information from reference images. Recent NR-IQA

approaches have employed Bag of Words [6, 7], and DCT
transformation [8], etc. In general, existing approaches belong to one of the above three categories and only work for
their respective scenario. One objective of this paper is to
develop a unifying approach for both FR-IQA and NR-IQA,
hence maximizing the applicability of the IQA model.
In parallel with perceptual/subjective IQA, various objective measures have been employed in multimedia. The most
widely-used one is Mean Square Error (MSE) or its variants,
due to its simplicity and general effectiveness. MSE simply
measures the average per-sample difference between two signals. Since MSE is convex and differentiable, it is easy to use
optimization approaches for finding solutions to various models based on MSE. Unfortunately, it is well understood (e.g.,
[9]) that pixel-wise MSE (or its variants) is not a good measure for perceptual quality, primarily due to the fact that no
structural information of the image is considered in computing MSE. Some attempts have been tried to remedy this. For
example, in [10], a “perceptual-aware” MSE was proposed by
adding Gaussian filter or gradient operator, which helped to
improve the correlation between the human perceptual score
and the objective metric.
In this work, we aim at building a structure-preserving
MSE (SPMSE) which not only retains the computational efficiency and nice mathematical properties of MSE but also
leads to the development of effective IQA metrics. Our new
formulation of MSE is developed by employing the ‘kernel
trick’: we use HOG feature [11], an effective and efficient descriptor for describing object structures, as an quality kernel
between two images, and show that the resultant formulation
leads to many of the desired properties. For FR-IQA, experimental results show that SPMSE performs statistically better
than the well-known SSIM method and leads to very competitive performance compared with with other state-of-the-art
methods on three benchmark datasets.
Moreover, we show that the proposed SPMSE can be employed in recent Bag-of-Words based models for NR-IQA
[7, 6]. In such existing models, the distortion image is represented by a feature vector which is the coefficients under
a codebook. However, most existing approaches focus on
designing hand-crafted features to be used by the codebook
training, rather than optimal representation under the code-

book. In other words, the feature encoding step, which should
contribute to the final quality metric significantly, has been
largely ignored. In this work, we propose and empirically
compare several coding schemes for NR-IQA based on the
SPMSE framework, and show that, compared to vector quantization or sparse coding, the proposed method, structure preserving coding, is more effective for NR-IQA models.
The rest of the paper is organized as follows: Section 2
reviews the related work. Section 3 describes SPMSE for
FR-IQA in details and introduces our SPMSE encoding for
NR-IQA. In Section 4, experimental results on widely-used
datasets are reported; and finally in Section 5, conclusions are
made and future improvements and issues are discussed.
2. RELATED WORK
We review the related work on FR-IQA and recent advanced
methods in NR-IQA.
FR-IQA: One of the most widely-used and influential
FR-IQA method is Structure SIMilarity Index (SSIM). It is
based on the assumption that the underlying image quality
score is highly related to the image structure. For a pair of
a reference image s and a distortion image t, SSIM compares them with image luminance, contrast and structure as:
s µt +C1 )(2σst +C2)
SSIM (s, t) = (µ(2µ
2 +µ2 +C1)(σ 2 +σ 2 +C2) , where, for image
s
s
t
t
i, j, µi is the local mean intensity, σi is the local variance
and σij is the local covariance. Visual Information Fidelity
(VIF) is another IQA approach that captures the signal statistics for image fidelity assessment. In [12], it was argued that
the HSV space is appropriate for full-reference image quality
assessment, owing to distinctive features of high-quality and
low-quality images in this space. In [13], the author provides
a gradient similarity method for image quality assessment.
For a thorough survey of modern IQA development, please
refer to [14]. In contrast to the methods discussed above, the
proposed framework starts from the widely used MSE and
applies kernel method to the objective function for preserving
image structure.
NR-IQA: When images are transferred to some specific
domain, e.g., the DCT domain, local descriptors may be modeled by some parametric distribution, based on this, some previous works [8, 15, 16] on NR-IQA have focused primarily
on Natural Scene Statistics (NSS). On the other hand, inspired by the success of Bag-of-Words approaches in computer vision, [7] uses visual codebook to assess image quality. Quality measure of a new image is obtained by computing the average of quality scores of the codewords, weighted
by their distances to visual words in the image. However,
the method requires a large number of codewords and precomputed Gabor-filters. Other than hand-crafted features, [6]
proposes an unsupervised feature-learning method based on
raw image patches. The proposed method is similar to [7]
in term of its codebook-based encoding. However, our goal
is to learn the features based on raw image patches for both

non-distortion images and distortion images.
3. THE PROPOSED SPMSE FRAMEWORK
3.1. Structure Persevering Mean Square Error
Given two signals s, t ∈ RN , the objective function of MSE
is ||s−t||22 /N . In SPMSE, we introduce a non-linear structure
extractor term for each signal as N1 ||φ(s) − φ(t)||22 where φ
is a mapping function, which maps the original data space to
a new feature space. The objective function of SPMSE is:
1
||φ(s) − φ(t)||22
N
1
= (hφ(s)φ(s)i − 2 hφ(s)φ(t)i + hφ(t)φ(t)i)
N
(1)

SP M SE(s, t) =

In Eq. 1, the SPMSE is guaranteed to be non-negative, and
thus it can be viewed as a distance measure. Introducing a
kernel operation, we can re-write SPMSE as:
SP M SE(s, t) =

1
(K(s, s) − 2 × K(s, t) + K(t, t)) (2)
N

where K is a valid Mercer kernel [18], which can be viewed
as a non-linear feature similarity measure for the signals. In
the next section we will discuss how to choose K.
3.1.1. Kernel Selection
As definition in [18]: “a kernel is a function that returns the
inner product between the images of two inputs in some feature space”. The intuition of the kernel method is to measure the similarity between two data vectors in a new feature space. The most widely used kernels for images (signals) are polynomial kernels and RBF kernels [18]. A polynomial kernel is given by K(x, y) = (hx, yi + R)d where R
and d are kernel parameters. The RBF kernel is defined as
||x−y||2

K(x, z) = exp −2σ2 . However, trivially bringing them to
the proposed objective function is not a good choice for FRIQA, since the resultant MSE-based kernel function is still
based on pixel-wise computation and hence losing the sight
of the image structure distortion, which has been argued to be
an essential factor for IQA [4, 2].
Thus, one of our goals is to find a proper kernel that
helps to retain structural information of an image. Inspired
by its success in object detection, e.g. [11], we employ Histogram of Oriented Gradient(HOG) as image quality descriptor. HOG is one of the most-used low-level vision features
for object detection and recognition, and the essential thought
behinds HOG is that the local appearance and structure in images can be described by its gradient distribution. Based on
the following theorem, we show it can be incorporated into
our proposed framework as a valid kernel function.

Theorem 1: HOG operator is a valid kernel function.
Proof : Let θi and Mi be the orientation and magnitude
of gradient at pixel i. Then the HOG feature of each pixel is
represented by a hard binning indicator.

θi
=n−1
1 if 2π
(3)
δin =
0 otherwise
For each image
P block P , the oriented gradient is represented
as σ(P ) =
i∈P Mi · δi . When measuring the similarity
between patches from two different images, it is equivalent to
match the patches in the feature space. Thus, we can represent
the similarity between image patches in the feature space with
a linear kernel:
X
X
XX
K(P, Q) =
Mi · δi
Mi0 · δi0 =
Mi δiT δi0 Mi0
i0 ∈Q

i∈P

=

XX

i∈P i0 ∈Q

Mi Mi0 δiT δi0

0

i∈P i ∈Q

(4)
where P, Q are two patches from two images. Since Mi Mi0
is a non negative scalar and δit δi0 is the inner product of two
vectors, then we can substitute two linear kernel KM (i, i0 ) =
Mi · Mi0 , Kδ (i, i0 ) = δiT δi0 in Eq. 4. Thus, K(P, Q) is a
valid kernel [18] and it provides a kernel view of HOG.
It is worth noting that, in contrast to [13], where the metric is simply based on the similarity of gradient value from
two signals. In the proposed framework, inspired by the success of using HOG for object detection, we utilize the property of the HOG for image structure description. Specifically,
in Theorem 1, KM (i, i0 ) measures the similarity of gradient
0
magnitude of two pixels and Kδ (i, i0 ) measures the similarity
of gradient orientations of two pixels. Thus, instead of measuring the pixel similarity in MSE, the proposed SPMSE can
be viewed as a structure similarity measure for image patches
(e.g. 8 × 8 rectangles in HOG).

the nearby bases of the encoded data. Based on these observations, we compare different coding schemes and then propose
a novel feature coding scheme for NR-IQA, which supports
feature learning with the proposed SPMSE metric.
Let X be a set of M-dimensional feature vectors extracted from images, i.e., X = [x1 , x2 , ....., xN ] ∈ RM ×N .
C = [c1 , c2 , ..., cN ] is a set of code coefficients for X based
on codebook B = [b1 , b2 , ...bK ] ∈ RM ×K . C can be generated by different coding schemes for image representation.
Based on [19, 17, 20], the proposed coding scheme solves the
following optimization problem:
argmin
c

N
X

||xi − Bci ||22 + λ||Di ci ||22 + µ|ci |

where Di ∈ RK×K is a diagonal matrix, with each element
in the diagonal representing the SPMSE score of input image
patch i and code basis j, i.e.Di(j,j) = SP M SE(xi , bj ). The
second term in Eq. 5 gives the input patch freedom to decide
proportion of similar structure bases in the codebook, while
the third term is the sparse regularization term which makes
the nonlinear representation of the features. Unfortunately,
the above objective function is computational expensive. To
alleviate this, we propose an approximation scheme by relaxing the sparse term in the objective function to 1T ci = 1, ∀i,
which still achieves sparsity if we set small values in the solution to zero. Since the Eq. 5 can be decomposed, the encoded
feature ci can be obtained by solving the following optimization problem:
argmin
c

J = ||xi − Bci ||22 + λ||Di ci ||22
(6)

subject to 1T ci = 1;
The Lagrangian function of Eq. 6 is:
L = ||xi − Bci ||22 + λ||Di ci ||22 + τ (1T ci − 1)

In Section 3.1, we proposed a SPMSE framework for the FRIQA problem, which captures image local structures instead
of measuring the pixel-wise error. Noting Eq. 1 is convex
and differentiable, we can easily build an objective function
to minimize. We now show how the idea may be extended to
handle NR-IQA problems.
In recent NR-IQA approaches [7, 6, 15, 17], different features have been designed. However, feature coding has been
largely ignored. In other words, how to efficiently encode
the features for NR-IQA is still not well addressed. In [7],
hard vector coding was used, and in [6], the authors argue soft
coding is better, while [17] argues sparse coding is more efficient. In [19], locality linear coding is proposed, the authors
observed that the non-zero coefficients are often assigned to

(7)

where τ is Lagrangian multiplier. Taking the derivation of L
and setting it to zero, we can obtain:
τ 1 = 2B T xi − 2B T Bci − 2λDi2 ci

3.2. Structure-Persevering Coding

(5)

i=1

(8)

T

Trick 1: 1 ci = 1 and xTi Bci is a scalar, Eq. 8 can be written
as:
τ 1 + 2xTi Bci 1 = 2B T xi 1T ci − 2B T Bci − 2λDi2 ci
+ 21xTi Bci

(9)

1
⇒ − (τ + xTi Bci )1 = (B T B − B T xi 1T + λDi2 − 1xTi B)ci
2
(10)
Trick 2: xTi xi 1T ci is a scalar, thus it can be added on the
both sides:
1
− (τ + xTi Bci )1 + xTi xi 1T ci 1 = (B T B − B T xi 1T
2
+ λDi2 − 1xTi B)ci
+ 1xTi xi 1T ci
(11)

MSE
SSIM
VIF
IFC
MAD
SPMSE

Table 1. PLCC comparison of different FR-IQA models
LIVE(779 images) TID2008(1300 images) CSIQ(750 images)
0.8739
0.7649
0.8882
0.9451
0.8530
0.9188
0.9604
0.8938
0.9321
0.9268
0.8007
0.8912
0.9394
0.8306
0.8881
0.9364
0.8876
0.9213

1
⇒ − (τ + xTi Bci − 2xTi xi 1T ci )1 = (B T B − B T xi 1T
2
+ λDi2 − 1xTi B
+ 1xTi xi 1T )ci
(12)
Finally, the closed form solution can be obtained after normalization as :
cei = ((B T − 1xTi )(B T − 1xTi )T + λDi2 ) \ 1
ci = cei /1cei

(13)

Compared to hard vector quantization encoding [7] which
represents images from a single basis, the approximation
scheme of Eq. 5 will achieve much smaller error because of
the use of multiple bases (soft coding). It is worth noting that
the method in [19] is based on pixel-wise representation, lacking the structural information captured by our SPMSE-based
scheme. Moreover, we empirically observed that the coding
results from [17] tend to select codebook bases that were from
images under different distortions, while code bases from our
approach tend to belong to images of similar distortion.
4. EXPERIMENTS
4.1. FR-IQA Evaluation Protocol
Database for FR-IQA evaluation: To evaluate the proposed
framework, we tested it on three benchmark IQA datasets:
LIVE[21], TID2008[22], and CSIQ[12]. The images in
these datasets are generated with different type of distortion
and associated with human/subjective opinion score. The
LIVE database contains 29 reference images and 779 distorted images with 5 different distortions: JPEG2000 compression (JP2K), JPEG compression (JPEG), additive white
noise(AWN), Gaussian blur (GB), and Fast fading (FF). The
TID2008 database contains 25 reference images and 1700 distorted images with 17 different noise types. Since the last
four distortions (totally 400 images) are not structure distortions, e.g. intensity shift, which is a highly subjective task for
people to distinguish with, we reported the results on first 13
distortions. This protocol also has been used in [6, 17]. The
CSIQ contains 30 reference images and 866 distorted images
generated from JP2K, JPEG, AWN, GB, and pink Gaussian
noise, the contrast change is also not the structure distortion

0.8279
0.8962
0.9226
0.8599
0.8762
0.9096

for us to deal with. Thus, the number of images from CSIQ is
750.
Evaluation: We evaluate the performance of different methods using Pearson Linear Correlation Coefficient
(PLCC) and Spearman Rank Order Correlation Coefficient
(SROCC). PLCC is considered as a measurement of the prediction accuracy and SROCC is viewed as an evaluation of
how well the relationship between the predicted score and the
subject opinion score can be described. A good IQA model
should have high PLCC and SROCC.
4.2. NR-IQA Evaluation Protocol
Database for NR-IQA evaluation and codebook construction: We use LIVE database for evaluation and adopt CSIQ
databse for codebook construction based on the following reasons: First, there is no overlap between CSIQ dataset and
LIVE dataset. Second, both CSIQ and LIVE contain four
types of distortion: JP2K, JPEG, GB, AWN. Thus, it is reasonable to use codebook generated from CSIQ to represent
the images in LIVE instead of TID2008 which has much more
noise types than CSIQ. For each image in CSIQ, we randomly
extract 10000 7 by 7 raw patches, then using K-means clustering to generate the codebook. In our experiment, the codebook is fixed by 10000 × 49. This protocol is also used in
[7].
NR-IQA Regression and Evaluation: The predicted
score is calculated from linear support vector regression
(SVR) directly. Since codebook is constructed from unlabeled data, in LIVE database, we randomly pick 80% images
associate with human subject score to train the SVR and remaining 20% for testing. Moreover, we repeat the train-test
scheme 100 times for cross-validation. It is worth noting that
both the training set and the testing set only contain the distorted images. Finally, we use max-pooling to represent image feature.
4.3. Comparison with FR-IQA and NR-IQA algorithms
In this sub-section, we first compare the results of the proposed method with state-of-the-art FR-IQA models including
SSIM [2], VIF[3], IFC[4], MAD [12]. Table 1 and Table 2 list
the results of SROCC and PLCC of different FR-IQA models
respectively. The results are reported from the original papers

with default parameter settings. It is worth noting that PLCC
results are reported after logistic regression (Eq. (14) and Eq.
15) between predicted score and subject opinion score, which
follows the instruction reported in [23].
From Table 2 and Table 1, we can draw the following conclusions. First, the proposed method outperforms a large margin to MSE and is superior to SSIM. Second, the proposed
method is comparable to other state-of-the-art method, e.g.,
VIF, MAD, in terms of average resuls among three benchmark datasets. Moreover, in Table 3, we compare the speed1
of the proposed method and other top 3 FR-IQA metrics. It
can be seen that the proposed is efficient in terms of computation time.
Table 3. Speed Comparison with Top 3 metrics in FR-IQA to
MSE
MSE
SSIM SPMSE
VIF
MAD
Time(s)
0.0021 0.031
0.043
0.974
2.07
ratio to MSE
1
15
20
458
986
In Table 4 and Table 5, we report the results of our encoding scheme with comparison of state-of-the-art NR-IQA
methods and other encoding schemes. The compared methods including BIQI [7], CORINA[7], DIIVINE [16] and BLIINDS (SVM) [8] and we also compared our encoding methods with hard encoding (HC), sparse encoding (SC) [20] and
locality linear encoding (LLC) [19]. From the result we can
see that our proposed achieves best result among all the encoding schemes which have same codebook, meanwhile, our
result is comparable to state-of-the-arts models, e.g., CORINA. Noting that the evaluation of the proposed method
only employs general procedures of BOW, the results can
be further improved by employing a more powerful regressor (e.g. random forest) or using precomputed features (e.g.,
NSS) instead of raw image patches.

Quality(x) = β1 logistic(β2 , (x − β3 )) + β4 x + β5 (14)

logistic(τ, x) =

1
1
−
2 1 + exp(τ x)

(15)

5. DISCUSSION AND FUTURE WORK
We proposed a simple yet effective approach for image quality assessment. First, we proposed a structure-preserving
MSE-like error function for FR-IQA, and the experimental
results show that our method is competitive with respect to
the state-of-the-art methods and in particular, outperforms the
1 All

the codes are implemented by Matlab and obtained from original
authors’ webpage. The HOG computation part is written in C and compiled
by Matlab.

Table 4. SROCC comparison of different NR-IQA models on
LIVE
Method
JP2K JPEG AWN
GB
FF
ALL
PSNR
0.872 0.885 0.941 0.764 0.875 0.867
SSIM
0.939 0.946 0.965 0.909 0.941 0.913
BIQI
0.856 0.786 0.972 0.910 0.762 0.819
CO0.943 0.955 0.976 0.969 0.906 0.942
RINA
DIVI0.913 0.910 0.984 0.921 0.863 0.916
INE
BLI0.929 0.955 0.956 0.923 0.889 0.931
INDS
SPMSE 0.936 0.948 0.952 0.958 0.872 0.930
LLC
0.921 0.941 0.942 0.932 0.862 0.909
HC
0.919 0.948 0.945 0.908 0.905 0.917
SC
0.926 0.958 0.952 0.941 0.852 0.921

Table 5. PLCC comparison
LIVE
Method
JP2K JPEG
PSNR
0.873 0.874
SSIM
0.920 0.955
BIQI
0.809 0.901
CO0.951 0.965
RINA
DIVI0.922 0.921
INE
BLI0.935 0.968
INDS
SPMSE 0.947 0.951
LLC
0.931 0.941
HC
0.921 0.950
SC
0.929 0.965

of different NR-IQA models on
AWN
0.928
0.982
0.954

GB
0.774
0.891
0.829

FF
0.869
0.939
0.733

ALL
0.855
0.906
0.821

0.987

0.968

0.917

0.935

0.988

0.923

0.888

0.917

0.980

0.938

0.896

0.930

0.971
0.943
0.965
0.959

0.970
0.942
0.929
0.945

0.899
0.872
0.883
0.892

0.934
0.919
0.917
0.925

well-known SSIM. Second, we showed that the proposed approach can be applied to the NR-IQA framework as well,
through incorporating it in a coding scheme. Even with only a
fixed and unoptimized codebook, the experimental results still
showed performance comparable to current approaches. Future efforts include at least two possible exentions: a learningbased method for selecting a kernel function more efficiently,
and codebook learning for improved NR-IQA.

6. ACKNOWLEDGMENT
Yilin Wang and Baoxin Li were supported in part by a grant
(#1135616) from the National Science Foundation (NSF).
Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.

MSE
SSIM
VIF
IFC
MAD
SPMSE

Table 2. SROCC comparison of different FR-IQA models
LIVE(779 images) TID2008(1300 images) CSIQ(750 images)
0.8756
0.7118
0.9060
0.9479
0.8742
0.9247
0.9636
0.8731
0.9282
0.9259
0.7589
0.8827
0.9438
0.8694
0.9604
0.9564
0.8887
0.9353

7. REFERENCES
[1] Keigo Hirakawa and Thomas W Parks, “Image denoising using
total least squares,” Image Processing, IEEE Transactions on,
vol. 15, no. 9, pp. 2730–2742, 2006.

Weighted
0.8362
0.8946
0.9130
0.8383
0.9142
0.9179

of strategy,” Journal of Electronic Imaging, vol. 19, no. 1, pp.
011006–011006, 2010.
[13] Anmin Liu, Weisi Lin, and Manish Narwaria, “Image quality
assessment based on gradient similarity,” Image Processing,
IEEE Transactions on, vol. 21, no. 4, pp. 1500–1512, 2012.

[2] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli, “Image quality assessment: from error visibility to
structural similarity,” Image Processing, IEEE Transactions
on, vol. 13, no. 4, pp. 600–612, 2004.

[14] Weisi Lin and C-C Jay Kuo, “Perceptual visual quality metrics: A survey,” Journal of Visual Communication and Image
Representation, vol. 22, no. 4, pp. 297–312, 2011.

[3] Hamid R Sheikh and Alan C Bovik, “Image information and
visual quality,” Image Processing, IEEE Transactions on, vol.
15, no. 2, pp. 430–444, 2006.

[15] Anush Krishna Moorthy and Alan Conrad Bovik, “A twostep framework for constructing blind image quality indices,”
Signal Processing Letters, IEEE, vol. 17, no. 5, pp. 513–516,
2010.

[4] Hamid R Sheikh, Alan C Bovik, and Gustavo De Veciana, “An
information fidelity criterion for image quality assessment using natural scene statistics,” Image Processing, IEEE Transactions on, vol. 14, no. 12, pp. 2117–2128, 2005.
[5] Damon M Chandler and Sheila S Hemami, “Vsnr: A waveletbased visual signal-to-noise ratio for natural images,” Image
Processing, IEEE Transactions on, vol. 16, no. 9, pp. 2284–
2298, 2007.
[6] Peng Ye, Jayant Kumar, Le Kang, and David Doermann, “Unsupervised feature learning framework for no-reference image
quality assessment,” in Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp.
1098–1105.
[7] Peng Ye and David Doermann, “No-reference image quality
assessment based on visual codebook,” in Image Processing
(ICIP), 2011 18th IEEE International Conference on. IEEE,
2011, pp. 3089–3092.
[8] Michele A Saad, Alan C Bovik, and Christophe Charrier,
“Blind image quality assessment: A natural scene statistics approach in the dct domain,” Image Processing, IEEE Transactions on, vol. 21, no. 8, pp. 3339–3352, 2012.
[9] Zhou Wang and Alan C Bovik, “Mean squared error: love it
or leave it? a new look at signal fidelity measures,” Signal
Processing Magazine, IEEE, vol. 26, no. 1, pp. 98–117, 2009.
[10] Wufeng Xue, Xuanqin Mou, Lei Zhang, and Xiangchu Feng,
“Perceptual fidelity aware mean squared error,” .
[11] Navneet Dalal and Bill Triggs, “Histograms of oriented gradients for human detection,” in Computer Vision and Pattern
Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on. IEEE, 2005, vol. 1, pp. 886–893.
[12] Eric C Larson and Damon M Chandler, “Most apparent distortion: full-reference image quality assessment and the role

[16] Anush Krishna Moorthy and Alan Conrad Bovik, “Blind image quality assessment: From natural scene statistics to perceptual quality,” Image Processing, IEEE Transactions on, vol. 20,
no. 12, pp. 3350–3364, 2011.
[17] Lihuo He, Dacheng Tao, Xuelong Li, and Xinbo Gao, “Sparse
representation for blind image quality assessment,” in Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 1146–1153.
[18] John Shawe-Taylor and Nello Cristianini, Kernel methods for
pattern analysis, Cambridge university press, 2004.
[19] Jinjun Wang, Jianchao Yang, Kai Yu, Fengjun Lv, Thomas
Huang, and Yihong Gong, “Locality-constrained linear coding for image classification,” in Computer Vision and Pattern
Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010,
pp. 3360–3367.
[20] Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Y Ng,
“Efficient sparse coding algorithms,” .
[21] Hamid R Sheikh, Muhammad F Sabir, and Alan C Bovik, “A
statistical evaluation of recent full reference image quality assessment algorithms,” Image Processing, IEEE Transactions
on, vol. 15, no. 11, pp. 3440–3451, 2006.
[22] Nikolay Ponomarenko, Vladimir Lukin, Alexander Zelensky,
Karen Egiazarian, M Carli, and F Battisti, “Tid2008-a database
for evaluation of full-reference visual quality assessment metrics,” Advances of Modern Radioelectronics, vol. 10, no. 4, pp.
30–45, 2009.
[23] Video Quality Experts Group et al., “Final report from the
video quality experts group on the validation of objective models of video quality assessment,” VQEG, Mar, 2000.

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 12, NO. 4, JUNE 2010

233

A Bayesian Approach to Automated
Creation of Tactile Facial Images
Zheshen Wang, Student Member, IEEE, and Baoxin Li, Senior Member, IEEE

Abstract—Portrait photos (facial images) play important social and emotional roles in our life. This type of visual media
is unfortunately inaccessible by users with visual impairment.
This paper proposes a systematic approach for automatically
converting human facial images into a tactile form that can
be printed on a tactile printer and explored by a user who is
blind. We propose a deformable Bayesian Active Shape Model
(BASM), which integrates anthropometric priors with shape and
appearance information learnt from a face dataset. We design
an inference algorithm under this model for processing new face
images to create an input-adaptive face sketch. Further, the model
is enhanced by input-specific details through semantic-aware
processing. We report experiments on evaluating the accuracy of
face alignment using the proposed method, with comparison with
other state-of-the-art results. Furthermore, subjective evaluations
of the produced tactile face images were performed by 17 persons
including six visually-impaired users, confirming the effectiveness
of the proposed approach in conveying via haptics vital visual
information in a face image.
Index Terms—Image matching, image shape analysis, pattern
recognition, tactile graphics.

I. INTRODUCTION

D

IGITAL visual information in graphical forms (e.g., digital images, maps, diagrams, etc.) has become prevalent in
the information era and the sighted people can easily enjoy the
added value of graphical contents. Unfortunately, people with
visual impairment are partially or totally deprived of this benefit.
Although modern computer technologies have provided various
text-to-Braille/audio solutions that enable convenient access to
text, computer users with visual impairment still cannot access
graphical contents without the assistance of sighted people. The
typical procedures for manually producing tactile graphics by
sighted professionals are in general time-consuming and laborintensive, and hence the coverage is extremely limited. Further,
there is no on-demand and independent availability if the production has to be done by third-party professionals. What are
still missing are automatic approaches that support real-time and
independent access to graphical contents by users with visual
impairment.

Manuscript received June 02, 2009; revised October 07, 2009 and December
31, 2009; accepted January 05, 2010. First published March 22, 2010; current
version published May 14, 2010. This work was supported in part by an NSF
grant (Award # 0845469). The associate editor coordinating the review of this
manuscript and approving it for publication was Dr. Nicu Sebe.
The authors are with the Department of Computer Science and Engineering,
Arizona State University, Tempe, AZ 85287 USA (e-mail: zheshen.wang@asu.
edu; baoxin.li@asu.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TMM.2010.2046267

Studies along this direction typically need to first address the
fundamental issue of image simplification, due to the extremely
limited bandwidth of tactile perception compared with that of
vision. Existing work relies on either computer-aided manual
processing (e.g., using drawing software) or simple image processing steps such as edge detection. Despite the existence of
some initial attempts [1]–[3], this fundamental step towards automating the creation of tactile graphics from images remains to
be largely unsolved. One prominent challenge is that low-level
image processing techniques such as edge detection cannot ensure to retain semantically meaningful information, especially
if the techniques are expected to work for any types of graphics.
For example, broken and scattered edge segments may serve
only to confuse a blind user if they are directly mapped to tactile
lines; and attempts to clean up the edges, such as linking short
ones to form a long contour, may do harm if those processing
steps are purely driven by the data.
In this paper, we limit the scope of our study to a special
type of graphic, human facial images, for the special value that
they have in a person’s social and emotional life. We aim at developing a systematic approach to automatic conversion of a
human face image into its tactile form. Limiting the scope to
this special type of images enables us to introduce higher-level
semantics for guiding lower-level image processing steps in designing robust algorithms for automated visual-to-tactile conversion. Exploiting the constraints imposed by knowing that the
image contains a face, we first propose a deformable Bayesian
Active Shape Model (BASM), which integrates anthropometric
facial priors with both shape and appearance information learnt
from a face dataset, for modeling human faces. Then a statistical-sampling-based inference procedure is introduced under
the model, for obtaining a data-adaptive version of the model
for any given face image. Serving as a starting point, this model
enables additional semantic-aware processing steps that are designed to enrich the sketchy face model with more input-specific
details, resulting in the final tactile face images. As such, the
proposed approach combines anthropometric prior knowledge,
learnt model generality, and given data specificity to automatically create an informative tactile representation of the original
face image. Such a tactile representation can be readily rendered
by a tactile printer, and thus potentially provide a desired solution to the problem of creating on-demand tactile faces independently by a user with visual impairment. Fig. 1 illustrates
the overall processing flow of the proposed approach.
The rest of the paper is organized as follows. We review related work in Section II. In Section III, we present the proposed
visual-to-tactile face conversion approach based on a novel
BASM, followed by details of obtaining anthropometric face

1520-9210/$26.00 © 2010 IEEE

234

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 12, NO. 4, JUNE 2010

Fig. 1. Overall processing flow of the proposed approach (corresponding section numbers are specified in the parentheses).

priors in Section IV and computing of the shape likelihood in
Section V. Semantic-aware enrichment steps for creating the
final tactile face images are described in Section VI. We present
face alignment results with comparisons to other state-of-the-art
methods and report systematic user evaluation of the outputs
produced by our approach in Section VII. Section VIII concludes the paper with brief discussion on future work.
II. RELATED WORK
Conventional approaches to manual creation of tactile
graphics involve many tedious tasks [4] that are time-consuming
and labor-intensive. Automated approaches to visual-to-tactile
conversion have been the focus of some recent studies. Some
existing work can only handle simple line-drawing graphics
(e.g., [5] and [6]), with little effort dealing with acquired images
such as portrait images that we attempt to address in this paper.
For acquired images, the work of [1] relied on simple image
processing steps such as negation and edge detection, and Way
et al. [2], [3] proposed to simplify images mostly by edge
detection. The system developed in [7] resorts to Photoshop
for image simplification, which still requires some manual
efforts from a sighted person and thus does not address the
need of an automated solution. In [8], a multimodal approach
was proposed to present digital graphics. Although the concept
of semantics-aware processing was introduced to modulate the
edge detection step, refined solutions to handle specific types
of graphics remain to be developed.
Since we focus on human face images in this paper, we also
briefly review recent face alignment work in the below, which
is a key component in our approach. Face alignment is an active research area with many research papers in recent years.
In the pioneering work of Active Shape Model (ASM) [9], the
contours of major facial features are represented by a set of feature points, and in matching a model to a given image, feature
points are updated iteratively by searching along profiles around
the current positions and fitting to a set of model parameters.
Bayesian Tangent Shape Model (BTSM) [10] is another derivation of ASM proposed to infer shape parameters by the EM algorithm. While being useful, ASM may suffer from the local

minima problem if the optimization of shape points is based on a
gradient decent search scheme [11]. To alleviate this problem, a
hierarchical CONDENSATION approach was discussed in [12]
to search the MAP estimates of shape configurations.
Deformable model-based approaches often encounter difficulties in achieving desired specificity to a particular instance
while retaining enough model generality. A common remedy
is to impose prior knowledge on possible shape deformation.
Typical prior information utilized is linear shape deformation
subspace learnt from training images (e.g., [9] and [10]). Nevertheless, it often strongly restricts the deformation and biases
towards the training set. A number of approaches have been proposed to remedy this problem. Kernel PCAs were proposed to
extend the linear PCA subspace in order to retrieve more shape
variations [13], [14]. Huang et al. [11] created separated deformable models for each face component and use a probability
distribution function to encode the interrelationship among parameters of all modeled components by constrained Gaussian
Process Latent Variable Model. Liang et al. [15] integrated the
Markov Network search with the global shape prior to improve
the alignment. Gu et al. proposed a shape regularization model,
which incorporates nonlinear shape prior from a mixture of constrained Gaussian components with extra noises [16]. While improved robustness for exaggerating expressions and large occlusions was shown, shape priors in the work are still purely built
upon the training set. Other forms of prior information include
generic properties of local curves (e.g., continuity and smoothness) [17]. There also approaches that use fewer number of features points (10 to 20) [29], [31], which do not provide desired
level of detail for tactile conversion. Other contributions to the
face alignment problem include [18]–[22], which are less relevant to the focus of our task.
In our recent work [23] along the same direction of this study,
we reported preliminary results with a simpler approach which
did not take into consideration the anthropometric priors in the
modeling. Consequently, the results were not as good as desired,
albeit encouraging. Also, the evaluation of [23] was preliminary,
with only one blind user. In this paper, we propose to use domain
knowledge of the shape, i.e., anthropometric face constraints, as
the prior in developing the BASM. Such a prior reflects common
biological features of human faces and thus is potentially useful
for providing desired constraints in generating physically-meaningful shapes from a generic model. We also report experiments
of more comprehensive evaluation with 17 people including six
visually-impaired persons.
III. PROPOSED APPROACH
In this section, we present the proposed approach for creating
tactile facial images automatically. This approach consists of
three major steps. We first model human faces using a novel
BASM, in which a deformable shape model of human faces is
first learnt from a training set, with prior anthropometric constraints incorporated in the model. Then, given a test face image,
the set of model parameters that best explains the image data is
estimated through Bayesian inference with statistical sampling
approach. With the face model and the input image aligned, we
further employ a semantic-aware processing step to enrich the
sketchy model in producing the final tactile face image. These

WANG AND LI: A BAYESIAN APPROACH TO AUTOMATED CREATION OF TACTILE FACIAL IMAGES

Fig. 2. Left: Anchor point-based face model. Right: Point-paths and corresponding key points.

three steps are described in the following subsections A to C respectively, with elaborated details presented in Sections IV–VI.
A. BASM for Face Modeling
ASM is widely used for modeling landmark-based shapes.
However, traditional ASM suffers from two major drawbacks.
First of all, ASM shape model is purely built on training images. Furthermore, in ASM, all variations are jointly captured by
eigenvectors and eigenvalues of the training data, which makes
it difficult to manipulate parameters to generate desired shapes
corresponding to specific facial expressions and/or poses.
Although human face shape varies among different people,
the variations are bounded by biological constraints that can
be estimated by anthropometric measures of the head and face.
Such strong domain knowledge can be used as prior information
for constraining shape generalization. Based on this, we propose
a novel BASM that embeds prior knowledge of anthropometric
face measurements into an active shape model. Further, separate
parameters for scaling, rotation, and local shape variations are
explicitly defined in the proposed BASM so that the deformable
model can be more accurately controlled with parameters that
are physically intuitive.
We start with an anchor points-based face shape model
anas in [9], where human faces are characterized by
chor points,
. Specifically, we
adopt the 58-anchor point model from [24], where major
facial contours are captured by 58 landmarks around the
eyebrows, the eyes, the nose, the mouth, and the chin/jaw
(Fig. 2-Left). Coordinates of all facial landmarks are denoted as
. The objective of shape
modeling is to form a parameterized model for representing
by varying a limited
any face shape from a basic shape
number of parameters, i.e.,
(1)
where is the set of parameters of the shape model.
In traditional ASM [9], [24], principal component analysis (PCA) is directly applied on the training data matrix
(with proper alignment), which consists
training images.
of the coordinates of landmarks of all the
Then each shape in the training set can be approximated using
the mean shape and a weighted sum of the first largest eigenvectors:

235

is the mawhere is the mean shape and
is given
trix of the first eigenvectors and
. Vector defines the set of parameters of a
by
deformable model and (2) allows us to generate new shapes by
varying the elements of within suitable limits. In such modeling, all possible deformations are based on the variations in
the training data and are jointly controlled by without intuitive correspondence between the parameters and specific shape
deformations.
In the proposed BASM, we define a key point for each of
the seven point-paths shown in Fig. 2. Instead of applying PCA
on the training data matrix directly, we first normalize the face
shape by scaling the eye distances to a fixed value, and then
align each point-path by moving the key points to the prior positions that are determined from anthropometric face measurements (normalized to the fixed eyes distance). Then each face
shape can be represented as
(3)
in which is the scaling factor, is the offsets of path alignis a mapping from seven key points to 58 anchor
ments,
points by duplicating the offset of each key point for all the anchor points in the corresponding point-path, and is the shape
after the adjustments.
for all adFurther, we form matrix
justed training shapes and perform PCA on this matrix. Now
each original face shape can be further formulated by
(4)
where
is the mean of
and are the
eigenvector matrix and eigenvalues, respectively. Note that
and are obtained from instead of the original ; thus, they
are only responsible for local shape variations of each component (the corresponding key point of the component is fixed),
which is different from that in ASM. For convenience, we use
and for
and in the rest of the paper.
In order to further generalize this deformable model, we introduce two more parameters: to control the aspect ratio of the
key point set, and to control the horizontal off-plane rotation.
This gives us
(5)
where indicates horizontal off-plane rotation and denotes
.
the net effect the parameter has on
With (5), we can generate new shapes by varying the param. Fig. 3 illustrates the respective effects
eter
of the parameters on shape deformations. Intuitively, is for
deforming all the component shapes with their corresponding
key points fixed, is computed from prior knowledge of human
face shape and is used for adjusting the positions of each face
component, corresponds to the aspect ratio of the face region, controls the scale of the shape, and controls horizontal
off-plane 3-D rotation. Specifically, given a 2-D shape , horizontal off-plane 3-D rotation of angle is defined as
(6)

(2)

236

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 12, NO. 4, JUNE 2010

Fig. 4. Proposed statistical sampling algorithm.

Fig. 3. Effects of varying different parameters in BASM.

in which

is the 3-D version of with additional depth
is the rotation center, and is a 1-to-58 duplication mapping of the 3-D coordinates. For simplicity, we
assume a single face depth model that was obtained from averaging 330 3-D face scans from 111 different people [25]. The
rotation center is assumed to be located at half of the face width
behind the tip of the nose.
Compared with (2), (5) incorporates prior shape information
by introducing parameter (the offsets of each point-path from
the positions of prior key points). Furthermore, (5) explicitly
employs separate parameters for different deformations, making
it possible for imposing appropriate constraints on each parameter so as to generate more physically-meaningful shapes. Prior
key points are computed based on the face position (obtained
from a face detection step) and anthropometric face constraints.
More will be discussed in Section IV.
B. Bayesian Parameter Update With Statistical Sampling
For a given face image, the generic face model obtained above
needs to be updated to best match to the input. This requires
given
the update of the model parameter
the input image as observation. We formulate this process as
a Bayesian estimation problem, i.e., the estimation of the posterior density of given an input image :
(7)

In general, the above density is multimodal and the dependency model on will be highly nonlinear. Thus, using a parametric form for the density would be challenging. Consequently,
we propose to use a statistical-sampling-based algorithm for the
is apestimation problem, as done in [23]. Essentially,
proximated by a set of samples of with proper weights. At the
beginning, the samples are drawn around the parameters initialized by face detection and a generic face model. The samples
will then be updated iteratively based on the given image. This
leads to a particle filtering scheme as summarized in Fig. 4.
Proper constraints need to be imposed in generating random
samples in the parameter space since many samples correspond only to implausible configurations (i.e., invalid facial
structures). A hyper-rectangle based on the eigenvalues of the

training data matrix or more complex constraints can be set for
in ASM (e.g., as done in [23]). However, the nature of ASM
(in which all variations are jointly controlled by ) prevents
accurate constraints from being applied for generating physically-meaningful shapes. In the proposed BASM, parameters
are separated for different deformations; thus, it provides a
more natural framework for imposing appropriate constraints.
These are defined in the following.
: Uniform distribution within the hyper-rectangle as defined
in [23], which is learnt from the training data. (The reason we
choose uniform distribution is that is for controlling detailed
shape variations which are mostly related to appearance and expression. It is not meaningful to assume any specific distribution on a limited number of face images obtained from random
subjects.)
and : Gaussian distributions for and each element in ,
with means and variances obtained from anthropometric face
priors. Details will be discussed in Section IV.
Rough face scale can be estimated by the bounding box of
the detected face region. The range of uniform random sampling
is set according to the assumed performance of face detector
to compensate for its inaccuracy.
Uniform distribution within the allowed rotation range.
in our experiments.
We use
The likelihood of each generated sample will be computed
and used to update the weight of this sample. This is largely
based on the comparison of the gradient patterns in the input
image and the learnt profiles of the training images. (Details
of Step 2.1 are to be presented in Section V.) The iterative
algorithm terminates when reaching the predefined number of
iterations.
C. Semantic-Aware Model Enrichment
Although aligned facial landmarks and the connected paths
are able to retain the major shape contours of a human face,
they are still too simplistic for final tactile representation. With
obtained semantic information of the face (i.e., the positions of
the face components), we employ a set of processing steps to
further enrich the components of the sketchy model depending
on their respective semantics. For example, we enrich the model
by adding more details, using edge segments from edge detection. We also use the strength of the gradient on a major contour
to modulate the tactile pattern (e.g., line width) in rendering this
contour. Further, Braille annotations can be added to facilitate
understanding. More details on these strategies will be described
in Section VI.

WANG AND LI: A BAYESIAN APPROACH TO AUTOMATED CREATION OF TACTILE FACIAL IMAGES

237

Fig. 6. Anthropometric distances.
Fig. 5. Anthropometric landmarks.

IV. ANTHROPOMETRIC FACE PRIORS
As discussed in Section III-A, one essential difference between the proposed BASM and ASM is the incorporation of
anthropometric prior information into the model. The book Anthropometry of the Head and Face by Farkas [26] describes elegant methods and results for measuring human head and face
based on thousands of human subjects. The measurements of
human face and head are approximately described by Gaussian
distributions with means and standard deviations. In our work,
we adopt model priors including reference key point set [used
for computing in (5)] and the sampling constraints for and
(Section III-B) from anthropometric statistics provided by this
book. In the following, we first describe the chosen landmarks
and distances in Section IV-A, and then the computation of the
priors for BASM in Section IV-B.

Fig. 7. Key points and their prior distributions.

With these constraints, we can further compute the prior key
points of face components (Section III-A) and parameter constraints for sampling (Section III-B) as shown in the following.
Prior Key Points : Given the pre-calculated anthropometric
can be computed by using the following
face constraints,
equations:

A. Anthropometric Face Measures
From [26], we select 13 landmarks and seven distances which
are relevant to the seven point-paths in the 58-anchor-point
model. We add a virtual landmark as a reference point. Fig. 5
illustrates all the landmarks. The seven distances adopted from
[26] include
. (“ ” and “ ” denote horizontal and
vertical distances, respectively.)
B. Anthropometric Face Priors for BASM
Based on the statistical data from [26], we first compute four
ratios that are scale and position invariant. These ratios are computed based on the distances
as defined
below and illustrated in Fig. 6. The distance
between the
two eyes is used as a reference, and all the ratios are computed
:
with respect to

The ratios
as anthropometric constraints.

and

are recorded

In the training stage, average eye distance
and average
positions of all training data
, and
are
, and
used as inputs. In the test stage,
are also set as the mean values of the training data by approximation in our experiments.
Prior Distributions for Random Sampling: For sampling
and in (5), we use Gaussian distributions (illustrated in Fig. 7

238

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 12, NO. 4, JUNE 2010

for ) with variances calculated by combining the standard deviations of the anthropometric distance measurements used in
– :
computing
(8)
Fig. 8. Two examples of illumination invariant feature.

(9)
The distributions defined in (9) are for face components except the eyes. To compensate for the inaccurate eye positions
(since we used an approximation discussed earlier), we define
the following Gaussian distributions to capture the possible inaccuracy, leading to probabilistic prior key points for the eyes:
(10)
(Since we have normalized the eye distance and aligned the eyes
to fixed positions, the referred distributions are of zero mean.)
In this case, the random parameter sampling step consists of two
stages: sample prior eye positions according to the distributions
in (10) first and then sample the aspect ratio and other prior
path positions by using distributions in (8) and (9), respectively.

Fig. 9. Illustration of local normal lines.

V. COMPUTING THE SHAPE LIKELIHOOD
The likelihood computation for a sample in Step 2.1 of the algorithm of Fig. 4 is an essential step for the statistical sampling
procedure. In this section, we describe the details of estimating
. We use
the likelihood of the generated random samples,
both local gradient profile [9] and edge in this evaluation. Let
denote the gradient pattern computed along the line perpendicular to the boundary of a shape instance through landmark
represents the edge map of image .
point . In addition,
We define the likelihood as
Fig. 10. Mean gradient profile is very sensitive to pose. Mean gradient profile
at p computed (a) over all the training images, (b) over the nearly frontal pose
set, (c) over the turning-right pose set, and (d) over the turning-left pose set look
quite different from each other. (Right/left is in terms of the face in the image.)

(11)
The likelihood consists of two terms: model-driven term
, which is the joint probability of the gradient
profiles at the local landmarks given the shape configuration
; and data-driven term
, which measures how well
a generated face shape matches the detected edges on the face.
These will be discussed in more detail in the following after the
introduction of an illumination invariant feature in Section V-A.
A. Illumination Invariant Feature
To ensure the computed gradient profiles to be more or less
invariant to illumination, we first preprocess the image by
adopting the method from [11]: the image is first divided into
patches and then normalized with respect to local illumination
conditions, which are approximated by a low-pass version of
the local patch, as shown in (12):
(12)
where is the original image patch, is a low-pass filter, and
is the image patch after this “normalization”. A smoothing step
follows to eliminate the “blocky artifacts”. (We use Gaussian
low-pass filter for smoothing in our experiments.) Fig. 8 shows

two examples, where the images on the right would lead to more
balanced gradient computation for both sides of the faces.
B. Pose-Dependent Local Gradient Profile
The local appearance models, which describe local image features around each landmark, are modeled as the first derivative
of the intensity pattern, , computed along the line perpendicular to the boundary of a shape instance through landmark
point . As illustrated in Fig. 9, for landmarks on the chin path,
only patterns on the inward side are considered (
pixels in our experiments); for all other landmarks, gradient patterns lie on both sides of the point are extracted (
pixels in our experiments). Note that, in the training set, the gradient profiles at each anchor point vary from image to image,
and from pose to pose. For instance, the mean gradient profile
of computed over faces that turn left could dramatically differ
from the mean gradient profile of
computed over faces that
turn right, as illustrated in Fig. 10.
Therefore, using the mean profile averaged across all poses,
as done in [9], may not give a good template for the corresponding anchor point. To remedy this, we propose a pose-

WANG AND LI: A BAYESIAN APPROACH TO AUTOMATED CREATION OF TACTILE FACIAL IMAGES

239

is the number of landmarks and
is the
in which
distance between the sample gradient pattern for landmark
and the average gradient pattern among training data for . If
pose specified by (13) is taken into consideration, (13) can be
rewritten as

(15)
between two gradient patTo define the distance metric
terns and , we use the Bhattacharyya distance [27]
Fig. 11. (a) Mean gradient profile at p computed over all the training images in subset 0. (b) Gradient profile at p for a testing image. (c) Five cluster
centroids obtained from clustering in subset 0. The score of matching (b) to (a)
would be very low, whereas (b) matches well to the fifth cluster centroid in (c),
indicating that k-means clustering can capture more appearance variations in
the training set than simply using the mean gradient profile.

(16)
which was found empirically to be better than simple Euclidean
distance in our experiments.
For a hypothesized face shape with horizontal off-plane rotation angle , the algorithm chooses the corresponding template
as the
set to compute a matching score at testing landmark
minimum distance between and one of the five centroids:
(17)

Fig. 12. Partitions of landmarks: yellow—S , red—S , and green—S .

dependent local appearance model. Specifically, we divide the
entire training image set into three subsets based on the pose
variations:

(13)

We also observed that when the face turns left/right, the
collection of landmarks located on the left/right side of the face
would have smaller contribution to the likelihood computation.
Given the rough orientation of a shape instance, the likelihood
model can be further improved such that the image measurements of different set of landmarks are weighted differently.
landmarks into
To implement this idea, we partition the
, and , corresponding to the set of right-side,
three sets,
middle, and left-side landmarks (see Fig. 12). If a sample model
parameter tells that the face turns to the right, the landmarks
will be assigned smaller weights than those in
and
in set
in computing the distance of (16); it is similar for other
poses. This yields the following enhanced likelihood model:

For each landmark , we first apply k-means clustering (with
, and ,
five clusters) to all the gradient profiles of in set
respectively, and then record the five cluster centroids for each
for .
set as the templates, denoted as
Fig. 11 illustrates that this scheme with five templates for each
pose can better capture appearance variations in the training set
than simply using the mean gradient profile.
(18)

C. Matching Using Weighted Bhattacharyya Distance
, the shape will
Given a configuration
be compared to its corresponding training set dependent on the
[see (13)]. In addition, the model-driven likelihood
value of
term is defined as

and
contains the
where
weights for the three landmark sets, respectively.

(14)

As mentioned in the beginning of this section, the data-driven
term in likelihood estimation measures how well a face shape

D. Data-Driven Likelihood Term

240

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 12, NO. 4, JUNE 2010

Fig. 13. Illustrations of data-driven part: Face shapes in left and right images
are specified by configuration  and  , respectively. (Two edge maps are the
same.)

sample fits to the detected edges on the face. This is formally
achieved by computing the term as
(19)
In (19), denotes the total number of edge pixels encompassed
by all the 3 3 windows centered at each point located on the
facial feature contour that fall inside of the bounding box of the
true face obtained by face detection; denotes the total number
of edge pixels that fall inside of the bounding box of the true face
gives us the ratio of
obtained by face detection. Such
the model edge pixels to the true face edge pixels. As illustrated
in Fig. 13, the face shape in the left image matches the edge
map better than that in the right one; thus ideally,
.
VI. SEMANTIC-AWARE PROCESSING FOR MULTIMODEL
TACTILE RENDERING
In previous sections, we proposed BASM for face alignment.
However, the connected aligned landmarks (e.g., the white face
model in Fig. 9) are still too sketchy to convey sufficient visual
information in producing a useful tactile face representation. In
order to make the final tactile face images more informative to
visually-impaired people through touching, further enrichment
steps are proposed in this section, as elaborated below.
A. Component-Specific Enrichment
In tactile representation, in addition to the contours of the
major facial features like eyes, mouth, and nose, it is also critical to keep other informative edges in final tactile rendering.
To achieve this goal, we enrich the basic face model obtained
from the face alignment algorithm with the edges detected by a
Canny edge detector. An adaptive edge refining step is used to
filter out the redundant details. This process is semantic-aware
in the sense that the refining step varies depending on where on
the face the algorithm is applied. Specifically, we perform the
following processing:
• Eyes, eyebrows, jaw and nose: We keep their shape contours and the nearby edge segments to render some details
such as wrinkles in the original image.
• Mouth: We keep the edges in the mouth region which is
defined as an enlarged bounding box centered at the aligned
mouth contour.
• Chin and face region: We define the face region by mirroring the aligned contour of the chin up and use the projected curve as the upper bound of the face region. In

Fig. 14. Visual-tactile conversion process: (a) Original image. (b) Result of
face alignment. (c) Edge-enriched and gradient enhanced representation. (d)
Tactile printout from a thermal enhancer.

order to make the major face components salient for tactile sensing, edge details within this region (except those
retained by other steps) are cleaned up.
• Hair, ears, neck, and shoulders: We estimate the outer region of the portrait based on the aligned face shape and
retain the edge/texture details if these components exist in
the image.
B. Other Enhancements
The edge-enriched face sketch can be transformed to tactile
form by printing it out through a tactile embosser or a thermal
enhancer. In this stage, we exploit the strength of the gradient to
modulate the tactile patterns in generating the tactile graphics.
For example, we use denser dot patterns for areas with strong
gradients, thicker lines for major facial features, and thinner
lines for the secondary features including wrinkles, fine edges
around the eyes, and the mouth.
It is also possible to insert Braille annotations to the final tactile graphics to further assist the blind user in comprehending
the tactile printout. These annotations may come from the
face alignment step (e.g., Braille text “nose” placed close to
the aligned nose contour), or may even be extracted from the
metadata of the underlying image. These types of annotations,
if combined with a multimodal system such as an interactive
tactile touchpad (e.g., IVEO touchpad [28]) may convey more
information than the tactile lines alone.
Fig. 14 illustrates the results of the tactile conversion process
with an example, in which (a) is the input image, (b) the face
alignment result overlaid on the original image, (c) the edgeenriched and gradient enhanced face shape, and (d) the final
actual tactile printout from a thermal enhancer.
Fig. 15 illustrates the results of using two alternative ways for
generating tactile faces without the semantic-aware processing
technique. Obviously, simple sketch of a face without hair, neck,
shoulder, etc. [Fig. 15(a)] is not a desired representation of the
human face in Fig. 14(a). In addition, generating a tactile face by
using simple edge detection without high-level guidance (e.g.,
how to set the parameters for edge detector) is not able to produce a desired result as well. Fig. 15(b) is an edge map of

WANG AND LI: A BAYESIAN APPROACH TO AUTOMATED CREATION OF TACTILE FACIAL IMAGES

Fig. 15. Results without semantic-aware processing: (a) Using only the
matched face shape of Fig. 14(a). (b) Using the edge map of Fig. 14(a)
generated from Canny edge detector with default parameters.

Fig. 14(a) generated from Canny edge detector with default parameters, in which the face region is completely messed up by
small line segments. In addition, due to the binary property of
swell-paper (either flat or raised which is thermal sensitive),
any attempt to generate a tactile face directly from a gray-scale
image would typically fail, since the entire face region which is
nonwhite would be raised and all the face components would be
unrecognizable.
VII. EXPERIMENTS AND EVALUATIONS
To evaluate the effectiveness of the proposed approach, we
performed both objective evaluation on the proposed BASMbased face alignment algorithm (Section VII-A) and subjective
evaluation on the usefulness of the tactile faces produced by the
approach (Section VII-B).
A. Objective Evaluation on Face Alignment
We used four face image databases to evaluate the performance of the BASM face alignment algorithm. The first database, IMM [24], comprises 240 images from 40 different subjects. We used 200 images from all subjects (five images of different scenarios from each subject). The different scenarios are
listed in the following and sample images with our alignment
results are presented in Fig. 16: 1) Full frontal, neutral expression, diffuse light; 2) Full frontal, “happy” expression, diffuse
light; 3) Rotated approximately 30 degrees to the right, neutral
expression, diffuse light; 4) Rotated approximately 30 degrees
to the left, neutral expression, diffuse light; 5) Full frontal, neutral expression spot light added at the person’s left side. (Currently we only handle horizontal off-plane rotations in the current system; thus, the sixth scenario of each subject with arbitrary head rotations was not included in our experiments.) Images of first 30 persons were used for training (150 in total); the
remaining 50 images were for testing. We do not include any
off-frontal images in creating the deformable model. In other
words, off-frontal shapes generated in the sampling stage are
obtained by applying 3-D horizontal off-plane rotation with approximated depth and rotation axis on the built frontal model.
Off-frontal images of the first 30 persons [60 images, right part
of Fig. 16(a)] are only used in extracting the local gradient patterns of each landmark, which are taken into account in calculating pose-dependent shape likelihood. (In our previous work
[23], we use off-frontal case of training persons for both creating the model and extracting the reference gradient patterns.)
The major reason of using such an approximation is that anthro-

241

pometric face constraints are not available for off-frontal faces
with arbitrary rotation angles. This reveals one limitation of our
approach that it is not able to handle off-frontal rotations with
large angle.
The second data set, denoted as “AR200”, consists of 200 images of 40 randomly selected subjects (20 male and 20 female)
under five scenarios (“Neutral expression”, “Smile”, “Anger”,
“Left light on”, and “Right light on”) from Section 1 of the AR
face database [32].
The third data set, denoted as “FERET100”, consists of 100
images of 50 randomly selected subjects (33 male and 17 female) from Color FERET database [33]. Since in Color FERET
database, different subjects have different numbers of images,
we select two basic cases with suffix “fa” and “fb” in the titles
(we skipped faces with left/right rotations since most off-frontal
faces in FERET database are of nearly 90-degree rotation angle,
which is beyond the scope we aim at in this paper). Both cases
are of large variations of lighting condition (e.g., with side-way
lighting), face size, and skin color (e.g., subject with very dark
skin). We manually annotated these 300 images to obtain the
ground-truth data.
In addition, we also experimented with a 30-person face database (denoted “30-person data set”) that was independently captured in our lab with varying lighting conditions and poses. The
resolution of the images in this data set is much lower than the
first three data sets.
Fig. 16 shows the results of all five scenarios of one subject
from the IMM test set, and Fig. 17 shows more results with
varied expressions and lighting conditions from both training
and testing sets of IMM database. We can see that the BASMbased face alignment algorithm works well with all scenarios
of images. (Results are slightly better for frontal images than
for off-frontal cases, because off-frontal images were not used
for creating the deformable model.) It is worth pointing out that
BASM is able to capture subtle variations of face components
due to different expressions (e.g., the mouth regions of the images in the first row of Fig. 17), which is helpful for conveying
important information in the final tactile representation.
We quantitatively analyzed the performance of the algorithm
by computing the average matching errors for the anchor points
based on the ground-truth. Fig. 18 reports the average errors
per anchor points in terms of point-paths (normalized to innereye-corner-distance) and corresponding standard deviation over
all samples for the above three data sets. (Due to the 58-anchor-point face model, the inner-eye-corner-distance instead of
the iris-to-iris distance is used for normalization.) Comparing
the accuracies among different point-paths, best performance is
achieved on eyes; it is slightly better for eyebrows than for nose
and mouth; the worst case occurs on chin. In terms of different
scenarios, the algorithm works the best for frontal pose with
neutral expression; errors increase when the subject is smiling
or one side light is on. Worst cases occur on faces with horizontal rotations. This is not surprising since we did not include
any off-frontal images in creating the deformable model. It is
worth pointing out that, although the training was done using a
subset of the IMM images, the testing results for the AR200 and
FERET100 images are equally good, despite the acquisition environments of the databases differ greatly. (The results for the

242

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 12, NO. 4, JUNE 2010

Fig. 16. Sample results of images under five scenarios of IMM dataset.

Fig. 17. Face alignment results: 1st row—frontal with some variations of expressions and lighting. 2nd row—turn right. 3rd row—turn left.

TABLE I
FACE ALIGNMENT RESULTS OF THE PROPOSED APPROACH

AR200 and FERET100 images are actually slightly better on
average, but the IMM sets contain more challenging off-frontal
images.) This suggests that our method is very robust with respect to new databases that it never saw in the training stage.
Overall, the average error per anchor point is about 8.6 pixels
for the IMM training set, 9.0 pixels for IMM test set, 8.8 pixels
for AR200, and 7.8 for FERET100. Since the average height of
face regions in the database is about 200 pixels, the error on average is less than 5% of the height of the face region and thus
can be deemed as small. The results in terms of the percentage
of images with average error per anchor point (normalized to
the inner-eye-corner distance) within certain error bounds are
summarized in Table I (the average inner-eye-corner distances
of IMM, AR200, and FERET 100 are 46 pixels, 48 pixels, and
51 pixels, respectively). These results improve upon those reported in [23] and are at least comparable to what presented in
[11], [16], and [30], although it is difficult to make direct comparison since the landmark model, test images, and ground-truth
are different. In [22], the best accuracy of errors no-greater-than

8 pixels was reported as 63.9% while the average height of the
faces for test is about 180 pixels; Gu et al. achieved an average
misalignment error 3.49 pixels with all faced normalized to a
width of 120 pixels [16]; Liang et al. reported 98.5% and 93.5%
cases of errors less than 7.5 pixels on two data sets with the entire images resized to 200–300 pixels [30]. Face sizes in these
three papers are much smaller than the test samples we used
(e.g., the average face size of IMM database is about 200 190
pixels and it is even slightly larger of AR200 and FERET100);
thus, the reported errors appear smaller than the results presented in this work. In addition, in our evaluation, we trained
the model and the algorithm with a fixed set of images from
the IMM database; then the trained model/algorithm was tested
on other databases that are completely independent of the IMM
database. However, in the experiments of the above three papers, the training and testing sets were formed in such a way
that both sets contain images from all the underlying databases.
Apparently, our evaluation protocol is much more demanding
than that used in the above papers. We attribute the robustness
and generalizability of our algorithm with respect to new image
databases to the incorporation of the Bayesian prior.
In addition, we also performed separated analysis on subjects who wear glasses or with beards/moustaches. For cases
of wearing glasses, we analyzed all 13 subjects (65 images)
with glasses in AR200 data set. Sample visual results and
quantitative evaluations (including normalized mean errors
and corresponding standard deviations over all samples, as
shown in Fig. 18) of the eyes and the eyebrows are illustrated
in Fig. 19. Compared to Fig. 18(c), no obvious degeneration

WANG AND LI: A BAYESIAN APPROACH TO AUTOMATED CREATION OF TACTILE FACIAL IMAGES

243

Fig. 18. Face alignment results on multiple data sets. (Point-paths: 1—chin, 2—left eye, 3—right eye, 4—left eyebrow, 5—right eyebrow, 6—mouth, 7—nose.):
(a) IMM training set: five scenarios from the first 30 subjects. (b) IMM test set: five scenarios from the last ten subjects. (c) AR200 data set: five scenarios from 40
random subjects. (d) FERET100 data set: two frontal sets from 50 random subjects.

Fig. 21. Comparison between the (left) ground-truth and the (right) obtained
result with 4.8 pixel error per anchor point.
Fig. 19. AR200-Cases of wearing glasses: (a) Visual results. (b) Quantitative
results of eyes and eyebrows over all samples.

Fig. 20. IMM-Cases with beards and/or moustaches: (a) Sample visual results.
(b) Quantitative results of chin and mouth over all samples.

happens for point-paths of eyes and eyebrows when subjects
are wearing glasses. For subjects with beards and/or moustaches, we analyzed all 14 subjects with light to heavy beards

and/or moustaches from IMM database. Fig. 20(a) illustrates
two sample results, in which the bottom image is from the
subject who might have the heaviest beards among all subjects
in IMM database. The result is still reasonably good. More
visual results can be found in Fig. 17 (i.e., second and sixth
column). Fig. 20(b) presents normalized average error plots
for point-paths of the chin and the mouth and corresponding
standard deviations over all samples. Compared to Fig. 18(a)
and (b), accuracies for mouth and chin are comparable to the
overall average results.
Note that the ground-truth shapes labeled manually are not always precise. As illustrated in Fig. 21—left, anchor point 48 and
58 (two top landmarks of the nose) are not aligned horizontally.
This suggests that the so-called ground-truth is not perfect, and
thus, a relatively large error computed based on the ground-truth
needs not mean the matching is poor. For example, the anchor
points 5–9 (lower part of the chin) in the two images of Fig. 21
can be deemed as perfect fit to the image, but the corresponding

244

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 12, NO. 4, JUNE 2010

Fig. 22. Face alignment results on the 30-person data set: Top row—our previous results presented in [23]; bottom row—BASM results.

Fig. 23. Tactile face images—Set I.

anchor points are not exactly at the same positions. These observations explain part of the reasons that we got close-to-zero percentage for cases of “no greater than 0.1 inner-eye-corner-distance”.
For the 30-person data set, a few sample results are shown in
Fig. 22, demonstrating the robust performance of the algorithm.
Comparing the first row of Fig. 22 from our previous approach
presented in [23] and the second row from the BASM approach
for the same images, obvious improvements can be observed.
In terms of convergence of the iterative sampling process, in
our experiments, we observed that it actually converges very fast
if the center of the initial face model is placed reasonably close
to the ground-truth center (i.e., when the face detection result
is reasonably accurate). All reported results in the paper were
generated with four iterations and 200 random samples in each
of the iterations.
B. Subjective User Evaluation
The ultimate goal of our work is to automatically generate
tactile form of face images for visually-impaired people.
In Section VII-A, we have shown the performance of the
BASM-based approach on face alignment. In this section,
we will present user evaluations of the tactile images created
by the proposed approach. The evaluation was done by both
blind-folded sighted users and visually-impaired users.
Six visually-impaired users and 11 blind-folded sighted users
participated in our experiments. The six visually-impaired users
include five blind persons and one low-vision person (who was
blind-folded in the experiments). Five of them are Braille users
(the other people use screen reader) and only two of them have
a little experience with tactile graphics (limited to simple geometric shapes, such as triangle, square, etc.) Since most of the
users do not have any experience with tactile graphics, a short
training step was performed before the main evaluation experiment. In the training phase, the users were given some sample

tactile face images of the same format as all the experimental
images to explore and to get with the layout of the image and
various patterns for different face components. Assistances were
provided upon request during the training phase. Blindfolded
users are people who did not have any experience with tactile graphics. They were also given the training beforehand. Although the end user of the technology will be people with visual impairment, at this stage of study, to verify that the approach does maintain key “visual” features, it was found that recruiting blindfolded sighted individuals for the evaluation was
very helpful since they are able to compare what they feel by
touching against what they have seen.
Both groups of users were required to explore some tactile
face images generated from our approach and answer the following questions: 1) Can you recognize each face component including the mouth, eyes, eyebrows, nose, and chin/jaw? 2) Can
you recognize the pose of the person, i.e., is he/she turning left
or turning right? 3) Can you recognize the gender of the person?
4) Association: Can you identify two images that represent the
same person?
Images in Fig. 23 were used for all the questions; images in
Fig. 24 were used for association questions only. Tables II and
III present the resultant statistics of the visually-impaired group
and the blindfolded group, respectively.
Observations from Tables II and III are summarized below.
1) Major face components: They were successfully identified by most of the users from both groups except the left
eyebrow of Fig. 23(e) which is close to some curves of
the hair. In addition, in the same image, curves of clothes
were misunderstood as chin/jaw by one of visually-impaired users.
2) Pose: Both groups achieved high accuracies (only one user
out of six or 11 users at most was wrong for each image).
3) Gender: This was found to be a tough question. Still, more
than 50% of the users in each group got it right for most of

WANG AND LI: A BAYESIAN APPROACH TO AUTOMATED CREATION OF TACTILE FACIAL IMAGES

245

Fig. 24. Tactile face images—Set II.

TABLE II
RESULTANT STATISTICS (NUMBER OF CORRECT CASES) OBTAINED FROM SIX
VISUALLY-IMPAIRED USERS

TABLE III
RESULTANT STATISTICS (NUMBER OF CORRECT CASES) OBTAINED FROM 11
BLIND-FOLDED USERS

in performance for these two types of approaches. Overall,
both groups achieved high accuracies on most of the tasks,
while in general, not surprisingly, users from the blind/low
vision group were much faster in interpreting the results.
Finally, we report an interesting experiment of human identity recognition based on tactile face images, which was performed with blindfolded participants only. The objective of this
experiment is to test whether the proposed approach is able to
retain the distinctive characteristics of the facial features. The
participants were given two tactile face images generated from
our approach and asked to give the identity of each person in the
two tactile images, respectively, by choosing from five names of
five persons that they know very well. For example, we asked
the participants: “Can you tell who this person is, chosen from
Cindy, Troy, Jessie, Michael, and Daniel?” The results were very
encouraging: all of the participants were able to correctly recognize the identity of the persons on the two images. This suggests that the automatically created tactile representation indeed
retains some distinctive visual features.
VIII. CONCLUSION AND FUTURE WORK

the images. The blind/low vision group performed slightly
better than the blindfolded group. Length of hair was the
main feature used for distinguishing genders by most of
the users. Some of them also used sizes of face and eyes
for identifications. One of the blind users rejected to use
the length of hair as a discriminative criterion, since she
has short hair herself. Curves of the shoulder part, which
are easily confused with women’s long hair, caused most
of the mistakes.
4) Association: Most users found it is the most difficult task,
but very interesting on the other hand. Criteria used by
different users varied. Most of them relied on properties of
hair, such as length or density of hair. Some of them used
contours of neck and shoulder regions. In addition, shapes
of the chin, mouth, and eyes and width of the entire face
contributed to some decisions as well.
5) Other observations: Most users used eye positions as spatial references for locating other face components. However, it was very interesting that some of the users followed
a different way. They started from chin and explored the
face bottom-up. We did not observe any salient difference

We proposed a systematic approach to automatic conversion
of facial images into their tactile form. A novel modeling
framework, BASM, was proposed, which enables the incorporation of anthropometric priors and facilitates the development
of a Bayesian inference algorithm based on statistical sampling. Compared to our recent attempt [23] in addressing this
challenging and practical problem, the proposed approach
has achieved significant improvement in both accuracy and
robustness. Further, comprehensive user evaluation has been
reported, based on a group of 12 users including six blind
individuals. The results suggest that the proposed approach
provides a promising solution to the challenging problem of automatic creation of tactile face images. To our knowledge, this
is the first symmetric study on the problem. Generalizing the
approach to other types of graphics and building an end-to-end
system using the current approach are among our future tasks.
ACKNOWLEDGMENT
The authors would like to thank Dr. T. Hedgpeth for valuable
feedback and suggestions on tactile graphics representation and
Dr. D. Colbry for helpful discussions on applying 3-D face scan
data for face alignment.
REFERENCES
[1] S. Ina, “Presentation of images for the blind,” presented at the SIGCAPH Comput. Phys. Handicap, 1996.

246

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 12, NO. 4, JUNE 2010

[2] T. P. Way and K. E. Barner, “Automatic visual to tactile translation. I.
Human factors, access methods and image manipulation,” IEEE Trans.
Rehab. Eng., vol. 5, no. 1, pp. 81–94, Mar. 1997.
[3] T. Way and K. Barner, “Visual to tactile translation, Part II: Evaluation
of the TACTile image creation system,” IEEE Trans. Rehab. Eng., vol.
5, no. 1, pp. 95–105, Mar. 1997.
[4] P. K. Edman, Tactile Graphics. New York: AFB Press, May 1992.
[5] “Tactile graphics project: http://tactilegraphics.cs.washington.edu,”
Univ. Washington, Seattle.
[6] “The science access project: http://dots.physics.orst.edu,” Oregon State
Univ., Corvallis.
[7] R. E. Ladner, M. Y. Ivory, R. Rao, S. Burgstahler, D. Comden, S. Hahn,
M. Renzelmann, S. Krisnandi, M. Ramasamy, B. Slabosky, A. Martin,
A. Lacenski, S. Olsen, and D. Groce, “Automating tactile graphics
translation,” presented at the ACM SIGACCESS Conference on Assistive Technologies, Baltimore, MD, 2005.
[8] Z. Wang, X. Xu, and B. Li, “Enabling seamless access to digital graphical contents for visually-impaired individuals via semantic-aware processing,” EURASIP J. Image Video Process., vol. 2007, pp. 1–14, 2007.
[9] T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham, “Active shape
models—Their training and application,” Comput. Vis. Image Understand., vol. 61, pp. 38–59, 1995.
[10] Y. Zhou, L. Gu, and H.-J. Zhang, “Bayesian tangent shape model:
Estimating shape and pose parameters via Bayesian inference,” presented at the IEEE Computer Society Conf. Computer Vision and Pattern Recognition (CVPR), 2003.
[11] Y. Huang, Q. Liu, and D. Metaxas, “A component based deformable
model for generalized face alignment,” presented at the IEEE 11th Int.
Conf. Computer Vision (ICCV), 2007.
[12] J. Tu, Z. Zhang, Z. Zeng, and T. Huang, “Face localization via hierarchical CONDENSATION with Fisher boosting feature selection,” presented at the IEEE Computer Society Conf. Computer Vision and Pattern Recognition (CVPR), 2004.
[13] C. Twining and C. Taylor, “Kernel principal component analysis and
the construction of non-linear active shape models,” presented at the
British Machine Vision Conf. (BMVC), 2001.
[14] F. D. L. Torre and M. H. Nguyen, “Parameterized kernel principal component analysis: Theory and applications to supervised and unsupervised image alignment,” presented at the IEEE Computer Society Conf.
Computer Vision and Pattern Recognition (CVPR), 2008.
[15] L. Liang, F. Wen, Y.-Q. Xu, X. Tang, and H.-Y. Shum, “Accurate face
alignment using shape constrained Markov network,” presented at the
IEEE Computer Society Conf. Computer Vision and Pattern Recognition (CVPR), 2006.
[16] L. Gu and T. Kanade, “A generative shape regularization model for
robust face alignment,” presented at the Eur. Conf. Computer Vision
(ECCV), 2008.
[17] M. Kass, A. Witkins, and D. Terzopoulos, “Snakes: Active contour
models,” Int. J. Comput. Vis., vol. 1, pp. 321–331, 1988.
[18] T. F. Cootes, G. J. Edwards, and C. J. Taylor, “Active appearance
models,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 23, no. 6, pp.
681–685, Jun. 2001.
[19] V. Blanz and T. Vetter, “A morphable model for the synthesis of 3D
faces,” presented at the ACM SIGGRAPH, 1999.
[20] J. Coughlan and S. Ferreira, “Finding deformable shapes using loopy
belief propagation,” presented at the European Conf. Computer Vision
(ECCV), Copenhagen, Denmark, 2002.
[21] X. Liu, “Generic face alignment using boosted appearance model,” presented at the IEEE Conf. Computer Vision and Pattern Recognition
(CVPR), 2007.
[22] H. Wu, X. Liu, and G. Doretto, “Face alignment via boosted ranking
model,” presented at the IEEE Computer Society Conf. Computer Vision and Pattern Recognition (CVPR), 2008.

[23] Z. Wang, X. Xu, and B. Li, “Bayesian tactile face,” presented at the
IEEE Computer Society Conf. Computer Vision and Pattern Recognition (CVPR), 2008.
[24] M. B. Stegmann, B. K. Ersboll, and R. Larsen, “FAME—A flexible
appearance modeling environment,” IEEE Trans. Med. Imag., vol. 22,
no. 10, pp. 1319–1331, Oct. 2003.
[25] D. Colbry and G. Stockman, “Canonical face depth map: A robust
3D representation for face verification,” presented at the IEEE Computer Society Conf. Computer Vision and Pattern Recognition (CVPR),
2007.
[26] L. G. Farkas, Anthropometry of the Head and Face, 2nd ed. New
York: Raven, 1994.
[27] A. Bhattacharyya, “On a measure of divergence between two statistical
populations defined by probability distributions,” Bull. Calcutta Math.
Soc., vol. 35, pp. 99–109, 1943.
[28] [Online]. Available: http://www.viewplus.com/products/touch-audiolearning/IVEO/.
[29] S. Milborrow and F. Nicolls, “Locating facial features with an extended active shape model,” presented at the Eur. Conf. Computer Vision (ECCV), 2008.
[30] L. Liang, R. Xiao, T. Wen, and J. Sun, “Face alignment via component-based discriminative search,” presented at the Eur. Conf. Computer Vision (ECCV), 2008.
[31] S. Romdhani and T. Vetter, “3D probabilistic feature point model for
object detection and recognition,” presented at the IEEE Computer
Society Conf. Computer Vision and Pattern Recognition (CVPR),
2007.
[32] A. M. Martinez and R. Benavente, The AR Face Database, CVC Tech.
Report #24, 1998.
[33] P. J. Phillips, H. Moon, S. A. Rizvi, and P. J. Rauss, “The FERET
evaluation methodology for face recognition algorithms,” IEEE Trans.
Pattern Anal. Mach. Intell., vol. 22, no. 10, pp. 1090–1104, Oct. 2000.

Zheshen Wang (S’07) received the B.S. and M.S.
degrees from Xiamen University, Xiamen, China, in
2003 and 2006 respectively, both in computer science
and engineering. She is currently pursuing the Ph.D.
degree in the Department of Computer Science and
Engineering, Arizona State University, Tempe.
Her research interests include cognitive visual
analysis, image/video understanding, computer
vision, and pattern recognition.

Baoxin Li (S’97–M’00–SM’04) received the Ph.D.
degree in electrical engineering from the University
of Maryland, College Park, in 2000.
He is currently an Assistant Professor of computer
science and engineering with Arizona State University, Tempe. He was previously a Senior Researcher
with SHARP Laboratories of America (SLA),
Camas, WA, where he was the Technical Lead in developing SHARP’s Hi-Impact Sports Technologies.
He was also an adjunct faculty member with the
Portland State University, Portland, OR, from 2003
to 2004. His research interests include pattern recognition, computer vision,
multimedia processing, and statistical methods in visual computing.

Classification of Diabetic Retinopathy Images Using Multi-Class
Multiple-Instance Learning Based on Color Correlogram Features
Ragav Venkatesan, Parag Chandakkar, Baoxin Li, Senior Member, IEEE, and Helen K. Li, MD


Abstract— All people with diabetes have the risk of
developing diabetic retinopathy (DR), a vision-threatening
complication. Early detection and timely treatment can reduce
the occurrence of blindness due to DR. Computer-aided
diagnosis has the potential benefit of improving the accuracy
and speed in DR detection. This study is concerned with
automatic classification of images with microaneurysm (MA)
and neovascularization (NV), two important DR clinical
findings. Together with normal images, this presents a 3-class
classification problem. We propose a modified color autocorrelogram feature (AutoCC) with low dimensionality that is
spectrally tuned towards DR images. Recognizing the fact that
the images with or without MA or NV are generally different
only in small, localized regions, we propose to employ a multiclass, multiple-instance learning framework for performing the
classification task using the proposed feature. Extensive
experiments including comparison with a few state-of-art image
classification approaches have been performed and the results
suggest that the proposed approach is promising as it
outperforms other methods by a large margin.

I. INTRODUCTION
Diabetic retinopathy (DR) is a common cause of
blindness among the diabetic population. Despite various
advances in diabetes care over the years, loss of vision is
still a potentially devastating complication in people with
diabetes. The risk of severe vision loss can be reduced
significantly by timely diagnosis and treatment of DR.
Maximizing the efficiency and accuracy of assessing DR
severity levels could help prevent vision disabilities and
their resulting high cost to the society.
The conventional process of evaluating retinal fundus
images in DR diagnosis is laborious and prone to error or
reviewer fatigue. Recent years have seen many research
efforts on developing computer-assisted detection and
evaluation of diabetic retinal lesions [1, 2, 3, 4, 5]. While
progresses have been made, the lack of a unified and
systematic solution or system that has been widely accepted
in ophthalmology indicates that the central problems have
not been solved. For example, a content-based retireval
system for retinal images is discussed in [3], where the
retrieved images appear to be largely similar in appearance
but not similar in terms of clinical relevance, and thus the
practical usefulness of the approach is unclear.
Two important DR clinical findings are microaneurysm
(MA) and neovascularization (NV). Among others (e.g.,
R. Venkatesan, P. Chandakkar, and B. Li are with the Arizona State
University, Tempe, AZ, 85281 USA (phone: 480-965-1735; fax: 480-9652751; e-mail: baoxin.li@asu.edu).
H. K. Li is with Weill Cornel Medical College/The Methodist Hospital,
The University of Texas Health Science Center Houston, and Thomas
Jefferson University. (e-mail: hli@commuityretina.com).

intra-retinal hemorrhages, exudates, etc.), MA is
characteristic of non-proliferate diabetic retinopathy (NPDR)
or background retinopathy. On the other hand, the early
proliferate diabetic retinopathy (PDR) stage is characterized
by neovascularization (NV), which is the formation of
abnormal new blood vessels. Therefore, MA and NV are two
clinically important lesion types to consider. In this study,
we focus on the following 3-category classification problem:
classifying a given DR image as one of the three types, a
normal image (no DR), an image with MA, or an image with
NV. Given the aforementioned importance of MA and NV
in DR diagnosis, such a 3-category classification problem
has the potential of contributing to developing computerbased systems for delivering clinically relevant results.
Image classfication is a well-studied topic in the field
of computer vision. State-of-the-art approaches include those
relying on robust features and classfication algorithms that
have been developed in the past decade. For example, the
scale-invariant feature transform (SIFT) features coupled
with a bag-of-words (BoW) approach [6] has been shown to
be very effective. In such an approach, distinctive and
repeatable image features like the differene-of-Gaussian
(DoG) points are detected. An image patch for each feature
is extracted from which a feature descriptor is then
computed. The descriptors are further encoded into a visual
word via some learned codebook. Such features can then be
used by classifiers such as support vector machines (SVM)
to perform image classification. Such approaches are in
general inadeqaute for our 3-category classification problem
due to two factors: (i) the BoW features are mostly global
and thus lacking the desired discriminating local features
which are critical for DR image classificaiton; and (ii) the
difference among images of the three categories under
consideration often boils down to only localized regions of
the images, rendering an SVM classifer ineffective when it
is applied to a global feaure. The second factor is further
complicated by the fact that often the images are labeled on
a per-image basis (i.e, without exact information regarding
which regions define the label of the an image). This
prevents the direct application of SVM on a region basis.
In this paper, we develop a spectrally-tuned color autocorrelogram and use it as the feature for classfication. Color
correlograms [7] implicitely describe the global correlation
of local spatial correlation of colors. This is a strong
representation of textures with considerably small
dimensionality. The proposed method further enhances such
strength by spectrally tuning such features based on the
observation that all DR images are relatively saturated in the
red channel. Moreover, recognizing the fact that the images
with or without MA or NV are in general only different in

© 2012 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works. DOI: 10.1109/EMBC.2012.6346216}

small, localized regions, and that the images are in general
only labeled at the per-image level, we propose to employ a
multi-class, multiple-instance learning (MIL) [8] framework
for performing the classification task using the proposed
feature. Together, the development of a spectrally-tuned
auto-correlagram and the employment of an MIL scheme
lead to a novel classfication approach that is able to
overcome typical challegens in DR image classification.
The remainder of this article is organized as follows:
Section 2 describes the proposed algorithm. Section 3
provides the experimental results, including comparison with
other approaches. Section 4 provides concluding remarks.

Figure 1. Histograms of some natural images. The variation in
the histogram is clearly noticed within the three sample images.

II. PROPOSED APPROACH
The color correlogram of an image as proposed in [7] is
a table indexed by color pairs, where the 𝑘th entry for the
color pair (𝑖, 𝑗) specifies the probability of finding a pixel of
color 𝑗 at a distance 𝑘 from a pixel of color 𝑖. Let 𝐼 be a
square image of side 𝑁. Typically, to reduce the
dimensionality of the features, the image is quantized into 𝑚
color bins, say 𝑐1 , 𝑐2 , … 𝑐𝑚 . Let 𝐶(𝑝) denote the color of a
pixel 𝑝 = (𝑥, 𝑦) ∈ 𝐼. An 𝐿∞ -norm is then caluclated to
measure the distance between two pixels, say 𝑝1 = (𝑥1 , 𝑦1 ),
𝑝2 = (𝑥2 , 𝑦2 ), with |𝑝1 − 𝑝2 | = max(|𝑥1 − 𝑥2 |, |𝑦1 − 𝑦2 |).
The histogram of the image 𝐼 (denoted by ℎ𝑖 ) with respect to
each bin is now calculated. This is equivalent to calculating
the number of pairs of pixels, such that |𝑝1 − 𝑝2 | = 𝑘 and
dividing it by the total number of pixels belonging to that
particular bin in the entire image. We limit our search
neighborhood to 3𝑋3. Also, we analyse the distribution of
only those pixels which lie in the same bin (𝑖 = 𝑗). Thus the
features are called color auto-correlogram (AutoCC). This
also ensures that the number of bins in an image is also the
dimensionality of the feature space as the histogram of
correlations of the bins becomes the feature vector.
To restrict the dimensionality of AutoCC features to 44,
Li in [9] proposed a quantization scheme modeled after
human vision, taking into account the color spectrum of
natural images as shown in Fig. 1. However, as illustrated in
Fig. 2, where we can observe that the spectrum for DR
images is significantly different from that of the natural
images considered in [9]. It can be noticed from Fig. 2 that
the DR images have a saturated spectrum in the red channel.
For AutoCC features to work on their best, we require
uniformly shaded bins. To this end, we introduce histrogram
equalization in the red channel, before performing any
feature extraction. Effectively, this results in a spectrallytuned set of features. We elaborate this in the following
subsections, with illustration on the benefits of the
spectrally-selective tuning. A classifier is then trained based
on the extracted features from a training set.
A. Quantizer design
All the images in the training set are considered and the
red channels of the images are equalized. Once equalization
is performed, we convert the training images into a 64-bin
non-uniformly quantized one-channel image. The choice of

Figure 2. Histograms of typical fundus images. The top image is
diagnosed as a normal image, the middle image as MA and the
bottom image NV. It can be noticed clearly how the red channel
for all the classes are saturated.

the 64-bins was empirically determined from a codebook
generation scheme using k-means clustering, as described
below.
We first extract all the unique shades (<R,G,B> triplets)
in the training set. Once all the unique shades present in the
database are extracted, they are arranged in a 𝑁𝑋3 matrix.
Clustering based on k-means is then performed on the above
mentioned matrix. The centroids produced from the k-means
are the codebook of quantization bins. Once the codebook is
generated, feature extraction can be performed.
Figures 3 and 4 show a visualization of this clustering
approach for the quantization scheme proposed in [9] and
the new approach respectively. It can be noticed that while
the approach in Fig. 3, designed for natural images, has a
varied density of points associated with each codeword, the
proposed approach has a uniform density. Though the
codewords are uniformly distributed in the color space,
densities of shades are non-uniform in Fig. 3, while in the
proposed approach, though the codewords are distributed
non-uniformly, the associated shades per codeword are
uniform. This results in reduced entropy of the feature space
and also utilizes the bandwidth provided by the 64
dimensions to full extent in the proposed feature space. It
can also be observed that most of the clusters in the original
approach, though uniformly placed, are empty. This wastes
many dimensions in the original AutoCC, while the
proposed approach utilizes the 64 dimensions more

efficiently. This also implies that the semantic information
useful for a later classifier learning stage is distributed
evenly along the 64 dimensions, which is a desirable
property to have for a feature space.

Step 2. For any pixel, a neighborhood of 3𝑋3 is considered.
The pixel value of the center pixel is compared with
the pixel value of all the 8 pixels in the
neighborhood. A count is made of the number pixels
that hold the same value as the pixel under
consideration. This count gives us the local spatial
distribution of the pixels.
Step 3. This count is added to that bin of a spatial
distribution histogram of the same 64 bins to which
the pixel under consideration belong. This process is
repeated for all the pixels in the instance.

Figure 3. Quantization of AutoCC approach proposed in [9].

Figure 4. Quantization of the proposed AutoCC approach.

B. Feature extraction and classifier design
With the quantization scheme established above,
features are extracted to support a multiple instance learning
(MIL) approach. As was already discussed earlier, in
general, only small regions of an image contribute its being
classified as MA (or PDR in general) or NV (or NPDR in
general). Also, the regions that contribute to the
classification are in general unavailable, but only the class of
the entire image is known (as is the case with all the datasets
mentioned in Section III). Therefore, we propose to adopt
the MIL framework [8] for the classification of the DR
images. In an MIL formulation, one or a subset of instances
of features determines the class of the entire bag of features
(i.e., the entire image). A classifier built on this approach
will divide the feature space into regions where any instance
falling on a certain region will decide the class of the bag
from that instance.
To support the MIL approach, each image is first
cropped to maximally fit a bounding box on the circular
retinal region of the fundus image. The image is then
quantized into 64 bins by using the quantizer discussed
above. The quantized image is then divided into 64 (8X8)
equal non-overlapping blocks. Each block is now considered
an instance and each image is a bag of instances. AutoCC
features for every instance are then extracted using the
following algorithm.
Algorithm for Computing AutoCC:
Step 1. The 64-bin histogram for the instance is recorded

Step 4. The vector thus formed is divided by the count of
global distribution of pixels to get the color autocorrelogram of the instance. This results in a 64
dimensional feature vector for every instance.
Once the features are extracted they are arranged in an
instance-bag model and are used to train a multi-class
multiple instance learning approach. Now, each block of
features can be seen as an instance and each image is a bag.
The implementation of the multi-class learning algorithm is
based on a multiple instance learning toolbox publicly
available at http://www.cs.cmu.edu/~juny/MILL/. We use
the “citation KNN” technique proposed by Wang and
Zucker [8] to solve the MIL problem. Among all the
documented MIL based methods, the authors opt for
Citation-KNN because; 1. The feature space is separable by
correlation distance, 2. Citation-KNN provides better
accuracy than other methods.
III. EXPERIMENTAL RESULTS
The dataset used to evaluate the features consists of 425
images, assembled manually from well-known databases
including DIARETDB0, DIARETDB1 [10], STARE [11]
and Messidor (http://messidor.crihan.fr). In total, there are
160 normal images, 181 NPDR images (mostly MAs) and
84 PDR images (mostly NVs).
The proposed features are evaluated against some
commonly-used features in the medical imaging literature,
such as Gabor features [12] and semantic of neighborhood
color moment histogram features (HNM) [13]. These
features are used in conjunction with the powerful SVM
classifier to form classification schemes. Furthermore, we
also consider a state-of-the-art classification scheme in
computer vision, SIFT+BoW+SVM, which has been found
effective in many computer vision tasks. (Whenever SVM is
used, our implementation was based on the well-known
LibSVM toolbox [14].) Finally, to provide a case that purely
evaluates the benefit of the proposed spectrally-turned
AutoCC feature, we also evaluate a scheme using the
original AutoCC features in [9] together with the MIL
approach in this paper. All these approaches are listed in the
first column of Table 1.
To better assess the performance of these various
classification schemes on our dataset, we employed 5-fold
cross-validation for multiple runs, and then calculated the

mean accuracy of classification over all runs. The results are
summarized in Table 1. From the table, the proposed
approach, which relies on the proposed feature in
conjunction with the MIL formulation, achieved 87.6%
accuracy, outperforming all the competing approaches by a
large margin. This suggests that the proposed method is very
promising for DR image classification.
It is interesting to note that, the (generally speaking)
powerful approach of “SIFT+BoW+SVM” fares poorly in
this study. We made some observations that may explain
this. We found that SIFT predominantly captures the optic
disk, nerves and some of the edges of the images. It does
capture some keypoints but it fails to capture the difference
between MA and NV.
To further understand where the misclassified are
placed and how the classification is performing with respect
to individual classes, the authors have also provided
confusion matrices in Table 2 for the “original
AutoCC+MIL” scheme and
the “spectrally-tuned
AutoCC+MIL” scheme respectively. From the matrices,
when using the proposed feature with only a 64-dimensional
space, the MIL approach performs the classification almost
equally well for all the classes. In contrast, the original
AutoCC feature has difficulty distinguishing between NPDR
and PDR. This illustrates the efficiency of the quantizer
designed to spectrally tune the features. Note that, in this
specific comparison, the classifier is the same (i.e., MIL) and
thus the gain is purely due to the improved feature design.
Table 1. Mean accuracy of various methods.

Approach
SIFT+BoW+SVM
Gabor features+SVM
HNM + SVM
Original AutoCC+MIL
Proposed Algorithm

Mean Accuracy
51.14 %
64.71 %
75.76 %
78.01 %
87.61 %

Table 2a. Confusion matrix for original AutoCC+MIL.

Normal
NPDR
PDR

Normal
79.34
5.31
2.46

NPDR
14.11
92.19
52.61

PDR
6.54
2.5
44.93

Table 2b. Confusion matrix for the proposed approach.

Normal
NPDR
PDR

Normal
88.12
3.05
1.41

NPDR
8.11
86.05
8.59

PDR
3.77
10.9
90

IV. CONCLUSIONS AND FUTURE WORK
In this paper, we proposed a spectrally-tuned AutoCC
feature and an MIL framework based on this feature for DR
image classification. Experiments with comparison with
commonly-used
image
classification
approaches
demonstrated that the proposed method is able to achieve
significant performance gain. While being potentially

promising, the proposed method needs to be further
evaluated and improved with larger dataset. This is currently
being pursued.
ACKNOWLEDGEMENT: The authors acknowledge support
from AHRQ under Grant R21 HS19792-01A1. The views
expressed in this paper are purely those of the authors and do
not represent official endorsement by AHRQ.
REFERENCES
[1] S. Ravishankar, A. Jain and A. Mittal, "Automated feature
extraction for early detection of diabetic retinopathy in fundus
images.," in IEEE conference on computer Vision and Pattern
Recognition (CVPR)., 2009.
[2] M. Goldbaum, N. Katz, S. Chaudhuri and M. Nelson, "Image
understanding for automated retinal diagnosis," in
Proceedings of the annual symposium on computer
application in medical care., 1989.
[3] A. Gupta , S. Moezzi, A. Taylor, S. Chatterjee, R. Jain, L.
Goldbaum and S. Burgess, "Content-based retrieval of
opthalmological images.," in International conference on
image processing (ICIP), 1996.
[4] M. Ahmad Fadzil, "Gaussian Bayes Classifier for Medical
Diagnosis and Grading: Application to Diabetic Retinopathy,"
in IEEE EMBS Conference on Biomedical Engineering, 2010.
[5] A. Hani, H. Nugroho, H. Nugroho, L. Izhar, N. Ngah, T.
George, M. Ismail, E. Hussein and G. Pin, "Toward a Fully
Automated DR Grading System," in 5th European Conference
of the International Federation for Medical and Biological
Engineering, 2012.
[6] D. Lowe, "Distinctive image features from scale-invarient
keypoints.," IJCV, vol. 60, pp. 91-110, 2004.
[7] J. Huang, S. Kumar, M. Mitra, W. Zhu and R. Zabih, "Image
indexing using color correlograms," in IEEE computer society
conference on computer vision and pattern recognition
(CVPR), 1997.
[8] J. Wang and J.-D. Zucker, "Solving the multiple-instance
problem: A lazy learning approach," in 17th International
conference of Machine Learning, 2000.
[9] M. Li, "Texture moment content-based image retrieval," in
IEEE ICME, 2007.
[10] T. Kauppi, V. Kalesnykiene, J. Kamarainen, L. Lensu, I. Sorri,
A. Raninen, R. Voutilainen, H. Uusitalo, H. Kalviainen and J.
Pietila, "Diaretdb1: Diabetic retinopathy database and
evaluation protocol," Procdeeings of medical image
understanding and analysis (MIUA), pp. 61-65, 2007.
[11] B. McCormick and M. Goldbaum, "STARE=Structered
analysis of the retina: Image processing of TV fundus image.,"
1975.
[12] B. Manjunath and W. Ma, "Texture features for browsing and
retrieval of image data," IEEE transactions on pattern
analysis and machine intelligence (PAMI-special issue on
digital libraries), vol. 18, pp. 837-842, August 1996.
[13] Q. Chen, X. Tai, Y. Dong, S. Pan, X. Wang and C. Yin,
"Medical image retrieval based on sementic of neighborhood
color moment histogram," in The 2nd international conference
on bioinformatics and biomedical engineering (ICBBE), 2008.
[14] Chang, Chih-Chung and C.-J. Lin, "{LIBSVM}: A library for
support vector machines," ACM transactions on intelligent
systems and technology, vol. 2, no. 3, pp. 27:1 - 27:27, 2011.

Joint Conditional Random Field of Multiple Views with Online Learning for
Image-based Rendering
Wenfeng Li and Baoxin Li
Department of Computer Science and Engineering, Arizona State University
{Wenfeng.Li, Baoxin.Li}@asu.edu

Abstract
There are many applications, such as image-based
rendering, where multiple views of a scene are considered
simultaneously for improved analysis through employing
strong correlation among the set of pixels corresponding to
the same physical scene point. While being a useful tool for
modeling pixel interactions, Markov Random Field (MRF)
models encounter challenges in such cases since they
assume strong independence of the observed data for
tractability, rendering it difficult to take advantage of
having multiple correlated views. In this paper we propose
joint Conditional Random Field (CRF) for multiple views
in the context of virtual view synthesis in image-based
rendering. The model is enabled by the adoption of
steerable spatial filters for capturing not only the pixel
dependence in a single image but also their correlations
among multiple views. Furthermore, a novel on-line
learning scheme is proposed for the CRF model, which
learns the CRF parameters from the same input data for
synthesizing virtual views. This effectively makes the model
adaptive to the input and thus optimal results can be
expected. Experiments are designed to validate the
proposed approach and its effectiveness.

1. Introduction
Given multiple images captured from different
viewpoints of a 3D scene, to synthesize a photorealistic
virtual view from an arbitrary viewpoint is a main goal of
image-based rendering (IBR) [4, 5], which has many
applications and has received much attention in the relevant
fields. In [4], Shum and Kang reviewed various IBR
techniques and classified them into three categories based
on whether the scene geometry is assumed or utilized:
rendering with explicit geometry, rendering with implicit
geometry, and rendering without geometry. In the first
category, the virtual view can be produced by projecting
pixels from all the reference images. View-dependent
texture-mapping [6], 3D warping [7], and layered-depth
images [8] are typical methods that belong to this category.
If the camera geometry, usually presented as a projection

978-1-4244-2243-2/08/$25.00 ©2008 IEEE

matrix, is known, the 3D coordinate of every point is
determined by the 2D coordinate on image and the depth z.
The depth z may be obtained by stereo matching [9] using
only the input images, which remains to be a challenge in
general if a dense depth map with accuracy is desired, or by
other special techniques (e.g., using a laser ranger finder or
structured light), which may be difficult to assume for
many IBR applications. The third category of IBR
techniques, such as light field rendering [10], use a large
number of cameras to capture many views and do not
assume the scene geometry. In between these two extremes,
other approaches attempt to find the best trade-off between
demanding more images and requiring more accurate scene
geometry. Our work in this paper belongs to this category,
where only a few views (6 to 10 in our experiments) are
used to synthesize a virtual view without computing
accurate scene geometry.
We adopt the basic formulation for IBR as in [11], where
virtual view synthesis is expressed as a maximum
likelihood estimation (MLE) problem. In [11], a texture
dictionary is used as the prior in a Markov Random Field
(MRF) model. Woodford et al. [20, 3] extended this work
by using a different MRF prior with field of experts and
pairwise dictionaries. While being useful for modeling
pixel interactions, MRF assumes strong independence of
the observed data for tractability. Recently, Conditional
Random Field (CRF) [2] was proposed, which does not
suffer from such limitation. CRF directly models the
posterior as a Gibbs distribution and allows arbitrary
dependencies among the observed data. As spatial
dependencies among pixels and textures are abundant in
natural images, CRF draws a lot of interest from
researchers in image processing and computer vision, and it
has been applied to image segmentation [21], image
labeling [22], and stereo matching [19], etc. In this paper,
we propose a joint CRF framework that models not only the
pixel dependence within a single image but also the
correlation of the pixels across multiple views.
The power of CRF first comes from its flexible graph
structure. As pointed out by Roth and Black [12], MRF is
limited by their neighborhood structures. Most models used
in image and vision problems are the 4-connectness
neighborhood models. High-order models are possible but
the learning is difficult [13, 14]. In contrary, the graph

model used in CRF is not restricted and it even does not
have to be a graph. Earlier CRF application such as speech
recognition uses simple chain graph [2]. Lattice structure
and spatial filter is used in recent work for images [1, 17,
22]. In this paper, we propose to model the pixel
dependencies within a local neighborhood based on the
outputs of linear steerable filters across multiple views. The
steerability of the filters leads to efficiency in computation
since only a limit number of filtering directions are needed.
The support of the filter can include more than a few of
pixels and thus improve the robustness to noise.
CRF also facilitates better parameter learning. In MRF,
the relative effects of the prior and the data likelihood are
weighted by a regularization coefficient, which is fixed
with few exceptions such as in the work of Zhang and Seitz
[23]. We show in this paper how the learning can be done
with a stochastic method to obtain a free-form curve for the
unknown parameters. Further, we argue that the learning is
performed on-line using only the given views of a scene (as
opposed to an offline scheme based on images of different
scenes). This essentially adapts the parameters of the model
to the specifics of the given views and thus optimal results
may be expected.
In Section 2, we first briefly present the basic
formulation for IBR and then propose the joint CRF model
of multiple views, followed by an on-line learning
algorithm for estimating the model parameters. Inference of
a virtual view under the learnt model is straightforward and
hence is discussed briefly. Experimental validation is given
in Section 3, and Section 4 concludes the paper.

2. Proposed Approach
2.1. Probabilistic Formulation of IBR
The virtual view synthesis problem can be described as:
given a set of captured views X ={X j | j = 1, …, N} of a
3D scene, compute a virtual view Y, where X j = {x j (r, c )}
and Y = {y(r, c)}, with x and y representing pixels. For
simplicity, X j and Y are always treated as 2D matrices. We
assume known camera geometry in the form of the
projection matrices for both the reference views X j ’s and
the virtual view Y. (Otherwise, self-calibration and virtual
view specification need to be applied first.) Figure 1
illustrates the geometry of the virtual view and the
reference views. For each pixel y(r, c) in the virtual view, it
can be back-projected to a 3D ray which connects the
virtual camera optical center and the image point. The
projection of this ray on each reference view forms the
epipolar lines. The only undetermined value is the depth z.
The configuration of all z is noted as D={z(r, c)}.
Following Bayes’ rule:
P( X | Y , D) P(Y , D)
(1)
P(Y , D | X ) =
P( X )
For given observed data X, (1) can be written as:

(2)
P(Y , D | X ) ∝ P( X | Y , D) P(Y , D)
Although D is in the posterior term, the goal of virtual
view synthesis is different from stereo matching. Here Y is
the main term of interest, and D is only a by-product. This
means that as long as synthesized view Y is good enough, a
poor estimate of D is not of concern. For example, for
regions of an image with little textures, accurate per-pixel
depth may be difficult to obtain. However, the synthesized
view could still look realistic for those regions.

z
x

1

xj

y(r,c)
Figure 1: Geometry of virtual view and reference views.

A baseline approach to evaluating (2) is to consider the
pixels as i.i.d. random variables, and thus the posterior can
be written as the product of the per-pixel posterior,
N

∏∏ P( x

j

| y , z ) P ( y, z )

(3)

( r ,c ) j =1

With any likelihood model, one can find an MLE
solution by focusing on that term only. To improve upon
MLE, the prior term needs to be considered, which may be
modeled through a potential function in random fields such
as in [11]:
(4)
P( y, z ) = ∏ P( y v , z )
V

where V is defined as a set of neighbor indices on site (r, c),
and 4-neighbor model is commonly used.
Incorporating a simple prior model still does not fully
account for the strong spatial dependencies in the observed
data. For example, assume that there is a strong edge in all
the reference views, a corresponding edge should be
expected in the virtual view. In below, we will show how to
use a CRF model to capture such dependencies.

2.2. Joint CRF of Multiple Views
The virtual view synthesis problem can be formulated as
one of labeling the pixels of the virtual image Y with a finite
label set C, which is the color space, given the set of
observed images {X j }. Let G = (V, E) be a graph such that
Y is indexed by the vertices of G. Then (X,Y) is a
conditional random field if, when conditioned on X, the
random variables y obey the Markov property with respect
to the graph, i.e., P(y u |X, y w , w ≠ u) = P(y u |X, y w , ,
w~u), where u, w are 2D coordinates in the image and w ~
u means w is defined as neighbors of u in G. We construct
the following CRF model

(5)



1
exp ∑ λ k f k ( y |e , X ) + ∑ µ k g k ( y | v , X ) 
Z
v∈V , k
 e∈E ,k

where Z is a normalizing constant. Following the
terminologies in [17], gk is called the association potential
and fk the interaction potential. λk and µk are the parameters
of the CRF. y|s is the set of components of y associated with
the sub-graphs of S. By defining the association and
interaction potentials in ways that capture the dependency
of pixels not only within the virtual view but also across
other given images, we effectively obtain a joint CRF of
multiple views, in which the interaction of the pixels are
realized through the epipolar geometry and
locally-supported spatial filters, as illustrated in Figure 2.
The details of the potential functions will be discussed in
subsequent subsections, where it will also become clear
that the pixel interactions can be in terms of color and other
filter outputs. This gives us a way of modeling both the
color and the texture of the synthesized view globally
conditioned on the reference views.

X1

XN
x

1

xN

and this has to be done for each pixel on all possible z
values. Fortunately, we show in the below that this is not
necessary. For one point (r, c) with z, the pixels sampled
from the reference views are determined by the camera
geometry. If the negative logarithm of the probability is
used as an energy function, the energy that needs to be
minimized is:
E =

N

∑ || y − x

j

|| 2

(6)

j =1

where the Euclidian distance in the RGB color space is
used to measure the similarity of pixels. As the function has
a quadratic form, the best color that minimizes (6) is the
mean value of all x j ’s.
In order to be robust to noise, an energy function with
robust kernel is often used and we write the association
potential with negative energy function as
N

g = −∑ min(|| y − x j ||2 ,τ )

(7)

j =1

where τ is a cut-off threshold. However, to find a y to
minimize (7) is not trivial as there is no close form solution.
An iterative algorithm has to be involved by starting with
an initial guess.
8 views

P (Y | X ) =

300 depths

y
Figure 2: Joint conditional random field of multiple views.

Note that, in (5), the depth D is not expressed explicitly
as it is only used to compute point correspondence but not
the CRF inference. (We will show later that depth is
actually coupled with pixel colors by the geometry
constraint.)
2.2.1
Association potential
In the proposed CRF-based modeling in (5), we use the
association potential g(y, X) to measure the similarity of
the synthesized pixel and the pixels from input images.
Hence the association potential plays the role of the
likelihood. We first study how the likelihood P(x j |y, z)
should be modeled, where x j denotes the point on (r j , c j )
corresponding to y. In principle, to compute the likelihood,
y and z has to be enumerated in the color space and the
depth space. In practice, z is considered to follow a uniform
distribution within certain range and can be discretized
evenly into a finite set of sampling points. The range may
be estimated from feature points used in the camera
calibration stage. While enumerating z alone is tractable,
enumerating both z and y is impractical, since the
dimension of the color space is 2563 for 24-bit color images

Ground truth:

MLE result:

Figure 3: Using Maximum Likelihood Estimation to
synthesize one point (marked as red) leads to error.

2.2.2
Interaction potential
One problem with the likelihood model is that it is based
on individual pixels without considering the strong
dependency among pixels. For example, texture-less
regions may always give the most consistent color and may
overshadow small textures. This is illustrated in Figure 3,
where a point on a bar pattern (marked in red) is mistakenly
synthesized as the surrounding background color. This can
be a serious problem in virtual view synthesis as the MLE
solution is often used as a starting point to update certain
prior model that is typically defined on a neighborhood. In
the example of Figure 3, the MLE solution is too far from
the truth and thus it is unlikely to be useful for updating the
prior model to obtain the correct value. Woodford et al.
proposed a solution in [3], where multiple modes are

computed and stored in memory so that the likelihood and
prior term can be optimized at the same time.
Considering the fact that a simple pixel-value-based
likelihood modeling is not sufficient, we propose to use an
interaction potential function to capture the correlations
among the reference views. Specifically, we define the
interaction potential in such a way that it measures the
similarity of the pixels across views in terms of edge and
texture. This is achieved by using a set of spatial filters to
robustly track pixel variance across multiple views (the
filters are further discussed in Section 2.2.3). For each y on
site (r, c), its corresponding points on the reference views
with a given z are {x j } and the responses with the k-th filter
on those points are { rk j }. The interaction potential term can
be written as a negative energy function
N

f k = −∑ min(|| s k − rk j ||2 ,τ )

(8)

modeling of pixel dependency through measuring the
similarity among the outputs of the steerable filters, which
is illustrated in Figure 4. In the figure, the filters in the first
column have the following mathematical expression:
(10)
H ( x, y ) = G n ( x ) * G 0 ( y )
where Gn(u) denotes the n-th order Gaussian derivative
filter on direction u. Each row in Figure 4 is a rotated
version of its first filter. According to the order of Gaussian
derivative, each row has different number of rotated filters
as the bases. This is determined by the steerable filter
theorem [16] that states that any n-th order Gaussian
derivative filter can be steered by n+1 rotated bases. For
instance, for the first order Gaussian derivative filter, its
rotated version can be represented by a linear combination
of the two bases in the first row.
(11)
Rθ ( x, y) = R1 cos θ + R2 sin θ

j =1

where sk is the k-th filter response on the synthesized view.
The role of the interaction potential is to model the
dependence of the synthesized pixels (textures on the
synthesized image) conditioned on the observed data
(textures on the reference images). However if sk is
computed by filtering the synthesized image, it can only be
computed after initial values of y are obtained. Relying on
result from only the association potential will undermine
the use of the interaction potential. We use a two stage
strategy: in the first stage, association and interaction
potential are used simultaneously to search for a best depth
z for each pixel, but sk is computed from rk j in the same way
as y is computed from x j ; In the second stage, we optimize
the random fields while sk is computed from y. By doing so,
textures are preserved and reinforced, false responses to
textures may rise but will be penalized in the second stage.
In our implementation, we use only one filter set (the
first row of Figure 4) which captures the color gradients.
This can be justified by reports from the literature that
intensity gradient is the richest feature in natural images
and is most useful for feature tracking. More filters can be
used as in [13] to further improve the performance but at
the cost of more computational power. Hereafter, we drop
the subscript k for the interaction potential and write the
CRF model as


1
P(Y | X ) = exp ∑ λ f + ∑ µ g 
Z
( r ,c )
 ( r ,c)

N

g = −∑ min(|| y − x j ||2 ,τ g )
j =1
N

f = −∑ min(|| s − r j || 2 ,τ f )

(9)

j =1

where λ and µ are the CRF parameters.
2.2.3
Using steerable spatial filters
The multi-view CRF defined above relies on the

Figure 4: Steerable linear spatial filters.

Figure 5: One input image after being applied the two basis
filters separately (top left and top right). Note that one
detects vertical edges and the other detects horizontal
edges. After being steered to strongest gradient orientation
(bottom), all edges are detected.

This steerability feature provides an efficient way to
handle the rotation of the image patches across multiple
views. Using the first row as example, which can be viewed
as edge detector, for each point on the images, the filter
responses R1 and R2 with two bases are first computed, then
the orientation with the strongest response can be obtained
analytically [15,16] by θ = tan-1(R1/R2). A new filtered
image is generated by steering all points to the orientation
of its strongest response as illustrated in Figure 5.

The mean of filter responses on all color channels is used
to find the maximum gradient since the gradient on each
color channel may not be consistent. Once the orientation is
determined, new responses are computed on each color
channel separately. This can be efficiently done by using
(11) as the filters are steerable. Using gradient on three
channels may seem redundant as they are not independent.
However, our experiments show that doing so slightly
improve the performance. In our experiments, the
orientations are not used again when computing the
similarity measure. This worked out fine since the
possibility of multiple points having the same filter
response in three channels only with different orientations
is small.

2.3. Online Learning
To learn the CRF parameters in (5), there has to be a set
of images with depth map as ground truth. Obtaining such
data is expensive and/or time-consuming. Even if this may
not be an issue if we use synthesized data from some
simulation software, one wonders if the parameters learned
from a training set are optimal or even good enough for a
new set of data. To address this problem, we introduce the
following online learning approach: we learn the model
parameter by using one reference view as the ground truth
and attempting to synthesize this view from other views. In
this way, the parameter learning is purely based on the
input data itself and thus presumably adaptive to the input
(which is indeed the case as will be illustrated later).
The task of learning is to estimate the parameter Θ = {λ,
µ} which best explains the given data according to the
model in (5). It is equivalent to maximizing the following
log conditional likelihood
L(Θ) = ∑ (λ f + µ g ) − log Z
r ,c
(12)
T
= ∑ Θ F − log Z
r ,c

where F =[f;g]. Differentiating with respect to Θ, we have
∂L(Θ)
(13)
= ∑F − ∑F
∂Θ
r ,c
r ,c
p ( y,z )

The second term in (13) denotes the expectation under
probability distribution p(y, z). In CRF with simple graph
like chain model, the expectation can be computed
efficiently with a dynamic programming method, similar to
the forward-backward algorithm for hidden Markov
models. However, for images, the CRF graph model is
more complex, making it intractable to compute the
expectation. Scharstein and Pal [19] used graph-cuts to
minimize the energy function when the partition function in
their model is constant. We use a gradient descent method
which is similar to the one used in [1] where parameters are
learned by penalizing the difference between the mode of
the conditional distribution and the ground truth. From

practical consideration, we limit the learning to the center
of cropped virtual view to guarantee that each pixel in the
virtual view can be mapped to a valid area of all reference
views on all possible depth.
CRF parameters must be adaptive to input signals which
are {x j } and {r j } in this particular application. The
dimension is 6N, where N is the number of reference views.
Using such high dimensional data to model the parameters
is difficult and unnecessary. Note that the goal of the
association potential and the interaction potential is to find
a reconstructed color y and filter response s by measuring
the consistency among {x j } and {r j }. For a good
estimation, y≈x 1 ≈…x N and s≈r 1 ≈…r N , therefore we can
use y and s as input signals. The motivation of the
parameter design is that when a filter response is strong, the
model should trust more on the interaction potential and
less on the association potential. In this paper we focus on
exploiting such weighting effect by writing λ as a constant
and µ as a function of the norm of s.
As the form of the function µ(||s||) is unknown, we use a
non-parametric function with 128 discrete points µ[i], i=0,
… 127 and for an arbitrary value of ||s||, µ is computed by a
linear interpolation of two points with indices nearest to
||s||. The filters are also designed to limit the norm of the
responses in the range of 0 to 127.
A gradient search method must have a good
initialization. We observe that the MLE method can
synthesize most pixels reasonably well and thus we can use
the MLE solution for the initialization. In practice, by
letting λ to a very small value, i.e., λ = 0.05 and µ[i]=1 for
all i, we obtain a model which is almost identical to an MLE
model and this will be used to initialize the learning stage.
The gradient descent problem can be described as, given
an objective function F(x) at point x', find the direction ∆x
so that F(x'+∆x) < F(x'), and x' is updated with x'+∆x. Based
on (5), for one pixel in the virtual view based on the current
CRF parameters, the objective function can be defined as
(14)
F = −µ (|| s ||) g − f
For the training step, the depth z of one pixel can be
chosen as the value that renders the synthesized color
closest to the real color, and we can obtain s' for this
(ground truth) depth z. Let its corresponding objective
function value be F', the goal is to let F'<F. Since in (14) all
other terms are fixed except µ, the new parameters should
let µ(||s| |)<µ(|| s'| |). This could be achieved by
decreasing the parameter points µ[ceil(||s||)] and
µ[floor(||s ||)]. Note that, in principle, to re-evaluate the
new parameters, the whole image must be synthesized for
each processed pixel. To avoid such inefficiency, we
accumulate the desired update for µ(||s||) for all pixels of
an image and make the update only once. The entire
procedure is summarized in the following algorithm.

On-line learning of the CRF parameters
1. Pick one reference view from the input images as
training target view.
2. Use the other N-1 reference views and the current
parameters to synthesize a view.
3. Check all pixels that have large errors and take the
following vote:
-Initialize ∆= [0, 0, … 0]
-Use the ground truth pixel to find the best depth z
and compute its corresponding filter response s'.
-If s' > s
Increase ∆[ceil(| | s '| | ) ] and ∆[floor( | | s ' | |) ]
Else
Decrease ∆[ceil( || s ' | |) ] and ∆[floor( | | s ' || ) ]
End if
4. Update parameters with µ =µ - step ·∆
5. Exit if meets stop criteria, otherwise go to 2

7 demonstrates that the root-mean-square error decreases
and the image quality increases. This proves that the
adaptation of parameters is effective.

Figure 7: Synthesized virtual view with updated
parameters in learning.

2.4. Inference

Figure 6: Two sets of CRF parameters learned from two
different image sets. Green curves show the transition of
parameters from initial value (blue) to final result (red) in
every 10 iterations.

Figure 6 shows how µ is updated through learning. We
empirically fixed the maximum iteration to be 50 as the
stop criteria. Note that the result is not a monotonous curve.
It is also found that the parameters greater than 60 are not
updated at all. This is because in the images used for
training, no filter response has the value greater than 60 and
thus those parameters will not affect the inference with the
conditional model. This also shows that the proposed
CRF-based model and its parameter learning are indeed
adaptive to the input data, verifying the idea of online
learning. This point is further illustrated by the two plots in
Figure 6 that are two sets of parameters learned from two
different datasets, showing the dramatic differences in the
learnt parameters. With parameters updated in learning, a
sequence of virtual view can also be synthesized and Figure

The random field optimization has to be approximated as
the problem is NP-hard. Gibbs sampling could work well in
such a problem but may take a long time to converge. We
use a two-stage inference procedure. First we find initial
values of y and s from reference images. In the second stage
s is computed by filtering y and both the association
potential and the interaction potential are used to maximize
the conditional probability. As the result from the first stage
is good enough, we found that a simple Iterated Conditional
Mode (ICM) approach suffices this objective. For each
point, a better mode is probed by looking at its neighbors, if
the color of one of its neighbors gives greater value of
conditional probability, it is accepted as the new color.

3. Experiments
We use the datasets from [3,11,13] in our experiments.
For comparison, we also implement the MLE method and
its variation with a robust kernel as in (7). To find the best
color y to minimize (7), a Gaussian mixture model (GMM)
is used to find the most consistent cluster of pixels with an
EM algorithm [18]. The cluster of pixels is fitted with a
Gaussian component and outliers are modeled with a
uniform distribution. We also tested the mean-shift
algorithm to estimate the best color iteratively as suggested
by [3], which performed slightly worse than the GMM
method in terms of the quality of the synthesized images.
This is because GMM method is more robust to the cut-off
threshold used in the robust kernel. Therefore, in the
following, we only compare with the mixture model.
The leave-one-out test result is listed in Figure 9 where
one view is used as the ground truth and a new view is
synthesized with its projection matrix. Eight closest views
are used as reference views. The results show that MLE

tends to smooth the synthesized images and sometimes
blends different layers, which renders visually pleasing
images but creates significant artifacts where occlusion
occurs. GMM works much better to handle occlusion. The
proposed CRF-based method outperforms the above two
methods in all experiments in term of both
root-mean-square (RMS) error and error rate which is
defined in [3] as the percent of pixels with sum of squared
errors greater than 1000. As we do not have the exact
configuration to reproduce the other works for detailed
comparison, based on the results, we only claim that our
results are comparable to the state-of-the-art approaches
such as [3] and [20] in both RMS error and visual quality.
Figure 10 are two complete virtual views synthesized
with our approach. Notice that our results with CRF
preserve some fine details like the stem and textures on the
leaves, comparing with magnified blocks from MLE
results.
We also tested our method with drastically different
reference views. The results are given in Figure 11. As
expected, the performance is degraded compared with the
case with close reference views. But still the performance is
reasonable especially given the large difference of the
reference views. Those failures are mainly due to breaking
of Lambertian surface assumption where there is strong
reflection and the colors for one point are different from
different viewpoints.
For the speed performance in term of rendering time, the
proposed method requires 3 times more than the MLE
method, which is comparable to those MRF methods as
reported by [3] (with about 5 to 8 times of the MLE method
in their implementation). While applying filters on all
reference views and steering the results require extra time,
our experiments show this time is relative much smaller
than the rendering time and thus can be ignored.

4. Conclusion and Future Work
In this paper, we proposed joint CRF of multiple views
for virtual view synthesis. We also presented an online
learning algorithm for estimating the optimal parameters
for the model. Our experiments show that the model is
effective and the learning algorithm is a feasible solution.
For future work, we expect to expand the set of filters,
which will demand modification to the learning algorithm.
We will also extend the framework to views with only
weak calibration.

5. References
[1] M. F. Tappen, C. Liu, E. H. Adelson, and W. T. Freeman,
Learning Gaussian conditional random fields for low-level
vision, CVPR 2007.
[2] J. Lafferty, A. McCallum and F Pereira, Conditional random
fields: Probabilistic models for segmenting and labeling
sequence data, ICML 2001, pp. 282-289.

[3] O. Woodford, I. Reid, and A. Fitzgibbon, Efficient new-view
synthesis using pairwise dictionary priors, CVPR 2007.
[4] H. Y. Shum and S. B. Kang, A review of image-based
rendering techniques, IEEE/SPIE Visual Communications
and Image Processing (VCIP), pp. 2-13, 2000.
[5] C. Zhang and T. Chen, A survey on image-based rendering:
representation, sampling and compression, Signal
Processing: Image Communication, vol. 19(1), pp. 1-28,
January 2004.
[6] Paul E. Debevec, George Borshukov, and Yizhou Yu.
Efficient view-dependent image-based rendering with
projective texture-mapping. In 9th Eurographics Rendering
Workshop, Vienna, Austria, June 1998.
[7] W. Mark, L. McMillan, and G. Bishop. Post-rendering 3d
warping. In Proc. Symposium on I3D Graphics, pp. 7–16,
1997.
[8] J. Shade, S. Gortler, L.-W. He, and R. Szeliski, Layered
depth images, In Computer Graphics (SIGGRAPH’98)
Proceedings, pp. 231–242, 1998.
[9] D. Scharstein, Stereo vision for view synthesis, CVPR 1996,
pp. 852-858.
[10] M. Levoy and P. Hanrahan. Light field rendering, In
Computer Graphics (SIGGRAPH’96) Proceedings, pp.
31–42, 1996.
[11] A. W. Fitzgibbon, Y. Wexler and A. Zisserman, Image-based
rendering using image-based priors, ICCV 2003, pp.
1176-1183.
[12] S. Roth and M. J. Black, Steerable random fields, ICCV
2007, pp. 1-8.
[13] S. Roth and M. J. Black. Fields of experts: A framework for
learning image priors, CVPR 2005, pp. 860–867.
[14] J. J. McAuley, T. S. Caetano, A. J. Smola and M. O. Franz,
Learning high-order MRF priors of color images, ICML
2006, pp. 617-624.
[15] D. G. Jones and J. Malik, A computational framework for
determining stereo correspondence from a set of linear
spatial filters, ECCV 1992, pp. 395-410.
[16] W. T. Freeman and E. H. Adelson, The design and use of
steerable filters, IEEE Trans. on Pattern Analysis and
Machine Intelligence, 13(9), pp. 891-906, 1991.
[17] S. Kumar and M. Hebert, Discriminative fields for modeling
spatial dependencies in natural images, NIPS 2003.
[18] D. J. Miller and J. Browning, A mixture model and EM based
algorithm for class discovery, robust classification, and
outlier rejection in mixed labeled/unlabeled data sets, IEEE
Trans. Pattern Analysis and Machine Intelligence, 25(11),
pp. 1468-1483, 2003
[19] D. Scharstein and C. Pal, Learning conditional random fields
for stereo, CVPR 2007.
[20] O. Woodford, I. Reid, P. Torr and A. Fitzgibbon, Fields of
experts for image-based rendering, BMVC 2006.
[21] J. Verbeek and B. Triggs, Scene segmentation with CRFs
learned from partially labeled images, NIPS 2007.
[22] X. He, R. S. Zemel, M. A. Carreira-Perpinan, Multiscale
conditional random fields for image labeling, CVPR 2004,
pp. 695-702.
[23] L. Zhang and S. M. Seitz, Parameter estimation for MRF
stereo, CVPR 2005, pp. 288-295.

Ground truth

RMS
Error rate

Synthesized image and its difference to the ground truth
MLE

GMM

20.82
11.09%

26.40
11.49%

CRF

14.16
9.46%

RMS
21.18
18.52
13.41
Error rate
15.67%
12.52%
12.45%
Figure 9: Leave-one-out test on Edmontosaurus and plant & toy data. Shown to the left of each synthesized view is the error frame.
Below: Blocks
from MLE

Figure 10: Complete synthesized view with CRF, top: Plant
& toy, two blocks synthesized with MLE for comparison;
bottom: Monkey.

Figure 11: Synthesized image with drastically different
reference views (top), using N=8. Two farthest reference
views (bottom) used to render the new view. This
not-so-good result is intentionally kept to illustrate the
robust of the algorithm when the input views are very
different.

Efficient Unsupervised Abnormal Crowd Activity Detection Based on a
Spatiotemporal Saliency Detector
Yilin Wang
Arizona State University
Tempe, Arizona

Qiang Zhang
Samsung
Pasadena, California

Baoxin Li
Arizona State University
Tempe, Arizona

ywang370@asu.edu

zhangtemplar@gmail.com

baoxin.li@asu.edu

Abstract

Typically, video features such as optical flow, motion trajectory, and spatialtemporal interest points, lack of semantic meanings required by the abnormality detection. In the
supervised case, label information could be directly utilized
to build the connection between video features and video labels. Thus, unsupervised video abnormality detection is inherently more challenging than its supervised counterpart.
In this paper, we start from visual saliency, which has attracted a lot of interests in the vision community in recent
years. One early work that is widely known is the approach
by Itti et al. [19]. Since then, a lot of different models have
been proposed for computing visual saliency. Moreover, visual saliency often depends on not only a static scene but
also the changes in the scene. To this end, spatiotemporal saliency has been proposed, which tries to capture regions attracting visual attention in the spatiotemporal domain. Spatiotemporal saliency has been applied to vision
tasks such as video summarization, human-computer interaction [18], and video compression. However, these approaches only focus on the video objects or foreground, but
ignore irregular motion pattern changes, which is an essential part in abnormality event detection. On the other hand,
the saliency information can be regarded as an abstract of
the video frame (image) [32, 36], which may be exploited to
enable unsupervised abnormality detection. How to achieve
this is the objective of our approach.
In this paper, we study unsupervised video abnormality detection based on a spatiotemporal saliency detector
by investigating two related challenges: (1) how to model
the interaction between video content and spatiotemporal
saliency systematically so as to augment video analysis using the information from saliency detection, and (2) how to
use spatiotemporal saliency information to enable unsupervised video analysis. In addressing these two challenges,
we propose a novel spatiotemporal visual saliency detector for video content analysis, based on the phase information of the video. With the saliency map computed using the proposed method, we analyze how it can be used
for two fundamental vision tasks, namely saliency detec-

Approaches to abnormality detection in crowded scene
largely rely on supervised methods using discriminative
models. In this paper, we presents a novel and efficient
unsupervised learning method for video analysis. We start
from visual saliency, which has been used in several vision
tasks, e.g., image classification, object detection, and foreground segmentation. To detect saliency regions in video
sequences, we propose a new approach for detecting spatiotemporal visual saliency based on the phase spectrum of
the videos, which is easy to implement and computationally efficient. With the proposed algorithm, we also study
how the spatiotemporal saliency can be used in two important vision tasks, saliency prediction and abnormality
detection. The proposed algorithm is evaluated on several
benchmark datasets with comparison to the state-of-the-art
methods from the literature. The experiments demonstrate
the effectiveness of the proposed approach to spatiotemporal visual saliency detection and its application to the above
vision tasks.

1. Introduction
Automatic abnormality detection for online multimedia
content has been an active area in recent years due to it potential applications for crowded surveillance[30], social media behavior monitoring[35, 38, 34]and event retrieval[7].
Early approaches [30, 8, 24] focus on either generating discriminative model for semantic indexing the video or decompose it into semantic parts. These approaches, which
rely on frame-based video labels, have been shown effective
on certain datasets. Unfortunately, frame-based labels are
in general hard to obtain. Especially, for massive YouTube
videos, it is too labor-and time-intensive to obtain labeled
sets large enough for robust training. Thus, the unsupervised approach would be more desirable. This paper studies
unsupervised video abnormality detection.
1

tion and abnormal event detection. We evaluate the performance of the proposed algorithm using several widely
used datasets, with the comparison to the state-of-the-art in
the literature. Our main contribution can be summarized as
following: (1) A parameter free approach to enabling unsupervised video event detection. Neither normal examples
nor abnormal examples are required for abnormality detection; (2) A novel and efficient framework for spatiotemporal
saliency detection, which captures the global motion information and can be used to model complex activities. We
demonstrate the complexity of the proposed algorithm is
only O(N log N ), where N is the size of the input; and (3)
Comprehensive comparisons and evaluations using several
benchmark datasets on saliency detection and abnormality
detection are used to demonstrate that the effectiveness of
the proposed approach, suggesting its potential application
for future video analysis tasks.

2. The Proposed Method
2.1. Spectrum Analysis for Saliency Detection
There has been several explanations for why spectral domain based approach is able to detect saliency region from
the image. For example, In [3], it has been shown that human visual system will select a subset of objects to focus.
In other words, an attention competition exists among objects in the image. Only a small portion of objects, which
are more distinctive, will be popped out, and rest of objects, which are usually in a uniform or common patterns,
are suppressed. The spectral magnitude measures the total
response of cells tuned to the specific frequency and orientation. According to lateral surround inhibition, similarly
tuned cells will be suppressed depending on their total response, which can be modeled by dividing its spectral by
the spectral magnitude [37]. [13] provided another explanation from sparse representation, which states that, if the
foreground is sparse in spatial domain and background is
sparse in DCT domain (e.g., periodic textures), the spectral
domain based approach will highlight the foreground region
in the saliency map. In a word, given an image (or 2D signal), f (x, y), the saliency map can be calculated as:
S(x, y) = F −1 [h(m, n) ∗ A(m, n) · e−iP(m,n) ]

(1)

where h is a high pass filter and A, P represent amplitude
and phase of Fourier transform F .

2.2. Spectrum Analysis for Normal Videos
Spectrum analysis in spatiotemporal data, e.g., videos,
is still in its infancy. In [16], the author studied how motion patterns contribute to saliency. It demonstrate that by
setting the target object having different flicker rate, moving direction or motion velocity from other objects, the tar-

Figure 1. The spectrum analysis for normal videos. The first column is sample frame, the second column shows sampled video
points in frequency domain. Please see the figures in color print.

get object can be easily identified by human subjects. In
this paper, we model the video abnormality detection as a
spatiotemporal saliency detection problem, where normal
video frame is regarded as non-salient and abnormality is
perceptually salient.
In [23, 13], it has been shown that, for natural image,
the amplitude spectrum of background (non-salient region)
has higher value at lower frequency. Essentially, our observation demonstrates that, in the temporal domains, the
normal video frames, where object can be modeled in a uniform motion pattern , will have higher amplitude in lower
frequencies than higher frequencies. Thus, the phase information of temporal domain can be used for abnormality
modeling. We show that one can exploit such informative
observation through spectrum analysis.
In order to demonstrate the property of spectrum of normal videos, we generate two synthetic videos. The first
video contains a uniform background (black) and second
video with a moving object (33 by 33) which has its value
uniformly distributed (white). Moreover, the motion trajectory of the object is followed as the red circle with same
speed (we call it regular motion) in Figure 1. Specifically,
two points, which are sampled from background and motion
trajectory respectively, are further plotted in the frequency
domain through the time period.
From Figure 1, we can interpret the result as following:
1) if no global motion in the video, the background (even
with dynamic scenes, we show later) has higher response
in the lower frequency domain. 2)Since the result of the
spectrum obeys the symmetry [2], the amplitude from the
points within regular motion object also trend to be higher
in lower frequencies.

2.3. Modeling Video Abnormality via Saliency Detection
Based on the spectrum analysis of normal video, we
have observed some potential properties. Then two research
questions arises: 1) How to model the video abnormality
only using the information from amplitude spectrum? 2)

How to automatically find the abnormality in a video? Answering these questions leads us to further analyze the amplitude spectrum with phase information. Given a signal
f (x, y, t) it is first transformed to the frequency domain
F
f (x, y, t) −→ F (m, n, k), with the amplitude A(i, j, k) =
|F (m, n, k)| and phase P(m, n, k) = angle(F (m, n, k).
Based on the Fourier Transform and inverse Fourier Transform, we have:
f (x, y, t) = F −1 [F (m, n, k)]
h
i
= F −1 A(m, n, k) · ei·P(m,n,k)

2.4. Analysis
In this section, we provide evidence that, for a foreground object with irregular motion pattern, the proposed
spatiotemporal saliency detector can approximately obtain
its location in a video based on Parseval’s theorem [2].
Parseval’s theorem: The energy in u(t) equals the enR +∞
ergy in U (f ), where u(t) = f =−∞ U (f ) · ei2πf t df
Now, given a 3D signal and reconstruct it with only
phase information:

(2)

In order to extract the video abnormality, and inspired by
the saliency detection, we perform a high pass filtering on
the frequency domain in temporal dimension, which will
suppress the signals from normal videos. Then we model
the abnormality in a saliency fashion:

S(x, y, t) = g ∗ F −1 [
=g∗F

−1

F (m, n, k)
]
|F (m, n, k)|

[1(m, n, k) · e

i·P(m,n,k)

(6)
]

Based on Parseval’s theorem, we know the summation
across all the dimensions of f (x, y, t) is equal to the summation across all the frequency component in the frequency
domain. From the Eq 6, we can see that when only using
S(x, y, t) = g ∗ F −1 [h(k) ∗ A(m, n, k) · ei·P(m,n,k) ] (3)
the phase information it is equal to replace the amplitude
spectrum A(t) to a cube. In other words, all of the elements
where h(k) is the high-pass filter along the temporal diwhich have non-zero value in magnitude spectrum are set to
rection in the frequency domain, and g is a low pass filter
one. The region with repeat (regular) motion pattern creates
in spatiotemporal domain, e.g., 3D Gaussian filter, which
a high peak in the magnitude spectrum (Figure 1) is supsmooths the result. However, Eq 3 only considers the tempressed; while the region with salient (irregular motion patporal information for video abnormality detection, which
tern instead corresponds to the spread-out magnitude specmay involve the noise from background if the video contrum will pop-out. Additionally, based on the proposition in
tains global motion. To alleviate this issue, we further in[13], we can easily extend the sparse condition for saliency
corporate the spatial saliency information to refine the dedetection in the spatial domain to the spatiotemporal dotection results. The improved model is described as below:
main, which means the proposed method is also bounded
with the ratio of salient region and non salient region. Due
−1
i·P(m,n,k) to the space limits, we omit the proof.
S(x, y, t) =g ∗ F [h(k) ∗ l(m, n) ∗ (A(m, n, k) · e
)]
To verify the correctness of the proposed model, we gen(4)
erate one synthetic video to test the abnormality detection.
where l(m, n) is the high pass filter along the spatial direcThe video contains a dynamic background with two movtion in the frequency domain. In frequency domain, setting
ing squares with same texture. One of squares follows the
the spectrum magnitude to uniform can achieve similar efred circle and moves steadily (we call normal object), anfect of high pass filter. In order to reduce the computation
other moving square moving randomly (we call abnormal
cost, we further relax Eq 4 (further analysis shown in Sec
object). The motion trajectory of these square is defined as
2.4 ):
following:
S(x, y, t) = g ∗ F −1 [ei·P(m,n,t) ]

 

(5)
F (m, n, k)
x(t)
128 + 64cos( πt
= g ∗ F −1 (
)
32 )
Γ
(t)
=
=
1
πt
|F (m, n, k)|
y(t)
128 + 64sin( 32
)




πt
Eq 5 actually adopts the phase information of a video for
x(t)
64 + 32cos( 32 ) + ǫ
Γ
(t)
=
=
2
πt
saliency detection, it can be easily paralleled. The Fourier
y(t)
64 + 32sin( 32
)+ǫ
transform for multiple dimensional data can be computed
as a sequence of 1D Fourier transform on each coordinate
of the data. Thus the computation cost of the proposed
where ǫ is a random variable uniformly draw from [0, 128].
spatiotemporal saliency detector is O(M N T log(M N T ))
We view the object with trajectory Γ1 moving regularly. For
when the input data size is X ∈ RM×N ××T . If the
the Gauss filter used to smooth
√ the saliency map, we set the
data has P feature channel, then the computational cost is
standard deviation as 0.006 N 2 + M 2 and te filter size as
O(P M N T log(M N T )).
1 + 6σ, where N × M is the size of each frame.

In Fig. 2, we show some sample frames of the video
(top), the results from the proposed method (middle) with
the comparison to the results of the method proposed in
[11] (bottom), where the frame differences of two adjacent
frames are used as temporal information. From the figure,
we can find the proposed method highlight the moving objects over the dynamic background; in addition, the object
moving “irregularly” (i.e., with trajectory Γ2 ) gets higher
values in the saliency map than the other object (the one
with trajectory Γ1 ) does. In contrast, the method proposed
in [11] not only has problem in segmenting the moving objects from changing background, but also can’t discriminate
the one moving “irregularly” from the one moving regularly. One explanation could be that simply the frame differences of two adjacent frames can’t distinguish the object
moving irregularly from the object moving reguarly. This
reveals the potential of the proposed method to detect irregular events from the video (or abnormal events), as detailed
in the next section.

abnormality detection. The performance of the proposed
methods are compared with the existing methods, some of
which are state-of-the-art methods.

3.1. Simulation Experiment
In this section, we evaluate the proposed method on synthetic data. In [16], how three properties of motion, namely
flicker, direction and velocity, contribute to the saliency was
studied. In this section, we generate the synthetic data according to the their protocol. The input data is a short clip
where the resolution is 174 × 174 with 400 frames at the
frame rate of 60 frames per second. We put 36 objects of
size 5 × 13 in a 6 × 6 grid and a target object is randomly
selected out of those 36 objects. All the objects are allowed
to move within a 29×29 region centered at their initial position (and warped back, if they move out of this region). The
video is black-and-white. We design the following three
experiments:
1. Flicker: we set the objects on-off at a specified rate
and the target object at a different rate from the other
35 objects;
2. Direction: we set the objects moving in a specified
direction and the target object in a different direction.
The velocity of all the objects are the same;
3. Velocity: we set the objects moving in a specified velocity and the target object moves in a different velocity. The moving direction of the all the objects are the
same.

Figure 2. Top: some sample frames from the input video, where
the red circle indicates the trajectory Γ1 ; middle: the corresponding saliency maps; and bottom: the saliency map computed with
method described in [11]. For the saliency map, the warm color
indicates high value and cold color for low value. The video can
be found in the supplementary material. Please see the figures in
color print.

3. Experiment
Since the proposed method is based on saliency detection, to verify the correctness in saliency detection, we first
evaluate the proposed method on both syntheic data (Sec.
3.1) and two real-world video datasets (Sec. 3.2), CRCNSORIG and DIEM, for saliency detection. Then we evaluate the proposed method on several benchmark datasets on

All the other parameters are the same as used in [16]. According to [16], the target object could be easily identified
by human subjects, when its motion property (e.g., flicker
rate, moving direction, velocity) is different from the other
objects. We also include some “blind” trials, where the target object has the same motion property as the other 35 objects. In this case, the target object can’t be identified by the
human subjects, i.e., there is no salient region.
We apply the proposed method to the input data. For
comparison, we also evaluate the method proposed in [4]
and [13]. We use the area under receiver operating characteristic curve as the performance metric. The ground truth
mask is generated according to the location of the target object. The experiment result is shown in Figure 3.
From the experiment results, we can find that the proposed method detects the salient region much more accurately than [4] and [13] in all except the “blind” trials, which
should be as lower as possible in terms of abnormality.
However, [4] and [13] don’t survive in those “blind” trials. Surprisingly, [4] and [13] achieves quite similar performances, though [4] was supposed to achieve better result as
it include the differences of two adjacent frames as motion
(temporal) information.

Direction

Flicker

Velocity

0.7

0.8

0.7
0.65

0.75
0.6

0.7

Proposed
Bian[15]
Hou[8]

0.65
0.6
0.55
0.5

0.6

0.5

0.55

0.4

0.45

0.5

Proposed
Bian[15]
Hou[8]

0.3

0.45
0.4
100

150

200
250
300
350
400
450
Absolute differences of the flicker rate

0.2
500 −10

0

10
20
30
40
Absolute differences of the direction

50

Proposed
Bian[15]
Hou[8]

0.4
0.35
60 −0.1

0

0.1
0.2
0.3
0.4
Absolute differences of the velocity

0.5

0.6

Figure 3. The AUC on the synthetic data for the proposed method and two existing methods. For “Direction” and “Velocity”, we also
include some “blind” trials (X-axis has value 0), where the target object has exactly the same motion property as the other 35 objects. In
those trials, the target object can’t be identified by human subjects, i.e., there is no salient object [16].

Blind

Flicker

Direction

Velocity

Figure 4. Some visual sample of the synthetic data for different experiments.

In previous section, we test the proposed spatiotemporal saliency detector on synthetic videos, with the comparison to two other saliency detectors, where the proposed detector shows better performances in capturing the temporal information. In this section, we evaluate the proposed
spatiotemporal saliency detector on two challenging video
datasets for saliency evaluation, CRCNS-ORIG [20] and
DIEM [29]. For this experiment, we first convert each frame
into the LAB color space, then compute the spatiotemporal
saliency in each channel independently and the final spatiotemporal saliency is the summation of the saliency maps
of all three channels.
CRCNS-ORIG includes 50 video clips from different
genres, including TV programs, outdoor scenes and video
games. Each clip is 6-second to 90-second long at 30 frames
per second. The eye fixation data is captured from eight
subjects with normal or correct-normal vision. In our experiment, we downsample the video from 640 × 480 to
160×120 and keep the frame rate untouched, then apply the
our spatiotemporal saliency detector. To measure the performance, we compute the area under curve (AUC) and Fmeasure (harmonic mean of true positive rate and false positive rate). The experiment result is shown in Fig. 5, where
the area under curve (AUC) is 0.6639 and F-measure is

0.1926. Tab. 1 compares the result of the proposed method
with some state-of-art methods on CRCNS-ORIG, which
indicates that our method outperforms them by at least 0.06
regarding AUC.
1
0.9

True Postive Rate

3.2. Spatiotemporal Saliency Detection

0.8
0.7
0.6
0.5
0.4
0.3
0.2

CRCNS AUC=0.6729
DIEM AUC=0.6715

0.1
0

0

0.2

0.4

0.6

0.8

1

False Postive Rate
Figure 5. The receiver operating characteristic curve of the propose
method in CRCNS-ORIG dataset and DIEM dataset. The area
under the curve is 0.6639 and 0.6896 accordingly.

DIEM dataset collects data of where people look during
dynamic scene viewing such as film trailers, music videos,
or advertisements. It currently consists of data from over

Method
AWS [10]
HouNIPS [15]
Bian [4]
IO
SR [14]
Torralba [33]
Judd [21]
Marat [27]
Rarity-G [26]
CIOFM [17]
Proposed

AUC
0.6000
0.5967
0.5950
0.5950
0.5867
0.5833
0.5833
0.5833
0.5767
0.5767
0.6639

Method
AWS [10]
Bian [4]
Marat [27]
Judd [21]
AIM [6]
HouNIPS [15]
Torralba [33]
GBVS [12]
SR [14]
CIO [17]
Proposed

AUC
0.5770
0.5730
0.5730
0.5700
0.5680
0.5630
0.5840
0.5620
0.5610
0.5560
0.6896

Table 1. The result the proposed method compared with the results of the top ten existing methods on CRCNS dataset (left) and
DIEM dataset (right) according to [5]. From this table, we can find
that the propose method gets obvious better performances than the
state-of-arts on both two datasets.

250 participants watching 85 different videos. Each video
in DIEM dataset includes 1000 to 6000 frames at 30 frames
per second. Similarly as CRCNS, we downsample the video
to 1/4 (e.g., from 1280 × 720 to 320 × 180) while maintaining the aspect ratio and frame rate. We observe that each
video in DIEM dataset is consisted of a sequence of short
clips, where each clip has 30 to 100 frames. To properly
detect the saliency from those videos, we apply the window
function to our spatiotemporal saliency detector, where the
size of the window (along temporal direction) is 60-frame.
The experiment result is shown in Fig. 5 and Tab. 1, where
the AUC is 0.6896 and F-measure is 0.35. From the table,
we can find that the proposed method outperforms the stateof-arts by over 10%.
To compare the performances of combining four visual
cues via QFT and performances via summation of saliency
maps of each visual cues, we design the following experiment. We run 1000 simulations and in each simulation we
generate a r × c× 4 array, where r and c is a random number
between [1, 1000] and 4 is the number of feature channels.
We compute the saliency map with different methods then
measures their similarities via cross-correlation, where 0.91
is reported for QFT and FFT. After smoothing the saliency
map with a Gaussian kernel, the correlation is over 0.998.
For natural image, we could expect an even higher correlation.
This suggests that, we can compute the saliency map for
each visual cue independently and then add them together,
which will yield quite similar result by using quaternion
Fourier transform. In addition, the proposed method other
than QFT provides more flexibility, e.g., we can assign different weights to the visual cues as [21].
We also include the AUC of the proposed method for
each video from the CRCNS-ORIG (Figure 6) and DIEM
dataset (Figure 7).

Method
Optical flow [28]
Social force [28]
NN [9]
Sparse reconstruction [9]
Proposed

AUC
0.84
0.96
0.93
0.978
0.9378

Table 2. The result on UMN dataset. Note, we have cropped out
the region which contains the text “abnormal”, and results in frame
resolution 214 × 320. Please note that, most of those methods,
except the proposed one, need a training stage.

3.3. Abnormality Detection
In this section, we show how can we utilize the proposed
spatiotemporal saliency detector to detect abnormality from
the video.

Method
Social force [28]
MPPCA [22]
MDT [25]
Adam [1]
Reddy [31]
Sparse [9]
Proposed

Ped1
31%
40%
25%
38%
22.5%
19%
27%

Ped2
42%
30%
25%
42%
20%
N.A.
19%

Overall
37%
35%
25%
40%
21.25%
N.A.
23%

Table 3. The frame level EER (the lower the better) for UCSD
dataset. Please note that, most of those methods, except the proposed one, need a training stage. From the result, we can found
that the proposed method, even without traing stage or training
data, can still outperform social force, MPPCA.

For abnormality detection, we start with computing the
saliency map for the input video as described above. The
regions containing abnormalities can be detected by founding the region where the saliency value is above a threshold.
Then the saliency score of a frame is computed as the average of saliency value of the pixels in that frame, i.e.,
s(t) =

1 XX
X(i, j, t)
NM i j

(7)

where s(t) is the saliency score of tth frame, N × M
is the size of one frame, i, j, t are row, column and frame
index of the 3D saliency map accordingly. The frame with
high saliency score would contain abnormality. To show the
proposed method is not sensitive to the value of threshold,
we choose the average value of the saliency in the video as
threshold.
We evaluate the proposed method for abnormality detection in videos from two datasets: UMN abnormal dataset1
and UCSD dataset [25]. Abnormal detection has attracted
1 http://mha.cs.umn.edu/Movies/Crowd-Activity-All.avi

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

be
beverl
bever y01
beverly0
beverlly053
ga bever y06
game verlly07
gamecub y08
gameccube02
game ube04
gameccube05
game ube06
gamecube13
gameccube16
me ube1
m cube187
moonice23
monica03
a
samonnica04
c
i
c
c
sta a a005
standdaete 6
st nd rd st
staandard01
st ndard02
staandard003
s ndard 4
tvt−andard005
aa 6
tv−ctiord07
tv−adns01
tv− tv−ads01
an tv−ads02
tvn−ounads03
0
tv mu ce04
tv−−nesic01
tv−newws01
tv ne s01
tv−−news02
tv news03
tv−−news04
tv n ws 5
tv−−spews06
tv−sports 09
tv−spoorts001
tv−sp rts02
sport 3
tv−ortss04
tv ta 05
tv−−tallk01
tv−talkk03
tal 04
k0
5

0

Figure 6. The AUC of the proposed method for each video from CRCNS-ORIG dataset.
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

50

_p
An eop
le
BBtarct _br
C ic oo
BB _wi a_la klyn
ad C_ ldlif nd _1
ve w e_ sca
r il e
a t_b dlif ag pe
am dve bc4 e_sp le_9
i rt_ _b e
am _ib4 bra ees cial
i_ 01 vi _
arc is10 0_c a_pa102
tic 00 los int
c _be a_ eup _
do hilli_ ars close _7
_
c
do um plas 106 up_
c e t 6
do ume ntar ers_ x71
cu n y_ 12
ga men tary_ cora 80
m ta d l_
ga e_tr ry_p olph re
h me aile lan ins
ho airy_ _tra r_gh et_
me bi iler os e
k
hu _m ers _wr tbu
mm ovi _c ath
e a _
i
mo ngb _Ch bba l
mo vie_ irds arliege
_ _
v
m ie_ trai na b
ne usic trai ler_arrato
ws _r ler lic
ne _be ed_ _qu e_
a
ne ws_s e_p hot_ ntu
ne ws he ara chi
ws _u rry sit li_
_w s_ _d es
ni imb elec rink _7
pin ghtli ledo tion ing
fe
_
pingpon _in_n_m deb
pin gpo g_a mo acen
gp ng_ ngl zam
e
o
pla ng_ long _sh bi
ne no _sh ot_
sc t_e _bo ot
sp ottis arth die _9
or h_ _ s_
s t_b st jun 9
sp port arc arter gles
o _ e
sp rt_p foo lonas_12
ort ok tba _e
sp _sla er_ ll_b xtr
1
sp ort_ m_d 280 est_
o w
sp rt_w imb unk_ x640
ort im led 12
ste s_k ble on 80
wa en do _ba
r do n_ lt
tv_ t_lee _12 ma
tv_ ket _12 80x gi
un ch2 80 71
i_c _6 x7
ha 72 12
lle x5
ng 44
e_
fin

0

Figure 7. The AUC of the proposed method for each video from DIEM dataset.

a lot efforts from the researchers. However, most of the existing works require training stage, e.g., social force [28],
sparse reconstruction [9], MPPCA [22], i.e., they need
training data to initialize the model. The proposed method,
instead, does NOT need any training stage or training data.

The result on UMN abnormal dataset is shown in Tab.
2, where we compute the frame-level true positive rate and
false positive rate then compute the area under the ROC
(Fig. 9). Fig. 10 shows the result for videos of three
scenes, where we plot saliency value of each frame and
show some sample frames. The result on UCSD dataset
is shown in Tab. 3, where we report frame-level equal-error
rate (EER) [25]. Fig. 11 shows the ROC for UCSD dataset
with the proposed method; Fig. 8 shows eight samples
frames, where red color highlights abnormal regions. We
can find that, without training data, the proposed method
still outperforms several state-of-arts in the literature, e.g.,
social force, MPPCA.

4. Conclusion and Discussion
In this paper, we proposed a novel approach for detecting
spatiotemporal saliency, which was simple to implement
and computationally efficient. The proposed approach was
inspired by recent development of spectrum analysis based
visual saliency approaches, where phase information was
used for constructing the saliency map of the image. Recognizing that the computed saliency map captured the region of human’s attention for dynamic scenes, we proposed
two algorithms utilizing this saliency map for two important
vision tasks. These approaches were evaluated on several
well-known datasets with comparisons to the state-of-arts
in the literature, where good results were demonstrated.

5. Acknowledgment
The work was supported in part by an ARO grant
(#W911NF1410371) and an ONR grant (# N00014-15-12722). Any opinions expressed in this material are those of
the authors and do not necessarily reflect the views of ARO
or ONR.

Peds1: Wheelchair

Peds1: Skater
Scene 1

Peds1: Bike

Peds1: Cart

Scene 2
Peds2: Skater

Peds2: Bike

Figure 8. Some sample results for the UCSD datasets, where the
red color highlights the detected abnormal region, i.e., the saliency
value of the pixel is higher than four times of the mean saliency
value of the video. Please see the figures in color print.

Area under cuver: 0.937785
True Positive Rate

1
0.9
0.8
0.7

Scene 3

0.6

Figure 10. Some sample results for the UMN datasets, where we
pick one video for each scene. The top is the saliency value (Yaxis) for each frame (X-axis) and bottom are sample frames picked
from different frames (as shown by the arrow).
Area under curve

0.5
0.4
0.3
0.2
0.1
0

1

0

0.2

0.4

0.6

0.8

1

False Positive Rate

References
[1] A. Adam, E. Rivlin, I. Shimshoni, and D. Reinitz. Robust real-time unusual event detection using multiple fixedlocation monitors. PAMI, 30(3):555 –560, march 2008. 6
[2] A. Antoniou. Digital signal processing. McGraw-Hill
Toronto, Canada:, 2006. 2, 3
[3] D. M. Beck and S. Kastner. Stimulus context modulates competition in human extrastriate cortex. Nature neuroscience,
2005. 2
[4] P. Bian and L. Zhang. Biological plausibility of spectral
domain approach for spatiotemporal visual saliency. NIPS,
pages 251–258, 2009. 4, 6
[5] A. Borji, D. N. Sihite, and L. Itti. Quantitative analysis of

True Positive Rate

Figure 9. The ROC for the UMN dataset computed with the propose method.

0.9

X: 0.1978
Y: 0.8177

0.8
X: 0.2649
Y: 0.7456

0.7

X: 0.2792
Y: 0.7219

0.6
0.5
0.4
0.3

all: 0.8062
ped1: 0.7805
ped2: 0.8772

0.2
0.1
0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

False Positive Rate
Figure 11. The ROC for the UCSD dataset computed with the propose method.
human-model agreement in visual saliency modeling: a comparative study. 2012. 6
[6] N. Bruce and J. Tsotsos. Saliency based on information max-

imization. In NIPS, pages 155–162, 2005. 6
[7] L. Chen, Q. Zhang, P. Zhang, and B. Li. Instructive video
retrieval for surgical skill coaching using attribute learning.
In Multimedia and Expo (ICME), 2015 IEEE International
Conference on, pages 1–6, June 2015. 1
[8] X. Chen and K. S. Candan. GI-NMF: group incremental nonnegative matrix factorization on data streams. In Proceedings
of the 23rd ACM International Conference on Conference
on Information and Knowledge Management, CIKM 2014,
Shanghai, China, November 3-7, 2014, pages 1119–1128,
2014. 1
[9] Y. Cong, J. Yuan, and J. Liu. Sparse reconstruction cost for
abnormal event detection. In CVPR 2011, pages 3449 –3456,
june 2011. 6, 7
[10] A. Garcia-Diaz, X. R. Fdez-Vidal, X. M. Pardo, and R. Dosil.
Decorrelation and distinctiveness provide with human-like
saliency. In NIPS, pages 343–354. Springer, 2009. 6
[11] C. Guo, Q. Ma, and L. Zhang. Spatio-temporal saliency
detection using phase spectrum of quaternion fourier transform. In CVPR, 2008. 4
[12] J. Harel, C. Koch, and P. Perona. Graph-based visual
saliency. In NIPS, pages 545–552, 2006. 6
[13] X. Hou, J. Harel, and C. Koch. Image signature: Highlighting sparse salient regions. PAMI, pages 194–201, 2012. 2, 3,
4
[14] X. Hou and L. Zhang. Saliency detection: A spectral residual
approach. In CVPR, pages 1 –8, 2007. 6
[15] X. Hou and L. Zhang. Dynamic visual attention: Searching
for coding length increments. NIPS, 21:681–688, 2008. 6
[16] D. E. Huber and C. G. Healey. Visualizing data with motion.
In Visualization, 2005. VIS 05. IEEE, pages 527–534. IEEE,
2005. 2, 4, 5
[17] L. Itti and P. Baldi. Bayesian surprise attracts human attention. NIPS, 18:547, 2006. 6
[18] L. Itti, N. Dhavale, and F. Pighin. Realistic avatar eye and
head animation using a neurobiological model of visual attention. In Optical Science and Technology, pages 64–78.
International Society for Optics and Photonics, 2004. 1
[19] L. Itti, C. Koch, and E. Niebur. A model of saliency-based
visual attention for rapid scene analysis. PAMI, nov 1998. 1
[20] R. Itti, Laurent; Carmi. Eye-tracking data from human volunteers watching complex video stimuli. Online, 2009. 5
[21] T. Judd, K. Ehinger, F. Durand, and A. Torralba. Learning
to predict where humans look. In ICCV 2009, pages 2106
–2113, 29 2009-oct. 2 2009. 6
[22] J. Kim and K. Grauman. Observe locally, infer globally:
A space-time mrf for detecting abnormal activities with incremental updates. In CVPR 2009, pages 2921 –2928, june
2009. 6, 7
[23] J. Li, M. D. Levine, X. An, X. Xu, and H. He. Visual
saliency based on scale-space analysis in the frequency domain. PAMI, pages 996–1010, 2013. 2
[24] X. Li, S. Huang, K. S. Candan, and M. L. Sapino. Focusing
decomposition accuracy by personalizing tensor decomposition (PTD). In Proceedings of the 23rd ACM International
Conference on Conference on Information and Knowledge
Management, CIKM 2014, Shanghai, China, November 3-7,
2014, pages 689–698, 2014. 1

[25] V. Mahadevan, W. Li, V. Bhalodia, and N. Vasconcelos.
Anomaly detection in crowded scenes. In CVPR, pages 1975
–1981, june 2010. 6, 7
[26] M. Mancas. Computational attention: Modelisation and application to audio and image processing. PhD thesis, PhD.
Thesis, University of Mons, 2007. 6
[27] S. Marat, T. Ho Phuoc, L. Granjon, N. Guyader, D. Pellerin,
and A. Guérin-Dugué. Modelling spatio-temporal saliency
to predict gaze direction for short videos. IJCV, 82(3):231–
243, 2009. 6
[28] R. Mehran, A. Oyama, and M. Shah. Abnormal crowd behavior detection using social force model. In CVPR 2009,
pages 935 –942, june 2009. 6, 7
[29] P. K. Mital, T. J. Smith, R. L. Hill, and J. M. Henderson.
Clustering of gaze during dynamic scene viewing is predicted by motion. Cognitive Computation, 3(1):5–24, 2011.
5
[30] R. Raghavendra, A. Del Bue, M. Cristani, and V. Murino.
Optimizing interaction force for global anomaly detection
in crowded scenes. In Computer Vision Workshops (ICCV
Workshops), pages 136 –143, nov. 2011. 1
[31] V. Reddy, C. Sanderson, and B. Lovell. Improved anomaly
detection in crowded scenes via cell-based analysis of foreground speed, size and texture. In CVPRW, pages 55 –61,
june 2011. 6
[32] Q. Tian and B. Li. Simultaneous semantic segmentation of a
set of partially labeled images. In IEEE Winter Conference
on Applications of Computer Vision, 2016. 1
[33] A. Torralba. Modeling global scene factors in attention.
JOSA A, 20(7):1407–1418, 2003. 6
[34] Y. Wang, Y. Hu, S. Kambhampati, and B. Li. Inferring sentiment from web images with joint inference on visual and
social cues: A regulated matrix factorization approach. In
Proceedings of the Ninth International Conference on Web
and Social Media, ICWSM 2015, University of Oxford, Oxford, UK, May 26-29, 2015, pages 473–482, 2015. 1
[35] Y. Wang, S. Wang, J. Tang, H. Liu, and B. Li. Unsupervised
sentiment analysis for social media images. In Proceedings
of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July
25-31, 2015, pages 2378–2379, 2015. 1
[36] Z. Wang and B. Li. A two-stage approach to saliency detection in images. In Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on,
pages 965–968. IEEE, 2008. 1
[37] L. Zhaoping and P. Dayan. Pre-attentive visual selection.
Neural Networks, 19(9):1437–1439, 2006. 2
[38] D. Zhou, J. He, K. S. Candan, and H. Davulcu. MUVIR:
multi-view rare category detection. In Proceedings of the
Twenty-Fourth International Joint Conference on Artificial
Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 2531, 2015, pages 4098–4104, 2015. 1

Instructive Video Retrieval Based on Hybrid Ranking and
Attribute Learning
A Case Study on Surgical Skill Training
Lin Chen, Peng Zhang, Baoxin Li
Computer Science and Engineering
Arizona State University, Tempe, AZ, 85281

{lchen109, pzhang41, baoxin.li}@asu.edu
ABSTRACT
Video-based systems have been increasingly used in various training tasks in applications like sports, dancing, and
surgery. One key task to add automation to such systems is
to automatically select reference videos for a given training
video of a trainee. In this paper, we formulate a new problem
of instructive video retrieval and propose a solution using
both attribute learning and learning to rank. The method
first evaluates a user’s skill attributes by relative attribute
learning. Then, the most critical skill attribute in need of
improvement is selected and reported to the user. Finally, a
hybrid ranking learning to rank method is employed to retrieve instructive videos from a dataset, which serve as reference for the user. Two main technical problems are solved in
this method. First, we combine both skill and visual feature
to characterize skill superiority and context similarity. Second, we propose a hybrid ranking approach that works with
both pair-wise and point-wise labels of the data. The benefit of the proposed method over other heuristic methods is
demonstrated by both objective and subjective experiments,
using surgical training videos as a case study.

Categories and Subject Descriptors
I.4 [Image Processing and Computer Vision]: Application

Keywords
Video Retrieval; Attribute Learning; Learning to Rank

1.

INTRODUCTION

Video-based training systems have found many applications in sports, dancing, and surgery, etc., where cameras are
used to record and/or monitor performance of the trainees.
Wherever expert supervision is scarce/expensive, a system
that can provide automatic feedback based on a trainee’s
performance would be desired for a trainee’s self-improvement.
One important type of feedback is to provide verbal instructions based on analysis of the trainee’s performance and ilPermission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
MM’14, November 03–07, 2014, Orlando, FL, USA.
Copyright 2014 ACM 978-1-4503-3063-3/14/11 ...$15.00.
http://dx.doi.org/10.1145/2647868.2655050.

lustrative videos of other higher-skilled performers. We term
this as instructive video retrieval.
Instructive video retrieval is a new problem that is different from traditional video retrieval tasks such as nearlyduplicated video retrieval and concept retrieval.In this new
problem, we need to focus on whether the retrieved video
is helpful for skill improvement. To our knowledge, this is
a new research effort with little prior art. One challenge of
such task lies in the inherent vagueness and subjectivity of
the term “instructive”, which makes it difficult to formulate
a retrieval cost function. In this work, the instructiveness of
a video is defined by considering three criteria: specificity,
superiority and similarity. Specificity requires that the illustrative video should be selected w.r.t. a trainee’s skill
weakness and a corresponding verbal instruction would be
provided telling why this video is recommended. Superiority
refers to that the illustrative video should represent a better skill on the weakness aspect (attribute) of the trainee.
Similarity means that the illustrative video should have a
similar operation context (e.g., performing similar actions)
to the trainee’s video so that it is easy for the trainee to
figure out how to imitate and improve skills.
Considering these criteria, in this paper, we design an instructive video retrieval method based on attribute learning
and learning to rank, using surgical training as a case study.
Our hybrid ranking approach combines both pair-wise (relative) and point-wise (binary) data, making it flexible in handling variable levels of availability of labeled training data
in a practical application. The major contribution of this
paper is three-fold. First, we formulate the new problem of
instructive video retrieval and propose a learning-based solution. Second, in the chosen case study, both skill attributes
and operation context features are designed to implement
the general approach for delivering good performance. Last,
we present a hybrid ranking SVM to take advantage of both
pair-wise and point-wise labels for more effective ranking
under sparse and noisy labels.

2. RELATED WORK
Human skill assessment is a research topic with a long
history originated from psychology where skill is defined as
the relation between task difficulties and the resources paid
for and quality gained from the task[3]. There are generally
two ways for automatic skill evaluation. One is to build a
model for different skill levels, and use the model distance
from the user’s performance for skill assessment, e.g., [2].
This method provides an overall skill assessment but it is not

1045

specific enough to tell trainees how to improve. The other
is to provide performance metrics based on motion feature
statistics, e.g., [1]. But these mere metric are difficult to
use for coaching. Our attribute learning method adopted
skill attributes from domain knowledge[2]. Therefore, it can
not only evaluate different skill aspects, but also generate
instructions understandable to trainees. Moreover, we can
retrieve an illustrative video via attribute analysis, which is
one big step further than only skill evaluation.
Attribute learning maps low-level image feature to intermediate visual attributes, e.g. fur, color, and stripe, rather
than high-level category labels, e.g. zebra. The introduction
of semantic attributes greatly improves human-computerinteraction capability and boosts many learning applications
with human involved [6]. Our work is related to multiattribute image retrieval, since we will consider all skill attributes for instructive video selection. The multi-attribute
fusion function can either be designed from heuristics, e.g.,
L1 norm of matching score on each attribute[8], or be learned
from model assumption, and the model can be either generative graphical model, e.g., Bayesian network[9], or discriminative function, e.g., SVM[10]. We formulate our problem
in a discriminative function since generative models requires
more training samples while heuristic fusion is difficult due
to the vagueness of instructiveness.
Learning to rank [4] is different from traditional heuristic method where ranking metrics is learned from labels.
There are generally three types of labels for LR problem:
point-wise, pair-wise and list-wise. However, there are new
challenges in the instruction video retrieval problem. First,
due to the high cost of labeling real training videos by expert surgeons, the training labels are typically very sparse.
Second, the labels may be noisy because of the vagueness
in instructiveness concept and the subjective nature of the
problem. To this end, we present a hybrid-ranking SVM
method to take advantage of both point-wise and pair-wise
labels. There are other hybrid methods. For example, [5]
uses isotonic regression to optimize pair-wise margin among
samples of different relevance levels where point-wise relevance constraint can be further incorporated. In [7], pairwise margin between different relevance levels are formulated
in list-wise NDCG cost function and the function can be
solved by unconstraint optimization. These methods only
use point-wise label and their hybrid only appears in cost
function. In contrast, our method makes use of both pointwise and pair-wise labels and we can thus even differentiate
pairs on the same relevance level.

3.

Figure 1: Illustrating the FLS system.
Table 1: Primitive Actions of Peg Transfer
Name
Description
Lift
Move grasper to a peg with object and raise
the object off
Transfer Move the two graspers together and pass the
object from one grasper to another
Place
Move grasper to an empty peg and release
the object
ish one peg transfer cycle. For six objects being transferred
from left to right and backward, there are totally 12 cycles
in one training session.
Surgical skill can be evaluated by a set of attributes. Following the popular Global Rating Index for Technical Skills[2],
we adopt several important skill attributes most relevant
to the peg transfer task for skill evaluation and instructive
video retrieval. These attributes are listed in Tab. 2.

4. METHODOLOGY
Given a clip of trainee’s operation, our task is retrieving and recommending an instructive video clip for skill improvement. We propose to solve this task in three steps as
elaborated below.

4.1 Critical Skill Attributes Selection
To retrieve instructive videos for a trainee’s skill improvement, we must first figure out the operation weakness. A
lowest attribute value does not always mean the most urgent attribute in need of improvement. The weakest attribute should be the one that the trainee did poorly while
most other people are significantly better.
We first perform normalization across different attributes
so that the values of the skill attributes would reflect their
respective urgency to be improved, shown in Eq (1):
a′k,i = Pk (a≥ak,i + Sk ), (1≤k≤K)

(1)

where ak,i is the attribute value of the k-th skill attribute
Ak (1≤k≤K) for the i-th video Vi . Pk () is the probability
distribution of attribute Ak on a pre-defined video database
and the Sk is the significance threshold of Ak . In fact, Eq
(1) is an “inverse” normalization into attribute importance,
because higher skill attribute value represents less urgent
attribute to improve and will leads to lower normalization
value. The success of (1) lies in a well-constructed database
with video clips from people of different skill levels.

SURGICAL TRAINING TASK

The Fundamentals of Laparoscopic Surgery (FLS) training box has been widely used in minimally-invasive surgery
training. The system is a box simulating the human body
and a trainee uses tools going into the box through two small
holes to perform surgical actions. The trainees (Fig. 1 Left)
can only watch the inside of the box through a monitor,
which is captured by an on-board camera (Fig. 1 Right).
In this training procedure, a trainee is required to lift one
of the six objects with a grasper in one hand, transfer the object to another hand, and then place it on a peg on the other
side of the board. Once all six objects have been transferred,
the process is reversed from one side to the other. The peg
transfer operation consists of 3 primitive actions in Tab. 1.
Ideally, we should perform each primitive action once to fin-

Table 2: Skill Attributes for Surgical Skill
Attribute
Define
Time and motion
Unnecessary moves and time cost
Flow of operation
Moves smoothly without stop
Bimanual dexterity Cooperation between hands
Instrument handling Tentative operations and errors

1046

The second processing step is attribute comparison. Given
a user’s performance video Vi , and a retrieved video Vj , we
measure how much Vj can help Vi on the attribute Ak . This
is called the utility of Vj to Vi on Ak , and is measured by the
difference between the normalized attribute values in (1) as
uk,i,j = (a′k,j − a′k,i )/(a′k,i ), (1≤k≤K)

Table 3: The ranking result of pair-wised, pointwised and hybrid ranking SVM in simulated data.
Noise
Cosine
Kendall
Pair Point Hybrid Pair Point Hybrid
1%
0.39
0.47
0.49
0.25
0.32
0.33
10% 0.28
0.40
0.42
0.18
0.26
0.28
30% 0.15
0.19
0.24
0.09
0.12
0.15

(2)

4.2 Skill and Operation Context Features
Let f q,d be the ranking feature derived from the pair of a
trainee’s video Vq and a reference video Vd . fq,d should consist of both skill context and operation context to embody
the specificity, superiority, and similarity.
Specificity is determined by the normalized attribute importance in (1). Superiority principle can be described by
the difference between the normalized attribute value of Vq
and Vi in (2). So we define the skill context feature as:
(s)

f q,d = [aq , ad , uq,d ]

Table 4: The classification and ranking accuracy (%)
on surgical video data of three primitive actions.
Lift
Transfer
Drop
Point/Hybrid 78.1/80.7
83.7/84.4
83.4/88.4
Pair/Hybrid 75.7/76.3
90.0/89.8
89.3/90.4
take advantage of both point-wise and pair-wise labels. Pairwise ranking SVM, also called ranking SVM[6], tries to find
a projection vector w to satisfy the pair label under maximal margin assumption. Point-wise ranking SVM tries to
find a projection vector and a set of intersections related to
different relevance level in maximal margin spirit, e.g. CO
SVM[4]. The pair-wise and point-wise methods have their
advantages and disadvantages. The pair-wise label is very
expensive, but it can provide precise order between two samples. The point-wise label is less expensive, but it can’t rank
samples of the same relevance level. Since video labeling is
very expensive, the labeling can be very sparse. It is better
to combine the two label types for more effective ranking.
For example, we can include the pair-wise label between relevant samples to augment the point-wise label.
The direct combination the cost function and constraints
in [6] and [4] is not desirable, because it fix the margin thresh
of point-wise and pair-wise sample to equal value, i.e. 1.
Our idea is to introduce an extra variable K as the margin
thresh, and reformulate SVM as below:
Xm
Xm
1
kwk22 + C1
ε i + C2
γi
min
i=1
i=1
w,ε,w0 ,γ,K 2

(3)

where aq /ad are the normalized attributes value vector of
Vq /Vd as aq = [a′k,q ] and uq,d is the utility vector of Vd to
Vq as uq,d = [uk,q,d ], where k = 1, · · · , K.
Similarity principle means Vd should have a similar operation context as Vq , e.g., using the same hand and with similar
surroundings. To measure this, first we divide the operation
context of an image I into blocks Bn , n = 1, . . . , N . Each
Bn is described by a block context vector bn = [c1 , . . . , cM ]
where cm is the area portion of the m-th region category,
e.g., object and peg, in block Bn . Since FLS box is a controlled environment with significant color difference among
region categories, the area portion cm can be obtained by
color segmentation and tracking. Second, we describe the
similarity between a frame I in Vq and its peer frame I ′ in
Vd based on block similarity. Each block may have different importance for the similarity by its relative position to
the grasper. To address this, we take the block containing
the grasper tip of I as the reference center, and order all
blocks by the distance and angle to the center. Suppose
Bn , n = 1, . . . , N are already ordered by the above rule, so
the first block B1 contains the reference center. The blocks
Bn′ of frame I ′ will follow the same order so that Bn′ and
Bn are of the same location. The block context vector bn
already conveys block context, since it is derived from object segmentation. So the block similarity can be directly
defined as the inner product of block context vectors, and
the similarity between I and I ′ is defined as:
s(I; I ′ ) = [hb1 , b′1 i, . . . , hbN , b′N i].

(1)

=

[s(I0 ; I0′ ), s(I1 ; I1′ )],

(1)

(4)

5. EXPERIMENTS
We first show the efficiency of hybrid-ranking SVM by
both simulation and real surgical video data. Then we show
the effectiveness of our instructive video retrieval system.
For each of the three primitive actions, our surgical video
dataset consist of 240 video clips on different skill levels, i.e.
from novice to expert. So there are totally 720 video clips.

(5)

where I0 /I1 (I0′ /I1′ ) is the start/end frame of Vq (Vd ).
The final ranking feature of video pair (Vq , Vd ) is:
(s)

(v)

fq,d = [fq,d , fq,i ]

(7)

where w is the ranking model; yi = 1(−1) means xi is su(2)
perior (inferior) to xi ; zi = 1(−1) means xi is instructive/noninstructive; ε and γ are slack variables. This problem can
be solved by quadratic programming.

Then the operation context feature of video pair (Vq , Vd ) can
be represented as
(v)
f q,i

(2)

s.t. yi  hw, xi − xi i≥1 − εi
zi  (hw, xi i + w0 )≥K − γi
γi ≥0, εi ≥0.

(6)

5.1 Simulation Experiment

which will guarantee specificity, superiority, and similarity
principles for the following learning to rank.

We first compared our Hybrid-based SVM approach (7)
with two baseline approaches of pair-wise method[6] and
point-wise method[4]. The simulated dataset are uniformly
generated with 150 point-wise samples and 150 pairs-wise
samples, whose labels are decided by a randomly generated
projection vector w. Additionally, noise is added into the la-

4.3 Hybrid Ranking SVM
With the selected attribute and extracted ranking features, we now propose to employ hybrid ranking SVM to

1047

bel for robustness test. Tab. 3 shows the experiment result
of the three approaches when noise are added by 1%, 10%
and 30%. Two measures are adopted to compare the performance. The first measure is the direction cosine similarity
between the projection vector of trained model and the originally generated one; the other measure is the Kendall rank
correlation coefficient of the projection result. Result in Tab.
3 shows that the point-wise label is much more efficient than
pair-wised label in learning the true projection angle. However, the hybrid-ranking SVM can still improve the ranking
accuracy by combining both point-wise and pair-wise labels.
And the improvement is more significant with the increase
of noise level.

Table 5: Subjective evaluation results (%) of instructive video selection.
Lift
Transfer
Drop
Instructive 98.1/74.8
98.3/83.1
99.2/73.4
Superiority 61.1/21.3/17.6 73.2/15.5/11.3 68.1/16.1/15.8

6. CONCLUSIONS
We formulated a new problem of instructive video retrieval
and developed a solution in the case study of surgical training video. In defining the problem based on three criteria,
we allow the otherwise abstract and subjective task to be attacked by relative attribute learning. To facilitate a realistic
solution, considering the poor availability of labeled data, we
proposed a new hybrid ranking method to approximate instructive score from both attribute skill and visual operation
context features which are designed to embody the three criterions, taking advantage of both point-wise and pair-wise
labels. Stimulation and real data experiments demonstrated
the approach is effective and thus can be a promising solution to the instructive video retrieval problem.

5.2 Surgical Data Classification and Ranking
The training data were labeled in the following procedure. Given a random action clip as query, we randomly
pick another two clips from the clips with significant higher
attribute values. Then the expert surgeon would evaluate if
the returned clips is instructive or not instructive. If both
of the two clips are instructive, the expert would further
provide a pair-wise label indicating which clip is more instructive. The labeling process follows the three criterions
discussed above. For each of the 3 primitive actions, we
generated 30 queries and 8 pairs of clips for each query.
Similarly, we illustrate the benefit of hybrid ranking SVM
by comparison with both point-wise and pair-wise SVM. We
train hybrid ranking SVM with both the point and pair labels acquired in the above process, while the point-wise and
pair-wise SVM are trained with their corresponding labels.
Tab. 4 shows the classification and the ranking accuracies of
the hybrid-ranking SVM based on the combination of both
point and pairwise labels, compared with the purely pointwise and pair-wise based approaches. Each entry shows the
accuracy in lift/transfer/drop action. The results show that
our hybrid approach provides better accuracy than the two
baseline methods.

7. ACKNOWLEDGMENTS
The work was supported in part by National Science Foundation (NSF) (Grant 0904778). The views expressed in this
material are those of the author(s) and do not necessarily
reflect the views of NSF.

8. REFERENCES
[1] D. S. Alexiadis, P. Kelly, P. Daras, N. E. O’Connor,
T. Boubekeur, and M. B. Moussa. Evaluating a
dancer’s performance using kinect-based skeleton
tracking. In ACM MM, 2011.
[2] J. D. Doyle, E. M. Webber, and R. S. Sidhu. A
universal global rating scale for the evaluation of
technical skills in the operating room. The American
Journal of Surgery, 2007.
[3] P. M. Fitts and J. R. Peterson. Information Capacity
of Discrete Motor Responses. J Exp Psychol, Feb.
1964.
[4] T.-Y. Liu. Learning to rank for information retrieval.
Found. Trends Inf. Retr., Mar. 2009.
[5] T. Moon, A. Smola, Y. Chang, and Z. Zheng.
Intervalrank: Isotonic regression with listwise and
pairwise constraints. In WSDM, 2010.
[6] D. Parikh and K. Grauman. Relative attributes. In
ICCV, Nov 2011.
[7] C. Renjifo and C. Carmen. The discounted cumulative
margin penalty: Rank-learning with a list-wise loss
and pair-wise margins. In MLSP, Sept 2012.
[8] W. Scheirer, N. Kumar, P. N. Belhumeur, and T. E.
Boult. Multi-attribute spaces: Calibration for
attribute fusion and similarity search. In CVPR, June
2012.
[9] W. Scheirer, N. Kumar, K. Ricanek, P. Belhumeur,
and T. Boult. Fusing with context: A bayesian
approach to combining descriptive attributes. In
IJCB, Oct 2011.
[10] B. Siddiquie, R. Feris, and L. Davis. Image ranking
and retrieval based on multi-attribute queries. In
CVPR, 2011.

5.3 Instructive Video Selection
Finally we compare our video with a baseline method that
randomly selects one expert video clip of the primitive action. The subjective comparison protocol is as follows. We
recruited 6 subjects who had no prior knowledge on the
dataset. For each subject, 10 clips for each primitive action are given as query. For each query, both our hybrid
ranking SVM and the baseline method will return an instructive clip. So there are totally 60 queries for each of
the 3 primitive actions. For each returned video pairs, the
subjects need to evaluate whether the video is instructive
in improving the operation in the query video, and which
one is more instructive. The comparative result in Tab. 5
shows that our hybrid ranking SVM performs much better than the baseline method in selecting instructive videos.
The first row shows the percentage of instructive coaching
videos returned by our hybrid/baseline approaches. The second row shows the percentage that the returned video of
our approach is more/equal/less instructive than the baseline approach. Result shows that, the returned videos by our
approach are almost all instructive, apparently higher than
the baseline approach. Even in the case that both returned
videos from two approaches are instructive, our approach
would recommend more instructive videos than the baseline
approach by large majority.

1048

ROBUST GROUND PLANE DETECTION WITH NORMALIZED HOMOGRAPHY IN
MONOCULAR SEQUENCES FROM A ROBOT PLATFORM
Jin Zhou and Baoxin Li

Dept. of Computer Science & Engineering
Arizona State University, Tempe, AZ 85287, USA

Abstract

are more popular and attractive since they are typically
more cost-effective and also there is no stringent
requirement on camera calibration (as in stereoscopic
approaches). For example, in [8], optical flow is used in
computing the surface normal for different image
patches, which are then grouped to detect the ground
floor. The optical-flow-based approaches tend to be
computationally costly and not robust to unpredictable
motion of a mobile platform.
In this paper, aiming at designing a practical system to
detect the ground plane with monocular vision, we
propose a method that makes use of a sparse set of
features detected from a corner detector. The motion of
these features is modeled by a homography
transformation and in turn used in detecting the ground
plane. Further, we utilize the constraints arising from the
target application to greatly simplify the homography
model, resulting in more efficient and robust algorithms.
Moreover, the inherent "virtual plane problem" is solved
naturally. Consequently, compared with prior work on
the same task using plane homography (such as [5]), the
proposed method performs better in finding the real
ground plane. Also the proposed method is not restricted
to translational motion as in [11].

We present a homography-based approach to detect the
ground plane from monocular sequences captured by a
robot platform. By assuming that the camera is fixed on
the robot platform and can at most rotate horizontally,
we derive the constraints that the homograph of the
ground plane must satisfy and then use these constraints
to design algorithms for detecting the ground plane. Due
to the reduced degree of freedom, the resultant algorithm
is not only more efficient and robust, but also able to
avoid false detection due to virtual planes. We present
experiments with real data from a robot platform to
validate the proposed approaches.

Index Terms - Vision-based navigation, homography
1. INTRODUCTION
In mobile robot navigation, one of the fundamental
problems is obstacle avoidance. Among various
possibilities, vision-based obstacle avoidance has always
been one attractive option. This is even more so
nowadays with the availability of low-cost imaging
sensors and compact yet high-performance processors as
a result of recent technology development. There are
different vision-based approaches to obstacle avoidance.
In this paper, we focus on the problem of ground plane
detection, which entails naturally obstacle detection,
since in many applications the mobile robot may be
considered to maneuver on a planar surface and thus
obstacle detection may be reduced to the problem of
ground plane detection: With the ground plane detected,
other objects can be viewed as obstacles if they are on
the direction of movement and outside of the ground
plane.
Various approaches have been proposed to address the
problem of ground detection. For example, simple
approaches identify the ground floor using color
information [2,4,7]. While being simple to implement,
these approaches are suitable only for very specific
environments. To handle general environments, a few
systems attempt to recover the structure of the scene. To
this end, different techniques have been used. For
example, the work in [3,6] uses stereo vision for this
purpose. Presently, monocular vision based approaches

1-4244-0481-9/06/$20.00 C2006 IEEE

2. HOMOGRAPHY-BASED GROUND PLANE
DETECTION
Our task is to detect the ground plane from a monocular
sequence captured by a camera mounted on a robot
platform navigating on a planar surface. In theory, in the
monocular sequence, points on the same plane share a
homography transformation in two views. That is, for a
set of point correspondences {xi <-* xi '} in two images,
if all of the points are coplanar, then there is a
homography matrix H such that
(1)
Xi =Hx1
where x represents a homogeneous image coordinate
(x, y,w)T , and H is a 3 by 3 matrix. Since x is

represented by homogeneous coordinates, Eqn. (1) is
true up to an unspecified scale factor. Thus H has only
eight degrees of freedom. To determine such an H, four
non-degenerated point correspondences are required
since each point correspondence provides two
independent constraints, although in practice typically

3017

ICIP 2006

more points are used to improve the accuracy.
Apparently, different planes have different
homographies, and thus theoretically if we find a
homograhy that includes at least three points on the
ground, it corresponds to the ground plane. Thus Eqn.
(1) suggests a way for detecting the ground plane
through grouping the detected feature points into
coplanar sets, with each set sharing a common
homography. If we assume that the ground plane
contains the most feature points, then we can detect it by
searching for a dominant homography that accounts for
the most feature points in two views. We can then use
this homography to determine if any other feature point
is on the plane and thus achieving the detection of the
ground plane.

Fig 1. Illustration of the coordinate systems in realation to
the ground plane.

We define the world coordinates system such that the
y-axis is perpendicular to ground plane and the origin is
at the same height as the camera, as illustrated in Figure
1. Based on the world coordinates system, we can
have Ci = (x0, zi )T . The ground plane has coordinates
go = (noT,d)T so that for points on the plane we have
noT0X + d = 0, where n = (0,1, O)T .
From [9], when P= K[II0] , P'= K'[R t] , this
plane correspond to a homography given by
H=K'(R-tnTld)K-1
(3)
We can adjust (rotate and translate) the world
coordinates system so that it is the same as that of the
first camera. Then the new camera matrices become:
P1= K[I ], P2 '= K[R2R1 1 I-R2AC]
(4)
where AC=(C2-Cl) and n' = Rln . Thus
H = K(R2RI- + R2ACnRjT Id)
(5)
Since R-1 = RT, we have
H =KR2 (I+ACn" I d)R1-1 K-1
(6)
For a mobile platform moving on the plane, if the
camera is fixed on the robot, we can have R2 = R1AR
where AR indicates the rotation around y-axis. More
specifically, if the robot rotate 0 degree on the plane,
then
cos(O) 0 sin(O)
AR =Ry(0)
1
0
0
(7)
- sin(O)
0 cos(O)
From (6), we can compute the normalized
homography H from any H:
H = (KR1)- H(KR1) = AR(I+CnT I d) (8)
For the ground plane, since n0 = (0,1, O)T , with
AC = (x0, 0, z0 )T, fH has the following form:
L cos(6) xo / d sin(S)
1
0
Hi= 0
(9)
- sin(0) z0od cos(0)j
Eqn. (9) shows that the normalized homography of the
ground plane has just 3 degree of freedom, namely, 0,
However, in order to compute
xo / d and z0 / d
normalized homography, we still need to know K and

2.1. The Virtual Plane Problem
Even when the assumption that the ground plane
contains the most points is satisfied, there still exists
potential problem in the above procedure, which we
name as the virtual plane problem. That is, we may find
a virtual plane, which may contain some ground plane
points as well as some other obstacle points. Such a
plane may contain more feature points than the actual
ground plane does, although this plane does not
correspond to a physical planar object (and hence the
term "virtual"). This problem can easily occur since the
number of feature points from an automatic feature
detector is agnostic to the scene objects and thus where
the feature points may be is unpredictable. In this paper,
we exploit additional constraints arising from the target
application to limit the search space for finding the
dominant homography. The resultant approach solves
the virtual plane problem naturally while lending itself to
a more efficient and robust search algorithm, as detailed
in subsequent sections.
3. HOMOGRAPHY FOR THE GROUND PLANE
Different planes have different homographies between
two given views. For robot navigation on the ground
plane, if the camera mounted on a mobile robot is fixed,
there is some special pattern for the homography of the
ground plane (we assume that the robot is moving via
wheels just like a car). We derive this special pattern
analytically in the following.
In general, we can set the camera matrices of two
different views as:
P1 = K1R [II -C1]

P2 =K2R2[I I-C2]

(2)

where Pi is 3 x 4 camera matrix, K. represents the internal
camera matrix, Ri the camera rotation matrix, and Ci
the camera center coordinates ([9]). For a single camera,
we can set K1 =K2 =K.

.

3018

R,. While K may be obtained by calibration, there is no

H'= K 'H [p,

straightforward way for determining R, . Moreover, if
we choose different world coordinates system, we will
have different R, . Here, we prove that if we keep the yaxis of the world coordinate system unchanged (i.e.
always perpendicular to the ground plane), different
values of R, do not change the form of (9).
Proof: Since the y-axis is unchanged, the world
coordinate system can only rotate around the y-axis.
Suppose RI and RI' represent rotation matrix in
different world coordinates system respectively, then we
have RI'= R,AR,, where AR, is rotation around y-axis

RI=[r,

=Ry((p)

(10)

Then we have
H '(KR,')- H(KR ') = AR,- HAR,
=

with AC'= AR,-'AC
Since AC = (x0,0, z0o), with (10), we have
AC'= (Xo 7 0, Z )T
= AR(I + AC'n T / d),

Therefore, the new normalized homography based on
RI' still has the form as (9). Consequently, the above
observation provides us with the freedom to choose R1 .
In the next section, we will present a simple technique to
determine RI as well as techniques to search the ground
plane based on the normalized homography.
It is interesting to note that Eqn. (9) also provides the
rotation and displacement of the two underlying views.
This information can be useful for a robot.

BASED

rl x r3

r3]

With K and RI determined, we can compute the
normalized homography from the original homography
by Eqn. (8). The ideal normalized homography of the
ground plane has the form of Eqn. (9) which has just 3
degree of freedom. Thus searching for the ground plane
can be formulated as searching for a dominant
normalized homography that has the form of Eqn. (9).
To compute the normalized homography, we first
normalize the coordinates of the points: for a point x, its
normalized coordinates are given by x = (KR1)-' x . Then
the homography computed from the normalized points is
the normalized homography since:
x' Hx = (KR,))H(KR,) 'x

(11)

= AR(AR1-V1AR1+ AR1-'ACnTAR1I d)

4.
GROUND
DETECTION
NORMALIZED HOMOGRAPHY

r2

=

4.2 Ground Plane Detection

AR1-'AR(I + ACnT I d)AR1

= ARAR-1 (I + ACnT I d)AR1

{tj p i
r3 p21/ p2||'

The following analysis explains why this approach
works. For convenience, we set the origin of world
coordinate system on the ground plane (Translation of
the world coordinates system does not change RI ). Then
for a ground plane point X = (u, 0, v, 1)T its image point
is
x=K[R1 t]X = K[rl, r2 , r3, t](U,0, V, 1)T
=K[rl, r3, t](U, V, 1)T = K[rl, r3 , t]X
Therefore, H = K[rl, r3,t] .
Note that RI needs to be computed only once as long as
the camera is fixed on the robot platform or can only
rotate around the y-axis.

cos(q)
AR

11p,

I

r2

so

0 sin(q)
1
0
0
sin(q) 0 cos(q)j

P2 P3],

(KR,)x' =H(KR,)- 'x
= Ai
x'i

After we obtain a normalized homography, we try to
fit it to the normalized homography model for the
ground, given in the form of Eqn. (9). To search for a
dominant model, we can use the RANSAC scheme.
Based on the nature of our problem, we modify the basic
RANSAC scheme to obtain the following algorithm.
Algorithm: Loop for N feature points.
a) Randomly select a point, get its four closest
neighbors that are not collinear; Use these 5 points to
compute the normalized homography H.
b) Fit H to Eqn. (9). If it fails to fit, go to (a).
Otherwise, use this model to find more inliers and
recompute H and fit again to find more inliers until the
number of inliers do not increase.
At the termination of the procedure, the model with the
most inliers is declared as the ground plane.

ON

4.1 Determination of RI
We design a very simple approach to determine R1 as
follows:
1) Take an image with the camera.
2) For several (. 4 ) ground points, manually pick their
corresponding points on the image.
3) Set the 2d coordinates of the ground plane points {x }.
With their image coordinates {i } , compute the
homography H such that xi Hx,
4) Estimate R, from H:

3019

5. EXPERIMENTS

environments," Proc. IEEE/RSJ Int. Conf: Intel. Robot &
Systems, France, vol. 1, pp. 373-379, Sept. 1997.
[5] N. Pears and B. Liang, "Ground plane segmentation for
mobile robot visual navigation," In IROS 2001, vol. 3, pp.
1513-1518, 2001.
[6] R. Mandelbaum, L. McDowell, L. Bogoni, B. Beich, and M.
Hansen, "Real-time stereo processing, obstacle detection, and
terrain estimation from vehicle-mounted stereo cameras," In
Proc. 4th IEEE Workshop on Applications of Computer Vision,
Princeton NJ, pp. 288-289, 1998.
[7] S. Lenser and M. Veloso, "Visual sonar fast obstacle
avoidance using monocular vision," In Proc. of the IEEE/RSJ
IROS 2003.
[8] Y. Kim and H. Kim, "Layered ground floor detection for
vision-based mobile robot navigation," Proc. IEEE Int. Conf:
on Robotics and Automation, vol. 1, pp. 13-18, 2004.
[9] R. Hartley and A. Zisserman. Multiple view geometry in
computer vision. Cambridge University, Cambridge, 2nd
edition, 2003.
[10] J. Zhou and B. Li, "Homography-based Ground Detection
for A Mobile Robot Platform Using a Single Camera" In
Proc. ICRA 2006.
[11] B. Liang, N. Pears and Z. Chen, "Affine Height
Landscapes for Monocular Mobile Robot Obstacle
Avoidance", In Proc. Intelligent Autonomous Systems, pp.
863-872, August 2004.

The experiments are based on the robot platform of
Fig. 2(a). We use a paper with checkerboard patterns to
measure R1, as shown in Fig. 2(b). We manually pick 7
grid points on the image. With these 7 image points and
their corresponding coordinates on paper. We compute
the homography and further determine R1 with methods
introduced in Section 4.
With K and R1 known, we can apply our approach to
the image sequences taken by the robot. Fig. 3 illustrates
sample results, where we also compare with the
approach of searching for the dominant homography
directly without using the constraints introduced in this
paper. Harris corner detector [1] was used in the
experiments in feature detection (green crosses in the
images). Feature correspondence was done based on the
method reported in our previous paper [reference omitted
for anonymity]. Fig 2 (d) is the best results of the direct
search method, which include more non-ground points.

(a)

(J)

Fig 2. (a) The robot platform used in the experiments. (b) The
set-up for computing the rotation matrix of the camera on the
robot.

6. CONCLUSION
In this paper, we proposed a new method to detect the
ground plane from monocular sequences captured by a
robot platform. The core idea is to use the fixed cameraground configuration to impose a strong constraint in
searching the dominant homography. We designed
complete algorithms and tested with real data from a
robot platform. The results demonstrate the advantages
of the proposed method.

References
[1] C. J. Harris and M. Stephens, "A combined corner and edge
detector," In 4th Alvey Vision Conference Manchester, pp 147151, 1988.
[2] J. Hoffmann, M. Jungel and M. Lotzsch, "Vision Based
System for Goal-Directed Obstacle Avoidance",8th Int.
Workshop on RoboCup 2004.
[3] K. Sabe, M. Fukuchi, J.-S. Gutmann, T. Ohashi, K.
Kawamoto, and T. Yoshigahara, "Obstacle Avoidance and Path
Planning for Humanoid Robots using Stereo Vision," Proc. Int.
Conf: on Robotics and Automation (ICRA'04), New Orleans,
pp. 592 - 597, Vol.1 April 2004.
[4] L. M. Lorigo, R. A. Brooks, and W. E. L.Grimsou,
"Visually-guided obstacle avoidance in unstructured

Fig 3. (a)-(b) illustrate two views with feature points and their
correspondence (indicated by red/dark line segments). (c) is the
detected ground plane (green/light crosses) with the proposed
approach in this paper. (d)-(f) is the results by simply
searching for a dominant homography without using the
constraint utilized in the proposed approach. (e)-(f) are
situations with the virtual planes detected as the ground. In all
figures, red/dark crosses indicate non-ground feature points.

3020

Trending Chic: Analyzing the Influence of Social Media on Fashion Brands
Lydia Manikonda

Ragav Venkatesan

Subbarao Kambhampati

Baoxin Li

arXiv:1512.01174v2 [cs.SI] 8 Mar 2016

Department of Computer Science, Arizona State University, Tempe AZ 85281
{lmanikon, ragav.venkatesan, rao, baoxin.li}@asu.edu

Abstract
Social media platforms are popular venues for fashion brand
marketing and advertising. With the introduction of native advertising, users don’t have to endure banner ads that hold very
little saliency and are unattractive. Using images and subtle
text overlays, even in a world of ever-depreciating attention
span, brands can retain their audience and have a capacious
creative potential. While an assortment of marketing strategies are conjectured, the subtle distinctions between various
types of marketing strategies remain under-explored. This paper presents a qualitative analysis on the influence of social
media platforms on different behaviors of fashion brand marketing. We employ both linguistic and computer vision techniques while comparing and contrasting strategic idiosyncrasies. We also analyze brand audience retention and social
engagement hence providing suggestions in adapting advertising and marketing strategies over Twitter and Instagram.

1

Introduction

The impact of fashion in society has been a well-studied
topic even during the era of print and visual media based
advertising (Diana 2000; Kawamura 2005). The marketing
and advertising strategies involved in fashion are often qualitatively different from other product marketing and advertising. While in most products it is important to emphasize
the necessity or the quality of the product, fashion advertisements are tailor-made to match the tastes and sensibilities of
the target audience.
Social media is an an incredible tool at the fashion industry’s disposal for marketing. By leveraging social media, brands can take control of public perception which
is one among the many important factors in fashion marketing (Recklies 2006). The continuous feedback received
by the brands via the likes and comments on their social media posts lets them gauge and further viralize their
base in the market. There are several studies that focus on
understanding the growing interest in social media marketing (Dubois and Duquesne 1993; Kim and Ko 2012;
Kim and Ko 2010). The importance of fashion branding
on social media is becoming even more pronounced as networks like Instagram are revolutionizing this field. According to the well-analyzed editorials in The Guardian and
The New York Times, it is the social media that decides
what you wear and Instagram is titled as the fashion’s new
front row (Cartner-Morley 2015b; Cartner-Morley 2015a;
Friedman 2015). Existing literature (Hu, Manikonda, and

Kambhampati 2014) shows that Instagram alone has a significant share of posts that belong to fashion category.
In this paper, we consider the top-20 fashion brands and
investigate how they use Twitter and Instagram by observing their native profiles. We analyze their styles and strategies of advertisement. Although textual analysis is interesting, we predominantly focus on visual analysis owing to the
overwhelming number of images used by the brands in advertising. Using the neural network based deep image features similar to those extracted by Khosla et al. (Khosla,
Das Sarma, and Hamid 2014), we find out how the two types
of brand marketing strategies – direct marketing and indirect
marketing are used. Our analysis revealed that brands that
have a larger number of visibility tend to utilize the direct
marketing strategy.
The summary of our contributions is as follows:
• A characterization of how top-20 fashion brands use the
social media primarily to compare and contrast the posts
on Twitter and Instagram.
• Using deep features obtained from the visual content, an
investigation about two marketing strategies – direct marketing and indirect marketing.
We hope that the distinctions discovered in this paper
can inspire marketing researchers to study the reason behind these inferences. To the best of our knowledge, there is
no existing work on how fashion brands use different social
platforms in terms of characterizing their behavior through
analyzing textual and visual content. It is important to understand the distinctions and similarities so that the new businesses can adapt these ideas to promote their businesses and
establish their brands.

2

Analysis

The dataset used in this analysis comprises of top-20 fashion brands (in terms of the number of followers) on Instagram and Twitter (D = {b1 , b2 , ...b20 } in Table 1) according
to the survey conducted by Harper’s Bazaar (Bazaar 2015).
Harper’s Bazaar is a monthly fashion magazine that delivers a perspective into the world of fashion, beauty and popular culture and is considered as a good style resource for
women.

2.1

Group statistics

Since Twitter was founded in 2006 and Instagram in 2010,
we see in Figure 1 that most brands had their accounts cre-

Table 1: Top-20 brands used in this study
Nike – b1
Louis Vuitton (LV) – b3
Michael Kors (MK) – b5
Dior – b7
Gucci – b9
Burberry (Brb) – b11
Fendi – b13
Converse – b15
Free People (FP) – b17
Ralph Lauren (RL) – b19

Adidas Originals (AO) – b2
Dolce Gabbana (DG) – b4
Adidas – b6
Louboutin World (LW) – b8
Prada – b10
Vans – b12
Armani – b14
Jimmy Choo (JC) – b16
Calvin Klein (CK) – b18
Cartier – b20

ated on Twitter first. Figure 1 shows the timeline of when
the brands created their accounts and first posted a tweet or
photo on Twitter or Instagram respectively. Among all these
brands the first post was made by Vans in 2008 on Twitter
and by Michael Kors in 2011 on Instagram. Every month (on
an average) a minimum of 18 posts and a maximum of 124
posts on Instagram and a minimum of 20 posts and a maximum of 619 posts on Twitter were made by these brands.
The frequency of posts per month by each brand using
eq. 2 is computed along with the average number of likes,
average number of comments and the average number of
hashtags for all posts of brands 2. Suppose n(t,b)
and n(i,b)
j
j
are the number of posts made on Twitter and Instagram by
the brand b respectively in the time period j1 . p = {t, i} refer
to Twitter and Instagram respectively,
X
Mi(p) = 1{ m(p)
(1)
i }∀i,
is the vector that is an indicator function (1) for each time
period, indicating whether any posts at all were made during
that time period. It is 1 if any posts were made and 0 if not.
Then,
ωp = P

Pt

Popular Trends or topics: We use the LDA approach (Blei, Ng, and Jordan 2003) to understand how the
brands focus on different topics using the textual features. To
discover the topics, we use the Twitter LDA package (Zhao
et al. 2011). We consider the captions (for Instagram posts)
or tweet text (for Twitter posts) attached with all the posts of
a brand and mine the topics across all the brands on the both
the platforms. Using LDA, we found 10 topics across all the
brands on both the platforms. Table 2 presents the 10 words
associated with the discovered topics along with the brands
which used the topic ID in the same corresponding row on
both Twitter and Instagram.
Table 2: Topic IDs and their corresponding words
ID
0
1

,
(p)

(2)

Mi

is the average frequency of posts made by a brand on the
platform p.
From Figure 2, we notice that the brands Michael Kors
(MK) and Burberry (Brb) created their accounts at the same
time and have similar number of posts. But in terms of the
number of followers, MK has 24% more number of followers than Brb, follows twice the number of people followed by Brband gets 3 times as many likes and comments
than Brb. Brands like Free People (FP) created an Instagram
account during summer of 2011 and post pictures with a
very high frequency (124 pictures on average every month).
When we observe the number of likes and comments, they
are around 20k and 136 respectively which are neither high
nor low compared to other brands. These examples suggest
that the rate at which a brand makes posts on these platforms
do not have any effect on the visibility of posts.
Users can make posts on Twitter in two forms: (a) submissions that are uploaded to and hosted on the Twitter’s
server itself and (b) cross-shared posts where tweets posted
from another application appear on Twitter timeline. On an
1

average, 87% of tweets contain pictures of the products or
models showcasing the brand’s products.
For all the brands on the two platforms in a head-to-head
comparison on the number of posts, followers and friends,
we find some surprising insights through Figure 2. The key
observation is that the posts made on Instagram get very high
visibility in terms of the large number of likes compared to
Twitter. We associate this with two patterns we observed in
our analysis 1) People tend to like images more than text
posts and Instagram allows only texts 2) Instagram doesn’t
allow cross-sharing unless an image exists, while Twitter allows and followers may not be receptive of posts that involve
web urls. It is also a noticeable fact that most of these brands
have more number of followers on Instagram than on Twitter.

For convenience purpose, we use a monthly time period

2
3
4
5
6
7
8
9

Words
red, contact, make, pack, collection, team, hit, online, time,
stores
show, louis, fashion, vuitton, men’s, collection, gucci,
watch, opening, live
collection, bag, discover, show, shoeoftheday, style, botique, fashion, wearing, watch
show, live, personalized, moment, autumn/winter, runway,
wearing, collection, british, london
win, photo, pair, signed, metro, entered, sean, big, attitudes,
submission
armani, giorgio, wearing, show, fashion, celebs, summer,
emporio, collection, awards
rl, collection, regram, polo, vans, rad, photo, mix, fall, hope
styletip, conditions, merci, accessories, gnrales, live, peuxtu, participation, jetsetgo, timeless
fashion, blog, love, streetstyle, photo, today, fashionista,
happy, inspiration, fashionphotography
collection, show, spring, runway, fall, discover,fashion,
live, dress, backstage

Brands
Louboutin
Cartier
Louis Vuitton

World,

Gucci, Fendi
Burberry

Dolce Gabbana, Prada,
Armani
Ralph Lauren
Michael Kors, Adidas

Dior, Calvin Klein

We can notice that the runway luxury fashion brands like
Louis Vuitton, Dolce Gabbana, Burberry, etc., focus on the
same topics in Twitter and Instagram. Very few brands like
Nike, Adidas Originals, Vans, Converse and Free People focus on different topics on the two platforms. Nike being the
most popular brand on Instagram focuses mainly on topic 7
whereas on Twitter focuses on topic 0 whose words suggest
that Twitter might be used for correspondence or queries. We
find that brands like Burberry focus mainly on British style
and includes men’s collections and Michael Kors focuses on
the styles and accessories. The active brand according to the
number of friends – Louboutin World uses both the networks
to contact customers online.

Figure 1: A timeline showing the creation dates of accounts by the brands on Twitter and Instagram

Figure 2: Different statistics showing the brand behavior on Instagram and twitter

2.2

Visual Features

In analyzing the visual content on both the social networks,
we use the dataset of images collected from the brand accounts on Twitter and Instagram and we extract deep features for each image present in our dataset. In this article, we
use the overfeat networks’ image features (Sermanet et al.
2014) for two reasons. As argued by Khosla et al., overfeattype features are particularly capable of extracting representations that are well-suited for internet images and abstract
tasks. Overfeat is a stable implementation that makes use of
GPU (we used Tesla K40) in the efficient extraction of features for large scale image datasets.
We use the network’s 22nd layer representation for each
image as the feature vector corresponding to that image. We
then perform clustering (using k-means) on this space and
use those clusters to study the different marketing strategies
utilized by the brands and how it affects the visibility of their
products. We obtain k clusters for each brand on the two
platforms separately. These clusters represent the different
types of content categories present in the images for example – sunglasses, watches, floral patterns, etc. Figure 3 (a)
indicates that brands which post similar textual topics across
these platforms post different types of visual content.
We identified two distinct and common strategies that
brands use – direct product marketing (DM) and indirect
product marketing (IM). DM focuses on the product and
IM uses attributes that are not but related to the product for
marketing. For instance, a bag that is photographed by itself on a pedestal is DM, while a fashion model (person)
holding the bag and the bag being vignetted is an example

of IM. We often find both, while we find that IM is more
effective, particularly when used with celebrities. The same
can be observed in the following cluster analysis over the
said feature space. Figure 3 displays the brands b1 – Dolce
Gabbana, b2 – Gucci, b3 – Michael Kors along the cluster
types – C1 – Products, C2 – Runway/Redcarpet events, C3
– Portraits for both Instagram (top row) and Twitter (bottom
row). Brand b1 focuses on direct marketing on Instagram
but doesn’t make posts of category C1 where as it focuses
on indirect marketing w.r.t category C3 . Brand b2 follows
the similar trend as b1 . Whereas, brand b3 primarily focuses
on indirect marketing no matter what category it is. While
b1 and b2 have mean likes of 27245 and 25280 respectively,
brand b3 has 47941 likes on average for the said clusters.
Similar pattern is spread across many other similar brands,
strongly favoring indirect marketing and aligning with the
past research (Bakhshi, Shamma, and Gilbert 2014).
Among all the brands, Nike has the largest number of followers and the number of likes received for a post. Adidas
focuses on the same topics as Nike giving us a good case
study. Each row in Figure 3 corresponds to a cluster category where Nike and Adidas both post similar kinds of photos except that Nike and Adidas has a unique cluster focusing on the tank tops and track jackets respectively. Both the
brands focus on direct and indirect marketing in very similar
patterns. Direct marketing for shoes and indirect marketing
for equipment and attire. We notice that Nike and Adidas acquire similar number of likes for most similar categories. On
the idiosyncratic categories we find that Nike posting tank
tops get significantly larger number of likes than the track-

(a)

(b)

(c)

Figure 3: a) Michael Kors vs Gucci vs D& G, b) Nike vs Adidas, c) Armani vs Prada

suits of Adidas. Nike also gets significantly more likes due
to the presence of their Tennis celebrities Rafael Nadal and
Serena Williams in Instagram and this is the major cause of
Nike having more likes and followers than Adidas.
We extend similar analysis to two runway brands Prada
and Armani, which focus on similar topics. Figure 3 shows
that even if both the brands has some common clusters, there
are distinctive cluster categories. Prada posts often contain
floral patterns with no architecture and not much focused on
products. Where as, Armani has photos with text, photos that
focus on indoor architecture and photos of products. Prada
having developed significant following for is floral pattern
earns four times as many likes and comments for that cluster as compared to the comparable cluster in Armani. While
the men in tuxes in both brands earn similar number of likes
and comments, the architecture cluster of Armani gets significantly less number of likes and comments.
Following the above discussed trends, we find time and
again that posts made by brands practicing IM strategies
have more visibility. We hope that these explorations could
draw the attention of market researchers that IM leads to
more visibility in terms of obtaining more likes and comments for posts on Instagram.

3

Conclusions

Our work employs linguistic and visual analyses on the posts
made by top-20 fashion brands on Twitter and Instagram.
We investigate how brands focus on different topics on different social media and how certain types of visual cues associated with marketing strategies can obtain more visibility. Textual analysis revealed that in spite of the number of
hashtags a post contains or how frequently a brand makes
posts online, do not contribute to visibility. Visual analyses
show that even if the textual topics are same on both the platforms, brands adapt different posting styles w.r.t visual content. However, it was evident from the analysis that brands
exercising indirect marketing are gaining more visibility in
terms of the number of likes and comments. Through this
research we hope to open up new discussions about the role
of visual content on social media in learning about fashion
trends and inspire marketing researchers to study the reasons
behind these findings.

References
[Bakhshi, Shamma, and Gilbert 2014] Bakhshi, S.; Shamma, D. A.;
and Gilbert, E. 2014. Faces engage us: photos with faces attract
more likes and comments on instagram. In Proc. CHI.

[Bazaar 2015] Bazaar, H. 2015. Instagram’s top 20 fashion brands
(bit.ly/1ooovmx). Technical report.
[Blei, Ng, and Jordan 2003] Blei, D. M.; Ng, A. Y.; and Jordan,
M. I. 2003. Latent dirichlet allocation. J. Mach. Learn. Res. 3.
[Cartner-Morley 2015a] Cartner-Morley, J. 2015a. How sociality
barbie proves that Instagram has turned into reality TV. Technical
report.
[Cartner-Morley 2015b] Cartner-Morley, J. 2015b. Instagram: welcome to fashion’s new front row. Technical report.
[Diana 2000] Diana, C. 2000. Fashion and its social agendas: Class,
gender, and identity in clothing. Chicago: Chicago Univ. Press.
[Dubois and Duquesne 1993] Dubois, B., and Duquesne, P. 1993.
The market of luxury goods: Income versus culture. European
Journal of Marketing 27(1):35 – 44.
[Friedman 2015] Friedman, V. 2015. Balmain, Chloe and the Instagram Imperative. Technical report.
[Hu, Manikonda, and Kambhampati 2014] Hu, Y.; Manikonda, L.;
and Kambhampati, S. 2014. What we instagram: A first analysis
of instagram photo content and user types. In Proc. ICWSM.
[Kawamura 2005] Kawamura, Y. 2005. Fashion-ology: an introduction to fashion studies. Berg.
[Khosla, Das Sarma, and Hamid 2014] Khosla, A.; Das Sarma, A.;
and Hamid, R. 2014. What makes an image popular? In Proc.
WWW.
[Kim and Ko 2010] Kim, A. J., and Ko, E. 2010. Impacts of luxury fashion brands social media marketing on customer relationship and purchase intention. Journal of Global Fashion Marketing
1(3):164–171.
[Kim and Ko 2012] Kim, J. A., and Ko, E. 2012. Do social media
marketing activities enhance customer equity? an empirical study
of luxury fashion brand. Journal of Business Research 65(10):1480
– 1486. Fashion Marketing and Consumption of Luxury Brands.
[Recklies 2006] Recklies, D. 2006. Understanding and managing
customer perception. University Press.
[Sermanet et al. 2014] Sermanet, P.; Eigen, D.; Zhang, X.; Mathieu,
M.; Fergus, R.; and LeCun, Y. 2014. Overfeat: Integrated recognition, localization and detection using convolutional networks. In
Proc. ICLR.
[Zhao et al. 2011] Zhao, W. X.; Jiang, J.; Weng, J.; He, J.; Lim, E.P.; Yan, H.; and Li, X. 2011. Comparing twitter and traditional
media using topic models. In Proceedings of the 33rd European
Conference on Advances in Information Retrieval, ECIR’11.

A Generic Approach to Object Matching and Tracking
Xiaokun Li1, Chiman Kwan1, Gang Mei1,
and Baoxin Li2
1

Signal/Image Processing Group, Intelligent Automation Inc.,
Rockville, MD 20855, USA
2 Computer Science and Engineering Dept., Arizona State University,
Tempe, AZ 85287, USA

Abstract. In this paper, a generic approach to object matching and fast tracking in
video and image sequence is presented. The approach first uses Gabor filters to
extract flexible and reliable features as the basis of object matching and tracking.
Then, a modified Elastic Graph Matching method is proposed for accurate object
matching. A novel method based on posterior probability density estimation
through sequential Monte Carlo method, called as Sequential Importance Sampling (SIS) method, is also developed to track multiple objects simultaneously.
Several applications of our proposed approach are given for performance evaluation, which includes moving target tracking, stereo (3D) imaging, and camera stabilization. The experimental results demonstrated the efficacy of the approach
which can also be applied to many other military and civilian applications, such as
moving target verification and tracking, visual surveillance of public transportation, country border control, battlefield inspection and analysis, etc.
Keywords: Image analysis, feature extraction, object matching, real-time
tracking.

1 Introduction
In image analysis and recognition, automatically recognizing objects of interest is always a challenging problem, and has been a research topic for many years. In recent
years, detecting and tracking moving object in video is becoming a more interesting
research topic and alluring more research efforts. In low level computer vision, one
fundamental problem in object recognition and tracking is feature extraction as the
result of extraction will directly affect the recognition performance. Another tough problem in object recognition is the matching between target and template. One reason for
these difficulties is that, in real world, the object of interest always has some orientation
difference and shape deformation as compared to its template in database. The goal of
this paper is to develop an efficient method for object recognition and verification. The
proposed method is based on Gabor filter-based Elastic Graph Matching (EGM) which
has been successfully used in image texture analysis, face and fingerprint recognition
[1-5]. But, by applying a new template-based matching method as the initialization of
EGM, which is invariant to object rotation and size, we can overcome the limitations of
conventional EGM and extend its applicability to more general cases such as stereo
imaging, object tracking, and image sequence stabilization.
A. Campilho and M. Kamel (Eds.): ICIAR 2006, LNCS 4141, pp. 839 – 849, 2006.
© Springer-Verlag Berlin Heidelberg 2006

840

X. Li et al.

Another important issue discussed in this paper is object tracking. In video/image
analysis, object tracking often becomes a more desirable problem after recognition.
An automatic algorithm is needed to answer the questions: what is trace of the detected object? Or, is the object in the current frame the one I am looking for? Once the
object is detected, people usually want to know its status and position in the subsequent frames. In the real world, the object of interest is moving in 3D space, meaning
the features of the object, which are projected onto 2D image, are also changing along
the temporal axis. This makes object tracking a very challenging problem. Even with
the difficulties mentioned above, many new methods and exciting results have been
obtained in recent years, e.g. [6-12]. Unlike the current methods, we propose to use
Sequential Importance Sampling (SIS) method to track moving object in real-time,
which has several important advantages in object tracking. First, SIS is based on posterior probability density estimation through sequential Monte Carlo (MC) method.
The samples used for tracking are weighted properly via MC and updated with current
observation while keeping track of a slowly varying change. Second, with SIS, tacking can be completed simultaneously by using the estimated posterior density.
In this paper, a generic approach for object matching and tracking is presented. The
approach consists of three steps. The first step is Gabor filter-based feature extraction
which provides an efficient way for selecting object features. The second step is an
improved Elastic Graph Matching for object matching. The last step is a novel approach to simultaneously tracking multiple object in video/image sequence. Our
method is based on posterior probability density estimation through sequential Monte
Carlo methods.
The paper is organized as follows. Section 2 gives out the technical details on how
object feature extraction is formulated with Gabor filter. Section 3 describes our approach on using EGM for object matching. In Section 4, one efficient solution for fast
tracking is presented. Section 5 provides some experimental results both in video and
image sequence.

2 Gabor Filter-Based Feature Extraction
In human visual system (HSV), research has shown that people are sensitive to both
specific orientation and spatial frequencies of object of interest. For feature representation and extraction, wavelets are good at representing orientation and frequency
characteristic of object of interest. A Gabor filter bank can act as a simple form of
wavelet filter bank. Because of its simplicity and optimum joint spatial/spatialfrequency localization, Gabor filter has attracted many research efforts [4-5, 13-19]
and has been applied in many image analysis and computer vision-based applications,
e.g. face and fingerprint analysis and recognition [1-3].
Gabor filter bank is a group of 2-D filters which record the optimal jointed localization properties of region of interest in both spatial and spectral domain. Typically,
an image is filtered with a set of Gabor filters which have different or preferred orienK
tations and spatial frequencies. To be specific, an image I ( x ) is filtered with a set of
Gabor wavelets as follows,

A Generic Approach to Object Matching and Tracking

K K
K K K K
( wI )( k , x0 ) = φkK (x0 − x ) I ( x ) dx

∫

where φ kK is the Gabor wavelet (filter) defined by
K
K2 K2
2
KK
k
k x
σ
K
)[exp(
)
−
exp(
−
)]
φkK ( x ) = 2 exp( −
ikx
2
2σ
2
σ

K

841

(1)

(2)

iφ μ

with k = k v e controlling the orientation and the scale of the filters. By varying v
and μ , we can get different Gabor filters with different orientations and scales. In
our implementation, μ controls the orientation and is assigned by any value of 0, 1,
2, to 7 and v controls the spatial frequency and is assigned from 0, 1, and 2 with
k v = (π / 2) / 2 v and φμ = (μπ ) / 8 . After filtering with a set of Gabor filters (24 filters

from the above choice of v and μ ), the outputs on each pixel in the image form a 24dimensional vector called “jet”. The amplitude of the jet represents whether a pixel
has significant gradient value in both orientation and frequency. Thus, it can be used
to determine if this pixel is a good feature for object matching and tracking.

3 Matching
In order to correspond two images from two different sensors, called as image level
matching, or find the correspondence from target to template of reference image/database for object recognition and verification, called as object level matching,
we have to solve the feature correspondence (matching) problem. With the feature
points detected in the previous section, we propose to use an improved Elastic Graph
Matching method to solve the matching by finding the corresponding features in the
target frames. Some more detailed description of EGM can be found in [5, 20]. In
most cases, due to the possible arbitrary relative positioning of the sensors with different field of view (FOV), conventional EGM method may never converge to the
correct position because of the position, orientation, and scale difference between
target and template, and thus we propose a coarse-to-fine strategy to perform robust
matching. We first roughly match the two images (target image and template image)
or find the object of interest in target image by searching with template, and then use
EGM method to tune the matching result. The template matching with unknown rotation and size can be formulated using a non-orthogonal image expansion approach
[21]. In this method, image or object of interest will be recovered by a delta function
at the template location. The convolution equation can be expressed as:

g (r ) = f (r;θ 0 ) * δ (r − r 0 ) + n(r )

(3)

where the position vector is r = [rx , ry ] , * is 2-D convolution, and f is the template
T

(image or object of interest) at r 0 . The orientation and size differences between target and template are represented by a vector θ (where θ 0 is the true parameter set).
θ T = [ s ,φ ]
(4)

842

X. Li et al.

where s is the size and φ is the rotation, a rotated and resized template can be given
as
1
f (r ;θ ) = f ( M (φ )r )
(5)
s
⎡cos φ − sin φ ⎤
where M (φ ) = ⎢
⎥ . In this coarse step, maximum likelihood (ML) can be
⎣ sin φ cos φ ⎦
used to estimate the parameter set θ and use delta restoration method [22] for location estimation r̂ . The cost function of ML can be described as
2

l (θ , r | g ) = f (r ;θ ) * δ (r − r ) − g (r )
The maximum likelihood solution is then obtained by minimizing Eq. (6) as

(6)

{rˆ,θˆ} = arg minl (θ , r | g )

(7)
To solve the optimization problem, a Linear Least Square Estimate (LLSE) of the
delta function can be considered to use. More details can be found in [22].
Even with the method mentioned above, two images or two objects may never be
able to correlate with each other exactly due to local structural and depth variations.
However, this is addressed naturally by the elasticity of the matching graph in the
algorithm. In this paper, we present one improved EGM method which uses Gabor
jets as inputs. Its main steps are given as follows:
Algorithm: Enhanced Elastic Graph Matching (EGM)
Step 1: Find approximate position: We use the novel template matching with unknown rotation and size parameter to identify the initial correspondence/matching
between target and template of reference image/database. From the correspondences,
some corresponding pairs of pixels from target and template are selected as features
whose magnitudes of the jets are obviously larger than that of other pixels.
Step 2: Verify position: We first average the magnitudes of the jets of each feature
point. The jets to each pixel are termed as “bunch”. Then, we assign the average
value to the processed bunch and compute the similarity function S a without phase
comparison.
∑ a j a 'j

Sa ( J , J ' ) =

j

∑ a 2j ∑ a 'j2
j

j

where a j is the average value of the jth bunch. Alternatively, we can compute the
similarity function Sφ with phase.

∑ a j a 'j cos(φ j − φ 'j ) 2 ]
Sφ ( J , J ' ) ≈

j

∑ a 2j ∑ a 'j2
j

j

A Generic Approach to Object Matching and Tracking

843

If the similarity is larger than the predefined threshold, the result by template matching is acceptable. Otherwise, error message will be generated and the EGM process is
stopped.
Step 3: Refine position and size: To the current bunch graph, we vary its position
and size to tune the correspondence. For each bunch, check the four different pixels
( ±3 , ±3 )displaced from its corresponded position in the target image. At each position, we check two different sizes with a factor of 1.2 smaller or larger the bunch
graph.
Step 4: Refine aspect ratio: A similar relaxation process as described in Step 3 is
performed. But at this time, we apply the operation only to x and y dimensions independently.

4 Tracking
In the previous section, we discuss the feature correspondence between target and
template, or two input images, or an image pair of two video sequences. When people
want to know the status of object of interest in a single image/video sequence, target
tracking becomes an interesting research topic. Since object is located in 3D space
and projected onto 2D image, some features of the object will appear and some will
disappear when target is moving or sensor is moving. This is an inevitable challenge
facing any conventional method of feature tracking.
Under a weak perspective camera model, the motion of a planar rigid object can be
approximated by a 2D affine group. Although the set of jets is defined on an object of
interest, e.g. human face, which is definitely not an ideal planar object. But, if deformation of each feature point is allowed, one can still get a good approximation to the
jet motions. Therefore, we model the jet motions as a 2D affine transformation plus a
local deformation. We also assume the motion change between two subsequent
frames is small. Unlike conventional methods, we propose to use Markov chain
Monte Carlo techniques [23] for tacking. Specifically, the Sequential Importance
Sampling (SIS) algorithm is used as motion predictor to find the correspondence
features in two subsequent frames (t frame and t+1 frame) and on-line select features
by updating new weights. In the SIS approach, object motion is formulated as the
evaluation of the conditional probability density p( X t | Z t ) . At time t, p( X t | Z t ) is
approximated by a set of its samples, and each sample is associated with a weight
reflecting its significance in representing the underlying density (importance sampling). The basic steps of SIS are given as follows:
SIS Algorithm
Let S t = X t( j ) , j = 1,..., m} denote a set of random draws that are properly weighted

{

}

by the set of weights Wt = {wt( j ) , j = 1,..., m} with respect to the distribution π t . At
each time step t,

844

X. Li et al.

Step 1. Random draw xt(+j1) from the distribution g t +1 ( xt +1 | xt( j ) ) ;
Step 2. Compute

u t(+j1) =

π t +1 ( xt(+j1) )
π t ( xt( j ) ) g t +1 ( xt(+j1) | xt( j ) )
wt(+j1) = u t(+j1) wt( j ) .

Then ( xt(+j1) , wt(+j1) ) is a properly weighted sample of π t +1 .
In this algorithm, g (⋅) is called the trial distribution or proposal distribution and com⎧⎪ ( x − x ) 2 ⎫⎪
exp ⎨− t +1 2 t ⎬ . Thus, the SIS can be applied
⎪⎭
⎪⎩
σ1
2πσ 1
recursively for t=1,2, …, to accommodate an ever-changing dynamical system

puted by g t +1 ( X t +1 | X t ) =

1

In our tracking algorithm, after obtaining a predicted xt(+j1) , we check it with the
measured value in t+1 frame. Based on the measured feature points from the frame at
t+1, a matching error is computed for the mapped set and the measured set. According to the matching error, ut(+j1) is computed and wt(+j1) is then updated. Note that we do
not specify any uncertainty model for individual feature points, which may be too
complex to be modeled by a simple function, since it needs to account for inaccuracies in 2D approximation, uncertainty due to noise, non-rigidity of object of interest,
etc. In our method, the local deformation at each jet is used to account for these
factors.
Another issue during our implementation is we reduce the motion parameter space
from 2-dimension (x and y directions) to one-dimension ( θ ). Here, we can consider a
rigid object subject to motion which can be modeled by a transformation f parameterized by a parameter vector θ . Let X 0 denote an original parameterization of the
object. X 0 can be a set of jets. Let X = f (θ , X 0 ) denote the transformation of X 0
into X . Under a small and continuous motion assumption, X would be similar to
X 0 and θ would be very close to θ 0 .

X = f (θ , X 0 ) = f (θ 0 , X 0 ) + J θ (θ − θ 0 ) + o(⋅)

(8)

≈ X 0 + J θ (θ 0 )(θ − θ 0 )
where o(⋅) denotes higher-order terms and J 0 (⋅) is the Jacobian matrix with respect
to θ . Consider the 2-D affine motion f (θ ,⋅) as
⎡a
f (θ ,⋅) = ⎢ 11
⎣a 21

⎛ Tx ⎞
a12 ⎤
⎥ (⋅) + ⎜⎜ T ⎟⎟
a 22 ⎦
⎝ y⎠

Vector {a11 ,..., a 22 } represents 2D affine rotation and {Tx , T y } represents translation.

A Generic Approach to Object Matching and Tracking

845

We can compute the Jacobian matrix using
y0 0 0
⎡x
∂X
=⎢ 0
J θ (θ 0 ) =
∂θ θ0 ⎣ 0 0 x 0 y 0

1 0⎤
(9)
0 1⎥⎦
In our tracking algorithm, we take Δθ as X and use X to find the new position by
computing the Jacobian matrix.
Algorithm: SIS-based Tracking
Initialization: The relative camera/sensor motion with a transformation group is
modeled first. The motion parameters constitute a state vector distributed according to
a density function π(t). Then, we track the evolution of π(t) over time t using the SIS
algorithm, by which π(t) is represented by a set of samples x(t,j) with proper weights
w(t,j), j=1,2,…
Step 1. Find a set of feature points of object of interest in the first frame (t=0).
Step 2. For time t>0, track the set of (feature) points from t to t+1 by performing the
following:
a) Each sample x(t+1,j) of π(t+1) is used to map/predict the set of feature
points to time t+1.
b) Based on the measured feature points from the frame at time t+1, a matching error is computed from the mapped set.
c) The matching error is used to control the updating of w(t,j) to get w(t+1,j).
Step 3. At time t+1, we compute the expectation (the weighted mean) of the samples
x(t+1,j) to get the motion at that moment, and the points corresponding to the
mean give the feature set at that time.
Step 4. For the frame at time t+1, use the EGM algorithm with small elasticity to
fine-tune the result.

5 Experiments and Applications
Many tests on image/video sequences have been performed with our proposed algorithm. In this section, three tests are selected to illustrate the efficiency and possible
applications of the algorithm.
5.1 Dancer Matching and Tracking

The test data sets were acquired from public domain (the website of Microsoft Research Labs). The dynamic scenes were captured by eight cameras at different views
with a common synchronization control. The data set from each camera consists of
100 images (24-bits true color, 1024 x 768) at 3 frames per second. We performed
two different tests on them. One is finding feature correspondence between two

846

X. Li et al.

images acquired from two different cameras. Another one is target tracking in a single
video stream.
Fig. 1 shows the feature correspondence results of two images captured from two
cameras with different view points. The pixels are correctly corresponded.
Fig. 2 shows the tracking of a dancer by using video sequence from a single camera. Again our algorithm worked very well and we were able to track the dancer even
though her movements were very drastic.
•

Matching

Fig. 1. Finding feature correspondence from two different-view images. (a) and (b) are two
different-view images captured at the same time. (c) and (d) show the extracted feature points.
(e) is the result of feature correspondence of (c) and (d).

•

Tracking

Fig. 2. Object tracking: (a) and (b) are two subsequent images acquired from same camera at
different time instances. (c) shows the selected feature points from (a). The tracking result is
given in (d).

A Generic Approach to Object Matching and Tracking

847

5.2 Stereo (3D) Image Generation

One important application of image matching is stereo imaging. After finding the feature correspondence between two different-view images, we can use the theories of
multi-view geometry [24] to generate stereo image. The testing video sets for stereo
imaging were collected by two video cameras with the resolution of 640 x 480 and the
frame rate of 10 f/s. We first find the correspondence of the two first frames of the two
input video to create a stereo image pair. Then, the features of next stereo pairs for correspondence were tracked by using our proposed tracking method in each video stream
independently. Fig. 3 shows the corresponded images and the stereo image.

Fig. 3. Stereo imaging: (a) and (b) are two different-view images. (c) and (d) display the selected
feature points for feature correspondence. (e) shows a red-cyan stereo image created from the
feature correspondence (Reader can watch the 3D image with any red-cyan 3D glasses).

5.3 Target Tracking and Stabilization

One experiment was performed to show how the proposed algorithm can be applied to
small target tracking and image sequence stabilization (also known as sensor motion

Fig. 4. Target tracking

848

X. Li et al.

compensation). The testing data has the resolution of 336 x 244 and the frame rate of
30 f/s. In the test, we first manually located a target of interest, e.g. a moving vehicle
shown in Fig. 4 (a). Then, Gabor filter-based feature extraction and SIS-based tracking algorithms were performed to track the moving target in image sequence. The
result is given in Fig. 4 (d). As we can see from the result, the small moving target can
be successfully tracked in cluttered environment.
For sensor motion compensation, we modeled the camera motion as a 2D affine transformation. Stabilization is then achieved in the following steps. First, we extracted a set
of feature points from the first frame. Next, we used the algorithm to track the feature set
in the sequence. Stabilization was then done by warping the current frame with respect to
the reference frame (the first frame) using the estimated motion parameter.

6 Summary
In this work, we have examined the problem of target matching and tracking in
video/image sequence including the data acquired in noisy environment. We proposed
a Gabor attribute matching and tracking algorithm based on an improved EMG and
statistical sampling method. As described in the introduction section, there are many
methods for object matching and tracking. Our algorithm differs from other matching
methods in that we use Gabor attribute as features and extend the typical EMG
method by introducing an efficient template matching method. . Consequently, our
method is suitable for more applications with target rotation and size variations. Most
importantly, we develop SIS-based method for real-time tracking. The advantage of
our tracking algorithm is that we track target without requiring any assumptions to
input data, such as Gaussian distribution, motion type and direction, rigid and nonrigid target, and do not need to predict the motion speed and direction (even allow
rotation in depth) as our approach continually select good feature for tracking by
updating weight at each computation. Another advantage is our tracking is intended
for verification – the model templates by Gabor filter can be easily incorporated into
the tracker because of its formulation and parameterization. Low computational cost
is also one advantage of the algorithm. In our experiments, the tracker processes input
data in real-time on an ordinary PC (CPU: Intel P4 - 2GHz, and Memory: 1024 MB).

Acknowledgements
This work was supported in part by a grant from the U. S. Missile Defense Agency,
HQ0006-05-C-7277. The authors would like to thank Mr. Richard Felner and Mr.
Steve Waugh for their helpful suggestions and comments.

References
1. Biometric Systems Technology, Design and performance Evaluation, Wayman, J.; Jain,
A.; Maltoni, D.; Maio, D. (Eds.), Springer, (2005)
2. Handbook of Fingerprint Recognition, Maltoni, D.; Maio, D.; Jain, A.; Prabhakar, S.
Springer, (2003)

A Generic Approach to Object Matching and Tracking

849

3. Handbook of Face Recognition, Li, Stan Z, Jain, A.; (Eds.), Springer, 2005.
4. D. A. Clausi and H. Deng: Fusion of Gabor filter and co-occurrence probability features
for texture recognition,” IEEE Trans. of Image Processing, Vol. 14, No. 7, (2005) 925-936
5. L. Wiskott, J. Fellous, N. Kruger, C. Malsburg: Face recognition by Elastic Bunch Graph
Matching, Intelligent Biometric Techniques in Fingerprint and Face Recognition, CRC
Press, Chapter 11, (1999) 355-396
6. A. Baumberg and D. Hogg: An efficient method for contour tracking using active shape
models, In Proc. IEEE Workshop on Motion of Non-Rigid and Articulated Objects, (1994)
194-1999
7. M. Black and A. Jepson: Eigen Tracking: Robust matching and tracking of articulated objects using a view-based representation, Int. J. computer Vision, vol. 20, (1998) 63-84
8. D. Freedman and M. Brandstein: A subset approach to contour tracking in clutter, In Proc.
Int. Conf. Computer Vision (1999)
9. D. Huttenlocher, J. Noh, and W. Rucklidge: Tracking nonrigid objects in complex scenes,
In Proc. Int. Conf. Computer Vision (1993)
10. M. Isard and A. Blake: Contour tracking by stochastic propagation of conditional density,
in Proc. Eur. Conf. Computer Vision (1996)
11. C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland: Pfinder: Real-time tracking of the
human body, IEEE Trans. Pattern Anal. Machine Intell., Vol. 19 (1997) 780-785
12. Y. Yacoob and L. Davis: Tracking rigid motion using a compact-structure constraint, in
Proc. Int. Conf. Computer Vision (1999)
13. J. G. Daugman, “Complete discrete 2D Gabor transform by neural networks for image
analysis and compression,” IEEE Trans. on Acoustics, Speech and Signal Processing, Vol.
36(7) (1988) 1169-1179
14. R. L. DeValois and K. K. DeValois, Spatial Vision, Oxford Press (1988)
15. J. P. Jones and L. A. palmer: An evaluation of the 2D Gabor filter model of simple receptive fields in cat striate cortex, J. of Neurophysiology, Vol. 58 (1987) 1233-1258
16. A. J. Bell and T. J. Sejnowski: The independent components of natural scenes are edge filters, Vision Research, Vol. 37(23) (1997) 3327-3338.
17. B. A. Olshausen and D. J. Field: Emergence of simple-cell receptive field properties by
learning a sparse code for natural images, Nature, Vol. 381 (1996) 607-609
18. M. Potzach, N. Kruger, and V. D. Malsburg: Improving object recognition by transforming Gabor filter responses, Network: Computation in Neural Systems, Vol. 7(2) (1996)
341-347.
19. S. E. Grigorescu, N. Petkov, and P. Kruizinga: Comparion of texture features based on
Gabor filters, IEEE Trans. on Image Processing, Vol. 11(10) (2002)
20. D. Beymer: Face recognition under varying pose, In Proc. IEEE Conf. Computer Vision
and Pattern Recognition, (1994) 756-761
21. R. M. Dufour, E. L. Miller, N. P. Galatsanos: Template matching based object recognition
with unknown geometric parameters, IEEE Trans. on Image Processing, Vol.11(12)
(2002)
22. A. Abu-Naser, N. P. Galatsanos, M. N. Wernick, and D. Schonfeld: Object recognition
based on impulse restoration using the expectation maximization algorithm, J. Opt. Soc.
Amer. A: Opt. Image Sci., Vol. 15 (1998)
23. J. Liu and R. Chen: Sequential Monte Carlo methods for dynamic systems, J. Am. Stat.
Assoc. Vol. 93 (1998) 1031-1041
24. R. Hartley and A. Zisserman: Multiple View Geometry in Computer Vision, Second Edition, Cambridge Press (2003)

2866

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 23, NO. 7, JULY 2014

Max-Margin Multiattribute Learning
With Low-Rank Constraint
Qiang Zhang, Lin Chen, Student Member, IEEE, and Baoxin Li, Senior Member, IEEE

Abstract— Attribute learning has attracted a lot of interests in
recent years for its advantage of being able to model high-level
concepts with a compact set of midlevel attributes. Real-world
objects often demand multiple attributes for effective modeling.
Most existing methods learn attributes independently without
explicitly considering their intrinsic relatedness. In this paper,
we propose max margin multiattribute learning with low-rank
constraint, which learns a set of attributes simultaneously, using
only relative ranking of the attributes for the data. By learning
all the attributes simultaneously through low-rank constraint, the
proposed method is able to capture their intrinsic correlation
for improved learning; by requiring only relative ranking, the
method avoids restrictive binary labels of attributes that are often
assumed by many existing techniques. The proposed method is
evaluated on both synthetic data and real visual data including
a challenging video data set. Experimental results demonstrate
the effectiveness of the proposed method.
Index Terms— Multi-task learning, relative attribute, low rank,
attribute learning, surgical skill.

I. I NTRODUCTION

I

N VISUAL computing tasks involving modeling of visual
objects, such as image-based object class recognition, it
has been recognized that some mid-level visual properties, or
“attributes”, of the objects are not only helpful but even critical
to solving the problem [1], [2]. Attributes of a visual object
(or object class) characterize the object (or the class) in terms
of semantically meaningful features such as “being blue in
color”, “having long legs” etc., and thus effectively help to
bridge the gap between low-level visual features and highlevel concepts like the object class. Learning classifiers based
on such attributes has the potential advantage of being able
to model a large number of categories using a compact set
of attributes. Further, a well-defined attribute set may also be
applied to unseen categories.
In practice, obtaining sufficient amount of labeled data
for attribute learning is challenging, especially since many
intuitively useful attributes are often subjective in nature. For
example, for an attribute concerning the size of the bear in
Fig. 1(a) (even if it is for a binary property of being big or
Manuscript received August 27, 2013; revised February 10, 2014; accepted
April 24, 2014. Date of publication May 7, 2014; date of current version
May 27, 2014. The work was supported by the National Science Foundation
under Grant 0904778. The associate editor coordinating the review of this
manuscript and approving it for publication was Prof. Gang Hua.
The authors are with the Department of Computer Science and Engineering,
Arizona State University, Tempe, AZ 85287 USA (e-mail: qzhang53@asu.edu;
lchen109@asu.edu; baoxin.li@asu.edu).
Color versions of one or more of the figures in this paper are available
online at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TIP.2014.2322446

Fig. 1. Describing the objects in the images with attributes: they are both
animals; they both have four legs; (a) is larger than (b); (a) is more dangerous
than (b); b is more likely to be found around human; etc.

small), there may not exist a single “correct” ground-truth
label. Several recent efforts have attempted to address this
issue. In [3], the concept of relative visual attributes was introduced to allow learning with only relative labels, which are
presumably easier to obtain. Similar ideas have been applied
to other applications such as distance metric learning [4], face
verification [5], and human-machine interaction [6].
For properly modeling objects in real-world problems,
we typically need more than one attribute. For example,
for images of animals illustrated in Fig. 1, we may utilize
attributes concerning questions like, “is it an animal?”,
“is it wild?”, “is it dangerous?”, etc. There may be intrinsic relatedness among the attributes used to describe the
same object if the attributes are indeed properties of the
underlying object. For example, “being dangerous” is usually
(negatively) correlated with “found around people”. Learning
these attributes independently, as is done in most existing
work, cannot capture such intrinsic relatedness. We hypothesize that considering correlations among the attributes may
contribute to improving the individual attribute learners.
In this paper, we explore approaches to learning multiple
attributes jointly. We propose a novel formulation termed
Max-Margin Multi-attribute Learning with Low-rank
Constraint and develop an algorithm for obtaining a solution
under this model. The proposed approach learns a set
of attributes simultaneously under the multi-task learning
framework, where learning each attribute is viewed as one
task. By learning all the attributes simultaneously with
low-rank constraint, the proposed approach is able to capture
the intrinsic relatedness of the attributes. It also makes the
proposed methods more robust when there are outliers or no
sufficient data for certain attributes. In addition, instead of
requiring absolute labeling of the training data, the proposed

1057-7149 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

ZHANG et al.: MAX-MARGIN MULTIATTRIBUTE LEARNING WITH LOW-RANK CONSTRAINT

method utilizes the relative rankings between pairs of the
inputs, which are more flexible and effective for describing
the data. For evaluation, we first design synthetic experiments
to systematically evaluate the model and the algorithm, and
then perform experiments with two image datasets and one
video dataset. The video dataset was from surgical training,
which is a challenging example of intricate relatedness
among attributes describing the object of interest. Improved
performance of the proposed method over alternative solutions
suggests that it is a promising solution to multi-attribute
learning.
The key contribution of the work lies in the novel formulation of learning a set of attributes simultaneously only
based on the relative ranking of the data and the proposed
algorithm for obtaining solutions under the formulation. The
rest of this paper is organized as follows: we first briefly review
some related work in Section II; then Section III presents
the proposed methods; the experimental results are described
in Section IV; and the paper concludes with discussion in
Section V. In this paper, we will use upper case bold font
(e.g., X) for matrices and lower case bold font (e.g., x) for
vectors.
II. R ELATED W ORK
In this section, we briefly review some related work on
multi-task learning and (relative) attribute learning. There is
a huge amount of relevant work in the literature and our
review focuses only on those we deem as closely related to
the proposed method.
Multi-task learning, in which a set of tasks are learned
simultaneously, has been applied in many applications including Web page categorization [7], Web image and video
search [8], face verification [9] and disease prediction [10].
A typical multi-task learning method can be formulated as
follows:
nt
T
1  1 
l( f t (Xit ), Lit ) + λ({ f t })
min
{ ft } T
nt
t

(1)

i

where f t is a classifying/regression function, l(·) a loss
function (e.g., squared error), Lit the ground truth response
for data Xit (e.g., data labels), and (·) a penalty term for
encouraging common structures of f t ’s.
Given different loss functions and penalty terms, many models have been proposed for multi-task learning. In [11] it was
assumed that the classifying functions are close to each other,
and thus hinge loss was used for the loss function, and the
deviations of the classification functions from their means were
used as the penalty. Although being intuitive, this assumption
is too restrictive and may not be valid for real-world problems.
In [12] the l1 /lq mixture norm was used as the penalty term,
where for q ≤ 2, the sets of regression functions exhibit group
sparsity, i.e., the regression functions will select a common
sets of features. In real applications, different tasks would have
their task-specific components besides the shared components,
and thus [13] proposed to decompose the regression functions
into two components, where the task-specific component is
assumed to be sparse and regularized by a l1 norm, and

2867

the shared common component is regularized by the l1 /lq
mixture norm. Similar idea was proposed in [14], where the
l1 norm was replaced by lq /l1 mixture norm to capture the
irrelevant tasks. The relatedness among the tasks could also be
captured by a low-rank structure. For example, [15] assumed
the regression functions were linearly dependent and the trace
norm was used as the penalty term. The trace norm has been
also used in [16]– [18]. However, these methods require ground
truth labeling (e.g., binary labels, real-valued scores) for the
training data, which may be difficulty to obtain in many realworld applications.
Attribute learning has seen increasing application in visual
processing in recent years, which is especially useful for largescale dataset, where learning classifiers for data of each category is not practical [19]. In addition, often the attributes can
be transferred to unseen categories, different datasets or even
different applications (e.g., zero-shot learning [20]). However,
most existing work utilizes binary labels or categorical labels,
which is not only too restrictive but also unnatural. As a result,
relative learning has been proposed. For example, in [4] the
relative ranking of data points was used for learning a distance
metric function, and the relative rank of some facial attributes
was used for face verification in [5]. In [3], the ranking
functions were learned from relative ranking of images and
then used to describe the images; and in [6] a subject provides
relative ranking as feedback to improve the performances of
classifiers. There are also efforts on automatically extracting
attributes [21]–[24]. However, most existing work learns each
attribute independently, ignoring their intrinsic relatedness,
which may be extremely helpful especially if the labeling is
sparse.
The proposed method attempts to alleviate the requirement
of knowing exact labels (through using only relative rankings
in learning) while explicitly modeling intrinsic relatedness of
the attributes in the learning task (through a multi-task learning
framework with a low-rank constraint). As a result, the method
achieves a few desired benefits that are not available in existing
methods. Such benefits are demonstrated in experiments with
both synthetic data and real images/videos, with comparison
to typical existing solutions.
III. P ROPOSED M ETHOD
The proposed method is capable of learning a set of
attributes from only relative rankings. Given the ranking
information Et and Ft , where Et is the set of pairs (i, j ) that
Data i is better than Data j for Attribute t, and Ft for the
set of pairs being similar for Attribute t, we want to learn a
classifier Wt , such that,
WtT (Xit − X j t ) ≥ 1 ∀(i, j ) ∈ Et
|WtT (Xit − X j t )| ≈ 0 ∀(i, j ) ∈ Ft
where Xit is the representation of Data i for Attribute t.
In many scenarios, e.g., image classification, we may need
to learn multiple attributes and those attributes are likely
to be correlated, as illustrated in the examples in Fig. 1.
Conventional attribute learning approaches learn these
attributes independently, and thus their intrinsic relatedness is

2868

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 23, NO. 7, JULY 2014

not utilized. We propose to learn the attributes simultaneously
under the multi-task learning framework. One popular multitask learning model assumes that the classifiers of different
tasks are similar and their differences to their mean are small.
By combining this idea with relative learning, we obtain
a baseline approach termed Multi-Task Relative Learning
(MTRL), which is formally defined as
minW,,γ

s.t.

λ
1 
|Wt −
Wτ |22
2
2
T
t
τ


t
+ρ1
i j + ρ2
γitj
T

1

|Wt |22 +

(i, j )∈Et
(i, j )∈Ft
T
t
Wt (Xit − X j t ) + i j ≥ 1
−γitj ≤ WtT (Xit −X j t ) ≤ γitj itj

≥ 0; γitj ≥ 0 (2)

where Xit is the representation of i t h data for Task t, Wt is
the tt h column of W (i.e., classifier of Task t) and |Wt |22 is
related to the margin of the classifier for Tasks t. This problem
can be solved by quadratic programming in its dual form, and
the details are included in Appendix A.
In the above baseline approach, the usage of a common
component has limited the form of correlation that the formulation could model (e.g., when the two tasks are negatively correlated). To this end, we model the correlation
among the tasks by linear dependence, which is more flexible
than MTRL. If we put the classifiers into the columns of
a matrix, the resultant matrix would be low-rank, i.e., its
nuclear norm would be small. Thus, we can formulate this new
solution as
minW,,γ λ|W|∗ +
s.t.

T

1

2



|Wt |22 + ρ1

itj + ρ2

t
(i, j )∈Et
T
t
Wt (Xit − X j t ) + i j ≥ 1
−γitj ≤ WtT (Xit − X j t ) ≤ γitj itj ≥



min λ|Z|∗ +
s.t.

T

1

2

|Wt |22 + ρ1



0; γitj ≥ 0

(3)



γitj

(i, j )∈Ft
T
Wt (Xit − X j t )

(6)

where Y is the Lagrange multiplier, ·, ·	 is the inner product
and μ is related to the Lipschitz
the primal problem
 constant of 

f (Z, W, b, ) = λ|Z|∗ + 21 t |Wt |22 + ρ1 itj + ρ2 γitj .
The problem in Eqn. 5 can be solved via block coordinate
descent, by considering the following two sub-problems:
Low-rank problem: fix W, b, , γ , μ and Y to solve Z, i.e.,
μ
(7)
min λ|Z|∗ + Y, W − Z	 + |W − Z|2F
Z
2
Ranking problem: fix Z, μ and Y to solve W, b,  and γ , i.e.,
1
μ
|W − Z|2F + Y, W − Z	 +
|Wt |22
2
2
t


t
t
+ρ1
i j + ρ2
γi j
T

γitj
s.t.

(i, j )∈Et
(i, j )∈Ft
T
t
Wt (Xit − X j t ) + i j ≥ 1
−γitj ≤ WtT (Xit −X j t ) ≤ γitj

itj ≥ 0; γitj ≥ 0 (8)

In summary, the overall algorithm for solving the problem
of Eqn. 3 can be described in Algorithm 1.
In the following subsections, we will present specific methods for solving the two sub-problems of Eqn. 7 and 8., and
then analyze the overall algorithm. The convergence analysis
of the algorithm is included in Appendix B, where we show
the proposed problem is convex and the proposed algorithm
will converge to its global optimum.
A. Solving the Low-Rank Problem

≤ γitj
(4)

By applying the Augmented Lagrange Multiplier
(ALM) method to the equality constraint W = Z,
we have:
minW,,γ L(Z, W, b, γ , , μ, Y)
s.t.

L(Z, W, b, , γ , μ, Y) = λ|Z|∗ + Y, W − Z	


1
μ
|Wt |22 + ρ1
itj + ρ2
γitj
+ |W − Z|2F +
2
2 t

(i, j )∈Ft

itj + ρ2

t
(i, j )∈Et
T
t
Wt (Xit − X j t ) + i j ≥ 1 − γitj ≤
itj ≥ 0; γitj ≥ 0 W = Z

with

minW,,γ

where | · |∗ is the nuclear norm or the sum of singular values
of the matrix for casting the low-rank constraint. We refer the
proposed solution in Eqn. 3 as Max-Margin Multi-Attribute
Learning with Low-Rank Constraint.
This problem is equivalent to the following problem by
introducing a slack variable Z, which separates the low-rank
constraint from the others:

W,,γ

Algorithm 1 Max-Margin Multi-Attribute Learning With
Low-Rank Constraint

WtT (Xit − X j t ) + itj ≥ 1
−γitj ≤ WtT (Xit −X j t ) ≤ γitj itj ≥ 0; γitj ≥ 0 (5)

For the low-rank problem, we want to find the optimal Z for Eqn. 7, which is a convex problem. It has
been shown in [25] that the optimal solution to the problem minX λ|X|∗ + 12 |X − W|2F can be computed via a singular value thresholding algorithm, i.e., USλ ()VT , where
UVT ← svd(W) is the singular value decomposition and
S· (·) is the thresholding operator:
⎧
b≥a
⎨b − a
Sa (b) = 0
a ≥ b ≥ −a
(9)
⎩
b+a
otherwise

ZHANG et al.: MAX-MARGIN MULTIATTRIBUTE LEARNING WITH LOW-RANK CONSTRAINT

Thus the optimal solution to Eqn. 7 is Z∗ = US λ ()VT ,
where UV ← svd(W +
T

μ

1
μ Y).

2869

of the dual form of the ranking problem is |Et | + 2|Ft |. After
we solve the problem in Eqn. 15, we can compute the classifier
according to Eqn. 12.

B. Solving the Ranking Problem


2
2
By recognizing
t |Wt − Zt |2 and
 |W − Z| F =
Y, W − Z	 =
t Yt , Wt − Zt 	, the problem in Eqn. 8
can be decomposed into T independent smaller problems,
where each smaller problem is associated with only one
attribute/task:
1
μ
minW,,γ |Wt − Zt |2F + Yt , Wt − Zt 	 + |Wt |22
2 
2

+ ρ1
kt + ρ2
γlt
s.t.

k
T
Wt Ekt

l

+ kt ≥ 1

− γlt ≤ WtT Flt ≤ γlt kt ≥ 0; γlt ≥ 0

(10)

where we use Ekt = Xit − X j t ∀(i, j ) ∈ Et , Flt = Xit −
X j t ∀(i, j ) ∈ Ft , k, l to re-index (i, j ) ∈ Et and (i, j ) ∈ Ft .
By applying the Lagrange multipliers, for Eqn. 10 we can
have:
μ
max min |Wt − Zt |2F Yt , Wt − Zt 	
α,β,δ,η,ζ w,,γ 2

1
ρ1 k + αk (1 − k − WtT Yk ) − ηk αk
+ |Wt |22 +
2
k

+
ρ2 γl + βl (WtT Zl − γl ) + δl (−WtT Zl − γl ) − ζl γl
l

s.t. α, β, δ, η, ζ ≥ 0

(11)

By checking the gradients, we have:


μZt − Yt + k αkt Ekt + l (δlt − βlt )Flt
Wt =
1+μ

(12)

0 ≤ αkt ≤ ρ1

(13)

0 ≤ βlt + δlt ≤ ρ2

(14)

Accordingly, we have the dual form for the problem in
Eqn. 11, which is a quadratic programming problem:
1
min utT Kt ut + ftT ut
ut 2
s.t.lbt ≤ ut ≤ ubt
AtT ut = 0

(15)

with
ut = [α T , −β T , δ T ]T
⎤
⎡ T
Et Et EtT Ft EtT Ft
Kt = ⎣ FtT Et FtT Ft FtT Ft ⎦
FtT Et FtT Ft FtT Ft
ft = [EtT (Yt − μZt ) − 1, FtT (Yt − μZt ), FtT (Yt − μZt )]T
lbt = [0e|TEt | , −ρ2 e|TFt | , 0e|TFt | ]T
ubt = [ρ1 e|TEt | , 0e|TFt | , ρ2 e|TFt | ]T

At = [0|Ft |×|Et | , −I|Ft |×|Ft | , I|Ft |×|Ft | ]
bt = ρ2 e|Ft |

where en ∈ Rn×1 is a all-1 vector, 0m×n ∈ Rm×n is all-0
matrix, In×n ∈ Rn×n is the identity matrix. Thus the dimension

C. Analysis of the Algorithm
The proposed algorithm involves two major sub-problems.
For the low-rank problem, the most time consuming step is the
singular value decomposition for a matrix of dimension D × T
(D is the input dimension), where the typical complexity for
an exact decomposition is O(min (T D 2 , T 2 D)). However, we
may not be interested in a full/exact decomposition, but only
the singular vectors whose singular value are sufficiently large
(e.g., PROPACK [26]). For the classification problem, we are
solving T quadratic programming problems of dimension n t ,
with n t the number of data points for the t − th task.
The proposed problem in Eqn. 3 is convex and the proposed
algorithm will converge to its global optimum. The proof is
given in Appendix B. For the stopping criterion, we compute
|W−Z|2F
. If this value is sufficiently small (e.g., 10−6 ), we will
|W|2F
terminate the optimization. In our experiments, we observed
the convergence was reached within 100 iterations.
There are three parameters required for the proposed
algorithm: λ (controlling the weight of the nuclear norm
term), μ (controlling the weight of the term |W − Z|2F ) and
σ (controlling the increasing speed of μ). The selection of λ
depends on the correlation among the tasks: if high correlation
among the tasks is expected, we should use a large λ (i.e.,
|W|∗ should be small); otherwise, we should set λ to a small
value. When λ = 0, the proposed method is equivalent to the
relative attribute learning method, where each task is solved
independently. For μ, we utilizes the analysis in [27] and set
it to 1.25λ
|W|2 . For ρ, we use ρ = 1.2.
IV. E XPERIMENTAL R ESULTS
We evaluated the proposed approach on both synthetic data
(Section IV-A) and real image/video data sets (Section IV-B,
IV-C). The proposed method is compared with the relative
attribute method of [3], where each attribute is learned independently, and with the multi-task relative learning method,
where the classifying/ranking functions of the attributes are
assumed to share a common component. Since no validation
set is available for the real datasets (and they are too small to
support creation of a validation set), we did not rely on crossvalidation for parameter tuning. Instead, in the experiments
we used the following fixed parameters for the proposed
method and the multi-task relative attribute learning method:
λ = 10000, ρ1 = 100 and ρ2 = 100. Default parameters were
used for relative attribute learning.
A. Simulated Experiment
In this section, we evaluate the proposed method on
synthetic data. We generate T = 10 tasks and the feature
dimension of each tasks is D = 1000. The ground truth
classification function (or ground truth ranking function) for
Task t is Wt , where Wt is the t − th column of W. W is

2870

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 23, NO. 7, JULY 2014

Fig. 3. The result for simulation experiment when the ground truth ranking
functions of the tasks are linear independent. The matrix consisted of ground
truth ranking functions as its columns has maximal singular value 0.7 and
minimal singular value 0.6.

Fig. 2. The result for simulation experiments with varying P (a) (λ = 104 )
and λ (b) (P = 100), where the dashed curves correspond to the result of the
proposed method, dot curve for the MTRL and solid curve for the relative
attributes method. We compute accuracy (y axis of red curves) of the learned
ranking functions and the correlation (y axis of green curves) between the
learned ranking functions and ground truth ones. The X axis are the p (a)
and λ (b) accordingly.

generated as:
W0 = rand(D, T ) − 0.5
svd(W0 ) → UVT
W = U(:, 1 : r )(1 : r, 1 : r )VT (:, 1 : r )
where r = 2 is the desired rank of W. Note that by
generating the ground truth classification function in this
way, the classifiers are not necessarily similar or to share
a common component. We uniformly draw the data X for
each task, and each set of data contains 1000 data points.
For Task t, we randomly select P pairs as the training pairs,
i.e., (i, j ) ∈ E if WtT Xi − WtT X j ≥ τ ; or ( j, i ) ∈ E if
WtT Xi − WtT X j ≤ −τ ; otherwise (i, j ) ∈ F, where τ is
the predefined margin. The proposed algorithm is applied
to the training pairs to learn the ranking function for the
tasks, with comparison with the relative attribute (refer as
“Relative”) method and also the baseline (i.e., Multi-Task
Relative Learning or MTRL) method. We also test different
combinations of λ (from 10−4 , i.e., low requirement of the

low-rank constraint, to 107 , i.e., high requirement on the lowrank constraint) and P (from 10 to 1000), where the results
are shown in Fig. 2.
From Fig. 2(a), we can observe that, although the accuracy
and the correlation increase with more training pairs, i.e.,
larger P, the proposed method consistently performs better
than the other two competitors. Especially, when P = 1000,
the correlation between the ranking functions learned by the
proposed method and the ground truth ones is about 0.9,
which is significantly better than 0.68 achieved by the relative attribute method. The results indicate that the proposed
method is more likely to recover the ground truth ranking
functions than the relative attribute method, when given the
same number of training pairs. The performance of MTRL is
significantly lower. This could be explained by the assumption
made by its formulation: the classification functions of the
tasks should be similar (or share a common component), which
is not always true in the generation of the data (e.g., the classification functions can be negatively correlated).
Fig. 2(b) illustrates the performance of the proposed
approach with different settings for the parameter λ, which
controls the contribution of the low-rank constraint. From
the plot, we can observe that the performance is stable for a
wide range of λ(λ ∈ [10, 104 ]) and the best result is obtained
when λ = 104 .
We also performed simulations using data whose ground
truth ranking functions are not correlated, i.e., the functions
are linear independent by setting r = 10. The results are shown
in Fig. 3, from which we can find that, the proposed method
(dashed curve) obtained similar results as the relative attribute
learning method (solid curve) in both accuracy and correlation. However, the MTRL (dotted curve) obtained obviously
worse performances in both accuracy and correlation. This
demonstrates that the proposed method is robust to different
correlation levels of the tasks, and its performance is still comparable to that of the relative attribute learning method even
when the tasks are totally linear independent. The performance
of MTRL method, however, degrades dramatically when the
assumption about the relatedness of the tasks does not hold.

ZHANG et al.: MAX-MARGIN MULTIATTRIBUTE LEARNING WITH LOW-RANK CONSTRAINT

Fig. 4. The computation time of the proposed approach given different
number of training pairs, with the comparison to the relative attribute learning
methods and MTRL method. For the time axis (y-axis), we use logarithm.

Fig. 5.
The histogram of Pearson’s correlation coefficients among the
tasks for both two datasets. From these histograms, we can observe that the
attributes are correlated, as there are non-trivial mass covering the regions
towards −1 or 1. Note that, Pearson’s correlation coefficient measures only
linear dependency, and thus even if it is low, the tasks could still be highly
dependent.

For understanding the computational efficiency of the proposed method, we note that its formulation as well as solutions
bear similarity to MTRL, which is well-understood to have a
polynomial complexity over the number of constraints. Hence
the proposed method is expected to have the same order of
complexity over the number of training pairs. To empirically
verify this, we use Fig. 4 to depict the running times of the
proposed approach under different numbers of training pairs,
with the comparison to the relative attribute learning method
and the MTRL method. It can be observed that the proposed
method, while being more expensive than the basic relative
attribute learning method, is indeed in par with the MTRL
method in terms of asymptotic time complexity. Note that both
axes of Fig. 4 are with logarithm for better illustration.
B. Learning Attributes for Images
To evaluate the performances of the proposed algorithm
on real data, we utilize two datasets, (1) Outdoor Scene
Recognition (OSR) Dataset [28] containing 2688 images from
8 categories; (2) A subset of the Public Figure Face Database

2871

(PubFig) [5] containing 800 images from 8 random identities
(100 images each). We directly used the processed data1
from [3] and the same experiment settings. To demonstrate
that the attributes in these datasets indeed exhibit correlation,
we first computed the histogram of the pairwise correlation
coefficients among the tasks for each of the datasets, and the
results are shown in Fig. 5. It is evident from these plots
that the tasks are correlated. For example, one can observe
that there is a non-trivial mass covering beyond the interval
[−0.5, 0.5] in either of the plots. Note that, both the rank of
classifier matrix (W) and the correlation coefficients between
the tasks are some measurements of the dependency. For
ideal case (perfectly dependent), the rank should be 1 and the
correlation coefficient should be +/−1 cross different tasks.
Next we report the ranking accuracy of the proposed method
and compare with the relative attribute method (“Relative” in
short) and the multi-task relative attribute learning method
(M T RL in short). All the results on the two datasets are
summarized in Table I. From Table I we can observe that
the proposed method outperforms the other methods in both
cases except that the Relative* row of (A). [3] has an insignificant gain over our method, even with much more training
pairs (see also the caption of the Table). Additionally, we
can observe that the performance gain of our method over
Relative or M T RL (when all trained under the same protocol
with only 5% of the training pairs used in [3]) varies. This
could be explained by possible varying degree of correlation
among the tasks in the two datasets, as alluded by Fig. 5.
However, we note that the correlation coefficient used in Fig. 5
measures only linear dependency and thus it is not proper
to draw any quantitative conclusion. Additionally, the lowrank constraint would generally work better when there are
many tasks considered jointly (comparing with the feature
dimension) [15]. This is consistent with the results in the
Table (e.g., better gain by the proposed in (B) than in (A)).
The low-rank constraint used in the proposed method is more
flexible than forcing the tasks to share common components
in capturing the intrinsic relatedness of the attributes, which
explains the gain of the proposed over M T RL.
C. Evaluating Surgical Skills From Videos
In this experiment, the data were videos collected from
the Fundamentals of Laparoscopic Surgery (FLS) trainer
box (www.flsprogram.org), which is a simulation-based training platform and has been widely used in many hospitals/
training centers for minimally-invasive surgery training. The
system has an on-board camera capturing a trainee’s operation
inside the box and the video is shown on a monitor. There
are a set of standard operations defined for the FLS training
system. Our experiment was based on data captured from
the “Peg Transfer” operation, as illustrated in Fig. 6. In this
operation, a trainee is required to lift one of the six objects
with a grasper by his non-dominant hand, transfer the object
midair to his dominant hand, and then place the object on a peg
on the other side of the board. Once all six objects have been
1 downloaded at http://filebox.ece.vt.edu/~parikh/relative.html#data

2872

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 23, NO. 7, JULY 2014

TABLE I
R ANKING A CCURACY FOR OSR (A) AND P UB F IG (B) F ROM D IFFERENT M ETHODS . I N [3], OVER 20 , 000 T RAINING PAIRS W ERE U SED AND THE
R ESULTS A RE R EPORTED H ERE AS “R ELATIVE *”. I N O UR E XPERIMENT, W E R ANDOMLY P ICKED O NLY 5% OF T HOSE T RAINING PAIRS FOR
E VALUATING THE T HREE M ETHODS , A S S HOWN IN ROW 2 TO ROW 4 OF E ACH TABLE . F OR THE P ROPOSED M ETHOD , W E F IXED λ TO 10000

Fig. 6. Illustrating the FLS system: (a) the FLS system (white), (b) and
(c) frames captured by onboard camera showing the operation within the FLS
trainer box.
TABLE II
P RIMITIVE A CTIONS IN P EG T RANSFER

transferred, the process is reversed from one side to the other.2
The Peg Transfer operation consists of several primitive
actions (or therbligs [31]) as building blocks of manipulative
surgical activities, which are defined in Table II. Ideally,
these primitive actions are all necessary in order to finish one
peg-transfer cycle. Since there are six objects to transfer leftto-right and then backwards, there are totally 12 cycles in one
training session. Our experiment was based on video recordings from the FLS system on-board camera capturing training
sessions of resident surgeons in their different residency years.
2 For more details of the FLS trainer box and the “peg transfer” operation,
we refer the readers to [29] and [30].

1) The Attribute-Learning Task: Given a video from an
operation described above, we segment it into multiple clips,
with each clip containing only one primitive action, e.g., lift.
For providing automatic feedback to a trainee, we need to
infer the motion skill from those clips, which is deemed a very
difficult tasks, due to the semantic gap between the low-level
visual feature and the high-level motion skill. We apply the
proposed method to this challenging problem by first defining
a set of attributes (Table III), which are designed according to domain knowledge on surgical skill evaluation [32].
These attributes describe varying aspects of motion skill and
are easier to infer from the visual features. With these skilldefining attributes learned, we can provide a trainee with a
more detailed rating of his/her performance, rather than a
slim score. For example, when they find they have a low
performance in attributes “instrument handling”, they would
spend more time in improving their handling of instruments.
2) Feature Extraction: Based on the attributes defined
above, we design the following feature extraction scheme.
We first utilize random forest (RF, learned from a training set) to segment the pixels of each frame into “tool”
and “background”, based on the color information. With the
segmentation results, we perform morphological operation and
blob analysis to extract the tool tips and orientations of the
tools controlled by the left and right hands. After that, the
motion features V used for skill attribute (Table III) learning
are generated in 3 steps. In the first step, a few types of motion
information are estimated to represent a trainee’s operation, as
summarized in Table IV. In the second step, we extract motion
signatures from each of the motion features in Table IV.
The motion signatures are 1-dimensional temporal signals
(Table V) to further compact the motion information. In the
last step, final motion features are extracted from each motion
vector and its motion signatures as follows: in the time domain,
we divide a signature into equal temporal bins; and we also
divide the Fourier transform result into equal frequency bins.
In each temporal or frequency bin, the maximal, minimal, and
average values are kept.
3) Experiment Results: We selected 10 representative
videos from trainees of different skill levels, where each video
is a full training session consisting of 12 Peg Transfer cycles,

ZHANG et al.: MAX-MARGIN MULTIATTRIBUTE LEARNING WITH LOW-RANK CONSTRAINT

2873

TABLE III
T HE ATTRIBUTE U SED IN T HIS PAPER , W HICH A RE D EFINED A CCORDING TO [32]. N OTE , W E O NLY S ELECT THE ATTRIBUTES
W HICH A RE R ELAVANT TO THE O PERATIONS IN O UR S IMULATED S URGICAL V IDEOS

TABLE IV

TABLE VI

M OTION I NFORMATION . ROI I S A R EGION A ROUND G RASPER T IPS
TO I NCLUDE O BJECT U NDER O PERATION

T HE E XPERIMENTAL R ESULT IN E VALUATING M OTIONS S KILLS OF
S URGICAL S IMULATIONS : ( A ) T HERBLIG “L IFT ” AND ( B ) T HERBLIG
“T RANSFER ”. C OL 2 I S THE N UMBER OF D ISSIMILAR PAIRS ; C OL 3 I S
N UMBER OF S IMILAR PAIR . N OTE FOR ATTRIBUTE R AND I OF
T HERBLIGS “T RANSFER ”, W E D ON ’ T E NOUGH G ROUND T RUTH

THE

TO

C OMPUTE THE A CCURACY

TABLE V
M OTION S IGNATURES , W HERE Y (t) R EPRESENTS A NY M OTION
I NFORMATION IN TABLE IV, E . G ., V (t), V̂ (t) AND M(x, t)

which leads to 12 video clips for each therblig. Thus we
have in total 120 clips for each therblig. We manually label
the relative rankings for 150 pairs of clips, following the
guidelines provided by FLS (available on the FLS website).
For each pair of clips, we label the attributes described in
Table III as either “left is better than right”, “right is better than
left” or “unsure”. Then five-fold random split (one fold for
testing and remaining folds for training) is applied to evaluate
the proposed method with the comparison to the other two
methods. Due to space limitation, we only show the results
of two therbligs “lift” and “transfer” in this paper, which are
presented in Table VI.
From Table VI, we can find that, the proposed method
(Col 3) and MTRL (Col 4) obtained significantly better result
than the relative attribute method (Col 5), except for the
attribute “Bimanual dexterity” for therblig “Lift” (Table VI(A)
Row 4) and the attribute “Depth perception” for therblig
“Transfer” (Table VI(B) Row 7). The improvement can be
explained by the explicit consideration of intrinsic relatedness
of those attributes in the proposed method and MTRL. The
proposed method is on average better than MTRL, although
MTRL achieves similar average accuracy in Table VI(B). This
could be due to the fact that both the MTRL constraint and the
proposed low-rank constraint did similarly well in capturing
the correlation among the attributes for that particular action.
However, as discussed earlier, the flexibility of the low-rank
constraint in the proposed method would in general lead to
a better performance, which is also evidenced by the overall

better performance of the proposed method in Table VI (and
in particular in (A)).
V. D ISCUSSION AND C ONCLUSION
In this paper we proposed a novel approach Max-Margin
Multi-Attribute Learning with Low-Rank Constraint.
Compared with existing methods in the literature, the
proposed method learns a set of attributes simultaneously so
that the intrinsic relatedness could be captured. In addition,
it only require the relative ranking of the attributes instead
of binary labels, leading to a more flexible solution to many
learning applications in which absolute labels are difficult to
obtain. We evaluated the proposed method on both simulated
data and real image/video data, and compared its performance
with the relative attribute method and the MTRL method,
both being representative of typical alternative solutions. The
experimental results demonstrated that the proposed method
is more effective in capturing the intrinsic correlation among
the tasks (or attributes) and delivers higher accuracy than the
competing methods.
It is worth mentioning that, the proposed method is based
on the assumption that the set of tasks are related. If indeed

2874

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 23, NO. 7, JULY 2014

the tasks are related, the proposed method is able to, as shown
in our experiments, outperform the “relative attribute” method
which treats each task independently. However, if the tasks
are independent, the performance of the proposed method
may degrade, as it would force a correlation model on the
independent tasks. However, for real problems with multiple
attributes describing the same underlying object of interest, it
is reasonable to assume that completely independence among
the attributes would be rare, and thus the proposed method
is expected to able to deliver good performance in general.
Nevertheless, it will be an interesting future direction to
explicitly explore possible relationship between the performance of the method and the degree of relatedness among
the tasks/attributes. In addition, considering that the current
method relies on only the low-rank constraint, another future
task is to investigate modeling of more complicated intrinsic
relationship among the attributes with outlier handling.

which can be written as the following quadratic programming
problem:
1
minu uT Ku + fT u
2
s.t. lb ≤ z ≤ ub Au ≤ b
(21)

A PPENDIX A
A LGORITHM FOR MTRL

where en ∈ Rn×1 is a all 1 vector, 0m×n ∈ Rm×n is all
0 matrix, In×n ∈ Rn×n is identity matrix, K|E|×|E| (i, t; j, s) =
T (yit )(y j s ), K|E|×|F| (i, t; j, s) = T (yit )(z j s ) and
K|F|×|E| (i, t; j, s) = T (zit )(z j s ). The mapping function
(·) is defined in Eqn. 17.
After we solve the quadratic problem in Eqn. 21
=
[α T , −β T , δ T ]T , we
with optimal solution u ∗
1
T
can
=
[ λ W0 , V1T , . . . , VtT ]T
=

 compute (W)
α
(Y
)
+
(δ
−
β
)(Z
)
and
then
recover
it
jt
jt
jt
t
i it
j
classifier of each attribute as Wt = W0 + Vt .
As the proposed method can be formulated into a quadratic
programming problem, the convergence and global optimality
of the solution is guaranteed. The dimension of quadratic
programming problem is T (|E| + 2|F|) × 1 with T as the
number of tasks, |E| and |F| as the number of constraints cast
by relative rankings. The dimension of the problem and the
computational cost could be high, when there are a lot of pairs
of relative rankings. To solve this issue, we could utilize the
idea of active constraints.

According to [11], Eqn. 2 is equivalent to the following
problem with appropriate parameters (λ, ρ1 , ρ2 ):
minW,,γ

T

1
t

2

+ρ1

|Vt |22 +


λ
|W0 |22
2

itj + ρ2



γitj

(i, j )∈Et
(i, j )∈Ft
T
(Vt + W0 ) (Xit − X j t ) + itj ≥
−γitj ≤ (Vt + W0 )T (Xit − X j t )
≤ γitj itj ≥ 0; γitj ≥ 0

s.t.

1
(16)

with Wt = Vt + w0 . According to [11], we can also define
the following mapping functions:

1
(Xit ) = [
Xit , 0, . . . , 0, Xit , 0, . . . , 0]
(17)
Tλ
√
(W) = [ T λw0 , V1 , . . . , Vt , . . . , VT ]
(18)
and get the following formulations:
minW,,γ
s.t.

T



1
|(W)|22 +
ρ1
itj + ρ2
γitj
2
t

(i, j )∈Et
 (W)(Xit − X j t ) + itj ≥
−γitj ≤ T (W)(Xit − X j t )
≤ γitj itj ≥ 0; γitj ≥ 0
T

(i, j )∈Ft

1
(19)

T
2
2
Obviously |(W)|22
=
and
t (|Vt |2 + λ|w0 |2 )
T
T
 (W)(Xit − X j t ) = (Vt + W0 ) (Xit − X j t ).
By writing (Xi − X j ) = Yk for (i, j ) ∈ E and (Xi − X j ) =
Zl for (i, j ) ∈ F and applying Lagrange multipliers we, can
get the dual form of Eqn. 2:


1 
αk (Yk ) +
(δl − βl )(Zl )|22 −
αk
minα,β,λ |
2
k

s.t.

0 ≤ βl , δl ≤ ρ2

l

0 ≤ αk ≤ ρ1 0 ≤ βl + δl ≤ ρ2

k

(20)

with
u = [α T , −β T ,
⎡
K|E|×|E|
K = ⎣ K|F|×|E|
K|F|×|E|

δ T ]T ∈ RT (|E|+2|F|)×1
⎤
K|E|×|F| K|E|×|F|
K|F|×|F| K|F|×|F| ⎦
K|F|×|F| K|E|×|F|

f = [−e|TE| , 0e|TF| , 0e|TF| ]T
lb = [0e|TE| , −ρ2 e|TF| , 0e|TF| ]T
ub = [ρ1 e|TE| , 0e|TF| , ρ2 e|TF| ]T

A = [0|E|×|E| , −I|F|×|F| , I|F|×|F| ]
b = ρ2 e|F| ∈ RT |F|×1

A PPENDIX B
C ONVERGENCE A NALYSIS
We will show that the proposed algorithm (Section III)
will converge. In this section, we will use Yk to represent
the variable Y computed in kt h iteration. First, we can easily
identify that, the two sub-problems, “low rank problem” and
“classification” problem are convex. We define the space
 = {Z, W, b, , γ |WtT (Xit − X j t ) + itj ≥ 1 & − γitj ≤
WtT (Xit − X j t ) ≤ γitj & it ≥ 0& γit ≥ 0 ∀i, t}, which is
obvious convex, and the analysis will be within this space.
Lemma 1: Yk is bounded.
Proof: Since Zk+1 is optimal for the low-rank problem with Wk , bk ,  k , γ k , μk and Yk , we have 0 ∈
∂ L(Z,Wk ,bk , k ,μk ,Yk )
k
k
∗
k k
. That is 0 ∈ ∂
Z
∂Z
∂Z − Y + μ (Z − W ),
∂
Z
∗
k+1
k+1
k
k
k
so we have Y
∈ ∂Z , where Y
= Y − μ (Z − Wk ).
According to [27] Theorem 4 and Lemma 1, Yk+1 is bounded.
This ends the proof of Lemma 1.
Lemma 2: the sequences Zk , Wk , bk ,  k , μk will converge
to the optimal solution.

ZHANG et al.: MAX-MARGIN MULTIATTRIBUTE LEARNING WITH LOW-RANK CONSTRAINT


Proof:  we define f (W, b, ) = λ|W|∗ + 21 t
|Wt |22 + ρ i it as the objective function of the primal
problem (Eqn. 3). We have:
L(Zk+1 , Wk+1 , bk+1 ,  k+1 , γ k+1 , μk , Yk )
= min L(Z, W, b, , γ , μk , Yk )
Z,W,b,

≤
≤

min

L(Z, W, b, , γ , μk , Yk )

min

f (W, b, , γ ) = f ∗

Z=W,b,
Z=W,b,
k+1

As Zk+1 − W
= μ1k (Yk+1 − Yk ) and the boundedness
of Yk , we have limt →∞ Zk − Wk = 0. Thus (W∗ , b∗ ,  ∗ ) =
limt →∞ (Wk , bk ,  k ) is the feasible solution of the primal
problem.
In addition, we have
f (Wk+1 , bk+1 ,  k+1 , γ k+1 )
= L(Zk+1 , Wk+1 , bk+1 ,  k+1 , γ k+1 , μk , Yk )
1
−
(|Yk+1 |2F − |Yk |2F ) + |Wk+1 |∗ − |Zk+1 |∗
2μk
1
≤ f∗−
(|Yk+1 |2F − |Yk |2F ) − |Wk+1 − Zk+1 |∗
2μk
1
1
≤ f∗−
(|Yk+1 |2F − |Yk |2F ) − k |Yk+1 − Yk |∗
2μk
μ
1
= f ∗ − O( k )
(22)
μ
where for the last step, we use the boundedness of Yk
(Lemma 1). Thus we have limt →∞ [ f (Wk+1 , bk+1 ,  k+1 )] =
limt →∞ f ∗ − O( μ1k ) = f ∗ .
Besides, by |Z|∗ ≥ |W|∗ − |Z − W|∗ , we have
f (Wk+1 , bk+1 ,  k+1 , γ k+1 )
= L(Wk+1 , Wk+1 , bk+1 , γ k+1 ,  k+1 )
≥ L(· · · ) − λ|Zk+1 − Wk+1 |∗
λ
≥ L(· · · ) − |Yk+1 − Yk |∗
μ
λ
λ
≥ L(· · · ) − O( ) ≥ f ∗ − O( )
μ
μ

(23)

where we short L(Zk+1 , Wk+1 , bk+1 ,  k+1 , γ k+1 ) by L(· · · ).
Combining Eqn. 22 ( f ∗ − f (Wk+1 , bk+1 ,  k+1 , γ k+1 ) ≥
O( μ1k )) and Eqn. 23 ( f ∗ − f (Wk+1 , bk+1 ,  k+1 , γ k+1 ) ≤
O( μλk )), we have | f (Wk+1 , bk+1 ,  k+1 )− f ∗ | ≤ max ( μ1k , μλk ).
As μk+1 = μk × σ and if we choose σ > 1, we have
limt →∞ | f (Wk+1 , bk+1 ,  k+1 ) − f ∗ | = 0. This proves the
convergence.
ACKNOWLEDGMENT
Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and
do not necessarily reflect the views of the National Science
Foundation.
R EFERENCES
[1] V. Ferrari and A. Zisserman, “Learning visual attributes,” in Proc. Adv.
Neural Inform. Process. Syst., 2007, pp. 433–440.
[2] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth, “Describing objects by
their attributes,” in Proc. IEEE Conf. CVPR, Jun. 2009, pp. 1778–1785.

2875

[3] D. Parikh and K. Grauman, “Relative attributes,” in Proc. IEEE ICCV,
Nov. 2011, pp. 503–510.
[4] M. Schultz and T. Joachims, “Learning a distance metric from relative
comparisons,” in Proc. Adv. NIPS, 2004, p. 41.
[5] N. Kumar, A. Berg, P. Belhumeur, and S. Nayar, “Attribute and simile
classifiers for face verification,” in Proc. IEEE 12th Int. Conf. Comput.
Vis., Oct. 2009, pp. 365–372.
[6] D. Parikh, A. Kovashka, A. Parkash, and K. Grauman, “Relative
attributes for enhanced human-machine communication,” in Proc. 26th
AAAI Conf. Artif. Intell., 2012.
[7] J. Chen, L. Tang, J. Liu, and J. Ye, “A convex formulation for learning
a shared predictive structure from multiple tasks,” in Proc. 26th Annu.
Int. Conf. Mach. Learn., 2009, pp. 137–144.
[8] O. Chapelle, P. Shivaswamy, S. Vadrevu, K. Weinberger, Y. Zhang,
and B. Tseng, “Multi-task learning for boosting with application to
web search ranking,” in Proc. 16th ACM SIGKDD Int. Conf. Knowl.
Discovery Data Mining, 2010, pp. 1189–1198.
[9] X. Wang, C. Zhang, and Z. Zhang, “Boosted multi-task learning for face
verification with applications to web image and video search,” in Proc.
IEEE Conf. CVPR, Jun. 2009, pp. 142–149.
[10] J. Zhou, L. Yuan, J. Liu, and J. Ye, “A multi-task learning formulation
for predicting disease progression,” in Proc. 17th ACM SIGKDD Int.
Conf. Knowl. Discovery Data Mining, 2011, pp. 814–822.
[11] T. Evgeniou and M. Pontil, “Regularized multi–task learning,” in Proc.
10th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2004,
pp. 109–117.
[12] G. Obozinski, B. Taskar, and M. I. Jordan, “Joint covariate selection
and joint subspace selection for multiple classification problems,” Statist.
Comput., vol. 20, no. 2, pp. 231–252, 2010.
[13] A. Jalali, P. Ravikumar, S. Sanghavi, and C. Ruan, “A dirty model for
multi-task learning,” in Proc. Adv. Neural Inform. Process. Syst., vol. 23.
2010, pp. 964–972.
[14] P. Gong, J. Ye, and C. Zhang, “Robust multi-task feature learning,” in
Proc. 18th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining,
2012, pp. 895–903.
[15] S. Ji and J. Ye, “An accelerated gradient method for trace norm
minimization,” in Proc. 26th Annu. Int. Conf. Mach. Learn., 2009,
pp. 457–464.
[16] R. K. Ando and T. Zhang, “A framework for learning predictive
structures from multiple tasks and unlabeled data,” J. Mach. Learn. Res.,
vol. 6, pp. 1817–1853, Nov. 2005.
[17] J. Chen, J. Liu, and J. Ye, “Learning incoherent sparse and low-rank
patterns from multiple tasks,” ACM Trans. Knowl. Discovery Data,
vol. 5, no. 4, p. 22, 2012.
[18] J. Chen, J. Zhou, and J. Ye, “Integrating low-rank and group-sparse
structures for robust multi-task learning,” in Proc. 17th ACM SIGKDD
Int. Conf. Knowl. Discovery Data Mining, 2011, pp. 42–50.
[19] O. Russakovsky and L. Fei-Fei, “Attribute learning in large-scale
datasets,” in Proc. ECCV 2010 Workshop Parts Attributes, vol. 1.
[20] C. Lampert, H. Nickisch, and S. Harmeling, “Learning to detect unseen
object classes by between-class attribute transfer,” in Proc. IEEE Conf.
CVPR, Jun. 2009, pp. 951–958.
[21] T. Berg, A. C. Berg, and J. Shih, “Automatic attribute discovery and characterization from noisy web data,” in Proc. ECCV, 2010, pp. 663–676.
[22] J. Wang, K. Markert, and M. Everingham, “Learning models for object
recognition from natural language descriptions,” in Proc. British Mach.
Vis. Conf., 2009.
[23] D. Parikh and K. Grauman, “Interactively building a discriminative vocabulary of nameable attributes,” in Proc. IEEE Conf. CVPR,
Jun. 2011, pp. 1681–1688.
[24] S. Branson et al., “Visual recognition with humans in the loop,” in Proc.
ECCV, 2010, pp. 438–451.
[25] J.-F. Cai, E. J. Candès, and Z. Shen, “A singular value thresholding
algorithm for matrix completion,” SIAM J. Optim., vol. 20, no. 4,
pp. 1956–1982, 2010.
[26] R. M. Larsen, “Lanczos bidiagonalization with partial reorthogonalization,” DAIMI PB, vol. 27, no. 537, pp. 1–101, 1998.
[27] Z. Lin, M. Chen, and Y. Ma, “The augmented Lagrange multiplier
method for exact recovery of corrupted low-rank matrices,” in Proc.
NIPS, 2011, doi: 10.1016/j.jsb.2012.10.010.
[28] A. Oliva and A. Torralba, “Modeling the shape of the scene: A holistic
representation of the spatial envelope,” Int. J. Comput. Vis., vol. 42,
no. 3, pp. 145–175, 2001.
[29] Q. Zhang, L. Chen, Q. Tian, and B. Li, “Video-based analysis of
motion skills in simulation-based surgical training,” in IS & T/SPIE
Electron. Imag., Int. Soc. Opt. Photon., vol. 8667, pp. 86670A–86670A,
Mar. 2013, doi: 10.1117/12.2005177.

2876

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 23, NO. 7, JULY 2014

[30] Q. Zhang and B. Li, “Relative hidden Markov models for evaluating
motion skill,” in Proc. IEEE Conf. CVPR, Jun. 2013, pp. 548–555.
[31] S. Jun et al., “Robotic minimally invasive surgical skill assessment based
on automated video-analysis motion studies,” in Proc. 4th IEEE RAS
EMBS Int. Conf. BioRob, Jun. 2012, pp. 25–31.
[32] J. Doyle, E. Webber, and R. Sidhu, “A universal global rating scale
for the evaluation of technical skills in the operating room,” Amer. J.
Surgery, vol. 193, no. 5, pp. 551–555, 2007.

Lin Chen received the B.S. and M.S. degrees in
computer science from Shandong University, Shandong, China, in 2008 and 2011, respectively. He
is currently pursuing the Ph.D. degree in computer science and engineering with Arizona State
University, Tempe, AZ, USA. His research interests include computer graphics, geometry processing, image/video processing, computer vision, and
machine learning, specialized in attribute learning,
multitask learning, and motion analysis.

Qiang Zhang received the B.S. degree in electronic information and technology from Beijing Normal University, Beijing, China, in 2009, and the
Ph.D. degree in computer science from Arizona
State University, Tempe, AZ, USA, in 2014. Since
2014, he has been with Samsung Semiconductor
Inc., Pasadena, CA, USA, as a Senior Software
Engineer in Computer Vision. His research interests
include image/video processing, computer vision,
and machine vision, specialized in sparse learning,
face recognition, and motion analysis.

Baoxin Li (S’97–M’00–SM’04) received the Ph.D.
degree in electrical engineering from the University
of Maryland, College Park, MD, USA, in 2000.
He is currently an Associate Professor of Computer Science and Engineering with Arizona State
University, Tempe, AZ, USA. From 2000 to 2004,
he was a Senior Researcher with SHARP Laboratories of America, Camas, WA, USA, where he
was the Technical Lead in developing SHARP’s
HiIMPACT Sports technologies. From 2003 to 2004,
he was also an Adjunct Professor with Portland
State University, Portland, OR, USA. He holds nine issued U.S. patents. His
current research interests include computer vision and pattern recognition,
image/video processing, multimedia, medical image processing, and statistical
methods in visual computing. He was a recipient of the SHARP Laboratories’
President Awards in 2001 and 2004, the SHARP Laboratories’ Inventor of the
Year Award in 2002, and the National Science Foundation’s CAREER Award
from 2008 to 2009.

A General Framework for Sports Video Summarization with its Application to
Soccer
Baoxin Li,Ha0 Pan, Ibrahim Sezan

Sharp Laboratories of America
Camas, WA 98607, USA
ABSTRACT
We propose a general framework for indexing and summarizing
sports broadcast programs and its specific application to soccer.
The framework is based on a high-level model of sports
broadcast video using the concept of an event, defined according
to domain-specific knowledge for different types of sports. Thus
it covers most sports including those that have the action-andstop pattern (e.g., baseball) and those containing continuous
actions (e.g., soccer). In particular, within this general
framework, using soccer as an example, we propose a novel
approach to automatic event detection, which is based on
automatic analysis of the visual and aural signals in the media.
An MPEG-7 compliant prototype browsing system has been
implemented to demonstrate the results.

1. INTRODUCTION
With the increasing amount of audio-visual data that are
broadcast or available in a prerecorded format, there is an
emerging need for efficient media management including
browsing, filtering, indexing, and retrieval. Among the
challenges facing current media management systems is the
automatic extraction of the media content descriptions. This
paper propose a general framework for automatic event detection
and video summarization in the sports video domain, and its
specific application to soccer.
We start with a general model of a sports broadcast video at a
high level, with eventlnon-event breakdown of the video.
According to this model, a sports broadcast video is composed
of sparse event segments that are interleaved with other not-sointeresting segments, which are referred to as non-event
segments. The definition of an event can be specialized to a
specific sport based on specific domain knowledge (e.g., an
event can be a pitch in baseball or a goal in soccer). This is a
unifying model due to the fact that it applies to different types of
sports, including those that have the action-stop pattern, e.g.,
baseball and American football, and those that are continuous
without obvious breaks, e.g., continuous-action sports such as
soccer and ice hockey. Automatic event detection algorithms can
be developed on the basis of this general model. In particular, we
propose a novel algorithm for soccer event detection using the
proposed continuous-action model.
Once event segments are detected, indexing is performed on the
basis of starting and end points of these segments. Video
summarization is implemented by concatenating event segments
thereby forming a condensed version of the video. In comparison
with existing methods and especially those that aim at extracting
only the “exciting” segments of sports programs automatically
(e.g., [1,2]), the proposed framework extracts events that are
either objectively defined using the domain knowledge of the

0-7803-7663-3/03/$17.00 0 2 0 0 3 IEEE

underlying sports, or defined based-on the selection that a
human operator makes during the production of the broadcast
program, e.g., replays provided during the broadcast. Hence the
results obtained by the proposed approach are more robust in
practice.
In Section 2, we provide an event-based modeling of sports
broadcast video supporting both stop-action and continuousaction sports. In Section 3, we develop a new algorithm for
soccer in order to demonstrate the application of the model to
continuous-action sports. Experiments are presented in Section
4. We conclude with discussion and conclusions in Section 5.

2. MODELING SPORTS VIDEO USING EVENTS
In many types of sports, although a typical game broadcast lasts
say, a few hours, only parts of this time contain real actions.
These important parts occur semi-periodically but sparsely
during the game, The remaining time is typically less important
(e.g., change of players, time-outs, etc). These types of sports are
so-called “action-and-stop” sports. For an action-and-stop sport,
if all actions have been extracted, then a user can follow,
understand, and even enjoy the game by viewing only the action
clips. Examples include baseball and American football, where
all pitches and all plays, respectively, contain every detail that a
user wants to know about the underlying game. On the other
hand, there are also many other sports that are almost continuous
without any break, and thus it is impossible to cut out any
portion of the video without compromising a user’s ability for
grasping every detail of the game. However, for this type of
sports, since the video is full of consecutive actions, what a user
would desire is some reasonable classification of the actions,
which allows access to those parts of the video that are more
interesting or exciting than other parts of the video. Examples of
continuous-action sports include soccer, ice hockey, etc.

In consideration of the above distinction between action-andstop sports and continuous-action sports, we extend our earlier
work [31 (which is for the former type of sports) by introducing a
general modeling, using “event” in an abstract sense. We model
the video as a sequence of “events” interleaved with %onevents’’,with ‘‘event’’ being defined as the basic segment of time
during which an important or exciting action occurs, as
illustrated in Fig. 1. When instantiated for a specific sport, the
event can be, for example, a pitch in a baseball game, and a goal
or a goal attempt in soccer. Obviously, an event is a complete
action and can contain multiple video shots; thus the modeling is
at a higher level than the breakdown of video into shots.
For an action-and-stop type sport such as baseball, by defining
event to be every pitch, the modeling in Fig. 1 effectively breaks
down an input video into pitches and segments of idle time. In
this case, the modeling is reduced to that proposed in [3].

I11 - 169

ICASSP 2003

Beginning of

I

-------

our modeling has a unique advantage over the above mentioned
methods that practically rely on the automatic understanding of
the content by a computer. We rely on automatic detection of
replay segments using low-level features and production
patterns, as described in the next section, which is a much more
suitable task for a computer than understanding the content.

End of
Video

Video
Non-ercnl

Figure 1. A general model of a sports video in t e r m af“event”.
The inner loop (in dashed lines) indicates the passibility that
two or more imporbant actions can occur consecutively.

3. EVENT DETECTION IN SOCCER VIDEO

For a continuous-action game, there is no break per se.
Therefore, the problem becomes how to define event so that it
represents those actions that are of greater importance and
interest. While there are different methods for detecting
“exciting” segments in video by using, for example, audio
information, we claim that “excitement” is a subjective concept
and thus is not easy to model, let alone to detect automatically by
a computer. We propose a new approach to defining event in this
case. We define important events as those actions in a game that
are replayed by the broadcaster. By this strategy, we effectively
shift the task of judging the importancelexcitement of an action
to the broadcaster, who is generally in a much better position to
make this judgment than any automatic algorithm. This method
assumes that the actions that are replayed by a broadcaster are
typically more exciting or important than other actions. We
believe that this is a reasonable assumption. In this case, the
modeling in Fig. 1 is effectively reduced to that of Fig. 2.

I
c-!ka

Beginning
of Video

1

Ordinary
ACli””

I

&
Endof

Video

I

Figure 2. The effective modeling for continuous-action sports.
Obviously, compared with Fig. I, here we know that event in
Fig. 1 refers to “exciting action”, and “non-event” refers to
“ordinary (less exciting) action”. It is for this reason, we have
emphasized that event in the general modeling is used in an
abstract sense, since there may be actual actions going on during
a non-event period. For action-and-stop sports, even if we can
use the play-cenuic modeling (as in [31), it is also possible to use
the modeling in Fig. 2. In the latter case, the modeling would
focus on distinguishing, for example, exciting pitches from
others in a baseball game, rather than detecting every pitch.

With the modeling in Fig. 1 and Fig. 2, the major task of video
analysis becomes the detection of all the events in a given video.
While it is straightforward to show that the general framework
can he specialized to action-stop type of sports such as baseball
(see [3] for more discussion), in this section, we propose a novel
event detection algorithm for broadcast video of soccer, which is
used as an example for continuous-action sports. The algorithm
is largely based on v i d e o h d i o analysis of the input video, and
they make use of tw’o types of prior knowledge in extracting
semantics from broadcast sports video: (I) domain knowledge,
and (2) production knowledge. The detection algorithms
represent these two types of knowledge in terms of rule bases
where rules are expressed in terms of low-level visual and aural
features that are automatically computed from the media.
As discussed in Section 2, for continuous-action sports, the
event in the modeling of Fig. I is actually defined as
excitinglimponant actions that have been picked out by the
broadcaster for re-playing. In soccer, these exciting actions most
likely contain goal attempts and surely contain goals. We utilize
the knowledge of the established production pattems in soccer
broadcast programs. The sequential relationship between an
exciting action and its replay is shown in Fig. 3. An event
includes an exciting action and the setup action leading to that
action. An exciting action is usually followed by one or more
close-up shots of the key player(s), andlor the audience, andlor
the coach, andlor the referee, and then followed by the replay of
the action. In other words, an exciting action has two versions,
live and replay, with some close-up segments separating the two
parts. We make use of this triplet production pattem in detecting
the exciting events. In the following, we describe the detection
of a replay segment, its associated close-up segment, and the
associated exciting action segment, respectively. The overall
diagram o f the algorithm is shown in Fig. 4.

The proposed modeling is readily applicable to media database
management applications, where common operations such as
indexing, retrieval, logging, and annotation, etc. can all benefit
from the breakdown of a video into the event and non-event
segments. For example, events can form meaningful indexing
points for semantic analysis, and video summarization can be
achieved by concatenating the event segments.

Event

Figure 3. The relationship between an exciting action and its
replay. Events are defined as the action that is replayed plur the
setup action leading to that action.

2.1 Comparison with Earlier Work
Compared with earlier work in this area, the proposed modeling
is (a) more general as it unifies two major types of sports,
namely action-and-stop and continuous-action sports, and (b) it
has obvious advantages as we describe below. For action-andstop type of sports, the proposed approach is an extension of
earlier work, where individual papers either handle a specific
sport [4], or can handle only playbreak type of pattems [3,51.
The proposed modeling is also different from prior work in the
case of continuous-action sports ([1,2,61) in that our modeling
defines exciting actions based on the selection made by a
production expert, namely on replays. Thus in terms of accuracy
or meaningfulness in determining whether an action is exciting,

111 - 170

Video

Audio
Length of Replay

Figure 4. The general framework of the soccer algorithm.

3.1 Detection of replay segments
Replay detection is the first step in the algorithm. We have
developed methods for detecting replays with sufficient
reliability and accuracy ([7,X]). Due to space limitation, we can
only briefly mention that the method of [7] is mainly based on
the detection of slow-motion generated by fielaframe repetition,
and that the method of [8] is mainly based the detection of
transitional logos that sandwich replay segments. More details
can be found in these references.

3.2 Detection of close-up segments
We use two visual features to detect close-up segments: (I) the
ratio of the number of pixels belonging to the soccer field to the
total number of pixels in a frame; and (2) scene cuts, discussed
respectively as follows.
(1) Ratio of the field pixels to the total number of pixels:
In close-up segments, players (or referees) are shot from a close
distance. Therefore, the frame is not dominated by the colors of
the soccer field. Otherwise, the field is shot from a long distance
and thus a large portion of the frames is the soccer field, as
shown in Fig. 5 . The dominant color of a soccer field varies from
time to time and game to game and can be calibrated ([E]). In
computing the above ratio for distinguishing close-ups, we
assume that the dominant color is green.

Figure 5. A frame From a live segment (left); and a frame From
the associated close-up segment (right).
The color of green can be defined in any appropriate color space.
In this paper, green is calculated in the normalized RGB color
space, and only the normalized R and G color components, as
defined below, are considered. This is intended to alleviate the
effects of varying color in the case of different fields, weather
conditions, camera settings, etc. A pixel is classified as being
green if its normalized (R',G')components satisfy the following
conditions: R'c0.5, C>0.5, and R'+G'< 1.0, where

Fig. 6 shows a typical example of the computed ratio over a
period of time. One can see that in normal shots, the ratio is
close to 1. In close-up shots, the ratio is close to 0; in replay
segments, the ratio vanes.

Cl",C."O

Figure 6. Ratio of the number OF green pix& to the number of
pixels in B frame.

(2) Scene cuts:
Usually there is a sudden scene cut between the live action and
the close-up segment. This scene cut is at the end of the live
action and at the starting point of the close-up segment.
Sometimes, a close-up segment between the live action and its
replay consists of several different shots of players, coaches, and
audiences, and therefore consists of several scene cuts, as
illustrated in Fig. 7, Close-up segments are characterized by the
low ratio of the number of green pixels to the number of total
pixels in a frame. The earliest scene cut in this time period with
low (number of green pixelsitotal number of pixels) ratio is
chosen as the starting point of the close-up segment and the end
point of the live action. In the example shown in Fig. 1,it is the
Scene cut 0.
L i v e action

Replay

Close."*

h
\I

t
scenecut
n

I

scene
CYI

I

4

I

4

scene

scene

cut2

cut3

n

I,

Figure 7. Demonstration of determination of the starting paint of
the close-up segment and the end-point of the live exciting action.
Any appropriate scene cut detection algorithm can be used. In
our implementation, we utilize the difference between color
histograms of every two adjacent frames. The color histogram is
calculated in the normalized RGB color space and each of the
normalized R and G color components is evenly divided into 16
bins. As a result, there are a total number of 256 bins. The
difference between two color histograms is measured using the
mean-square error (MSE) criterion. A large change (larger than a
threshold computed dynamically from differences in recent past
frames) in the color histogram difference identifies a scene cut

3.3 Detection of starting points of exciting events
Tne last step of the proposed soccer algorithm is to detect the
starting points of exciting event segments. Although the
semantic content of the live version and replay version of an
exciting action is the same, the camera angles used to capture
them in the broadcast video are usually different. In other words,
frames in replays look structurally very different from those in
the live version. This is demonstrated in Fig. 8 using frames
taken from a typical soccer broadcast. Even with the state-of-theart content analysis technology, it is extremely difficult. if not
impossible, to reach the conclusion that the contents of these
frames belong to the same physical action, using purely only the
video as input. Therefore, although the replay segment has been
detected, one can hardly expect to find the corresponding live
action based on content analysis. In the following, we propose a
method for determining the starling point of the event, i.e., the
starting point of the set-up action.
Due to the nature of soccer games, except for a few cases, such
as a free kick and a corner shot, there is no objective way of
defining the starting instances of exciting events. Besides, before
exciting actions take place, the video shots in broadcast soccer
games are typically unbroken. A shot usually lasts for several
minutes without scene transitions. Therefore, one cannot use
scene cuts or transitions in detecting the starting points of the
event segments. In our algorithm, we choose the starting points
of the event associated with the detected replay segments as
either 15 or 30 seconds prior to the starting points of the closeup segments. We make use of the following two criteria in

I11 - 171

choosing between I S and 30 seconds. If either of the two criteria
favors 30 seconds, then we place the starting point at 30 seconds
before the end point of the action. Otherwise we place the
startine voint at 15 seconds before the end uoint of the action.

and over again to recreate and share their excitement with
others.) On the average, the summaries that include the replay
segments were less than 18% of the original video in length,
which means a user can review all the exciting events and their
replays of a full game by spending only 16 minutes on watching
the summary.

All the data used in our experiments are MPEG-encoded streams
of 320x240 frame resolution, captured by an inexpensive TVtuner PC card. All the algorithmic modules further reduce the
input resolution to 160x120 before computation. This suggests
that the proposed algorithms do not require very high quality
input. In our experiments, the event detection algorithm achieved
faster than 30-frameslsecond computational performances on a
Pentium Ill-800MHz PC.

Figure 8. Images of a live broadcast of B goal (tap row) and
images of the correspondingreplay (bottom raw).
Criterion 1. Energy spectrum of the audio signal. An exciting

action and its setup are typically accompanied by the audience's
and/or the commentators' increased level of excitement, which is
usually reflected in the audio tracks. In our experiments, the
energy in the lk-6.4kHz frequency band is calculated by taking
FFI of the Hamming-window filtered input audio signal, If the
average audio energy over the 30-second duration is larger than
the average energy over the 15-second duration, then a 30second long event segment is favored; otherwise, a 15-seconds
long event segment is favored. (We also investigated the
possibility of using the audio pitch contour to reflect the degree
of the excitement of commentators. However, in our
experiments, two commentators with significantly different
pitch baffled the method)
Criterion 2. Duration of detected replay segments. We have
observed that the lengths of replays are proportional to the
excitement of the actions in the replays. When an action is more
exciting, it is reasonable to give it more time to see how the
event is built up. Thus if the duration of a detected replay
exceeds certain threshold, a 30-second event segment is favored.

An MPEG-7 compliant prototype browsing system has been
implemented to visualize the results, which will be demonstrated
during the conference.

5. CONCLUSIONS
We have proposed a general framework for sporls video
analysis, which includes a unifying model for modeling both
action-stop and continuous action sports. We have demonstrated
the generic nature of the model by successfully applying it to
different sports, and particularly to soccer in this paper. We have
evaluated the performance of the proposed framework and
algorithm through extensive experiments. Another important
advantage of the proposed event based model is the fact that it
facilitates synchronization and merging with independently
generated rich metadata, which typically have an event-based
granularity. Upon synchronization and merging, the resultant
composite metadata contain not only video indexing points of
event segments, but also independent rich metadata that are
typically generated by human experts [9].

6. REFERENCES

3.4 Generating the game summary
We have used the following four methods for generating
highlight summaries:
( I ) Concatenation of [exciting event segment + close-up
segment + replay segment), for all replays ;
(2) Concatenation of [exciting event segment + close-up
segment], for all replays;
(3) Concatenation of (exciting event segment), for all replays:
(4) Concatenation of all replay segments.

4. EXPERIMENTAL RESULTS
We have performed extensive experiments by applying the
proposed algorithms to soccer video from different broadcasters,
and video captured from different recording sources. Excellent
results have been obtained. Particularly, for two sample
sequences of two different games, with a total duration 120
minutes, the algorithm detected all the five goals in the games.
All other 21 detected actions were exciting goal attempts. The
resulting summaries were evaluated by a group of soccer fans in
our laboratory. They all agreed that the summaries provided
them with all the relevant and exciting events and that their
viewing experience was smooth in that the summary flowed
naturally. (While hard-core fans will mostly prefer to enjoy the
live broadcast at its entirety; they nay enjoy the summaries as a
quick review of the game. In fact, in our tests, the fans enjoyed
watching the summaries and the exciting parts of the game over

[I] D. Yow, B-L. Yeo, M. Yeung, and B. Liu, "Analysis and
Presentation of Soccer Highlights From Digital Video",
Proc. 2"dAsian Conference on Computer Vision, 1995.
[2] Y. Rui, A. Gupta, and A. Acero, "Automatically extracting
highlights for TV baseball programs," Proc. ACM
Multimedia 2000, Los Angeles, CA, 2000.
[3] B. Li and M. I. Sezan, "Event detection and summarization
in sports video," Prnc. IEEE Workshop on Conrent-Based
Access to Video and Image Libraries, 2001,
[4] T. Kawashima, K. Tateyama, T. iijima, and Y. Aoki,
"Indexing of Baseball Telecast for Content-based Video
Retrieval", Proc. IEEE-ICIP 1998.
[5] D. Zhong and S-F. Chang, "Structure Analysis of Sports
Video Using Domain Models", Prnc. IEEE lnfemnrional
Conf on Mulrimedia and Expo, 2001, Tokyo,
. Japan.
.
161 L. Xie, S:F. Chang, "Structure analysis of soccer video
with hidden Markov models," Proc. IEEE-ICASSP 2002.
[7] H. Pan, P. van Beek, and M.I. Sezan, "Detection of Slowmotion Replay Segments in Sports Video for Highlights
Generation", Proc. IEEE-ICASSP, 2001.
[8] H. Pan, B. Li, and M. I. Sezan, "Automatic detection of
replay segments in broadcast sports programs by detection
of logos in scene transitions, Proc. IEEE ICASSP 2002.
191 B. Li. I. Erricco. H. Pan. and M. Ibrahim Sezan. "Brideine
The Semantic Gap in Sports Video", Proc. of SPlE/l&T
Electronic Imaging, January 2003.

.~

111 - 1 7 2

1

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

Finding Needles of Interested Tweets in the
Haystack of Twitter Network
Qiongjie Tian

Jashmi Lagisetty

Baoxin Li

Computer Science and Engineering
Arizona State University
Email: qiongjie.tian@asu.edu

Computer Science and Engineering
Arizona State University
Email: jlagiset@asu.edu

Computer Science and Engineering
Arizona State University
Email: baoxin.li@asu.edu

Abstract—Drug use and abuse is a serious societal problem.
The fast development and adoption of social media and smart
mobile devices in recent years bring about new opportunities
for advancing computer-based strategies for understanding and
intervention of drug-related behaviors. However, the existing
literature still lacks principled ways of building computational
models for supporting effective analysis of large-scale, often
unstructured social media data. Part of the challenge stems
from the difficulty of obtaining so-called ground-truth data that
are typically required for training computational models. This
paper presents a progressive semi-supervised learning approach
to identifying Twitter tweets that are related to personal and
recreational use of marijuana. Based on a small, labeled dataset,
the proposed approach first learns optimal mapping of raw
features from the tweets for classification, using a method of
weakly hierarchical lasso. The learned feature model is then
used to support unsupervised clustering of Web-scale data.
Experiments with realistic data crawled from Twitter are used to
validate the proposed approach, demonstrating its effectiveness.

I. I NTRODUCTION
Drug use/abuse is among the serious societal problems in
the modern age. According to a 2011 report [1], in the United
States alone, illicit drug use costs the society more than $193
billion annually and the number is increasing. The impact is
also widespread: In 2013, about 24.6 million Americans 12
years old or older were illicit drug users [2]. Accordingly, a lot
of research efforts have been devoted to understanding druguse-related behaviors and the analysis of potential benefits
and limitations of various intervention strategies. A key step
in such drug-use-related research is the collection of user
behavior data.
Most conventional approaches to user data collection are
based on recruitment of participants who would provide inputs
to a drug-use-related study, e.g., by answering questionnaires
carefully designed to gather various types of behavioral and/or
demographical data [3][4]. But there are some well-known
limitations in such efforts. For example, the sample size is
typically small, as it is in general very costly to involve
a large population in such studies. More importantly, such
questionnaires in general rely on a participant’s explicit recall
of his/her drug-use behavior, which could be a limiting factor
on its own (e.g., issues like incorrect memory or intentional
omission of some facts).
The phenomenal growth of social media and smart mobile
devices has led to more and more drug-use-related data

appearing online. For example, there are many drug-related
discussion groups on Facebook, many drug-use-related questions asked and answered on Yahoo!Answers, as well as many
drug-related tweets on Twitter.
Such user-generated social media may be collected at a
much larger scale (than an explicit user survey) and thus have
the potential of offering realistic insights into understanding
of substance-use behaviors, their situational factors, and social
contexts. A few recent efforts illustrate this nicely. In [5],
Christine Lee et al. found that the substance-use related
behaviors have similar patterns in data from traditional surveybased approaches and those from social media. In [6], Jennifer
Whitehill et al. studied the relationship between mobile usage
of social networking sites (e.g. Facebook and Twitter) and the
alcohol use in a large street festival. In [7], Joris Hoof et al.
conducted one study on analyzing Facebook profiles to show
that some Facebook profile elements can be the indicators of
real-life behaviors. In [8], Sarah Stoddard et al. examined the
influence of young people’s social networking behaviors on
their alcohol and other drug use.
While having demonstrated to some extent the potential of
using social media for substance-use research, these existing
efforts also revealed the challenges of building computational
models for analyzing largely-unstructured social-media. For
example, some user attributes that may be readily available
from an explicit survey now need complex inference strategies
to figure them out. Further, any approach that relies on
training from some labeled dataset cannot be easily extended
to large-scale analysis. In this paper, we address some of
these challenges in the context of illicit marijuana use and its
manifestation on Twitter. Specifically, we propose one semisupervised approach to studying the user behaviors of the
illicit marijuana use using noisy, unstructured and large-scale
Twitter data. To our knowledge, this is the first work to study
marijuana use behaviors using large-scale Twitter data.
II. R ELATED W ORKS
In this section, we briefly review some related work on
study of use of marijuana and other substance, including both
traditional methods of recruiting participants and more recent
approaches using social media data.

IEEE/ACM ASONAM 2016, August 18-21 2016, San Francisco, CA, USA
978-1-5090-2846-7/16/$31.00 ©2016 IEEE

447

A. Participant-recruitment Based Research

•

Johnston et al. conducted follow-up surveys on young adults
regarding their behaviors related to drug use in [9]. Similar
recruitment based approaches were also used to study the
effect of marijuana use in adolescent on their depressive
symptoms and IQ development [10] [11].
As noted earlier, these population-survey-based efforts are
usually very time-consuming merely for the stage of data
collection. Another point to note is that the above-mentioned
efforts focused more on finding features or trends from the data
rather than developing computational approaches for modeling
user behaviors.

•

•

Class One: Tweets in this class are related to personal
recreational use of marijuana. They are posted by individual users instead of some official accounts (for example,
those for newspaper, companies, or medical institutes).
Class Two: In this class, all tweets are related to marijuana but not in the sense of recreational use. For
instance, they may discuss the medical or prescription use
of marijuana, or report some news involving marijuana.
Class Three: This is for those tweets having no identifiable relationship with marijuana use.

Figure 1 illustrates several real examples for each of the three
classes defined above.

B. Research Using Social Media
The non-medical use of Adderall (one psychostimulant
drug) among college students using Twitter were studied in
[12], where the frequencies, percentages and means were
analyzed, and the experiments showed that their findings
were similar to traditional survey-based methods. To study
the smoking behavior on Twitter, Myslin et al. collected
tweets from Twitter and performed content and sentiment
analysis [13]. Cavazos-Rehg et al. also performed content
analysis of tweets but with a pro-marijuana Twitter handle
(@stillblazingtho) plus the demographics of the handle’s followers [14]. Volkow et al. reported risks of the recreational
use of marijuana like the risk of addiction, effect on brain
development, relation to mental illness and so on in [15].
Krauss et al. studied the hookah smoking behavior on Twitter
in [16]. Leah et al. reported their research on how posts on
Twitter changed after legalizing recreational use of marijuana
in two states [17]. Katsuki et al. studied the youth non-medical
use of prescription medications (NUPM) on Twitter in order
to model the frequency of NUPM-related tweets and identified
the illegal access to drug abuse via online pharmacies in [18].
While demonstrating the great potential of using social
media for substance-use-related analysis, these existing efforts
have yet to be extended to Web-scale data. In particular,
we have not seen specific computational models for analyzing Web-scale Twitter data for understanding marijuana-userelated behaviors.
III. P ROBLEM D EFINITION
To study the behavior of marijuana users on Twitter, a
fundamental problem is to identify tweets that are related to
some underlying users who use marijuana. This problem is
more subtle than it appears. For example, one cannot simply
rely on using the keyword “marijuana” to search the tweets for
solving the problem. There are several complicating factors.
First, many “street names” are used to describe marijuana.
Second, there may be many tweets that involve medical or
research-oriented references to marijuana but they are not at
all useful for a study on illicit marijuana use. Considering these
factors, we propose to classify a tweet into one of following
three categories:

Fig. 1: Demos to show three classes: (a) is for Class One, (b)
is for Class Two and (c) is for Class Three.
Various text-based features may be extracted for the task
of classifying the tweets. Also, as evident from the related
work, it is important to consider social interactions among the
underlying users. Furthermore, all these features are not mutually independent, and their intricate correlation may provide
additional evidence for improved classification. Considering
these, and with the goal of classifying large-scale tweets
in mind, we now discuss our overall approach, which is
illustrated in Figure 2. In the approach, we first extract a
set of basic features from each tweet. Then, utilizing a small
labeled training set, we learn a good feature mapping that
takes into consideration both some basic features and their
interactions, based on weakly-hierarchical lasso. The learned
feature mapping model is used to process the large-scale data.

448

where the ith data point is xi ∈ R1×d , i ∈ {1, · · · , N } which
is normalized, and its label is yi ∈ {1, 2, 3} and the coefficient
to learn is w ∈ Rd×1 . In this paper, the discriminant function
is chosen to be one-vs-one linear SVM. The implementation
details are provided as follows. We first train one linear
regression model by optimizing Eqn.2.
1
(2)
min kXw − yk22 + kwk22
w
2
where X ∈ RN ×d and y ∈ RN ×1 . Then we apply one-vs-one
linear SVM to s = Xw ∈ RN ×1 to find the label for each
tweet.
N
X
min kvk22 + C
ξi
(3)

#
$%&
(
(

*

)

+
'

(
!

"

+

(
)

Fig. 2: It shows the entire framework of our methodology.

v

In the following, we first present the basic set of features
designed for our task. These features are extracted from either
the content of the underlying tweets or the social interactions
among the corresponding users, as elaborated below.
A. Content-based Features
•

•

•

•

The length of the tweet: For each tweet, its length can be
one useful feature. For example, the tweets from ordinary
users may be generally shorter than those from official
accounts.
Favorite Count & Retweeted Count: It shows how many
people think the tweet is favorite and the number people
who retweet this post. This is in general useful for
measuring how influential the tweet is.
The number of Hash-Tags: This calculates how many
trends one tweet mentions. Our original dataset were
obtained by crawling using selected street names of
marijuana. The tweets with more trends are likely to be
classified as Class Three or Two, instead of Class One.
TF-IDF on Unigram: Unigram is one common feature
used to capture characteristics of one tweet. We build
TF-IDF for unigrams of each tweet and use it as one
feature.

B. User-based Features
•

Number of followings and followers: Each user on Twitter can follow others or be followed. However for some
official accounts or famous people, they are likely to have
a smaller number of followings but a large number of
followers. These users are unlikely to post tweets related
to personal and recreational use of marijuana.
Number of Tweets: This records how many tweets one
user has already posted, capturing the level of Twitter
activity of the user.

i=1

s.t. yi (si ∗ v + b) ≥ 1 − ξi ∀i

(4)

where ξ is non-negative.
However, in practice, the linear model is inadequate for
capturing the high degree of non-linearity that typically exists
in our problem, which has been shown in our experiments.
To allow some level of nonlinearity while maintaining computational efficiency, we introduce to the problem 2nd -order
interaction terms with a weakly hierarchical structure, as
described in [19][20]. The resultant model is given in Eqn.5.
y = f (z)
z = xw +

(5)
1
2

d X
d
X
i

xi xj Qi,j

j

where z is called the z-term of x (for simplicity) and the
discriminant function f (·) is given in Eqn.3 (one-vs-one linear
SVM in this paper) and xi is the ith dimension of the data
point x and Qi,j ∈ R is the coefficient for the interaction
between ith and j th dimensions of the feature space.
To solve the classification problem under this new model,
we formulate the following optimization problem in Eqn.6.
1X
λ3
min
(f (zi , v) − yi )2 + λ1 kwk1 + kQk1
(6)
w,v,Q
2 i
2
s.t. kQ.,j k1 ≤ |wj | for j = {1, · · · , d}
where
zi is the z-term of xi as defined in Eqn.5, kQk1 =
P
i,j |Qi,j | and v is the model parameter of the discriminant
function (the one-vs-one linear SVM).

IV. L EARNING F EATURE M APPING F ROM A S MALL
DATASET
Considering the computational efficiency needed for processing Web-scale data, we may employ a linear classifier as
the baseline for doing the classification, as given by Eqn.1.

A. Solving the Optimization Problem
Solving Eqn.6) directly is difficult. Hence we simplify this
optimization problem by a two-step process: We first learn
parameters w and Q and then learn the model parameter v of
the discriminant function.
For parameters w and Q, we model them as one regression
model as Eqn.7 when we do not consider the discriminant
function.
λ3
1X
(zi − yi )2 + λ1 kwk1 + kQk1
(7)
min
w,Q
2 i
2

yi = f (xi w)

s.t. kQ.,j k1 ≤ |wj | for j = {1, · · · , d}

•

(1)

449

where zi is the z-term of xi as defined in Eqn.5. Then after
w and Q are obtained, we learn v of the discriminant function
by optimizing Eqn.3.
Converting Eqn.6 into Eqn.7 and Eqn.3 allows us to solve
the original optimization problem. By solving Eqn.7 and
Eqn.3, we can obtain the model parameters v, w and Q which
satisfy the original problem (Eqn.6) as well. However, since
we add more constraints on these parameters in the process
of simplification, the obtained v, w and Q are only the local
optima of Eqn.6.
While the details for solving Eqn.7 can be found in [19],
a brief description is given below. From Eqn.7, we can see
that this optimization problem is non-convex because of the
existence of constraints, and as a result, we cannot solve
it using convex optimization approaches. Thus in [19], one
convex relaxation by setting w = w+ − w− is given, where
w+ and w− are nonnegative. The convex relaxation version is
given as Eqn.8.

interaction, the dataset representation is converted as {x̃i , i ∈
{1, · · · , N }} where x̃i is given by Eqn.11.

λ3
1X
(ẑi − yi )2 + λ1 (w+ + w− ) + kQk1 (8)
2 i
2

In this section, we evaluate the performance of our approach
with comparison with several typical existing methods.

min

w+ ,w− ,Q

s.t. kQ.,j k1 ≤ wj+ + wj−

(9)

Pd Pd
where ẑi = xi · (w+ − w− ) + 12 j k xi,j xi,k Qi,j . A lot of
convex optimization approaches can be used to solve Eqn.8,
such as FISTA [21].
After we obtain the parameters w and Q, the original
problem will become equivalent to the support vector machine
which can be solved using sequential minimal optimization.
V. C LUSTERING WITH T HE L EARNED F EATURE M APPING
A supervised approach cannot be directly applied to Webscale datasets as manually-labeled data are in general in a
much smaller scale. A semi-supervised approach would rely
on unsupervised clustering to first identify the structures of
the data and then employ a small amount of labeled data to
annotate the structures. For example, using K-means clustering, we can group a dataset into different clusters. For data
points in each cluster, if we assume that they have the same
labels, we can randomly select a small number of data points
for labeling and then use the labels to annotate the clusters.
Assuming k groups in a dataset, a basic K-means algorithm
is equivalent to solving the following problem (Eqn.10):
min
πj ,j∈{1,··· ,k}

k X
X

kxv − cj k22

(11)

where the element at (j, k) in the matrix Ri is the product of
the j th and k th dimension which is xi,j xi,k . It is easy to see
2
x̃i ∈ R1×(d+d ) . For the new representation, the interaction
of the feature dimension is captured by parameters w and Q
which are learned from the small labeled dataset (see Section
IV). By treating the learned parameters as a kernel, we can
have the new clustering as Eqn.12.
min
πj ,j∈{1,··· ,k}

k X
X

(x̃v − cj )M (x̃v − cj )T

(12)

j=1 v∈πj

where the learned metric matrix M = diag((w; vec(Q))) ∈
2
2
R(d+d )×(d+d ) .
VI. E XPERIMENTS

A. Dataset Construction

for j = {1, · · · , d}

wj+ , wj− ≥ 0 for j = {1, · · · , d}

x̃i = (xi , vec(Ri ))

(10)

j=1 v∈πj

where cj is the j th centroid and πj is the j th cluster.
As we have presumably found a feature mapping scheme
in the previous section by maximizing classification accuracy
for the labelled data, it is natural to use the learned feature
mapping for the clustering stage. Denote the dataset as {xi , i ∈
{1, · · · , N }}. Consider the influence of the 2-order feature

For constructing a small labelled dataset, instead of crawling
random tweets online, we first use a list of keywords as
one filter to remove unrelated tweets. These keywords are
defined based on several Web sources and some government
documents1 .
The final keyword list was determined to be: marijuana,
weed, blunt, cannabis, pot, reefer, buds, 420, mary jane, blaze.
With the final list, the Twitter API 2 is utilized to crawl data.
The time period we crawled is from January 09 to January 15
in 2016 and all tweets are in English. We crawled a total of
1,166,441 tweets. Among these we randomly labeled 10,000
with comparable proportion for each class (see Table 1 for
exact composition in terms of class labels). This small labelled
dataset was annotated by two people reading the tweets to
decide their labels.
B. Learning the Feature Mapping
In this part, to compare with commonly used classifiers
like linear classifier (Eqn.1) and linear SVM, we split the
10,000 tweets randomly into two parts: training set of 8,000
tweets and testing set with 2,000 tweets. Since in the our
approach, we need to compute the feature interaction terms
which is defined as the z-term in Eqn.5, we have to reduce the
dimension of the original feature vectors. In this experiment,
we use LDA [22] to do dimension reduction of TF-IDF of
Unigram in the feature sets for our approach. For random
guess, we randomly assign one label to every data point and
then compute the accuracy based on Eqn.13.
PNt
I(yi == ŷi )
(13)
e = i=1
Nt
1 In this paper, we use this forum (www.rehabs.com) and this official
document(https://vva.org/wp-content/uploads/2014/12/street-terms.pdf).
2 https://dev.twitter.com/rest/public

450

where yi is the ground-truth label of the tweet xi and ŷi is
the predicted label and
(
1 if x is true
(14)
I(x) =
0 otherwise
The experiment results are shown in Table II and The
confusion matrix of our approach is shown in Table I.
RG
0.326

LC
0.462

SVM
0.677

Ours
0.976

TABLE II: The table shows the performance of each baseline
and our method. RG: random guess; LC: linear classifier;
SVM: linear SVM.
From Table. II, we can easily see that our algorithm stands
out. Compared with the modified linear classifier (Eqn. 1 and
Eqn. 3) with our algorithm, the difference is that we consider
the interaction terms (the z-term) defined in Eqn.5. Thus these
results also show that it is necessary to consider feature selection scheme using weakly hierarchical lasso. Furthermore, our
approach performs better than linear SVM. This is also easy
to understand because of the nonlinearity introduced in our
formulation (Eqn. 5). Nonlinearity comes from the z-term.
To further show the performance of each algorithm, the
confusion matrices are shown in Table. I. It shows that our
approach performs best in all of the three classes. From Table
I(a), we can see that LC cannot distinguish Class 1 and Class
2. For example, for Class 2, almost the same number of tweets
are classified into Class 1 and Class 2. The baseline with SVM
performs better than LC, but the error is still significant.
Our approach effectively solves the problem of how
to fuse features and provides the optimal feature selection/combination scheme. It is possible to analyze which
features (or their interactions) are most influential. Table III
shows the top three main factors which affect the classification
performance and their corresponding coefficients. From this
retweet num
4.41e-05

TF-IDF1
4.53e-01

table, it can be seen that the number of retweets and also the
TF-IDF of Unigram play important roles in distinguish these
three classes. We can also see that the content of the tweets
is most important for classification. Based on the results of
Table III, the top interactions are from the two TF-IDF feature
dimensions. This is also demonstrated by the experiment
results (see Table IV).
TF-ID2 * TF-ID2
1.844e-1

k
10
100
200
300
400
500
1000

m̄1
0.411
0.320
0.333
0.318
0.291
0.282
0.239

m̄2
0.647
0.594
0.594
0.602
0.542
0.542
0.495

(a) The baseline

TF-IDF2
3.62e-01

TABLE III: Illustration of top-3 main factors. The second one
and the third one are from TF-IDF of Unigram.

TF-IDF1 * TF-IDF1
-2.258e-1

C. Clustering Structure on the Web-Scale Data
In this part, we apply the learned feature mapping scheme
to the large dataset, which contains not only the labeled data
points but also unlabeled ones. To show the clustering structure
of the partially labeled dataset, we perform two experiments:
one using one baseline which is KMeans and the other one is
our method based on Eqn. 12. For a good clustering outcome,
we assume in each cluster, a majority of data points belong
to the same class. To evaluate the performance of the results,
we present two metrics (Eqn. 15) to show whether any class
is dominant in a given cluster. In each cluster, there may be
three classes with sizes n0 , n1 and n2 (in non-increasing order)
respectively. If one class does not exist, it means its size is
zero.
n1
n2
m2 =
(15)
m1 =
n0
n0
In our experiment, the large dataset is partially labeled and
thus when we compute m1 and m2 , we only consider the
labeled data in each cluster. Then the average is computed for
the entire dataset. These two metrics are presented to measure
what is the difference between the dominant class and the
others. If the values of these metrics are small, then they shows
that compared with the size of the dominant class, the others
are small.
In our experiment, we choose the number of clusters to be
k ∈ {10, 100, 200, 300, 400, 500, 1000}. In this way, we can
learn the effect of the number of clusters on the clustering
performance. The experiment results are shown in Table V.

TF-ID1 * TF-IDF2
7.884e-2

TABLE IV: This table shows top-3 interaction factors and their
corresponding coefficients.

k
10
100
200
300
400
500
1000

m̄1
0.381
0.280
0.240
0.263
0.243
0.228
0.116

m̄2
0.555
0.487
0.436
0.485
0.423
0.427
0.320

(b) our approach

TABLE V: Experiment results on studying the clustering
structure of partially labeled dataset. (a) for the baseline and
(b) for ours. They show the size of the other class compared
with the dominant one.
From Table V, we can see that our clustering approach
by employing the learned feature mapping scheme performs
better than the baseline. As the number of clusters goes up, m̄1
and m̄2 of KMeans and our approach become small, which
means that the percentage of the dominant class becomes large.
Compared with the baseline, the percentage of the dominant
class is much larger since the corresponding metrics’ values
are smaller. The average percentage of the dominant class is
shown in Fig.3.
VII. C ONCLUSION AND F UTURE W ORK
We presented one semi-supervised approach to analysis of
Twitter data related to marijuana use, using web-scale data,

451

C1
C2
C3

C1
0.4616
0.3820
0.1751

C2
0.3108
0.3920
0.3146

C3
0.2276
0.2260
0.5103

C1
C2
C3

(a) LC: modified linear classifier

C1
0.6060
0.1820
0.1259

C2
0.1508
0.6120
0.0780

C3
0.2432
0.2060
0.7962

(b) SVM: linear SVM

C1
C2
C3

C1
0.9831
0.0020
0.0014

C2
0.0130
0.9920
0.0410

C3
0.0039
0.0060
0.9576

(c) Ours: our approach

TABLE I: Three confusion matrices for three algorithms: LC(a), SVM(b), Ours(c).

1
baseline
Ours

0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
0

100

200

300

400

500

600

700

800

900

1000

1100

Fig. 3: It shows the average percentages of the dominant
class plotted based on the experiment result at each k ∈
{10, 100, 200, 300, 400, 500, 1000}

which includes: learning the optimal feature mapping scheme
and grouping the entire data using an improved clustering
algorithm. The experimental results demonstrated the effectiveness and efficiency of our approach.
There are still some limitations we need to work on. For
example, when we learn the feature mapping scheme, we
relax the problem to be one easier one, and thus the learned
parameters are only locally optimal. Another problem is how
to incorporate features reflecting temporal patterns of user
behaviors.
ACKNOWLEDGMENT
This work was supported in part by a grant (#1135616) from
National Science Foundation. Any opinions expressed in this
material are those of the authors and do not necessarily reflect
the views of the NSF.
R EFERENCES
[1] U. D. o. J. N. D. I. Center, “The economic impact of illicit drug use on
american society,” Product No. 2011-Q0317-002, 2011.
[2] S. Abuse and M. H. S. Administration, “Results from the 2013 national
survey on drug use and health: Summary of national findings,” NSDUH
Series H-48, vol. 14, no. 4863, 2014.
[3] K. J. Quintelier, K. Ishii, J. Weeden, R. Kurzban, and J. Braeckman,
“Individual differences in reproductive strategy are related to views
about recreational drug use in belgium, the netherlands, and japan,”
Human Nature, vol. 24, no. 2, pp. 196–217, 2013.
[4] J. C. A. Lacson, J. D. Carroll, E. Tuazon, E. J. Castelao, L. Bernstein,
and V. K. Cortessis, “Population-based case-control study of recreational
drug use and testis cancer risk confirms an association between marijuana use and nonseminoma risk,” Cancer, vol. 118, no. 21, pp. 5374–
5383, 2012.

[5] C. Lee, “Recruitment through social networking sites: Are substance use
patterns comparable to traditional recruitment methods?” in Medicine 2.0
Conference. JMIR Publications Inc., Toronto, Canada, 2014.
[6] J. M. Whitehill, M. A. Pumper, and M. A. Moreno, “Emerging adults
use of alcohol and social networking sites during a large street festival:
A real-time interview study,” Substance abuse treatment, prevention, and
policy, vol. 10, no. 1, p. 1, 2015.
[7] J. J. van Hoof, J. Bekkers, and M. van Vuuren, “Son, youre smoking
on facebook! college students disclosures on social networking sites as
indicators of real-life risk behaviors,” Computers in human behavior,
vol. 34, pp. 249–257, 2014.
[8] S. A. Stoddard, J. A. Bauermeister, D. Gordon-Messer, M. Johns, and
M. A. Zimmerman, “Permissive norms and young adults alcohol and
marijuana use: The role of online communities,” Journal of Studies on
Alcohol and Drugs, vol. 73, no. 6, pp. 968–975, 2012.
[9] L. D. Johnston, Monitoring the Future: National Survey Results on Drug
Use, 1975-2008: Volume II: College Students and Adults Ages 19-50.
DIANe Publishing, 2010.
[10] R. M. Schuster, R. Mermelstein, and L. Wakschlag, “Gender-specific
relationships between depressive symptoms, marijuana use, parental
communication and risky sexual behavior in adolescence,” Journal of
youth and adolescence, vol. 42, no. 8, pp. 1194–1209, 2013.
[11] N. J. Jackson, J. D. Isen, R. Khoddam, D. Irons, C. Tuvblad, W. G.
Iacono, M. McGue, A. Raine, and L. A. Baker, “Impact of adolescent
marijuana use on intelligence: Results from two longitudinal twin studies,” Proceedings of the National Academy of Sciences, p. 201516648,
2016.
[12] C. L. Hanson, S. H. Burton, C. Giraud-Carrier, J. H. West, M. D.
Barnes, and B. Hansen, “Tweaking and tweeting: exploring twitter for
nonmedical use of a psychostimulant drug (adderall) among college
students,” Journal of medical Internet research, vol. 15, no. 4, 2013.
[13] M. Myslı́n, S.-H. Zhu, W. Chapman, and M. Conway, “Using twitter
to examine smoking behavior and perceptions of emerging tobacco
products,” Journal of medical Internet research, vol. 15, no. 8, 2013.
[14] P. Cavazos-Rehg, M. Krauss, R. Grucza, and L. Bierut, “Characterizing
the followers and tweets of a marijuana-focused twitter handle,” Journal
of medical Internet research, vol. 16, no. 6, 2014.
[15] N. D. Volkow, R. D. Baler, W. M. Compton, and S. R. Weiss, “Adverse
health effects of marijuana use,” New England Journal of Medicine, vol.
370, no. 23, pp. 2219–2227, 2014.
[16] M. J. Krauss, S. J. Sowles, M. Moreno, K. Zewdie, R. A. Grucza, L. J.
Bierut, and P. A. Cavazos-Rehg, “Peer reviewed: Hookah-related twitter
chatter: A content analysis,” Preventing chronic disease, vol. 12, 2015.
[17] L. Thompson, F. P. Rivara, and J. M. Whitehill, “Prevalence of
marijuana-related traffic on twitter, 2012–2013: a content analysis,”
Cyberpsychology, Behavior, and Social Networking, vol. 18, no. 6, pp.
311–319, 2015.
[18] T. Katsuki, T. K. Mackey, and R. Cuomo, “Establishing a link between
prescription drug abuse and illicit online pharmacies: Analysis of twitter
data,” Journal of medical Internet research, vol. 17, no. 12, 2015.
[19] J. Bien, J. Taylor, and R. Tibshirani, “A lasso for hierarchical interactions,” Annals of statistics, vol. 41, no. 3, 2013.
[20] Y. Liu, J. Wang, and J. Ye, “An efficient algorithm for weak hierarchical lasso,” in Proceedings of the 20th ACM SIGKDD international
conference on Knowledge discovery and data mining. ACM, 2014, pp.
283–292.
[21] A. Beck and M. Teboulle, “A fast iterative shrinkage-thresholding algorithm for linear inverse problems,” SIAM journal on imaging sciences,
vol. 2, no. 1, pp. 183–202, 2009.
[22] Q. Gu, Z. Li, and J. Han, “Linear discriminant dimensionality reduction,”
in Machine Learning and Knowledge Discovery in Databases. Springer,
2011, pp. 549–564.

452

A TWO-STAGE APPROACH TO SALIENCY DETECTION IN IMAGES
Zheshen Wang, Baoxin Li
Dept. of Computer Science & Engineering
Arizona State University, Tempe, AZ 85287, USA
ABSTRACT
Researches in psychology, perception and related fields
show that there may be a two-stage process involved in
human vision. In this paper, we propose an approach by
following a two-stage framework for saliency detection. In
the first stage, we extend an existing spectrum residual
model for better locating visual pop-outs, while in the
second stage we make use of coherence based propagation
for further refinement of the results from the first step. For
evaluation of the proposed approach, 300 images with
diverse contents were manually and accurately labeled.
Experiments show that our approach achieves much better
performance than that from the existing state-of-art.
Index Terms— Image Processing, Image Analysis,
Object detection, Pattern Recognition
1. INTRODUCTION
Studies [1] in psychology and cognition fields have found
that, when looking at an image, our visual system would
first quickly focus on one or several “interesting” regions of
the image before further exploring the contents. These
regions are often called salient regions. Visual saliency is in
general too subjective a concept to be strictly defined since
it is closely related to the viewer’s personal preferences,
experiences, intentions (e.g., a specific searching task), and
etc. Nevertheless, there exist some simple principles that
underpin the process of selecting salient
regions. For example, when one is
shown the image in Figure 1 without
being given any instruction, in general
the attention will be immediately caught
by the bar in column 2 and row 3, since
Figure 1.
its orientation is quite different from
others. In this paper, our study is focused on the detection of
this type of saliency that is task, experience and preference
independent.
Many efforts have been devoted to saliency detection [37]. Most task independent research follows a bottom-up
framework [3] and is often based on searching for regions
with maximum local contrast of color, intensity, orientation,
etc [5,6]. Some recent work started to seek regional and
global features, such as the contrast of region histograms
and spatial distributions of colors [4]. [7] proposed a new

1-4244-1484-9/08/$25.00 ©2008 IEEE

965

spectrum residual method which is able to locate visual
“pop-outs” very quickly through capturing “noise” in the
logarithmic magnitude-frequency curve of a given image.
This idea is very simple and seemingly effective. However,
it suffers from several drawbacks when it is directly used
for saliency detection.
In this paper, we propose a two-stage approach for
saliency detection, which is inspired by the typical twostage processing in human visual system. In the first stage,
we extend the spectrum residual method [7] by introducing
an automatic channel selection module and a decision
reversal module. In the second stage, we propagate the
potentially incomplete salient regions based on their
similarity and proximity, which are among the basic Gestalt
grouping principles in visual perception. To evaluate the
performance of our approach, we accurately labeled 300
images from the image database of [4] as the ground-truth
and calculated the average precision, recall and F-measure
of our approach and compared with other methods. Results
show that our method achieves much better performance
than the existing approaches, suggesting that our two-stage
approach is a promising model for saliency detection.
2. TWO STAGES OF VISUAL PROCESSING
Many researches in psychology, perception and cognition,
and neuroscience indicate that the human visual system
follow two sequential stages in visual perception: The first
stage, called pre-attentive stage, processes all the
information available fast but coarsely, while the second
stage (named focused attention stage) processes only part of
the input information with more intensive efforts of
exploration [8, 9].
In our approach, we attempt to follow the same
procedure: In the first stage, we use a method based on the
spectrum residual model [7] to quickly locate the visual
pop-outs from the entire image. In this stage, the algorithm
extracts only coarse “unusual” regions. In the second stage,
our approach takes Gestalt features—similarity and
continuity into consideration and propagates the result from
the first stage based on local coherence so as to capture
some details that are missed in the first stage. These two
stages are described in detail in Sections 3 and 4
respectively.

ICASSP 2008

3. COARSE SALIENT REGIION DETECTION
BASED ON THE SPECTRUM RESIDUAL MODEL
It has been shown that natural images have a spectrum with
the amplitude A( f ) obeying the so-called 1/f law [11,12],
as illustrated in Figure 2-a,b. On a log-log scale, the
frequency-orientation averaged amplitude curve lies
approximately on a straight line (Figure 2-(c)). When
represented in frequency-log(amplitude) scale, it becomes a
curve with similar trends for any natural images (Figure2(d)).

(a)
(b)
(c)
(d)
Figure 2. An example of an image and its Fourier spectrum
curves: (a) Original image; (b) frequency-orientation averaged
amplitude curve; c. (b) on log-log scale; d. frequencylog(orientation averaged amplitude) curve;

It was argued in [7] that the local spiky parts in the
frequency-amplitude curve correspond to those sharp
changes in the original image, which may be used for
saliency detection. Following the basic steps from [7], we
first compute the Fourier Transform of an input image, and
then take the difference between the transform and its
smoothed version (amplitude only). The residual is used in
conjunction with the original phase to compute an inverse
Fourier transform, which is smoothed with a Gaussian filter
to obtain a saliency map. While this simple procedure has
been reported to give better performance than Itti’s classic
framework [3], there are two significant issues that have not
been addressed. Firstly, when applying the method to color
images, one faces the problem of how to use the spectrum
residual model properly. For example, would it be sufficient
to process only the luminance channel, or is it effective to
process the R,G,B channels separately? Secondly,
depending on the contents of the image, often the spectrum
residuals may actual correspond to the background rather
than to the (foreground) salient region. We term this as the
“saliency reversal” problem. In the following two
subsections, we propose two techniques to address these
issues respectively.
3.1. Channel Selection
If a region in an image is deemed as a salient region, at least
one of its visual channels should be different from the rest.
In this study we consider the HSV color space. If we only
capture one of the channels and the actual contrast mainly
resides in other channels, the algorithm would fail. Figure 3
shows an example: (a) is the original image. If we do
saliency detection in the hue channel, we can get (c) in
which the salient red region is preserved (The brighter, the
more salient). If the salient detection is only from the

966

luminance channel, the result would almost lose its target, as
in (d). (Our experiments have shown that for the hue or the
saturation channel of natural images, the frequencylog(amplitude) curves have similar nice properties of that
from the luminance channel.)

(a)
(b)
(c)
(d)
Figure 3. An example of channel selection: (a) Original image; (b)
Gray (Intensity) image of (a); (c) Saliency map for hue channel;
(d) Saliency map for intensity channel.

To this end, we designed an automatic technique to
select the most effective channel. We first compute the
saliency maps for each of the three channels: intensity, hue,
and saturation. We can get three saliency maps in which
each pixel has a saliency score within [0,1]. Then we use kmeans clustering for binary clustering, the initial centroids
are set as 0 (for non-salient) and 1 (for salient region). We
further select the saliency map with the largest distance
between two centroids.
EffectiveC hannel arg max ( centroid 1 x  centroid 2 x )
x

where x can be the hue, the saturation or the intensity
channel. This is based on the assumption that the contrast
between the salient and non-salient regions should be
maximized before saliency can be detected effectively. This
strategy also effectively help us to avoid the problem of
selecting optimal thresholds for segmenting salient and nonsalient regions, since segmentation is implicitly done by the
clustering step. In our experiment using 300 images, in
47.7% of the cases the hue channel is selected, with 33.7%
for saturation and 18.7% for intensity.
There may be a potential problem in applying the
spectrum residual model to the hue channel: the hue is
typically represented as angles; when applying Fourier
transform to the angles, we need to set a cutoff point as the
zero point. No matter where we set the cutoff, colors in the
two sides of the cutoff would have the largest difference (in
terms of their angular values) although they are very close
colors to each other. This might suggest a potential problem
to the spectrum residual model, since two close colors in a
smooth region would have extremely different values and
thus depict sharp changes. However, we found that if the
problem appears within a salient region, it would even
somewhat help saliency detection, since an original smooth
part of the salient region may pop out more due to the extra
frequency introduced by the cutoff of the hue band.
Although, if the cutoff point lies in the non-salient region, it
may generate false target, it is unlikely for both the salient
and non-salient regions to share a lot of common colors, and
thus again this situation would not cause too big a problem.
3.2. Saliency Reversal

As discussed earlier, saliency is
distinguished by contrast of visual
properties. There are two basic
Example 1
cases of contrast patterns: smooth
background with cluttered salient
region (Figure 4-Example1) and
smooth salient region with clutter
Example 2
background (Figure 4-Example2).
Figure 4. Two cases
Unfortunately,
the
spectrum
of salient region,
residual model is only useful for
Example1 and 2 left:
cluttered regions but not for real
original image, right:
salient regions. In cases as Figure
raw saliency map.
4-Example1, they happen to
coincide. However, in cases like Example2, they are
different. To deal with the latter case, we use the following
technique: we reverse the decision based on the spatial
distribution of salient pixels in the raw saliency map. We
calculate the spatial variance as follows:
(ri  ri ) 2  (ci  ci ) 2
¦
iR
,
var( R)
size( R )
­Yes, var(background )  E u var( rawsalientregion)
Inverse

®
¯ No, otherwise

people tend to organize visual elements into groups or
unified entities when certain principles are applied [10].
Main Gestalt grouping principles include similarity,
continuation, closure and proximity. These principles
describe visual coherencies from different aspects. Some
existing work has developed computable descriptors for
each principle [13]. Since our test images are highly diverse
and thus some of the principles may not apply (e.g., closure),
we experimented with only the most general principles—
similarity and proximity, and designed the following
method to further process the results from the first step.
In order to retrieve missing regions that should form a
unified entity with the extracted regions from the saliency
map, we design the following propagation strategy based on
similarity and proximity.
(1) Select the largest saliency component in the raw
saliency map. (We assume that we are only looking for the
largest salient region.). Let’s call it “SR”.
(2) Divide “SR” into blocks. Find the most representative
block “RB”, for each block “x” and “y” in “SR”, we
defined “RB” as the block in “SR” which has the most
similar blocks in “SR”:
RB arg max (CountNum ( dist ( x, y )  threshold )) .
xSR

Here, R can be a raw salient region or the background; i is a
pixel in R; ri and ci are row and column coordinates
respectively; ri and ci denote the average row and column
coordinates of all pixels in R; size( R) returns the total pixel
number in R. E is a threshold in (0,1] . In all the experiments
on this paper, E is fixed to be 0.85, which works well
across all the images. This technique would inverse the raw
saliency map when the variance of the original background
part is much smaller than that of the raw salient region.
Experiments show that this strategy is quit effective in
dealing with the problems illustrated earlier.
4. SALIENCY MAP REFINEMENT BASED ON
GESTALT GROUPING PRINCIPLES
After the first stage,
we can only get the
rough locations and
regions of saliency.
(a)
(b)
(c)
(d)
For
example,
in
Figure 5 An example of partially
Figure 5-c the top of
detected: (a) Original image; (b)
the tower is missed in
Raw saliency map; (c) Binary raw
the
binary
raw
saliency map; (d) Saliency map after
saliency
map.
propagation.
Naturally, it is desired
to have the entire tower detected as a single salient entity.
While the top of the tower is not as conspicuous as the
middle part, a second stage of the visual processing would
group it with the body of the tower. Gestalt refers to
theories of visual perception which attempt to describe how

967

ySR

(3) Find all the neighboring blocks “NB”of “SR” (We use
8 neighbors here.) and compare each of them with “RB”.
If dist ( NB, RB )  threshold , add this block to “SR”.
(4) Repeat (3), until no new blocks can be added to “SR”.
(5) Output “SR” as the propagated saliency region.
We measure the distance between two blocks in HSV
color space:
dist ( x, y )

( H x  H y ) 2  ( S x  S y ) 2  (V x  V y ) 2

Ai ( A H , S ,V ; i x, y ) denotes the average value of
channel A in block i.
5. RESULTS, EVALUATION AND ANALYSIS
In our experiment we use the first 300 images from [4] and
manually labeled the salient regions accurate-to-contour.
We calculate average precisions, recalls and F-measure and
further compare our results with Itti’s bottom-up framework
(Codes downloaded from http://www.saliencytoolbox.net)
and intensity only spectrum residual model.

5.1. Image Database, Ground-truth and Performance
Measurements
[4] established a huge image database for saliency detection
and supplied the ground-truth based on bounding boxes.
Such bounding-box-based ground-truth is far from accurate.
As illustrated in Figure 6, apparently, result (b) is much
more precise than (c), however, they may have very close
precisions based on the bounding-box-based ground-truth
(d). If we use ground-truth (e), the difference between (b)
and (c) would be obvious. More accurate ground-truth leads

(a)
(b)
(c)
(d)
(e)
Figure 6. An example of result and different ground-truth: (a)
Original image; (b) Our result; (c) An assumed result; (d)
Ground-truth of [4]; (e) Our ground-truth;

to more reliable evaluations. For this consideration, we have
labeled 300 images accurate-to-contour manually for our
evaluations.
With a ground-truth saliency map G, for any detected
salient region mask A, we use following measurements:
g a / g
Pr ecision
g a / a , Re call

¦
x

x

x

¦

¦

x

x

x

x

x

¦

x

x

F-measure is the weighted harmonic mean of precision and
recall, with a non-negative D :
FD

(1  D ) u Pr ecision u Re call
D u Pr ecision  Re call

where D is set to be 0.5 as done usually. If both the
precision and the recall are zero, we simply set FD to zero.

5.2 Results and Comparisons

Original images

Raw saliency maps

Final saliency region

Ground-truth
Figure 7. Some examples, results and ground-truth

rough location and region of the salient objects, but also
roughly keep the contours right. Table 1 shows the statistics
Average
Precision

In this paper, we proposed a two-stage approach for visual
saliency detection. For the first state, we extended the
spectrum residual model of [7] by introducing automatic
channel selection and decision reversal. In the second stage,
we develop a coherence propagation strategy based on two
basic Gestalt principles. We manually labeled 300 images
accurate to contour as the ground-truth for evaluations.
Experiments show that our approach performs much better
than state-of-art methods, suggesting that this is a promising
model for saliency detection.

7. REFERENCES

Figure 7 shows sample results from our approach. We can
see that, the final saliency regions not only capture the

Method

higher recall, but its precision is still low. Stage I of our
approach doubles the original precision and increases the
recall by 6 percentages at the same time and also has a much
higher F-measure value, due to the proposed extensions. By
adding the second stage, precision decreases a little (less
than 1 percentage), but recall and F-measure rise by more
than 4 percentages and 1 percentage respectively. In
summary, our two-stage approach achieves much better
performance than Itti’s framework and the original spectrum
residual approach.
6. CONCLUSIONS

Average
Recall

Average
F-measure

Itti’s bottom-up
0.5445
0.1825
0.3052
framework
Original spectrum
0.3013
0.5944
0.3453
residual model
Stage I only
0.6230
0.6571
0.5986
Two-stage
0.6162
0.7043
0.6122
approach
Table 1 Comparisons of results among different methods

of results from our approach and other methods. For such a
challenging image database, classic bottom-up framework
has a very poor recall. Intensity-only method has a much

968

[1] VF Leavers, “Preattentive computer vision: towards a twostage computer vision system for the extraction of qualitative
descriptors and the cues for focus of attention”, Image and Vision
Computing, 12(9): 583-599, 1994.
[2] Jeremy M. Wolfe, Todd S. Horowitz, “What attributes guide
the deployment of visual attention and how do they do it?”, Nature
Reviews of Neuroscience, Nature publishing Group, 5: 1-7, 2004.
[3] L. Itti and C. Koch, “A Saliency-Based Search Mechanism for
Overt and Covert Shifts of Visual Attention”, Vision Research,
40(10-12): 1489-1506, 2000.
[4] T. Liu, J. Sun, N. Zheng, X. Tang and H. Shum, “Leanring to
Detect A Salient Object”, CVPR, 2007.
[5] L. Itti and C. Koch, “Computational Modeling of Visual
Attention”, Nature Reviews Neuroscience, 2(3): 194–203, 2001.
[6] L. Itti, C. Koch, E. Niebur, et al., “A Model of Saliency-Based
Visual Attention for Rapid Scene Analysis.” IEEE Transactions on
Pattern Analysis and Machine Intelligence, 20(11):1254–1259,
1998.
[7] X. Hou, L. Zhang, “Saliency Detection: A Spectral Residual
Approach”, CVPR, 2007.
[8] A. H. C. van der Heijden, “Two Stages in Visual Information
Processing and Visual Perception?”, Visual Congnition, 3 (4):
325–361, 1996.
[9] Neisser, U. Cognitive psychology, Appleton-Century-Crofts,
New York, 1967.
[10] Desolneux A., Moisan L. Morel J.-M, “Computational
Gestalts and Perception Thresholds”, Journal of Physiology-Paris,
March 2003, 97(2): 311-324(14), 2003.
[11] D. Ruderman, “The Statistics of Natural Images”,
Network:Computation in Neural Systems, 5(4): 517–548, 1994.
[12] A. Srivastava, A. Lee, E. Simoncelli, and S. Zhu, “On
Advances in Statistical Modeling of Natural Images”, Journal of
Mathematical Imaging and Vision, 18(1): 17–33, 2003.
[13] S.Bileschi, L.Wolf, “Image representations beyond histograms
of gradients: The role of Gestalt descriptors”, CVPR, 2007.

Signal Processing: Image Communication 39 (2015) 264–279

Contents lists available at ScienceDirect

Signal Processing: Image Communication
journal homepage: www.elsevier.com/locate/image

Color image demosaicking using inter-channel correlation
and nonlocal self-similarity
Kan Chang a,n, Pak Lun Kevin Ding b, Baoxin Li b
a
b

School of Computer and Electronic Information, Guangxi University, Nanning, Guangxi 530004, China
Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85287, USA

a r t i c l e in f o

abstract

Article history:
Received 1 May 2015
Received in revised form
30 August 2015
Accepted 8 October 2015
Available online 23 October 2015

Color demosaicking is used to reconstruct full color images from incomplete color ﬁlter
array samples captured by cameras with a single sensor array. In reconstructing naturallooking images, one key challenge is to model and respect the statistics of natural images.
This paper presents a novel modeling strategy and an efﬁcient color demosaicking algorithm. The approach starts with joint modeling of the color images, which supports
simultaneous representation of inter-channel correlation and structural information in an
image. The inter-channel correlation is explored by measuring the channel difference
signals in the gradient domain, while the structural information is explored by nonlocal
low-rank regularization. An efﬁcient algorithm is then proposed to solve the joint formulation, by dividing the minimization problem into two sub-problems and solving them
iteratively. The effectiveness of the proposed approach is demonstrated with extensive
experiments on both noiseless and noisy datasets, with comparison with existing state-ofthe-arts color demosaicking methods.
& 2015 Elsevier B.V. All rights reserved.

Keywords:
Color demosaicking
Joint modeling
Inter-channel correlation
Nonlocal self-similarity

1. Introduction
To capture natural images, consumer-grade digital
cameras usually use a single sensor array covered by a
color ﬁlter array (CFA), which allows only one color to be
measured at each pixel. The most commonly used array is
the Bayer pattern [1], which measures the green (G) image
on a quincunx grid and the red (R) and blue (B) images on
rectangular grids [2].
In order to estimate the missing two color values for
each sensor element, color demosaicking (also demosaicing or debayering) is usually applied. It is important
because almost all consumer-grade digital cameras
nowadays use this technique to output full-color-plane
images. The easiest way to ﬁll the missing colors for a
n

Corresponding author.
E-mail addresses: changkan0@gmail.com (K. Chang),
kevinding@asu.edu (P.L.K. Ding), baoxin.li@asu.edu (B. Li).
http://dx.doi.org/10.1016/j.image.2015.10.003
0923-5965/& 2015 Elsevier B.V. All rights reserved.

given pixel is to utilize basic interpolation methods such as
bilinear interpolation or bicubic interpolation. Although in
smooth regions of an image, this kind of nonadaptive
algorithms is able to provide satisfactory results, they
usually fail in textured regions and edges, where artifacts
such as zipper effect become a problem. Therefore, a good
demosaicking algorithm should make use of the distribution of edges to avoid generating these artifacts, e.g., not
interpolating across edges [2]. Moreover, due to the correlation among three color channels, one should not treat
each color image separately when designing the demosaicking algorithm [3]. Based on these two considerations,
many advanced demosaicking algorithms have been proposed. We brieﬂy review 4 kinds of methods as follows:

 Techniques of the ﬁrst kind focus on utilizing the interchannel correlation. For example, in Cok's work [4], the
ratio of samples from different colors, i.e., G/R and G/B,
were interpolated. Hamilton and Adam [5] assumed
that the color difference signals G–R and G–B are

K. Chang et al. / Signal Processing: Image Communication 39 (2015) 264–279







smooth. Gunturk et al. [6] observed that by using an
appropriate wavelet basis, the high-frequency subbands
of the color components are highly correlated, and
hence
proposed
a
projection-onto-convex-sets
technique-based approach. Li [7] believed that the color
difference rule is more suitable for CFA data after
gamma correction than the color ratio rule, and proposed another iterative approach. Recently, Jaiswal et al.
[8] proposed to combine the low-pass ﬁlter-based
method and the color difference interpolation-based
method to generate a more accurate prediction. Different from the standard color difference interpolation,
Kiku et al. [9,10] proposed to interpolate the differences
between observed and tentatively estimated pixel
values by using the guided ﬁlter.
The second kind of techniques locally estimates the
most suitable interpolation direction so as to avoid
interpolating across edges. For instance, Hamilton and
Adams [5] used the second derivative to estimate the
interpolation direction. In Hirakawa's work [11], the
local homogeneity of the image was exploited. For each
pixel, the direction that has the most homogeneous
neighborhood was selected. Menon et al. [12] chose the
interpolation direction according to the smoothness of
the G–R and G–B, where the R or B values are available.
Zhang and Wu [13] proposed an optimal directional
ﬁltering of the G–R and G–B signals by linear minimum
mean square-error estimation (LMMSE) technique.
Another successful class of methods is based on nonlocal self-similarity of the images. Buades et al. [14]
showed that the interpolation artifacts such as zipper
effect can be eliminated by involving image selfsimilarity and redundancy. In their recent work [15],
image similarities were used to reﬁne the locally interpolated image, but channel difference signals were
interpolated instead of the channels themselves. Zhang
et al. [16] exploited the image nonlocal redundancy to
suppress the interpolation errors generated in the initial
demosaicked results. Different from Buades's work [14],
a nonlocal adaptive thresholding (NAT) method rather
than the nonlocal means ﬁltering (NLM) was proposed
to improve the local estimate.
Techniques of the fourth kind regard the demosaicking
problem as reconstructing the full images from the
sampled data, and then make use of prior knowledge
about color images to solve the inverse problem. Saito
and Komatsu [17,18] introduced the total variation (TV)
regularization of color differences and color sums into
an energy functional. When reconstructing an image,
Menon et al. [19] required the smoothness of each
channel signal and the smoothness of the highfrequency components of color difference signals. In
order to improve the regions near edges, an adaptive
technique was also analyzed. The regularization term
proposed by Gao et al. [20] assumed that after using the
Laplacian operator, the band difference images are
sparse. In addition, similar to NAT [16], principal component analysis (PCA) was also used to exploit the
spatial representation. Based on the dictionary learning
theory [21], Mairal et al. [22] assumed that patches in

265

natural images admit a sparse representation over the
learned dictionary.
In addition to the two basic considerations mentioned
before, it is also important to realize that the sensor
readings are usually corrupted by noise. Therefore, the
noiseless assumption by many demosaicking techniques is
often violated in practice. Zhang et al. [23] argued that
denoising should not be put after demosaicking because
the noisy sensor readings are the roots of many color
artifacts in demosaicked images. They then proposed a
PCA-based spatially adaptive denoising algorithm for CFA
data. Besides the “denoising ﬁrst and demosaicking later”
scheme, the other possible strategy is joint demosaicking–
denoising (JDD). Several JDD methods have been proposed,
such as total least square-based method [24], machine
learning-based method [25], LMMSE-based method [26],
space-varying ﬁlters (SVF)-based method [27], and total
variation minimization (TVM)-based method [28].
In this paper, we formulate the color image demosaicking problem as one of reconstructing the full color
images from the noiseless/noisy measurements acquired
by the sensors of digital cameras, and then present a new
and efﬁcient demosaicking scheme. The two main contributions are: (1) we establish joint modeling (JM) for
color images, where inter-channel correlation and structural image priors are explored simultaneously to ensure a
more reliable and robust estimation; (2) we propose an
effective algorithm to solve the inverse problem where the
minimization functional is formulated by using JM under a
regularization-based framework. The inverse problem is
divided into two sub-problems and they are solved iteratively. The ﬁrst sub-problem contains low-rank constraint
of the nonlocal similar patches in an image, while the
other one can be solved by the proposed Split-Bregmanbased algorithm. It should be noted that: (1) since the
sensing matrix could be with any kind of CFA pattern, our
algorithm is color ﬁlter independent; (2) our algorithm
depends on properly modeling for color images, and the
reconstruction does not distinguish demosaicking from
denoising. Therefore, it could be considered as a JDD
method.
The rest of the paper is structured as follows. In Section
2, we formulate the demosaicking problem in a perspective of image reconstruction. Section 3 elaborates the
design of JM for color images. The new functional which
contains regularization terms formed by JM, and the
details of solving the inverse problem are provided in
Section 4. Extensive experimental results and discussion
are given in Section 5, where both noiseless and noisy
datasets are tested. Finally, we summarize this paper in
Section 6.

2. Problem statement
Assume that an original image to be sampled has a size
of p  q, and it is represented as X ¼ ½XTR ; XTG ; XTB T , where
XR , XG , XB are its R, G, B component, respectively. Here we
reshape each color channel of the original image as a
single vector, i.e., XfR;G;Bg A RN , and N ¼ p  q. Let

266

K. Chang et al. / Signal Processing: Image Communication 39 (2015) 264–279

Y ¼ ½Y TR ; Y TG ; YTB T be the CFA mosaicked image. Thus the
procedure of down-sampling a color image can be written
as
Y ¼ ΦX

ð1Þ

where Φ is the sensing (down-sampling) matrix with a
CFA pattern, such as the commonly used Bayer pattern.
Since Y has a much smaller number of pixels than X,
recovering X from Y is a typical ill-posed problem. To cope
with the ill-posed problems, prior knowledge about nature
images plays a critical role and is usually applied as a
regularization term, i.e., we resort to the following minimization problem:
^ ¼ argminΨ ðXÞ
X
X

s:t: Y ¼ ΦX

ð2Þ

where Ψ ðXÞ is a suitable regularization term which is able
to well depict prior knowledge about color images. Note
that Φ in problem (2) can represent any CFA pattern,
which means (2) is an universal problem for all kinds of
CFA patterns.
Traditional types of regularization (i.e., Ψ ðXÞ) are only
designed for gray images. One of the best known regularization terms is TV norm, which was ﬁrst introduced
in image denoising [29,30]. Other regularization terms
include requiring the images to be sparse in wavelet or
discrete cosine transform domain, or using dictionary
learning-based sparse representation methods [21,31], etc.
For demosaicking problems, there exists a strong correlation among different color channels. If the inter-channel
correlation can be well utilized, there is no doubt that the
quality of demosaicked images could be signiﬁcantly
improved. On the other hand, as pointed out by other
articles [32,16], jointly exploring spatial information of
images is also necessary for obtaining high reconstruction
quality, especially for those images with weak interchannel correlation. Therefore, how to design a Ψ ðXÞ
which can best model both inter-channel correlation and
image structured information for color images becomes
one of the essential problems. In addition, given a mathematical formulation of Ψ ðXÞ, how to effectively solve the
minimization problem (2) also matters a lot. In the

following two sections, these two questions will be
answered respectively.

3. Joint modeling for color images
3.1. Inter-channel modeling
The TV-based image restoration model is a useful tool
to preserve sharp edges in images while removing noise
and other unwanted ﬁne scale details. It is effective when
the gradient of the underlying signal or image is sparse.
Since color images contain signals in R, G, and B channels,
the TV-based regularization term can be formulated as

Ψ ðXÞ ¼ J DXR J 1 þ J DXG J 1 þ J DXB J 1

ð3Þ

½DTh ; DTv T

where D ¼
and Dh , Dv denote horizontal and
vertical ﬁnite difference operators, respectively.
A simple way of modeling inter-channel correlation is
to assume that the color differences are constant within a
given object [2,32,7], this assumption has been extensively
used for the interpolation of the R and B channels in many
demosaicking methods. Hence if we calculate the TV norm
of color difference signal of a nature image, it is obvious
that this value should be small. Therefore, we can change
the TV-based regularization term to

Ψ ðXÞ ¼ J DðXR  XG Þ J 1 þ J DðXG  XB Þ J 1 þ J DðXB  XR Þ J 1
ð4Þ
Compared with the traditional TV-based regularization
term (3), the new regularization term (4) can better
describe the characteristics of color images. To better
illustrate how it works, Fig. 1 visualizes the original signal
of each channel and the three color difference signals in
gradient domain for Kodak image 5, where absolute value
is displayed and the dynamic range is ½0; 80. From Fig. 1
we have the following observations:
1. Thanks to the usage of inter-channel correlation, all the
three color difference signals DðXR  XG Þ; DðXG  XB Þ;
DðXB  XR Þ are signiﬁcantly sparser than DXR ; DXG ; DXB ,
which suggests that minimizing the new regularization

Fig. 1. Horizontal gradient comparison for Kodak image 5, from left to right and from top to bottom: DXR ; DXG ; DXB ; DðXR  XG Þ; DðXG  XB Þ; DðXB  XR Þ (the
right 6 images are displayed in the dynamic range [0, 80]).

K. Chang et al. / Signal Processing: Image Communication 39 (2015) 264–279

term (4) would achieve much better results than the
traditional one (3).
2. DðXR  XG Þ; DðXG  XB Þ, and DðXB  XR Þ differ from each
other. Due to the fact that measuring each color difference signal in gradient domain reveals a different aspect
of the characteristics of color images, none of these
three color difference signals is negligible.
It should be pointed out that the sparsities of
DðXR XG Þ, DðXG  XB Þ, and DðXB  XR Þ do vary from one
image to another. Since there are much higher spectral
correlation and lower color saturation in Kodak images
(see Fig. 4) than in IMAX images (see Fig. 3) [16], Kodak
images usually have smaller TV norm of color difference
signals than IMAX images. Thus, it is believed that minimizing (4) is more effective for Kodak images.
Since the regularization term (4) mainly explores interchannel correlation, here we mark it as Ψ ICM ðXÞ, where
ICM is short for “inter-channel modeling”. In addition to
inter-channel correlation, other structured prior knowledge of images should also be explored, especially for
those images with weak inter-channel correlation. By
better modeling different kinds of prior knowledge within
natural images, one can greatly reduce the uncertainty
about the unknown signal and achieve a more accurate
reconstruction of X.
3.2. Image structure modeling and joint modeling-based
minimization problem
Recent researches argue that a signiﬁcant property of
natural images is nonlocal self-similarity, which depicts
the repetitiveness of higher level patterns (e.g., textures
and structures) globally positioned in images [33]. The
nonlocal self-similarity property has been successfully
introduced in applications such as video denoising [34,35],
compressed sensing [36], image inpainting, image
deblurring [37], and color image demosaicking [14–16].
Unlike previous demosaicking methods [14–16], here we
characterize this property by the form of nonlocal low
rank regularization for the general problem (2).
As nonlocal self-similarity implies that a sufﬁcient
number of similar patches can be found for any exemplar
patch, we divide the image into numbers of overlapped
pﬃﬃﬃﬃﬃ pﬃﬃﬃﬃﬃ
square patches with size P s  P s . To better explain, we

show the procedure of extracting a group of similar patches for the ith exemplar patch Xi in Fig. 2. First of all, we
calculate the l2 distances between Xi and its neighboring
patches within a W s  W s searching window. A small l2
distance indicates that the neighboring patch being measured is similar to Xi . After computing all the l2 distances
in the searching window, the most similar Np patches are
found and then grouped into a matrix Mi ¼ ½Xi0 ; Xi1 ;
Xi2 ; …; XiNp  1 , where Xil stands for the lth similar patch of
Xi and Xi0 ¼ Xi . It should be noted that, for the problem of
demosaicking, all R, G, and B channels should be involved.
Therefore we ﬁrst concatenate R, G, and B values in each
patch to a single vector and then perform searching and
grouping for them, i.e., Xil ¼ ½XTRi ; XTGi ; XTBi T , and
l
l
l
XfR;G;Bgi A R Ps .
l
Since we assumed that all the patches in matrix Mi are
similar (small l2 distances among patches in Mi ), it is
reasonable to require that Mi has low rank property.
However, in practice, the low rank assumption might fall
because of many reasons. For example, the textures within
the patches are too complex, the reconstructed image has
low quality, or the image is corrupted by noise (the latter
two problems would affect the accuracy of searching and
grouping). Therefore, we introduce a new low rank matrix
Li to approximate Mi , and suppose that
Li þSi ¼ Mi

ð5Þ

where Si stands for the outliers, and here we assume that
the Frobenius norm of matrix Si is small. The establishment
of Li , which can be seen in Fig. 2, relies on Mi . Given Mi , we
can construct Li by solving the following problem:
L^ i ¼ argmin J Mi  Li J 2F þ λRankðLi Þ

The ﬁrst term of the object function in (6) requires that the
Li should be close to Mi , while the second term demands Li
should be a low rank matrix. λ is the parameter to trade off
between two constraints. Problem (6) can be approximated by problem (12) in Section 4.2, and the algorithm to
solve (12) will be discussed later.
To prove that the assumptions we made above are
correct, here we generate Mi and Li directly on several
color images, and present the rank comparison along with
the mean square error (MSE) between Mi and Li in Table 1.
Note that MSE equals to J Si J 2F =ð3  Np  P s Þ, and in this
experiment we set Np ¼40 (which is also the number of

Grouping

Solving (6)

Low rank matrix Li

Similar patch

Exemplar patch

xi 0

ð6Þ

Li

Similar patches

Searching window

267

xi1 xi 2

xiNp−1

Matrix Mi
Fig. 2. Procedure of generating matrix Mi and low rank matrix Li .

268

K. Chang et al. / Signal Processing: Image Communication 39 (2015) 264–279

1
þ J ΦX  Y J 22
2

Table 1
Rank comparison and MSE results.
Images

Ave. rank of Mi

Ave. rank of Li

MSE

Kodak 3
Kodak 7
Kodak 11
IMAX 4
IMAX 17

39.98
39.82
39.78
39.62
40.00

15.91
9.45
23.48
19.94
31.26

0.888
0.928
0.756
0.799
0.811

and
LðLi ; εÞ ¼ log detððLi LTi Þ1=2 þ εIÞ

i

where ISM stands for “image structure modeling”, and F~ i is
a newly deﬁned operator which extracts Xi from the image
X and form Mi , i.e., F~ i X ¼ Mi .
Jointly considering ISM and ICM, and incorporating
them to the general problem (2), we have the following
minimization problem:
^ fL^ i gg ¼ argminαð J DðXR XG Þ J 1 þ J DðXG XB Þ J 1
fX;
X;fLi g


βX ~
J F i X  Li J 2F þ λRankðLi Þ
þ J DðXB XR Þ J 1 Þ þ
i

s:t: Y ¼ ΦX

ð8Þ

where α and β are parameters to trade off inter-channel
correlation-based regularization and image structured
information-based regularization.
While the above modeling has the desired property of
capturing two types of prior information within color
images, one challenge remains to be addressed: developing an efﬁcient solution under this framework. In the next
section, an iterative algorithm is developed to efﬁciently
solve the minimization problem (8).

4. Proposed algorithm for color image demosaicking
4.1. Joint modeling-driven iteration
Problem (8) contains a rank-minimization, which is NPhard. Usually we use the nuclear norm J  J  to replace the
matrix rank [38,39]. Recently, Done et al. [36] argued that
non-convex surrogate could better approximate rank than
nuclear norm, and proposed using a log detðÞ surrogate
function. By using surrogate function and turning the
original constrained problem (8) into an unconstrained
problem, we have
^ fL^ i gg ¼ argminαð J DðXR XG Þ J 1 þ J DðXG XB Þ J 1
fX;
X;fLi g

þ J DðXB XR Þ J 1 Þ þ


βX ~
J F i X  Li J 2F þ λLðLi ; εÞ
2

i

ð10Þ

where ε is a small constant. Since X and fLi g are separable,
we can divide problem (9) into the following two subproblems and solve them iteratively:

columns in matrix Mi ), Ps ¼36, Ws ¼20. From this table we
can conclude that: (1) Li is a low rank matrix but Mi is not.
(2) Li is close to Mi , which also veriﬁes that the Frobenius
norm of Si is small.
Based on the above analysis, we now build a new regularization term for image X as follows:
X
Ψ ISM ðXÞ ¼ ð J F~ i X  Li J 2F þ λRankðLi ÞÞ
ð7Þ

2

ð9Þ

8

β P ~
>
>
^
>
J F i X  Li J 2F þ λLðLi ; εÞ
> fL i g ¼ argmin2
>
>
fLi g
i
>
>
<
^ ¼ argminαð J DðXR  XG Þ J 1 þ JDðXG  XB Þ J 1 þ J DðXB  XR Þ J 1 Þ
X
X
>
>
>
 1
>
P ~
>
β
>
>
J F i X  Li J 2F þ J ΦX  Y J 22
þ
>
:
2 i
2

ð11Þ
4.2. Solving sub-problems
When considering the fLi g sub-problem, we treat every
Li separately, i.e., for the ith exemplar patch, we resort to
L^ i ¼ argmin J F~ i X Li J 2F þ λLðLi ; εÞ

ð12Þ

Li

The solution of (12) at the kth iteration can be written
as [36]
Lki ¼ UðΣ  λ diagðwk  1 ÞÞ þ VT

ð13Þ

1=ðσ
εÞ, σ
denotes the jth singular
UΣ V is the singular value decomposition
(SVD) of F~ i X, and ðxÞ þ ¼ maxðx; 0Þ. In practice, one iteration is enough for executing (13). By choosing F~ i X as the
initial value of L0i , the solution of (12) becomes

where wkj  1 ¼
value of Lki  1 ,

k1
þ
j
T

L^ i ¼ UðΣ  λ diagðZÞÞ þ VT

k1
j

ð14Þ

where Z j ¼ 1=ðΣ j þ εÞ.
When considering the X sub-problem, to facilitate
uniform expression, we can rewrite it as
 1
βX ~
~
^ ¼ argminα J DCX
J1 þ
J F i X  Li J 2F þ J ΦX  Y J 22
X
2
2
X
i
ð15Þ
where
2
D
~ ¼6
D
40

0

0

0

and

2

I

6
C¼40
I

D

0

3

7
0 5;
D

I

0

3

I

7
I 5;

0

I

However, this problem is not easy to solve. To simplify
the problem, we introduce a new variable and turn the
original problem (15) into
 1
X
^ ¼ argminα J d J þ β
^ dg
J F~ i X Li J 2F þ J ΦX Y J 22
fX;
1
2
2 i
X;d
~
s:t: d ¼ DCX

ð16Þ

K. Chang et al. / Signal Processing: Image Communication 39 (2015) 264–279

After turning the constrained problem (16) into an
unconstrained problem, we can use the Split-Bregman
method [40,41] to solve it. The tth Bregman iteration can
be written as
8
 1
β P ~
>
>
Xt ¼ argmin
J F i X  Li J 2F þ J ΦX  Y J 22
>
>
2
2
>
X
i
>
>
>
>
μ t1 ~
t 1 2
<
þ Jd
 DCX  b
J2
2
ð17Þ
>
μ
> t
t1 2
t
>
~
>
d ¼ argminα J d J 1 þ J d  DCX
b
J2
>
>
2
>
d
>
>
: t
t 1
t
t
~
b ¼b
þ DCX  d
Since the “X” subproblem contains strictly convex
quadratic function, there exists closed form solution for X,
which satisﬁes
X T
~ T ðDCÞ
~ þβ
ðΦT Φ þ μðDCÞ
F~ i F~ i ÞXt ¼ ΦT Y
i

~ T ðdt  1  bt  1 Þ þ β
þ μðDCÞ

X T
F~ i Li

ð18Þ

i

~ T ðDCÞ
~ þ
However, directly calculating ðΦ Φ þ μðDCÞ
P T
β i F~ i F~ i Þ  1 in every Bregman iteration is too costly.
Therefore, an iterative method is highly desirable. Here,
the conjugate gradients squared (CGS) method [42] is
applied.
t
When updating d , closed form solution by the
shrinkage (or soft thresholding) formula [43] is given as
T

t
~
d ¼ shrinkðDCX
þb
t

t 1

; α=μÞ

ð19Þ

For a vector A and a constant c, the shrinkage operation
is deﬁned as
shrinkðA; cÞ ¼ maxðjAj c; 0Þ  sgnðAÞ

ð20Þ

4.3. Summary of the proposed algorithm

269

Set Xk ¼ Xt
End for
If J Xk  Xk  1 J 2 = J Xk J 2 o 10  4
Break
End If
End for
X^ ¼ Xk

However, performing JM-CDM for the whole image is too
expensive, especially for those high-deﬁnition images; moreover, setting the same regularization parameters for the whole
image is not suitable. Therefore, we propose to divide the
image into numbers of image blocks, and then set regularization parameters and execute JM-CDM algorithm for each
image block. It is obvious that if the inter-channel correlation
of an image block is strong, we should enlarge the value of α
(or reduce the value of β). To determine the suitable values for
α and β, given the initial value of the jth image block X0j , we
ﬁrst calculate the average TV norm of channel differences, i.e.,
AD ¼ ð J DðX0Rj  X0Gj Þ J 1 þ J DðX0Gj X0Bj Þ J 1
þ J DðX0Bj  X0Rj Þ J 1 Þ=NB

ð21Þ

where X0Rj , X0Gj and X0Bj are the initial R, G, and B values of the
jth image block, and Nb is the number of pixels in an image
block. A large value of AD suggests low inter-channel correlation, thus a small α (or a large β) should be assigned.
Nevertheless, the accuracy of AD depends on the quality
of X0 . Through experiment we found that sometimes the
AD of X0 largely differs from the real value (i.e., the AD of
the original image). Therefore we cannot rely solely on AD
of X0 to tune α and β, which indicates that other metrics
should also be introduced. Zhang et al. [16] mentioned in
their work that average saturation is a good tool to evaluate the inter-channel correlation, which is computed as
Xqﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
AS ¼
ð22Þ
ðr n  yn Þ2 þ ðg n  yn Þ2 þ ðbn  yn Þ2 =N B
n

In light of all derivations above, a detailed description of the
proposed algorithm, named JM-CDM (joint modeling-based
color demosaicking method), is provided in Algorithm 1. The
algorithm will stop if the relative change of X is smaller than a
predeﬁned threshold, or the maximum number of iterations is
reached. Note that the inputs include X0 , which is an initial
guess of X. Although any traditional demosaicking method
could be applied to generate X0 , we ﬁnd that a high quality of
X0 is crucial to get the best performance of JM-CDM. The
impact of a lower quality of X0 will be discussed in Section 5.1.
Algorithm 1. JM-CDM.
Input: X0 , Y,

α, β, λ, μ

Find similar patches for each exemplar patch in X0 according to l2
distances.
Outer loop for k ¼ 1; 2; …K
Group sets of similar patches, i.e., form F~ i Xk  1 .
Update every Lki by (14).
0

0

d ¼ 0; b ¼ 0
Inner loop for t ¼ 1; 2; …; T do
Update Xt by using CGS method to solve (18).
t

Update d via (19).
t
t 1
t
t
~
þ DCX
d
b ¼b

where rn, gn and bn are the color components of the nth
pixel in an image block, and yn ¼ r n þ g n þbn . Although AS
of X0 also differs from AS of the original image, combing AD
and AS makes the detection of inter-channel correlation
more robust. Given AD and AS, the empirical values of α
and β will be listed in Section 5.1.
According to the above description, we design Algorithm 2
for demosaicking the whole image, where J is the number of
image blocks. Note that for those noise-contaminated cases, in
order to decide the suitable α; β and to get a better initial
image for patch-by-patch searching and grouping, for the jth
image block Xj , JM-CDM can be executed several times (M is
the number for calling JM-CDM in Algorithm 2).
Algorithm 2. Block-based demosaicking.
Input: Y, λ, μ
Using an existing demosaicking algorithm to obtain the initial value
X0 .
for j ¼ 1; 2; …J
for m ¼ 1; 2; …M
1
calculate AD and AS based on Xm
, and set
j
m1
, Yj ,
Xm
j ¼ JM-CDM(Xj

α, β, λ, μ)

α, β.

270

K. Chang et al. / Signal Processing: Image Communication 39 (2015) 264–279

Fig. 3. IMAX test images. From left to right and from top to bottom, these images are labeled as 1 to 18.

End for
End for
return Final Demosaicked Image

5. Experimental results
5.1. Setting up
In this section, extensive experimental results are presented to evaluate the performance of the proposed
algorithm. All experiments are performed in Matlab 7.14
on a Dell computer with Intel(R) Core(TM) i5-2500 processor, 8.00G memory, and 64-bit Windows 7 operating
system. To facilitate evaluation and further exploration of
the method proposed above, we have also published the
source code, see footnote 11.
Our algorithm is performed on two datasets: IMAX2
(see Fig. 3, size of each image: 500  500) and Kodak3 (see
Fig. 4, size of each image: 768  512). The second dataset
has been a popular choice for testing the performance of
demosaicking methods for quite a long time, while the
images in the ﬁrst dataset have weaker inter-channel
correlation and they are considered to be more challenging. In the experiment, we ﬁrst down sample the original
color images into CFA images according to the Bayer pattern and then reconstruct the full color images from the
CFA mosaicked data by using demosaicking methods.
When executing our algorithm, each image in IMAX or
Kodak dataset is divided into 4 non-overlapping image
1
2
3

http://www.public.asu.edu/bli24/SourceCodeForJM-CDM.html
http://www4.comp.polyu.edu.hk/cslzhang/CDM_Dataset.htm
http://www.csee.wvu.edu/xinl/demo/demosaic.html

blocks (i.e., J ¼4). In noiseless situation, we set M ¼1 in
Algorithm 2; when considering noisy measurements, in
order to get a more precise patch by patch searching, we
ﬁx M¼2. In each image block, the patch size Ps is set to 36,
the size of the searching window Ws is 20, and we choose
the number of similar patches Np in a searching window as
40. For JM-CDM algorithm, the maximum iteration number for outer loop is set as 20. We ﬁnd that it is not
desirable to solve the X subproblem to full convergence. In
practice, 10 times of inner loop is enough. For trade-off
parameters, we ﬁx μ as 0.1, λ as 50, and α as 0.005. β is set
adaptively according to AS and AD values of each initial
demosaicked image block. Through experiment, we
empirically ﬁnd that the values of β in Table 2 can achieve
satisfactory results.
As mentioned before, the quality of X0 has great impact
on performance of JM-CDM. The main reasons are: (1) It
affects the accuracy of extracting the similar patches sets.
2) Since the ISM regularization requires fLi g to be close to
F~ i X in every iteration, a low quality of X0 might keep JMCDM from getting a high quality of output. 3) A low quality
of X0 also leads to inaccurate AD and AS values, which
would eventually results in unsuitable tuning of β.
To evaluate the performance of JM-CDM with different
X0 , we chose several demosaicking methods to generate
X0 , including successive approximation (SA) method [7],
the adaptive homogeneity-directed (AHD) method [11],
regularization approaches to demosaicking (RAD) [19],
directional linear minimum mean square-error estimation
(DLMMSE) method [13], and minimized-Laplacian residual
interpolation (MLRI) [10]. Fig. 5 displays progression of
Color Peak Signal to Noise Ratio (CPSNR) results achieved
by JM-CDM with different X0 , where horizontal axis is the
iteration number of the outer loop in JM-CDM. One can
ﬁnd that JM-CDM converges fast when choosing MLRI as

K. Chang et al. / Signal Processing: Image Communication 39 (2015) 264–279

271

Fig. 4. Kodak test images. From left to right and from top to bottom, these images are labeled as 1 to 24.

Table 2
Tuning of β.
AD

½0; 6:5Þ
½6:5; 8Þ
½8; 10Þ
½10; 12Þ
½12; 1Þ

AS
½0; 21Þ

½21; 26Þ

½26; 31Þ

½31; 36Þ

½36; 1Þ

0.08
0.15
0.22
0.29
0.36

0.15
0.15
0.22
0.29
0.36

0.22
0.22
0.22
0.29
0.36

0.29
0.29
0.29
0.29
0.36

0.36
0.36
0.36
0.36
0.36

the initial method. This phenomenon also indicates that
setting the maximum iteration number as 20 is able to get
satisfactory results when MLRI is selected. Table 3 shows
average CPSNR results obtained by JM-CDM with different
methods. It can be observed that, ﬁrstly, the output of JMCDM is signiﬁcantly superior to all the X0 generated by
different initial methods; secondly, choosing MLRI to
produce X0 leads to the highest CPSNR results for JM-CDM.
Therefore, based on the above discussion, in the following
experiments, MLRI is applied as the initial method.
5.2. Results on noiseless datasets
We now test JM-CDM on noiseless IMAX and Kodak
datasets. 9 recent representative methods are compared,
including: SA [7], extended color total-variation regularization (ECTV) [17], AHD [11], RAD [19], DLMMSE [13], NAT
[16], exploitation of inter-color correlation (EICC) [8], MLRI
[10] and self-similarity and spectral correlation adaptive
algorithm (SSSC) [15].
The CPSNR results for IMAX dataset and Kodak dataset
are listed in Tables 4 and 5, respectively, where the bold
values indicate the best method. We can see that JM-CDM
achieves signiﬁcantly higher CPSNRs than the other

competing algorithms when testing IMAX dataset. However, SSSC is slightly better than JM-CDM on Kodak images.
The advantage of using JM-CDM lies in that it can get
satisfactory results on both datasets, which suggests that
JM-CDM derives large beneﬁts from fully utilizing the interchannel correlation and image structured information.
Usually, the demosaicking errors are more likely to
appear around the color edges, which account for only a
small portion of the image pixels [16]. Therefore, besides
CPSNR, we need other kinds of metrics to reveal how many
abrupt or unnatural changes of color occur in a demosaicked image. One good choice is zipper effect ratio (ZER),
which was ﬁrst proposed by Lu et al. [44] to evaluate the
color edge preservation performance of demosaicking
methods. Tables 6 and 7 show ZER of all 10 methods on the
two datasets. It is clear that JM-CDM produces much less
zipper artifact than the other methods on IMAX dataset,
while ZER results of JM-CDM are close to SSSC on Kodak
dataset. This conclusion is consistent with the PSNR results
reported before.
Fig. 6 shows the cropped and zoomed in CDM results of
the 10 methods for IMAX image 17. Due to the fact that
inter-channel correlation is weak in IMAX image 17, SA,
ECTV, AHD, RAD and DLMMSE generate lots of pixels with

272

K. Chang et al. / Signal Processing: Image Communication 39 (2015) 264–279

38.5

36
35.5

38

35

CPSNR(dB)

CPSNR(dB)

37.5
34.5
34
33.5

37
36.5

33
36
MLRI
DLMMSE
AHD
SA
RAD

32.5
32
31.5

0

20

40

60

80

100

iteration number

MLRI
DLMMSE
AHD
SA
RAD

35.5
35

0

20

40

60

80

100

iteration number

Fig. 5. Progression of CPSNR (dB) achieved by JM-CDM with different initial methods. (a) IMAX image 2. (b) Kodak image 5.

Table 3
Average CPSNR (dB) results by JM-CDM with different initial methods.

IMAX
Initial input
JM-CDM output
Kodak
Initial input
JM-CDM output
All 42
Initial Input
JM-CDM output

αnew ¼ αð1 þ 2σ 1 þ 3σ 2 MX Þ

[7]

[11]

[19]

[13]

[10]

32.66
34.09

33.44
35.72

34.20
35.85

34.40
35.69

36.56
37.55

39.57
41.03

38.26
41.24

39.99
40.97

40.63
41.45

40.05
41.06

36.61
38.06

36.20
38.88

37.51
38.78

37.96
38.98

38.56
39.55

zipper effect; although the results generated by NAT, EICC,
MLRI, SSSC and JM-CDM are similar, one can still ﬁnd the
details of JM-CDM results are clearer and more natural
than NAT, EICC, MLRI and SSSC.
5.3. Results on noisy datasets
In Section 5.2, the performance of JM-CDM is
impressive in the absence of sensor noise. In this subsection, to evaluate the robustness of JM-CDM, different levels
of noise are added to the measurements Y. As pointed out
by Hirakawa et al., the readout noise (for both CMOS and
CCD sensors) takes the following general form [24]:
~ jÞ ¼ Yði; jÞ þ ðσ 1 þ σ 2 Yði; jÞÞnði; jÞ
Yði;

experiment we empirically tune these two parameters in
noisy situation as

ð23Þ

~ jÞ are the ideal and measured sensor
where Yði; jÞ and Yði;
values at pixel location (i,j), respectively. nði; jÞ is Gaussian
noise with zero mean and unit variance, and σ 1 ; σ 2 are
sensor dependent parameters.
To get satisfactory results, the values of α and β for JMCDM should be enlarged when considering the noisecontaminated measurements. The reason lies in that the
observed color samples Y~ are no longer reliable due to
sensor noise, and increasing α and β actually reduces the
importance of data-ﬁdelity term in problem (9). In this

ð24Þ

βnew ¼ βð1 þ 2σ 1 þ3σ 2 MX Þ

ð25Þ
0

where MX is the mean value of X , and the original values
of α and β have been discussed in Section 5.1. It should
also be pointed out that in practice, increasing the ISOsensitivity of a digital camera usually induces more noise.
Thus one should increase α and β for our method
accordingly in a high ISO-sensitivity case.
Since most compared methods in Section 5.2 are
designed under the noiseless assumption, here we need to
compare with other methods which are robust to noise.
However, many noise-considered demosaicking methods
can only work under the additive Gaussian noise
assumption (i.e., in Eq. (23), σ 1 4 0; σ 2 ¼ 0). Representative
methods include PCA-DLMMSE [23], JDD-LMMSE [26],
JDD-SVF [27], JDD-TVM [28] and PCA-MLRI. For PCADLMMSE and PCA-MLRI, the PCA-based CFA denosing
method [23] is applied ﬁrst, after which DLMMSE [13] and
MLRI [10] are respectively used for demosaicking. JDDLMMSE reproduces color images through an LMMSE ﬁltering of the G–R and G–B difference images and a
wavelet-based denoising process, while JDD-SVF and JDDTVM adopt space-varying ﬁlters and an extended form of
TV minimization respectively for joint denoising and
demosaicking.
When
considering
additive
Gaussian
noisecontaminated sensor data, we set PCA-MLRI as the initial
method for JM-CDM. The average CPSNR results for both
IMAX and Kodak dataset are listed in Table 8. The results in
Table 8 indicate that JM-CDM is signiﬁcantly superior to
all the other methods on additive Gaussian noisecontaminated IMAX dataset. More speciﬁcally, when testing IMAX dataset, JM-CDM beats the second best algorithm by 1.19 dB on average when σ 1 ¼ 5, and 1.27 dB on
average when σ 1 ¼ 10. However, the performance of

K. Chang et al. / Signal Processing: Image Communication 39 (2015) 264–279

273

Table 4
CPSNR (dB) results by different methods on IMAX dataset (noiseless situation).
Images

[7]

[17]

[11]

[19]

[13]

[16]

[8]

[10]

[15]

JM-CDM

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
Ave.

23.39
31.92
31.17
33.48
29.04
31.00
37.54
36.33
32.16
34.90
35.70
35.93
37.95
36.32
36.30
27.01
25.64
32.03
32.66

24.05
32.53
31.55
33.68
29.50
31.64
36.88
36.89
33.09
33.58
36.80
36.51
38.20
36.78
36.94
28.25
26.47
32.94
33.24

26.02
32.99
31.15
33.30
30.33
32.77
35.90
35.94
33.77
35.49
36.33
35.90
38.11
36.68
36.81
29.39
28.04
32.99
33.44

26.15
33.01
32.58
36.12
30.62
33.59
37.50
37.14
34.58
36.04
37.05
36.50
38.07
36.56
37.00
30.10
29.62
33.37
34.20

26.85
33.66
32.47
34.12
31.02
33.89
38.54
37.31
34.54
36.21
37.29
36.50
38.72
37.15
37.25
30.49
29.19
34.07
34.40

28.91
35.00
32.44
35.75
33.86
37.97
35.83
37.42
37.10
38.63
39.57
38.84
40.74
38.62
38.95
33.64
32.74
35.19
36.18

29.19
35.14
33.34
36.26
34.18
38.47
36.89
38.65
37.29
38.43
39.94
39.72
41.06
38.92
39.32
35.20
33.26
36.10
36.74

28.85
35.08
33.77
37.47
33.75
38.28
37.40
36.80
36.53
38.53
39.97
39.66
40.52
38.73
38.91
35.17
32.46
36.26
36.56

28.85
34.38
31.52
33.35
33.37
37.18
35.50
36.96
36.45
37.82
39.02
37.76
40.22
38.21
38.62
33.40
32.74
34.26
35.54

29.31
35.70
34.50
37.91
34.43
39.94
39.64
38.87
38.14
39.67
40.60
40.61
41.50
38.88
39.64
35.65
34.12
36.75
37.55

Table 5
CPSNR (dB) results by different methods on Kodak dataset (noiseless situation).
Images

[7]

[17]

[11]

[19]

[13]

[16]

[8]

[10]

[15]

JM-CDM

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
Ave.

38.33
39.86
40.73
39.54
36.26
39.48
42.46
37.14
43.15
41.71
39.99
42.36
34.72
34.12
38.91
42.65
40.72
37.03
41.04
41.71
39.59
38.79
42.92
36.39
39.57

38.33
39.94
41.17
39.87
36.48
39.52
42.43
37.18
43.00
41.87
40.00
42.37
34.92
34.52
38.96
42.60
40.99
37.31
41.06
41.80
39.62
39.06
42.88
36.84
39.70

35.01
39.10
40.99
38.51
35.27
38.64
41.38
34.72
42.10
40.47
38.37
41.50
31.21
34.99
37.88
42.28
39.04
34.89
39.37
40.15
37.59
37.45
42.49
34.92
38.26

37.28
39.94
42.15
40.76
37.89
40.14
43.63
36.04
43.06
42.32
40.14
42.94
33.72
36.12
40.09
43.20
40.85
37.02
40.43
41.93
39.70
39.43
43.67
37.24
39.99

38.40
40.85
42.56
40.43
37.97
41.46
43.21
37.20
44.24
42.55
41.02
43.37
34.71
36.79
39.79
44.65
41.58
37.75
41.96
42.53
40.39
39.68
44.20
37.83
40.63

34.31
39.72
41.72
39.49
35.62
36.77
42.32
33.56
41.48
40.80
37.74
40.85
30.56
36.03
38.43
39.84
38.76
34.90
38.41
40.43
36.88
38.20
43.16
34.60
38.11

35.46
40.32
41.85
40.05
36.60
39.49
43.24
35.01
42.40
41.53
39.08
42.37
31.83
36.45
39.05
43.10
39.81
35.60
40.17
41.01
38.27
38.81
44.00
35.69
39.22

36.69
40.74
42.83
40.80
37.53
40.21
43.66
35.89
43.38
42.24
40.10
43.12
33.03
37.42
39.34
43.69
40.85
36.73
40.90
42.02
39.28
39.73
44.69
36.30
40.05

41.06
41.02
43.65
41.19
37.97
42.26
42.99
38.95
44.32
42.70
41.34
44.05
36.41
37.56
40.49
45.21
41.62
37.59
42.56
42.95
41.65
39.44
43.99
38.13
41.21

38.87
40.72
43.50
40.55
39.02
42.61
44.99
37.89
44.34
42.78
41.79
43.97
33.94
37.72
40.70
44.90
41.19
37.25
42.90
43.26
40.85
40.12
44.33
37.25
41.06

JM-CDM is close to JDD-TVM on additive Gaussian noisecontaminated Kodak dataset. The reason is that the
assumption of strong inter-channel correlation made by
JDD-TVM can be met on Kodak images.
Cropped parts of the demosaicked IMAX image 1 are
shown in Fig. 7. Detailed inspection of the restored images
reveals that the competing methods PCA-DLMMSE, JDDLMMSE, JDD-SVF, JDD-TVM and PCA-MLRI generate many
noise-caused artifacts (and also zipper artifacts).

Compared with other methods, JM-CDM is more effective
in reducing the amount of sensor noise-caused artifacts.
Moreover, the proposed approach well preserves the ﬁne
image structures and achieves the best visual quality
among the tested solutions.
Only a few demosaicking algorithms consider signaldependent noise (i.e., in Eq. (23), σ 1 Z 0; σ 2 4 0). Therefore
under this assumption we only compare our method with
joint demosaicing and denoising algorithm based on a

274

K. Chang et al. / Signal Processing: Image Communication 39 (2015) 264–279

Table 6
ZER by different methods on IMAX dataset (noiseless situation).
Images

[7]

[17]

[11]

[19]

[13]

[16]

[8]

[10]

[15]

JM-CDM

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
Ave.

0.445
0.280
0.281
0.084
0.190
0.211
0.093
0.253
0.251
0.288
0.373
0.136
0.044
0.177
0.249
0.575
0.612
0.237
0.265

0.365
0.220
0.216
0.060
0.161
0.164
0.084
0.329
0.161
0.185
0.201
0.084
0.031
0.112
0.180
0.359
0.470
0.181
0.198

0.325
0.229
0.247
0.061
0.134
0.118
0.125
0.265
0.188
0.220
0.291
0.102
0.030
0.143
0.210
0.424
0.483
0.193
0.210

0.425
0.285
0.289
0.074
0.205
0.201
0.117
0.328
0.253
0.293
0.381
0.142
0.047
0.189
0.285
0.508
0.583
0.249
0.270

0.362
0.237
0.257
0.079
0.155
0.147
0.094
0.294
0.211
0.247
0.342
0.122
0.037
0.163
0.242
0.476
0.505
0.214
0.232

0.299
0.191
0.253
0.063
0.117
0.076
0.141
0.239
0.135
0.158
0.226
0.074
0.024
0.106
0.152
0.379
0.344
0.169
0.175

0.298
0.190
0.227
0.063
0.129
0.092
0.119
0.262
0.125
0.157
0.166
0.074
0.024
0.106
0.154
0.247
0.345
0.134
0.162

0.296
0.188
0.207
0.050
0.125
0.089
0.103
0.234
0.118
0.154
0.164
0.070
0.026
0.110
0.161
0.232
0.342
0.130
0.156

0.318
0.214
0.283
0.106
0.147
0.110
0.171
0.282
0.156
0.197
0.242
0.091
0.031
0.133
0.173
0.343
0.374
0.165
0.196

0.276
0.167
0.181
0.040
0.117
0.070
0.055
0.200
0.089
0.111
0.120
0.044
0.018
0.087
0.110
0.219
0.259
0.119
0.127

Table 7
ZER by different methods on Kodak dataset (noiseless situation).
Images

[7]

[17]

[11]

[19]

[13]

[16]

[8]

[10]

[15]

JM-CDM

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
Ave.

0.036
0.064
0.038
0.041
0.116
0.024
0.037
0.035
0.012
0.012
0.041
0.010
0.104
0.098
0.072
0.013
0.035
0.091
0.026
0.022
0.036
0.026
0.020
0.032
0.043

0.036
0.063
0.036
0.056
0.117
0.023
0.036
0.036
0.012
0.013
0.043
0.010
0.103
0.095
0.078
0.012
0.047
0.099
0.025
0.023
0.037
0.026
0.019
0.033
0.045

0.084
0.067
0.028
0.047
0.125
0.027
0.031
0.072
0.009
0.012
0.050
0.008
0.196
0.092
0.068
0.015
0.047
0.124
0.041
0.029
0.053
0.035
0.014
0.048
0.055

0.065
0.069
0.032
0.027
0.115
0.021
0.031
0.079
0.013
0.011
0.051
0.008
0.174
0.104
0.065
0.013
0.052
0.106
0.032
0.024
0.041
0.026
0.016
0.040
0.051

0.038
0.044
0.030
0.028
0.100
0.014
0.030
0.042
0.008
0.008
0.036
0.007
0.132
0.080
0.060
0.008
0.043
0.090
0.021
0.020
0.029
0.022
0.014
0.033
0.039

0.137
0.062
0.028
0.057
0.163
0.061
0.037
0.129
0.017
0.017
0.077
0.011
0.283
0.112
0.077
0.045
0.071
0.150
0.067
0.039
0.090
0.045
0.016
0.073
0.078

0.102
0.062
0.030
0.048
0.137
0.029
0.031
0.086
0.010
0.012
0.062
0.008
0.256
0.104
0.071
0.015
0.071
0.135
0.049
0.036
0.062
0.043
0.015
0.062
0.064

0.068
0.045
0.023
0.029
0.107
0.021
0.026
0.067
0.007
0.009
0.045
0.006
0.198
0.079
0.055
0.011
0.052
0.131
0.034
0.025
0.044
0.027
0.011
0.046
0.047

0.019
0.068
0.025
0.038
0.093
0.011
0.034
0.028
0.007
0.008
0.029
0.006
0.097
0.072
0.067
0.008
0.042
0.107
0.016
0.015
0.021
0.027
0.017
0.031
0.037

0.037
0.046
0.021
0.036
0.092
0.009
0.019
0.045
0.005
0.005
0.033
0.004
0.124
0.084
0.049
0.007
0.053
0.099
0.012
0.014
0.028
0.022
0.014
0.035
0.036

total least squares (JDD-TLS) [24], and ECTV [17] methods.
When considering signal-dependent noise, the demosaicked results of JDD-TLS are set as the initial values for
our method. The average CPSNR results are provided in
Table 9, demonstrating that JM-CDM can achieve signiﬁcant improvement over JDD-TLS and ECTV when
measurements are with signal-dependent noise. Fig. 8
displays subject results of the reproduced Kodak image 5.

It is obvious that JM-CDM is better than the other two
methods in suppressing artifacts.

5.4. Computational complexity
Now we discuss the computational complexity of JMCDM. In the beginning of JM-CDM, patch by patch
searching is needed. Assume that the patch size is

K. Chang et al. / Signal Processing: Image Communication 39 (2015) 264–279

275

Fig. 6. Visual quality comparison for IMAX image 17 (noiseless situation). From left to right and from top to bottom: SA, ECTV, AHD, RAD, DLMMSE, NAT,
EICC, MLRI, SSSC, JM-CDM, original image

Table 8
Average CPSNR (dB) results on noisy sensor data (additive Gaussian noise).
ðσ 1 ; σ 2 Þ

Dataset

[23] þ
[13]

[26]

[27]

[28]

[23] þ
[10]

JM-CDM

(5, 0)

IMAX
Kodak
All 42

32.77
35.40
34.28

32.69
35.69
34.40

32.31
35.57
34.17

32.95
36.06
34.72

34.04
35.25
34.73

35.23
35.97
35.66

(10, 0)

IMAX
Kodak
All 42

31.06
32.40
31.83

30.91
32.41
31.77

30.58
32.19
31.50

31.28
33.10
32.32

31.78
32.32
32.09

33.05
33.31
33.20

pﬃﬃﬃﬃﬃ pﬃﬃﬃﬃﬃ
P s  P s , the searching window is of size W s  W s , then
the computational complexity for ﬁnding similar patches
for an exemplar patch is OðW 2s  P s Þ. If the number of
exemplar patches in an image (or an image block) is Ne,
executing patch by patch searching needs calculating Ne 
OðW 2s  P s Þ absolute values (or squarings). When updating
Li , one SVD is needed. Since Li has the same size as F~ i X, the
computational complexity for SVD is OðP 2s  N p þN 3p Þ [45],

where Np is the number of patches in a patch group. Since
the number of Li to be updated is Ne and we need to repeat
updating all fLi g K times, the computational complexity
adds to K  Ne  OðP 2s  N p þ N 3p Þ. When solving the X subproblem, the most complex part is using CGS to solve (18).
In an iteration of CGS, 2 matrix–vector products, 2 inner
products, 6 SAXPY (adds a vector, which is multiplied by a
scalar, to another vector) are contained [46]. Since the

276

K. Chang et al. / Signal Processing: Image Communication 39 (2015) 264–279

Fig. 7. Visual quality comparison for IMAX image 1, σ 1 ¼ 10; σ 2 ¼ 0. The left one is the original image; the right 6 are the results of PCA-DLMMSE, JDDLMMSE, JDD-SVF, JDD-TVM, PCA-MLRI, and JM-CDM, respectively (from left to right and from top to bottom).

Table 9
Average CPSNR (dB) results on noisy sensor data (signal-dependent
noise).
ðσ 1 ; σ 2 Þ

Dataset

[24]

[17]

JM-CDM

(5, 0.1)

IMAX
Kodak
All 42

27.01
28.59
27.91

28.26
28.88
28.61

28.97
30.22
29.68

(10, 0.1)

IMAX
Kodak
All 42

26.14
27.56
26.95

26.70
27.09
26.92

27.53
29.06
28.41

(5, 0.2)

IMAX
Kodak
All 42

25.04
26.57
25.92

24.25
24.23
24.27

25.88
27.72
26.93

dominating part of the computation is the matrix–vector
product, the computational complexity of one iteration in
CGS is OðN2 Þ, where N is the number of pixels in an image.
If an average of NCGS iterations is needed for CGS method,
the complexity for solving (18) adds to N CGS  OðN2 Þ.
Because in Algorithm 1, the outer loop is K and the inner
loop is T, the total computational complexity of solving the
X sub-problem becomes K  T  NCGS  OðN 2 Þ. By dividing
the image into J image blocks, this complexity can be


reduced to J  K  T  NCGS  O ðNJÞ2 .
The average computational times for demosaicking one
color image are listed in Table 10, where all the methods
are implemented by Matlab (N/A indicates that this
method has not been tested under the related noise setting). The average runtime of SSSC [15] is not shown here
because the implement provided by its authors is written
in C language. We can see that in noiseless situation, JMCDM has the second highest computational complexity
among all the algorithms (still lower than NAT). JM-CDM

has even higher complexity in noisy situation than in
noiseless situation. The reason is that to get a more accurate patch by patch searching, JM-CDM is executed for two
times in noisy situation. Note that to reduce the complexity, the test images are divided into many image blocks
(see Algorithm 2). Moreover, exemplar patches in an image
block are independent from each other. Therefore, it is
possible to use parallelization techniques such as GPU
hardware to further speed up JM-CDM, especially for patch
by patch searching, solving the X sub-problem and
updating fLki g.
5.5. Test on real raw CFA images
Besides the IMAX and Kodak datasets, the JM-CDM is
applied to the real single-sensor captured images. Several
raw CFA images are tested. These raw CFA images are
generated by different digital cameras and they are
representative of different types of nature images. However, due to the length limitation of this paper, here we
only show the demosiacked results of two raw CFA images
“lighter” (2175  1448)4 and “penguin” (2481  1647)5.
Before applying our JM-CDM algorithm, we need to
process raw images in a realistic camera pipeline. Therefore, we use dcraw,6 a popular software package for raw
image processing. Due to the high resolution of these raw
images, we divide each one into 64 image blocks when
executing JM-CDM. Since the demosaicking process is
done in linear space [25], further image correction is
needed for the results of JM-CDM. After being transformed
to the sRGB space, the demosaicked results of the two raw
4
5
6

http://www.rawsamples.ch/index.php/en/canon
http://www.moosepeterson.com/d2x/noise.html
http://www.cybercom.net/dcofﬁn/dcraw

K. Chang et al. / Signal Processing: Image Communication 39 (2015) 264–279

277

Fig. 8. Visual quality comparison for Kodak image 5, σ 1 ¼ 5; σ 2 ¼ 0:1. From left to right: JDD-TLS, ECTV, JM-CDM, original image

Table 10
Average run time (s) comparison.
Methods

σ 1 ¼ 0, σ 2 ¼ 0

σ 1 40, σ 2 ¼ 0

σ 1 4 0, σ 2 4 0

[7]
[17]
[11]
[19]
[13]
[16]
[8]
[10]
[23] þ [13]
[26]
[27]
[28]
[23] þ [10]
[24]
JM-CDM

0.75
2.02
21.35
1.21
18.78
875.56
4.76
1.34
N/A
N/A
N/A
N/A
N/A
N/A
656.78

N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
126.77
25.90
1.45
21.32
104.55
N/A
1121.56

N/A
8.87
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
339.11
1153.04

Fig. 9. Demosaicked result of raw ﬁle “lighter”. Left: demosaicked image, Right: roomed in area.

Fig. 10. Demosaicked result of raw ﬁle “penguin”. Left: demosaicked image, Right: roomed in area.

278

K. Chang et al. / Signal Processing: Image Communication 39 (2015) 264–279

CFA images are showed in Figs. 9 and 10. From these ﬁgures we can see that the details of demosaicked images are
preserved well, few pixels have zipper effect or false color,
and the color edges are very clear.

[11]
[12]

[13]

6. Conclusion
This paper focuses on color images demosaicking problem. First, joint modeling of color images was proposed,
which considers both inter-channel correlation and
structural image information. Based on the key idea of
joint modeling, an algorithm named JM-CDM was proposed to reconstruct the color images from the CFA images. To evaluate the robustness of JM-CDM, we tested it on
both noisy and noiseless datasets. Experimental results
demonstrated that the proposed method outperforms
many existing state-of-the-arts demosaicking methods in
terms of PSNR and ZER. In addition, the demosaicked
results of real CFA images by JM-CDM were also given. Our
future work includes extending the algorithm to other
applications such as joint demosaicking and superresolution and video CFA demosaicking.

[14]
[15]

[16]

[17]

[18]

[19]
[20]

[21]

Acknowledgment
[22]

This work was supported by the Natural Science
Foundation of China via Grants 61401108 and 61261023,
the Natural Science Foundation of Guangxi Zhuang
Autonomous Region via Grant 2013GXNSFBA019272. The
support provided by China Scholarship Council via Grant
201306665002 during a visit of K. Chang to Arizona State
University is also acknowledged. B. Li was partially supported by the Natural Science Foundation via Grants
1135616 and 0845469.

[23]

[24]
[25]

[26]

[27]

References
[1] B.E. Bayer, Color imaging array, US Patent 3, 971, 065, July 20, 1976.
[2] B.K. Gunturk, J. Glotzbach, Y. Altunbasak, R.W. Schafer, R.
M. Mersereau, Demosaicking: color ﬁlter array interpolation, IEEE
Signal Process. Mag. 22 (1) (2005) 44–54.
[3] D. Menon, G. Calvagno, Color image demosaicking: an overview,
Signal Process.: Image Commun. 26 (8) (2011) 518–533.
[4] D.R. Cok, Signal processing method and apparatus for producing
interpolated chrominance values in a sampled color image signal, US
Patent 4,642,678, February 10, 1987.
[5] J.F. Hamilton, J.E. Adams, Apparatus for utilizing a digitized image
signal, US Patent 5,629,734, May 13, 1997.
[6] B.K. Gunturk, Y. Altunbasak, R.M. Mersereau, Color plane interpolation using alternating projections, IEEE Trans. Image Process. 11 (9)
(2002) 997–1013.
[7] X. Li, Demosaicing by successive approximation, IEEE Trans. Image
Process. 14 (3) (2005) 370–379.
[8] S.P. Jaiswal, O.C. Au, V. Jakhetiya, Y. Yuan, H. Yang, Exploitation of
inter-color correlation for color image demosaicing, in: Proceedings
of IEEE International Conference on Image Processing (ICIP), Paris,
2014, pp. 1782–1786.
[9] D. Kiku, Y. Monno, M. Tanaka, M. Okutomi, Residual interpolation for
color image demosaicking, in: Proceedings of IEEE International
Conference on Image Processing (ICIP), Melbourne, 2013, pp. 2304–
2308.
[10] D. Kiku, Y. Monno, M. Tanaka, M. Okutomi, Minimized-Laplacian
residual interpolation for color image demosaicking, in: Proceedings

[28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]

of IS&T/SPIE Electronic Imaging, International Society for Optics and
Photonics, San Francisco, 2014, pp. 90230L-1–90230L-8.
K. Hirakawa, T.W. Parks, Adaptive homogeneity-directed demosaicing algorithm, IEEE Trans. Image Process. 14 (3) (2005) 360–369.
D. Menon, S. Andriani, G. Calvagno, Demosaicing with directional
ﬁltering and a posteriori decision, IEEE Trans. Image Process. 16 (1)
(2007) 132–141.
L. Zhang, X. Wu, Color demosaicking via directional linear minimum
mean square-error estimation, IEEE Trans. Image Process. 14 (12)
(2005) 2167–2178.
A. Buades, B. Coll, J. Morel, C. Sbert, Self-similarity driven color
demosaicking, IEEE Trans. Image Process. 18 (6) (2009) 1192–1202.
J. Duran, A. Buades, Self-similarity and spectral correlation adaptive
algorithm for color demosaicking, IEEE Trans. Image Process. 23 (9)
(2014) 4031–4040.
L. Zhang, X. Wu, A. Buades, X. Li, Color demosaicking by local
directional interpolation and nonlocal adaptive thresholding, J.
Electron. Imaging 20 (2) . 023016-1–023016-15.
T. Saito, T. Komatsu, Demosaicing approach based on extended color
total-variation regularization, in: Proceedings of IEEE International
Conference on Image Processing (ICIP), IEEE, San Diego, 2008,
pp. 885–888.
T. Saito, T. Komatsu, Demosaicing method using the extended color
total-variation regularization, in: Proceedings of IS&T/SPIE Electronic Imaging, International Society for Optics and Photonics, San Jose,
2008, pp. 68170C-1–68170C-12.
D. Menon, G. Calvagno, Regularization approaches to demosaicking,
IEEE Trans. Image Process. 18 (10) (2009) 2209–2220.
D. Gao, X. Wu, G. Shi, L. Zhang, Color demosaicking with an image
formation model and adaptive PCA, J. Vis. Commun. Image Represent. 23 (7) (2012) 1019–1030.
M. Aharon, M. Elad, A. Bruckstein, K-SVD: an algorithm for designing
overcomplete dictionaries for sparse representation, IEEE Trans.
Signal Process. 54 (11) (2006) 4311–4322.
J. Mairal, M. Elad, G. Sapiro, Sparse representation for color image
restoration, IEEE Trans. Image Process. 17 (1) (2008) 53–69.
L. Zhang, R. Lukac, X. Wu, D. Zhang, PCA-based spatially adaptive
denoising of CFA images for single-sensor digital cameras, IEEE
Trans. Image Process. 18 (4) (2009) 797–812.
K. Hirakawa, T.W. Parks, Joint demosaicing and denoising, IEEE
Trans. Image Process. 15 (8) (2006) 2146–2157.
D. Khashabi, S. Nowozin, J. Jancsary, A.W. Fitzgibbon, Joint demosaicing and denoising via learned non-parametric random ﬁelds,
IEEE Trans. Image Process. 23 (12) (2014) 4968–4981.
L. Zhang, X. Wu, D. Zhang, Color reproduction from noisy CFA data of
single sensor digital cameras, IEEE Trans. Image Process. 16 (9)
(2007) 2184–2197.
D. Menon, G. Calvagno, Joint demosaicking and denoising with
space-varying ﬁlters, in: Proceedings of IEEE International Conference on Image Processing (ICIP), IEEE, Cairo, 2009, pp. 477–480.
L. Condat, S. Mosaddegh, Joint demosaicking and denoising by total
variation minimization, in: Proceedings of IEEE International Conference on Image Processing (ICIP), IEEE, Orlando, 2012, pp. 2781–
2784.
L. Rudin, S. Osher, E. Fatemi, Nonlinear total variation based noise
removal algorithms, Phys. D: Nonlinear Phenom. 60 (1) (1992)
259–268.
T. Chan, S. Esedoglu, F. Park, A. Yip, Recent developments in total
variation image restoration, Math. Models Comput. Vis. 17 (2005)
1–18.
M. Elad, M. Aharon, Image denoising via sparse and redundant
representations over learned dictionaries, IEEE Trans. Image Process.
15 (12) (2006) 3736–3745.
X. Li, B. Gunturk, L. Zhang, Image demosaicing: a systematic survey,
in: Proceedings of IS&T/SPIE Visual Communications and Image
Processing, International Society for Optics and Photonics, San Jose,
2008, pp. 68221J-1–68221J-15.
A. Buades, B. Coll, J.-M. Morel, A review of image denoising algorithms, with a new one, Multiscale Model. Simul. 4 (2) (2005)
490–530.
H. Ji, C. Liu, Z. Shen, Y. Xu, Robust video denoising using low rank
matrix completion, in: Proceedings of IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), IEEE, San Francisco, 2010,
pp. 1791–1798.
H. Ji, S. Huang, Z. Shen, Y. Xu, Robust video restoration by joint
sparse and low rank matrix approximation, SIAM J. Imaging Sci. 4
(4) (2011) 1122–1142.

K. Chang et al. / Signal Processing: Image Communication 39 (2015) 264–279

[36] W. Dong, G. Shi, X. Li, Y. Ma, F. Huang, Compressive sensing via
nonlocal low-rank regularization, IEEE Trans. Image Process. 23 (8)
(2014) 3618–3632.
[37] J. Zhang, D. Zhao, W. Gao, Group-based sparse representation for
image restoration, IEEE Trans. Image Process. 23 (8) (2014)
3336–3351.
[38] B. Recht, M. Fazel, P.A. Parrilo, Guaranteed minimum-rank solutions
of linear matrix equations via nuclear norm minimization, SIAM Rev.
52 (3) (2010) 471–501.
[39] E.J. Candès, X. Li, Y. Ma, J. Wright, Robust principal component
analysis, J. ACM 58 (3) (2011) 11.
[40] T. Goldstein, S. Osher, The split Bregman method for l1-regularized
problems, SIAM J. Imaging Sci. 2 (2) (2009) 323–343.
[41] J.-F. Cai, S. Osher, Z. Shen, Split Bregman methods and frame based
image restoration, Multiscale Model. Simul. 8 (2) (2009) 337–369.

279

[42] P. Sonneveld, CGS: a fast Lanczos-type solver for nonsymmetric
linear systems, SIAM J. Sci. Stat. Comput. 10 (1) (1989) 36–52.
[43] E.T. Hale, W. Yin, Y. Zhang, Fixed-point continuation for l1-minimization: methodology and convergence, SIAM J. Optim. 19 (3)
(2008) 1107–1130.
[44] W. Lu, Y.-P. Tan, Color ﬁlter array demosaicking: new method and
performance measures, IEEE Trans. Image Process. 12 (10) (2003)
1194–1210.
[45] G.H. Golub, C.F. Van Loan, Matrix Computations, Johns Hopkins
University Press, Baltimore, MD, 2012.
[46] R. Barrett, M. Berry, T.F. Chan, J. Demmel, J. Donato, J. Dongarra,
V. Eijkhout, R. Pozo, C. Romine, H.V. Der Vorst, Templates for the
Solution of Linear Systems: Building Blocks for Iterative Methods,
SIAM, Philadelphia, PA, 1994.

DEVELOPING A REAL-TIME LOW-COST SYSTEM FOR SURGICAL SKILL TRAINING
AND ASSESSMENT
Gazi Islam1

Baoxin Li1

Kanav Kahol2

1

Arizona State University, Tempe, AZ, USA
Public Health Foundation of India, New Delhi, India
1
{gislam,baoxin.li@asu.edu}, 2kanav.kahol@phfi.org
2

ABSTRACT
Advances in the medical field require new and
progressive ways to intensify surgical resident training
and surgical skills learning. To address this need, in
addition to traditional training using realistic surgical
objects, simulator-based training programs have been
widely adopted. Both training methods need constant
presence of a competent surgeon to assess the surgical
dexterity of a trainee, which is not only costly but also
very subjective. This paper proposes a novel video-based
approach for assessing the trainee’s hand and tool
movements in minimally invasive surgical training.
Videos capturing surgical training actions are analysed to
automatically extract major skill measuring features and
to provide real-time performance feedback to the trainee.
The method has the potential of being extended to skill
evaluation in real surgeries at low cost.
Index Terms— Surgical training, skill assessment,
feature extraction, computer vision.
1.

INTRODUCTION

Surgery as a field of medicine requires both psychomotor
proficiency and cognitive proficiency [1]. Surgeons have
to learn the art of executing fine motor movements while
maintaining tissue integrity within the human body. This
is a challenging task, one that requires significant
investment from senior surgeons in mentoring the
residents. However, this one-to-one apprenticeship model
has significant limitations. First, there is a dearth of
trained surgeons who can be effective mentors. Secondly
even with the trained surgeons, surgical skill assessment
remains subjective.
Reports have shown that medical errors are still a
leading cause of deaths in the United States and a large
proportion of those errors are surgical errors [2]. Due to
the demand for greater accountability and patient safety in
health care delivery, effective training for improved
performance in surgery is becoming increasingly
important. One methodology that has recently emerged as
a viable alternative to conventional apprenticeship model
Thanks to NSF Grant No. 0904778 for funding.

is simulation-based training programs. Surgical
simulation allows surgeons to practice and improve their
skills in safe environment [3].
Simulators employing sophisticated sensors for
measuring a trainee’s movements are often out of reach of
residency programs due to cost consideration and
operational complexity. Further, many simulators
introduce sensor mechanisms that may interfere with a
trainee’s movements and introduce adaptations that are
not seen in real surgery [4]. Therefore, it is desired to
develop low-cost, non-interfering systems that support
objective skill assessment while being able to be
employed in both simulated environments and in the
future in real surgical environments.
In this work, we present a system for simulationbased surgical training in minimally invasive surgery
(MIS). Unlike open surgery, MIS uses keyhole incisions
in human body to operate in constrained environments
[5]. This type of surgery requires specific psychomotor
skills and hand-eye coordination that can be trained
through box-trainers and computer-enhanced simulation
trainers [6]. The key idea of the proposed method is to
employ cameras to capture a trainee’s surgical actions and
then to analyse the captured video for skill assessment
and for providing feedback. Surgical movements are
analysed by computing motion vectors in the videos using
an optical flow algorithm [7]. Based on the computed
optical flow, feedback scores assessing surgical
performance are computed. We assess surgical
proficiency as a multidimensional vector composed of
motion smoothness, surgical gesture proficiency and
number of errors. We test the hypothesis that this system
can accurately capture surgical proficiency by
differentiating between performance of experts,
intermediates and novices.
Our system is capable of giving trainees both realtime and summative feedback. Real-time feedback
provides measures to surgeons during training, which
helps them dynamically control the learning experience.
Summative feedback helps them get a summary of their
performance and additional measures. Duo to its non-

intrusive nature, the technique has the potential of being
be useful to assess a surgeon’s performance in an actual
surgical operation.
2.

BACKGROUND AND RELATED WORK

A simulator box developed by the Fundamentals of
Laparoscopic Surgery (FLS) is one of the widely used
simulators endorsed by the American College of Surgeons
(ACS) [8]. It consists of a number of non-procedure
specific simulation exercises incorporating most of the
psychomotor skills necessary for basic laparoscopic
surgery. Our system is built on top of the FLS simulator.
In iterative practice sessions on FLS, a trainer receives
only summative feedback after a certain number of
practice sessions. Currently there are two feedback
systems available. One requires the presence of an
observer to monitor exercise completion time and
committed errors and sends it to FLS. FLS scrutinize the
features and send back the result (pass or fail) in 4-6
weeks. The other requires constant presence of a
competent surgeon or post-hoc analysis on a recorded
practice of a trainer to subjectively assess the surgical
dexterity of the trainee by providing a composite score,
which is a costly process and often lacks objectivity.
Several video and sensor-based systems have been
developed to quantitatively access skill level [9].
However, existing systems have some drawbacks. For
example, the integration of wearable sensors interferes
with surgical skill execution [10]. Also, no work has been
done that combines features from both hand and surgical
tool movement [9]. This is critical in surgery wherein a
surgeon’s hand and tool movements are an important
component of ensuring patient safety. Many approaches
are specific to given simulators and thus may not be
transferable to assess a real-life surgery. The focus of the
proposed work in this paper is to develop a robust system
that is free from all of these drawbacks.

3.

METHODOLOGY

The proposed system in this research is designed to work
with the FLS trainer box. It analyzes the video image
rendered from one camera inside the box and two external
cameras (Fig. 1). The idea is to extract hand and tool
movement features to compute skill-measuring metrics or
scores for feedback to a user during and after training. We
elaborate the key components of the system below.
3.1. Object segmentation
Since the tool/object movement and their jerkiness and
placement can inform decisions about a surgeon’s
proficiency, an object segmentation algorithm is required.
We use Camshift (continuously adaptive mean-shift) in
OpenCV [7] for object segmentation. The method uses
Gaussian distribution to estimate the pixel distribution.
Exploiting the unique glove and object colors, the
algorithm works successfully in detecting a surgeon’s
hands (Fig. 2) and the surgical objects (Fig. 3).

Fig. 2. Illustrating the results of hand detection.

Fig. 3. Illustrating the results of object detection.

3.2. Motion detection
A Motion Template algorithm has been used in this
project to find the silhouettes of the hands and objects and
for motion tracking. The algorithm employs frame-toframe differencing technique to find object silhouette and
motion history images. From the silhouette of the object it
finds the gradient of the resultant movement (Fig. 4).

Fig. 4. Illustrating motion detection

3.3. Feature extraction

Fig. 1. Proposed system

Smooth steady motion is one of the most important
features for assessing surgical skill [11]. Motion
redundancy or extra hand movement is another important
factor that is made mostly by the inexperienced trainees
due to the learning to operate using long instruments and

eye-hand coordination in translating the 2-dimentional
image of the operating field from the videoo screen into a
3-dimentional mental image [5]. Im
mprecision in
laparoscopic surgery due to tremor has long been a
concern. Tremor consists of a “mechhanical-reflex”
component which is thought to originate frrom the central
nervous system and has a frequency rangge of 8–12 Hz
[11]. To capture these, both left and right hand and tool
motion gradient values are recorded in degrree (Fig. 5) and
later interpreted into 4 gestures – left, right,, up and down.
The gestures and their features and their sequences are
known to distinguish the proficiency leveels [11]. Three
features are derived from the duration of eacch gesture.

Redundant motions in vertical direction are
o depth. This motion is
responsible for poor perception of
analysed for both the hands and tools (Fig. 8). The plots
suggest that as the level of experttise increases, perception
of depth increases hence redun
ndant motion in vertical
direction reduces.
Tremors are very short movements and very difficult
to analyse subjectively. Howeveer, the motion detection
algorithm is able to detect very subtle
s
movement and the
system calculates tremors for botth hands and tools. Fig. 9
shows tremor features where it shows an increment of
tremor as the experience level decreases. Due to the use
of longer instruments, very subtlee hand movement causes
greater tool movement. ANOVA among the different skill
levels returns very significant for all three features
(p<0.05).

Fig. 5. Sample curves of Gradient of M
Motion.

3.4. Detecting Error
In peg-board transfer, one important traininng task on FLS,
dropping an object outside the reach oof the tool is
considered as error. The system automaticaally counts the
number of object when an exercise is complete and
records any missing object as an error (Fig. 6).

oothness.
Fig. 7. Motion smo

Fig. 8. Perception of depth.
Fig. 6. Error detection in peg-board traansfer.

4.

ED VIDEOS
EXPERIMENTS WITH RECORDE

Videos from users consisting of medicall students and
different post graduate year (PGY) of surrgical residents
have been collected while they were practiicing pegboard
transfer on the FLS box. The peg transferr exercise tests
hand-eye coordination, ambidexterity and depth
perception [8]. Each exercise is captureed with three
synchronous cameras- two cameras for captturing the hand
movements and one camera for captuuring the tool
movement. In total 52 videos were colleected from 32
novices (Medical Students), 12 intermediiates (PGY1 PGY3) and 8 experts (PGY5 - PGY4). Each video is
analyzed for the first 1000 frames. Fig. 7 shows motion
smoothness values of three levels of expeertise for both
hands and tools. The plot clearly shows thaat experts have
higher numbers of longer motion hennce smoother
movements than intermediates and novices.

Fig. 9. Hand/tooll tremor.

Fig.10. LDA an
nalysis.

We also used Random Forrest for classifying each
gesture data. 66% of the data for both hand and tool

gestures are used as training data. The rest 34% of data is
used for testing. Each gesture shows significantly high
true positive rate. Motion smoothness, redundancy and
tremor value for both hand and tool gestures are
normalized and Linear Discriminant Analysis (LDA) is
performed. Fig. 10 shows the LDA analysis result of both
hand and tool gestures where the data roughly conform to
3 clusters in both cases.
For error detection, each tool video was analyzed at
the end of the exercise. Sensitivity of the system is 100%
for our dataset, meaning no single drop was missed.
5.

APPLICATIONS OF THE SYSTEM

5.1. Efficient skill learning
The proposed video-based surgical skill assessment
technique can provide immediate feedback; hence it is
also being tested as a tool for efficient skill learning.
Initial results (Fig. 11) show steeper learning curve
however more data is being collected for analysis to
strengthen the hypothesis.

Scores

100

w/o on-screen
feedback

50

w/ on screen
feedback

0
1

3

5

7 No9of11
13 15 17 19
Trials

Fig.11. Improved average learning curve

Fig. 12. Video data captured from actual surgery

Scores

Comparison of scores
5
4
3
2
1
0

Experts' score
Smooth
Efficient
Precise

Surgical Video

Fig. 13. Cholecystectomy analysis

5.2. Testing in Real Surgery
Since the video-based approach is free from any type of
sensor integration, this technique can be potentially useful
for assessing performance in an actual surgical operation
(Fig. 12). We captured video of actual Cholecystectomy
surgery and performed some testing of the system. Fig. 13
shows the analysis of 15 Cholecystectomy videos where
smoothness, efficiency and precision scores for each
surgery were accessed by the system and compared with

the expert surgeons’ ratings. Each score shows very high
correlation with the experts’ scores.
6.

CONCLUSION

This paper proposed a video-based surgical skill
assessment technique. Algorithms were designed to
analyze videos capturing of a surgeon’s hand and surgical
tool movements in skill assessment. Preliminary results
using data from surgical residents training and real-life
surgeries have demonstrated the feasibility of the
proposed approach. Utilizing video data of the surgery
rather than costly wearable sensors is not only costeffective but also non-interfering and thus the research
points to a promising direction for developing low-cost
and non-intrusive surgical skill assessment techniques.
7.

REFERENCES

[1] Treves F. A Manual of Operative Surgery. London:
Cassell and Company, 1891. pp. 26-27.
[2] C. Richardson et al., To Err Is Human: Building a
Safer Health System, Nat’l Academies Press, 2000.
[3] Satava, R., The Revolution in Medical Education The Role of Simulation. Journal of Graduate Medical
Education 2009. 1(2): p. 172-175.
[4] Kahol, K. and M. Vankipuram. Hand motion
expertise analysis using dynamic hierarchical activity
modeling and isomap. 19th International Conference
on Pattern Recognition (ICPR), 2008.
[5] Aggarwal, R., et al., A competency-based virtual
reality training curriculum for the acquisition of
laparoscopic psychomotor skill. Am. J. Surg., 2006.
191(1): 128-133.
[6] Pamela B. Andreatta, E., et al., Laparoscopic Skills
Are Improved With LapMentor™ Training - Results
of a Randomized, Double-Blinded Study. Annals of
Surgery, 2006. 243(6): p. 854-863.
[7] Bradski, G. and A. Kaehler, Learning OpenCV:
Computer Vision with the OpenCV Library2008,
Sebastopol, CA: O"Reilly Media, Inc.
[8] Fundamentals
of
Laparoscopic
Surgery...the
definitive laparoscopic skills enhancement and
assessment module, http://www.flsprogram.org/
[9] Chen, J.Y., M., Sharma, R., Visual modelling and
evaluation of surgical skill. Pattern Analysis and
Applications, 2003. 6: p. 1-11.
[10] Aizuddin, M., et al. Development of Sensor System
for Effective Evaluation of Surgical Skill.
IEEE/RAS-EMBS Intl. Conf. on Bio. Rob., 2006.
[11] C. Riviere, R. S. Rader, and N. V. Thakor, “Adaptive
cancelling of physiological tremor for improved
precision in microsurgery,” IEEE Trans. Biomedical
Engineering, vol. 45, pp. 839–846, July 1998.

HTML Paper

ADAPTIVE VIDEO BACKGROUND REPLACEMENT
Baoxin Li and M. Ibrahim Sezan
Sharp Laboratories of America
5750 N.W. Pacific Rim Blvd.
Camas, Washington 98607, USA

ABSTRACT
A video background replacement algorithm is proposed, which
is based on background subtraction with adaptive background
modeling. Like one early work ([1]) in this direction, it does not
require a blue screen but needs only a pre-recorded background
scene image. The problem is formulated as one that detects
statistical outliers with respect to the given background. A twopass process, which refines initial segmentation based on the
statistics on a pixel’s neighborhood, is adopted in order to
suppress false positives in the background region while
increasing detection rate for the foreground object. Experiments
with real image sequences are presented, along with
comparisons with some other existing methods, illustrating the
advantages of the proposed algorithm.

1. INTRODUCTION
Conventional video superimposition methods used in virtual
studio environment have been largely based on the so-called
chroma-key technique [2], which always requires a monocolored physical background (typically a blue screen). With a
physical blue screen as the background, and assuming
foreground objects that do not have the same color, backgroundforeground separation can be easily achieved using color-based
segmentation, and the foreground objects can be put on a new
background. Recently, researchers have been proposing new
approaches that aim at eliminating the requirement of a physical
blue screen (thus enabling the background of a video to be
replaced e.g., in a consumer application). Recent work includes
references [1,3]. Although not always explicitly stated as so,
many other efforts on object detection in the computer vision
field such as [4,5,6] could also potentially provide a possible
approach to the background replacement problem.
Methods of [1] and [3] represent two distinctive types of
approaches: pixel-based and global-modeling-based approaches,
respectively. The method of [1] has the virtue of potentially
being able to provide accurate object boundaries in segmentation,
and computation is at minimum. But it may have difficulty in
handling large background change. Global methods such as [3]
handle large variation through explicitly modeling the
background as well as the foreground. However, in general they
cannot provide accurate boundaries. Also, it is difficult to obtain
accurate object (or background) models for complex problems. In
[4,5,6], although background modeling was done on a pixel-bypixel basis, it was intended for a moving object detection
problem. Therefore, finding a person could be only finding an
approximate blob, and detailed contours of the person are usually
not of concern.

In this paper, we present another approach to the background
replacement problem. In order to obtain accurate object
boundary, we adopt a pixel-by-pixel approach as in [1].
However, instead of using a fixed probability function for
classifying all pixels, we model the variations of each pixel by
keeping their respective means and standard deviations. The
mean is used to compute the difference between a new image and
the background, and the standard deviation is used to guide the
computation of the probability function. The background model
is adjusted dynamically with time. Furthermore, to handle small
spatial variations of a background pixel due to factors such as
aliasing during acquisition and small camera or background
object motion, a local support is utilized to calculate the
probability function. In Section 2, we describe our approach,
with examples to illustrate the idea. We then present
experimental results in Section 3, along with comparisons with
two previous approaches. In Section 4, we provide further
discussion and point out possible future extension of the work.

2. THE PROPOSED METHOD
The problem at hand can be explained in reference to Fig. 1,
where (a) shows the background image, and (b) is one frame of a
video sequence with foreground object. Ideally, we would like to
obtain a video containing the foreground object on a new
background image (i.e., virtual background). This seemingly
simple task is complicated by the following facts: 1) the
background and the foreground can be anything of any color.
Therefore, colors of background and foreground can coincide. 2)
the background scene can change due to acquisition noise,
shadow of foreground object, etc. 3) we are not only interested in
detecting foreground objects, but also want accurate object
boundaries. It would be intrusive to human eyes even if only
some small parts were falsely detected as foreground.
The proposed approach is illustrated by the flow chart in Fig. 2.
In order to provide pixel-level accuracy on object boundaries,
our approach is primarily based-on pixel-by-pixel background
modeling, and the segmentation is done by detecting statistical
outliers with respect to the given background (detailed in Section
2.1). To handle small background motion (or in general, small
spatial variations), we employ a second process for suppressing
false positives and increasing detection rate (Section 2.2).
For the representation of each pixel, we use its chromatic
components r and g which are computed by r=R/(R+G+B), and
g=G/(R+G+B). The reason of choosing the (r,g) space is that it
is relatively invariant to shadowing (which is common when a
foreground object appears). Also r and g are easy to compute
provided that (R,G,B) is the original input. In order to handle

385
2001 IEEE International Conference on Multimedia and Expo
ISBN 0-7695-1198-8/01 $17.00 © 2001 IEEE

0-7695-1198-8/01/$10.00 (C) 2001 IEEE

grey regions in an image, the intensity I should also be
considered. To avoid computing I, we use s=R+G+B instead.

(a)
(b)
Figure 1. The problem of video background replacement:
given a pre-recorded background image (a) and a video
with foreground object (b), the task is to extract the
foreground object and then superimpose it onto a new
background image (virtual background). (The black arrow
in (b) is added for analysis purpose. See text).

Input
Video

Initial Pixel
Classification

Refined BG/FG

Segmentation

Adapted
Background

Pre-recorded
Background
Image

Video With
New BG

Background
Adaptation

New
Background

Figure 2. Flow chart of the proposed approach

2.1 Background Modeling and Its Adaptation
To account for the variations of a given background pixel, simple
Gaussian modeling has been used in some background
subtraction algorithms, and often the model is updated as time
proceeds [4,5]. However, we have found that the Gaussian
assumption is in general not true, and even not close enough in
some cases. For example, in Fig. 3, we have plotted the
histogram for the r and g components of the pixel pointed by the
black arrow in Fig. 1(b), over a 100-frame period. It is obvious
that Gaussian distribution is not a good approximation in this
situation.

Figure 3. Histograms of r and g components for the arrowpointed pixels in Fig. 1(b) over one hundred frames.
There are more complex background modeling methods (e.g. the
Gaussian mixture models as in [6]) that are typically used in
surveillance applications where background variations are very
dramatic, and there are much background data to be utilized. In
fact, with a large amount of data, one may even perform nonparametric estimation of the distribution without specific
assumption about the underlying distribution. In the background
replacement problem as defined above, in general we do not have

much background data to support such estimation. In our
experiments, we assume that we have only one pre-recorded
background image. Thus we have adopted the following
approach in modeling the background: for each pixel x, we keep
its mean mx and standard deviation (STD) σx (mx and σx are 3-D
vectors with r, g, and s components). The background model
starts with the pre-recorded background image as the mean, and
with initial STD. The model is then updated after N frames in the
video have been observed. Since the video now contains
foreground objects, we can only use those pixels in the updating
process which are classified as background. Assuming that we
have a new mean and STD computed in time using N successive
frames in the video, the background model will be updated by the
following simple approach:

m x(new ) = α m x(old ) + (1 − α )m x(current )

σ x(new ) = α σ x(old ) + (1 − α )σ x(current )

with 0<α<1. Obviously, a smaller α would favor recent
measurements while a larger α leads to slow adaptation. Note
that for those regions that are classified as foreground, the
background model remains intact during the updating. The
above procedure is implemented by the “Background
Adaptation” box in Fig. 2, which uses the result of “Refined
BG/FG Segmentation” discussed in Sect. 2.2.
In the “Initial Pixel Classification” step in Fig. 2, each pixel of an
input video frame is compared with its corresponding mean mx in
the background model, and a difference vector dx=(drx, dgx, dsx)
is obtained. σx is used to guide how the difference dx contributes
to the classification of a pixel. Ideally, if the difference is large
compared with σx, we should decrease the probability of
classifying the current pixel as background. Formally, we define
the probability of a pixel x belonging to the background as

φ (dr , dg x , ds x ) if | d x |> c x
P( x ∈ BG ) =  1 x
φ 2 (drx , dg x , ds x ) otherwise

where cx is a constant vector, and a convenient choice is cx=2σx.
|dx|=(|drx|, |dgx|, |dsx|), and |dx|>cx is interpreted as any
component of |dx| being larger than its counterpart in cx. φ1 and
φ2 are probability functions and can be conveniently set to joint
normal distributions of (drx,dgx,dsx). Note that since the idea is to
treat the pixels differently according to their deviations from their
respective means, we can use joint normal distributions for φ1 and
φ2 respectively even though the Gaussian assumption is not valid.
In that case, using a larger threshold cx should compensate for
inaccuracies in the modeling. In our experiments, we have used
probability functions similar to those defined in [1] for φ1 and φ2.

2.2 Enforced Detection for Refined Segmentation
The mean and the STD may well categorize the temporal
variation of a static background that is relatively homogeneous.
However, a real world scene (especially an outdoor scene) often
contains highly textured objects and/or objects subject to small
motion, such as trees. Highly textured objects could cause spatial
and temporal aliasing, especially in consumer grade cameras. The
aliasing is worsened if it is tangled with small object motion. In

386
2001 IEEE International Conference on Multimedia and Expo
ISBN 0-7695-1198-8/01 $17.00 © 2001 IEEE

(1)

0-7695-1198-8/01/$10.00 (C) 2001 IEEE

Fig. 4 (top), two consecutive frames of an indoor background
video are shown. The differences between these two frames are
computed and shown in the bottom row (Notice that the
difference images are not scaled.). One can find dramatic changes
on the plant, even though its leaves are not moving (no wind)
during acquisition, With such a large variation, a background
pixel is very likely to fall outside the range of any background
modeling. This can cause false classification of a pixel,
regardless of the fact that the detection is based on simple
thresholding of pixel-to-pixel difference or on more complex
models.
To address this problem, a spatial neighborhood of a pixel is
considered to enforce the detection of background pixels.
Formally, the probability of a pixel x belonging to the
background is determined from its neighborhood N(x) as

P(x ∈ BG ) = max P( y i ∈ BG ) .
yi ∈N ( x )

(2)

P(x ∈ BG ) = min P( y i ∈ BG )
yi ∈N ( x )

(3)

when x lies in potential foreground regions.
The above enforced detection process based on Eqs. (2) and (3)
constitute the “Refined BG/FG Segmentation” box in Fig. 2. It
needs to be emphasized that the benefit of refining the
segmentation using Eqs. (2) and (3) cannot be obtained by
simply using morphological filtering. For example, the latter
would simply close some small holes that we want to retain.
Finally, the detected foreground objects are superimposed onto a
new background image. With the segmentation result given by a
probability for each pixel, we can perform the superimposition
using a feathering process as proposed in [1], in order to reduce
the artifacts due to misclassification. To this end, the output
image is computed as a weighted sum of the input image and the
new background image, with the weights being determined by
the probability values obtained during segmentation.

3. EXPERIMENTAL RESULTS
In this section, several experimental results are presented, with
both indoor and outdoor sequences. We also compare our results
to those obtained by two existing methods: pixel based method of
[1] and global method of [3].
Fig. 5 illustrates two relatively simple examples where the
backgrounds are simple scenes of office wall. It is worth pointing
out that in the example in the bottom row, there is a small breach
between the person’s left arm and his waist, which varies in size
and shape with the person’s movement. This type of detail is
difficult to catch with temporal tracking approaches (such as
contour tracking), but they would be obvious to a viewer if they
were not classified correctly.
Figure 4. An example showing how textured background
can significantly vary due to aliasing. Top: two
consecutive frames in a background video. Bottom: R
and G differences between these two frames.
Notice that by doing this, we may achieve higher detection rate
for background pixels, but at the same time we also suppress the
probability of a pixel to be classified as foreground object. The
result may be a broken or “eroded” foreground object. To
alleviate this problem, it is better to enforce the background
detection only in potential background region, and suppress the
detection in potential foreground region. To this end, we have
used a two-pass process: an initial segmentation is done by using
pixel-by-pixel comparison using Eqn. (1). Then a morphological
filtering is applied to find large and connected components,
which are used as potential foreground regions. This achieves the
effect of fusing local detection probability with global shape
information (through the assumption of spatial connectivity for
foreground objects).
With the initial segmentation and subsequent application of Eqn.
(2) for better background detection, a similar approach is applied
to potential foreground regions in order to increase the detection
rate for foreground objects:

Figure 5 Taking an instant trip to Hawaii. Two frames of the
input video are shown on the left, and corresponding frames
after background replacement are shown on the right.
In the example shown in Fig. 6, we compare our result with those
of [1] and [3]. Fig. 6(a) is one original frame, and the only
foreground object is the person . To obtain a global model for
the background, we have used a two-component Gaussian

387
2001 IEEE International Conference on Multimedia and Expo
ISBN 0-7695-1198-8/01 $17.00 © 2001 IEEE

0-7695-1198-8/01/$10.00 (C) 2001 IEEE

mixture model for (r,g,s), and the EM algorithm was used to
compute the model parameters. The model was then used to
classify all pixels in the video with foreground object (the
foreground object was not modeled). The result is shown in (b).
The algorithm of [1] generated the result in (c). For this
sequence, there was slight camera motion during acquisition,
which caused the errors in (c). Our result is given in (d). Note
that we did not perform any morphological postprocessing to
smooth the holes in the result from our approach.

(a)

(c)

information except for updating the background model. Other
methods of utilizing temporal information such as contour
tracking could potentially improve the performance. However, to
achieve robustness, temporal tracking algorithms often depend
on some assumed a priori knowledge about the underlying
object, and thus is less generic. Specifically, one may hope
contour tracking to be helpful in the example of Fig. 7, but
cannot expect it to catch the breach in Fig. 5 (bottom row).
Future work along this direction includes using temporal
information for improving segmentation results in domain
specific problems, e.g., using domain- tracking approaches.

(b)

(a)

(b)

(c)

(d)

(d)

Figure 6. Detected foreground object. (a) Original video. (b)
Result from global modeling of the background using
Gaussian mixture models. (c) Result of pure pixel-by-pixel
classification. (d) Result of the proposed method.
Fig. 7 is an example with outdoor background. The result using
pure pixel-to-pixel comparison algorithm with Eqn. (1) is shown
in (c) (similar result was obtained by method of [1]). In this case,
like the example in Fig. 3(c), the variations along the tree
branches have baffled the statistical modeling of the pixels.
However, with the enforced detection provided by Eqs. (2) and
(3), we were able to obtain the favorable result in (d).
Current software implementation of the proposed approach runs
at 10 frames /second for QCIF-sized video on a Pentium-III 800
MHz PC (with the background updating window size N=12).

4. DISCUSSION AND CONCLUSIONS
We have proposed an adaptive video background replacement
approach. The approach is adaptive in the sense that the
background model is adaptive in time, and the detection is
adaptive to the spatial local statistics of a pixel. The proposed
approach attempts to gain accuracy on the boundaries of the
foreground object which is typically absent in most of the global
statistical modeling approaches. In the meantime, it also tries to
overcome shortcomings of the pure pixel-by-pixel comparison
approach. Both of the goals are achieved by maintaining a
dynamic pixel-by-pixel background model and considering a
spatial
neighborhood
during
background/foreground
classification. One limitation of the proposed approach is its
dependency on a morphological filter to obtain an initial
segmentation, which is usually computationally expensive and
not always reliable. Also, the approach has not utilized temporal

Figure 7. Results of an outdoor consumer video. (a) Prerecorded background image; (b) Video with foreground
object; (c) Background replacement using pixel-to-pixel
comparison only; and (d) Result using the proposed method.

5. REFERENCES
[1] R.J. Qian and M.I. Sezan, "Video Background Replacement
without a Blue Screen". Proc. of IEEE International
Conference on Image Processing. Kobe, Japan, Sept. 1999.
[2] D.J. Chaplin, "Chroma Key Method and Apparatus". U.S.
Patent 5249039, 1995.
[3] Y. Raja, S.J. Mckenna, and S. Gong, "Segmentation and
Tracking Using Colour Mixture Models". Proc. of Asian
Conference on Computer Vision. Hongkong, Jan. 1998.
[4] K.P. Karmann, A.V. Brandt, and R. Gerl, "Moving Object
Segmentation Based on Adaptive References Images". in
Signal Processing V: Theories and Application, Elseview
Science Publishers, 1990.
[5] C.R. Wren, A. Azarbayejani, T. Darrell and A.P. Pentland,
"Pfinder: Real-time Tracking of the Human Body". IEEE
Trans. Pattern Analysis and Machine Intelligence, Vol.19,
No.7, pp.780-785, 1997.
[6] W.E.L. Grimson, C. Stauffer, R. Romano, and L. Lee,
"Using Adaptive Tracking to Classify and Monitor
Activities in a Site". Proc. of IEEE Conference on Computer
Vison and Pattern Analysis. Santa Barbara, CA, June 1998.

388
2001 IEEE International Conference on Multimedia and Expo
ISBN 0-7695-1198-8/01 $17.00 © 2001 IEEE

0-7695-1198-8/01/$10.00 (C) 2001 IEEE

1292

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 18, NO. 6, JUNE 2009

Exploiting Motion Correlations in 3-D Articulated
Human Motion Tracking
Xinyu Xu, Student Member, IEEE, and Baoxin Li, Senior Member, IEEE

Abstract—In 3-D articulated human motion tracking, the curse
of dimensionality renders commonly-used particle-filter-based
approaches inefficient. Also, noisy image measurements and imperfect feature extraction call for strong motion prior. We propose
to learn the correlation between the right-side and the left-side
human motion using partial least square (PLS) regression. The
correlation effectively constrains the sampling of the proposal
distribution to portions of the parameter space that correspond
to plausible human motions. The learned correlation is then
used as motion prior in designing a Rao–Blackwellized particle
filter algorithm, RBPF-PLS, which estimates only one group of
state variables using the Monte Carlo method, leaving the other
group being exactly computed through an analytical filter that
utilizes the learned motion correlation. We quantitatively assessed
the accuracy of the proposed algorithm with challenging HumanEva-I/II data set. Experiments with comparison with both the
annealed particle filter and the standard particle filter show that
the proposed method achieves lower estimation error in processing
challenging real-world data of 3-D human motion. In particular,
the experiments demonstrate that the learned motion correlation
model generalizes well to motions outside of the training set and
is insensitive to the choice of the training subjects, suggesting the
potential wide applicability of the method.
Index Terms—Partial least square regression, particle filtering,
Rao–Blackwellized particle filter (RBPF), 3-D articulated human
motion tracking.

I. INTRODUCTION

T

RACKING articulated human motion in video has many
applications including video-based surveillance, gesture
analysis, and human computer interaction. The task is challenging due to many reasons. First, practical human motion
models typically have a large number of degrees of freedom,
and, thus, pose estimation involves searching for a complex
and multimodal density in a high-dimensional state space.
This renders the powerful and commonly-used particle filter
(PF) [1] inefficient due to the “curse of dimensionality” and
particle degeneracy [2]. Second, features extracted from the
images are not sufficiently reliable for inferring the pose fully
and robustly due to issues like noise, self occlusions, and lack
of depth information. In addition, there is an inherent pose
ambiguity in kinematic analysis of motion in depth from only
Manuscript received May 19, 2008; revised February 05, 2009. First published April 14, 2009; current version published May 13, 2009. The associate
editor coordinating the review of this manuscript and approving it for publication was Dr. Ilya Pollak.
X. Xu is with the Sharp Laboratory of America, Camas, WA 98684 USA
(e-mail: xxu@sharplabs.com).
B. Li is with the Department of Computer Science and Engineering, Arizona
State University, Tempe, AZ 85281 USA (e-mail: baoxin.li@asu.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TIP.2009.2017131

Fig. 1. Evolution of the right and left knee joint angles.

2-D data. Although using controlled scene environment and
multiple cameras could ease this problem, as done in [14], the
challenges associated with search over a high-dimensional state
space based on noisy measurements still remain.
In this paper, we explicitly model the motion correlation
among body parts in the original state space, and utilize the
correlation in a statistical tracking framework to constrain the
sampling to portions of the parameter space that correspond to
plausible human poses, thus reducing the number of samples
that is needed to capture the peaks of the posterior distribution.
The correlation model is derived based on the symmetry and
periodicity of typical human motion such as walking and
running. For example, there exists strong correlation among the
motion of the right-side and the left-side of the body in normal
walking, as illustrated in Fig. 1, which plots the evolution of
the left knee and the right knee joint angles during walking. In
general, motion correlation may exist between any two parts
of the body, such as the upper-body and the lower-body, as
will be discussed in Section VI-B. However, to facilitate clear
presentation and without loss of generality, we consider only
the correlations between the right-side and the left-side of the
body in presenting the proposed approach. The essence of our
approach is to employ such correlation as strong motion prior
in a probabilistic inference framework to reduce the sampling
burden of the traditional sequential Monte Carlo (SMC) method
in tracking, mitigating problems caused by model ambiguities,
self-occlusion, and poor image observations.
Denote the left-side poses as (response variables) and the
right-side poses as (predictor variables). This paper proposes
to learn the correlation between
and
using partial least
square (PLS) regression [5], [6]. By projecting the training vectors of and into their latent structure, PLS regression generates deterministic, predictive coefficients that capture the latent relationship between and . It will be shown with experiments that these coefficients also generalize well for future
analytical prediction of from in a probabilistic inference
framework. We then utilize the learned correlation model in designing a new Rao–Blackwellized particle filter (RBPF) for 3-D

1057-7149/$25.00 © 2009 IEEE

XU AND LI: EXPLOITING MOTION CORRELATIONS IN 3-D ARTICULATED HUMAN MOTION TRACKING

articulated people tracking, which samples only a subspace of
the state variables with the Monte Carlo method, with the remaining variables being estimated with an analytical filter based
on the learned correlation model. The proposed RBPF-PLS algorithm combines the benefits of the representation power of PF,
the efficiency of the exact filter, and the effectiveness of motion
correlations in constraining the search of the state space into the
most likely regions, thus achieving “smart sampling.”
The remaining part of this paper is organized as follows.
In Section II, we briefly review the related work. Section III
presents the proposed approach for learning the motion correlation. The full tracking algorithm employing the motion
correlation model is presented in Section IV, followed by
experimental results (Section V) and discussion (Section VI).
We conclude in Section VII.
II. RELATED WORK
There is a large body of literature on 3-D articulated motion
tracking but relatively little work on explicitly exploiting the
motion correlations to develop methods that cope with inference
over high-dimensional state space. The efforts dealing with inference over high-dimensional state space are most related to
our method, and they can be categorized into two classes: statistical sampling and dimensionality reduction.
A. Statistical Sampling
Statistical sampling of basic particle filtering converges quite
slowly to the modes of the underlying distributions, especially
when the observation likelihood peaks in the tail of the prior.
Various improvements have been proposed. Cham and Rehg
[17] combined a mixture-of-Gaussians state model representation with CONDENSATION sampling. Choo and Fleet [10]
used a hybrid Monte Carlo (HMC) filter, which at each time step
runs a collection of Markov chain Monte Carlo (MCMC) simulations initialized using a particle filtering approach. Sidenbladh
et al. [4], [15] used particle filtering with importance sampling
based on either a learned walking model or a database of motion
snippets, to focus search in the neighborhood of known trajectories. Deutscher and Reid [14] proposed an annealing framework
in a multicamera setting to speed up CONDENSATION [16].
They reported very good results for unconstrained full-body motion, for images with clean, dark background.
Sminchisescu and Triggs [8] proposed a hybrid search algorithm that combines inflated covariance-scaled sampling and
descent-based continuous optimization. The method makes
the dynamical noise adaptive by estimating the covariance
of the posterior likelihood and using this for noise scaling.
Adaptively identifying and sampling parameters with high
variance is useful, but kinematic parameters usually have quite
strong interactions that make simple axis-aligned sampling
questionable. Also, the principal axes of the covariance change
drastically depending on the viewing direction, and, thus, no
camera-position independent noise inflation suffice, as pointed
out by [24]. In [18], Sminchisescu and Triggs also used a modified MCMC algorithm to explore the multiple local minima
inherent in fitting a 3-D model to given 2-D image positions of
the joints.
Despite these advances, statistical sampling of a high-dimensional space in visual tracking remains to be a challenging task,

1293

especially in face of noisy and incomplete measurements in 3-D
tracking applications.
B. Dimensionality Reduction
Many problems in computer vision and pattern recognition
suffer from the “curse of dimensionality.” Dimensionality
reduction is shown to be effective in discovering the intrinsic
structure of the high-dimensional data and in alleviating this
problem. Spectral manifold learning techniques [21]–[23] have
been shown to be capable of unfolding the highly nonlinear
structure of the motion data [13], [26], [27]. But they do not
provide a probabilistic density model over the poses, and lack
a mapping from the latent to the pose space and a model of
the dynamics, all of which are often needed in typical tracking
tasks. Elgammal and Lee [13] fitted a radial basis function
(RBF) model to the lower-dimensional data after performing
local linear embedding (LLE) [21]. However, the model devoid
of a latent space prior is not fully probabilistic and there are no
mappings to the original space. Such a mapping is needed in a
particle filtering framework since image evidence needs to be
generated from each particle in the original pose space in order
to assign a likelihood score. Li et al. [25] proposed to learn a
mixture of factor analyzers to approximate the pose manifold
locally and then use a set of linear mixture functions to enforce
global coordination between the local factor analyzers. The
model does not guarantee to preserve structural properties of
the data in the latent space, as spectral methods do.
Sminchisescu and Jepson [11] augmented spectral embeddings (i.e., Laplacian Eigenmaps [23]) with both Gaussian mixtures latent space priors and RBF mappings to reconstruct pose
from latent positions. The mapping is more difficult to compute,
and the model is trained piece-wise. Recent work [28] complements it by combining the geometric and computational properties of spectral embeddings with the probabilistic formulation
and the mappings offered by latent variable models.
Gaussian process latent variables models (GPLVM) [29] provides probabilistic inverse mapping, and has been used in [12]
for 3-D articulated motion tracking and in [3] for pose synthesis
given kinematics constraints. In general, GPLVM lacks a latent
space prior. To compensate this, the authors of [12] used an augmented (latent, ambient) state for tracking, which renders the
state estimation problem even higher dimensional.
In summary, dimensionality reduction is effective to reduce
the dimension of the pose space, but it lacks a consistent way
of relating the latent and the observed quantities. Since it does
not explicitly model the motion constraints in the original state
space, the learned latent coordinates may not be directly useful
for visual tracking.
C. Proposed Method
Our method generates smart samples to capture the peaks of
the posterior density by characterizing the correlations among
state variables. This allows us to partition the dynamics into
individual groups. The method presented in this paper builds
upon and extends beyond our previous work [39], where we reported a preliminary exploration of the basic idea of correlation
modeling.

1294

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 18, NO. 6, JUNE 2009

and
is a
where
scalar denoting the regression coefficient, and the residual
error. For multiple-valued left-side pose matrix , we gather
into an
coefficient matrix . Then
(1) becomes
(2)
Given training data (responses) and (predictors), our obwhich captures
jective is to use PLS regression to estimate
the motion correlation and can be used for future prediction of
from a given new .
Fig. 2. Human body model. This is the same model as used in [15].

A. Learning Motion Correlation: The Algorithm

RBPF has been studied in 2-D tracking [9], [37], [43], fault diagnosis for robot [36], and integrated aircraft navigation [35]. In
our previous work [43], we integrated a dependency relationship
between the location of an object and its scale that is imposed
by the camera-scene configuration into the RBPF for 2-D object tracking. That dependency model cannot be applied to 3-D
human tracking. Likewise, there is no direct way of extending
the methods in [9], [35]–[37], and [43] for 3-D human tracking.
In this paper, we propose to learn such dependency model from
limited amount of data and then use the learned model to design
an RBPF for articulated 3-D human tracking, thus achieving
both smart sampling and dimension reduction at the same time.
III. LEARNING MOTION CORRELATION USING
PLS REGRESSION

In 3-D articulated human tracking, although the number of
manifest variables is large, there may be only a few underlying
or latent variables that account for most of the variations in both
the predictors and the responses [11]. Our idea is to use PLS
regression to extract these latent variables that account for the
manifest predictor variation as much as possible, and meanwhile
model the responses well. Thus, only strong systematic variation
in with good correlation to should be included in the prediction model. PLS regression achieves this by maximizing the
sample covariance between a projection of and a projection
of as follows [40]:

(3)

We adopt the approach of [15] to model a human body as ten
cylinders connected at revolute joints, the angles of which are
represented by Euler angles (see Fig. 2). Each limb has a local
coordinate system with the axis directed along the limb. Rigid
transformations are used to specify relative positions and orientations of parts and to change coordinate frames. A kinematic
tree, with the torso at its root, is used to order the transformations between the coordinate systems of different limbs [15].
The state-space is 25-D, including 6-D for the global position
and orientation, 3-D for the head orientation, and 8-D for the
left-side and the right-side joint angles, respectively.
Motivated by the observation that there exists strong correlation between the left-side and the right-side body motion in
specific human activities like walking and jogging, we design
a method based on PLS regression [5] to capture this correlation. Clearly the global position and orientation and the head
orientation should not be included in this correlation analysis.
Specifically, we arrange observations of the right-side and the
left-side joint angles into two matrices, and , respectively,
and
in size
, where the columns
correspond to the joint angles (variables) and the rows to the observations at discrete time. For a single column vector in , it
is approximated by a linear combination of the column vectors
of

(1)

where and denote the weight vectors for and , respectively, and
is the covariance matrix between
and .
Equation (3) is equivalent to

(4)
In (4), the maximal correlation is balanced with the requirement
to explain as much variance as possible in both the -space and
the -space.
We use NIPALS [6] to implement the PLS regression. It finds
weight vectors and such that

(5)
where
denotes the sample covariance between the score vectors (the latent vectors) and . The NIPALS
algorithm starts with random initialization of the -space score
vector and repeats a sequence of the following steps until convergence [6]:

(6)

XU AND LI: EXPLOITING MOTION CORRELATIONS IN 3-D ARTICULATED HUMAN MOTION TRACKING

1295

Fig. 3. (a) Cumulative variance captured by PLS regression for the S2 R and L matrix. (b), (c) Project each training point of left-side pose L onto the 3-D latent
space for S2 (b) and S3 (c). The first 3 LVs are used for the projection. Before projection, the R and L have 8 DOF.

After this process, the mean-centered
matrix and the
matrix are decomposed into the form
mean-centered
(7)
(8)
where the
are
matrices of the extracted latent vecmatrix and the
matrix represent
tors (LVs), the
matrix and the
matrices of the loadings, and the
matrix are the matrices of the residuals. Each column vector
in and , denoted by and , are the - and -space latent
. Each column vector
vectors, they are given as
in and , denoted by and , correspond to the vectors of
loadings, they are computed as coefficients of regressing on
and on , respectively:
and
[5].
The correlation matrix is then finally given by [5]
(9)
In our work, for the training right-side poses , the left-side
. For a given
poses estimated by PLS regression is
testing right-side poses , the predicted left-side poses is
.
This PLS-regression-based learning method has several advantages. First, it is superior to principle components regression
(PCR) [38] in constructing a good predictive model in that PCR
uses PCA to extract latent vectors [4], [15], [19], [20], which
space, but may
yields informative directions in the predictor
surface. In
not be associated with the shape of the response
contrast, PLS regression finds components from that are also
relevant for . Second, by using PLS, our method needs only
a small amount of training data to learn a reliable model. This
will also be demonstrated with experiments in Section V. Third,
the training points can be in any random order, eliminating the
need of motion alignment. Lastly, motion variation among different subjects can be accounted for by learning from multisubject motion data simultaneously. For doing so, we only need to
concatenate the training right-side (left-side) poses of multiple
, and then we follow the same resubjects into a single
gression procedures as described before.
B. Learning Motion Correlation: Results and Analysis
We use primarily HumanEva-I data set [32] in evaluating our
motion correlation learning method, for human walking activity.
The HumanEva-I dataset contains four subjects (we name them

S1, S2, S3, and S4) performing a set of six actions each in three
separate trials (two with synchronized motion and video, and
one with motion capture alone). The dataset is split into three
disjoint subsets: training, validation and testing. Trial 3 contains
only motion capture data and is organized into the training segment. Trial 1 contains synchronized motion capture and video,
and is partitioned (equally) into the validation and training segments. In all cases, trial 2 is reserved in its entirety for testing,
and the motion capture for trial 2 is withheld. All the activities
of subject S4 are withheld for testing.
We use S1, S2, and S3 walking activity trial 1 training set to
learn the correlation, and then evaluate the learned correlation
by predicting the left-side poses for the corresponding validation
set. In each case, before PLS regression, both and are made
mean-centered, and the number of training joint angle vectors is
roughly 450, corresponding to poses obtained from 450 frames.
In order to get a reliable predictive model, we randomly partition
the data into ten-fold and the model is obtained by ten-fold cross
validation. Since our objective is to construct a good prediction
model instead of dimensionality reduction, we keep all the eight
LVs.
1) Variance Captured and Analysis of the Latent Space:
Fig. 3(a) shows the cumulative variance captured by PLS
LVs
regression for and of S2. We can see that 8
explain 100% of the variance of , and 81.94% of the variance
of . It is often true that the higher the variance captured, the
lower the prediction error. However, increasing the number
of training points may not necessarily increase the percentage
of the variance captured by the model, indicating that even
with a small amount of training data the method should still
be effective. In Fig. 3(b) and (c), we project each training
of S2 and S3 onto the 3-D latent space spanned
point in
by the three LVs that capture the most variance. That is, the
coordinate of each point in the trajectory of Fig. 3(b) and (c) is
. Fig. 3 shows that the latent trajectory
of S2 is less smooth than S3, this is mainly due to the large
motion variation in the S2 motion capture data. In (b) and
(c), the 3-D latent trajectories for S2 and S3 are very similar,
indicating that people walk in a similar way intrinsically (from
the perspective of the latent space).
2) Correlation Map: Fig. 4 illustrates the cross correlation
between each variable of the right-side poses (vertically) and
each of the left-side poses (horizontally) computed for subjects
S1, S2, nd S3 in the original pose space. The correlation maps

1296

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 18, NO. 6, JUNE 2009

Fig. 4. Cross correlation between R and L for subjects S1 (left), S2 (middle), and S3 (right) in HumanEva-I dataset.

Fig. 5. (a) Predicted left elbow versus the true left elbow for S2 trial 1 validation sequence. (b), (c) Motion correlation is learned from S1 (b) and S2 (c) trial 1
training sequence, and we predict the left-side poses of the S1/S2/S3/S5 trial 1 validation sequences. The x-axis in (b) and (c) denote the index of the left-side joint
angle, the y -axis in (b) and (c) denote the prediction RMSEs.

for S2 and S3 look very similar, indicating that they walk in
similar styles.
3) Prediction Error and Reliability of the Correlation Model:
Fig. 5(a) plots the evolution of the PLS-predicted left elbow joint
angle and the corresponding ground truth in S2 trial 1 validation sequence, both in radian. The Euclidean distance between
the ground truth and the predicted angle is 0.062 radian which
is negligible comparing to the large range of angle variation
, implying that the prediction is very accurate.
The reliability of the motion model is evaluated based on the
prediction root mean square errors (RMSEs) obtained by cross
validation among different subjects. That is, we learn the motion correlation from one subject, and then apply it to predict
the left-side poses of the other subjects, and we compute the prediction RMSEs. For left-side joint angle , the RMSE is com, where
is the th
puted as
left-side joint angle at frame , and
the corresponding ground
truth joint angle. In Fig. 5(b) and (c), the motion prior is learned
from S1 and S2 trial 1 training sequence respectively, and we
predict the left-side poses for the S1/S2/S3/S5 trial 1 validation sequences. It can be seen from Fig. 5(b) and (c) that the
prediction error is the lowest when the training and testing subject is the same. This conforms to our intuition. Also, except
for the left knee, the errors for all the other variables are small,
indicating that the correlation model is reliable. The relatively
large error in the left knee might be caused by the fact that knee
joint presents more motion variations than the other joints, and,
hence, the correlation model does not capture the left and right
knee correlation as well as it does with other joints. Fig. 5(b)
and (c) also tells us that the walking style of S5 is quite different from S1, S2, and S3 because of the large error gap among

them, and that S2 walks in a similar fashion to S3. Note that
the reliability analysis presented here is solely based on PLS
learning and PLS prediction, no temporal inference (i.e., online
tracking) has been incorporated. The effectiveness of the learnt
correlation for online tracking is presented in Section V.
IV. THREE-DIMENSIONAL HUMAN TRACKING WITH THE
LEARNED MOTION CORRELATION
We formulate the tracking problem as one of estimating the
posterior probability distribution over the state-space given a
. Particle Filtering (PF)
sequence of image observations
[1] represents a distribution by a set of random particles with
associated weights. While having many good properties, this
method becomes impractical when the dimensionality of the
state-space becomes large. In this work, we marginalize the
left-side pose variables by utilizing the learned motion correin (9)]. The marginalization is
lation [through the matrix
realized by a variance-reduction technique, Rao-Blackwellisation [41]. The resultant method is often called RBPF. The
key idea of RBPF is to partition the original state-space into
(root variables) and
(leaf variables), such that
two parts,
is a distribution that can be computed exactly
is estimated
conditional on , and the distribution
using Monte Carlo methods like particle filtering [1]. The justification for this decomposition follows from the factorization
of the posterior probability [42]:
(10)
In our work,
includes the global position and orien, head orientation
, plus the right-side
tation
, i.e.,
consists of the
poses

XU AND LI: EXPLOITING MOTION CORRELATIONS IN 3-D ARTICULATED HUMAN MOTION TRACKING

1297

Fig. 6. Proposed RBPF-PLS for 3-D articulated human motion tracking. We follow the same set of notations as in Partitioned Sampling [7] to facilitate comftive
study of the two algorithms in Section VI-A.

left-side poses. If the same number of particles is used in a standard PF and a RBPF, the latter will provide better estimates for
is smaller
two reasons: first, the dimensionality of
; second, optimal algorithms may be used to
than
estimate the leaf variables.
Given the learned motion correlation and the image mea, we rewrite (10) into
surements

denotes the average interframe angular difference estimated
from the training poses. The noise is drawn from a normal distribution with diagonal covariance where the standard deviation
of each body angle equals to the maximum absolute interframe
angular difference [14]. As in [11], we further enforce a hard
prior that eliminates any particle corresponding to implausible
body poses to reduce the search space. For example, we discard
particles whose angles exceed anatomical joint limits and which
produces interpenetrating limbs.
B. Kalman Prediction for the Leaf Variables

(11)
In the second line of (11), the formula in the rectangle repreis realized by
sents the conventional PF, and
Kalman filtering. In the third line of (11), the likelihood evaluation is computed until after the entire state vector is obtained.
Fig. 6 depicts the flow of the proposed algorithm, where the
symbol denotes resampling, * denotes convolving with dynamics, and represents multiplication by the observation density. At the first step (Section IV-A), we propagate the root variaccording to a first-order temporal dynamic model. At
ables
the second step (Section IV-B), we predict the leaf variables analytically using Kalman Filtering prediction, assuming a linear) and
Gaussian sub-structure between the right-side ( in
the left-side
poses conditional on the learned correlation
. At the third step (Section IV-C), the predicted root and leaf
. At
variables are weighted by the image likelihoodp
the fourth step (Section IV-D), we use resampling with replacement to select particles with large weights and discard particles
with small weights. And in the last step (also in Section IV-D),
we update the leaf variables using auxiliary measurements.
Similar to PF, RBPF represents the posterior density by a
set of weighted particles. But unlike PF, each particle now
, which we denote
maintains not only a sample from
, but also a parametric representation of the distribution
as
, which consists of the mean vector of the leaf
variables,
, and the estimation error covariance for
. So each particle is
the leaves
. The details of the
represented by a triplet
algorithm are presented below.
A. Temporal Dynamics
While sophisticated dynamic models (e.g., [33]) may be
learned with sufficient amount of data, without assuming the
availability of such data (which is not easy to obtain for the
3-D human tracking task), we use only a simple first-order
Gauss–Markov dynamic model for its simplicity
(12)

Once the root variables are propagated, the leaf variables can
be analytically computed by making use of the motion correlation using a Kalman filter. Conditional on the root variables
and the correlation model, the leaf variable is estimated using
a linear-Gaussian sub-structure specified by (13)

(13a)
(13b)
Equation (13a) relates the leaf state at time
to the leaf state
at time . Equation (13b) relates the leaf state at time to the
auxiliary measurement , which is simply the leaf state estimates in the previous time step (see Section IV-D for details).
The random variables and represent the process and measurement noise. The variance of the process noise and the measurement noise is denoted by and respectively, which are
defined similarly as done for the root variable dynamics. While
matrix and can take different forms depending on the application, in this work they are set to be identity matrices.
The leaf variables are predicted by (14)
(14a)
(14b)
Equation (14a) predicts the leaf mean one step ahead, where
the matrix denotes the learned motion correlation. Equation
(14a) means that each time as the right-side joint angles increase
from
to , the left-side joint anby
. Essentially, it implies that
gles increase by
the predicted left-side angular velocity can be approximated by
a linear combination of the right-side angular velocity through
the correlation model . Equation (14b) performs leaf covariance prediction.
from
One may propose to learn the correlation model
the angular velocity data blocks (instead of the joint angles).
However, we found that regressing on joint angles is more accurate and reliable for tracking than regressing on joint angular
velocity. Fig. 7 shows the S5 tracking results and the tracking

1298

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 18, NO. 6, JUNE 2009

Fig. 7. Tracking S5 with motion correlation learned from the angular velocity of S5 training motion capture data. The average tracking error is 58.21 mm which
is slightly larger than using motion prior learned from joint angle data (Fig. 10 top right error graph). This suggests that learning motion correlation from angular
velocity may lead to less accurate tracking.

Fig. 8. Cumulative variance captured by the correlation model learned with S5
training joint angular velocity data.

is the full state vector,
is the set of
where
is the edge distance map.
projected points and
Silhouette-Based Likelihood: The foreground silhouettes are
largely invariant to clothing, lighting, pose motion and environment and, hence, provide a reliable feature for a general motion
capture system. Silhouette maps (Fig. 9, left) are generated by
background subtraction [32]. Each background pixel is modeled
as a mixture of Gaussian distributions, which is learned over
1000 background images using EM. Assuming that the distribution over foreground is uniform, the following classification
criterion for each pixel is obtained:
(16)

Fig. 9. Left: Four image silhouettes. The image observations are incomplete in
that some limbs are missing and the data is noisy. Right: the edge map.

error with correlation learned from the angular velocity of
the S5 training data. The average tracking error is 58.21 mm,
which is slightly higher than using motion prior learned from
joint angle data. This suggests that learning motion correlation
from angular velocity may lead to less accurate tracking. Fig. 8
shows the cumulative variance captured by the correlation
model learned with S5 training angular velocity data, eight
LVs capture 100% of the variance in , whereas only 59% of
the variance in is discovered, indicating that the correlation
model learned with angular velocity may not be reliable for
future predictions.
C. Image Likelihood
We constructed image likelihood on the basis of two image
features: edges and foreground silhouette, as in [14] and [34].
Edge-Based Likelihood: For edge-based likelihood, we first
use gradients that have been thresholded to obtain binary edge
maps; this map is then convolved with a Gaussian kernel to yield
an edge distance map which indicates the proximity of a pixel
to an edge (Fig. 9, right). The edge negative log-likelihood is
then computed by projecting onto the image sparse points along
the edges of all cylinders of the model and computing the mean
square error (MSE) of the edge map responses [14]
(15)

denotes the R, G and B values of the pixel in conwhere
sideration,
and represent the mean (1 3 vector) and diagonal covariance matrix (3 3) of the th Gaussian mixture
is the total number of Gaussian mixtures.
component, and
is used to control the risk of the misclassification. Two sets
of optional postprocessing routines are implemented based on
the connected component analysis. First, connected components
that are below a threshold in size can be filtered out. Second, all
but the largest connected component can be filtered out.
The silhouette negative log-likelihood is then computed as the
MSE between the predicted and the observed silhouette values
over a number of sample points inside the projection of the limbs
(17)
where
is the set of projected points inside of the silhouette
.
map
Finally, the edge and silhouette measurements are combined
to give the particle weight
(18)
Measurements from more than one camera can be combined
similarly
(19)

D. Resampling and Kalman Update
Resampling with replacement is performed after weighting
each state particle with the likelihood. Following that, Kalman

XU AND LI: EXPLOITING MOTION CORRELATIONS IN 3-D ARTICULATED HUMAN MOTION TRACKING

1299

Fig. 10. Track subject S5 using correlation model learned from S5 training (top), S1 trial 1 training (middle) and S1S2S3 trial 1 training data (bottom). The
right-most column shows the error graph for each case. Note that in the bottom row, we learn correlation from three subjects (S1, S2, and S3) simultaneously. We
use four camera views and 1000 particles for all the three sequences, every 15th frame from camera 2 are shown here.

update, (20a)–(20c), is performed to update the leaf variables.
is the Kalman gain which aims at minimizing the
In (20a),
posterior error covariance. Equation (20b) incorporates a meainto the a priori leaf state estimate to obtain an
surement
is simply
improved a posteriori leaf state estimate. Here,
the leaf state estimates in the previous time step. One may quescannot serve as obsertion that the previous leaf variables
vations since they are not the true image measurements. This is
practically not an issue because from previous steps they have
already incorporated the information from the true measurements (edges and silhouettes). Essentially the auxiliary obsercan be viewed as being indirectly linked to the root
vation
variables and the true measurements through a non-Gaussian
. Equation (20c) updates the
nonlinear function
posterior error covariance
(20a)
(20b)
(20c)
V. TRACKING RESULTS AND ANALYSIS
We have done extensive experiments to track circular walking
performed by various subjects in the HumanEva-I/II data set
(subject S1, S2, S3, and S4) [32] and subject S5 (the S5 sequence does not belong to HumanEva-I/II). The data were captured using multiple cameras. We used the ground truth motion
capture data in HumanEva-I to estimate the motion correlation.
In all the experiments, the first frame was initialized with the
ground truth. The training images and the testing images are
always disjoint. The image measurements for S5 sequence are
clean and good, but the image silhouette and edge maps obtained for the HumanEva-I/II sequence are quite noisy and contain missing limbs.

We have quantitatively evaluated the performance of our
method and compared it with annealed particle filtering (APF)
[14] and standard PF [1] using the error measure proposed in
[34]. First, we computed the absolute error in 3-D for each
particle in each frame based on 15 virtual markers that corresponds to the locations of joints and limb endpoints: for each
was computed as the
particle , the full pose error
average distance in millimeters of all virtual markers
with respect to the true pose
(21)
returns the 3-D location of marker for the body
where
model . Then, we used the optimistic error [34] which is the
minimum error over randomly-sampled 100 particles regardless
of their weight to compute the posterior distribution error for
each frame. For a sequence of frames, we computed the average distance as the mean error over frames.
A. Insensitivity of the Learned Correlation to the Choice of
Training Subjects
Our experiments show that the learned correlation model
is insensitive to the choice of the training subjects. As shown
in Fig. 10, we track S5 using the correlation learned from S5
(top row), S1 (middle), and S1S2S3 (bottom), respectively.
Four cameras and 1000 particles per frame are used for the
three sequences. If we learn the correlation model from subject
S1, and apply it to track S5, the tracking performance (error
51.66 mm, middle row in Fig. 10) does not degrade comparing
to learning correlation from S5 and applying it to track the
same subject S5 (error 57.81 mm, top row in Fig. 10). In this
particular experiment, the error is even lower when the training
and testing subjects are different than when they are the same.
In addition, as depicted in the bottom row of Fig. 10, we can

1300

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 18, NO. 6, JUNE 2009

Fig. 11. Track S1 (top), S3 (middle) in HumanEva-I, and S4 (bottom) in HumanEva-II. The three sequences use the same correlation model which is learned from
S1S2S3 trial 1 training data. The number of cameras used for tracking is 7 (top), 5 (middle), and 4 (bottom), respectively. 1000 particles are used to track S1 and
S4 and 600 particles for S3. We did not apply hard prior for these three sequences. Due to the limited space, every 15th frame from one view is shown here.

Fig. 12. (a) Error on HumanEva-I S1, S2, and S3 trial 1 validation sequences. (b), (c) Quantitative comparison with APF and PF on S5 (b) and S1, S2, S3 (c).

learn a correlation model from multisubject motion data (the
concatenation of S1, S2, and S3 joint angle data), and apply
it to track other subject (S5) that is not in the training set
of correlation learning. This suggests that our algorithm is
insensitive to the choice of the training subject. Notice that, the
subject walks circularly and severe self-occlusion happens periodically. Nevertheless, our method maintains good tracking.
The errors of the first few frames are always very small since
we initialized the first frame with the ground truth.
B. Generalization Ability of the Learned Correlation
Fig. 11 shows the results on tracking S1, S3 in HumanEva-I,
and S4 in HumanEva-II. The three sequences use the same correlation model which is learned from S1S2S3 trial 1 training
data. Although the training data does not include any information from S4, the performance of tracking S4 is still very good.
This demonstrates that the learned motion correlation is able to
generalize to different walking styles and to motions outside of
the training data. Note that the image data for the S1 and S3
sequences are very noisy, and in some frames, some limbs are
missing in almost all the camera views (see Fig. 9). In this case,
the learned motion correlation plays a critical role in dealing
with the missing data. Fig. 12(a) shows the tracking error on
HumanEva-I S1, S2, and S3 sequences. Note that part of the
tracking error is due to the inaccurate static body measurement

like the limb lengths provided in the training data. The quantitative error for S4 is not shown in Fig. 12(a) as the ground truth
for S4 in HumanEva-II is not available.
C. Quantitative Comparison With APF and PF
We compared our method against APF and PF quantitatively
using both S5 and HumanEva-I/II datasets. Fig. 12(b) shows
the tracking errors on S5 obtained by RBPF-PLS, APF, and
PF, respectively. In order to fairly compare these three methods
using the S5 sequence, we used the same configurations for the
three methods: 1000 particles per frame (for APF, it is equal to
five layers annealing with 200 particles per layer), a likelihood
based on silhouettes and edges, first-order temporal dynamics,
applying a hard prior that discard particles corresponding to infeasible body poses, and four camera views. For RBPF-PLS, the
motion prior was learned from S1 trial one training sequence.
This experiment with S5 sequence demonstrates that the proposed method performs slightly better than both APF and PF in
the case when the image measurements are clean and good.
With more challenging HumanEva-I/II data sets, tracking
is more difficult because of the incomplete and noisy image
observations and self-occlusions. Fig. 12(c) plots the tracking
error on S1, S2, S3 trial 1 validation sequences obtained by
the three methods. The mean error over three subjects obtained
by RBPF-PLS is 148.67 mm, which is significantly lower than
those obtained by APF (171.09 mm) and PF (165.22 mm).

XU AND LI: EXPLOITING MOTION CORRELATIONS IN 3-D ARTICULATED HUMAN MOTION TRACKING

1301

Fig. 13. (a) Particle filtering propagate samples all over the 2-D fmeter space. (b) The proposed RBPF-PLS partitions the state space into two parts, R and L. The
correlation between R and L is used to constrain the propagations of the samples to only portion of the fmeter space that are more likely to conform to plausible
motions, leading to reduced sampling volume. (c), (d) Projections of the ten random particles, obtained from particle filtering (c) and our method RBPF-PLS (d),
onto the two sample frames.

Fig. 14. Algirithm of partitioned sampling [7].

In summary, the proposed method performs significantly
better than both APF and PF when the image measurements are
incomplete and noisy, and it performs equally well with APF
when the image measurements are already good. This implies
that RBPF-PLS is advantageous over APF in that RBPF-PLS
can deal with bad image measurements caused by imperfect
feature extraction which is often true in the real world applications; thus, it will be more robust than both APF and PF when
being applied across various imaging conditions.
VI. FURTHER DISCUSSION
A. RBPF-PLS Versus Particle Filtering Versus Partitioned
Sampling
In this subsection, we compare our method with particle filtering [1] and partitioned sampling [7] at the algorithmic level
in order to better understand the proposed algorithm and its
properties.
For simplicity, we discuss the problem in a 2-D state space.
As shown in Fig. 13(a), particle filtering propagates all the variables of a state sample jointly, so the samples will be populated
all over the 2-D plane. A lot of samples are needed to cover the
2-D plane in order to locate the good samples, and the number
of samples will grow exponentially as the dimension of the configuration space increases. Our method, Fig. 13(b), instead of
searching the entire 2-D plane for the most likely region, divides
the search into two stages: it first propagates the root variable
along the horizontal axis only, before attempting to populate
the dark-shaded area ; next, the correlation is used to spread
the leaf variables conditional on the propagated root variables,
constraining the propagations of the samples to only portions
of the parameter space that are more likely to conform to plausible motions, rather than spreading the particles all over the
parameter space. Thus, we call this approach “smart sampling.”
This strategy greatly reduces the sampling volume, and only a
small number of particles may be sufficient to reach the peak
of the posterior density. Fig. 13(c) and (d) illustrates with two

sample frames the projections of the ten random state particles,
obtained from particle filtering (c) and RBPF-PLS (d), onto the
image plane. It can be seen that some of the samples from particle filtering are, indeed, scattered into a larger area even into
some almost implausible regions.
We now compare our method with Partitioned Sampling (PS)
[7]. The PS algorithm is shown in Fig. 14, where the symbols are
defined similarly as in Fig. 6. The commonality between PS and
our method is that both divide the state space into two or more
partitions, and sequentially apply the dynamics for each partition. But unlike our method, PS performs likelihood evaluation
and re-sampling for each partition separately. That is, PS assumes that the likelihood can be factorized as a product of likelihoods of individual partitions. This requires the construction
of the observation density for each partition (i.e., body parts),
which further means that separate body parts need to be localized in order to compute the observation density for each partition. However, it is a challenging task itself to independently
localize separate body parts during online tracking. In contrast,
our method evaluates the likelihood based on the entire state
space, eliminating the need of localizing the individual body
parts.
B. Upper-Lower Partition Versus Right-Left Partition
There might be other ways of partitioning the state space besides the left-right partition that have been used in this work.
To assess the impact of the partition, we now analyze the performance of the system when the parameter space is divided
according to an upper-lower partition.
In the upper-lower partition, the root variables consist
of the global orientation and position, head orientation and
upper-body joint angles, the leaf variables comprise of the
lower-body joint angles. Fig. 15 depicts the tracking results
using the correlation learned from the upper-lower partition, the
right-most figure gives the tracking error in millimeters. We can
see that the upper-lower partition gives worse results than the
right-left partition, especially when the arm/leg is occluded by

1302

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 18, NO. 6, JUNE 2009

Fig. 15. Tracking results using the correlation learned from upper-lower partition. The results are inferior to the results obtained from right-left partition.

each of the four cameras; 2) one Kalman Filter associated with
each particle, resulting in a Kalman filter bank. Our empirical
study shows that, with 1000 particles, about 28% of the time is
spent on Kalman filter, and about 72% of the time is spent on
likelihood computation. As pointed out in [31], only one Riccati
Riccati recursions is needed if the leaf states do
instead of
not appear in the measurement relation, which will lead to a
substantial reduction in the computation time. However, in our
problem, the image observations rely on both the leaf and the
Riccati recursions are needed. We
root variables, and, thus,
point out that the computation efficiency can be considerably
improved by adapting the sample size in particle filters through
KLD-sampling [30].
Figure 16. Dynamic Bayesian network representation of the proposed method.

the other arm/leg. This is because the correlation matrix learned
from the right-left partition naturally captures the motion symmetries, while the upper-lower partition does not. Therefore,
upper-lower partition does not deal with self-occlusion as good
as the right-left partition.
This analysis leads to a more general question as to what
should be used as the root variables in an RBPF framework.
We attempt to answer this question from the perspective of the
Dynamic Bayesian Network representation of the entire system
shown in Fig. 16. As pointed out by Murphy [42], if we look
at a single time slice at in Fig. 16, no arc enters into . This
does not depend on other variables. So, we point
means that
out that the root variables should be those that can independently
evolve over time. For 3-D articulated motion tracking, either
right-side joint angles or left-side joint angles could behave as
the root variables. But in some other applications, for example,
2-D object tracking, the object scale evolution heavily depends
on the object position evolution. Hence, in 2-D object tracking,
object position components typically serve as the root variables,
as shown in our previous work [43]. Nevertheless, the choice of
the partition still needs one’s intervention. An automatic way of
partitioning the state space goes beyond the scope of this paper,
and will be one of our future investigation efforts.
C. Time Efficiency
Currently, the MATLAB implementation of the proposed
method with 1000 particles runs 40 sec per frame on a 3 GHz
Pentium 4 CPU, which is slightly faster than APF and significantly faster than PF. The high computation cost of RBPF-PLS
is incurred by: 1) 1000 likelihood evaluations per frame for

VII. CONCLUSION
We proposed a tracking approach which marginalizes part
of state variables through a RBPF using the motion correlation
learned by PLS regression through an RBPF. The motion correlation is justified by the symmetry between the right-side and
the left-side poses in human activities like walking. We show
that the motion correlation can be learned from modest amount
of training data, and it is effective for tracking a range of human
walking styles despite weak and noisy image measurements and
severe occlusions. Incorporating the correlation model into the
statistical RBPF tracking framework enhances its capability in
dealing with incomplete measurements. We compared the proposed method with APF and PF, both algorithmically and experimentally. We found that, with our method, the sequential importance sampling is done in a reduced state-space, and that the
correlation effectively constrains the propagations of the samples to portions of the parameter space that are more likely to
conform to plausible poses. As a result, both the accuracy and
the efficiency of tracking are improved.
In this paper, tracking was accomplished with rather simple
first-order dynamics and image observations that are noisy
and incomplete. The quality of our results with such simple
dynamics and simple appearance models demonstrates the
predictive ability of the correlation model and the inference
power of the RBPF. More sophisticated likelihood and dynamic
models should produce even better results. We also expect that
the tracking performance will be further improved by using
kernel PLS regression to account for the potential nonlinearity
in the motion correlation of different body parts.
REFERENCES
[1] N. J. Gordon, D. J. Salmond, and A. F. M. Smith, “Novel approach to
nonlinear/non-Gaussian Bayesian state estimation,” IEE Proc. F, vol.
140, no. 2, pp. 107–113, 1993.

XU AND LI: EXPLOITING MOTION CORRELATIONS IN 3-D ARTICULATED HUMAN MOTION TRACKING

[2] J. S. Liu and R. Chen, “Sequential Monte Carlo methods for dynamic
systems,” J. Amer. Statist. Assoc., vol. 93, pp. 1032–1044, 1998.
[3] K. Grochow, S. L. Martin, A. Hertzmann, and Z. Popovic, “Style-based
inverse kinematics,” in Proc. ACM SIGGRAPH, Aug. 2004, vol. 23, no.
3, pp. 522–531.
[4] H. Sidenbladh, M. J. Black, and D. J. Fleet, “Stochastic tracking of 3D
human figures using 2D image motion,” in Proc. ECCV, 2000, vol. 2,
pp. 702–718.
[5] R. Rosipal and N. Kramer, “Overview and recent advances in partial
least squares,” presented at the Subspace, Latent Structure and Feature
Selection Statistical and Optimization, Perspectives Workshop, 2005,
SLSFS.
[6] H. Wold, “Path models with latent variables: The NIPALS approach,”
in Quantitative Sociology: International Perspectives on Mathematical
and Statistical Model Building, H. M. Blalock, Ed. New York: Academic, 1975.
[7] J. MacCormick and M. Isard, “Partitioned sampling, articulated objects, and interface-quality hand tracking,” in Proc. ECCV, 2000, vol.
2, pp. 3–19.
[8] C. Sminchisescu and B. Triggs, “Estimating articulated human motion
with covariance scaled sampling,” Int. J. Robot. Res., vol. 22, no. 6, pp.
371–393, 2003.
[9] Z. Khan, T. Balch, and F. Dellaert, “A Rao-Blackwellized particle filter
for Eigen tracking,” in Proc. CVPR, 2004, no. 2, pp. 980–986.
[10] K. Choo and D. Fleet, “People tracking using hybrid Monte Carlo filtering,” in Proc. ICCV, 2001, vol. 2, pp. 321–328.
[11] C. Sminchisescu and A. Jepson, “Generative modeling for continuous
non-linearly embedded visual inference,” in Proc. ICML, Banff,
Canada, 2004, pp. 759–766.
[12] R. Urtasun, D. J. Fleet, A. Hertzmann, and P. Fua, “Priors for people
tracking from small training sets,” in Proc. ICCV, 2005, vol. 1, pp.
403–410.
[13] A. Elgammal and C. S. Lee, “Inferring 3D body pose from silhouettes using activity manifold learning,” in Proc. CVPR, 2004, vol. 2,
pp. 681–688.
[14] J. Deutscher and I. Reid, “Articulated body motion capture by stochastic search,” Int. J. Comput. Vis., vol. 61, no. 2, pp. 185–205, 2004.
[15] H. Sidenbladh, M. Black, and L. Sigal, “Implicit probabilistic models
of human motion for synthesis and tracking,” in Proc. ECCV, 2002,
vol. 1, pp. 784–800.
[16] M. Isard and A. Blake, “CONDENSATION—Conditional density
propagation for visual tracking,” Int. J. Comput. Vis., vol. 19, no. 1,
pp. 5–28, 1998.
[17] T. Cham and J. Rehg, “A multiple hypothesis approach to figure
tracking,” in Proc. CVPR, 1999, vol. 2, pp. 239–245.
[18] C. Sminchisescu and B. Triggs, “Hyperdynamic importance sampling,”
in Proc. ECCV, 2002, vol. 1, pp. 769–783.
[19] R. Urtasun, D. Fleet, and P. Fua, “Monocular 3D tracking of the golf
swing,” in Proc. CVPR, 2005, vol. 2, pp. 932–938.
[20] D. Ormoneit, H. Sidenbladh, M. J. Black, and T. Hastie, “Learning and
tracking cyclic human motion,” in Proc. Advances in Neural Information Processing Systems 13, 2001, pp. 894–900.
[21] S. Roweis and L. Saul, “Nonlinear dimensionality reduction by locally
linear embedding,” Science, 2000.
[22] J. Tenenbaum, V. Silva, and J. Langford, “A global geometric
framewok for nonlinear dimensionality reduction,” Science, 2000.
[23] M. Belkin and P. Niyogi, “Laplacian eigenmaps and spectral techniques for embedding and clustering,” in Proc. Advances in Neural Information Processing Systems 14, 2002, pp. 585–591.
[24] C. Sminchisescu and B. Triggs, “Kinematic jump processes for monocular 3D human tracking,” in Proc. CVPR, 2003, vol. 1, pp. 69–76.
[25] R. Li, M.-H. Yang, S. Sclaroff, and T.-P. Tian, “Monocular tracking of
3d human motion with a coordinated mixture of factor analyzers,” in
Proc. ECCV, 2006, pp. 137–150.
[26] Q. Wang, G. Xu, and H. Ai, “Learning object intrinsic structure for
robust visual tracking,” in Proc. CVPR, 2003, vol. 2, pp. 227–233.
[27] K. Toyama and A. Blake, “Probabilistic tracking in a metric space,” in
Proc. ICCV, 2001, vol. 2, pp. 50–57.
[28] A. Kanaujia, C. Sminchisescu, and D. Metaxas, “Spectral latent variable models for perceptual inference,” in Proc. ICCV, 2007.
[29] N. Lawrence, “Probabilistic non-linear component analysis with
Gaussian process latent variable models,” J. Mach. Learn. Res., vol. 6,
pp. 1783–1816, 2005.

1303

[30] M. D. Fox, “Adapting the sample size in particle filters through KLD
sampling,” Int. J. Robot. Res., vol. 22, no. 12, pp. 985–1003, 2003.
[31] T. Schon, F. Gustafsson, and P.-J. Nordlund, “Marginalized particle filters for mixed linear/nonlinear state-space models,” IEEE Trans. Signal
Process., vol. 53, no. 7, pp. 2279–2289, Jul. 2005.
[32] L. Sigal and M. J. Black, HumanEva: Synchronized Video and Motion Capture Dataset for Evaluation of Articulated Human Motion TR
CS-06-08, Brown Univ., Providence, RI, 2006.
[33] T. Jaegglli, E. K-Meier, and L. V. Gool, “Multi-activity tracking in
LLE body pose space,” presented at the IEEE Human Motion—Understanding, Modeling, Capture and Animation Workshop, 2007.
[34] A. Balan, L. Sigal, and M. Black, “A quantitative evaluation of videobased 3D person tracking,” in Proc. IEEE Workshop VS-PETS, 2005,
pp. 349–356.
[35] P.-J. Nordlund and F. Gustafsson, “Sequential Monte Carlo filtering
techniques applied to integrated navigation systems,” in Proc. Amer.
Control Conf., 2001, vol. 6, pp. 4375–4380.
[36] N. de Freitas, R. Dearden, F. Hutter, R. M.-Menendez, J. Mutch, and
D. Poole, “Diagnosis by a waiter and a Mars explorer,” Proc. IEEE,
vol. 92, no. 3, pp. 455–468, Mar. 2004.
[37] D. Schulz, D. Fox, and J. Hightower, “People tracking with anonymous
and id-sensors using Rao-Blackwellized particle filters,” presented at
the Int. Joint Conf. Artificial Intelligence (IJCAI), 2003.
[38] W. F. Massy, “Principal components regression in exploratory statistical research,” J. Amer. Statist. Assoc., vol. 60, pp. 234–256, 1965.
[39] X. Xu and B. Li, “Learning motion correlation for tracking articulated
human body with a Rao-Blackwellized particle filter,” presented at the
ICCV, 2007.
[40] T. D. Bie, N. Cristianini, and R. Rosipal, “Eigenproblems in pattern
recognition,” in Handbook of Geometric Computing: Applications in
Pattern Recognition, Computer Vision, Neuralcomputing, and Robotic,
E. Bayro-Corrochano, Ed. Heidelberg, Germany: Springer-Verlag,
Aug. 2005.
[41] G. Casella and C. P. Robert, “Rao Blackwellisation of sampling
schemes,” Biometrika, vol. 83, no. 1, pp. 81–94, 1996.
[42] K. Murphy and S. Russell, “Rao-Blackwellized particle filtering for
dynamic Bayesian networks,” in Sequential Monte Carlo Methods in
Practice, A. Doucet, Ed. New York: Springer-Verlag, 2001, ch. 24.
[43] X. Xu and B. Li, “Adaptive Rao-Blackwellized particle filter and its
evaluation for tracking in surveillance,” IEEE Trans. Image Process.,
vol. 16, no. 3, pp. 838–849, Mar. 2007.

Xinyu Xu (S’05) received the Ph.D. degree in
computer science and engineering from Arizona
State University, Tempe, in 2008.
She is currently with the Sharp Laboratory of
America (SLA), Camas, WA. Her research interests
include computer vision, image/video processing,
pattern recognition, and machine learning.

Baoxin Li (SM’04) received the Ph.D. degree in electrical engineering from the University of Maryland,
College Park, in 2000.
He is currently an Assistant Professor of Computer
Science and Engineering, Arizona State University,
Tempe. He was previously a Senior Researcher with
SHARP Laboratories of America (SLA), Camas,
WA, where he was the Technical Lead in developing
SHARP’s Hi-Impact Sports™ technologies. He was
also an adjunct faculty member with Portland State
University, Portland, OR, from 2003 to 2004. His
research interests include pattern recognition, computer vision, multimedia
processing, and statistical methods in computing.

1206

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 37,

NO. 6,

JUNE 2015

Relative Hidden Markov Models for Video-Based
Evaluation of Motion Skills in Surgical Training
Qiang Zhang, Student Member, IEEE and Baoxin Li, Senior Member, IEEE
Abstract—A proper temporal model is essential to analysis tasks involving sequential data. In computer-assisted surgical training,
which is the focus of this study, obtaining accurate temporal models is a key step towards automated skill-rating. Conventional learning
approaches can have only limited success in this domain due to insufficient amount of data with accurate labels. We propose a novel
formulation termed Relative Hidden Markov Model and develop algorithms for obtaining a solution under this formulation. The method
requires only relative ranking between input pairs, which are readily available from training sessions in the target application, hence
alleviating the requirement on data labeling. The proposed algorithm learns a model from the training data so that the attribute under
consideration is linked to the likelihood of the input, hence supporting comparing new sequences. For evaluation, synthetic data are
first used to assess the performance of the approach, and then we experiment with real videos from a widely-adopted surgical training
platform. Experimental results suggest that the proposed approach provides a promising solution to video-based motion skill evaluation. To further illustrate the potential of generalizing the method to other applications of temporal analysis, we also report experiments
on using our model on speech-based emotion recognition.
Index Terms—Relative hidden markov model, relative learning, temporal model, emotion recognition, surgical skill

Ç
1

INTRODUCTION

H

UMAN capability in mastering body motion is the key
in domains such as sports, rehabilitation, surgery and
dance. Computer-based approaches have been developed
over the years for facilitating acquiring (e.g., training in
sports and surgery) or regaining (e.g., in rehabilitation)
such motion-related skills by human subjects. One central
task faced by systems using such approaches is the analysis
of motion skills based on some temporal sensory data. With
such analysis, skill metrics may be extracted and assigned
to a given movement and feedback may accordingly be provided to the subjects for taking actions to improve the
underlying skill. For example, [1] utilized control trajectories and motion capture data for human skill analysis, [2]
reported motion skill analysis in sports using data from
motion sensors, [3] studied computational skill rating in
manipulating robots, and [4] considered hand movement
analysis for skill evaluation in console operation.
Among others, surgery-related applications have
attracted increasing interests, where motion expertise is
the primary concern. To improve their motion expertise,
surgeons often have to go through lengthy training processes. In recent years, simulation-based surgical training
platforms have been developed and widely applied in
surgical education. One prominent example is the Fundamentals of Laparoscopic Surgery (FLS) Trainer Box
(www.flsprogram.org). With such platforms, it is possible
to develop computational approaches to provide objective



The authors are with the Computer Science and Engineering, Arizona
State University, Tempe AZ 85287.
E-mail: {qzhang53, baoxin.li}@asu.edu.

Manuscript received 24 Feb. 2014; revised 21 Sept. 2014; accepted 25 Sept.
2014. Date of publication 1 Oct. 2014; date of current version 8 May 2015.
Recommended for acceptance by D. Xu.
For information on obtaining reprints of this article, please send e-mail to:
reprints@ieee.org, and reference the Digital Object Identifier below.
Digital Object Identifier no. 10.1109/TPAMI.2014.2361121

and quantifiable performance metrics, overcoming the
shortcomings in traditional training that relies on costly
practice of direct supervision by senior surgeons. Recognizing the sequential nature of motion data, many analysis approaches utilize state-transition models, such as the
Hidden Markov Model (HMM). For example, [5] provided an HMM-based method to evaluate surgical residents’ learning curve. The method first constructs
different HMMs for each different levels of expertise, and
then calculates a probability distance between the expert
and a novice resident. The magnitude of the probability
distance is used to rate the level of the novice resident.
HMM was also adopted in [6] to measure motion skills in
surgical tasks, where a recorded video is first segmented
into basic gestures based on velocity and angle of movement, with segments of the gestures corresponding to the
states of an HMM. In [7], Hierarchical Dircichlet process
hidden Markov model (HDPHMM [8]) was utilized,
which relaxed the requirement of predefining the number
of the states for the model.
One practical difficulty in these approaches is that they
require the skill labels for the training data since the HMMs
are typically learned from sets of data streams with corresponding skill levels. Labeling the skill of a trainee is currently done by senior surgeons, which is not only a costly
practice but also one that is subjective and less quantifiable.
Thus it is difficult, if not impossible, to obtain a large amount
of data with sufficiently reliable skill labels for HMM training. This problem has also been encountered in other fields
such as image classification. For example, in [9], it was
argued that using binary labels to describe images is not
only too restrictive but also unnatural and thus relative
visual attributes were used and classifiers were trained
based on such features. Relative information has also been
used in other applications, e.g., distance metric learning [10],
face verification [11], and human-machine interaction [12].

0162-8828 ß 2014 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution
requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

ZHANG AND LI: RELATIVE HIDDEN MARKOV MODELS FOR VIDEO-BASED EVALUATION OF MOTION SKILLS IN SURGICAL TRAINING

In this paper, we propose a novel formulation termed
Relative Hidden Markov Model and develop an algorithm for
obtaining a solution under this model. The proposed
method utilizes only relative ranking (based on certain attribute of interest, or motion skill in the surgical training application) between pairs of inputs, which is easier to obtain
and often more consistent. This is especially useful for
applications like video-based surgical training, where the
trainees go through a series of training sessions with their
skills improving over time, and thus the time of the sessions
would already provide natural relative ranking of the skills
at the corresponding time. The proposed algorithm effectively learns a model from the training data so that the attribute under consideration (i.e., the motion skill in our
application) is linked to the likelihood of the inputs under
the learned model. The learned model can then be used to
compare new data pairs. For evaluation, we first design synthetic experiments to systematically evaluate the model and
the algorithm, and then experiment with real data captured
on a commonly-used surgical training platform. The experimental results suggest that the proposed approach provides
a promising solution to the real-world problem of motion
skill evaluation from video.
The key contribution of the work lies in the novel formulation of learning temporal models using only relative information and the proposed algorithm for obtaining solutions
under the formulation. Discussion of its relationship to the
latent support vector machine is also provided to assist
the understanding of why the proposed formulation is suitable for the proposed scenarios. Additional contributions
include the specific application of the proposed method to
the problem of video-based motion skill evaluation in surgical training, which has seen increasing importance in recent
years. An earlier exposition of the proposed method can be
found in [13]. This current paper represents a full exploration of the method, including a new learning algorithm that
is more efficient, new comparative analysis of the method,
and new and updated experiments. In particular, to illustrate that the proposed model is general in nature but not
confined to video-based skill analysis, we report its application to a different problem, emotion recognition using
speech. To facilitate further exploration and validation by
other researchers, source code accompanying this paper has
been made publicly available.1
In the remainder of this paper, we first review some of
the related work in Section 2 and describe basic notations of
the HMM in Section 3. The proposed method is then presented in Section 4, including a new algorithm for obtaining
solutions in Section 4.3 and discussion of its relationship to
latent support vector machine in Section 4.4. The proposed
method is evaluated on three types of data in Section 5,
including synthetic data (Section 5.1) and videos from surgical simulation systems (Section 5.2), and speech data
(Section 6). The paper is concluded in Section 7. In this
paper, we use upper-case bold font (e.g., X) for matrices,
lower-case bold font (e.g., x) for vectors. We use Xi to represent ith sequence, Xit for the tth frame of sequence Xi .

1. The code is available at www.public.asu.edu/~bli24/Code
SoftwareDatasets.html.

2

1207

RELATED WORK

In this section, we first review two categories of existing
work, discriminative learning for hidden Markov models
and learning based on relative information, which are most
related to our approach. Distinction between our proposed
method and the reviewed work will be briefly stated. We
also briefly discuss a few more related efforts on skill evaluation in surgery.
Discriminative learning for HMM. Maximum-likelihood
methods for learning HMM (e.g., the forward-backward
algorithm) in general do not guarantee the discrimination
ability of the learned models. To this end, several discriminative learning methods for HMM have been proposed. In
[14], a discriminative training method for HMM was proposed based on perceptron algorithms. The methods iterates between the Viterbi algorithm and the additive update
of the models. Hidden Markov support vector machine
(HM-SVM) was proposed in [15], which combines SVM
with HMM to improve the discrimination power of the
learned model. These methods are “supervised” in nature,
and thus the labeling of the state sequence is required for
the training data, which limits their practical use. In [16],
another discriminative learning method for HMM was proposed, which only requires the labels of the training sequences. The method initializes the HMMs with maximumlikelihood method and then updates the models with SVM.
One drawback is that, the updated models do not always
lead to valid HMMs, which could be problematic for a
physics-driven problem where the model states have real
meanings (like the gesture elements in [6]). Our proposed
method requires neither the labeling of the states nor the
class label for the training sequences, which are difficult to
obtain or even not accessible in many applications. Instead,
only a relative ranking of the training data is used, and the
resultant model is a valid HMM.
Learning with relative information. Several methods for
learning with relative information have been proposed
recently. In [10], a distance metric is learned from relative
comparisons. Considering the limited training examples
for object recognition, [17] proposes an approach based on
comparative objective similarities, where the learned
model scores high for objects of similar categories and
low for objects of dissimilar categories. In [11], comparative facial attributes were learned for face verification. The
method of [9] learns relative attributes for image classification and the problem is formulated as a variation of
SVM. Similar idea was also been used in [12] for the purpose of human-machine interaction. In [18], relative attribute feedback, e.g., “Shoe images like these, but sportier”,
is used to improve the performance of image search. Relative information between scene categories has also been
used to enhance the performances of scene categorization
in [19]. These approaches are mostly for image-based
attributes, whereas our current task is on modeling
sequential data, for which it is natural to assume that the
most relevant attributes (e.g., motion skills) are embedded
in a temporal structure. This is what our proposed
method attempts to address. Efforts has been observed for
estimating the true continuous label of the data from a set
of pairwise ranking of training data [20], [21]. However,

1208

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

those methods do not directly learn a model for ranking/
labeling new data.
Skill evaluation for surgical simulations. Objective evaluation of surgical skills has been a topic of research for many
years. The authors of [22], [23] used the time of each data,
total path traveled and the number of hand movements to
rate the surgical skills. It is evident that some of the criteria
recommended in these studies (e.g., time of completion)
may be relatively easily measured with proper sensory
data, while some others cannot be (e.g., respect for tissues).
A technique proposed in [24] called task deconstruction
was implemented in a recent system by [25]. They used
Markov Models to model a sequence of force patterns or
positions of the tools. They showed that their Markov Models were suitable for decomposing a task (such as suturing)
into basic gestures, and then the proficiency of the complex
gesture could be analyzed. While this study offered an
intriguing approach to expertise analysis, it required an
expert surgeon to provide specifications for building the
topology of the model; hence it cannot be easily generalized
to new procedures. A similar idea was also utilized in [26].
Jun et al. [27] proposed to segment the training data into
modular sub-procedures or therbligs and performance is
measured over each sub-procedure.

3

BASIC NOTATIONS OF HMM

In this section, we briefly describe HMM and introduce
some basic notations that will be used later. An HMM
can be defined by a set of parameters: the initial transition
probabilities p 2 RK1 , the state transition probabilities A 2
RKK and the observation model ffk gK
k¼1 , where K is the
number of states. There are two central problems in HMM:
1) learning a model from the given training data; and 2) evaluating the probability of a sequence under a given model,
i.e., the decoding problem.
In the learning problem, one learns the model (u) by maximizing the likelihood of the training data (X):
u : max
u

Y
Xi 2X

pðXi juÞ  max
u

X

log pðXi juÞ;

(1)

Xi 2X

where X is the set of i.i.d. training sequences.
One efficient solution to the above problem is the wellknown Baum-Welch algorithm [28]. Another scheme,
namely the segmental K-means algorithm [29], may also be
used to seek a solution, and it has been shown that the likelihoods under models estimated by either of the two algorithms are very close [29]. When the training data include
sequences of multiple categories, multiple models would be
learned and each model will be learned from data of each
category independently.
In the decoding problem, given a hidden Markov model,
one needs to determine the probability of a given sequence
X being generated by the model. Generally we are more
interested in the probability associated with the optimal
state sequence (z ), i.e., pðX; z juÞ ¼ maxz pðX; zjuÞ. The optimal state path can be found via the Viterbi algorithm. To
use HMM in classification, we first compute the probability
of the given sequence drawn from each model, then we
choose the model yielding the maximal probability.

4

VOL. 37,

NO. 6,

JUNE 2015

PROPOSED METHOD

Based on the previous discussion, we are concerned with a
new problem of learning temporal models using only relative information. This is a problem arising naturally in
many applications involving motion or video data. In the
case of video-based surgical training, the focus is on learning to rate/compare the performance of the trainees from
recorded videos capturing their motion. To this end, in recognition of some fruitful trials of HMMs in this application
domain, we propose to formulate the task as one of learning
a Relative Hidden Markov Model, which not only maximizes
the likelihood of the training data, but also maintains the
given relative rankings of the input pairs. In its most basic
form, the proposed model can be formally expressed as (following the notations defined in Eqn. (1))
u
s:t:

:

max
u

Y

Xi 2X
i

pðXi juÞ
(2)

F ðX ; uÞ > F ðX ; uÞ; 8ði; jÞ 2 E;
j

where F ðX; uÞ is a score function for data X given by model
u, which is introduced to maintain the relative ranking of
the pair Xi and Xj and E is the set of given pairs with prior
ranking constraint. Different score functions may be
defined, e.g., data likelihood and data likelihood ratio, as
described in the following sections in Section 4.1 and
Section 4.2.
From this formulation, the difference between the proposed method and any of the existing HMM-based methods
is obvious. In an existing HMM-based method, a set of models is trained using the training data of each category independently. That is, explicit class labels are required for each
training sequence. The proposed model has the following
unique features:


The model does not require explicit class labels.
What needed is only a relative ranking.
 The model explicitly considers the ranking constraint between given data pairs, whereas
independently-trained HMMs in existing methods
cannot guarantee it.
 Only one model is learned for the entire set of data.
There are two benefits: more data for training and
less computation during testing.
Our method is also different from the existing work on
learning with relative attributes in that it models sequential
data and the relative ranking information is capsulated in a
temporal dynamic model of HMM (albeit new algorithms
are thus called for), which has demonstrated performance
in modeling physical phenomena like human movements.
In the following sections, we present two instantiations
of the general model expressed in Eqn. (2), and develop the
corresponding algorithms in each case. It will become clear
that the first model (Section 4.1), while being intuitive, has
some practical difficulties, which motivated us to develop
the improved model of Section 4.2. Both models/algorithms
are presented (and evaluated later in Section 5) for the progressive nature of the methods and for facilitating the
understanding of the improved model and algorithm of
Section 4.2, which is the recommended solution.

ZHANG AND LI: RELATIVE HIDDEN MARKOV MODELS FOR VIDEO-BASED EVALUATION OF MOTION SKILLS IN SURGICAL TRAINING

4.1 The Baseline Model
While one may use different score functions for F in Eqn. (2)
for comparing the input pairs, upon successful training the
likelihoods of the sequences should reflect the original ranking. Hence we may set F ðXi ; uÞ ¼ pðXi juÞ. With this, the formulation in Eqn. (2) can be rewritten as
Y
u : max
pðXi juÞ
u
i
X 2X
(3)
s:t:
pðXi juÞ > pðXj juÞ; 8ði; jÞ 2 E:
It has been proven in [30] that, the marginal likelihood is
dominated by the likelihood with the optimal path and their
difference decreases exponentially with the length (number
of frames) of a sequence. This idea was used in segmental
K-means algorithm and similarly we can approximate the
marginal data likelihood pðXjuÞ by the likelihood with optimal path pðX; z juÞ (when there is no ambiguity, we will
use z for z ), which can be written as:
log pðX; zjuÞ ¼ log pðX1 jfz1 Þ þ log pðz1 Þ
þ

T 
X

log pðXt jfzt Þ þ log Aðzt jzt1 Þ

(4)

t¼2

For some observation models, e.g., multinomial (more
details in Appendix A), we can write log pðXi ; zi juÞ ¼
uT hðXi ; zi Þ. Accordingly, Eqn. 3 can be finally written as
X
u : max uT
hðXi ; zi Þ
u2V
i:Xi 2X
(5)
T
i i
T
j j
s:t:
u hðX ; z Þ  u hðX ; z Þ þ r; 8ði; jÞ 2 E;
where r  0 defines the required margin between the logarithms of likelihood for a pair of data and V defines the set
of valid parameters for the hidden Markov model, i.e.:
X
euðiÞ ¼ 1;
uðiÞ 	 0 ;
i:uðiÞ2logðpÞ

X

e

uðiÞ

X

¼1;

i:uðiÞ2logðAj Þ

euðiÞ ¼ 1;

i:uðiÞ2logðfj Þ

Xi 2X
i

ði;jÞ2E

(7)
u ½hðX ; z Þ  hðX ; z Þ þ ij  r; 8ði; jÞ 2 E
ij  0;
P
where g is the weight for the penalty term ði;jÞ2E ij . For
initialization, we can set ij ¼ 0. We will defer the optimization algorithm for Eqn. (7) to Section 4.3. After the model is
learned, it can be used to a testing pair: For each sequence
s:t:

T

i

j

j

we evaluate the data likelihood via the Viterbi algorithm
and use the logarithm of the data likelihood as the score of
the data. By definition, the obtained scores can be used to
compare the pair.

4.2 The Improved Model
In the model described in Eqn. (7), we compare the logarithm of the data likelihood, which is, according to Eqn. (4),
roughly proportional to the length of the data. Thus a shorter
sequence is likely to have a larger score. This means that the
learned model would be biased towards shorter sequences.
If the observation describes a long, periodic event, e.g.,
repeating an action multiple times within a sequence, we
may consider normalizing the logarithm of the data likelihood by the number of frames of the observation. However,
this cannot be applied directly for non-periodic observations
like sequences from surgical simulation, where the length of
a sequence (corresponding to the time taken for completing
a task) is one of the skill metrics.
To overcome the above practical problem, we consider
an improved version. Recall that in HMM, we classify a
sequence based on the model with which the sequence gets
the maximal likelihood, i.e., it is the ratio of data likelihood
with different models that decides the label of the data. For
zju1 Þ
example, if log pðX;^
pðX;~
zju2 Þ > 0, then we assign X to Model u1 .
Thus we propose to use the ratio of the data likelihoods of
zju1 Þ
two HMMs as the score function, i.e., F ðX; uÞ ¼ log pðX;^
pðX;~zju2 Þ,
where we “partition” the original model into two models
(or, effectively, we train a pair of HMMs simultaneously).
This results in the following improved model:
:

u 1 ; u2

max
u1 ;u2

g

X

^i ju1 Þ þ
log pðXi ; z

i2X1

X

~j ju2 Þ
log pðXj ; z

j2X2

X

ij

ði;jÞ2E

^i ju1 Þ
^j ju1 Þ
pðXi ; z
pðXj ; z

log
þ ij  r
j j
j j
~ ju2 Þ
~ ju2 Þ
pðX ; z
pðX ; z
ij  0;

s:t:

log

(6)

where i : uðiÞ 2 logðAj Þ is the set of the indexes which corresponds to the jth row of matrix A.
For the model in Eqn. (3), we assumed that every pairwise ranking constraint provided in the data is correct (or
valid). However, in real data, there may be outliers in such
training pairs. To handle this, we further introduce some
slack variables  and h, and accordingly Eqn. (5) can be written as following:
X
X
hðXi ; zi Þ  g
ij
u : max uT
u2V

1209

(8)
where X1 is the set of data associated with Model u1 (X2 for
^i is the optimal path for sequence xi with Model
Model u2 ), z
i
z for optimal path with Model u2 .
u1 and ~
i i

pðX ;^
z ju1 Þ
^i Þ  uT2 hðXi ; z
~i Þ, we can
¼ uT1 hðXi ; z
With log pðX
j ;~
zj ju2 Þ
rewrite the model in Eqn. (9) (similar to Eqn. (7)):

"P
u

:

^i Þ
hðXi ; z

j2X2

~j Þ
hðX ; z

max uT P
u2V

"
s:t:

i2X1

u

T

j

#
g

X

ij

ði;jÞ2E

#
^i Þ  hðXj ; z
^j Þ
hðXi ; z
þ ij  r
~j Þ  hðXi ; ~
zi Þ
hðXj ; z

(9)

ij  0;
where u ¼ ½uT1 ; uT2 T . The optimization algorithm for Eqn. (9)
will be presented in Section 4.3. After we learn the model
with the improved algorithm, we can apply it to a given
pair by first computing their likelihoods with respect to the

1210

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

”sub-models” given by u1 and u2 (with the Viterbi algorithm), and then we use the logarithm of the ratio of the
data likelihoods as the score to rank/compare the pair.
The learned models u1 and u2 serve as a unified model to
rank the data. We may view them as the centers of two clusters, where the distances of the data to those two centers
can be related to the ranking score.
It needs to be emphasized that the improved model is
not equivalent to a supervised HMM with two classes. In a
2-class HMM setting, two models will be independently
trained with their respective training sets. Here, the proposed model trains two ”sub-models” jointly with only relative ranking constraints. Specifically, if there is no further
information for X, we could assume that X1 ¼ fijði; jÞ 2
E; 8jg and X2 ¼ fjjði; jÞ 2 E; 8ig, and thus there could be
overlaps between X1 and X2 (which will become clear in the
experiment with synthetic data in Section 5). This situation
is not even allowed by a supervised HMM setting. We do
not require any extra properties for X1 and X2 .

4.3 Algorithms for Updating the Model
One important step of both the baseline algorithm and the
improved algorithm is updating the models, as formulated
in Eqn. (7) and Eqn. (9) accordingly. It is a nonlinear programming problem (due to the nonlinear equality constraint). In our previous paper, we solved it by the primaldual interior point method, which is of dimension
Kð1 þ K þ DÞ þ jEj (or 2Kð1 þ K þ DÞ þ jEj) with 2jEj þ
Kð1 þ K þ DÞ (or 2jEj þ 2Kð1 þ K þ DÞ) linear inequality
constraints and 1 þ K þ D (or 2ð1 þ K þ D)) nonlinear
equality constraints for the baseline model (or the improved
model). Although the Hessian matrix is diagonal, the
computational cost could be still very high when there are a
large number of training pairs. In this section, we propose a
new algorithm by utilizing the special structure of the problems in Eqn. (7) and Eqn. (9).
Eqn. (7) (similarly for Eqn. (9)) can be written in the following form:
min f T u þ g1T 

u; 

:

s:t:

: Au þ  	 r

u;

Ceu ¼ 1

(10)

P
For example, for Eqn. (7), we have f ¼  Xi 2X hðXi ; zi Þ, A
and C are constructed according to Eqns. (7) and (6).
Eqn. (10) is a nonlinear programming problem (due to
the nonlinear equality constraint). To solve this problem,
we first introduce a slack variables f, where log f ¼ u. Then
Eqn. (10) can be rewritten into the following problem:

s:t:

:

u;;f

log f ¼ u
u 	 0;   0; 0 	 f 	 1:

JUNE 2015

According to Eqn. (11), f will be a valid hidden Markov
model (or hidden Markov model pairs ½f1 ; f2  for the
improved model). We then apply the Augmented Lagrange
multiplier method to the equality constraint log f ¼ u of the
problem in Eqn. (11):
:

u; ; f

min f T u þ g1T þ
u;;f

< ; u  log f > þ
s:t:

m
ku  log fk22
2

: Au þ  	 r
Cf ¼ 1

(12)

u 	 0;   0; 0 	 f 	 1;
where  is the Lagrange multiplier and m is some nonnegative constant. In Eqn. (12), the nonlinear equality constraint
is removed.
Eqn. (12) can be solved via block coordinate descent by
iterating between the following two sub-problems:
Sub-problem 1: fix f to solve u and , which is
u; 

:

min f T u þ g1T þ
u;;f

< ; u  log f > þ
s:t:

m
ku  log fk22 ;
2

(13)

: Au þ  	 r
u 	 0;   0:

It is a quadratic programming problem with linear inequality constraints.
Sub-problem 2: fix u and  to solve f, which is
m
f : min < ; u  log f > þ ku  log fk22
f
2
(14)
Cf ¼ 1
0 	 f 	 1:
It is a nonlinear problem with linear constraints.
Given the special structures of C, where each column has
one and only one element being nonzero (recall Eqn. (6)),
Sub-problem 2 can be separated into a set of smaller
problems:
m
fk : min < k ; uk  log fk > þ kuk  log fk k22
2
fk
(15)
T k
1 f ¼1

where k is the set of indexes of columns, whose values are
nonzero at the kth row of C. Those smaller problems are
again a nonlinear problem with linear constraint, whose
dimensions are only K (number of states) or D (number of
feature dimensions).
To solve this problem we can use the primal-dual interior
point method, whose gradient and hessian are computed as
k þ mk log fk  mk uk
;
fk
 k

  m log fk þ muk þ m
;
H¼L
fk 
 fk
J¼

min f T u þ g1T 

: Au þ  	 r
Cf ¼ 1

NO. 6,

0 	 fk 	 1;

u 	 0 ;   0:

u; ; f

VOL. 37,

(11)

where Lð
 
 
Þ converts a vector to a diagonal matrix. In addition, we can compute the starting point of the problem in
Eqn. (15) as: by taking the gradient of the objective function

ZHANG AND LI: RELATIVE HIDDEN MARKOV MODELS FOR VIDEO-BASED EVALUATION OF MOTION SKILLS IN SURGICAL TRAINING

Fig. 1. The experiment result with different numbers of states: (a) the
computational time (blue solid curve) and number of iterations needed
for convergence (green dashed curve); (b) the accuracy of the improved
method. The X-axis is the number of states.

with regard to log fk , we have k þ mðlog fk  uk Þ ¼ 0, i.e.,
fk ¼ eðu

k þk Þ
m

. The linear constraint can be solved simply by
P k k
k k
projection, i.e., fk ¼ N1 eðu þ m Þ , where N ¼ eðu þ m Þ .

Algorithm 1. Algorithm for the Baseline (Improved)
Model
Input: X, E, r, g, s (, X1 and X2 )
Output: f
Initialization: Initialize f (or f1 and f2 ) via ordinary HMM
1:25
learning algorithm,  ¼ logu
juj and m ¼ juj ;
2

2

while not converged do
^ and z
~) for each sequence
Compute the optimal path z (or z
with f (or f1 and f2 );
solve Sub-problem 1;
solve Sub-problem 2;
update  ¼  þ mðu  log fÞ and m ¼ m  s;
check convergence;
end while

Finally, we briefly summarize the algorithms for the
baseline model (Eqn. (7)) and the improved model (Eqn. (9))

1211

Fig. 2. The accuracy of the improved method: (a) with different g (r is
fixed to 10), which controls the weight of the penalty term with slack variables; (b) with different r (g is fixed to 1; 000), which controls the margin
of the learned models.

below (noting the similarity in form of the algorithms and
thus putting them compactly together):
According to [31], the proposed method will converge to
the local minimum of the problem in Eqn. (10). And for
kulog fk
convergence, we check kuk 2 . If it is smaller than some
2
6
value, e.g., 10 , the algorithm will be terminated. In initialization, juj2 is the vector L2 norm of of u.
Remarks on the Parameters. The parameter g controls the
weight of the penalty term with the slack variables, which is
similar to the functionality of C in support vector machines
[32]. The parameter r controls the desired gap of the score
pðXi ;zi juÞ
 er 8ði; jÞ 2 E
pðXj ;zj juÞ
pðXi ;^
zi ju1 Þ pðXi ;~
zj ju2 Þ
 er 8ði; jÞ
zi ju2 Þ pðXi ;^
zj juÞ
pðXi ;~

of two data points, i.e.,

in the base-

line model and

2 E in the

improved model. In Section 5.1, we will evaluate different
parameter settings (Fig. 2), which leads us to set g ¼ 1; 000
and r ¼ 10 in our final experiments. The parameter s controls the convergence speed of the algorithm, which should
be a positive number larger than 1. s is typically within
1:1  1:5, and 1:25 is used in this paper.
The proposed algorithm, compared with the one used in
[13], has lower computational cost, due to the removal of

1212

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 37,

NO. 6,

JUNE 2015

TABLE 1
Comparing the Method in [13] and the Proposed Method for Updating the Baseline Model, with
Regarding to the Problem Size, Number of Linear Constraints and Nonlinear Constraints
Proposed Method

Method in [13]
Problem Size
# Linear Const.
# Nonlinear Const.

Kð1 þ K þ DÞ þ jEj
2jEj þ Kð1 þ K þ DÞ
1þKþD

Sub-problem 1

Sub-problem 2

Kð1 þ K þ DÞ þ jEj
2jEj þ Kð1 þ K þ DÞ
0

K(or D)
1+2K(or 1+2D)
0

For Sub-problem 2 of the proposed method, it can be divided into several smaller problems.

the nonlinear equality constraint with augmented Lagrange
multiplier. For Sub-problem 1, the quadratic term is a diagonal matrix and many solvers (e.g., CPLEX) can solve it quite
efficiently. Sub-problem 2 is a nonlinear minimization problem with linear equality constraints; however, it can be
decomposed into several smaller problems.
A comparison between the method in [13] and the proposed method for updating the baseline model is shown in
Table 1. In Section 5.1, we will also compare the computational time of those two methods under varying E on synthetica data (Fig. 6).

4.4 Relationship to Existing Methods
The proposed method is related to latent support vector
machine [33]. Given a training set of input-output pairs
fðxi ; yi Þgni¼1 , where yi 2 f1; 1g, Latent SVM tries to learn a
predictor of the form:
fw ðxÞ ¼ max wT Cðx; zÞ;
z

(16)

where w is the parameter of the predictor, Cðx; zÞ is the feature mapping function and z is the latent variable. The training stage of Latent SVM can be formulated as the following
problem:
X
1
maxð0; 1  yi fw ðxi ÞÞ:
(17)
min kwk22 þ C
w 2
i
Latent SVM is a non-convex problem, as the latent variable
is unknown, and the coordinate descent approach is used
for solving this problem.
Given a training set fðxi ; yi Þgni¼1 , where xi ¼ ðxLi ; xR
i Þ is a
pair of sequences and yi 2 f1; 1g is the ranking of the pair,
by defining the feature mapping function as Cðxi ; zi Þ ¼
R
L R
½hðxLi ; zLi Þ  hðxR
i ; zi Þ, with the latent variable zi ¼ ðzi ; zi Þ
being a pair of state sequences for the pair xi ¼ ðxLi ; xR
i Þ, we
have
X
1
min kwk22 þ C
i
w 2
i
  


	
R
þ i  1 (18)
s:t:
yi max wT h xLi ; zLi  h xR
i ; zi
zL
;zR
i i

i  0:
We can find that Eqn. (18) is similar to our baseline
model (Eqn. (7)), except for the following differences.
1)

In Eqn. (18), the L2 norm is applied to the parameter of the predictor w (which is related to the margin). In the proposed methods we require w to be a

valid hidden Markov model while defining a fixedmargin, i.e., r. Thus the proposed method can
always guarantee the learned model is a valid hidden Markov model.
2) In Eqn. (18), the two state sequences z (i.e., the latent
variables) are optimized jointly, where no known
efficient solution is available. In the proposed
method, the two state sequences are optimized separately with regarding to the likelihood, which can be
solved efficiently via dynamic programming (i.e.,
the Viterbi algorithm);
3) Given the model learned by the latent SVM, we can
only rank a pair of sequences. However, the model
learned by the proposed method is capable of not
only ranking a pair of sequences but also assigning a
score for each sequence.
Those differences make the proposed method (both the
baseline model and the improved model) more suitable for
modeling the sequential data, e.g., video, speech.

5

EXPERIMENTS

In this section, we evaluate the proposed methods, including the baseline method and the improved method, using
both synthetic data (Section 5.1) and realistic data collected
from the surgical training platform FLS box (Section 5.2).
The performance of the proposed methods is compared
with a supervised 2-class HMM. (Lacking a comparative
approach in the literature that is both unsupervised and
works with only relative rankings, this is believed to be a
reasonable way of generating a reference point to assess the
proposed methods.)
Since we do not have the label information for the training data, we train the HMM as follows. For the HMM algorithm, we initialize the two sets as X1 ¼ fijði; jÞ 2 E; 8jg and
X2 ¼ fjjði; jÞ 2 E; 8ig. Each of the sets is then used to train a
HMM. Note, the data generated from data-generating Models u2  u5 could be included in both X1 and X2 . Thus existing discriminative learning methods for HMM could not be
applied here.

5.1 Evaluation with Synthetic Data
To evaluate the proposed method, we generate synthetic
data as follows. We first generate six different HMMs (u1 to
u6 , referred as data-generating models), from each of which
we draw 200 sequences, with the length being uniformly
distributed between 80 to 120. Each data-generating model
has five states. For the sequences from each data-generating
model, we randomly assign 50 of them to the training set

ZHANG AND LI: RELATIVE HIDDEN MARKOV MODELS FOR VIDEO-BASED EVALUATION OF MOTION SKILLS IN SURGICAL TRAINING

1213

Fig. 3. The results of four methods on training set (dashed curve) and
testing set (solid curve) with different numbers of training pairs.

and the remaining to the testing set. We assume there exists
a score function such that F ðXi Þ > F ðXj Þ if and only if
Xi  uk , Xj  ul and k < l. That is, the sequences from a
data-generating model with a lower index are viewed to
have a higher score (or ranking) than those from a datagenerating model with a higher index. A set of pairs
fði; jÞjXi  uk ; Xj  ukþ1 ; k ¼ 1; . . . ; 5g are then formed
accordingly, some of which are then randomly selected as
the training pairs E.
We use the proposed methods and also HMM to learn
models from the training pairs. The learned models are
then used to evaluate the testing set, i.e., how many testing pairs that they rank the same as the ground truth. The
result of the methods with different numbers of training
pairs is summarized in Fig. 3, where due to the computational time it takes, we do not have the results for the
baseline method when there are more than 3;750 training
pairs. From Fig. 3, we can find that the improved method
achieves the best results on both the training set and the
testing set; and the HMM method gives the worse result.
In addition, the performance of both of the proposed
methods stabilized after certain number of training pairs.
However the performance of the HMM method drops
dramatically when the number of training pairs reaches
about 6;250. It can be explained by that the two HMMs
share a lot of common data (for those generated by
u2  u5 ) and the models are trained independently without considering their discrimination ability. Normalizing
the logarithm of the data likelihood does not improve the
performance of baseline method, which could be
explained by that, all the sequences have roughly the
same length, i.e., 80  120.
Fig. 4 shows the logarithm of the data likelihood ratio
with the models learned by the improved method, when
about 1; 250 training pairs are provided. This clearly demonstrates that, although we formed the training pairs only
with data from data-generating models of adjacent
indexes (i.e., i and i þ 1), the learned model is able to
recover the strict ranking of the original data. We can also
try to classify the data into six models, by thresholding the
logarithm of data likelihood ratio, where, for the model
learned with the improved method, the classification accuracy is 86:44 and 98:60 percent for testing and training
respectively.

Fig. 4. The logarithm of the data likelihood ratio with the models learned
by the improved method. Top: the result for the testing set. Bottom: the
result for the training set. The data are grouped (as the section partitioned by the red lines) according to the data generation model from
which they are synthesized.

Convergence and Speed. For empirically understanding the
convergence behavior of the improved method, we plot in
Fig. 5 the objective value in the model as a function of the
number of iterations. We can find that the improved method
converges fairly quickly (within about 14 iterations) and the
value of the objective function monotonically increases.
We also compare the computational time of the optimization method in [13] (shown as the red/upper curve) and
the proposed optimization method (in Section 4.3 and
shown as the green/lower curve) in solving the improved
model under varying number of training pairs in Fig. 6. In
[13], a primal-dual interior point method is utilized to
update the model; while in this paper, we design an augmented Lagrange multiplier method which utilizes the special structure of the objective function of the problem. From
the plot, we can find that the proposed optimization
method has a much lower computational cost than the one
proposed in [13].
Parameter Selection. To understand the effect of parameters to the performances of the improved method, including accuracy and computation cost, we evaluate it with

Fig. 5. The convergence behavior of the improved method, where
around 1; 250 training pairs were used. The blue curve/axis shows the
value of the objective function, and the green curve/axis shows the number of constraints satisfied.

1214

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

Fig. 6. The computation time for solving the improved model with the
method proposed in [13] (red/upper curve) and the method proposed in
Section 4.3 (green/lower) under varying number of training pairs. For
illustration purpose, we use log-log plot, where X-axis is the number of
training pairs (from around 125 to around 9; 000) and Y-axis is the computation time in unit second (from about 20 to around 6; 000). The time is
measured in Matlab on a dual-core PC platform.

varying combination of parameters. First we learn the
model with varying numbers of states (K), from 6 to 30.
The result is shown Fig. 1. From Fig. 1b, we can find that,
though the accuracy for the training data increases with
the number of states, the accuracy for testing doesn’t following this trend, which indicates a potential risk of overfitting. The computational time and number of iteration
until convergence get minimum when the number of states
is 11-13. We also do experiment with different combinations of g (controlling the weight of the penalty term with
slack variables) and r (controlling the margin of the
model), where the experiment result is shown in Fig. 2.
From this experiment we can find that, g 2 ½1; 1;000 and
r 2 ½4; 32 are good choices.
It is obvious from this experiment that the sequences are
different from (or similar to) each other only because they
are from different (or the same) data-generating models,
whereas their relative ranking can be arbitrarily defined. In
the end, the proposed methods will learn a temporal model
to reflect the defined rankings. This suggests that, as long as
we can assume there are some data-generating models for
the given sequential data, we can use the proposed methods
to learn a relative HMM. This is the basis for applying the
approach to the surgical training data in the following section, where it is reasonable to assume that movement patterns of subjects with different skill levels may be modeled
by different underlying HMMs while the ranking can be
based on the time of training, which reflects the skill level of
the subject at the time.

5.2 Skill Evaluation Using Surgical Training Video
We now evaluate the proposed method using real videos
captured from the FLS trainer box, which has been widely
used in surgical training. The data set contains 546 videos
captured from 18 subjects performing the “peg transfer”
operation, which is one of the standard training tasks a resident surgeon needs to perform and pass. The number of
frames in each video varies from 1; 000 to 6; 000 (depending
on the trainees’ speed in completing a training session). The

VOL. 37,

NO. 6,

JUNE 2015

data set covers a training period of four weeks, with every
trainee performing three sessions each week.
In the training, the subject needs to lift six objects (one
by one) with a grasper by the non-dominant hand, transfer the object midair to the dominant hand, and then place
the object on a peg on the other side of the board. Once all
six objects are transferred, the process is reversed, and the
objects are to be transferred back to the original side of
the board. The videos capture the entire process inside the
trainer box, showing how the tools and objects are moved
by the subject. The motion skill is related to how well the
subjects perform in such operation. In the existing practice, senior surgeons rate the performance of the trainees
based on such videos. Our goal is to perform the rating
automatically with the proposed model.
Based on the reasonable assumption that the trainees
improve their skills over time (which is the whole point of
having the resident surgeons going through the training
before taking the exam), the time of recording is used to
rank the recorded videos within each subjects’ corpus (i.e., a
later video is associated with a better skill). Other than this
relative ranking, there are no other labels assumed for the
video, e.g., there is no rank information between videos of
different subjects (which would be hard to obtain anyway,
since there is no clearly-defined skill levels for a group of
trainees with diverse background). Based on this, we randomly pick 300 pairs for training, similar to the experiment
using synthetic data.
Feature Extraction. We use the “bag of words” approach
for feature extraction from the videos as follows. The spatiotemporal interest point detector [34] is applied to obtain the
histogram-of-gradient (HoG) features, which was found to
be useful in target application in the literature [35]. K-means
(k ¼ 100) is then used to build a code book for the descriptors of the interest points. Finally, the code book is used to
obtain a histogram of interest points for each frame, and
thus each video is represented as a sequence of histograms.
This representation, compared with the existing way of
using bag of words in action recognition, i.e., transforming
each video into a single histogram, can better capture the
temporal information of the data. For all three methods, we
set the number of states to ten.
After learning the models from the training data, we
compute the score of the test data as the logarithm of data
likelihood (for the baseline method) or the logarithm of the
data likelihood ratio (for the improved method and the
HMM). We compare these scores for each pair of the testing
data (within each subject) and compute the percentage of
correctly labeled pairs (recall that, we use their time of
recording as ground truth). To demonstrate the advantage
of the proposed method, we also compare with the “relative
attribute” method [9] (referred as “SVM” in the following
discussions), which relies on ranking SVM. For “relative
attribute”, we represent each video as a histogram by accumulating the sequence of histograms of the video along the
temporal direction.
The result is summarized in Table 2, where the improved
method obtained a significantly better result than the other
approaches, including “relative attribute”. Surprisingly, the
baseline method even performed slightly worse than the
HMM method. This is largely due to the wide range of

ZHANG AND LI: RELATIVE HIDDEN MARKOV MODELS FOR VIDEO-BASED EVALUATION OF MOTION SKILLS IN SURGICAL TRAINING

TABLE 2
The Result for Experiment on Evaluating Surgical Skills
Method
# Pairs
Accuracy

SVM

HMM

Baseline

Improved

6335
78:91%

6363
79:39%

6215
77:54%

6993
87:25%

There are 8;015 pairs in total (only 300 for training), excluding the comparisons among data of different subjects.

variations of the length of the input sequences. Fig. 7 shows
the computed scores with the learned models, where for
better illustration purpose we group them by their subject
ID and within each subject’s corpus we sort the videos by
their recording time. From the figure, we can find that the
improved method (bottom) reveals a more clear trend for
the data than both the HMM method (top) and the baseline
method (middle), i.e., the scores of the data increase over
times (X-axis) for each subject (segments within the red
lines). It is worth emphasizing that only one joint model is
learned from ranked pairs of subjects with potentially varying skill levels. Still the learned model is able to recover the
improving trend, independent of the underlying skill levels.
As shown in Fig. 7, the model learned with the proposed
method can be used for comparing not only the videos of
the same subjects but also the videos from different subjects,
where the logarithm of data likelihood ratio can be used as a
measurement of the skills. However, it is not possible to
quantitatively measure the accuracy in comparing videos
from different subjects, due to the lack of ground truth
information for videos from different subjects.

1215

It is also interesting to look at what the jointly-learned
models look like. Fig. 8 depicts the two models learned by
the improved method in this real-data based experiment.
From the figure, we can see that the two models have different transition patterns. For example, the transition from
State 8 to States 2 and 5 are only observed in Model 1. This
may be linked to different motion patterns for data of different surgical skills, with the hidden states corresponding to
some underlying action elements (and thus the transition
patterns vary with the skill).

6

ADDITIONAL VALIDATION USING SPEECH DATA

Although the proposed approach was evaluated above in
the context of motion skill analysis in surgical training,
using visual data as the input, the approach itself is general
and applicable for other applications involving temporal
data. To show that the proposed method can be used to
solve temporal inference problems other than video-based
motion skill assessment, we now consider an exemplar
problem, speech-based emotion recognition, where the attribute of interest (the underlying emotion of a speaker) needs
to be inferred from sequential data. Emotion recognition
has received attention from researchers due to its broad
applications. For example, in human-machine interaction,
better responses can be made if the emotional state of the
human can be recognized. Existing work on this in the literature mainly focuses on developing models for assigning
the labels like “pleasing”, “angry” and “neural” to the data,
e.g., [36], [37], [38], [39]. Most of the those efforts are

Fig. 7. Top: the logarithm of the data likelihood ratio from two models learned by HMM. Middle: the logarithm of data likelihood with the model learned
by the baseline method. Bottom: the logarithm of the data likelihood ratio with the models learned by the improved method. The red vertical lines separate the data of different subjects, where X-axis is the corresponding subject ID. Within each subjects’ corpus, the videos are sorted according to
their time of recording.

1216

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 37,

NO. 6,

JUNE 2015

Fig. 8. The two component models (Model 1 for X1 and Model 2 for X2 ) learned by the improved method, where we only draw the edges with a transition probability larger than 0.01 and ignore self transitions. The number attached to each edge indicates the transition probability.

supervised in natural, i.e., the ground truth labeling for the
training data is required. For example, [40] used support
vector machines, [36] used hidden Markov models, both utilizing fully-labelled data. The ground truth data typically
require manual labeling by human, which is an error-prone
process especially if absolute labels must be assigned to
ambiguous data. With the proposed model, we can support
learning with only relative labels like “Audio a is more
pleasing than Audio b”, which is easier to obtain and also
less error-prone.
In this experiment, we use Utsunomiya University
Spoken Dialogue Database For Paralinguistic Information
Studies (UUDB)[41](http://uudb.speech-lab.org), which
contains 4840 assets labeled across six dimensions (pleasantness, arousal, dominance, credibility, interest and positivity) on a scale of 1 to 7. The ground truth is based on the
average of scores of three annotators. For our experiment,
we pick the assets which are longer than 1 second to ensure
the effectiveness of emotional recognition, which results in
991 assets, where half of the data are used for training and
the remaining for testing. For generating the ground truth
pairs, we randomly picks 1000 pairs from the training
assets. Note that, we say two assets are similar, if the difference of the labeled scores of two assets is within the range
of ð1; 1Þ.
For feature extraction, we use Hidden Markov Model
Toolkit (HTK)[42], where the MFCC coefficients are
extracted with the following configurations: sampling rate
is 100 HZ, windows size is 25 millisecond, number of filter
bank channels is 26, cepstral liftering coefficient is 22 with
12 cepstral parameters and the feature vector is normalized.
K-means is applied to the MFCC coefficients of all the training data to generate a code book of 64 elements. Finally,
each data is converted to a sequences of histograms. We use
the same set of parameters as the previous experiment.
The experimental results are reported in Table 3, where
we also provide a comparison to the relative attribute [9] as

referred by “SVM”. From the table, we can find that the
improved method consistently outperforms than both plain
HMM and also the baseline method in all six dimensions.
We also find that the baseline method gets low accuracy on
this experiment, which can be explained by that the length
of the audio (in number of temporal frames) varies dramatically and the baseline method obviously cannot handle this
variation very well.

7

DISCUSSIONS AND CONCLUSIONS

In this paper, we presented a new formulation for the problem of learning temporal models using only relative information. Algorithms were developed under the formulation,
and experiments using both synthetic and real data were
performed to verify the performance of the proposed
method. In essence, the proposed method attempts to learn
an HMM with relative constraints. Such a setting is useful
for many practical applications where relative attributes are
easier to obtain while explicit labeling is difficult to get. The
application of video-based surgical training was the focus
of this study, and the evaluation results using realistic data
suggests that the proposed method provides a promising
solution to the problem of motion skill evaluation from
TABLE 3
The Result for Experiment on UUDB Datasets
Dimension

SVM

Pleasantness
Arousal
Dominance
Credibility
Interest
Positivity
Average

75:25%
82:11%
74:13%
69:15%
76:91%
68:08%
74:27%

Improved

Baseline

HMM

77:30%
86:95%
87:95%
76:68%
81:90%
74:99%
81:28%

57:96%
55:74%
63:04%
55:11%
62:56%
67:84%
53:14%

75:05%
69:55%
77:32%
71:74%
78:07%
70:36%
73:72%

We evaulate the accuracy of ranking pairs with the learned models compared
with the ground truth ones.

ZHANG AND LI: RELATIVE HIDDEN MARKOV MODELS FOR VIDEO-BASED EVALUATION OF MOTION SKILLS IN SURGICAL TRAINING

videos. For future work, we plan to extend the proposed
method to cover different observation models so that more
types of applications may be handled. That also includes
investigating alternative feature spaces which may be more
effective for the target problem.

[7]

APPENDIX A

[10]

For multinomial observation model, i.e., pðXt jfzt Þ ¼
QD
Xt ðlÞ
, where D is the dimension of each frame,
d¼1 fzt ðlÞ
Xt ðlÞ is the lth dimension of Xt and fzt are the parameters of
observation model with State zt , we can further define the
following variables for each sequence Xi :
n 2R
i

K1

Oi 2 RKD

[8]
[9]

[11]
[12]
[13]



: n ðkÞ ¼ d zi1 ¼ k ;
X
: Oi ðk; dÞ ¼
Xit ðdÞ;
i

[14]

t:zt ¼k

Mi 2 RKK

: Mi ðk; lÞ ¼

T
X

 

d zit1 ¼ k d zit ¼ l ;

[15]

t¼2

where dð
Þ is Dirac Delta function. Then the log likelihood
with the optimal path can be written as:

[17]

logpðXi ; zi juÞ
X
X
¼
ni ðlÞlog pðlÞ þ
Mi ðk; lÞlog Aðk; lÞ
l

þ

X

[16]

[18]

k;l

Oi ðk; dÞlog fk ðdÞ

[19]

k;d

(19)

[20]

where u ¼ ½log p; vecðlog AÞ; vecðlog fÞ, hðXi ; zi Þ ¼ ½ni ;
vecðMi Þ; vecðOi Þ and vec converts matrix to vector.

[21]

¼ uT hðXi ; zi Þ;

ACKNOWLEDGMENTS
The work was supported in part by a grant (#0904778) from
the National Science Foundation (NSF). Any opinions
expressed in this material are those of the authors and do
not necessarily reflect the views of the NSF.

[22]

[23]
[24]

REFERENCES
[1]

[2]
[3]

[4]
[5]

[6]

F. Duan, Y. Zhang, N. Pongthanya, K. Watanabe, H. Yokoi, and T.
Arai, “Analyzing human skill through control trajectories and
motion capture data,” in Proc. IEEE Int. Conf. Autom. Sci. Eng.,
Aug. 2008, pp. 454–459.
K. Watanabe and M. Hokari, “Kinematical analysis and measurement of sports form,” IEEE Trans. Syst., Man, Cybern. A, Syst.,
Humans, vol. 36, no. 3, pp. 549–557, May 2006.
S. Suzuki, N. Tomomatsu, F. Harashima, and K. Furuta, “Skill
evaluation based on state-transition model for human adaptive
mechatronics (HAM),” in Proc. 30th Annu. Conf. IEEE Ind. Electron.
Soc., 2004, vol. 1, pp. 641–646.
S. Satoshi and H. Fumio, “Skill evaluation from observation of discrete hand movements during console operation,” J. Robot., vol.
2010, 2010.
J. Rosen, M. Solazzo, B. Hannaford, and M. Sinanan, “Task
decomposition of laparoscopic surgery for objective evaluation of
surgical residents’ learning curve using hidden Markov model,”
Comput. Aided Surgery, vol. 7, pp. 49–61, 2002.
K. Kahol, N. C. Krishnan, V. N. Balasubramanian, S. Panchanathan,
M. Smith, and J. Ferrara, “Measuring movement expertise in surgical tasks,” in Proc. 14th Annu. ACM Int. Conf. Multimedia, 2006,
pp. 719–722.

[25]

[26]

[27]

[28]

[29]

1217

Q. Zhang and B. Li, “Video-based motion expertise analysis in
simulation-based surgical training using hierarchical Dirichlet
process hidden markov model,” in Proc. Int. ACM Workshop Med.
Multimedia Anal. Retrieval, 2011, pp. 19–24.
E. Fox, “Bayesian nonparametric learning of complex dynamical
phenomena,” Ph.D. thesis, MIT, Cambridge, MA, USA, 2009.
D. Parikh, and K. Grauman, “Relative attributes,” in Proc. Int.
Conf. Comput. Vis., Nov. 2011, pp. 503–510.
M. Schultz, and T. Joachims, “Learning a distance metric from relative comparisons,” in Proc. Advances Neural Inf. Process. Syst.,
2004, p. 41.
N. Kumar, A. Berg, P. Belhumeur, and S. Nayar, “Attribute and
simile classifiers for face verification,” in Proc. IEEE Int. Conf. Comput. Vis., Sep. 29, 2009–Oct. 2, 2009, pp. 365–372.
D. Parikh, A. Kovashka, A. Parkash, and K. Grauman, “Relative
attributes for enhanced human-machine communication,” in Proc.
AAAI Conf. Artif. Intell., 2012.
Q. Zhang and B. Li, “Relative hidden Markov models for evaluating motion skill,” in Proc. IEEE Conf. Comput. Vis. Pattern Recog.,
2013, pp. 548–555.
M. Collins, “Discriminative training methods for hidden Markov
models: Theory and experiments with perceptron algorithms,” in
Proc. ACL-02 Conf. Empirical Methods Natural Language Process.,
2002, pp. 1–8.
Y. Altun, I. Tsochantaridis, and T. Hofmann, “Hidden Markov
support vector machines,” in Proc. 20th Int. Conf. Mach. Learning,
2003, vol. 20, no. 1, p. 3.
A. Sloin and D. Burshtein, “Support vector machine training for
improved hidden Markov modeling,” IEEE Trans. Signal Process.,
vol. 56, no. 1, pp. 172–188, Jan. 2008.
G. Wang, D. Forsyth, and D. Hoiem, “Comparative object similarity for improved recognition with few or no examples,” in Proc.
23rd IEEE Conf. Comput. Vis. Pattern Recog., 2010, pp. 3525–3532.
A. Kovashka, D. Parikh, and K. Grauman, “Whittlesearch: Image
search with relative attribute feedback,” in Proc. IEEE Conf. Comput. Vis. Pattern Recog., Jun. 2012, pp. 2973–2980.
I. Kadar and O. Ben-Shahar, “Small sample scene categorization
from perceptual relations,” in Proc. IEEE Conf. Comput. Vis. Pattern
Recog., Jun. 2012, pp. 2711–2718.
S. Guo, S. Sanner, T. Graepel, and W. Buntine, “Score-based
Bayesian skill learning,” in Proc. Eur. Conf. Mach. Learn. Knowl.
Discovery Databases, 2012, pp. 106–121.
P. Dangauthier, R. Herbrich, T. Minka, and T. Graepel, “Trueskill
through time: Revisiting the history of chess,” in Proc. Adv. Neural
Inf. Process. Syst., 2007, pp. 337–344.
N. R. Howells, M. D. Brinsden, R. S. Gill, A. J. Carr, and J. L. Rees,
“Motion analysis: A validated method for showing skill levels in
arthroscopy,” Arthroscopy: J. Arthroscopic Related Surgery, vol. 24,
no. 3, pp. 335–342, 2008.
H. Lin, I. Shafran, D. Yuh, and G. Hager, “Towards automatic skill
evaluation: Detection and segmentation of robot-assisted surgical
motions,” Comput. Aided Surgery, vol. 11, no. 5, pp. 220–230, 2006.
A. G. Gallagher, E. M. Ritter, H. Champion, G. Higgins, M. P.
Fried, G. Moses, C. D. Smith, and R. M. Satava, “Virtual reality
simulation for the operating room: proficiency-based training as a
paradigm shift in surgical skills training,” Ann. Surgery, vol. 241,
no. 2, p. 364, 2005.
J. Rosen, J. Brown, L. Chang, M. Sinanan, and B. Hannaford,
“Generalized approach for modeling minimally invasive surgery
as a stochastic process using a discrete Markov model,” IEEE
Trans. Biomed. Eng., vol. 53, no. 3, pp. 399–413, Mar. 2006.
K. Kahol, N. C. Krishnan, V. N. Balasubramanian, S. Panchanathan, M. Smith, and J. Ferrara, “Measuring movement expertise in
surgical tasks,” in Proc. 14th Annu. ACM Int. Conf. Multimedia,
2006, pp. 719–722.
S.-k. Jun, P. Singhal, M. Sathianarayanan, S. Garimella, A. Eddib,
and V. Krovi, “Evaluation of robotic minimally invasive surgical
skills using motion studies,” in Proc. Workshop Performance Metrics
Intell. Syst., 2012, pp. 198–205.
L. E. Baum, T. Petrie, G. Soules, and N. Weiss. (1970). A maximization technique occurring in the statistical analysis of probabilistic
functions of Markov chains. Ann. Math. Statist. [Online]. 41(1),
pp. 164–171. Available: http://www.jstor.org/stable/2239727
B. Juang and L. Rabiner, “The segmental k-means algorithm for
estimating parameters of hidden Markov models,” IEEE Trans.
Acoustics, Speech Signal Process., vol. 38, no. 9, pp. 1639–1641, Sep.
1990.

1218

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

[30] N. Merhav and Y. Ephraim, “Maximum likelihood hidden Markov modeling using a dominant sequence of states,” IEEE Trans.
Signal Process., vol. 39, no. 9, pp. 2111–2115, Sep. 1991.
[31] D. P. Bertsekas, “Constrained optimization and Lagrange multiplier methods,” in Computer Science and Applied Mathematics, Boston, MA, USA: Academic, vol. 1, 1982.
[32] C.-C. Chang and C.-J. Lin. (2011). LIBSVM: A library for support
vector machines. ACM Trans. Intell. Syst. Technol. [Online]. 2,
pp. 27:1–27:27, Software available at http://www.csie.ntu.edu.
tw/ cjlin/libsvm.
[33] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan,
“Object detection with discriminatively trained part-based models,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 9, pp. 1627–
1645, Sep. 2010.
[34] I. Laptev, “On space-time interest points,” Int. J. Comput. Vis.,
vol. 64, no. 2, pp. 107–123, 2005.
[35] Q. Zhang, L. Chen, Q. Tian, and B. Li, “Video-based analysis of
motion skills in simulation-based surgical training,” in Proc. Int.
IS&T/SPIE Electron. Imaging, 2013, p. 86670A.
[36] T. L. Nwe, S. W. Foo, and L. C. De Silva, “Speech emotion recognition using hidden Markov models,” Speech Commun., vol. 41, no. 4,
pp. 603–623, 2003.
[37] B. Schuller, G. Rigoll, and M. Lang, “Hidden Markov model-based
speech emotion recognition,” in Proc. IEEE Int. Conf. Acoustics,
Speech, Signal Process., 2003, vol. 2, pp. II–1.
[38] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech emotion recognition: Features, classification schemes, and databases,”
Pattern Recognit., vol. 44, no. 3, pp. 572–587, 2011.
[39] A. Tarasov and S. J. Delany, “Benchmarking classification models
for emotion recognition in natural speech: A multi-corporal
study,” in Proc. IEEE Int. Conf. Autom. Face Gesture Recognit. Workshops, 2011, pp. 841–846.
[40] K. H. Kim, S. Bang, and S. Kim, “Emotion recognition system
using short-term monitoring of physiological signals,” Med. Biol.
Eng. Comput., vol. 42, no. 3, pp. 419–427, 2004.
[41] H. Mori, T. Satake, M. Nakamura, and H. Kasuya. (2008). UU
database: A spoken dialogue corpus for studies on paralinguistic
information in expressive conversation. Proc. Int. Conf. Text, Speech
Dialogue, pp. 427–434 [Online]. Available: http://uudb.speechlab.org/
[42] P. C. Woodland, J. J. Odell, V. Valtchev, and S. J. Young. (1994).
Large vocabulary continuous speech recognition using htk. Proc.
IEEE Int. Conf. Acoustics, Speech, Signal Process., vol. 2, pp. II/125–
II/128 [Online]. Available: http://htk.eng.cam.ac.uk/

VOL. 37,

NO. 6,

JUNE 2015

Qiang Zhang received the BS degree in electronic information and technology from Beijing
Normal University, Beijing, China, in 2009 and
the PhD degree in computer science from
Arizona State University, Tempe, Arizona in
2014. Since 2014, he has been with Samsung,
Pasadena, CA, as a staff research scientist in
computer vision. His research interests include
image/video processing, computer vision and
machine vision, specialized in sparse learning,
face recognition, and motion analysis. He is a student member of the IEEE.

Baoxin Li (S’97-M’00-SM’04) received the PhD
degree in electrical engineering from the University of Maryland, College Park, in 2000. He is currently an associate professor of computer
science and engineering with Arizona State University, Tempe. From 2000 to 2004, he was a
senior researcher with SHARP Laboratories of
America, Camas, WA, where he was the technical Lead in developing SHARP’s HiIMPACT
Sports technologies. From 2003 to 2004, he was
also an adjunct professor with the Portland State
University, Portland, OR. He holds nine issued US patents. His current
research interests include computer vision and pattern recognition,
image/video processing, multimedia, medical image processing, and
statistical methods in visual computing. He won the SHARP Laboratories’ President Award twice, in 2001 and 2004. He also received the
SHARP Laboratories’ Inventor of the Year Award in 2002. He received
the National Science Foundation’s CAREER Award from 2008 to 2009.
He is a senior member of the IEEE.

" For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

On Predicting Twitter Trend: Factors and Models
Peng Zhang

Xufei Wang

Baoxin Li

Computer Science and Eng. Dept.
Arizona State University
Tempe, USA
Pzhang41@asu.edu

Computer Science and Eng. Dept.
Arizona State University
Tempe, USA
Xufei.wang@asu.edu

Computer Science and Eng. Dept.
Arizona State University
Tempe, USA
Baoxin.li@asu.edu

Abstract—In this paper, we predict hashtag trend in Twitter
network with two basic issues under investigation, i.e. trend
factors and prediction models. To address the first issue, we
consider different content and context factors by designing
features from tweet messages, network topology, user behavior,
etc. To address the second issue, we adopt prediction models that
have different combinations of the two basic model properties,
i.e. linearity and state-space. Experiments on large Twitter
dataset show that both content and context factors can help trend
prediction. However, the most relevant factors are derived from
user behaviors on the specific trend. Non-linear models are
significantly better than their linear counterparts, which can be
further slightly improved by the adoption of state-space models.

discuss their relevance for better understanding of the topic. On
the other hand, existing methods typically only use simple
(non-)linear regression or classification models [2] [4], which
are in general inadequate for handling complex trend dynamics
on large-scale social networks. In this paper, we investigate
several type of prediction models which are the different
combination of two basic model properties, i.e. (non-)linearity
and (non-)state-space modeling. The validity analysis of the
features and models is evaluated on a large Twitter dataset.
#us
1200

1000

800

Keywords— information diffusion, trend prediction, Twitter.

I.

600

400

INTRODUCTION

200

Information diffusion is the process of propagation through
network links. Being able to predict or simulate such a process
may lead to many applications in, e.g. politics, economics [1].
In this work, we predict information diffusion on macro level
as information trends of some underlying topic. We use Twitter
network as a case study and focus on the prediction of hashtag
trend which is a set of tweets grouped by a hashtag, i.e. a string
starting with the character #, to represent a topic, event, etc. A
hashtag trend is measured by the number of users and tweets
involved in each time interval. Different hashtag trends may
evolve with different patterns due to many relevant factors
interacting in complex dynamics. Two basic issues lie in the
effective prediction of hashtag trend (or information trend):
relevant trend factors and appropriate prediction models.
Current research of information trend prediction regarding
the above two issues are reviewed below together with our
efforts in this papers. On one hand, many relevant trend factors
have been identified, which can generally be categorized into
two categories, i.e., context and content factors. Content
factors describe the content of trend through lexical, semantic,
and sentimental analysis. For example, LDA topic distribution
[2] is used to predict hash-tag trend. There are also simple
content features, such as the fraction of tweets containing URL
[2], the fraction of retweet/mention in a trend [3]. Context
factors generally describe the network environment, e.g.
density and centrality. User behavior is also recognized as
important context factors, e.g. retweet ratio [3]. Despite these
findings, most existing works use only one type of factors for
prediction except only two recent work [2] [4]. In this paper,
we combine all these factors for trend prediction and further

ASONAM'13, August 25-29, 2013, Niagara, Ontario, CAN
Copyright 2013 ACM 978-1-4503-2240-9 /13/08 ...$15.00

5

10

15

20

25

30

35

40

45

(a)
(b)
Fig. 1. (a) Illustration of hashtag trends. The horizon axis is time, and vertical
axis is count. The blue/green line is the number of tweets/ users respectively.
(b) Illustration of trend users (‫ܷݐ‬௛ ) and trend border users (ܾܷ௛ ). The blue
(red) dots are users have (not) adopt the hashtag h.

II. HASHTAG TREND PREDICTION
In this paper, uppercase letters ܷ, ܶ and their variations by
prefix and subscripts represent a set of users and tweets
respectively. The lowercase ‫ ݑ‬is for a user,݄ is for a hashtag,
and ‫ ݐ‬is for a time (interval). For example, ܶ௛ ሺ‫ݐ‬ሻ is the set of
tweets with hashtag݄ posted in time ‫ݐ‬, and ܷ௛ ሺ‫ݐ‬ሻ is the users
who post a tweet in ܶ௛ ሺ‫ݐ‬ሻ. Let ‫ܷݐ‬௛ ሺ‫ݐ‬ሻ be trend user of ݄ in ‫ݐ‬
as the set of users already adopted ݄ at that time, i.e. ‫ܷݐ‬௛ ሺ‫ݐ‬ሻ ൌ
‫ڂ‬௧ఛୀଵ ܷ௛ ሺ߬ሻ . Then, trend border ܾܷ௛ ሺ‫ݐ‬ሻ are the followers of
‫ܷݐ‬௛ ሺ‫ݐ‬ሻ who still have not adopted ݄ (Fig. 1 (b)).The notation
of set cardinality is ȁ ή ȁ . Time index ‫ ݐ‬is often omitted for
brevity when no confusion arises. The trend popularity of ݄
under prediction is ሾሺȁܶ௛ ሺ‫ݐ‬ሻȁሻ ǡ ሺȁܷ௛ ሺ‫ݐ‬ሻȁሻሿ.
A. Trend Factors
The content and context factors for trend prediction are
summarized in Table I. Details are given below.
Content factors. Since trend popularity contains both tweet
ܶ௛ ሺ‫ݐ‬ሻ and user ܷ௛ ሺ‫ݐ‬ሻ, our trend factors also include the two
aspects. In fact, the two sets can be divided into subsets from
different views as different content features.
The ܷ௛ ሺ‫ݐ‬ሻ can be split into three subsets based on the role
of users. The first subset ( ‫ܷ݋‬௛ ሺ‫ݐ‬ሻ ) consists of old users

1427

2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining
rejoining trend ݄ on ‫ݐ‬, i.e. ܷ௛ ሺ‫ݐ‬ሻ ‫ܷݐ ת‬௛ ሺ‫ ݐ‬െ ͳሻ (Fig. 1(b)). The
remaining two groups are new users with different relation to
‫ܷݐ‬௛ ሺ‫ ݐ‬െ ͳሻ. Specifically, the second subset (݂ܷ௛ ሺ‫ݐ‬ሻ) are either
the followers of ‫ܷݐ‬௛ ሺ‫ ݐ‬െ ͳሻ or users retweeting from ‫ܷݐ‬௛ ሺ‫ ݐ‬െ
ͳሻ. The third subset (‫ܷݏ‬௛ ሺ‫ݐ‬ሻ) are ‘self-motivated’ users who
either publish trend tweet by themselves or retweet from users
outside our network (we only have part of the entire Twitter
network). In fact,‫ܷ݋‬௛ ሺ‫ݐ‬ሻ implies the trend’s consistency by the
action of old users; ݂ܷ௛ ሺ‫ݐ‬ሻ indicates the trend’s propagation
from trend users; ‫ܷݏ‬௛ ሺ‫ݐ‬ሻ suggests the trend’s growth from the
information outside the networks.
We can also split ܶ௛ ሺ‫ݐ‬ሻ into ‫ܶ݋‬௛ ሺ‫ݐ‬ሻ, ݂ܶ௛ ሺ‫ݐ‬ሻ, and ‫ܶݏ‬௛ ሺ‫ݐ‬ሻ
which are the tweets produced by ‫ܷ݋‬௛ ሺ‫ݐ‬ሻ or ݂ܷ௛ ሺ‫ݐ‬ሻ or ‫ܷݏ‬௛ ሺ‫ݐ‬ሻ
respectively. Another division of ܶ௛ ሺ‫ݐ‬ሻ is by the type of tweets:
retweet (‫ܶݐݎ‬௛ ሺ‫ݐ‬ሻ) as the propagation of the old trend messages;
mention set (݉ܶ௛ ሺ‫ݐ‬ሻ) as the discussion among trend users; the
set remains (݊ܶ௛ ሺ‫ݐ‬ሻ) as tweets of new information. The three
subsets play different roles in trend diffusion. The ‫ܶݐݎ‬௛ ሺ‫ݐ‬ሻ
shows the propagation of ݄ by retweet; ݉ܶ௛ ሺ‫ݐ‬ሻ indicates
stickiness of ݄ by the discussion with among users; ݊ܶ௛ ሺ‫ݐ‬ሻ
implies new information injected into trend ݄. The subset with
URL (‫݈ܶݎݑ‬௛ ሺ‫ݐ‬ሻ) is also considered, since they are more likely
to be retweeted due to the limited characters in a tweet.
Context factors describe the network environment which
consist of both link topology (Structure context) and node
attribute (Node context). For efficiency and relevancy, only
two user sets (Fig. 1(b)) are considered: ‫ܷݐ‬௛ as the direct
producers of ݄; ܾܷ௛ as the direct reader of ݄.
Structure context factor. Three sub-graphs are considered:
the sub-graph of ‫ܷݐ‬௛ or ܾܷ௛ , and the bipartite graph between
ܾܷ௛ and ‫ܷݐ‬௛ which contains most pipelines of propagation to
new users. For each of these sub-graphs, the network topology
is described by three factors, i.e. centrality (in-degree), density,
and reciprocity (c.f. 11~17, Table I).
Node context factors describe the network users by their
actions and profiles. Besides the features [3] (c.f. 26~29, Table
I) on interaction, i.e. retweet and mention, as users’ influence
or will to propagate a trend, two other types of features are
designed as users’ information input/output, i.e. activeness and
stimulus. Activeness is a user’s frequency to generate
information. The general activeness of user ‫ ݑ‬in time ‫ ݐ‬is:
ܽܿ‫ݐ‬ሺ‫ݑ‬Ǣ ‫ݐ‬ሻ ൌ ߙ ή ܽܿ‫ݐ‬ሺ‫ݑ‬Ǣ ‫ ݐ‬െ ͳሻ ൅ ȁܶሺ‫ݑ‬Ǣ ‫ݐ‬ሻȁ ,

(8)

where ߙ is the decay coefficient and ܶሺ‫ݑ‬Ǣ ‫ݐ‬ሻ is the set of tweets
posted by ‫ ݑ‬in ‫ݐ‬. Trend activeness (ܽܿ‫ݐ‬௛ ሺ‫ݑ‬Ǣ ‫ݐ‬ሻ) is similar to (8)
by replacing ܶሺ‫ݑ‬Ǣ ‫ݐ‬ሻ with ܶ௛ ሺ‫ݑ‬Ǣ ‫ݐ‬ሻ, i.e. the subset of ܶሺ‫ݑ‬Ǣ ‫ݐ‬ሻ
with hashtag ݄. The ܽܿ‫ݐ‬௛ ሺ‫ݑ‬Ǣ ‫ݐ‬ሻ implies the participation of ‫ ݑ‬in
݄, and ܽܿ‫ݐ‬௛ ሺ‫ݑ‬Ǣ ‫ݐ‬ሻΤܽܿ‫ݐ‬ሺ‫ݑ‬Ǣ ‫ݐ‬ሻ ‫ א‬ሾͲǡͳሿ reflects ‫’ݑ‬s interest on ݄.
Stimulus is the volume of information received by a user. The
general stimulus of user ‫ ݑ‬in time ‫ ݐ‬is:
‫݉݅ݐݏ‬ሺ‫ݑ‬Ǣ ‫ݐ‬ሻ=ߚ ή ‫݉݅ݐݏ‬ሺ‫ݑ‬Ǣ ‫ ݐ‬െ ͳሻ+ σ௨೔‫א‬௙௥௜௘௡ௗሺ௨ሻ ȁܶሺ‫ݑ‬௜ Ǣ ‫ݐ‬ሻȁ, (9)
where ߚ is the decay coefficient,݂‫݀݊݁݅ݎ‬ሺ‫ݑ‬ሻ are the friends of
‫ݑ‬, and ܶሺ‫ݑ‬Ǣ ‫ݐ‬ሻ is same as (8). Trend stimulus (‫݉݅ݐݏ‬௛ ሺ‫ݑ‬Ǣ ‫ݐ‬ሻ) is
similar to (9) by replacing ܶሺ‫ݑ‬Ǣ ‫ݐ‬ሻ with ܶ௛ ሺ‫ݑ‬Ǣ ‫ݐ‬ሻ as the subset
of ܶሺ‫ݑ‬Ǣ ‫ݐ‬ሻ with hashtag ݄. The ‫݉݅ݐݏ‬௛ ሺ‫ݑ‬Ǣ ‫ݐ‬ሻΤ‫݉݅ݐݏ‬ሺ‫ݑ‬Ǣ ‫ݐ‬ሻ ‫ א‬ሾͲǡͳሿ
reflects ‫’ݑ‬s specific attention on trend ݄.

TABLE I. SUMMARY OF CONTENT AND CONTEXT TREND FEATURES.
Index

Description

1~3
4
5~7
8~10

Portion of ‫ܶݐݎ‬௛ (retweet) or ݉ܶ௛ (mention) or ݊ܶ௛ (new) in ܶ௛
Portion of ‫݈ܶݎݑ‬௛ (URL) tweets in ܶ௛ as ȁ‫݈ܶݎݑ‬௛ ȁΤȁܶ௛ ȁ
Portion of ‫ܷ݋‬௛ (old) or ݂ܷ௛ (followers) or ‫ܷݏ‬௛ (self-motivated) in ܷ௛
Portion of ‫ܶ݋‬௛ or ݂ܶ௛ or ‫ܶݏ‬௛ (tweets from ‫ܷ݋‬௛ or ݂ܷ௛ or ‫ܷݏ‬௛ ) in ܶ௛

11 Ratio between border and trend users: ȁܾܷ௛ ȁΤȁ‫ܷݐ‬௛ ȁ (Fig. 1(b))
12~13 Max/Average of out-degree prestige of trend user ‫ܷݐ‬௛ . (Fig. 1(b))
14 Density [5] of the sub-graph formed by ‫ܷݐ‬௛ Ǥ
15 Density [5] of the bipartite graph between ‫ܷݐ‬௛ and ܾܷ௛ .
16 Reciprocity [5] of the sub-graph formed by ‫ܷݐ‬௛ .
17 Reciprocity [5] of the bipartite graph between ܾܷ௛ and ‫ܷݐ‬௛ .
18~19 Average general activeness (ܽܿ‫ݐ‬ሺ‫ݑ‬Ǣ ‫ݐ‬ሻ) over ‫ܷݐ‬௛ or ܾܷ௛ . (See (8))
20 Average trend activeness (ܽܿ‫ݐ‬௛ ሺ‫ݑ‬Ǣ ‫ݐ‬ሻ) over users in ‫ܷݐ‬௛ . (See (8))
21 Average of ܽܿ‫ݐ‬௛ ሺ‫ݑ‬Ǣ ‫ݐ‬ሻΤܽܿ‫ݐ‬ሺ‫ݑ‬Ǣ ‫ݐ‬ሻ over ‫ܷݐ‬௛ .
22~23 Average trend stimulus (‫݉݅ݐݏ‬௛ ሺ‫ݑ‬Ǣ ‫ݐ‬ሻ) of ‫ܷݐ‬௛ or ܾܷ௛ . (see (9))
24~25 Average of ‫݉݅ݐݏ‬௛ ሺ‫ݑ‬Ǣ ‫ݐ‬ሻΤ‫݉݅ݐݏ‬ሺ‫ݑ‬Ǣ ‫ݐ‬ሻ over ‫ܷݐ‬௛ or ܾܷ௛ .
26~27 Percentage of interaction tweets of ‫ܷݐ‬௛ or ܾܷ௛ up to time ‫ݐ‬.
28~29 Interaction received by ‫ܷݐ‬௛ or ܾܷ௛ divided by all their tweets up to ‫ݐ‬
Note. In this table, time index ‫ ݐ‬is usually ignored for brevity.

B. Prediction Models
A prediction model can be characterized by two properties,
i.e. (non-)linearity and (non-)state-space, whose combination
leads to four categories. In each category, we test one typical
model as the variations to the models in other categories. We
start with a linear non-state-space ARX [6] model whose
direct state-space extension is linear dynamic system (LDS)
[6]. The non-linear ARX (NARX) can be implemented by a
feed-forward neural network, whose state-space extension is
recurrent nonlinear ARX (RNARX) [7]. Usually, non-linear
models can describe more complex dynamics, and state-space
models have a better memory of history. All models are
summarized in Table III, together with two baseline methods.
The first is Last Predictor which predicts by the last value, i.e.
‫ܡ‬ොሺ‫ ݐ‬൅ ͳሻ ൌ ‫ܡ‬ሺ‫ݐ‬ሻ . The second is Mean Predictor which
predicts by the mean up to ‫ݐ‬, i.e. ‫ܡ‬ොሺ‫ ݐ‬൅ ͳሻ ൌ ‫ି ݐ‬ଵ ή σ௧ଵ ‫ܡ‬ሺ‫ݐ‬ሻ.
III.

EXPERIMENTS AND DISCUSSION

Our Twitter dataset is collected for the purpose of Arab
Spring. The collection involved manually defined hashtags and
geographic regions related to the following countries: Egypt,
Libya, Syria, Bahrain and Yemen. We collected 16.1 million
Tweets by 0.67 million users from February 1, 2011 to August
31, 2011, which amounts to approximately 10% of the all
Tweets hosted by Twitter following the above hashtag and
geographic constraints. We select the 336 most popular
hashtags with at least 5000 related tweets for trend prediction
task, which sums up to 28.3 million tweets (> 16.1million
because a tweet can has more than one hashtag).
We evaluate prediction by Mean Square Error (MSE). All
prediction results in this section are estimated by 10-fold crossvalidation. The features in Table I are normalized before feed
into prediction models. The decay coefficients ߙ and ߚ for
activeness (8) and stimulus (9) are 0.1.
Relevance of Trend Factors is analyzed by random forest
(RF) which use the trend factors and popularity measures in

1428

2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining
the past 5 days to predict trend (similar result in longer days).
The mean variable importance [8] of each factor over the 5
days for user (ܷ௛ (t)) prediction are presented in Fig.2. Similar
result can be found for ܶ௛ (t), and omitted here due to limited
space. Fig. 2 leads to the following points. First, node context
factors on user behaviors are more important, e.g. trend
stimulus/activeness. Second, the factors with specific to the
information trend behavior are more important. For example,
the trend stimulus/activeness is derived from user’s trend
related action, and factors on the topological structures
between trend user and border are also important.

RNARX. The slight improvement may due to the fact that
long history might not be as important for Twitter trend. In
fact, for Non-state-space models, i.e. ARX and RF, the
performance will not change significantly with more than 5day history. Another reason is that state-space models with
gradient decent training might not be competent at the
complicated cost function surface of high dimensional feature
space. Further feature selection or regularization might help.
TABLE III. BEST PREDICTION MSE OF DIFFERENT MODELS.
Type
Model
ࢀࢎ(t): Mean (Std)
ࢁࢎ(t): Mean (Std)
L-NS

ARX

0.151 (0.009)

0.169 (0.006)

L-S

LDS

0.149 (0.009)

0.166 (0.007)

NL-NS

NARX

0.133 (0.007)

0.146 (0.008)

RF

0.130 (0.010)

0.142 (0.008)

NL-S

RNARX

0.128 (0.008)

0.139 (0.009)

Baseline

Last

0.175 (0.015)

0.198 (0.018)

variable importance

1
content
structure context
Node context

0.8
0.6
0.4
0.2
0

5

10

15

20

Mean
0.219 (0.017)
0.235 (0.016)
Note: L and NL is short for Linear and Non-Linear; S and NS is short for
State-space and Non-State-space. Last and Mean is two baseline predictors.

25

factor index
Fig. 2. The variable importance (normalized to [0,1]) of different trend factors
for user (ܷ௛ (t)) prediction. The factor index is the same as in Table I.
TABLE II. MSE OF PREDICTION RESULT FROM DIFFERENT TREND FACTORS.
Factors
None

ࢀࢎ(t): Mean (Std)
0.140 (0.009)

ࢁࢎ(t): Mean (Std)
0.155 (0.008)

Content

0.136 (0.012)

0.151 (0.010)

Structure Context

0.134 (0.010)

0.147 (0.009)

Node Context

0.131 (0.008)

0.146 (0.009)

All
0.130 (0.010)
0.142 (0.008)
Note: All/None means prediction with all/no trend factors.

We also compare trend factors by the prediction result
with the same RF model in Fig.2. From Table II, we have the
several points. First, trend factors really help trend prediction,
because the MSE with all trend factors is significantly better
than that without any factors. Second, the MSE of node
context, structure context, and content factors coincide with
the variable importance in Fig. 2. However, the performance
gap is not significant, which might because these factors are
complementary and have importance on their own aspects.
Comparison of Prediction Models is summarized in
Table III, where models are trained with all the features in
Table I. For each model, we tried several setups and present
the best result. For ARX, RF, and NARX model, we go
through order 1 to 5. For LDS we check the order up to 4. For
both the NARX and RNARX, the network layers vary from 1
to 2, and the hidden node number varies from 2 to 10.
The following observations come from Table III. First, the
hashtag trend is predictable on some degree, for the prediction
result of all models are significantly better than the baseline
methods. Second, nonlinearity is critical for prediction, which
is clear from the performance gap between linear models, i.e.
ARX and LDS, and their non-linear counterparts, i.e. NARX
and RNARX. Third, state-space is helpful but not as important
as nonlinearity. In fact, the performance gap between ARX
and LDS is not significant. The situation is similar for RF and

IV.

CONCLUSION AND DISCUSSION

In this paper, we study the two basic problems in trend
prediction, i.e., important factors and appropriate models, with
focus on Twitter network. We investigate different factors on
both tweet content and network context for trend prediction.
We also compare prediction models as the combination of two
basic model properties, i.e., (non)linearity and (non-)statespace modeling. We report some insightful findings from
comparative experiments on large Twitter dataset. Future work
includes the following two directions. First, the semantics and
sentiments of a trend is relevant to its diffusion process, so
further investigation on feature design is worthwhile. Second, a
network may change over time and thus adaptive models may
be needed to account for this issue.
ACKNOWLEDGMENT
The work was supported in part by a grant (#1135616)
from the National Science Foundation. Any opinions expressed
in this material are those of the authors and do not necessarily
reflect the views of the NSF.
REFERENCES
[1]
[2]
[3]
[4]

[5]
[6]
[7]
[8]

1429

Sheng Yu and Subhash Kak, "A Survey of Prediction Using Social
Media," arXiv:1203.1647, 2012.
Zongyang Ma, Aixin Sun, and Gao Cong, "Will this #hashtag be popular
tomorrow?," in SIGIR, 2012, pp. 1173-1174.
Jiang Yang and Scott Counts, "Predicting the Speed, Scale, and Range
of Information Diffusion in Twitter," in ICWSM, 2010
O. Tsur and A. Rappoport, "What’s in a hashtag?: content based
prediction of the spread of ideas in microblogging communities," in
WSDM, 2012, pp. 643-652.
M. E. Newman, Network: an introduction.: Oxford Press, 2010.
Lennart Ljung, System Identification: Theory for the User (2nd edition).:
Prentice Hall, 1998.
Simon Haykin, Neural Networks and Learning Machines, 3rd edition.:
Prentice Hall, 2008.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of
Statistical Learning: Data Mining, Inference, and Prediction (2nd
edition).: Springer, 2009.

Image Rectification for Stereoscopic Visualization
Without 3D Glasses
Jin Zhou and Baoxin Li
Department of Computer Science & Engineering,
Arizona State University,
Tempe, AZ, 85287, U.S.A.
{Jin.Zhou, Baoxin.Li}@asu.edu

Abstract. There exist various methods for stereoscopic viewing of images,
most requiring that a viewer wears some special glasses. Recent technology
developments have resulted in displays that enable 3D viewing without glasses.
In this paper, we present results from our proposed approach to automatic
rectification of two images of the same scene captured by cameras at general
positions, so that the results can be viewed on a 3D display. Both simulated and
real data experiments are presented.
Keywords: Image rectification, stereo, 3-D visualization.

1 Introduction
Recent display technologies have led to various low-cost 3-D displays that enable
stereoscopic viewing without inconvenient 3D glasses (see [1] for examples). While
the underlying technologies may vary from one manufacture to another, the basic
principle of many 3D displays can be illustrated by Fig. 1, where a parallax barrier is
used in LCD to direct the light rays from the pixels to the viewer’s right and left eyes,
respectively. There are many potential applications of this type of displays. However,
from Fig. 1, it is obvious that, to enable the stereoscopic viewing capability of a 3D
display, one must have the “correct” left and right eye image pair. If the image pair is
captured by a standard stereo rig giving proper disparities (i.e., that conforms to the
constraints of the display), then this problem is solved. However, true stereo media is
scarce, and general consumers rarely use stereo cameras. These unfortunate facts limit
the otherwise great potential of the 3D displays.
In this paper, we present results from our algorithms for addressing the following
image rectification problem: Given any two images of the same scene from general
viewpoint, rectify them so that they look like a true stereo pair like that from a
standard stereo rig required by a 3D display. If this problem is solved, we can enable
stereoscopic viewing of a lot of media without requiring stereoscopic acquisition.
Existing image rectification approaches are mostly for stereo matching rather than
forming a true stereo pair, which is our goal.
H. Sundaram et al. (Eds.): CIVR 2006, LNCS 4071, pp. 495 – 498, 2006.
© Springer-Verlag Berlin Heidelberg 2006

496

J. Zhou and B. Li

2 Advantages of the Proposed Approach
Detailed description of the proposed algorithm is available through a technical report
from cubic.asu.edu. In this section, we highlight the key points of the approach as
follows. Firstly, we proposed a new rectification algorithm, starting from the
calibrated case. This leads to an intuitive interpretation of the algorithm, which is not
available from existing derivations. Secondly, we use a physically meaningful
constraint for removing the ambiguity in estimating the required homographies for
rectification. The resultant image pair does not have visual distortion that may be
present in most existing algorithms such as [3][4]. Thirdly, we designed several
practical schemes for shifting the rectified images such that they conform to the
disparity requirement preferred by the 3D display.

Fig. 1. An illustration of how a parallax-barrier-based 3D display works

3 Demonstration of the Experimental Results
We present three types of experiments to validate the proposed algorithms, as detailed
in the following. Note that, although we can only present sample images here, in our
experiments, the rectified pairs were further validated by visualizing on a SHARP 15in 3D display, which will be used in the demo on CIVR 2006.
Experiment I. In the first experiment, we use the data made available on the Internet

by the Interactive Visual Media Group at Microsoft Research, which contains
accurate camera calibration information and thus allows us to verify the key
components of our algorithms without having to use a fundamental matrix estimated
from raw data. The data set contains videos captured from 8 different cameras. We
select a pair of images from any two cameras (illustrated in Fig. 2, the left column).
After applying our rectification algorithm, we got the rectified results as illustrated in
Fig. 2 (center column), where the epipolar lines becomes horizontal and also aligned.
This satisfied the epipolar constraints. Moreover, we can find that the image do not
have obvious visual distortion, demonstrating that the ambiguity in the rectification is
removed by the proposed algorithm. As a comparison, we also rectify the pair using
the given camera matrices (i.e., using the algorithm for the calibrated case, e.g.
see[2]), obtaining the results in Fig. 2 (right column). The shift of the right image is

Image Rectification for Stereoscopic Visualization Without 3D Glasses

497

due to the large distance between the two cameras. Other than that, there is no big
difference between the results from the calibrated and the uncalibrated algorithms.
Experiment II. In the second type of experiments, we use OpenGL to simulate views
from different camera positions, which can be precisely controlled. This provides us

Fig. 2. Left column: original pair. Center column: rectification results based on fundamental
matrix. Right column: results based on the proposed method.

Fig. 3. (a) and (b) are standard stereo pair. (c) is the view after rotating camera on (b)’s
position. (d) and (e) are rectified results on (a) and (c) purely based on image. (d) and (e)
apparently form a stereo pair.

498

J. Zhou and B. Li

with virtual stereo pairs of any desired configuration (e.g., any baseline).In the
example illustrated in Fig. 35, the scene is made up of two planes at different depth.
Fig. 3 (a) and (b) shows a desired stereo pair. Fig. 3 (c) is a view after rotating the
camera of (b) (note that this is different from simply rotating the image). We now
rectify (a) and (c), with the desired camera configuration as the goal. Ideally, after the
rectification, we should have a pair like (a) and (b). Fig. 3 (d) and (e) are the results,
which are almost the same as the true stereo images (a) and (b) except for a horizontal
translation. Note that in this type of experiments, the algorithm for the uncalibrated
case is used, meaning that we have to estimate the fundamental matrix from the
synthesized images. The ambiguity of the horizontal translation arises from the fact
that our algorithm relies on the fundamental matrix, and thus one of the proposed
shifting schemes should be used before visualization on a 3D display.
Experiment III. The third experiments use real images captured by a hand-held camera
from different positions. Sample results are given in Fig. 4, in which the left column is
the original image pair, the center column the results of the method in ([3]), and the
right column our results. It is obvious that there is shear distortion in the center column.

Fig. 4. Left column: original pair. Center column: rectified results based on Hartley’s method.
Right column: results of the proposed method.

Acknowledgements. Some of the experiments were based on data made public on the
Internet by the Interactive Visual Media Group at Microsoft Research.

References
1. May, P.: A Survey of 3-D Display Technologies. Information Display. 32 (2005) 28-33.
2. Fusiello, A., Trucco, E., Verri, A.: A compact algorithm for rectification of stereo pairs.
Machine Vision and Applications. 12 (2000), 16-22.
3. Hartley, R., Zisserman, A.: Multiple view geometry in computer vision. 2nd edition.
Cambridge University, Cambridge (2003).
4. Loop, C., Zhang, Z.: Computing rectifying homographies for stero vision. Proc. of IEEE
Conf. on Computer Vision and Pattern Recogntion (1999), 125-131.

Computer Vision and Image Understanding 114 (2010) 1139–1151

Contents lists available at ScienceDirect

Computer Vision and Image Understanding
journal homepage: www.elsevier.com/locate/cviu

On implementing motion-based Region of Interest detection on multi-core CELL
Avin Kumar, Baoxin Li *
Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85281, USA

a r t i c l e

i n f o

Article history:
Received 23 January 2009
Accepted 17 March 2010
Available online 4 May 2010
Keywords:
Region of Interest detection
Global motion estimation
Multi-core CELL
Real-time object detection

a b s t r a c t
Region of Interest (ROI) detection is a well-studied problem in computer vision for applications such as
video surveillance and vision-based robotics. ROI detection may be done using background subtraction
schemes with change detection and background estimation. When the camera is not static, these
schemes will be ineffective and hence there is a need for global motion estimation (GME) to compensate
the camera motion. Robust GME algorithms often require high computation power, rendering them
unsuitable for real-time, embedded vision applications. In this article, we use a multi-core processor platform – CELL, to meet the computational requirements of the ROI detection system and to explore the feasibility of potential usage of such heterogeneous processor architecture for vision applications. In
particular, we analyze the algorithmic components of a typical GME-based ROI detection system and
show how to make efﬁcient use of the parallel and vector computation capabilities in the CELL cores
for maximizing the gain on speed performance. We have also ported our system on a Sony PS3 system
and promising results have been achieved. Based on the study, various design aspects and implementation challenges are discussed which are believed to be useful for future work in porting vision algorithms
on multi-core architectures for real-time embedded applications.
Ó 2010 Elsevier Inc. All rights reserved.

1. Introduction
Region of Interest (ROI) or object detection is a well-studied
problem in computer vision applications such as video surveillance, trafﬁc analysis, autonomous robots, collision avoidance systems in automotive, and so on [21–23,25,26]. ROI detection is also
a pre-processing step for more complex vision tasks such as object
tracking, event detection and analysis [18,24,28]. Motion-based
ROI or object detection is a widely-used approach, which is relatively well-posed (e.g., via direct background substraction) if the
imaging sensor is static during acquisition. However, the problem
becomes challenging if the sensed video exhibits global camera
motion as in the case of a camera mounted on a moving platform
such as a robot or an unmanned aerial vehicle (UAV). In such cases,
the camera motion can render simple change detection schemes
ineffective and thus there is a need for compensating for the global
camera motion. Global motion estimation (GME) has been studied
for a wide range of vision applications such as object detection and
tracking and also has been used for video coding applications. In
fact, MPEG 4 Advanced Simple Proﬁle (ASP) performs GME for motion estimation, which improves the coding efﬁciency considerably. In general, GME is a complex task and has demanding

* Corresponding author.
E-mail addresses: akannur@asu.edu (A. Kumar), baoxin.li@asu.edu (B. Li).
1077-3142/$ - see front matter Ó 2010 Elsevier Inc. All rights reserved.
doi:10.1016/j.cviu.2010.03.013

processing requirements, making it impractical to be used in
real-time, embedded vision applications.
In recent years, the evolution of multi-core architectures in
mainstream processors has enabled real-time processing capabilities for implementing computationally expensive algorithms. The
current focus in processor technology is to design fast, low power
architectures to make multi-cores feasible for real-time battery
powered embedded solutions. A growing number of chip vendors
have announced multi-core system on chips (SoC) designs recently,
including Broadcom (MIPS), PMC-Sierra (MIPS), Freescale (PowerPC), Cavium (MIPS), TI (DaVinci-ARM11/DSP), ARM Ltd. (ARM),
NEC (ARM), and Centrality (ARM9/DSP). This provides a motivation
to study and analyze computer vision algorithms, which are in
general computationally costly, on multi-core processors and to
explore design implications and algorithmic tradeoffs needed to
efﬁciently harness the processing power and achieve real-time system performance. Thus far, a few basic vision algorithms have been
demonstrated for real-time performance on multi-cores [3,5,10],
and it remains to be seen how a complete vision system may be
ported onto multi-core platforms.
In this article, as a representative application in computer vision, we implement a real-time motion-based ROI detection system on a multi-core CELL platform, assuming a moving camera.
Detecting moving objects (soldiers, armed vehicles like tanks) in
videos captured from an unmanned aerial vehicle (UAV) can be
as a target application for the studied motion-based ROI system.

1140

A. Kumar, B. Li / Computer Vision and Image Understanding 114 (2010) 1139–1151

The system is analyzed using a top-down design approach starting
from algorithmic analysis down to architecture level mapping of
computations. The analysis involves data level parallelization of
module computations, vectorization of computations to efﬁciently
utilize the SIMD capabilities, and data ﬂow optimizations to minimize memory transfer latencies, all of which have jointly contributed to the real-time implementation of the ROI detection system.
The article is structured henceforth in the following manner. Section 2 gives a summary of the related works, Section 3 outlines
the system components and algorithmic analysis, Section 4 introduces the CELL architecture and provides the implementation details of the ROI detection system on CELL, and Section 5 presents
the experimental results and analysis on the simulator and the
Sony Play station (PS3) platform. We conclude our work in
Section 6.

[5], medical imaging, digital content distribution (particularly onthe-ﬂy transcoding), aerospace and defense [2,9,12,19,20]. A CELL
customized for embedded low-power applications is not far from
realization and serves as a motivation to explore implementing
computer vision systems/algorithms on CELL.
3. Overview of a motion-based ROI detection system
In this section we outline the ROI detection system which is
chosen as a representative computer vision application for the
implementation and analysis on a multi-core CELL architecture. A
system block diagram of the ROI detection system is show in
Fig. 1. Such a processing ﬂow and the algorithmic components
have been widely used in computer vision-based applications,
and thus we only present a brief discussion on the sub-systems.

2. Related works

3.1. Global motion estimation

Automatic object detection from images/video is in general a
challenging task. With the assumption that an object of interest
is subject to motion, the task may be achieved using motion-based
ROI detection. More speciﬁcally, we deﬁne ROI as any ‘foreground
motion’ which may represent a semantically interesting moving
object in a video. ‘Change detection’ schemes involving frame differencing and morphological postprocessing steps are computationally efﬁcient and provide a feasible solution for real-time ROI
detection [4,28]. For videos acquired on a moving sensor, GME is
typically employed to compensate the sensor motion before applying any change detection algorithm. Among others, feature-based
GME is a popular class of GME algorithms [6,17,23], which we have
adopted in our ROI system implementation and will be addressed
brieﬂy in Section 3.1. Commonly used feature detectors are typically heavy on computation and thus pose as bottlenecks for
real-time processing. A pipelined hardware architecture to realize
a multi-layer Harris corner detector in real-time was proposed by
Lu and Fu [15].
Multi-core processors provide a new paradigm shift to the problem in hand: to develop a complete vision system for real-time
embedded applications [7,14,27]. Advantages of using multi-core
based system implementation over ASIC speciﬁc architectures are
lower costs owing to fast development time, re-programmability
with algorithmic modiﬁcations, and re-usability of software.
Researchers on architecture have proposed multi-core chips speciﬁc for vision-based applications to meet the speed, low-cost
and low-power requirements in embedded solutions [8]. Zaglewski
and Wojcikowski [27] proposed a simple dedicated 8-bit processor
core capable of performing low-level operations such as background subtraction, moving object extraction or geometrical transformation of the image, and intermediate or higher level
processing such as object indexing, blob size and shape estimation
or basic trajectory analysis. Miao et al. [16] present a programmable vision chip for real-time vision applications. The chip architecture has a combination of a SIMD processing element array and
row-parallel processors, which can perform pixel-parallel and
row-parallel operations at high speed.
Heterogeneous multi-cores are fairly new breed of multi-cores
chips tailored for speciﬁc application domain needs. Signal-Processing On-Demand Architecture (SODA), a low power DSP with
an ARM core and four wide SIMD processing elements running at
400 MHz, is one such application speciﬁc multi-core developed
for wireless applications [13]. IBM, Toshiba and Sony jointly came
up with a multi-core architecture CELL Broadband Engine (referred
henceforth as CELL), primarily for gaming applications in Sony Play
station (PS3). Beyond gaming CELL has already demonstrated its
performance capabilities for applications such as video coding

Feature-based GME is a well-studied class of algorithms for
compensating the camera motion in ROI detection. The algorithmic
components for GME in our study are derived from the common
principles of feature-based GME scheme. The ﬁrst step is ‘feature
extraction’, which involves detection of key points in a given frame.
We use Harris corner detector (HCD) in our system for feature
extraction. The second step is ‘feature correspondence’, which involves establishing a mapping between the feature points in the
two participating frames. For this process we evaluate the Sum
of Absolute Difference (SAD) of a small patch (5  5) centered on
the feature points. The pair of patches that yields the lowest SAD
is kept as the correspondence. Those feature points that have no
good correspondences (controlled by the SAD threshold) are
eliminated. The ﬁnal step is ‘robust parameter estimation’, which
estimates the motion parameters from the feature point correspondences.The feature correspondence set will have feature
points both from the background and from the foreground objects.
The motion in the scene is a mixture of camera motion and foreground object motion and hence there is a need for a robust motion
estimator to recover the camera motion, which we assume to be
the dominant motion. A known robust estimator technique is the
random sample consensus algorithm (RANSAC), which tries to ﬁnd
the best subset among feature correspondences that explain the
dominant motion. For our motion modeling we use a six-parameter afﬁne model for the camera motion. Afﬁne model is a reasonably accurate model for the camera motion in applications such
as processing videos captured by UAVs.
3.2. Change detection
The global camera motion model obtained from GME can be
used for global motion compensated ROI detection using a simple
change detection based scheme. This involves three steps viz global motion compensation (GMC), frame differencing with thresholding, and morphological postprocessing. We use afﬁne model
as an example, and thus GMC involves afﬁne warping of the previous image using an afﬁne motion model A such that the new
^j Þ in the current frame due to camera
mapped pixel locations ð^
xj ; y
motion would be described by the following equations:

^xi ¼ a0 þ a1  xi þ a2  yi
^i ¼ b0 þ b1  xi þ b2  yi
y
^i Þ ¼ Iprev ðxi ; yi Þ
Iwarp ð^xi ; y

ð1Þ
ð2Þ
ð3Þ

We then perform simple frame differencing between the
warped image and the current image and threshold it to obtain a
coarse binary ROI mask.

A. Kumar, B. Li / Computer Vision and Image Understanding 114 (2010) 1139–1151

1141

Fig. 1. Block diagram of the ROI detection system.

ROImask ¼ jIcurr  Iwarp j P IThreshold

ð4Þ

The binary mask ROImask so generated can be noisy and needs
postprocessing to merge small blobs and remove noise. We employ
a simple morphological ﬁlter to do the postprocessing of ROImask.
Some sample results from the Coastguard and Stefan sequences
are shown in Fig. 2.

Fig. 2. ROI detection on Coastguard QCIF sequence (frame 15), Stefan CIF sequence
(frame 5).

4. Implementing the ROI detection system on CELL
In this section, we present a systematic design for implementing the ROI detection system on CELL. As shown in Fig. 3, CELL processor is composed by a Power-Architecture-compliant Power
Processor Element (PPE) and eight Synergistic Processor Elements
(SPE) [11]. These processing cores are connected through an Element Interconnect Bus (EIB), which also connects them with the
memory through a Memory Interface Controller (MIC), and the I/
O controller (IO). The PPE is composed of a 64-bit PowerPC (PPU)
and a 2-level cache hierarchy. Each of the SPEs is composed by a
Synergistic Processor Unit (SPU), a Local Store (LS) and a Memory
Flow Controller (MFC) that contains the direct memory access
(DMA) engine. The PPU is a 2-way multithreaded in-order processor with fully power compliant instruction set architecture (ISA)
and single instruction multiple data (SIMD) instructions implementing the Altivec/VMX Instruction Set. This core runs the operating system, and acts as the master of the system. The SPU is also
in-order, and has a newly architected ISA, completely SIMD. The
SIMD functional unit in each SPU can perform four single-precision
ﬂoating point operations per cycle, leading to an unprecedented 32

1142

A. Kumar, B. Li / Computer Vision and Image Understanding 114 (2010) 1139–1151

Fig. 3. CELL Broadband Engine (CELL BE) architecture [11].

FP ops/cycle potential performance in a commodity component.
Each SPU has only direct access to its own LS, and thus if it wants
to communicate to another element outside the SPE, it has to perform a DMA transaction through its MFC.
4.1. System design on CELL
We now present the implementation details of the ROI detection system on CELL. We explore the generic principles for multicore system implementation, discuss the applicability with respect
to CELL, and systematically choose the best design aspects for the
ROI system under study. The topics covered include concurrency
scheme selection, system control ﬂow between PPE and SPE, optimization strategies on SPE for algorithmic components to achieve
the best module and system performance.
4.1.1. Concurrency scheme
To make efﬁcient use of the CELL architecture, the workload of
the system has to be distributed among the powerful SPE SIMD engines. The main objective of the parallelism scheme is to achieve
best throughput (speed) and good load balancing. Load balancing
is critical and can lead to performance bottleneck if some of the
cores are under-utilized or over-loaded, which will bring down
the system throughput. This is severe in cases where the data ﬂow
propagates through a chain of cores. Conventional concurrency or
parallelization schemes can be broadly classiﬁed as ‘Data Level Parallelism’ and ‘Function Level Parallelism’. Function Level Parallelism (FLP) involves breaking down a (large) task into multiple
non-overlapping (smaller) subtasks with inter-data dependencies
in the subtasks. In Data Level Parallelism (DLP), the workload data
is divided into non-overlapping blocks and distributed to the cores
for processing. Each core thus works on different block of data,
computes the result, and sends back to the main system task.
The FLP scheme can be very efﬁcient if load balancing of the
core work tasks can be achieved. In such a scheme, the inter-task
data dependency is a critical design aspect and we need fast ways
of communication among the cores. In CELL, the main task is the
control task running on PPE which runs the operating system
and manages the tasks scheduled on SPE. The data transfers from
the PPE to SPEs and SPE to SPE use EIB which provides very high
data rates (134.4 GB/s peak at half processor speed) and thus the
FLP scheme seems to be a good option. However load balancing
is very difﬁcult and due to unequal workload proﬁle for the modules of our system, FLP is not used.

The DLP scheme is relatively easier to implement since most of
the modules in our system can be designed to work on a localized
block of data and hence can be parallelized efﬁciently to achieve
good load balancing among the SPEs. There are some drawbacks
in a DLP-based scheme, such as the need for synchronization between the main task on PPE and the core tasks on multiple SPEs
to maintain data integrity and ﬂow control, a more sophisticated
control code on PPE, and duplication of code in the SPEs (since
all cores will perform the same set of data operations). However,
these can be handled well in the CELL architecture and will be discussed later in the paper.
4.1.2. System schedule
As discussed earlier, we follow DLP concurrency scheme with
PPE acting as the control engine involved in data management
and task synchronization with SPEs which share bulk of the computationally expensive operations. The control ﬂow diagram for
the ROI detection process is shown in Fig. 4. A control block data
structure is created which stores information regarding current
and previous frame data pointers, frame number, image block size
for the SPE, output buffer pointers for the SPE to store the task results and other control information which needs to be shared between the PPE and the SPE. In our proposed system design, the
PPE is used as the control unit which manages the data and allocates task to the SPEs to perform all the computations. The PPE
populates the control block prior to each task processing and then
communicates to the SPEs to start processing of the control block
by writing to the mailbox of individual SPEs. The input image is divided into image blocks for each SPE to work independent of others
as shown in Fig. 5. The SPE reads the control block, fetches the inputs from the main memory to its local store (LS) using DMA (discussed in Section 4.2.1) and performs the appropriate task
governed by the control ﬂow. The synchronization between the
PPE and the SPE is done using the inbound and outbound mailboxes of the SPE and all data transfers between the SPE LS and
the main memory are DMA based controlled by Memory Flow Control (MFC) unit of each SPE.
4.2. System optimization
To efﬁciently port an algorithm on CELL and achieve peak performance from all the SPEs, we must optimize several aspects of
the system software to make the best use of the architecture. These
include setting up efﬁcient data ﬂow strategies between the main

A. Kumar, B. Li / Computer Vision and Image Understanding 114 (2010) 1139–1151

1143

Fig. 4. System schedule of ROI detection system on CELL.

Fig. 5. Input video frame data partitioning on CELL.

memory and the internal local store, SIMD instruction utilization,
loop transformation and pipelining. In this section we discuss in
detail each of the above components which are necessary to efﬁciently port the system onto CELL for maximized speed
performance.

4.2.1. Efﬁcient data ﬂow between local store and external main
memory
All data ﬂows from the SPE LS to the main memory are done
through DMA. Each individual SPE has a limited LS memory size
(256 KB) and hence we cannot afford to have huge memory buffers
to hold data during computations. Since the tasks such as feature
extraction and change detection can be done on a smaller subblocks (smaller than an image block allocated to the SPE) we can
implement the algorithms using smaller buffers and also align
DMA transfers in the background to increase the throughput. We
have used two such techniques for DMA transfers – DMA double
buffering and DMA List based transfer.
DMA double buffering involves two sets of transfer and receive
buffers which will be used alternatively in every sub-block processing cycle. This will increase the throughput since the SPE need
not wait for the entire block data to be transferred from the main

memory and in turn can process on an already transferred subblock while the DMA hides the next sub-block transfer in the background. The point to be considered is to select a suitable sub-block
size to stagger such DMA transfers. We need to access contents
starting from different addresses in the main memory simultaneously (current frame and previous frame data) and such accesses
can be initiated using the DMA list feature of MFC. DMA listing will
allow SPE to initiate multiple DMA transfers and need not intervene in between each transfer. One important aspect of achieving
peak DMA performance is to have memory aligned buffers and a
proper DMA transfer size. The cache line size in CELL is 128 bytes
and we have aligned all our frame buffers to 128 byte boundary.
DMA transfers with sizes greater than 1024 bytes show peak performance [1] which we have considered while using the sub-block
size.

4.2.2. SIMD optimization
SPE cores are based on the SIMD architecture with 128 wide
vector instructions capable of performing the same operation on
multiple data (in the vector). Moreover, all load and store accesses
are on quad-word (128 bits) and scalar accesses and computations
are not efﬁciently handled on the SPE. To identify whether our

1144

A. Kumar, B. Li / Computer Vision and Image Understanding 114 (2010) 1139–1151

algorithmic components support SIMD compatibility, we need to
analyze our modules for vector and scalar load percentages. It is
also necessary to derive the amount of integer and ﬂoat operations
and vector element widths needed in computations to pack efﬁciently the scalar data on the 128 wide vectors. Our analysis for
the ROI system is tabulated in Table 1. We see that most modules
show good amount of vector load percentage which further boosts
our choice of picking CELL for our system design. Since the register
width is 128 bits, we ﬁt all our vector operations to this size. To
exemplify this, in corner detection, we pack 16 pixels in one vector
(16  8 bits) for the SIMD based code. More speciﬁcally, a sample
illustration of the gradient calculation involved in HCD is shown
in Fig. 6. Similar strategies are also used in other steps of HCD. Results on performance gain due to scalar and SIMD optimized code
will be presented in the simulation section.

4.2.3. Loop transformations
Loop transformations are a popular class of architectural techniques to increase speed and performance. The SPE has a large register ﬁle size (128) and efﬁcient use of this ﬁle can lead to great
performance beneﬁts as seen from our simulation results. We analyzed the register ﬁle usage in the scalar code and found potential
usage of the register ﬁle to unroll the loops to include more operations. Another criterion to be considered while optimizing loops is
the memory access patterns. For example, if the frame buffers are
stored in row major order, then our loops should be organized such
that the accesses for computation follow this order, i.e., we access
the elements by reading through rows. Irregular and unaligned
memory accesses will introduce load/store penalty stall cycles in
the SPE and can diminish the achievable performance boost.

4.2.4. SPE speciﬁc optimizations
The key to achieve peak SPE performance is to vectorize the
code to the extent it can be handled by the register ﬁle in the
SPE. Since the register size is 16 bytes, our implementation strategy was to pack as much data in a register to utilize the SIMD
instruction capabilities. Further, since memory accesses are on a
16 byte alignment, having just the input data aligned to this
boundary is not sufﬁcient, but we must be able to access them with
16 byte alignment. Modules such as Harris corner detector, warping, change detection, postprocessing have good regularized memory access patterns and can be used while SIMD optimizing the
code. In addition to these, we have also applied other techniques
to minimize the computations such as partial result reuse, life time
analysis of registers to understand the extent of register ﬁle usage
which helped us to unroll the loop to higher extent than supported
by compiler. The SPE does not possess hardware branch prediction
units and hence to achieve good prediction efﬁciency we have used
branch hint directive for compiler to optimize the code better. The
SPU has two pipelines (even and odd) which can execute simultaneously two classes of instructions, and we can pipeline a load/
store operation and a computation instruction on these pipelines
in one cycle. The only pre-requisite is to have no data interdependencies between the two instructions in which case there will be
stall in the pipeline. To improve the dual issue rate we need to
interpret the code at assembly level and re-organize the computations to minimize the data interdependencies. For our implementation we have relied on the compiler to improve the dual issue
rate, but care is taken while using the intrinsic to align the fetch
and computation instructions. For all our algorithms, we have
managed to achieve good memory access by efﬁciently preparing
data lists to match the architectural constraints. To exemplify,
the list generated for feature correspondence include pixel values

Table 1
SIMD Compatibility analysis in ROI system modules.
Module

Scalar workload (%)

Vector workload (%)

Vector width

Vector element width (bits)

Float operations (%)

Harris corner detector
Feature correspondence
RANSAC
GMC
Frame difference
Post-processing

28
15
20
8
1
5

72
85
80
92
99
95

32–128
8–200
16–128
32–128
8–128
8–128

32
32
32
32
8
8

80
0
90
70
0
0

Fig. 6. SIMD implementation of gradient on CELL.

A. Kumar, B. Li / Computer Vision and Image Understanding 114 (2010) 1139–1151

1145

5.1. Simulation setup

Fig. 7. Harris corner detector performance with algorithmic and SIMD optimizations for Coastguard QCIF sequence. SC Unopt is a ﬂoating point version, SC Opt
ﬂoating point with compiler optimization, SC Integer Opt is integer version of harris
corner detector with compiler optimization and loop unrolling, SIMD Integer Opt is
SC Integer Opt with further hand optimization.

at a 5  5 patch for SAD calculations which are packed in 32 byte
vector with additional padding bytes. This is beneﬁcial in two
ways: ﬁrst it gives aligned storage of data needed for DMA transfers; and second it helps in vectoring the computations in SPE.

4.2.5. Algorithmic speciﬁc optimizations
We identiﬁed that, Harris corner detector involved the highest
complexity in terms of computations as it had a large number of
ﬂoating point calculations. A module load analysis for the system
indicates over 78% of computation time is taken by the corner
detector and we need to identify the potential algorithmic noise
tolerances to bring down the number of computation cycles (refer
Fig. 8). We ﬁrst perturbed the Gaussian smoothing kernel which is
used to smoothen the gradients during the corner response calculation. The modiﬁed kernel can now be implemented using shifts
instead of ﬂoating point operations. Further we removed all ﬂoating point calculations and implemented them using ﬁxed point
arithmetic which gave considerable performance gains in the corner detection stage which are highlighted in Fig. 7.

5. Experimental results and analysis
In this section, we ﬁrst present the simulation details and the
analysis of our ROI detection system on the CELL Software Development Kit (SDK). The simulation environment presents a good
platform to design, develop and optimize a system before porting
onto the real-time CBE system such as PS3, the results from which
will be covered later in the section.

Simulations were based on CELL SDK 3.0 running on Fedora
Core 7. The input test sequences were given as ﬁles in raw YUV format with 4:2:0 subsampling mode and only the luminance component is used for detection. Standard test sequences with varied
camera motion such as Coastguard (steady camera pan with foreground object motion), Stefan (faster camera motion with background motion), custom test sequence captured from handy-cam
from real-time trafﬁc, surveillance, indoor scenarios with a varied
combinations of camera motions were used to test and proﬁle
the system. For proﬁling the system, we used hardware counters
in SPE to calculate the module computation times and dynamic
proﬁling for module optimizations. Our simulation setup consists
of a PPE core running at 3.2 GHz, and up to eight SPEs running at
3.2 GHz. The analysis results presented are by default for the
Coastguard QCIF sequence (unless stated otherwise) and similar
proﬁling results were observed for other test sequences as well.
5.2. Computational load analysis
We proﬁled our system to understand the workload percentage
of each of our functional blocks in the algorithm. For this purpose
we used our scalar un-optimized code which was data partitioned
and load-balanced among available SPEs. The result of the simulation workload percentage breakdown is shown in Fig. 8. We can
see that, Harris corner detector takes the bulk of the processing
workload due to its algorithmic complexities and expensive computations. Feature correspondence is also computationally intensive due to the full search correspondence scheme used and
afﬁne warping has expensive ﬂoating point operations. Such analysis helped us to focus on optimizing the key workload modules for
our system for which we used dynamic proﬁling capabilities in the
simulation environment. This exempliﬁes a typical ﬁrst step for a
designer to begin with in porting a vision system to CELL.
5.2.1. Module analysis
We used dynamic proﬁling capabilities from the SDK to proﬁle
the system modules and identify the performance bottlenecks and
optimize the code for achieving high throughput (target frame
rate). For this we inserted proﬁling check points in critical sections
of the modules and captured metrics which will help in code optimization. For our discussion we take Harris corner detector module
as a representative example for the system and similar analysis
were done for optimizing other modules as well. The proﬁled measures for module optimization are tabulated in Table 2.
We used scalar un-optimized (referred as SC Unopt) version, a
compiler optimized scalar version (SC Opt) and a ﬁne level optimized code (FL Opt) using the optimization strategies described
in Section 4.2.5. We can see from Table 2, that SC Unopt version

Fig. 8. Computational load percentage breakdown on CELL SDK for core system modules before and after optimization. Coastguard sequence used for proﬁling.

1146

A. Kumar, B. Li / Computer Vision and Image Understanding 114 (2010) 1139–1151

Table 2
Dynamic proﬁling results for Harris corner detection on Stephan sequence. Table shows comparison of proﬁled metrics for various levels of optimization.
Codebase

CPI

CLK cycles (Mega)

Single cycle (%)

Dual cycle (%)

Data dep stalls (%)

Load store stalls (Mega)

Corners

Boost

SC Unopt
SC Opt
FL Opt

2.98
1.79
1.46

2008.4
317.1
186.1

25.8
34.4
39.1

3.5
8.6
11.3

57.1
29.4
26.8

676.5
28.2
8.82

357
357
329

1.00
6.63
10.8

of the code is severely impacted by high load store stall cycles, high
dependency stall cycle percentage due to scalar accesses. As a ﬁrst
step we used the compiler optimization to improve the scalar nature of the code and improve the scalar memory accesses. We had

to modify the scalar code to facilitate the compiler optimization.
As a result we see considerable performance boost in the SC Opt
version of the code. There are limitations to the extent the compiler can optimize the code for it cannot see the true data depen-

Fig. 9. Performance boost with optimized frameworks.

Fig. 10. System load balancing among four SPEs.

A. Kumar, B. Li / Computer Vision and Image Understanding 114 (2010) 1139–1151

dencies in the code and the memory access patterns. Our SIMD and
algorithmic noise tolerant optimization techniques used in FL Opt
massively reduced the computations due to integer operations and
reduction in the number of multiplications.

5.3. System proﬁling
We now present simulation results for the ROI system which involves three codebases. The comparison is made for our scalar
data-partitioned un-optimized code with an optimized version of
the scalar code using compiler optimization (Compiler Opt) and
ﬁne level hand optimized along with compiler optimization (Fine
Level Opt). Fig. 9 present module-wise performance boost achieved
by the Compiler Opt and Fine Level Opt versions of system with re-

1147

spect to the un-optimized version. We see performance boost in
the optimized frameworks in all the modules.
We next proﬁled system performance for Fine Level Opt version
of the code for different SPE scaling factors. The results showed expected scaling behavior when we scaled up to four SPEs. We can
also see from Fig. 10, our data partitioning scheme has efﬁciently
load-balanced the work among the SPEs. However we saw the scaling boost was not the expected case when we scaled to eight SPEs
as seen in Fig. 11. A discussion on this is presented next.
5.3.1. Scaling with eight SPE
We found the performance boost, when we scaled up to eight
SPEs, offered lesser gain when compared to four SPEs. When we
analyzed the cause for this behavior, we found the memory transfer overheads in case of eight SPE cores has signiﬁcantly increased

Fig. 11. System performance boost with SPE scaling (with no modiﬁcations to control ﬂow for eight SPEs).

Fig. 12. System throughput gains with SPE scaling (with DMA stagger in control ﬂow for eight SPEs).

1148

A. Kumar, B. Li / Computer Vision and Image Understanding 114 (2010) 1139–1151

Fig. 13. System throughput gains with SPE scaling.

Fig. 14. Performance boost with optimized frameworks on PS3.

Fig. 15. System performance boost with SPE scaling on PS3.

A. Kumar, B. Li / Computer Vision and Image Understanding 114 (2010) 1139–1151

as seen from Fig. 11. The reason for this is that EIB has four rings for
communication and are efﬁciently utilized by our communication
model when we scale up to four SPEs [1]. However when we scale
to eight SPEs, since each SPE issues an individual DMA transfer
from the main memory to its LS, there would be contention for
the EIB rings and conﬂicts occur which result in memory latencies
in DMA communications and hence starvation of SPEs, thereby
decreasing the effective throughput. Similar observations have
been reported and extensively analyzed by Jimenez-Gonzalez
et al. [11]. We would have to employ a streaming model for two
sets of four SPEs, thereby avoiding EIB bus conﬂicts and reducing
memory latencies in the case of eight SPEs to achieve the expected
performance scaling. As a simple design change, we staggered the
DMA transfers such that the odd and even SPEs share the bus in a
mutually exclusive manner. The performance of the system im-

1149

proved and we could gain scaling boost in the system performance
as seen in Fig. 12. It can be observed from Fig. 12 the scaling gains
as we scale the number of SPEs are not reaching their theoretical
expected values (2, 4 and 8 for two SPEs, four SPEs and eight
SPEs respectively) which we discuss in Section 5.6. A system
throughput scaling gain chart for a set of test sequences is shown
in Fig. 13.
5.4. Proﬁling on PS3
In this subsection we present the proﬁling results of the ROI
detection system on a real Sony PS3 platform. The PS3 platform
has a CELL BE with a dual threaded PPE core running at 3.2 GHz
and up to six cores for user programs each running at 3.2 GHz
(the remaining two SPEs are unavailable for application programs).

Fig. 16. Left column shows detection results without GME, middle column detection results with GME and right column shows the feature points and the inliers matched
during GME.

1150

A. Kumar, B. Li / Computer Vision and Image Understanding 114 (2010) 1139–1151

We ported our ROI detection system used with SDK 3.0 on to the
PS3 and proﬁled the system for performance numbers (the Stefan
sequence was used for subsequent illustrations).
From the performance boosts illustration in Figs. 14 and 15 we
observe that the proﬁling results on PS3 show similar trend as in
the SDK. However, the achievable performance scaling will be lesser due to the memory transfer overheads on the PS3. We suspect
that the SDK cannot model the DMA transfers accurately and hence
we might get a slightly skewed performance boosts. Nevertheless
we still see the throughput rates are comparable in both cases
which must encourage developers to spend good amount of efforts
in designing and optimizing system on SDK ﬁrst.
5.5. Real-time performance evaluation of the ROI detection system
We have conducted exhaustive simulations on CELL SDK and
PS3 using a wide range of diverse test sequences in different resolutions and environments. We used a set of standard test video sequence – Coastguard (QCIF) and Stefan (CIF), and six custom
sequences (in QCIF and CIF) from outdoor and indoor environments captured by a handy-cam, all exhibiting variety of camera
motion patterns such as panning and rotation. Some sample detection results are shown in Fig. 16. We see the proposed ROI framework is capable of handling the camera motion whereas the
detection will fail without global motion compensation. The ﬁgure
also includes the features (corner points marked in green) used for
GME and the inliers (marked in red) from the background modeled
by RANSAC during GME. The system throughput expressed as target frame rate achieved for test sequences in SDK and PS3 are presented in Tables 3 and 4 respectively. For the simulations we have
used the same threshold for corner detection (the number of corner points are varying based on the video content), the same number of RANSAC iterations (30) and the same block sizes for DMA
transfers (size varied for QCIF and CIF sequences). The variation
in the target frame rates at a particular resolution video (QCIF or
CIF) are mainly a function of feature points and the computation
times in feature correspondence and RANSAC (feature set size),
and the associated memory transfers vary based on these.

Table 4
Target frame rate for test sequences on PS3.
Sequence

1 SPE

2 SPE

4 SPE

Stefan
Surveillance
Coastguard
Ball
Car
Indoor

29.1
29.7
96.94
103.77
105.82
96.52

53.5
53.8
185.26
197.13
197.13
184.77

104.15
104.9
330.9
286.2
345.71
334.34

 Shared bus (EIB) limits the scaling gains in case of scaling to
eight SPEs, since we are now slowing the system down by
allowing only four SPEs to communicate at a time.
 In CELL there are certain restrictions on the data width for arithmetic operations such as there are no 32 bit integer multipliers
and this limits vectorization of certain algorithmic computations (e.g.: Corner response calculation in harris corner need
32 bit integer multiplications which takes ﬁve CLK cycles as
multipliers are 16 bit).
5.7. Other system enhancements
We have implemented a basic change detection scheme for the
ROI detection. From Fig. 16, we see that there is still scope for
improvement. For postprocessing, we implemented a simple morphological dilation operation, which can be replaced by a more
complex postprocessing step involving region merging while
rejecting noisy blobs (blobs with small area) to make ROI detection
more accurate. We believe these additional processing steps can be
done in real-time by following the design guidelines highlighted in
our work. In the absence of camera motion, there will be a lot of
system overhead in the ROI detection scheme presented and one
can add a pre-processing step to detect whether there is a need
for GME. Handling large motions is difﬁcult on a single scale
GME process and there is a potential need for multi-scale GME to
accurately handle large camera motions. One interesting question
to answer would be how to design such multi-scale algorithms
on multi-core platform.

5.6. Limits on scaling gains

6. Conclusions

We now discuss the limiting factors for performance speedup
with scaling the number of SPEs on CELL for ﬁne level optimized
ROI detection system. Our analysis indicates the following reasons
which might limit the scaling gains.

We have presented the implementation of a complete motionbased ROI detection system for mobile camera platforms on CELL,
including detailed analysis and comparison of the performance.
Through this study, we have highlighted the critical design aspects
to be focussed in-order to efﬁciently implement a vision system on
multi-core platforms. The simulation results are in favor of potential real-time implementation of such complex systems on CELL for
sequences of resolution up to four times the CIF resolution. The
scaling gains with the number of cores may saturate due to the scalar contents in the system, memory accesses and transfers, and
architectural limitations. From our study we conﬁrmed that local
algorithms (operated on a neighborhood – HCD, ﬁltering, change
detection) can be handled well on multi-cores using DLP to achieve
a good load-balanced system. Further, algorithms should be com-

 Memory transfers and synchronization will increase with SPE
scaling, which can be a limiting factor in achieving the theoretical gains.
 Scalar content in the system modules will not scale with the
number of SPEs. The scalar percentage of some major modules
such as Harris corner detection, feature correspondence and
RANSAC are high and this will limit the gains achievable due
to data partitioning as we increase the number of SPEs
(beyond 8).
Table 3
Target frame rate for test sequences on CELL SDK.
Sequence

Size

Sequence characteristics

Avg. feature points

1 SPE

2 SPE

4 SPE

Coastguard
Car
Ball
Indoor
Stefan
Trafﬁc composite
Surveillance

QCIF
QCIF
QCIF
QCIF
CIF
CIF
CIF

Steady camera pan with foreground object motion
Static and moving foreground with camera pan
Camera pan and rotate with moving foreground object
Camera pan with foreground human object with small motion in indoor environment
Fast camera and foreground motion along with background motion
Multiple moving objects (vehicles and humans) in trafﬁc with camera motion
Low camera motion from overhead surveillance camera

120
105
75
180
280
175
225

111.42
117.51
122.88
105.64
30.94
34.85
31.85

201.3
201.93
220.63
186.51
57.86
66.96
60.24

378.91
382.99
405.83
363.18
105.21
132.04
116.53

A. Kumar, B. Li / Computer Vision and Image Understanding 114 (2010) 1139–1151

puted using ‘‘particle” concept (list distribution in feature correspondence) to efﬁciently use the available cores. The implementation of a representative computer vision application in this paper
encourages us to probe for future research directions in developing
parallel vision algorithms considering the growing trends in multicore processor technology.

[13]

[14]

References

[15]

[1] T.W. Ainsworth, T.M. Pinkston, On characterizing performance of the cell
broadband engine element interconnect bus, in: International Symposium on
Networks-on-Chip, Princeton, NJ, May 2007, pp. 18–29.
[2] O. Bockenbach, H. Bartsch, S. Schuberth, Implementing real-time adaptive
ﬁltering for medical applications on the cell processor using a generic
multicore framework, in: Proceedings of the International Society for Optical
Engineering SPIE, February 2008, pp. 6812–6814.
[3] T.P. Chen, D. Budnikov, C.J. Hughes, Y.K. Chen, Computer vision on multi-core
processors: articulated body tracking, in: International Conference on
Multimedia and Expo, July 2005, pp. 1862–1865.
[4] S.Y. Chien, Y.W. Huang, B.Y. Hsieh, S.Y. Ma, L.G. Chen, Fast video segmentation
algorithm with shadow cancellation, global motion compensation, and
adaptive threshold techniques, IEEE Transactions on Multimedia 6 (5) (2004)
732–748.
[5] K. Daloukas, C.D. Antonopoulos, N. Bellas, Implementation of a wide-angle lens
distortion correction algorithm on the cell broadband engine, in: Proceedings
of the 23rd International Conference on Conference on Supercomputing, ACM
New York, NY, USA, 2009, pp. 4–13.
[6] D. Farin, P.H.N. de With, Evaluation of a feature-based global-motion
estimation system, in: Proceedings of the SPIE – The International Society for
Optical Engineering, Munich, Germany, July 2005, pp. 1–12.
[7] A.L. Fisher, P.T. Highnam, Real-time image processing on scan line array
processors, in: IEEE Computer Society Workshop on Computer Architecture for
Pattern Analysis and Image Database Management, June 1985, pp. 484–489.
[8] J. Goodenough, R.J. Meacham, J.D. Morris, N. Luke Seed, P.A. Ivey, A single chip
video signal processing architecture for image processing, coding, and
computer vision, IEEE Transactions on Circuits and Systems for Video
Technology 5 (5) (1995) 436–445.
[9] P. Harvey, R. Mandrekar, Y. Zhou, J. Zheng, J. Maloney, S. Cain, K. Kawasaki, G.
Lafontant, H. Noma, K. Imming, T. Plachy, D. Questad, Packaging the cell
broadband engine microprocessor for supercomputer applications, in:
Electronic Components and Technology Conference, May 2008, pp. 1368–1371.
[10] K. Hiwada, Real-time 3D face tracking with cell broadband engine, Toshiba
Leading Innovation 62 (6) (2007) 56–59.
[11] D. Jimenez-Gonzalez, X. Martorell, A. Ramirez, Performance analysis of cell
broadband engine for high memory bandwidth applications, in: IEEE
International Symposium on Performance Analysis of Systems and Software
(ISPASS), April 2007, pp. 210–219.
[12] N. Kato, K. Takeuchi, S. Maeda, M. Shimbayashi, R. Sakai, H. Nozue, J. Amemiya,
Digital media applications on a CELL software platform, in: International

[16]

[17]

[18]
[19]

[20]

[21]

[22]

[23]
[24]

[25]

[26]

[27]

[28]

1151

Conference on Consumer Electronics, Las Vegas, NV, January 2006, pp. 347–
348.
Y. Lin, H. Lee, M. Woh, Y. Harel, S. Mahlke, C. Chakrabarti, K. Flautner, T. Mudge,
SODA: a high-performance DSP architecture for software-deﬁned radio, IEEE
Transactions on Circuits and Systems for Video Technology 27 (1) (2007) 114–
123.
R.M. Lougheed, D.L. McCubbrey, Multi-processor architectures for machine
vision and image analysis, in: International Conference on Parallel Processing,
July 1985, pp. 493–497.
C.L. Lu, L.C. Fu, Hardware architecture to realize multi-layer image processing
in real-time, in: IEEE Industrial Electronics Society (IECON), Taipei, Taiwan,
November 2007, pp. 2478–2483.
W. Miao, Q. Lin, W. Zhang, N.J. Wu, A programmable SIMD vision chip for realtime vision applications, IEEE Journal of Solid-State Circuits 43 (6) (2008)
1470–1479.
C. Micheloni, G.L. Foresti, F. Alberti, A new feature clustering method for object
detection with an active camera, in: International Conference on Image
Processing ICIP, Orlando, FL, October 2004, pp. 2587–2590.
A. Mittal, S. Banerjee, A. Valilaya, M. Balakrishnan, Real time vision system for
collision detection, Computer Science and Informatics 25 (1) (1995) 13–29.
M.W. Riley, J.D. Warnock, D.F. Wendel, Cell broadband engine processor:
design and implementation, IBM Journal of Research and Development 51 (5)
(2007) 545–557.
V. Sachdeva, M. Kistler, E. Speight, T.H.K. Tzeng, Exploring the viability of the
cell broadband engine for bioinformatics applications, in: IEEE International
Parallel and Distributed Processing Symposium, March 2007, pp. 1–8.
Z. Szlavik, L. Kovacs, C. Benedek, L. Havasi, I. Petras, A. Licsar, A. Utasi, L. Czuni,
T. Sziranyi, Behavior and event detection for annotation and surveillance, in:
International Workshop on Content-based Multimedia Indexing, London, UK,
June 2008, pp. 117–124.
T. Ying-Li, M. Lu, A. Hampapur, Robust and efﬁcient foreground analysis for
real-time video surveillance, in: IEEE Computer Society Conference on
Computer Vision and Pattern Recognition, San Diego, CA, June 2005, pp.
1182–1187.
X. Wang, M. Sugisaka, W. Xu, Feature point-based object tracking, Transactions
of the Institute of Electrical Engineers of Japan 124 (8) (2004) 1585–1591.
L.Q. Xu, B. Lei, Real-time outdoor video surveillance with robust foreground
extraction and object tracking via multi-state transition management, Pattern
Recognition Letters 27 (15) (2006) 1816–1825.
Y. Wang, D. Zhang, T. Tretter, P. Wu, Real time motion analysis toward
semantic understanding of video content, in: International Society for Optical
Engineering (SPIE), Beijing, China, July 2005, pp. 1–14.
Y. Zhanfeng, P.L. Narasimha, P. Topiwala, A robust real-time object detection
and tracking system, in: Proceedings of the International Society for Optical
Engineering SPIE, Orlando, FL, April 2008, pp. 108–109.
R. Zaglewski, M. Wojcikowski, Multi-core processor system for real-time
image processing in embedded computer vision applications, in: International
Conference on Information Technology, May 2008, pp. 1–4.
K. ZuWhan, Real time object tracking based on dynamic feature grouping with
background subtraction, in: IEEE Conference on Computer Vision and Pattern
Recognition, Anchorage, AK, June 2008, pp. 1–8.

arXiv:1503.06917v1 [cs.CV] 24 Mar 2015

Unsupervised Video Analysis Based on a
Spatiotemporal Saliency Detector
Qiang Zhang, Yilin Wang, and Baoxin Li,
Department of Computer Science, Arizona State University, Tempe AZ
qzhang53,ywang370,baoxin.li@asu.edu
March 25, 2015
Abstract
Visual saliency, which predicts regions in the field of view that draw
the most visual attention, has attracted a lot of interest from researchers.
It has already been used in several vision tasks, e.g., image classification,
object detection, foreground segmentation. Recently, the spectrum analysis based visual saliency approach has attracted a lot of interest due to its
simplicity and good performance, where the phase information of the image is used to construct the saliency map. In this paper, we propose a new
approach for detecting spatiotemporal visual saliency based on the phase
spectrum of the videos, which is easy to implement and computationally
efficient. With the proposed algorithm, we also study how the spatiotemporal saliency can be used in two important vision task, abnormality detection and spatiotemporal interest point detection. The proposed algorithm is evaluated on several commonly used datasets with comparison
to the state-of-art methods from the literature. The experiments demonstrate the effectiveness of the proposed approach to spatiotemporal visual
saliency detection and its application to the above vision tasks.

1

Introduction

In the recent years modeling and detection of visual saliency has attracted a
lot of interest in the vision community. One early work that is widely known is
the approach by Itti et al. [1]. Since then, a lot of different models have been
proposed for computing visual saliency. Such models may be roughly divided
into two groups: bottom-up models (or stimulus driven) that are mainly based
on low-level visual features of the scene, and top-down model (goal-driven) that
employs information and knowledge about a visual task. A survey of both groups
of methods was reported in [2]. Visual saliency analysis has been applied with
success to other vision tasks including object detection [3], image classification
[4] and foreground segmentation [5].
Recently, spectral-based approach has gained increased interest due to its
simplicity and good performance. In [6], the spectrum residual together with the
1

phase information was used to construct a saliency map. In [7] it was found that
it is the phase information rather than the spectrum leads to a better saliency
map. However, there was a lack of theoretic justification for such methods until
[8], where it was shown that, if the background is sparsely supported in the
DCT domain and the foreground is sparsely supported in the spatial domain
the foreground will receive high value on the computed saliency map.
In the real world, the visual field-of-view of a human may constantly change,
and thus visual saliency often depend on not only a static scene but also the
changes in the scene. To this end, spatiotemporal saliency has been proposed,
which tries to capture regions attracting visual attention in the spatiotemporal
domain. Spatiotemporal saliency has been applied to vision tasks such as video
summarization [9], human-computer interaction [10], video compression [11],
and abnormality detection [12].
In this paper we propose a novel spatiotemporal visual saliency detector for
video analysis, based on the phase information of the video. With the saliency
map computed using the proposed method, we analysis how it can be used for
two fundamental vision tasks, namely abnormality detection and spatiotemporal
interest point detection. We evaluate the performance of the proposed algorithm
using several widely used datasets, with the comparison to the state-of-art in
the literature.
The proposed method, compared with the existing work on spatiotemporal
saliency in the literature, has several advantages. First, it computes the saliency
information from the entire video span, which is different from many existing
approaches in the literature. For example, [7] computes temporal information
by only the differences of two adjacent frames, which is insufficient for modeling
complex activities, as shown in the experiment part. Second, the proposed approach is easy to implement and computationally efficient. The core parts of the
algorithm involve only a three-dimensional Fourier transform, whose complexity is only O(N log N ), where N is the size of the input. Last but not least, no
training stage or prior information is needed for the proposed approach, which
is a significant advantage for applications like abnormality detection.
The rest of the paper is organized as follows: in Sec. 2 we describe the
proposed method including the analysis and the relationships with the existing methods; Sec. 3 evaluate the proposed spatiotemporal saliency detector in
saliency detection on both synthetic dataset and two real video dataset; studies
of how the spatiotemporal saliency computed by the proposed method can be
used for two important vision tasks, abnormality detection and spatiotemporal
interst point detection, is presented in Sec. 4; and the paper is concluded in Sec.
5.

2

Proposed Method

As reviewed above, spectrum analysis based approaches to visual saliency has
seen some success, although the existing work has been primarily on predicting
salient objects for a given (static) image. For a dynamic scene, temporal in-

2

formation should be taken into consideration for properly predicting the salient
objects. For example, it was found in [13] that objects attract more visual attention if they move differently than their neighbors. Considering this, we propose
to compute the saliency map of dynamic scenes by utilizing the phase information of the temporal domain together with the phase information of the spatial
domain. In the proposed method, we compute the saliency map for 3D data
X ∈ RM×N ×T as:



 −1 Y 2


Z = F
(1)
|Y| 

where Y = F (X), F is 3D discrete Fourier transform and F −1 is the corresponding inverse transform. After we get the saliency map, we smooth it with
a 3D Gaussian smooth filter. The 3D Fourier transform can be computed as:

=

Y(u, v, w)
XXX

=

X

=

X

t

i

X(i, j, t)e−i2π(

+ wt
T

)

j

e

−i2π wt
T

XX

e

−i2π wt
T

X

t

t

(2)
vj
ui
M+N

i

X(i, j, t)e−i2π( M + N )
ui

vj

j

e

ui
−i2π M

i

X

ui

X(i, j, t)e−i2π M

j

i.e., the 3D Fourier transform can be computed as a sequence of 1D Fourier
transforms on each coordinate.
The proposed method detects spatiotemporal saliency, which has been also
discussed in some existing works. For example, in [7], the detection was done by
combining color information of one frame and the differences of this frame to
the previous one with quaternion Fourier transform. As a result, the temporal
information is limited to two adjacent frames and is insufficient for modeling
complex scenes. On the other hand, the spatiotemporal saliency proposed in
this paper considers the temporal information over a long temporal span up to
the entire video.
The method in Eqn. 1 evaluates the saliency of a region by exploring the
information of the entire video. In some situations, we may also be interested
in detecting a region that is salient within a temporal window of the video.
For example, if a video contains multiple scenes, each capturing a different
activity, we may be more interested in analysis the saliency within each scene
instead of the entire video. For this reason, we propose multiscale analysis for
spatiotemporal saliency, which is inspired by short-time Fourier transform. We
first apply the window function to the input signal X · w(i, j, t), where · is
the element-wise multiplication and w(i, j, t) the window function centered at
position (i, j, t), which is nonzero for only a small support (i.e., the size of window
function). The saliency map is computed for the windowed signal:
Y =
Z(i, j, t) =

F [X · w(i, j, t)]


Y
−1
F
|Y|
3

(3)

By sliding the window function on the input video, we still obtain the saliency
map for the entire video. The size of the sliding window determines the temporal
resolution: with a larger window, more global information of the input is revealed
but the resolution is lower; with a smaller window, resolution is improved. The
window function can be applied in either temporal direction, spatial direction
or both. As a result, we can perform saliency detection from varying scales,
which enables us to reveal the information at different spatiotemporal resolution,
similar to short time Fourier transform.
Combining different visual cues is important for not only scene saliency but
also spatiotemporal saliency. In this paper, we proposed to compute the saliency
map for each cue independently then compute the summation of saliency maps
from all visual cues. In [11], quaternion Fourier transform (QFT) is utilized to
combine the three-channel color information and frame differences. However,
the QFT could be very expensive (e.g., time consuming) when applied in spatiotemporal domain. In fact we find that (Appendix A): given a data with four
feature channels, the saliency map computed with QFT is very similar with the
sum of saliency maps computed with FFT over each feature channel.
Finally, we summarize the proposed algorithm below:
Algorithm
Input: data X, Gaussian filter g, window function w
Output: saliency map Z
For each window location
For each feature channel
Apply w to the input X;
Compute Fourier coefficient Y = F [X];
Y
;
Extract the phase information Ŷ = |Y|
2



Do the inverse transform Z = F −1 [Ŷ] ;
Smooth saliency map Z = Z ∗ g;
End
Combine the Z of all channels together;
End
where W is the window function. Currently, we only apply the window function
along temporal direction and rectangle window is used. The size of the window is
depending on the data. By incorporating the phase information of the temporal
domain, the proposed method can not only suppress the background, as achieved
by visual saliency for images, but also suppress the object which is static or
moving “regularly”.

2.1

Analysis

There has been several explanations for why spectral domain based approach is
able to detect saliency region from the image. For example, [14] explained by its
biological plausibility that saliency map exists in the primary visual cortex (V1),
4

which is orientation selective and lateral surround inhibition [15]. The spectral
magnitude measures the total response of cells tuned to the specific frequency
and orientation. According to lateral surround inhibition, similarly tuned cells
will be suppressed depending on their total response, which can be modeled
by dividing its spectral by the spectral magnitude [16]. [8] provided another
explanation from sparse representation, which states that, if the foreground is
sparse in spatial domain and background is sparse in DCT domain (e.g., periodic
textures), the spectral domain based approach will highlight the foreground
region in the saliency map.
Motion, like color and texture, is also perceptually salient. [17] studied how
three properties of motion, namely flicker, direction and velocity, contribute to
this saliency. By setting the target object having different flicker rate, moving
direction or motion velocity from the other objects, the target object can be
easily identified by human subjects, i.e., being salient. In spectral, the target
object and other objects can be mapped to two different bands (frequency and
orientation), where the band corresponding to the target object has a much
lower response than the band for the other object. Thus if we set the magnitude
of the spectral to one, as the proposed method dose, the band for the other
objects will be suppressed more than the target object, which makes the target
object “poped out” in the output. In Sec. 3.1, we will verify this analysis with
experiments on synthetic data.

2.2

Relationship to Existing Works

Our method is related to some existing works and based on the way in which
they computed the temporal information, we can roughly divide them into two
categories:
1. Methods of the first category represent the temporal information by the
motion, e.g., by frame differences [9] or by more dedicated motion estimation method including homography of adjacent frames [18] and phase
correlation [14]. However, methods of this temporal information typically
have limited temporal span, e.g., two adjacent frames ([19] tried to compute the frame differences of frames at a predefined sets of temporal spans),
thus they are not sufficient for modeling the complex motion patterns.
2. In this category, the saliency of a spatiotemporal cuboid (refer as cuboid
later) is measured by the “differences” of this cuboid to other cuboids of
the video or the template in the dictionaries, which may require high computational cost and/or require additional training data. The “differences”
of cuboids can be measured by distances [20], relative entropy [21] mutual
information [22] and coding length increments [23].
The proposed method is different from these methods. First, it does not rely
on prior knowledge. Instead, it explores within the input video to detect the
potential “outliers”. Second, the “outliers” are found by exploring the whole
temporal span. This makes the proposed algorithm be able to detect salient
5

patterns from complex dynamic background. In addition, the propose method
has low computational costs and is easy to implement. Fourier transform for
multiple dimensional data can be computed as a sequence of 1D Fourier transform on each coordinate of the data, thus the computational cost of 3D Fourier
transform for data X ∈ RM×N ×T is O(M N T log(M N T )). Thus the total computation cost for the proposed algorithm is O(KM N T log(M N T )), where K is
the number of feature channels.

3

Experiment

In this section, we evaluate the proposed method in saliency detection on both
syntheic data (Sec. 3.1) and on two real image datasets (Sec. 3.2), CRCNS-ORIG
and DIEM. The performance of the proposed methods are compared with the
existing methods, some of which are state-of-art.

3.1

Simulation Experiment

In this section, we evaluate the proposed method on synthetic data. In [17], how
three properties of motion, namely flicker, direction and velocity, contribute to
the saliency was studied. In this section, we generate the synthetic data according to the their protocol. The input data is a short clip where the resolution
is 174 × 174 with 400 frames at the frame rate of 60 frames per second. We
put 36 objects of size 5 × 13 in a 6 × 6 grid and a target object is randomly
selected out of those 36 objects. All the objects are allowed to move within a
29 × 29 region centered at their initial position (and warped back, if they move
out of this region). The video is black-and-white. We design the following three
experiments:
1. Flicker: we set the objects on-off at a specified rate and the target object
at a different rate from the other 35 objects;
2. Direction: we set the objects moving in a specified direction and the
target object in a different direction. The velocity of all the objects are
the same;
3. Velocity: we set the objects moving in a specified velocity and the target
object moves in a different velocity. The moving direction of the all the
objects are the same.
All the other parameters are the same as used in [17]. According to [17], the
target object could be easily identified by human subjects, when its motion
property (e.g., flicker rate, moving direction, velocity) is different from the other
objects. We also include some “blind” trials, where the target object has the
same motion property as the other 35 objects. In this case, the target object
can’t be identified by the human subjects, i.e., there is no salient region.
We apply the proposed method to the input data. For comparison, we also
evaluate the method proposed in [14] and [8]. We use the area under receiver
6

operating characteristic curve as the performance metric. The ground truth
mask is generated according to the location of the target object. The experiment
result is shown in 1.
Direction

Flicker
0.8

Velocity

0.7

0.7
0.65

0.75
0.6
0.7

0.6

Proposed
Bian[15]
Hou[8]

0.65
0.6
0.55

0.5

0.55

0.4

0.45

0.5

Proposed
Bian[15]
Hou[8]

0.5
0.3
0.45
0.4
100

150

200
250
300
350
400
450
Absolute differences of the flicker rate

0.2
500 −10

0

10
20
30
40
Absolute differences of the direction

50

Proposed
Bian[15]
Hou[8]

0.4
0.35
60 −0.1

0

0.1
0.2
0.3
0.4
Absolute differences of the velocity

Figure 1: The AUC on the synthetic data for the proposed method and two
existing methods. For “Direction” and “Velocity”, we also include some “blind”
trials (X-axis has value 0), where the target object has exactly the same motion
property as the other 35 objects. In those trials, the target object can’t be
identified by human subjects, i.e., there is no salient object [17].

Blind

Flicker

Direction

Velocity

Figure 2: Some visual sample of the synthetic data for different experiments.
From the experiment results, we can find that the proposed method detects
the salient region much more accurately than [14] and [8] in all except the “blind”
trials. For the “blind” trials, the AUC for the proposed method significantly
reduces, which shows that the proposed method is also robust. However, [14] and
[8] don’t survive in those “blind” trials. Surprisingly, [14] and [8] achieves quite
similar performances, though [14] was supposed to achieve better result as it
include the differences of two adjacent frames as motion (temporal) information.

7

0.5

0.6

3.2

Spatiotemporal Saliency Detection

In previous section, we test the proposed spatiotemporal saliency detector on
synthetic videos, with the comparison to two other saliency detectors, where
the proposed detector shows better performances in capturing the temporal
information. In this section, we evaluate the proposed spatiotemporal saliency
detector on two challenging video datasets for saliency evaluation, CRCNSORIG [24] and DIEM [25]. For this experiment, we first convert each frame into
the LAB color space, then compute the spatiotemporal saliency in each channel
independently and the final spatiotemporal saliency is the summation of the
saliency maps of all three channels.
CRCNS-ORIG includes 50 video clips from different genres, including TV
programs, outdoor scenes and video games. Each clip is 6-second to 90-second
long at 30 frames per second. The eye fixation data is captured from eight subjects with normal or correct-normal vision. In our experiment, we downsample
the video from 640 × 480 to 160 × 120 and keep the frame rate untouched, then
apply the our spatiotemporal saliency detector. To measure the performance, we
compute the area under curve (AUC) and F-measure (harmonic mean of true
positive rate and false positive rate). The experiment result is shown in Fig. 3,
where the area under curve (AUC) is 0.6639 and F-measure is 0.1926. Tab. 1
compares the result of the proposed method with some state-of-art methods on
CRCNS-ORIG, which indicates that our method outperforms them by at least
0.06 regarding AUC. The per-video AUC score is shown in Fig. 9 in Appendix
A.1.
1

True Postive Rate

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2

CRCNS AUC=0.6729
DIEM AUC=0.6715

0.1
0

0

0.2

0.4

0.6

0.8

1

False Postive Rate

Figure 3: The receiver operating characteristic curve of the propose method in
CRCNS-ORIG dataset and DIEM dataset. The area under the curve is 0.6639
and 0.6896 accordingly.
DIEM dataset collects data of where people look during dynamic scene viewing such as film trailers, music videos, or advertisements. It currently consists
8

Method
AWS [26]
HouNIPS [27]
Bian [14]
IO
SR [6]
Torralba [31]
Judd [29]
Marat [28]
Rarity-G [33]
CIOFM [34]
Proposed

AUC
0.6000
0.5967
0.5950
0.5950
0.5867
0.5833
0.5833
0.5833
0.5767
0.5767
0.6639

Method
AWS [26]
Bian [14]
Marat [28]
Judd [29]
AIM [30]
HouNIPS [27]
Torralba [31]
GBVS [32]
SR [6]
CIO [34]
Proposed

AUC
0.5770
0.5730
0.5730
0.5700
0.5680
0.5630
0.5840
0.5620
0.5610
0.5560
0.6896

Table 1: The result the proposed method compared with the results of the
top ten existing methods on CRCNS dataset (left) and DIEM dataset (right)
according to [35]. From this table, we can find that the propose method gets
obvious better performances than the state-of-arts on both two datasets.
of data from over 250 participants watching 85 different videos. Each video in
DIEM dataset includes 1000 to 6000 frames at 30 frames per second. Similarly
as CRCNS, we downsample the video to 1/4 (e.g., from 1280 × 720 to 320 × 180)
while maintaining the aspect ratio and frame rate. We observe that each video
in DIEM dataset is consisted of a sequence of short clips, where each clip has 30
to 100 frames. To properly detect the saliency from those videos, we apply the
window function to our spatiotemporal saliency detector, where the size of the
window (along temporal direction) is 60-frame. The experiment result is shown
in Fig. 3 and Tab. 1, where the AUC is 0.6896 and F-measure is 0.35. From the
table, we can find that the proposed method outperforms the state-of-arts by
over 10%. The per-video AUC score is shown in Fig. 10 in Appendix A.1.

4

Application of Spatiotemporal Saliency

In the previous section, we show that the proposed method is able to detect the
saliency region in the video. The saliency detection for image has been used more
and more in other visual tasks, e.g., image segmentation, object recognition.
A natural question arises that can we also appliy the spatiotemporal saliency
detection for some important vision tasks. In this section, we show how can
we applied the spatiotemporal saliency computed by the proposed methods to
two important vision tasks, i.e., abnormality detection (4.1) and spatiotemporal
interest pointer detection (4.2).

4.1

Abnormality Detection

According to our previous analysis, the salient region should be different from
the neighbor, both spatially and temporally. This spatiotemporal saliency shares
9

Method
Optical flow [37]
Social force [37]
Chaotic invariants [40]
NN [38]
Sparse reconstruction [38]
Interaction force [41]
Proposed

AUC
0.84
0.96
0.99
0.93
0.978
0.9961
0.9378

Table 2: The result on UMN dataset. Note, we have cropped out the region which
contains the text “abnormal”, and results in frame resolution 214 × 320. Please
note that, most of those methods, except the proposed one, need a training
stage.
a lot of common to the concept of abnormality in video. Thus in this section,
we show how can we utilize the proposed spatiotemporal saliency detector to
detect abnormality from the video.
For abnormality detection, we start with computing the saliency map for
the input video as described above. The regions containing abnormalities can
be detected by founding the region where the saliency value is above a threshold.
Then the saliency score of a frame is computed as the average of saliency value
of the pixels in that frame, i.e.,
s(t) =

1 XX
X(i, j, t)
NM i j

(4)

where s(t) is the saliency score of tth frame, N × M is the size of one frame, i,
j, t are row, column and frame index of the 3D saliency map accordingly. The
frame with high saliency score would contain abnormality.
We evaluate the proposed method for abnormality detection in videos from
two datasets: UMN abnormal dataset1 and UCSD dataset [36]. Abnormal detection has attracted a lot efforts from the researchers. However, most of the
existing works require training stage, e.g., social force [37], sparse reconstruction [38], MPPCA [39], i.e., they need training data to initialize the model. The
proposed method, instead, dose NOT need any training stage or training data.
The result on UMN abnormal dataset is shown in Tab. 2, where we compute
the frame-level true positive rate and false positive rate then compute the area
under the ROC (Fig. 4). Fig. 5 shows the result for videos of three scenes, where
we plot saliency value of each frame and show some sample frames. The result on
UCSD dataset is shown in Tab. 3, where we report frame-level equal-error rate
(EER) [36]. Fig. 6 shows the ROC for UCSD dataset with the proposed method;
Fig. 7 shows eight samples frames, where red color highlights abnormal regions.
We can find that, without training data, the proposed method still outperforms
several state-of-arts in the literature, e.g., social force, MPPCA.
1 http://mha.cs.umn.edu/Movies/Crowd-Activity-All.avi

10

Area under cuver: 0.937785
True Positive Rate

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0

0.2

0.4

0.6

0.8

1

False Positive Rate
Figure 4: The ROC for the UMN dataset computed with the propose method.
Method
Social force [37]
MPPCA [39]
MDT [36]
Adam [42]
Reddy [43]
Sparse [38]
Proposed

Ped1
31%
40%
25%
38%
22.5%
19%
27%

Ped2
42%
30%
25%
42%
20%
N.A.
19%

Overall
37%
35%
25%
40%
21.25%
N.A.
23%

Table 3: The frame level EER (the lower the better) for UCSD dataset. Please
note that, most of those methods, except the proposed one, need a training
stage. From the result, we can found that the proposed method, even without
traing stage or training data, can still outperform social force, MPPCA.

4.2

Spatiotemporal Saliency Point Detector

The regions which attracts human’s attention most would contribute most to
people’s perception of the scene. The saliency map computed with the proposed
method will hightlight those regions. Thus we propose to sample the interest
points based on the saliency map of the data, which we refer as spatiotemporal
saliency point detector (STSP).
To detect interest point, we also starts with computing the saliency map Z
for the input data X. Then we apply non-maximum suppression on the saliency
map to sample the interest points: an interest point is selected at (x, y, t) if and
only if
Z(x, y, t)
Z(x, y, t)

≥
≥

ρ
Z(i, j, k) ∀(i, j, k) ∈ N (x, y, t)

11

(5)

Scene 1

Scene 2

Scene 3
Figure 5: Some sample results for the UMN datasets, where we pick one video
for each scene. The top is the saliency value (Y-axis) for each frame (X-axis)
and bottom are sample frames picked from different frames (as shown by the
arrow).

12

Area under curve
1

True Positive Rate

0.9

X: 0.1978
Y: 0.8177

0.8
X: 0.2649
Y: 0.7456

0.7

X: 0.2792
Y: 0.7219

0.6
0.5
0.4
0.3

all: 0.8062
ped1: 0.7805
ped2: 0.8772

0.2
0.1
0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

False Positive Rate

Figure 6: The ROC for the UCSD dataset computed with the propose method.

Peds1: Wheelchair

Peds1: Skater

Peds1: Bike

Peds1: Cart

Peds2: Skater

Peds2: Bike

Figure 7: Some sample results for the UCSD datasets, where the red color highlights the detected abnormal region, i.e., the saliency value of the pixel is higher
than four times of the mean saliency value of the video.
where ρ is a predefined threshold (e.g., 2µ) and N (x, y, t) is the set of positions
near (x, y, t).
Similar as [44], for each interest points (x, y, t), we extract a descriptor within

13

its neighbor area characterized as (x, y, t, σ, τ ), where (x, y, t) is the center, σ,
τ are the spatial and temporal scales (we use 18 × 18 × 10, 25 × 25 × 14 and
36 × 36 × 20 here) accordingly. The neighbor is further divided into multiple
subblocks (e.g., 3 × 3 × 2 along spatial and temporal direction accordingly); for
each subblock, we computed the 3D gradient g = [gx , gy , gt ]; then we quantize
the orientations of the gradients into a histogram of four bins; finally the histogram of each subblock is normalized to unit l1 norm and histograms of all
subblocks is concatenated into one histogram, i.e., the descriptor for interest
point (x, y, t).
Compared with existing spatiotemporal interst point detectors, which mostly
choose the location where the gradient is strong and stable cross different scales.
However, the gradient is a low level information and is insufficient to capture
the complex dynamics as the human vision does. Instead, the proposed method
explores the relationship of each location over all spatial and temporal span,
which is able to model complex dynamics in the video.
For evaluation, we use three datasets: Weizmann dataset [45], KTH dataset
[46] and UCF sports dataset [47]. Since the method is proposed for detecting
interest points, we only compare it with several state-of-art spatiotemporal interest point detectors including Harris3D [44], Gabor [48], Hessian3D [49] and
dense sampling [50], where the result are summarized in [50]. The parameters
of the detectors are set as suggested by the paper accordingly.
Fig. 8 shows the saliency map for some sample frames of videos from UCF
sports action dataset and KTH dataset. From the figure, we can found that
the saliency map computed with the proposed method highlights the moving
region while suppressing the background. The proposed method is also robust
to moving background (Row 1), clutter background (Row 2) and scale variation
(Row 3). In addition, from Row 3 to 4, we can found the moving parts of
body, e.g., hands, get higher saliency value (red color) then other body parts.
The spatiotemporal saliency interest points will be mostly sampled from those
hightlighted regions and augment the description of the action of interest.
To quantitatively evaluate the performances of different detector, we use
the interest points detected by those detector to train a classifiers for activity
recognition. We use both histogram of gradient (HoG) and histogram of optical
flow (HoF) as the descriptor. Bag of words is used to represent the video, where
each input is represented as a histogram of words in the codebook (size of of
codebook is k = 2000); then classifier (support vector machine with χ2 kernel)
is applied to those histograms to classify the input. For Weizmann dataset and
UCF sports dataset, we use leave-one-out scheme for training and testing; for
KTH dataset, we follow the standard partition in [46].
Tab. 4 reports the performances of different detectors on three dataset, where
we test extracting feature on the original video and also extracting feature on
the saliency map of the original video (refer as “proposed*”). From the table we
find that, the proposed method (and “proposed*”) achieves the best result over
all three datasets. Especially “proposed*” achieved the best results for KTH
dataset and Weizmann data; “proposed” achieved the best results for UCF
sports action dataset. For video with simple background(e.g., KTH dataset and
14

Method
Harris3D
Gabor
Hessian3D
Dense
Proposed
Proposed*

Weizmann
85.6%
N.A.
N.A.
N.A.
84.5%
95.6%

KTH
91.8%
88.7%
88.7%
86.1%
88.0%
92.6%

UCF sports
78.1%
77.7%
79.3%
81.6%
86.7%
85.6%

Table 4: The performances of different detectors on three datasets. For “proposed*”, we extract the descriptor on the saliency map instead of on the video.
Weizmann dataset), extracting descriptor on saliency map instead of the video
itself could be a better choice.

Figure 8: Some samples frames (left) from UCF sports action dataset (Row 1,
2) and KTH dataset (Row 3, 4) with their saliency maps (right).

15

5

Conclusion and Discussion

In this paper, we proposed a novel approach for detecting spatiotemporal saliency,
which was simple to implement and computationally efficient. The proposed approach was inspired by recent development of spectrum analysis based visual
saliency approaches, where phase information was used for constructing the
saliency map of the image. Recognizing that the computed saliency map captured the region of human’s attention for dynamic scenes, we proposed two
algorithms utilizing this saliency map for two important vision tasks. These approaches were evaluated on several well-known datasets with comparisons to the
state-of-arts in the literature, where good results were demonstrated. For the
future work, we will focus on theoretical analysis of the proposed method and
the analysis on the selection of the window function.

A

Appendix

To compare the performances of combining four visual cues via QFT and performances via summation of saliency maps of each visual cues, we design the
following experiment. We run 1000 simulations and in each simulation we generate a r × c × 4 array, where r and c is a random number between [1, 1000] and
4 is the number of feature channels. We compute the saliency map with different methods then measures their similarities via cross-correlation, where 0.91 is
reported for QFT and FFT. After smoothing the saliency map with a Gaussian
kernel, the correlation is over 0.998. For natural image, we could expect an even
higher correlation.
This suggests that, we can compute the saliency map for each visual cue
independently and then add them together, which will yield quite similar result
by using quaternion Fourier transform. In addition, the proposed method other
than QFT provides more flexibility, e.g., we can assign different weights to the
visual cues as [29].

A.1

Supplementary Results

We also include the AUC of the proposed method for each video from the
CRCNS-ORIG (Fig. 9) and DIEM dataset (Fig. 10).

References
[1] L. Itti, C. Koch, and E. Niebur, “A model of saliency-based visual attention
for rapid scene analysis,” PAMI, vol. 20, no. 11, pp. 1254 –1259, nov 1998.
[2] A. Borji and L. Itti, “State-of-the-art in visual attention modeling,” PAMI,
vol. PP, no. 99, p. 1, 2012.
[3] B. Alexe, T. Deselaers, and V. Ferrari, “Measuring the objectness of image
windows,” PAMI, vol. 34, no. 11, pp. 2189 –2202, nov. 2012.
16

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

be
beverl
b ve y0
beeverrly031
beverly05
ga bevverlly06
g me er y0
gaamecubly087
g mecube0
gaamecubee042
g mecub 05
gaamecubee06
gamecub 13
gameccubee16
me ub 17
mocube18
m nice2
moonica033
samonnicaa04
stcacadica005
s nd et 6
sttaandaarde0st
st nd rd 1
staandaard002
st nd rd 3
staandaard004
tv− nd rd05
a a 6
tv−ctiordn07
tv−ads01
tv− tv−aads01
an tv− ds02
tvn−ounads003
tv−musce04
ne ic01
tv−
ws 1
tv−new
tv−news001
tv−news02
tv−news03
n s 4
tv−
tv− neews005
tv−spows06
sp rts 9
tv−
tv−spoorts001
tv−sports02
sp rt 3
tv−ortss04
tv−talk05
ta 01
tv−
tv−tallkk03
tal 04
k0
5

0

Figure 9: The AUC of the proposed method for each video from CRCNS-ORIG
dataset.
[4] G. Sharma, F. Jurie, and C. Schmid, “Discriminative spatial saliency for
image classification,” in CVPR 2010, june 2012, pp. 3506 –3513.
[5] Q. Li, Y. Zhou, and J. Yang, “Saliency based image segmentation,” in
Multimedia Technology (ICMT), 2011 International Conference on, july
2011, pp. 5068 –5071.
[6] X. Hou and L. Zhang, “Saliency detection: A spectral residual approach,”
in CVPR 2007, june 2007, pp. 1 –8.
[7] C. Guo, Q. Ma, and L. Zhang, “Spatio-temporal saliency detection using
phase spectrum of quaternion fourier transform,” in CVPR 2008, june 2008,
pp. 1 –8.

17

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

50

_p
An eopl
BBtarct e_br
C ic oo
B _w a_ kly
ad BC_ ildlif lands n_1
ve w e_ ca
r il e
a t_b dlif ag pe
am dver bc4_ e_sp le_9
i t
b e
am _ib4 _bra ees cial
i_i 010 via_ _10
arc s10 _c pa 2
tic 00 los int
ch _bea a_cl eup__
do illi_ rs_ ose 7
c
do ume plas 106 up_
t 6
c
do ume ntar ers_ x71
cu nt y_ 12
ga men ary_ cora 80
m ta d l_
ga e_tr ry_ olph re
ha me_ ailer plan ins
ho iry_ tra _gh et_e
me bi iler os
hu _mokers _wr tbu
mm vi _ca ath
e
_
mo ingb _Ch bbag l
v
mo ie_ irds arlie e
_
_
m vie_ trai na b
ne usic trail ler_arrato
w _ er li
ne s_be red_ _qu ce_
a
n ws e_ hot nt
ne ews _she para _chi u
ws _u rry sit li_
_w s_ _d es
nig imb elec rink _7
pin htlif ledo tion ing
_
e
pingpon _in_n_m deb
g
g
m
pin po _a ozacen
gp ng_ ngl am
e
o
pla ng_ long _sh bi
ne no_ _sh ot_
sc t_e bo ot
sp ottis arth dies_9
or h_ _
_
s t_b st jun 9
sp port arcearter gles
ort _fo lo s_
sp _p o na 12
or ok tba _e
sp t_sla er_1 ll_b xtr
sp ort_wm_d 280 est_
o
sp rt_w imb unk_ x640
ort im led 12
ste s_k ble on 80
wa en do _ba
r d n
l
tv_ t_leeo_12_ma t
tv_ ketc _12 80x gi
un h2 80 71
i_c _6 x71
ha 72 2
lle x5
ng 44
e_
fin

0

Figure 10: The AUC of the proposed method for each video from DIEM dataset.
[8] X. Hou, J. Harel, and C. Koch, “Image signature: Highlighting sparse
salient regions,” PAMI, vol. 34, no. 1, pp. 194–201, jan. 2012.
[9] Y.-F. Ma, X.-S. Hua, L. Lu, and H.-J. Zhang, “A generic framework of user
attention model and its application in video summarization,” Multimedia,
IEEE Transactions on, vol. 7, no. 5, pp. 907 – 919, oct. 2005.
[10] L. Itti, N. Dhavale, and F. Pighin, “Realistic avatar eye and head animation
using a neurobiological model of visual attention,” in Optical Science and
Technology, SPIE’s 48th Annual Meeting. International Society for Optics
and Photonics, 2004, pp. 64–78.
[11] C. Guo and L. Zhang, “A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression,” Image
Processing, IEEE Transactions on, vol. 19, no. 1, pp. 185 –198, jan. 2010.
[12] D. Gao, S. Han, and N. Vasconcelos, “Discriminant saliency, the detection
of suspicious coincidences, and applications to visual recognition,” PAMI,
vol. 31, no. 6, pp. 989 –1005, june 2009.

18

[13] B. Ölveczky, S. Baccus, and M. Meister, “Segregation of object and background motion in the retina,” Nature, vol. 423, no. 6938, pp. 401–408, 2003.
[14] P. Bian and L. Zhang, “Biological plausibility of spectral domain approach
for spatiotemporal visual saliency,” NIPS, pp. 251–258, 2009.
[15] E. P. Simoncelli and O. Schwartz, “Modeling surround suppression in v1
neurons with a statistically-derived normalization model,” 1999.
[16] L. Zhaoping and P. Dayan, “Pre-attentive visual selection,” Neural Networks, vol. 19, no. 9, pp. 1437–1439, 2006.
[17] D. E. Huber and C. G. Healey, “Visualizing data with motion,” in Visualization, 2005. VIS 05. IEEE. IEEE, 2005, pp. 527–534.
[18] Y. Zhai and M. Shah, “Visual attention detection in video sequences
using spatiotemporal cues,” in Proceedings of the 14th ACM Multimedia,
ser. MULTIMEDIA ’06. New York, NY, USA: ACM, 2006, pp. 815–824.
[Online]. Available: http://doi.acm.org/10.1145/1180639.1180824
[19] L. Zhang, M. Tong, and G. Cottrell, “Sunday: Saliency using natural statistics for dynamic analysis of scenes,” in Proceedings of the 31st Annual
Cognitive Science Conference. AAAI Press Cambridge, MA, 2009, pp.
2944–2949.
[20] H. J. Seo and P. Milanfar, “Static and space-time visual saliency detection
by self-resemblance,” Journal of Vision, vol. 9, no. 12, 2009. [Online].
Available: http://www.journalofvision.org/content/9/12/15.abstract
[21] Y. Li, Y. Zhou, J. Yan, Z. Niu, and J. Yang, “Visual saliency based on
conditional entropy,” ACCV 2009, pp. 246–257, 2010.
[22] V. Mahadevan and N. Vasconcelos, “Spatiotemporal saliency in dynamic
scenes,” PAMI, vol. 32, no. 1, pp. 171–177, 2010.
[23] S. Ban, I. Lee, and M. Lee, “Dynamic visual selective attention model,”
Neurocomputing, vol. 71, no. 4, pp. 853–856, 2008.
[24] R. Itti, Laurent; Carmi, “Eye-tracking data from human volunteers
watching complex video stimuli,” Online, 2009. [Online]. Available:
CRCNS.org
[25] P. K. Mital, T. J. Smith, R. L. Hill, and J. M. Henderson, “Clustering
of gaze during dynamic scene viewing is predicted by motion,” Cognitive
Computation, vol. 3, no. 1, pp. 5–24, 2011.
[26] A. Garcia-Diaz, X. R. Fdez-Vidal, X. M. Pardo, and R. Dosil, “Decorrelation and distinctiveness provide with human-like saliency,” in Advanced
Concepts for Intelligent Vision Systems. Springer, 2009, pp. 343–354.

19

[27] X. Hou and L. Zhang, “Dynamic visual attention: Searching for coding
length increments,” NIPS, vol. 21, pp. 681–688, 2008.
[28] S. Marat, T. Ho Phuoc, L. Granjon, N. Guyader, D. Pellerin, and A. GuérinDugué, “Modelling spatio-temporal saliency to predict gaze direction for
short videos,” IJCV, vol. 82, no. 3, pp. 231–243, 2009.
[29] T. Judd, K. Ehinger, F. Durand, and A. Torralba, “Learning to predict
where humans look,” in ICCV 2009, 29 2009-oct. 2 2009, pp. 2106 –2113.
[30] N. Bruce and J. Tsotsos, “Saliency based on information maximization,”
in Advances in neural information processing systems, 2005, pp. 155–162.
[31] A. Torralba, “Modeling global scene factors in attention,” JOSA A, vol. 20,
no. 7, pp. 1407–1418, 2003.
[32] J. Harel, C. Koch, and P. Perona, “Graph-based visual saliency,” in Advances in neural information processing systems, 2006, pp. 545–552.
[33] M. Mancas, “Computational attention: Modelisation and application to
audio and image processing,” Ph.D. dissertation, PhD. Thesis, University
of Mons, 2007.
[34] L. Itti and P. Baldi, “Bayesian surprise attracts human attention,” NIPS,
vol. 18, p. 547, 2006.
[35] A. Borji, D. N. Sihite, and L. Itti, “Quantitative analysis of human-model
agreement in visual saliency modeling: a comparative study,” 2012.
[36] V. Mahadevan, W. Li, V. Bhalodia, and N. Vasconcelos, “Anomaly detection in crowded scenes,” in CVPR 2010, june 2010, pp. 1975 –1981.
[37] R. Mehran, A. Oyama, and M. Shah, “Abnormal crowd behavior detection
using social force model,” in CVPR 2009, june 2009, pp. 935 –942.
[38] Y. Cong, J. Yuan, and J. Liu, “Sparse reconstruction cost for abnormal
event detection,” in CVPR 2011, june 2011, pp. 3449 –3456.
[39] J. Kim and K. Grauman, “Observe locally, infer globally: A space-time
mrf for detecting abnormal activities with incremental updates,” in CVPR
2009, june 2009, pp. 2921 –2928.
[40] S. Wu, B. Moore, and M. Shah, “Chaotic invariants of lagrangian particle
trajectories for anomaly detection in crowded scenes,” in CVPR 2010, june
2010, pp. 2054 –2060.
[41] R. Raghavendra, A. Del Bue, M. Cristani, and V. Murino, “Optimizing interaction force for global anomaly detection in crowded scenes,” in
Computer Vision Workshops (ICCV Workshops), 2011 IEEE International
Conference on, nov. 2011, pp. 136 –143.

20

[42] A. Adam, E. Rivlin, I. Shimshoni, and D. Reinitz, “Robust real-time
unusual event detection using multiple fixed-location monitors,” PAMI,
vol. 30, no. 3, pp. 555 –560, march 2008.
[43] V. Reddy, C. Sanderson, and B. Lovell, “Improved anomaly detection in
crowded scenes via cell-based analysis of foreground speed, size and texture,” in Computer Vision and Pattern Recognition Workshops (CVPRW),
2011 IEEE Computer Society Conference on, june 2011, pp. 55 –61.
[44] I. Laptev, “On space-time interest points,” IJCV, vol. 64, no. 2, pp. 107–
123, 2005.
[45] L. Gorelick, M. Blank, E. Shechtman, M. Irani, and R. Basri, “Actions as
space-time shapes,” PAMI, vol. 29, no. 12, pp. 2247–2253, December 2007.
[46] C. Schuldt, I. Laptev, and B. Caputo, “Recognizing human actions: a local
svm approach,” in ICPR 2004, vol. 3, aug. 2004, pp. 32 – 36 Vol.3.
[47] M. Rodriguez, J. Ahmed, and M. Shah, “Action mach a spatio-temporal
maximum average correlation height filter for action recognition,” in CVPR
2008, june 2008, pp. 1–8.
[48] P. Dollar, V. Rabaud, G. Cottrell, and S. Belongie, “Behavior recognition via sparse spatio-temporal features,” in Visual Surveillance and Performance Evaluation of Tracking and Surveillance, 2005. 2nd Joint IEEE
International Workshop on, oct. 2005, pp. 65 – 72.
[49] G. Willems, T. Tuytelaars, and L. Van Gool, “An efficient dense and scaleinvariant spatio-temporal interest point detector,” ECCV 2008, pp. 650–
663, 2008.
[50] A. Kläser, “Learning human actions in video,” Ph.D. dissertation, Université de Grenoble, jul 2010. [Online]. Available:
http://lear.inrialpes.fr/pubs/2010/Kla10

21

SCALE-ADAPTIVE EIGENEYE FOR FAST EYE DETECTION IN WILD WEB IMAGES
Xu Zhou

Yilin Wang

Peng Zhang

Baoxin Li

Department of Computer Science and Engineering, Arizona State University
Tempe, AZ 85287, USA
ABSTRACT
Detecting eyes in images is fundamental for many computer
vision applications including face detection, face recognition,
and human-computer interaction. Most existing methods are
designed and tested on datasets acquired under controlled lab
settings (e.g., ﬁxed scale, known poses, clean background,
etc.), leaving their performance to be further examined on
real-world, uncontrolled images, such as on-line images. This
paper presents an effort on developing a fast and accurate eye
detector for on-line images for which the acquisition condition is unknown and varies from one image to another, resulting in unpredictable background and variable scales for the
eyes/faces. The key idea is to develop a scale-adaptive EigenEye approach, which employs an approximate scale estimated
from face detection to modulate the pre-trained EigenEye basis in searching for the best match in a test image. The effort
also includes building a 2845-image dataset with accuratelyannotated eye locations and size, which will be made public to
the community for future comparative study. Evaluation using this dataset, with comparison with a few leading state-ofthe-art approaches, demonstrates the advantages of the proposed method.
Index Terms— Eye detection, object detection, feature
extraction, scale adaptive
1. INTRODUCTION
Eyes are among the most salient facial features in facial images. Accordingly, eye detection is often a fundamental module in many applications involving facial image analysis. For
example, many face recognition engines rely on eye detection (explicitly or implicitly) for pre-aligning the face images before any training/testing algorithm is applied. Some
applications (like creating tactile facial images [18]) require
high-precision eye detection. Although it has been intensively
studied over the past years, fast and accurate eye detection remains a challenging task especially if uncontrolled imaging
conditions are considered. Lacking an accurately-annotated
dataset of uncontrolled real-world images has prevented a direct comparison of various approaches proposed thus far, and
hence hindering our understanding of the real performance of
these algorithms. Our study in this paper attempts to bridge

,(((

these gaps.
We start with reviewing some most recent approaches in
the literature. Such recent studies typically report signiﬁcant
improvement over earlier approaches ([2],[12],[16],[15],[17]),
and thus our review will not go back to those earlier methods. A method based on independent components analysis
(ICA) was proposed in [7]. This approach was reported to
achieve a detection rate of 97.3% on 1500 images from the
FERET dataset. One main limitation of this approach is the
requirement of a ﬁxed scale of test images, for example the
assumption that all the images have been pre-aligned to a
ﬁxed size (as is the case in FERET). Another state-of-the-art
approach is the feature-versus-context approach in [4]. This
approach was reported to achieve an average detection rate
of 97% on the AR, XM2VT and ASL datasets. As will be
demonstrated later, one key drawback of this method is its
inaccuracy in detecting the correct size of the eyes. In addition, there are also some other recent efforts on eye detection.
For example, in [10], textural characteristics of eye regions
and non-negative matrix factorization (NMF) based image
reconstruction are considered for eye detection. In [6] and
[14], eye detection based on rank order ﬁlter is introduced.
All these approaches reported similar good performance on
either standard or proprietary datasets.
Despite the promising result of the above methods, they
were all based on standard face datasets with well-aligned
and normalized face images or proprietary data. In this
study, by manual annotation, we built a dataset of 2845
images taken from the Face Detection Dataset and Benchmark (FDDB) [8] from the Faces in the Wild project (viswww.cs.umass.edu/lfw/). We have manually labeled the eye
region and eye center location as the ground truth. Inspired
by the success of eigen analysis in face recognition (e.g. [13],
[11], [10]), we adopt an EigenEye approach. Furthermore,
recognizing the drastic scale variation in the wild images, we
propose a scale-adaptive scheme, in which EigenEye trained
from high-resolution eye images are updated according to an
estimated eye scale (from face detection) for eye detection.
These considerations result in a fast (due to mostly linear
eigen space projections and distance computation) and robust
(achieved by eigen analysis and scale adaptivity) eye detector, which outperforms other state-of-the-art techniques on
this challenging set of wild Web images. The dataset will be



,&,3

made public to support further fair comparative studies by
researchers working on this topic.
2. PROPOSED APPROACH
In this section, we introduce a scale-adaptive EigenEye
scheme for eye detection. The objective is to achieve robustness of detection in wild Web images through eigen
analysis and scale adaptivity. The main idea for achieving
scale adaptivity is to resize the eigen bases obtained in the
training stage, based on an estimated eye scale. We ﬁrst outline the general framework of the method in Section 2.1, and
then elaborate the scale-adaptive scheme in Section 2.2, with
the overall eye detection algorithm summarized in Section
2.3.
2.1. The General Framework
The proposed EigenEye approach consists of two stages:
training and detecting. In the training stage, a training set
of high-resolution eye images are collected. (In the current
study, 861 such images of size 120 × 160 are used, although
these can be updated as needed.) These images are used in
eigen analysis to extract the top eigen bases, i.e. EigenEyes.
In the current study, the ﬁrst 60 EigenEyes are kept. Every training eye image is then projected into this subspace
spanned by EigenEyes, i.e. EigenEye space. In the detecting
stage, we ﬁrst perform face detection to obtain the candidate
face region. Then we search for eyes throughout the face
region as the best match to the training eyes in the EigenEye
space. Obviously, for wild Web images, the size of the eye in
the test image can be very different from that of the training
set. Hence the matching needs to be done only after both
the test image and the EigenEye space have been normalized
in the same scale. To this end, we propose to modulate the
eigen space by adapting the EigenEyes based on the size of
the detected face region. Adapting the EigenEyes to a test
image instead of normalizing the test image with respect to an
EigenEyes is motivated by the fact that the eyes to be detected
will be in general at lower resolutions than training eye images or EigenEyes. Therefore, adapting the high-resolution
EigenEyes to a lower-resolution test image can better ensure the matching is done with images at the same imaging
resolution. We will elaborate the approach in the Section 2.2.
2.2. Scale-Adaptive EigenEye for Eye Detection
Scale-Adaptive Eigeneye (SAE)
Eigen analysis or principal component analysis (PCA) is a
mathematical procedure that projects a set of observations of
possibly correlated variables into a set of linearly uncorrelated variables called principal components. To be speciﬁc,
let X = [x̂1 , x̂2 , ...x̂N ] be the data matrix where x̂i = xi − μ
is the centralized vector of the i-th sample xi ∈ RD so that

x̂i = 0. PCA takes the eigenvectors V = [v1 , v2 , ...vK ] related to the K biggest eigenvalues i , 1 ≤ i ≤ K of the covariance matrix C = XX T as its orthonormal transform matrix.
Then, a D dimensional sample x is converted into a K dimensional subspace by y = V T (x − μ) which can be explained
from two aspects.
PCA-based face recognition and detection has been intensively studied [11],[9], where each PCA eigenvector vi
is called eigenface. Eigenface recognition is in general very
fast, since projection into eigenfaces is just a linear transformation. Similar idea can be used to develop an EigenEye
scheme for eye detection. However, as discussed above, the
limit of an EigenEye detector is that the resolution of test
image should be the same as that of the EigenEyes. A traditional way of scale adaption is to resize the test image before
EigenEye projection.
It is intuitive to think that instead of calculating PCA only
once at the original scale and enlarge every test image, we can
compute PCA eigenvectors every time at the scale of a test
image. To do this, we ﬁrst resize the training data to the scale
of the test image, then perform PCA. One obvious drawback
of this method is that the computational cost will be very high
if we use high resolution and large number of training data
(as is often the case) which make it impossible for practical
applications.
To alleviate the above problems, we propose to adapt the
EigenEyes to the (in general) smaller-scaled test image. The
Nystrom method ([1],[5]) gives a way of calculating the approximate eigenvectors and eigenvalues of a matrix K, using
those of a submatrix A. Inspired by this, we give a solution
in a more general situation: approximating eigenvectors and
eigenvalues of a matrix A with scale s, using those of a matrix
K with scale t, t > s. Here we introduce an linear re-scale
operatorD, given training data matrix X, the eigen decomposition of XX T is V ΛV T . Then Xs = DX is the re-scaled
training set, from the eigen decomposition of XX T , we can
get:
(DX)(DX)T = DXX T DT = DV ΛV T DT

(1)

If the columns of DV are orthogonal, then we get exactly
the new eigenvectors we want, but in general cases, they are
not. At this point, let
L = DV Λ1/2

(2)

then the SVD (singular value decomposition) of L would be
L = VL ΛL SL T , now we rewrite 1:
DV ΛV T DT = LΛLT = VL Λ2L VLT

(3)

Since the columns of VL are orthogonal to each other, we
already got the new eigenvectors. The above discussion gives
the exact eigen decomposition of re-scaled covariance matrix.
Based on this, we can derive a straightforward way to get the



Fig. 2. Illustrating the manually-obtained ground truth.
3. EXPERIMENTS AND RESULTS
Fig. 1. Sample images from the FDDB dataset, illustrating
the varying scale, pose, and background, etc.

approximate eigeneye basis. To be speciﬁc, we only need to
perform PCA on the original training set X once, and select
ﬁrst K eigenvectors (eigeneye basis)and corresponding ﬁrst
K eigenvalues. For each eigeneye basis vi , 1 ≤ i ≤ Kof V ,
we resample it to scale s by multiply the operatorD to obtain
a Vs , then we construct L using 1 and do SVD to get re-scaled
eigeneye basis.

We now describe the evaluation of the proposed scaleadaptive EigenEye method (SAE) and compare with several
state-of-the-art methods. First, we compare the reconstruction errors of our proposed SAE method with two other
general PCA methods. Second, two of the most recent eye
detection approaches are chosen for comparison, namely, the
ICA based method (ICA) [7], and the feature-versus-context
(FVC) method [4]. We also included the eye detector provided by OpenCV in the comparison for its wide availability.
We built a dataset of 2845 images based on the FDDB face
database [8] which contains faces with different scales and
background, as illustrated in Fig. 1. The eye detection performance is evaluated by two metrics, i.e. precision of eye
center and precision of detected bounding box.

2.3. Eye Detection: the Complete Procedure
With the previous preparation, we now describe the complete
procedure for our eye detector. This involves the following
ﬁve steps of processing for any given image to be tested.
Step 1. Face detection using OpenCV.
Step 2. Scale estimation. Suppose that the size of detected
face image is m × n. Its scale compared to our training image
is s = n/N , where N is width of the training face images.
Then the size of candidate eye bounding box is estimated as
x × s × y × s, where x × y is the size of eyes in the training
set.
Step 3. Eigeneye adaptation. Resample the EigenEyes
V by scale s following Section 2.2. The dimension of the
modiﬁed eigeneye basis ,i.e. each column of Vs , is x × s ×
y × s.
Step 4. Computing matching scores. We use sliding window to get the matching sore of all candidate eye blocks to
training eye region images. Since the OpenCV face detection result can’t always cover precisely the entire face region,
we do a scale search instead of using one ﬁxed scales. Repeat step 2 to 4 on different scales. For each window, several
matching score will be recorded and each corresponding to a
certain scale.
Step 5. Detecting true eye region from the candidate
blocks. The matching scores of all locations in the search
range are used to identify the ﬁnal eye locations.

3.1. Ground Truth
To provide a fair comparison using wild Web images, efforts
were devoted to manually annotate each of the 2845 images.
For the dominant face in a given image, the eyes are manually marked by 4 points: the center of the right eye center, the
center of the left eye, the upper-left corner and lower-right
corner of the left eye. Fig. 2 illustrates some of the examples
(for the cropped face region only, for better visualization).

3.2. Accuracy in Detecting the Eye Center
For a given face image, each of the approaches reports the
two detected eyes as two bounding boxes whose center is
estimated eye center. Then we use Euclidean distance to
measure the distance between the ground-truth eye centers
and detected eye centers. If this distance is smaller than one
third of the width of the ground-truth bounding box (which is
roughly half of the pupil size), it is deemed as a hit. The hit
rate based on this protocol is given in Table 1 for comparison.
From Table 1, it can be seen that the OpenCV eye detector
performs poorly on this dataset. The proposed method (SAE)
is comparable to FCV in the precision of eye center. Note
that the OpenCV face detector based on Adaboost classiﬁers
and Harr-like features achieved 91% detection rate on our
wild Web face image dataset. Accordingly, in order to avoid
introducing the inaccuracy of the face detector to the eye



Method
Accuracy

SAE
97.36%

FVC
97.25%

ICA
92.4%

OpenCV
29.3%

Table 1. Eye center hit rate.
Method
Precision
ratio area
JSC

SAE
55.1%
0.69
48.86%

FVC
43.55%
2.98
25.56%

ICA
33.54%
2.45
22.97%

Fig. 3. First row from left to right: original image; SAR eye
detection result. Second row from left to right: FVC eye detection result; ICA eye detection result. White box is ground
truth and black box is detected result.

Table 2. Precision and Ratio of eye bounding box.
detectors, in this paper, the eye detection accuracy is only
calculated on those images with successful face detection.

3.3. Accuracy in Detecting the Eye Box
An accurate eye detector should also precisely estimate the
size of the eye, i.e. obtaining a proper bounding box. One
possibility is to compute the following metrics that measure the similarity between the ground-truth and the detected
bounding boxes:
precision =


(Groundtruth area Detected box area)
Detected box area

While these metrics are intuitive, they are not very effective for our study. For example, the precision can still be
100 if the detected box is overly enlarged and fully cover
the ground-truth box. Hence we also introduce two other
complementary measurements, i.e. the ratio area and Jaccard
similarity coefﬁcient (JSC):

are better than other approaches. Hence overall the proposed
method is deemed as the best among all four approaches. Fig.
3 provides an example illustrating the results from the approaches evaluated in this study. Another beneﬁt of the proposed method is its speed performance. We ran all the approaches on a PC with AMD A8-3500M APU 1.50GHz and
8.00GB RAM. In terms of the average execution time for one
image, our method spends only 45% and 50% of the times
needed by the FVC approach and the ICA approach respectively.
As mentioned previous, upon a decision on the paper, we
will make our data set publically available, with the hope of
providing a common basis for comparative study by other
researchers.

4. CONCLUSIONS AND FUTURE WORK

3.4. Summary of the Results and Discussion

We presented an eye detector using scale-adaptive EigenEyes
for wild Web images. We built a dataset with manually annotated ground truth for evaluating competing algorithms.
Based on the dataset, the comparative experiments have
demonstrated the advantage of the proposed method, in terms
of both overall accuracy and speed performance. There are
a few directions for further exploration. Firstly, the current
version of the proposed method relies on only a small training
set of 861 eyes. Conceivably, its performance may be further
improved by using a better training set. Secondly, our method
currently does not use any color information. Incorporating
color may help further improve the performance especially
for reducing false detection. There are other many eigen
problems in pattern recognition applications [3], and one future direction is to explore the proposed idea for achieving
scale-adaptivity in such problems.

The results presented in Table 1 and Table 2 suggest that the
proposed SAE approach is slightly better than FVC in terms
of locating eye center, while being much better than FVC in
terms of estimating eye bounding box.Both SAE and FVC

Acknowledgments: This work was supported in part by a
National Science Foundation grant (#1135616). Any opinions
expressed in this material are those of the authors and do not
necessarily reﬂect the views of NSF.

ratio area =
JSC =

Detected box area
Groundtruth area


(Groundtruth area Detected box area)
(Groundtruth area Deteted box area)

The average performance numbers of the competing approaches are given in Table 2. (The OpenCV eye detector is
not reported here because of its extremely low performance.)
Table 2 shows that our method is consistently superior to
other methods on the precision and JSC of bounding box.
Besides, the area of its bounding box is also close to the
ground truth.



5. REFERENCES
[1] C. J. Burges, “Geometric methods for feature extraction and
dimensional reduction,” in Data mining and knowledge discovery handbook. Springer, 2005, pp. 59–91. 2
[2] E. B. Corrochano, Handbook of Geometric Computing: Applications in Pattern Recognition, Computer Vision, Neuralcomputing, and Robotics. Springer Science & Business Media,
2005. 1
[3] T. De Bie, N. Cristianini, and R. Rosipal, “Eigenproblems in
pattern recognition,” in Handbook of Geometric Computing.
Springer, 2005, pp. 129–167. 4
[4] L. Ding and A. M. Martinez, “Features versus context: An
approach for precise and detailed detection and delineation of
faces and facial features,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 32, no. 11, pp. 2022–2038,
2010. 1, 3

[15] X. Tang, Z. Ou, T. Su, H. Sun, and P. Zhao, “Robust precise
eye location by adaboost and svm techniques,” in Advances in
Neural Networks–ISNN 2005. Springer, 2005, pp. 93–98. 1
[16] P. Wang, M. B. Green, Q. Ji, and J. Wayman, “Automatic eye
detection and its validation,” in Computer Vision and Pattern
Recognition-Workshops, 2005. CVPR Workshops. IEEE Computer Society Conference on. IEEE, 2005, pp. 164–164. 1
[17] Y. Wang, S. Wang, J. Tang, H. Liu, and B. Li, “PPP: Joint
pointwise and pairwise image label prediction,” in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2016. 1
[18] Z. Wang, X. Xu, and B. Li, “Bayesian tactile face,” in Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE
Conference on. IEEE, 2008, pp. 1–8. 1

[5] C. Fowlkes, S. Belongie, F. Chung, and J. Malik, “Spectral
grouping using the nystrom method,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 26, no. 2, pp.
214–225, 2004. 2
[6] L. Gan and Q. Liu, “Eye detection based on rank order ﬁlter
and projection function,” in Computer Design and Applications
(ICCDA), 2010 International Conference on, vol. 1. IEEE,
2010, pp. V1–642. 1
[7] M. Hassaballah, T. Kanazawa, and S. Ido, “Efﬁcient eye detection method based on grey intensity variance and independent
components analysis,” Computer Vision, IET, vol. 4, no. 4, pp.
261–271, 2010. 1, 3
[8] V. Jain and E. G. Learned-Miller, “Fddb: A benchmark for face
detection in unconstrained settings,” UMass Amherst Technical
Report, 2010. 1, 3
[9] Y. Liao and X. Lin, “Blind image restoration with eigen-face
subspace,” Image Processing, IEEE Transactions on, vol. 14,
no. 11, pp. 1766–1772, 2005. 2
[10] C. W. Park, K. Park, and Y. Moon, “Eye detection using eye ﬁlter and minimisation of nmf-based reconstruction error in facial image,” Electronics letters, vol. 46, no. 2, pp. 130–132,
2010. 1
[11] B. Poon, M. A. Amin, and H. Yan, “Pca based face recognition and testing criteria,” in Machine Learning and Cybernetics, 2009 International Conference on, vol. 5. IEEE, 2009,
pp. 2945–2949. 1, 2
[12] W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery, “Numerical recipes in c: the art of scientiﬁc computing,”
Cambridge University Press, Cambridge, MA,, vol. 131, pp.
243–262, 1992. 1
[13] P. Quintiliano and A. Santa-Rosa, “Face recognition based on
eigeneyes,” PATTERN RECOGNITION AND IMAGE ANALYSIS C/C OF RASPOZNAVANIYE OBRAZOV I ANALIZ IZOBRAZHENII, vol. 13, no. 2, pp. 335–338, 2003. 1
[14] J. Ren and X. Jiang, “Eye detection based on rank order ﬁlter,”
in Information, Communications and Signal Processing, 2009.
ICICS 2009. 7th International Conference on. IEEE, 2009,
pp. 1–4. 1



Bayesian Tactile Face
Zheshen Wang, Xinyu Xu, Baoxin Li
Computer Science and Engineering
Arizona State University, Tempe, AZ, 85281, USA
{zheshen.wang, xinyu.xu, baoxin.li}@asu.edu

Abstract
Computer users with visual impairment cannot access
the rich graphical contents in print or digital media unless
relying on visual-to-tactile conversion, which is done
primarily by human specialists. Automated approaches to
this conversion are an emerging research field, in which
currently only simple graphics such as diagrams are
handled. This paper proposes a systematic method for
automatically converting a human portrait image into its
tactile form. We model the face based on deformable Active
Shape Model (ASM)[4], which is enriched by local
appearance models in terms of gradient profiles along the
shape. The generic face model including the appearance
components is learnt from a set of training face images.
Given a new portrait image, the prior model is updated
through Bayesian inference. To facilitate the incorporation
of a pose-dependent appearance model, we propose a
statistical sampling scheme for the inference task.
Furthermore, to compensate for the simplicity of the face
model, edge segments of a given image are used to enrich
the basic face model in generating the final tactile printout.
Experiments are designed to evaluate the performance of
the proposed method.

1. Introduction
With the text-to-speech/text-to-Braille technology,
nowadays computer users with visual impairment can
independently access digital textual information. However,
when it comes to digital graphical contents, there still exist
major barriers for a computer user who is blind to
independently access the vast amount of digital graphical
media such as graphs, diagrams and images. Traditional
approaches to solving this problem rely on third-party
sighted professionals, tactile graphic specialists (TGS), to
manually convert the digital graphics into their tactile
forms. This process is typically time-consuming and
labor-intensive. Thus it is imperative to develop automatic
approaches that can convert visual digital graphics into
their tactile forms so that the computer users with visual
impairment may gain immediate access to graphical
contents with such technologies.

978-1-4244-2243-2/08/$25.00 ©2008 IEEE

Due to the extremely limited bandwidth of tactile
perception compared with that of vision, image
simplification is a key step in automatic visual-to-tactile
conversion. Unfortunately, in the literature, this key step
has not been fully studied. The work of [1] relied on simple
image processing steps such as negation and edge
detection. Way et al [2] proposed to simplify images also
mostly by edge detection. However, without high-level
guidance, it is difficult to choose the best parameters for
such low-level image processing steps. For example, major
information, which should be kept, may be lost due to the
failure of edge detection. There are also additional
problems of broken edges or scattered edge segments that
may serve only to confuse a blind user if they are simply
mapped to tactile lines as done in [1, 2]. Any attempt to
clean up the edges, such as linking short ones to form a long
contour, may do harm if those processing steps are purely
driven by the data. The system developed in [3] resorts to
Photoshop for image simplification, which actually
involves the manual efforts of a sighted person and
therefore does not address the need of automated solutions
that support independent access of a blind user.
To overcome these drawbacks, in this paper, we propose
a systematic approach to automatic conversion of a human
portrait image into its tactile form. By limiting ourselves to
the specific yet commonly-encountered problem, we are
able to exploit various constraints imposed by this
high-level knowledge of knowing the image containing a
face. In particular, instead of simplifying the results from
edge detection, we explicitly model human faces using
deformable Active Shape Model (ASM) [4]. Prior shape
and appearance statistics of human faces are first learned
from a set of training face images. Given an image with
human portrait, the set of deformable parameters that best
explain the image data are estimated through Bayesian
inference with statistical sampling. This leads to a Bayesian
framework to the tactile conversion task. Upon the match
of the face model to the image, edge segments are
selectively used to enrich the representation of the aligned
face shape, which is by design very simple. Furthermore, in
generating the tactile graphics, we exploit the strength of
the gradient to modulate the tactile patterns to add desired
layer of complexity to the tactile lines. As such, the
proposed method combines the model generality with data

specificity to create an informative tactile representation of
the original face image. To the best of our knowledge, this
is the first reported systematic attempt to this challenging
problem of automatically creating tactile faces from
images.
In Section 2, we briefly discuss related work. In Section
3, we describe the shape and appearance model and also
analyze the advantage of the proposed appearance model
based on clustering of the gradient profiles along the shape
model. Section 4 presents our method for updating a prior
model with the given data, including the steps for creating
the final tactile graphics by enriching the simple face model
with detected edge segments. We report systematic
evaluation of our method in Section 5, and then conclude in
Section 6 with discussion on future work.

2. Related work
Visual-to-tactile conversion has been an active research
field recently. Currently active work focuses primarily on
simple line-drawing graphics only [5,6], with little effort
dealing with natural images such as portrait images that we
attempt to address in this paper.
Face alignment is an active research area with many
research papers in recent years. In the pioneering work
ASM [4], each feature point is sampled by a local profile
search, and then a Gaussian shape prior model obtained
from PCA is used to regularize the search results
iteratively. Bayesian Tangent Shape Model (BTSM) [7] is
another derivation of ASM proposed to infer shape
parameters more accurately and robustly by the EM
algorithm. While being useful, ASM has some known
drawbacks. First, ASM tends to get stuck in local minima
[8][9] due to the fact that it considers the local optimization
of shape points independently [9]; Second, it has difficulty
with generalization [10] because using PCA often restrict
the local deformations too much.
A number of approaches have been proposed to remedy
these problems. For the first problem, a hierarchical
CONDENSATION is proposed in [11] to search the MAP
estimates of shape configurations. Coughlan et al [12]
introduced Markov Random Field model into face
alignment to model both the local image structure and the
shape prior. Liang et al [13] integrated the Markov network
search with the global shape prior to achieve more accurate
alignment results but the shape prior is still built by PCA.
Huang et al [9] generate new shape by maximizing local
probability that describes how desirably the component
shape parameters fit the observation, given a probability
distribution function (PDF) encoding the interrelationship
among parameters of all components modeled by
Constrained-GPLVM [14]. To address the second problem,
Jiao et al. [8] suggest using Gabor wavelet features to
represent the local appearance. Hu et al. [15] utilized a
wavelet network to replace the PCA-based appearance

model, and demonstrate improved alignment under
illumination changes and occlusions. In [10], face
alignment is treated as a process of maximizing the score of
a trained two-class classifier that is able to distinguish
correct alignment from incorrect alignment.
Our approach addresses those two problems from three
aspects. First, we employ importance sampling to find the
shape parameters that maximize the posterior PDF in a
Bayesian formulation, attempting to avoid getting stuck at
local minima. Second, as opposed to making the
assumption that the prior appearance model is multivariate
Gaussian, we perform clustering in learning the appearance
model from the training set. This effectively results in a
multi-modal representation, which turns out to be able to
generalize better. The generalization difficulty of ASM is
also due to the fact that an instance of shape configuration
is compared with only the appearance variations learnt
from the training set, without considering pure data
evidence of the current image. This will force the shape to
only deform in the ways learnt from the training set. Thus
as the third remedy, we combine model-driven part with
data-driven part in the likelihood function in our approach.
The data-driven term measures how well the global face
contour fits to the true face contour, and the model-driven
term measures how well the facial features conform to the
prior knowledge of shape configurations obtained from the
training set. This improvement increases the generalization
ability of the algorithm to unseen images.

3. Prior shape and appearance model
3.1. Statistical shape model
Let Ε = {( x, y ) ∈ ℜ2 } be the image plane. Human faces
are characterized by N landmark points [4], pi=(xi, yi),
i=1,...N. In the IMM [16] face image database used in our
system, the following facial features were manually
annotated using 58 landmarks around the eyebrows, the
eyes, the nose, the mouth, and the jaw (Fig. 1). Seven point
paths were used in total, three closed and four open. The
shape examples are aligned into a common coordinate
frame using Generalized Procrustes Analysis (GPA) [17].
Let fi= (xi1, yi1, xi2, yi2, ..., xiN, yiN)T be the feature vector
that represents the i-th face image in the training set. Given
M training images, we form matrix F=[f1, f2, ..., fM]. We
want to seek a parameterized model of the form f=Ψ(b)
from the training set, where b is the parameter of the shape
model, such that new shapes can be synthesized using this
model by varying b within reasonable limits. Specifically,
we apply PCA to the positions of the landmarks over the
training set, F, to find the eigenvectors φi and the
eigenvalues λi of the covariance matrix S of F, as done in
ASM [4,18]. Then a shape in the training set can be
approximated using the mean shape and a weighted sum of
the first t largest eigenvectors, i.e.

(1)
f ≈ f + Φb
where f is the mean shape. Φ=[φ1, φ2,..., φt] is the matrix
of the first t eigenvectors, and b=(b1, b2,..., bt)T given by
b = Φ T (f - f) . The vector b defines a set of parameters of a
deformable model. Eq. (1) allows us to generate new
examples of the shapes by varying the elements of b within
suitable limits, such as done in [4] by
(2)
−3 λk ≤ bk ≤ 3 λk

In order to reduce the dimensionality of the parameter
space, we only use the first t eigenvectors and the first t
elements in b, t is chosen such that p percent of the
variation is retained, i.e.

t

(a)
(b)
(c)
(d)
Figure 3: The mean gradient profile is very sensitive to pose.
Mean gradient profile at p1 computed over all the training images
(a), over the nearly frontal pose set (b), over the turning left pose
set (c) and over the turning right pose set (d) look quite different
from each other.

p

∑ λ ≥ 100 ∑ λ .
i

i

i =1

-3 s.d.

mean

(a)

+3 s.d.
(b)



Figure. 2: Effect of varying b1
between ±3 s.d.

Figure 1: Facial landmarks in
the face model [16].
Fig. 2 shows the effect of varying the first element of b in turn
between ±3 standard deviations from the mean value, leaving all
other elements of b at zero. The corresponding shape f is
reconstructed using Eq. (1).

3.2. Local appearance model
The local appearance models, which describe local
image features around each landmark, are modeled as the
first derivative of the profile, gj, computed along the line
perpendicular to the boundary of a shape instance fi through
landmark point pj [4]. Traditional approaches [4][8]
assumed the local appearance are distributed as
multivariate Gaussian, and the quality of fitting a gradient
profile gs at test image location s to the j-th model is
computed as the Mahalanobis distance from gs to the j-th
model mean. We found a few shortcomings with this
method. First, one multivariate Gaussian may not capture
the large number of pose and expression variations present
in the training set. Second, the model mean is computed
across all the training images, but the mean gradient profile
is very sensitive to the pose (turn left/turn right). For
instance, the mean gradient profile of p1 computed over
examples that turn left could dramatically differ from the
mean gradient profile of p1 computed over examples that
turn right, as being verified by Fig. 3. Hence the
distinguishable features will be averaged out if the mean
gradient profile is computed across different pose set.

(c)
Figure 4: (a) The mean gradient profile at p51 computed over all
the training images in subset Γ. (b) The gradient profile at p51 for a
testing image. (c) The 5 cluster centroids obtained from clustering
in subset Γ. The score of matching (b) to (a) would be very low,
whereas (b) matches well to the 5th cluster centroid in (c),
indicating that k-means clustering can capture more appearance
variations in the training set than simply using the mean gradient
profile.

To remedy these shortcomings, we adopt a
pose-dependent local appearance model. Specifically, we
divide the entire training image set into three subsets based
on the pose variation:
{I ∈ Γ if the face in image I is nearly frontal}
(3)
{I ∈ Θ if the face in image I turns left}

{I ∈ Λ if the face in image I

turns right}
Further, for each landmark pj, we apply k-means clustering
to all the gradient profiles of pj in set Γ, Θ and Λ
respectively, keeping 5 clusters for each set. We record the
5 cluster centroids (for each set) and denote them as
g vj ( Γ), g vj (Λ ), g vj ( Θ) , for v=1,…, 5, and j=1,…, N. Fig. 4
illustrates that k-means clustering can capture more
appearance variations in the training set than simply using
the mean gradient profile. For a hypothesized face shape,
we first compute which pose set the image belongs to based
on the value of b1, then the fitting score at testing landmark
pj is computed as the exponential of the minimum distance
between gj and the 5 centroids,

h : b1 → pose set
if − 3 λ1 ≤ b1 < −0.5 λ1 , h(b1 ) = Θ

(4)

if − 0.5 λ1 ≤ b1 < 0.5 λ1 , h(b1 ) = Γ
if

0.5 λ1 ≤ b1 < 3 λ1 , h(b1 ) = Λ

)

(

2
(5)
s = exp − min g j − gvj (h(b1 )) 2 
 v =1,...5

where g vj ( h(b1 )) represents the v-th cluster centroid

computed from all the gradient profiles at pj in training
subset h(b1). Function h returns the pose subset based on
the value of b1.

4. Bayesian parameter update with statistical
sampling
Statistical analysis of the positions of a number of
landmarks located on training images provides a generic
face template, average face f and basis vectors Φ, which
account for the generalities of face variations in the training
set. However, image specificities have to be taken into
consideration to update the coefficient vector b such that
the reconstructed shape best matches the image data. In
addition to the coefficient vector b, we also estimate the
global translation (X, Y) of the face shape. This leads to the
parameter set θ = ( X , Y , b ) with b=[b1, b2, ..., bt]T.

(

Initialize θ = ( X , Y , b ) as θ = X 0 , Y0 , 0, 0, 0

)

Loop until stop-criterion satisfied
1. Generate H samples θ i , i =1, ..., H , Eq. (7)
2. Compute likelihood p ( I | θ )
2.1 Construct shape from the parameters, E q.(8)
2.2 Compute the likelihood for each sample, Eq. (11)
3. Resampling proportional to the likelihood

Figure 5: The proposed algorithm.

Given a new image, the parameters can be calculated by
estimating the posterior density of θ in a Bayesian
framework:
(6)
p (θ | I ) = p ( I | θ ) p (θ ) / p ( I ) ∝ p ( I | θ ) p (θ )
In our algorithm, the parameters are updated iteratively. At
the beginning, the coordinates of the global translation, (X,
Y), are initialized by face detection; and the elements of the
vector b are initialized to all zeros. In general, the posterior
probability density, p(I|θ), is highly non-linear and
multi-modal. Thus no parametric form of p(I|θ) should be
assumed. Thus, we use a set of weighted samples to
approximate the posterior PDF iteratively. This leads to a
CONDENSATION-like statistical sampling scheme. One
advantage of statistical sampling is that it allows us to know
the rough pose of the current shape instance, therefore we
can compute pose-dependent likelihood (Sec. 4.2). Our
work differs from prior CONDENSATION scheme [11] in

that we introduced the pose-dependent likelihood model,
which was found to account for a large portion of the
improved performance. The proposed algorithm is shown
in Fig. 5, where the stop criterion is currently set as a fixed
number of iterations. When the algorithm terminates, the
mean shape computed from the weighted samples is kept as
the final model. In the following, we discuss each step in
detail.

4.1. Generate random samples
Face detection and prior statistics on the suitable limits
of bk effectively constrain the parameters to be within a
certain region in a high-dimensional parameter space. This
allows us to generate H random samples that are uniformly
distributed within a hyper-rectangle:
θi = ( X i , Yi , b i ) X ir −1 − δ X ≤ X ir ≤ X ir −1 + δ X 


(7)
| Yi r −1 − δY ≤ Yi r ≤ Yi r −1 + δ Y




| − 3 λk ≤ bikr ≤ 3 λk , k = 1,...t.


One could model p(θ) using a mixture of Gaussians with a
kernel density estimate for each Gaussian component [9],
or even learn a prior non-parametric density from the
training set (which is only meaningful if the number of
training images is large enough). However, in our current
system, we found random sampling with uniform
distribution works quite well.

4.2. Likelihood model p(I|θ)
The usefulness of the generated samples needs to be
evaluated based on the image measurement, i.e. how well
the reconstructed face shape from a parameter sample fits
the image data. The local image measurement given a face
configuration is defined as the gradient profiles estimated
along the line perpendicular to the model boundary through
each model point, as in [19].
Reconstruct shape from one parameter sample. Given a
parameter sample θ i = ( X i , Yi , b i ) , the corresponding face is
reconstructed by
dx = X i − X a

dy = Yi − Ya

(8.1)

(8.2)
f i = Tdx , dy ( f ) + Φb i
In Eq. (8.1), we compute the offset between the face
location (Xi, Yi) of a model instance and the center of the
bounding box that encompasses the landmarks of the mean
shape f (computed from training images). Then in Eq.
(8.2), the i-th shape instance is reconstructed by moving the
mean shape to the current face location with transformation
Tdx , dy ( f ) and adding the intrinsic shape deformation Φbi .
Function Tdx , dy ( f ) performs a translation by (dx, dy).

Likelihood model. Let gj denote the gradient profile
computed along the line perpendicular to the boundary of a
shape instance fi through landmark point pj. We define the
likelihood as
p( I | θi ) = η i p( g1 , , g N | θi ) + (1 − η )i p(G (θi ))
(9)
 N

= η i ∏ p( g j | θi )  + (1 − η )i p(G (θi ))
 j=1

The likelihood consists of two terms: model-driven part
p(g1, ..., gN| θi ), which is the joint probability of the gradient
profiles at the N local landmarks given the shape
configuration θi; And data-driven part p(G(θi)) which
measures how well the global face contour fits to the true
face contour. The data-driven part increases the ability of
the algorithm in generalizing to unseen images, and the
facial features can be more accurately located than only
using the model-driven part. As illustrated in Fig. 6.

(a)
(b)
Figure 6: k-means clustering and data-driven part in the likelihood
function increase the accuracy of face alignment. (a) Alignment
result with neither technique. (b) Alignment result with both
k-means clustering and data-driven likelihood.

The likelihood at each landmark, p(gj| θi ), is given by
2
(10)
p( g j | θi ) = exp  − min g j − gvj ( h(bi1 )) 2 
 v=1,...5

where g vj ( h( bi1 )) represents the v-th cluster centroid

(

)

computed from all the gradient profiles at pj in training
subset h(bi1), as discussed in Sec. 3.2.
The data-driven part, p(G(θi)), is computed as
α
(11)
p ( G (θi ) ) =
β
α denotes the total number of edge pixels encompassed by
all the 3x3 windows centered at each point located on the
facial feature contour that fall inside of the bounding box of
the true face obtained by face detection. β denotes the total
number of edge pixels that fall inside of the bounding box
of the true face obtained by face detection. p(G(θi)) gives us
the ratio of the model edge pixels to the true face edge
pixels.
Weighting differently for different landmarks. We also
observed that when the face turns left (right), the collection
of landmarks located on the left (right) side of face will
have smaller contribution to the likelihood computation.
Suppose we know the rough orientation of a shape instance,
the likelihood model can be further improved such that the
image measurements of different set of landmarks are
weighted differently. In our statistical sampling
framework, it is easy to achieve that because bi1 gives the

rough orientation of a face instance. In particular, we
partition the N landmarks into three sets, S1, S2 and S3,
corresponding to the set of right-side, middle and left-side
landmarks (Fig. 2). If 0.5 λk ≤ bik < 3 λk , we deem the
current face is turning right, then the landmarks in set S1
will be assigned smaller weights than those in S2 and S3; If
−3 λk ≤ bik < −0.5 λk , we deem the current face is turning
left, then the landmarks in S3 will be assigned smaller
weights than those in S1 and S2. If −0.5 λk ≤ bik < 0.5 λk ,
all the three sets are viewed equally important. This yields
the following enhanced likelihood model:


2 
min g j ( fi ) − gvj ( h( bi1 ))  


∑
2
T
v=1,...,5
  c1   j∈s1

2 
   
p( I | θi ) = η iexp  −  c2  i ∑ min g k ( fi ) − g vk ( h(bi1 )) 2  
v=1,...,5
  c3   k∈s2


 ∑ min g ( f ) − g ( h( b )) 2  
vl
i1
2 
 l∈s v =1,...,5 l i

 3
 
+ (1 − η )i p(G (θi ))
(11)
In the re-sampling step, we multiply samples with large
image likelihood and discard samples with small
likelihood. In implementation, we also perform a local
search for mean shapes of eyes, nose, mouth, and the jaw
respectively, after the final re-sampling step.

4.3. Edge based enrichment
In the tactile representation, in addition to the contours of
the major facial features like eyes, mouth, and nose, it is
also essential to keep other informative edges in the final
tactile rendering. To achieve this, we enrich the basic face
model obtained from our face alignment algorithm with the
edges detected by a Canny edge detector. An adaptive edge
refining step is used to filter out the redundant edges. E.g.,
for eyes, eyebrows, jaw and nose, we keep their shape
contours and the nearby edge segments; For mouth, we did
not use the contour of the landmarks obtained from the face
alignment algorithm; rather, in order to better reflect the
expressions conveyed by the mouth such as smiling, we
keep the original edges that are located inside a bounding
box around the detected mouth landmarks.

4.4. 2.5-D tactile graphical representation
The edge-enriched portrait image can be transformed to
tactile form by printing it out using an embosser or a
thermoform machine. In this stage, we exploit the strength
of the gradient to modulate the tactile patterns in generating
the tactile graphics. For example, we use denser dot
patterns for areas with strong gradients, thicker lines for
major facial features, and thin lines for the secondary
features including wrinkles, fine edges around the eyes and
the mouth.

(a)
(b)
(c)
(d)
(e)
Figure 7: The entire tactile conversion process. First row shows original images, 2nd row shows the results of face alignment, 3rd row
depicts the edge-enriched representation, 4th row illustrates the tactile printouts from a thermoform machine.
Figure 7 illustrates with examples the steps of the tactile
conversion process. The first row lists 5 input images, the
second row illustrates the face alignment results, the 3rd
row is the edge-enriched face renderings, and the 4th row
shows the pictures of the actual tactile printouts.

5. Experiment set-up, results and evaluation
5.1. Face alignment evaluation
We used two face image databases to evaluate the
performance of the proposed face alignment algorithm. The
first database, IMM [16], comprises 240 images of 40
different human faces. Each person has 6 poses: 4 full
frontal poses with varying lighting, 1 pose turning
approximately 30˚ to the right, and 1 pose turning
approximately 30˚ to the left. We use 30 persons for
training and the remaining 10 persons are used for testing.
Fig. 8 shows some sample results of the alignment
algorithm for different poses and lighting conditions. We
also quantitatively analyzed the performance of the
algorithm by comparing the estimated shape with the
ground truth shape and plotting the error, as shown in Fig.
9. We can see that the average error per anchor point is
about 10 pixels for both training and test images. Since the
average height of face regions in the database is about 200
pixels, the error is only about 5% of the height of the face
region, and thus can be deemed as small.

(a)
(b)
Figure 9: (a) Average errors per anchor point of training images.
(b) Average errors per anchor point of test images.

Our results are also comparable to what presented in [9]
although it is difficult to make straightforward comparison
since the databases are different. In our results, the
percentages of images with average per anchor point error
less than or equal to 13.3 pixels are 84% and 74% for the
training set and the test set respectively. These are
compared to 89.7% and 57.8% reported in [9] for group1
and group2 respectively, for images with errors less than or
equal to 12 pixels cases. (The average height of the face
region in the database used in [9] is about 180 pixels. Hence
we compare our 13.3-pixel error against their 12-pixel
error.)
In order to illustrate the robustness of the algorithm in
fitting to novel images whose acquisition environment
differs greatly from the IMM database, we independently
captured a 30-person face database with varying lighting
conditions, expressions and poses. The resolution of the
images is also much lower than the first set. A few sample
results are shown in Fig. 10, demonstrating the robust
performance of the algorithm.

Figure 8: Face alignment results. The faces in the 1st row are frontal with some variations of expressions and lighting. The faces in the 2nd
and 3rd row have off-plane rotations.
Table 1: The resultant statistics obtained from the first experiment
of tactile representation evaluation.
Image
Figure 10: Face alignment results on the second data set.

5.2. Tactile representation evaluation
In order to evaluate the effectiveness of the tactile
graphics, we did two sets of experiments. The objective of
the first experiment is to evaluate whether the tactile
portrait image can effectively convey the important and
informative facial features. Six volunteers including 5
blind-folded sighted persons and 1 user who is blind
participated in the experiment. Although the end user of the
technology will be people with visual impairment, at this
stage of study, to verify that the approach does maintain
key "visual" features, it was found that using blind-folded
sighted individuals for the evaluation is very helpful since
they are able to compare what they feel by touch against
what they see. (This is especially true for the second
evaluation experiment.)
For each user, we asked him/her to touch the 5 tactile
portraits presented in Fig. 7, last row, one by one. They
need to answer the following 4 questions: (1) Can you
recognize each face component including the mouth, eyes,
eyebrows and nose? (2) Can you recognize the pose of the
person, i.e. is he/she turning left or turning right? (3) Can
you recognize the gender of the person? (4) Can you
identify two images that represent the same person? The
results were collected in Table 1, where (for example) 5/6
means 5 out of the 6 testers got that answer correctly and
“Association” refers to Question 4.

a
b
c
d
e

Left eye, Right eye,
AssociNose Mouth Pose Gender
eyebrow eyebrow
ation

6/6
6/6
6/6
4/6
5/6

5/6
6/6
5/6
4/6
5/6

6/6
5/6
6/6
4/6
4/6

6/6
5/6
6/6
5/6
5/6

6/6
6/6
6/6
5/6
5/6

5/6
6/6
3/6
3/6
5/6

5/6

The chart indicates a few interesting facts. First, the user
who is blind was much faster than other blind-folded
sighted persons in interpreting the results; she gave all
correct answers except the gender of image e. She could
even correctly recognize facial expressions (smiling,
frown) based on the shape of mouth (open/close,
wide/thin). She started to identify the face components
mainly based on the shape of the mouth and the nose,
whereas the blind-folded sighted person relied mainly on
the hair and the global contour to get started. Second,
gender recognition is a little harder than other tasks because
it is difficult to differentiate hair from shoulder, if purely
relying on the haptic sense. Third, the length of the
right/left eyebrow and the presence/absence of ears play
critical role in delivering the pose information (turning
left/right) since it is usually true that when the face turns
right, the length of left eyebrow will be longer and the ear
will become visible on the image. Fourth, pose recognition
has direct effect on the identification of nose and mouth,
because if the pose is not correctly recognized, then the user
tends to explore the wrong parts of the image for the
nose/mouth. Because of the importance of correct pose
recognition, we can print in Braille the pose of the face on

the tactile graphics; this will significantly help the
understanding of tactile portraits.
The objective of the second experiment is to analyze
whether the system is able to retain the distinctive
characteristics of the facial features. Five blind-folded
sighted persons participated in the experiments, and two
tactile images (Fig. 11) are used for their exploration. The
participants were asked to give the identity of each person
on the two tactile images, chosen from 5 persons whom
they all visually know very well. For Example, we asked
the participants: “Can you tell who this person is, chosen
from Cindy, Troy, Jessie, Michael, and Daniel?” The
results were very encouraging: all of the 5 users were able
to correctly recognize the identity of the persons on the two
images, which shows that the converted tactile
representation does a good job in retaining the distinctive
visual characteristics.

(a)

(b)

(c)
(d)
Figure 11: The two orignal images (a) and (c) and the
corresponding tactile forms (c) and (d) used in the second
experiment for tactile graphics evaluation.

6. Conclusion and Future Work
This paper proposed a novel solution for automatic
conversion of digital portrait images into their tactile forms.
Using a Bayesian inference framework, the approach
utilizes a statistical-sampling-based algorithm for aligning
a prior face model with a given image, which allows us to
avoid getting stuck in the local minima. The likelihood
model in our approach includes a model-driven part and
data-driven part, which increases the robustness of the
algorithm when generalizing to unseen images. A
pose-dependent likelihood model was proposed to facilitate
the match between the data and the model. The aligned
contours of the facial components are enriched with local
informative edge segments to improve its capability in
retaining the most distinctive characteristics of the faces.
Experiments and evaluation on both face alignment and
tactile conversion show that the proposed approach is very
effective, suggesting this is a promising approach to the
challenging problem. A full-scale evaluation with a large
number of blind users with diverse educational/training

background, level of exposure to tactile graphics, level of
visual memory, etc. will be among our future tasks.

References
[1] Satoshi Ina. Presentation of images for the blind. ACM
SIGCAPH Computers and the Physically Handicapped, 56:
10-16, 1996.
[2] T. Way, K. Barner. Automatic visual to tactile translation,
part I: human factors, access methods and image
manipulation. IEEE Transactions on Rehabilitation
Engineering, vol. 5, pp. 81-94, Mar. 1997.
[3] R.E. Ladner, M.Y. Ivory, R. Rao, S. Burgstahler, D. Comden,
S. Hahn, et al. Automating tactile graphics translation. The
7th International ACM SIGACCESS Conference on
Computers and Accessibility (ASSETS '05), pp. 50-57.
[4] T. F. Cootes, C. J. Taylor, D. H. Cooper, J. Graham. Active
shape models – their training and application. Computer
Vision and Image Understanding, 61(1):38-59, 1995.
[5] http://dots.physics.orst.edu.
[6] Tactile Graphics Project at University of Washington:
http://tactilegraphics.cs.washington.edu.
[7] Y. Zhou, L. Gu, and H. Zhang. Bayesian tangent shape
model: estimating shape and pose parameters via Bayesian
inference. CVPR (1), pp. 109-116, June 2003.
[8] F. Jiao, S. Li, H.-Y. Shum, and D. Schuurmans. Face
alignment using statistical models and wavelet features. In
Proc. CVPR (1), pp. 321–327, 2003.
[9] Y. Huang, Q. Liu, D. Metaxas1. A component based
deformable model for generalized face alignment. In Proc.
ICCV, 2007.
[10] X. Liu. Generic face alignment using boosted appearance
model. In Proc. CVPR, pp.1-8, 2007.
[11] J. Tu, Z. Zhang, Z. Zeng, T. Huang. Face localization via
hierarchical CONDENSATION with Fisher boosting feature
selection. CVPR (2), pp. 719-724, 2004.
[12] J. Coughlan and S. Ferreira. Finding deformable shapes
using loopy belief propagation. In Proc. ECCV, 2002.
[13] L. Liang, F. Wen, Y.-Q. Xu, X. Tang, and H.-Y. Shum.
Accurate face alignment using shape constrained Markov
network. In Proc. CVPR (1), pp. 1313-1319, 2006.
[14] N. D. Lawrence. Probabilistic non-linear principal
component analysis with Gaussian process latent variable
models. In Journal of Machine Learning Research 6, pp
1783–1816, 2005.
[15] C. Hu, R. Feris, and M. Turk. Active wavelet networks for
face alignment. In Proc. 14th British Machine Vision
Conference, Norwich, UK, 2003.
[16] M. B. Stegmann, Bjarne K. Ersbøll, and Rasmus Larsen.
FAME - a flexible appearance modeling environment. IEEE
Trans. On Medical Imaging, 22(10): 1319-1331, 2003.
[17] J. C. Gower. Generalized procrustes analysis. Psychometrika,
40:33-50, 1975.
[18] G. J. Edwards, A. Lanitis, C. J. Taylor, T. F. Cootes.
Statistical models of face images – improving specificity.
Image and Vision Computing, vol. 16, pp. 203-211, 1998.
[19] T. F. Cootes and C. J. Taylor. Statistical Models of
Appearance for Computer Vision, pp. 12-28, 37-43, 2004.

POWER-AWARE CONTENT-ADAPTIVE H.264 VIDEO ENCODING
Avin Kumar Kannur and Baoxin Li
Department of Computer Engineering
Arizona State University
approach is to efficiently allocate the computational resources to
the key modules of the encoder based on the perceptual
relevance of the regions in the frame [5].

ABSTRACT
H.264 is a computationally intensive video codec striving
for achieving the best quality for the compressed video.
The computational complexity poses as a challenge for
power-constrained applications. We present a system level
complexity reduction for H.264 video encoding by
allocating resources based on computational complexity
and quality trade-off. We develop a framework which
allocates the computational power of the encoder adaptive
to video contents and also scales with the available battery
power using a ROI classification method. Analysis is done
to profile the key modules of the encoder which can be
power-optimized while allocating resources. The results
of the encoder module analysis are combined with the
motion content analysis to obtain a power efficient
encoder parameter set which reduces the computations
and hence the power consumed. Our simulation results on
the JM H.264 framework confirm our hypothesis and
computational savings of more than 50% with quality
degradation less than 1% is achieved thereby extending
it’s feasibility for battery powered wireless devices.

In this paper we present a Region of Interest (ROI) based
resource allocation at the encoder on similar lines as proposed
by Yang et al in [5], with a more generic content and motion
adaptive power efficient encoder parameter selection framework
on H.264. The paper is henceforth structured in the following
manner. Section 2 covers the complexity analysis of the key
functional blocks of encoder, Section 3 covers our adaptive
parameter set selection process, Section 4 describes the
experimental setup and simulation results and we draw
conclusion from our work in Section 5.

2. COMPLEXITY ANALYSIS
In this section we first analyze the complexity of the key
functional blocks of the encoder. In particular we study the nonnormative part of the encoder, Motion Estimation (ME) and
Rate Distortion (RD) optimization, which are the key
enhancement modules incorporated in the standard, but are also
heavy on processor cycle consumption. Based on the analysis,
we then define a series of coding modes; each corresponding to
a different level of complexity which can be used for ROI based
resource allocation.

Index Terms — H.264 Video Encoding, Wireless
devices, ROI coding, power optimization.

2.1. Motion Estimation Profiling
H.264 performs motion estimation at different pixel resolution
(full, half and quarter pixel accuracy) and can have multiple
reference frames when encoding an inter frame. The distortion
metric used in ME and the search range are also some of the key
elements in ME. The computational complexity of the ME
process can be described as below.

1. INTRODUCTION
Multimedia applications such as real time video capture and
streaming in mobile wireless devices (e.g., video telephony)
have great potentials. H.264/AVC has rich tool sets to heavily
compress the video contents at desired quality and generate
packetized bit streams for such wireless transmission. This
efficiency comes at the cost of increased computational
complexity and demanding processing power requirements. The
battery technology has not progressed at the rate needed to meet
the computational power requirements of multimedia rich
wireless devices. In critical wireless applications such as combat
mission, battery-powered video sensors need to perform poweraware processing to maximize the information conveyed. This
problem can be addressed from various points of views: on the
architecture level such as designing low power processor
architecture, multi-cores with multiple hardware execution
engines, media instruction sets [1-3]; developing ASIC to
hardware-accelerate the encoder; employing multi-threaded
software, and a combination of these techniques [4]. Another

978-1-4244-2354-5/09/$25.00 ©2009 IEEE

­°
N Fullpel = N MEmetric * N 2 * (2 * SR + 1) 2 * N Ref
(1)
®
2
2
°̄N subpel = ( N MEmetric + N interp ) * N * (2 * SR + 1) * N Ref
where NFullpel and Nsubpel are integer pel and fraction pel full
search ME computations (an upper bound, in practice sub-pel
search is done at best full search pel location) for a MB of size
NXN, with search range SR and reference blocks NRef . NMEmetric
is the ME metric computation cost, Ninterp is the interpolation
cost for a MB in fraction pel ME. In H.264, the interpolation
filter uses 6 tap filter for half pel and 2 tap filter for quarter pel
ME.

925

ICASSP 2009

We analyzed the effect of the above parameter set on various
test sequences. As an example the results for one test sequence
(Foreman_CIF with baseline profile) are detailed below. Fig. 1
illustrates the complexity of encoding and the quality as a
function of the number of reference frames. It is found that for
sequences with moderate motion such as the Foreman sequence,
increasing the number of reference frames will not yield
significant quality improvement. However, the average encoding
time per inter frame almost linearly increases with the number of
reference frames, which is also expected from Eqn. (1).

RDcos t ( M * ) = arg min( D + λR )

This process includes ME, estimation of rate (DCTquantization, entropy coding and Inverse quantization, IDCT)
for all possible partition modes and requires high computational
power. The quality of the encoded sequence depends on whether
this optimization is done or not and is evident from the PSNR
gains in Fig 3. We found 5-10% increase in coding complexity
with RD optimization enabled. There are many research works
published to expedite the mode selection process and a few are
listed in [9, 10].

Encoder Complexity based on Number of
Reference Frames in ME
37.15

35

37.39

37.47

Encoder Complexity with RD Optimization

37.5
40

30
Quality & Complexity

Quality & Com plexity
Measures

40

25
20
15.18

15
10.32

10
5.45

5
1.50

0
1

5
10
Number of Reference Frames

PSNR (Y)

15

37.39

36.93

37.38

35
30
25
20
15
10

5.00

5.26

RD Off

RD On
RD Mode

Fig 1. ME Reference frames selection complexity on a test
Foreman CIF sequence (15fps, constant QP = 24). The Quality
metric is PSNR Y (dB) and the Complexity metric shown is
average encoding time per frame.

Quality & Complexity
Measures

37.35

4.74

SAD

SSE

Distortion Measure

PSNR (Y)
Avg Time/Frame

We partition the modes into the following mode sets in the
increasing order of complexity with ModeSet 0 being the least
complex set. The grouping is made take into account the overall
computation time in ME with a particular mode selected. Based
on video content, the percentage of modes selected from each
ModeSet varies. For example, in smooth regions ModeSet 0 will
be used and in foreground regions where there are objects
ModeSet 1 and 2 will be predominantly used.

37.39

4.88

RD On with skip

Fig 3. RD optimization complexity chart on Foreman CIF Test
sequence.

Encoder Complexity based on Distortion
Metric Selection
37.29

5.23

5
0

Encoding Time Per Frame

40
35
30
25
20
15
10
5
0

(2)

M

­
°
ModeSet 0 = { INTER 16 X 16 , SKIP , DIRECT }
°
ModeSet
1 = { INTER 16 X 8 , INTER 8 X 16 , INTER 8 X 8}
®
° ModeSet 2 = { INTRA 16 X 16 , INTRA 4 X 4 , INTER 8 X 4 ,
°
¯ INTER 4 X 8 , INTER 4 X 4}

5.72

Hadamard SAD
PSNR (Y)
Avg Time/Frame

(3)

3. ROI BASED ENCODER PARAMETER
SELECTION

Fig 2. ME Distortion Metric Selection complexity on Foreman
CIF sequence (15fps, constant QP = 24).
The distortion metric used in ME for block matching can be
SAD, SSE or Hadamard SAD. We observe from Fig. 2 that
Hadamard SAD yields the best matches in ME and hence the
best overall quality (PSNR values). The difference in encoding
time with each of the ME metrics can be captured in NMEmetric (1
ADD for SAD, 1MUL & 1 ADD for SSE). Since ME can take up
to 60% of encoding time, considerable research has been done
in this area and some are presented in articles [6-8].

In our previous work on Region Of Interest (ROI) based rate
control [11], we have proposed an algorithm that segments the
video into ROI and background based on motion analysis. ROI
segmentation can also be used to efficiently allocate
computational resources to region of importance and hence
minimize the complexities in the static background regions, as
done in [5]. In the following, we exploit this idea by combining
ROI-based encoding with a power scalable encoding parameter
set selection process.

2.2 RD Optimization Profiling

3.1 Adaptive Parameter Selection
Our ROI algorithm combines macro-blocks (MB) based on
Motion Vector and Distortion into foreground and background
regions and classifies them in different slice groups using the
explicit mode in Flexible Macroblock Ordering (FMO). The
slices so generated are independently decodable and can be

H.264 performs RD optimization with multiple coding modes
for Intra and Inter frames. The mode selection algorithm
computes the best possible mode (M*) from a set of partition
modes M, minimizing the overall cost using Lagrangian method
using the relation given below.

926

SRROI _ Slice = min((SRNon _ ROI _ Slice + floor(( FC − 1) * PF ) / 10),16)

efficiently coded with ROI-based rate control [11]. Also, we can
allocate more computational resources for encoding slice groups
which are critical such as ROI information. The static
background slice group which has little or no motion can be
coded with a restrictive parameter set to speed up the encoding
process. We describe two such implementations below.

N Re f = ceil (( FC * 0.75) + PF * 0.125)

For non ROI slice group:
­66 < PF < 100 : {ModeSet0,1,2}, Full , Half & Q _ PelME, HadamardSAD
°
®33 < PF < 66 : {ModeSet0,1}, Full & Half & Q _ PelME, SSE
°0 < PF < 33 : {ModeSet0}, FullPelME, SAD
¯

3.1.1. Parameter selection based on ROI

When the battery reaches critical state (<33% remaining), we
reduce the resolution of MV (for Non ROI slice) by performing
ME at integer pel accuracy. The distortion measure in ROI
classification will still recover the regions in the background
which might potentially get degraded due to the compromise
made above.

From our analysis in Section 2, we identify the encoder
parameter sets from the statistics obtained from a database of
video sequences. While coding the background slice group, we
pick a subset of block partition modes for RD optimization from
Eqn. (3) which are less expensive computationally and exclude
those coding modes which are complex from the coding
decision. Since background slice is with less or no motion,
excluding smaller partition modes (ModeSet 2) will not affect
quality much. Further we can restrict the search range for ME to
a smaller search space. The slice group with ROI is still coded
with all partition modes and higher search range to maintain
good perceptual quality.
RDP Performance Curve

25% power: 32.38 dB

38

50% power: 32.59 dB

37

PSNR Y (dB)

36
35

0.17 bpp

34

0.13 bpp
0.085 bpp

33
32
31
30
10

30

50

70

90

100

75% power: 32.96 dB
100% power: 33.04 dB
Fig 5. Subjective quality at different power levels for Foreman
CIF sequence frame 93 at 128 Kbps.

Power Available (%)

(a)
Computation Saving Plot

4. EXPERIMENTAL RESULTS

90.00%

% reduction in ME Time

80.00%

We have conducted simulations based on the JM 12.4
H.264/AVC encoder. The encoder is setup in the baseline
profile with two explicit slice grouped FMO and CAVLC
entropy coding. The number of reference frames is set to 5, and
the search range is 16, with RD optimization and loop filter
enabled, and fast full search ME method employed. All the test
video sequences are of CIF resolution with 4:2:0 sub-sampling
mode and encoding rate is 15 fps.
Fig. 4 shows a plot of Rate Distortion Power (RDP)
performance curve and computational savings (in terms of ME
time) in power scaled encoding mode for the Foreman sequence.
We observe from the plots that there is a tradeoff in quality and
power savings. The subjective quality is still maintained due to
our ROI based allocation process as observed from Figure 5. We
conducted simulations in both Constant Bit Rate (CBR) and
Variable Bit Rate (VBR) modes for several other sequences and
the results are tabulated in Tables 1 and 2 respectively. The
“Fixed” part is without ROI allocation and the “Adaptive” part
is with ROI based allocation at 100% power. We can see that in
sequences such as Hall and Foreman, where the activity is
localized, the ROI classification yields the best performance
which can be seen in the reduction in the encoding time
(translating into computation power saving). On the contrary in

70.00%
60.00%
0.17 bpp

50.00%

0.13 bpp
40.00%

0.085 bpp

30.00%
20.00%
10.00%
0.00%
10

30

50

70

90

100

Power Available (%)

(b)
Fig 4. Plot showing the performance of power scaled encoding
(a) RD Power Performance curve, (b) Computational savings,
for Foreman CIF sequence.

3.1.2. Power scaled parameter selection
To achieve power scalable encoding, the computational
resources must be allocated based on available battery power. In
addition, we should also consider the current frame complexity
while allocating the resources. We use two factors, Frame
Complexity factor (FC) derived from PSNR drop ratios [11] and
power factor (PF) (% battery remaining) to modulate the
encoder parameter set as below.

927

[11] Avin Kumar Kannur and Baoxin Li, "An Enhanced Rate Control
scheme with motion assisted slice grouping for low bit rate coding in
H.264", ICIP, 2008.

sequences such as coastguard, bus and mobile which have
global and/or camera motion, the total computational saving is
lower, but still significant at around 50%. The instantaneous
plots of PSNR at various power levels shown in Fig. 6 and 7
confirm a smooth variation of quality in the encoded sequence.

25%

PSNR Variation Plot

50%
75%

40

Original

5. CONCLUSIONS
PSNR Y (dB)

35

In this paper we have presented system level encoder complexity
reduction by adaptively selecting the parameter sets based on
video contents and available battery power. Simulation results
demonstrate power-scaled encoding efficiently allocates the
computational resources with ROI quality preserved. The
performance boost gained might be lower in real time video
encoders which use fast ME algorithms and are developed on
pipelined DSP with multithreaded software and hardware
accelerators which will be a future direction to probe further on
this topic. However, our approach can still be used for poweroptimizing the battery-powered wireless devices for real time
video capture and streaming applications.

30

25

85

80

75

70

65

60

55

50

45

40

35

30

25

20

15

5

10

0

20
Frame Number

Fig 6. PSNR variation plot for Stefan CIF sequence at different power
levels. Bit rate 256 Kbps, 15 fps with 3 GOPS.
50%

PSNR Variation Plot

75%
100%

39

6. REFERENCES

38
37
PSNR Y (dB)

[1] Tung-Chien Chen , Yu-Han Chen, Chuan-Yung Tsai, Sung-Fang
Tsai, Shao-Yi Chien, and Liang-Gee Chen, “2.8 to 67.2 mW low-power
and power-aware H.264 encoder for mobile applications,” Symposia on
VLSI Technology and Circuits, p 222-3, 2007.
[2] Steven Ge, Xinmin Tian and Yen-Kuang Chen, “Efficient
multithreading implementation of H.264 encoder on Intel hyperthreading architectures,” Information, Communications and Signal
Processing, vol 1, pages 469-473, 2003.
[3] Jui-Chin Chu, Wei-Chun Ku, Shu-Hsuan Chou, Tien-Fu Chen, and
Jiun-ln Guo, “An embedded coherent-multithreading multimedia
processor and its programming model,” 44th ACM/IEEE Design
Automation Conference, p 652-7, 2008.
[4] “Goto, K., Hatabu, A., Nishizuka, H., Matsunaga, K., Nakamura,
R., Mochizuki, Y., and Miyazaki, T, H.264 video encoder
implementation on a low-power DSP with low and stable computational
complexity,” IEEE Workshop on Signal Processing Systems Design
and Implementation, p 101-6, 2006.
[5] Yang Liu , Zheng Guo Li, and Yeng Chai Soh, “Region-of-interest
based resource allocation for conversational video communication of
H.264/AVC,” IEEE Transactions on Circuits and Systems for Video
Technology, v 18, n 1, p 134-9, Jan. 2008.
[6] Koziri, M.G, Dadaliaris, A.N., Stamoulis, G.I., and Katsavounidis,
I.X, “A novel low-power motion estimation design for H.264,” IEEE
18th International Conference Application-specific Systems,
Architectures and Processors, p 247-52, 2007.
[7] Tung-Chien Chen, Yu-Han Chen, Chuan-Yung Tsai, and Liang-Gee
Chen, “Low power and power aware fractional motion estimation of
H.264/AVC for mobile applications,” IEEE International Symposium on
Circuits and Systems, p 4 pp, 2006.
[8] Tung-Chien Chen, Yu-Han Chen, Sung-Fang Tsai, Shao-Yi Chien,
and Liang-Gee Chen, “Fast algorithm and architecture design of lowpower integer motion estimation for H.264/AVC,” IEEE Transactions
on Circuits and Systems for Video Technology, v 17, n 5, p 568-77,
May 2007.
[9] Yongsu Jo, Yong-Goo Kim, and Yungho Choi, “Fast mode decision
algorithm using optimal mode predictions for H.264-based mobile
devices,” International Conference on Consumer Electronics, p 2 pp,
2007.
[10] Hyungjoon Kim, and Altunhasak, Y, “Low-complexity
macroblock mode selection for H.264-AVC encoders,” ICIP, p 765-8
Vol.2, 2004.

36
35
34
33
32
31
30

96

90

84

78

72

66

60

54

48

42

36

30

24

18

6

12

0

29

Frame Number

Fig 7. PSNR variation plot for Foreman CIF sequence at
different power levels. Bit rate 128 Kbps, 15 fps with 3 GOPS.
Table 1. CBR simulation results. (Rate: 256 Kbps, 100 frames, 15fps)

Sequence

Encoding Time (s)

Foreman
Stephan
Coastguard

Fixed
617.53
560
624.55

Mobile

621.35

Hall
Bus

621.68
628.56

Adaptive
332.2
420.8
315.55

PSNR (Y) dB
Fixed
37.65
31.71
30.43

Adaptive
37.32
31.22
30.21

338.06

29.62

29.27

356.37
392.71

39.39
30.51

39.3
30.14

Table 2. Variable Bit Rate (VBR) simulation results. (QP_I=24,
QP_P=28, 100 frames, 15fps, CIF 4:2:0)

Sequence

PSNR (Y) dB

Fixed

Adaptive

Fixed

Adaptive

636
641.14

308.71
359.55

38
36.78

37.99
36.73

Coastguard

650.06

396.83

35.79

35.76

Mobile

712.59

442.77

35.68

35.65

Hall

598.55

309.53

38.68

38.66

Bus

676.5

456.75

36.15

36.11

Foreman
Stephan

928

Encoding Time (s)

IEEE SIGNAL PROCESSING LETTERS, VOL. 23, NO. 4, APRIL 2016

449

Compressive Sensing Reconstruction of Correlated
Images Using Joint Regularization
Kan Chang, Pak Lun Kevin Ding, and Baoxin Li, Senior Member, IEEE
Abstract—This letter proposes a novel compressive sensing
reconstruction method for correlated images by using joint regularization, where a compensation-based adaptive total variation (CATV) regularization and a multi-image nonlocal low-rank
(MNLR) regularization are included. In CATV, local weights are
assigned to the residual values in the gradient domain so as to
constrain the regularization strength at each pixel. In MNLR, the
search of similar patches goes across different images so that both
self-similarity and inter-image similarity are explored. Afterward,
an efficient algorithm is proposed to solve the joint formulation,
using a Split-Bregman-based technique. The effectiveness of the
proposed approach is demonstrated with experiments on both
multiview images and video sequences.
Index
Terms—Compressive
sensing,
motion
estimation/disparity
estimation
(ME/DE),
nonlocal
low-rank
regularization (NLR), total variation.

I. I NTRODUCTION

T

HIS LETTER focuses on the compressive sensing (CS)
reconstruction of a set of correlated images, each of which
is independently acquired by the CS technique [1], [2]. The correlated images could be multiview images which represent a
scene from different view points, or a series of video frames
which are taken at different time points. More specifically, the
CS measurement of the original ith image is acquired by
y i = Φi u i

(1)

where ui ∈ RN stands for the original ith image, yi ∈ RM
is the measurement of the ith image, and Φi ∈ RM ×N is the
measurement matrix. Usually, M << N , and we call M/N the
subrate of CS.
To reconstruct the underlying images from such an underdetermined system, one common way is to employ image
Manuscript received December 13, 2015; revised February 03, 2016;
accepted February 04, 2016. Date of publication February 11, 2016; date of
current version March 03, 2016. This work was supported by the Natural
Science Foundation of China under Grants 61401108 and 61261023, and the
Natural Science Foundation of Guangxi under Grant 2013GXNSFBA019272.
The work of B. Li was supported by the Natural Science Foundation under
Grants 1135616 and 0845469. The associate editor coordinating the review of
this manuscript and approving it for publication was Prof. Deanna Needell.
K. Chang is with the School of Computer and Electronic Information,
the Guangxi Key Laboratory of Multimedia Communications and Network
Technology (Cultivating Base), and the Key Laboratory of Multimedia
Communications and Information Processing, Guangxi University, Nanning
530004, China (e-mail: changkan0@gmail.com).
P. L. Kevin Ding and B. Li are with the Department of Computer Science
and Engineering, Arizona State University, Tempe, AZ 85287 USA (e-mail:
kevinding@asu.edu; baoxin.li@asu.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/LSP.2016.2527680

prior knowledge for regularizing the solution to the following
minimization problem:
ûi = argmin Ψ(ui )
ui

s.t. yi = Φi ui

(2)

where Ψ is the regularization term denoting image prior.
Different types of intra-image-based regularization have been
investigated, including total variation (TV) minimization [3],
nonlocal low-rank regularization (NLR) [4], nonlocal meansbased regularization [5], autoregressive model [6], dictionary
learning-based sparse representation [7], etc.
Besides intra-image prior information, inter-imagestructured sparsity also needs to be explored for correlated
images. The most direct way to do this is to require a joint
sparsity of the whole image set, such as [8]–[11]. However,
such methods are sensitive to motion or disparity. Improvement
may be obtained by using the neighboring images to help
reconstruct the current one, such as [12]–[20]. In [21], correlation between a pair of images was directly estimated in
the compressed domain so as to reduce the computational
complexity.
The main contributions of this letter are listed as follows.
First, we propose a compensation-based adaptive TV regularization approach, where the reliability of each compensated
pixel is considered. Second, we extend the existing NLR from
single-image pattern to multiple-image pattern. Note that different from the similar model in [22], we additionally use optical
flow (OF) fields to guide the central points of search windows.
Finally, we jointly incorporate these two types of regularization
into a minimization problem and design an optimization algorithm for CS reconstruction of correlated images. Experiments
show that our proposed algorithm is capable of achieving
significantly better reconstruction than several state-of-the-art
methods. To facilitate evaluation and further exploration of the
proposed algorithm, we will publish the source code on the
third author’s webpage.1
II. P ROPOSED J OINT R EGULARIZATION
A. Compensation-Based Adaptive TV Regularization
To explore inter-image correlation, we can utilize disparity estimation/disparity compensation (DE/DC) for multiview
images or motion estimation/motion compensation (ME/MC)
for video sequences. As have been proved in [17]–[20], requiring small prediction error in the gradient domain is able to get
satisfactory results. In [17], the compensation-based TV (CTV)
regularization term was written as
ΨCTV (ui ) = D(ui − si )1
1 [Online].

Available: http://www.public.asu.edu/~bli24/

1070-9908 © 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

(3)

450

IEEE SIGNAL PROCESSING LETTERS, VOL. 23, NO. 4, APRIL 2016

where D = [DTh , DTv ]T with Dh , Dv denoting the horizontal
and vertical finite difference operators, respectively, and
si =

1
(Fi−1 ui−1 + Bi+1 ui+1 )
2

(4)

with Fi−1 and Bi+1 standing for the forward and backward
compensation operators by using the optical flow fields of the
(i − 1)th and (i + 1)th images, respectively. We only used
the closest images to build the compensated results because:
1) usually a further distance between two images leads to a
less accurate compensated image and 2) the computational
complexity of estimating the OF fields is high.
Unfortunately, such an si is not always reliable on images
with fine details, multiview images with large disparity, or video
sequences with complex motion. Therefore, simply minimizing
the regularization term (3) may sometimes deteriorate the quality of reconstructed images. Considering the reliability at each
pixel in si , we propose to add local weights to the residual values in the gradient domain, leading to a compensation-based
adaptive TV (CATV) regularization
ΨCATV (ui ) = Wi  (D(ui − si ))1

(5)

where Wi denotes the vector of local weights, and  is the
Hadamard product. If an unreliable predicted value occurs at a
pixel position in si , a small weight should be assigned to it.
To appropriately build Wi , a good spatial information indicator is needed. Here, the second derivative-based indicator called
difference curvature [23] is utilized. It is a pixel-based indicator which can effectively discriminate edges from flat regions
or noises. For the mth pixel, it is defined as
Cm = ||eηη | − |e ||
eηη =
e =

e2x exx

+ 2ex ey exy +
e2x + e2y

(6)
e2y eyy

e2y exx + 2ex ey exy + e2x eyy
e2x + e2y

(7)
(8)

where ex and ey represent the first-order gradients of the pixel
along x (horizontal) and y (vertical) directions, respectively;
exx , eyy , and exy are the second-order gradients of the pixel.
With Cm , the mth local weight in Wi is computed as
wm =

1
1 + θCm

(9)

where θ is the contrast factor.
It should be noted that our Wi is designed for residual in
the gradient domain, which is different from other works such
as [24] and [25] (i.e., Cm is computed on (ui − si ) instead of
ui ). In practice, since the original images are not available, the
previously reconstructed images are needed to calculate Wi .

Fig. 1. Example of intra-image and inter-image search. OF fields are used to
guide the central points of search windows in adjacent images. The sizes of
search windows in different images are the same.

Xj . It is reasonable to expect the formed matrix Xj has a lowrank property (in practice, Xj is only approximately low rank
due to noises and artifacts [4]). Hence for the ith image, NLR
is calculated as

 1
2
R̂j ui − Lj F + λRank(Lj )
(10)
ΨNLR (ui ) =
2
j
where R̂j stands for extracting similar patches for the jth exemplar patch, i.e., Xj = R̂j ui ; λ is a tradeoff parameter; The
newly introduced low-rank matrix Lj is close to Xj .
For correlated images, just exploring nonlocal low-rank
property inside an image is not enough. Therefore, we propose
to extend (10) to a new multi-image nonlocal low-rank (MNLR)
regularization, which is written as
ΨMNLR (U) =

1
i

j

2

R̃i,j U − Li,j 2F + λRank(Li,j )



(11)

where U = [uT1 , uT2 , . . . , uTn ]T , and n is the number of correlated images. Different from R̂j in (10), for the jth exemplar
patch in the ith image, operator R̃i,j extracts its similar patches
from the (i − 1)th, ith, and (i + 1)th images.
To find similar patches in three neighboring images, both
intra-image and inter-image searches are needed. Fig. 1 illustrates how the search works. Recall that we have obtained OF
fields when calculating si in (4). Thus, here they can be reused
to guide the central points of search windows in the (i − 1)th
and (i + 1)th images. More specifically, given a central point of
a window in the ith image, we locate its motion/disparity vectors in the OF fields and use these vectors to find where this
point is in the adjacent images. This procedure helps to find the
most similar patches, especially in cases where large disparity
or motion occurs among neighboring images.
Note that higher quality of reconstruction can be achieved if
the search goes across more images. However, doing so would
cause heavier computational burden. Through experiments, we
found that searching in three continuous images has achieved
significant improvement over single-image NLR [4]. In addition, we only have the OF fields for each pair of adjacent
images, which means the guidance for the central points of
search windows is not available in further images.

B. Multi-Image Nonlocal Low-Rank Regularization
NLR [4] is an efficient tool for describing single image
characteristics. To apply it, a target image is first divided into
overlapped patches with size Sp × Sp . As a similarity metric, the l2 differences between the jth exemplar patch and the
candidate patches within a search window are computed. After
that, Np similar patches are found and grouped into a matrix

III. O PTIMIZATION A LGORITHM FOR CS
R ECONSTRUCTION U SING J OINT R EGULARIZATION
Since CATV and MNLR impose different prior knowledge
within correlated images, jointly considering them can get satisfactory results. By doing so, the minimization problem for
reconstructing a set of correlated images becomes

CHANG et al.: CS RECONSTRUCTION OF CORRELATED IMAGES USING JOINT REGULARIZATION

TABLE I
M EAN PSNR S ( D B) C OMPARISON FOR M ULTIVIEW I MAGES
R ECONSTRUCTION

TABLE II
M EAN PSNR S ( D B) C OMPARISON FOR V IDEO S EQUENCES
R ECONSTRUCTION

451

Since rank-minimization problem is NP-hard, here, we
have replaced Rank(Li,j ) in (11) with L(Li,j , ε), which is a
log det(·) surrogate function [26] and



1/2
+ εI
(13)
L(Li,j , ε) = log det Li,j LTi,j
where ε is a small constant and I denotes the identity matrix.
We use log det(·) here because it can better approximate rank
than the widely used nuclear norm [4].
To efficiently solve problem (12), we introduce a new
variable d and let d = D̃(U − CU). Then, our joint
regularization-driven Split-Bregman iteration [27] can be
written as

⎧
 
⎪
Lk+1
= argmin{Li,j } i j (λL(Li,j , ε)
⎪
i,j
⎪
⎪
⎪
⎪
⎪
+ 12 R̃i,j Uk − Li,j 2F )
⎪
⎪
⎪
⎪
⎪
⎪
Uk+1 = argminU Y − HU22
⎪
⎪
⎪
⎪
 
⎪
2
⎨
+β i j (R̃i,j U − Lk+1
i,j F )
(14)
⎪
⎪
+γdk − D̃(U − CU) − bk 22
⎪
⎪
⎪
⎪
⎪
⎪
dk+1 = argmind γ2 d − D̃(I − C)Uk+1 − bk 22
⎪
⎪
⎪
⎪
⎪
⎪
+αW̃  d1
⎪
⎪
⎪
⎪
⎩ k+1
k
k+1
k+1
k+1
b

= b + D̃(U

− CU

)−d

where γ is a tradeoff parameter.
When considering the “L” subproblem, we treat every Li,j
separately, and the solution can be written as [4]
k
T
Lk+1
i,j = Q(Σ − λ diag(g ))+ V

(15)

where QΣVT is the singular value decomposition (SVD) of
R̃i,j Uk , and thin SVD is applied in our implementation. glk =
1/(σlk + ε), σlk denotes the lth singular value of Lki,j , and
(x)+ = max(x, 0).
When solving the “U” subproblem, we use CUk to approximate CUk+1 and get the closed-form solution as follows:
⎛
⎞−1


T ⎠
Uk+1 = ⎝HT H + β
R̃T
i,j R̃i,j + γ D̃ D̃
⎛

1
{Û, {L̂i,j }} = argmin Y − HU22
U,{Li,j } 2
+ αW̃  D̃(U − CU)1

  1
R̃i,j U − Li,j 2F + λL(Li,j , ε) (12)
+β
2
i
j
where α, β, and λ are tradeoff parameters, Y = [y1T , y2T ,
. . . , ynT ]T , H = diag(Φ1 , Φ2 , . . . , Φn ), W̃ = [W1T , W2T , . . . ,
WnT ]T D̃ = diag(D, D, . . . , D), and
⎡
⎤
0
B2
0
···
0
0
B3 /2
···
0 ⎥
⎢F1 /2
⎢ .
.. ⎥
.
.
.
⎢
⎥
..
..
..
C = ⎢ ..
. ⎥.
⎣ 0
0
Bn /2⎦
· · · Fn−2 /2
0
0
···
0
Fn−1

i

j

⎝γ D̃T (dk + D̃CUk − bk ) + HT Y + β


i

j

⎞
k+1 ⎠
.
R̃T
i,j Li,j

(16)
To compute (16), conjugate gradient (CG) method is used.
When dealing with the “d” subproblem, closed-form solution by the shrinkage formula [28] is given as
dk+1 = shrink(D̃(I − C)Uk+1 + bk , αW̃/γ).

(17)

With a vector x and a threshold Ts , we have
shrink(x, Ts ) = max(|x| − Ts , 0)  sgn(x).

(18)

Note that the max operator here is implemented for each
spatial index independently.
Our algorithm, named joint regularization-based compressive sensing reconstruction (JR-CSR), is summarized as

452

IEEE SIGNAL PROCESSING LETTERS, VOL. 23, NO. 4, APRIL 2016

Fig. 2. Visual quality comparison for the ninth frame in City (subrate = 0.20). From left to right: DC-TV, DC-JTV, NLR-CS, JM-RCI, JR-CSR.

Algorithm 1. JR-CSR
Input: Ũ0 , H, Y, α, β, λ, γ
Outer loop for t = 0, 1, . . . , T
Use Ũt to update C and {R̃i,j }.
If t ≤ T0 W̃ = [1, 1, . . . 1]T Else Update W̃ by (9)
Set d0 = D̃(I − C)Ũt , b0 = 0, U0 = Ũt
Inner loop for k = 0, 1, . . . , K
Update each Lk+1
i,j by (15).
k+1
Update U
by using CG method to solve (16).
Update dk+1 via (17).
bk+1 = bk + D̃(Uk+1 − CUk+1 ) − dk+1
Set Ũt+1 = Uk+1
If Uk+1 − Uk 2 /Uk+1 2 < 10−4 Break
End for
End for
return Û = ŨT +1
Algorithm 1. Ũt and Uk denote the results in the outer loop
and the results in the inner loop, respectively. To get an accurate result, C, W̃, and {R̃i,j } are updated several times in the
outer loop. Note that in the first T0 iterations, W̃ is fixed. This
was found empirically to be able to improve the convergence
while leading to a better result. The inner loop will stop if the
relative change of U is smaller than a predefined threshold, or
the maximum number of iterations is reached.
IV. E XPERIMENTAL R ESULTS
This section evaluates the performance of JR-CSR. All
experiments were performed in MATLAB 2013b on a Lenovo
computer with Intel(R) Core(TM) i7-4790 processor, 8.00G
memory. Structurally random matrices (SRM) [29] were used
as Φi , and we had the measurement yi = Φi ui . To generate
Ũ0 , TVAL3 [30] software2 was utilized for each image. To
construct C, OF implementation3 of [31] was used. In this OF
implementation, successive over-relaxation (SOR) was applied
to solve the linear system resulting from the energy functional
of [32], where the assumptions of brightness constancy, gradient constancy, and piecewise smooth flow field were combined.
T , K, and T0 were set to 15, 25, and 5, respectively. θ in (9)
was set to 0.8, patch size Sp × Sp for MNLR was 6 × 6, and
the number of similar patches Np was 45. α, β, λ, and γ were
tuned according to each subrate.
The test datasets included four multiview image sets [half
size Monopoly (MP), Tsukuba (TK), Venus and Art] from
2 [Online].
3 [Online].

Available: http://www.caam.rice.edu/~optimization/L1/TVAL3/
Available: http://people.csail.mit.edu/celiu/OpticalFlow/

TABLE III
AVERAGE CPU T IME ( S ) FOR R ECONSTRUCTING O NE F RAME IN Foreman

the middlebury multiview database4 and four video sequences
[352 × 288 Foreman (FM), Football (FB), City, and Bus]. We
tested the first five views of each multiview image set and the
first 20 frames of each video sequence. Only grayscale images
were considered. Four state-of-the-art algorithms were compared, including NLR-CS [4], JM-RCI [17], DC-JTV [20], and
DC-TV [19]. For fair comparison, TVAL3 [30] was applied to
obtain the initial results for all competing algorithms.
The average peak signal-to-noise ratio (PSNR) results of
each multiview image set and each video sequence are given
in Tables I and II, respectively. One can see that JR-CSR beats
all the benchmark methods in all cases. For example, in Table I
at a subrate of 0.10, JR-CSR gets 3.05 dB PSNR improvement
over the second best method, i.e., NLR-CS. For visual quality
comparison, please see Fig. 2.
The average running times for reconstructing one frame in
Foreman are listed in Table III. We can find that JR-CSR is the
slowest algorithm. The main computational burdens are introduced by iteratively updating {Li,j }, U, and C. For each Li,j ,
the complexity of thin SVD is O(Sp2 Np r), where r is the rank
of R̃i,j U. To update U and C, CG and SOR are used to solve
the related linear systems, respectively. Given a linear system
Ax = z, assume that Nn is the number of nonzero entries in
matrix A, and Nc is the condition number of A. To get the
√ vector x, the complexity of one iteration of CG is O(Nn Nc ),
while O(Nn ) is required for one iteration of SOR. To speed
up JR-CSR, parallelization techniques may be the best choice.
Other solutions, such as updating C in JR-CSR only once,
removing either CATV or MNLR from (12), etc., would lead
to different levels of quality loss.
V. C ONCLUSION
In this letter, we proposed two types of regularization, including CATV and MNLR, for CS reconstruction of correlated
image sets. After incorporating the two regularization terms
into the minimization problem, we designed an optimization
algorithm called JR-CSR. Through experiments, we found that
JR-CSR is able to deliver the best performance among all the
tested methods, which demonstrates the effectiveness of the
proposed joint regularization.
4 [Online].

Available: http://vision.middlebury.edu/stereo/data

CHANG et al.: CS RECONSTRUCTION OF CORRELATED IMAGES USING JOINT REGULARIZATION

R EFERENCES
[1] D. L. Donoho, “Compressed sensing,” IEEE Trans. Inf. Theory, vol. 52,
no. 4, pp. 1289–1306, Apr. 2006.
[2] E. J. Candès, J. Romberg, and T. Tao, “Robust uncertainty principles:
Exact signal reconstruction from highly incomplete frequency information,” IEEE Trans. Inf. Theory, vol. 52, no. 2, pp. 489–509, Feb.
2006.
[3] L. Rudin, S. Osher, and E. Fatemi, “Nonlinear total variation based
noise removal algorithms,” Phys. D: Nonlinear Phenom., vol. 60, no. 1,
pp. 259–268, 1992.
[4] W. Dong, G. Shi, X. Li, Y. Ma, and F. Huang, “Comressive sensing via
nonlocal low-rank regularization,” IEEE Trans. Image Process., vol. 23,
no. 8, pp. 3618–3612, Aug. 2014.
[5] J. Zhang, S. Liu, D. Zhao, R. Xiong, and S. Ma, “Improved total variation
based image compressive sensing recovery by nonlocal regularization,”
in Proc. IEEE Int. Symp. Circuits Syst. (ISCAS), 2013, pp. 2836–2839.
[6] X. Wu, W. Dong, X. Zhang, and G. Shi, “Model-assisted adaptive recovery of compressed sensing with imaging applications,” IEEE Trans.
Image Process., vol. 21, no. 2, pp. 451–458, Feb. 2012.
[7] W. Dong, G. Shi, X. Li, L. Zhang, and X. Wu, “Image reconstruction
with locally adaptive sparsity and nonlocal robust regularization,” Signal
Process.: Image Commun., vol. 27, pp. 1109–1122, 2012.
[8] J. Ma, G. Plonka, and M. Y. Hussaini, “Compressive video sampling
with approximate message passing decoding,” IEEE Trans. Circuits Syst.
Video Technol., vol. 22, no. 9, pp. 1354–1364, Sep. 2012.
[9] C. Li, H. Jiang, W. Paul, and Y. Zhang, “A new compressive video sensing
framework for mobile broadcast,” IEEE Trans. Broadcast., vol. 59, no. 1,
pp. 197–205, Mar. 2013.
[10] M. Hosseini and K. N. Plataniotis, “High-accuracy total variation with
application to compressed video sensing,” IEEE Trans. Image Process.,
vol. 23, no. 9, pp. 3869–3884, Sep. 2014.
[11] P. Nagesh and B. Li, “A compressive sensing approach for expressioninvariant face recognition,” in Proc. IEEE Int. Conf. Comput. Vis. Pattern
Recognit. (CVPR), 2009, pp. 1518–1525.
[12] T. T. Do, Y. Chen, D. T. Nguyen, N. Nguyen, L. Gan, and T. D. Tran,
“Distributed compressed video sensing,” in Proc. Int. Conf. Image
Process. (ICIP), 2009, pp. 1393–1396.
[13] J. Nebot, Y. Ma, and T. Huang, “Distributed video coding using compressive sampling,” in Proc. Picture Coding Symp. (PCS), 2009, pp. 1–4.
[14] L. W. Kang and C. S. Lu, “Distributed compressive video sensing,”
in Proc. Int. Conf. Acoust. Speech Signal Process. (ICASSP), 2009,
pp. 1169–1172.
[15] V. Thirumalai and P. Frossard, “Distributed representation of geometrically correlated images with compressed linear measurements,” IEEE
Trans. Image Process., vol. 21, no. 7, pp. 3206–3218, Jul. 2012.
[16] Y. Liu, M. Li, and A. Dimitris, “Motion-aware decoding of compressedsensed video,” IEEE Trans. Circuits Syst. Video Technol., vol. 23, no. 3,
pp. 438–444, Mar. 2013.

453

[17] K. Chang and B. Li, “Joint modeling and reconstruction of a
compressively-sensed set of correlated images,” J. Visual Commun.
Image Represent., vol. 33, pp. 286–300, 2015.
[18] K. Chang, T. Qin, W. Xu, and Z. Tang, “Reconstruction of multi-view
compressed imaging using weighted total variation,” Multimedia Syst.,
vol. 20, no. 4, pp. 363–378, 2014.
[19] M. Trocan, E. W. Tramel, J. E. Fowler, and B. Pesquet, “Compressedsensing recovery of multiview image and video sequences using signal
prediction,” Multimedia Tools Appl., vol. 72, no. 1, pp. 95–121, 2014.
[20] Y. Liu, C. Zhang, and J. Kim, “Disparity-compensated total-variation
minimization for compressed-sensed multiview image reconstruction,” in
Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP), 2015,
pp. 1458–1462.
[21] V. Thirumalai and P. Frossard, “Correlation estimation from compressed
images,” J. Visual Commun. Image Represent., vol. 24, no. 6, pp. 649–
660, 2013.
[22] H. Yoon, K. S. Kim, D. Kim, Y. Bresler, and J. C. Ye, “Motion adaptive patch-based low-rank approach for compressed sensing cardiac cine
MRI,” IEEE Trans. Med. Imag., vol. 33, no. 11, pp. 2069–2085, Nov.
2014.
[23] Q. Chen, P. Montesinos, Q. Sun, P. Heng, and D. Xia, “Adaptive total
variation denoising based on difference curvature,” Image Vis. Comput.,
vol. 28, no. 3, pp. 298–306, 2010.
[24] W. Dong, X. Yang, and G. Shi, “Compressive sensing via reweighted
TV and nonlocal sparsity regularisation,” Electron. Lett., vol. 49, no. 3,
pp. 184–186, 2013.
[25] E. J. Candès, M. B. Wakin, and S. P. Boyd, “Enhancing sparsity by
reweighted l1 minimization,” J. Fourier Anal. Appl., vol. 14, no. 5,
pp. 877–905, 2008.
[26] M. Fazel, H. Hindi, and S. P. Boyd, “Log-det heuristic for matrix
rank minimization with applications to hankel and euclidean distance
matrices,” in Proc. Amer. Control Conf., 2003, pp. 2156–2162.
[27] T. Goldstein and S. Osher, “The split bregman method for l1 regularized
problems,” SIAM J. Imag. Sci., vol. 2, no. 2, pp. 323–343, 2009.
[28] E. T. Hale, W. Yin, and Y. Zhang, “Fixed-point continuation for l1minimization: Methodology and convergence,” SIAM J. Optim., vol. 19,
no. 3, pp. 1107–1130, 2008.
[29] T. T. Do, L. Gan, N. H. Nguyen, and T. D. Tran, “Fast and efficient compressive sensing using structurally random matrices,” IEEE Trans. Signal
Process., vol. 60, no. 1, pp. 139–154, Jan. 2012.
[30] C. Li, W. Yin, H. Jiang, and Y. Zhang, “An efficient augmented lagrangian
method with applications to total variation minimization,” Comput.
Optim. Appl., vol. 56, no. 3, pp. 507–530, 2013.
[31] C. Liu, “Beyond pixels: Exploring new representations and applications for motion analysis,” Ph.D. dissertation, Department of Electrical
Engineering and Computer Science, Massachusetts Inst. Technol.,
Cambridge, MA, USA, 2009.
[32] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert, “High accuracy optical flow estimation based on a theory for warping,” in Proc. Eur. Conf.
Comput. Vis. (ECCV), 2004, pp. 25–36.

2016 IEEE Conference on Computer Vision and Pattern Recognition

PPP: Joint Pointwise and Pairwise Image Label Prediction
Yilin Wang1 Suhang Wang1 Jiliang Tang2 Huan Liu1 Baoxin Li1
1
Department of Computer Science, Arizona State Univerity
2
Yahoo Research
{yilinwang,suhang.wang,huan.liu,baoxin.li}@asu.edu

jlt@yahoo-inc.com

Abstract
Pointwise label and pairwise label are both widely used
in computer vision tasks. For example, supervised image classiﬁcation and annotation approaches use pointwise
label, while attribute-based image relative learning often
adopts pairwise labels. These two types of labels are often considered independently and most existing efforts utilize them separately. However, pointwise labels in image
classiﬁcation and tag annotation are inherently related to
the pairwise labels. For example, an image labeled with
“coast” and annotated with “beach, sea, sand, sky” is more
likely to have a higher ranking score in terms of the attribute
“open”; while “men shoes” ranked highly on the attribute
“formal” are likely to be annotated with “leather, lace up”
than “buckle, fabric”. The existence of potential relations
between pointwise labels and pairwise labels motivates us
to fuse them together for jointly addressing related vision
tasks. In particular, we provide a principled way to capture the relations between class labels, tags and attributes;
and propose a novel framework PPP(Pointwise and Pairwise image label Prediction), which is based on overlapped
group structure extracted from the pointwise-pairwise-label
bipartite graph. With experiments on benchmark datasets,
we demonstrate that the proposed framework achieves superior performance on three vision tasks compared to the
state-of-the-art methods.

Figure 1. An Illustrative Example of poinwise labels and pairwise
labels. Pointwise label “4 door” is better than the pairwise label to
describe presence of 4 door in a car, while “sporty” is better to use
pairwise label to describe the car style, as the right is more sporty
than the left. For example it is hard to label the middle (we ask
10 human viewer – 40% agree with the non sporty and 60% agree
with sporty, but 100% agree with middle one is more sporty than
the left one and less sporty than right one).

ﬁcation often capture high-level image content, while tags
in tag annotation are likely to describe a piece of information in the image, such as “high heel, buckle, leather” in a
shoe image. In [21], these two tasks are considered together
because the labels and tags may have some relations in an
image. Recently, due to the semantic gap between lowlevel image features and high-level image concepts, human
nameable visual attributes are proposed to solve the vision
tasks[7, 14, 1, 13]. However, for a large variety of attributes,
the pointwise binary setting is restrictive and unnatural. For
example, it is very difﬁcult to assign or not assign “sporty”
to the middle car in Figure 1 because different people have
different opinions. Thus, pairwise approaches [17, 11, 12]
have been proposed, which aim to learn a ranking function
to predict the attribute strength for images. For example, in
Figure 1. most of the people would agree that the middle
car is more “sporty” than the left one and less “sporty” than
the right one
Pointwise and pairwise labels have their own advantages
as well as limitations in terms of labeling complexity and
representational capability. Labeling complexity: given 10
images, we only need 10 sets of class categories/tags. However, we need to label at least 45 image pairs to capture the
overall ordering information. (Although the ranking rela-

1. Introduction
The increasing popularity of social media generates massive data at an unprecedented rate. The ever-growing number of images has brought new challenges for efﬁcient and
effective image analysis tasks, such as image classiﬁcation,
annotation and image ranking. Based on the types of labels, we can roughly divide the supervised vision tasks into
two categories – pointwise label based approaches and pairwise label based approaches. Pointwise approaches adopt
pointwise labels such as image categories or tags as training targets [10, 20, 6, 19, 8, 23, 24]. Class labels in classi1063-6919/16 $31.00 © 2016 IEEE
DOI 10.1109/CVPR.2016.646

6005

tion is considered as transferable, e.g. A  B&B  C ⇒
A  C). Representational capability: pointwise labels
such as tags/class labels imply the presence of content properties such as whether a shoe is made of leather, contains a
heel, buckle, etc. While pairwise labels capture the relations in a same property, e.g., A has a higher heel than B.
Solely relying on pointwise labels may cause ambiguity or
produce noisy data for the models as in the example of assigning “sporty” to the middle car in Figure 1, while only
using pairwise labels may also cause problems when the
images have very similar properties.
As pointwise and pairwise labels encapsulate information of different types and may have different beneﬁts for
vision problems and recommendation systems[22] , we develop a new framework for fusing different types of training
data by capturing their underlying relations. For example,
in Figure 2, the tags, “leather, cognac, lace up” may suggest
the left shoe with a higher score on the “formal” attribute,
while the “high heel” may indicate the right shoe with a
lower score on the “comfort” attribute. On the other hand,
the higher score on “formal” and “comfort” with tag “Oxford” could help label the left image as “shoe” and enable
the rare tag annotation such as “wingtip”. To the best of
our knowledge, there are only a few recent works that fused
pointwise and piarwise labels [18, 4]. However, they simply combined regression and ranking in the loss functions
for ranking tasks and totally ignored the relations between
pointwise labels and pairwise labels.
In this paper, we investigate the problem of fusing pointwise and pairwise labels by exploiting their underlying relations for joint pointwise label prediction such as, image
classiﬁcation and annotation, and pairwise label prediction,
e.g., relative ranking. We derive a uniﬁed bipartite graph
model to capture the underlying relations among two types
of labels. Since traditional approaches cannot take advantages of relations among pointwise and pairwise labels,
we proceed to study two fundamental problems: (1) how
to capture relations between pointwise and pairwise labels
mathematically; and (2) how to make use of the relations
for jointly addressing vision tasks. These two problems are
tackled by the propose framework PPP and our contributions are summarized as follows:

In the remaining of the paper, we ﬁrst give a formal
problem deﬁnition and basic model in Section 2. Then the
proposed framework and an optimization method for model
learning is presented in Section 3. Experiments and results
are demonstrated in Section 4, with further discussion in
section 5.

2. The Proposed Method
Before detailing the proposed framework, we ﬁrst introduce notations used in this paper. We use X ∈ Rn×d
to denote a set of images in the database where n is the
number of images and d is the number of features. Note
that there are various ways to extract features such as SIFT,
Gist or the features learned via deep learning frameworks.
Let Yt ∈ Rn×c1 and Yc ∈ Rn×c3 be the data-tag and
data-label matrices which represent the pointwise labels.
Y(i, j) = 1 if the i-th image is annotated/classiﬁed with
j-th tag/class label, Y(i, j) = 0 otherwise. Given a ﬁxed
training set D, a candidate pair set P can be drawn. The
pair set implied by the ﬁxed training set D uses pairwise
labels. In the proposed framework, given a pair of images < a, b > on the attribute q, if ya  yb , then a has
a positive attribute score y(a, q, 1) = |ya − yb |, and a negative score y(a, q, 2) = 0; while b has a positive attribute
y(b, q, 1) = 0, and a negative score y(b, q, 2) = |ya − yb |.
Thus, the pairwise label is deﬁned as Yr ∈ Rm×c2 , where
m is the number of pairs drawn from training samples and
c2 = 2q where q is the number of attributes. For example, let < a, b > be the ﬁrst pair, the pairwise label
Yr (1, 2(q − 1) + 1) represents how likely the ya  yb and
Yr (1, 2(q − 1) + 2) represents how likely ya ≺ yb on attribute q.

2.1. Baseline Models
In our framework, pointwise labels are considered for
classiﬁcation and annotation tasks. For classiﬁcation, we
assume that there is a linear classiﬁer Wc ∈ Rd×c3 to map
X to the pointwise label Yc as Yc = XWc . Wc can be
obtained by solving the following optimization problem:
min Ω(Wc ) + L(Wc , Yc , D)
Wc

(1)

where L() is a loss function and Ω is a regularization
penalty to avoid overﬁtting, D is the training sample set.
Here we employ least square for loss function L.
For tag annotation, we also assume that there is a linear
function Wt ∈ Rd×c1 which captures the relation between
data X and pointwise label Yt as Yt = XWt . Similarly,
the optimization problem to learn Wt is:

• We provide a principled approach to modeling relations between pointwise and pairwise labels;
• we propose a novel joint framework PPP, which can
predict both pointwise and pairwise labels for images
simultaneously; and

min Ω(Wt ) + L(Wt , Yt , D)
Wt

• We conduct experiments on various benchmark
datasets to understand the working of the proposed
framework PPP.

(2)

For pairwise label based approaches, a simple and successful approach to utilizing the pairwise label is Rank
6006

Figure 2. The demonstration of capturing the relations between pointwise label and pairwise label via bipartite graph. For example, the
attribute “formal” with tags “leather, lace up, congnac” will form a group via the upper bipartite graph, while label “sandal” with attribute
“less formal” and tags “high heel, party” will form a group via the lower bipartite graph.

SVM, whose goal is to learn a model W that achieves little
loss over a set of previously unseen data, using a prediction
function. Similar to RankSVM, in our framework, the original distribution of training examples are expanded into a
set of candidate pairs and the learning process is over a set
of pairwise feature vectors as:

min L(W, Yr , P ) + Ω(Wr )
W

2.2. Capturing Relations between Poinwise and
Pairwise Labels
In the previous subsection, we deﬁned three tasks that
use pointwise and pairwise labels separately. Capturing the
relations between pointwise and pairwise labels can further
pave a way for us to develop a joint framework that enables
interaction between classiﬁcation, annotation and ranking
simultaneously.
First, the relations between attributes and tags can be denoted as a bipartite graph as shown in Figure 2. We assume
that B ∈ Rc2 ×c1 is the adjacency matrix of the graph where
B(i, j) = 1 if both the i-th tag and the j-th attribute cooccur in the same image and B(i, j) = 0 otherwise. Note
that in this paper, we do not consider the concurrence frequencies of tags and attributes and we would like to leave
it as one future work. From the bipartite graph, we can
identify groups of attributes and tags where attributes and
tags in the same group could share similar properties such
as semantical meanings. A feature X(:, i) should be either
relevant or irrelevant to the attributes and tags in the same
group. For example, Wr (i, j) indicates the effect of the i-th
feature on predicting the j-th attribute; while Wt (i, k) denotes the impact of the i-th feature on the k-th tag. Therefore we can impose constraints on Wt and Wr together,
which are derived from group information on the bipartite
graph, to capture relations between attributes and tags.
We can adopt any community detection algorithms to
identify groups from the bipartite graph. In this paper, we

(3)

where P is a set of training pairs. The loss function L is
deﬁned over the pairwise difference vector x:

L(W, Yr , P ) =



l(t(ya −yb ), f (w, a−b))

((a,ya ,qa ),(b,yb ,qb ))∈P

(4)
where the transformation function t(y) transforms the difference of the labels [18]. In our framework, the transformation function is deﬁned as t(y) = sign(y).
Note that one may form a uniﬁed model by simply
adding all the above objective functions together. Such an
approach would still essentially treat the component models
as independent tasks (albeit trade-off among them might be
considered via weighting), since no explicit relations among
them are considered.
6007

use a very simple way to extract groups from the bipartite
graph – for the j-th attribute, we consider the tags that connect to that attribute in the bipartite graph as a group, i.e.,
B(i, j) = 1. Note that a tag may connect to several attributes thus extracted groups via the aforementioned process have overlaps. Assume that G is the set of groups we
detect from the attribute-tag bipartite graph and we propose
to minimize the following term to capture relations between
attributes and tags as:
ΩG (Wt,r ) =

d 

i=1 g∈G

 
αg wgi 2

(5)

where Wt,r = [Wt , Wr ] and αg is the conﬁdence
of the group g and wgi is a vector concatenating
{Wt,r (i, j)}j∈g . For example, if g = {1, 5, 9}, wgi =
[Wt,r (i, 1), Wt,r (i, 5), Wt,r (i, 9)] . Next we discuss the
inner workings of Eq. (5). Let us check
terms in Eq. (5) red  
lated to a speciﬁc group g, i=1 wgi 2 , which is equal to
adding a 1 norm on the vector g = [wg1 , wg2 , . . . , wgd ], i.e.,
g1 . That ensures a sparse solution of g; in other words,
some elements of g could be zero. If gi = 0 or wg2 2 = 0,
the effects of the i-th feature on both the attribute and tags
in the group g are eliminated simultaneously.
Similarly, we build the bipartite graph to capture the
underlying relations for the attributes and class labels. In
[21], it was suggested that the co-occurrence of tags and labels should also be considered. Thus, we build a mixture
bipartite graph to extract the group information between
class labels, tags, and attributes. The group regularization
ΩG2 (Wt,r,c ) is similar to Eq. 5 and illustration is shown
in Figure 2, where a tag or an attribute will connect to the
class label if they are associated with each other. Note that
a group extracted from Figure 2 could include a class label,
a set of attributes and a set of tags.

2.3. The Proposed Framework
With the model component to exploit the bipartite graph
structures, the proposed framework is to solve the following
optimization problem:
min L(Wc , Yc , D) + L(Wt , Yt , D) + L(Wr , Yr , P )
W

+ λ(Wc 2F + Wt 2F + Wr 2F )
+ αΩG1 (Wt,r ) + βΩG2 (Wt,r,c )
(6)
In Eq. 6, the ﬁrst six term is from the basic models to predict
the class label, tags and ranking order. The seventh and
eighth term are to capture the overlapped structure of the
output, which is controlled by α and β respectively. The
group regularization is deﬁned as blow:
ΩG (Z) =


i∈G

Zg 2 =

d 


zig 2

3. An Optimization Method for PPP
Since the group structures are overlapped, directly optimizing the objective function is difﬁcult. We propose to use
Alternating Direction Method of Multiplier (ADMM)([25,
2]) to optimize the objective function. We ﬁrst introduce
two auxiliary variables P = [Wt , Wr ]M1 and Q =
[Wt , Wr , Wc ]M2 . M1 ∈ {0, 1}(c1 +c2 )×c2 (c1 +c2 ) is deﬁned as: if i − th tag connects to the jth attribute then
M1 (i, (c1 +c2 )(j−1)+i) = 1, otherwise it is zero. The definition of M2 ∈ {0, 1}(c1 +c2 +c3 )×c3 (c1 +c2 +c3 ) is similar to
M1 . With these two variable, solving the overlapped group
lasso on W is transfered to the non-overlapped group lasso
on P and Q, respectively. Therefore, the objective function
becomes:
min L(Wc , D) + L(Wt , D) + L(Wr , P )

W,P,Q

+ αΩG (P) + βΩG2 (Q)
+ λ(Wc 2F + Wt 2F + Wr 2F )
s.t.P = [Wt , Wr ]M1 ; Q = [Wt , Wr , Wc ]M2 ;
(8)
which can be solved by the following ADMM problem:
min L(Wc , Yc , D) + L(Wt , Yt , D) + L(Wr , Yr , P )

W,P,Q

+ λ(Wc 2F + Wt 2F + Wr 2F ) + αΩG (P)
+ βΩG2 (Q) + Λ1 , P − [Wt , Wr ]M1 
+ Λ2 , Q − [Wt , Wr , Wc ]M2 
μ
+ P − [Wt , Wr ]M1 2F
2
μ
+ Q − [Wt , Wr , Wc ]M2 2F
2

(9)
where Λ is the Lagrangian multiplier and μ is a scaler to
control the penalty for the violation of equality constrains
P = [Wt , Wr ]M1 and Q = [Wt , Wr , Wc ]M2 . Noting
that the loss function L has lots of choices, we use the least
square loss function in this paper.

3.1. Updating W
To update W, we ﬁx the other variable except W and
remove terms that are irrelevant to W. Then the Eq. 9 becomes:


min
xWt − yt 22 +
xWc − yc 22
W

x∈D

+



x∈D

(xi − xj )Wr − yr 22

xi ,xj ∈P

+ λ(Wc 2F + Wt 2F + Wr 2F )
1
μ
+ (P + Λ1 ) − [Wt , Wr ]M1 2F
2
μ
1
μ
+ (Q + Λ2 ) − [Wt , Wr , Wc ]M2 2F
2
μ

(7)

i=1 i∈G

6008

(10)

Setting the derivative of Eq. 10 w.r.t Wt to 0, we get:

and negative label for label transformation. Setting the Eq.
10 w.r.t Wr to zero, we can obtain:

t
tT
XTD XD Wt + λWt + Wt (Mt1 MtT
1 + M2 M2 )
(11)
1
μ
1
= XT Y + [(P + Λ1 )Mt1 + (Q + Λ2 )Mt2 ]
2
μ
μ

r
rT
XTP XP Wr + λWr + Wr (Mr1 MrT
1 + M2 M2 )
(19)
1
μ
1
= XTP Yr + [(P + Λ1 )Mr1 + (Q + Λ2 )Mr2 ]
2
μ
μ

where Mt1 is the part of M1 corresponding to Wt . Directly
getting the close form solution from Eq. 11 is intractable.
t
tT
On the other hand XTD XD + 12 λI and Mt1 MtT
1 +M2 M2 +
1
2 λI are symmetric and positive deﬁnite. Thus, we employ
eigen decomposition for each of them:
XTD XD
t
tT
Mt1 MtT
1 + M2 M2

1
+ λI = U1 Σ1 UT1
2
1
+ λI = U2 Σ2 UT2
2

Similar to Wc , with eigen decomposition, we can get the
closed form solution for Wr as:
r (s, t) =
W

+

1
μ
(Q + Λ2 )Mt2
2
μ

(12)

(14)

t and Wt as:
Then, we can get W
(15)

t UT
W t = U1 W
2

(16)

(17)

c UT
W c = U1 W
3

(18)

After removing terms that are irrelevant to P, Eq. 9 becomes:
μ
min P−[Wt , Wr ]M1 2F +αΩG (P)+T r(Λ1 P) (22)
P
2
When applied to the collection of group for the parameters,
P, ΩG (P)) no longer have overlapping groups. We denote
j −th group in i-th row as Pi,j = P(i, (c1 +c2 )(j −1)+1 :
(c1 + c2 )j). Hence, we can solve the problem separately for
each row of P within one group by the following optimization:
μ
Λ1ij 2
)F
min αPi,j 22 + Pi,j − (([Wc , Wr ]M1 )i,j −
Pi,j
2
μ
(23)
Note that Eq. 23 is the proximal operator [27] of μ1 (P )i,j
applied to (([Wc , Wr ]M1 )i,j −
Λ1ij
μ .

Λ1ij
μ ).

Let ZP
i,j

=

([Wc , Wr ]M1 )i,j −
The solution by applying the
proximal operator used in non-overlapping group lasso to
each sub-vector is:
⎧
α
⎨0
if ZP
i,j 2 ≤ μ
P
α
)
=
Pi,j = prox(ZP

−
Z
i,j
i,j 2
μ
⎩ Z
ZP
otherwise
P
i,j

Similarly, setting the derivative of Eq. 10 w.r.t Wc to
zero and apply the eigen decomposition, we have the closed
form solution of Wc :
c (s, t) = Zc
W
σ1s + σ3t

(21)
1
r
μ Λ1 )M1

3.2. Updating P

1
μ
(P + Λ1 )Mt1
2
μ

t (s, t) = Zt (s, t)
W
σ1s + σ2t

μ
2 [(P

(20)

where Zr =
+
+
+ (Q +
1
r
T
Λ
)M
]U
,
U
,
σ
are
eigen
vector
and
eigen
values
for
4
4
2
5
μ 2
1
T
XP XP + 2 λI, and U5 , σ5 are eigen vector and eigen value
1
r
rT
for Mr1 MrT
1 + M2 M2 + 2 λI.
U4 [XTP Yr

(13)
Multiplying UT1 and U2 from left to right on both sides,
t = UT Wt U2 and Zt = UT [XT Yt +
and letting W
1
1
D
μ
1
1
t
t
2 [(P + μ Λ1 )M1 + (Q + μ Λ2 )M2 ]]U2 , we can obtain:
t Σ2 = Zt
t + W
Σ1 W

Zr
+ σ5t

r UT
W r = U4 W
5

whereU1 , U2 are eigen vectors and Σ1 , Σ2 are diagonal
matrices with eigen value on the diagonal. Substituting Eq.
12 into Eq. 11:
U1 Σ1 UT1 Wt +Wt U2 Σ2 UT2 = XTD Yt +

σ4s

i,j2

(24)

3.3. Updating Q
Similar to P, we can update Q by proximal operator
used in non-overlapping group lasso to each sub-vector of
Q:
⎧
β
⎨0
if ZQ
i,j 2 ≤ μ
Q
Q
β
Qi,j = prox(Zi,j ) = Zi,j 2 − μ Q
⎩
Zi,j otherwise
ZQ

where Zc = UT3 [XTD Yc + μ2 (Q + μ1 Λ2 )]Mc2 and U3 , σ3
are the eigen vector and eigen value for the symmetric and
1
positive deﬁnite matrix Mc2 McT
2 + 2 λI.
Noting that for Wr , which input is data pairs, we can
use the same learning process by using the transform label
function mentioned above. For example, we regard the pair
difference as one data sample for XP and use the positive

i,j2

(25)
where Qij = Q(i, (c1 +c2 +c3 )(j −1)+1 : (c1 +c2 +c3 )j)
Λi,j
and ZQ
i,j = ([Wt , Wr , Wc ]M2 )i,j − μ
6009

3.4. Updating Λ1 , Λ2 and μ
After updating the variables, we now need to update the
ADMM parameters. According to [2], they are updated as
follows:
Λ1 = Λ1 + μ(P − [Wt , Wr ]M1 )

(26)

Λ2 = Λ2 + μ(Q − [Wt , Wr , Wc ]M2 )

(27)

μ = min(ρμ, μmax )

(28)

Here, ρ > 0 is a parameter to control the convergence speed
and μmax is a large number to prevent μ from becoming too
large.
With these updating rules, the optimization method for
our proposed method is summarized in Algorithm 1
Algorithm 1 The algorithm for the proposed framwork
Input: XD ∈ RN ×d and XP ∈ Rm×d and corresponding
label Yt , Yc and Yr
Output: c1 tags label c2 relative score and c3
class label for each data instance
1: Initialize random Sample training set D and drawn random pair set P from D.
2: Setting μ = 10−3 , ρ = 1.1, μmax = 108 and building
M1 and M2
3: Precompute the eigen decomposition
4: repeat
t , W
t and W
r
5:
Calculate W
6:
Update Wt , Wr and Wc by Eq. 16, Eq. 21, and
Eq. 18, respectively.
7:
Calculate ZP and ZQ
8:
Update P and Q
9:
Update Λ1 , Λ2 and μ
10: until convergence
11: Using max pooling for testing use XW to predict tags,
relative relation and labels.

3.5. Convergence Analysis
Since the sub-problems are convex for P and Q, respectively, Algorithm 1 is guaranteed to converge because they
satisfy the two assumptions required by ADMM. The proof
of the convergence can be found in [2]. Specially, Algorithm 1 has dual variable convergence. Our empirical results show that our algorithm often converges within 100
iterations for all the datasets we used for evaluation.

3.6. Time Complexity Analysis
The main computation cost for W involves the eigen decomposition on XT X+ 12 βI, while other terms that involve
eigen decomposition is very fast because the feature dimension of MMT is small. The time complexity for eigen decomposition is O(d3 ). However, in Algorithm 1 the eigen

decomposition is only computed once before the loop and
dimension reduction algorithm can be employed to reduce
image feature dimensions d. The computation cost for Z is
O(nd2 ) due to the sparsity of M. The computation of P
depends on the proximal method within each group. Since
there are c2 groups which have the group size c1 + c2 for
each feature dimension, the total computation cost for P is
O(dc2 (c1 + c2 )) and it is similar for Q. It is worth noting
that P and Q can be computed in parallel for each feature
dimension.

4. Experiment
In this section, we conduct experiments to evaluate the
effectiveness of PPP. After introducing datasets and experimental settings, we compare PPP with the state-of-the-art
methods of tag prediction, classiﬁcation and ranking.

4.1. Experiments Settings
The experiments are conducted on 3 publicly available
benchmark datasets.
Shoe-Zappo dataset [26]: It is a large shoe dataset
consisting of 50,025 catalog images collected from Zappos.com. The images are divided into 4 major categories
shoes, sandals, slippers, and boots. The tags are functional types and individual brands such as high-heel, oxford, leather, lace up, and pointed toe. The number of tags is
147 and 4 relative attribute is deﬁned as “open” , “pointy”,
“sporty” and “comfortable”. The ground truth is labeled
from AmazonTurk.
OSR-scene dataset [16]: It is a dataset for out door
scene recognition with 2688 images. The images are
divided into 8 category named as coast, forest, highway, inside-city, mountain, open-country, street and tallbuilding. 6 attributes with pointwise label and pairwise label are provided by [17] named by natural, open, perspective, large-objects, diagonal-plane and close-depth.
Pubﬁg-face dataset [13]: It is a dataset containing 800
images from 8 random identities (100 images per person)
named Alex Rodriguez, Clive Owen, Hugh Laurie , Jared
Leto , Miley Cyrus, Scarlett Johansson , Viggo Mortensen
and Zac Efron. We use the 11 attributes with pintwise label
and pairwise label provided by [17]. The example attributes
are named as masculine-looking, white, young, smiling and
etc.

4.2. Performance Comparison
We compare PPP with the following representative algorithms:
• SVM [3]: It uses the state of the art classiﬁer SVM for
classiﬁcation with linear kernel; We also apply it to tag
prediction by considering tags as a kind of labels;
6010

• GLasso [28]: The original framework of group lasso
is to handle high-dimensional and multi-class data. To
extend it for joint classiﬁcation and tag prediction, we
also consider tags as a kind of labels and apply GLasso
to learn the mapping of features to tags and label. Note
that it does not make use of the pointwise and pairwise
label bipartite graph. We use the implementation in
[15];

Table 1. Performance comparison in terms of classiﬁcation. The
number after each dataset means the class label number.

Method
SVM
GLasso
sLDA
LS
FT
RD
PPP

• sLDA [21]: It is a joint framework based on topic
models, which learns both class labels and annotations
given latent topics;
• LS [9]: A multi-label classiﬁcation method that exploits the label correlation information. To apply LS
for joint classiﬁcation and tag prediction, we consider
tags as a kind of labels and use tag and label relations
to replace the label correlation in the original model;
and
• FT [6]: It is one of the state-of-the art annotation
method which is based on linear mapping and coregularized joint optimization. To apply it for classiﬁcation, we consider labels as tags to annotate; and
• RD: It predicts labels and tags by randomly guessing.
• MultiRank [5]: It is a ranking method based on the assumption that the correlation exists between attributes,
where the ranking function learns all attributes together via multi task learning framework.

Zappos(4)
67.41 %
78.31%
74.32%
84.46%
84.69%
25.01%
89.39%

OSR(8)
42.21 %
50.11%
46.33%
61.22%
59.38%
12.51%
62.33%

Pubﬁg (8)
50.77%
59.13%
56.21%
66.56%
67.45%
12.50%
74.95%

Since OSR and Pubﬁg contain a small number of attributes, we leave one random-picked attribute for pairwise
prediction and use the rest for tag annotation. Especially,
to evaluate the performance of tag annotation, we rank all
the tags based on their relevant scores and return the top K
ranked tags. We use the average precision AP @K as the
evaluation metric which has been widely used in the literature [6, 21]. Meanwhile since the data samples are balanced, we use accuracy as the metric to evaluate the classiﬁcation performance. The comparison results are shown in
Table 1 and Table 2 for classiﬁcation and tag annotation,
respectively. We repeat 10 times for the training-testing process and report the average performance.
From the tables, we make the following observations:
• The proposed method that utilizes pairwise labels to
predict pointwise labels tends to outperform the methods which solely rely on pointwise labels. These results support that (1) pairwise attributes can provide
evidence for the pointwise label prediction; especially
for the Pubﬁg dataset that contains 8 label classes, our
method utilizes information from pairwise attributes
signiﬁcantly improve the classiﬁcation performance.
(2) The performance of tag prediction AP @K indicates that the pairwise attributes contain important information for tag prediction;

• RA [17]: It is the method for image ranking based on
relative attributes.
Note that for all the baseline methods, none of them can
utilize both pointwise and pairwise labels. Although we
get the performance of the proposed framework by jointly
predicting both pointwise and pairwise labels, we present
our results for each task separately for a clear comparison.
Moreover, we could use more advanced features, e.g., CNN
feature, however, to compare with other methods fairly, we
adopt the original feature provided by each datasets, which
can easily show the performance gain from the proposed
model.

• Our method with model components to capture relations between pairwise and pointwise labels outperforms those without. For example, compared to
GLasso, the proposed framework, modeling the relations via the bipartite graph, gains remarkable performance improvement for both classiﬁcation and tag prediction; and

4.3. Pointwise label Prediction
For pointwise label prediction, our method is compared
with SVM, Glasso, sLDA, LS, FT, and RD. For all the baseline methods with parameters, we use cross validation to determine their values. For the Shoe dataset, we use the same
data split and features (990 gist and color features) in [26].
It contains 11102 data samples for training and 2400 data
sample for testing. For OSR and Pubﬁg, we use the same
data split and features in [17].

• Most of the time, the proposed framework PPP performs the best among all the baselines, which demonstrates the effectiveness of the proposed algorithm.
There are two major reasons. First, PPP jointly performs pointwise and pairwise label prediction. Second, PPP captures relations between labels by extracting group information from the bipartite graph, which
6011

Table 2. Performance comparison in terms of tag recommendation.

Method
SVM
GLasso
sLDA
LS
FT
RD
PPP

Zappo (147 tags)
AP @3
AP@5
AP @10
50.57% 40.89% 38.53%
64.34% 59.81% 55.37%
62.57% 57.22% 51.63%
74.76% 66.62% 61.85%
67.37% 59.52% 51.98%
1.44%
1.43%
1.44%
77.10% 71.08% 62.95%

AP @1
68.51%
87.11%
90.15%
94.19%
98.62%
20.00%
96.69%

OSR (5 tags)
AP@2
AP @3
63.69% 60.12%
83.34% 80.87%
88.06% 84.78%
93.19% 92.75%
94.45% 92.22%
20.01% 20.01%
94.21% 90.14%

Pubﬁg (10 tags)
AP @1
AP@3
AP@5
46.21% 38.31% 34.12%
90.12% 88.76% 86.41%
91.12% 87.93% 84.17%
93.71% 92.66% 91.91%
92.45% 91.50% 90.16%
10.01% 10.00% 10.01%
94.48% 93.67% 92.71%

Table 3. The average ranking accuracy on three dataset

Method
RA
MultiRank
PPP

Zappos
70.37%
76.12%
79.67%

OSR
76.10%
84.93%
88.40%

Pubﬁg
71.23%
74.91%
76.32%

works as the bridge for building interactions between
pointwise and pairwise labels.

4.4. Pairwise label Prediction
For pairwise label prediction, we generate pairs drawn
from the training set used in the pointwise label prediction. For the Shoe dataset, we use 300 pairs; while for OSR
and Pubﬁg, we use 100 pairs (the number suggested in [5])
drawn from training set. We compute the average ranking
accuracy with standard deviation by running 10 rounds of
each implementation. The results are shown in Table 3.
Moreover, we also plot in Figure 3 to show how average accuracy changes with different sizes of training samples on
the attributes on the Shoe dataset (due to the space limits,
we omit the ﬁgure on OSR and Pubﬁg).
From Table 3 and Figure 3, we can have the following
observations:

Figure 3. Learning curve of average ranking accuracy with regarding to different numbers of training pairs.

which makes the correlated attributes have strong overlaps, providing a discriminative way to capture the correlation between attributes.

5. Conclusion

• The proposed method that leverages pointwise labels
to predict pairwise labels often outperforms the methods which only use pairwise labels. These results support that pointwise labels can help the pairwise label
prediction;

In this paper, we propose a novel way to capture the relations between pointwise labels and pairwise labels. Moreover, PPP provides a new viewpoint for us to have a better understanding how pointwise and pairwise labels interact with each other. Experiments demonstrated : (1) the
advantages of the proposed methods for pointwise label
based tasks including image classiﬁcation, tag annotation
and pairwise label based image ranking; and(2) the importance of considering the group correlation between pointwise labels and pairwise labels.

• The performance of the ranking accuracy varies with
the number of the training pairs. With a small amount
of labeled data, e.g., 10 pairs, the proposed method
signiﬁcantly outperforms relative attribute methods,
which demonstrates that the pointwise labels contain
important information for attribute ranking;

6. Acknowledgement

• The comparison based on multi-task attribute learning methods and our method demonstrates that simply combining the attributes together fails to differentiate these attributes which are not related to other
attributes, while our methods use group structures,

The work was supported in part by ONR grants
N 00014−15−1−2722 and N 00014−15−1−2344. Any
opinions expressed in this material are those of the authors
and do not necessarily reﬂect the views of ONR.
6012

References
[1] T. L. Berg, A. C. Berg, and J. Shih. Automatic attribute discovery and characterization from noisy web data. In Computer Vision–ECCV 2010, pages 663–676. Springer, 2010.
[2] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein.
Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and
R in Machine Learning, 3(1):1–122, 2011.
Trends
[3] C.-C. Chang and C.-J. Lin. Libsvm: A library for support
vector machines. ACM Transactions on Intelligent Systems
and Technology (TIST), 2(3):27, 2011.
[4] L. Chen, P. Zhang, and B. Li. Fusing pointwise and pairwise labels for supporting user-adaptive image retrieval. In
Proceedings of the 5th ACM on International Conference on
Multimedia Retrieval, pages 67–74. ACM, 2015.
[5] L. Chen, Q. Zhang, and B. Li. Predicting multiple attributes
via relative multi-task learning. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages
1027–1034. IEEE, 2014.
[6] M. Chen, A. Zheng, and K. Weinberger. Fast image tagging.
In Proceedings of the 30th international conference on Machine Learning, pages 1274–1282, 2013.
[7] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing
objects by their attributes. In Computer Vision and Pattern
Recognition, 2009. CVPR 2009. IEEE Conference on, pages
1778–1785. IEEE, 2009.
[8] D. Jannach, M. Zanker, A. Felfernig, and G. Friedrich. Recommender systems: an introduction. Cambridge University
Press, 2010.
[9] S. Ji, L. Tang, S. Yu, and J. Ye. Extracting shared subspace for multi-label classiﬁcation. In Proceedings of the
14th ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 381–389. ACM, 2008.
[10] Y.-G. Jiang, G. Ye, S.-F. Chang, D. Ellis, and A. C. Loui.
Consumer video understanding: A benchmark database and
an evaluation of human and machine performance. In Proceedings of the 1st ACM International Conference on Multimedia Retrieval, page 29. ACM, 2011.
[11] A. Kovashka and K. Grauman. Attribute adaptation for personalized image search. In Computer Vision (ICCV), 2013
IEEE International Conference on, pages 3432–3439. IEEE,
2013.
[12] A. Kovashka and K. Grauman. Attribute pivots for guiding relevance feedback in image search. In Computer Vision
(ICCV), 2013 IEEE International Conference on, pages 297–
304. IEEE, 2013.
[13] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar.
Attribute and simile classiﬁers for face veriﬁcation. In Computer Vision, 2009 IEEE 12th International Conference on,
pages 365–372. IEEE, 2009.
[14] C. H. Lampert, H. Nickisch, and S. Harmeling. Learning
to detect unseen object classes by between-class attribute
transfer. In Computer Vision and Pattern Recognition, 2009.
CVPR 2009. IEEE Conference on, pages 951–958. IEEE,
2009.
[15] J. Liu, S. Ji, J. Ye, et al. Slep: Sparse learning with efﬁcient
projections.

[16] A. Oliva and A. Torralba. Modeling the shape of the scene: A
holistic representation of the spatial envelope. International
journal of computer vision, 42(3):145–175, 2001.
[17] D. Parikh and K. Grauman. Relative attributes. In Computer Vision (ICCV), 2011 IEEE International Conference
on, pages 503–510. IEEE, 2011.
[18] D. Sculley. Combined regression and ranking. In Proceedings of the 16th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 979–988.
ACM, 2010.
[19] B. Sigurbjörnsson and R. Van Zwol. Flickr tag recommendation based on collective knowledge. In Proceedings of
the 17th international conference on World Wide Web, pages
327–336. ACM, 2008.
[20] G. Toderici, H. Aradhye, M. Paşca, L. Sbaiz, and J. Yagnik.
Finding meaning on youtube: Tag recommendation and category discovery. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 3447–3454.
IEEE, 2010.
[21] C. Wang, D. Blei, and F.-F. Li. Simultaneous image classiﬁcation and annotation. In Computer Vision and Pattern
Recognition, 2009. CVPR 2009. IEEE Conference on, pages
1903–1910. IEEE, 2009.
[22] S. Wang, J. Tang, Y. Wang, and H. Liu. Exploring implicit hierarchical structures for recommender systems. In Proceedings of the Twenty-Fourth International Joint Conference on
Artiﬁcial Intelligence, IJCAI 2015, Buenos Aires, Argentina,
July 25-31, 2015, pages 1813–1819, 2015.
[23] Y. Wang, Y. Hu, S. Kambhampati, and B. Li. Inferring sentiment from web images with joint inference on visual and
social cues: A regulated matrix factorization approach. In
Proceedings of the Ninth International Conference on Web
and Social Media, ICWSM 2015, University of Oxford, Oxford, UK, May 26-29, 2015, pages 473–482, 2015.
[24] Y. Wang, S. Wang, J. Tang, H. Liu, and B. Li. Unsupervised
sentiment analysis for social media images. In Proceedings
of the Twenty-Fourth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July
25-31, 2015, pages 2378–2379, 2015.
[25] D. Yogatama and N. Smith. Making the most of bag of
words: Sentence regularization with alternating direction
method of multipliers. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages
656–664, 2014.
[26] A. Yu and K. Grauman. Fine-grained visual comparisons
with local learning. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 192–199.
IEEE, 2014.
[27] L. Yuan, J. Liu, and J. Ye. Efﬁcient methods for overlapping
group lasso. In Advances in Neural Information Processing
Systems, pages 352–360, 2011.
[28] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49–
67, 2006.

6013

Synchronizing Disparate Video Streams from Laparoscopic
Operations in Simulation-based Surgical Training
Zheshen Wang, Baoxin Li
Computer Science and Engineering, Arizona State University, Tempe, AZ, 85281
Abstract- In this paper, we propose a novel approach for
synchronizing multiple videos captured from common
laparoscopic operations in simulation-based surgical
training. The disparate video sources include two hand-view
sequences and one tool-view sequence that does not contain
any visual overlap with the hand views. The synchronization
of the video is essential for further visual analysis tasks. To
the best of our knowledge, there is no prior work dealing
with synchronization of completely different visual streams
capturing different aspects of the same physical event. In the
proposed approach, histograms of dominant motion (HoDM)
are extracted and used as features for each frame. Multi-view
sequence correlation (MSC), computed as accumulated
products of pairwise correlations of HoDM magnitudes and
co-occurrence rate of pairwise HoDM orientation patterns, is
proposed for ranking possible configurations of temporal
alignment. The final relative shifts for synchronizing the
videos are determined by maximizing both the overlap length
of all sequences and the MSC scores through a coarse-to-fine
search procedure. Experiments were performed on 41 groups
of videos of two laparoscopic operations, and the
performance was compared to start-of-the-art method,
demonstrating the effectiveness of the proposed approach.

I.

surgery. Since the FLS system has only an on-board
camera capturing the tool movement, additional cameras
need to be deployed to capture the hand movements of a
trainee for automated analysis. Fig. 1-(b), (c) and (d)
display snapshots of two hand-view videos captured by
two stand-alone cameras and a tool-view video captured
by the on-board analog camera in the FLS trainer box.
One important preprocessing step in automated analysis
from these videos from different cameras is to accurately
synchronize them1. Unlike other multi-video alignment
tasks, our problem is uniquely challenging due to lack of
any overlap between the tool view and the hand views.

(a)

(b)

INTRODUCTION

Since laparoscopic surgery emerged as an alternative
(and in some cases, a replacement) to traditional open
surgery, simulation-based surgical training has become a
standard practice for training surgeons [1-2]. Fig. 1-(a)
illustrates such a training system, an FLS (Fundamentals
of Laparoscopic Surgery) trainer box [3] produced by the
Society of American Gastrointestinal and Endoscopic
Surgeons, which has been widely adopted by hospitals in
the U.S. With the FLS system, a resident surgeon is
required to practice on simulated operations and pass an
exam at the end of the training. Conventional training and
evaluation require senior surgeons to supervise and rate
the trainee by inspecting the trainee’s action in real-time.
This is deemed as very costly.
There have been efforts on automating the evaluation in
simulation-based surgical training by using videos
capturing the movement of the trainee. The work of [4] is
such an example although it is in the domain of robotic

(c)
(d)
Fig. 1. (a) FLS trainer box for laparoscopic simulations with an
external monitor showing the video captured by the on-board
camera; (b) Tool view (from the on-board camera); (c) and (d):
Hand views captured by two external cameras.

In addition, observable hand motions in the hand views
are typically very subtle and thus it is difficult to detect
distinctive poses (which are commonly used in
synchronizing general human actions). Further, hand
motion in this application is seriously self-occluded and
movements of both hands and tools are often very noisy
(e.g. due to shaky hands of novice), which makes it

1
Building an integrated system with the cameras synchronized by
design may avoid this preprocessing requirement. But that cannot happen
in near term since it would require changing the current practice, which
would take many years to occur.

1

still a challenging task. Most existing work focuses on
general human actions (such as walking, waving, etc.) or
vehicle activities in which motion trajectory is usually
well-defined and self-occlusion is not very obvious. In
videos from our application, the motion of the hands and
the tools is subtle, and serious self-occlusion occurs in the
hand views. Also, in the tool view, the objects often have
similar colors (e.g., synthetic human tissues), making
point correspondence difficult. All these make it extremely
difficult to performing reliable tracking and point-based
correspondence based processing. Introducing other
physical mechanisms such as placing a stop watch or
performing a shutter flash [17] is not immediately
applicable as they require changing the current practice.
Junejo and Dexter etc. proposed self-similarity matrix
(SSM) for capturing view invariant features for video
synchronization and human action recognition [18, 19].
Similarity between each frame and all other frames based
on motion trajectory or optical flow motion vectors are
computed and stored into a matrix (or an image). Then for
each point along the diagonal, histograms of gradient
directions are extracted based on a pre-defined log-polar
structure around this point. For synchronization, final
time-shift is obtained by minimizing accumulated pairwise
frame distance of the histograms. This approach showed
impressive results on videos with dramatic view angle
differences (i.e. top view and side view of human actions).
We implemented the core idea for comparison.

difficult to perform reliable 2D or 3D point tracking and
make use of motion trajectory for synchronization.
In this work, we propose an approach for aligning three
videos (two hand views and one tool view) of laparoscopic
operations on the FLS platform. From the videos, optical
flows are first extracted at interesting points that give large
frame differences. The flows are assumed to capture the
dominant motion in a frame (since they are from regions
with large frame differences). Histograms of such
dominant motion (HoDM) (magnitude and orientation) of
each frame are then computed and used as features. For
estimating the relative shifts of two sequences with respect
to one reference video, a multi-view sequence correlation
(MSC) metric (and the corresponding algorithm) is
proposed for ranking possible configurations of the
alignment. The MSC score consists of two parts including
pairwise correlation of motion magnitudes and cooccurrence rate of pairwise motion orientations. The first
term captures quantitative correlations of motion
magnitudes, while the second one promotes frequently
repeated patterns of orientation which reflects the
correlation of the motion in different views as determined
by the physical setup of the system. Experimental results
from real data captured in two local hospitals demonstrate
the effectiveness of the proposed method.
The rest of the paper is organized as follows. We
review exiting literature of video synchronization in
Section 2. The proposed approach including the HoDM
feature and the MSC algorithm are presented in Section 3
followed by experiments, analysis and comparisons in
Section 4. We conclude in Section 5 with a brief
discussion on future work.

III. PROPOSED APPROACH
We consider the problem of synchronizing n videos
q1,…, qn of lengths M1, …, Mn respectively captured from
different views of the same physical event. Without loss of
generality, we assume that q1 is the reference sequence. A
synchronization algorithm is designed to return the
optimal offsets < k2(0) ,..., kn(0) > for sequence q2, …, qn

II. RELATED WORK
In computer vision, temporal synchronization of videos
is often jointly considered with spatial alignment for
recognition and reconstruction. Some existing work relies
on spatio-temporal interesting points [5, 6] to determine
optimal match of two videos. Color, intensity, texture and
other visual features [7-9] are often used for establishing
point correspondence between the two scenes and further
associating the videos both spatially and temporally. Such
methods require sufficient visual similarity of the
sequences, which cannot be satisfied in our problem.
To accommodate large view angle variation, epipolar
geometry is introduced in some papers [10-13] for
recovering spatial relationships of different scenes.
However, they often assume the cameras are well
calibrated or in the worse case there exists enough static
scene points which can be accurately associated from
different views for estimating the geometric relationship
of the cameras. Motion trajectory is another popular
feature used for video synchronization [10, 14-16].
Reliable tracking of individual points in multiple views is

with respect to sequence q1, as defined in the below:

< k2(0) ,..., kn(0) >= arg max(γ ⋅ L+ MSC(s1,..., sn ))䯸1≤ k2,..., kn ≤U (1)
<k2 ,...,kn >

L

where U specifies an upper limit of misalignment; L is the
maximum length of overlap of all the sequences under
configuration <k2, …, kn>,

L = min( M 1 , M 2 − k2 , ..., M n − kn )

(2)

MSC ( s1 ,..., sn ) denotes a correlation measure (to be
L

defined in Section 3.2) among the truncated videos of the
overlapped sub-sequences s1 ,..., sn ,

sl = {q ( kl ) +1 ,..., q ( kl +1+ L ) } , l = 1,..., n , k1 = 1

(3)

The factor γ is a constant for controlling the relative

2

of HoDM magnitude at time t and Cpo is a co-occurrence
measure of the frame pair ( sa( t ) , sb( t ) ) in terms of the entire

contribution of the terms. Intuitively, Eq. (1) gives an
optimal synchronization configuration when both of the
overlapping length and the correlation among the
overlapped sub-sequences are maximized. Note that we
have assumed the same frame rate for all videos
(extending to different frame rates is straightforward). In
following subsections we present the details of our
approach.

sequences:

C pr ( sa(t ) , sb(t ) ) = R ( H ( a m (t ) ), H ( b m(t ) ))
C po ( sa(t ) , sb( t ) ) =

A. Histogram of Dominant Motion (HoDM)

PN(t )  arg max | s(t )  s(t E ) | , 1 b t b L  E

­1, R( H (θ (t ) ), H (θ (t ') )) > τ ;
f (s (t ) , s ( t ') ) = ®
¯0, otherwise.

sa( t ') is an co-occurring instance of frame sa(t ) , and R is the
correlation coefficient for two 1D histograms.
Intuitively, Cpr captures the coherence of motion (in
terms of magnitude) in the two underlying views, while
Cpo measures how frequent a particular orientation pattern
occurs in the given videos even if the corresponding
motion magnitudes may not be positively correlated. This
is intended to account for cases where the observed
motions in the two views are different due to difference in
the camera view angles. Note that the cameras are static
during acquisition, and thus geometric relationships
among different views should be consistent for the entire
videos, although they are unknown and difficult to model
in practice. In addition, hand motions and corresponding
tool movements are also physically constrained by the the
trainer box and the shape of tools. Therefore, under a
particular alignment configuration of the videos, if a
pairwise motion pattern (especially for the orientation)
occurs many times in the videos, it is very likely that the
corresponding pattern arises from the underlying
relationship of cameras, and thus the given alignment
configuration should be supported. When the correlation
of motion magnitudes is larger, the evidence is stronger.
Fig. 2 illustrates three particular cases of different Cpr
and Cpo values. In Case 1, although Cpr is large
(magnitudes of both frames are large), Cpo would
eventually diminish the product since no additional
occurrence of this pairwise orientation pattern appears
anywhere else, which implies that this pair of orientations
not well reflect the hidden camera relationship. On the
contrary, in Case 2, the orientation pair (i.e. (up, left))
repeats many times, which makes Cpo large. However, as
the magnitudes of both elements are very small (they
might be just noises), the final product remains small. In
the third case, the Cpr term is reasonable large and the final
product is further promoted by Cpo due to frequent cooccurrences of the pairwise orientation pattern.

(4)

(5)

Histograms of magnitude and orientation of dominant
motion H (m( t ) ) and H (θ (t ) ) are then computed
separately and further used as features for each frame.

B. Multi-view Sequence Correlation (MSC)
Based on HoDM features, we further compute multiview sequence correlation (MSC) scores for all possible
configurations of temporal offsets. In this work, the
correlation among n sequences of length L is defined as
the sum of all pairwise sequence correlations:
L

¦ ¦

a =1,..., n b =1,.., n
a <b

SC ( sa , sb )

(6)

L

where the correlation between a sequence pair (Sa , Sb) is
defined as
L

SC ( sa , sb ) = ¦ C pr ( sa(t ) , sb(t ) ) ⋅ C po ( sa(t ) , sb(t ) )
L

(10)

where τ is a constant threshold for judging whether frame

where E is the interval for computing the frame
difference. Then the motion of interesting point i in frame
t is computed as the magnitude and orientation of 1-level
Lucas-Kanade optical flows motion vector [20].

MSC ( s1 ,..., sn ) =

t '≠ t

In Eq. (9), f computes the number of co-occurrence of the
input frame pair:

(i , j )s( t )

( mi(t ) , Ri(t ) )  LucasKanade( Pi (t ) ) , 1 b i b N

L
1 L
(¦ f ( sa(t ) , sa( t ') ) + ¦ f ( sb( t ) , sb( t ') )) (9)
L t '=1,
t '=1,
t '≠ t

As discussed previously, the observable motion in the
videos captured from our application presents many
challenges. For example, trajectories based on tracking
feature points cannot be reliably computed. In this work,
we propose histograms of dominant motion (HoDM) as
one feature, which does not rely on tracking points across
more than a few frames. Given a video sequence s of L
frames, after a smoothing step (i.e. using a 3D Gaussian
low-pass filter), we first extract motion vectors for a set of
interesting points. The interesting points at frame s(t) are
defined as the top N pixels having the largest absolute
frame differences.

(8)

(7)

t =1

By accumulating pairwise correlations of frames, large
length of overlap among all sequences is implicitly
preferred. In Eq. (7), Cpr denotes the pairwise correlation

3

a Penrose drain which has been slit along its long axis, and
then tie the knot using an intracorporeal knot. One must
place at least three throws that include one double throw
and two single throws on the suture and ensure the knots
are square and would not slip. As we mentioned
previously, hand movements in both of the operation does
not contain any obvious key poses. Motions are
complicated while variations of different sequences on the
same operation are comparably very large.

Fig. 2. Illustration of pairwise correlation term Cpr and pairwise
co-occurrence term Cpo: HoDM magnitudes are interpreted by
the gray-scale color of the markers (the darker the larger
magnitude) and HoDM orientations are indicated by the arrows
above each marker.

(a) “Pegboard transferring” (from left to right): Lift an object
with a grasper in one’s non-dominant hand; Transfer the object
midair to the dominant hand; Place the object on a peg on the
other side of the board.

C. Fast Implementation
Theoretically, the proposed approach can be used for
synchronizing arbitrary number of sequences of arbitrary
length to the finest frame rate simultaneously. It can also
be easily extended to sequences with different frame rate
by interpolating the original sequences and increasing the
search range. In this work, we applied it for aligning three
sequences with an upper limit of misalignment 150
frames, since in our data acquisition the videos are already
coarsely aligned by starting the cameras one immediately
after another. In addition, we used a two-level coarse-tofine search scheme: On the coarse level, MSC scores are
computed for all possible configurations of offsets with an
interval (e.g. 3 frames) and the optimal result on this level
is returned where the MSC score is maximized. Then a
finer search with frame accuracy is performed within a
local temporal range for obtaining the final result.

(b) “Intracorporeal knot” (from left to right): Place a suture
precisely through two marks on a Penrose drain; Place a throw
on the suture; Tie the knot using an intracorporeal knot.
Fig. 3. Illustrations of two operations.

Hand motion was recorded by two digital camcorders
(480X720 pixel, 30fps) from upper-frontal and frontal
view angles respectively. Although the cameras are fixed,
they are not required to be accurately calibrated. In this
work, we only assume that operator’s two hands are
clearly visible in the field of view of the camera during the
operation. In our experiments, we first cropped out the
hand regions, concatenated them horizontally and then
down-sampled the frames to 60X120 pixel. Fig. 4
illustrates cropped hand regions from the two hand views.
In the tool view, movements of the tools are captured
by the on-board analog camera and converted to digital
format using an A/D converter. The converted videos are
480X720 pixel with frame rate of 30fps. The center tool
region (360X720 pixel) is further cropped out (refer to
Fig.3) and down-sampled to 60X120 pixel.

IV. EXPERIMENTS
To verify the effectiveness of the proposed approach,
we carried out experiments on 41 sets of videos (each set
containing 3 videos including two hand views and one tool
view) of surgical simulation videos from real resident
surgeons in two local hospitals. In this section, we first
introduce the experimental data, groundtruthing and error
metric in Section 4.1. Results and comparisons to a startof-the-art method are presented in Section 4.2.

A. Data and Settings
We captured 41 sets of videos (123 videos) of two
laparoscopic simulations: “Pegboard transferring” (19
sets) and “Intracorporeal knot” (22 sets) from 22 resident
surgeons. As illustrated in Fig. 3, “Pegboard transferring”
requires the operator to lift the six objects one by one with
a grasper first in one’s non-dominant (i.e. left) hand,
transfer the object midair to the dominant hand and then
place each object on a peg on the other side of the board.
For “Intracorporeal knot” operation, it requires the
operator to place a suture precisely through two marks on

(a) From hand view 1.
(b) From hand view 2.
Fig. 4. Cropped hand regions from two hand views.

In data collection, subjects were asked to hold the
needle-drivers at the reference markers (i.e. two green
markers on each side, as shown in Fig. 1-(b)) for a while
(by counting “1-2-3” loudly) before starting operation. For
two hand views, the first frame right after “3” (in audio) is
deemed as the starting frame; for tool view, we use the

4

first frame when the two needle-drivers start to leave the
reference points. Since true ground-truth of alignment is
not available, we manually checked the segmented frames
back and forth for starting frames (as we defined earlier)
of each video, which can be as accurate as ± 2 frames.
In our experiments, videos from hand view 1 are used
as reference. We truncated videos from hand view 1 at the
ground-truth starting frames and set them as reference
positions. Total length of 1800 frames (i.e. 60 seconds) is
kept for all videos for computing MSC scores with a twolevel coarse-to-fine search (i.e. interval=3 frames for the
coarse level and 1 for the fine level). Synchronization
error is computed as the absolute difference of estimated
frame offset and ground-truth offsets.

equivalent to about only 0.2 second under a 30fps frame
rate, and thus is believed to be small.
TABLE I
AVERAGE SYNCHRONIZATION ERRORS AND COMPARISONS
Proposed
Synchronization Local SSM descriptor [19]
errors (frame)
Mag
Ori
Mag&Ori
Mag&Ori
Peg
8.32
8.21
8.26
1.26
Hand
view 2
Intra
1.36
1.32
1.41
1.27
Peg
9.53
9.53
9.53
5.95
Tool
view
Intra
10.91 11.28
11.05
6.77

To verify the assumption of smoothness of motion
(which is required for our coarse-to-fine search scheme)
and the effectiveness of this approach for a larger
misalignment range, we performed full per-frame search
with possible time-shift up to 300 frames for 3 sets videos
for each operation. Average frame errors of
synchronization are very close to the results using twolevel coarse-to-fine search (difference within about 1
frame).
For comparison, we implemented the core idea of a
state-of-the-art method, the local self-similarity matrix
(SSM) descriptor [19], and applied it to our data. For fair
comparisons, we used the same feature (i.e. HoDM
magnitude, HoDM orientation or both) and the same
distance metric (i.e. correlation coefficients) for
computing SSM. Radii of local log-polar structures are set
as 10, 30 and 60 frames (i.e. 1/3, 1 and 2 seconds
respectively). As suggested by [19], we computed the
accumulated Euclidean distance for final prediction.
Results of using different combinations of features are
reported in Table 1. For both operations, the proposed
method outperforms local SSM descriptor in most cases.
In the case of “Intracorporeal knot” operation of hand
view 2, our approach is slightly worse (the difference is
less than 0.14 frame). However, since the accuracy of the
ground-truth has a 2-frame margin, a difference of errors
less than 2 frames is practically not very meaningful. In
summary, the results in Table 1 suggest that the proposed
method is effective and can outperform the existing state
of the art approach.

B. Results and Analysis
We validated the proposed method on all the 41 sets of
videos collected from actions performed by resident
surgeons in local hospitals during their normal training
period in their residency. Fig. 5 reports the
synchronization errors of hand view 2 and tool view of
each set respectively and the average errors are listed in
Table 1 (last column).

(a) “Pegboard transferring”.

V. CONCLUSION AND FUTURE WORK
We proposed an approach for synchronizing two handview videos and one tool view video in simulation-based
laparoscopic operations. After extracting histograms of
dominant motion (HoDM) of each frame, multi-view
sequence correlation (MSC) is computed as accumulated
products of pairwise correlation of motion magnitudes and
co-occurrence rate of pairwise orientation. Experiments
were performed on 41 sets of videos of two laparoscopic
operations from real resident surgeons. The method
achieved very accurate alignment between the two hand

(b) “Intracorporeal knot”.
Fig. 5. Synchronization error (frame) plots of both operations.

As we can see in Fig. 5, errors of hand view 2 are
much smaller than those of the tool view. Average errors
of hand view 2 for both operations are below 1.5 frames,
which have achieved the limit of accuracy of the
manually-labeled ground-truth. For the tool view, average
errors of both operations are about 6 frames, which is

5

Changes in Video Sequences,” Proceedings of International
Conference on Pattern Recognition (ICPR), 2006.
[8] Y. Ukrainitz and M. Irani, “Aligning Sequences and Actions
by Maximizing Space-Time Correlations,” Proceedings of
European Conference on Computer Vision (ECCV), 2006.
[9] C. Dai, Y. Zheng, and X. Li, “Accurate Video Alignment
Using Phase Correlation,” IEEE Signal Processing Letters,
vol. 13(12): 737-740, 2006.
[10] A. Whitehead, R. Laganiere, and P. Bose, “Temporal
Synchronization of Video Sequences in Theory and in
Practice,” Proceedings of Workshop Motion and Video
Computing, vol. 2, 2005.
[11] C. Lei and Y. Yang, “Tri-focal tensor-based multiple video
synchronization with subframe optimization,” IEEE Trans.
On Image Procssing, vol. 15(9): 2473-2480, 2006.
[12] F.L.C. Padua, R.L. Carceroni, G.A.M.R. Santos, K.N.
Kutulakos, “Linear Sequence-to-Sequence Alignment,” IEEE
Trans. on Pattern Analysis and Machine Intelligence, vol.
32(2): 304-320, 2010.
[13] P. A. Tresadern, I. D. Reid, “Video synchronization from
human motion using rank constraints”, Computer Vision and
Image Understanding, vol. 113: 891-906, 2009.
[14] C. Rao, A. Gritai, M. Shah, and T.S. Mahmood, “ViewInvariant Alignment and Matching of Video Sequences,”
Proceedings of IEEE International Conference on Computer
Vision (ICCV), 2003.
[15] Y. Caspi, D. Simakov, and M. Irani, “Feature-Based
Sequence-to-Sequence Matching,” International Journal of
Computer Vision, vol. 68(1): 53-64, 2006.
[16] D. Wedge, P. Kovesi, D. Huynh, "Trajectory Based Video
Sequence Synchronization," Proceedings of Digital Image
Computing: Techniques and Applications, 2005.
[17] P. Shrestha, H. Wed, M. Barbieri and D. Sekulovski,
“Synchronization of multiple video recordings based on still
camera flashes,” Proceedings of ACM Multimedia, 2006.
[18] I.N. Junejo, E. Dexter, I. Laptev, and P. Pérez, “Cross-view
action recognition from temporal self-similarities,”
Proceedings of European Conference on Computer Vision
(ECCV), 2008.
[19]
E.
Dexter,
P.
Pérez
and
I.
Laptev,
“
Multi-view Synchronization of Human Actions and Dynamic
Scenes,” Proceedings of British Machine Vision Conference,
2009.
[20] B. D. Lucas and T. Kanade T, “An iterative image
registration technique with an application to stereo
vision,” Proceedings of Imaging Understanding Workshop,
1981.

views and with only small errors in synchronizing the tool
view to the hand views. Compared to existing work, our
approach does not require accurate camera calibration or
sophisticated detection and tracking. It is also extremely
view-invariant (i.e. able to deal with completely different
visual scenes: the hand views and the tool views) and
robust under subtle object motion in videos of
laparoscopic simulations.
There are some limitations in the current approach too.
For examples, the HoDM feature is constrained to static
cameras and static background although the MSC
algorithm is theoretically feature-independent. For further
improvements, orientation can be incorporated in
computing pairwise correlations of frames and magnitudes
can also be considered in estimating pairwise pattern cooccurrences.
Acknowledgement: The authors were partially
supported during this work by an NSF grant (Award #
0904778), which is greatly appreciated.

REFERENCES
[1] A. G. Gallagher, E. M. Ritter, H. Champion, G. Higgins, M.
P. Fried, G. Moses, C. D. Smith and R. M. Satava, “Virtual
Reality Simulation for the Operating Room. ProficiencyBased Training as a Paradigm Shift in Surgical Skills
Training,” Annals of Surgery, vol. 241: 364-372, 2005.
[2] L. Sutherland, P. Middleton, A. Anthony, J. Hamdorf, P.
Cregan, D. Scott and G. J. Maddern, “Surgical Simulation: A
Systematic Review,” Annals of Surgery, vol.243: 291-300,
2006.
[3] FLS trainer box: http://www.flsprogram.org/trainerbox.php.
[4] H.C. Lin, I. Shafran, D. Yuh, G.D. Hager, “Towards
automatic skill evaluation: Detection and segmentation of
robot-assisted surgical motions,” Computer Aided Surgery,
vol. 11(5), 2006.
[5] D. Wedge, D. Huynh, and P. Kovesi, “Using Space-Time
Interest Points for Video Sequence Synchronization,”
Proceedings of IAPR Conference on Machine Vision
Applications, 2007.
[6] J. Yan and M. Pollefeys, “Video Synchronization via SpaceTime Interest Point Distribution,” Proceedings of Advanced
Concepts for Intelligent Vision Systems, 2004.
[7] M. Ushizaki, T. Okatani, and K. Deguchi, “Video
Synchronization Based on Co-Occurrence of Appearance

6

3D ARTICULATED HUMAN BODY TRACKING USING KLD-ANNEALED
RAO-BLACKWELLISED PARTICLE FILTER

Jayanth Madapura I, Baoxin Li 2
'Dept. of Electrical Engineering, Arizona State University, Tempe, AZ, USA
2Dept. of Computer Science & Engineering, Arizona State University, Tempe, AZ, USA
{jayanth.madapura, baoxin.li}@asu.edu
outperforms regular Particle filter in terms of accuracy
with same computational constraints [8]. In addition,
when there is a large degree of freedom, RBPF becomes
significantly less intensive than regular Particle filter.
Even so, both APF and RBPF still face challenges in
terms of speed performance if they are to be used for realtime tracking with a high-dimensional articulated human
model.
In this paper a new approach is proposed, in which the
simulated annealing strategy is incorporated into a RaoBlackwellised Particle filter to build an Annealed RaoBlackwellised Particle filter (ARBPF), achieving better
accuracy. Further, the speed performance of the ARBPF
is also enhanced by adapting the number of particles in
each iteration of the ARBPF using Kullback-Leibler
Divergence (KLD) sampling [4]. We thus obtain a tracker
named KLD-ARBPF, whose performance gain in terms of
both accuracy and speed will be demonstrated with both
synthetic and real data.
A description of the proposed KLD-Annealed RaoBlackwellised Particle filter is provided in Section 2. An
articulated human body model is given in Section 3, to
which the tracker is applied. Results for simulated data
and real video with articulated human body motion are
presented in Section 4. Section 5 concludes the paper.
2. KLD-ANNEALED RAO-BLACKWELLISED
PARTICLE FILTER
The basic idea of the proposed approach is to combine
simulated annealing and adaptation of the number of
particles to form a Kullback-Leibler Divergence Annealed
Rao-Blackwellised Particle filter (KLD-ARBPF). A RaoBlackwellised Particle filter with annealing is
theoretically superior to regular Rao-Blackwellised
Particle filter or regular Particle filter due to the attractive
properties of reduced variance and the annealing effect
which enhances the accuracy of the estimate.
The proposed KLD-ARBPF algorithm is given in Table
1, with steps explained in detail in the following
paragraphs.
Step 1: Rao-Blackwellization of state space: The joint
posterior estimate can be directly obtained by a generic
Sequential Monte Carlo method. However in the instance
when there exists a tractable substructure between some
states in the state model, specific states can be

ABSTRACT

The difficulties introduced by large degrees of freedom
are still a challenge in articulated human body tracking. In
this paper, an efficient tracker is proposed based on the
integration of a set of statistical techniques including
KLD sampling, Rao-Blackwellisation, and Particle
filtering. This results in a KLD-Annealed Particle filter
with Rao-Blackwellisation, which can address the key
issues in 3D human tracking, such as accuracy, stability,
and speed simultaneously. Both synthetic and real data
were used in our experiments to demonstrate the
improved performance of the proposed tracker.

Keywords - Articulated human body tracking, Particle
filter, Rao-Blackwellisation, Simulated annealing,
Kullback-Leibler divergence.
1. INTRODUCTION
Articulated human body models with 25 or more degrees
of freedom have been extensively used in 3D tracking
from multiple cameras. Accuracy, stability, and speed are
of important concern in articulated human body tracking.
Estimation with a state model of large degrees of freedom
typically encounters many challenges in achieving all the
above three simultaneously. The regular Particle filter [1]
used in tracking the dynamic states become numerically
intensive as the number of particles used in the estimation
increases with the dimensionality of the state model,
although the accuracy of estimation increases with the
number of particles used. Any inaccuracy in the modeling
may also lead to unstable behavior of the algorithm which
is hard to diagnose due to the intricate relations among the
state variable.
Extensive research efforts have been devoted to address
these various issues. For example, the problem of
accuracy has been addressed to an extent by a recent
technique [2] in which a heuristic global optimization
strategy, simulated annealing, is fused with the Particle
filter. The resultant Annealed Particle filter (APF) was
shown to perform better than the regular Particle filter in
articulated human body tracking. Also, the RaoBlackwellised Particle filter (RBPF) [3] is used in a class
of state-space models in which a tractable structural
relation exists between the state variables. RBPF

1-4244-1017-7/07/$25.00 ©2007 IEEE

1950

ICME 2007

marginalized out of the posterior, leading to more
efficient algorithm. This tweak also reduces the variance
of the estimates. If the state model has the state set X =
{JR, L} and there exists tractable substructure between R
and L. The state L can be marginalized out. This is
justified by the following decomposition,

bounds e, 8, minimum number of samples

St

p

using Particle filter. In such a partitioned state space, root
states Rt and leaf states Lt are sampled at every time
sample using steps la. and lb.
Step 2: In this step a likelihood measure (Xte, zt ) is
used to compute the weights of each of the particles. The
weight of each particle is given by,
=

exp

(-V (Xt: Zt ))

O,n = O,nX

=

if

(3)

1

2

=p(Zt

Rt ) pRt Rt-l

falls into empty bin b then
k +1

b mark bin as non-empty
if n > n
then
Xmin

n.
n

=

k -1
2e

2,1I-

2

1 +

9(k- 1)

3
9

9(k -1)

Zi-}

end if
end if
=

n+1

while(n < nX and n < nX

)

4. Normalize the weights and form the state-space
w

Wt

n

i

and Xt =

Rt, Lt}

i=1t
5. Sampling states using Simulated Annealing

Xt =Simulated _ Annealing(Xt, wt )

KLD is always positive. KLD is zero if the distributions
are identical. Eqn. (4) gives the number of particles n that
guarantees with probability i-6 that KLD is less than e.
3
k -l

Xn
k

Particle filter. The relation between the number of
particles used and the accuracy of the estimates is
determined by a metric called Kullback-Leibler
divergence which measures how best the MC estimate of
the posterior density can match the true posterior density.
The smaller the divergence value, the better is the match.
Thus, the problem of achieving an optimal tradeoff
between the accuracy and speed reduces to the problem of
determining the number of particles at each iteration of
the Particle filter such that, with a probability the error
between the true posterior and the MC posterior estimate
is minimum. Suppose that we have two distributions p
and q, the KLD [4] is defined as,

X)

Rt Rt-l:t

Zx, ) wheret {Rt ,Lt }
wt p(zt
3. Update particle size nx using KLD transformation

(2)

qq(x))

O,k = 0

b.) Sample Lt from p (Lt Rt1, Rt-1, Lt-7, Zt)
2. Re-weighting using likelihood function

Step 3: The number of particles used to estimate the
state is a deciding factor in the accuracy and speed of the

KLD(p, q) = Zx p(x) log

=

n

do
1. Sampling the states in both partitions
a.) Sample Rt from

(1)
p(R,L IZ) =p(L R,Z)p(R Z)
The distribution p (L R, Z) is computed exactly using
Kalman filter while the distribution p (R z) is estimated

wt

i 1.n}, observation Zt,

Inputs: st-1 {Xt- , wt_

6. Kalman update for leaf states
Lt

=

Lt

Kalman Update (Lt, Zt (leaf), state space parameters)

where Zt = {Zt (root), Zt (leaf )} * Update
7. Formulate the particle set

2

9(k -1) + 9(k (4)
where z1 a is the upper i-6 quantile of the standard normal
2e {

Xt-

S~{K(xwi)i 1.}

distribution and k is the number of bins of the MC
posterior density estimate with support.
Step 4: The weights of the particles are then
normalized.
Step 5: Sampling states using Simulated Annealing:
In regular Particle filter if posterior density contains many
isolated modes, MCMC methods might give poor results
if at any time it gets stuck in the mode closest to the
starting point in spite of the fact that the mode is not a
very good representation of the true total probability.

return

St

Table 1. The proposed KLD-ARBPF algorithm
Simulated annealing is one such heuristic method which
helps the filter to move towards the global maxima of the
posterior weighting function. It performs the task of
stochastically searching the posterior for global maxima.
The process of sampling the states using simulated
annealing is described in detail in [2].

1951

Step 6: Kalman update for leaf states: The leaf states
are updated using the time update and measurement
update equations of Kalman filter.
Step 7: The particle set is formulated. Mean state can
be computed from this particle set using,

Y (n) = YJ(n - 1) + A.abs(randn) and Y4 (n) = B.Y2(n -1) + Y (n)
The measurement equations with observation noise v1(n)
and v2(n) are given by,
Y (n)OBs = Y, (n) + C.vl (n) and Y2(n)OBs = Y2(n) + D.v2 (n)
A, B, C, D form the system parameters.
()
We first compare a simple RBPF and Annealed RBFP
(xt) = i=1
(t
t
to demonstrate that the introduction of annealing into
3. ARTICULATED HUMAN BODY MODEL AND
RBFP improves accuracy of the tracker. Fig. 2 through
TRACKING
Fig. 4 illustrates this comparison. In Fig. 2 and Fig. 3,
plots show Y1 and Y2 location coordinates of the moving
The proposed algorithm in Sect. 2 is applied to articulated
point, with actual value in yellow, observation in red and
human body tracking with a model illustrated in Fig. 1. In
tracked
results in blue. Fig. 2 is for a regular RBPF while
this model, the location and orientation of each of the
3
is
for Annealed RBPF. A close look reveals that
Fig.
body parts is determined from the joint angles in the
ARBFP perform slightly better than RBPF. This can be
articulated model. We use the model of [5] with 25
seen from Fig. 4, where we plot MSE of tracked results:
degrees of freedom as shown in Fig 1.
ARBPF in green and RBPF in red (over several runs).
1.5
0

01

3

0 g00oa

sl7

RBPF: Leaf state estimated by Kalman
1.5
Actual value

RBPF: Root state estimated by PF

Actual value

]

~~, Observation
Nois

1

05

CZ.
2

global trans
global rot [0

x

y

-0.50

z

0 l

Fig 1. Articulated human body model with 25 DOFs.

~~Noisy Observation

-050

100

50
Time in sec--->

100

50
Time in sec--->

Fig 2. Left: RBPF: Root state estimated by PF; Right: RBPF:
Leaf state estimated by Kalman filter

Our prior study has shown that a strong dependency
relation exists between the set of variables on the righthand side of the human and those on the left-hand side
[6]. This structure is exploited naturally by the proposed
KLD-ARBPF through attributing the variables to either
the root or the leaf variables. Specifically, the large 25
DOF state space is partitioned into two. The root partition
consists of the following: global translation and rotation,
head, right shoulder joint and right hip joint rotations. The
leaf partition consists of the following: left shoulder joint
and left hip joint rotations. The root partition is estimated
using the KLD Annealed Particle filter portion of the
proposed algorithm (Steps 1-5) while the leaf partition is
determined by the Kalman filter (Step 6). In Fig 1, the
parts and joints represented by yellow and purple form the
root partition and those in blue form the leaf partition.
4. EXPERIMENTAL RESULTS
To systematically evaluate the proposed approach and
compare it with existing related work, we first design
simulated experiments as follows. Consider a point
moving on the 2-D plane. A true path of the point is
generated based on a non-linear motion model, and nonGaussian noise is then added to the actual path to simulate
a noisy measurement of the actual path. Then we evaluate
various algorithms to 'track' the true path using the noisy
path as the measurement. The state model has the state set
X = IY1, Y2}. The system equations and the relation
between the two states are given by,

Annealed RBPF: Root state estimated by PF

1.5

Actual value

A 1lAtAnneal
RBPFvalue
Anneal RBPF
value1

4

Noisy Observation
cz0.5-c 0O5-

^ 1

a.

0

C/)

Annealed RBPF: Leaf state estimated by Kalman
1.5

Actual velue
RBPFvalue
* Anneal
Noisy Observation

C/

-0.50

50

05 0

100

Time in sec--->

20

40

60

Time in sec--->

80

100

Fig 3. Left: ARBPF: Root state estimated by PF; Right:
ARBPF: Leaf state estimated by Kalman Filter.
A

:2 104

w

A 2X 10

MSE of ARBP

-MSEoRP

U

8w

\CCZ)°[

Cv)CZ~ 4,
a)

>

MSE of ARBPF
MSE of RBPF

w

2

2

6
8
Simulation Run --->
4

10

CD

7

'

2

8
4
6
Simulation Run --->

10

Fig 4. MSE between filtered output and ground truth of
ARBPF and RBPF. Left: for root state; Right: for leaf state.
Next, we compare a KLD-RBPF with a regular RBPF.
This is intended to verify the claimed gain in speed by
introducing the KLD sampling. Table 2 lists speed and
MSE performance for the two trackers in 10 independent
runs with random initialization and noise. It is found that,
with roughly the same MSE level, the KLD-RBPFis about
4 times faster than the regular RBPF.

1952

Time
(sec)
4.875
4.938
4.797
4.890
4.890
4.875
4.86
4.734
4.859
4.984

KLD-RBPF
Root
MSE

Leaf
MSE

Regular RBPF (N = 1000)
Time Root
Leaf
(sec)
MSE
MSE

8.4525
9.7110
14
7.7193
16
3.0789
7.5902
4.5509
7.7823
12

13
17
20
13
27
5.0727
12
7.6201
12
19

16.41
16.19
16.19
16.17
16.09
15.97
16.13
16.16
16.19
17.11

(xI0-4)

(xI0-4)

(xI0-4)

11
8.92
16
5.43
14
3.54
7.24
4.53
8.45
11

5. CONCLUSIONS
The proposed KLD-ARBPF aims at providing a practical
solution to the problem of articulated human body
tracking in terms of accuracy, stability and speed.
Simulated annealing is incorporated into the RaoBlackwellised Particle filter to provide a more efficient
tracker compared to regular RBPF. Further, KLD
sampling is used to adapt the filter to provide better
speed. Experiments results have shown that the proposed
KLD-ARBPF has improved accuracy and speed when
compared with the exiting related techniques, for both
simple synthetic data and real video with articulated
human motion.

(XIO-4)

16
15
24
8.86
23
5.82
11
7.24
13
18

-.;2
L-n..... tne TT
in rDTh'U ana
1S tne
i aTon
&LIJ-Jrr
riein
RB.uomparison
Detween
±1.

1.

1-

regular RBPF.

Finally, by combining KLD sampling and simulated
annealing together with Rao-Blackwellisation, we expect
to see performance gain in terms of both speed and
accuracy. While this was found to be the case for the
simulated data, we now present results from such a KLDARBPF tracker and compare with a regular RBFP, using
real 3D articulated human walking sequences as input. In
a typical run of the experiment, the elapsed time with
regular RBPF was 767.250 seconds for 1 sec (24 frames)
video with 1000 particles, and the mean sum of absolute
difference between the ground truth and the tracked poses
was found to be 2.7906. In experiments with KLDARBPF, the elapsed time, with 10 layers of annealing,
was 261.265 seconds for 1 sec (24 frames) video with
initial seed particle size of 1000, and the mean sum of
absolute difference between the ground truth and the
tracked results was found to be 2.6894. It can be clearly
observed from above elapsed time readings that
articulated human body tracking using KLD-ARBPF is
faster than the regular RBPF. In our experiments, KLDARBPF was found to be consistently 3 folds faster than
the regular RBPF. From the mean sum of absolute
difference readings the tracking accuracy of the KLDARBPF is found to be better when compared to that of
regular RBPF. One example of such experiments is given
in Fig. 5. (The experiments were performed using Matlab
on a 1.7GHz Pentium processor and 1 GB RAM. The
code was based on straightforward implementation of the
algorithm and was not optimized. This explains the
overall slow speed performance in either of the cases.)
The comparison of the results between regular RBFP and
regular Annealed Particle filter can be found in [6].
In conclusion, along with a tracking performance that
outperforms the regular RBPF because of the annealing
effect, the KLD-ARBPF tracker is much faster as well, as
predicted.

Fig 5. Articulated Human Body Tracking with KLD(top) and regular RBPF (bottom).
6. REFERENCES
[1] S. Arulampalam, S. Maskell, N. J. Gordon, and T. Clapp, "A
Tutorial on Particle filters for On-line Non-linear/Non-Gaussian
Bayesian Tracking", IEEE Transactions on Signal Processing,
Vol. 50(2), pages 174-188, February 2002.
[2] J. Deutscher, A. Blake, and I. Reid, "Articulated body motion
capture by annealed Particle filtering", Proceedings of IEEE
CVPR, 2000.
[3] A. Doucet, N. de Freitas, K. Murphy, and S. Russell. "RaoBlackwellised Particle filtering for dynamic Bayesian
networks". Proc. of the Sixteenth Conference on Uncertainty in
Artificial Intelligence, pages 176-- 183, Stanford, 2000.
[4] D. Fox, "Adapting the sample size in Particle filters through
KLD-sampling", International Journal of Robotics Research
(IJRR), 22, 2003.
[5] H. Sidenbladh, M.J. Black, and D.J. Fleet, "Stochastic Tracking
of 3D Human Figures Using 2D Image Motion", ECCV (2),
702-718, 2000.
[6] X. Xu and B. Li, "3D human body tracking using RaoBlackwellised Particle filter with a prior motion model",
manuscript submitted to ICIP 2007.
[7] G. Casella and C. Robert. "Rao-Blackwellisation of sampling
schemes", Biometrika, 83(1):84--94, 1996.
[8] X. Xu, B. Li, "Adaptive Rao-Blackwellised Particle filter and its
evaluation for tracking in surveillance", IEEE Transactions on
Image Processing, vol. 16, issue 3, pp. 838-849, 2007.

1953

Instant Tactile-Audio Map: Enabling Access to Digital 

Maps for People with Visual Impairment 

Zheshen Wang, Baoxin Li

Terri Hedgpeth, Teresa Haven

Arizona State University
Computer Science & Engineering
699 South Mill Avenue
Tempe, AZ 85281, USA
001-480-965-1735
{zheshen.wang, baoxin.li}@asu.edu

Arizona State University
Disability Resource Center
Matthews Center, P.O. Box 873202
Tempe, AZ 85287, USA
001-480-965-3366
{terrih, teresa.haven}@asu.edu
may make use of textual descriptions of the directions given by
the map services, using text-to-Braille/speech software, an instant
and direct access to the map image itself is still unavailable.
Although there exist techniques that contribute to helping tactile
specialists in converting a map image into a tactile format, the
procedures involved are typically time-consuming and labor
intensive, thus preventing the individual who is blind from
accessing the map image in real-time and independently of others.
In this paper, we propose a solution for creating interactive
tactile-audio map automatically, which aims at making digital
visual maps of local directions accessible for visually impaired
people in real-time. To the best of our knowledge, there is no
existing systematical solution for solving such a challenging task
automatically. This work was motivated by complaints on lacking
access to maps by people who are blind in our field study.

ABSTRACT
In this paper, we propose an automatic approach, complete with a
prototype system, for supporting instant access to maps for local
navigation by people with visual impairment. The approach first
detects and segments texts from a map image and recreates the
remaining graphical parts in a tactile form which can be
reproduced immediately through a tactile printer. Then, it
generates an SVG (Scalable Vector Graphics) file, which
integrates both text and graphical information. The tactile
hardcopy and the SVG file together are used to provide a user
with interactive access to the map image through a touchpad,
resulting in a tactile-audio representation of the original input
image. This supports real-time access to the map without tedious
conversion by a sighted professional. Evaluations with six users
who are blind show that the created tactile-audio maps from our
prototype system convey the most important map information and
are deemed as potentially useful for local navigation.

Mill
Avenue

Categories and Subject Descriptors
I.4 [Image Processing and Computer Vision] Text detection,
segmentation and aligning, tactile graphics recreation; H.5.2
[Information Interfaces and Presentation]: User Interface –
haptic I/O, interactive devices and strategies, auditory feedback.

General Terms
Algorithms, Design, Experimentation, Human Factors.

Keywords

Figure 1. Interactive tactile-audio map exploration.

Tactile map, accessibility, multi-modal system, visual impairment

Imagine that a person who is blind is planning a walk to her/his
new office from home. With on-line map services, she/he can
easily obtain the directions and a corresponding digital map
image. However, she/he cannot access the map image itself. With
assistance from a sighted people, the person may obtain some
verbal descriptions of the map/directions, but the geospatial grasp
of the underlying map by a sighted people cannot be conveyed
easily to the person with visual impairment with such purely
verbal descriptions. Our interactive tactile-audio map system
attempts to give the user direct access to the rich geospatial
information in a map. The basic usage model of the system is
illustrated in Figure 1, where a printout of the created tactile map
is placed on a touchpad so that the user can explore the map
freely by hands while getting instant audio feedback on where her
fingers are touching.

1. INTRODUCTION
Maps are widely used by people for their travel planning and
navigation, even for local and short trips. Free on-line map
services, such as Google Map [1] and MapQuest [2], further make
instant access to geographical information easily available in our
everyday life. Unfortunately, people with visual impairment are
partially or completely deprived of such benefits. Even if they
Permission to make digital or hard copies of all or part of this work for

personal or classroom use is granted without fee provided that copies are 

not made or distributed for profit or commercial advantage and that 

copies bear this notice and the full citation on the first page. To copy

otherwise, or republish, to post on servers or to redistribute to lists,

requires prior specific permission and/or a fee.

ASSETS’09, October 25-28, 2009, Pittsburgh, Pennsylvania, USA. 

Copyright 2009 ACM 978-1-60558-558-1/09/10...$10.00. 


43

In recent years, efforts have been devoted to improving the
tedious manual conversion procedure and preserving more
information via latest computer technology. A semi-automated
visual-to-tactile conversion approach for graphics was proposed
in [5]. However, it aimed at assisting tactile specialists instead of
end users with visual impairments and it was not designed for
maps but graphics from textbook (e.g. charts). In [6], the concept
of a virtual tactile map was described, which aims at supporting
both map exploration and learning of a route using hand gestures
as user feedback. In such a scheme, users can only obtain audio
descriptions of the map rather than a direct haptic exploration of
an actual tactile map. The idea was tested with only one blind
user. Another project reported in [7] created an interactive tactile
map of the Michigan State University campus by using CAD
software and Nomad electronic touch tablet. Benjamin et al
proposed an interactive SVG map [8] for people with visual
impairment, which employed similar framework to that of our
work. However, in the work of both [7] and [8], the essential task
of creating a tactile/SVG map was done manually (albeit
facilitated by some graphics drawing software). An automated
module is indispensible if the system is to be used by the actual
user independent of assistance from sighted people.

Figure 2 further illustrates the details of a prototypical setup of an
initial system used in this study for implementing the proposed
approach, where the blue arrows depict the processing flow
involved in the system. While in this figure, we used the Laser
Printer and a Thermal Enhancer for creating the tactile printout,
other alternatives (e.g., using a tactile embosser) are also possible.

Figure 2. Real setup of the proposed system in our lab
(Combination of the inkjet/laser printer and the thermal
enhancer can be replaced by a tactile embosser).

3. A MULTI-MODAL AND INTERACTIVE
APPROACH TO ACCESSIBLE MAP

In the remaining parts of the paper, we first review related work
on assistive techniques for creating tactile maps from a visual
form. Then we describe our interactive and multi-modal approach
for creating tactile-audio map in Section 3, followed by system
evaluations with the actual end-users in Section 4. We conclude in
Section 5 with brief discussion on limitations of the approach and
our future work.

Automatic tactile-audio map creation is the key component of our
proposed interactive solution. The step takes a single map image
(without any associated meta-data) as input and generates output
image for easy tactile reproduction and an SVG file which
combines graphical contents with audio annotations. Constrained
by our application scenario, where maps are typically of a
walking distance, a map image generally consists of a network of
roads/streets, local landmarks and their corresponding text labels.

2. RELATED WORK
Traditionally, tactile maps are created manually as general tactile
graphics. Edman’s book [3] provides detailed guidance for
creating tactile graphics of various types including tactile maps.
Here, we briefly summarize some key subtasks that a tactile
specialist may complete during a tactile map creation process.
First of all, specialists usually spend some time to determine the
best method to use based on the characteristics of the map (e.g.,
the scale and the complexity of the map) and what a user will use
the map for (which helps selecting the most important information
for tactile representation). Then, they would typically draw an
outline (e.g. major roads and borders) with the most informative
and important elements or use a scanned image, printout, or
digital file to create an outline of the graphics by drawing on top
of it. Further, they add textures to regions for representing
different geographical features (such as mountains, water areas,
etc.) and insert pre-designed tactile symbols for indicating
different types of landmarks (such as houses, traffic lights,
railroad, etc.). After that Braille texts are created, serving as
annotations for the graphics or explanations for the legends. The
above procedure for creating a tactile map is typically timeconsuming and labor-intensive, and thus does not support instant
and independent access to maps by visually-impaired users. In
addition, due to the limited bandwidth and resolution of haptic
sense, tactile maps should be kept simple [3-4], and thus relying
on purely tactile representations created in the above process
prevents a lot of information from being included.

Figure 3. Processing flow of the proposed approach.
Figure 3 illustrates the major processing flow of the proposed
solution for creating a tactile-audio map from its visual form.
Given a pre-stored digital map image, our approach first detects
the embedded text and segments them from the graphical parts.
Then, the extracted texts and the remaining graphics are
processed separately. For the text, a series of aligning,
segmentation, enhancing and recognition steps are performed for
further obtaining the contents. For the graphics, we retain only the
most important information while removing unnecessary details
and represent the simplified map image in an appropriate form for

44

text detection result, and (c) shows the estimated centers and
bounding boxes of the detected text regions.

tactile printing. We further integrate the text and graphical parts
in a compact SVG form which enables interactive exploration of
the map using a touchpad. By using our prototype system, the
process requires only minimal action from the user (e.g., clicking
and choosing a map image and then sending it to the program for
processing), and the entire procedure of creating the output will
be done automatically without any manual intervention.
In the following, we first present text detection, segmentation and
aligning algorithms (Section 3.1). Recreation steps of the
graphical parts are then introduced in Section 3.2, with multimodal integration of the tactile-audio map contents being
described in the end of this section.

(a)
(b)
(c)
Figure 4. An example of text detection.

3.1.2 Connected Text Segmentation

3.1 Text Detection, Segmentation and
Alignment

In maps, text labels for nearby streets are often closely located,
resulting in connected texts from the detection stage. Figure 6-(a)
depicts one such example. Simple local grouping strategy
performed in the previous step takes the connected components as
a single text region, which causes potential troubles for the
following character recognition. In the below, we propose an
algorithm for segmenting connected text regions.

The first technical challenge in this step is how to accurately
extract texts from graphics. Text detection in images has been a
research topic for many years. Although a lot of efforts have been
devoted (such as [9, 10]), it remains to be fully solved due to the
complexity of the problem. For text recognition, current popular
Optical Character Recognition (OCR) software is relatively
mature for well segmented, normalized, aligned and enhanced
text images, but it still has difficulties with texts in complex cases,
such as low resolution, special fonts, varying sizes, tilted texts,
etc. In maps, all the above difficulties may occur and thus text
detection and recognition are non-trivial tasks. In our proposed
solution, we first detect text regions by examining local spatial
frequencies. Segmentation is then performed for connected text
regions. We estimate the orientations of the extracted text regions
and then align them to be uniformly horizontal. Images of text
regions are further enhanced before being sent to the OCR
component. Details of the processing steps are presented in the
below (Figure 4, 6, 8 and 10 illustrate each step with a sample
map and real output results from our prototype system).

Input: Binary image T of connected texts.
Output: Binary images T’of segmented texts.
1. Tthin = Morphthin(T);
2. L = HoughLine (Tthin);
3. L’= MergeLines (L);
4. T’ = GetSegs (T, L’)
5. return T’;
Figure 5. Connected region segmentation algorithm.

3.1.1 Text Detection
Given an image I, for each pixel Ix,y we compute its local spatial
frequency as the sum of spatial frequencies along horizontal,
vertical and two diagonal directions:

f x, y = f x(,hy) + f x(,vy) + f x(,dy1 ) + f x(d, y2 )
in which f x,( hy) =

(c)

(1)

∑ (Ix+k, y − Ix+k+1, y ) , fx(,vy) = ∑ (Ix, y+k −Ix, y+k+1) ,

(d)

k =−h

(e)

h−1


(d )
f x(,dy1 ) = ∑ (Ix+k , y+k − I x+k +1, y+k +1) , fx, y2
 = ∑(Ix+k, y−k −Ix+k+1, y−k+1) .
k =−h

(b)

h−1

h−1

k=−h
h−1

(a)




k=−h 


Then, we generate a binary map of the text regions by
thresholding the spatial frequency map f computed in (1)

⎧1, f x , y > threshold
Tx , y = ⎨
⎩0, otherwise

(f)
(g)
(h)
Figure 6. Intermediate results of connected text segmentation
procedure: (a) Original map image of connected texts; (b)
Binary image of detected texts (input for the algorithm in
Figure 5); (c) Skeleton of the text region (result of step 1); (d)
Detected straight lines (result of step 2); (e) Merged lines (result
of step 3); (f)-(h) Binary images of segmented texts (result of
step 4).

(2)

We further estimate the centers and bounding boxes of the text
regions by local grouping and averaging. Figure 4 shows an
example, in which (a) is the original map image, (b) is the binary

45

Given a binary image with connected text regions (e.g., Figure 6
(b)), the skeleton of the text regions is first extracted by using
Morphological Thinning algorithm [11] (results shown in Figure
6-(c)), and Hough transformation [12] follows for detecting
straight lines (results shown in Figure 6-(d)). We further merge
the lines with similar orientations and retrieve the original binary
text regions for each line by checking the nearby regions of the
line segments in the original binary image. Figure 5 gives the
processing steps of the proposed algorithm and Figure 6 illustrates
intermediate results from the entire procedure.

3.2 Tactile Graphics Recreation
In the above, we have discussed strategies for processing texts in
the map. For the remaining graphic parts, our approach recreates
them in a tactile form. To accommodate the limited bandwidth
and resolution of the haptic sense, only the most important
information for the map (e.g. key symbols, road/street network) is
kept while other distracting or non-essential details (e.g. color) are
discarded. However, the recreation here is not simple edge
detection. It includes image enhancement, refinement,
background removal and special tactile symbols (e.g.,
raised/sunken dots for indicating interactive text regions)
insertion. The recreated map is then converted into JPEG format
that can be reproduced in a tactile form directly. Figure 9
describes the major steps for recreating the graphical parts of an
input map and Figure 10 presents some intermediate results of the
entire process.

3.1.3 Orientation Estimation and Normalization
In the previous steps, we obtained separated text regions of a map
image. However, the extracted text regions are usually slanted,
which is challenging for current popular OCR software. Thus,
before moving onto the OCR stage, we estimate the local
orientations of the text regions and further align the regions to be
horizontal. Since typically in a map image, each text region
contains only a single row, we first find out the ellipse that has the
same normalized second central moments as the region and then
compute the orientation as the angle between the major axis and
the horizontal axis. Figure 7 illustrates this idea. We use
regionprops function in Matlab [13] in our implementation.

Input: A gray-scale map image I with text pixel
removed.
Output: A binary map image with specific
symbols for text regions I’.
1. I = ContrastEnhance (I);
2. Ibw = Thresholding (I);
3. Ibw = NoiseFilter (Ibw);
4. Ibw = Morphclose (Ibw) [11];
5. I’ = InsertSymbol (Ibw).

(a)

(b)

6. return I’

(c)

Figure 7. Illustrations of orientation estimation and alignment
of text regions.

Figure 9. Tactile graphics recreation algorithm.

3.1.4 Text Region Post-processing
To this point, we have obtained separated and aligned text
regions. However, the regions are still in the form of an image
which can be noisy due to the low image resolution or its complex
background. To facilitate accurate character recognition, we
further perform a series of steps including enlarging the image
size, increasing the contrast and further binarizing the image.
Figure 8 presents some sample results from this step. Compared to
Figure 7-(c), texts in Figure 8 are of much better quality for
further OCR.

(a)

(b)

(c)

(d)

(e)

(f)

3.1.5 OCR and Abbreviation Expansion
In our prototype system, we use Nuance-Omnipage OCR SDK 16
[14] for character recognition. In order to ensure proper audio
interpretation of some of the abbreviations commonly used in
maps, we further expand such cases into their complete forms
before integrating them into the SVG file. For example, we
convert “E.”, “W.”, “S.”, “N.” to “East”, “West”, “South” and
“North” respectively, “Ave.” to “Avenue”, “St.” to “Street”,
“Ln.” to “Lane”, “Rd.” to “Road”, etc.

Figure 10. Intermediate results of tactile map graphics
recreation: (a) Gray-scale map image with text pixels
removed (input image for the algorithm shown in Figure 9);
(b) Contrast enhanced image (result of step 1); (c) Binary
image (result of step 2); (d) Noises filtered image (result of
step 3); (e) Further refined image (result of step 4); (f) Final
result with tactile symbols for interactive text regions inserted
(result of step 5).

Figure 8. Post-processing results of Figure 7-(c).

46

default printer and the integrated SVG file will be opened by the
default SVG viewer automatically.

In consultation with tactile specialists and the actual users who
are blind, we determined that “single raised dot” and “single
sunken dot” are good options for indicating the interactive text
regions (regions for audio descriptions) which are associated with
roads/streets or landmarks. For solid regions, we add an additional
sunken square surrounding the “raised dot” in order to make the
dot easily recognizable. Figure 11 shows a few examples.

3.3.3 Thermal Enhancer with Swell-paper
Tactile printout of the recreated map can be easily reproduced
using a regular printer plus a thermal enhancer with swell-paper.
Swell-paper, also known as Minolta or Microcapsule paper, is
regular paper with a special coating of heat sensitive chemicals.
When exposed to heat (e.g., going through a thermal enhancer),
embedded microcapsules of alcohol in the regions with black ink
burst and make the surface of the paper swell up. A graph/image
can be drawn or printed on the swell-paper directly using an
inkjet/laser printer or transferred using a photocopier from a
printed copy on regular paper. If we transfer a visual map onto
swell-paper directly, the text labels would be too cluttered to
recognize. In addition, because only one level of relief is possible
for swell paper, as we can imagine, except for the informative
lines and dots, any non-blank background would also rise, which
would completely mess up the entire map. Thus, any attempts to
transfer a visual map onto swell-paper as an exact tactile version
of the original visual copy would typically fail. This underlines
that our graphics recreation step is indispensable for producing an
informative tactile map. A thermal enhancer is a simple heater
designed for heating printed swell-paper.

Figure 11. Tactile symbols used for interactive text regions.

3.3 Multi-modal Integration and Rendering
The recreated map image is then reproduced through a tactile
embosser or using a regular printer plus a thermal enhancer with
swell-paper. As we mentioned previously, the graphics of a tactile
map must be simple for easy haptic exploration. However, the
reduced visual information is possible to be compensated for to
some extent by an increase in information conveyed by audio. In
our approach, we integrate the extracted text contents and the
recreated graphical map image into a single SVG file, which
supports an interactive tactile-audio exploration of the map by
using a tactile touchpad. In this section, we discuss the multimodal integration of the entire system including automatic SVG
file generation, user interface and a brief introduction to the
hardware devices used in our prototype system.

3.3.4 Tactile Embosser
An alternative way of producing a tactile copy of the map is using
a tactile embosser (we use ViewPlus Cub Jr. Embossor in our
experiments), which punches dots into the printer paper. Graphics
are represented by dots with different densities for different visual
intensities, i.e. dense dots for dark regions, sparse dots for light
regions, and limited number of dot patterns for representing
different visual patterns. Compared to using a thermal enhancer
with swell-paper, using a tactile embosser requires only one-step
reproduction and the paper is much cheaper. However, the
produced tactile copy is not easily visible to a sighted person,
which might be a disadvantage if the resultant map may need to
be used by a sighted person in any manner (e.g., for verifying
whether the given direction is correct).

3.3.1 Automatic SVG generation
Scalable Vector Graphics (SVG) is a family of specifications of
XML-based file format for describing two-dimensional vector
graphics. By using SVG format, audio descriptions can be
spatially associated with graphical contents naturally. As we saw
in Figure 1 and 2, with a touchpad connected to the computer
equipped with SVG viewing software (such as IVEO Viewer [15]
and Adobe SVG viewer [16] which are freely available on-line),
the map information is interactively accessible for the users who
are visually impaired.

3.3.5 Interactive Tactile Touchpad
A produced tactile map is finally placed on top of a touchpad, a
touch sensitive tablet connected to a computer. With the
corresponding SVG map, it can react, when different regions of
the graphics are pressed, by producing voice descriptions for that
region. In our experiments, we use IVEO Touchpad [17] (as
shown in Figure 1 and 2) with IVEO Viewer software [15] for
such interactive exploration. In practice, a simple calibration step
is required before the system can be used. By following audio
instructions, a user can independently perform the calibration step
by touching the top-left and bottom-right corners of the bounding
box of the map. Then, users can explore the tactile map on the
touchpad and instantly get audio annotations associated with the
map graphics by simply pressing the inserted symbols (illustrated
in Figure 11).

SVG format was also adopted in the previous work for tactile
graphics rendering (e.g. in [8]). However, to the best of our
knowledge, most existing work creates SVG graphics relying on
third-party software (such as IVEO Creator [17] and Adobe
Illustrator [18]) and requires manual efforts for drawing graphics,
creating interactive regions, adding text descriptions and sound
effects, etc. In our approach, we developed an automatic SVG
generator, which takes the extracted texts and recreated graphics
as input and outputs an SVG file including all the multi-media
information without any human intervention.

3.3.2 User Interface
The proposed tactile-audio map recreation approach is completely
automatic: Given a digital map image, by right-clicking the image
and choosing our program, the map image can be recreated
automatically within 20 seconds on a regular PC (e.g. 2GHz CPU
and 2GB memory). The simplified map image will be sent to the

4. SYSTEM EVALUATION WITH END
USERS
We performed limited objective evaluation on the text extraction
module, which achieved about 100% accuracy detection and 90%

47

accuracy recognition based on a dataset of 23 maps (among 549
text labels in the maps, all of them were successfully detected and
584 of them were correctly recognized). Since the modules can be
improved and some of modules are based third-party software,
our focus of evaluation of the work at this stage is on subjective
assessment (by users who are blind) of the usefulness of the
interactive tactile-audio maps created by our approach. We
recruited six volunteers who are blind as the evaluator of our
system, including two female and four male subjects, aging from
20 to 40. One of them is a Braille proofreader, two are college
students, and the remaining three are local residents. Each of them
was asked to assess their experiences with maps and tactile
graphics respectively (Question 1 and 2 in Figure 15) by giving a
self-rating from 0-“never used before” to 10-“very familiar with”.
Figure 12 shows the statistics. The participants have diverse prior
exposure to maps and tactile graphics. For example, participant 4,
who was a pilot before he lost his sight, rated his experience with
maps as 10; participant 6, who has never used map before, rated
her map experience as 0. Overall, they are familiar with maps to
some extent (average score = 6.3) and most of them do not have
much experience with tactile graphics (average score = 4.5).

a. Map 1

b. Map 2

c. Map 3

Figure 12. Statistics of survey questions 1-2: Prior map/tactile
graphics experience. Q1-Average map experience score = 6.3;
Q2-Average tactile graphics experience score = 4.5.

4.1 Navigation Tasks
We selected five examples, from relatively simple to relatively
complex, for our evaluation. Figure 13 illustrates the visual maps
and their corresponding tactile versions produced from our system.
Visually, our converted tactile versions of the maps faithfully
depicted the original road/street network, and the embedded text
labels were accurately indicated by tactile symbols (raised or
sunken dots) which are associated with audio annotations.

d. Map 4

Navigation tasks were designed for evaluating how effective our
interactive tactile maps conveying geographical information for
local route planning and walking navigation. A short training
section was given before the main course of navigation tasks in
order to prepare the user to be familiar with the layout of the maps,
the tactile patterns used in the maps, and the interactive tactileaudio features provided by our approach. (The users were given a
sample tactile-audio map for free explorations with instructions or
help (if necessary) from a sighted instructor.)

e. Map 5
Figure 13. Maps used for user evaluation: Left columnOriginal visual maps; Right column-Corresponding tactile
maps. Red circles: landmark 1; Green squares: landmark 2.
In the main course, we asked each participant to touch the five
examples one by one, locate two landmarks (such as an
intersection of roads, a park, a structure, etc.) in each map
(indicated by circles in Figure 13) and figure out the route from
one landmark to the other. If the participant successfully points
out a landmark or is able to trace the route between two

48

deemed our interactive tactile-audio map as very informative in
conveying geospatial information, quite easy to use, and helpful
for walking navigation (the average score of the overall rating
questions achieves 8 out of 10).

landmarks, we record the score of her/his task completion as 1,
otherwise as 0. Figure 14 illustrates the accuracies (average scores)
of their performances. We can see that all participants were
correct for most landmarks/routes (average score up to 1 for many
cases) except landmark 1 in map 2 and the route in map 5 (two of
the participants failed in these two tasks). Another observation in
our experiment is that most participants were slow with the first
map although it is the simplest one among the five examples.
Most of them were relatively fast with map 2-4 and much slower
with the last one, which is much more complex than the other four
maps.

(1) Before today, how experienced were you with
maps (either visual map or tactile map)?
(2) Before today, how experienced were you with
tactile graphics?
(3) How easy is it to follow the roads/streets in the
maps?
(4) How easy is it to locate the landmarks?
(5) How easy is it to locate the tactile symbols
(raised/sunken dots) for interactive text regions?
(6) How easy is it to associate the audio
descriptions
to
the
corresponding
roads/streets/landmarks?
(7) How easy is it to trace the route from a specific
landmark to another?
(8) Overall, do you feel the interactive map is easy
to use?

Figure 14. Navigation performances: Average score of
locating landmark 1 = 0.93; Average score of locating
landmark 2 = 1; Average score of route tracing = 0.87.

(9) Does the interactive tactile map provide all the
information you need from a map?
(10) Overall, do you feel it is helpful to use such an
interactive
tactile-audio
map
for
walking
navigation?

4.2 User Experience Survey
The six volunteers also participated in a follow-up survey after
the navigation tasks. Survey questions are presented in Figure 15.
Question 1 and 2 are self-rating questions of prior experiences
with maps or tactile graphics (as we discussed in the beginning of
Section 4). Question 3-7 were designed for assessing the clarities
of the tactile patterns used in our tactile maps (e.g., solid regions
for landmarks, raised/sunken dots for interactive text regions),
Question 8-10 are for evaluating the overall effectiveness of the
interactive tactile-audio map, and the last two questions are used
for collecting other comments and feedbacks. The first ten
questions are rating questions with possible scores from 0 to 10
(from extremely negative to extremely positive).

(11) If you didn’t give a full mark to question (9),
what other information you need but not provided
in the interactive tactile-audio map?
(12) Do you have any other comments/suggestions
regarding the interactive tactile-audio map?
Figure 15. Follow-up survey questions.
The first participant gave relatively lower scores to all the rating
questions. This was partially due to a flaw of our experiment
procedure in the beginning of the experiments with the first
participant. As we mentioned in the previous part, the touchpad
needs to be calibrated before it is used. For the first user, we
skipped the calibration steps for the second map, which caused
inaccurate correspondences of the interactive text regions between
the tactile copies and the SVG file (Although the maps are of the
same size, offsets may still occur since the paper were not placed
at the exact same position on the touchpad). We immediately
revised the experiment procedure by including a calibration step
for each map for the remaining experiments and it actually did not
have any impact on the navigation performance of the first
participant. However, it still left a negative impression of the
clarities of the patterns to the first participant. Excluding the
ratings from first user, the average scores of Question 3-7 and 8
10 achieve 7.2 and 8.5 respectively.

Figure 16 and 17 show the statistics of Question 3-7 and Question
8-10 respectively. We can see that most of the users found that the
patterns used in the tactile maps are informative (the average
score is around 7 out of 10). There are two major disadvantages
mentioned by the participants that prevented them from giving
higher scores: (1) Some of the users were trained to access tactile
printouts by using two hands. When they trace the route from one
landmark to another, they have to hold the first landmark by one
hand, which makes it difficult for them to follow the roads by the
other hand (a single hand). (2) Some of the users, who do not
have much map experience, were confused by discontinued
segments of roads with the same name. To the first case, we
would suggest the users use a marker (e.g. a pin) to mark the
found landmarks when they start to explore the rest parts of the
map. Regarding the second problem, we would provide more
instructions of using maps before the experiments for users who
do not have much map experience. These revisions may be used
in our future experiments. Overall (Question 8-10), most of the
participants were excited with the interactive map exploration and

Other suggestions from the participants include: (1) To add
additional information, such as locations of audio-enabled traffic
lights (for planning a relative safer walking route), locations of
restaurants and major stores, to the map; (2) To add more
interactive text regions to a long road, which may helpful for

49

problem, which is important for interface design and real system
development. Extensive subjective evaluations, including
evaluations of using a created audio-tactile map, creating audiotactile maps from general online maps of walking distance and
comparisons to tactile maps created by traditional manual
procedures are also listed in our future work.

tracing a long road; (3) To make the system portable for real-time
outdoor walking navigation. Those suggestions will be considered
in our future work for further improvements.

6. ACKNOWLEDGMENTS
The authors would like to thank Dirk Colbry for helpful
discussion concerning SVG graphics scaling and IVEO Touchpad
calibration.
This work is supported in part by an NSF grant (Award #
0845469).
Figure 16. Statistics of survey questions 3-7: Clarity questions.
Average score = 6.8 (7.2 excluding the first user).

7. REFERENCES
[1]	 Google Map: http://maps.google.com/.
[2]	 MapQuest: http://www.mapquest.com/.
[3]	 Edman, P.K. 1992. Tactile Graphics. AFB Press, New York:
American Foundation for the Blind.

[4]	 Eriksson, Y. 1999. How to make tactile pictures understandable
[5]	

Figure 17. Statistics of survey questions 8-10: Overall
assessments. Average score = 8.0 (8.5 excluding the first user).

[6]	

5. CONCLUSION AND FUTURE WORK

[7]	

In this paper, we proposed a solution for creating interactive
tactile-audio map for visually impaired people. Taking a single
map image as the input, our system automatically creates an
accessible tactile map augmented with audio descriptions.
Interactive exploration on the tactile map image with audio
feedback allows better access to map information than pure
textural descriptions or traditional static tactile maps. The
automatic and instant processing procedure and the easy-to-use
user interface of our approach enable direct and independent
access to geographical information for visually impaired people.
Evaluations with six users who are blind confirmed the
effectiveness of the proposed approach. In addition to its potential
assistance to a user who is blind, the components of our approach
may also be helpful for people who routinely make tactile maps
by hand for users who are blind.

[8]	
[9]	

[10]

[11]
[12]

The proposed approach and especially the implemented prototype
system have some limitations. In the text extraction step, text
segmentation and alignment components may fail when the input
region contains multiple lines of text. The post-processing (image
enhancing) steps are also imperfect. For graphics conversion,
current simplification steps are mainly based on low-level
features without high-level semantic guidance, which may lead to
broken roads in the final tactile map if the resolution of the input
image is low or the roads in the map are interrupted by text labels.
More sophisticated solutions to these problems are required for
further improvement. In addition, more efforts need to be devoted
to the study on the human-computer interaction aspect of the

[13]
[14]
[15]
[16]
[17]
[18]

50

to the blind reader. 65th IFLA Council and General Conference,
Bangkok, Thailand.
Jayant, C., M. Renzelmann, D. Wen, S. Krisnandi, R. Ladner, D.
Comden. 2007. Automated Tactile Graphics Translation: In the
Field. In proceedings of the 9th International ACM
SIGACCESS Conference on Computers and Accessibility
(ASSETS): 75-82.
Schneider, J. and T. Strothotte. 1999. Virtual Tactile Maps. In
proceedings of the 8th International Conference on HumanComputer Interaction (HCI). Vol. 1: 531-535.
Michael J. Hudson. 1998. The Michigan State University
Talking Tactile Map Project: Advancement Through
Collaboration. CSUN.
Campin, B., W. McCurdy, L. Brunet, E. Siekierska. 2003. SVG
Maps for People with Visual Impairment. SVG OPEN
conference.
Liu, Y., S. Goto, T. Ikenaga. 2005. A Robust Algorithm for Text
Detection in Color Images. In proceedings of the 8th
International Conference on Document Analysis and
Recognition (ICDAR): 399-405.
Kim, K. I., K. Jung, J. H. Kim. 2003. Texture-based approach
for text detection in images using support vector machines and
continuously adaptive mean shift algorithm. IEEE Transactions
on Pattern Analysis and Machine Intelligence (PAMI): vol.
25(12): 1631-1639.
Gonzalez, R and R. Woods. 2008. Digital Image Processing.
Third Edition, Prentice Hall.
Duda, O. and P. Hart. 1972. Use of the Hough Transformation to
Detect Lines and Curves in Pictures. Comm. ACM, vol. 15: 11–
15.
Matlab software: http://www.mathworks.com/.
Nuance-Omnipage OCR SDK 16:
http://www.nuance.com/omnipage/capturesdk/.
IVEO Viewer:
http://www.viewplus.com/support/downloads/IVEOViewer/.
Adobe SVG Viwer: http://www.adobe.com/svg/viewer/install/.
IVEO touchpad and IVEO creator:
http://www.viewplus.com/products/touch-audio- learning/IVEO/
Adobe Illustrator:
http://tryit.adobe.com/us/cs4/illustrator_d/?sdid=EPTCQ.

Circular Generalized Cylinder Fitting for 3D Reconstruction in Endoscopic
Imaging Based on MRF
Jin Zhou1, Ananya Das2, Feng Li2, Baoxin Li1
1
Dept. of Computer Science & Engineering, Arizona State University
2
Division of Gastroenterology & Hepatology, Mayo Clinic Arizona
{jinzhou,Baoxin.Li}@asu.edu, {das.ananya,li.feng}@mayo.edu

Abstract
Endoscopy has become an established procedure for the
diagnosis and therapy of various gastrointestinal (GI)
ailments, and has also emerged as a commonly-used
technique for minimally-invasive surgery. Most existing
endoscopes are monocular, with stereo-endoscopy facing
practical difficulties, preventing the physicians/surgeons
from having a desired, realistic 3D view. Traditional
monocular 3D reconstruction approaches (e.g., structure
from motion) face extraordinary challenges for this
application due to issues including noisy data, lack of
textures supporting robust feature matching, nonrigidity of
the objects, and glare artifacts from the imaging process,
etc. In this paper, we propose a method to automatically
reconstruct 3D structure from a monocular endoscopic
video. Our approach attempts to address the above
challenges by incorporating a Circular Generalized
Cylinder (CGC) model in 3D reconstruction. The CGC
model is decomposed as a series of 3D circles. To
reconstruct this model, we formulate the problem as one of
Maximum a posteriori estimation within a Markov
Random Field framework, so as to ensure the smoothness
constraints of the CGC model and to support robust
search for the optimal solution, which is achieved by a
two-stage heuristic search scheme. Both simulated and
real data experiments demonstrate the effectiveness of the
proposed approach.

1. Introduction
Endoscopy using flexible video-endoscopes is a
universally used procedure for the diagnosis and therapy
of various pathologies of the GI tract. In addition to a
compact video camera, an endoscope is also equipped
with a light source and a manipulator which can be
controlled by the physician to remove some tissues or to
perform other operations. For this reason, it is considered
as the vehicle for minimal invasive surgery [18]. Since a
monocular video camera can provide only 2D images,
which are different from what the physician can see from
the actual sites of the body, efforts have been spent on the
development of stereo endoscopy systems [6,18,22]. A

978-1-4244-2340-8/08/$25.00 ©2008 IEEE

stereo endoscope can capture stereo images, which can be
viewed by a physician through special glasses and/or
displays [6,12]. Yet, due to practical limitations (e.g., the
required small size of the endoscope resulting in a very
small baseline for the stereo lens), stereo videos captured
from such systems is not comfortable to view and such
systems have not been widely adopted.
Alternative approaches have been proposed to create
stereo images from monocular video. In [5], two video
frames are rectified into a stereo pair. Such approaches
typically require the camera to move laterally, which is not
natural in endoscopic imaging. There are also approaches
that rely on multiple frames for 3D reconstruction
([3,4,10]). The work of [3] assumes that cameras only
have translational movement but not rotation, which is too
restrictive for an endoscope. The approaches of [4] and
[10] rely on factorization methods to estimate 3D
structure. Such methods work for only a weak perspective
camera model and usually demand complete point
correspondences, which are hard to obtain for endoscopic
images due to the lack of distinctive textures, non-rigid
motion, noise and glare, etc. Single-frame approaches such
as shape from shading [1,2] generally cannot produce the
accuracy required by clinical applications.
There are also methods that directly reconstruct 3D
structure of the organs by using additional equipments.
For example, endoscopic images can be combined with
3D data from Computed Tomography (CT), Magnetic
Resonance (MR), or Laser Range Finder (LSR)
([7,8,9,11]). Such methods are not only expensive but also
difficult to use and thus have only seen limited
application.
In this paper, we propose an approach to image-based
3D reconstruction, which uses only two frames from a
regular, widely-adopted monocular endoscope. While in
principle our approach belongs to the structure from
motion [16,17] category, we introduce a novel modelbased formulation that employs Circular Generalized
Cylinder (CGC) to model tube-like human organs seen
from an endoscope. While a basic CGC model may not be
able to capture small structure variations such as tumors, it
provides the basis for more complex modeling (e.g.,
through introducing local deformation). In this paper, we
will focus strictly on CGC-model-based reconstruction.

In contrast to existing CGC-based reconstruction
algorithms [19], which are based on object outer contours
that are unavailable in endoscopic images, we decompose
the CGC model into a series of 3D circles. This is more
intuitive and model better endoscopic images captured
inside the GI tract (for example). To reconstruct this
model from the data, we formulate the problem as one of
Maximum a posteriori (MAP) estimation within a Markov
Random Field (MRF) framework, so as to ensure the
smoothness constraints of the CGC model and to support
robust search for the optimal solution through an
optimization scheme. Through incorporating the prior
shape information and camera/scene configuration specific
to the application domain, we are able to obtain more
accurate reconstruction than typical structure-from-motion
techniques, as will be demonstrated in the experiments.
Further, texture mapping is straightforward with our
model, rendering it easy to create visually-appealing views
from the reconstructed 3D model (Section 4).
The remaining of the paper is organized as follows. In
Section 2, we first introduce the CGC model and then
formulate the 3D reconstruction problem as one of MRFbased MAP estimation. The estimation problem is solved
in Section 3 by a sampling-based heuristic search scheme,
facilitated by dimension reduction based on the nature of
typical endoscopic images. Experiments with simulated
and real data are presented in Section 4, demonstrating the
effectiveness of the proposed approach. We conclude in
Section 5 with discussion on future work.

Formally, we define a CGC model as:
M = {Ci | i = 1 n}

(1)

where Ci is a 3D circle that can be represented by its
center point c, the normal vector n, and the radius r:

{

}

C = c = ( x , y , z )T , r , n = ( n x , n y , n z )

(2)

Note that n has only 2 degrees of freedom, thus a 3D
circle has a total of 6 degrees of freedom.
In our method, the smoothness constraint for the CGC
model is enforced through MRF which will be introduced
in Section 2.2.
To specify 3D points on a circle, we first compute
points on a canonical circle and then map those points to a
general circle. Figure 2 illustrates a general 3D circle and
a canonical circle defined by

{

}

C0 = c0 = (0, 0, 0)T , r0 = 1, n0 = (0,1, 0)

(3)

which is a circle of unit radius lying on the x-z plane with
its center at the origin. Any point on this circle can be
written as pi = ( cos(θ ) 0 sin(θ ) ) .
T

2. MRF Based Circular Generalized Cylinder
Fitting
The basic assumption of our approach is that the
endoscope moves inside a tube-like organ such as the GI
tract. This leads to an intuitive CGC model, which can
serve as a prior constraint in 3D reconstruction from the
challenging real data. In the below, we first present the
CGC model (Section 2.1), and then propose a MAP
solution based on an MRF-based formulation for
reconstructing the model from the measured data (optical
flow) (Section 2.2). Section 2.3 briefly describes the
simple flow computation scheme used in the paper.

Figure 2. 3D circle model

A canonical circle can be transformed to any 3D circle
by scaling, rotation, and translation. Mathematically, this
transformation can be represented by a 3x4 matrix M:
C = MC0
(4)
where
 r 0 0
(5)
M = [ RS | c ], S = 0 r 0
0 0 1 
with R being a rotation matrix that converts normal n0 to n
n = Rn0
(6)

Figure 1. Circular Generalized Cylinder model.

2.1. The Circular Generalized Cylinder Model
We define the CGC model as shown in Figure 1. This is
essentially like a bent tube, which is represented by a
series of circles that move along a smooth path in 3D.

Equivalently, with the inverse Rv of R, we have
n0 = Rv n
(7)
where Rv can be computed by two Givens rotations on n,
Rv = Rx Rz
(8)
The Given rotations are defined as

0
0 
1

Rx = 0 cos(θ x ) sin(θ x ) 
0 − sin(θ x ) cos(θ x ) 

(9)

 cos(θ z ) sin(θ z ) 0 
Rz =  − sin(θ z ) cos(θ z ) 0 
(10)
 0

0
1
where θ denotes the rotation angle for the corresponding
axis. By applying Rz, the x coordinate of n becomes 0, and
by applying Rx, the z coordinate of n becomes 0. Therefore
we obtain an n0 in the form of Eqn. (3).

2.2. MRF-Based Model Reconstruction
Markov Random Field is a general tool to model
context dependent entities. It has been successfully applied
to various computer vision problems such as stereo
matching, image restoration, texture analysis, etc. It is an
efficient way to impose smoothness constraints, which is
desired in our CGC-based modeling. An MRF model can
be specified by a sum of two or more energy terms, which
is called clique potentials. An important special case is
when only cliques of size up to two are considered. In this
case, the energy can also be written as
U ( f ) = ∑ V1 ( f i ) + ∑ V2 ( f i , f i ' )
(11)
{i }∈C1

{i , i '}∈C2

where C1 are size-one cliques and C2 are size-two cliques.
(i, i ') ∈ C2 means i and i’ are neighbors. This equation is
also often written as
E = Ed + λ Es
(12)
where Ed is the data energy and Es the smoothness energy.
Ed can be viewed as corresponding to V1, and Es
corresponding V2.
Applying the MRF to our CGC model, we view each
circle as a site in a random field. A circle may have many
configurations, which is determined by its center, normal
and radius. The label of a site is its configuration. A
configuration may be good or bad, which is defined by an
energy score that measures how well the configuration
matches the given data (i.e., the endoscopic image and
features extracted from the image in our study). A smaller
energy suggests that the corresponding configuration is
closer to the truth. In our problem, we want the circles to
fit the data well not only individually but also as a whole.
The smoothness of human organs effectively requires
neighboring circles are similar to each other. Therefore,
we can define a model’s energy function as:
E d ( M ) = ∑ E (C )
C ∈M
(13)
Es ( M ) = ∑ E (C , C ')
( C , C ')∈V

where E(C) is the energy function of a circle and E(C, C’)
is the smoothness energy function of two neighboring

circles. With such MRF-based formulation, we can now
find the MAP solution to the problem of searching for the
optimal model configuration by solving an energy
minimization problem. In the following, we present the
details of the energy terms in the formulation.
2.2.1Data Energy
We want this term to be smaller when a circle fits the
observation better. This is equivalent to assuming the
following likelihood function that is typically used:
f (C | d ) ∝ e − E ( C )
(14)
where d is the observation. In this work, we define E(C) as
the error between the projected flow of the 3D circle
points and the measured optical flow in the image, i.e.,
1
E (C ) = ∑ E ( X )
n X ∈C
(15)
E ( X ) = P2 X − P1 X − ∇( P1 X )
where X is a point on the 3D circle C and n points are
sampled on C. P1 and P2 are camera matrices of the two
views (which are obtained from a calibration stage [16]).
P1X is the projection of X on the first view and P2X the
projection on the second view. ∇( P1 X ) is the optical flow
of P1X.

2.2.2Smoothness Energy
The smoothness energy is defined as:
E (C , C ') = θ (n, nc ) + θ (nc , n ') + α P(C , C ')
where
nc = (c '− c) / c '− c

(16)

 n1 i n2 
 n n 
 1 2 
with nc being the direction from c to c’ which are the
centers of C and C’ respectively. In (16), n is the normal
of C and n’ the normal of C’, θ (n1 , n2 ) the angle of two

θ (n1 , n2 ) = arccos 

vectors, α a scaling factor, and P (C , C ') the penalty on
conflicts of model configurations. There are two types of
conflicts. The first type is in the image domain, as
illustrated in Figure 3. In Figure 3 (b), two observed
circles overlap in the image, which should not be allowed
if we assume that the camera can see the entire contour of
each circle in the current frame. In practice, a hard
threshold is avoided by introducing the penalty term,
which is defined as
 0 o ≥ 0
Pim (C , C ') = 
(17)
 o o < 0
o = ∆ic − ∆ir , ∆ir = ir − ir ' , ∆ic = ic − ic '

where ∆ir is the difference of the radiuses of the circles,
∆ic the distance of circle centers, and o the measurement
of how closely is a circle inside another circle. If o is less
than zero, then one circle is inside another circle.

Otherwise, their contours intersect.

(a)

erroneous, hindering the latter model matching in Eqn.
(15). To alleviate this problem, in the flow estimation
stage, we keep several top motion vectors for each grid
point, and when using the flow in Eqn. (15), the best
configuration of the flow vectors will be chosen.

(b)

Figure 3. Conflict in image: (a) no conflict; (b) conflict.

Another type of conflict is in the 3D space where two
circles intersect, as shown in Figure 4(b). The penalty
score is defined as
 0 ∆θ ≤ 0
P3d (C , C ') = 
(18)
 ∆θ ∆θ > 0
∆θ = θ (n, n ') − θ 0 , θ 0 = arctan( ( z '− z ) / r )

where θ0 is the maximum rotation angle between two
neighboring circles in cases without conflict.

(a)

(b)

Figure 4. Conflict in 3D space: (a) no conflict; (b) conflict.

The total penalty score is defined as a weighted sum of
the above two types of penalties:
P (C , C ') = wPim (C , C ') + (1 − w) P3d (C , C ')
(19)

2.3. Flow Computation
Eqn. (15) requires the computation of the optical flow
for any given point. To achieve this, we first compute the
optical flow for all the points on a grid overlaid on the
image, and then we interpolate the flow for any other point
using the flow from the grid points. Figure 5 shows a
result of the computed flow for the grid points, where the
flow estimate is based on block matching.
It would be desired if we also incorporate the prior
shape model in the flow computation stage. However, this
is a chick-and-egg problem: in the stage of the flow
estimation, we have not fitted the model with the data yet
and thus it is impossible to incorporate the model. But if
no model is enforced at this stage, for images with largely
homogenous textures, the estimated flow may be

Figure 5. Optical flow for the grid points.

3. Model Search
In the MRF framework, there are a set of sites with each
site having a certain number of possible labels. In this
work, we use a set of depth levels as the sites. Thus the
model is composed of circles which lie on these depth
levels respectively. At each depth level, a circle has
various possible configurations which are the labels of the
site. In theory, there are infinite labels for each depth
level, since the circle can have any parameter C. Finding
the optimal configuration for the entire model is typically
solved through an optimization procedure. There are a lot
of MRF optimization techniques, such as graph cut [13],
ICM [14], loopy belief propagation [15], etc.
Unfortunately, they are not useful for our application since
in this case, the site number is small but the number of
labels is huge (infinite). This is in contrast to typical
problems using MRF, where site number is large while the
label number is small. Although using hierarchical or
additive MRF models may alleviate this problem, such
models would demand more artificial assumptions (e.g.,
an extremely-smooth CGC, which may be not practically
useful). Moreover, typical optimization schemes focus on
only minimizing certain energy function, which may not
support explicit incorporation of physical constraints. In
this work, we use the basic MRF model which is more
intuitive and natural for our problem, and we propose a 2stage search procedure (Section 3.2) that enforces physical
constraints arising from the application (e.g., the pose of
the endoscope camera). We first analyze the parameter
space of the labels in Section 3.1.

3.1. The Parameter Space to Search
Based on the model definition of Eqns. (1) and (2), the
general parameter space for a complete CGC model is:
S = F0 ⊗ F1 ⊗ ⊗ Fn −1
(20)
Fi = ix ⊗ iy ⊗ iz ⊗ inx ⊗ iny ⊗ inz ⊗ ir
where S is the entire model’s parameter space and Fi
circle i’s parameter space, with ix the real value space for
x coordinate of the i-th circle in the model, and so on. Eqn.
(20) indicates that the general search space has 7n
dimensions for n circles. We take the following measures
to reduce the search space.
First, we define a set of depth levels. We let the depth
levels to be equally spanned. Suppose we set n depth
levels and thus the model is
M = {Ci | i = 0,1 n − 1}
(21)
then
zi = z0 + i × ∆z
(22)
where ∆z is the distance between two neighboring depth
levels. n , z0 and ∆z can be estimated coarsely through
feature correspondence and camera calibration. Since we
assume that the model is a generalized cylinder, the radius
is the same for all circles. That is:
ri = r
(23)
which is also estimated coarsely using the feature
correspondence before the model fitting process. (This is
possible since the endoscope is more or less in the center
of the tube during acquisition.)
We further reduce the search space by assuming
nz = 1
(24)
since the normal n = (nx , ny , nz ) has only two degrees of
freedom. The reason we set nz = 1 is that in endoscopic
imaging, the camera can only look forward and thus the
optical axis is roughly along the circle’s normal.
With z, r and nz determined, the search space of a
circle is now reduced to
Fi = ix ⊗ iy ⊗ inx ⊗ iny
(25)
Now the search space for the model has 4n dimensions.
This may still be a huge space. For example, for n = 5
and 10 discrete values in each dimension, the space would
have 1020 points.

where Fi is the sample space from Fi , i is the set of N
circles with the minimal error.
b) Pick the best model based on the top N circles at
each level.
(27)
M * = arg min { E ( M )}
M ∈S

where S is the model’s search space given by
S = 0 ⊗ 1 ⊗ ⊗ n −1
M is the best model obtained in this stage.

Stage 2: Refine the model based on local search.
M ** = refine( M * )

3.2.1Sample Space in the Initial Search
In Stage 1, we first need to define a sample space Fi so
that we can pick a set of good circles for each depth level.
The sample space is defined based on the image grid as in
Figure 5. Basically, we let the circle center lie on the grid
points. (A better way is to use only the center region of the
grid since the center of the circles are unlikely located
close to the boundary of the image. But our focus here is
to illustrate how the initial search space can be simply
defined and thus we will not discuss the above scheme.)
Suppose that the grid is defined by

{

Stage 1: Initial model search, which has two steps:
a)Select top N circles at each depth level independently,
(26)
i = arg n min { E (C )}
C∈Fi

(

)

G = gij = xi , y j | i = 1

kx , j = 1

ky

}

(30)

then we compute the circle center c so that
g = K1c
(31)
where K1 is camera internal matrix. K1c is the image
projection of 3D point c.
Suppose c = ( x, y , z ) and g = ( x,y,1) , then
T

T

 x = (x − px ) z / f
(32)

 y = (y − p y ) z / f
where ( px , p y ) is the principle point, f the focal length.
We further simplify the space by setting the circle
normals to (0, 0, 1)T (based on the same argument for Eqn.
(24)). So the sample space is now defined by:
Fi =

iy

Since the parameter space is still too huge to support
exhaustive search, we design the following heuristic
search scheme, which consists of two stages:

(29)

Details of these two stages are discussed subsequently.

ix

3.2. The Search Strategy

(28)

*

inx

ix

{
= {y

⊗

iy

⊗

inx

⊗

iny

}
k }

= x j = (x i − px ) z / f | j = 1

kx

= (yi − px ) z / f | j = 1

y

j

= {0} ,

iny

(33)

= {0}

3.2.2Model Refinement based on Local Search
The best model obtained from the previous stage is
refined so that the model energy is further reduced. A
simple strategy is to smooth the model by adjusting the
centers and normal vectors of the circles by, for example

(a)

(b)

(e)

(c)

(f)
(g)
Figure 6. Experiment with simulated data.

ci ' = (ci −1 + ci + ci +1 ) / 3

(34)

ni ' = ci +1 − ci −1
While this is simple and fast, it may cause conflict when
the normals of the circles are changed. Therefore, we
perform a local search and choose the configuration that
has minimal energy:
C ' = arg min { E ( M *), C ∈ M *}
(35)
C '∈Fi

where Fi is the local sample space centered around C.
This allows small adjustment for parameters of C.

3.3. Summary of the Algorithm
Now that all the major components of the proposed
approach have been presented, we summarize them into
the algorithm shown below. The first three steps are from
traditional 3D reconstruction based on two views. In our
algorithm, we use these steps for initial estimation of the
radius and the depth levels. Then the search space is
defined and we invoke the proposed approach to search
for a good model. After we obtain a solution, we construct
the corresponding CGC model and perform texture
mapping using the input images.
Algorithm:
1. Compute optical flow from two images;
2. Perform camera calibration from the optical flow;
3. Perform triangulation and estimate depth levels and
the radius;
4. Search for a good model with small energy;
5. Generate meshes and texture mapping based on 3D
model.
It is worth noting that, while we presented the algorithm
only for two frames, it is obvious that the idea can be

(d)

(h)

directly extended to multiple frames, resulting in an
approach similar to exiting multi-frame structure-frommotion methods, although the computational complexity
needs additional attention.

4. Experiments
We present three experiments in this section to show the
effectiveness of the proposed approach. In the first
experiment, we verify our algorithm using simulated data
for which we have the ground truth model. Experimental
results from two different sets of real endoscopic images
are then provided to illustrate the performance of the
algorithm with real images. In our current implementation,
since no code optimization has been done, the speed
performance is not evaluated.
In all the three cases, we have submitted a video clip to
facilitate the visualization of the final results and their
comparison.
Experiment I: In the first experiment, we use
commercially available software POV-Ray [21] to
simulate a CGC model and generate two views from
inside the model, simulating what an endoscope would see
(Figure 6 (a-b)). Figure 6 (c) shows the torus model which
lies on the x-z plane. Figure 6 (a) and (b) are two views
generated from inside the torus. The camera’s view angle
is set to 120 degrees. A light source is set at the camera’s
position. Texture is generated using POV-Ray’s embedded
marble function. We first compute the optical flow which
is shown in (e). Based on the optical flow, we estimate the
camera parameter and do 3d reconstruction. Figure 6 (f) is
the reconstructed model based on the point cloud, which is
obviously irregular and inaccurate. Figure 6(g) is the result
using independent circle fitting (i.e., simply selecting the
best circle for each depth level independently without the

(a)

(e)

(b)

(c)

(f)
(g)
Figure 7. Experiment with real data.

smoothness constraint). We can find the resultant model is
not smooth and there are even intersections among the
circles. Figure 6(h) is the result based on MRF model
where smoothness constraints are included. Now the result
is not only smooth but also accurate, compared with the
ground truth model in (d).
Experiment II: This experiment was based on a real
endoscopic video, from which two frames are illustrated in
Figure 7 (a-b). Figure 7(c) is the computed optical flow.
(d) is the result of the structure-from-motion method. The
result is again very spiky and does not show useful
structures. Figure 7(f) is the result of model fitting without
considering the smoothness constraint. The 3D circles are
plotted in the image as in Figure 7(e). While each of
circles looks fine individually, the combined 3D model is
too bumpy to be realistic. Figure 7(h) is the result based on
MRF with smoothness constraints. Note that, the result of
(f), while deemed not good in this case, illustrates the
potential of our CGC model for handling cases where the
radius of the circle may change from site to site, which is
likely to happen in reality.
Experiment III: The third experiment was based on
another real endoscopic video clip in which some outlier
structures are present. This is intended to assess whether
the approach is robust in the presence of outliers.
Additionally, such data can also help to visually evaluate
the accuracy of the results even if there is no ground truth
available. Figure 8 (a) and (b) are two views selected from
the video. Figure 8 (c) is the result based on the structurefrom-motion method. Figure 8(d) is the result of single
circle fitting without MRF-based smoothness constraints.
(Again, while deemed not good in this case, this result
suggests that our CGC model may be extended to handle
variable radius.) The final result of our method is given in
(g). The model’s 3D circles are projected to the two input

(d)

(h)

images respectively, as shown in (e) and (f). We can see
that each circle matches very well with the corresponding
image except for the whitish outlier parts. By including 3D
points which are not on or near the CGC surface into the
obtained model, we can render the outliers together with
the CGC model. This is illustrated in Figure 9 (h). See also
the submitted video clip.

5. Conclusion and Future Work
In this paper, we proposed a MRF-based 3D
reconstruction method for endoscopic images. The method
incorporates a prior CGC model to help overcome the
challenges in the images acquired by an endoscope. The
method enforces a global structure and thus improves
upon typically existing approaches. To search for solutions
under the MRF-based CGC modeling, we proposed a 2stage heuristic search strategy that combines coarse initial
search with local refinement. Experiments demonstrate the
effectiveness of the proposed approach.
An immediately future step is to extend this model by
allowing not only the variation of the radius, but also
certain degree of deformation of the circle. This will
ensure the model to accurately fit the data, since, for
example, GI tract may not be perfectly circular.

6. References
[1] A. Tankus, N. Sochen, and Y. Yeshurun. Perspective shapefrom-shading by fast marching. In CVPR, Volume I, pages
43-49, 2004.
[2] C. H. Quartucci Forster and C. L. Tozzi. Towards 3D
reconstruction of endoscope images using shape from
shading, In XIII Brizilian Symposium on Computer
Graphics and Image Processing, pages 90-96, 2000.
[3] T. Thormahlen, H Broszio, and P. Meier. Three dimensional
endoscopy. In Falk Symposium, No. 124, 2002.

[4] C. Wu, Y. Chen, C. Liu, C. Chang, and Y. Sun. Automatic
extraction and visualization of human inner structures from
endoscopic image sequences. In Proceedings of the SPIE,
Volume 5369, pages 464-473, 2004.
[5] Q. Liu,
R.J. Sclabassi,
N. Yao, and
M. Sun. 3D
construction of endoscopic images based on computational
stereo. In Bioengineering Conference, pages 69-70, 2006.
[6] U. Mueller-Richter, A. Limberger, P. Weber, K. Ruprecht,
W. Spitzer, and M. Schilling. Possibilities and limitations of
current stereo-endoscopy. In Surgical Endoscopy, Volume
18, Number 6, pages 942-947, June 2004.
[7] J.P. Helferty and W.E. Higgins. Combined endoscopic
video tracking and virtual 3D CT registration for surgical
guidance. In ICIP, Volume 2, pages 961-964, 2002.
[8] D. Burschka, M. Li, M. Ishii, R. H. Tylor and G. D. Hager.
Scale-invariant registration of monocular endoscopic
images to CT-Scans for sinus surgery. In Medical Image
Analysis. Volume 9, Issue 5, pages 413-426, October 2005.
[9] H. Mitsuhiro, S. Naoki and N. Yoshihiko. Laser-scan
endoscope system for intraoperative geometry acquisition
and surgical robot safety management. In Medical Image
Analysis, volume 10, Issue 4, pages 509-519, August 2006.
[10] C. Wu, Y. Sun, and C. Chang. Three-Dimensional
Modeling from Endoscopic Video Using Geometric
Constraints Via Feature Positioning. In IEEE Transactions
on Biomedical Engineering. Volume 54, No. 7, July 2007.
[11] D. Tomazevic, B. Likar, and F. Pernus. 3-D/2-D registration
by integrating 2-D information in 3-D. In IEEE
Transactions on Biomedical Engineering. Volume 25, No.
1, January 2006.
[12] N. Yasushi. Recent Developments of Glass-Free 3DDisplays. In Joho Shori, Volume 45, No. 5, pages 410-515,
2004.
[13] Y. Boykov, O. Veksler, R. Zabih. Fast approximate energy
minimization via graph cuts. In IEEE transactions on
Pattern Analysis and Machine Intelligence, volume 23, No.

(a)

(e)

(b)

11, pages 1222-1239, 2001.
[14] J. Besag. On the statistical analysis of dirty pictures. In
Journal of the Royal Statistical Society B, Volume 48, No.
3, pages 259–302, 1986.
[15] J.S. Yedidia, W.T. Freeman and Y. Weiss. Generalized
Belief Propagation. Advances in Neural Information
Processing Systems, Volume 13, pages 689-695, 2000.
[16] R.I. Hartley and A. Zisserman, Multiple View Geometry in
Computer Vision, Cambridge Univ. Press, 2003.
[17] M. Pollefeys, L. J. V. Gool, M. Vergauwen, F. Verbiest, K.
Cornelis, J. Tops, R. Koch. Visual Modeling with a HandHeld Camera, In International Journal of Computer Vision,
Volume 59, Issue 3, pages 207-232, 2004.
[18] James R. Zekta Jr., Surgeons and the Scope, Cornell
University Press, 2003.
[19] C. Pan, H. Yan, G. Medioni and S. Ma. Parametric
Reconstruction of Generalized Cylinders from Limb Edges.
In IEEE Transactions on Image Processing, Volume 4, No.
8, 2005.
[20] P. Pietrzak, M. Arya, J. V. Joseph, H. PATEL (2006) ,
Three-dimensional Visualization in Laparoscopic Surgery,
BJU
International
98
(2),
253–256.
doi:10.1111/j.1464-410X.2006.06287.x, August, 2006.
[21] http://www.povray.org/
[22] F. Mourgues; F. Devemay and E. Coste-Maniere. 3D
reconstruction of the operating field for image overlay in
3D-endoscopic surgery, In Proceedings. IEEE and ACM
International Symposium on Augmented Reality.

(c)

(f)
(g)
Figure 8. Experiment with real data with outlier parts.

(d)

(h)

Fusing Pointwise and Pairwise Labels for Supporting
User-adaptive Image Retrieval
Lin Chen
Computer Sicence, Arizona
State University

lin.chen.6@asu.edu

Peng Zhang

∗

Baoxin Li

Alibaba Group

minghua.zp@alibabainc.com

Computer Sicence, Arizona
State University

baoxin.li@asu.edu

ABSTRACT
User-adaptive image retrieval/recommendation has drawn a
lot of research interests in recent years, owing to fast development of various Web applications where retrieving images
is a key enabling task. Existing challenges include the lack
of user-adaptive training data, the ambiguity of user query
and the real-time interactivity of a system. This paper proposes a hybrid learning strategy that fuses knowledge from
both pointwise and pairwise training data into one framework for attribute-based, user-adaptive image retrieval. Under this framework, we develop an online learning algorithm
for updating the ranking performance based on user feedback. Furthermore, we derive the framework into a kernel
form, allowing easy application of kernel techniques. The
proposed approach is evaluated on two image datasets and
experimental results show that it achieves obvious performance gains over ranking and zero-shot learning from either
type of training data independently. In addition, the online learning algorithm is able to deliver much better performance than batch learning, given the same elapsed running
time, or can achieve better performance in much less time.

(a) unstylish

(b) stylish?

(c) stylish

(d) more stylish? (e) more stylish?
Figure 1: stylish and unstylish cars. Considering
pointwise label, to most people (a) would be unstylish and (c) would be considered stylish. However, it is ambiguous to classify (b) to be stylish or
unstylish. Considering pairwise label, people may
have diﬀerent preference to compare whether (d) or
(e) is more stylish.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: [Miscellaneous]; I.4.8 [Image Processing and Computer Vision]:
[Scene Analysis]

1.

INTRODUCTION

The social media era has witnessed phenomenal growth
of user-generated images on the Internet. The ever-growing
number of images has brought about new challenges for eﬃcient image retrieval, and in turn, for applications that rely
on image retrieval. Conventional content-based image retrieval approaches learn some general ranking models purely
based on the underlying images. In recent years, adaptive
image retrieval [5][10][20] has emerged as a new trend, which
intends to satisfy a user’s specific requirements or preference. For example, in search of art images, some people
like realism paintings while some may prefer abstract art. A
retrieval engine being able to support such personalization
would have the best potential to deliver what a user is really looking for. In practice, it is still diﬃcult to learn a well
generalized model due to the lack of user-adaptive training
data. For example, in applications like on-line shopping, it
is unreasonable to assume a user’s personal preference data
have been made available a priori for training the system.
Often, desired is an on-line learning approach that accumulates such information over time interactively. Our approach adopts a model adaptation strategy and proposes a
new ranking model and an online learning algorithm. Such

General Terms
Algorithms, Design, Experiment

Keywords
Adaptive Image Retrieval, Attribute Learning, Learning to
Rank
∗Eﬀort performed while being a research fellow at ASU.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ICMR’15, June 23–26, 2015, Shanghai, China.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3274-3/15/06 ...$15.00.
DOI: http://dx.doi.org/10.1145/2671188.2749358.

67

a method is especially proper for applications that utilize
interactive input/feedback of a user in achieving adaptive
image retrieval/recommendation.
Beneath the above challenge of personalization lie the fundamental problems of semantic gap and intent gap in
general image retrieval. The semantic gap refers to the discrepancy between extractable low-level image features and
high-level semantic concepts of images, while the intent gap
refers to inadequacy of the representation of a query in
expressing a user’s true intent. In recent years, towards
bridging the semantic gap, methods exploiting semantic attributes of visual objects have attracted significant attention
in applications including object recognition [26][14][6], face
verification [13] and image search [25][12][18]. Instead of using low-level features, these approaches describe images by
high-level, human-nameable visual attributes, such as keep
hair color, presence of beard or mustache, presence of eyeglasses, etc., to describe human faces.
In the meantime, towards bridging the intent gap, learningto-rank approaches have been proposed. Recent literature
on this regard includes three types of approaches distinguished by how the training data are used: pointwise, pairwise and listwise approaches. The first two types of approaches have been adopted in image ranking problems.
Pointwise approaches [15][23] adopt category labels in the
training samples to learn a ranking function. For example,
to describe the car images in Fig. 1, the car in Fig. 1(c)
is categorized as a “stylish” car and the one in Fig. 1(a) is
labelled “unstylish”. In a diﬀerent way, pairwise approaches
[2][24][7][8][3] learn a ranking function by taking comparative sample pairs for training. For example, most people
would agree that the car in Fig. 1(c) looks more “stylish”
than the one in Fig. 1(a). Such pair of samples with relative
labels can be used to learn ranking functions for processing
new images.
Pointwise data and pairwise data have diﬀerent advantages and limitations in terms of data availability, labelling
complexity and representational capability, as elaborated
below.
Data availability In practical applications pointwise
and pairwise labels are not always available for every data
sample, especially considering the subjectivity of the labels.
For example, in pointwise labelling, most people would agree
that Fig. 1(c) is a “stylish” car and Fig. 1(a) is an “unstylish”
car, but it would be diﬃcult to tell whether Fig. 1(b) is a
“stylish” car. Some people may think it is “stylish” because of
the design of the headlights, while some others may deem the
body design unattractive. Similarly, ambiguity also exists in
pairwise data labelling. For example, comparing Fig. 1(d)
and 1(e), people may have diﬀerent opinions on which car
is more “stylish” because of subjective preference. When
ambiguity exists, it is better not to allow the data to be
labeled so as not to produce noisy labels.
Labelling complexity In general, pairwise data may
be more expensive to label. For example, given 10 images,
we only need to label 10 samples to assign each image into
one category. Also, category labels can be acquired from
other sources such as image tags. On the other hand, to
assign pairwise labels for all 10 images, we would need to
compare 45 pairs to completely capture the ranking information. (Although the relative relation is transferable such as
(A≻B)&(B≻C)⇒A≻C, it is diﬃcult to discover those “key
pairs” since we usually have to randomly pick pairs without

any prior knowledge.) We note, however, that sometimes it
is easier for a user to assign pairwise labels through comparison than having to give a pointwise label for a given
image.
Representational capability Pairwise data tend to
have stronger representational capability than pointwise data
in ranking problems, as pointwise label only implies the relative order of data samples from diﬀerent categories but not
those from the same category. In contrast, pairwise labels already give the relative order of every training pairs, and thus
contain more knowledge to learn a better ranking model.
As pointwise and pairwise labels encapsulate information
of diﬀerent types/amounts and may have diﬀerent availability, we set out to develop a new framework for fusing both
types of training data for improved ranking performance.
Most of current fusion approaches [16][19] only use pointwise
labels and the fusion only appears in the cost function. To
our best knowledge, the only work considering fusing pointwise and pairwise data is presented by Sculley [21] whose
object function is simply a linear combination of loss functions from regression and ranking.
In this paper, towards supporting adaptive image retrieval,
we propose a new ranking-based framework. Our approach
uses visual attributes to describe images, which helps to partially overcome the semantic gap problem. To alleviate the
problem of lacking adaptive training samples, our approach
attempt to maximize the utilization of all available training
data by fusing both pointwise and pairwise labelled data in
training. Compared to [21], our approach is formulated as a
soft margined SVM which is able to achieve better generalization performance. Furthermore, to support interactivity,
which is one natural way of gathering adaptive training data
on the fly, we derive an online learning algorithm which can
incrementally acquire a user’s online feedback to improve
the performance of the model incrementally with additional
amount of data. As will be demonstrated by experiments,
the proposed framework is able to take advantage of both
types of data and deliver better performance than the baseline approaches that use only one type of data for learning.
There are three key contributions of our work. (1) We
propose a new ranking framework termed “hybrid ranking”
which takes both pointwise and pairwise labelled data for
learning. (2) We propose an online learning algorithm for
our proposed hybrid ranking framework, which can better
support applications like adaptive image ranking. (3) We
derive our hybrid ranking framework into a kernel form so
that diﬀerent kernel functions (depending on the application) may be applied for better performance.
In the remaining of the paper, we first give a formal problem definition in Section 2. The proposed approach is presented in Section 3. Experiments and results are demonstrated in Section 4. We conclude the paper in Section 5.

2.

PROBLEM DEFINITION

Adaptive Image Retrieval We consider the following adaptive image retrieval procedure illustrated in Fig.
2: Given a training dataset including both the attribute
existence labels of images (pointwise) and the relative attribute strength labels of image pairs (pairwise), we first
train a general image ranking functions (the “Oﬄine trained
model” in the figure). This is used to retrieve images for
a user based on his/her query. Looking at the initial retrieval results, the user may interactively provide feedback,

68

like optimization using (relatively) labeled pairs. Both of
these two SVM models aim to optimize a project direction
w, such that ⟨w, w⟩ (i.e., the inverse of the margin) is minimized subject to the separability constraints (modulo margin errors in the non-separable case).
In the situation that the training data are very limited,
learning w based on both pointwise and pairwise datasets
jointly would become a necessity in order to achieve reasonable performance. To fuse information from both types of
data, the margins assigned to them should be diﬀerent. To
this end, we introduce a new superparameter ρ representing
the margin corresponding to the pairwise data. We propose
the hybrid ranking approach as follows:
∑∑ j
1
∥w∥2 + c1 τ1
(ξi + ξi∗j+1 )
min
w,b,ξ,ζ,η 2
i
j
∑
∑
+ c2 τ2
ζij + c2 τ3
ηij

Figure 2: Adaptive image retrieval via on-line feedback.

s.t. w · xji − bj ≤ − 1 + ξij ,

in forms of newly labelled attribute existence labels and/or
relative attribute strength labels. Such feedback is used as
new training samples by an on-line-learning algorithm for
updating the ranking function. As the feedback is specific to
a user, the updated ranking function (and thus the retrieval
engine) is presumably adapted to a user’s preferences and
hence achieving some level of personalization.
Hybrid Ranking To solve the above adaptive image
retrieval task, we propose a hybrid ranking SVM framework.
Given (1) the pointwise dataset P where each data sample
xi ∈ P is assigned a category label and (2) the pairwise
datasets including both the ordered pair set O and the unordered pair set S where for any pair (xi , xj ) ∈ O, xi ≻ xj
(e.g., xi has a stronger attribute than xj ), and for any pair
(xi , xj ) ∈ S, xi ∼ xj (e.g., xi has a similar attribute value
to xj ), we attempt to learn a ranking model taking both the
pointwise and pairwise data into consideration. This hybrid
ranking approach aims to capture as much information as
possible from all available data so as to achieve better ranking performance especially when labelled data are scarce.
Notations In this paper, we represent scalars as lower
case letters (e.g., x), vectors as bold face lower case letters
(e.g., x), matrices as capital letters (e.g., X) and sets as
calligraphic capital letters (e.g., X ). The standard inner
product between
the vectors u∈Rn and v∈Rn is denoted as
∑n
⟨u, v⟩ = i=1 ui vi with ui /vi the i-th element of u/v.

w · xj+1
− bj ≥1 − ξi∗j+1 ,
i
w · (xi − xj ) ≥ ρ − ζij , ∀(i, j) ∈ O,
|w · (xi − xj )| ≤ ηij , ∀(i, j) ∈ S,
ξij ≥0, ξi∗j ≥0, ζij ≥ 0; ηij ≥ 0
where xji ∈ Rn is an object (feature vector) with j =
1, ..., k − 1 denoting the class number, i = 1, ..., ij is the
index within class j, and k is the total number of classes.
(xi , xj ) is sample pairs, ξij and ξi∗j are non-negative slack
variables measuring the degree of misclassified data, ζij and
ηij are soft margin slack variables for pairwise ranking, c1
and c2 are super parameters controlling the weight for the
pointwise and pairwise data, τi is the weight function penalizing diﬀerent training datasets according to the data
size. Specifically, let n1 , n2 and n3 denote the data sizes
of the pointwise, ordered and unordered pairwise datasets
respectively, then τi = ∑3ni n , i = 1, 2, 3. Note that if only
j=1

j

pointwise data are provided then the framework is equivalent to regression, and if only pairwise data are provided the
framework becomes pairwise ranking.
In the following discussion, we focus on the image retrieval
task which can be simplified as a hybrid ranking model
with “binary type ” of pointwise label (i.e., existence/nonexistence of certain attribute). For clarity, in the following, we use xP
i to denote the i-th pointwise training sample
S
xi ∈P, and xO
i (xi ) denotes the diﬀerence of the i-th ordered(unordered) pairwise training sample as xp − xq for
any (xp , xq )∈ O(S). Then the ranking model can be formulated as the following primal form of the hybrid learning
problem:
∑
1
min
∥w∥2 + c1 τ1
ξi
w,b,ξ,ζ,η 2
i
∑
∑
+ c2 τ 2
ζi + c2 τ3
ηi

3. PROPOSED APPROACH
In this section we propose a general hybrid ranking SVM
framework and an online algorithm to solve this problem.

3.1 Hybrid Ranking SVM
To make the best use of the available knowledge, we propose a hybrid ranking SVM which takes both pointwise and
pairwise labelled data samples for learning.
The approach presented in [23] for ordinal regression learns
a number of parallel hyperplanes by the large margin principle as a ranking model. One implementation of the approach
tries to maximize a fixed margin for all the adjacent classes.
Relative attributes [18] applies pairwise learning-to-rank approach on image attributes for image ranking. This approach learns a ranking function for each human-nameable
attribute of an image. The relative “strength” of an attribute
is measured by some distance metrics learned through SVM-

s.t. yi · (w · xP
i + b)≥1 − ξi ,

(1)

xO
i

w·
≥ ρ − ζi ,



S
w · xi  ≤ ηi ,
ξi ≥0, ζi ≥ 0; ηi ≥ 0.
This formulation can be solved by quadratic programming.

69

Algorithm 1 The Mini-Batch Online Learning Algorithm
Input:
1. Training set A with data type flags;
2. Parameters ρ, c1 , c2 , t̃, k;
Output: wt̃ ;
1: Set w0 = 0;
2: for t = 1, 2, ..., t̃ do
3:
Choose At ⊆ A where |At | = k uniformly at random;
4:
Set AP
t = {i ∈ At : (xi , yi ) ∈ P∧yi ⟨xi , w t ⟩ < 1},
n1 = |AP
t |;
5:
Set AO
t = {i ∈ At : (xi , yi ) ∈ O∧⟨xi , w t ⟩ < ρ},
n2 = |AO
t |;
S
6:
Set AS
t = {i ∈ At : (xi , yi ) ∈ S, n3 = |At |;
1
7:
Set ηt = (n1 +n2 +n3 )t ;
8:
Set τ1 = ∑3n1 n , τ2 = ∑3n2 n , τ3 = ∑3n3 n ;
j=1 j
j=1 j
j=1 j
∑
9:
Set wt ← (1 − ηt )wt−1 + ηt (c1 τ1 i∈AP yi xi +
t
∑
∑
c2 τ2 i∈AO xi ) + c2 τ3 i∈AS sgn ⟨xi , wt ⟩xi );
t
t
10: end for
11: return wt ;

approximate objective function:
∑
1
P
ℓ1 (w; (xP
f (w; At ) = ∥w∥2 + c1 τ1
i , yi ))
2
i∈At
∑
∑
+ c2 τ2
ℓ2 (w; xO
ℓ3 (w; xS
i ) + c2 τ3
i )
i∈At

i∈At

(3)
We employ the stochastic gradient methods in our algorithm. The sub-gradient of Eq. (3) at iteration t is given
by
∑
P P
∇t =wt − c1 τ1
χℜ+ (1 − yiP ⟨xP
i , w t ⟩)yi xi
i∈At

− c2 τ 2

∑

O
χℜ+ (ρ − ⟨xO
i , w t ⟩)xi

(4)

i∈At

− c2 τ 3

∑

S
sgn ⟨xS
i , w t ⟩xi

i∈At

where χA (x) is the eigenfunction and sgn(x) the symbolic
function. Then the weight vector can be updated by
wt+1 = wt − ηt ∇t
with the step size ηt = (n1 +n12 +n3 )t . After a predetermined
number t̃ of iterations, we output the final wt̃ as the learned
projection model. The pseudocode of our algorithm is given
in Algorithm (1). It can be shown that our proposed framework have the same convergence property with [22] and thus
we can terminate the procedure at a random stopping time
and in at least half of the cases the last hypothesis is an
accurate solution.

3.2 Mini-Batch Online Learning Algorithm
We now propose an online learning algorithm for the hybrid ranking SVM for adaptive image retrieval. In the retrieval application, we first train a general ranking function
for the user. Based on the retrieval results, the user may
provide feedback (new category and relative labels) according to their preferences. Then our online learning approach
will update the ranking function based on the newly labelled
data to make the ranking results better fit to the user’s personal needs.
The constrained quadratic programming problem of Eq.
(1) can be cast as an unconstraint empirical loss minimization with a penalty term for the norm of the classifier that
can be learned in the following form:
∑
1
P
min ∥w∥2 + c1 τ1
ℓ1 (w; (xP
i , yi ))
w 2
i∈P
∑
∑
+ c2 τ 2
ℓ2 (w; xO
ℓ3 (w; xS
i ) + c2 τ 3
i )
i∈O

3.3

Kernelization

We further derive the framework into the kernel form
which strengthens our approach to learn non-linear model.
Note that although the derivation is based on the online
learning form, it can be generalized to batch learning since
we are considering mini-batch learning in this paper.
Instead of considering predictors which are linear functions of the training instances x themselves, we consider
predictors which are linear functions of some implicit mapping ϕ(x) of the instances. Then the original optimization
problem can be redefined as:
∑
1
min ∥w∥2 + c1 τ1
ℓ1 (w; (ϕ(xi P ), yiP ))
w 2
i∈P
(5)
∑
∑
+ c2 τ 2
ℓ2 (w; ϕ(xO
ℓ3 (w; ϕ(xS
i )) + c2 τ3
i ))

(2)

i∈S

where

i∈O
P
ℓ1 (w; (xP
i , yi ))

ℓ2 (w; xO
i )

= max{0, 1 −

= max{0, ρ −




S
ℓ3 (w; xi ) = ⟨xS
i , w⟩ .

yiP ⟨xP
i , w⟩},

i∈S

where

⟨xO
i , w⟩},

P
P
P
ℓ1 (w; (ϕ(xP
i ), yi )) = max{0, 1 − yi ⟨ϕ(xi ), w⟩},
O
ℓ2 (w; ϕ(xO
i )) = max{0, ρ − ⟨ϕ(xi ), w⟩},

ℓ3 (w; ϕ(xS
i ))

=

(6)

|⟨ϕ(xS
i ), w⟩|.

Next we will directly derive the primal problem into the
kernel form. For each t, let xj represents the data sample,
and let

Inspired by the Pegasos algorithm [22], we also considered
the mini-batch algorithm which utilize k (1≤k≤m) examples
at each iteration. We initiate the model by setting w0 to the
zero vector. In iteration t of the algorithm, given a training
set A with m samples of both pointwise and pairwise data (a
flag bit is used to identify the type of the data), we choose a
subset At ⊆ A with k examples uniformly at random among
the training subset. Thus we will optimize the following

αt [j] = |{t′ ≤t : it′ = j∧yjP ⟨wt′ , ϕ(xP
j )⟩ < 1}|,
βt [j] = |{t′ ≤t : it′ = j∧⟨wt′ , ϕ(xO
j )⟩ < ρ}|,
∑
S
γt [j] =
sgn ⟨ϕ(xj ), wt′ ⟩, ∀j ∈ {t′ ≤t : it′ = j},
j

70

then Eq. (5) and (6) can be rewritten as
wt+1 =

n1
∑
1
(c1 τ1
αt+1 [j]yjP ϕ(xP
j )
λt
j=1

+ c2 τ 2

n2
∑

n3
∑

βt+1 [j]ϕ(xO
j ) + c3 τ3

j=1

γt+1 [j]ϕ(xS
j )).

j=1

According to the Representer Theorem [9], the optimal
solution to Eq. (2) can be expressed as a linear combination
of the training instances, thus we can rewrite w as:
w=

n1
∑

α[j]ϕ(xP
j )+

j=1

n2
∑

β[j]ϕ(xO
j )+

j=1

n3
∑

(a) OSR

γ[j]ϕ(xS
j ),

j=1

Let ϑ be the whole parameter vector and D be the whole
training dataset include all three types of labeled data
ϑ = [α[1· · ·n1 ], β[1· · ·n2 ], γ[1· · ·n3 ]],
T
O
T
S
T T
D = [ϕ(xP
[1···n1 ] ) , ϕ(x[1···n2 ] ) , ϕ(x[1···n3 ] ) ] ,

and di is the i-th in D, n = n1 + n2 + n3 , then the objective function can be written in the following kernel form
through a kernel operator K(x, x′ ) = ⟨ϕ(x), ϕ(x′ )⟩, yielding
the inner products after the mapping ϕ(·):
min
ϑ

(b) Shoes

n
1 ∑
ϑ[i]ϑ[j]K(di , dj )
2 i,j=1

+ c1 τ 1

n1
∑

max{0, 1 − yiP

i=1

+ c2 τ 2

n2
∑

ϑ[j]K(xP
i , dj )}

j=1

max{0, ρ −

i=1

+ c2 τ 3

n
∑

Figure 3: Learning curve of average ranking accuracy and corresponding standard deviation with regard to diﬀerent number of pairwise or pointwise
training samples on both dataset.

n
∑

ϑ[j]K(xO
i , dj )}

OSR and 6000 samples (due to memory limitation) from
Shoes for test. The result shows that our approach apparently outperforms all baseline approaches in average ranking
accuracies while generating lower standard deviations.
Fig. 3 illustrates how the ranking accuracy changes with
diﬀerent size of training samples on the attribute “Natural”
of OSR and “Open” of Shoes. Specifically, the result in
Fig. 3 Left is achieved by keeping the pointwise data size
fixed at 100 and increasing the size of pairwise data. The
blue curve shows the average ranking accuracy and standard deviation of our approach and the red and cyan curves
shows the result of baseline approaches. In Fig. 3 Right,
results of our approach (blue curve) compared with baseline approaches are shown where the size of training pairs
is fixed at 100 and the size of training samples increases
gradually. The results show that, in both configurations,
our approach achieves obvious higher ranking accuracy and
lower standard deviation than all baseline approaches. Besides, the result implies that, when fewer training samples
were used, a higher performance gain was observed. For
example, the best performance gain is 11% compared with
Relative Attributes and 14% compared with pointwise SVM
in “natural”, when only 10 training samples or pairs are fed.
Fig. 4 shows some examples of ranked image pairs. In
each column, the top image is more “natural” than the bottom image according to the ranking groundtruth. Fig. 4(a)
is the comparison of our hybrid approach with Relative Attributes. The first three pairs (columns) are correctly ranked
by our approach but incorrectly ranked by Relative Attributes, e.g., coast is more natural than highway, forest is
more natural than inside city, mountain is more natural than
buildings. The last three pairs are incorrectly ranked by our

j=1

n3 m
∑
∑
|
ϑ[j]K(xS
i , dj )|
i=1 j=1

4. EXPERIMENTS
We evaluate our approach on two datasets with augmented
relative attribute labels: (1) the Outdoor Scene Recognition(OSR) dataset [17][18] with 2688 images and 7 attributes, and the Shoes dataset [1][11] with 14568 images
and 10 attributes. We directly use the features provided
with the dataset of 512-dimensional gist descriptor for the
OSR and 960-dimensional gist descriptor plus 20-dimensional
color histogram for the Shoes.

4.1 Accuracy of Hybrid Ranking
We first demonstrate that the proposed approach is capable of utilizing information from both type of labeled data
with the comparison of three baseline approaches: Relative
Attributes [18], pointwise SVM for ordinal regression [23]
and CRR [21] which optimizes regression and ranking simultaneously. We compute the average ranking accuracy
with standard deviation by running 10 rounds of each implemented approach. The average ranking accuracy is evaluated by the frequency of correctly ranked pairs. The parameters are selected by cross validation of 5 randomly selected
small subsets per dataset.
Tables 1 and 2 demonstrate the average ranking accuracy with standard deviation of each attribute per dataset.
Pointwise and pairwise training samples are randomly selected as 100 for OSR and 200 for Shoes with the rest of

71

Attribute Name
Natural
Open
Perspective
Size-large
Diagonal-plane
Depth-cloth
Average

Hybrid Ranking(%)
91.56±0.89
88.50±0.62
83.40±0.78
72.93±1.04
80.35±1.15
87.06±0.87
83.97±0.80

CCR(%)
88.75±0.90
87.25±0.82
82.23±0.81
70.62±1.21
78.42±1.08
85.24±0.95
82.08±0.96

Relative Attribute(%)
87.09±2.08
86.83±1.76
80.20±1.69
67.89±1.55
76.25±1.76
84.27±1.66
80.42 ±1.75

Pointwise SVM(%)
88.86±2.24
85.72±1.26
80.56±1.73
65.17±3.53
76.61±2.73
82.32±1.51
79.87±1.78

Table 1: Ranking accuracies and standard deviation of 6 attributes on the OSR dataset when the number of
training samples and pairs are 100 for each attribute.
Attribute Name
Point at the front
Open
Bright in color
Covered with ornaments
Shiny
High at the heel
Long on the leg
Formal
Sporty
Feminine
Average

Proposed Method(%)
82.25±0.79
76.02±0.83
64.40±0.76
71.19±0.58
79.60±0.52
80.71±0.75
75.19±0.92
75.78±0.90
80.24±0.90
83.37±0.68
76.88±0.76

CRR(%)
81.42±0.82
74.24±0.80
62.43±0.75
70.02±0.61
78.28±0.68
78.93±0.77
73.32±0.82
74.45±0.91
78.25±0.95
82.42±0.70
75.38±0.78

Relative Attribute(%)
80.61±1.08
71.72±0.88
59.38±1.86
68.88±0.72
76.94±0.66
77.43±0.99
72.44±1.07
72.37±1.10
77.39±0.90
81.38±0.71
73.85±1.00

Pointwise SVM(%)
79.13±1.53
69.10±1.78
58.63±0.76
59.10±3.87
74.12±1.30
76.59±1.60
69.08±2.19
70.76±1.57
68.98±1.70
81.86±1.50
70.74±1.78

Table 2: Ranking accuracies and standard deviation of 10 attributes on the Shoes dataset when the number
of training samples and pairs are 200 for each attribute.
approach but correctly ranked by Relative Attributes. The
reason for the incorrect classification may be that our approach assigned a wrong category label to the scene. For instance, our approach also classified the bottom image of the
fifth column as forest because of a tree appears in the scene.
Fig. 4(b) illustrates the comparison of our approach with
pointwise SVM. The first three pairs are correctly ranked
by our approach but incorrectly ranked by pointwise SVM,
e.g., coast is more natural than street, mountain is more
natural than open county and forest is more natural than
inside city. The last three pairs are incorrectly ranked by
our approach but correctly ranked by pointwise SVM.
Based on these experiments, the potential performance
gains of our approach appear to come from the extra information captured from diﬀerent types of labels. In particular,
the smaller standard deviation may result from the joint use
of both information sources that help to denoise the training
process.

(a) Hybrid v.s. Relative Attribute

4.2 Zero-Shot Learning
To further evaluate the proposed approach, we now consider the popular application of zero-shot learning. Given
some training samples from some “seen” categories and some
“unseen” categories without any training samples, zero-shot
learning would predict the category labels of new samples.
We compare our approach with the baseline approach Relative Attributes since [18] has shown that this approach outperforms most of the state of the art approaches on this
regard. We followed the same parameter prediction rules of
unseen categories as [4]. We adopted the same super parameters in Sect. 4.1 for model training. The average ranking
accuracies with corresponding standard deviations are re-

(b) Hybrid v.s. Pointwise SVM
Figure 4: Samples illustrating the ranking results.
In groundtruth the top image is more “natrual” than
the bottom image. The left three column is correctly
ranked by the proposed approach while incorrectly
ranked by the baseline. The right three is inverse.

72

Accuracy
Online Learning
Batch Learning

70%
0.003 s
0.006 s

72.5%
0.004 s
0.098 s

75%
0.006 s
0.104 s

77.5%
0.009 s
0.231 s

80%
0.042 s
0.451 s

82.5%
0.105 s
1.558 s

85%
0.156 s
6.308 s

Table 3: Elapsed times in order to achieve the same ranking accuracy (the less the better). The first row
shows given ranking accuracies, and the second/third row shows the times needed for online/batch learning
respectively to achieve the corresponding accuracy.

(a) Diﬀerent unseen categories

Figure 6: Average ranking accuracy of 10 attributes
on the Shoes dataset by running the algorithms for
0.1 seconds. In each group the first bar is the result
of batch learning and the second bar is the result of
online learning.

(b) Diﬀerent training pairs
Figure 5: Learning curve of zero-shot learning accuracy with regard to diﬀerent unseen category numbers and training sample size on both dataset.

algorithm on the shoes dataset. For the online learning algorithm, the super parameters are set as c1 = 0.2, c2 = 3,
ρ = 0.1. In training, we first construct a data pool mixed
with both pointwise and pairwise data, and then randomly
pick one data sample without considering the specific label
type from the data pool for training. For batch learning,
we construct the training dataset as half pointwise and half
pairwise samples.
Fig. 6 illustrates the average ranking accuracy of 10 attributes by running both implemented approaches 10 rounds
for a small time interval T (T =0.1 second in this experiment), simulating very limited training data availability. In
each group, the first bar (blue) shows the result of batch
learning and the second bar (red) shows the result of online
learning. The results shows that in the same elapsed time of
0.1 second, the online learning algorithm clearly outperforms
batch learning. The highest performance gain is obtained on
the attribute “Sporty” by 9.69% and the lowest performance
gain is for the attribute “Formal” by 4.63%.
Table 3 collects the elapsed time after both approaches
achieved the same ranking performance from 70% to 85%.
The results show that the batch learning approach takes
longer time than online learning to achieve the same accuracy. With the ranking accuracy increased, the time diﬀerence become much more obvious. For example, batch learning takes double time (0.006s vs 0.003s) than online learning

ported by running each experiment 10 rounds. Assuming
the data follows a Gaussian distribution, we estimated the
mean and the covariance matrix of each (seen and unseen)
category and assigned the category label of the new sample
through maximum likelihood.
Fig. 5(a) shows the accuracy as a function of the number
of unseen categories. For each seen category 30 images are
left out for category parameter prediction, and 10 pointwise
and 10 pairwise labelled samples are randomly picked for
training. Results show that the ranking accuracy decreases
as the number of unseen category increases. Our approach
outperforms the baseline approach by around 3%.
Fig. 5(b) shows how the accuracy changes with the number of training pairs. In each run, 2 unseen categories and
30 images from the other seen categories are left out. 10
pointwise labelled samples are randomly picked for hybrid
approach. Results show that ranking accuracies of both approaches increase with the increase of training pairs. Our
approach yields performance gains by around 3% compared
with the baseline approach.

4.3 Online Learning Evaluation
In this subsection, we compare the performance of the
proposed online learning algorithm with the batch learning

73

to achieve the accuracy of 70%, and takes 40 times (6.308s
vs 0.156s) more time to achieve the accuracy of 85%.

[10] A. Kovashka and K. Grauman. Attribute adaptation
for personalized image search. In Proc. of ICCV ’13,
pages 3432–3439, Dec 2013.
[11] A. Kovashka, D. Parikh, and K. Grauman.
Whittlesearch: Image search with relative attribute
feedback. In Proc. of CVPR’12, pages 2973–2980,
June 2012.
[12] N. Kumar, P. Belhumeur, and S. Nayar. Facetracer: A
search engine for large collections of images with faces.
In Proc. of ECCV ’08, pages 340–353.
Springer-Verlag, 2008.
[13] N. Kumar, A. Berg, P. Belhumeur, and S. Nayar.
Attribute and simile classifiers for face verification. In
Proc. of CVPR ’09, pages 365–372, Sept 2009.
[14] C. Lampert, H. Nickisch, and S. Harmeling. Learning
to detect unseen object classes by between-class
attribute transfer. In Proc. of CVPR ’09, pages
951–958, June 2009.
[15] P. Li, Q. Wu, and C. J. Burges. Mcrank: Learning to
rank using multiple classification and gradient
boosting. In Proc. of NIPS ’08, pages 897–904. Curran
Associates, Inc., 2008.
[16] T. Moon, A. Smola, Y. Chang, and Z. Zheng.
Intervalrank: Isotonic regression with listwise and
pairwise constraints. In WSDM, 2010.
[17] A. Oliva and A. Torralba. Modeling the shape of the
scene: A holistic representation of the spatial envelope.
Int. J. Comput. Vision, 42(3):145–175, May 2001.
[18] D. Parikh and K. Grauman. Relative attributes. In
Proc. of ICCV ’11, pages 503–510, Nov 2011.
[19] C. Renjifo and C. Carmen. The discounted cumulative
margin penalty: Rank-learning with a list-wise loss
and pair-wise margins. In MLSP, Sept 2012.
[20] J. Sang, C. Xu, and D. Lu. Learn to personalized
image search from the photo sharing websites.
Multimedia, IEEE Transactions on, 14(4):963–974,
Aug 2012.
[21] D. Sculley. Combined regression and ranking. In Proc.
of SIGKDD’10, pages 979–988, New York, NY, USA,
2010. ACM.
[22] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos:
Primal estimated sub-gradient solver for svm. In Proc.
of ICML ’07, pages 807–814. ACM, 2007.
[23] A. Shashua and A. Levin. Ranking with large margin
principle: Two approaches. In Proc. of NIPS ’03,
pages 961–968. MIT Press, 2003.
[24] M.-F. Tsai, T.-Y. Liu, T. Qin, H.-H. Chen, and W.-Y.
Ma. Frank: A ranking method with fidelity loss. In
Proc. of SIGIR ’07, pages 383–390. ACM, 2007.
[25] D. Vaquero, R. Feris, D. Tran, L. Brown,
A. Hampapur, and M. Turk. Attribute-based people
search in surveillance environments. In Proc. of
WACV’09, December 2009.
[26] Y. Wang and G. Mori. A discriminative latent model
of object classes and attributes. In Proc. of ECCV’10,
pages 155–168. Springer-Verlag, 2010.

5. CONCLUSIONS
We proposed a hybrid ranking framework for supporting
adaptive, attribute-based image retrieval. We evaluated the
proposed approach on two image datasets. The results show
that through capturing the information from both relative
attribute strength (pairwise) and absolute attribute scale
(pointwise), our method is able to achieve better ranking
performance than Relative Attribute and pointwise SVM,
which are current leading approaches that learn the ranking function purely based on either pairwise or pointwise
data. We also proposed an online learning algorithm for the
proposed framework and derived the formulation into the
kernel form. The experiments of online learning and batch
learning show that our online learning algorithm can achieve
much better ranking performance than batch learning given
the same running time or can achieve better performance in
much less time. The results also suggest that the less training data are available, the more relative performance gains
can be obtained by our approach than independent learning.

6. ACKNOWLEDGMENTS
The work was supported in part by a grant from National
Science Foundation and a grant from Army Research Oﬃce.
Any opinions and conclusions expressed in this material are
those of the author(s) and do not necessarily reflect the view
of sponsors.

7. REFERENCES
[1] T. L. Berg, A. C. Berg, and J. Shih. Automatic
attribute discovery and characterization from noisy
web data. In Proc. of ECCV’10, pages 663–676,
Berlin, Heidelberg, 2010. Springer-Verlag.
[2] C. Burges, T. Shaked, E. Renshaw, A. Lazier,
M. Deeds, N. Hamilton, and G. Hullender. Learning to
rank using gradient descent. In Proc. of ICML ’05,
pages 89–96. ACM, 2005.
[3] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W.
Hon. Adapting ranking svm to document retrieval. In
Proc. of SIGIR ’06, pages 186–193. ACM, 2006.
[4] L. Chen, Q. Zhang, and B. Li. Predicting multiple
attributes via relative multi-task learning. In Proc. of
CVPR ’14, pages 1027–1034, July 2014.
[5] N. Elahi, R. Karlsen, and E. J. Holsbø. Personalized
photo recommendation by leveraging user modeling on
social network. In Proc. of IIWAS ’13, pages
68:68–68:71. ACM, 2013.
[6] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth.
Describing objects by their attributes. In Proc. of
CVPR ’09, pages 1778–1785, June 2009.
[7] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An
eﬃcient boosting algorithm for combining preferences.
J. Mach. Learn. Res., 4:933–969, Dec. 2003.
[8] T. Joachims. Optimizing search engines using
clickthrough data. In Proc. of KDD ’02, pages
133–142. ACM, 2002.
[9] G. Kimeldorf and G. Wahba. Some results on
tchebycheﬃan spline functions. Journal of
Mathematical Analysis and Applications, 33(1):82 –
95, 1971.

74

Evaluating Multi-Class Multiple-Instance Learning
for Image Categorization
Xinyu Xu and Baoxin Li
Department of Computer Science and Engineering, Arizona State University
xinyu.xu@asu.edu, baoxin.li@asu.edu

Abstract. Automatic image categorization is a challenging computer vision
problem, to which Multiple-instance Learning (MIL) has emerged as a
promising approach. Typical current MIL schemes rely on binary one-versus-all
classification, even for inherently multi-class problems. There are a few
drawbacks with binary MIL when applied to a multi-class classification
problem. This paper describes Multi-class Multiple-Instance Learning (McMIL)
to image categorization that bypasses the necessity of constructing a series of
binary classifiers. We analyze McMIL in depth to show why it is advantageous
over binary MIL when strong target concept overlaps exist among the classes.
We systematically valuate McMIL using two challenging image databases, and
compare it with state-of-the-art binary MIL approaches. The McMIL achieves
competitive classification accuracy, robustness to labeling noise, and
effectiveness in capturing the target concepts using smaller amount of training
data. We show that the learned target concepts from McMIL conform to human
interpretation of the images.
Keywords: Image Categorization, Multi-Class Multiple-Instance Learning.

1 Introduction
Multiple-Instance Learning (MIL) is a generalization of supervised learning in which
training class labels are associated with sets of patterns, or bags, instead of individual
patterns. While every pattern may possess an associated true label, it is assumed that
pattern labels are only indirectly assessable through bag labels [1]. MIL has been
successfully applied to many applications such as drug activity prediction [8], image
indexing for content-based image retrieval [1, 13, 21, 5, 2, 14, 6], object recognition
[6], and text classification [1].
In many real-world multi-class classification problems, a common treatment
adopted by traditional MIL is “one-versus-all” (OVA) approach, which constructs c
binary classifiers, fj(x), j=1,…,c, each trained to distinguish one class from the
remaining, and labels a novel input with the class giving the largest fj(x), j=1,…,c.
Although OVA delivers good empirical performances, it has potential drawbacks.
First, it is somewhat heuristic [15]. The binary classifiers are obtained by training in
separate binary classification problems, and thus it is unclear whether their realvalued outputs are on comparable scales [15]. This can be a problem, since situations
often arise where several binary classifiers acquire identical decision values, making
the class prediction based on them very obscure. Second, binary OVA has been
Y. Yagi et al. (Eds.): ACCV 2007, Part II, LNCS 4844, pp. 155–165, 2007.
© Springer-Verlag Berlin Heidelberg 2007

156

X. Xu and B. Li

criticized for its inadequacy in dealing with rather asymmetric problems [15]. When
the number of classes becomes large, each binary classifier becomes highly
unbalanced with many more negative examples than positive examples. In the case of
non-separable classes, the class with smaller fraction of instances tends to be ignored,
leading to degraded performances [18]. Third, with respect to Fisher consistency, it is
unclear if OVA targets the Bayes rule in absence of a dominating class [10].
Recently, a Multi-class Multiple-Instance Learning (McMIL) approach has been
proposed which considers multiple classes jointly by simultaneously minimizing a
Support Vector Machine (SVM) objective function. In this paper we focus on the
systematic evaluation of the McMIL using image categorization as a case study, and
we show that why it is advantageous over binary OVA approaches.

2 Related Work
There has been abundant work in the literature on image categorization. Due to space
limit we only review the MIL-based approaches. Andrew et al. [1] proposed MI-SVM
and applied it to small-scale image annotation. [12] proposed Diverse Density MIL
framework and [13] used it to classify natural scene images. [20] proposed EM-DD,
which views the knowledge of which instance corresponds to the label of the bag as a
missing attribute. [2] performed an instance-based feature mapping through a chosen
metric distance function, and a sparse SVM was adopted to select the target concepts.
[6] proposed MILES for image categorization and object recognition. MILES does
not impose the assumption relating instance labels to bag labels, and it does not rely
on searching local maximizers of the DD functions, which makes MILES quite
efficient. [14], within the CBIR setting, presented MISSL that transforms any MI
problem into an input for a graph-based single-instance semi-supervised learning
method. Unlike most prior MIL algorithms, MISSL makes use of the unlabeled data.
Most of these works belong to the binary MIL approaches, and thus potentially suffer
from the drawbacks mentioned in the previous section.
One work by [5] is mostly related with ours. They proposed DD-SVM. Their major
contribution is that the image label is no longer determined by a single region; rather
it is determined by some number of Instance Prototypes (IPs) learned from the DD
function. A nonlinear mapping is defined using the IPs to map every training image to
a point in a bag feature space. Finally, for each category, an SVM is trained to
separate that category from all the others. Note that the multi-class classification here
is essentially achieved by OVA. The method described in this paper differs from DDSVM in that a multi-class feature space is built that enables us to train the SVM only
once to yield the multi-class label, leading to increased classification accuracy.

3 Methodology of McMIL
For completeness of the paper, we describe the McMIL approach in this section. Let
us first define the problem and the notations. Let D={(x1, y1),…, (x|D|, y|D|)} be the
labeled dataset, where xi, i=1,...,|D| is called a bag in the MIL setting, and
yi ∈ {1,…,c} denotes the multi-class label. In the MIL model xi={xi1,…,xi|xi|}, where
xij ∈ \ d represents the jth instance in the ith bag. Our goal is to induce a classifier from
patterns to multi-class labels, i.e. a multi-class separating function f: \ d Æ y.

Evaluating Multi-Class Multiple-Instance Learning for Image Categorization

157

3.1 Learning Instance Prototypes
The first step in the training phase of the McMIL is to learn the IPs for each category
according to the DD function [12]. Because the DD function is defined based on the
binary framework, McMIL learns a set of IPs specifically for every class: the ith DD
function uses all of the examples in the ith class with positive labels and some examples
from other classes with negative labels. McMIL applies EM-DD to locate the multiple
local peaks by using every instance in every positive bag with uniform weighs as the
starting point of the Quasi-Newton search. Then the IPs with larger DD values and are
distinct from each other are selected to represent the target concepts [5].
3.2 Multi-class Feature Mapping
Let IPm =

{( I

1
m

, I m2 ,..., I mnm

)} , m ∈ {1, 2,..., c} be the set of instance prototypes learned by

EM-DD for the mth category. Each I mk = ⎡ xmk , smk ⎤ , k=1,2,...,nm denotes the kth IP in the
⎣
⎦
mth class, and nm is the number of IPs in the mth class. The learned IP consists of two
parts: the ideal attribute value x and the weight factor s. To facilitate the multi-class
classification, a similarity mapping kernel ψ is defined which maps a bag of instances
into a multi-class feature space ] based on a similarity metric function h:
\ d ×\ d → \ . The mapping ψ is defined as
ψ(xi)= ⎡ h( xi , I11 ),..., h( xi , I1n1 ),..., h( xi , I c1 ),..., h( xi , I cnc ) ⎤
⎣

T

⎦

(1)

The similarity function h is defined as
⎧
⎩

h( xi , I mk ) = max j =1,..., x ⎨exp ⎡⎢ − xij − xmk
i

⎣

2
smk

⎤ ⎫ , k = 1,..., n , m ∈ {1,..., c}
m
⎥⎦ ⎬⎭

(2)

Each feature corresponds to the similarity between a bag and one IP, which is the
similarity between I mk and the instance in xi that is closest to I mk . Note that while this is
similar to the bag feature space mapping that was proposed by [5], an important
difference between these two mappings is that, the mapping in our method
simultaneously incorporate the similarity between a bag and each IP of every category
into one feature matrix, while in binary MIL [5, 2, 6], separate feature matrices are
needed, each of which only considers the similarity between a bag and each IP of one
category that is under consideration.
For a given training set of bags with multi-class labels D={(x1, y1),…, (x|D|, y|D|)} ,
applying the above mapping yields the following kernel similarity matrix
S=
n
n
1
1
⎡h1 ⎤ ⎡h(x1, I1 ),...., h(x1, I1 1 ),...., h(x1, Ic ),...., h(x1, Ic c ) ⎤
⎢
⎥
⎢h ⎥
n1
nc
1
1
⎢ 2 ⎥ = ⎢h(x2 , I1 ),...., h(x2 , I1 ),...., h(x2 , Ic ),...., h(x2 , Ic ) ⎥
⎥
⎢M ⎥ ⎢LLLLLLLLLLLLLLLLLL
⎥
⎢ ⎥ ⎢
n
1
1
n
⎢⎣h|D| ⎥⎦ ⎢⎣h(x|D| , I1 ),..., h(x|D| , I1 1 ),..., h(x|D| , Ic ),..., h(x|D| , Ic c )⎥⎦

(3)

158

X. Xu and B. Li

3.3 Multi-class SVM Training and Classification
Given the kernel similarity matrix S, the multi-class classification is achieved by
multi-class SVM [16, 19] which simultaneously allows the computation of a multiclass classifier by considering all the classes at once. The formulation is
|D|
1 c
min ∑ wmT wm + C ∑ ∑ ξim
w,b ,ξ 2
m =1
i =1 m ≠ y
(4)
T
s.t. wy ψ ( xi ) + by ≥ wmTψ ( xi ) + bm + 2 − ξim ,
i

i

i

ξim ≥ 0, i = 1,...,| D |, m ∈ {1,.., c} y .
i

where w is a vector orthogonal to the separating hyper-plane, |b|/||w|| is the
perpendicular distance from the hyper-plane to the origin, C is a parameter to control
the penalty due to misclassification. Nonnegative slack variables ξi are introduced in
each constraint to account for the cost of the errors.
In the testing phase, we first project the test image into the feature space ] using
the learned IPs. The decision function is then given by
(5)
arg max m=1,...,c ( wmTψ ( x) + bm )
which is the same with the OVA decision making. Note that in multi-class SVM,
there are c decision functions but all are obtained by solving one problem Eq. (4).
3.4 The Learned Target Concept
Each instance prototype represents one kind of target concept that distinguishes one
class from another. Thus it is important to visualize the target concept. In order to do
that, in each class, for each feature that is defined by one positive instance prototype,

People

Portrait

Scene

Structure

Fig. 1. The target concepts of four categories in the SIMPLIcity-II data set. The target concepts
are represented by conceptual regions. For “People”, the various shapes of human are returned
as target concepts. For “Portrait”, the target concepts are mainly skin and hair with varying
colors. For “Scene”, the major target concepts are mountain, sky, sea water and plants. For
“Structure”, the target concepts are various kinds of building structures like walls, windows and
roofs.

we find the training image that maximizes the corresponding feature, then we go
further to locate the region that is closest to the corresponding prototype. The target
concepts for a category are defined by the regions that maximize the similarity

Evaluating Multi-Class Multiple-Instance Learning for Image Categorization

159

between images and the instance prototypes. Fig. 1 shows the target concepts of four
classes in the SIMPLIcity-II dataset (Sec. 4).

4 The Advantage of Direct Multi-Class MIL
The McMIL approach has advantage over binary MIL when strong target concept
overlaps exist among the classes. To show the advantage of McMIL, let us suppose
we have a dataset that has three categories: People, Portrait and Scene. Typical People
IPs include “hair”, “face”, “clothes”, “building” and “water”, and typical Portrait IPs
include “hair”, “face” and “clothes”. Fig. 2(a) shows the characteristic of the feature
vectors for a People image and a Portrait image. We can see that the People IPs are
inclusive of Portrait IPs. Now let us simplify the problem by assuming that the target
concept of People are summarized by “face” and “non-face”, and the target concept of
Portrait is summarized by the “face” IP.

(a)

(b)

(c)

Fig. 2. (a) The characteristic of the feature vectors for a People image and a Portrait image.
Top: the features for a People image defined by Portrait IPs. Bottom: the features for a Portrait
image defined by People IPs. (b) and (c) illustrate the advantage of McMIL over OVA MIL. (b)
The feature space of the People and the Portrait classifier in the OVA MIL approach. (c) The
multi-class feature space in the proposed McMIL approach.

Let us first consider the OVA MIL approach. OVA MIL will construct 3 binary
classifiers. Fig. 2(b) shows the feature space of the People and the Portrait classifier in
the OVA MIL approach. In the People classifier (top part of Fig. 2(b)), the features
are defined in terms of People IPs: face and non-face. It is more likely that the three
People training images will fall into circle A, because the similarity between a People
bag and People IPs (face or non-face) would be large. And we can expect that the
three Portrait training images will more likely fall into circle B since the similarity
between a Portrait bag and the face IP is large but the similarity between a Portrait
bag and a non-face IP would be small. In the same sense, the three Scene images will
fall into circle C. Because the bags are clearly separable in the People feature space,
we would expect that People binary classier will do a good job in classifying a new
test point. Now let us turn to Portrait binary classifier whose feature is only defined
by face IP. As shown in the bottom part of Fig. 2(b), the People and Portrait bags will
become linearly non-separable in the Portrait feature space in that the feature values
defined by Portrait face for the People bags and the Portrait bags are both likely to be

160

X. Xu and B. Li

large, and so People bags and Portrait bags will overlap with each other. As a result,
Portrait classier will be more likely to make prediction errors in classifying a new test
input. Now let us predict the label for a new test image whose true label is People. We
feed it into each of the three binary classifier and we get the following decision
values: People classifier 0.3218, Portrait classifier 0.3825, and Scene classifier 0.7726. Note that to classify this new image into the right category, the Portrait
classifier should give us a negative decision value. But because of Portrait classifier’s
poor job in classification, the new image will be associated with a wrong label Portrait
since 0.3825>0.3218>-0.7726.
Now let us turn to multi-class SVM classification whose feature space is illustrated
in Fig. 2(c). Note that the multi-class feature space is defined by the IPs of all the
classes, so McMIL has three features defined by People face (face 1), Portrait
face(face 2) and non-face. In this multi-class feature space, the probability that the
multi-class SVM makes a prediction error is greatly reduced because the dimension of
the feature space is enlarged and thus any linearly non-separable points are likely to
become separable in a higher dimensional feature space. This is essentially similar to
applying a kernel to make linearly non-separable feature space nonlinearly separable.
Actually the multi-class feature matrix S is a kernel similarity matrix, the kernel
mapping is defined by Eq. (1).

5 Evaluation of McMIL
5.1 Experiment Design
Using image categorization as a case study, we evaluate the performance of McMIL
on two challenging image data sets: COREL database [2, 5, 6], and SIMPLIcity-II.
The images in SIMPLIcity-II are selected from the SIMPLIcity image library [11,
17]. The COREL image database consists of 10 categories, with 100 images per
category. The SIMPLIcity-II data set consists of 5 categories, with 100 images per
category. Images in both data sets are in JPEG format of size 384 × 256 or 256 × 384.
Using the COREL dataset, we compared the classification accuracy of McMIL with
that of a few state-of-the-art methods including MILES [6], 1-norm SVM [2], DDSVM [5], k-means-SVM [7], Hist-SVM [4] and MI-SVM [1]. Using COREL dataset,
we also comprehensively evaluate McMIL against DD-SVM. Using SIMPLIcity-II,
we compared the robustness of McMIL with that of DD-SVM in classifying images
where strong target concept overlaps exist among the classes. To facilitate fair
comparison with other methods, we use the same feature matrix for all the methods.
In all of the experiments, images within each category were randomly partitioned
in half to form a training set and a test set. We repeated each experiment for 5 random
splits, and reported the average of the results obtained over 5 random test sets, as done
in the compared approaches in the literature.
5.2 Results
Classification accuracy on COREL data set. Table 1 illustrates the classification
accuracy for each class obtained by McMIL and that of DD-SVM and MILES. For
McMIL, we presented the results on its two variants: one is McMIL90, which, instead

Evaluating Multi-Class Multiple-Instance Learning for Image Categorization

161

of using all the examples in all the other classes with negative labels, uses 10 images
per negative class to learn the IPs for each class. Another variant is McMIL450 which
uses all the examples in all the other classes with negative labels to learn the IPs for
each class. We are surprised to know that McMIL90 performs much better than DDSVM using 450 negative examples and MILES. This further implies that McMIL is
able to achieve higher classification accuracy by using much smaller amount of
training negative examples. The left table in Table 2 illustrates the average
classification accuracies of McMIL in comparison with other approaches. McMIL90
performs much better than Hist-SVM, k-means-SVM, MI-SVM, 1-norm SVM.
McMIL90 performs slightly better than DD-SVM and MILES. Moreover, McMIL450
performs much better than Hist-SVM, k-means-SVM, MI-SVM and 1-norm SVM
with a large margin, and it performs comparably well with MILES. McMIL450
outperforms DD-SVM, though the difference is not statistically significant.
Table 1. Classification accuracies (in %) for each class obtained by McMIL90, McMIL450,
DD-SVM and MILES on the COREL dataset
Class

McMIL90

McMIL450

0
1
2
3
4
5
6
7
8
9

71.6 ± 4.7
68.0 ± 6.8
76.8 ± 2.7
90.8 ± 2.7
100.0 ± 0.0
81.6 ± 6.1
94.4 ± 1.9
94.4 ± 3.4
67.2 ± 4.7
88.8 ± 1.6

70.4 ± 3.8
69.6 ± 4.19
74.8 ± 2.66
91.6 ± 3.37
100.0 ± 0.0
79.6 ± 5.32
95.6 ± 2.6
96.4 ± 2.6
62.8 ± 8.37
88.8 ± 4.04

DDSVM
67.7
68.4
74.3
90.3
99.7
76.0
88.3
93.4
70.3
87.0

MILES
68.8
66.0
75.7
90.3
99.7
77.7
96.4
95.0
71.0
85.4

Table 2. The classification accuracy (in %) obtained by McMIL and other methods (with the
corresponding 95% confidence intervals) using the COREL (left table) and SIMPLIcity-II
(right table) dataset

162

X. Xu and B. Li

Sensitivity to labeling noise. In multi-class classification, the labeling noise is
defined as the probability that an image is misclassified. It is important to show the
sensitivity of McMIL to labeling noise because in many practical applications, it is
usually impossible to get a “clean” data set and the labeling process is often
subjective [6]. In this experiment, we used 500 images from Class 0 to Class 4, with
100 images per class. The training and test sets have equal size. In the case of multiclass classification, training set with different level of labeling noise is generated as
follows: for the i-th class, i=0,…,4 we randomly pick d% training images and modify
their labels to j, j ≠ i, and the probability of j taking any value other than i is equal; for
the i-th class, i=0,…,4, we also randomly pick d% images from among all the other
classes (200 images in this case) and modify their labels to i. We compared McMIL
with DD-SVM for d=0, 2, 4, …, 20 and report the average classification accuracy
over 5 random test sets, as shown in fig. 3 (a). We use 50 positive images and 200
negative images per class to learn the IPs for both McMIL and DD-SVM. We observe
that for d=0, 2, 4, 6, 8, the classification accuracies of McMIL are 2% higher than
DD-SVM. As we increase the percent of training images with label noise from 10 to
20, the accuracy of McMIL remains roughly the same, but the accuracy of DD-SVM
drops very quickly. This indicates that our method is less sensitive to labeling noise
than DD-SVM.

(a)

(c)

(b)

(d)

Fig. 3. Compare McMIL with DD-SVM in terms of sensitivity to labeling noise (a), sensitivity
to number of negative images (b), sensitivity to number of categories (c), and sensitivity to
number of sample images (d)

Sensitivity to number of training negative images. We used 1000 images from
Class 0 to Class 9 (training and test sets are of equal size) to analyze the performance
of McMIL as the number of negative examples used for learning IPs for each class

Evaluating Multi-Class Multiple-Instance Learning for Image Categorization

163

varies from 90 to 450. That is, in the ith (i = 1,…,5) experiment, we randomly choose
10 × i negative examples from the 50 training images of each of the other classes, and
run the experiment for 5 random splits. In all these experiments, the number of
positive examples remains 50 in learning the IPs for the jth class, j=1,…, 10. As
indicated by fig. 3(b), even when the number of negative images drops to 90, McMIL
still performs very well. This is an advantage over binary MIL since the performance
of most binary MIL will degrade significantly as the amount of negative training data
decreases. Why the performance of direct multi-class MIL does not degrade with the
decreasing of the amount of negative training examples? We attempt to answer this
question as follows: for the OVA MIL approaches, a series of one-versus-all binary
classifiers are obtained, each trained to separate one from all the rest. This requires
the IPs learned for each class not only to represent the target concepts of the
underlying class but also distinguish the underlying class from the rest. This is
because, in the definition of DD [12], the target concepts are those intersection points
of all the positive bags without intersecting the negative bags, so the intersection
points of all the positive bags which also intersect with negative bags should be
excluded by learning from a large number of negative examples. However, in the
direct multi-class MIL, the requirement for the IPs to distinguish one class from
another is lessened, because the “distinguishing” power of the classifier can be offset
by learning in the multi-class feature space, due to the nice property of multi-class
feature matrix analyzed in Sec. 4. Therefore in order to accurately capture the target
concept of the underlying class (i.e. the instance prototypes), the number of training
negative examples used for McMIL can be much smaller than that used in DD-SVM.
Sensitivity to number of categories. In this experiment, a total of 10 data sets are
used, the number of categories in a data set varies from 10 to 19. A data set with i
categories contains 100 × i images from Class 0 to Class i-1. In learning IPs for each
class, we use the images in the current class with positive labels and all the images in
the other classes with negative labels. As shown by fig. 3(c). McMIL outperforms
DD-SVM consistently by a large margin for each data set.
Sensitivity to number of training images. We compared McMIL with DD-SVM as
the size of training set varies from 100 to 500. 1,000 images from Class 0 to Class 9
are used, with the size of the training sets being 100, 200, 300, 400, and 500 (the
number of images from each category is size of the training set/10). As indicated in
fig. 3(d), McMIL performs consistently better than DD-SVM as the number of
training images increases from 100 to 500.
Classification accuracy on SIMPLIcity-II. We have built SIMPLIcity-II that is
specifically designed to test the robustness of our method in classifying images where
strong target concept overlaps exist among the classes. While being small, the
SIMPLIcity-II dataset is in fact a very challenging one for various reasons. First, the
images in a semantic category are not “pure”. Therefore target concept overlaps occur
more frequently than in the COREL dataset. Second, the data contain noise in that a
negative bag may contain positive instances as well. Third, the images are very
diverse in the sense that they have various background, colors and combinations of
semantic concepts, and images are photographed at a wide-range or close up shots.

164

X. Xu and B. Li

The right table of Table 2 shows the average classification accuracy of McMIL and
DD-SVM on the SIMPLIcity-II dataset over 5 random test sets (training and test sets
are of equal size). Despite the aforementioned difficulties, McMIL has achieved
reasonably good results on this dataset, demonstrating that this is indeed a promising
approach. We also found that DD-SVM made much higher classification error than
McMIL in classifying People and Portrait images: DD-SVM misclassifies 9.2% of
People images into Portrait and 8.6% Portrait images into People; while McMIL only
produces 6.4% and 6.8% for the two classes respectively.

6 Concluding Remarks
In this paper, we described a multi-class multiple instance learning approach. We
have carried out systematic evaluation to demonstrate the benefits of direct McMIL.
Our empirical studies show that, in addition to overcoming the drawbacks of OVA
MIL approaches, McMIL is able to achieve higher classification accuracy than most
of the existing OVA MIL approaches even using only a small amount of training data,
and it is less sensitive to labeling noise. These two benefits are significant in practical
applications. The ability of McMIL in classifying images with strong target concept
overlap is found to be superior to DD-SVM. One drawback of the McMIL is that as
the number of classes becomes large, it becomes computationally inefficient since the
gradient search for the IPs takes quite a lot of time. One solution to increase the
efficiency of McMIL is to use only a small amount of negative examples; another is
to bypass searching for the IPs by combining MILES [6] with McMIL. Our future
work will explore these possibilities.

References
1. Andrews, S., Tsochantaridis, I., Hofmann, T.: Support vector machines for multipleinstance learning. NIPS 15, 561–568 (1998)
2. Bi, J., Chen, Y., Wang, J.Z.: A sparse support vector machine approach to region-based
image categorization. CVPR (1), 1121–1128 (2005)
3. Chang, C.-C., Lin, C.-J.: LIBSVM: a library for support vector machines. Software
available at http://www.csie.ntu.edu.tw/ cjlin/libsvm2001
4. Chapelle, O., Haffner, P., Vapnik, V.N.: Support vector machines for histogram-based
image classification. IEEE Trans. Neural Networks 10(5), 1055–1064 (1999)
5. Chen, Y., Wang, J.Z.: Image categorization by learning and reasoning with regions.
Journal of Machine Learning Research 5, 913–939 (2004)
6. Chen, Y., Bi, J., Wang, J.Z.: MILES: Multiple-Instance Learning via Embedded Instance
Selection. IEEE Trans. on PAMI. 28(12), 1931–1947 (2006)
7. Csurka, G., Bray, C., Dance, C., Fan, L.: Visual Categorization with Bags of Keypoints.
In: ECCV 2004 Workshop on Statistical Learning in Computer Vision, pp. 59–74 (2004)
8. Dietterich, T.G., Lathrop, R.H., Lozano-Pérez, T.: Solving the multiple-instance problem
with axis-parallel rectangles. Artificial Intelligence 89, 31–71 (1997)
9. Hsu, C.-W., Lin, C.-J.: A comparison of methods for multi-class support vector machines.
IEEE Transactions on Neural Networks 13(2), 415–425 (2002)

Evaluating Multi-Class Multiple-Instance Learning for Image Categorization

165

10. Lee, Y., Lin, Y., Wahba, G.: Multicategory support vector machines: theory, and
application to the classification of microarray data and satellite radiance data. Journal of
the American Statistical Association 99, 67–81 (2004)
11. Li, J., Wang, J.Z.: Automatic linguistic indexing of pictures by a statistical modeling
approach. IEEE Trans. on PAMI 25(9), 1075–1088 (2003)
12. Maron, O., Lozano-Pérez, T.: A framework for multiple-instance learning. NIPS 10, 570–
576 (1998)
13. Maron, O., Ratan, A.L.: Multiple-instance learning for natural scene classification. ICML,
341–349 (1998)
14. Rahmani, R., Goldman, S.A.: MISSL: multiple-instance semi-supervised learning. ICML,
705–712 (2006)
15. Schölkopf, B., Smola, A.J.: Learning with Kernels-Support Vector Machines,
Regularization, Optimization and Beyond, pp. 211–214. MIT Press, Cambridge, MA
(2002)
16. Vapnik, V.N.: Statistical Learning Theory. Wiley, New York (1998)
17. Wang, J.Z., Li, J., Wiederhold, G.: SIMPLIcity: Semantics-sensitive Integrated Matching
for Picture Libraries. IEEE Trans. on PAMI 23(9), 947–963 (2001)
18. Wang, L., Shen, X., Zheng, Y.F.: On L-1 norm multi-class Support Vector Machines. In:
Proc. 5th Intl. Conf. on Machine Learning and Applications (2006)
19. Weston, J., Watkins, C.: Multi-class support vector machines. In: Verleysen, M. (ed.) Proc.
ESANN 1999, D. Facto Press, Brussels (1999)
20. Zhang, Q., Goldman, S.A.: EM-DD: An improved multiple-instance learning technique.
NIPS 14, 1073–1080 (2002)
21. Zhang, Q., Goldman, S.A., Yu, W., Fritts, J.E.: Content-based image retrieval using
multiple-instance learning. In: Proc. ICML, pp. 682–689 (2002)

Supporting Navigation of Outdoor Shopping Complexes for Visually-impaired Users through
Multi-modal Data Fusion
Archana Paladugu, Parag S. Chandakkar, Peng Zhang and Baoxin Li
Computer Science and Engineering
Arizona State University
{apaladug,pchandak,pzhang41,baoxin.li}@asu.edu
ABSTRACT
Outdoor shopping complexes (OSC) are extremely difficult
for people with visual impairment to navigate. Existing GPS
devices are mostly designed for roadside navigation and
seldom transition well into an OSC-like setting. We report
our study on the challenges faced by a blind person in
navigating OSC through developing a new mobile
application named iExplore. We first report an exploratory
study aiming at deriving specific design principles for
building this system by learning the unique challenges of the
problem. Then we present a methodology that can be used to
derive the necessary information for the development of
iExplore, followed by experimental validation of the
technology by a group of visually impaired users in a local
outdoor shopping center. User feedback and other
experiments suggest that iExplore, while at its very initial
phase, has the potential of filling a practical gap in existing
assistive technologies for the visually impaired.
Index Terms — Visual Impairment, Outdoor
navigation, Touch Interface, GPS, Design, User Study.
1. INTRODUCTION
Navigating an unfamiliar outdoor space is a challenging task
for visually impaired (VI) people. The use of technological
devices in conjunction with specialized training and
accessible architecture of buildings and roads has made this
task possible. However, there are still many factors that
make this task stressful and sometimes dangerous for VI
users. The situation is worsened especially for outdoor
shopping complexes (OSC), whose building layout is often
cluttered with haphazard placement of stores, parking
spaces, walkways and driving roads, etc (Fig. 1). Existing
GPS devices seldom provide useful navigation formation for
a VI user in such environment .
The first problem towards building a system for OSC
navigation is the lack of awareness among developers
regarding the inadequacy of the current systems in place and
lack of studies stating ways to overcome the existing
information deficit. This paper reports our study on the

challenges faced by VI users when navigating OSC and lists
the necessary information required to bridge this gap.
Secondly, the required information is currently only
available from one source. We propose a solution to fuse
information from different sources. Thirdly, once the
necessary information is in place, our application catering to
VI users has presented a voice over for visual data. We
evaluate these techniques and propose a system design that
takes after the design paradigms obtained from our user
studies. This paper delves into these three aspects that are
required for the development of this system.
The focus on OSC was motivated both by their direct
relevance to the life of VI users and by the fact that
conventional map services do not provide navigation for an
OSC. Typical OSC are fairly complex in layout and thus
serve well as representatives of general outdoor building
complexes. Besides building a functional iPhone app that is
freely available to our participants, the work contributes to
developing general principles and guidelines in designing
interface schemes and information representation paradigms
for supporting mobile-device-based navigation assistance for
people with visual impairment. The paper is organized as
follows. The literature is presented in Section 2. We present
the user survey on the current deficit and present our
assessment of required information in Section 3. We present
our proposed approach covering details of data fusion,
design of the interface and schema used for data presentation
in Section 4. Our evaluation is presented in Section 5. The
conclusion and discussion is presented in Section 6.
2. RELATED WORK
Navigation assistance for visually impaired users includes
accessible infrastructure, specialized orientation and
mobility training [1], and technological aids [2].
Technological aids, which are the focus of this work, may be
devices that give live help on site or tools that help the user
to prepare for the journey ahead of time. A review of
dedicated devices for on-site navigation help using GPS,
sonar, laser, RFID, etc. can be found in [3]. Recent years
have seen new systems that are built upon general-purpose

© 2013 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works. DOI: 10.1109/ICME.2013.6607564}

touchscreen mobile devices, moving away from the
requirement of dedicated hardware. The most dominant
players of this type are these three iPhone apps: Ariadne
GPS
(www.ariadnegps.eu),
MotionX
GPS
(gps.motionx.com)
and
Lookaround
GPS
(www.senderogroup.com). Google’s Intersection Explorer
allows touch-based walk able path exploration. Recent years
have seen add-on techniques to make the underlying maps
more helpful by using audio, haptic [8] and spatial tactile
feedback [7]. There are relatively fewer methods/systems for
supporting exploration and spatial learning of locations (to
help a user’s planning of a trip ahead of time). Examples
include tactile map systems [4] and verbal description
generation methods [5].
Despite the existence of the aforementioned work, the
reality remains that none of them can support navigation in
an OSC setting largely due to the lack of adequate mapping
information [6], even with the newer type of systems like
Ariadne GPS. A solution to this deficit is to obtain
information for various sources and fuse them before
integrating them into the application. Some prior work on
map registration and geo-spatial data conflation addresses
the problem of combining data from different sources to
obtain the information for the applications that require
additional geo spatial information. A method was proposed
in [9] to align successive images taken aerially with an
overall map of the region using feature based registration
and mosaicking techniques. Linear features like active
contours to register images were proposed in [13]. A
technique that uses feature matching using RMS estimation
on affine transform was proposed in [14]. Street data were
used as control points and then cross correlation was
employed to obtain matching in [15]. None of these methods
are directly applicable for the following reasons. Firstly,
these methods have prior knowledge of the structure of their
maps and available information in their maps. Our data
fusion uses information obtained from standard maps as well
as shopping directories crawled from the web which can be
viewed as pseudo-maps. Secondly, detecting control points
now becomes an altogether different task due to the
dissimilarity between two images on a lower-level. The data
fusion for our application needs to be flexible enough to
work with a reasonable accuracy for images obtained online
with large variation in their structure. Further, current design
of some of those systems is essentially based on the concept
of making the underlying information (targeted at sighted
users) more accessible to VI users, without taking into the
real needs of the VI user in navigating a place.
The importance of having a system in place can be
emphasized by the fact that Google and Bing maps are
working on integrating floor plans of malls into their map
service. Google Maps has a user interface [17] published for
the user to identify three control points and scale the map on
the top of their street view map for user input. Bing maps
has also handled similar problem [18] without the need for

human intervention on popular shopping malls. Still these
map services are unable to provide navigation inside OSC,
and there is a need for a framework with data fusion to truly
build an application for a VI user in navigating OSC, which
is the focus of our study.
3. USER SURVEY ON CHALLENGES FACED
We started an exploratory study with a group of visually
impaired smart phone users in order to understand the
challenges and identify deficits of existing solutions. .
Feedback from our participants and online surveys of
visually-impaired communities helped us to conclude that
that the iPhone appears to be the most used and preferred
smart phone among users with visual impairment. Most of
the users are familiar with the voice-over feature on the
phone. Three of the five users we interviewed reported that
they collect information from the store before planning their
visit. The information asked for includes directions from
transit stop to the store entrance, landmarks that can identify
the store, etc. The users reported a significant ease in
navigation using GPS devices when walking on mainstream
streets as opposed to walking inside an OSC. All our
participants reported that seeking help from nearby humans
would be their final resort. A summary of the key findings in
the case study is given below:










An on-demand description of their surroundings is
always helpful for them to orient themselves.
Description of the surroundings can be effectively done
in terms of egocentric or allocentric methods. The
usability and preference of this description varies
widely from user to user and by location.
Some users prefer to have the description of distances
given in steps, as opposed to feet; low-vision users may
still enjoy the availability of a zoomed map.
Tactile landmarks are preferred to assert the location.
Using angles for direction is not usable. But the users
are familiar with terms like “diagonally right/left”.
It is not a good idea to direct them to walk through
parking lots. It is often unsafe to do so and would
involve hitting the cars with the cane.
Longer safer routes are preferred over shorter routes
with parking lots or obstacles.
Extra information: Restroom location, traffic conditions
on streets encountered, user created markers for future
reference, etc. would be great add-ons.

Part of the study involved asking our participants to
navigate a chosen local shopping complex with the help of
the two aforementioned iPhone apps. It is to be noted that
these apps helped only for navigating the major streets
bordering the shopping complex but failed to provide much
assistance for navigating the complex itself.

3.1. Design Guidelines for Building Apps for OSC
Based on our case study, we established a set of guidelines
for developing an effective iPhone app in addressing the
deficits of existing solutions. We summarize them below:
Avoid additional screens as much as possible: A VI user
would face frustration in using finger gestures to get back
and forth in a multi-layer menu and thus a flat structure
should be used as much as possible in the interface.
Less is more: Our users disliked navigating through a page
filled with too many buttons. They asked for a few
functionalities that convey a lot of information, during our
case study questionnaire.
Layered information delivery: Users with varying abilities
will need different amount of information. Having
information interlaced with gestures is preferred, so
additional information is presented only on-demand.
Orientation and Mobility training: Most of the blind users
have undergone the O&M training. The app needs to be
consistent with the protocols in presenting the information.
Supporting user notes: Every user from our case study
picked up some different cues about his/her surroundings. A
mechanism for the user to record his/her own markings
would enhance the usefulness of the application.
Supporting user customization: Low-vision and completelyblind users have different requirements. Users may prefer
measuring distance in different ways. The app should allow
some user-level customization to support such features.
4. PROPOSED DESIGN
We now present the design of a novel iPhone app that aims
at addressing the challenges faced by VI users in navigating
OSC. Largely based upon the design principles which were
derived from the exploratory study, the proposed design and
implementation attempts to overcome the challenges from
the following four aspects: an information fusion technique,
an information representation structure that is appropriate
for mapping-related tasks in an OSC setting, an intuitive
interface and interaction scheme, and the support for usercustomization to cater to individual needs of the users.
These are shown in the figure 1.
4.1. Tiered Information Creation
The major part of the problems faced when using map-based
technologies for navigation in OSC is the lack of required
information. Publicly available map services such as Google
maps do not have the desired level of details for typical
OSC. However, most shopping centers maintain and publish
maps with rich annotations. Also, a sighted volunteer may be
able to label a satellite image of a shopping complex as to
where are the parking lots etc. Finally, a user while using the

Fig. 1. The tiered information representation scheme used to
support the application.

app, may want to insert his/her notes to a location.
Considering all these possibilities, we adopt a tiered
representation for all the mapping information. Figure 1
illustrates how this is currently implemented. In Figure 1, the
base layer corresponds to the map that is typically available
from a GPS system, the second layer is the layout map given
by the shopping center, the third layer illustrates an image of
the same locale with additional labels, and the fourth layer is
used to store user notes.
4.1.1. Base Layer Information
As seen in Figure 1, the base layer contains information
available in the typical mobile applications using
Google/Bing/Apple/other maps. This information is
sometimes sparse, and from our studies, often inaccurate in
an OSC setting. The locations of individual stores and bus
stations are more often than not, inaccurate and haphazardly
placed. Using this information for VI users is often
dangerous for this reason. In our scheme, we find additional
sources to obtain this kind of information with reasonable
accuracy and map it on top of the base layer.
4.1.2. Second Layer: Information from the Web
The most vital information when navigating an OSC is the
location of the stores inside the mall. This is the kind of
information that is not available on the Google maps. One
way to obtain this sort of information is to devise a method
to integrate the Google maps with the store directories
available on the website of OSC. . This can be viewed as a
map-to-store map registration problem. We try to register
the shopping mall directory to its corresponding Google
map. The purpose is two-fold. Shopping directory has much
more information as compared to maps and by overlaying
shopping directory onto Google maps, we can still retain the
GPS coordinate information.

Control point detection is a major step in any
registration problem. We propose the use of shop centers as
the control points as the shapes of stores in both Google
maps and shopping directory have high-level similarity. We
also employ road and parking lot detection but are not used
for extracting control points since shopping directory may
not have them. Firstly, we detect yellow-colored major roads
and orange-colored freeways from Google maps by simple
color segmentation. Black-colored text labels on Google
maps are detected by using the same principle. Roads in
Google Maps always have text labels on them and they have
a lower-limit on their width. We use this fact to distinguish
parking lots from roads. The roads are detected by using
region-growing image segmentation technique in
combination with distance transform. Distance transform
allows us to monitor the width of the road. If it falls below
the predefined minimum width or if there is no “white” road,
then the region-growing stops. Text labels act as seed points
while performing region-growing.

Our problem inherently has lot of outliers because of
“extra” information present in the shopping directory and
thus we need an algorithm, which can perform better in
presence of noise and outliers. Therefore we propose the
use of Coheren

(a)

(b)

(c)
Fig 3. Control point detection in (a) Google maps and (b) shopping
mall directory (c) Registration of Google map and shopping
directory

Coherent Point Drift (CPD) algorithm proposed in [16]. It
considers the registration problem as probability density
estimation problem. It represents the first data point set by
Gaussian Mixture Model (GMM) centroids and tries to fit
the second data point set by maximizing the likelihood. The
results obtained are indeed promising and they are shown
below:
Fig 2. Robust street detection

The next procedure involves detecting stores in Google
maps and the corresponding shopping directory. In Google
maps, each shop has a label associated with it and it also has
a border. Thus we can detect stores by extracting the entire
portion which has the same color as the background color of
the text label. The convex hull of the portion is calculated
and those having arbitrary shape (not resembling to
rectangle) are discarded. This is done based on the
assumption that each shop has a regular shape. To detect
stores from shopping directory, we ask user to select points
which represent colors of the stores in the shopping
directory. After this, the stores are detected by simple color
segmentation. The control points in the two binary images
are the centroids of each disconnected component (store).
The centers are overlaid onto the original image for
visualization purpose in figure 4.

Fig 4. Registration of two sets of control points using CPD

4.1.3. Additional Annotations
According to our user consensus, they would like to know
the location and the extent of parking lots, walkways and bus
route information. This kind of metadata is not available on
any mapping service. But the imagery of the maps shows a

clear distinction that can be utilized to demarcate these
areas. In our scheme, we do parking lot and walkway
detection. The parking lot detection is carried out as
described in the previous section. The walkways in front of
stores can be detected by using the grey color gradient used
consistently at this scale. We add bus route information
obtained from the metadata to obtain all the additional data
requested in the user survey.
4.1.4. User Defined Tags
In spite of providing additional user friendly information,
there were some elements of each location that the visually
impaired users highlighted as useful in navigation. This
information ranged from the presence of water fountains,
notes on location of shrubs on the walkways, narrowness of
some passages, etc. Such information could only be gathered
from the users themselves. We collected this information
from our participants, orientation and mobility instructors
and spouses of VI participants and geo tagged this
information as a layer.
4.2. Supporting User Customization
The necessity of supporting user customization has been
concluded earlier. In our current implementation, we support
the following three types of customization. The first
customization provided is the user preference on distance
metric. Some users preferred the usage of steps and blocks
to usage of absolute distance in terms of feet. We introduce
a user-based calibration feature that calculates the step-tofeet ratio. The user is asked to walk 20 steps prior to the visit
in a familiar environment to allow for calibration. The
second customization is the method used to convey
direction. Users prefer egocentric or allocentric method of
description, depending on the location and complexity of the
surroundings. The third customization provided is based on
the level of vision of the user. The totally-blind users
interact with the system using the predestinated gestures and
through voice-over output. The low-vision users can have
the additional freedom of interacting with a spatial zoomable map and larger font sizes.
4.3. An Intuitive Interface for Supporting Necessary
Functionalities
Based on our case study, user input and our understanding of
the existing GPS devices, we propose the following interface
to help users navigating OSC. Our application uses iPhones
accessibility mode and uses the standard voice-over
gestures. A user can scroll though the buttons without
knowing their spatial locations, using one finger swipe. A
double tap anywhere on the screen selects an item. We
refrain from introducing too many additional gestures to
make the app as simple to use as possible.

Homepage: The homepage of the app consists of an entry
into all the possible functions of the app. We adhered to the
design paradigm inferred from the user study, stating the
user preference of not having too many screens to navigate
and having a few necessary functions on the screen. We
have designed our homepage in such a way that the user can
obtain all the important information by staying on the
homepage. We interlaced various gestures to help access
additional information. Figure 2 shows a screenshot of our
homepage and an overview of all the functions.
Where am I? “Where am I” is a popular functionality
provided by most of the existing GPS applications. A sample
result for this function can be illustrated as follows: “Facing
North near 661 Meadow Avenue”. Once inside a shopping
space, a description in terms of an address is no longer
relevant information. We modify this functionality to make it
more informative and convey necessary information in a
tiered manner. When a user double taps on the “Where am
I?” button, our application reads out the orientation of the
user, the closest landmark and the nearest landmarks in
either directions as shown in Figure 2. He repeats this
gesture to listen to this information again. The user has an
option to ask for more information after he listens to this
information. A pre-assigned gesture (3 taps on the iPhone)
provides information about the nearest streets and any user
tagged notes if they exist around this location. He repeats
this gesture to listen to this information again.

Fig. 4. An overview of iExplore’s homepage and a sample output
for each of the proposed functions can be seen above.

Points of Interest: Most applications and GPS devices
support points of interest, where a predestined number of
landmarks around the current location are listed out to the
user in terms of distance and direction. We include this
feature in our application, except that the information
associated with this button is according to the tiered
representation discussed above.

Where is my destination?: The directions provided by most
applications are hard to use for VI users. We propose a
scheme to provide blind-friendly directions. Once inside an
OSC, the user can be standing inside a parking lot, a store or
at a major landmark. Our description of the destination
location takes into account that users do not like walking
through the parking lots, considers the safety quotient of
streets, includes description of places in-between in terms of
stores lining the route and major streets on the way. The user
inputs the name of the store or landmark he is interested in
using the speech input feature provided. A verbal description
(as seen in Figure 2) is then generated tailored to the user
preference of egocentric or allocentric methodology, in
terms of their distance preference. We use Dijkstra's
algorithm to compute the shortest path among the walkable
options. Using this path, our description takes into account
relative positions of the landmarks and additional tagged
information about the surroundings
5. EVALUATION
The proposed design has been implemented as an iPhone
app called iExplore, which was tested by four participants
for navigating a local outdoor shopping complex. We
summarize some major outcomes of the tests below.
Although this method is applicable to any outdoor shopping
complex, for the sake of user studies, we picked one local
mall and conducted our experiments at this location. The
statistics of this experiment are presented below. The dataset
consists of 6 outdoor shopping complexes. We used the
additional features of the API to obtain street map with and
without labels. We tested the accuracy of map registration
and parking lot detection on this dataset. The dataset was
chosen on the basis of availability of store map directories
on their websites.
5.1. Accuracy of the Map Registration Technique
After registering Google maps to the corresponding
shopping directory, we develop an evaluation scheme to
validate proposed method. It is described as follows: We
detect stores in Google maps as well as in the registered
image. Now, we estimate the percentage of pixels of the
Google map which overlap with the pixels of registered
image. This also validates our shop detection algorithm.
Figure 5(a) and 5(b) show the result of shop detection onto
the Google map and the shopping directory. There is a
66.45% overlap in the area marked by our approach when
compared to the original store directory and the ground
truth. The parking lot detection is 100 percent accurate.
5.2. User Feedback on Overall System Design
Upon loading the app on the user’s iPhone, we asked them
to take time and get familiar with the buttons and tabs on the

(a)

(b)

Fig 5. Store detection in (a) Google map (b) Registered image

home screen. The users were asked to think aloud (make
explicit comments) while using the app. They were not
provided any additional training on the usage. After the
users were familiar with all the functionalities, they were
asked questions about the interface, its usability, the
intuitiveness of each button, easiness of navigating the
buttons, etc. The users were encouraged to ask us questions
to clarify any aspect they found confusing. One key
observation every participant made was that the interface has
very few selectable items on the screen. They were able to
summarize the key components upon closing the app. They
also reported to have received more useful information than
they would have hoped for (compared with other systems)
from each functionally buttons.
5.3. Testing the Proposed Functionalities
The users were taken back to the testing site, where we
conducted our initial case studies. We started at the bus
station for uniformity and to simulate a real-time scenario.
The users were asked to test each of the functionalities and
were asked to walk around. We tagged them with a
volunteer for safety reasons and the volunteer collected
feedback from the users. The user was asked to change the
settings to his preference of distance, terminology for
directions, etc. Once done, the user is asked to go back to
the home screen. He is asked to find the “Where am I”
button and access the information. We then asked the user to
point out the direction using their hand, towards one of the
landmarks mentioned. This test was aimed at testing out the
accuracy of information conveyed as well as user
understanding from the verbal descriptions provided. The
users got their directions right with 100% accuracy.
5.4. Evaluating Navigation Support
To evaluate the support provided by iExplore for navigation,
we defined preset routes with 2 turns and asked the users to
use the app to find the destination. If the participant took a
wrong turn, we recorded a miss for that segment of the
journey and let the user turn around. The user was asked to
use the app to gain a better understanding of his
surroundings. The system failed to orient the user in the right
direction once and one out of the four users asked for

assistance once during the experiment. At the end of the
stretch, the user was asked to describe his surroundings and
the relative positions of the store he walked by. This test
aimed at validating our assumption that the users liked
exploring and gaining information about their surroundings.
We did not intend this test to be a memory game, but most
users were able to figure out the relative positions of the
stores they walked by with 100% accuracy.
6. CONCLUSIONS AND DISCUSSION
We developed an iPhone application, iExplore, which
provides assistance in navigating an outdoor shopping
complex, for VI users. This application has the potential to
turn highly inaccessible locations (like OSC) into blindfriendly and navigable locations. The current version is not
yet a perfect solution that solves every issue, but the users
from our case study indicated high confidence in exploring
the area with the help of this application and relying less on
asking for help. We also put forth a summary of some design
principles that we learnt through our studies. We then
presented a way to obtain the relevant information from
multiple sources and use a tiered presentation to effectively
present the information. Our study was conducted and the
proposed app was tested in one outdoor shopping location.
Our framework has the following limitations. Firstly, map
registration methods on unseen, non-standardized images
provide a relatively low accuracy. But using non-rigid
registration helps get an approximate location for each store
which can be utilized to give the users a proximate location.
Secondly, the store directories available online for download
often contain noisy information and legend information that
is hard to work with. We worked around this problem by
picking our dataset to have moderately complex maps. Our
future work will include a detailed study for various layouts
and locations. Thirdly, there is a huge element of user
subjectivity in our design. We plan to overcome this by
increasing the number of subjects in our experiments and
testing the application in different OSC locations.
Acknowledgement: The work was supported in part by a
grant (#1135616) from the National Science Foundation.
Any opinions expressed in this material are those of the
authors and do not necessarily reflect the views of the NSF.
7. REFERENCES
[1] Blasch,B., Wiener, W., Welsh, R. Foundations of
orientation and mobility, 2nd Ed. AFB Press, New
York, NY, USA, 1997.
[2] Giudice, N.A., & Legge, G.E. (2008). Blind navigation
and the role of technology. In A. Helal, et al (Eds.),
Engineering handbook of smart technology for aging,
disability, and independence (pp.479-500): John Wiley
& Sons., 2nd Ed.

[3] N. Fallah, L. Apostolopoulos, K. Bekris, E. Folmer, The
user as a sensor: Navigating users with visual
impairments in indoor spaces using tactile landmarks.
Proc. CHI 2012.
[4] Z. Wang, B. Li, T. Hedgpeth, T. Haven. Instant tactileaudio map: enabling access to digital maps for people
with visual impairment. ACM SIG ASSETS 2009.
[5] Kalia, A.A., Legge, G.E., Roy, R., & Ogale, A. (2010).
Assessment of Indoor Route finding Technology for
People who are Visually Impaired. Journal of Visual
Impairment & Blindness, 104(3), 135-147.
[6] Kitchin, R. M., Jacobson, R. D. Techniques to collect
and analyze the cognitive map knowledge of persons
with visual impairment or blindness: issues of validity.
Journal of Visual Impairment & Blindness (1997), 360376.
[7] Yatani, K., Banovic, N., Truong, K. N.. SpaceSence:
Representing Geographical Information to Visually
Impaired People Using Spatial Tactile Feedback. Proc.
CHI 2012, ACM (2012),415-424.
[8] Yang, R., Park, S., Mishra, S. R., Hong, Z., Newsom,
C., Joo, H., Hofer, E., Newman, M. W. Supporting
spatial awareness and independent wayfinding for
pedestrians with visual impairments. Proc. ASSETS
2011, ACM Press(2011), 27-34.
[9] Lin, Y., Place, W., & Angeles, L. (2007). MapEnhanced UAV Image Sequence Registration and
Synchronization of Multiple Image Sequences.
[10] Wang, C., Stefanidis, A., Croitoru, A., & Agouris, P.
(2008). Map Registration of Image Sequences Using
Linear Features. Photogrammetric Engineering &
Remote Sensing, 74(1), 25–38.
[11] Zitová, B., & Flusser, J. (2003). Image registration
methods: a survey. Image and Vision Computing,
21(11), 977–1000.
[12] Chen, C.-C., Knoblock, C. a., & Shahabi, C. (2007).
Automatically Conflating Road Vector Data with
Orthoimagery. GeoInformatica, 10(4), 495–530.
doi:10.1007/s10707-006-0344-6
[13] Wang, C., Stefanidis, A., Croitoru, A., & Agouris, P.
(2008). Map Registration of Image Sequences Using
Linear Features. Photogrammetric Engineering &
Remote Sensing, 74(1), 25–38.
[14] Roux, M., Nationale, E., Ima, D., & Cedex, F. P.
(1996). Automatic registration of SPOT images and
digitized maps, Ima, 46, 8–11.
[15] Chen, C.-C., Knoblock, C. a., & Shahabi, C. (2007).
Automatically Conflating Road Vector Data with
Orthoimagery. GeoInformatica, 10(4), 495–530.
[16] Jian, Bing, and Baba C. Vemuri. "Robust point set
registration using gaussian mixture models." Pattern
Analysis and Machine Intelligence, IEEE Transactions
on 33.8 (2011): 1633-1645.

2015 IEEE 15th International Conference on Data Mining Workshops

Sentiment Analysis for Social Media Images
Yilin Wang
School of Computer Science
Arizona State University
Tempe, Arizona
Email: ywang370@asu.edu

Baoxin Li
School of Computer Science
Arizona State University
Tempe, Arizona
Email: baoxin.li@asu.edu

lives1 , we can assess a person’s emotional wellness based on
the emotion and sentiment inferred from her photos on social
media platforms (in addition to existing emotion/sentiment
analysis effort, e.g., see [1] on text-based social media).
In this paper, our goal is to automatically infer human sentiments (positive, neutral and negative) from photos shared
on Flickr and Instagram. While sentiment analysis of photos
is still in its infancy, a number of tools have been proposed
during past two years. A popular approach is to identify
visual features from a photo that are related to human
sentiments, such as objects (e.g., toys, birthday cakes, gun),
human actions (e.g., crying or laughing), and many other
features like color temperature. However, such an approach
is often insufﬁcient because the same objects/actions may
convey different sentiments in different photo contexts. For
example, consider Figure 1: one can easily detect the crying
lady and girl (using computer vision algorithms such as face
detection and expression recognition). However, the same
“crying” action conveys two clearly different sentiments: the
“crying” in Figure 1a is obviously positive as the result of
a successful marriage proposal. In contrast, the tearful girl
in Figure 1b looks quite unhappy thus expresses negative
sentiment. In other words, the so-called “visual affective
gap” [2] exists between rudimentary visual features and
human sentiment embedded in a photo. On the other hand,
one may also consider inferring the sentiment of a photo via
its textual descriptions (e.g., titles) using existing off-theshelf text-based sentiment analysis tools [3]. Although these
descriptions can provide very helpful context information
of the photos, solely relying on them while ignoring the
visual features of the photos can lead to poor performance
as well. Consider Figure 1 again: by analyzing only the text
description, we can conclude that both Figure 1a and 1b
convey negative sentiment as the keyword “crying” is often
classiﬁed as negative sentiment in standard sentiment lexicon
. Last, both visual feature-based and text-based sentiment
analysis approaches require massive amounts of training data
in order to learn high quality models. However, manually
annotating the sentiment of a vast amount of photos and/or
their textual descriptions is time consuming and error-prone,

Abstract—In this proposal, we study the problem of understanding human sentiments from large scale collection of
Internet images based on both image features and contextual
social network information (such as friend comments and
user description). Despite the great strides in analyzing user
sentiment based on text information, the analysis of sentiment
behind the image content has largely been ignored. Thus,
we extend the signiﬁcant advances in text-based sentiment
prediction tasks to the higherlevel challenge of predicting the
underlying sentiments behind the images. We show that neither
visual features nor the textual features are by themselves
sufﬁcient for accurate sentiment labeling. Thus, we provide a
way of using both of them, and formulate sentiment prediction
problem in two scenarios: supervised and unsupervised. We
develop an optimization algorithm for ﬁnding a local-optima
solution under the proposed framework. With experiments on
two large-scale datasets, we show that the proposed method
improves signiﬁcantly over existing state-of-the-art methods.
In the future, we are going to incorporating more information
on the social network and explore sentiment on signed social
network.

I. I NTRODUCTION
A picture is worth a thousand words. It is surely worth
even more when it comes to convey human emotions and
sentiments. Examples that support this are abundant: great
captivating photos often contain rich emotional cues that
help viewers easily connect with those photos. With the
advent of social media, an increasing number of people start
to use photos to express their joy, grudge, and boredom
on social media platforms like Flickr and Instagram. Automatic inference of the emotion and sentiment information
from such ever-growing, massive amounts of user-generated
photos is of increasing importance to many applications
in health-care, anthropology, communication studies, marketing, and many sub-areas within computer science such
as computer vision. Think about this: Emotional wellness
impacts several aspects of people’s lives. For example, it
introduces self-empathy, giving an individual greater awareness of their feelings. It also improves one’s self-esteem and
resilience, allowing them to bounce back with ease, from
poor emotional health, and physical stress and difﬁculty. As
people are increasingly using photos to record their daily
978-1-4673-8493-3/15 $31.00 © 2015 IEEE
DOI 10.1109/ICDMW.2015.142

1 http://www.pewinternet.org/2015/01/09/social-media-update-2014/

1584

face recognition [12], object detection [13] and feature
selection [14], etc. Speciﬁcally, the work in [15] brings
more attention to NMF in the research community, where
the author proposed a simple multiplicative rule to solve
the problem and showed the factor coherence of original
image data. [16] shows that if adding orthogonal constrains,
the NMF is equivalent to K-means clustering. Further, [17]
presents a work that shows, when incorporating freedom
control factors, the non-negative factors will achieve a better
performance on classiﬁcation. In this paper, motivated by
previous NMF framework for learning the latent factors,
we extend these efforts signiﬁcantly and propose a comprehensive formulation which incorporates more physicallymeaningful constraints for regularizing the learning process
in order to ﬁnd a proper solution.

(a) “Girlfriend crying a lot when I(b) Crying baby after her toy was
proposed to her”.
taken

Figure 1: An example shows affective gap.

presenting a bottleneck in learning good models.

III. T HE P ROPOSED F RAMEWORKS
The weaknesses discussed in the foregoing motivate the
need for a more accurate automated framework to infer
the sentiment of photos, with 1) considering the photo
context to bridge the “visual affective gap”, 2) considering
a photo’s visual features to augment text-based sentiment,
and 3) considering the availability of textual information,
thus a photo may have little or no social context (e.g.,
friend comments, user description). Here we provide two
frameworks based on supervised learning and unsupervised
learning.

II. R ELATED W ORK
In this section, we review the related work on sentiment
analysis and the methods for matrix factorization.
Sentiment analysis on text and images: Recently, sentiment analysis has shown its success in opinion mining on
textual data, including product review [4], [5], newspaper
articles [3], and movie rating [6]. Besides, there have been
increasing interests in social media data [7]–[10], such as
Twitter and Weibo data. Unlike text-based sentiment prediction approaches, [7], [10] employed mid-level attributes of
visual feature to model visual content for sentiment analysis.
[8] provides a method based on low-level visual features and
social information via a topic model. While [9] tries to solve
the problem by a graphical model which is based on friend
interactions. In contrast to our approach, all such methods
restrict sentiment prediction to the speciﬁc data domain. For
example, in Figure 1, we can see that approaches using
pure visual information [7], [10] may be confused by the
subtle sentiment embedded in the image. e.g., two crying
people convey totally different sentiment. [8], [9] assume
that the images belong to the same sentiment share the
same low-level visual features is often not true, because
positive and negative images may have similar low-level
visual features, e.g., two black-white images contain smiling
and sad faces respectively. Recent, deep learning has shown
its success in feature learning for many computer vision
problem, [11] provides a transfer deep neutral network
structure for sentiment analysis. However, for deep learning
framework, millions of images with associated sentiment
labels are needed for network training.In real world, such
label information is not available and how to deal with
overﬁtting for small training data remains a challenging
problem.
Non-negative matrix factorization(NMF): Our proposed
framework is also inspired by recent progress in matrix
factorization algorithms. NMF has been shown to be useful
in computer vision and data mining applications including

A. Supervised Sentiment Analysis
We propose an efﬁcient and effective framework [18],
named RSAI (Robust Sentiment Analysis for Images), for
inferring human sentiment from photos that leverages these
partial solutions. Speciﬁcally, to ﬁll the visual affective gap,
we ﬁrst extract visual features from a photo using lowlevel visual features (e.g., color histograms) and a large
number of mid-level (e.g., objects) visual attribute/object
detectors. It is formed by merging the low-level visual
features to the detected mid-level objects and mapping them
to a dictionary. On the other hand, to learn the image’s
context, we analyze the image’s textual description and
capture its sentiment based on sentiment lexicons. Finally,
with the help from ANPs and image context, RSAI infers
the image’s sentiment by factorizing an input image-features
matrix into three factors corresponding to image-term, termsentiment and sentiment-features. Similarly, the learnt image
context can be used to constrain image-term and termsentiment factors. Last, the availability of labeled sentiment
of the images can be used to regulate the product of imageterm, term-sentiment factors. We pose this factorization as
an optimization problem where, in addition to minimizing
the reconstruction error, we also require that the factors
respect the prior knowledge to the extent possible. We derive
a set of multiplicative update rules that efﬁciently produce
this factorization, and provide empirical comparisons with

1585

Table I: Notations
Notation
X
T
S
V
T0
S0
V0
R0

Dimension
n×m
n×t
t×k
m×k
n×t
t×k
m×k
n×k

corpus 2 . Since this corpus is constructed without respect
to any speciﬁc domain, it provides a domain independent
prior on word-sentiment association. It should be noted
that the English usage in social network is very casual
and irregular, we employ a stemmer technique proposed in
[19]. As a result, the ill-formed words can be detected and
corrected based on morphophonemic similarity, for example
“good” is a correct version of “goooooooooooood”. Besides
some abbreviation of popular words such as “lol”(means
laughing out loud) is also added as prior knowledge. We
encode the prior knowledge in a word sentiment matrix
S0 where if the ith word belongs to jth sentiment, then
S0 (i, j) = 1, otherwise it equals to zero. In addition to the
prior knowledge on lexicon, our second prior knowledge
comes from the Visual Sentiment Ontology (VSO) [7],
which is based on the well known previous researches on
human emotions and sentiments [20], [21]. The sample ANP
sentiment scores are shown in Table 2. Similar to the word
sentiment matrix S0 , the prior knowledge on ANPs V0 is the
sentiment indicator matrix. Our last prior knowledge focuses
on the prior knowledge on the sentiment label associated
with the image itself. As our framework essentially is a semisupervised learning approach, this leads to a domain adapted
model that has the capability to handle some domain speciﬁc
data. The partial label is given by the image sentiment matrix
R0 where R0 ∈ Rn×k .
To solve the optimization problem above, we employ the
alternating multiplicative updating scheme shown in [17] to
ﬁnd the optimal solutions. First, we use ﬁxed V and S to
update T as follows:

Description
Input data matrix
Data-term matrix
Term-sentiment matrix
Feature-sentiment matrix
Prior knowledge on T
Prior knowledge on S
Prior knowledge on V
Prior knowledge on the labels

several competing methodologies on two real datasets of
photos from Flickr and Instagram. We examine the results
both quantitatively and qualitatively to demonstrate that our
method improves signiﬁcantly over baseline approaches.
We assume that all the images can be partitioned into K
sentiment (K = 3 in this paper as we focus on positive,
neutral and negative. Our goal is to model the sentiment
for each image based on visual features and available text
features. Let n be the number of images and the size of
contextual vocabulary is t. Table 1 lists the mathematical
notation used in this section.
Since, we can then easily cluster the images with similar
word frequencies and predict the cluster’s sentiment based
on its word sentiment. Meanwhile, for each image, which
has m-dimensional visual features, we can cluster the images
and predict the sentiment based on the feature probability.,
our framework takes these n data points and decomposes
them simultaneously into three factors: photo-text, textsentiment and visual feature-sentiment. In other words, our
model tries to solve the following optimization problem:




2
2
min X − T SV T F + α V − V0 F

T SV

2

2

+ β T − T0 F + γ S − S0 F
2

+ δ T S − R0 F
subject to T ≥ 0, S ≥ 0, V ≥ 0

Tij ← Tij

[XV S T + βT0 + δR0 S T ]ij
[T SV T V S T + βT + δT SS T ]ij

(2)

Next, we use the similar update rule to update S and V :

(1)


Sij ← Sij

[T T XV + γS0 + δT T R0 ]ij

[T T T SV T V + γS + δT T T S]ij

[X T T S + αV0 ]ij
Vij ← Vij
[V S T T T T S + αV ]ij

where X ∈ Rn×m represents input data matrix, and
T ∈ Rn×t indicates the text features. That is, the ith row of
matrix T corresponds to the posterior probability of the ith
image’s contextual social network information referring to
the t text terms (vocabulary). Similarly, S ∈ Rt×k indicates
the posterior probability of a text belonging to k sentiments.
Finally, V ∈ Rm×k represents the sentiment for each ANP.
The regularization term T0 is the term-frequency matrix for
the whole word vocabulary (which is built based on textual
descriptions of all photos). It is worth noting that the nonnegativity makes the latent components easy to interpret.
Besides, we also introduce three types of prior knowledge
for model regularization: (1) sentiment-lexicon of textual
words, (2) the normalized sentiment strength for each ANP,
and (3) sentiment labels for each image. The ﬁrst prior
knowledge is from a public sentiment lexicon named MPQA

(3)

(4)

The learning process consists of an iterative procedure
using Eq (3), Eq (4) and Eq (5) until convergence. The
description of the process is shown in Algorithm 1.
B. Unsupervised Sentiment Analysis
The vast majority of existing methods are supervised,
relying on labeled images to train sentiment classiﬁers.
Unfortunately, sentiment labels are in general unavailable for
social media images, and it is too labor- and time-intensive to
obtain labeled sets large enough for robust training. In order
2 http://mpqa.cs.pitt.edu/

1586

Algorithm 1 Multiplicative Updating Algorithm
Input: X, T0 , S0 , V0 , R0 , α, β, γ, δ
Output: T, S, V
Initialization: T, S, V
while Not Converge do
Update T using Eq(2) with ﬁxed S,V
Update S using Eq(3) with ﬁxed T,V
Update V using Eq(4) with ﬁxed T,S
End

the same set of images, it is reasonable to assume that they
share the same sentiment label space. More speciﬁcally,
the sentiment of Ii should be consistent with that of its
associated textual information pi . Let U0 ∈ Rn×k be
the sentiment label space where U0 (i, j) = 1 if the i-th
data instance belongs to cj , and U0 (i, j) = 0 otherwise.
We propose the following formulation to incorporate visual
information with textual information based on nonnegative
matrix factorization:

2
2

min Xv − Uv VvT  + α Xt − Ut VtT 
UV

to utilize the vast amount of unlabeled social media images,
an unsupervised approach would be much more desirable.
This paper studies unsupervised sentiment analysis.
Typically, visual features such as color histogram, brightness, the presence of objects and visual attributes lack the
level of semantic meanings required by sentiment prediction.
In supervised case, label information could be directly utilized to build the connection between the visual features and
the sentiment labels. Thus, unsupervised sentiment analysis
for social media images is inherently more challenging than
its supervised counterpart. As images from social media
sources are often accompanied by textual information, intuitively such information may be employed. However, textual
information accompanying images is often incomplete (e.g.,
scarce tags) and noisy (e.g., irrelevant comments), and thus
often inadequate to support independent sentiment analysis.
On the other hand, such information can provide muchneeded additional semantic information about the underlying
images, which may be exploited to enable unsupervised
sentiment analysis. How to achieve this is the objective of
our approach.
In this paper, we study unsupervised sentiment analysis
for social media images with textual information by investigating two related challenges: (1) how to model the interaction between images and textual information systematically
so as to support sentiment prediction using both sources of
information, and (2) how to use textual information to enable
unsupervised sentiment analysis for social media images.
In addressing these two challenges, we propose a novel
Unsupervised SEntiment Analysis (USEA) [22] framework,
which performs sentiment analysis for social media images
in an unsupervised fashion.
Without label information, it is challenging for unsupervised sentiment analysis to connect visual features with
sentiment labels. Textual information associated with social
media images may be exploited to help, as it provides
semantics about the underly images and in particular rich
sentiment signals such as sentiment words and emotion
symbols may be found in the textual ﬁelds. Hence, to exploit
textual information, we investigate (1) how to incorporate
textual information into visual information; and (2) how to
model sentiment signals in textual information.
Since visual and textual information are two views about

F

2
U 0 F

F

2
U 0 F )

+ β(Uv −
+ Ut −
subject to Uv ≥ 0, Ut ≥ 0; ||U0 (i, :)||0 = 1, i ∈ {1, 2, ..n}
U0 (i, j) ∈ {0, 1} j ∈ {1, 2, ..k}
(5)
where α controls how textual information contributes to
the model and || · ||0 is 0 , which counts the number of
nonzero entries in the vector. Uv ∈ Rn×k and Ut ∈ Rn×k
are the sentiment label spaces learned from visual information and textual information, respectively. The term of
2
2
β(Uv − U0 F + Ut − U0 F ) ensures that these two
types of information should share the sentiment label space
U0 . Vv ∈ Rmv ×k and Vt ∈ Rmt ×k indicate the sentiment
polarities of visual and textual features, respectively.
Since visual and textual information are two views about
the same set of images, it is reasonable to assume that they
share the same sentiment label space. More speciﬁcally,
the sentiment of Ii should be consistent with that of its
associated textual information pi . Let U0 ∈ Rn×k be
the sentiment label space where U0 (i, j) = 1 if the i-th
data instance belongs to cj , and U0 (i, j) = 0 otherwise.
We propose the following formulation to incorporate visual
information with textual information based on nonnegative
matrix factorization:


2
2
min Xv − Uv VvT F + α Xt − Ut VtT F
UV

2

2

+ β(Uv − U0 F + Ut − U0 F )
subject to Uv ≥ 0, Ut ≥ 0; ||U0 (i, :)||0 = 1, i ∈ {1, 2, ..n}
U0 (i, j) ∈ {0, 1} j ∈ {1, 2, ..k}
(6)
where α controls how textual information contributes to
the model and || · ||0 is 0 , which counts the number of
nonzero entries in the vector. Uv ∈ Rn×k and Ut ∈ Rn×k
are the sentiment label spaces learned from visual information and textual information, respectively. The term of
2
2
β(Uv − U0 F + Ut − U0 F ) ensures that these two
types of information should share the sentiment label space
U0 . Vv ∈ Rmv ×k and Vt ∈ Rmt ×k indicate the sentiment
polarities of visual and textual features, respectively.
Textual information contains rich sentiment signals. First,
some words may contain sentiment polarities. For example,
some words are positive such as “happy” and “terriﬁc”;
while others are negative such as “gloomy” and “disap-

1587

where Dt is a diagonal matrix with jth element on the diag1
. In Eq. (6), solving Vt
onal D(j, j) = 2Vt (j,:)−V
t 0(j,:)2
directly is intractable. Since Dt and UTt Ut are symmetric
and positive deﬁnite, we employ eigen decomposition for
them as:
UTt Ut = U1 Λ1 UT1
(11)
Dt = U2 Λ2 UT2

pointed”. The sentiment polarities of words can be obtained via some public sentiment lexicons. For example,
the sentiment lexicon MPQA contains 7,504 human labeled
words which are commonly used in the daily life with
2,721 positive words and 4,783 negative words. Second,
some abbreviations and emoticons are strong sentiment
indicators. For example, ”lol”(means laughing out loud) is
a positive indicator while “:(” is a negative indicator. Let
Vt0 ∈ Rmv ×k be the matrix coding sentiment signals in
textual information where Vt0 (i, j) = 1 if i-th word belongs
to cj and Vt0 (i, j) = 0 otherwise. To model sentiment
signals, we force the learned sentiment polarities of textual
features to be consistent with those indicated by sentiment
signals. Furthermore, not all textual features in Ft contain
sentiment polarities and Vt should be sparse. We propose
the following formulation to achieve these two goals as:
min

Vt − Vt0 2,1

where U1 , U2 are eigen vectors and Λ1 , Λ2 are diagonal
matrices with eigen values on the diagonal. Substituting
UTt Ut and Dt in Eq. (6), we have:
Vt U1 Λ1 UT1 + δU2 Λ2 UT2 Vt = XtT Ut + δDt Vt0 (12)
Multiplying UT2 and U1 from left to right on both sides:
UT2 Vt U1 Λ1 + δΛ2 UT2 Vt U1 = UT2 (Xt Ut + δDt Vt0 )U1
(13)
t = UT Vt U1 and Q = UT (Xt Ut + δDt Vt0 )U1 ,
Let V
2
2
t Λ1 + δΛ2 V
t = Q, then we can obtain
Eq. (9) becomes V

the Vt and Vt as:

(7)

X2,1 is the 2,1 of the matrix X, which ensures the row
sparsity of X [?].
The signiﬁcance of textual information in unsupervised
sentiment analysis for social media images is two-fold.
First, textual information bridges the semantic gap between
visual features and sentiment labels. Second, we are allowed
to do sentiment analysis for social media images in an
unsupervised scenarios by modeling textual information via
Eqs. (6) and (7).
By combining the above discussion, we can have the
following initial framework, which provides a potential
solution to inferring sentiments by jointly considering visual
information and corresponding contextual information:

2
2

min Xv − Uv VvT F + α Xt − Ut VtT F
UV

2

t (s, l) = Q(s, l)
V
δλs2 + λl1
t UT
V t = U2 V
1

λs2

where
is the s-th eigen value of Dt and λl1 is l-th eigen
value of UTt Ut .
Update Vv . If U0 , Ut , Vt and Uv are ﬁxed, by setting
the derivation of the objective function to zero, Vv can be
easily obtained as Vv = XTv Uv (UTv Uv )−1 . Moreover, we
can easily verify updating Vv will monotonically decrease
the objective function.
Update Uv : If Vv , Ut , Vt and U0 are ﬁxed, Uv can be
obtained by the following optimization problem:
2

2
minJ (Uv ) = Xv − Uv VvT F + β Uv − U0 F
Uv
(15)
s.t.
Uv ≥ 0

2

+ β(Uv − U0 F + Ut − U0 F )
+ γVt − Vt0 2,1
s.t. Uv ≥ 0, Ut ≥ 0,

(8)

The Lagrangian function of Eq. (11) is :

2
2
minL(Uv ) = Xv − Uv VvT F + β Uv − U0 F

UT0 U0 = I; U0 ≥ 0
There are 5 components, i.e. Uv , Vv , Ut , Vt and U0 , in
Eq. (4). Thus it is difﬁcult to optimize all the components
simultaneously. In the following parts, we demonstrate an
alternating algorithm to optimize the objective function by
updating each component iteratively.
Update Vt : If U0 , Uv , Vv and Ut are ﬁxed, then
the objective function is decoupled and the constrains are
independent of Vt . Thus we can optimize Vt separately
and ignore the term without Vt , leading to the following:

2
2
minJ (Vt ) = Xt − Ut VtT F + δ Vt − Vt0 F
(9)

Uv

− T r(ΓUv )

(16)

where Γ is Lagrangian multiplier. Taking the deviation of
J (Uv ) and using the KKT condition (Γ(s, l)Uv (s, l) = 0),
we can obtain:
(−Xv Vv + Uv VvT Vv + βUv − βU0 )sl (Uv )sl = 0 (17)
which leads to the following update rule for Uv :

((Xv Vv )+ + Uv (VvT Vv )− + βU0 )sl
(Uv )sl ← (Uv )sl
((Xv Vv )− + Uv (VvT Vv )+ + βUv )sl
(18)
where (X(s, l))+ = (|X(s, l)| + X(s, l))/2, (X(s, l))− =
(|X(s, l)| − X(s, l))/2 and X = X+ − X− .

Vv

where δ = αγ . Taking the derivation of J (Vt ) and setting
it to zero, we can obtain the following form:
(−XTt Ut + Vt UTt Ut ) + δDt (Vt − Vt0 ) = 0

(14)

(10)

1588

Update Ut : It is worth noting that the procedure of
solving Ut is exactly the same as that of Uv . Thus, we
omit the solution of Ut here.
Update U0 : With Uv , Ut , Vt and Vv ﬁxed, the sentiment label U0 can be obtained by solving the following
optimization problem:
minJ (U0 ) = Uv − U0 2F + Ut − U0 2F
U

s.t.

UT0 U0 = I; U0 ≥ 0

(19)

The Lagrangian function of Eq. (17) is:
minJ (U0 ) = Uv − U0 2F + Ut − U0 2F
U

+ T r(Λ(UT0 U0 − I)) − T r(ΓU0 )

(20)

where Λ and Γ are Lagrangian multipliers. Taking the
derivation of J (U0 ) and using KKT conditions we can
obtain
(U0 − Uv + U0 − Ut + U0 Λ)sl (U0 )sl = 0

(21)

which leads the following update rule for U0 :

(U0 )sl ← (U0 )sl

(Uv + Ut + (U0 Λ)− )sl
((U0 Λ)+ + 2U0 )sl

(22)

Note that updating U0 needs updating the Lagrangian
multiplier Λ as well. To obtain Λ, we sum over s and get
Λ(s, s) = (UT0 Uv − I + UT0 Ut − I)s,s . The offdiagonal
elements of Λ are approximately obtained from non-negative
value of U0 , leading to Λ(s, t) = (UT0 Uv −I +UT0 Ut −I)st .
Overall, we can obtain Λ by combining the diagonal values
and off-diagonal values.
Algorithm 2 The proposed USEA
Input: {Xv , Xt , Vt0 } α, β, γ
Output: k sentiment label for each data instance.
Initialization: Ut , Uv , Vv , Vt
while Not Converge do
Update Vt using Eq.(14) and compute Vv =
XTv Uv (UTv Uv )−1 .
Computing (Xv Vv )+,− , (Xt Vt )+,− , (VvT Vv )+,−
and (VtT Vt )+,−
Update Uv using Eq. (18), similarly update Ut
Computing Λ
Update U0
End
Using max-pooling for U0 to predict sentiment labels.

IV. P RELIMINARY R ESULTS
We now quantitatively and qualitatively compare the
proposed model on image sentiment prediction with other
candidate methods. We also evaluate the robustness of the
proposed model with respect to various training samples
and different combinations of prior knowledge. Finally, we
perform a deeper analysis of our results.
A. Experiment Settings
We perform the evaluation on two large scale image
datasets collected from Flickr and Instagram respectively.
The collection of Flickr dataset is based on the image IDs
provided by [8], which contains 3,504,192 images from
4,807 users. Because some images are unavailable now,
and without loss of generality, we limit the number of
images from each user. Thus, we get 120,221 images from
3921 users. For the collection of the Instagram dataset, we
randomly pick 10 users as seed nodes and collect images by
traversing the social network based on breadth ﬁrst search.
The total number of images from Instagram is 130,230 from
3,451 users.
Establishing Ground Truth: For training and evaluating
the proposed method, we need to know the sentiment labels.
Thus, 20,000 Flickr images are labeled by three human subjects, the majority voting is employed. However, manually
acquiring the labels for these two large scale datasets is
expensive and time consuming. Consequently, the rest of
more than 230,000 images are labeled by the tags, which was
suggested by the previous works [8], [23]3 . Since labeling
the images based on the tags may cause noise issue, and for
better reliability we only label the images with primary sentiment labels, which include: positive, neutral and negative.
It is worth noting that the human labeled images have both
primary sentiment labels and ﬁne grained sentiment labels.
The ﬁne grained labels, including: happiness, amusement,
anger, fear, sad and disgust, are used to for ﬁne grained
sentiment prediction.
Feature extraction: the proposed method has the ability
to incorporate visual and textual information. For visual
information, we follow the recent approaches [7], [10] by
using mid-level visual features. The visual features are extracted by a large-scale visual attribute detectors [7] and the
feature dimension is 1200. Text-based features are formed by
the term frequency in user proﬁles, image captions and comments. It is worth noting that textual features, which contain
user descriptions, friends’ comments and image captions, are
preprocessed by stop word removing and stemming
Since RSAI is supervised approach, the comparison methods for include: Senti API4 , SentiBank [7], EL [8] and the
baseline method.
3 More

details can be found in [8] and [23]
text based sentiment prediction API

4 http://sentistrength.wlv.ac.uk/,a

1589

Senti API is a text based sentiment prediction API, it
measures the text sentiment by counting the sentiment
strength for each text term.
• SentiBank is a state-of-the-art visual based sentiment
prediction method. The method extracts a large number
of visual attributes and associates them with a sentiment
score. Similar to Senti API, the sentiment prediction is
based on the sentiment of each visual attributes.
• EL is a graphical model based approach, it infers the
sentiment based on the friend interactions and several
low level visual features.
• Baseline: The baseline method comes from our basic
model. To compare it fairly, we also introduce R0 with
the basic model which makes the baseline method have
the ability to learn from training data.
The results of comparison are shown in Table 2. We
employ 30% data for training and remaining for testing.
To verify the reliability of tags labeled images, we also included 20000 labeled Flickr images with primary sentiment
label. Especially, the classiﬁer setting for SentiBank and EL
followed the original papers. The classiﬁer of Sentibank is
logistic regression and for EL it is SVM. From the results
we can see that, the proposed method performs best in both
datasets. Noting that proposed method improved 10% and
6% over state-of-the-art methods [7]. Noting that the number
we reported in Table 2 is the precision accuracy for each
method.
From the table, we can see that, even though noise exists
in the Flickr and Instagram dataset, the results are similar
to the performance on human labeled dataset. Another
interesting observation is that the performance of EL on
Instagram is worse than on Flickr, one reason could be
that the wide usage of ”picture ﬁlters” lowers discriminative
ability of the low level visual features, while the models
based on the mid level attributes can easily avoid this ﬁlter
ambiguity.
For unsupervised approach USEA, we compared with the
following sentiment analysis algorithms:
5
• Senti API: . This API is natural language processing
API that performs unsupervised sentiment prediction
using word-based sentiment. The method only uses
textual information.
• USEA-T: A variant of the proposed method that only
considers the textual information including user proﬁles, image captions and friends’ comments.
• Random: It predicts sentiment labels of images by
randomly guessing.
Moreover, noting that SentiBank [7] and EL [8] are
originally proposed for supervised sentiment analysis. We
extend them to unsupervised scenarios by replacing original
classiﬁers such as SVM or logistic regression with Kmeans. However, the clusters identiﬁed by K-means have

no sentiment labels and we determine their sentiment labels
with the Euclidean distance to the ground truth. We use
SentiBank-K, and EL-K to represent these modiﬁcations.
Table 3 lists the comparison results and we make several
key observations:
• Most of the time, textual based approaches obtain
slight better performance than Random. These results
support - (1) textual information is often incomplete and
noisy and thus often inadequate to support independent
sentiment analysis; and (2) textual information contains
important cues for sentiment analysis.
• The proposed framework often obtains better performance than baseline methods. There are two major
reasons. First textual information provides semantic
meanings and sentiment signals for images. Second we
combine visual and textual information for sentiment
analysis. The impact of textual information on the
proposed framework will be discussed in the following
subsection.

•

V. F UTURE W ORK
In the future, we will employ crowdsourcing tools, such as
AmazonTurk6 , to obtain high-quality, manually-labeled data
to test the proposed method. Furthermore, inspired by the
recent development of advanced deep learning algorithms
and their success in image classiﬁcation and detection tasks,
we will follow this research direction to perform the sentiment analysis via deep learning. In order to have a robust
trained architecture and network parameters, we will focus
on the deep learning models that work for smaller dataset.
Moreover, beyond sentiment analysis, we will study social
event and social response [24], [25] via visual data in the
social media.
R EFERENCES
[1] M. De Choudhury, S. Counts, and M. Gamon, “Not all moods
are created equal! exploring human emotional states in social
media,” in Sixth International AAAI Conference on Weblogs
and Social Media, 2012.
[2] J. Machajdik and A. Hanbury, “Affective image classiﬁcation
using features inspired by psychology and art theory,” in
Proceedings of the international conference on Multimedia.
ACM, 2010, pp. 83–92.
[3] B. Pang, L. Lee, and S. Vaithyanathan, “Thumbs up?: sentiment classiﬁcation using machine learning techniques,” in
Proceedings of the ACL-02 conference on Empirical methods
in natural language processing-Volume 10. Association for
Computational Linguistics, 2002, pp. 79–86.
[4] B. Liu, “Sentiment analysis and opinion mining,” Synthesis
Lectures on Human Language Technologies, vol. 5, no. 1, pp.
1–167, 2012.

5 http://sentistrength.wlv.ac.uk/

6 https://www.mturk.com/mturk/welcome

1590

Table II: The comparison results of different supervised methods for sentiment analysis.
20000 Flickr
Flickr
Instagram

Senti API
0.32
0.34
0.27

SentiBank
0.42
0.47
0.56

EL
0.47
0.45
0.37

Baseline
0.48
0.48
0.54

RSAI
0.52
0.57
0.62

Table III: The comparison results of different unsupervised methods for sentiment analysis.
Method
Senti API
SentiBank-K
EL-K
USEA-T
USEA
Random

Flickr (#20,000)
32.30%
41.32%
36.39%
37.90%
55.22%
32.81%

Flickr (#140,221)
34.15%
41.12%
42.90%
40.22%
56.18%
33.12%

[5] M. Hu and B. Liu, “Mining and summarizing customer
reviews,” in Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining.
ACM, 2004, pp. 168–177.
[6] B. Pang and L. Lee, “A sentimental education: Sentiment
analysis using subjectivity summarization based on minimum
cuts,” in Proceedings of the 42nd annual meeting on Association for Computational Linguistics.
Association for
Computational Linguistics, 2004, p. 271.
[7] D. Borth, R. Ji, T. Chen, T. Breuel, and S.-F. Chang, “Largescale visual sentiment ontology and detectors using adjective
noun pairs,” in Proceedings of the 21st ACM international
conference on Multimedia. ACM, 2013, pp. 223–232.
[8] Y. Yang, J. Jia, S. Zhang, B. Wu, J. Li, and J. Tang, “How do
your friends on social media disclose your emotions?” 2014.
[9] J. Jia, S. Wu, X. Wang, P. Hu, L. Cai, and J. Tang, “Can we
understand van gogh’s mood?: learning to infer affects from
images in social networks,” in Proceedings of the 20th ACM
international conference on Multimedia. ACM, 2012, pp.
857–860.
[10] J. Yuan, S. Mcdonough, Q. You, and J. Luo, “Sentribute:
image sentiment analysis from a mid-level perspective,” in
Proceedings of the Second International Workshop on Issues
of Sentiment Discovery and Opinion Mining. ACM, 2013,
p. 10.

Instagram (#131,224)
37.80%
46.31%
43.21%
36.41%
59.94%
33.05%

[15] D. D. Lee and H. S. Seung, “Algorithms for non-negative
matrix factorization,” in Advances in neural information processing systems, 2001, pp. 556–562.
[16] C. H. Ding, X. He, and H. D. Simon, “On the equivalence
of nonnegative matrix factorization and spectral clustering.”
in SDM, vol. 5. SIAM, 2005, pp. 606–610.
[17] C. Ding, T. Li, W. Peng, and H. Park, “Orthogonal nonnegative matrix t-factorizations for clustering,” in Proceedings of
the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2006, pp. 126–135.
[18] Y. Wang, Y. Hu, S. Kambhampati, and B. Li, “Inferring
sentiment from web images with joint inference on visual
and social cues: A regulated matrix factorization approach.”
in ICWSM, 2015, p. 21.
[19] B. Han and T. Baldwin, “Lexical normalisation of short
text messages: Makn sens a# twitter,” in Proceedings of the
49th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume 1. Association for Computational Linguistics, 2011, pp. 368–378.
[20] C. Darwin, The expression of the emotions in man and
animals. Oxford University Press, 1998.
[21] R. Plutchik, Emotion: A psychoevolutionary synthesis.
Harper & Row New York, 1980.

[11] Q. You, J. Luo, H. Jin, and J. Yang, “Robust image sentiment
analysis using progressively trained and domain transferred
deep networks,” 2015.

[22] Y. Wang, S. Wang, J. Tang, H. Liu, and B. Li, “Unsupervised
sentiment analysis for social media images,” in International
Joint Conference on Artiﬁcial Intelligence, 2015.

[12] Y. Wang, Y. Jia, C. Hu, and M. Turk, “Non-negative matrix
factorization framework for face recognition,” International
Journal of Pattern Recognition and Artiﬁcial Intelligence,
vol. 19, no. 04, pp. 495–511, 2005.

[23] A. Go, R. Bhayani, and L. Huang, “Twitter sentiment classiﬁcation using distant supervision.”

[13] D. D. Lee and H. S. Seung, “Learning the parts of objects
by non-negative matrix factorization,” Nature, vol. 401, no.
6755, pp. 788–791, 1999.
[14] M. Das Gupta and J. Xiao, “Non-negative matrix factorization
as a feature selection tool for maximum margin classiﬁers,”
in Computer Vision and Pattern Recognition (CVPR), 2011
IEEE Conference on. IEEE, 2011, pp. 2841–2848.

[24] Y. Hu, A. John, F. Wang, and S. Kambhampati, “Et-lda: Joint
topic modeling for aligning events and their twitter feedback.”
in Proceedings of the 6th AAAI Conference, 2012.
[25] Y. Hu, L. Manikonda, and S. Kambhampati, “What we
instagram: A ﬁrst analysis of instagram photo content and
user types,” 2014.

1591

Proceedings of 2010 IEEE 17th International Conference on Image Processing

September 26-29, 2010, Hong Kong

TENSOR COMPLETION FOR ON-BOARD COMPRESSION OF HYPERSPECTRAL IMAGES
Nan Li and Baoxin Li
Computer Science and Engineering
Arizona State University, Tempe, AZ
Email:{nanli2,baoxin.li}asu.edu
ABSTRACT
We present a new image compression scheme for hyperspectral images based on the newly-emerged matrix/tensor
completion theory. Unlike typical transform-coding based
methods, the proposed approach does not require any transform to be performed by the imaging sensor when doing
on-board compression. Only a small set of pixels on a sparse
set of locations on the imaging sensor needs to be captured
and transmitted for each image. The decoder side relies
on matrix/tensor completion for reconstructing the original
images. Hence the scheme can drastically reduce the computation and bandwidth requirements on the on-board imaging
sensors. Experiments show that the proposed method is able
to obtain compression performance close to JPEG2000 while
enjoying the afore-mentioned unique beneﬁts.
Index Terms— Hyperspectral image compression, matrix completion, tensor completion.
1. INTRODUCTION
With the continuously-improving resolution and precision
of satellite imaging sensors, the size of the captured ground
hyperspectral images is increasing dramatically. For example, the AVIRIS instrument1 with 224 sensors can produce
approximately 140 megabytes (MB) of image data for every
512 scans as a “scene”. Thus compression is necessary for
both the storage and transmission of the hyperspectral images.
In general, lossless compression cannot provide an adequate
compression ratio, and thus lossy compression techniques
are needed. Many transform-coding techniques have been
used for hyperspectral image compression. For example,
3D wavelet-based techniques [1][2] have been shown to be
effective. In addition, Principal Component Analysis (PCA)
for spectral decorrelation in conjunction with JPEG2000
[3][4][5] has also been a popular technique. Note that, for
an on-board compression system, the transform-coding approaches require that the entire image (or all the images in
the cases utilizing spectral analysis) be captured from the
sensor and transmitted to the encoder for analysis and compression. This, plus the computational resources required by
1 http://aviris.jpl.nasa.gov/

978-1-4244-7994-8/10/$26.00 ©2010 IEEE

517

the encoder, can pose as practical challenges in terms of both
bandwidth and CPU cycles for the satellite platform, especially if all the processing steps need to be done in real-time.
In this paper, we present a new compression scheme
for hyperspectral images based on the newly-emerged matrix/tensor completion theory. Unlike typical transformcoding based methods, the proposed approach does not require any transform to be performed by the imaging sensor
when doing on-board compression. Instead, only a small
set of pixels from a sparse set of locations on the imaging
sensor needs to be captured and transmitted for each image.
The decoder, which is presumably on the ground and thus
has more computational resources, relies on matrix/tensor
completion for reconstructing the original images. Hence
the scheme can drastically reduce the computation and bandwidth requirements on the on-board imaging sensors. Based
on the analysis of the properties of hyperspectral images, we
propose a sampling strategy, which maximally exploits the
correlation among the neighboring bands in a hyperspectral
image set. Furthermore, an adaptive threshold selection technique is proposed for the reconstruction algorithm, which
ensures fast convergence of the algorithm by updating some
key algorithmic parameters based on the sampling rate and
the distribution of the singular values of the tensor. Experiments using a publically-available hyperspectral image data
set veriﬁed that the proposed method can obtain compression
performance close to JPEG2000 while enjoying the aforementioned unique beneﬁts.
In Sect. 2, we discuss the problem formulation and provide the theoretical background for matrix/tensor completion.
In Sect. 3, based on the properties of hyperspectral images,
two speciﬁc techniques are proposed in designing the algorithm. Experimental results are provided in Sect. 4. We conclude with a brief discussion on future work in Sect. 5.
2. PROBLEM FORMULATION AND THEORETICAL
BACKGROUND
2.1. Image Compression via Matrix Completion
As alluded in the previous section, the proposed compression scheme attempts to compress an image by keeping only a

ICIP 2010

small set of the the original pixel values and discarding all the
remaining. If the scheme is implemented on the imaging sensor, then the discarded pixels do not even need to be captured
and transmitted in the the ﬁrst place. Intuitively, for such a
scheme to work with any image content, the small set of chosen pixels need to be randomly distributed on the image. With
only a small set of pixels, the estimation or reconstruction of
the entire image can be viewed as a missing value estimation
problem. Traditional estimation approaches such as linear interpolation may be used to ﬁll in the missing pixels. However,
such approaches will not work if the compression ratio is high
and thus only a small percentage (e.g., less the 10 percent) of
the original pixels is kept.
Recently, Candes et al. [6][7][8] proposed the matrix
completion theory, which can recovery a low rank matrix
from only a few of its entries. If we view an image as a matrix, then it appears that we could use the matrix completion
approach to solve the missing pixel reconstruction problem
discussed above and then achieve compression by keeping
only a small number of pixels. However, a typical image,
when viewed as a matrix, is not really low-rank. Fortunately,
most image can be viewed as “practically low-rank”. For
example, in Fig. 1(right), we plot the singular values of the
Lena image (left) in descending order. It is obvious from the
plot that the singular values decay quickly and the top (e.g.
10%, corresponding to the middle image) of the singular values account for the most energy of the signal. In this sense,
we can say that a natural image is practically low-rank, and
hence the matrix completion technique may be used.
Unfortunately, this naive approach would not work in
practice. On one hand, for natural images, the small singular
values may still contribute to the details of the image and thus
even if we view the image as “practically low-rank”, the rank
cannot be too small (e.g., it may need to be at least 30% of the
image size). On the other hand, the sampling rate (i.e., how
many pixels to keep) for a matrix completion algorithm to
work is directly linked to the rank of the matrix. If the rank of
the matrix is about 30% of its height (assuming a square image for simplicity of discussion), then the required sampling
rate would be too high (e.g, keeping more than 50% of the
pixels), in which case the compression ratio is not practically
useful (and of course even a simple interpolate method would
sufﬁce if the the sampling rate is so high).
The situation is dramatically improved in the case of hyperspectral images: the set of images in general depicts strong
inter-image correlation. Hence if we view all the images as an
entity, the rank of this entity may be very small compared with
the dimension of the entity. This is the basic idea behind the
proposed method. To complete this idea, we need to design
efﬁcient reconstruction algorithm. In the below, we brieﬂy
review the theoretical background of matrix completion and
its extension to the tensor case so that a set of images can be
processed. The details in implementing a working algorithm
is then presented in the next section.

Fig. 1. Singular values (right) of the Lena image (left).
2.2. Matrix Completion
According to the matrix completion theory, we can recover
all elements of a low-rank matrix M with a high probability
from only a small number of its entries, by solving the convex
optimization problem,
min :

rank(X),

s.t.

XΩ = MΩ

(1)

where X is a low-rank matrix that is computed as an approximation to M , and entries of M in set Ω are known while the
remaining entries are missing.
As rank is not a convex function, (1) is an NP-hard problem. In practice, the nuclear norm is used in ﬁnding a solution, leading to the following problem ([7][9]):
min :

X∗ ,

s.t.

XΩ = MΩ .

(2)

Using Lagrangian multiplier, the above problem can be
converted to
1
argmin λX∗ + XΩ − MΩ 2F
(3)
2
X
where λ is a constant.
Let X = U ΣV T be the singular value decomposition of
X. Cai et al. [10] deﬁned the “shrinkage” operator,
Dτ (X) := U Dτ (Σ)V T , Dτ (Σ) = diag(σi − τ )+ ,
and proved that (4) gives a solution to (3) by iteratively using
the shrinkage operation,
1
Dτ (Y ) = argmin{ X − Y 2F + τ X∗ },
(4)
2
X
where σi are singular values and τ is the threshold.
2.3. Tensor Completion
The matrix completion idea can be extended to processing
a set of images, leading to the tensor completion problem,
which can be solved by extending (3) to,
1
argmin λX ∗ + XΩ − MΩ 2F ,
(5)
2
X
where X and M are n-mode tensors.
Deﬁne unf oldk (X ) := Xk as converting a tensor into a
matrix on k-th mode, and deﬁne f oldk (Xk ) := X for converting matrix to tensor. Then the Frobenius norm of a tensor
is the same as the Frobenius norm on each mode unfold. Using tensor nuclear norm deﬁnition from Liu et al. [11],
n
1
X ∗ =
Xi ∗ ,
(6)
n i=1

518

(5) can be converted to,
n
n
1 
λ
Xi ∗ +
Xi − Mi |2F
(7)
argmin
n i=1
2n i=1
X
From [11], it is not difﬁcult to see that the tensor completion
solution can be obtained by weighted-averaging the results
from each mode in each iteration of the shrinkage operation.
Xi = Dτ (Yi )

 n
(8)
βi f oldi (Xi )
i=1
YΩ =
n
β
i=1 i
Ω
3. PROPOSED TENSOR COMPLETION APPROACH
FOR HYPERSPECTRAL IMAGE COMPRESSION
From the tensor completion solution proposed above, we can
recover a tensor with high probability using only a small number of entries from randomly sampling the original tensor.
However, if we directly use this sampling scheme for hyperspectral image compression, the result cannot be optimal
since this does not take into consideration of the properties of
the hyperspectral images. For example, images from neighboring bands typically are highly correlated. Hence it is desired to keep distinct samples from neighboring images in order to maximize the information encapsulated in the small
set of samples. To this end, we propose a special sampling
scheme, which is elaborated in Sect. 3.1. Furthermore, some
critical parameters in the tensor completion algorithm need
to be controlled for convergence. We propose an adaptive
threshold selection strategy in Sect. 3.2.
3.1. Group Non-overlap Sampling
Hyperspectral images are collected as a data set in contiguous spectral channels (bands) with wavelengths in nanometers (nm). Therefore, there is strong correlation in the spectral dimension. To reveal the correlation patterns along the
spectral dimension among the hyperspectral images, we compute the pairwise correlation coefﬁcients between the images
of different bands. We ﬁrst convert each image matrix into a
vector and then compute the correlation coefﬁcient between
each pair. A sample result from the AVIRIS Moffett Field
sc1 set is showed in Fig.2, where the brightness of a pixel
is proportional to the values of the coefﬁcients, with 1 being
depicted in white.

Fig. 2. Pairwise correlation coefﬁcients of the AVIRIS Moffett Field sc1 data set.
From the above plot of correlation coefﬁcients, we can see
that the spectral bands can be naturally grouped into 4 subsets. The bands in each group are highly correlated with each

519

other. Based on the observation, we propose a so-called group
non-overlap sampling (GNS) scheme to sample the pixels in
a set of nearby bands so as to maximally retain the original
information. Consider a sampling rate R (e.g., 12.5%). We
ensure that the sampling locations are not overlapping in consecutive 1/R (e.g., 8) bands. Then, we should have sampled
all the pixel locations after 1/R bands (note that within each
band, the sampling locations are still random). As the number of bands in the subsets is not always integer multiple of
1/R, the remaining bands in a subset are combined with the
preceding (already sampled) bands to generate the 1/R-band
window as shown in Fig.3.

Fig. 3. Sampling the remaining bands at the end of a subset.
3.2. Adaptive Threshold Selection
Recall from Equations (3) and (4), the solution of matrix
completion relies on an important threshold τ . It was proposed in [8] that an adaptive threshold τ should be used for
fast convergence,
τk−1
(9)
, with τ0 = Lf
ρ
where ρ is the step size and Lf > 0 is a constant.
Since this adaptive decending threshold accelerates convergence while leads to a lower-ranked solution in matrix
completion, it is critical for us to achieve the same thing in
tensor completion. From (8), the tensor completion solution
can be viewed as weighted average of matrix completion in
each iteration. Therefore, we propose to extend (9) to obtain
the adaptive thresholds for the tensor completion algorithm,
τk =

τi,k−1
, i = 1...n, with τi,0 = Lf i
(10)
ρi
where ρi is the step size on each mode.
The step size controls convergence and the rank of the intermedia reconstruction, and thus it is directly related to the
singular values of the reconstruction. Therefore, it is desired
to make the value adaptive to the singular values. To gain
more insights on this, we plotted the singular value distribution for each mode of the tensor and found that the singular
values decease faster on the spectral dimension than on the
other two spatial dimensions. Thus we choose a higher ρ for
the spectral mode. At the same time, the thresholds should be
made adaptive to the sampling rate (α) as well, since the sampling rate controls the amount of information (the higher sampling rate, the more details are kept, or more singular values
should be kept, thus requiring a lower threshold). Combining all the above analysis, we propose the following adaptive
threshold selection scheme,
τi,k =

ρi = δi ∗ α, τi,k =

τi,k−1
, i = 1...n, with τi,0 = Lf i (11)
ρi

where ρi is the step size on each mode and the constant scalars
δi are so chosen such that δspectral > δspatial1 , δspatial2 .
4. EXPERIMENTAL RESULTS
We used the the commonly-used AVIRIS Jasper Ridge sc1
and Moffett Field sc1 sets in our experiments and compared
with 2D-JPEG2000 and 3D-JPEG2000, which were used in
most recent work in the literature. The reference results were
obtained from Kakadu software version 6.3.12 , in which 2DJPEG2000 uses Part-1 options for images and 3D-JPEG2000
multicomponent transform (MT) uses Part-2 options of the
JPEG2000 standard. The results we obtained are similar to
those in [1][3][5]. Sample results for two different bit rates
are summarized in Fig. 4, from which we can see that our approach is able to achieve the performance close to JPEG2000
in most bands especially those in the middle. This veriﬁes
that the proposed hyperspectral image compression scheme
is a feasible and effective approach. Therefore, when the
on-board processing capability and bandwidth are extremely
limited, our scheme can be a viable alternative to existing
transform-coding based techniques.

typical transform-coding based methods, our approach does
not require much computation on the satellite side and hence
the speed performance is in general not a serious concern (the
reconstruction is done on the ground). Our future work includes extending the group non-overlap sampling scheme to
general cases of R based on a better or qualitative analysis of
the correlation among the bands.
6. REFERENCES
[1] E. Christophe and W.A. Pearlman, “Three-dimensional
SPIHT coding of volume images with random access and
resolution scalability,” EURASIP Journal on Image and
Video Processing, vol. 2008, pp. 13, 2008.
[2] J. Wu, K. Jiang, Y. Fang, L. Jiao, and G. Shi, “Hyperspectral image compression using distributed source coding and 3D SPECK,” in Proceedings of SPIE, 2009, vol.
7494, p. 74940Z.
[3] W. Zhu, Q. Du, and J.E. Fowler, “Segmented PCA and
JPEG2000 for hyperspectral image compression,” in Proceedings of SPIE, 2009, vol. 7455, p. 74550I.
[4] G. Carvajal, B. Penna, and E. Magli, “Uniﬁed lossy and
near-lossless hyperspectral image compression based on
JPEG 2000,” IEEE Geoscience and Remote Sensing Letters, vol. 5, no. 4, pp. 593–597, 2008.
[5] J.T. Rucker, J.E. Fowler, and N.H. Younan, “JPEG2000
coding strategies for hyperspectral data,” in International Geoscience and Remote Sensing Symposium, 2005,
vol. 1, p. 128.

1.0bpppb, Jasper Ridge sc1

0.5bpppb, Jasper Ridge sc1

[6] E.J. Candes and T. Tao, “The power of convex relaxation:
Near-optimal matrix completion,” arXiv, vol. 903, 2009.
[7] Emmanuel Candes and Benjamin Recht, “Exact matrix
completion via convex optimization,” Foundations of
Computational Mathematics, vol. 9, no. 6, pp. 717–772,
Dec. 2009.
[8] A. Beck and M. Teboulle, “A fast iterative shrinkagethresholding algorithm for linear inverse problems,”
SIAM Journal on Imaging Sciences, vol. 2, no. 1, pp.
183–202, 2009.

1.0bpppb, Moffett Field sc1

0.5bpppb, Moffett Field sc1

5. CONCLUSION AND FUTURE WORK

[9] B. Recht, M. Fazel, and P.A. Parrilo, “Guaranteed
minimum-rank solutions of linear matrix equations via
nuclear norm minimization,” SIAM Review, preprint
available at http://arxiv.org/abs/0706.4138, 2007.

We presented a hyperspectral image compression scheme using matrix/tensor completion. In addition to introducing a
general idea, we also proposed speciﬁc techniques for implementing a compression algorithm, including the group nonoverlap sampling scheme and the adaptive threshold selection
scheme. We compared our method with the newest version
of JPEG2000 on a commonly-used AVIRIS data set. Unlike

[10] J.F. Cai, E.J. Candes, and Z. Shen,
“A singular value thresholding algorithm for matrix completion. Technical report,”
Preprint available at
http://arxiv.org/abs/0810.3286, 2008.
[11] J. Liu, P. Musialski, P. Wonka, and J. Ye, “Tensor completion for estimating missing values in visual data,” in
ICCV2009, 2009.

Fig. 4. SNR Comparison with JPEG2000 on each bands.

2 http://www.kakadusoftware.com/

520

Machine Vision and Applications (2011) 22:303–321
DOI 10.1007/s00138-009-0241-8

ORIGINAL PAPER

Rapid modeling of cones and cylinders from a single calibrated
image using minimum 2D control points
Jin Zhou · Baoxin Li

Received: 21 October 2008 / Revised: 26 November 2009 / Accepted: 19 December 2009 / Published online: 9 January 2010
© Springer-Verlag 2010

Abstract Three-dimensional modeling from a single image
is a very useful technique in applications, such as augment
reality, architecture, urban digitalization, heritage preservation, etc. While the modeling of lines and planes has been well
studied, a convenient way of modeling cones and cylinders,
which are commonly encountered objects, is not available
yet. Although fully automatic approaches exist, they cannot
guarantee the precision demanded by a real application, and
are not appropriate for modeling a complex environment,
where users need full control of the modeling process. In
addition, approaches relying on the detection of full object
contours cannot be directly transformed to simple interactive
modeling procedures. This paper presents novel approaches
to modeling cone and cylinder objects based on a set of minimum two-dimensional control points. The approaches enable
intuitive, fast and accurate modeling from a single image.
Both synthetic and real data experiments demonstrate the
advantages of the proposed approach: desired accuracy with
minimal user interactions.
Keywords Image based modeling · 3D reconstruction ·
Single view geometry

Electronic supplementary material The online version of this
article (doi:10.1007/s00138-009-0241-8) contains supplementary
material, which is available to authorized users.
J. Zhou · B. Li (B)
Department of Computer Science and Engineering,
Arizona State University, Tempe, AZ 85281, USA
e-mail: baoxin.li@asu.edu
J. Zhou
e-mail: jinzhou@asu.edu

1 Introduction
Three-dimensional (3D) reconstruction of objects from
images has been an active research topic in computer vision
for many years. Existing approaches can be classified into
two general categories: fully automatic approaches and semiautomatic/interactive approaches. While being theoretically
attractive, fully automatic approaches (e.g., [7,14,16]) often
rely on heavy assumptions about the objects/scenes to be
modeled and they still face a lot of fundamental challenges
in practical applications. For example, the lack of reliable
feature correspondences and dense depth estimation often
renders many algorithms inadequate for even a simple scene/
object.
In the meantime, interactive modeling from images has
seen significant progresses. Through integrating a user’s
inputs into the modeling algorithms, an interactive approach
has the potential of overcoming many challenges faced by a
fully automatic technique. One early such approach is Facade
[6] (which was later integrated into the commercial software
Canoma [2] and was a source of inspiration for ImageModeler [11]). Another system, ShapeCapture [20], adds more
automation to interactive modeling. The approach of Horry
et al. [10] fits a simple 3D mesh model to a photo, which
is fast but suffers from the lack of accuracy. Criminisi [5]
proposed methods for measuring the length of vertical or
horizontal lines from an single uncalibrated image. One of
the most recent systems is the “PhotoMatch” tool in the SketchUp software [22], a commercially available 3D modeling
tool by Google, which interactively constructs a model from
an image, using built-in 3D modeling operations such as twodimensional (2D) sketch plus push/pull. However, most of the
systems only deal with lines, planes, cubes, prisms or cues
on certain planes. While cone-like and cylinder-like structures are abundant in man-made environments (e.g., cups,

123

304

cylindrical pillars/columns, pipes, etc.), current state-of-art
image-based modeling software either does not provide a tool
for modeling cones and cylinders, or requires sophisticated
procedures or strong prior knowledge for the modeling. In
this paper, we presents a novel modeling approach for cones
and cylinders based on a minimal set of 2D image control
points on the object. The resultant techniques enable easy,
fast and accurate modeling from a single image. We have
implemented the techniques as a SketchUp plug-in based
on the algorithms described in this paper, demonstrating the
potential of the approach for being used in practical software
for supporting easy and rapid cone and cylinder modeling
from a single image.
Cones and cylinders are special kinds of surface of revolution (SOR) objects and SOR reconstruction from images
has been developed in the past [1,4,23–26]. The SOR reconstruction tool in Facade [1] assumes that full 3D information
of the axis of the SOR object is already known. Utcke and
Zisserman [23] proposed an algorithm which can only do projective reconstruction of SOR objects. In [24–26], an algorithm purely based on the silhouettes was proposed, with the
result having two ambiguities. The approaches in [4,28] can
metric-reconstruct SOR objects from uncalibrated images.
However, both of them rely on two cross section contours to
estimate the camera’s internal matrix orientation. In reality, it
may not be possible to observe two cross section contours for
a cone object. In addition, specifying the two cross section
contours is not trivial. The approach proposed in this paper
requires only one cross section contour and two edge lines,
which are easier to obtain. Furthermore, interactive procedures typically require a user to specify the control points
or lines. The methods based on two cross sections require a
user to specify at least 10 points (since a general ellipse has
5 degrees of freedom). By exploiting the symmetry of the
silhouette of the SOR objects, our approach requires only at
most five control points, making the modeling process much
faster and easier.
Other related work includes free-form surfaces modeling. The system by Zhang et al. [30] allows users to interactively draw a 3D mesh out of a 2D image. The method
by Prasad et al. [17] incorporates user-specified data such
as surface normals and object contours into their optimization-based modeling procedure. Hoiem et al. [9] and Saxena
et al. [19] proposed a fully automatic approach to recovering a 3D mesh from the image based on learning and Markov
random fields. ShapeCapture [20] models free-form surfaces
based on multiple images and requires a user to specify point
correspondences, which is not convenient. These methods
focus on general free-form surfaces and are not appropriate
for cones and cylinders modeling.
The core feature of our approach is that a user only needs
to click and move several 2D control points on the image
to model the object. Based on the pose information (rela-

123

J. Zhou, B. Li

tive pose between the camera and the object), we consider
two different scenarios. In the first case, we assume that the
objects stand on the ground and that the camera orientation
with respect to the ground plane is already determined. The
second case does not have these constraints. While the first
case can be viewed as a special case of the second, it requires
fewer points and thus is much easier to handle from a user’s
perspective. The practical motivation of this classification is
that many objects of interest in the real world stand on the
ground and that the camera orientation can be easily calibrated based on two sets of parallel lines, which are also
common in man-made environments (see Sect. 2 for orientation calibration).
Table 1 summarizes the required control points for different scenarios. While the degree-of-freedom of the problems
matches intuitively the number of control points times 2 (for
x and y) in our approach, this does not imply that a straightforward reconstruction algorithm will entail with any control points (of enough number) since all the control points a
user can click are only 2D image points. Consequently, one
of the contributions of the work is the reconstruction algorithm using only 2D image points. For example, in the case
of a cone standing on the ground, the “edge point” p2 (see
Table 1) we ask a user to click generally does not have the
same depth as the vertex does. Imagine that the cone is very
close to the camera. Then only a small portion of the cone is
visible in the image domain. That is, p2 is only an “apparent”
edge point in the image but not the true outmost point on the
physical contour of the bottom circle of the cone. Therefore,
clicking on the vertex p1 and the “edge point” p2 does not
immediately give us the 3D cone.
The rest of the paper is organized as follows. Section 2
describes techniques for camera calibration. Section 3
presents the modeling algorithms for each type of the objects.
Synthetic and real data experiments are presented in Sect. 4
and the conclusions are drawn in Sect. 5.

2 Camera calibration
Image-based modeling systems usually start with camera calibration, which has been thoroughly discussed in [8]. In our
system, we require that the camera internal parameters be
determined before modeling the objects of general poses. For
objects standing on the ground, we require both the internal
parameters and the orientation of the camera are determined
before the modeling process starts. Since the camera internal
parameters are independent of the scene, we can calibrate
the camera beforehand, by either using calibration objects
or the scene information (e.g., vanishing points). We may
also simply use the EXIF tags in the image itself. Once the
camera is calibrated, we can use the information to model
the objects of general poses from any image captured by the

Rapid modeling of cones and cylinders from a single calibrated image

305

Table 1 Classification of objects and the control points to model them

Cylinders

Cones

Cone Sections

2

2

4

4

4

5

Standing On
the Ground

# of control
points

General
poses

# of control
points

camera, as long as it does not change its view angle. To model
objects standing on the ground with fewer control points,
knowing only the internal parameters is not enough and thus
we need to estimate the camera orientation relative to the
ground plane based on the scene information in the image.
Robertsone and Cipolla [18] gives an approach to calibrating
the camera and estimating the camera orientation based on
two orthogonal vanishing points on horizontal planes. This
scheme is also used in PhotoMatch [22]. In this section, we
give a brief introduction to this technique. Note that other
calibration techniques also exist.
For a single image, two orthogonal vanishing points give
one constraint on the camera parameters:

−1
v2 = 0
v1T K K T

Fig. 1 An example of camera calibration from two sets of parallel lines
at orthogonal directions

(1)

where v1 and v2 are two orthogonal vanishing points and K
is the camera’s internal matrix. Usually, we can assume that
the principal point is at the image center, there is no skew
and the pixels are squares. Thus, the only unknown is the
focal length, which can be determined from the above constraint. (Note that this method will fail when one vanishing
point approaches infinity.) In practice, such two vanishing
points can be determined from two sets of parallel lines at
orthogonal directions, as in Fig. 1.
In real 3D environments, the objects such as buildings
typically stand on the ground plane. Therefore, we define the
world coordinate system such that its y-axis is perpendicular to the ground plane (as in Fig. 2, x–y–z; x–y–z is the

Fig. 2 The world coordinate system and the camera orientation R

camera coordinate system). Under this coordinate system, the
objects standing on the ground plane can have a very simple representation that supports the development of a simple
reconstruction method. This coordinate system definition is

123

306

similar to that in SketchUp, where the z-axis is perpendicular
to the ground.
To estimate the camera’s orientation under the world coordinate system, we can further use the vanishing points. Suppose that the vanishing point along the x direction is v1 , then
we have
KR(1, 0, 0)T = v1
Since K is known, we can move it to the right-hand side of
the equation and get
R(1, 0, 0)T = K −1 v1 → r1 = K −1 v1 /||K −1 v1 ||
where r1 is the first column vector of R, which can be written
as
R = [r1 , r2 , r3 ]
Given another orthogonal vanishing point, say v2 , another
column vector of R can be determined. Since R is an orthogonal matrix, i.e., the column vectors are perpendicular to
each other, the last column vector can be determined from
two known column vectors.
Recently, a few circle-based calibration algorithms have
been proposed [3,13,27,29]. They usually use two or more
parallel circles for calibration and orientation estimation.
This technique can be useful for camera calibration when
the parallel lines are not present in the scene, for instance,
when only an SOR object is in the scene and its cross section contours are visible. The disadvantage is that circles are
harder to specify than lines.
Recovering the camera orientation can be useful for modeling objects standing on the ground. For instance, for cones
and cylinders standing on the ground, only two points are
sufficient for modeling. However, for objects of other general poses, the camera orientation is not important. When
there are no orthogonal vanishing points on the horizontal
plane, we can simply set R to be an identity matrix and use
the modeling algorithms for objects of general poses.

J. Zhou, B. Li

we have
x  KR[I |0] X̃ → λx = K R X → X = λM −1 x

(2)

where M = K R is usually called “infinite homography”,
x  ( px , p y , 1)T is the homogeneous coordinates of the
image point p and X̃  (x, y, z, 1)T is the homogeneous
coordinates of the 3D point X . This equation shows that the
3D position of an image point has only one degree of freedom (DoF), namely λ. Thus, with one constraint added to
this point, its 3D position can be determined.
3.2 Normalization of image points and the normalized view
Equation (2) suggests that there is a linear relationship
between the 3D point X and its image point x. We can further simplify the relationship by normalizing the image point
with M −1 ,
x̂ = M −1 x = (KR)−1 x
where x̂ is called the “normalized point”. With this, (17)
becomes
X = λx̂
It will become clear that the normalized points are useful for
the reconstruction of the objects. We call the image transformed with M −1 the “normalized view”. From now on, we
will operate on the normalized view and thus all points are
normalized points.
3.3 Rotations
In the normalized view, if the camera is rotated by Rt , then
a point x̂ in the new view and its corresponding point x̂ in
original view is related by
x̂  = Rt x̂

(3)

Since
1 
1
1
X = Rt X = Rt λx̂ = Rt x̂
λ
λ
λ

3 Modeling based on 2D control points

x̂  =

This section presents the algorithms for cone and cylinder
modeling based on several 2D control points. We first introduce some basic tools and notations in Sects. 3.1–3.3, and
the present the proposed modeling approaches in Sects. 3.4
and 3.5. The degeneracy cases are discussed in Sect. 3.6.

Equation (3) suggests a way for estimating the pose of an
object or the camera orientation based on the correspondence
of normalized image points, which will later be used in our
modeling process.
A rotation may be decomposed into three basic rotations,
around the x-, the y- and the z-axis, respectively. The rotation
around the x-axis is
⎡
⎤
1
0
0
⎢
⎥
(4)
Rx (θ ) = ⎣ 0 cos(θ ) − sin(θ ) ⎦

3.1 Point reconstruction
From the previous section, we can obtain K and R. Suppose
that the camera center is the origin of the world coordinate
system. For a 3D point and its corresponding image point,

123

0

sin(θ )

cos(θ )

Rapid modeling of cones and cylinders from a single calibrated image

The rotation around the y-axis is
⎡
⎤
cos(θ ) 0 sin(θ )
⎢
⎥
R y (θ ) = ⎣ 0
1
0 ⎦
− sin(θ )

cos(θ )

0

And the rotation around z-axis is
⎡
⎤
cos(θ ) − sin(θ ) 0
⎢
⎥
Rz (θ ) = ⎣ sin(θ )
cos(θ ) 0 ⎦
0

(5)

0

the “standard coordinate system” of the cone. When the camera coordinate system is the same as the standard coordinate
system of the cone, we call the captured image a “standard
view” of the cone. Similar to the cone model, a cone section
model is represented as
M0 = (c = (0, yc , z 0 )T , r, yv , yt )

(6)

1

where yt is the height of the top.
In general, a cone can be represented by
M = (R, d, M0 )

The proposed modeling algorithms estimate the orientation
by sequentially rotating the camera along different axes.
3.4 Cone modeling
We define two types of cones: (1) the cones standing on the
ground (i.e., the cones whose axis is perpendicular to the
ground plane); and (2) general cones whose axis is unknown.
We will show that for the first case, two control points are
sufficient for reconstructing the cone, and that for the second case, four points are needed (as illustrated in Fig. 3b, c,
respectively).
3.4.1 The cone model
We first define a coordinate system for a cone such that its
representation is simple. Figure 4 illustrates the cone coordinate system (O–X –Y –Z ) and the camera coordinate system
(O–x–y–z). Note that we have intentionally set the origins
of these two coordinate systems to be the same. In this way
we can rotate the camera such that its coordinate system can
match the cone coordinate system. The cone model can be
represented as
M0 = (c = (0, yc , z 0 )T , r, yv )

307

where R is the rotation matrix of the cone coordinate system under the world coordinate system, and d is the depth of
the cone axis in the standard coordinate system of the cone.
Figure 5 shows a general cone model under the world coordinate system.
With the model defined, the problem becomes how to compute the model parameters based on several 2D image points
as shown in Fig. 3, i.e.,
M = f ( p1 , . . . , pn )
For the cones standing on the ground, n = 2, with p1 being
the cone vertex and p2 the bottom point on one edge. For a
general cone, n = 4: The first two points are same as previous case, p3 is a point on the bottom circle, and p4 is a point
on another edge (except the vertex). We first consider the
reconstruction from the standard view (see Fig. 3a). Reconstruction of cones standing on the ground and general cones
can be transformed to this case by rotating the camera.

(7)

where c is the center of a cross section circle with radius
r , and yv the height of the vertex of the cone. Without loss
of generality, we can define z 0 = 1, which means that the
depth of the cone axis is 1. We call this coordinate system
Fig. 4 Coordinate system of the camera (O, x, y, z) and the cone
(O, X, Y, Z )

Fig. 3 Cone reconstruction from different views. a The standard view;
b A view where the cone stands on the ground; and c A view where the
cone is of a general pose

Fig. 5 The world coordinate system and a general cone

123

308

J. Zhou, B. Li

3.4.2 Reconstruction of cones from the standard view
From the standard view, two points in Fig. 3a are sufficient
for 3D reconstruction of a cone. Basically, we need to determine the three parameters in (7) from the vertex point p1
and another edge point p2 . Since we assume that the depth
of the cone axis is 1, the 3D position of the vertex can be
obtained from its image point p1 based on the constraint that
it is on the cone axis. Equation (8) illustrates the process for
the estimation of the vertex height yv .
V = λ p̂1 = λ(0, y1 , z 1 )T
λz 1 = 1

⇒

Fig. 6 Use two points to reconstruct a cone in the standard view

λ = 1/z 1

(8)

yv = λy1 = y1 /z 1

ĉ2 = λ(0, y0 − (b/a)x0 , 1)T

where V are the 3D coordinates of the vertex. Note that we
have used the normalized image points.
Now the problem boils down to determining the remaining parameters yc and r . Given one edge line of a cone in the
standard view, we can determine any cross section touching
the line. We use the property of SOR from [13]: the apparent
contour is tangent to an imaged cross section at any point of
contact. Under projective transformation, when projected to
a horizontal plane, the apparent contour is still tangent to the
cross section, which is now a circle. Figure 6 illustrates this
property.
We can transform the standard view to the horizontal plane
by rotating the camera with a right angle:
⎡
⎤
1 0 0
⎢
⎥
(9)
Hg = Rx (−π/2) = ⎣ 0 0 1 ⎦
0

−1

0

On the horizontal plane, the cross section becomes a circle
tangent to the projected edge line (from the above property).
Combined with another constraint that the circle center is
on the y-axis, this allows us to determine the circle using
any point of contact, which is provided by a user, say p2 .
Specifically, we can compute the cross section passing p2 as
follows:
1. Transform the normalized view to the horizontal plane
with Hg
p̂2 = Hg p̂2  (x0 , y0 , 1)T

p̂1 = Hg p̂1
lˆ = p̂1 × p̂2 = (a, b, c)T

(10)

where lˆ is the line passing the transformed point p̂1
and p̂2 .
2. From Fig. 6, we can compute the circle in the horizontal
plane which is tangent to lˆ at p̂2 :

123

→ Pcenter = Hg−1 ĉ2 = λ(0, −1, y0 − (b/a)x0 )T





r = |λ| 
 p̂2 − ĉ2 
 = |λ| x02 + (bx0 /a)2

(11)

From the definition that the depth of the cone axis is 1, we
have
λ(y0 − (b/a)x0 ) = 1

⇒

λ = 1/(y0 − (b/a)x0 )

(12)

Thus,
yc = λ × −1 = −1/(y0 − (b/a)x0 )


x02 + (bx0 /a)2
1 + (b/a)2
=
r=
|y0 − (b/a)x0 |
|y0 /x0 − (b/a)|

(13)

The reconstructed model is subject to a scale factor, since we
assumed that the cone axis is at depth 1. This freedom can
be removed if we know the 3D position of any point on the
cone.
3.4.3 Reconstruction of cones standing on the ground
In general, the cone is not in the “standard view” and we
need to determine the object pose R, such that by rotating
the camera with R −1 , we may transform the normalized view
to the standard view of any cone in the image. For the cones
standing on the ground, we can simply rotate the camera
around the y-axis. Since in the normalized view, the camera’s y-axis is the same as the Y -axis of the world coordinate
system, which is parallel to the cone axis, the rotation can be
estimated by transforming the vertex onto the y-axis in the
normalized view, i.e., we have
R −1 = R y (θ )
R y (θ ) p̂1 = (0, a, b)T
For cone sections standing on the ground, since the vertex is
not available, we need another two points on another edge
such that the vertex can be determined through the intersection of two edge lines. Table 1 depicts the points needed to

Rapid modeling of cones and cylinders from a single calibrated image

309

Fig. 7 Using four points to
reconstruct a cone section
standing on the ground

reconstruct a cone section standing on the ground. In Fig. 7,
we show an example of reconstructing a cup standing on the
ground. The four points are shown as green circles in Fig. 7a.
The vertex (blue circle) is automatically computed from two
edge lines. The algorithm first rotates the view to the “standard view” of the cone as shown in Fig. 7b, where the vertex
lies on the y-axis. The green box shows the size and position
of the original image. The red horizontal line is the x-axis
in the standard view and the vertical red line is the y-axis.
From the standard view, the camera rotates again around the
x-axis with a right angle, so the overhead view of the ground
is obtained in Fig. 7c, where the cross section contour of the
top/bottom becomes a circle (note that the black pad on table
becomes a circle too). From Fig. 7c, the 3D position and the
size of the top/bottom circle can be computed using the algorithm in the previous section. Figure 7d shows the 3D mesh
of the cone section based on the estimated parameters.
3.4.4 Reconstruction of general cones
Similar to the reconstruction of cones standing on the ground,
we need to find a camera rotation such that the view is transformed to the standard view, in which the camera y-axis is
parallel to the cone axis and the camera z-axis intersects the
cone axis. We show that four points as specified in Fig. 3c
are sufficient for determining the required R. Basically, we
exploit the following two properties:

Property 1 When the axis of the SOR object lies on the
y–z plane of the camera, its apparent contour on the image
exhibits symmetry with respect to the center vertical line. In
this case, the camera’s x-axis is the same as the X -axis of
the cone coordinate system (see Fig. 4).
Property 2 When the camera’s z-axis is parallel to the axis
of the SOR object, the cross section circle is imaged as a
circle. In this case, the camera’s z-axis is on the Y -axis of
the cone coordinate system.
From these two properties, we design a two-step algorithm to determine the camera rotation, with Step one based
on Property 1 and Step two based on Property 2. In Step 1, we
synthetically rotate the camera such that the two edge lines in
the new image are symmetric about the imaged y-axis, as in
Fig. 8b. The rotation is constructed by three steps: (1) rotate
around the z-axis so that the vertex is transformed onto the
imaged y-axis; (2) rotate around the x-axis so that the vertex
is transformed to the origin on the image; (3) rotate around
the z-axis so that the two edge lines are symmetric about
the imaged y-axis. After the rotation, the camera coordinate
system is shown in Fig. 8a, where the x-axis is the same as
the X -axis and the z-axis passes through the cone vertex.
Formally, the rotation is:
Rs = Rz (θ3 )Rx (θ2 )Rz (θ1 )

(14)

123

310

J. Zhou, B. Li

From (15) and (16) we have
⎤
⎡
a
0
0
e /2 ⎦
C = ⎣0
c

f
0 e /2
where
c = c cos2 (θ ) − e sin(θ ) cos(θ ) + f sin2 (θ )
Since a circle has the constraint that c = a, we have
c cos2 (θ ) − e sin(θ ) cos(θ ) + f sin2 (θ ) = a
⇒ ( f − a)t 2 − et + (c − a) = 0 where t = tan(θ )

Fig. 8 a Camera coordinate system after Step 1; b the resultant image
after Step 1; c Camera coordinate system after Step 2; d the resultant
image after Step 2

Equation (17) provides us with two solutions. We can choose
one by requiring that the fifth control point is in the front face
of the cone.
To obtain the rotation angle, first we need to fit the ellipse,
i.e., to determine the parameters of E. From Fig. 8b, we
estimate the ellipse from three points: the vertex p̂1s and the
contour points p̂2s and p̂3s . A line tangent to a conic on a point
gives two constraints
l = E p̂2s

such that

(17)

where l = p̂1s × p̂2s

(18)

Rz (θ1 ) p̂1 = (0, a, b)T

A point on a conic gives one constraint:

Rx (θ2 )(Rz (θ1 ) p̂1 ) = (0, 0, c)T

p̂3sT E p̂3s = 0

and θ3 is determined by computing the average angle of
two transformed edge lines. After the rotation, the points
are transformed to p̂is = Rs p̂i , as in Fig. 8a.
In Step 2, the camera is rotated around the x-axis such that
the contour ellipse is transformed to a circle, as in Fig. 8d.
Formally,
⎡
⎤
1
0
0
⎢
⎥
Rc = Rx (θ ) = ⎣ 0 cos(θ ) − sin(θ ) ⎦
(15)

Since E is homogeneous, it has only 3 degrees of freedom
and we can simply set f = 1. Thus, E can be determined
from (18) and (19).
After Step 2, the camera coordinate system is as in Fig. 8c.
It is easy to rotate the camera again to the world coordinate
system by rotating around the x-axis by π/2. Combine all
the rotations, the cone pose can be determined by

0

sin(θ )

cos(θ )

such that
Rx (θ )E Rx−1 (θ ) = C

(16)

where E is the ellipse of a cross section contour and C is a
circle. Since the ellipse is symmetric about the imaged y-axis
after the rotation of Step 1, it can be represented as
⎡
⎤
a
0
0
E = ⎣0
c
e/2 ⎦
0 e/2
f
which means that, for any image point (x, y) on the ellipse,
we have
ax 2 + cy 2 + ey + f = 0

123

R −1 = Rx (−π/2)Rc Rs

(19)

(20)

Figure 9 demonstrates the algorithm step by step. The green
box refers to the original size of the image. The vertical and
horizontal red lines are the y- and x-axes of the image. In
Fig. 9a, we manually pick five points on the edge of the cup.
The red lines are edges generated by four edge points and
they intersect at the vertex of the cone, which is shown by
the blue circle. In Fig. 9b, we compute Rz (θ1 ) in (7) so that
the vertex is transformed onto the y-axis. In Fig. 9c, we compute Rx (θ2 ) so that the vertex is transformed to the origin. In
Fig. 9d, we compute Rz (θ3 ) so that the edge lines are symmetric about the y-axis. An ellipse is fit to the cup bottom
and the result is shown in Fig. 9e. In Fig. 9f, we compute Rc
so that the ellipse is transformed to a circle. After applying
Rx (−π/2) to Fig. 9f, we obtain the standard view of the cone
(see Fig. 9g) and the vertex coordinates are determined. Figure 9h illustrates the mesh of the reconstructed cone model.

Rapid modeling of cones and cylinders from a single calibrated image

311

Fig. 9 Use five points to model
of a general cone section

3.5 Cylinder reconstruction
A cylinder is a special type of cones with the vertex at
infinity. It can be more easily reconstructed than cones,
especially in the un-calibrated case. Similar to cone recon-

struction, we define two types of cylinders: (1) the cylinders standing on the ground with the axis perpendicular to
the ground plane; and (2) general cylinders whose axis is
unknown. We will show that for the first case, two 2D points
are sufficient for reconstructing the cylinder, and that for

123

312

J. Zhou, B. Li

Fig. 10 Cylinder
reconstruction from different
views. a Two points from the
standard view of a cylinder;
b the view of a cylinder standing
on the ground; c The view of a
cylinder of general pose

the second case, four control points are needed (shown in
Fig. 10b, c, respectively).
3.5.1 The cylinder model
Similar to cones, we first define a cylinder in its standard
coordinate system as illustrated in Fig. 11,
L 0 = (c = (0, yb , 1) , r, yt )
T

(21)

where c is the circle of the cylinder’s bottom, yb its height, r
the radius, and yt the height of the top.
A general cylinder is defined by
L = (R, d, L 0 )

(22)

which is shown in Fig. 12.
Thus, the cylinder modeling problem becomes how to
compute the model parameters based on the 2D points on
the image, i.e.,
L = g( p1 , . . . , pn )
For the cylinders standing on the ground, n = 2, with p1
being one endpoint of the top and on one edge, and p2 being
one endpoint on the bottom and the other edge. For a general
cylinder, n = 4: the first two points are two end points of
left edge and another two points are on the other edge. Similar to the case of cones, we first consider the reconstruction
from the standard view (see Fig. 10a). Reconstruction of cylinders standing on the ground and general cylinders can be
transformed to this case by rotating the camera.

Fig. 11 Coordinate system of the camera (O, x, y, z) and the cylinder
(O, X, Y, Z )

123

3.5.2 Reconstruction of cylinders from the standard view
From the standard view, two image points shown in Fig. 10a
are sufficient for 3D reconstruction of a cylinder. The problem is to estimate the parameters in (21) from p1 and p2 . We
can directly use the algorithm in Sect. 3.4.2:
p̂1 = Hg p̂1  (x1 , y1 , 1)T

p̂2 = Hg p̂2  (x2 , y2 , 1)T
lˆ = p̂1 × p̂2 = (a, b, c)T

(23)

yt = 1/(y1 − (b/a)x1 )
yb = 1/(y2 − (b/a)x2 )

1 + (b/a)2
r=
y1 /x1 − (b/a)

(24)

3.5.3 Reconstruction of cylinders standing on the ground
Similar to the case of cones, we need to find R y , such that by
rotating the camera with R y , the normalized view is transformed to the standard view of the cylinder. We use the symmetry property again: in the standard view, two edges of the
cylinder are symmetric about the y-axis. Using two edge
lines, the rotation can be estimated by transforming the lines
in a general view to those in the standard view. Since in the
normalized view, the edge lines are vertical, they are determined by only the x coordinate. By picking two points at
up-left and bottom-right corners, a cylinder on the ground
can be reconstructed, as in Fig. 13. To estimate the rotation

Fig. 12 The world coordinate system and a general cylinder

Rapid modeling of cones and cylinders from a single calibrated image

313

Fig. 13 Use two points to
reconstruct a cylinder standing
on the ground. a Two points for
modeling the cylinder standing
on the ground. b The standard
view of the cylinder after
camera rotations. c An overhead
view of the horizontal plane, in
which cross sections become
circles. d 3D mesh based on the
modeling results

angle, we first compute the angles to transform p1 and p2
onto the y-axis. Then the averaged angle is used to form the
final solution,
R

−1

= R y ((θ1 + θ2 )/2)

R y (θ1 ) p̂1 = (0, a, b)T
R y (θ2 ) p̂3 = (0, c, d)T

v̂ = ( p̂1 × p̂2 ) × ( p̂3 × p̂4 )
We want to find an rotation R1 such that
R1 v̂  v̂0 = (0, 1, 0)T
Rotating v̂ to v̂0 can be decomposed as two rotations: first
rotating around the z-axis so that v̂ is moved to the y-axis
on the image and then rotating around the x-axis so that v̂ is
moved to infinity on the y-axis. Formally:

3.5.4 Reconstruction of general cylinders

R1 = Rx (θ2 )Rz (θ1 )

For a general cylinder not standing on the ground, we need
four points (Fig. 10c) to reconstruct the cone. p3 and p4 are
not necessarily the endpoints of the edge. Based on the two
edges, we can estimate R by transforming the two edges to
those in the standard view, i.e., being symmetric about the
y-axis and vertical. Based on this property, we developed a
two-step algorithm to estimate the rotation. First, we transform the two edges to vertical lines in the image, just as those
in the case of cylinders standing on the ground (see Fig. 10b).
Then we use the algorithm in the previous subsection to further estimate another rotation.
For the first step, we estimate the rotation by transforming the imaged vanishing point in the vertical direction to
(0, 1, 0)T , which is the coordinates of vertical vanishing point
in standard view. The original vanishing point can be obtained
by intersecting two edge lines:

such that:
Rz (θ1 )v̂  (0, a, b)T
Rx (θ2 )(Rz (θ1 )v̂)  (0, c, 0)T
Figure 14 shows an example of reconstructing a cylinder of
a general pose. We first rotate the camera such that the two
edge lines become vertical as in Fig. 14c. Figure 14d is the
standard view of the cylinder and Fig. 14e gives the overhead
view of the cross section. Figure 14f illustrates the 3D mesh.
3.6 Degenerate cases
There exist some degenerate cases for which the proposed
approach cannot deal with. For cones, degeneracy occurs
when the imaged vertex of a cone lies inside the imaged
cross section contour and thus it is impossible to find the

123

314

J. Zhou, B. Li

Fig. 14 Use four points to
reconstruct a cylinder of a
general pose. a Four points to
use. b The intersection point of
two edges is rotated to the
y-axis by rotating around the
z-axis. c The intersection point
is further rotated to infinity by
rotating around the x-axis.
d The standard view of the
cylinder by rotating around the
y-axis such that two edge lines
are symmetric about the imaged
y-axis. e An overhead view of
the cylinder where the cross
section becomes a circle. f 3D
mesh based on the modeling
results

line passing through the imaged vertex and tangent to the
cross section contour. An example is when an upright cone
is imaged from the overhead view. For cylinders, degeneracy
occurs when the cylinder is imaged from above/bottom such
that the further circle is totally occluded by the closer circle,
thus no edge lines can be found.

useful application of our system, i.e., augmented reality. In
Sect. 4.3, we make a qualitative comparison of our system
and other state-of-art systems on cone and cylinder modeling
from a single image.

4 Experimental results and discussion

In practice, there may be accuracy issues when a user clicks
a point on an image, and the errors may affect the final reconstructed model. In this subsection, we analyze the modeling
accuracy using Monte Carlo simulation, in which Gaussian
noise (with standard deviation of 1 pixel) is added to the
ground truth image points before the modeling process. We

To evaluate our algorithms, both synthetic data and real data
are used. In Sect. 4.1, we evaluate the accuracy based on synthetic data. In Sect. 4.2, we use real data to demonstrate the
speed and usability of our approach. We also demonstrate a

123

4.1 Accuracy evaluation with synthetic data

Rapid modeling of cones and cylinders from a single calibrated image

315

test the modeling accuracy under different object orientation
setup. For each orientation setup, we run the simulation for
1,000 times. There are numerous configurations for a cone’s
shape, orientation and position in an image. In the simulation, we first put the object in the center of the image and then
rotate it around different axes. Since cones can be viewed as
a special kind of cone sections, i.e., the top radius is zero, we
evaluate only cylinders and cone sections.
For objects standing on the ground, only the rotation
around the y-axis is needed to be estimated. Therefore, we
rotate the object around the camera Y -axis, and the object
will move horizontally in the image. For objects of a general
pose, we rotate the object around its own center so that the
object is always in the center of the image (but with different orientations). Since the image of an object remains the
same if it rotates about its own axis (which is the same as the
y-axis), we only rotate the object about the x- and z-axes.
For simplicity of visualization, we rotate around the x-axis
and the z-axis independently (experiments did show the correlation of errors between the two angles is small). Table 2
summarizes all the experimental configurations. For cylin-

Table 2 Experimental configurations: axes to rotate about in the experiments
Cylinders

Cone sections

On ground

Y

Y

General

X, Z

X, Z

ders and cone sections standing on the ground, we rotate it
about camera Y -axis and for the objects in general poses, we
rotate them around their own x- and z-axes independently.
In the simulation, the image size was set to be 640 × 480;
the vertical field of view is 40◦ ; the rotation (about each axis)
varied from −50◦ to 50◦ , with an interval of 10◦ . The cylinder
parameters were set as L 0 = (c = (0, yb = 0.2, 1)T , r =
0.2, yt = 0.2) and the cone parameters were set as M0 =
(c = (0, yc = −0.2, 1)T , r = 0.2, yv = 0.4, yt = 0.2).
Figure 15 provides some examples of the generated objects.
Since the Euclidean reconstruction is up to a scale, we use
the relative size to measure the modeling error. We first scale
the reconstructed model such that the bottom radius is equal

Fig. 15 Examples of generated
objects. The columns are for
different angles of rotation. The
rows are for different objects
and rotation around different
axes. Note that in the actual
experiments, the angles range
from −50 to 50

123

316

to one, then we divide the scaled parameters by the ground
truth such that all ideal result is one. We call the results “normalized value”. For cones, we measure its height and the cap
radius. For cylinders, we measure the height.
Figures 16, 17, 18 and 19 present the modeling and orientation accuracy in terms of standard deviation. From the
plots we can found that
1. Rotation around the z-axis does not have much impact on
the accuracy (the dashed lines in the figures). The errors
are stable.
2. Rotation around the x-axis has significant influence on
the accuracy. When the rotation angle becomes larger,

Fig. 16 Modeling accuracy for general cones

Fig. 17 Modeling accuracy for cones on the ground

123

J. Zhou, B. Li

the error becomes larger (except for the orientation of the
cylinder). An extreme rotation around the x-axis leads to
the degenerate case, which was described in Sect. 3.6.
3. For objects standing on the ground, when the rotation
angle around the y-axis becomes larger, the error
becomes smaller. The reason is that a unit angle (or
length) occupies more space on the image when it is
further away from the image center (see Fig. 15).
4. The accuracy for objects on the ground is better than
that for general objects. For cones on the ground, the
maximum modeling error is below 2%, while for general cones, the average is about 2% and the error reaches
to 11% when the x rotation angle is 50◦ . For cylinders

Rapid modeling of cones and cylinders from a single calibrated image

317

Fig. 18 Modeling accuracy for general cylinders

Fig. 19 Modeling accuracy for cylinders on the ground

on the ground, the maximum modeling error is below
0.8%.
Overall, the modeling is accurate (with an error below 2%)
when the angles are between −40 and 40. When the rotation
angle around the x-axis is out of this range, the object pose is
near to degenerate configuration and the modeling becomes
inaccurate and unstable.
4.2 Results using real data
Several sample experiments using real images are presented
to demonstrate the effectiveness and capability of the proposed approach.

The first experiment (Fig. 20) illustrates the reconstruction of two cone-shaped objects (coffee cups), using the cone
section model. In the scene, one cup stands on the table and
the other one lies on the table and is partially occluded by
the first one. Each cup is reconstructed by five points. For the
partially occluded cup, we pick one endpoint of the upper
edge behind the first cone. Although this point is invisible in
the image, we are able to pick it through moving a point on
the image such that the line passing this point and another
visible endpoint matches with the edge of the cup. From
the cup standing on the ground, the camera orientation is
estimated. Then using the orientation information, the table
face is reconstructed. We scale both cup models such that

123

318
Fig. 20 Reconstruction of two
cups. a, b Use five points to
reconstruct the cone sections;
c One view of the reconstructed
3D models; d Augmented reality
using the 3D model information

Fig. 21 Using the developed
Google SketchUp plug-in to
model the objects. a The input
image. b Using four points to
reconstruct a cone section
standing on the ground. c Using
two points to reconstruct a
cylinder standing on the ground.
d The resultant models

123

J. Zhou, B. Li

Rapid modeling of cones and cylinders from a single calibrated image

319

Fig. 22 Using the developed
Google SketchUp plug-in to
model the cup under occlusion
and on a cluttered background

Fig. 23 Additional examples
from the developed Google
SketchUp plug-in. The first row
shows the modeling from a
photo downloaded from a public
website

123

320

J. Zhou, B. Li

Table 3 A comparison of the capabilities of modeling from a single or
multiple images
Software

No. of images
Single image

Multiple images

Our system

Yes

No

Facade

Yes

Yes

SketchUp PhotoMatch

Yes

No

ImageModeler

Yes

Yes

PhotoModeler

No

Yes

ShapeCapture

No

Yes

iWitness

No

Yes

their lowest points touch the table. Figure 20c illustrates the
full 3D results. To further visualize and validate the reconstructed 3D structure, we insert three 3D tea pots into the
scene (Fig. 22d), illustrating an application of augmented
reality.
We developed a plug-in for SketchUp using its Ruby API
[21]. The plug-in is able to do cones and cylinders modeling
based on 2D control points, which is useful for image-based
modeling but not available yet in SketchUp. The modeling
process is illustrated in video demos. Figure 21 shows some
snapshots. When the user starts the modeling tool, the plugin will generate several default control points (blue squares)
and a default model based on these points are generated To
model an object in the image, and the user only need to click
the control points and move them onto the object contour
(Fig. 21b, c). When the user moves the control points, the
model contour is reconstructed and drawn (in dark green) in
real time. Therefore, the user can accurately adjust the control points by matching the reconstructed model contour with
the real object contour. Switching to other tools will finalize
the reconstructed model (Fig. 21d).

Figure 22a illustrates another example of modeling a general cone with cluttered background and significant occlusion. Figure 22b shows the initial five points generated by
our plug-in. Figure 22c illustrates an intermediate step and
Fig. 22d shows the final reconstructed cup with texture
mapped from the photo.
Sample results from some additional experiments are
given in Fig. 23. The first row in Fig. 23 is downloaded
from internet and we do not know the actual focal length.
The focal length of this photo is estimate using SketchUp’s
built-in PhotoMatch tool. Even with this approximated focal
length, we can still model the cone object fast and accurately (in terms of contour matching). All the real data experiments using the developed SketchUp plug-in are recorded in
short video demos which are available at http://www.public.
asu.edu/~jzhou19/modeling/rapid_modeling.htm. The average time of modeling one object is about 30–50 s.
4.3 A qualitative comparison of our system and other
state-of-the-art systems
We now present a qualitative comparison of our system and
other state-of-art systems for cone and cylinder modeling.
Some existing software packages or approaches simply do
not support cone/cylinder modeling from a single image. For
those that support such modeling, they may require known
axis of the cones/cylinders or a known 3D plane. In addition,
their modeling procedures are quite complex and totally different from ours. For instance, SketchUp PhotoMatch
requires a user first pick the center point of the circle on
a plane, which is difficult to achieve on the image (especially considering perspective distortion). Such comparisons
are summarized in Tables 3 and 4. Table 3 compares the
capabilities of modeling from a single or multiple images.
PhotoModeler [15], ShapeCapture [20] and iWitness [12]
have to rely on multiple image for modeling. Table 4 further

Table 4 A comparison of single image based modeling systems
Software

Objects
Standing on the ground

Our system

Facade
SketchUp PhotoMatch

ImageModeler

123

General

Cylinders

Cones

Cylinders

Cones

Click two points on the contour, one is the top-left and
another is bottom-right. Both control points are easy
to find in the image
Assuming known 3D information of the axis; users
click the contour points
First construct a circle on a known 3D plane by
clicking the circle center and control the radius; then
push/pull the circle to a cylinder
First construct a circle on a known 3D plane by clicking 3 points on the plane; then push/pull the circle to
a cylinder

Yes

Yes

Yes

Yes

No

No

No

No

No

No

No

No

Rapid modeling of cones and cylinders from a single calibrated image

321

compares the systems which can do modeling from a single
image. It shows that all other systems cannot handle cylinders or cones with general pose. For cylinders standing on
a determined 3D plane, we also briefly describe the modeling procedures in the table. For example, Facade requires
that the axis is already reconstructed; SketchUp PhotoMatch
requires the user to click the center of the top or bottom circle of the cylinder, which is difficult to do; ImageModeler
requires the user to click three points to reconstruct a circle
on a plane, which is hard to control, especially when the contour is small in the image. Comparing to these systems, our
system requires much simpler interactions, i.e., clicking two
points on the apparent contour of the object.
In summary, our system not only provides capabilities of
general cone and cylinder modeling, which some other systems cannot handle, but also support much easier and intuitive
interaction for modeling cones and cylinders, compared with
existing methods.

7. Fitzgibbon, A., Zisserman, A.: Automatic camera recovery for
closed or open image sequences. In: Proceedings of the 5th European Conference on Computer Vision (ECCV ‘98), vol. I, pp. 311–
326. Springer, Berlin (1998)
8. Hartley, R., Zisserman, A.: Multiple View Geometry in Computer
Vision. Cambridge University Press, Cambridge (2004)
9. Hoiem, D., Efros, A., Hebert, M.: Automatic photo pop-up. ACM
Trans. Graph 24(3), 577–584 (2005)
10. Horry, Y., Anjyo, K.-I., Arai, K.: Tour into the picture: using a
spidery mesh interface to make animation from a single image. In:
Proceedings of the 24th Annual Conference on Computer Graphics
and Interactive Techniques (SIGGRAPH ‘97), pp. 225–232. ACM
Press/Addison-Wesley, New York (1997)
11. ImageModeler: http://www.realviz.com/ (2008)
12. iWitness: http://www.iwitnessphoto.com/ (2009)
13. Kim, J.-S., Gurdjos, P., Kweon, I.-S.: Geometric and algebraic
constraints of projected concentric circles and their applications to camera calibration. IEEE Trans. Pattern Anal. Mach. Intell. 27(4), 637–642 (2005)
14. Nister, D.: Automatic passive recovery of 3D from images and
video. In: Proceedings of the 2nd International Symposium on
3DPVT 2004, pp. 438–445 (2004)
15. PhotoModeler: http://www.photomodeler.com/ (2008)
16. Pollefeys, M., Koch, R., Gool, L.: Self-calibration and metric
reconstruction inspite of varying and unknown intrinsic camera
parameters. Int. J. Comput. Vis. 32(1), 7–25 (1999)
17. Prasad, M., Fitzgibbon, A.: Single view reconstruction of curved
surfaces. CVPR 02, 1345–1354 (2006)
18. Robertsone, D., Cipolla, R.: An image-based system for Urban
navigation. In: Proceedings of British Machine Vision Conference
(2004)
19. Saxena, A., Sun, M., Ng, A.Y.: Make3D: learning 3-D scene structure from a single still image. In: IEEE Transactions on Pattern
Analysis and Machine Intelligence (2008)
20. ShapeCapture: http://www.shapecapture.com (2008)
21. SketchUp-Ruby-API:
http://code.google.com/apis/sketchup/
(2008)
22. SketchUp: http://www.sketchup.com/ (2007)
23. Utcke, S., Zisserman, A.: Projective reconstruction of surfaces of
revolution. Pattern Recognit. 2781, 265–272 (2003)
24. Wong, K.-Y., Mendona, P., Cipolla, R.: Reconstruction of surfaces
of revolution from single uncalibrated views. Image Vis. Comput.
22(10), 829–836 (2004)
25. Wong, K.Y.K., Cipolla, R.: Structure and motion from silhouettes.
In: Proceedings of the Eighth IEEE International Conference on
ICCV 2001, vol. 2, pp. 217–222 (2001)
26. Wong, K.Y.K., Mendonca, P.R.S., Cipolla, R.: Camera calibration
from surfaces of revolution. IEEE Trans. Pattern Anal. Mach. Intell. 25(2), 147–161 (2003)
27. Wu, Y., Li, X., Wu, F., Hu, Z.: Coplanar circles, quasi-affine invariance and calibration. Image Vis. Comput. 24(4), 319–326 (2006)
28. Wu, Y., Wang, G., Wu, F., Hu, Z.: Euclidean reconstruction of a
circular truncated cone only from its uncalibrated contours. Image
Vis. Comput. 24(8), 810–818 (2006)
29. Ying, X., Zha, H.: Camera calibration using principal-axes aligned
conics. In: Proceedings of the 8th Asian Conference on Computer
Vision (ACCV 2007), pp. 138–148 (2007)
30. Zhang, L., Dugas-Phocion, G., Samson, J.S., Seitzt, S.M.: Single view modeling of free-form scenes. In: Proceedings of the
2001 IEEE Computer Society Conference on CVPR 2001, vol. 1,
pp. I-990–I-997 (2001)

5 Conclusions
We have presented approaches to 3D modeling of cones and
cylinders from a single calibrated image based on a minimum
number of 2D control points. The approach utilizes computer
vision techniques in combination with simple manual manipulation of a few points on the image. Experiments show that
our approach is able to achieve accurate and fast 3D modeling of cones and cylinders from a single image. Future
work along this direction includes evaluating the interactive
scheme and revising the procedure for better human–computer interaction.

References
1. Borshukov, G.: New Algorithms for Modeling and Rendering
Architecture from Photographs. Electrical Engineering and Computer Science, Master’s Thesis. University of California at Berkeley
(1997)
2. Canoma: http://www.metacreations.com/products/canoma (2008)
3. Chen, Q., Wu, H., Wada, T.: Camera calibration with two arbitrary
coplanar circles. In: European Conference on Computer Vision,
pp. 521–532 (2004)
4. Colombo, C., Del Bimbo, A., Pernici, F.: Metric 3D reconstruction
and texture acquisition of surfaces of revolution from a single uncalibrated view. IEEE Trans. Pattern Anal. Mach. Intell. 27(1), 99–
114 (2005)
5. Criminisi, A.: Accurate Visual Metrology from Single and Multiple
Uncalibrated Images. Springer, New York (2001)
6. Debevec, P., Taylor, C., Malik, J.: Modeling and rendering architecture from photographs: a hybrid geometry- and image-based
approach. In: Proceedings of the 23rd Annual Conference on
Computer Graphics and Interactive Techniques (SIGGRAPH ‘96),
pp. 11–20. ACM Press, New York (1996)

123

530

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 11, NO. 5, MAY 2002

A Generic Approach to Simultaneous Tracking and
Verification in Video
Baoxin Li, Member, IEEE, and Rama Chellappa, Fellow, IEEE

Abstract—In this paper, a generic approach to simultaneous
tracking and verification in video data is presented. The approach
is based on posterior density estimation using sequential Monte
Carlo methods. Visual tracking, which is in essence a temporal
correspondence problem, is solved through probability density
propagation, with the density being defined over a proper state
space characterizing the object configuration. Verification is
realized through hypothesis testing using the estimated posterior
density. In its most basic form, verification can be performed as
and two hypotheses 1
follows. Given a measurement vector
and 0 , we first estimate posterior probabilities ( 0 ) and
( 1 ), and then choose the one with the larger posterior
probability as the true hypothesis.
Several applications of the approach are illustrated by experiments devised to evaluate its performance. The idea is first tested
on synthetic data, and then experiments with real video sequences
are presented, illustrating vehicle tracking and verification, human
(face) tracking and verification, facial feature tracking, and image
sequence stabilization.
Index Terms—Importance sampling, Monte Carlo method, object verification, visual tracking.

I. INTRODUCTION

P

ROGRAMMING a computer to recognize objects is a
difficult problem, and has been a research topic for many
years. Recently, there has been increasing interest in integrating
the temporal information available in video for improved
recognition performance. However, in most cases, the temporal
information is exploited only in tracking an object, with less
emphasis on using temporal information for recognition. In
its most crude form, temporal information can be exploited
through voting, i.e., recognition is done on each frame, and
a vote is taken to give the final decision. While this may be
helpful in some cases, much information is being left out in
this crude approach, such as temporal coherence in the shape
changes of an object in consecutive frames.
With video data, recognition often becomes a verification
problem. That is to say, an algorithm is needed to answer the
question: is this the object seen at a previous time? Or, is this

Manuscript received May 1, 2001; revised February 6, 2002. This work was
supported by the Advanced Sensors Consortium (ASC) sponsored by the U.S.
Army Research Laboratory under the Federated Laboratory Program, Cooperative Agreement DAAL01-96-2-0001. The associate editor coordinating the
review of this manuscript and approving it for publication was Dr. Thiow Keng
Tan.
B. Li is with Sharp Laboratories of America, Camas, WA 98683 USA (e-mail:
bli@sharplabs.com).
R. Chellappa is with the Center for Automation Research, University of
Maryland, College Park, MD 20742 USA (e-mail: rama@cfar.umd.edu).
Publisher Item Identifier S 1057-7149(02)04781-4.

the object I was asked to look for? And often, there are only
a small number of candidates to verify against, with the candidates being templates obtained from earlier parts of the video.
The problem can be illustrated by the examples of monitoring
a vehicle entering and then leaving a parking lot, and a person
entering and then leaving a bank. In these scenarios, tracking is
needed first; then the algorithm needs to do incremental verification while maintaining track on the object. Obviously, in these
applications, temporal information from video, if properly exploited, would facilitate recognition.
In this paper, a generic approach to simultaneous object
tracking and verification is proposed. The approach is based
on posterior probability density estimation through sequential
Monte Carlo methods. Tracking, which is in essence a temporal
correspondence problem, is formulated as a probability density
propagation problem, with the density being defined over a
proper state space characterizing the object configuration.
With a novel reparametrization, many tracking applications
involving different representations such as edge maps, intensity
templates, and feature point sets are uniformly processed by
the same algorithm. Examples will also show how the approach
can be applied to tasks like image sequence stabilization.
In addition to performing tracking, the algorithm also gives
verification results as time proceeds; this is realized through
hypothesis testing using the estimated posterior probability
densities.
The paper is organized as follows. Section II shows how
tracking can be formulated as a Bayesian inference problem,
and also gives a brief introduction to sequential Monte Carlo
methods. Section III describes our approach to simultaneous
tracking and verification. Applications of the approach are
illustrated by experiments in Section IV. Section V relates our
approach to other work. We conclude in Section VI.
II. THEORETICAL BACKGROUND
A. Tracking as a Bayesian Inference Problem
Tracking is the processing of measurements obtained from
an object in order to maintain an estimate of its current state,
which typically consists of kinematic components (position,
velocity, etc.) and other components (signal strength, “feature”
denote the state to be estimated,
information, etc.). Let
be the measurement vector (observations up to :
and let
). The subscript denotes a discrete time
and
are in general random quantities.
index. Both
be the distribution of
; we then have the joint
Let
:
, where
distribution

1057-7149/02$17.00 © 2002 IEEE

LI AND CHELLAPPA: GENERIC APPROACH TO SIMULTANEOUS TRACKING AND VERIFICATION IN VIDEO

is the likelihood of
Bayes theorem, we have

, having observed

. Using

which is the posterior distribution of , and is what Bayesian
inference attempts to estimate. Assuming we have obtained
, tracking is solved; knowing
, by definition we
know everything about the current state of the object, including
its location and other dynamics in the state vector. Thus
tracking can be formulated as a Bayesian inference problem,
as the objects of the inference. Note that in this
with
is a time-variant quantity;
formulation, the posterior
at time is evolved from
in a tracking problem,
at time
. In this sense, tracking is also a
density propagation problem.
In reality, instead of obtaining the posterior density itself, a
Bayesian inference task may focus on only estimating some
properties of the density, such as moments, quantiles, highest
posterior density regions, etc. All these quantities can be ex.
pressed in terms of posterior expectations of functions of
is
The posterior expectation of a function

The integration in this expression has until recently been the
source of most of the practical difficulties in Bayesian inference, especially in high dimensions. In most applications, anais impossible. Alternatives inlytic evaluation of
clude numerical evaluation, for example, using classical quadrature. However, in high dimensions, this method is computationally prohibitive since too many samples will be required to yield
reasonable accuracy.
An important alternative technique is Monte Carlo integration
(e.g., [14], [21]), which, in general, is often suitable for high-dimensional integration. The basic Monte Carlo method approximates a definite integral by uniformly sampling from the domain
of integration, and averaging the function values at the samples.
There are two major limitations to the basic Monte Carlo approach: 1) the accuracy improves only linearly with the number
of samples and 2) more samples are needed if the integrand
has peaks in some small regions and is very small elsewhere.
Common methods of handling these limitations (especially the
second one) include importance sampling, rejection sampling,
Gibbs sampling (which can be treated as a single-component
Markov Chain Monte Carlo (MCMC) approach), etc. There also
exist techniques for combining these methods for improved performance. One elegant approach is the sequential importance
sampling method, which is discussed in the next subsection.
B. Sequential Monte Carlo Methods for Dynamic Systems
and observation
in a tracking problem are
The state
time-variant quantities of a time-variant system, for which the
state space model is a popular way of analyzing. When the
system is linear and the noise model is Gaussian, the use of the
Kalman filter has been a common practice, and optimal results
can be obtained with the Kalman filter. Extended Kalman filters

531

and other approximations have been utilized to handle nonlinear
and/or non-Gaussian systems (e.g., [1], [28]).
In recent years, Monte Carlo methods have been proposed
for the analysis of nonlinear and/or non-Gaussian dynamic systems (e.g., [7], [22]). One class of Monte Carlo methods is the
so-called sequential importance sampling (SIS) methods. In SIS
methods, at time , the dynamic density is approximated by a set
of its samples, with proper weights. This is basically importance
sampling [14] used in a sequential fashion. SIS has some of the
good properties of both importance sampling and MCMC. On
the one hand, the samples are properly weighted, and the significance of a sample is represented by its weight, which can
be updated by incorporating current observations. On the other
hand, by re-using the samples, SIS can keep track of a slowly
varying density. Applications of SIS can be found, for example,
in target position tracking [13], in the Bayesian missing data
problem [23], and in contour tracking [20].
We now give a brief description of the SIS approach.
Following [27], one first characterizes a probabilistic dynamic
system as a sequence of evolving probability distributions
, indexed by discrete time , where
is the state
variable at time . This is more general than the state space
model and can handle other problems such as the Bayesian
missing data problem. In this setting, the posterior estimation problem in a state space model is a special case, with
. An SIS algorithm, which summarizes the
aforementioned methods [13], [23], [20], is then designed as
shown in the following algorithm (for a complete treatment,
see [27]).

Let
denote a set of
random draws that are properly weighted by the
with respect

set of weights
At each time step
to
Draw
Compute

from

and

Then
of

is a properly weighted sample

In this algorithm,
is called the trial distribution or prowill ultimately
posal distribution. Although any choice of
deliver samples from the desired distribution (subject to regularity conditions; see, for example, [32]), the rate of convergence
and
.
will depend crucially on the relationship between
, such as
There are different proposals about the choice of
in [23] and [22]. The latter is used in our work, which will be
described in detail in Section III-D.
When a sequence of observations induced by a dynamic object parametrized by is available, such as a video sequence of

532

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 11, NO. 5, MAY 2002

a moving person, the posterior density estimation can be solved
in a sequential way using the SIS algorithms. With a state-space
model, the weight can also be deduced from the likelihood of
, which incorporates the current observathe state
tion [22], [20].
III. SIMULTANEOUS TRACKING AND VERIFICATION
POSTERIOR ESTIMATION

VIA

With the SIS algorithm, tracking is immediately solved by
to some parametrization of the object that
setting the state
be the
includes the location of the object. For example, let
two-dimensional (2-D) translation of a 2-D object. Using the
at each time . If
SIS algorithm, one can get the density of
the expectation of is used as the estimate of the true translation of the object, tracking is achieved after approximating the
expectation using the estimated density.
Assuming that tracking has been done, we now consider
verification. In Section III-A, we first put the verification
problem into a probabilistic setting. Section III-B introduces a
reparametrization which leads to a dynamic system of lower
dimensionality. Section III-C gives our algorithm for simultaneous tracking and verification based on the SIS method.
Section III-D discusses implementation issues of the algorithm.

be, for example, a face template (intensity image), a set of intensity discontinuity points (edge map), or the parametric contour
of an object. Let
denote the transformation of
into . Under the small and continuous motion assumption,
is similar to , meaning that has only a “small” difference
from , with being the parameter for the identity transform:
. Expanding
at gives

(2)
denotes higher order terms, and
is the Jacowhere
bian matrix with respect to . This expansion shows that the
can be viewed as the original
plus a
transformed object
. From a practical point
changing term caused by a
of view, only the difference is important; knowing this, temporal
, the vector
is a good
correspondence is solved. Given
under the small motion asparametrization of all possible
as the state vector.
sumption. Thus, we propose to use
is easy to obtain for 2-D affine
The Jacobian matrix
or simpler transformations. For example, let
be the location of one edge point. For a 2-D affine transfordefined by
, let
mation

A. Verification via Posterior Probability

; then the Jacobian matrix is

classes
(e.g.,
Assume that there are
different people). Let
be a parametrization of the object,
which can be, for example, the intensity image of the object
(viewed as a vector). In general, is a random vector governed
. Given an observation ,
by the a priori density
the Bayesian maximum-a-posteriori-probability rule chooses
as the solution to the verification
is the posterior probability of class
problem, where
given the observation .
can be computed through
integrating the posterior density as
(1)
is the posterior density of class , with being
where
some proper region. Therefore, the verification problem can be
is
solved best (in the Bayesian sense) if the posterior
denote the event “class
first estimated. Let hypothesis
causes the observation .” We then use
and
interchangeably.
is high-dimensional (for example,
Unfortunately, when
an image vector), posterior estimation through empirical approaches is not realistic, even with SIS methods. Given limited
observation data, the estimates would be inaccurate at best, and
meaningless at worst. If, however, the object can be characterized by a vector of low dimensionality, then the SIS method
would be an effective tool for posterior estimation.

computed as
(3)
is used, it refers to
. That
From now on, whenever
is to say, the dynamic system under consideration will be the
. It is worth pointing out that
one governing the evolution of
(2) is valid for general transformations other than 2-D affine or
Euclidean similarity transformations. Also, since 2-D affine is
linear in each component of , the higher-order terms in (2) are
is accurate for
zero, implying that the parametrization with
a 2-D planar object even under large motion (this is also true for
the 2-D translation and Euclidean similarity groups, see [25]).
C. The Algorithm
An algorithm for simultaneous tracking and verification is
Rectify the templates onto the first
frame of the sequence.
Initialization

draw
random samples from
Updating at time
invoke the SIS algorithm to
obtain an updated set of samples for
At time

B. Reparametrization
Consider a rigid object subject to motion which can be modeled by a transformation parametrized by a parameter vector .
denote an original parametrization of the object.
can
Let

evaluate the mean value

of
Compute the posterior probability
according to (1), with

being a hypercube around

LI AND CHELLAPPA: GENERIC APPROACH TO SIMULTANEOUS TRACKING AND VERIFICATION IN VIDEO

Choose as true the hypothesis
giving the
maximum probability (or sum of probabilities up to ).
As shown previously, the algorithm is initialized by rectifying
the templates to the first frame of the sequence. This step corresponds to the object detection process, which will not be considered in this paper. The rectification refers to registering the
template to the scene, which is usually done approximately by
the detection process. With this algorithm, this registration need
not be very accurate, since in the second step, random samples
will be drawn around the initial point, and will capture the real
of the
value with a probability depending on the variance
. In theory, this probability
population and the sample size
goes to 1 with large and .When the state vector is some
parametrization of the underlying object shape or appearance,
it is not hard to understand the rationale behind the verification step in the previous algorithm. It remains to be explained
why we can still do verification using the algorithm when is
. The argument is that given the observation history , the
true hypothesis should generate a density of higher peak and
more concentrated shape than a false hypothesis would, since
the transformed object should be “close” to the original one
under small motion assumption. In an ideal (deterministic) situation the measurement of the object at time can be related to its
model (true hypothesis) by a unique , while no transform can
relate a false hypothesis (a model different from the right one)
to the measurement without incurring a large matching error.
From a Bayesian point of view, recall that

where
is the same for all the hypotheses and, thus, has
is the prior density of .
no effect on verification, and
is usually assumed to be unimodal, for example Gaussian.
, which should be treated as a function of
when
is given, reflects the likelihood of the event “the observation
being incurred by .” Without considering occlusion, in genshould peak at the true which causes the obsereral
vation, and decrease when deviates from the true value. It is
is unimodal when reflects
reasonable to assume that
. This is especially true
a small change with respect to some
in a verification problem where the templates are usually obtained from earlier frames of the sequence. The single subject
assumption is automatically satisfied through tracking: only a
local region needs to be considered, and within this region the
assumption of a single subject is reasonable.
should
In summary, under the previous assumptions,
be unimodal with the peak at the true value of . This is the
basis for the verification step. Later, in our experiments, it will
is indeed shaped as predicted. It will
be observed that
also be shown that when the single object assumption is violated, the verification can fail temporarily (e.g., Fig. 9).
It must be pointed out that, when the system dynamics are
known (such as a constant velocity model), another type of information is available to assist verification: the prediction error.
, with the mean value of state
In the algorithm, at time

533

at time known, prediction can be done using the system
dynamics, and the prediction error gives an additional indication of whether the hypothesis is true. Ideally, a true hypothesis gives a smaller prediction error. In the application examples
in this paper, we only assume that the system is governed by a
first-order Gauss-Markov process (random walk). Thus we will
not exploit the information from a prediction error perspective.
In the verification algorithm, a threshold needs to be specified to define the integral region . To avoid choosing an ad hoc
, one can compute the (e.g., 95%) confidence interval around
the mean, and then test the hypotheses based on the length of
the interval in each dimension. However, we will use the probability rather than the length of the confidence interval since the
former is more intuitive.
Notice that in the previous algorithm, multiple hypotheses are
kept during the tracking and verification process, meaning that
several dynamic systems are maintained simultaneously. This
is natural in applications—the actual scenario could be either of
the following: the system has to identify a given object (template) from multiple objects in view (such as tracking a person
who entered the bank a moment ago, based on a sequence containing multiple persons); or the system has to identify a single
object as one of several possible candidates (templates) (such
as classifying the person seen in a sequence as one of the candidates). Our verification experiments in the next section will
concentrate on the latter case. Maintaining multiple systems increases the computational complexity. Fortunately, the systems
can be processed in parallel.
D. Issues in Implementing the Algorithm
In introducing the algorithm in Section III-C, we did not
specify details about implementing the SIS steps. We were
intentionally obscure on that subject, because many techniques
exist for improving the SIS algorithm; thus we wanted the
algorithm to be generic without the constraint of a specific
implementation. In this subsection, we discuss some implementation issues, especially the techniques used in this work.
Choice of the Proposal Distribution: As mentioned earlier,
is important in practhe choice of the proposal distribution
tice. When the system dynamics are described by a state space
model, with transition kernel (transition kernels are general versions of transition probabilities of a discrete state space model)

where
of

is the state and
is

the model parameters, a good choice

With this choice of
,
is proportional to the measurewhich is given by
ment likelihood of a particular sample

Thus the measurement is easily incorporated into the weight
has been used in
update of the samples. This choice of
[22], [13], [20], etc., and is what is used in our implementation
of the SIS algorithm.

534

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 11, NO. 5, MAY 2002

Fig. 1. Left: A car parametrized by intensity edges. This is what we used to synthesize the sequence in Fig. 2. Right: Another vehicle used as the alternative
hypothesis.

Resampling: Other techniques also exist for improving the
performance of the SIS algorithm, such as resampling (e.g.,
in [22]), rejection sampling (e.g., in [39]), adaptive direction
sampling [12], Rao–Blackwellization [10], etc. However, most
of these techniques do not always work well. For example,
Rao–Blackwellization was intended for reducing the estimation
variance, but it does not always work efficiently (see [11]).
While it is possible to incorporate these techniques into the
sampling step in the algorithm in Section III-C, our current
implementation uses the resampling technique given in [22].
, samples
The resampling scheme generates, at time
by resampling
with probabilities

where the summation is over the set of samples. This is also
partly inspired by the successful application of this simple resampling technique in contour tracking in [20] (where the algorithm is called CONDENSATION).
Measurement Likelihood: In applications, the measurement
likelihood is obtained, for example, from the measurement
equation of a state space model. Thus for different applications,
one could have different expressions for the measurement likelihood. We will, however, use a simple measurement likelihood
formula for all experiments in this work—a truncated Gaussian
model. Of course, a Gaussian model may not be accurate for
real applications, but it turns out that in the tracking problem,
if the tracker is initialized well, a Gaussian model can be a
reasonable approximation. This is especially helpful when
it is difficult to specify the true measurement distribution
analytically. Specifically, we define

if
otherwise
and
are the measurement likelihood and the prewhere
diction error, for sample at time , respectively, with being a
threshold and a constant. If we assume that is time-homocan be dropped.
geneous, then the subscript of
is problem-dependent; thus
Note that the prediction error
we will not specify it until we are dealing with a specific application of the algorithm.
System Dynamics: As discussed earlier, the system dynamics, if known, can be incorporated into the SIS steps [for
], and the prediction error can be used to assist
choosing
in hypothesis testing. However, in all the experiments in this

paper, we only assume that the system is governed by a first
order Gauss–Markov process (random walk). This enables the
algorithm to work on unknown dynamics. To be specific, the
conditional distribution density of state
given
is of the
form (for simplicity, only the one-dimensional case is given)

where
far

is the variance, which controls, roughly speaking, how
can be from .
IV. APPLICATIONS

In this section, we describe experiments that illustrate the applications of the proposed algorithm. Different input representations have been used in the examples. We first test the algorithm
using synthetic sequences (based on edges), since synthetic data
allow us to compare estimates with the true values. Next, applications to vehicle (based on edges) and human/face (based
on intensity images) tracking and verification are demonstrated
with real video data. A facial feature tracking application example is then presented, based on Gabor attributes on a set of
grid points over a face. Finally, we give an interesting application of the algorithm to image sequence stabilization.
is 200 for all the experiments in this
The sample size
paper. In general, using more samples would lead to more accurate approximation to the density, but also increase the compucan provide
tational complexity. We have found that
fairly good results for the experiments reported in this section.
The sequences used in the experiments have a frame resolution
of 240 320 pixels unless specified otherwise.
A. Test on Synthetic Data
In the experiments using synthetic data, we assume that the
sequence contains a car receding in the field of view, with its
motion specified by a 2-D affine transformation. The car (in the
form of its intensity edges) is shown in Fig. 1 (left).
While in real applications, the observations are obtained from
real images through preprocessing techniques, e.g., [6], in this
synthetic test, we simply transformed the template with an affine
transformation at each time and then added independent noise
to each edge pixel. The noise is uniformly distributed on a recentered at the current edge pixel, with
gion
controlling the noise magnitude. In addition, we discard each
edge pixel from the observation with probability , and we
let each background pixel be falsely detected as an edge with
probability . An example of such an artificial “observation”
sequence is shown in Fig. 2 (see also Movie 11). Although this
1Each sequence referred to in this paper has a corresponding video clip at
www.cfar.umd.edu/~baoxin/IP.html.

LI AND CHELLAPPA: GENERIC APPROACH TO SIMULTANEOUS TRACKING AND VERIFICATION IN VIDEO

Fig. 2.

Sample frames of a synthetic sequence (see text), simulating a car driving away (see also Movie 1). The original frame size is 240

535

2 320.

Fig. 3. The posterior densities for x and y translations at frames 7 and 49. The left column is obtained when the template is the true object appearing in the
sequence; the right column is from a false candidate. The crosses indicate the ground truth at the corresponding frames.

may not model the real observation well, it is obvious that the artificial measurements are highly contaminated by non-Gaussian
noise. The prediction error is computed by

if
otherwise
is the edge point set from frame ,
the edge point
where
the number of points in .
set from the model image, and
is a constant and a threshold.
is the
norm. This is
in a sense like an averaged partial Hausdorff distance [17].

With
being the parametrization of the template, we first
from the synthetic sequences.
estimate the posterior
The true and alternative hypotheses (both shown in Fig. 1) are
then both tested against the sequence. Fig. 3 shows an example
of the estimated posterior probability densities for and translation, with the ground truth marked by a cross, for the true and
alternative (false) candidates, respectively. The resemblance of
the two densities results from the fact that the two templates
are very similar. Yet, a close look at the figure shows that the
true template generates more concentrated densities with higher
peaks.
To better show the effectiveness of the algorithm, we plot
], tothree of the estimated motion parameters [i.e.,
gether with the ground truth, in Fig. 4. This figure clearly shows

536

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 11, NO. 5, MAY 2002

Fig. 4. True (dashed) and estimated (solid) parameters versus frame index. Left: obtained from the true hypothesis. Right: obtained from the false hypothesis.
Note that the parameters should be interpreted as   (see text).

0

Fig. 5. Posterior probability in a region specified by 6 centered at the mean. The solid and dashed lines are for the true and false hypotheses respectively. Left: 6 =
[ ; ; ; ; 5; 5] (equivalent to the marginal probability of the translation parameters on a region of size 10 10). Right: 6 = [0:1; 0:1; 0:1; 0:1; 5; 5] .

2

1111

Fig. 6.

Left: sample frames of a sequence (Movie 2). Middle and right: tracking results for true and false hypotheses, respectively, (Movies 3, 4).

that the estimates are closer to the ground truth when the hypothesis is true.
Using the verification algorithm, we compute the posterior
. Fig. 5 shows the
probability around the mean of the state
computed as a function of time . From the figure, it is obvious that the true hypothesis always gives the higher posterior
probability.
B. Vehicle Tracking and Verification
We now present an application to vehicle tracking and verification in real video data, using intensity edges as templates.
With real sequences, one needs first to detect the object and
rectify the template to the scene. As mentioned in Section III,

we assume that this step is done. In our experiments, the algorithm was initialized manually, before invoking the tracking
and verification algorithm. The sequences used were acquired
by a hand-held video camera. Sample frames of a sequence are
shown in Fig. 6 (also Movie 2). In the sequence, the car is receding from the camera, but the camera is subject to zooming
and panning, resulting in complex dynamics. This type of sequence may arise from applications involving a moving camera.
We model the motion using the 2-D affine group, and test the
two hypotheses against the cluttered sequence. The two candidates are from the previous example (Fig. 1), one true and one
false. As an example, Fig. 7 shows samples of the estimated posterior probability densities for the two translation components

LI AND CHELLAPPA: GENERIC APPROACH TO SIMULTANEOUS TRACKING AND VERIFICATION IN VIDEO

Fig. 7.

[1 . . .

Fig. 8.
;

Estimated densities for translation components at frames 7 and 49. Left: true hypothesis. Right: false hypothesis.

1

The posterior probability P for the true (solid) and false (dashed) hypotheses respectively. Left: marginal
; 5; 5] . Right: 6 = [0:1; 0:1; 0:1; 0:1; 5; 5] .

;

537

of the state vector . The computed posterior probabilities are
plotted in Fig. 8. From the figure, it is obvious that the true hypothesis almost always gives the higher posterior probability. If
we also use the posterior probability in a retrospective way (consider up to time ), at any time there is no difficulty in getting
the correct verification result despite the resemblance between
the two candidates.
] onto
In Fig. 6, we overlap the templates [warped by
the original sequence. It is observed that the true hypothesis
matches the scene well, while the false one starts to have trouble
after first few frames (see also Movies 3, 4). Note that the 2-D
affine transform is only an approximation of the vehicle motion
in this real video; therefore, even the true template cannot exactly match the scene.
In this experiment, since edges are used, the prediction error
is again given by (4).

P

in the translation components with

6=

C. Face/Human Tracking and Verification
Verifying faces in video has broad applications such as
surveillance. Unlike the situation for face recognition, surveillance cameras typically have low resolution. Therefore, feature
extraction is difficult, and it is desirable to use the image in a
holistic way. In the following experiments, we use the intensity
face image as the cue for verification. The templates are simply
18 pixels in size) extracted from
face regions (about 20
the sequence (at a time not overlapping with the video clip
used for tracking and verification). The motion is modeled by
the 2-D affine group. A very simple criterion is adopted for
computing the prediction error: the summed absolute difference
(SAD), as used in most block-based compression schemes such
as MPEG. Although more complex methods could be used
to handle situations such as illumination changes, we found

538

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 11, NO. 5, MAY 2002

Fig. 9. Left (Movie 5): sample frames of a sequence. Middle (Movie 6): templates overlaid on the video when the hypotheses are true. Right (Movie 7): when
the hypotheses are false.

1

Fig. 10. The posterior probability P for the hypotheses M1 (solid) and M2 (dashed), with 6 = [
right persons. Right: the templates are verified against the wrong persons.

that SAD is easy to compute, and sufficient for the purpose of
illustrating how the algorithm works. Of course, the algorithm
is capable of using other prediction error criteria.
Fig. 9 (left column) (also Movie 5) shows sample frames of
a sequence with two persons moving around, whose face templates are to be verified from the video. In the middle and right
columns (also Movie 6, 7), we overlap the templates on the
video. For easy visualization, a black block is used for the template corresponding to the face of the man in the white shirt (denoted by M1), and a white block for the template corresponding
to the face of the second man (denoted by M2). The middle
column illustrates the situation where the algorithm is correctly
initialized, meaning that the templates are correctly put on their
respective persons. The figure and movies show that tracking is
maintained all the time for M1, and is able to recover from occlusion for M2. Fig. 10 (left) shows the computed probability
for this case. Note that during the time that M2 is occluded, the
probability drops sharply, while M1 maintains high probability
across all frames.
The right column in Fig. 9 shows an interesting case: we
switch the hypotheses—put the templates on the wrong persons.
It is observed that M2 eventually gets dropped to the cluttered
background, while M1, first sticking to the wrong person, is attracted to the right person after the men meet. Fig. 10 (right)
shows the computed probability for this situation. The curve for
M2 (low probability) conveys a lack of confidence, while the

;

...

;

1

;

5 5]
;

. Left: the templates are verified against the

curve for M1 shows that, after it is attracted to the right person,
the tracker is confident of what it is verifying since the probability is high. It is worth pointing out that during the short period before occlusion occurs, M1 also gives a high probability
even though the tracker is on the wrong person. The reason is
that during that period, M1 is tracking the face against a clean
background (the wall); thus the high probability reflects the notion that the tracker would rather stick to the wrong face than
move to the background since the wrong face is at least more
face-like than the background. Note that this does not affect verifying which person is M1, since the probability is still lower
than when M1 is on the right person [given in Fig. 10 (left)].
This sequence can be thought of as a benign situation except for the occlusion part since the motion is mainly translation
and the video quality is good. We now experiment with a challenging sequence of fair visual quality where the motion has
large scaling components. Sample frames of the sequence are
shown in Fig. 11 (also Movie 8). The task is to verify the man
carrying a suitcase as one of two templates (extracted from the
two men walking in the middle at a later time in the sequence).
Fig. 12 shows an example of the estimated densities of
and
. The computed probability is plotted in Fig. 13. Note
that although there are several difficult frames, the algorithm is
able to pick the true hypothesis based on the cumulative probability. In the middle and right columns in Fig. 11 (also Movies 9,
10), the template (shown as a white block for easy visualization)

LI AND CHELLAPPA: GENERIC APPROACH TO SIMULTANEOUS TRACKING AND VERIFICATION IN VIDEO

Fig. 11.

539

Left (Movie 8): sample frames of a sequence. Middle and Right: tracking results for true and false hypotheses, respectively (Movies 9, 10).

1

1

Fig. 12. Posterior densities for a , and a at frames 4 and 24. Left: true hypothesis. Right: false hypothesis. Note that, with the false hypothesis, the
densities have not only bad shapes, but also wrong mean values, resulting in the wrong shape and location of the template in Fig. 11 (right).

is overlaid on the sequence. It is obvious that when the hypothesis is true, the algorithm approximately estimates the scaling
of the template, while under the false hypothesis, even though
the block is deformed so greatly (indicating the competition in
trying to verify a person against the wrong template), the template eventually gets stuck in the cluttered background.
Real Application Example: Finally, we use an example to illustrate how the algorithm can be used in surveillance applications. In this example, we assume that we have two cameras at
the back and front doors of a building. The camera at the back
door captures a person entering the building; sample frames of
this sequence are shown in Fig. 14. Later, the camera at the front
door finds that two persons are leaving the building, and we want

to know which is the person who entered the building a few
moments ago. We assume that the cue for tracking and verification is an intensity template, detected by the first camera (for
example using background subtraction), as shown in Fig. 14.
Notice that in this example, part of the torso is included in the
template, since in the video from the front-door camera the faces
are too small to support verification (as may be the case in many
surveillance applications).
Fig. 15 shows the results of tracking, where we have overlaid
the template onto the frames. The left column is the original
sequence (Movie 11), and the middle one (Movie 12) is the result when we put the template onto the right person. The right
column (Movie 13) is the result when we put the template onto

540

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 11, NO. 5, MAY 2002

will be getting larger. This type of information can be used
to choose an appropriate dynamic model.
• As pointed out in Section IV-C, although in the experiments we only use posterior probabilities for verification, prediction error is another type of information that
is helpful for verification. This type of information is important especially when the candidates are very similar.
For instance, in this example, since the sizes of the persons are small, and both of them wear white T-shirts and
dark pants, more detailed comparison involving prediction
error may be required.
D. Facial Feature Tracking for Verification
Fig. 13. Posterior probability P for the true (solid) and false (dashed)
hypotheses respectively, with 6 = [0:3; 0:3; 0:3; 0:3; 4; 4] .

the wrong person. Notice that the samples are drawn from a population. When this population covers large and translations
(in this example, the population is a zero-mean Gaussian with
variance four pixels for the translations), it is found that even
though we put the template onto the wrong person, it is eventually attracted to the right person. Thus verification is easily
achieved. If we use a population with smaller variances in the
translation components (in this example, two pixels), we obtain
the results given in Fig. 16, where the tracker sticks to the wrong
person. However, we find that the computed posterior probability gives correct verification results as before. The quality of
the tracking is best viewed using the accompanying movies.
Some comments are helpful in understanding this example
well.
• We are solving a “verification” problem; therefore the algorithm only tells us which of the two persons is more
likely the person who entered. It does not judge whether
either of the two persons “is” the person who entered
(which is a recognition problem).
• This example is a challenging one since the template and
the video to verify against are obtained under different
lighting conditions (the former is in shadow, the latter
in bright sunshine). Thus SAD may not be a good error
metric. Even though we use this simple error measure here
for illustration purposes, it is unquestionable that a better
error metric should be used for robustness and accuracy in
real applications.
• We only use image intensity as the cue, but color information can be easily incorporated into the algorithm. For example, one can use color templates instead of the grayscale
templates.
• Other variations of this example are also of practical interest. For example, when two persons enter the building,
and later one person leaves, the system may be tasked to
find who has left. This problem can be solved similarly.
• Although we only use simple dynamics in our experiments, in specific applications, we could do better by using
a priori information about the problem. For example, in
the problem illustrated here, if the cameras are fixed, we
know that people leaving the building will be approaching
the camera; thus the image of each person in a sequence

Facial feature tracking is useful in applications such as facial
expression analysis, face-based person authentication, videobased face recognition, etc. When a short video sequence is
available, the face recognition problem can be solved better if
temporal information is properly exploited. For example, videobased segmentation can be used to help in face detection, as in
[18], [34], [36]. Our algorithm, when applied to facial feature
tracking, provides not only tracking but also verification results.
This additional gain comes from the fact that one can choose
s for different purposes. If
corresponds to a feadifferent
ture set extracted from the first frame of a sequence, then the
reprealgorithm is suitable for pure tracking. However, if
sents some templates from a candidate list, then the algorithm
is naturally good for tracking-for-verification: When a template
and the sequence belong to the same person, the tracking results should reflect a coherent motion induced by the same underlying shape; whereas, a more random motion pattern would
be observed when a template and the sequence belong to different persons. This idea becomes clear from the following experiments.
Fig. 17 illustrates the facial feature tracking experiments (see
also Movies 16, 17). The features to be tracked are defined as
a set of Gabor attributes (jets) on a grid. The main motivation
behind this is the successful application of Gabor filters to face
recognition (e.g., [29], [24], [31]). The motion of facial feature
points is modeled by a global 2-D affine transformation (accounting for head motion) plus a local deformation (accounting
for residual motion due to inaccuracy of the 2-D affine model
and other factors such as facial expressions). Motion of both
types is estimated simultaneously by the tracker: global motion
is tracked by importance sampling, and residual motion is handled by incorporating local deformation into the measurement
likelihood in computing the prediction error. Due to space limit,
we refer the readers to [26] for a more complete presentation on
facial feature tracking using the proposed method.
E. Image Sequence Stabilization
We now describe a simple experiment showing how the proposed algorithm can be applied to image sequence stabilization
(also known as sensor motion compensation). Recall that the algorithm tracks an object through estimating the density of a state
vector. If we model the camera motion by a certain transformation group, and use the transformation parameter as the state
vector, then motion compensation is solved after the tracking is
done. In this experiment, we model the camera motion as a 2-D

LI AND CHELLAPPA: GENERIC APPROACH TO SIMULTANEOUS TRACKING AND VERIFICATION IN VIDEO

Fig. 14.

541

Left: Sample frames from the video taken by the back-door camera. Right: Extracted template.

Fig. 15. Tracking and verification results. Left (Movie 11): sample frames of a sequence. Middle (Movie 12): tracking result when the template is put on the right
person. Right (Movie 13): tracking result when the template is put on the wrong person. Note that in this situation, the tracker is attracted to the right person even
if it is put on the wrong person, because a sampling population with large variance in the translational components is used. For easier visualization, we use white
boundaries to highlight the templates.

Fig. 16. Left (Movie 11): sample frames of a sequence. Middle (Movie 14): results when the template is put on the right person. Right (Movie 15): results when
the template is put on the wrong person. Note that in this situation, the tracker is forced to stick to the wrong person by using a sampling population with small
variance in the translational components. For easier visualization, we use white boundaries to highlight the templates.

affine transformation. Stabilization is then achieved by the following procedure. First, we extract a set of feature points from
the first frame (used as reference frame). Next, we use the algorithm to track the set of feature points in the sequence. Stabilization is then done by warping the current frame with respect to the
reference frame using the estimated motion parameter (obtained
by evaluating the mean value of the state vector). In this experiment, SAD was again used to measure the prediction error. The
SAD was computed on a 5 5 block centered at each feature
point.

Fig. 18 illustrates the stabilization results. The top row shows
two consecutive frames from the original sequence, with the
tracked feature points highlighted. The feature points were detected using a public-domain feature detector [3] based on the
KLT algorithm [33]. The bottom row shows the frame differences before and after stabilization. For a better visual effect,
see Movie 18 for the original sequence and Movie 19 for the
sequence after stabilization.
The stabilization example given here is intended only to illustrate how the proposed algorithm can be applied to tasks

542

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 11, NO. 5, MAY 2002

Fig. 17.

Tracking results when the template and video belong to the same person (top) and different persons (bottom). See also Movies 16 and 17.

Fig. 18. Image sequence stabilization example. Top: two frames from the original sequence with tracked feature points highlighted. Bottom: frame difference
before (left) and after (right) stabilization (gray level re-scaled for easy visualization). See also Movies 18 and 19.

other than tracking, and the resulting stabilization algorithm still
has much room for improvement (for example, in the previous
experiment, a perspective transformation might be better than
2-D affine, since there is significant 3-D depth variation in the
scene), which is not within the scope of this paper. It is also
worth pointing out that this solution to stabilization has an important advantage: no feature correspondence is required.
V. RELATED WORK
There has been extensive work in the literature on visual
tracking, e.g., [2], [4], [9], [16], [20], [37], [38]. We can only
give a few examples here. Reference [20] proposed a contour
tracking algorithm using a random sampling method; this
was one of the first efforts to use sampling methods in visual
tracking, and the approach can be treated as a special realization of the SIS algorithm. In terms of tracking, our approach
has extended beyond contour tracking, compared with the
CONDENSATION algorithm. Of course, a more important
aspect is that our approach solves verification in addition to
tracking. Also, by using the general formulation in this paper,
it is straightforward to incorporate other mature statistical
techniques into tracking/verification algorithms. For example,

one can easily extend our algorithm by using the generic Monte
Carlo algorithm proposed in [27]. Meanwhile, the general
formulation makes it obvious to apply the algorithm to other
problems such as image sequence stabilization, structure from
motion, etc.
Reference [16] proposed a tracking approach based on edge
matching. Contours are in a sense features at a lower level compared with intensity images, but at a higher level compared with
edges. Thus with contours, efficient computation is easier to
achieve. On the other hand, the approach in [16] has the virtue
that virtually no model is needed; the algorithm simply works
on the sequence (after extracting edges) through matching based
on the Hausdorff measure [17]. An eigenspace-based approach
was presented in [4] which can directly handle intensity images.
To achieve tracking, a training process is needed to construct the
eigenspace. In terms of tracking, the proposed approach differs
from the previous methods in that it can handle edges, contours,
intensity images, or other representations in a uniform way.
There are fewer reports on verification from video. In fact, we
are not aware of any other work that explicitly attempts to solve
the verification problem in addition to tracking. In aforementioned face recognition/verification work, none of the trackers

LI AND CHELLAPPA: GENERIC APPROACH TO SIMULTANEOUS TRACKING AND VERIFICATION IN VIDEO

can give verification results as our algorithm does, although we
believe that some of them handle the tracking part well. This is
the major feature that makes our algorithm different from pure
tracking methods.
There is no doubt that verification is an important aspect
of video-related applications. When verification is the concern,
contours alone are in general insufficient. While edges may be
enough for distinguishing, for example, different type of vehicles, they are not expected to work well for verifying more
complex objects such as faces. In such cases, we may have to
go back to the intensity images (or equivalent transform domain
representations). These different representations can be handled
uniformly by our algorithm to give tracking plus verification results.
Image sequence stabilization is another well-researched
problem (e.g., [19], [5], [15], [30], [8], [35]). Generally
speaking, image stabilization algorithms can be roughly
classified into two categories: feature-based (e.g., [30]) and
optical-flow-based (e.g., [8], [35]) approaches. Feature-based
approaches depend on the correspondence of feature points
between two frames, while optical flow based approaches try
to fit the extracted flow to a motion model. Since the flow
field is estimated from the whole image, optical flow based
approaches tend to be more robust. The use of our algorithm for
stabilization is an example of a feature-based approach. But it
differs from most feature-based methods such as [30] in that it
does not require feature detection except in the first (reference)
frame, and thus no feature correspondence is required. The
correspondence between features is effectively established by
the sampling process: good candidates for correspondence are
enforced through importance sampling, and unlikely correspondences are filtered out. Since no feature correspondence
is needed, one can increase the number of features to gain
robustness, and the only increase in complexity is due to the
computation of prediction error (with other feature-based
methods, this would cause significant difficulty and complexity
in solving for correspondences).
VI. DISCUSSION AND CONCLUSIONS
This paper presents a generic simultaneous tracking and
verification algorithm based on sequential Monte Carlo sampling methods. With this algorithm, applications using edges,
contours, intensity images, or other suitable representations
can all be unified under the same framework. Experiments on
synthetic and real data have been presented. Applications to
vehicle/human(face) tracking and verification, facial feature
tracking for verification, and image sequence stabilization
are illustrated with experiments. The results suggest that the
algorithm provides a promising approach to stochastic tracking
and verification.
For easier appreciation of the dynamic character of the examples in this paper, a World Wide Web (WWW) site has been
established where the sequences referred to in this paper can be
watched as MPEG movies.
It is worth pointing out that although in the examples in this
paper the motion model is assumed to be 2-D, it need not be stationary in time, meaning that the motion can vary from frame to

543

frame. Thus, this model can describe complex dynamics, such
as the one shown in Fig. 6 (Movie 2). Besides, we did not assume any specific system dynamics, other than a simple firstorder Gauss–Markov model. In some applications, the system
dynamics are known, or can be learned from sample data. In
these situations, incorporating the system dynamics should yield
better performance than using the simple Gauss–Markov model.
A problem remaining to be solved is dealing with more complex motions. Recall that we deduced the re-parametrization
based on the small motion assumption through a Taylor series
expansion. Even though (2) is general, it represents a parametric
motion model. It may be difficult to obtain a parametric model
for complex motions such as those involving 3-D rotation of the
object. Fortunately, the parametrization is independent of the algorithm in the sense that the algorithm is applicable as along as
a low-dimensional dynamic system is assumed.
The time complexity of the algorithm is another issue that
needs to be addressed and that has not been fully investigated
yet. However, our experiments suggest that real-time implementation would not be difficult. To give a rough idea about the
speed of the algorithm, in the face verification example illustrated in Fig. 11, the algorithm operated at 3 frames/s on an
Ultra 5 Sparc workstation for a single hypothesis. The algorithm was able to achieve 4 frames/s on the same workstation
for the stabilization example, where image size was 237 348.
Since the algorithm has not yet been integrated into a real-time
system, the time consumed includes overhead such as reading
and writing image files from hard disk, etc. Also, no code optimization of any sort has been performed yet. Therefore, we
believe that real-time implementation is possible. Further investigation is needed for accurate characterization of the algorithm’s speed performance. Future work also includes efficient
real-time implementation of the algorithm.
REFERENCES
[1] B. Anderson and J. Moore, Optimal Filtering. Englewood Cliffs, NJ:
Prentice-Hall, 1979.
[2] A. Baumberg and D. Hogg, “An efficient method for contour tracking
using active shape models,” in Proc. IEEE Workshop on Motion of NonRigid and Articulated Objects, 1994, pp. 194–199.
[3] S. Birchfield. An implementation of the Kanade–Lucas–Tomasi feature
tracker. [Online]. Available: http://www.vision.stanford.edu/birch/klt.
[4] M. Black and A. Jepson, “EigenTracking: Robust matching and tracking
of articulated objects using a view-based representation,” Int. J. Comput.
Vis., vol. 20, pp. 63–84, 1998.
[5] P. Burt and P. Anandan, “Image stabilization by registration to a reference mosaic,” in Proc. DARPA Image Understanding Workshop, 1994,
pp. 425–434.
[6] J. Canny, “A computational approach to edge detection,” IEEE Trans.
Pattern Anal. Machine Intell., vol. 8, pp. 679–698, 1986.
[7] B. P. Carlin, N. Polson, and D. Stoffer, “A Monte Carlo approach to
nonnormal and nonlinear state-space modeling,” J. Amer. Statist. Assoc.,
vol. 87, pp. 439–500, 1992.
[8] Z. Duric and A. Rosenfeld, “Stabilization of image sequences,” Univ.
Maryland, College Park, Tech. Rep. CAR-TR-778, 1995.
[9] D. Freedman and M. Brandstein, “A subset approach to contour tracking
in clutter,” in Proc. Int. Conf. Computer Vision, 1999.
[10] A. E. Gelfand and A. F. M. Smith, “Sampling-based approaches to calculation marginal densities,” J. Amer. Statist. Assoc., vol. 85, pp. 398–409,
1990.
[11] C. J. Geyer, “Conditioning in Markov chain Monte Carlo,” J. Comput.
Graph. Statist., vol. 4, pp. 140–154, 1995.
[12] W. R. Gilks, G. O. Roberts, and E. I. George, “Adaptive direction sampling,” Statistician, vol. 43, pp. 179–189, 1994.

544

[13] N. Gordon, D. Salmond, and A. Smith, “Novel approach to nonlinear/non-Gaussian Bayesian state estimation,” in Proc. IEE Radar
Signal Processing, vol. 140, 1993, pp. 107–113.
[14] J. Hammersley and D. Handscomb, Monte Carlo Methods. New York:
Wiley, 1964.
[15] M. Hansen, P. Anandan, K. Dana, G. van der Wal, and P. Burt,
“Real-time scene stabilization and mosaic construction,” in Proc. ARPA
Image Understanding Workshop, 1994, pp. 457–465.
[16] D. Huttenlocher, J. Noh, and W. Rucklidge, “Tracking nonrigid objects
in complex scenes,” in Proc. Int. Conf. Computer Vision, 1993.
[17] F. Hausdorff, Set Theory, 3rd ed. New York: Chelsea, 1978.
[18] A. J. Howell and H. Buxton, “Toward unconstrained face recognition
from image sequences,” in Proc. Int. Conf. Automatic Face and Gesture
Recognition, 1996, pp. 224–229.
[19] M. Irani, B. Rousso, and S. Peleg, “Recovery of ego-motion using image
stabilization,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 1994, pp. 454–460.
[20] M. Isard and A. Blake, “Contour tracking by stochastic propagation of
conditional density,” in Proc. Eur. Conf. Computer Vision, 1996.
[21] M. H. Kalos and P. A. Whitlock, Monte Carlo Methods. New York:
Wiley, 1986.
[22] G. Kitagawa, “Monte Carlo filter and smoother for non-Gaussian nonlinear state space models,” J. Comput. Graph. Statist., vol. 5, pp. 1–25,
1996.
[23] A. Kong, J. Liu, and W. Wong, “Sequential imputations and Bayesian
missing data problems,” J. Amer. Statist. Assoc., vol. 89, pp. 278–288,
1994.
[24] M. Lades, J. Vorbruggen, J. Buhmann, J. Lange, C. v. d. Malsburg, and
W. Konen, “Distortion invariant object recognition in the dynamic link
architecture,” IEEE Trans. Comput., vol. 42, pp. 300–311, 1993.
[25] B. Li and R. Chellappa, “Simultaneous tracking and verification via sequential Monte Carlo methods,” in Proc. IEEE Conf. Computer Vision
Pattern Recognit., 2000.
, “Face verification through tracking facial features,” J. Opt. Soc.
[26]
Amer. A, vol. 18, pp. 2969–2981, 2001.
[27] J. Liu and R. Chen, “Sequential Monte Carlo methods for dynamic systems,” J. Amer. Statist. Assoc., vol. 93, pp. 1031–1041, 1998.
[28] C. J. Masreliez, “Approximate non-Gaussian filtering with linear state
and observation relations,” IEEE Trans. Automat. Contr., vol. AC-20,
pp. 107–110, 1975.
[29] B. S. Manjunath, R. Chellappa, and C. v. d. Malsburg, “A feature based
approach to face recognition,” in Proc. IEEE Conf. Computer Vision
Pattern Recognition, 1992, pp. 373–378.
[30] C. Morimoto and R. Chellappa, “Fast electronic digital image stabilization,” in Proc. Int. Conf. Pattern Recognit., 1996, pp. 284–288.
[31] K. Okada, J. Steffens, T. Maurer, H. Hong, E. Elagin, H. Neven, and C.
v. d. Malsburg, “The Bochum/USC face recognition system,” in Face
Recognition: From Theory to Applications, H. Wechsler, P. J. Phillips,
V. Bruce, F. Soulie, and T. Huang, Eds. Berlin, Germany: SpringerVerlag, 1998.
[32] G. O. Roberts and A. F. M. Smith, “Simple conditions for the convergence of the Gibbs sampler and Hastings–Metropolis algorithm,”
Stochast. Proc. Applicat., vol. 49, pp. 207–216, 1994.
[33] J. Shi and C. Tomasi, “Good features to track,” in Proc. IEEE Conf.
Computer Vision and Pattern Recognition, 1994.
[34] M. Seibert and A. M. Waxman, “Combining evidence from multiple
views of 3-D objects,” Proc. SPIE, vol. 1611, 1991.
[35] S. Srinivasan and R. Chellappa, “Image stabilization and mosaicking
using the overlapped basis optical flow field,” in Proc. IEEE Int. Conf.
Image Processing, 1997.
[36] M. Turk and A. Pentland, “Eigenfaces for recognition,” J. Cogn. Neurosci., vol. 3, pp. 72–86, 1991.
[37] C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland, “Pfinder:
Real-time tracking of the human body,” IEEE Trans. Pattern Anal.
Machine Intell., vol. 19, pp. 780–785, 1997.
[38] Y. Yacoob and L. Davis, “Tracking rigid motion using a compact-structure constraint,” in Proc. Int. Conf. Comput. Vis., 1999.
[39] S. Zeger and M. R. Karim, “Generalized linear models with random
effects: A Gibbs sampling approach,” J. Amer. Statist. Assoc., vol. 86,
pp. 79–86, 1991.

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 11, NO. 5, MAY 2002

Baoxin Li (S’97–M’00) received the B.S. and M.S.
degrees in electrical engineering from the University
of Science and Technology of China in 1992 and
1995, respectively. He received the Ph.D. degree
in electrical engineering from the University of
Maryland, College Park, in 2000.
He is currently with Sharp Laboratories of
America, Inc., Camas, WA, working on multimedia
analysis for consumer applications. He was previously with the Center for Automation Research,
University of Maryland, working on face and object
tracking and verification in video, automatic target recognition, and neural
networks. His interests include pattern recognition, computer vision, neural
networks, and multimedia processing.

Rama
Chellappa
(S’78–M’79–SM’83–F’92)
received the B.E. (Hons.) degree from the University
of Madras, India, in 1975 and the M.E. (Distinction) degree from the Indian Institute of Science,
Bangalore, in 1977. He received the M.S.E.E. and
Ph.D. Degrees in electrical engineering from Purdue
University, West Lafayette, IN, in 1978 and 1981,
respectively.
Since 1991, he has been a Professor of electrical
engineering and an Affiliate Professor of computer
science at the University of Maryland, College Park.
He is also affiliated with the Center for Automation Research (Director) and the
Institute for Advanced Computer Studies (Permanent Member). Prior to joining
the University of Maryland, he was an Assistant (1981-1986) and Associate
Professor (1986-1991) and Director of the Signal and Image Processing Institute (1988-1990) with the University of Southern California, Los Angeles. Over
the last 21 years, he has published numerous book chapters and peer-reviewed
journal and conference papers. He has edited a collection of papers on digital
image processing (published by IEEE Computer Society Press), co-authored a
research monograph on artificial neural networks for computer vision (with Y. T.
Zhou) published by Springer-Verlag, and co-edited a book on Markov random
fields (with A. K. Jain) published by Academic Press. His current research interests are face and gait analysis, 3-D modeling from video, automatic target
recognition from stationary and moving platforms, surveillance and monitoring,
hyperspectral processing, image understanding, and commercial applications of
image processing and understanding. He was co-Editor-in-Chief of Graphical
Models and Image Processing.
Dr. Chellappa has served as an associate editor of the IEEE TRANSACTIONS
ON SIGNAL PROCESSING, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND
MACHINE INTELLIGENCE, IEEE TRANSACTIONS ON IMAGE PROCESSING,
and IEEE TRANSACTIONS ON NEURAL NETWORKS. He is now serving as
the Editor-in-Chief of the IEEE TRANSACTIONS ON PATTERN ANALYSIS
AND MACHINE INTELLIGENCE. He served as a member of the IEEE Signal
Processing Society Board of Governors during 1996-1999. Currently, he is
serving as the Vice President of Awards and Membership for the IEEE Signal
Processing Society. He has received several awards, including NSF Presidential
Young Investigator Award, an IBM Faculty Development Award, the 1990
Excellence in Teaching Award from School of Engineering at USC, and the
1992 Best Industry Related Paper Award from the International Association of
Pattern Recognition (with Q. Zheng), the 2000 Technical Achievement Award
from the IEEE Signal Processing Society. He was elected as a Distinguished
Faculty Research Fellow (1996-1998) at the University of Maryland. He is a
Fellow of the International Association for Pattern Recognition. He has served
as a General and Technical Program Chair for Server IEEE international and
national conferences and workshops.

BAYESIAN METHODS FOR FACE RECOGNITION FROM VIDEO
Rama Chellappa. Shaohua Zhou

Baoxin Li

*

Center for Automation Research

Sharp Laboratories of America

EE Department, University of Maryland
College Park, MD 20742

5750 NW Pacific Rim Blvd.
Camas, WA 98607

ABSTRACT
Face recognition (FR) from video necessi tates sim ultaneously solv­
ing two 'ask s , recognit ion and tracking .

To accommodate the video,
time series state space model is introduced in a Bayesian ap­
proach. Given this model, the goal reduces to estimating the pos­
terior distribution of the state vector given the observations up to
the present. The Sequential Importance Sampling (SIS) technique
a

is

denoted by 7I"t{xe) pt(Xtlyo:d with YO:t = {Yo. Yl, ... , Ytl·
T he SIS technique can be invoked to generate a numerical solu­
tion. Ultimately, we need to estimate the posterior distribution of
the identity, 7I"t(nt) pt(ntlyo:t). where nt is the human identity
variable at time t.
Presented here are two methods' for approximating the dis­
tribution 7I"t(nt) under different experimental scenarios but same
still-to-video setup. Method I [5] parameterizes the model with
only an affine tracking state, denQted by 9t, and ap proximates
and propagates 7I"t(9t) using the SIS algorithm. The distribution
7I",(n,) is estimated by marginalizing 7I"t(8,) over a proper affine
region around the posterior mean E.-(xt). Method II [6] param­
eterizes the model with the affine tracking state 8t and the recog­
nizing identity variable nt, approximates and propagates the joint
distribution 1I"t(9t, nt } using the SIS algorithm. The distribution
7I"t(nt) is a free estimate from 7I"t(8" nt}, i.e., the true marginal
dis tri bution of 71". (8" nt).
Section 2 introduces a general time series state space model
and briefly reviews the SIS algorithm that approximates its so­
lution. Sections 3 and 4 respectively describe the experimental
scenarios and presents the two aforementioned methods and their
results. Section 5 concludes the paper.
t,

invoked to generate a numerical solution to this model. How­

goal is to estimate the posterior distribution of
identity of humans for recognition purposes. Presented here
are two methods to approximate the above distribution under dif ­
ever, the ultimate

the

ferent experimental scenarios.
I. INTRODUCTION

Bayesian analysis of video has recently gained significant attention
in the computer vision comm unity since the seminal work by Isard
and Blake [I]. In their effort to solve the problem of visual track­
ing, they introd uced a time series state space model parameterized
by a tracking state vector (e.g. affine parameters) and developed
the CONDENSATION algorithm to provide a numerical approxima­
tion to the p osterior distribution of the s tate vector, and to propa­
gate it over time according to the state equation. This has been
extended to many areas [2, 3], including face recognition [4, 5, 6].
Refer to [7, 8) for surveys and [9) for experiments on face rec o g­
niti on.
Experiments reported in [9] evaluate still-to-still scenarios, where
the gallery and the probe consist of both still facial images. Some
well-known s till-to-sti ll FR approaches include Principal Compo­
nent Analysis (PCA) [10), Linear Discriminant Analysis (LOA)
[11,12], and Elastic Graph Matching (EGM)[13]. Typically, recog­
nition is performed based on an abstract representation of an im­
age after suitable geometric and photometric normalizations are

=

=

2. SIS ALGORITHM
A general t ime series state space model consists of the following
three components:
I. State ·equation governing the state evolution:

(I)
where Ut is the state noise and 91("') the state e volvi ng function.
Denote the state transition probability as Pt(xtlXt- d.
2. Observation equation depicting the observational behavior:

performed.

we define the gallery and probe as follows: the
gallery consists of still facial templates and and the probe consists
of video sequences containing the facial region . There are many
instances where still -to-video algorithms are useful. Denote the
gallery set as H
{It, h . . . , IN}, indexed by the identity vari­
able n, which.lies in a finite sample spaceJV = {I, 2, ... , N}.
. We also adopt the time series state space model to characterize
the evolving dy namics or/and identity in the probe video. Let Xt
be the state vector and Yt be the observation respectively at time
t. Given this model, the goal reduces to computing the posterior
distribution of the state vector given the observations up· to time

(2)

Following [9],

where Vt is the observation noise and ht ( ., .) the observation func­

tion. Denote the likelihood as Pt(ytlxt}.
3. Prior probability pu(xu) and statistical independence:

=

•

Partially supported by the DARPA Grant NOOO 14-00-\-0908.

0-7803-7402-9/02/$17.00 ©20021EEE

Ut J..

u., Vt

J.. v.; t, 82: 1 &

t '" 8.

(3)

Using this model, we attempt to compute the filtering poste­
rior probability 7I"t( X t)
p(xtlyo:t}. If tL model is linear with
Gaussian noise, it is analytically solvable by a Kalman filter which
essentially propagates the mean and variance of a Gaussian dis­
tribution over time. For nonlinear and non-Gaussian cases, an ex­
tended Kalman filter (EKF) is proposed to arrive at an approximate

IV -4068

=

analytic solution. Recently, the SIS technique, a special case of
Monte Carlo method, [I, 14, IS, 16] bas been used to provide a
numerical solution and to propagate an arbitrary distribution over
time.

The essence of Monte Carlo method is to represent an arbitrary
probability distribution 7I"(x} closely by a set of discrete samples.

It is ideal to draw i.i.d. samples {x(m)}�=1 from 7I"(x). However
it is often difficult to implement, especially for non-trivial distri­
butions. Instead, a set of samples {x(m)}�=1 is drawn from an
importance function 9(X} which is easy to sample from, then a
weight

(4)
is assigned to each sample. This technique is called Importance
Sampling (IS). It can be shown[16] that the importance sample set
8 = {(X(m) , w(m»}�=1 is properly weighted to the target dis­
tribution 7I"(x}. To accommodate a video, importance sampling

is used in a sequential fashion, which leads to SIS. SIS propagates
8t-1 according to thesequentialimportance!unctiongt(xtlxt_l},
and calculates the weight using

For a complete description of the SIS method, refer to

measurement. The likelihood is assumed to be time-invariant and
modeled as a truncated Gaussian:

p(Yt18t)

=

{

(v'21rl7o)-lexp{-(et}2/(217�)}

K

if letl � 6
otherwlSe,

(6)

where 6 is a threshold and K a constant. The error et is computed
as
et

=

1

NJ

�

JM)·

J!i)

IIJM)II'IIJ�J)II,

(7)

where NJ is the number of jets, JM) the jet for the j-th grid point

in the template

and J!J) its counterpart in the current frame. It

needs to be emphasized that J!il is found by first applying to the
grid the affine motion with 8, followed by a local search.

Using SIS, the tracking problem can be numerically solved by
approximating 7I"t(8t}. For pure tracking, we let the template be
the facial part in the first frame. Fig. 2 shows some pure tracking
results. For both tracking and recognition, we use templates in the
gallery set. In order to evaluate 7I"t ( nt = n} for template n in the
gallery, we first invoke SIS to obtain 71",(8,), then compute it as
follows:
7I"t(nt

[14,16].

=

n

)

=

i

7I"t(8t}d8t,

(8)

where A is a proper region interval around the posterior mean
E,,(O,}. The complete algorithm I is summarized below.

3. METHODI
This method[S] bas been tested on a database containing 19 sub­
jects. In building the database, each person was asked to sit on a
chair at a fixed distance from the camera so that the scale was ap­
proximately the same for all persons, and to move hislber head and
make any desired facial expression, simulating an automatic teller
machine or access control scenario. Fig. I shows some sample
frames from a probe video and some templates in the gallery.

Algorithm I
Initialization: Rectify the template

grid

onto the first frame

using EGM. Draw M random samples from po(Oo).
Tracking and Recognition:

Tracking: at time t > 0, invoke the SIS algorithm to obtain
an updated set of samples for 11",(9,). To compute the likelihood

of each sample, a local search around each node is performed to
account for the deformation before computing the m atching error.

Recognition and Mean Shape Evaluation: at any time t >
E,,(O,), plus a local search;
and a final matching score is calculated using the mean shape; the
posterior probability is computed in an interval around E,,(8t).

0, the tracked set of jets is given by

Fig.

Fig. 1. I. Sample frames from the video (top) and templates (bot­
tom). The templates will be referred to as face 1, face 2, ... , etc.,
counting from left, and obviously face 3 in the bottom row is the

2 shows pure tracking results with tracked grid points su­

perimposed on the image. Even under difficult situations as shown
in Fig. 2, tracking is successfully maintained. Fig. 3 shows the pos­
terior probability 7r, ( n, ) and the matching scores computed using
the mean shape.

true hypothesis.

The state vector Xt is taken to be affine tracking parameter 8t,
which obeys a first-order Gaussian-Markov model, i.e., the transi­
tion probability, assumed time-invariantp(8tI8t_1), obeys a Gaus­
sian distribution. In addition, a local deformation is introduced to
account for the residual motion due to inaccuracies in affine mod­
eling and other factors such as facial expressions. The observation
'Yt is taken to be Gabor-filtered jets [13] defined on a sparse grid,
shown in Fig. 2. Note that it is the grid in the template image
that undergoes the affine motion and local deformation. The local
deformation is implemented by performing a local search around
each grid point for its best match when Updating the likelihood

Fig. 2. I. Tracking results. Note the rotation in depth in the upper
row and the large in-plane rotation in the lower row.

IV -4069

:

.
...... ......
:............. , .. . ,
.

.

...
.. \ ..

..

Fig. 3. I. Posterior probabilities and matching scores. Solid line is

from the true hypothesis (face 3 in Fig.2). Dashed lines are from
face 1 (left) and face 5 (right) corresponding to wrong hypotheses,
respectively.

4. METHODII
This method[6] has been tested on a database collected

as

part of

the HumanID project by by National Institute of Standards and
Technology and University of South Florida researchers. It con­

Fig. 5. II. Example frames in one probe video. The image size
is 720x480 while the actual face size ranges approximately from
20x20 in the first frame to 60x60 in the last frame.

or eigenfaces (see Fig. 4 for the top 10 eigenfaces) and it is mod­
eled as a transformed, noise-corrupted ver si on of some template
in the gallery, i.e.,I(YI,8t) = In. + VI, where 1(.,.) is time­
invariant image transformation function. Assume the likelihood to
be a ti me-invariant truncated Laplacian.

tain s 30 subjects walking towards a camera in order to simulate
typical scenarios in visual surveillance. There are 30 subjects, each
having one face template in the gallery and one video in the probe.
The complete face gallery is shown in

Fig.

4. Fig. 5 gives some

example frames in one probe video. Note that the gallery was cap­

under different lighting circumstances from the probe and
face in the probe is of low resolution, small size, and con­
siderable scale ch ang e .
tured

that the

P(Yt 18 t,nt)

-

_

{ �-l exp(-lIvdl/�)
K

if IIv,1I ::;; 6
otherwise,

(9)

where 6 is a threshold and K a constant.
By em ploying the SIS tec hniq ue, the joint distribution of the
state vector and the identity variable, 'll"t(9t, nt), is estimated at
current time and then propagated to the next, govemed by the
evolving equations for the state vector and the identity variable.
The posterior distribution of the identity variable, .... (n.), is just a
free estimate, i.e., the marginal of'll"t(8t, nt}. Algorithm II is sum­
marized below. We have worked with t wo versions of Algorithm

II. Algorithm IIa is a brute-force implementation; Algorithms lIb
is a more efficient

imp lem entat ion.

Details are in [6].

Algorith m II
Initialization: Draw M random samples jointly from po(8o)
and the uniform prior po(na) ..
Tracking and Recognition:
Tracking: at time t > 0, invoke the SIS algorithm to obtain an

Fig. 4. II. The face g a llery (upper) with image size 48x42. The top
\0 eigenfaces (l ower).
.

updat ed set of sa mp le s for '11",(81, nl)'
Recognition: a t any time t > 0, m arginal izin g 'll"t(9t, nt}
over 8t g ive s rise to 'll"t(nt). Conditional entropy H(ntlyo:t) and
MMSE estimate of Be are computed accordingly.

The time series state space model is now parameterized by
both a ffi ne track ing parameters and identity variable, respectively
characterizing the dynamics and identity of human, i.e., Xt
(8t• nt). So, pt(xdxt-d pt(8tI81-I}pt(ntlnt-d. We assume
thatpt(8tlfh-l) is a t ime-invariant Gaussian, and that there is tem­
poral invariance in the identity, i.e., pt(ntlnt-d i(nt - nt-I),
wh ere i(.) is an indicator function. The observation YI is taken
to be a reconstructed image from top 300 principal components
=

=

=

IV

Fig. 6 pr esents the plot of the posterior prob abi lity '11"1 (nt)
against frame instance for probe video shown in Fig. S. Suppose
that the correct identity is c. From Fig. 6, we ca,n easily observe
that the posterior probability '11"1 (e) increases as time proceeds and
eventually approaches I, and all others 'll"1 ( n, i: e) go to 0 finally.
Refer to [6] for a justification for such convergence and more de­
tailed discussions on the evolution of 'll"t(nc } .

- 4070

To change a viewing angle, we use the notion of entropy [17],
which essentially measures the average uncertainty about a ran ­
dom variable. It is well known that among all distributions tak­
i ng values on {l, . .. ,N}, the unifonn distribution yields maxi­
mum log2 N and the degenerate case yi elds the minimum 0, i.e.,
o � H � log2 N. In the context of this problem, conditional
entropy H(n,lyo:,) captures the evolving uncertain ty of the iden­
tity vari abl e giv en observat io ns YO:I. How ev er, the knowledge of
p(YO:I) is needed to compute H(n,lyo:,), we simply assume that it
is degenerate in the actual observations Yo:t s in ce we observe only
this particular sequence, i. e. , p (yo: l )
6(yo:1 - Yo:t } . Now,

gies. In addition, the still templates in the gallery can be general­
ized [18] to v id eos.
6. REFERENCES
[1]

Gordon , D. 1. Sa lmon d, and A. F. M. Smith, "Novel ap­
proach to nonlinear/non-gaussian bayesian state estimation,"
lEE Proceedings on Radar and Signal Processing, v ol . 140,
pp. 107-113,1993.

[2] N. 1.

=

H(n,IYo:t)

= -

L

p(ntIYo:t)log2P(nelYo:t).

(10)

[3]

G. Q ian and R. Chellappa, "Structure from motion using

sequential monte carlo methods," Proc. of ICCY, pp. 614-

R.E'"

Fig. 7 presents the conditional entropy H(nllllo:c) against t and
the MMSE estimate of the scale parameter GI against t, both ob­
tained using Algorithm' IIa. Fig. 7 shows the decreasing con­
ditional entropy H(ntlyo:n) and the increasing sca le parameter,
which matches with the scenario: a subject walking towards a cam­
era. In Fig. 5, the tracked face is superimposed on the image using
a black bounding box.

M. Isard and A. Blake, "Contour tracking by stochatic prop­
agation of conditional d ensi ty," Proc. ofECC Y, 1996.

621,2001.
[4]

B. Li and R. Chellappa, "Simultaneous tracking and v erifi­
cation via sequential posterior estimation:' Proc. of CYPR,
pp. 110-117, 2000.

[5] B. Li and R. Chel lappa, "Face verification through tracking
facial features," Submined to JOSA 2001.
[6] S. Zhou and R. Chellappa, "Probabilistic face recgnition
from video," Submitted to ECCY 02.

[7] R. Chellappa, C. L. Wilson, and S. Sirohey, "Human and
machine recognition of faces, a survey," Proc. of IEEE, vol.
83,pp.70S-740, 1995.
[8]

W. Y. Zh ao, R. Chellappa, A. Rosenfeld, and P: J. Phill ips ,
"Face recognition: A literature s urvey," UMD CAR-TR-948,
2000.

•

•

tt

i

.

...

:..:

..

II

...

•

•

•

g
H
•
_.-

•

•

•

'Il"t(nt) against time instance, ob­
tained by Algorithm IIa (left) and Algorithm lIb (right).

Fig. 6. II. Posterior probability

[9] P. 1. Philipps, H. Moon, S. A. Rivzi, and P. 1. Rauss,
"The feret evaluation metboJodogy for face-recognition algo­
rit hms," IEEE Trans . PAMI,vol. 22, no. 10, pp. 1090-1104,
2000.
[10]

M. Turk and A. Pentland, "Eigenfaces for recognition:' Jour­
nal of Cognitive Neutoscience, v ol . 3, pp. 72-86, 1991.

[ 1 1] K. Etemad and R. Chellappa, "Discriminant analysis for
recognition of human face images," Journal ofOptical Soci­
ety of America A, pp. 1724-1733, 1997.
[12] P. N. B elh ume ur, 1. P. Hespanha, and D. J. Kriegman,
"Eigenfaces vs. fisherfaces: Reco gni tio n using class specific
linear pr oj ectio n," IEEE Trans. PAMI, vol . 19, 1997.

.

.

.

"-. .

.

..

[13] M. Lades, 1. C. Vorbruggen, 1. Buhmann, 1. Lange, C von der
Malsburg, R. P. Wurtz, and W. Konen, "Distortion invariant
object recognition in the dynamic link architecture," IEEE
Trans. Computers, v o l . 42, pp. 300-311,1993.

.

H(ntlyo:t} (left) and MMSE es­
of scale parameter (right) against time instance. Both are
obtained using Algori thm lIb.

Fig. 7. II. Conditional entropy

[14]

timate

G od si ll , and C. Andrieu, "On sequential
monte carlo sampling methods for bayesian filtering," Statis­
tics and Computing, vol. 10, no. 3, pp. 197-209,2000.

[IS] G. Kitagawa, "Monte carlo filter and smoother for non­
gaussian nonlinear state space models," J, Computational
and Graphical Statistics, vol. S, pp. 1-2S, 1996.
[16] J. S. Liu and

S. CONCLUSION

R. Chen,

"Sequential monte carlo for dynamic

systems," Journal of the American Statistical Association,

We have presented Bayesian methods for face recognition from a
a gallery of still templates. In both
cases, a time series state space model is need to accommodate the
video and SIS algorithms provide the numerical solutions to the
model. But, the posterior probability of the identity giv en the ob­
servations up to present, 1I't (ne), is estimated using different strateprobe video, compared with

A. Doucet, S. 1.

vol. 93, pp. 10 3 1- 1041, 1998.

[17] T. M. Cover and J. A. Thomas, Elements ofInformation The­
ory, Wiley, 1991.
[18]

IV - 4071

V. Krueger and S. Zhou, "Exemplar-based face recognition
from video," submitted to ECCY 02.

MAP ESTIMATION OF EPIPOLAR GEOMETRY BY EM ALGORITHM
AND LOCAL DIFFUSION
Wenfeng Li Baoxin Li
Department of Computer Science & Engineering
Arizona State University, Tempe, AZ 85282 USA
E-mail: {wenfeng.li, baoxin.li}@asu.edu
ABSTRACT
Finding epipolar geometry for two images is a fundamental
problem in computer vision. While this typically relies on
feature point correspondence, the epipolar constraint can
also be used for improving the accuracy of correspondence.
We propose a probabilistic framework for estimating the
epiploar geometry, in which the geometry and the feature
correspondence are estimated iteratively at the same time.
Using the EM algorithm to maximize a posteriori, our
approach updates feature correspondence with estimated
epipolar geometry. The correspondence is further improved
with local diffusion on a prior Markov Random Field model.
In turn, more accurate epipolar geometry is recovered.
Experiments show this approach produces more accurate
fundamental matrix compared with typical methods and can
handle some challenging situations such as view rotation
and scale changes.
Index Terms— Epipolar Geometry, EM Algorithm, MAP,
local diffusion
1. INTRODUCTION
Two perspective images of a common scene are constrained
by the so-called epipolar geometry [1]. It can be described
as, given any point x in the first image, if it is the projection
from a 3D point X in the scene, the projection x' in the
second image must be on a line determined by x which is
called epipolar line. The epipolar geometry can be written as
xFx' = 0
where F is a 3×3 matrix called the fundamental matrix.
A typical way to find the epipolar geometry from two
images includes two main stages. In the first stage, two sets
of feature points are detected in the two images separately,
and then inter-image correspondence is established for the
features. Among others, commonly-used feature detection
and correspondence methods include those based on Harris
corner detector [2] and those using Shift Invariant Feature
Transform (SIFT) [3]. In the second stage, the fundamental
matrix is estimated using the corresponded features. This
usually starts with a linear solution, followed by nonlinear
optimization (e.g., LMedS [4]). Most methods for this stage
can be viewed as maximum likelihood estimation (MLE),
and the quality of the estimate mainly relies on the accuracy
of the feature correspondence, which remains a challenge

1-4244-1437-7/07/$20.00 ©2007 IEEE

despite many years of research. On the other hand, if the
epipolar constraint is known, it makes the correspondence
problem much easier since possible matches for a given
feature are constrained to points on an epipolar line.
In this paper, we propose a probabilistic framework, in
which the Expectation-Maximization algorithm (EM) is
used to estimate the epipolar constraint and feature
correspondence iteratively at the same time. Explicit
correspondence is avoided by a probabilistic representation.
For more reliable probabilistic description of feature
matching between two images, we encode the smoothness
on adjacent pixels in two ways. Pixels around a corner
within a small patch are treated as a unit by enforcing planar
constraint. A prior is approximated by a Markov Random
Field model to aggregate support from neighbor regions.
We use local diffusion to solve this model. Therefore this
problem becomes Maximum A Posteriori (MAP) estimation.
2. RELATED WORK
MAP has been proposed to replace simple RANSAC and
MLE methods for epipolar geometry estimation [5][6].
Cham and Cipolla [5] present a multiscale method for
feature matching in uncalibrated image mosaicing. They use
parameters propagated from a coarse level as prior for the
fine level estimation. This is pointed out as being
problematic in [6] since the fine level data is not
independent of that of the coarse level. Torr and Davidson
[6] also use a multiscale scheme. The posterior distribution
is passed from coarse level to fine level by the technique of
sampling-importance-resampling and MCMC. While such
multiscale methods prove to be an efficient way in solving
the matching problem, texture details may be lost in the
coarse level, causing some feature points useless. Not
limiting to feature points, Domke and Aloimonos [7]
present a probabilistic framework on frequency space,
where all points could be used for matching after applying
Gabor filter. A practical difficulty is that the method can be
computationally very costly.
3. PROPOSED METHOD
3.1. EM Algorithm-based Formulation
We first describe a general formulation of the problem.
Given two perspective images I = {I0, I1} from a common
scene, a feature point detector generates two sets of feature

V - 201

ICIP 2007

points U0={u0j} and U1={u1k}, where j=1,…,m and
k=1,…,n , with m and n being the numbers of features in the
two images respectively. These points are projection of a set
of 3D points X={xi}, i = 1,…q. In general m is not equal to
n, and thus q is also undetermined. For some point in I0,
there may be no corresponding point in I1 and vice versa.
But for each point in X, there must be a pair of projections
in U0 and U1. Since the correspondence is unknown, a
hidden value J is introduced to model the projection from X
to Ui, i=0,1, which also can be viewed as a geometry
transformation from X to Ui. Since the viewpoints of I0 and
I1 are in general different, a transformation R is further
introduced. The goal is to maximize the posteriori
probability of parameters Ĭ=G given the data measurement
U (the union of U0 and U1) and the hidden value T={J, R},
where G is the epipolar geometry expressed by a
fundamental matrix. This is equal to maximizing the
logarithm of the joint distribution, which is proportional to
the posteriori by the Bayes rule
Ĭ* arg max log{P(U, Ĭ)} arg max log ¦ P(U, T, Ĭ) (1)
4

4

T

This computation needs to be done with all possible T,
which results in combinatorial explosion and should be
avoided. This is exactly the prime motivation of using EM
algorithm here. By Jensen's inequality, a lower bound is
obtained by transforming a log of sums to a sum of logs
P (U, ȉ, Ĭ )
log ¦ P ( U, T, Ĭ ) log ¦ f t (T)
f t ( T)
T
T
(2)

P (U, T, Ĭ )
t ¦ f (T) log
f t (T)
T
Where f t(T) is defined as a probability distribution given
an arbitrary transformation T from the hidden value space
and it is constrained by
¦ f t ( T) 1
t

T

A Lagrangian function can be written as
P (U, T, Ĭ)
·
§
L( f t ) ¦ f t (T) log
 O ¨ ¦ f t ( T )  1¸
t
f
(
T
)
T
¹
© T
Taking the derivative and solving the Lagrange
optimizer, we obtain
f t ( T) P ( T | U , Ĭ t )
From (2), given current guess of parameters Ĭt, an
optimal lower bound of the objective function is
P ( U, T, Ĭ t )
¦T f t (T) log f t (T)

P( U, T, Ĭ t )
log P ( U, Ĭ t )
P (T | U, Ĭ t )
T
Therefore the two step EM algorithm can be written as:
E-step: Given current guess Ĭt, compute
(3)
f t ( T) { P ( T | U , Ĭ t )

¦ P(T | U, Ĭ t ) log

M-step: Update Ĭ by maximizing the lower bound of
objective function as
(4)
Ĭ t 1 arg max[log P ( U, Ĭ t )]
4

3.2. The Likelihood Model
To obtain the likelihood of data U given parameters Ĭ, a
simple approach is to consider the probability of each point
in U as i.i.d. (we also assume that U0 and U1 play symmetric
roles in the computation). For a 3D point xj, assuming it
projects to uia on Ii. The probability that it also projects to
u(1-i)b on I1-i is determined by both the geometry constraint
G and the distance between uia and u(1-i)b in terms of feature
descriptor measurement, noted as pG and pM. The objective
function becomes

log P (U, Ĭ)

¦ log  p(u
i

¦¦ log{ p
i

G

ij

| 4) p ( 4 )

j

(5)

(uij | 4) p M (u ij | 4) p (4)}

j

For the geometry constraint, we adopt the commonly
used reprojection error which is
dG d uia , Fu'  d (u ' , Fuia )
where d is a Euclidian distance function. Note that not every
point has a corresponding point in another image due to
occlusion and failure of feature detector. We use a
contaminated Gaussian model as a robust penalty function
(6)
pG (u | 4) (1  H G ) exp(d G2 / 2V G2 )  H G
The matching probability pM is defined as
(7)
p M (u | 4) (1  H M ) exp( d M2 / 2V M2 )  H M
where dM is Euclidian distance defined on pixel intensity or
color values, which depends on image format.
3.3. Feature Matching
We use Harris corner as feature point. For each feature
point, we assume that it is at the center of a small planar
surface comprised of 5×5 pixels. When a planar patch
matches to another view, they undergo certain geometry
transformation which can be modeled by a homography,
written as
ª xº
ª x'º
ª A tº« »
»
«
y'
s« y» «
v 1»¼ « »
«¬ 1 »¼ ¬
«¬ 1 »¼
where A is a 2×2 matrix, t is a translation vector and s is a
scale factor. Since the translation is already considered in
the projection hidden value J and if the patch is centered
well with the feature point, t can be set to zero. A can be
decomposed as
0º
ªO
A R (T ) R (M ) DR (M ), D « 1
»
¬ 0 O 2¼
where R(ș) and R(ĳ) is a rotation matrix with rotation angle
ș and ĳ, ș is the patch rotation angle, and D and ĳ
determines the patch deformation ratio and direction [8]. In

V - 202

practice, patch matching is only sensitive to rotation while
relatively insensitive to deformation given the fact that most
patches in the images are not drastically deformed.
Therefore we can safely approximate A with a rotation
matrix parameterized by ș. 16 evenly spaced rotation angles
are tested at the first matching iteration, sum of absolute
difference (SAD) is used as matching score and the rotation
angle with the smallest SAD is picked as an initial value for
the EM algorithm.
When the fundamental matrix is available, let l and l' be
the epipolar lines that pass the two patches to be matched, it
is easy to prove that two continuous pixels a and b along the
epipolar lines will not change their relative position. As
shown in Figure 1, the patch rotation angle ș is obtained by
T cos 1 (l  l ' )

l

ș

b

a

a

b

l'

Figure 1: Estimate patch rotation from epipolar lines.

The scale factor s can be estimated as follows: Draw an
epipolar line l with the center point, draw two epipolar lines
l', l" with the top center point a and the bottom center point
b. The line segment that is perpendicular to l and through
the center point in the transformed image intersects l' and l"
at a' and b'. Then s is approximately estimated by
a ' b'
s
ab

l'

a

a'

l
l"
b
b'
Figure 2: Estimate patch scale from epipolar lines.

When a detected feature lies on object contours, the
patch model is not accurate if pixels of the patch come from
both the background and the foreground. Such features give
unreliable information for matching but their inconsistent
votes will typically be dominated by other correctly
matched corners, assuming that more features do not lie on
the object contours.
3.4. The Prior Model
In the likelihood model we consider the probability of each
feature as i.i.d. for simplicity. In real images, features are

highly correlated. Adjacent pixels are still adjacent after
being transformed by function T to an image from another
view, which is also called smoothness constraint. We use a
Markov Random Field (MRF) [9] to encode preference of
surface smoothness into a prior model pP
1
p P (T)
exp( E P (T))
ZP
where ZP is a normalizing factor and the potential function
EP is the sum of clique potentials. The idea of using MRF is
to aggregate support from neighbors and to use this support
as a prior. There are two potential issues with using MRF
here. First, we do not have cliques for sparse feature points;
secondly, it requires a large memory and expensive
computation for probabilities of all possible T. We propose
a local diffusion strategy to construct cliques for each
feature point by expanding to its neighbor regions. The
diffusion is undertaken only in local minimums obtained
from the likelihood computation. We use nonlinear
diffusion from [10] for its capability to handle multiple
ambiguities in clique. The implementation can be described
as follows. For a point u0j, if it finds a local minimum of
match score at u1k with T', we also compute matching
scores for 8 neighbor patches of u0j with T' and a shift on
disparity d=-3,…,+3. We rewrite the log of probabilities in
Eqn. (5) with E to represent energy. The diffusion process
updates E as
E m E0  ¦ E S
N8

where E0 is the initial energy from log(pMpG). ES is the log
of the smoothed probability distribution pS
p S ¦ exp( d 2 ) p M pG
d

The updating is iterated only twice since the diffusion
region is very small.
3.5. The Complete Algorithm Workflow
With all components introduced above, we present the
complete algorithm as follows:
Step 1. For each image, a set of Harris corners are
detected. Feature matching is performed with all
combination of corner pairs from two images. The matching
score is stored to compute pM.
Step 2. For a point u0j, if its best match is u1k, and the best
match for u1k is also u0j, such consistent matched pairs are
selected as seeds. We run random sampling and standard 7point algorithm to compute fundamental matrix from these
seeds. The difference from normal RANSAC is that the
criterion is the probability in Eqn. (5) and all points are
computed, not only those matched pairs.
Step 3. The fundamental matrix that produces the best
result will be used as the initial value for EM algorithm.
Matching score is re-computed with the estimated
fundamental matrix.
Step 4. Use random sampling or standard nonlinear
optimization to find a better fundamental matrix based on

V - 203

re-computed matching score. If no better result can be found,
exit, otherwise go to step 3.
Note that when we use Eqn. (5) to maximize the
posteriori, we do not enforce the uniqueness constraint. It
means that one point may be matched to two or more points.
The reason is that explicitly enforcing uniqueness is a
deterministic method and to find exclusively matched pairs
to maximize the posteriori requires a combinatorial
explosion of enumeration of all possible pairs. Since local
diffusion is used to enforce the smoothness constraint, it in
some way remedies the loss of the uniqueness constraint
and our experiments show that this works reasonably well
with object occlusions (as in Figure 4).

[7]

J. Domke and Y. Aloimonos, "A probabilistic notion of
correspondence and the epipolar constraint," Proc. 3DPVT
(International Symposium on 3D Data Processing Visualization and
Transmission), 2006.
[8] R. Hartley and A. Zisserman, Multiple view geometry in computer
vision, Cambridge University Press, Cambridge, UK, 2000.
[9] S. Geman and D. Geman, "Stochastic relaxation, Gibbs distribution,
and the Bayesian restoration of images," IEEE Trans. on Pattern
Analysis and Machine Intelligence, 6(6), pp.721-741, 1984.
[10] D. Scharstein and R. Szeliski, "Stereo matching with nonlinear
diffusion," International Journal of Computer Vision, 28(2), pp.155174, 1998.
[11] OpenCV, http://sourceforge.net/projects/opencvlibrary

4. EXPERIMENTAL RESULTS
Sample experimental results are listed in Figure 3-5. We
compare our results with the implementation of typical
RANSAC and LMedS from OpenCV [11]. While OpenCV
works fine in normal situations and gives the same result as
our method, there are difficult cases where it gives
obviously wrong result, as in Figure 3. Although there are
many parameters used in both the EM algorithm and local
diffusion in our approach, we use the same set of parameters
for all experiments, which shows the performance is robust
to parameter settings.

OpenCV LMedS

5. CONCLUSION
We present an approach to estimating the epipolar geometry
and feature point correspondence more accurately and
efficiently with EM algorithm. Experiments show the
effectiveness of the approach, in comparison with typical
existing techniques. By applying local diffusion and
adaptive matching, our feature point matching is found to be
relatively invariant to image rotation and scale changes. We
plan to extend the system to be invariant to illumination
change and able to segment pixels on different surfaces.
This would lead to a surface reconstruction method.

EM
Figure 3: Top: Incorrect epipolar geometry due to mismatches on

zebra patterns. Bottom: Correct epipolar geometry with our
method.

6. REFERENCES
[1]
[2]
[3]
[4]
[5]
[6]

Q-T. Luong and O.D. Faugeras, "The fundamental matrix: Theory,
algorithms, and stability analysis," International Journal of
Computer Vision, 17(1), pp. 43-75, 1996.
C. Harris and M. Stephens, "A combined corner and edge detector,"
Proceedings of the 4th Alvey Vision Conference, pp. 147-151, 1988.
D. Lowe, "Distinctive image features from scale-invariant
keypoints," International Journal of Computer Vision, 60(2), pp. 91–
110, 2004.
Z. Zhang, "Determining the epipolar geometry and its uncertainty: A
review," Tech. Rep. 2927, Institut National de Recherche en
Informatique et en Automatique, July 1996.
T.J. Cham and R. Cipolla, "A statistical framework for long-range
feature matching inuncalibrated image mosaicing," In Proc. Int. Conf.
Computer Vision and Pattern Recognition, pp.442-447, 1998.
P.H.S. Torr and C. Davidson, "IMPSAC: synthesis of importance
sampling and random sample consensus," IEEE Trans. on Pattern
Analysis and Machine Intelligence, 25(3), pp.354-364, 2003.

Figure 4: Epipolar geometry computed on two images with view
rotation and object occlusions.

Figure 5: Epipolar geometry computed on two images with view
rotation and scale change.

V - 204

2016 23rd International Conference on Pattern Recognition (ICPR)
Cancún Center, Cancún, México, December 4-8, 2016

Video2Vec: Learning Semantic Spatio-Temporal
Embeddings for Video Representation
Sheng-Hung Hu

†

School of Computer Engineering
Arizona State University
Tempe, Arizona 85281
Email:shenghun@asu.edu

Yikang Li

School of Electrical Engineering
Arizona State University
Tempe, Arizona 85281
Email:yikangli@asu.edu

Abstract—We propose to learn semantic spatio-temporal embeddings for videos to support high-level video analysis. The first
step of the proposed embedding employs a deep architecture
consisting of two channels of convolutional neural networks
(capturing appearance and local motion) followed by their corresponding Gated Recurrent Unit encoders for capturing longerterm temporal structure of the CNN features. The resultant
spatio-temporal representation (a vector) is used to learn a
mapping via a multilayer perceptron to the word2vec semantic
embedding space, leading to a semantic interpretation of the
video vector that supports high-level analysis. We demonstrate
the usefulness and effectiveness of this new video representation
by experiments on action recognition, zero-shot video classification, and “word-to-video” retrieval, using the UCF-101 dataset.

I. I NTRODUCTION
Many computer vision applications involve general scene
understanding based on videos. Examples include video-based
action/event recognition, vision for human-computer interaction, and video surveillance, etc. One fundamental task in these
applications is to establish certain mapping from a raw video
input to some high-level semantic labels such as action or
event categories, or gesture-based commands, etc. Typically,
raw video data would first be processed for feature extraction
before any technique for establishing the above mapping is
applied. The quality of the features, or more generally, the
representation of the videos, plays an important role and can
have significant impacts on subsequent analysis tasks.
Some well-known video features include STIP [1], HOG3D
[2], and Dense Trajectories [3], all having been widely used
in video-based analysis. These are often called “hand-crafted”
features, since they were deliberately designed based on reasonable considerations. In contrast, techniques relying on deep
neural networks for directly learning features from videos
have been seen in recent years. For example, a CNN-based
architecture was proposed to fuse appearance and optical flow
features to form a (frame-level) video descriptor in [4]. Typically, features learned from such approaches are based on the
output of the last rectified linear unit (ReLU) layer in a CNN
that contains many hidden layers acting as progressive feature
extractors. Other than using optical flow as part of the input
(hence capturing some local motion), a CNN-based approach
†

These two authors have equal contributions.

978-1-5090-4847-2/16/$31.00 ©2016 IEEE

†

Baoxin Li
School of Computer Science
Arizona State University
Tempe, Arizona 85281
Email:Baoxin.Li@asu.edu

like [4] does not have the capacity to model global temporal
evolution of the video/features, which may be the key to
higher-level semantic analysis. A naive approach of averaging
frame-level representations to form a global representation
would not solve the problem as the temporal structure is no
longer maintained.
In this work, we aim at learning proper vector representations for videos so as to support a set of common semantic
analysis tasks. We employ deep architectures as the basic
building blocks for their demonstrated performance. In order to
capture the temporal structure of an underlying video, one may
utilize Recurrent Neural Networks (RNNs) on top of framelevel CNNs. Two well-known alternatives are the Long-Short
Term Memory (LSTM) and Gated Recurrent Unit (GRU) [5],
[6], which avoid gradient vanishing by implementing “gate
units” that decide what to “forget” and what to “memorize”.
In our approach, we choose GRUs to further encode framelevel CNN features, since they have comparative performance
to LSTM while requiring less computation cost, which is an
important consideration for video.
Furthermore, recognizing that the learned vector representations, although rich in spatio-temporal information, still
lack a semantic organization or clustering that would directly
facilitate a higher-level analysis task like action labeling, we
propose to learn an additional mapping from the learned
vector representations of the videos to the word2vec Semantic
Embedding Space (SES) [7]. This learning task relies on labels
of the videos in a training set (data-driven) and the SES
learned from Wikipedia documents with more than 1 billion
words (prior knowledge on semantic meanings of the labels).
Hence the final mapping effectively leads to an embedding
for the videos into the SES, enabling the utilization of the
word (label) semantics for any higher-level analysis task. The
overall framework is illustrated in Fig. 1, which will be further
elaborated in Section III.
To illustrate the usefulness of such a learned embedding,
we evaluate our approach using three video analysis tasks as
case studies: action recognition, zero-shot learning for action
classification, and semantic video retrieval. All experiments
are based on the commonly-used UCF-101 dataset [8], for
its diversity of contents (and hence challenges for an analysis
algorithm) as well as the ready availability of results from

811

Fig. 1: Overall framework of the proposed architecture for semantic spatio-temporal video embedding.

many state-of-the-art baselines (and hence making it easy to
assess the significance of any performance gains).
II. RELATED WORK
Conventional action recognition tasks utilize hand-crafted
descriptors such as dense features [1]–[3] to capture spatiotemporal information. Such features have found wide applications in the literature. However, in general hand-crafted
dense features lack semantic and discriminative capacity and
thus cannot effectively represent higher-level information [9].
Recent years have seen deep-learning approaches to video
feature extraction. Simonyan et al [4] introduced a method
to fuse both CNN appearance and optical flow features while
Wang et al [9] proposed descriptors that pool both low-level
hand-crafted features and CNN feature to represent videos.
Despite of the fusion process, the above deep-learning
approaches still do not fully leverage temporal information
of the given video. To overcome this limitation, several more
recent approaches [10]–[12] attempted to encode the video
by LSTM. LSTM can be considered as a gated RNN, which
is capable of discovering the implicit temporal structure of
the input sequences while avoiding the gradient vanishing
problem. In the above literature, representing videos by LSTM
was shown to have some advantages when modeling complex
temporal dynamics and competitive results on tasks like action
recognition have been reported. Unfortunately, all of the above
LSTM-based methods encode videos without considering semantic meanings of the representation. As a result, the learned
representation does not directly support high-level semantic
tasks such as semantic video retrieval (retrieving videos by
word descriptions never used in training) and zero-shot video
classification (recognizing unseen video categories).
Associating video representations with semantics has been
studied in various contexts including content-based video
retrieval [13]–[16]. In [11], [17], [18], attempts have been
made to generate semantic label sequences from video inputs.
However, these efforts do not seem to explicitly associate
videos with semantic meaning derived directly from the semantic labels of the videos (but rather relying on external
dictionaries). Lacking is a learned embedding of videos that
may directly lead to semantic interpretation of a novel video,

which may or may not have any textual labels. Such an
embedding could lead to a new presentation that supports highlevel semantic analysis. Our work in this paper attempts to
achieve this by learning the mapping between spatio-temporal
representations and the label vectors in the word2vec semantic
space.
The proposed semantic embedding will easily support zeroshot learning for video classification (to be further illustrated in
Section IV). Most existing zero-shot learning techniques focus
on static images and many rely on attributed-based representations [19]. In practice, it is difficult to obtain sufficient amount
of data for training comprehensive attribute-based representations for a large number of categories. Our representation is
advantageous on this regard since the basic embedding space
derives its semantics from general human knowledge base
(e.g., meanings of words learned from Wikipedia documents),
and only the last stage of mapping requires video labels to
train.
III. P ROPOSED A PPROACH
Given a dataset of video clips with corresponding semantic
labels, our goal is to learn a fixed-length vector representation
for each video so that the representation captures spatiotemporal information of the underlying video as well as the
semantics contained in the labels. The proposed approach
achieves this goal by a deep architecture illustrated in Fig. 1.
The approach consists of three major learning steps. The first
step extracts spatial and (local) temporal features using two
CNN channels. The second step encodes (more global) temporal structures of the video (in terms of learned CNN features)
by GRUs. The third step learns a mapping from the encoded
spatio-temporal video representations to a word2vec semantic
embedding space by an MLP. The proposed architecture is
an end-to-end learning model that can be trained by backpropagation with a loss computed by summing the results of
the hinge rank loss function and the softmax loss function at
the output layer of the model. We elaborate these steps below.
A. CNN-based spatio-temporal feature sequences
With a collection of video clips V where each clip v ⊂ V
contains a sequence of frames with a specific order {f1 , ..., fn }

812

Fig. 2: Illustrating the working of the proposed model: Video clips carry similar semantic meanings can vary greatly in terms
of spatio-temporal features (e.g., videos v6 “NBA” and v5 “Dribbling” are far from each other when only temporal encoding
is performed). The global temporal structure of these features is encoded by GRUs. A learned mapping further embed the
GRU-encoded spatio-temporal feature into a semantic embedding space, where diverse videos sharing the similar semantics
cluster together (e.g., “NBA” and “Dribbling” videos are projected to similar coordinate after embedding).

and label lv . We first pull out RGB frames to represent spatial
information and compute the dense optical flows from the
frames to represent (local) motion information. Both the dense
optical flows and the RGB frames are processed at 10fps, and
the optic flows are computed by using the implementation
described in [20].
Two pre-trained CNN models are then used to extract
appearance fv app and optical flow features fv of respectively. The “VGG-f” [21] pre-trained model on the ImageNet
ILSVRC-2012 dataset [22] is responsible for extracting appearance features while the pre-trained networks implemented
by [23] is used to extract optical flow features. We collect
fv app and fv of from the last rectified linear unit (ReLU)
layer from each pre-trained model.

variable-length GRUs. The choice of encoding separately is
based on the hypothesis that fv app and fv of contain different
and/or complementary types of information of the video at
different space-time scale and thus they should not be pooled
together at the frame level.
The Gated Recurrent Unit (GRU) uses the reset gate rn and
the update gate z n (both gates take values between zero and
one) to memorize and forget sequence states, thus avoiding
the gradient vanishing problem while maintaining the power
to discover temporal relationship. Given an input sequence
x = (x1 , ..., xN ), the GRU encoder feeds forward the input
by iterating the following equation from n = 1 to N :
rn = σ(Wr xn + Ur hn−1 )

B. GRU-based temporal encoding

n−1

n

n

z = σ(Wz xn + Uz h

Given the spatio-temporal feature sequences fv app and
fv of , we encode each of them independently with two

Fig. 3: A standard Gated Recurrent Unit [6]. GRU memorizes
a state variable h. The state variable can be either updating
or remaining the same, depending on the value of the update
gate z. The reset gate r controls the influence of previous input
sequence toward the current input.

n

(1)

)

(2)
n−1

h̃ = tanh(W xn + U (r • h
n

n

n−1

h = (1 − z ) • h

n

n

+ z • h̃

)

(3)
(4)

where hn is the model state at Step n, h̃n is the proposed state
update at Step n, σ() denotes the logistic sigmoid function,
• denotes element-wise product and Wr , Wz , UR , Uz denote
hidden variables [6].
The GRU architecture we use in this paper is shown in
the middle part of Fig. 1, where each GRU contains 1024
hidden units. We experimented with various numbers of units
and chose 1024 to best trade off complexity and performance.
Upon the GRU outputs, we implement a mean-pooling layer
to obtain a fixed-length representation. The outputs of our
GRU encoder are the appearance video representation Evapp
and the optical flow video representation Evof , both having
1024 dimensions.
Finally, a simple concatenation is performed to combine
the appearance and optical flow representations at the video
level: Ev = [Evapp ; Evof ]. This results in a temporally-encoded
representation Ev of 2048 dimensions.

813

TABLE I: 3-fold recognition accuracy on UCF-101 dataset

C. Semantic embedding using MLP
In order to embed semantic knowledge to the encoded video
representation Ev , an MLP is trained to serve as the mapping
function g(•) that projects Ev to its corresponding coordinate
g(Ev ) in the word2vec-transformed 500-dimensional semantic
embedding space.
Semantic label vectors m = (m1 , ..., mN ) are obtained
by feeding forward the given video labels l = (l1 , ..., lN )
through the word2vec network: mn = M (ln ). The word2vec
network M () is a skip-gram text model trained on Wikipedia
documents that containing 5.4 billion words. For labels that
contains multiple words, such as “riding horse” or “rock
climbing indoor”, we generate a single vector zn by averaging theP
word vectors for all unique words in the label ln :
mn = w1 w⊂ln M (ln ).
Two distinct loss functions are implemented and errors from
both loss functions are back-propagated to train the whole
model. To embed semantic label vectors mn , we use the hinge
rank loss function given as:
X
loss1 = −
max[0, margin−mn ·g(Ev )+mj ·g(Ev )] (5)

Algorithm

Accuracy (%)

Dense Trajectory (Wang et al) [3]

75.1

LRCN (Donahue et al) [11]

82.9

Composite LSTM model (Srivastava et al) [10]

84.3

Two-stream Convolution Net (Simonyan et al) [4]

88.0

Deep LSTM with 30 Frame (Ng et al) [12]

88.6

TDDs (Wang et al) [9]

91.5

Our result (softmax)

83.9

TABLE II: Zero-shot learning accuracy on UCF-101 dataset
Algorithm

Accuracy (%)

ConSE (Norouzi et al) [24]

10.5

SES (Xu et al) [25]

10.9

IAP (Lampert et al) [19]

12.8

DAP(Lampert et al) [19]

14.3

Our result

14.7

j6=n

The second loss function serves as a fine-tuning part and is
effective when performing video recognition task. It measures
the probability of g(Ev ) belonging to a specific label lv
through a softmax layer. The second loss function is defined
as:
N
X
loss2 = −
{lv = n}log(P (lv = n|g(Ev )))
(6)
n=1

eg(Ev )·wv
P (lv = n|g(Ev )) = PN
g(Ev )·wn
n=1 e

(7)

where {lv = n} equals to 1 when the video clip label lv
belongs to the n category and 0 otherwise. w is the softmax
layer weights.
Errors from both loss functions are summed together when
performing backpropagation and weight updating. Hence each
loss function can serve as a compensation and fine-tuning
method for the other.
Fig. 2 illustrates how the proposed model works for semantically encoding a video. Video clips with similar semantic
information may still be dramatically differently in the spatiotemporal domain, which is supposed to be captured by the
CNN and GRU networks. Upon the coding of the GRU
network, the spatio-temporal representation of the video is
embedded into the word2vec semantic space, where video clips
sharing similar semantic meanings would be close by, and the
learned mapping via MLP will enable processing/analyzing
new videos without labels (or whose labels were never used
in training the mapping).
IV. E XPERIMENTS AND R ESULTS
We evaluate the proposed video representation architecture
on UCF-101 dataset [8] (13320 video clips from 101 categories) by conducting three visual tasks: video action recognition, video zero-shot learning, and semantic video retrieval.

All three tasks are solved by learning video representations
using the same architecture. Dense optical flow and RGB
frames are extracted at 10fps. Each GRU unit contains 1024
hidden units. The MLP has 2048, 1200 and 500 nodes in its
input, hidden and output layers respectively. Learning rate for
the whole end-to-end structure is initialized as 0.0001 for the
first 15 epochs and then reduced by half for every 15 epochs.
The batch size is set to 30 videos per batch. The margin for
hinge rank loss computation, however, varies between tasks:
0.4 for zero-shot classification and 0.55 for video recognition
and semantic video retrieval.
A. Video Action Recognition
For video action recognition, we follow the three train-test
split rule in [8] to train and test our video representation
architecture. When testing, we categorize test videos to the
trained label that has maximum probability based on the
softmax layer output.
We compare our result performance with [3] and [4], [9]–
[12] as shown in Table I. Compared with [4], [9], our video
representation does not require late classifier-based fusion or
pooling method. Compared with [12], our video representation
does not train multi-layer LSTMs and thus is more time
efficient. We believe that applying late fusion and multilayer GRUs can potentially lead to further improvements.
However, in this paper, we focus on demonstrating that our
video representation can handle much high-level task.
B. Video Zero-Shot Learning
For video zero-shot classification, we perform 10 training
and testing experiments. For each experiment, we randomly
split 101 categories into two sub-datasets: training dataset with
51 categories and testing dataset with the rest 50 categories.
When testing, we categorize test videos to the ”unseen” test

814

TABLE III: Query word pool and their retrieval results. Query words are never seen in training dataset. Retrieve results are
the retrieved video clips denoted by their categories in UCF-101 dataset. Number in the brackets denoted how many retrieved
video clips belong to this category in the top10 hit list.
Query Labels

Top10 Retrieve Results

Query Labels

NBA

Basketball Dunk (10)

Extreme

Orchestra

Playing Cello (9), Playing Piano (1)

Tide

Army

Military Parade (10)

India

Music

Playing Sitar (9), Playing Piano (1)

Celebrate

Computer

Typing (10)

Home-run

Park

Biking (9), Golf Swing (1)

Boat

Summit

Cliff Diving (7), Skiing (2), Rope Climbing (1)

Toy

School

Skate Boarding (10)

Snow

Park

Biking (9), Golf Swing (1)

Acrobatics

Water

kayaking (10)

Ocean

FIFA

Soccer Penalty (8), Soccer Juggling (2)

Hurl

Club

Golf Swing (8), Soccer Juggling (2)

Hiking

Nature

Tai Chi (7), Hammering (2), Walking with Dog (1)

Swim

Beethoven

Playing Cello (8), Playing Voilin (2)

Jogging

Classical

Playing Cello (7), Playing Voilin (3)

Foam

Yankees

Baseball Pitch (10)

Hip-hop

Duel

Boxing Punching Bag (8), Punch(2)

Scramble

Lifting
Martial
Tumbling

Body Weight Squats (4), Rope Climbing (4), Pull
Ups (2)
Fencing (3), Archery (3), Boxing Punching Bag (3),
Balance Beam (1)
Trampoline Jumping (8), Throw Discus (1), Frisbee
Catch (1)

Mat
Parachuting
Hunting

Top10 Retrieve Results
Rock Climbing Indoor (5), Uneven Bars (2), Soccer
Juggling (2), Pole Vault (1)
Cliff Diving (4), Surfing (2), Throw Discus (2), Sky
Diving (1), Rafting (1)
Paying Tabla (4), Playing Sitar (2), Head Massage
(1), Cricket Shot (1), Mixing (1)
Military Parade (6), Long Jump (1), Band Marching
(1), Ice Dancing (1), Blowing Candles (1)
Baseball Pitch (5), Basketball Dunk (3), Field
Hockey Penalty (1), Frisbee Catch (1)
Kayaking (4), Rafting (2), Rowing (2), Cliff Diving
(1), Push Ups (1)
Yo-yo (4), Nun chucks (4), Pull Ups (1), Juggling
Ball (1)
Skiing (2), Ice Dancing (2), Cricket Bowling (1),
Pole Vault (1), Blowing Candles (1), Blow Dry Hair
(1), Rafting (1), Sky Diving (1)
Juggling Balls (5), Soccer Juggling (5)
Cliff Diving (4), Sky Diving (3), Kayaking (2),
Rafting (1)
Throw Discus (2), Mopping Floor (2), Baby Crawling (1), Javelin Throw (1), Cricket Shot (1), Blowing
Candles (1), Pull Ups (1)
Biking (5), Kayaking (4), Rafting (1)
Diving (5), kayaking (3), Cricket Bowling (1), Sky
Diving (1)
Biking (5), Skate Boarding (2), Soccer Juggling (1),
Skiing (1), Ice Dancing (1)
Blowing Candles (7), Pull Ups (1), Rope Climbing
(1), Juggling Balls (1)
Trampoline Jumping (6), Swing (4)
Pull Ups (6), Trampoline Jumping (2), Rope Climbing (1), Cricket Shot (1)
Rope Climbing (4), Pommel Horse (3), Trampoline
Jumping (2), Javelin Throw (1)
Diving (6), Cricket Bowling (2), Hand Stand Walking (1), Sky Diving (1)
Horse Riding (3), Kayaking (3), Nun chucks (3),
Frisbee Catch (1)

label that has maximum cosine similarity (nearest neighbor)
to our temporal and semantic embedded representation.
We compare our results with [19], [24], [25] as shown in
Table II. The proposed representation outperforms [19], [24],
[25] by around 2% and 4% and slightly outperforms the stateof-the-art attribute-based model DAP [19] by around 0.5%.
The superior performance of our proposed representation indicates the effectiveness of the proposed semantic embedding
technique that encodes semantics as well as spatio-temporal
information.

first train-test split rule in [8] to separate the UCF-101 dataset.
A word pool which contains 40 words were created manually
to serve as query words. When testing, we first feed forward
all test videos through the trained architecture and obtain
their semantic embedding representation. Then, for each query
word, our architecture retrieve top 10 test video clips that have
maximum cosine similarity (nearest neighbor) to the word2vec
transformed query word as demonstrated in Fig. 4. Note that,
for a query which contains multiple words, we perform the
same averaging method as described previously.

C. Semantic Video Retrieval

The query word pool and their retrieval results are shown in
Table 3. It shows that our model is capable of capturing both
visual contents and their semantics. One specific example is
the retrieved results using the query word “NBA”. All retrieved

To further demonstrate that the proposed representation can
perform “semantic association” of videos, we challenge it with
the semantic video retrieval task. For this task, we follow the

815

Fig. 4: Example result of our architecture performing the
semantic video retrieval task. By inputing the query word
“Acrobatics”, our architecture retrieves “Juggling Balls” and
“Soccer Juggling” video clips.

videos belongs to the “Basketball Dunk” category in the UCF101 dataset. Another more complex example is “Acrobatics”,
which leads us to “Juggling Balls” and “Soccer Juggling”
videos.
V. C ONCLUSION
In this paper, we proposed a video representation architecture that learns semantic spatio-temporal embeddings for
videos. Gated Recurrent Units are utilized to temporally encode deep CNN features while an MLP is trained to further
embed the learned spatio-temporal features into a semantic
space given by word2vec. We evaluated our representation
architecture on the UCF-101 dataset for action recognition,
zero-shot classification and semantic video retrieval. The
experimental results suggest that the proposed approach is
able to compute video representations useful and effective for
semantic visual analysis tasks. In particular, the semantic video
retrieval example demonstrates that the proposed approach
can support retrieving semantically meaningful videos simply
based on a word query.
ACKNOWLEDGMENT
The work was supported in part by ONR grants N0001415-1-2344 and N00014-15-1-2722. Any opinions expressed in
this material are those of the authors and do not necessarily
reflect the views of ONR or ARO.
R EFERENCES
[1] I. Laptev, “On space-time interest points,” International Journal of
Computer Vision, vol. 64, no. 2-3, pp. 107–123, 2005.
[2] A. Kläser, M. Marszałek, and C. Schmid, “A spatio-temporal
descriptor based on 3d-gradients,” in British Machine Vision
Conference, sep 2008, pp. 995–1004. [Online]. Available:
http://lear.inrialpes.fr/pubs/2008/KMS08
[3] H. Wang, A. Kläser, C. Schmid, and C.-L. Liu, “Action Recognition
by Dense Trajectories,” in IEEE Conference on Computer Vision &
Pattern Recognition, Colorado Springs, United States, Jun. 2011, pp.
3169–3176. [Online]. Available: http://hal.inria.fr/inria-00583818/en
[4] K. Simonyan and A. Zisserman, “Two-stream convolutional networks
for action recognition in videos,” CoRR, vol. abs/1406.2199, 2014.
[Online]. Available: http://arxiv.org/abs/1406.2199

[5] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
computation, vol. 9, no. 8, pp. 1735–1780, 1997.
[6] K. Cho, B. van Merrienboer, Ç. Gülçehre, F. Bougares, H. Schwenk,
and Y. Bengio, “Learning phrase representations using RNN encoderdecoder for statistical machine translation,” CoRR, vol. abs/1406.1078,
2014. [Online]. Available: http://arxiv.org/abs/1406.1078
[7] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of
word representations in vector space,” arXiv preprint arXiv:1301.3781,
2013.
[8] K. Soomro, A. R. Zamir, and M. Shah, “UCF101: A dataset of
101 human actions classes from videos in the wild,” CoRR, vol.
abs/1212.0402, 2012. [Online]. Available: http://arxiv.org/abs/1212.0402
[9] L. Wang, Y. Qiao, and X. Tang, “Action recognition with trajectorypooled deep-convolutional descriptors,” CoRR, vol. abs/1505.04868,
2015. [Online]. Available: http://arxiv.org/abs/1505.04868
[10] N. Srivastava, E. Mansimov, and R. Salakhutdinov, “Unsupervised learning of video representations using lstms,” CoRR, vol. abs/1502.04681,
2015. [Online]. Available: http://arxiv.org/abs/1502.04681
[11] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell, “Long-term recurrent convolutional
networks for visual recognition and description,” in CVPR, 2015.
[12] J. Y. Ng, M. J. Hausknecht, S. Vijayanarasimhan, O. Vinyals, R. Monga,
and G. Toderici, “Beyond short snippets: Deep networks for video
classification,” CoRR, vol. abs/1503.08909, 2015. [Online]. Available:
http://arxiv.org/abs/1503.08909
[13] H. Li, L. Liu, F. Sun, Y. Bao, and C. Liu, “Multi-level feature
representations for video semantic concept detection,” Neurocomputing,
vol. 172, pp. 64–70, 2016.
[14] M. Gitte, H. Bawaskar, S. Sethi, and A. Shinde, “Content based video
retrieval system,” Int J Res Eng Technol, vol. 3, no. 6, 2014.
[15] R. Veltkamp, H. Burkhardt, and H.-P. Kriegel, State-of-the-art in
content-based image and video retrieval. Springer Science & Business
Media, 2013, vol. 22.
[16] C. G. M. Snoek and M. Worring, “Concept-based video retrieval,”
Found. Trends Inf. Retr., vol. 2, no. 4, pp. 215–322, Apr. 2009.
[Online]. Available: http://dx.doi.org/10.1561/1500000014
[17] S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney, T. Darrell, and
K. Saenko, “Sequence to sequence-video to text,” in Proceedings of the
IEEE International Conference on Computer Vision, 2015, pp. 4534–
4542.
[18] S. Venugopalan, H. Xu, J. Donahue, M. Rohrbach, R. Mooney, and
K. Saenko, “Translating videos to natural language using deep recurrent
neural networks,” arXiv preprint arXiv:1412.4729, 2014.
[19] C. Lampert, H. Nickisch, and S. Harmeling, “Attribute-based classification for zero-shot visual object categorization,” Pattern Analysis and
Machine Intelligence, IEEE Transactions on, vol. 36, no. 3, pp. 453–465,
March 2014.
[20] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert, “High accuracy
optical flow estimation based on a theory for warping,” in Computer
Vision-ECCV 2004. Springer, 2004, pp. 25–36.
[21] K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman, “Return of
the devil in the details: Delving deep into convolutional nets,” in British
Machine Vision Conference, 2014.
[22] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
A large-scale hierarchical image database,” in Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE,
2009, pp. 248–255.
[23] G. Gkioxari and J. Malik, “Finding action tubes,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2015,
pp. 759–768.
[24] M. Norouzi, T. Mikolov, S. Bengio, Y. Singer, J. Shlens, A. Frome,
G. S. Corrado, and J. Dean, “Zero-shot learning by convex combination
of semantic embeddings,” arXiv preprint arXiv:1312.5650, 2013.
[25] X. Xu, T. Hospedales, and S. Gong, “Semantic embedding space for
zero-shot action recognition,” in Image Processing (ICIP), 2015 IEEE
International Conference on. IEEE, 2015, pp. 63–67.

816

J. Vis. Commun. Image R. 15 (2004) 393–424
www.elsevier.com/locate/jvci

Bridging the semantic gap in sports
video retrieval and summarization
Baoxin Li*, James H. Errico, Hao Pan, Ibrahim Sezan
SHARP Laboratories of America, 5750 NW Paciﬁc Rim Blvd., Camas, WA 98607, USA
Received 5 June 2003; accepted 5 April 2004
Available online 23 August 2004

Abstract
One of the major challenges facing current media management systems and related applications is the so-called ‘‘semantic gap’’ between the rich meaning that a user desires and the
shallowness of the content descriptions that are automatically extracted from the media. In
this paper, we address the problem of bridging this gap in the sports domain. We propose a
general framework for indexing and summarizing sports broadcast programs, with a high-level
model of sports broadcast video using the concept of an event, deﬁned according to domainspeciﬁc knowledge for diﬀerent types of sports. Within this general framework, we develop
automatic event detection algorithms that are based on automatic analysis of the visual and
aural signals in the media. We have successfully applied the event detection algorithms to different types of sports including American football, baseball, Japanese sumo wrestling, and
soccer. Event modeling and detection contribute to the reduction of the semantic gap by providing rudimentary semantic information obtained through media analysis. We further propose a novel approach, which makes use of independently generated rich textual metadata,
to ﬁll the gap completely through synchronization of the information-laden textual data with
the basic event segments. We implemented an MPEG-7 compliant browsing system for semantic retrieval and summarization of sports video using the proposed algorithms.
 2004 Elsevier Inc. All rights reserved.
Keywords: Semantic video analysis; Video summarization; Event detection

*

Corresponding author. Fax: 1-360-817-8436.
E-mail address: Baoxin.Li@ieee.org (B. Li).

1047-3203/$ - see front matter  2004 Elsevier Inc. All rights reserved.
doi:10.1016/j.jvcir.2004.04.006

394

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

1. Introduction
With the increasing amount of audio-visual data that are broadcast or available in
a prerecorded format, there is an emerging need for eﬃcient media management
including browsing, ﬁltering, indexing, and retrieval. One of the big challenges facing
current media management systems and the related applications is referred to as the
problem of bridging the ‘‘semantic gap’’ (or the so-called ‘‘data-meaning gap’’).
Media content descriptions, especially those that are automatically extracted from
the media, are too shallow compared to the meaning that users expect to capture
while they search for or browse audiovisual media, resulting in the semantic gap.
In some situations, even though manually generated rich annotations are available,
their sources may be disparate, and their contents and structure may not readily lend
themselves for use in cohesive combination with the audiovisual media and the limited low-level information provided by media analysis tools. This may be the case,
for example, for a broadcast video of a particular game, where there may be much
textual information about the particular game on the Internet but it is often very difﬁcult to use that information with the video, especially in association with diﬀerent
segments of the video for summarization and segment-based retrieval purposes.
Therefore, the semantic gap still exists.
In this paper, we address the semantic gap problem in the sports domain, particularly for sports broadcast video with an emphasis on indexing, retrieval, and summarization. Indexing, retrieval, and summarization of sports broadcast programs are
of major interest to sports content producers. A content producer routinely logs in
and retrieves exciting events of games for producing a game summary or highlights
video. Sports content providers are interested in providing searchable databases to
their customers, for example through a web based service, where customers are able
to place semantic queries to retrieve video segments of their favorite games or players, and create their own customized summaries by putting these segments together.
Consumers at home are often interested in viewing summaries of live broadcasts that
they have missed, and in non-linearly browsing a long broadcast game in a time eﬃcient manner by concentrating only on the interesting events of the game. The ongoing transformation from analog to digital that is taking place in sports content
production environments, the increasing availability of broadband connection to
homes, and increasing hard-disk personal storage capabilities in the home (e.g., personal video recorders) are creating a demand for semantic indexing, retrieval, and
summarization in both professional and consumer applications.
We start with a general model of a sports broadcast video at a high level, with event/
non-event breakdown of the video. We illustrate in this paper, with examples, that the
deﬁnition of an event can be specialized to a speciﬁc sport based on speciﬁc domain
knowledge (e.g., an event can be a pitch in baseball and a ﬁeld goal in American football). This unifying model applies to diﬀerent types of sports, including those that have
the action-stop pattern, e.g., baseball and American football, and those that are continuous without obvious breaks, e.g., continuous-action sports such as soccer and ice
hockey. We develop automatic event detection algorithms on the basis of this general
model for American football, baseball, sumo wrestling, and soccer.

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

395

We propose a two-step approach to the semantic gap problem. In the ﬁrst step, the
event detection algorithms automatically detect all the segments that contain interesting events of a particular game, where interesting events are deﬁned based on the domain knowledge and well-established production patterns such as slow-motion
replays. We show how domain knowledge can be used in posing parts of a speciﬁc event
detection problem as a canonical-scene detection problem. These detection algorithms
are extended to perform some limited recognition of detected events, such as ﬁeld goals
in American football, base steal attempts in baseball, and hits versus balls or strikes in
baseball. Automatic event detection provides signiﬁcant progress towards bridging the
semantic gap in the sports domain. However, the semantic gap is still not closed due to
the limitations of automatic extraction of semantics from the media itself.
Many researchers have been concentrating on further improving automatic algorithms for extraction of semantics from the media in order to further close the
semantic gap, with applications in the sports domain (Babaguchi et al., 2002; Choi
et al., 1997; Ekin and Tekalp, 2002; Gong et al., 1995; Kawashima et al., 1998;
Rui et al., 2000; Saur et al., 1997; Xie and Chang, 2002; Yow et al., 1995; Zhong
and Chang, 2001). For example, a recent eﬀort (Zhang and Chang, 2002a,b) uses
caption (scoreboard) recognition for extracting semantics and highlights of a sport
video. Although our methods included in the ﬁrst step are along the direction of
automatic semantics detection, we take a new approach in this paper that is especially suitable for the sports domain, and present it as the second step following
the automatic event detection step with the goal of completely closing the semantic
gap. In this second step, we make use of independently generated rich textual data
about the key events of a game authored by humans (stringers) watching the actual
game on site. Such data are commonly used in professional applications, for example
from SportsTicker. We develop methods for automatic analysis, parsing, and interpretation of such textual data and then automatically synchronizing it with the detected video event segments. Speciﬁcally, we propose an algorithm based on
dynamic programming for performing the synchronization. The event segment data
and the synchronized textual metadata are then merged into a rich media content
description, which bridges the semantic gap completely in video indexing, retrieval,
and summarization applications for major types of sports content.
The rest of the paper is organized as follows. In Section 2, we provide event based
modeling of sports broadcast video supporting both stop-action and continuous-action sports. In Section 3, we show how this model applies to event detection in stopaction sports and continuous-action sports. Section 4 is devoted to the second step of
our approach following event detection, namely the synchronization of independently generated metadata with the detected events. Experimental results are presented in Section 5. We conclude with discussion and conclusions in Section 6.

2. Modeling sports video using events
Sports are diﬀerent in nature, and thus there are dramatic diﬀerences in sports
broadcast video for diﬀerent sport types. For this reason, most research work in

396

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

the area of sports video analysis is speciﬁc to a certain sport (e.g., Kawashima et al.,
1998; Rui et al., 2000 for baseball, (Saur et al., 1997) for basketball, (Gong et al.,
1995; Yow et al., 1995) for soccer, (Babaguchi et al., 2002) for American football,
etc.), or at best to a few semantically similar sports (e.g., Li and Sezan, 2001; Zhong
and Chang, 2001). It is desirable to develop a general framework that works across
many types of sports. In this section, aiming at this goal, we propose a general modeling based on an abstract usage of the concept of ‘‘event,’’ such that most of the
sports programs can be modeled at a high-level by a simple concatenation of ‘‘event’’
and ‘‘non-event’’ video segments. Then, we show how this general model can be specialized to a given sport by using domain-speciﬁc knowledge. Indexing and summarization of sports video are straightforward under this modeling. We also compare
the proposed model with related approaches in the literature.
2.1. Models for action-stop and continuous-action sports
In many types of sports, although a typical game broadcast lasts say, a few hours,
only parts of this time contain real actions of the underlying game. These important
parts occur semi-periodically but sparsely during the game, but they contain the moments of intense action and are the essence of the game. The remaining time is typically less important (e.g., idle time when the ball is not in play, change of players,
pre-game performance or ceremonies, commercials, time-outs, etc.). These types of
sports are so-called ‘‘action-and-stop’’ sports. For an action-and-stop sport, if all actions have been extracted, then a user can follow, understand, and even enjoy the
game by viewing only the action clips. Examples include baseball and American
football, where all pitches and all plays, respectively, contain every detail that a user
wants to know about the underlying game. On the other hand, there are also many
other sports that are not of action-and-stop type, meaning that the underlying sport
is almost continuous without any break, and thus it is impossible to cut out any portion of the video without compromising a userÕs ability for grasping every detail of
the underlying game. However, for this type of sports, since the video is full of consecutive actions, what a user would desire is some reasonable classiﬁcation of the actions, which allows access to those parts of the video that are more interesting or
exciting than other parts of the video. Examples of continuous-action sports include
soccer, ice hockey, and basketball, etc.
In consideration of the above distinction between action-and-stop sports and continuous-action sports, we introduce the following general modeling, using ‘‘event’’ in an
abstract sense. We model the video as a sequence of ‘‘events’’ interleaved with ‘‘nonevents,’’ with ‘‘event’’ being deﬁned as the basic segment of time during which an important or exciting action occurs, as illustrated in Fig. 1. When instantiated for a speciﬁc
sport, the event can be, for example, a pitch in a baseball game, an attempt of oﬀense
(i.e., a play) in football, a bout in wrestling, and a goal or a goal attempt in soccer,
etc. Obviously, an event is a complete action and can contain multiple video shots; thus
the modeling is at a much higher level than the breakdown of video into shots.
For an action-stop type sport such as baseball, by deﬁning event to be every pitch,
the modeling in Fig. 1 eﬀectively breaks down an input video into pitches and seg-

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

397

Fig. 1. A general model of a sports video in terms of ‘‘event,’’ deﬁned as a segment of time during which
an important action occurs in the game. The inner loop (in dashed lines) indicates the possibility that two
(or more) important actions can occur consecutively.

ments of idle time. Similarly, an American football video can be decomposed into
plays and breaks with this modeling by deﬁning events as plays. In this case, the
modeling is reduced to that proposed in (Li and Sezan, 2001).
For a continuous-action game, as pointed out above, there is no break per se.
Therefore, the problem becomes how to deﬁne event so that it represents those actions that are of greater importance and interest compared with the others. While
there are diﬀerent methods for detecting ‘‘exciting’’ segments in video by using,
for example, audio information, we claim that ‘‘excitement’’ is a subjective concept
and thus is not easy to model, let alone to detect automatically by a computer. We
propose a new approach to deﬁning event in this case. We deﬁne important events as
those actions in a game that are replayed by the producer/broadcaster. Notice that
by using this strategy, we eﬀectively shift the task of judging the importance/excitement of an action to the broadcaster, who is generally in a much better position to
make this judgment than any automatic algorithm. This method is of course based
on the assumption that the actions that are replayed by a broadcaster are typically
more exciting or important than other actions that are not replayed. We believe that
this is a reasonable and valid assumption. In this case, the modeling in Fig. 1 is eﬀectively reduced to that of Fig. 2. Compared with Fig. 1, here we know that event in
Fig. 1 refers to ‘‘exciting action,’’ and ‘‘non-event’’ refers to ‘‘ordinary (less exciting)
action.’’ It is for this reason, we have emphasized that event in the general modeling
is used in an abstract sense, since there may be actions going on during a non-event
period.
Notice that, for action-stop sports, even if we can use the play-centric modeling
(as in Li and Sezan, 2001), it is also possible to use the modeling in Fig. 2. In the
latter case, the modeling would focus on distinguishing, for example, exciting pitches
from others in a baseball game, rather than detecting every pitch.
As we will show in Section 4, the modeling discussed here also facilitates synchronization and merging of independently generated rich textual metadata about the

Fig. 2. The eﬀective modeling for continuous-action type of sports.

398

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

game with events or interesting action segments since complete and detailed textual
annotations of sports games typically posses the granularity of an event or exciting
action. For instance, in soccer, any complete textual annotation will include the
description of goals and exciting goal attempts.
2.2. Comparison with earlier work
In recent years, automatic analysis of sports video has received signiﬁcant attention. Compared with earlier work in this area, the proposed modeling is: (a) more
general as it uniﬁes two major types of sports, namely action-stop and continuous-action sports, and (b) it has obvious advantages as we describe below. For action-stop type of sports, the proposed approach is an extension of earlier work
(e.g., Kawashima et al., 1998; Li and Sezan, 2001; Zhong and Chang, 2001), where
individual papers either handle a speciﬁc sport (Kawashima et al., 1998), or can handle only play/break type of patterns (Li and Sezan, 2001; Zhong and Chang, 2001),
which is only appropriate for action-stop type of sports.
The proposed modeling is also diﬀerent from prior work in the case of continuous-action sports. For instance, (Xie and Chang, 2002) focuses on structure analysis,
and exciting segments are extracted based the analysis results; (Yow et al., 1995) extracts soccer highlights based on analysis of the image content; (Rui et al., 2000) uses
some audio features in extracting highlights; (Ekin and Tekalp, 2002) breaks the video into shots and then analyzes them (the analysis of the shots may or may not result in classiﬁcation of the shots into important or non-important shots). In our
modeling, exciting actions are deﬁned based on the selection made by a production
expert, namely on replays. Therefore, in terms of accuracy or meaningfulness in
determining whether an action is exciting, our modeling has a unique advantage over
the above mentioned methods that practically rely on the automatic understanding
of the content by a computer. We rely on automatic detection of replay segments
using low-level features and production patterns, as described in the next section,
which is a much more suitable task for a computer than understanding the content.

3. Event detection
With the modeling in Fig. 1, the major task of video analysis becomes the detection of all the events in a given video. In this section, we propose event detection
algorithms that automatically extract interesting segments from the input video.
The algorithms are largely based on video/audio analysis of the input video, and they
make use of two types of prior knowledge in extracting semantics from broadcast
sports video: (1) domain knowledge, and (2) production knowledge. The detection
algorithms represent these two types of knowledge in terms of rule bases where rules
are expressed in terms of low-level visual and aural features that are automatically
computed from the media. By domain knowledge in sports, we mean the deﬁnition
of key events that are important for a particular sport, e.g., a play in American football, a hit in baseball, and a goal and its set up in soccer. Production knowledge

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

399

refers to techniques used in producing the broadcast video. These techniques are
used to help viewers follow the game in an entertaining, informative, and captivating
manner. They direct viewersÕ attention via well-accustomed production patterns that
viewers expect, such as scene transitions after plays, replays, dynamic broadcaster
logos sandwiching replay segments, certain camera angles that are sports and/or
event speciﬁc, and score board overlays. Most of the techniques have existed for
years and they have become established production practices. Therefore, it is possible to derive consistent video/audio features from video captured with those production techniques.
In short, while domain knowledge determines what constitutes an event for a given sport, it also helps to identify possible video/audio cues that may help the detection of an event, and we can further exploit scene characteristics arising from
established broadcast pattern to help the detection. For clarity, we describe our event
detection algorithms in the following for action-stop sports and continuous-action
sports, respectively. However, it will become clear that the building blocks of the
algorithms are actually similar, and the two types of sports are in fact processed
in the same fashion, as they can be uniﬁed under the same modeling of Fig. 1.
3.1. Action-stop sports
For action-stop sports, the modeling of Fig. 1 reduces to the modeling of a video
with action shots interleaved with breaks, and the event detection problem becomes
one of detecting the starting and end points of all actions. The action shots include,
for example, plays in American football and baseball. In the following, we describe
how the detection is achieved through video analysis by assuming domain knowledge
and established production patterns. The overall approach is depicted in Fig. 3, with
each component detailed subsequently.
3.1.1. Detecting the start-of-action via canonical scene detection
To shed some light on how the start point of an action can be detected, let us ﬁrst
use football as an example to examine how the broadcast video is typically captured.
The football game is usually captured by cameras placed at both sides of the ﬁeld
and at the two ends of the ﬁeld. A typical camera setup is three side cameras located
above the 25-, 50-, and 25-yard lines, plus two end cameras at both ends of the ﬁeld.
There may be other cameras at other locations, but most of the events are broadcasted using video captured by the three side cameras and the two end cameras,

Fig. 3. The overall approach for action-stop sports.

400

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

which are often called ‘‘game cameras,’’ since the essence of a game is really captured
by these cameras.
In the case of baseball, the game cameras include the one set behind the pitcher
and the one set behind the catcher. Similarly, in the example of sumo wrestling,
the game camera always perpendicularly looks at the umpire, who stands between
two players at the beginning of each bout. These relatively ﬁxed camera set-ups lead
to the observation that almost all the action shots start with similar scenes (henceforth ‘‘canonical scenes’’) in a given action-stop sport broadcast. Fig. 4 illustrates
the sets of canonical scenes for the cases of football, baseball, and sumo wrestling,
respectively. Obviously, one particular sport may have multiple canonical scenes,
but the number is limited due to the limited number of camera conﬁgurations.
(Although in theory there are almost inﬁnitely many possibilities in positioning
the cameras, in practice, only a few possibilities produce a captured video that is
the most convenient for the viewers to see the action. Hence for a given sport, all
broadcasters tend to use similar camera conﬁguration.) The problem of detecting
the start-of-action becomes one of detecting the canonical scenes from a given video.
The canonical scenes of a given sport can still vary from game to game and even
within the same game, as the cameras, albeit placed at ﬁxed locations, can still pan,
tilt, and zoom, and the capturing conditions can change during a long game (e.g.,
lighting condition changes with time and weather). Therefore, to detect the canonical
scenes, we have to identify invariant features that can be robustly extracted from the
video, which are detailed in the following.
3.1.1.1. Dominant colors. The canonical scenes contain certain colors that account
for suﬃciently many pixels in the image. For example, in football, the dominant color is some sort of green; in baseball, the colors are green (for the grass) and brown
(for the clay); in sumo, the color is some sort of yellow for the ring.

Fig. 4. Canonical scenes of the start-of-action for football, baseball, and sumo, respectively. A typical
passing play (A), and a typical kick in football (B), a typical pitch (C), and a typical base-steal in baseball
(D), and a typical bout in sumo (E).

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

401

3.1.1.2. Geometrical feature. The canonical scenes satisfy some special geometrical
constraints. For example, in football, a start frame contains parallel ﬁeld lines,
and players appear as small blobs; in baseball, the green and brown regions alternate
on the ﬁeld, which locates on the lower half of the image for a pitching scene (Fig.
4C); in sumo, the yellow ring is always on the lower half of the image with two symmetric skin-toned human blobs.
3.1.1.3. Motion feature. The camera is typically static at the start-of-action, when the
players pose for the action, and thus there is no global motion in canonical scenes.
While one may use other features, the above features turn out to be quite universal for the action-stop sports. Although implementation speciﬁcs may be diﬀerent for
speciﬁc sports (e.g., see Li and Sezan, 2001, 2002 for details), a common algorithmic
framework for detecting the canonical scenes is as follows: (1) detect frames in a video that contain given dominant colors; (2) test if the geometrical constraints are satisﬁed; (3) test if the motion constraint is satisﬁed. Temporal evidence accumulation
can be used for robustness. Formally, when detecting a start frame, a sliding window
of width w is always used. A start frame is declared only if more than p out of the w
frames in the current window are found to be start frame candidates. A recommended choice of p is such that p/w = 70%, which is determined on the basis of processing a huge number of games.
3.1.2. Detecting the end-of-action
In general, it is a challenging task to determine, through understanding the video
content, when the current play ends. However, usually there will be a camera break
right after an action is completed (e.g., after the ball is dead). Therefore we propose
to detect the end of a play by ﬁnding camera breaks after the play starts. One may
notice that this is an approximate solution based on the assumption that there will be
a camera break after a play ends. However, camera breaks (resulting in scene transitions) are an established way of professional video production. A good time for a
camera break is right after an event has just ﬁnished (i.e., a play just ended). Observations from extensive action-stop sports video have conﬁrmed the validity of this
assumption. This assumption enables us to model the end-of-play with camera
breaks, which are relatively easy to detect in a robust fashion, compared with
attempting to understand the high-level content of the video. Depending on the speciﬁc sport, the play may end at the ﬁrst camera break; it may also contain multiple
camera breaks (as in a typical baseball hit, see (Li and Sezan, 2001) for details). But
the play still ends at a camera break in the latter situation.
The scene transition arising from a camera break can be a clean scene cut (immediate switch to another camera), or a gradual transition (wipe, fade-in, fade-out, dissolves, and some special editing eﬀects, etc.) A clean scene cut can be detected by
thresholding the color histogram diﬀerences. It is more challenging to accurately detect gradual transitions. There are diﬀerent approaches in the literature for detecting
gradual transitions (e.g., Golin, 1999; Lu et al., 1999; Naphade et al., 1998), but none
is perfect for all types (wipe, dissolves, fades, etc.) (e.g., see Lienhart, 1999; Gargi
et al., 2000 for comparisons). For our problem, we exploited knowledge about the

402

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

underlying sports to facilitate accurate gradual transition detection, instead of solving the general problem. We use a simple technique to constrain the gradual transition detection problem by performing the following process:
1. Starting from a start-of-action at time t0, looking forward until hitting a clean
scene cut, or until time t0 + tp, whichever occurs ﬁrst. Denote the time thus found
as tclean_cut.
2. If tclean_cut < tlow, we will not look for gradual scene cut; otherwise, we try to ﬁnd a
gradual transition in [tlow, tclean_cut] with any appropriate method from the literature. If a gradual transition is detected, the end point will be set to the start of the
gradual transition, otherwise, to tclean_cut.
The above method utilizes the fact that a typical play has only a limited duration
(at least tlow and at most tp). Adding the duration constraint provides a better chance
of detecting transitional cuts since the task is now limited to a short time period right
after the start at time t0.
3.1.3. Adaptive detection
Notice that even for the same sport, the canonical scenes may still have dramatic
diﬀerences from game to game, due to diﬀerences in stadiums (e.g., resulting in different ﬁeld colors), time of the game (e.g., resulting in diﬀerent image characteristics
due to lighting), and capturing process (e.g., diﬀerent cameras and camera switching
frequencies), etc., as illustrated in Fig. 5 for football. In addition, the scene cut detection in Section 3.1.2 also relies on the video characteristics. Therefore, it is important
for the algorithm to have some adaptation or learning capabilities. In our algorithms, the dominant colors and the scene cut thresholds are learnt adaptively from
a given video. For learning the dominant colors, a coarse initial color speciﬁcation is
deﬁned, and the geometrical constraints are strongly enforced to detect some candidate frames. The reﬁned dominant colors are then computed from the candidate
frames after statistical outlier removal is used to ﬁlter out less likely frames. In the
example of football, a learning stage based on detecting ﬁeld lines is utilized to calibrate the ﬁeld color and to obtain the color histogram of the player blobs in a start
scene. In a football video, it is diﬃcult to ﬁnd frames that do not contain a view of
the ﬁeld, but contain multiple parallel (in physical world coordinate system) white
lines (with certain spacing) on a background dominated by green color. Therefore,

Fig. 5. Examples showing that a start frame may be dramatically diﬀerent in appearance in diﬀerent
games.

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

403

if we detect multiple lines in a frame with dominant green color, and those lines satisfy the parallelism constraints, then the frame is very likely from a ﬁeld scene. We
can then use that frame as a sample for calibrating the green color and computing
other statistics, as described above. Note that before the calibration is done, we still
need a coarse speciﬁcation for the color green. However, this speciﬁcation can be
very broad such that it covers almost any possible green color. We have found in
our experiments that this learning stage is very successful. Using the calibrated green
color, the algorithm could even ﬁlter out those (false) plays from diﬀerent games that
are shown during a break period in the current game (e.g., during half-time report),
when the ﬁeld color of those outside plays are not close to that of the current game
(of course the number of outside plays are signiﬁcantly smaller than the number of
plays in the current game).
3.1.4. Handling replays
The proposed approach will also detect replays (and include them in a summary) if the replay (of a play) is captured from the same camera angle. Ideally,
for summarization application, replays are excluded from the summary for the
sake of compactness. Also, replay segments should be identiﬁed and ﬁltered out
when we merge the event detection results with other independently generated
metadata (discussed in Section 4), which typically do not contain annotations
for replays. For that purpose, we have developed a set of methods for automatic
replay detection in sports video, as reported in (Pan et al., 2001, 2002). These
methods handle both slow motion and regular speed replays. With the additional
replay-detection module, one can also form a (at least) three-layer hierarchy of
summaries, with the base layer containing all the plays plus replays, the second
layer containing only the plays, and the third layer containing only the replays
(important events).
3.1.5. Simple event classiﬁcation
In the action-stop type sports, typically there are diﬀerent types of plays. For
example, in American football, there are passing plays, punts, ﬁeld goals, etc.; in
baseball, there are ball, strike, hit, base-steal, etc. Simple classiﬁcation of detected
plays may be achieved through scene analysis using domain knowledge. We have
developed various techniques for simple play classiﬁcation in our event detection
algorithms. Speciﬁcally, for American football, the algorithm is able to distinguish
a ﬁeld goal or an extra point attempt from other plays, based on the assumption that
the former is almost always captured from an end-zone camera, and thus the ﬁeld
lines are horizontal in the video (Fig. 4B). For baseball, based on whether or not
a pitch is immediately followed by a camera break that results in a ﬁeld scene, the
algorithm is able to distinguish a pitch with ﬁeld action (eﬀectively a hit in most
cases) from a pitch without ﬁeld action (eﬀectively a ball or a strike in most cases).
In the meantime, a base-steal attempt (especially from the ﬁrst base) is typically captured from a special camera angle (Fig. 4D)), and thus the algorithm is able to distinguish it from a regular pitch (Fig. 4C). Our experiments have shown that these
types of classiﬁcations are fairly accurate.

404

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

The above simple event classiﬁcation adds some richness to the rudimentary metadata (i.e., start and end index points of the event segments) generated by basic event
detection. The resultant metadata provides a user with more semantics. For example,
in baseball, with the pitches classiﬁed, the user is able to make a query for ‘‘all hits.’’
Moreover, as will be discussed in the next section, the classiﬁcation of detected
events will facilitate the synchronization of the video segments with independently
generated rich metadata.
3.2. Continuous-action sports
In this subsection, we present algorithms for event detection in continuous-action
type sports, using soccer as a primary example. One can ﬁnd that other sports in this
category, such as ice hockey, can be similarly handled, even though the implementation details may vary. Unlike in the case of action-stop sports, where the starting
scenes of ‘‘plays’’ are always shot from canonical angles, in a soccer broadcast, these
starting scenes of ‘‘plays’’ do not exist because the concept of ‘‘plays’’ does not apply.
Therefore, as discussed in Section 2, for continuous-action sports, the event in the
modeling of Fig. 1 is actually deﬁned as exciting/important actions that have been
picked out by the broadcaster for re-playing. Accordingly, the overall approach
for continuous-action sports is proposed in Fig. 6, where we show the options of
using replays and/or their corresponding live actions in the event summaries.
In the following, we use soccer as an example to describe the components in the
overall approach of Fig. 6. It will become clear that, while replay detection is the core
and common component for continuous-action sports, the live-action-detection
component in Fig. 6 can be specialized for a speciﬁc sport, again based on domain
knowledge and production convention. In the case of soccer, the sequential relationship between an exciting action and its replay is shown in Fig. 7. By utilizing this

Fig. 6. The overall approach for continuous-action sports.

Fig. 7. The relationship between an exciting action and its replay. Events are deﬁned as the action that is
replayed plus the setup action leading to that action.

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

405

Fig. 8. The overall diagram of the soccer algorithm. Visual features and aural features extracted from the
audio track are utilized.

prior knowledge of the established production patterns in soccer broadcast programs, we can design an algorithm for the live-action detection module. From
Fig. 7, an event includes an exciting action and the setup action leading to that action. An exciting action is usually followed by one or more close-up shots of the key
player(s) involved in the action, and/or the audience, and/or the coach, and/or the
referee, and then followed by the replay of the action. In other words, an exciting
action has two versions, live and replay, with some close-up segments separating
the two parts. We make use of this triplet production pattern in detecting the exciting
events. In the following, we describe the detection of a replay segment, its associated
close-up segment, and the associated exciting action segment, respectively. The overall diagram of the algorithm is shown in Fig. 8.
3.2.1. Detection of replay segments
Replay detection is the ﬁrst step in the algorithm. We have developed methods for
detecting replays with suﬃcient reliability and accuracy (Pan et al., 2001, 2002). Due
to space limitation, we can only brieﬂy mention that the method of (Pan et al., 2001)
is mainly based on the detection of slow-motion generated by ﬁeld/frame repetition,
and that the method of (Pan et al., 2002) is mainly based on the detection of transitional logos that sandwich replay segments. More details can be found in these
references.
3.2.2. Detection of close-up segments
We use two visual features to detect close-up segments: (1) the ratio of the pixels
belonging to the soccer ﬁeld to the total number of pixels in a frame; and (2) scene
cuts, discussed respectively as follows.
3.2.2.1. Dominant color ratio in a frame. In close-up segments, players (or referees)
are shot from a close distance. Therefore, the frame is not dominated by the colors
of the soccer ﬁeld. Otherwise, the ﬁeld is shot from a long distance and thus a large

406

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

Fig. 9. A frame from a live segment (left); and a frame from the associated close-up segment (right).

portion of the frames is the soccer ﬁeld, as shown in Fig. 9. The dominant color of a
soccer ﬁeld varies from time to time and game to game and can be learned by the
methods described in (Pan et al., 2002). We assume that the dominant color is green,
and therefore calculate the ratio of the number of green pixels to the total number of
pixels in a frame in order to distinguish the close-up frames.
The color of green can be deﬁned in any appropriate color space. In this paper,
green is calculated in the normalized RGB color space, and only the normalized R
and G color components, as deﬁned below, are considered. This is intended to alleviate the eﬀects of varying color in the case of diﬀerent ﬁelds, weather conditions,
0
camera settings, etc. A pixel is classiﬁed as being green if its normalized (R , G0 ) components satisfy the following conditions: R0 < 0.5, G0 > 0.5, and R0 + G0 6 1.0, where
R0 = R / (R + G + B), G0 = (R + G + B). It is also possible to use more advanced
methods such as (Ekin and Tekalp, 2003; Pan et al., 2002) to achieve adaptation
as we have done in Section 3.1.3.
By counting the number of green pixels in a frame, we calculate the ratio of the
number of green pixels to the total number of pixels in the entire frame. Fig. 10
shows a typical example of this ratio over a period of time. One can see that in normal shots, the ratio is close to 1. In close-up shots, the ratio is close to 0; in replay
segments, the ratio varies.
3.2.2.2. Scene cuts. Usually there is a sudden scene cut between the live action and
the close-up segment. This scene cut is at the end of the live action and at the starting
point of the close-up segment. Sometimes, a close-up segment between the live action
and its replay consists of several diﬀerent shots of players, coaches, and audiences,
and therefore consists of several scene cuts, as illustrated in Fig. 11. Close-up segments are characterized by the low ratio of the number of green pixels to the number
of total pixels in a frame. The earliest scene cut in this time period with low {number
of green pixels/total number of pixels} ratio is chosen as the starting point of the
close-up segment and the end point of the live action. In the example shown in
Fig. 11, it is Scene-cut 0.
3.2.3. Detection of starting points of live events
The last step of the proposed soccer algorithm is to detect the starting points of
exciting event segments that contain exciting action and the setup action leading

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

407

Fig. 10. Ratio of the number of green pixels to the number of pixels in a frame.

Fig. 11. Demonstration of determination of the starting point of the close-up segment and the end-point
of the live exciting action. Within the close-up segments, the {number of green pixels/total number of
pixels} ratio is low. There are four scene cuts within. The earliest scene cut in this time period is chosen as
the starting point of the close-up segment and the end point of the live action.

to that exciting action. Although the semantic content of the live version and replay
version of an exciting action is the same, the camera angles used to capture them in
the broadcast video are usually diﬀerent. The live version of the action usually employs a wide-angle camera shooting from a distance while the replay version usually
employs a narrow-angle camera shooting in close range. In other words, frames in
replays look structurally very diﬀerent from those in the live version. Therefore,
although the replay segment has been detected, one can hardly expect to ﬁnd the corresponding live action based on content analysis. For example, the approach proposed in (Babaguchi et al., 2000) will not work well in this case. In the following,
we propose a method for determining the starting point of the event, i.e., the starting
point of the set-up action.

408

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

Due to the nature of soccer games, except for a few cases, such as a free kick and a
corner shot, there is no objective way of deﬁning the starting instances of exciting
events. Besides, before exciting actions take place, the video shots in broadcast soccer
games are typically unbroken. A shot usually lasts for several minutes, and uses camera
pan and zoom, but lacks scene cuts or gradual transitions. Therefore, one cannot use
scene cuts or transitions in detecting the starting points of the event segments. In our
algorithm, we choose the starting points of the event associated with the detected replay
segments as either 15 or 30 s prior to the starting points of the close-up segments (or the
end points of live action segments). We make use of the following two criteria in choosing between 15 and 30 s. If either of the two criteria favors 30 s, then we place the starting point at 30 s before the end point of the action. If neither of the two criteria favors
30 s, then we place the starting point at 15 s before the end point of the action.
3.2.3.1. Criterion 1. energy spectrum of the audio signal. An exciting action and its
setup are typically accompanied by the audienceÕs and/or the commentatorsÕ increased level of excitement, which is usually reﬂected in the audio tracks. In our
experiments, if the average audio energy over the 30-s duration is larger than the
average energy over the 15-s duration, then a 30-s long event segment is favored;
otherwise, a 15-s long event segment is favored.
We have also investigated the possibility of using the audio pitch contour to reﬂect
the degree of the excitement of commentators. In theory, higher pitch is always associated with higher degree of excitement. In our experiments, however, the two male
commentators had signiﬁcantly diﬀerent pitch in their voices, and we could not distinguish who was speaking at any given time. Because it is commonplace to have two
commentators in broadcast soccer games, the pitch contour is not a desirable feature.
3.2.3.2. Criterion 2. duration of detected replay segments. We have observed that the
lengths of replays are proportional to the excitement of the actions in the replays.
For example, a replay of a goal is much longer than a replay of a foul. When an action is more exciting, it is reasonable to give it more time to see how the event is built
up. Therefore, if the duration of a detected replay exceeds certain threshold, then a
30 s long event segment is favored; otherwise, a 15 s long event segment is favored.
3.2.4. Generating the game summary
We have used the following four methods for generating highlight summaries:
(1) Concatenation of {exciting event segment + close-up segment + replay segment},
for all replays.
(2) Concatenation of {exciting event segment + close-up segment}, for all replays.
(3) Concatenation of {exciting event segment}, for all replays.
(4) Concatenation of all replay segments.
Note that since live event segments are restricted to be shorter than 30 s and closeup and replay segments are typically longer than a minute, the highlights generated by
Case (1) above is usually three times longer than the highlights generated by Case (3).

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

409

4. Bridging the semantic gap using multi-modal metadata synchronization
Rudimentary metadata extracted by event detection approaches described in Section 3 can bring a user one step forward towards closing the semantic gap, as the
extracted indexing points can be used for non-linear navigation and searching,
and the generated summaries provide a user with condensed and organized viewing
experience without loss of context (compared with simple fast-forwarding). But obviously this is not suﬃcient, since the rudimentary metadata cannot support more
complex operations such as querying based on the name of a player. Thus the semantic gap is still there. While it may not be realistic to expect media analysis tools to
provide every piece of information that a user may request, we have observed that
there are various sources of expert knowledge that produce streams of rich information describing multimedia programs. However, there are no existing means for
automatically combining the information into one composite information stream
seamlessly. Although the resolute user may choose to consume more than one of
these information streams, the user loses the synergistic value that a composite information stream would provide, something that is more valuable in a multimedia experience than just the sum of the constituent information streams.
In this section, we propose methods and systems that synchronize independently
generated rich textual metadata with the event segments extracted automatically in
Section 3. A concept diagram of the proposed approached is illustrated in Fig. 12.
In particular, we will show how results obtained through proposed event detection
algorithms can be merged with SportsTickerÕs play-by-play descriptions. (Covering
numerous professional and college sports, SportsTicker is now a leading real-time
sports news and information service whose clients include almost all major broadcasters and major sports teams.) The resultant description denotes all of the plays
in a game, and each of those plays is annotated with rich textual descriptions about
the play. The proposed approach is based on the fact that independently generated
textual metadata typically have event-based structure, and thus the textual information can be ﬁtted into the general modeling in Fig. 1. For this reason, the proposed
approach will work with any type of textural feed other than SportsTicker, as long as
they possess (or can be converted into) the event-based structure.
The resultant composite stream supports the presentation of the plays plus their
annotations in an integrated multimedia viewing experience (Section 5). For in-

Fig. 12. A concept diagram of the synchronization approach.

410

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

stance, as the viewer is watching a particular baseball play, the presentation interface
may pop up the batter and pitcherÕs statistics records. Obviously, with the video indexed with event segments and each segment associated with rich annotations, a
searchable database is also a direct application.
4.1. Challenges
There are many challenges in merging (synchronizing) the disparate streams of
event information (e.g., segmented video and annotations), such as:
 Event streams do not provide explicit cues or ﬂags to facilitate synchronization.
The video stream must be analyzed to derive synchronization cues. Similarly,
the annotations must be parsed and interpreted to form sync cues.
 The timebases for each stream are out of phase and of diﬀerent frequencies.
 There are gaps in the event sequence for each stream. Gaps may be caused by missed
detections, or there may simply be events that are not intended to be included in a
particular stream (e.g., replays are not included in the SportsTicker data stream).
 There are false detection events or unmatchable events in each stream.
 The cues that are available for synchronization have less than perfect reliability,
and they may only sparsely synchronize the event streams.
 The various cues that are available may be conﬂicting.
These challenges can be illustrated by a speciﬁc example of synchronizing SportsTickerÕs play-by-play descriptions to a video stream in real time, when the video
stream is captured live, and the play-by-play descriptions are received through Internet connection in real time. (Real-time synchronization is of great importance for
applications such as real-time highlights broadcasting on TV or on the Internet).
When SportsTicker data is published in real time, the individual records are transmitted immediately after they become ready (e.g., when a human annotator ﬁnishes
typing the record). Each play-by-play record roughly corresponds to a basic event of
the underlying game. Therefore, for example, in baseball, there are at least as many
records as there are pitches. Note that, since a record is sent whenever the human
operator ﬁnishes his/her typing, there is no good way of associating a time stamp
to a record. For example, although the time of transmission for a record roughly correlates to the time of the underlying event, there is no ﬁxed mapping between these
two time instances, since they have variable shifts due to the variation in the speed of
annotation of the human operator. We have noticed that the time diﬀerence between
a recordÕs time of transmission and the time of the event can be as large as a several
plays. The situation is worsened if we take other factors into consideration such as
networking, queuing, loss of connectivity delays, and retransmissions of earlier records (for correcting earlier errors) etc. In a sport with a game clock such as football,
the records do have the game clock information associated with each play. However,
in general the game clock information cannot be directly mapped into the time stamp
of an event in the video stream, due to the frequent periods when the clock is stopped
for timeouts, ball out of bounds, etc. For example, in football, the game clock counts

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

411

down 1 h of playing time (four 15-min periods) if there is no overtime, but a game
video will typically be at least 2 h, or as long as 4 h.
Most prior solutions to this problem involve making one stream dependent on
the other, such as manually annotating the video stream (the annotations are dependent on the video stream), or manually segmenting the video stream using an annotation list as a basis (the video segmentation becomes dependent on the
annotations). Some solutions may take one generated video segmentation stream
and manually associate a small number of independently generated annotations
to some of those segments. However, to the best of our knowledge, automated synchronization processes that overcome these challenges are not available for merging
independent event streams.
4.2. Proposed approach
While the idea depicted in Fig. 12 is general enough and applicable to problems
that involve synchronizing video streams with related information streams from different sources and of diﬀerent types, for convenience of presentation, the description of the methods will be centered around a prototypical example which
focuses on the synchronization of the detected plays from the event detection algorithms in Section 3 and the play-by-play descriptions of the SportsTicker data feed
to form a composite metadata description. In most cases, baseball will be used as
the primary example, since it represents one of most challenging sports for synchronization due to its complexity in terms of action granularity (e.g., at least nine innings each with multiple outs, variable but numerous pitches within one batter, etc.).
American football will also be mentioned during the discussion. For a continuousaction sport such as soccer, the synchronization task is rather straightforward. In
soccer, for each goal, the SportsTicker data contains the scoring time plus other
information such as the name of the scoring player. Since detected actions (Section
3.2) include the goals, the matching of the two lists can be achieved (e.g., using the
scores and/or scoring times) easily in a typical soccer game due to the relatively
small number of detected actions.
A high-level overview of the proposed synchronization system is shown in Fig.
13. In the most basic operation mode, the synchronization module merges the

Fig. 13. High-level block diagram of the prototypical system showing all the relevant modules.

412

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

two input sources using synchronization cues that are extracted from both sources.
Due to the afore-mentioned challenges in merging disparate sources, sometimes it is
desirable to utilize another type of information—e.g., the scoreboard OCR data—to
help the synchronization. Assuming that the OCR data are extracted from the same
video used in play detection, we use the ‘‘Assign’’ module to associate the play data
with the OCR data so that the synchronization module will still perform on two
sources except that one source (from play detection) has been enriched by the
OCR data. The various synchronization modules are described in the following sections in detail, in reference to Fig. 13, using baseball as a primary example.
4.2.1. Preprocessing SportsTicker for sync data
The SportsTicker data stream needs to be parsed and meaningful synchronization
points must be extracted. The utility that is designed to perform this needs to be specialized for each sport domain. The SportsTicker preprocessor utility design is nested
according to the layers of description deﬁned in the SportsTicker speciﬁcations. The
highest layer retrieves single messages based on speciﬁc start/end delimiter sequences.
The speciﬁc message parser layer breaks the message up into one or more records. The
parsed messages and records are assembled and output with the following data ﬁelds:
 Sync type: one of an enumerated list describing synchronization event type (e.g.,
Inning, Steal in the case of baseball; and Quarter, Game Clock in the case of
football)
 Scoreboard info: the current state of the game (e.g., pitch count, inning).
 Record type: one of an enumerated list describing the sport-speciﬁc record event
types (e.g., StrikeSwinging).
 SportsTicker text description: the textual label of this sync record, which should be
associated with the play.
In the case of baseball, there exist other additional primary and/or derived synchronization cues. For example, based on analysis of the adjacent SportsTicker data,
we may be able to identify groups of SportsTicker plays that have an increased probability of being associated with the same detected play.
4.2.2. Preprocessing detected plays for sync data
The play detection algorithmsÕ outputs must be parsed and meaningful synchronization points must be extracted. In the example of baseball, the preprocessor utility
receives as input the following data: start and end frame numbers for each detected
play; the number of shots detected for each play; and the detected play type (Section
3.1.5).
The output generated by the preprocessor utility is a list of detected plays with the
following data ﬁelds:
 Sync type: one of an enumerated list describing synchronization event types. This
enumerated list will be a subset of that of Sync data of SportsTicker.
 Start and End Frame Numbers.

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

413

4.2.3. Scoreboard OCR and its ﬁltering for extracting sync data
Although accurate recognition of characters in video remains to be a diﬃcult task
in general, in sports video, to facilitate OCR one can exploit the fact that the scoreboard banners used by broadcasters are relatively invariant. For example, in the
2001–2002 season, both ESPN and FOX used the same style of scoreboards for
all their baseball and football broadcasting. In reality, the broadcasting networks
would typically keep their scoreboard relatively ﬁxed in order not to confuse the
viewer. Therefore, video OCR becomes relatively easier with the assumption of
invariant scoreboard styles. Speciﬁcally, the OCR algorithm can assume, for a given
channel, that the locations of the scoreboard are known. Further, assumptions can
be made about the context of the scoreboard. That is, one can assume that the locations of the game score, the team names, the out count, etc. are known (to a certain
degree of accuracy).
Under these assumptions, a simple OCR approach has been developed, based on
matching pre-stored templates to pre-determined search regions of an input video,
which can extract the scoreboard information from the video. For baseball, it extracts counts of inning, out, ball, and strike, and top or bottom of an inning. Game
scores and current bases occupied can also be extracted using similar approaches.
For football, the approach extracts quarter numbers and the game clock, which
are deemed as the most important cues for synchronization.
The OCR data obtained on a frame-by-frame basis may contain errors. Sport-speciﬁc knowledge can be utilized to ﬁlter the row OCR data in order to obtain more
accurate results. For example, in baseball, generally, if the out count reliably changes
(goes up during a sub-inning, or is reset at the end of a sub-inning), then we know
that the ball-strike count should be reset. In all other times, the ball-strike count
should only increase. To ﬁx isolated errors in a given ﬁeld (e.g., the out count),
the simplest solution is to low-pass ﬁlter that ﬁeld. Note that although one can do
ﬁltering on the OCR data after the OCR process is ﬁnished, it is also possible to
incorporate the game speciﬁc knowledge into the OCR process. For example, if
the OCR module ﬁnds that the out value for current video frame is 1, then it should
expect the value to be most likely either 1 or 2 (but much less likely 0) in the subsequent frame.
4.2.4. Assign scoreboard sync data to detected plays
With the scoreboard OCR data, we will be able to augment each detected play
with a numerical set of tags that identiﬁes the status of the game at each play. In
most cases, this scoreboard status is unique for each play in the game. (In perfect
scoreboard OCR conditions, this sequence of scoreboard status might be suﬃcient
to facilitate the entire synchronization process, without considering other cues.) In
the case of baseball, the set of scoreboard numerical status is the combination of
resultant pitch count, number of outs, inning status, and running game score.
The detected play data stream and the scoreboard data will be combined in the
‘‘Assign’’ module (Fig. 13) before processing. The scoreboard data is a stream of
time-stamped events. The timebase for the scoreboard stream is the same as the play
detection algorithm, so it is straightforward to compare the two streams. Because we

414

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

cannot determine the exact point within a play that causes a scoreboard event, and
because the latency for entering the scoreboard event may vary over a non-negative
time range, we will only be able to use the scoreboard data to bind an individual
playÕs associated scoreboard state. This bounded scoreboard state may be a single
discrete point, or it may be a range.
4.2.5. Merge sync data using dynamic programming: synchronization
Merging the two types of data sources (the detected play lists augmented with
OCR data, and the textural SportsTicker data) can be formulated as the following
sequenced-pattern-matching problem: We denote the augmented play list as:
P ¼ fp1 ; p2 ; p3 ; . . . ; pN g;
where pattern pi stands for a play (a vector specifying its start and end points, its play
classiﬁcation, and corresponding scoreboard data, etc.). We denote the textural data
list as
T ¼ ft1 ; t2 ; t3 ; . . . ; tM g;
where pattern ti stands for an element of textual annotation (Section 4.2.1) containing scores, game clock and other information. In general, N „ M. The task is to
match these two sequences of patterns so that both the (local) dissimilarity between
a matched pair (pi,tj) and the global dissimilarity (sum of all local dissimilarities) are
small, subject to certain constraints (e.g., the ordering constraint: if pi is matched to
tj,pi + 1Õs match is constrained to tk with k P j). The local dissimilarity can be expressed for example in terms of the diﬀerences of the scoreboard data, etc.
This is a typical problem of global time alignment of two sets of patterns, and we
propose to perform this merging using dynamic programming to search for the optimal path, which leads to a match with the least dissimilarity. In this dynamic programming, the axes of the graph are the sequences of sync cues from various
input data sources, and the dissimilarity measure is represented in terms of various
transition probabilities. Speciﬁcally, the dynamic programming grid will have the
synchronization cues from the SportsTicker on the X-axis and the detected play synchronization cues on the Y-axis. The decision rules for the dynamic programming
operation are embodied in the transition matrix. This matrix deﬁnes the transitions
that are permitted from any node in the graph. The transition probability is a function of the transition arc (e.g. (1,1) corresponds to a diagonal arc) and the sync types
for the events (SportsTicker and detected plays) are associated with the destination
of the transition. The four components of the dynamic programming algorithm are
described in the following in detail.
4.2.5.1. Dynamic programming transitions. Each transition has a probability factor.
When traveling across the grid, a particular path of transitions is evaluated by taking
the product of all the individual transition probabilities. Transitions that should
never happen will be given the probability 0.0. For simplicity, all the transition probabilities out of a start node may be normalized with respect to the largest transition
probability.

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

415

4.2.5.2. Transition arc. The transition arc is deﬁned as the vector from the start node
to the end node for a single transition. It may be described using an x and y oﬀset
pair. In our implementation, we assume that paths cannot travel backward. This
means x and y are non-negative. Additionally, we constrain the system such that a
transition is non-null, so the (0,0) transition is invalid. Possible transitions are described below, with the help of Fig. 14.
 (1,1) Diagonal. This arc does not skip any cues from either axis. This should be
the highest probability arc in this detail level (which will be normalized to 1.0).
 (1,0) Slide horizontal. This arc repeats a detected play cue match with another
SportsTicker cue. This occurs if one detected play actually is two plays concatenated together, or if two SportsTicker cues are generated for a single play. Probability: medium.
 (0,1) Slide vertical. This arc repeats a SportsTicker cue match with another
detected play cue. This occurs if one SportsTicker event actually contains two
events, or if an actual play is split into two detected plays. Probability: very low.
 (2,0); (3,0); . . .Skip and Slide horizontal. This arc skips one or more SportsTicker
cues and then repeats a detected play cue match with another SportsTicker cue.
This occurs if three or more SportsTicker cues in a row are generated for a single
play, but the middle one(s) are not recognized by the system. This should not
happen because all SportsTicker cues are designed to be recognized. Probability
0.0.
 (0,2); (0,3); . . .Skip and Slide vertical. This arc skips one or more detected play
cues and then repeats a SportsTicker cue match with another detected play. This
occurs if one SportsTicker event actually contains three events, and the middle
detected play is disregarded. We will not allow this to happen. (Instead, a series
of (0,1) transitions are allowed, though of very low probability.) This transition
has probability 0.0.

Fig. 14. Possible transitions during dynamic programming.

416

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

Table 1
Baseball end node cue type transition matrix
I
S
B
H

0.2
0.2
0.2
1.0

0.2
0.2
1.0
0.4

0.2
1.0
0.4
0.4

1.0
0.4
0.4
0.4

H

B

S

I

 (2,1); (3,1); . . .Skip horizontal. This arc skips one or more SportsTicker cues. This
occurs if a play is missed (not detected). For multiple missed plays, the probability
can be calculated as the probability for (2,1) raised to the number of missed plays.
Probability: low.
 (1,2); (1,3); . . .Skip vertical. This arc skips one or more detected play cues. This
occurs if a detected play is a false alarm. For multiple false alarm plays, the probability can be calculated as the probability for (1,2) raised to the number of false
alarms. Probability: low.
 (2+,2+), . . .Skip horizontal and vertical. This arc skips one or more cues from
both axes in the same transition. This transition should never happen, so it will
have probability 0.0.
4.2.5.3. End node cue types. The probability of a particular transition is further described by the pair of sync cues referenced at the end node. Using baseball as an
example, if the End Node Cue Types are described by {H = ‘‘Pitch With Field Action’’; B = ‘‘Pitch Without Field Action’’; S = ‘‘Base Steal’’; I = ‘‘Inning Change’’},
then a typical End Node Cue Type Transition Matrix can be deﬁned as shown in
Table 1.
4.2.5.4. Scoreboard OCR transition probability. The scoreboard cue information will
be available as auxiliary data in the dynamic programming sequences. After evaluating the transition probabilities and scores for each transition, the scoreboard cue
information will be compared and the result will be used to further qualify the transition. For example, in the case of baseball, transitions that result in decreased inning
number should have very small probability (the probability should be zero if it is not
compensating for possible scoreboard OCR errors).
4.2.6. Rich output description
The output of the sync utility is a list of descriptions for the detected plays, augmented with SportsTicker data. Each line is a single play containing the following
data ﬁelds:
 Start and End Frame Numbers.
 Play-by-Play textual description detailing the play event, in a format that may
vary according to the play type.
 Scoreboard description in a ﬁxed format speciﬁc to each sport.

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

417

 Inter-play description describing a game status or condition that changes between
plays.
 Game description information such as the team names, the venue, etc.
The above synchronized description list is then translated into a usable format for
input to the target application, by the ‘‘Format’’ module in Fig. 13. In our prototype
system, we convert the description list into an MPEG-7 compliant XML description.

5. Experimental results
We have performed extensive experiments by applying the proposed algorithms to
diﬀerent types of sports, video from diﬀerent broadcasters, and video captured from
diﬀerent recording sources. Excellent results have been obtained, which we brieﬂy report in this section. All the data used in our experiments are MPEG-encoded streams
of 320 · 240 frame resolution, captured by an inexpensive TV-tuner PC card. All the
algorithmic modules, except for the OCR module in Section 4, further reduce the input resolution to 160 · 120 before computation. This suggests that the proposed
algorithms do not require very high quality input. In our experiments, the event
detection algorithms for all the sports achieved faster than 30-frames/s computational performances on a Pentium III-800 MHz PC. The synchronization module requires only nominal computation as only minimal video processing is involved.
5.1. Event detection and summarization
We report the results for the proposed event detection algorithms from experiments on American football, baseball, Japanese sumo wrestling, and soccer. In all
cases, the ground truth data are obtained by manual inspection, and the results from
the automatic algorithms are compared with the ground truth to compute the performance statistics.
For American football, the events are plays that include passing plays, kickoﬀs,
ﬁeld goals, extra-point attempts, etc. Table 2 tabulates the results for a total of
660 min of video composed of multiple games in our testing suite.
For baseball, the events are pitches including ball, strike, hit, base-steal, home
run, etc. The results obtained from a total of 337 min of video composed of seven
baseball video clips are given in Table 3. On the average, game summaries formed
by concatenating play segments, provided about 3–4 times reduction in viewing time
for both baseball and football.
We also applied the framework to Japanese sumo wrestling, with the deﬁnition of
events specialized to the bouts in a game. Two sumo sequences, one 60-min and one
52-min in duration, respectively, have been processed. We obtained 100% detection
rate in both cases, with no false alarms for the second sequence and two false alarms
for the ﬁrst sequence (the false alarms are in fact parts of historical ﬂashbacks of
other sumo matches, and thus are detected by the algorithms). A compaction ratio
of 20:1 was achieved in both cases when the length of event-based summary is com-

418

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

Table 2
American football event detection performance
Total plays
Perfect detection
Imperfect detectiona
Number of missed plays
Compaction ratiob
Detection rate
Number of false positivesc

688
581
77
30 (4.4%)
3.8:1
95.8%
26

Test data includes two 90-min, four 80-min, two 60-min, and one 40-min sequence, 660 min in total
(including eight diﬀerent games in diﬀerent stadiums and broadcast by three diﬀerent TV networks in a
two-year period). We observed no signiﬁcant performance diﬀerences between individual sequences (the
same is true for Table 3).
a
Imperfect detection refers to plays that are detected to be either a little longer or a little shorter than
desired.
b
Compaction ratio is deﬁned as the ratio between the lengths of the input video and the generated play
summary.
c
Some of the false alarms are from other games in half time report (thus are plays themselves by
deﬁnition).

Table 3
Baseball event detection performance
Total number of plays
Perfectly detected plays
Imperfect detection
Number of missed plays
Compaction ratio
Detection rate
Number of falsely alarm

509
453
47
9 (1.8%)
3.4:1
98.2%
24

The test data include seven 20-min, one 60-min, one 84-min, and one 53-min sequence, from seven diﬀerent
games broadcast by three diﬀerent TV networks.

pared with that of the original video. The dramatic compaction is a consequence of
extremely long rituals in a sumo tournament.
For its popularity, soccer was chosen as an example for the continuous-action
sports category. We have applied the proposed algorithm to two soccer sequences
of two diﬀerent games, with total duration 120 min. The algorithm was able to detect
all the ﬁve goals in the sequences. All other 21 detected actions were exciting goal
attempts. The resulting summaries were evaluated by a group of soccer fans in
our laboratory. They all agreed that the summaries provided them with all the relevant and exciting events and that their viewing experience was smooth in that the
summary ﬂowed naturally. (While hard-core fans will mostly prefer to enjoy the live
broadcast at its entirety; they have enjoyed the summaries of the games that they
have missed and for which they already knew the scores. They also enjoyed watching
the summaries and the exciting parts of the game over and over again to recreate and
share their excitement with others.) On the average, the summaries that include the
replay segments were less than 18% of the original video in length, which means a

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

419

user can review all the exciting events and their replays of a game by spending only
16 min on watching the summary.
5.2. Synchronized multimedia viewing
With the events detected, the proposed approach of Section 4 can be applied to
synchronize independent rich metadata with the segment descriptions obtained automatically. We have obtained successful synchronization results with the textual
metadata provided by courtesy of SportsTicker. Table 4 summarizes the sample results for one football sequence and one baseball sequence, where one can ﬁnd that
the performance is very accurate as on average 97% of the plays are exactly synchronized with their corresponding textual metadata, and the synchronization for
remaining plays is only oﬀset by at most one play.
Based on the resulting composite metadata, applications can be built to provide
novel and exciting ways of consuming the content. To visually demonstrate the beneﬁts of synchronizing the event detection results with independent rich metadata, we
have incorporated the results into an MPEG-7 compliant prototype browser featuring novel user interface paradigms for viewing sports summaries (we demonstrated
an earlier version of this system in Li et al., 2001). The rich media description is expressed in an MPEG-7 compliant XML document (van Beek and Smith, 2002). The
prototype system provides a synchronized multimedia viewing experience through
the implementation of a video-driven scroll window, which shows the textual information corresponding to the video segment that the viewer is watching. Screen shots
of the system are shown in Fig. 15 for baseball and football. The text window below
the video playback window contains textual annotations that are rolled upward in
synchronization with the video playback. The annotation for the event that is currently played back in the video window is displayed in gold color font. The annotation above the golden text is for the previous play that has been played back. The
annotation below is for the next event. The game information at the right hand side
of the rolling text window is also synchronized with the video playback and updated
accordingly.

Table 4
Sample synchronization results for one baseball sequence and one football sequence
Game

Length of
video
processed

Number
of plays

Number
of plays
detected

Exact
Synch

Near
Syncha

Single
Shifted
Synchb

Multi
Shifted
Synchc

Baseball

Yankees vs
Diamondbacks

66

66

64

2

0

0

American
football

SuperBowl
XXXVI

53 min
(three
innings)
94 min
(ﬁrst half)

78

77

76

0

1

0

a
b
c

Detected play and SportsTicker data are oﬀ by one-half play, or duplicate label.
The above oﬀset is by one play.
The above oﬀset is by two or more plays.

420

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

Fig. 15. Synchronized multimedia-viewing experience for baseball (A) and football (B)—two screen shots
from our prototype system. To the right hand side and lower-right corner of the main video window,
shown are the game statistics extracted from the SportsTicker data feed (the statistics are updated as the
video plays forward). Directly below the window is a list of scrolling textural descriptions of the underlying
game, with each line corresponding to a play, and the line in golden font corresponding to the current play
that is being played.

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

421

Note the semantic richness of the annotations in Fig. 15. For example, the type of
the action and the name of the player(s) involved in the action are included. Clearly,
the media description that is used in the enhanced browsing example shown in Fig.
15 can be used in indexing the video in a database. Due to the semantic richness of
the description, users will be able to capture rich meaning as they query the database.
Due to the segment structure of the description, users are also able to retrieve event
clips from diﬀerent games stored in the database according to their queries, e.g.,
using a favorite playerÕs name. Such retrieval and browsing applications can be used
in a production environment, or they can be oﬀered as a service, e.g., on the Internet.

6. Conclusions and discussion
We have proposed a general framework for sports video analysis, aiming at bridging the semantic gap, which is a major challenge for current multimedia management
systems. The proposed framework includes a unifying model for modeling both action-stop and continuous action sports. We have demonstrated the generic nature of
the model by successfully applying it to diﬀerent sports, particularly to the development of automatic event detection algorithms for American football, baseball, sumo
wrestling, and soccer.
Another important advantage of the proposed event based model is the fact that it
facilitates synchronization and merging with independently generated rich metadata,
which typically have an event-based granularity. Upon synchronization and merging, the resultant composite metadata contain not only video indexing points of
event segments and simple classiﬁcation of events contained therein, but also independent rich metadata that are typically generated by human experts. This composite description stream, when applied to the underlying video in the context of
database indexing and retrieval, enhanced nonlinear browsing, and summarization,
signiﬁcantly increases the value of the video content for both content providers and
end users.
We have evaluated the performance of the proposed framework and algorithms
through extensive experiments as reported in Section 5. The algorithms were implemented in software on ordinary PC platforms. We have achieved faster than realtime (i.e., processing at a rate faster than 30 frames/s) performance. The algorithms
that we have reported here constitute a suite of technologies that we refer to as HiImpact Sports (Dimitrova et al., 2002).
We would like to point out that algorithms presented in this paper are based on
deterministic reasoning, using rule bases that are derived from domain knowledge.
These algorithms are easy to implement and computationally eﬃcient. There exist
other alternative methods of probabilistic inference such as the Hidden-MarkovModel based approaches proposed in (Li and Sezan, 2001, 2002) and in other related
work (Boreczky and Wilcox, 1998; Eickeler and Muller, 1999; Leonardi and Migliorati, 2002; Wolf, 1997). Probabilistic approaches may be appropriate and have
advantages in some situations. For example, when it is diﬃcult to specify a set of
rules for inference, a probabilistic approach that is capable of learning may handle

422

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

the inference problem better. Also, a probabilistic approach may avoid the diﬃculty
of choosing hard thresholds by absorbing the uncertainties into the modeling itself.
One diﬃculty with probabilistic inference is that training is typically needed and it
may always encounter the generalization problem in practice.
Application of the proposed framework to other types of sports, and building
models and developing algorithms for types of sports video other than broadcast,
are among our future directions.

Acknowledgments
We thank SportsTickerand ESPN for providing us the textual and video data.
We greatly appreciate the permission granted to us by NFL and MLB for using
images of single frames from their TV broadcast content.

References
SportsTicker: Available from: <http://www.sportsticker.com>.
Babaguchi, N., Kawai, Y., Kitahashi, T., 2002. Event based indexing of broadcast sports video by
intermodal collaboration. IEEE Trans. Multimedia 4 (1).
Babaguchi, N., Kawai, Y., Yasugi, Y., Kitahashi, T., 2000. Linking live and replay scenes in broadcasted
sports video. In: Proceedings of ACM Multimedia 2000 Workshop on Multimedia Information
Retrieval (MIR2000), November 2000, pp. 205–208.
Boreczky, S., Wilcox, L.D., 1998. A Hidden Markov Model framework for video segmentation using
audio and image features. In: Proceedings of the IEEE International Conference on Acoustics, Speech,
and Signal Processing, Seattle, WA, 1998.
Choi, S., Seo, Y., Kim, H., Hong, K.-S., 1997. Where are the ball and players?: Soccer game analysis with
color-based tracking and image mosaick. In: International Conference on Image Analysis and
Processing, Florence, Italy, September 1997, pp. 196–203.
Dimitrova, N., Zhang, H.-J., Shahraray, B., Sezan, I., Huang, T., Zakhor, A., 2002. Applications of videocontent analysis and retrieval. IEEE Multimedia 9 (3), 42–55.
Eickeler, S., Muller, S., 1999. Content-based video indexing of TV broadcast news using Hidden Markov
Models. In: Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal
Processing, Phoenix, AZ.
Ekin, A., Tekalp, A.M., 2002. A framework for analysis and tracking of soccer video. In: Visual Com. and
Image Processing (VCIP), January 2002, San Jose, CA.
Ekin, A., Tekalp, A.M., 2003. Robust dominant color region detection and color-based applications for
sports video. International Conference on Image Processing.
Gargi, U., Kasturi, R., Strayer, S.H., 2000. Performance characterization of video-shot-change detection
methods. IEEE Trans. Circuit Systems for Video Technology 10, 1–13.
Golin, S.J., 1999. New metric to detect wipes and other gradual transitions in video. In: Proceedings of the
IS and T/SPIE Conference on Visual Communications and Image Processing, San Jose, CA.
Gong, Y., Sin, L.T., Chuan, C.H., Zhang, H.-J., Sakauchi, M., 1995. Automatic parsing of TV soccer
programs. IEEE Conf. Multimedia Systems Comput., 167–174.
Kawashima, T., Tateyama, K., Iijima, T., Aoki, Y., 1998. Indexing of baseball telecast for content-based
video retrieval. In: Proceedings of the IEEE International Conference on Image Processing, Chicago,
IL.
Leonardi, R., Migliorati, P., 2002. Semantic indexing of multimedia documents. IEEE Multimedia, April–
June, pp. 44–51.

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

423

Li, B., Errico, J., Ferman, M., Pan, H., van Beek, P., Sezan, M.I., 2001. Sports program summarization.
In: IEEE Conference on Computer Vision and Pattern Recognition 2001 Demonstrations, Kauai,
Hawaii, December.
Li, B., Sezan, M.I., 2001. Event detection and summarization in sports video. In: Proceedings of the IEEE
Workshop on Content-Based Access to Video and Image Libraries. IEEE CS Press, Los Alamitos, CA.
Li, B., Sezan, M.I., 2002. Event detection and summarization in American football broadcast video.
Proceedings of SPIE 4676, 202–213 (Proceedings of the IS&T/SPIE Conference on Storage and
Retrieval for Media Databases, San Jose, CA, 2002).
Lienhart, R., 1999. Comparison of automatic shot boundary detection algorithms. In: Proceedings of the
IS&T/SPIE Conference on Visual Communications and Image Processing, San Jose, CA.
Lu, H.B., Zhang, Y.J., Yao, Y.R., 1999. Robust gradual scene change detection. In: Proceedings of the
IEEE International Conference on Image Processing, Kobe, Japan.
Naphade, M.R., Mehrotra, R., Ferman, A.M., Warnick, J., Huang, T.S., Tekalp, A.M., 1998. A highperformance shot boundary detection algorithm using multiple cues. In: Proceedings of the IEEE
International Conference on Image Processing, Chicago, IL.
Pan, H., Li, B., Sezan, M.I., 2002. Automatic detection of replay segments in broadcast sports programs
by detection of logos in scene transitions. In: Proceedings of the IEEE ICASSP 2002, Orlando, FL.
Pan, H., van Beek, P., Sezan, M.I., 2001. Detection of slow-motion replay segments in sports video for
highlights generation. In: Proceedings of the IEEE International Conference on Acoustics, Speech, and
Signal Processing, Salt Lake City, UT.
Rui, Y., Gupta, A., Acero, A., 2000. Automatically extracting highlights for TV baseball programs. In:
Proceedings of the ACM Multimedia 2000, Los Angeles, CA.
Saur, D.D., Tan, Y.-P., Kulkarni, S.R., Ramadge, P., 1997. Automatic analysis and annotation of
basketball video. Proceedings of SPIE 3022, 176–187.
van Beek, P., Smith, J., 2002. Navigation and summarization. In: Majunath, B., Salembier, P., Sikora, T.
(Eds.), Introduction to MPEG-7. Wiley, New York.
Wolf, W., 1997. Hidden Markov Model parsing of video programs. In: Proceedings of the IEEE
International Conference on Acoustics, Speech, and Signal Processing, Munich, Germany.
Xie, L., Chang, S.-F., 2002. Structure analysis of soccer video with hidden Markov models. In:
Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing.
Yow, D., Yeo, B.-L., Yeung, M., Liu, B., 1995. Analysis and presentation of soccer highlights from digital
video. In: Proceedings of the Second Asian Conference on Computer Vision, Singapore.
Zhong, D., Chang, S.-F., 2001. Structure analysis of sports video using domain models. In: Proceedings of
the IEEE International Conference on Multimedia and Expo, August 2001, Tokyo, Japan.
Zhang, D., Chang, S.-F., 2002a. Event detection in baseball video using superimposed caption
recognition. In: Proceedings of the ACM Multimedia Conference.
Zhang, D., Chang, S.-F., 2002b. General and domain-speciﬁc techniques for detecting and recognizing
superimposed text in video. In: International Conference on Image Processing.
Baoxin Li received his Ph.D. in Electrical Engineering from the University of Maryland, College Park in
2000. He is currently a senior researcher with Sharp Laboratories of America, Inc., Camas, WA, working
on multimedia analysis for consumer applications. He was previously with Center for Automation Research at the University of Maryland, working on face and object tracking and veriﬁcation in video,
automatic target recognition, and neural networks. His interests include pattern recognition, computer
vision, neural networks, and multimedia processing.
James H. Errico received his B.S.E.E. from Massachusetts Institute of Technology in 1986 and his
M.S.E.E. from Illinois Institute of Technology in 1993. He is currently working with Sharp Labs of
America in Camas, WA, as System and Software Developer. Mr. Errico is interested in research related to
multimedia processing and multimedia systems, including sports video analysis and metadata synchronization, and personalized media selection and browsing. In addition to these ﬁelds of interest, Mr. Errico
has developed online handwriting recognition systems, order management systems for open outcry
trading, and hardware and software for two-way radio systems.
Hao Pan received the B.E. and M.E. degrees in Electronic Engineering from Tsinghua University, Beijing,
China, in 1991 and 1993, respectively, and Ph.D. degree in Electrical Engineering from University of
Illinois at Urbana-Champaign in 2001. From 1993 to 1996, he was an assist professor and later a lecturer

424

B. Li et al. / J. Vis. Commun. Image R. 15 (2004) 393–424

in the Department of Electronic Engineering, Tsinghua University. In 2001, he joined Sharp Labs of
America, Camas, WA, where he is now a senior researcher. His research interests include signal processing, multimedia, and pattern recognition.
Ibrahim Sezan received his Ph.D. in Electrical Engineering from Rensselaer Polytechnic Institute in 1984.
He is the Director of Information Systems Technologies (IST) Department at Sharp Laboratories of
America (SLA), Camas, WA. The IST department creates technologies and innovates in the areas of visual
system modeling, ﬂat panel display algorithms, video summarization and semantics extraction, smart
vision, and information management. Before joining SLA in 1996, he headed the Motion and Video
Processing Technology Area at Kodak Research Laboratories. He joined Kodak in 1984, where he
conducted and led research in areas including image restoration, digital radiography, and digital motion
picture processing and visual eﬀect generation.

VIDEO INPAINTING FOR LARGELY OCCLUDED MOVING HUMAN
Haomian WangI, Houqiang LiI, Baoxin Li2
'University of Science and Technology of China, Hefei, 230027, P.R. China
2Computer Science & Engineering, Arizona State University, Tempe, Arizona, U.S.A.
whaomian @ mail. ustc. edu. cn, lihq @ ustc. edu. cn, Baoxin.Li @ asu. edu
ABSTRACT

In this paper, a video inpainting approach is proposed,
which targets at repairing a video containing moving
humans that are largely or completely occluded or missing
for some of the frames. The proposed approach first
categorizes typically periodic human motion in a video into
a set of temporal states (called motion states), and then
estimates the motion states for the frames with missing
humans so as to repair the missing parts using other
undamaged frames with the same motion states. This
deviates from common approaches that directly repair the
pixels of the damaged parts. Experiments demonstrate that
the proposed method can well repair the damaged video
sequences without introducing strong artifacts that exist in
many existing techniques.
1. INTRODUCTION
Image/video inpainting serves to fill in the missing parts or
to replace the undesired parts of an image or a video
sequence. The technique has many applications including
restoration of corrupted images, surveillance video
processing, and video editing during post-production in the
movie industry, etc. In the case of video, one important task
in inpainting is to repair a moving object/human.
Current video inpainting methods (e.g., [1-7]) for repairing
moving objects in video range from those based on
extension of 2D image inpainting techniques to those that
directly employ spatiotemporal analysis. While most of
these methods work reasonably well for repairing not-solarge moving objects, they often suffer from noticeable
artifacts (e.g., loss of motion consistency) and/or oversmoothing in face of largely occluded moving objects.
Methods relying on local motion field (e.g. [7]) may also be
very sensitive to noise.
In this paper, we propose a novel method for the specific
task of repairing largely damaged moving human in video
sequences. Specifically, our experiments deal with video
with damaged frames in which a moving human is
completely missing. The potential application scenarios for
our method include, for example, virtually revealing a
walking human that is occluded by some other objects for a
short period in a video, or repairing video with regions of
humans being contaminated during transmission. Instead of
directly repairing the pixels of the missing part, as done in

1-4244-1017-7/07/$25.00 ©2007 IEEE

1719

most existing methods, our approach first categorizes
typically periodic human motion in a video into a set of
temporal states (called motion states), and then estimates the
motion state of a missing human for a given frame. The
estimate is then used to select a set of candidate frames from
other parts of the video. Finally, an optimal frame is chosen
from the candidates, using the best motion continuity as the
criterion. The moving human in the chosen frame is then
used to repair the given damaged frame.
In Section 2 we present the details of the proposed approach,
followed by sample experimental results in Section 3 and a
brief discussion of future work in Section 4.
2. PROPOSED VIDEO INPAINTING ALGORITHM
This section presents the proposed method. We first give an
overview of the algorithm in terms of its computational
steps, and then discuss the steps in detail. We make the
following assumptions about the video to be processed: the
camera is fixed; the scene is composed of stationary
background with a moving human (multiple humans can be
processed similarly in principle); the moving human to be
repaired has periodic motion. These assumptions are
approximately true for video from many applications. In
practice, some of these assumptions can be relaxed by
including additional processing modules (e.g., using a
sensor motion compensation module to remove the first

assumption).
Our approach consists of the following major steps:
Stepl Background inpainting: This is needed for
repairing damaged background regions. We employ
established existing approaches for this task.
Step2 Motion state estimation: The feature points of a
moving human in each undamaged frames are detected
(manually in current experiments), and a motion state vector
is computed based on the feature points.
Step3 Motion state classification: The motion state
vectors from the undamaged frames are clustered and then
labeled with the cluster indices.
Step4 Motion state prediction: A motion-state transition
model is constructed, which is used to predict the motion
states of the damaged frames.
Step5 Moving human inpainting: The moving human of
each damaged frame is copied from other undamaged
frames according to the predicted motion states. In this step,
graph-cut-based image inpainting [4] is used.

ICME 2007

The primary contribution of our approach lies in the use of
motion state prediction in repairing a missing human. An
accurate prediction for the motion of the missing human will
ensure optimal repair of the video in terms of maintaining
good motion continuity, which is a significant factor for
good visual quality of the repaired video. Since we repair
the damaged frame with nearby frames of the same motion
state in the same video (as opposed to creating the pixels in
some way), our approach introduces little visual artifacts
and blur, which are common issues in many existing
techniques.
We now present the details of the steps.
2.1. Background inpainting
Two types of backgrounds need to be repaired: that
occluded by stationary foreground objects and that occluded
by moving foreground objects. For the first type, many well
established still image inpainting methods can be used to
repair the background (e.g. [8-9]). The second type of
occlusion can be solved by using the background obtained
from other frames where the occlusion does not occur.
2.2. Motion state estimation
We define feature points as the joints of the moving
human's body. In our current experiments with video
containing a walking human, we manually detect 7 feature
points in the moving human to be repaired. For a man
walking from the left side to the right side, these feature
points correspond to the head, the elbow, the wrist of the
right hand, both knees, and both ankles, as illustrated in
Fig.1 Then we compute a motion-state vector for each
undamaged frame. The motion-state vector is defined as
follows. We first set one of the feature points as the
reference point (in our experiments we use the feature point
corresponding to the head). Then we set up a polar
coordinates system with the reference point as the origin.
Other six feature points will correspond to 6 angles with
respect to the origin, O01 02.,.... 06 (see Fig. 1). The motionstate vector is defined as:

Vi = (tgoilI,tg0i2.,
* I tg0i6) (1)
where the subscript i indicates the frame index i.

multiple humans, we may keep multiple vectors for each
frame.) Fig. 2 shows an example of two frames having the
same motion state. Then we classify the motion state vectors
based on K-means clustering. In the algorithm, we define
the distance between two motion-state vectors i andj as
6

D(inj)= wt I
t=l

t-jtI

(2)

where w, is a weight vector for controlling the contribution
of the feature points to the computation of the distance,
since according to our observation, the feature points in the
lower limbs play a bigger role than the feature points in the
upper limbs in determining a moving human's motion state.

Figure 2. Left: the 8ff frame of one sequence. Right: the 25th
frame of the same video. Since the moving human has similar
motion/pose in both frames, these frames are regarded as
having the same motion state.

Upon the completion of the clustering, we label the clusters
sequentially according to the temporal order of the
underlying frames. Then each undamaged frame can be
associated with a state index. (Again, in the case of multiple
humans, there will be multiple indices.)
2.4. Motion state prediction
In this step, we attempt to predict the motion state for those
frames with the human missing. This step is inspired by
human motion synthesis techniques, such as [10], where
typically a Markov model is used to model and synthesize
human motion. In our current study, since we handle only
simple and periodic motion, e.g. walking, running, and
jumping, we adopt a simple first order motion state
transition model as illustrated in Fig. 3, where the states can
transit only to its next state and itself

Origin

Figure 3. A simple first-order transition model for the
motion states of a video. State n goes back to state 1 due to
periodicity of the motion (in such as walking).

a

,

0

Figure 1. Illustration of the state-vector

2.3. Motion state classification
After the above step, each undamaged frame has its own
motion state vector, and the vector represents the motion
state of the moving human in this frame. (In the case of

1720

With this motion state transition model, we can predict the
motion states of the damaged frames. Assume that frame i is
a damaged frame and its neighbor frame i-i (or i+±) is a
good frame. Further assume that the motion state of frame i1 is statea. Then the motion state of frame i should be
state a or a + l(or a - 1) It is worth mentioning that, if the

model is extended into a general probabilistic graph model
to repair human with more complex motion, the prediction
of a state / for a given frame can be achieved using
= arg Max[P( a)]
(3)

which means that the most likely motion state,8 transiting
from state a will be chosen.
2.5. Moving human inpainting:
Assume that frame i is a damaged frame, and its predicted
motion state is state,. In a video, we can detect the set of
frames {Fi1,Fi2, .Fim3 all having the state label ,B. In this
step we choose a frame f E {Fil, Fi2
Fim } , and paste the
human contained inf into frame i. Our criteria in making the
choice is that, we choose a frame f whose neighboring
frames are most similar to the corresponding neighbors of
the damaged frame, in terms of the moving human.
Furthermore, if there are a series of damaged frames, e.g.
frames i, i+±,..., i+m, and for frame i+t, we choose framef,
then for frame i+t+±, we choose framef+1. These strategies
are intended to maximally maintain the motion continuity.
Once the best source frame f has been determined, the
moving object segmentation technique of [11] is used to
obtain the moving human in f An example is illustrated in
Fig. 4.
........

r igure 4.

Mvioving numan segmenmtaon.
The rest of the work is to paste the moving human into the
damaged frames. Simply pasting the moving human from an
undamaged frame into a damaged frame may lead to
obvious artificial effects. We largely avoid such problem
using graph cut based image inpainting technique. Graph
cut based image inpainting is to find the least visible seam
between the target and the source fragments in the
overlapped region, and the pasting path is across the least
visible seam. More details can be found in e.g. [4].
Another noticeable artificial effect may be caused due to the
difference of brightness between frames, since the exposure
of the camera may change if the scene or background
changes. To avoid such drawback, we bring in image editing
technique [12] to our algorithm. Using image editing
technique, we can merge one object from an image into a
new image and eliminate the artificial effects caused by the
difference of brightness or contrast between different
images.
3. EXPERIMENTAL RESULTS

1721

We apply our approach to different video sequences with a
moving human. In these video sequences, we manually crop
out a block covering the human, and then attempt to repair
the frame using our approach. Figures 5-7 demonstrate
three samples from our experiments, with the following
table summarizing the characteristics of the sequences (in
the table, the "source frame" column indicates which frames
are used to repair the damaged frames, and the number of
states is the state number n obtained in the clustering stage).
Video
Fig. 5
Fig.6
Fig.7

of
frame
60
80
70

#

I

#

of Damaged
states frame
13 ,14'
320*240 I 14
22
13th 15th
300* 100
14
256* 192
26 Ih 29th

Resolution

Source
frame

29th 30th
I 42th-44th

12'h-15th

For easy appreciation of the results, the corresponding video
is posted at http://home.ustc.edu.cn/-whaomian/
4. CONCLUSION AND FUTURE WORK
In this paper, we proposed a novel approach to repairing
largely occluded moving human in video. Our approach is
based on the estimation and prediction of the motion state of
the frames. Experiments show that our method can avoid
common artifacts in typical existing video inpainting

methods.
As a future direction, we will extend our work to repair
moving humans with more complex motion, potentially with
a probabilistic graphical with sophisticated state transitions.
Another important future direction is to develop automatic
method for detecting the feature points.

5. ACKNOWLEDGEMENT
This work is supported by NSFC General Program under
contract No. 60672161, 863 Program under contract No.
2006AAOlZ317, and NSFC Key Program under contract
No. 60632040.
6. REFERENCES
[1]
[2]

[3]
[4]
[5]

[6]

Y. Wexler, E. Shechtman, and M. Irani, "Space-time video
completion," Proc. EEE Conference on Computer Vision and
Pattern Recognition, vol.], pp. 120-127, 2004.
K. A. Patwardhan, G. Sapiro, and M. Bertalm'io, "Video
inpainting of occluding and occluded objects," Proc. IEEE
Intl. Conf Image Processing (ICIP) Genoa, Italy, 2005.
K. Patwardhan, G. Sapiro, and M. Bertalmio, "Video
inpainting under camera motion," IEEE Trans. Image
Processing, to appear.
Y.T. Jia, S.M. Hu, and R. Martin "Video completion using
tracking and fragment mergin", Visual Comput (2005)
21:601-610.
Y. Zhang, J. Xiao, and M. Shah. Motion layer based object
removal in videos. In Proc. IEEE Workshop on Applications
of Computer Vision, pages 516-521, 2005
J. Jia, T. Wu, Y. Tai, and C. Tang, "Video Repairing:

Inference of Foreground and Background Under Severe

[7]
[8]
[9]

Occlusion," Proc. IEEE Conf on Computer Vision and
Pattern Recognition, vol. 1, pp. 364-371, 2004.
Takaaki Shiratori, Yasuyuki Matsushita, Sing Bing Kang,
Xiaoou Tang, "Video Completion by Motion Field Transfer",
Proc. IEEE Conf Computer Vision and Pattern Recognition,
2006
M. Bertalmio, G. Sapiro, V. Caselles, and C. Ballester,
"Image inpainting," SIGGRAPH 2000, pp. 417-424, 2000.
M. Bertalmio, L. Vese, G. Sapiro, and S. Osher,
"Simultaneous texture and structure image inpainting," IEEE
Trans. on Image Processing, vol. 12:8, pp. 882-889, 2002.

[10] Y. Li , T. Wang , H. Shum, Motion texture: a two-level
statistical model for character motion synthesis, Proc.
Conference on computer graphics and interactive techniques,
July 23-26, 2002, San Antonio, Texas.
[11] S.Y. Chien, S.Y. Ma, L.G. Chen, Efficient Moving Object
Segmentation Algorithm Using Background Registration
Technique. IEEE Trans. Circuits and Systems for Video
Technology. July 2002, vol 12:577-586.
[12] Perez, P., Gangnet, M., Blake, A. 2003. Poisson image
editing. Proceedings ofACMSIGGRAPH, 313-318.

Figure 5

tigure

/

Figure 5 - Figure 7: In each figure, the top row is the original video, the middle row the video with damaged
frames, and the bottom row the video with repaired frames.

1722

838

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 3, MARCH 2007

Adaptive Rao–Blackwellized Particle Filter
and Its Evaluation for Tracking in Surveillance
Xinyu Xu and Baoxin Li, Senior Member, IEEE

Abstract—Particle filters can become quite inefficient when
being applied to a high-dimensional state space since a prohibitively large number of samples may be required to approximate
the underlying density functions with desired accuracy. In this
paper, by proposing an adaptive Rao–Blackwellized particle filter
for tracking in surveillance, we show how to exploit the analytical
relationship among state variables to improve the efficiency and
accuracy of a regular particle filter. Essentially, the distributions
of the linear variables are updated analytically using a Kalman
filter which is associated with each particle in a particle filtering
framework. Experiments and detailed performance analysis using
both simulated data and real video sequences reveal that the
proposed method results in more accurate tracking than a regular
particle filter.
Index Terms—Particle filter, Rao–Blackwellization, video-based
surveillance, visual tracking.

I. INTRODUCTION
ISUAL tracking is an important step in many practical
applications including video-based surveillance. In recent
years, particle-filter-based visual tracking has been extensively
studied (e.g., [1]–[5]). Particle filtering has been shown to offer
improvements in performance over some conventional methods
such as the Kalman filter in nonlinear/non-Gaussian environments [6]. However, the large number of samples required to approximate the posterior density render its use difficult in high-dimensional state spaces.
In some cases, the system model may have a “tractable
structure” such as with some components having linear dynamics that can be analytically estimated using exact filters
conditional on other components in a sequential Monte Carlo
(SMC) framework [7], [8] like particle filtering. In these cases,
exact filters like Kalman filter, the HMM filter, or any other
finite-dimensional optimal filters [10], could be exploited to
marginalize out the linear dynamics. This technique is referred
to as Rao–Blackwellization [11]. The resultant method is often
called Rao–Blackwellized particle filter (RBPF).
deGenerally, suppose that we have an estimator
pending upon two variables and , the Rao–Blackwell Theorem reveals that its variance satisfies [11]

V

(1)
Manuscript received November 7, 2005; revised September 11, 2006. The
associate editor coordinating the review of this manuscript and approving it for
publication was Dr. Zhigang (Zeke) Fan.
The authors are with the Department of Computer Science and Engineering,
Arizona State University, Tempe, AZ 85287 USA (e-mail: xinyu.xu@asu.edu;
baoxin.li@asu.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TIP.2007.891074

is non-negative, the variance of the
Since
is less than that of the original
estimator
. The formal justification can be found in, for
estimator
example, [8], [24]. One can interpret the Rao–Blackwell theorem by saying that the estimator obtained by the calculation of
is superior to the original
conditional expectation
, and the superiority manifests in the reduction in
one
the variance of the estimates.
denote the state
For the visual tracking problem, let
the observation, with subscript the
to be estimated and
time index. The key idea of RBPF is to partition the original
into two parts
(root variables) and
(leaf
state-space
is a distribution that can
variables), such that
be computed exactly conditional on the root variables, and the
will be estimated using Monte Carlo
distribution
methods such as particle filtering. The justification for this
decomposition follows from the factorization of the posterior
probability [10]
(2)
If the same number of particles is used in a regular particle
filter and a RBPF, intuitively, the latter will provide better estimates for two reasons: first, the dimension of
is smaller than
; second, optimal algorithms
may be used to estimate the “tractable substructure.” Refercan be efficiently upence [10] shows that
dated using Kalman filter when the initial uncertainty for leaves
is Gaussian, and the conditional probability distributions of the
observation model and system dynamics for the leaves are linear
functions of the leaf states. In this paper, we will show how
Kalman filter is combined with particle filtering to facilitate
tracking in a surveillance application, with a physically meaningful dependency model for the relationship between the leaf
and the root variables.
RBPF has been applied in some state estimation problems.
For example, in [12], RBPF is used to integrate out the subspace
coefficients in an Eigen Tracking problem. In the problem of
tracking multiple people using a combination of anonymous and
id sensors [13], RBPF is used to estimate the locations and identities of multiple objects, with Kalman filter being used to track
an individual person. In [14], Freitas et al. combines Kalman
filter with particle filter for fault diagnosis for a mobile robot,
where Kalman filters are applied over continuous states and the
samples are obtained over discrete states. In [15], the nonlinear
ball motion model and robot location are tracked using particle
filter while ball location and velocity are estimated by Kalman
filter. In integrated aircraft navigation [16], Kalman filter is used
to track the velocity components, and particle filtering is used to

1057-7149/$25.00 © 2007 IEEE

XU AND LI: ADAPTIVE RAO–BLACKWELLIZED PARTICLE FILTER

estimate the position components. A more generalized discussion regarding marginalized particle filter for mixed linear/nonlinear state-space models can be found in [9].
Although RBPF has been studied in the aforementioned application areas, its application in tracking for surveillance has
yet to be fully explored. In particular, we believe that a thorough quantitative performance comparison between RBPF and
a regular particle filter (PF) will help us to understand the advantages of RBPF. The key contribution of this paper is, thus,
two fold. First, with video-based surveillance as a case study, we
utilize the constraints imposed by typical camera-scene configuration to partition the original state space into two subspaces,
and a RBPF algorithm is proposed for tracking in surveillance.
Second, we carry out thorough evaluation of the proposed RBPF
algorithm in comparison with a regular PF [3] through experiments with both simulated data and real video. Our experiments show that the improvements of the proposed RBPF over
a regular PF manifest in four aspects: increased estimation accuracy, reduced estimates variance, reduced number of particles required to achieve the same level of accuracy, and reduced
weight variances.
The remaining of the paper is organized as follows. Section II
presents the proposed algorithm. In Section III, we evaluate
the performance of the proposed algorithm with comparison
to a regular PF using simulated data. Section IV presents the
tracking results on real video sequences. In Section V, related
work on dimension reduction in visual tracking problems is discussed. Some important discussions are presented in Section VI,
and, finally, we conclude in Section VII.
II. RBPF FOR TRACKING IN SURVEILLANCE
A. Partition the State Space
In typical surveillance applications, most of the time, the
tracked objects are constrained to move on a dominant plane
(e.g., the ground), and the camera is usually higher than the
tracked object. Fig. 1 illustrates such a camera-scene configuration, where (b) is a geometric representation of (a). In Fig. 1(b),
suppose a person is walking on the ground plane , the ground
is projected onto the image plane by camera , with the
vanishing line for the ground plane. Any scene point projected
onto the vanishing line is at the same distance from plane as
the camera center [17]. If a scene point is farther from than
the camera is, then its image lies “above” the vanishing line
and “below” if it is closer to the ground than the camera. So, if
the moving object is not higher than the camera, the image of
the object will always lie below the vanishing line, and when
it moves towards the camera, the scale of the object on the
image will get bigger as the coordinate on image plane gets
bigger, and vice versa. Fig. 1(b) clearly shows the dependence
of the scale change on the coordinate change in the image
domain. In these situations, the constraints imposed by the
camera-scene configuration can be exploited to deduce the
dependency relationship among the state variables.
Formally, in our work we use the following 8-D ellipse model
to describe the dynamics of the target (like [18])

839

Fig. 1. (a) Typical camera configuration in surveillance. (b) The dominant
plane is projected onto the image plane. If the person moves towards the
camera, the image domain y coordinate and the size of the person both become
larger.

where
represents the center of the ellipse,
the
motion velocity,
the horizontal and vertical half length
of the ellipse axes, and
the corresponding rates of
scale change on the axes. With the above idea, the scale change
of a moving object is related to its position along the axis,
so that the scale change can be estimated conditional on the
location components. This facilitates the partition of the original
8-D state space into two groups: the root variables containing
the motion information and the leaf variables consisting of
the scale parameters, which are denoted by

B. Overview of the Method
In this work, the root variables are propagated by a first order
system motion model defined by
(3)
where
is the transition matrix and
is random noise.
Conditional on the root variables, the leaf variables forms a
linear-Gaussian substructure specified by
(4)
where is a function encoding the conditional relation of on
from
to . The image observations used for computing
likelihood for both linear states and nonlinear states, denoted by
, are the color histogram within a sample ellipse and the intensity gradients along the ellipse boundary. Since both the color
histogram and gradient cues do not follow a linear-Gaussian relationship with the state variables, so the observation model is
give in a general form
(5)
where
is random noise and a nonlinear function. Due to
the nonlinear measurement relation with the states and the nonGaussian noise in measurements from the image, the object’s
states are typically better modeled by probabilistic multimodal
densities; hence, we use particle filter for sampling the posterior
density of the state variables.
Once the root variables are propagated one step ahead, the
leaf variables can be computed by making use of the dependency between the leaf and the root using a Kalman filter. The
reason why Kalman filter can be used is that: a) conditional on
the root variables, the leaf variables form a linear-Gaussian substructure specified by (4); b) we introduce auxiliary observations
, which are simply
, to serve as the observations used

840

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 3, MARCH 2007

Fig. 2. Relationship among various components in the modeling, similar to a
Dynamic Bayesian Network.

in the Kalman filter. Thus, the observations
lationship with state

is typically static, making
[20], the variance of
it difficult to track objects under dramatic and fast
tracking motion. In the proposed RBPF algorithm, the
variance of is made adaptive, the detail will be presented at algorithm Step 6).
. The minus
After this step, we have
sign in the superscript denotes that the corresponding
variable is a priori estimate and denotes an un-initialized value. In a regular PF, immediately after
this step, we are supposed to weight each sample
, but here,
by the observation likelihood
Kalman prediction is performed before weighting
each sample, which is intended to analytically predict
a new mean and covariance for the leaf variables by
making use of the dependency between and . See
Step b) for details.
b) Kalman prediction for leaf states according to

form a linear re(6)

c) We assume that the system and observation model for the leaf
variables are driven by Gaussian random noise
and
with and the system and the measurement noise covariance, respectively.
Fig. 2 illustrates the relationships between the state variables
and the observations in a structure similar to a dynamic Bayesian
network. The object motion at time , , only depends on the
as the motion evolution is described by
previous motion
a Markov process of order one. The scale of the ellipse at time
, , depends on previous ellipse scale,
, previous object
motion
and current motion . The observations for the
leaf at time , , only depends on current scale . The true
observations for deriving sample weight, , depends on both
current object motion, , and current scale .
C. Proposed Algorithm
Fig. 3 illustrates the proposed RBPF algorithm. Just like regular particle filters, RBPF represents the posterior density by a
. Each
set of weighted particles:
particle maintains not only a sample from
, which we
denote by , but also a parametric representation of the distri, which consists of the mean vector of the
bution
, and the estimation error covariance
leaf variables,
for leaves
[10]. So each particle is
. The proposed RBPF
represented by a triplet
algorithm will sample the nonlinear non-Gaussian motion
using particle filter, while apply Kalman filter to estimate the
scale parameters
and
conditional on the motion state. In
the following, we detail the steps of the algorithm.
1) Propagate samples
a) Sample the object motion according to

(8)
According to the Kalman filter model defined by (4)
and (6), we project forward the state and error covariance using (9). The first four formulas in (9) perform prediction for the mean of the leaf variables, and
the last two are the covariance and the observation
prediction, respectively. The first formula means that
each time the coordinate of the object increases
pixels from
to , the half length of ellipse vertical axis will be increased pixels accordingly. Note
that, here, the predictions for the leaf variables have
exploited the conditional relations between and ,
i.e., the scale increase from
to depends on the
coordinate increase from
to , and the locasampled in Step a) has also been
tion prediction
made use of. In the third formula, the scale change
will keep the aspect ratio during tracking. In practice,
parameter and should be adjusted according to the
angle between the ground plane and the image plane:
the larger the angle, and the higher the uncertainty
of the scale change conditional on the location. The
two parameters and influence the estimation accuracy of the proposed algorithm as they control the
change rate of the sample ellipse size with respect
to the object motion. The sensitivity of the estimation accuracy to the dependency model will be further
discussed in Section III-G. After this step, we have

(7)
The samples are propagated at each time step by (3).
Most existing tracking algorithms, be the regular particle filter [3], ICondensation [19], or Auxiliary PF

(9)
2) Evaluate

weight

for
.

each

particle:

XU AND LI: ADAPTIVE RAO–BLACKWELLIZED PARTICLE FILTER

841

a) Compute the color histogram for each sample ellipse
characterized by ellipse center
and
scale

target histogram and the hypotheses histogram measured by Bhattacharyya coefficient. The larger is,
.
the more similar the distributions are
Another weight is based on the gradient

(10a)
(12c)
where is the Kronecker delta function and
asbins of the histogram to a given
signs one of the
color at location . Pixels that are closer to the region center are given higher weights specified by
otherwise

(10b)

This is the same model as proposed in [18].
b) Compute the gradient for each sample ellipse charand scale
acterized by ellipse center
. The gradient of a sample ellipse is computed as an average over gradients of all the pixels on
the boundary
(11a)
is set to the maxwhere the gradient at pixel
imum gradient by a local search along the normal line
of the ellipse at location
(11b)
A simple operator is used to compute the gradient in
axis and axis for pixel

and, finally, the gradient at point
as

is computed
(11c)

c) Compute the weight. We first compute two weights
for each sample. One is based on color histogram similarity between the hypothetical region and the target
model
(12a)
where
(12b)
and stands for the color histogram of a sample hypothesis in the newly observed image, and represents the color histogram of the target model. Equation (12b) essentially gives the similarity between the

Note that, before feeding the gradient into a Gaussian
distribution to get a gradient weight, the gradient of a
sample ellipse is divided by the maximum gradient of
all the samples to normalize it into range [0, 1].
The final weight for each sample is given by
with

(12d)

If we assume that the color histogram cue is independent of gradient cue, the final weight could also be
computed as
.
As pointed out by the author in [9], only one Riccati
instead of Riccati recursions is needed if the linear
states do not appear in the measurement relation,
which will lead to a substantial reduction in the computation time, but this is not the case in our problem,
because from Step 2a)–c), we see that both the preand the linear states
dicted nonlinear states
have contributed to the computation of
the weights, that is, both the nonlinear states and
the linear states will appear in the measurement (5).
Riccati recursions should be used
Consequently,
in the Kalman time and measurement update (9) and
(13). We think that this is why the computation cost
of our proposed algorithm is slightly higher than the
standard PF, as will be discussed in Section VI-B.
Also, since both the predicted nonlinear states and
linear states are involved in obtaining the weights,
the weights represent the conditional likelihood of
not only the nonlinear but also the linear states, i.e.,
the weights signify how well the predicted states
“match” with the true measurement.
3) Select samples. Resampling with replacement
. The latest
measurements will be used to modify the prediction PDF
of not only the root variables but also the leaf variables.
.
After this step,
Some important remarks are due regarding to Steps 2) and
3). A common problem with the particle filter is the degeneracy problem: after a few iterations, some of the particle weights may become dominantly larger than other
negligible weights. In the literature, there are two primary
methods to overcome this problem [2]: one is a good choice
of importance density [3], [21], the other is the use of resampling [3], [22], [23]. Although resampling can effectively lessen the degeneracy problem in low-dimensional
state estimation, its capacity for alleviating degeneracy in
high-dimensional state estimation becomes very limited.
In Section IV, we will show that Rao–Blackwell technique
can do a very good job in reducing degeneracy in such a
situation.

842

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 3, MARCH 2007

4) Kalman update for leaf variables. Kalman update is accomplished by (13a)–(13c) over the selected sample set.
is the Kalman gain which aims at minimizing the posterior error covariance. As the measurement error covariance approaches zero, the Kalman gain weights the meamore heavily while the predicted measuresurement
is “trusted” less and less. On the other hand.
ment
as the a priori estimate error covariance
approaches
zero, the predicted measurement is trusted more than the
measurement. Equation (13b) incorporates a measurement
into the a priori leaf state estimate to obtain an improved a posteriori leaf state estimate. One may question
acting as obserthe feasibility of
are calculated from their previous
vations since
values and root variables in (9). This is practically not a
has gone through the entire estimaproblem since
tion process and, thus, already incorporates the information in the root variables and the true measurements (color
histogram and gradient). Essentially, the auxiliary obsercan be viewed as being invation
directly linked to the root variables and the true measurethrough a non-Gaussian nonlinear function
ments at

(13a)
(13b)
(13c)
After this step, we have
. Note that if
is large),
the leaf prediction uncertainty is large (e.g.,
the Kalman update should be done immediately after the
Kalman prediction step, so that the inaccurate prediction
of leaf variables can be immediately corrected by incorporating a new observation. On the other hand, if the leaf prediction uncertainty is small, but the dynamic motion model
(3) is not accurate, then Kalman update is due here so that
the Kalman update operates on only those “good samples”
(those samples in the vicinity of the true object location)
which have been selected by the Selection step.
5) Compute the mean state at time . Since resampling has
been done by Selection, so, now, the mean state can be
simply computed as the average of the state particles
(14)
6) Compute the new noise variance
If fixed-noise variance is employed in the system model to
track objects moving with dramatically and fast changing
velocity, the range of the samples may not be sufficient to
cover a large motion change. This argument has been confirmed by the following simulations. In these simulations, a
point moving on a 2-D plane with a certain velocity generates an actual path; noise is then added to the actual path to
simulate a noisy measurement of the actual path. After that
the proposed RBPF with Steps 1)–5) is invoked to track the
actual path. We then measure the estimation mean-square
error (MSE) for horizontal velocity under different noise

variances for three motion settings: moving with small constant velocity, moving with nonconstant velocity within a
small range, moving with dramatically changing velocity.
We found that when the velocity is small and constant, we
only need a small noise variance to reach the smallest MSE,
but if the velocity changes dramatically, we need a much
larger noise variance to reach the lowest MSE.
These experiments show that adaptive noise is needed to
track objects with dramatically and fast changing motion.
Intuitively, the noise variance should be proportional to
the prediction error (since it determines the quality of
tracking). If the prediction error is small, we only need
noise with small variance to absorb the residual motion;
if prediction error is large, we need a large noise variance
to cover potentially large variations in the system state.
assumes a normal distribution
In our work, the noise
, where denotes the noise variance that
needs to be made adaptive.
In a tracking problem, it is hard to directly compute the
prediction error since we never know the true states of the
target object. So, in both the real data experiment and the
is computed which is
simulations, a similarity measure
inversely proportional to the prediction error. Intuitively,
if this similarity measure is large, it means that with the
current noise variance the algorithm still maintains a good
tracking, and, thus, we only need to adjust the variance a
little bit. Otherwise, if the similarity measure is small, this
may well be due to the object’s dramatic velocity change
that has resulted in poor tracking, and, thus, the noise variance needs to be enlarged. Hence, the noise variance is
inversely proportional to this similarity measure, and is
computed by
(15)
where
is the lower bound to maintain a reasonable
is the upper bound to constrain
sample coverage and
is very small.
the variance below a certain threshold if
The similarity in the real data experiments is computed
by (12a) and (12b) as a Gaussian likelihood between the
histogram of the previous state (i.e., target model) and the
histogram of the current mean state. The similarity in the
synthetic data experiments is computed by (16) as the Mahanalobis distance from the estimated current mean location to the current observed location. Finally, we plug
into (15) to obtain the new variance
(16)
In (16),
is the mean location estimated by RBPF, the
observation at time , and the estimation error covariance
.
of
Note that not only the noise variance of the Monte Carlo
part can be made adaptive, the noise variance of the
and
Kalman filter (e.g., the system noise covariance
the measurement noise covariance ) can also be made
adaptive, which should capture the similarity between the
size of a sample ellipse at the current time step and the
size of a sample ellipse at the previous time step.

XU AND LI: ADAPTIVE RAO–BLACKWELLIZED PARTICLE FILTER

843

After the true and noisy paths are generated, both the proposed RBPF and a regular PF are invoked ten times to track the
true path with random initializations using the noisy path as the
measurement. 200 particles are used in the two algorithms. With
the actual path as the ground truth, the performance of these two
algorithms can be quantitatively compared. In the RBPF esti,
mation, the root, the leaf and the observations are:
(in order to maintain the linear substructure
is also incorporated in the computaof the leaf variables,
tion of the mean of the leaf variables as a dummy element),
. The root is estimated by regular particle filtering,
and the leaf variables are estimated using Kalman filter by as, which is different from the
suming the dependency
used in the path generation. This is
true dependency
intended to test if the proposed algorithm still works fine even
if the exact dependency model between the state variables is not
available.
B. Increased Estimation Accuracy

Fig. 3. RBPF algorithm for tracking in surveillance.

III. EVALUATION OF THE PROPOSED RBPF ALGORITHM

Fig. 4 shows the true path and the average path estimated by
RBPF and PF over ten Monte Carlo simulations. In Fig. 5(a),
the location root mean-square error (RMSE) obtained by PF and
RBPF are compared. The location RMSE at time is computed
is the number of MC simulations,
by (19), where
denotes the true location states at time , and denotes the estisimulation at time . Fig. 5(a)
mated location states in the
reveals that the proposed RBPF outperforms the regular PF in
reducing estimation errors by a large margin

In this section, we evaluate the performance of proposed
adaptive RBPF algorithm by comparative study between regular
particle filter [3] and the proposed adaptive RBPF algorithm.

(19)

A. Simulation Settings
In the simulation, a true path and a noisy path are generated:
a point moving on a 2-D plane with nonconstant velocity generates an actual path; noise is then added to the actual path to
simulate a noisy measurement of the actual path. Then RBPF
and PF are invoked to “track” the true path using the noisy path
as the measurement. The state of the moving point at any time is
, in which
corresponds to
given by
the horizontal
the horizontal and vertical position and
and vertical velocity. When generating the true path, we assume
. This is to facilitate utilizing dependency relation between the root and the leaf variables to apply RBPF. The state
is projected forward by a nonlinear non-Gaussian model (17),
denotes uniform random noise
where

C. Reduced Estimates Variance
One advantage of Rao–Blackwellization is that it can reduce
the variance of the state estimates. To verify this, the standard
deviation (STD) of the state over 10 MC simulations at each
time instant is computed [(20) gives the way of computing STD
for ]. Fig. 5(b) shows the STD comparison for the vertical
location . We found that the variance of the RBPF estimates
is much lower than that of the PF for any one of the four state
components
(20a)

(20b)

(17)
is created by (18), where
is a
The noisy measurement
vector of uniform random noise distributed within [ 1 1]
(18)

D. Reduced Particle Numbers
Another advantage of Rao–Blackwellization is that it can reduce the volume of the space over which we need to sample.
This in turn brings the benefit that far fewer particles are needed
to reach the same level of accuracy, compared with a conventional PF. Table I illustrates such results from the proposed algorithm. In Table I, as the number of particles increases in PF,

844

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 3, MARCH 2007

Fig. 4. Left: True path versus path estimated by the proposed RBPF. Right: True path versus path estimated by a particle filter.

Fig. 5. (a) Comparison of location RMSE obtained by RBPF and PF. (b) Comparison of standard deviation for the y coordinate.

Rao–Blackwellization. To verify this in our algorithm, we compute the standard deviations of weights of 200 particles in the
RBPF and PF at each time step. Fig. 6(a) clearly shows that the
standard deviation obtained by RBPF is evidently less than that
of PF. The significance for the weight variance reduction is discussed in depth in Section IV.

TABLE I
LOCATION RMSE FOR RBPF/PF RUNNING
WITH VARIOUS NUMBERS OF PARTICLES

the location RMSEs given by (21) are decreasing. However, the
location RMSE obtained by PF cannot be decreased any more
by simply increasing the number of particles when the number
reaches 500. At that time the location RMSE obtained by PF are
still larger than that obtained by RBPF with 200 particles. Thus,
we may conclude that for this simulation setting, the proposed
RBPF only needs 200 particles to reach the same of level of estimation accuracy obtained by PF running with 500 particles

(21)

E. Reduced Weight Variances
It has been revealed [23] that the variance of true weights
(the weights before resampling) can be decreased after applying

F. Dependence Analysis
As we mentioned in Section II-C, the dependency relationship between the leaf and the root affects the performance of
the adaptive RBPF algorithm. In practice, it is usually difficult
to obtain an exact dependency model unless the surface normal
of the dominant plane and the image plane are known. Thus,
it is necessary to analyze the impact of an inaccurate dependency model on the performance of the proposed algorithm. In
the evaluation for this purpose, the true path is the one in Fig. 4,
. Then, 15 different depenwith the dependency being
,
dencies are used to estimate the actual path, with
. Fig. 6(b) shows the estimated location RMSE
computed by (21) under various dependencies. We can tell that
the location errors fluctuate within a small range when the dependency scalars fall into a relatively large range around the true
dependency scalar 6. The estimation error has a peak when the
) from the true
tentative dependency is far away (e.g.,
dependency. This result validates our conjecture that the proposed RBPF is not very sensitive to the choice of dependency
since within a relatively large margin of the true dependency,
the estimation error of RBPF does not increase too much.

XU AND LI: ADAPTIVE RAO–BLACKWELLIZED PARTICLE FILTER

845

Fig. 6. (a) Weight STD comparison between RBPF and PF. (b) Location RMSE estimated by RBPF under different dependency models.

Fig. 7. Outdoor human tracking. Every ninth frame is shown to cover the entire sequence. Top row: RBPF tracker. Bottom row: PF tracker. The green dots are
the gradient contours around the mean state. PF tracker lags behind the true location, whereas RBPF tracker can maintain tracking all the way.

IV. REAL DATA EXPERIMENT
Extensive real data experiments have been carried out to evaluate the performance of proposed RBPF algorithm and compare
it with a regular particle filter.
One surveillance scenario is outdoor human tracking. The
test sequence for this scenario is from the CAVIAR project
(http://www.homepages.inf.ed.ac.uk/rbf/CAVIAR/).
The
ground truth data were also provided accompanying the video.
Fig. 7 illustrates the RBPF and PF tracking results on regularly
spaced frames extracted from a 35-frames-long sequence where
a person moves toward the window, stops there for browsing,
and continues moving along the corridor. We may notice that
the PF tracker lags behind the true location when the person
transits from standing to moving and it takes the PF tracker
several frames to catch the person. In contrast, the RBPF
tracker maintains good tracking during this process which
involves large motion velocity and orientation transition. We
also found that, when the person gets further from the camera
and consequently, the size becomes very small, the PF tracker
would not realistically capture the scale change of the object
and tend to deviate from the object; the RBPF algorithm, in
contrast, still maintains good tracking since it can analytically
update the scale change using the location information.
In addition, the reduction of the weight variance is confirmed
using the sequence in Fig. 7. This, in turn, brings the great ben-

efit of degeneracy alleviation. A suitable measure of degeneracy,
[23], [25], well explains why the
the effective sample size
reduction of weights variance alleviates degeneracy. It is de[3] where
refers to the
fined by
weights before resampling. Notice that the effective sample size
is less or equal to the particle number , and if the variance of weights is large,
would be small, implying severe
degeneracy. The researches by Kong et al. [25] and Doucet [28]
revealed that the variance of weights can only increases (statistically) over time and, thus, eventually leads to very small effective sample size. A straightforward yet brute force approach to
reducing this effect is to use a very large [3], but that is often
impractical in the case of high-dimensional state space. Some
other good approaches to avoiding degeneracy include choosing
good importance density and using resampling [2], [28]. In the
subsequent parts, we show that applying the Rao–Blackwell
technique can effectively reduce the weights variance and in turn
reduce the effect of degeneracy in the case of high-dimensional
state-space estimation.
To verify that the weight variance is decreased by RBPF, both
the RBPF and the regular PF with 200 particles are invoked ten
times with different random initializations to track the person in
Fig. 7. We compute the variance of the 200 normalized weights
at each time step. Finally, the weight variances at each instance
obtained by the ten runs are averaged, as shown in Fig. 8(a).
During the tracking process, the RBPF weight variances are ob-

846

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 3, MARCH 2007

Fig. 8. (a) Weight variance obtained by RBPF and PF using real data. (b) Effective sample size obtained by RBPF and PF.

Fig. 9. Particle weights distribution for the third RBPF (left) tracking frame and (right) third PF tracking frame in Fig. 7. A large number of particles in PF receive
very small weights and, hence, will not have offspring in the resampling stage, while the RBPF weights distribution presents reasonably large effective sample size.

viously lower than those from the PF. Based on the weight variance obtained in Fig. 8(a), the effective sample size for each
introframe is calculated by (22) which is an estimate of
duced in [2]. Fig. 8(b) shows that PF results in more severe degeneracy due to the relatively smaller effective sample size
(22)
Now we want to show why large effective sample size will
help alleviate degeneracy. To this end, we are interested at
knowing the weight of a given state sample. Fig. 9 illustrates
the distributions of particle weights for the third frame in
Fig. 7. The axis and axis denote the person’s horizontal
and vertical coordinate, respectively, and the axis represents
the normalized weight. We can see that, in the right plot from
PF, all but a few particles have negligible weights and, hence,
will not be selected in the resampling procedure. Thus, these
particles are actually wasted and those few particles with larger
weights are reselected many times. This process will lead to
inadequate representation of the posterior density within a
few iterations. Comparing to PF, the proposed RBPF produces
much larger effective sample size (left plot) since the shape of
the weight distribution has a relatively good spread instead of a
narrow peak like in PF.
A more challenging human tracking sequence is shown in
Fig. 10. The task is made difficult by the reflections from the
ground, the opposing window, occlusion due to the text on the

window, etc. Nevertheless, the RBPF algorithm obtained good
tracking, while the PF tracker was distracted by the similar color
of the ground when it encountered the red text.
Another test scenario in surveillance is vehicle tracking. One
sample result is shown in Fig. 11, in which the car is undertaking
complex motions like turning (rotation), translation and large
scale change. Particularly, we observe that the PF does not reflect the scale change realistically when the object becomes very
small due to the increasing distance from the camera, whereas
the proposed method is able to maintain good tracking in face
of the dramatic scale change.
V. RELATED WORK
A few techniques have been proposed to make particle
filtering applicable in high-dimensional state space. In [26],
Deutscher et al. developed an annealed particle filtering for
search in the high-dimensional space in the context of articulated 3-D human motion tracking. Essentially, a series of
weight distribution functions were arranged into layers, from a
relatively flat distribution to distributions with local and global
peaks. Initially they sampled a broad distribution with reduced
sensitivity and gradually they introduced the narrow peak so
that the sampling migrated towards the global peaks. Although
annealing works well in some applications, e.g., [26] and [27],
it has limitations as a general method for reducing dimensionality. The main problem is that it samples indiscriminately
within a certain energy band, regardless of whether the points
sampled are likely to lead out of the basin towards another

XU AND LI: ADAPTIVE RAO–BLACKWELLIZED PARTICLE FILTER

847

Fig. 10. Person moving in front of a shop window, shown for every 15th frame extracted from the whole sequence. Top row: RBPF tracker. Bottom row: PF
tracker. Frame index is 65, 80, 95, and 118. PF tracker was distracted by the ground when it encountered the red text whose color differs from the initial grey color
of the target object.

Fig. 11. Vehicle tracking. Top row: RBPF tracker; bottom row: PF tracker. When the size of the object becomes small as the vehicle moves far away from the
camera, the PF tracker cannot scale accurately and tends to be distracted by the background texture. On the other hand, RBPF tracker maintains good tracking,
capturing the scale change accurately.

local maximum, or whether they simply lead further up an
ever-increasing potential peak [29].
Other related techniques, instead of working directly on
reducing the size of the state-space, focus on adopting efficient
sampling scheme. For example, a well-known improved variant
of regular PF is ICondensation introduced in [19]. It is extended from the original Condensation algorithm by including
high level information as an additional source of knowledge to
sample the posterior. This method was used to track deformable
contours in highly cluttered environments.
Another variant of PF is the auxiliary particle filter (APF),
proposed by Pitt and Shephard [20]. It aims at enhancing the effectiveness of importance sampling. The advantage of the APF
is that it produces samples which, conditional on the current observations, are most likely to be consistent with the true state [2].
If the process noise is small, then the APF is often not so sensitive to the outliers as sequential importance resampling (SIR)
and the weight variance is decreased; but if the process noise is
large, the use of the APF then degrades performance due to the
[2].
poor approximation of
The proposed RBPF algorithm differs from the current techniques in that it makes use of the “structural” information inherent in the problem itself to analytically infer the state of the
linear dynamics, thus leaving only the nonlinear parts to be sampled. This directly leads to increased estimation accuracy.

VI. DISCUSSION
A. Failure Cases
In our work, the conditional relationship between various
components of the state space is abstracted into a constraint between the location and the scale components which is warranted
by the typical camera configuration in typical surveillance applications. Our basic assumption is that if the camera is mounted
relatively higher than the target object, the scale of the object
will become bigger as the coordinate of the object gets larger.
If the real scenario does not satisfy this assumption, although
the proposed RBPF may still work, the desired advantages of
RBPF might not be achieved and the performance might be
even poorer than a PF. On the other hand, if the aforementioned
basic assumption is satisfied, we found that the proposed RBPF
performs better than the regular PF especially when the object
moves with fast changing velocity between two successive
frames, or the object scale is changing relatively fast.
B. Computation Cost
Our real data experiments run at a 3.00-GHZ Pentium 4 CPU
with 200 samples. For typical image with resolution 320 240,
the processing frame rate is roughly 5 Hz. More work is needed
to improve the speed performance for real time processing, e.g.,
through optimizing the color histogram computation. In our

848

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 3, MARCH 2007

problem, if equivalent number of particles is used for RBPF and
PF, the RBPF algorithm is computationally more expensive than
PF (but the estimation accuracy of RBPF is higher than PF),
because our original target model has only eight dimensions,
and the dimensionality reduction from 8 to 4 is not enough to
offset additional computation required by RBPF over PF. On
the other hand, with the increasing dimensionality of the target
model, the computation cost of PF is increasing exponentially;
thus, the application of Rao–Blackwell technique in this case
is supposed to greatly improve the estimation efficiency. Our
conclusion is that, to achieve the same level of estimation
accuracy, RBPF needs far fewer particles than PF does; hence,
it is more efficient than PF.
C. Future Improvements to the Proposed Algorithm
When applying the proposed RBPF algorithm, an important
issue is to determine the dependency relationship between the
root and leaf variables. In general, this dependency could be a
conditional probability function or a deterministic scalar. In our
case, we simplify the problem by using a deterministic scalar,
and set it as a fixed value during the entire tracking process for
a particular scene, and this scalar has to be tuned for different
scenes. A better strategy to determine this dependency is to dynamically detect the surface normal with respect to the camera
through vanishing lines. In addition, we use an ellipse to model
the object shape (although the gradient-related measurements
go beyond the basic ellipse), which is not always a good model.
Deformable contours used in [5] may be explored in our future
work.
VII. CONCLUSION
In this paper, we proposed an adaptive RBPF for surveillance
tracking. We discussed how the dependency between state variables imposed by typical surveillance application could be utilized to improve the accuracy of a regular particle filter. Essentially, this is accomplished by partitioning the state variables
into separate groups, with the linear parts being computed by
Kalman filter and nonlinear part being estimated by particle
filter. Extensive comparative studies using both simulated and
real data have demonstrated the improved performance of the
proposed RBPF over regular particle filtering.
Rao–Blackwellization can be applied to a broader context
than surveillance tracking. Two key problems arise when applying RBPF to a real problem: the first is how to partition the
state space into two (or more) meaningful groups; the second is
what analytical filter should be used to efficiently estimate the
leaf variables conditional on the root variables. Often, these two
issues need to be solved based on the nature of the specific application. As a possible future working direction, we are investigating the potential of using learning approaches to find a proper
dependency model from a large number of state variables.
REFERENCES
[1] A. Doucet, J. Freitas, and N. Gordon, Sequential Monte Carlo Methods
in Practice. New York: Springer-Verlag, 2001, ch. 1.
[2] S. Arulampalam, S. Maskell, N. Gordon, and T. Clapp, “A tutorial on
particle filters for on-line non-linear/non-Gaussian Bayesian tracking,”
IEEE Trans. Signal Process., vol. 50, no. 2, pp. 174–188, Feb. 2002.

[3] N. J. Gordon, D. J. Salmond, and A. F. M. Smith, “Novel approach to
nonlinear/non-Gaussian Bayesian state estimation,” Proc. Inst. Elect.
Eng. F, vol. 140, no. 2, pp. 107–113, 1993.
[4] B. Li and R. Chellappa, “A generic approach to simultaneous tracking
and verification,” IEEE Trans. Image Process., vol. 11, no. 5, pp.
530–544, May 2002.
[5] M. Isard and A. Blake, “Contour tracking by stochastic propagation of
conditional density,” in Proc. Eur. Conf. Computer Vision, 1996, pp.
343–356.
[6] S. Arulampalam and B. Ristic, “Comparison of the particle filter with
Range parameterized and modified polar ekf for angle-only tracking,”
Signal Data Process. Small Targets, vol. 4048, pp. 288–299, 2000.
[7] A. Doucet, N. de Freitas, and N. Gordon, “An introduction to sequential Monte Carlo methods,” in Sequential Monte Carlo Methods in
Practice, A. Doucet, N. de Freitas, and N. Gordon, Eds. New York:
SpringerVerlag, 2001, pp. 3–13.
[8] A. Doucet, S. Godsill, and C. Andrieu, “On sequential Monte Carlo
sampling methods for Bayesian filtering,” Statist. Comput., vol. 10, no.
3, pp. 197–208, 2000.
[9] T. Schon, F. Gustafsson, and P.-J. Nordlund, “Marginalized particle filters for mixed linear/nonlinear state-space models,” IEEE Trans. Signal
Process., vol. 53, no. 7, pp. 2279–2289, Jul. 2005.
[10] K. Murphy and S. Russell, “Rao–Blackwellised particle filtering for
dynamic Bayesian networks,” in Sequential Monte Carlo Methods in
Practice, A. Doucet, N. Freitas, and N. Gordon, Eds. New York:
Springer-Verlag, 2001, ch. 24.
[11] G. Casella and C. P. Robert, “Rao Blackwellisation of sampling
schemes,” Biometrika, vol. 83, no. 1, pp. 81–94, 1996.
[12] Z. Khan, T. Balch, and F. Dellaert, “A Rao–Blackwellized particle filter
for Eigen tracking,” in Proc. IEEE Int. Conf. Computer Vision and Pattern Recognition, Washington, DC, 2004, vol. 2, pp. 980–986.
[13] D. Schulz, D. Fox, and J. Hightower, “People tracking with anonymous
and id-sensors using Rao–Blackwellised particle filters,” presented at
the Int. Joint Conf. Artificial Intelligence, Acapulco, Mexico, 2003.
[14] N. de Freitas, R. Dearden, F. Hutter, R. Morales-Menendez, J. Mutch,
and D. Poole, “Diagnosis by a waiter and a Mars explorer,” Proc. IEEE,
vol. 92, no. 3, pp. 455–468, Mar. 2003.
[15] C. Kwok and D. Fox, “Map-based multiple model tracking of a moving
object,” in Proc. RoboCup, 2004, pp. 18–33.
[16] P.-J. Nordlund and F. Gustafsson, “Sequential Monte Carlo filtering
techniques applied to integrated navigation systems,” presented at the
Amer. Control Conf., Arlington, VA, Jun. 2001.
[17] R. Hartley and A. Zisserman, Multiple View Geometry in Computer
Vision, 2nd ed. Cambridge, U.K.: Cambridge Univ. Press, Mar. 2004,
p. 220.
[18] K. Nummiaro, E. Koller-Meier, and L. V. Gool, “An adaptive colorbased particle filter,” Image Vis. Comput., vol. 21, pp. 99–110, 2002.
[19] M. Isard and A. Blake, “Icondensation: Unifying low-level and highlevel tracking in a stochastic framework,” in Proc. 5th Eur. Conf. Computer Vision, 1998, vol. 1, pp. 893–908.
[20] M. K. Pitt and N. Shephard, “Filtering via simulation: Auxiliary particle filters,” J. Amer. Statist. Assoc., vol. 94, no. 446, pp. 590–599,
1999.
[21] A. Doucet, “On sequential monte carlo methods for bayesian filtering,”
Tech. Rep., Dept. Eng., Univ. Cambridge, Cambridge, U.K., 1998.
[22] G. Kitagawa, “Monte Carlo filter and smoother for non-Gaussian nonlinear state space models,” J. Comput. Graph. Statist., vol. 5, pp. 1–25,
1996.
[23] J. S. Liu and R. Chen, “Sequential Monte Carlo methods for dynamic
systems,” J. Amer. Statist. Assoc., vol. 93, pp. 1032–1044, 1998.
[24] A. Doucet, N. J. Gordon, and V. Krishnamurthy, “Particle filters for
state estimation of jump Markov linear systems,” IEEE Trans. Signal
Process., vol. 49, no. 3, pp. 613–624, Mar. 2001.
[25] A. Kong, J. S. Liu, and W. H. Wong, “Sequential imputations and
Bayesian missing data problems,” J. Amer. Stat. Assoc., vol. 89, no.
425, pp. 278–288, 1994.
[26] J. Deutscher, A. Blake, and I. Reid, “Articulated body motion capture
by annealed particle filtering,” in Proc. IEEE Conf. Computer Vision
Pattern Recognition, 2000, vol. 2, pp. 126–133.
[27] R. M. Neal, “Annealed importance sampling,” Statist. Comput., vol.
11, pp. 125–139, 2001.
[28] A. Doucet, N. D. Freitas, K. P. Murphy, and S. J. Russell, “Rao–Blackwellised particle filtering for dynamic Bayesian networks,” in Proc.
16th Conf. Uncertainty in Artificial Intelligence, 2000, pp. 176–183.
[29] Soto, “A probabilistic approach for the adaptive integration of multiple visual cues using an agent network,” Ph.D. dissertation, Robotics
Institute, Carnegie Mellon Univ., Pittsburgh, PA, Sep. 2002.

XU AND LI: ADAPTIVE RAO–BLACKWELLIZED PARTICLE FILTER

Xinyu Xu received the B.S. degree from Qingdao
University in 2001 and the M.S. degree from the Beijing University of Posts and Telecommunications in
2004, both in computer science and engineering. She
is currently pursuing the Ph.D. degree at the Center
for Cognitive Ubiquitous Computing and the Department of Computer Science and Engineering, Arizona
State University, Tempe.
She was with the Alcatel Shanghai Bell, Co., Ltd.,
as a Mobile Service Solution Specialist from April
2004 to July 2004. Her research interests include
computer vision, cognitive visual analysis, machine learning, medical image
analysis, and pattern recognition.

849

Baoxin Li (S’97–M’00–SM’04) received the Ph.D.
degree in electrical engineering from the University
of Maryland, College Park, in 2000.
He is currently an Assistant Professor of computer
science and engineering with Arizona State University, Tempe. He was previously a Senior Researcher
with SHARP Laboratories of America (SLA),
Camas, WA, where he was the Technical Lead in developing SHARP’s Hi-Impact Sports Technologies.
He was also an adjunct faculty member with the
Portland State University, Portland, OR, from 2003
to 2004. His research interests include pattern recognition, computer vision,
multimedia processing, and statistical methods in computing.

INSTRUCTIVE VIDEO RETRIEVAL FOR SURGICAL SKILL COACHING USING
ATTRIBUTE LEARNING
Lin Chen, Qiang Zhang, Peng Zhang, Baoxin Li
Computer Science and Engineering, Arizona State University
{lin.chen.6, qzhang53, pzhang41, baoxin.li}@asu.edu
ABSTRACT
Video-based coaching systems have seen increasing adoption
in various applications including dance, sports, and surgery
training. Most existing systems are either passive (for data
capture only) or barely active (with limited automated feedback to a trainee). In this paper, we present a video-based
skill coaching system for simulation-based surgical training
by exploring a newly proposed problem of instructive video
retrieval. By introducing attribute learning into video for
high-level skill understanding, we aim at providing automated feedback and providing an instructive video, to which
the trainees can refer for performance improvement. This is
achieved by ensuring the feedback is weakness-specific, skillsuperior and content-similar. A suite of techniques was integrated to build the coaching system with these features. In
particular, algorithms were developed for action segmentation, video attribute learning, and attribute-based video retrieval. Experiments with realistic surgical videos demonstrate the feasibility of the proposed method and suggest areas
for further improvement.
Index Terms— Instructive Video Retrieval, Attribute
Learning, Coaching System
1. INTRODUCTION
Video-based coaching systems aim at helping people to improve their skills through capturing their performance via
video recordings that allow either on-line or off-line analysis. Applications of such systems include dance [1][2], sports
[3], and machine-operation training [4], etc. Traditionally,
the analysis is performed only by humans (e.g., coaches and
trainers). Recent years have witnessed increasing interests in
developing automated systems for doing such analysis for improved training, where the key task is vision-based motion
skill understanding since the coaching task often boils down
to providing corrective feedback to a trainee regarding his/her
movements.
Most existing methods may provide one of the following two types of feedback. The first type is an overall assessment with either a numeric rating [2] or a skill level [5].
While being useful for skill examinations, such type of feedback provides little suggestion to a trainee as to how to im-

Fig. 1. Illustration of the proposed skill coaching system
that retrieves an illustrative video as feedback while providing
specific and expressive verbal suggestions.
prove. Further, many methods [1][2][5] are based on comparison using some sort of standard action series or models.
This limits the applicability of such methods to complex tasks
where defining a standard action or model is impractical due
to the existence of a wide range of valid/perfect solutions. The
second type of feedback is some statistics computed from a
user’s training sessions, such as total execution time, movement counts, motion smoothness, etc. Although such statistics are more informative, they do not readily lead to corrective actions that the trainees may take to improve their performance.
In this paper, we present a video retrieval system (illustrated in Figure 1) for skill coaching in simulation-based
surgical training, which we defined as instructive video retrieval. We aim at providing automated video and verbal
feedback that has the following three features: (1) specificity:
the feedback should focus on a trainee’s skill weakness; (2)
superiority: the retrieved illustrative video should represent
a better skill than the trainee; (3) similarity: the retrieved illustrative video should have a similar operation context to the
trainee’s video. Note that although the focus of the paper is
on the specific application of simulation-based surgical training, the above features are deemed as critical to effective skill
coaching in general [6], and thus the proposed method can be
extended to other video-based skill coaching applications by

(a)

(b)

Fig. 2. The illustration of (a) the FLS system; (b) objectmotion distribution for action recognition.
integrating corresponding domain knowledge.
Different from traditional video retrieval such as nearlyduplicated video retrieval [7] or concept retrieval [8], which
are purely based on video content, the instructive video retrieval requires both low-level content analysis and high-level
semantic skill understanding. To our knowledge, this is still a
new research effort with little prior art. In this work, we introduce semantic attributes in video to bridge the gap between
inherent vagueness and the subjectivity of “instructive”.
The technical contribution of this work is threefold. First,
we propose a new video retrieval problem defined as “instructive video retrieval” and a corresponding effective algorithm
to solve this problem. This new problem has a wide variety
of applications and more efforts are worth investing. Second,
we extend image attribute learning into the video domain for
skill evaluation, which is useful to bridge the gap between
the low-level motion measurements and high-level skill understanding. Third, we develop a vision-based skill coaching
system for simulation-based skill training, which provides an
automatic and efficient way for self skill improvement without costly human supervision.
This study is primarily based on surgical training on the
Fundamentals of Laparoscopic Surgery (FLS) trainer box
(www.flsprogram.org), a simulation-based platform that has
been widely used in many hospitals for minimally-invasive
surgery training. The system is essentially a box simulating
the human body and a trainee is required to use tools going
into the box through small holes to perform actions like lifting and transferring objects inside the box (Fig. 2(a) left). The
trainee can see what is going on inside the box only through a
monitor that displays a live video (Fig. 2(a) right) captured by
an on-board camera. In the operation, a trainee is required to
lift one of the six objects with a grasper in his non-dominant
hand, transfer the object midair to his dominant hand, and
then place the object on a peg on the other side of the board.
Once all six objects have been transferred, the process is reversed from one side to the other.
2. PROPOSED METHOD
In this section, we present our proposed method for videobased skill coaching which performs three key tasks: (1) Decomposing a video clip into primitive action units; (2) Rating
each action unit using semantic attributes; (3) Retrieving an illustrative video for instruction. Fig. 3 presents a flow chart of
our system, outlining its major algorithmic components and

Fig. 3. An overview of the proposed approach. The green
components are only used in the training stage.
their interactions.
2.1. Primitive Action Segmentation
We first segment the given video into clips where each clip
only includes one primitive action. The FLS operation consists of 5 primitive actions [9] as building blocks of manipulative surgical activities: (L): lift an object from the peg, (T):
transfer an object, (P): place an object on the peg, (W): move
the grasper with an object, (U) move the object without an
object. Since the videos we consider exhibit predictable motion patterns arising from the underlying actions of the human
subject, we adopt the Hidden Markov model (MM) in the segmentation task. This allows us to incorporate domain knowledge into the transition probabilities, e.g., the Lift action is
followed by itself or by Loaded Move with high probability.
Following [10][4], we define each state as a primitive action.
The task of segmentation is then to find the optimal state path
for the given video.
Frame-level Feature Extraction The FLS box is a controlled environment including 4 types of objects: background,
rubber cubes, pegs, and tools/graspers. Due to the noisy video
clips, we designed a probability representation of the motion
information which served as features for action segmentation.
Specifically, we first use random forest (RF) [11] to obtain
the label probability pl (x) that a pixel x belongs to the object
label l. Then the tool orientation and tip region can be further detected by the spatial information based on the obtained
probabilities. Since all surgical actions occur in the region
around the grasper tip, the region is defined as the ROI region
(Fig. 2(b) left) to filter out other irrelevant background. With
comparison with the distribution of the background region,
we estimate the probability of each pixel x being “moving”
by frame differencing, which is denoted by pm (x). Then the
joint distribute pl (x)·pm (x) represents the probability that the
pixel x is the moving object l. This joint object-motion distribution suppresses the static clutter background in the ROI
so that only interested motion information will be reserved.
To further capture the spatial information, we further split the
ROI into blocks, as shown in Fig. 2(b) right, and describe the
object-motion distribution in each block by the Hu-invariant
moment [12]. Finally the moment vectors in each block are
cascaded into a frame-level descriptor for action recognition.
Random Forest as Observation Model After obtaining

Motion
Instrument
Instrument
target
Object
ROI
Fig. 4. The graphical model for Bayesian estimation of transition probability, where the symbols with circles are hidden
variable to be estimated, the symbols within gray circle are
observations and the symbols without circle are priors.
the frame-level descriptor, we further utilize RF for framelevel action recognition. Since RF is an ensemble classifier
with a set of decision trees and the output is based on majority
voting of the trees in the forest, the frame-level action distribution provides a good estimation of the observation model
for the HMM states. Assuming that there are N trees in the
forest and ni decision trees assign primitive action label i to
the input frame, we could view random forest chooses label i
with probability ni /N , which can be taken as the observation
probability for State (primitive action) i.
Bayesian Estimation of Transition Probability The
transition probability from State i to State j can be estimated
based on a small set of data as the ratio the number of (expected) transitions from i to j over the total number of transitions. However, one potential issue of this method is that,
in video segmentation we have limited training data in video
segmentation. Furthermore, the number of transitions among
different states (primitive action), is typically much less than
the total number of frames of the video. This will result in
a transition probability matrix which is dominated by diagonal elements. The resulting transition probability will degrade the benefit of using HMM for video segmentation, i.e.,
forcing desired transition pattern in the state path. Thus, we
propose to use a Bayesian approach for estimating the transition probability, employing the Dirichlet distribution which
enables us to combine the domain knowledge with the limited
training data for transition probability estimation. The model
is shown in Fig. 4.∑
Assuming αi ( j αi (j) = 1) is our domain knowledge
for the transition probabilities from State i to all states, then
we can draw the transition probability vector πi as πi ∼
dir(ραi ) where dir is the Dirichlet distribution as a distribution over distribution, and ρ represents our confidence of the
domain knowledge. Given the transition probability πi , the
count of transition from State i to all states follows a multinomial distribution:
∑
( j xi (j))! ∏
πi (j)ni (j) .
(1)
ni ∼ multi(ni |πi ) = ∏
n
(j)!
i
j
j
Because the Dirichlet distribution and multinomial distribu-

Table 2. Motion measurements.
Description
The motion of grasper tip.
Relative motion between grasper
tip and its operation target.
The motion area of objects in ROI.
The optical flow field in ROI.

Definition
v(t)
v̂(t)
vr (t)
A(t)
m(x, t)

tion are a conjugate pair, the posterior probability of transition
probability is just combining the count of transition among
state and domain knowledge (prior) as π ∼ dir(n
∑ i + ραi ).
When there are not enough training data, i.e., i ni (j) ≪ ρ,
πi would be dominated by αi , i.e., our domain knowledge; as
more training data become available, πi would approximate
to the counting of transitions in the data.
2.2. Attribute Learning for Action Rating
A fundamental challenge in instructive video retrieval is to
map computable visual features to semantic concepts that are
meaningful to a trainee. Recognizing the practical difficulty
of lacking sufficient amount of exactly labeled data for learning an explicit mapping, we introduce the concept of image
attribute into video domain to evaluate the underlying skill
of an action clip based on semantic attributes designed using domain knowledge. Following [13][14][15], we define 6
attributes to measure the skill level of the trainee’s operation,
which are listed in Table 1. With these semantic attributes, the
system will be able to expressively inform a trainee what is
the weakness in the operation, since the defined attributes are
all semantic concepts used in existing human-expert-based
coaching (and thus they are well understood).
Feature Representation To represent the defined attributes, new motion features are constructed for skill rating
in 3 steps. First, a few types of motion measurements, which
are summarized in Table 2, are calculated based on the previous object segmentation information (Sect. 2.1). Second, we
extract motion signatures from each of the motion measurement, which are summarized in Table 3. The motion signatures are 1-dimensional temporal signals that further compact
the motion information. Last, final motion feature are constructed from each motion vector and its motion signatures as
follows. In the temporal domain, we divide a signature into
equal temporal bins; in the Fourier domain, we also divide the
Table 3. Motion signatures. y(t)/y(x) represents any motion
vector in Table 2, e.g. v(t), vr (t), A(t), etc. ȳ(t) is the smooth
result of y(t). m is the shorthand for field motion m(x, t).
Name
Definition
Description
Velocity
|y(t)|
Instant velocity
∫t
Path
Accumulated motion energy
|y(x)|dx
0
Jitter
|y(t) − ȳ(t)|
Motion smoothness metric
∫ |⟨▽×m,m⟩|
CAV
Curl angular velocity
dx
∥m∥2

Attributes
Time and motion (T)
Flow of operation (F)
Bimanual dexterity (B)
Respect for issue (R)
Instrument handling (I)
Depth perception (D)

Table 1. Action attributes for surgical skill assessment.
Description
How efficiently a trainee can operate without unnecessary moves.
How smoothly a trainee can operate without frequently stops.
How well two hands can cooperate and work together.
How force is controlled in operation of objects as subjective evaluation of organ damage.
How well a trainee operates instruments without bad attempts and movements.
How good a trainee’s sense of depth to avoid failed operation on a wrong depth level.

frequency into equal bins. In each temporal and frequency
bin, the maximal, minimal, and average values are cascaded
into the final feature set.
Relative Attribute Skill Rating To rate the skills of each
primitive action video clip, we adopt relative attribute learning [16] to calculate relative ranking of the clips with respect to the defined semantic attributes. Formally, for the
k-th attribute, we are given a set of ordered pairs of clips
Ok = {(i, j)} and a set of un-ordered pairs Sk = {(i, j)},
where (i, j) ∈ Ok means the video clip vi has a better skill
performance than the video clip vj (i.e. vi ≻vj ) in terms of
the specified attribute, and (i, j)∈Sk means vi and vj have
similar skill performance (i.e. vi ∼vj ). Then the constructed
motion features can be fed into the following relative attribute
framework to learn the model wk for relative skill rating:
∑
∑
1 T 2
2
∥wk ∥2 + C( ε2ij +
γij
)
wk ,ε,γ 2
s.t. wTk (xi − xj )≥1 − εij , ∀(i, j) ∈ Ok ;
min

(2)

|wTk (xi − xj )|≤γij , ∀(i, j) ∈ Sk ;
εi ≥0, γij ≥0.
where xi is the feature vector extracted from the i-th video
clip, C is the trade-off constant to balance maximal margin
and pairwise attribute order constraints. The relative skill can
be compared by the attribute value wTk xi , which is used in
the subsequent retrieval of illustrative videos.
2.3. Illustrative Action Clip Retrieval
With the previous processing, the system will retrieve an illustrative video clip from a constructed video repository and
present it to a trainee as an instructive video.
Operation Weakness Detection We first need to figure
out what is the operation weakness. However, a lowest attribute value does not always mean the most urgent attribute
in need of improvement, especially when this attribute is “difficult” to most of the people. The weakest attribute should be
the one that the trainee did poorly while most other people
are significantly better. Thus, we use the average cumulative
distribution of one user’s training session to assess the performance strength of each attribute. Specifically, with K attributes, each clip vi can be characterized by a K-dimensinoal
vector [ai,1 , · · · , ai,K ], where ai,k = wTk xi is the k-th attribute value of vi based on its feature vector xi . The attribute

values of all clips (of the same action) in the data repository
forms an N × K matrix A whose column vector ak is the k-th
attribute value of each clip. Similarly, from a user’s training
session, for the same action under consideration, we have another set of clips with attribute matrix Â whose column vector
âk is the user’s k-th attribute values in the training session.
The performance strength of the k-th attribute sk can be calculated by
n
1∑
P (ak ≥âi,k ))
(3)
sk =
n i=1
where n is the total number of video clips in one training session of one primitive action. Note that higher sk means more
users are doing better than the current operation, which means
high importance the attribute need to improve.
Video Utility Evaluation We then need to figure out how
much a video clip vi is helpful with regard to a given training video, which we defined as the utility of vi on the k-th
attribute, denoted as ui,k . We measure the utility by normalized distance between the performance strength of these two
videos. Specifically, let sk and ŝk denotes the performance
strength of the input training video and the potential instructive video clip vi on the k-th attribute, the utility is calculated
as
sk − ŝk
ui,k =
(4)
sk
After the calculation of these measurement, we select the
best illustration video clip vi∗ from the data repository using
the following criterion:
vi∗ = arg max
i

K
∑

sk ·ui,k

(5)

k=1

The underlying idea of Function (5) is that a good feedback
video should have high utility on important attributes.
With the above attribute analysis, we also provide a verbal
description with regard to the 3 worst action attributes with an
absolute importance above a threshold 0.4, which means that
more than 60 percent of the pre-stored action clips are better
in this attribute than the trainee. If all attribute importance
values are lower than the threshold, we simply select the worst
one. With the selected attributes, we retrieve the illustration
video clips, inform the trainee about on which attributes he
performed poor, and direct him to the illustration video.

It is worth noting that in recommending an illustration
video, we defined concepts that are context dependent. That
is, the importance and utility values of an attribute depends
on the given data set. In practice, the data set could be a local
database captured and updated frequently in a training center,
or a fixed standard dataset, and thus the system allows adjustment of some parameters (e.g., the threshold 0.4) based on
the nature of the database.
3. EXPERIMENTS
We tested our framework on a 64-bit computer with Intel
Core i5-2500 CPU @ 3.30GHz and 8.00G RAM. Experiments have been performed using realistic training videos
capturing the performance of resident surgeons (includes experts and novices) in a local hospital during their routine training. A typical testing video contains about 4500 frames with
the frame rate of 30 FPS and the resolution of 480 × 720. For
acceleration, we down-sampled the resolution and frame rate
by 2. Experiments showed that our system takes on average
around 242 seconds to process such a video.
For evaluating the proposed methods, we selected 10 representative videos/subjects from trainees of different skill levels. Each video is a full training session consisting of 12 Peg
Transfer cycles which leads to 12 video clips for each primitive operation. We have a database of 240 clips for each of
the 3 primitive operations, i.e. lift, transfer, and place, with
the total number of clips being 720. The video can be downloaded from zhqiang.org/?p=207. We emphasize that, even
the same subject does not perform the same action identically
(and in fact the variability was observed to be very high), and
thus the clip dataset is very diverse, providing a reasonable
basis for evaluating our method. The exact frame-level labeling (which action each frame belongs to) was manually
obtained as the ground truth. For each primitive action, we
randomly select 150 pairs of video clips and then manually
label them by examining all the attributes previously defined.
This process manually determines which video in a given pair
should have a better skill according to a given attribute.
3.1. Action Segmentation
As discussed in the previous section, we first calculated the
frame-level action classification accuracies, then the classification scores (probabilities) are used as the observation into
an HMM to get a final action recognition result, which is
solved by the Viterbi algorithm. The confusion matrix of action recognition before and after employing HMM is shown
in Table 4, which is calculated by leave-one-video-out cross
validation. It can be seen that the frame-level recognition result is already high for some actions, which verifies the effectiveness of our proposed motion descriptors. The recognition
accuracies after using the HMM are significantly improved,
especially for actions L and P. The overall low accuracy for
actions L and P is mainly due to the trainee’s unsmooth operation that caused many unnecessary stops and moves, which

Table 4. Confusion matrix of the action recognition accuracies. Each cell is the accuracy (%) with/without HMM.
UM
L
LM
T
P
UM 87.6/88.0 0.2/0.2
0.6/0.8
11.5/10.3 0.8/0.8
L
21.9/36.1 43.4/28.5 21.8/15.8 13.0/13.3 0.0/6.3
LM 3.8/18.0 0.2/1.1
77.3/61.1 12.8/12.3 6.0/7.5
T
5.6/11.3 0.0/0.1
1.0/0.9
93.4/87.7 0.0/0.0
P
28.7/55.1 0.6/2.8
12.0/19.9 1.3/2.4
57.5/19.9
Table 5. Accuracy of attribute learning across primitive actions (%). Each row represents a different primitive action
and each column represents a different attribute.
T
F
B
R
I
D
L 92.7 95.1 97.2 92.5 97.4 87.5
T 90.0 97.9 82.9 N/A N/A 92.5
P 83.0 89.5 88.2 95.2 95.8 89.8
are hard to distinguish from UM and LM. The overall segmentation accuracy of expert videos is 93.5% while the accuracy of novice videos is 80.3%. The results show that the
proposed action segmentation method is able to deliver reasonable accuracy in face of some practical challenges.
3.2. Skill Attribute Evaluation
We verified the effectiveness of the learned attribute evaluator by the ranking accuracies. The ranking accuracy of each
attribute is derived by 10-fold cross validation on the 150 labeled pairs in each primitive action, which is shown in Table
5. The result in the table demonstrates that our attribute evaluator, albeit learned only from relative information, has a high
validity. In this experiment, only 3 primitive actions were
considered here, i.e. L, T, and P. We combined segments of
LM and UM with their corresponding subsequent operations
of L, T and P, since LM and UM can be considered as the
“preparation” step for the other operations. Some attributes
are not considered intentionally for some actions (the “N/A”
entries in Table 5) as it is not appropriate to assess the skills
of the actions by these attributes. The result shows that the
learned attribute learner achieves a significantly high accuracy for our defined skill attributes.
3.3. Instructive Video Evaluation
We compared our instructive video retrieval method with a
baseline method that randomly selects one expert video clip
of the primitive action. The comparison protocol is as follows.
First, a query clip is selected from the database. Then, the recommended clips are obtained from both the proposed method
and the baseline method. The two illustrative clips are paired
in random order and presented to 8 human evaluators to judge
which retrieved clip is more instructive. For each primitive
operation, totally 60 queries are generated and the subjective
evaluation result is summarized in Table 6. The “Instructive
rate” shows the percentage of the retrieved videos are deemed

Table 6. Subjective evaluation result of the instructive video
retrieval(%).
Instructive rate Comparative rate
L
93.3/83.3
50.0/40.0/10.0
T
96.7/73.3
63.3/26.7/10.0
P
95.0/76.7
60.0/26.7/13.3
instructive for the proposed/baseline approach. The “comparative rate” is the percentage of our proposed approach retrieving more/similar/less instructive videos than the baseline
approach. The result shows that both methods present high
instructive rate but the proposed method is persistently better
than the baseline method. The result is especially satisfactory
since the baseline method already employs expert videos, and
thus our method is able to tell which expert video clip is more
helpful to serve as an instructive reference. Since the proposed instructive video retrieval method is based on skill attribute analysis, this demonstrates the validity of the attribute
learning.
4. CONCLUSION AND FUTURE WORK
In this paper, we presented a video-based surgical skill coaching system, aiming at providing weakness specific, skill superior and content similar instructive feedback. To build the
system, we proposed a new problem defined as instructive
video retrieval together with an effective framework to solve
the problem by extending the idea of image attribute learning
into video for skill understanding. In building the system,
algorithmic innovations were made to incorporate domain
knowledge and to handle practical difficulties arising from
the real training platform. To our knowledge, this is the first
video-based approach to delivering a systematic solution to
the problem of automated skill coaching in simulation-based
surgical training. Experiments with real world videos capturing the training sessions of resident surgeons have demonstrated the effectiveness of the idea and the key algorithms.
5. ACKNOWLEDGMENT
The work was supported in part by the National Science
Foundation (NSF) and a grant from the U.S. Army Research
Office (ARO). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of sponsors.
6. REFERENCES
[1] Dimitrios S. Alexiadis, Philip Kelly, Petros Daras, Noel E.
O’Connor, Tamy Boubekeur, and Maher Ben Moussa, “Evaluating a dancer’s performance using kinect-based skeleton
tracking,” in Proc. of ACM MM’11, 2011.
[2] S. Essid, D. Alexiadis, R. Tournemenne, M. Gowing, P. Kelly,
D. Monaghan, P. Daras, A. Dremeau, and N.E. O’Connor, “An
advanced virtual dance performance evaluator,” in Proc. of
ICASSP’12, 2012.

[3] Yu Jin, Xiaoxiang Hu, and GangShan Wu, “A tai chi training
system based on fast skeleton matching algorithm,” in Proc. of
ECCV’12, 2012.
[4] K. Tervo, L. Palmroth, and H. Koivo, “Skill evaluation of human operators in partly automated mobile working machines,”
Automation Science and Engineering, IEEE Transactions on,
Jan 2010.
[5] J. Rosen, B. Hannaford, C.G. Richards, and M.N. Sinanan,
“Markov modeling of minimally invasive surgery based on
tool/tissue interaction and force/torque signatures for evaluating surgical skills,” Biomedical Engineering, IEEE Transactions on, May 2001.
[6] D. Coyle, The Talent Code: Greatness Isn’t Born. It’s Grown.
Here’s How., Random House Publishing Group, 2009.
[7] Jiajun Liu, Zi Huang, Hongyun Cai, Heng Tao Shen,
Chong Wah Ngo, and Wei Wang, “Near-duplicate video retrieval: Current research and future trends,” ACM Comput.
Surv., Aug. 2013.
[8] Sara Memar, LillySuriani Affendey, Norwati Mustapha, ShyamalaC. Doraisamy, and Mohammadreza Ektefa, “An integrated semantic-based approach in concept based video retrieval,” Multimedia Tools and Applications, 2013.
[9] Seung kook Jun, M.S. Narayanan, P. Agarwal, A. Eddib,
P. Singhal, S. Garimella, and V. Krovi, “Robotic minimally
invasive surgical skill assessment based on automated videoanalysis motion studies,” in Proc. of BioRob’12, June 2012.
[10] J. Rosen, J.D. Brown, L. Chang, M.N. Sinanan, and B. Hannaford, “Generalized approach for modeling minimally invasive surgery as a stochastic process using a discrete markov
model,” Biomedical Engineering, IEEE Transactions on,
March 2006.
[11] Antonio Criminisi, Jamie Shotton, and Ender Konukoglu,
“Decision forests: A unified framework for classification,
regression, density estimation, manifold learning and semisupervised learning,” Found. Trends. Comput. Graph. Vis., Feb.
2012.
[12] Ming-Kuei Hu, “Visual pattern recognition by moment invariants,” Information Theory, IRE Transactions on, February
1962.
[13] Krishna Moorthy, Yaron Munz, Sudip K Sarker, and Ara Darzi,
“Objective assessment of technical skills in surgery,” BMJ, 10
2003.
[14] Richard Reznick, Glenn Regehr, Helen MacRae, Jenepher
Martin, and Wendy McCulloch, “Testing technical skill via an
innovative bench station examination,” The American Journal
of Surgery, 1997.
[15] Jeffrey D. Doyle, Eric M. Webber, and Ravi S. Sidhu, “A universal global rating scale for the evaluation of technical skills in
the operating room,” The American Journal of Surgery, 2007.
[16] D. Parikh and K. Grauman, “Relative attributes,” in Proc. of
ICCV’11, Nov 2011.

Automated Description Generation for Indoor Floor Maps
Devi Paladugu, Hima Bindu Maguluri*, Qiongjie Tian*, Baoxin Li
Computer Science and Engineering, Arizona State University
{apaladug,hmagulur,qtian5,bli24}@asu.edu
of locations and thus might reduce the need for totally relying on
“asking for directions”. Say a student who is visually impaired
wants to go to a local library that he has never visited before (or
with which he has little familiarity). He could gain knowledge of
the key locations before he gets there, if a verbal description of
the major landmarks and their geographical relation are readily
available. An automatic indoor map description generator that can
be easily used by a visually-impaired user may thus greatly
benefit the blind population in promoting and supporting a more
active life style.

ABSTRACT
People with visual impairment can face numerous challenges
navigating a new environment. A practical need is to navigate
through unfamiliar indoor environments such as school buildings,
hotels, etc., for which commonly-used existing tools like canes,
seeing-eye dogs and GPS devices cannot provide adequate
support. We demonstrate a prototype system that aims at
addressing this practical need. The input to the system is the name
of the building/establishment supplied by a user, which is used by
a web crawler to determine the availability of a floor map on the
corresponding website. If available, the map is downloaded and
used by the proposed system to generate a verbal description
giving an overview of the locations of key landmarks inside the
map with respect to one another. Our preliminary survey and
experiments indicate that this is a promising direction to pursue in
supporting indoor navigation for the visually impaired.

2. SYSTEM AND DESIGN
The first problem towards building a system to provide verbal
description of a floor map is to determine the availability of a
usable digital map of good resolution. A majority of public
buildings have their floor plans published on their website.
However, in general, it is very difficult if not impossible for a
visually impaired user to go through all the links on a web site to
locate and assert that a file/link is indeed a floor plan. Secondly,
given an image of the floor plan, the components of the plan need
to be determined automatically. Thirdly, assuming all the
components of the map can be segregated, finding the right way
to describe these components is another challenging task. To the
best of our knowledge, there is no fully automatic solution that
completely solves the problems in an end-to-end system. This
demo will present a prototype system that we have developed for
working around the problems presented above. The system
provides a simple interface for a user to type in the search
keywords for the place he is interested in visiting. The system
then uses these keywords to extract the URL/domain of the
relevant website. A keyword-based search is employed to parse
all the URLs obtained from the website to find the appropriate
floor plan, if it indeed exists. Automated image process
algorithms are then deployed to extract useful landmarks in the
map. Finally, the system generates a verbal description that is
presented on the screen as well as read out to the user. The current
version of the system has been focused on public libraries, to
properly constrain the domain to some degree. The overview of
the system is illustrated in Figure 1.

Categories and Subject Descriptors
H.5.2 [Information Interfaces and Presentations]: Usercentered design. K.4.2 [Social issues]: Assistive technologies for
persons with disabilities.

Author Keywords
Visually Impaired, Indoor Floor maps, Navigation, Verbal
Description

1. INTRODUCTION
Assistive technologies for the visually impaired have seen
tremendous developments. Nevertheless, there still remain many
practical barriers for a blind individual who strives to lead an
independent and active life [1]. One such problem faced
repeatedly in their everyday life is navigation, especially when
visiting new places. Indoor navigation in unfamiliar places, in
spite of being a big hindrance factor, has not received enough
attention from researchers and developers. Sonar, camera, laser
based devices and infrared based signage are some devices
developed in the last decade to help with indoor navigation via
obstacle detection [1]. However, these devices require sensory
devices to be strategically placed inside the building, which is a
constraint. Further, these devices do not provide an overall sense
of the location or help with planning a visit to a new location.
People still largely depend on seeking human help around the
location to get directions.

2.1 WebCrawler Interface
We designed an interface that takes in a set of keywords or an
URL and downloads the relevant image or pdf files. We used
Google’s “I am feeling lucky” search URL format to obtain the
first link, given the keywords. Given this URL, we use the CURL
and DOM features in a simple PHP script combined with keyword
search to extract the required URLs containing the floor plan(s).

A simple and effective solution is to describe a floor map, when
available, in terms of a scheme of relative positions. Using a
relative scheme eliminates the need for having multiple devices
tracking user position in real-time. A system that generates a
verbal description combined with a user-friendly interface can be
very useful in helping the user to get a sense of relative positions

2.2 Key Landmark Localization
In this step, common landmarks of the domain (e.g., libraries) are
located in a map. The processing steps also include text detection
and recognition. Below are the major processing steps:

Copyright is held by the author/owner(s).
ASSETS’12, October 22–24, 2012, Boulder, Colorado, USA.
ACM 978-1-4503-1321-6/12/10

* Indicates equal contribution.

211

For words corresponding to key locations, we save the centroid of
each detected text box and the corresponding text for the
subsequent stages.

2.3 Verbal Description Generation
We performed a small-scale case study and analyzed how blind
users verbally describe a building that they are familiar with and
also tried to learn how they usually receive layout information
from sighted individuals. We asked a small group of blind users to
describe a building on campus that they are familiar with, to
another blind individual who is new to the building. We analyzed
the descriptions and tried to mimic the observations when
designing the verbal description, resulting in the “Blind-friendly
description model” shown in Figure 1.
The most natural way to describe a building as seen from our case
study can be summarized as follows: Describe the rooms or
landmarks encountered when walking straight into the main
entrance door. Then describe locations to the left and to the right
either with respect to the objects described previously or with
reference to the entrance. As an attempt to simplify this task, we
divide the map into three segments: “straight”, “left” and “right”.
It is intuitive to divide into three regions as shown in Figure 1.
Once we have each point categorized into a segment, we further
divide the map into grids, to give the user a sense of distance
between the relative points. Based on the grid location and
segment, we use simple grammatical rules to generate verbal
description.

3. SUMMARY
The contribution of this work is three-fold. First, a simple
interface that eliminates all the work needed from a user to obtain
a map is implemented. Second, automated algorithms were
designed to factorize the map into the necessary components
needed for description generation. Thirdly, a simple but efficient
way to describe a floor plan learnt from the visually impaired
users during a case study is proposed. The system will be
demonstrated on the conference.

4. ACKNOWLEDGEMENT
This material is based upon work supported in part by the
National Science Foundation under Grant No. 0845469. Any
opinions, findings, and conclusions or recommendations
expressed in this material are those of the authors and do not
necessarily reflect the views of the National Science Foundation.

Figure 1. An overview of the proposed system.
Pre-processing: This step includes removing the color
information, detection and removal of the legend, line removal.

5. KEY REFERENCES

Text Detection: We use spatial frequency variations along
vertical, horizontal and diagonal direction for each pixel to
estimate if a pixel contains text [2].

[1] Giudice, N.A., & Legge, G.E. (2008). Blind navigation and
the role of technology. In A. Helal, M. Mokhtari & B.
Abdulrazak (Eds.), Engineering handbook of smart
technology for aging, disability, and independence (pp. 479500): John Wiley & Sons.

Text Region Post-processing: Text blocks are detected and the
orientations of text are estimated and compensated for OCR.
OCR: We integrate Tesseract [3] to process the segmented text
regions to obtain the text corresponding to each region.

[2] Z. Wang, et al., "Instant tactile-audio map: enabling access
to digital maps for people with visual impairment," ACM
SIGACCESS conference on Computers and Accessibility,
Pittsburgh, Pennsylvania, USA, 2009.

Auto-correction and OCR: For initial refinement, we use a
standard dictionary and spell check on every detected word to
correct some bad detection and recognition results.

[3] http://code.google.com/p/tesseract-ocr/

212

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 10, NO. 6, JUNE 2001

897

Model-Based Temporal Object Verification
Using Video
Baoxin Li, Rama Chellappa, Fellow, IEEE, Qinfen Zheng, and Sandor Z. Der

Abstract—An approach to model-based dynamic object verification and identification using video is proposed. From image sequences containing the moving object, we compute its motion trajectory. Then we estimate its three-dimensional (3-D) pose at each
time step. Pose estimation is formulated as a search problem, with
the search space constrained by the motion trajectory information
of the moving object and assumptions about the scene structure. A
generalized Hausdorff metric, which is more robust to noise and
allows a confidence interpretation, is suggested for the matching
procedure used for pose estimation as well as the identification and
verification problem. The pose evolution curves are used to assist
in the acceptance or rejection of an object hypothesis. The models
are acquired from real image sequences of the objects. Edge maps
are extracted and used for matching. Results are presented for both
infrared and optical sequences containing moving objects involved
in complex motions.
Index Terms—Hausdorff matching, moving object recognition,
object recognition, video processing.

I. INTRODUCTION

F

OR many years, object recognition algorithms have been
based on a single image or a few images acquired from different aspects. While advances have been made in simple constrained situations such as indoor environments, object recognition in natural scenes remains a challenging problem. Among
the many difficulties, a prominent one is that in real applications, theoretically there exist infinitely many poses (orientations) for a given object. Therefore, two-dimensional (2-D) approaches, which are largely based on 2-D matching under some
simplified transformation group, will not solve the three-dimensional (3-D) object recognition problem. To overcome the need
for search in the viewpoint space, approaches based on geometric invariants have been proposed (for example, see [16] and
[18]). Although the invariance approach is theoretically attractive, it would be difficult to apply it to complex objects in natural
scenes. Appearance-based recognition schemes (for example,
see [11]) try to tackle the viewpoint problem by using visual
learning. In [11], the authors reported promising results for a test
data set. Although appearance-based approaches do not require
Manuscript received March 12, 1999; revised February 26, 2001. This work
was supported by the Advanced Sensors Consortium (ASC) sponsored by the
U.S. Army Research Laboratory under the Federated Laboratory Program, Cooperative Agreement DAAL01-96-2-0001. The associate editor coordinating
the review of this manuscript and approving it for publication was Dr. Michael
R. Frater.
B. Li is with Sharp Laboratories of America, Camas, WA 98607 USA (e-mail:
bli@sharplabs.com).
R. Chellappa and Q. Zheng are with the Center for Automation Research
University of Maryland, College Park, MD 20742 USA.
S. Z. Der is with the Army Research Laboratory, Adelphi, MD 20783 USA.
Publisher Item Identifier S 1057-7149(01)04479-7.

explicit feature extraction, their success relies on visual learning
from a training set. A good training set is not always easy to obtain. Besides, due to shape variations, training images always
contain some background region. Although when training, one
can set the background to a uniform value (as in [11]), it is not
always possible to black out the background at the recognition
stage—one needs to know the object type and its exact orientation in order to do so, which is what a recognition algorithm is
attempting to do. Backgrounds can greatly affect the projection
of an input image onto the eigenspace. In addition, when the
camera sensor is infrared, as in most surveillance applications,
the object signature becomes too variable to be characterized
by only a few images even at a fixed pose. In [9], some recognition algorithms including several learning algorithms were compared, using a large database containing over 17 000 images of
ten object classes. It was reported that even the best recognition results were unsatisfactory for this infrared database. One
possible explanation for the results in [9] is that when objects
have abundant pose variations, the appearance manifolds become heavily overlapped, making recognition harder. In such a
situation, one may have to resort to some geometric (shape) features, which, unfortunately, are again dependent on viewpoint.
An interesting observation is that when the object is moving,
human beings can quickly guess its pose, and then verify some
features unique to that pose. This suggests that additional information can be exploited to make object recognition more feasible when a video sequence is available. This paper presents a
technique for model-based temporal object verification/identification. In a sense, verification and identification are constrained
cases of recognition. To be specific, in this paper, identification
refers to the following problem: given an image sequence containing a moving object, to identify the object as one of a few
hypotheses; or, to identify the desired object in a sequence containing multiple objects. Identification is dynamic in that we
have a time-evolving scene due to object motion and possible
sensor motion. Verification is used in a slightly different situation, which answers the following questions: Is this the object
seen in the previous frames? and How confident of this am I?
This is especially interesting in situations of temporary loss of
tracking due to, for example, occlusion by other objects. Verification is in a sense similar to the tracking problem but here it
emphasizes the acceptance or rejection of a certain object hypothesis, rather than just tracking by using some features. Obviously, model-based verification/identification has many applications. For example, in visual autonomous surveillance as in
following a face in the crowd, the recognition problem can often
be reduced to the verification/identification problem.

1057–7149/01$10.00 © 2001 IEEE

898

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 10, NO. 6, JUNE 2001

Fig. 1. Typical identification/verification setup using video from a moving
camera platform.

In applications such as visual autonomous surveillance, the
camera itself is often moving during the acquisition process. A
general setup for this kind of problems is illustrated in Fig. 1.
Due to camera motion, a sensor motion compensation process
is often needed to remove the unwanted camera motion if we
want to detect the object based on its motion.
In this paper, from image sequences containing the moving
object, the 3-D pose of the object is estimated at each time step.
Pose estimation is formulated as a search problem, with the
search space strictly constrained by the motion trajectory information of the moving object and assumptions about the scene
version of the Hausdorff metric [1],
structure. A generalized
which is more robust to noise and allows a confidence interpretation, is suggested for the search problem. The pose evolution curves are used to assist in the acceptance or rejection
of an object hypothesis. Experiments on several sequences are
presented. The experiments demonstrate how the concepts and
algorithms for model-based temporal identification/verification
could work in real applications.
II. MATCHING BASED ON THE HAUSDORFF METRIC
The Hausdorff metric [7] is a mathematical measure for comparing two sets of points in terms of their least similar members.
and
Formally, given two finite point sets
, the Hausdorff metric is defined as
(1)
where

modifications improve the robustness in practice, the obtained
“distances” (a weighted one in [9] and a th ranked one in [5])
no longer possess the properties of a metric. That is to say they
are not real distances in the strict sense. We argue that being a
metric (i.e., obeying the axiomatic rules for a metric) is important because when doing identification or verification, generally
we have several hypotheses, and we need to use a measure that
can reflect our confidence in choosing one over the others. This
is not like detection or tracking, where one only needs to find an
optimal match for a given mask. For example, it’s easy to construct examples where a partial distance does not give a measure of similarity between point sets. Although these examples
are unlikely to occur, one does face difficulties when the models
are relatively simple point sets (with not too many points) while
the scene is highly cluttered. Therefore, the above-mentioned
modified versions of the Hausdorff distance do not necessarily
offer good measures for comparison among different models.
B.

Version of the Hausdorff Metric

Another equivalent representation of the Hausdorff metric is
(see [6])
(3)
with

where
is a set and a metric such that
is a metric
and
. In the image analysis context,
space, and
can simply be the set of all the image grid points, and is usually
norm, while and are two compact sets in the image
the
plane. In this paper, we use edges as the features for matching;
thus, and are just edge maps derived from intensity images.
or
operTo alleviate the instability in (3) due to the
average as follows:
ation, Baddeley [1] has suggested an

(2)
is an underlying norm. If a model image and a scene
and
image are first processed to give two characteristic point sets,
then the model-scene matching is realized by comparing the
point sets in terms of the Hausdorff metric. Intuitively, when
there are multiple models, recognition is simply done by
computing the corresponding Hausdorff distances between the
models and the scene, and then picking out the best match.
A. Some Modified Versions of the Hausdorff Metric
is
Although theoretically attractive, the Hausdorff metric
or
opernot directly usable in practice, because the
very sensitive to
ation in the definition makes and hence
noise—a single noisy point can pull the value of far from its
noise-free counterpart. Some modifications have therefore been
proposed in the literature. For example, in [9], a weighted sum
version was proposed and found to slightly improve the recognition rate; and in [5], a th ranked partial “distance”
was used to detect a model in a static scene. The same partial
“distance” was also used to track people in [8]. Although these

(4)
is the number of points in , and
.
where
is still a metric, and topologically equivSo defined
, but is more robust to noisy data since the
alent to
contribution of a single point has been weighted. Also, by using
the average, (4) has an “expected risk” interpretation: given ,
is one which maximizes
a set which minimizes
(if and
the pixelwise likelihood of
are treated as random sets). In applications, a cutoff function
, for a fixed
, is incorporated into (4)
to give

(5)
The resulting
equivalent to

is again a metric, and topologically
. Note that in practice it is unnecessary to

LI et al.: MODEL-BASED TEMPORAL OBJECT VERIFICATION USING VIDEO

899

compute
by its definition (i.e., by computing
), which is too expensive, especially with the
norm.
Instead, distance transformations [3] are used. Thus, using a
will not cause significant extra computation,
supporting set
although is larger than and .
C. Identification/Verification with
Given two point sets,
provides a similarity measure between them. When this measure is applied to the identification/verification problem, we are concerned not only with how
good the match is but also with where the match happens in
between a
the scene. It would be meaningless to compute
small model and a large scene image. Instead, usually a region
of interest (ROI) is detected first, and matching is carried out
between the ROI and the model. In particular, in identification
problems, given the edge map of an ROI from the scene image
,
, the task is to find a model
and models
and a transformation
such that

(6)
where is an allowed transformation group for the application.
will be regarded as the potential object appearing in
Such
is a metric, we can also interpret the
the current scene. Since
as a measure of
values
in the current frame. If
, then
confidence of choosing
the problem is reduced to detecting an object in the scene; in
addition, if the model is extracted from earlier frames in the sequence, the problem reduces to one of tracking and verification.
It is not hard to search over when is the translation group.
However it is difficult to consider other transformation groups
such as affine. Even if we consider only rotation and scale, the
search becomes a daunting task. The authors of [8] have proposed an efficient search scheme for rotation using the fact that
the image takes value only on a digitized grid. In Section III-B,
motion-based segmentation is used to minimize the need for
search over the scale space.

III. MODEL-BASED POSE ESTIMATION
OBJECT VERIFICATION

AND

In this section, we present an approach to pose estimation
version of the
and verification based on matching using the
Hausdorff metric, with the motion trajectory information from
motion analysis being used as a constraint to reduce the search
space. The model acquisition step is discussed in Section III-A.
Section III-B gives a brief overview of a framework for detection, tracking and segmentation of moving objects in video acquired by a moving platform. Pose estimation and object identification are discussed in Section III-C. Section III-D discusses
methods for excluding clutter from the ROI. The pose evolution curve is defined in Section III-E. Section III-F discusses
as a confidence measure, and a confithe interpretation of
dence figure is defined. Experimental results are presented in
Section IV.

Fig. 2. Two angles defining the object orientation with respect to the camera
under the assumption of level ground (i.e., the – plane is horizontal).

XY

A. Model Acquisition
When a 3-D object is subject to complex 3-D motion
with respect to the camera, in general, multiple views of the
object are needed for adequate modeling of the object. For a
matching-based approach, images from these views constitute
a model base. In general, there are two ways for constructing
a model base: by using computer aided design (CAD) models
or by extracting objects from real images. Three-dimensional
CAD models allow one to easily manipulate the object orientation. However most objects of interest do not come with CAD
models. In this paper, for the identification experiments, the
models are constructed from real images: model images were
taken at various camera depression angles, with the objects
rotating horizontally. This allows the approach to extend to real
applications easily: for any real object of interest, we can build
its model by acquiring a set of images of the object at different
viewpoints, hence relaxing the need for a 3-D CAD model.
Although in general, the orientation of a rigid object has three
degrees of freedom, some assumptions can be made for specific applications. For example, if the object is on nearly level
ground, as in most surveillance applications, its orientation can
be characterized by only two variables. If we use an object-centered coordinate system, the object orientation is equivalent to
the camera viewing angles, defined by two angles and as
illustrated in Fig. 2.
Notice that even under the above assumption, there are still
infinitely many orientations in theory. But some observations
can be made to determine the orientations that are characteristic.
For example, with fixed, although can vary from 0 to 360 ,
it is not necessary to store images at every degree of since
the object looks very similar when changes only by a small
number (say, less than 5 ). A similar argument is valid for ,
which takes values in the interval [0 , 90 ]. More constraints
can be included for a specific application. For example, in many
applications, the value of can only change within a small range
or can even be fixed. Research has shown that it seems that the
human visual system represents objects only by a few 2-D views
(e.g., [14]). Not much is known, however, about the number of
views required for a specific object. In this work, we represent
an object with a model base in which and take on only a
finite set of values.

900

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 10, NO. 6, JUNE 2001

Fig. 3.

Whole framework shown as a diagram of procedures.

H

Fig. 4. Dynamic verification with the
metric: a moving object is first detected and its edge map is used in the following frames for verification. The sequence
is infrared with frame size 128 128 pixels.

2

Besides orientation, scale is another variable that needs to
be considered. It is possible to transform the scene object, the
model, or both, to bring them to the same size before performing
matching. Each method has its advantages and disadvantages, as
discussed in [15]. For appearance-based approaches, the model
base (or the parametric space) is constructed at a fixed scale.
Therefore, during recognition, the object size needs to be normalized with respect to the model base. There is a potential
problem with the above normalization: if the object is at a much
lower resolution than the training images, normalization can
only bring the object to the same size, but not to the same scale
at which the training images were looked at. Considering this,
we propose to acquire the model images at a resolution higher
than that at which the object is most likely to appear in real applications. At the identification/verification stage, we bring the
model to the scale of the scene object. Note that this is essentially a downsampling process, and we are getting rid of detailed
information rather than trying to add more information. Equivalently, one can build a multi-scale model base which keeps several versions at different scales for an object at a certain orientation.
B. Framework for Moving Object Detection, Tracking, and
Segmentation
As stated in the introduction, in many applications, the
camera is moving during the acquisition process. Therefore
sensor motion compensation is typically required before one
can exploit the object-induced motion information. Sensor
motion compensation is also known in the literature as image
sequence stabilization. Roughly speaking, there are two
types of stabilization methods: feature-based and optical
flow-based. We believe that optical flow-based methods are

more appropriate if the camera is of the infrared type (as in
most surveillance applications) since reliable feature detection
is more difficult in infrared imagery. Also, the brightness
(thermal) constancy assumption is more appropriate for infrared images, which is essential to the computation of optical
flow. We follow the framework reported in [10] which integrates image sequence stabilization, moving object detection,
tracking and segmentation, to form the frontend of the recognition system. Stabilization is based on the optical flow-based
approach reported in [13], where the optical flow is modeled
as a weighted sum of basis functions, and permits accurate and
fast motion computation. The computed flow field is then used
to estimate the motion parameters. An affine transformation is
used to model the sensor motion. That is, the transformation
is defined by
between the pixels of frame and frame
(7)
and
are pixels of frame
where
and frame
, respectively.
After sensor motion compensation, changing parts are detected from the camera motion compensated frame differences.
These changing parts are segmented from the background to
form ROI’s, and then tracked and updated incrementally based
on the successive motion measurements. If the object is big
enough, it is also possible to get its boundary by motion-based
segmentation. Otherwise, a bounding box is used to define the
ROI. Segmentation greatly facilitates matching: recall from (5),
. In practice,
that a supporting set is needed for computing
a smaller is desired to facilitate the computation. Segmentation not only provides a small but also greatly decreases the
operation in (6). For example, we can
search region for the

LI et al.: MODEL-BASED TEMPORAL OBJECT VERIFICATION USING VIDEO

H

Fig. 5. Dynamic identification with the
to choose one out of the three hypotheses.

901

metric: at each frame the models are compared against the detected ROI according to (6), and the

estimate the scaling from a model to the scene using the size
of the ROI (this step, however, needs to account for the inaccuracies in the segmentation step). Another example is, when
attempting verification, if the sensor-induced motion is dominant, the affine parameters computed from (7) can be used to
estimate the scale factor between two ROI’s in corresponding
frames by
(8)
as will be illustrated by an experiment in Section IV.
C. Pose Estimation and Object Identification
The segmentation step in the previous section locates potential moving objects. If the object is subject to nearly translational
rigid motion, we can use the average of the flow field in the segmented area to approximate the object velocity . If the object
is too small to support the average computation, an alternative
way is to estimate the velocity from the change in the mass
center of the detected changing area. The details of the algorithm can be found in [17]. For the pose estimation algorithm,
only the direction of is used to assist search over the model
base, although the value of is potentially usable.

H

values are used

With
plotted in a regular – coordinate
lying
system [with horizontal -axis, vertical -axis and
in the upper-right quadrant], we can easily identify the constraints on the angles , imposed by the signs of the components of . For example, consider a forward moving object.
, we have
Assuming
, then
if
if
, then
if
, then totally determines the object orientation
in a top view;
, then
and
if
, etc.
One can find that constraints of the last type are most effective, and also most common in real applications. For example,
a typical surveillance camera may have between 0 and 90 .
Under the constraint provided by motion analysis, pose estimation and object identification problems are reduced to the following search problem: given an ROI, find the best matching
model from only the model images whose orientations
lie in the subspace
with
and
being two intervals, and
and
being estimated as above.
and
are application-dependent. In the exThe values of
. Formally,
periments reported in this paper, we let

902

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 10, NO. 6, JUNE 2001

by using (9), we estimate the object pose for the current frame
as
(9)
is the model with
where is defined as before, and
[note that in practice, a local search
orientation parameter
using (6) is still needed to account for the inaccuracy of the
segmentation step]. Obviously, (6) and (9) involve essentially
similar computations except that we use the former to choose
the object type and the latter to decide the orientation. In fact, if
we treat the same object at different poses as different classes,
then (9) is implied by (6).
The benefit from the constrained search is twofold: the search
is speeded up; and more importantly, by reducing the number
of candidates, the probability of false match is reduced. Further constraints can be obtained if we consider the relationship
among frames which are temporally close. In this work, we exploit this type of information through what we refer to as the
pose evolution curve as discussed in Section III-E.
Note that, in the worst case where motion analysis gives totally false information for, the constraints obtained above are no
longer valid. To deal with this situation, the algorithm should resort to basic full search whenever the confidence measure (see
Section III-F) of the current estimate drops below a threshold.

H

D. Excluding Clutter in the ROI
The detected ROI contains not only the potential object but
also background clutter. According to (5), every edge pixel
, which is undewithin the ROI could contribute to
sirable. When doing identification, the following technique is
: given a model
used to exclude clutter before calculating
and an ROI , we keep points in the ROI only if they are
. Here
is a transformed
within a certain distance of
under transformation . That is, a new ROI
version of
is formed by
and

(10)

where is a small positive number. On the other hand, if we
are attempting verification, the model is typically an ROI from
previous frames. In this situation, the segmentation boundary
estimated in Section III-B will also be used to constrain , and
a larger should be used to account for the inaccuracies in the
segmentation.
E. Pose Evolution Curve
By plotting the estimated pose over time, we get the pose evolution curve for the object. The curve is an additional indicator
of identification/verification confidence: under the continuous
motion assumption, the pose evolution curves should display
some smoothness in either the or domain. For example, if
the pose evolution curve suffers from random jitter between adjacent frames, chances are that the object hypothesis is wrong to
begin with. Of course, when objects are similar in shape, there
may not be enough information from the pose evolution curves
value should offer information
only, but the corresponding

Fig. 6.
values and confidence figures for the sequence in Fig. 5, from
frame 318 to 327: (a)
values versus frame index and (b) computed
confidence figure .

P

H

for identification purposes. The following quantity is defined
to give a quantitative description of the smoothness of the pose
evolution curve:
(11)
where
number of frames;
frame index;
pose evolution curve;
smoothed version of
( denotes convolution).
is a discrete window function whose support
Here,
, with I a small positive integer. In the experiments
is
and
in this paper,
. In general, given a sequence, a correct
hypothesis should yield a pose evolution curve with a smaller
than an incorrect hypothesis does.
When the number of model images is large, it is helpful to
also consider several top matches given by (9), instead of only
looking at the best matching one. If we keep several top matches
and plot the estimated poses in a common coordinate system at
each frame, we get the band of pose estimates. This pose information is also helpful for testing the hypotheses; if a hypothesis
is correct, the corresponding pose band should be more concentrated than that of a wrong hypothesis, unless the object’s appearance changes dramatically even with small changes in orientation. Again, to quantitatively evaluate how a pose band is

LI et al.: MODEL-BASED TEMPORAL OBJECT VERIFICATION USING VIDEO

903

Fig. 7. Sample images in the model base. The upper row is for model 1; the lower row for model 2.

H

Fig. 8. Dynamic identification with the
metric. The task is to identify the moving object in the scene as one of the two hypotheses. Top row: sample images
(of size 320 240) from one sequence with black tank moving around. The moving object has been highlighted by a bounding box defined at the segmentation
values and estimated poses (; ) at each frame for the two hypotheses. Even though the two
step. Middle row: two hypotheses. Bottom row: corresponding
hypotheses are similar in shape, at each frame, the algorithm gives smaller H values for hypothesis 1 than for hypothesis 2.

2

concentrated, the following quantity
matches are kept:

H

is defined, if the top

(12)

are as before, and
is the average of the pose
where
angles given by the top matches at frame . In general, given a
sequence, a correct hypothesis should yield a band of poses with
than an incorrect hypothesis does. The motivation
smaller
behind the above definitions is that and are in a sense like
sample mean square deviations.
Notice that the pose angle is periodic, i.e., 360 is equivalent to 0 . Therefore when calculating the average of angles [the
in (11) and the average over the anmoving average for
should be wrapped around with
gles in (12)], the value of
respect to 360 whenever the periodicity demands it.
F. Interpreting

as a Confidence Measure

has some nice properties such as
As mentioned earlier,
being a metric, improved robustness, “expected risk” interpretation, etc. Being a metric is important especially from a theoretical point of view. For example, we would like a measure
that gives the same result for
and
.
allow a confidence interpretation. For
These properties of
the identification problem, this means that at each step the

value for each model is treated as a measure of confidence in
choosing a certain model: the smaller this number is, the more
confident we are of choosing the model. If multiple models are
kept as frames are processed, although at some time we may
make the wrong choice, subsequent updates will hopefully provide the right choice.
For the verification problem, a confidence interpretation is
also helpful: whenever there is a sharp decrease in confidence,
what may have happened is that the object is no longer the previous one, or the orientation of the object has changed dramatically. This information can be used to update the model hypotheses. Verification, in this respect, is similar to a tracking
problem like that in [8]. But keeping a 3-D model of the object
allows one to tackle problems involving more general complex
3-D motions.
To conform with the common understanding of confidence,
i.e., with real numbers 1.0 and 0.0 representing the most and
least confidences respectively, we define a confidence figure
value as
based on the
(13)
, which means
The meaning is obvious: when
, we set
; and when
goes to infinity,
. The underlying reason for choosing function
is that it is a convex function on
and thus

904

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 10, NO. 6, JUNE 2001

Fig. 9. Confidence figure P versus frame index (computed every three
frames; note that nothing is shown for those frames in which the object is
invisible; same for Figs. 10 and 11).

Fig. 11. Bands of the pose estimates (angle ) versus frame index. (a)
Hypothesis 1 and (b) hypothesis 2. The computed C values [see (12)] are 20.9
and 80.4 for hypothesis 1 and hypothesis 2, respectively, strengthening the
confidence in choosing hypothesis 1.

module would not be used. This is equivalent to choosing (6) or
(9) based on the specific problem.
IV. EXPERIMENTAL RESULTS AND ANALYSIS
Fig. 10. Pose evolution curves (angle ) for the sequence shown in Fig. 8. The
computed S values [see (11)] are 8.1 and 64.9 for hypothesis 1 and hypothesis
2, respectively, strengthening the confidence in choosing hypothesis 1.

is still a metric, which is desirable for comparing multiple
hypotheses. It is worth pointing out that this confidence figure
, thus it will change if
is
depends on the measurement of
reparameterized (for instance, if the resolution of the image is
doubled). Therefore, it is better to use this figure for comparing
different hypotheses than to interpret it for a single hypothesis
is
along the temporal axis unless the size of the supporting
fixed for all frames.
In summary, the whole framework is illustrated by the diagram shown in Fig. 3. It is worth pointing out that we have
assumed that the object is moving and that detection and segmentation are based on analyzing the object motion. If the object is stationary and/or the detection and segmentation are accomplished by other means, the verification step still works (in
this case, pose variation may still exist due to sensor motion).
We have used the dotted-lined path to show that possibility although it is not implemented in the current work. Notice that,
if no pose variations are considered, then the pose estimation

Experiments have been performed with both infrared and
optical sequences. Several experimental results are presented
in this section. For convenience of presentation, the detailed
discussion of the results is mainly focused on two sequences,
one infrared and one optical. Experiments on other data will be
briefly listed. In the experiments presented in this paper, and
in (5) are fixed at 1 and 4 respectively, and in (10) is 5. The
edge maps were detected using Canny’s algorithm [4].
In the first experiment, simple verification is carried out on
an infrared sequence acquired by a helicopter flying toward a
tank. Due to the fast camera motion, the scaling effect becomes
significant within a few frames. Fig. 4 illustrates the verification
procedure. A moving object is first detected, which is possible
only after the dominant sensor motion is compensated. Then an
ROI is formed and processed to get the edge map of the object. This edge map, used as the model, is verified in subsequent
frames. In Fig. 4, the ROI from frame 302 is superimposed on
frames 310, 315, and 320, after the locations have been estimated using (6). Note the substantial scaling of the object—a
typical scale factor is 1.02 by (8) for two consecutive frames,
thus the object gets almost 1.4 times larger in frame 320 than
when it was in frame 302 (this is only during a period slightly
more than half a second with a 30 Hz frame rate). It would be

LI et al.: MODEL-BASED TEMPORAL OBJECT VERIFICATION USING VIDEO

905

Fig. 12. Each row contains sample images from an optical sequence. The first sequence is tested with the modelbase given by Fig. 7. The second sequence is
tested with models containing two cars, one of which being the moving car in the sequence, and rotation is allowed horizontally only.

very expensive if one wanted to handle this by searching over
the scale space using the Hausdorff metric. However, by compensating the scaling using (8) the algorithm is able to locate the
values (meaning high confidence).
tank and report small
In the above example, the object motion is approximately
2-D; thus verification is similar to a tracking problem after
the object has been detected. However, if the object is lost
somehow (e.g., due to occlusion), then re-appears later, the
algorithm should be able to verify if it is the previous object.
The idea becomes more obvious when the object motion is
3-D and induces dramatic orientation changes. In this situation,
even if simple tracking can be done on a frame-by-frame basis,
it is hard to say if it has found the same object because the
object looks too different in later frames than in earlier frames.
What might have happened is that the tracker has drifted away.
However, with a model in mind, verification can still be done
by updating the model according to the motion trajectory
information: if the confidence figure for certain model at the
current frame drops suddenly, but a new pose of the model
(either predicted by the trajectory or by a full search in pose
domain) gives high confidence, then the new pose will be kept
and the present object is verified to be the previous one. This
is represented by the feedback path from verification to pose
estimation in Fig. 3.
Fig. 5 shows how identification works for the same sequence
as in Fig. 4. Given the sequence, the task is to identify which of
the three hypotheses is present in the current sequence. In this
example the ground truth is model 2. In this experiment, object
contours derived from CAD models are used as models. For
each frame, an ROI is detected, then each model is warped to the
size of the ROI. To account for inaccuracies in the detection and
segmentation step, for each model, a local search in translation
space and in scale space is carried out according to (6), and
value is used for this model. It is clear from the
the best
figure that, although in some individual frames the algorithm
value for model
reports false identification results (i.e., the
2 is bigger than those for the others), by using multiple frames,
the overall confidence of choosing model 2 is higher than for
choosing others. To see this more clearly, we plotted in Fig. 6 the

value and the confidence figure for each model from frames
318 to 327. The overall confidence in model 2 is obvious.
In the next experiment, the sequences are optical images acquired by a hand-held video camera. Model images were similarly acquired, with the help of a turntable. The model base was
constructed as explained in Section III-A. Currently, the model
base contains two model tanks. Fig. 7 shows four model images
for each of the models. Accurate pose information can be obtained for the model images if the camera is under fine control,
as in [11]. In this work, the model pose was obtained manually
through visual inspection of the model image. The domain is
only coarsely divided, with taking only two values, 30 and
45 ; and for each , varies by approximately 5 in [0 , 355 ]
(see Fig. 2 for the definitions of the angles).
Fig. 8 illustrates how identification works when the object
is subject to 3-D motion. The sequence contains two objects,
and only the black one is moving. The task is to identify the
moving object as one of the two hypotheses (i.e., the object
needs to be identified as one of the two candidate models). Of
course, in this example, the ground truth is model 1 (the black
object). The moving object is first detected and segmented by
the aforementioned procedure; then identification and pose estimation are carried out in each frame. The first row in Fig. 8
shows sample frames from one of the sequences with the detected moving object highlighted by a bounding box (note that
only the black object is moving in this sequence). Below each
value and the estimated pose
frame are the corresponding
(two angles and ) computed according to (9). In the compupixels) in the translation
tation, again a local search (of size
space was performed. Although no search in scale space was
performed in this example, the result is good enough, implying
that the segmentation step gives a good estimate of the scale
factor between the model and the scene object (a search over
is used in other experiments in the paper). It
is obvious that, although the appearances of the objects are dramatically different, they have similar geometric shapes. Therefore it is in fact a difficult task to distinguish between these two
hypotheses only from their edge maps. Yet in each frame, the
algorithm is able to choose model 1 correctly, and the pose es-

906

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 10, NO. 6, JUNE 2001

Fig. 13. Each column contains sample images from an IR sequence. The sequences are named as rng 16 18; rng 17 20; rng 19 06; rng 19 07, and rng 19 18,
from left to right, respectively. Three hypotheses are assumed during verification. The object motion is assumed to be 2-D (no 3-D model of the objects is available
in this case).
TABLE I
PERFORMANCE OF THE ALGORITHM ON SOME TEST SEQUENCES

timates are close enough (see Fig. 8 for the estimated poses for
these sample frames).
Fig. 9 shows the confidence figure computed according to
(13) By plotting the estimated pose in each frame, we can get the
pose evolution curve for each hypothesis, as shown in Fig. 10.
The pose evolution curve is found to be able to strengthen the
confidence since the curve for hypothesis 1 is smoother than that
for hypothesis 2, which is reasonable under the assumption of
continuous motion. According to (11), the computed values
are 8.1 and 64.9 for hypothesis 1 and hypothesis 2, respectively.
Note that for the aforementioned reason, the value is not informative in this experiment; thus only angle is plotted in the
pose evolution curve.
As mentioned before, when the number of model images is
large, it is helpful to keep several top matches given by (9), and
consider the band of pose estimates. For the sample sequence in
Fig. 8, the pose bands are plotted in Fig. 11 (for angle only).
This pose information is illustrative: the band for hypothesis 1 is
more concentrated than that for hypothesis 2, thus strengthening
our confidence in choosing hypothesis 1. According to (11), the
computed values are 20.9 and 80.4 for hypothesis 1 and hypothesis 2, respectively.
We now briefly list the experimental results for other sequences, including optical and infrared imagery. Fig. 12 shows
two optical sequences in which the objects are subject to 3-D
motion. The first sequence (two_tank1) was tested with the

model base shown in Fig. 7. The second sequence (two_car3)
was tested with two models containing two cars at different
horizontal orientations, one of which being the moving car in
the sequence. Fig. 13 shows sample sequences from a large
FLIR database. Each of these sequences was tested with three
hypotheses which are edge maps from other sequences in the
database, with one being true hypotheses. The experimental
results are summarized in Table I. Since each sequence is
relatively long and the object is only big enough near the
terminal portion of the sequence, we used only the final few
frames in verification, as shown in the column “number of
frames used.” Note that there is no 3-D model available for
the sequences, and the object motion is largely 2-D in the last
few frames. Thus verification was done by using (6), and no
and values were computed. For sequences in Fig. 12, (9)
was used, and corresponding and values were computed.
From the table, it is obvious that although at some frames the
algorithm may be confused, there is not a single case where the
algorithm fails if all the frames are considered. Specifically,
and values alone are able to give correct results when they
are available.
V. SUMMARY
Object recognition is a well-researched area, and has been
approached from different aspects (e.g., [2], [12]). In this paper,

LI et al.: MODEL-BASED TEMPORAL OBJECT VERIFICATION USING VIDEO

we attempted to exploit temporal information in video to assist recognition. Specifically, in addition to using motion for
detection and segmentation, our approach uses the trajectory
of a moving object to predict its approximate pose. Also, the
motion coherence of an underlying object is measured through
the pose evolution curses. The experiments demonstrate how the
concepts and algorithms for dynamic identification/verification
work on real video. The paper is focused on studying the potential of the idea of temporal object verification using video.
The work reported here is intended to show how temporal information can be exploited to help verification/identification when
video is available, rather than to build an end-to-end system.
When building an end-to-end system, many system-level issues
arise, such as time and space complexity, which are not addressed in this paper.
In this work, although feature (edge) detection is necessary,
version of Hausdorff metric-based matching,
we use the
which does not require feature correspondence and is robust to
noise, hence relaxing the requirements on the feature detection
step. This may be attractive especially in applications that use
infrared sensor, since in this situation, feature-detection is not
reliable, and appearance-based approaches may also have difficulties. However, the work is not intended to replace other
methods such as appearance-based recognition approaches. In
fact, our major focus has been on how to utilize temporal information available in a video for better recognition. Some basic
ideas developed in the work are meaningful irrespective of the
specific representation of the object, such as pose prediction
based on the object’s trajectory, and computing motion coherence based on pose evolution curves, etc. Also, the approach
is proposed for a more constrained scenario: verification and
identification (with much fewer hypotheses than a recognition
algorithm usually assumes), and simple object motion has been
assumed. Even though this is a constrained scenario, similar situations can be found in many applications such as surveillance
systems. We have found the proposed method promising for verification/identification tasks.

ACKNOWLEDGMENT
The authors would like to thank Dr. R. Sims for providing the
FLIR data.

REFERENCES
[1] A. J. Baddeley, “Errors in binary images and an L version of the Hausdorff metric,” Nieuw Archief voor Wiskunde, vol. 10, pp. 157–183, 1992.
[2] R. Basri and D. Jacobs, “Recognition using region correspondence,” in
Proc. Int. Conf. Computer Vision, 1995.
[3] G. Borgefors, “Distance transformations in digital images,” in Comput.
Vis., Graph., Image Process., vol. 34, 1986, pp. 344–371.
[4] J. Canny, “A computational approach to edge detection,” IEEE Trans.
Pattern Anal. Machine Intell., vol. PAMI-8, pp. 679–698, 1986.
[5] D. Doria and D. Huttenlocher, “Progress on the fast adaptive target detection program,” RSTA Tech. Rep. ARPA IU Program, pp. 589–594,
1996.
[6] H. Federer, Geometric Measure Theory. Berlin, Germany: SpringerVerlag, 1967.
[7] F. Hausdorff, Set Theory, 2nd ed. New York: Chelsea, 1962.

907

[8] D. Huttenlocher, J. Noh, and W. Rucklidge, “Tracking nonrigid objects
in complex scenes,” in Proc. Int. Conf. Comput. Vis., Berlin, Germany,
1993, pp. 93–101.
[9] B. Li, R. Chellappa, Q. Zheng, and S. Der, “Experimental evaluation
of neural, statistical and model-based approaches to FLIR ATR,” Proc.
SPIE, vol. 3371, pp. 388–397, 1998.
[10] B. Li, Q. Zheng, and S. Der, “Moving object detection and tracking in
FLIR images acquired by a looming platform,” in Proc. Joint Conf. Information Sciences, Research Triangle Park, NC, 1998, pp. 319–322.
[11] H. Murase and S. Nayar, “Visual learning and recognition of 3-D objects
from appearance,” Int. J. Comput. Vis., vol. 14, pp. 5–24, 1995.
[12] R. P. N. Rao, “Dynamic appearance-based recognition,” in Proc. IEEE
Conf. Computer Vision Pattern Recognition, 1997, pp. 540–546.
[13] S. Srinivasan and R. Chellappa, “Image stabilization and mosaicking
using the overlapped basis optical flow field,” in Proc. IEEE Int. Conf.
Image Processing, Santa Barbara, CA, 1997.
[14] M. Tarr and S. Pinker, “Mental rotation and orientation-dependence in
shape recognition,” Cogn. Psychol., vol. 21, pp. 233–282, 1989.
[15] S. Ullman, High-Level Vision. Cambridge, MA: MIT Press, 1996.
[16] I. Weiss, “Geometric invariants and object recognition,” Int. J. Comput.
Vis., vol. 10, pp. 207–231, 1993.
[17] Q. Zheng and R. Chellappa, “Motion detection in image sequences acquired from a moving platform,” in Proc. IEEE Int. Conf. Acoustics,
Speech, Signal Processing, Minneapolis, MN, 1993, pp. 201–204.
[18] A. Zisserman, D. Forsyth, J. Mundy, C. Rothwell, J. Liu, and N. Pillow,
“3D object recognition using invariance,” Artif. Intell., vol. 78, pp.
239–288, 1995.

Baoxin Li received the B.S. and M.S. degrees in electrical engineering from the University of Science and
Technology, China, in 1992 and 1995, respectively.
He received the Ph.D. degree in electrical engineering
from the University of Maryland, College Park, in
2000.
He is currently with Sharp Laboratories of
America, Inc., Camas, WA, working on multimedia
analysis for consumer applications. He was previously with the Center for Automation Research,
University of Maryland, working on face and object
tracking and verification in video, automatic target recognition, and neural
networks. His interests include pattern recognition, computer vision, neural
networks, and multimedia processing.

Rama
Chellappa
(S’78–M’79–SM’83–F’92)
received the B.E. (Hons.) degree from the University
of Madras, Madras, India, in 1975 and the M.E.
(Distinction) degree from the Indian Institute of
Science, Bangalore, in 1977. He received the
M.S.E.E. and Ph.D. degrees in electrical engineering
from Purdue University, West Lafayette, IN, in 1978
and 1981, respectively.
Since 1991, he has been a Professor of electrical
engineering and an Affiliate Professor of computer
science with the University of Maryland, College
Park. He is also an Associate Director with the Center for Automation Research
and is also affiliated with the Institute for Advanced Computer Studies. Prior to
joining the University of Maryland, he was an Associate Professor and Director
of the Signal and Image Processing Institute with the University of Southern
California, Los Angeles. During the last 20 years, he has published numerous
book chapters and peer-reviewed journal and conference papers. Several of
his journal papers have been reproduced in collected works published by
IEEE Press, IEEE Computer Society Press, and MIT Press. He has edited
a collection of papers on Digital Image Processing (Santa Clara, CA: IEEE
Computer Society Press), co-authored a research monograph on Artificial
Neural Networks for Computer Vision (with Y. T. Zhou) (Berlin, Germany:
Springer-Verlag), and co-edited a book on Markov Random Fields (with A.
K. Jain) (New York: Academic). He also served as Co-Editor-in-Chief of
Graphical Models and Image Processing. His current research interests are

908

image compression, automatic target recognition from stationary and moving
platforms, surveillance and monitoring, automatic design of vision algorithms,
synthetic aperture radar image understanding, and commercial applications of
image processing and understanding.
Dr. Chellappa has served as an Associate Editor for the IEEE TRANSACTIONS
ON SIGNAL PROCESSING, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND
MACHINE INTELLIGENCE, IEEE TRANSACTIONS ON IMAGE PROCESSING, and
IEEE TRANSACTIONS ON NEURAL NETWORKS. He served as a member of the
IEEE Signal Processing Society Board of Governors from 1996 to 1999. He is
currently serving as the Editor-in-Chief of IEEE TRANSACTIONS ON PATTERN
ANALYSIS AND MACHINE INTELLIGENCE. He has received several awards,
including the 1985 NSF Presidential Young Investigator Award, a 1985 IBM
Faculty Development Award, the 1991 Excellence in Teaching Award from the
School of Engineering, University of Southern California, and the 1992 Best
Industry Related Paper Award from the International Association of Pattern
Recognition (with Q. Zheng). He has been recently elected as a distinguished
Faculty Research Fellow (1996–1998) at the University of Maryland. He is a
Fellow of the International Association for Pattern Recognition. He has served
as a General and Technical Program Chair for several IEEE international and
national conferences and workshops.

IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 10, NO. 6, JUNE 2001

Qinfen Zheng received the B.S. and M.S. degrees in
electrical engineering from the University of Science
and Technology, China, in 1981 and 1984, respectively. He received the Ph.D. degree in electrical engineering from the University of Southern California,
Los Angeles, in 1992.
From 1992 to 1994, he was an Assistant Research
Scientist with the Center for Automation Research,
University of Maryland, College Park. During
1994–1995, he was a Scientist with the Lockheed
Martin Laboratory, Baltimore, MD. Since 1996,
he has been an Associate Research Scientist with the Center for Automation
Research, University of Maryland. His research interests include image and
video analysis, automatic target detection/recognition, human identification,
motion analysis, and remote sensing.

Sandor Z. Der received the B.S. and M.S. degrees in
electrical engineering from Virginia Polytechnic Institute, Blacksburg, in 1986 and 1988, respectively.
He received the Ph.D. degree in electrical engineering
from the University of Maryland, College Park, in
1995.
He is currently with the U.S. Army Research
Laboratory, Adelphi, MD, working on image
exploitation, including automatic target recognition
and sensor modeling. He was previously with the
U.S. Army Night Vision and Electronic Sensors
Directorate, working on synthetic image generation, automatic target recognition, and sensor simulation. His interests include pattern recognition, computer
vision, neural networks, and optical sensors.

Extracting Key Frames from Consumer Videos
Using Bi-layer Group Sparsity
1

2

2

Zheshen Wang , Mrityunjay Kumar , Jiebo Luo , Baoxin Li

1

2

1

Computer Science, Arizona State University
699 S. Mill Ave., Tempe, AZ, 85281

Eastman Kodak Company
1999 Lake Ave., Rochester, NY, 14650

{zheshen.wang, baoxin.li}@asu.edu

{mrityunjay.kumar, jiebo.luo}@kodak.com
This paper presents a bi-layer group sparse representation to
exploit spatio-temporal correlation present in the video data for
key-frame extraction from consumer videos. Specifically, the
input video frames are first segmented into visually
homogeneous patches and the proposed bi-layer group sparsity,
which is derived by combining the traditional group sparse Lasso
and Moreau-Yosida regularization, is used to enforce group
sparsity at two levels simultaneously: (i) patch-to-frame, and (ii)
frame-to-sequence. The patch-to-frame level group sparsity
captures the spatial correlation among the frames whereas the
inter-frame temporal correlation is taken into account at the
frame-to-sequence group sparsity step. The grouped sparse
coefficients are further combined with high-level semantic-based
frame quality scores to select key frames.

ABSTRACT
Compared to well-edited videos with predefined structures (e.g.,
news or sports videos), extracting key frames from unconstrained
consumer videos remains a much more challenging problem due
to their extremely diverse contents (no pre-imposed structure)
and uncontrolled video quality (e.g., due to poor lighting or
camera shake). In order to exploit spatio-temporal correlation
present in the video for key frame extraction, we propose a bilayer group sparse representation in which the input video frames
are first segmented into homogeneous patches and group sparsity
is imposed at two levels simultaneously: (i) patch-to-frame, and
(ii) frame-to-sequence. The grouped sparse coefficients are
further combined with frame quality scores to generate key
frames. Extensive experiments are performed on videos from
actual end users. Results obtained by the proposed approach
compare favorably with existing methods to confirm its
effectiveness.

In the rest of this paper, we first describe the proposed approach
in Section 2. Experiments, results, and analysis are presented in
Section 3. We conclude the work in Section 4.

Categories and Subject Descriptors

2. BI-LAYER GROUP SPARSE
REPRESENTATION

I.2.10 [Vision and Scene Understanding]: Video Analysis.

In this paper, we propose a bi-layer group sparse representation
for key frame extraction from general consumer videos. Different
from traditional clustering-based methods, this approach relies
on signal-reconstruction perspective for key frame selection. In
particular, the proposed approach exploits the fact that frames
selected as key frames should reconstruct the input video with
minimum reconstruction error.

General Terms
Algorithms, Design, Experimentation.

Keywords
Key frame extraction, group sparsity, consumer video.

1. INTRODUCTION

Due to the fact that the expected number of key frames is always
very small compared to the length of the input video (i.e., sparse
selection of video frames), a sparse representation framework for
making such selection is appropriate.

Rapid growth in the use of digital cameras and camcorders has
made automatic key frame extraction a necessity for managing,
and browsing huge amounts of video data. The key frame
extraction problem has been investigated for years [1, 2],
primarily targeting structured and clean videos (e.g., sports and
news videos [3]). However, such algorithms are sub-optimal for
consumer videos as the latter record extremely diverse contents,
often lack pre-imposed structure, and usually exhibit low quality
due to factors such as poor lighting and camera shake. In
addition, sound track or metadata are typically not available for
unedited consumer videos.

2.1 Sparse Group Lasso
Recently, sparse learning via L1 regularization [4] and its various
extensions has received increasing attention in computer vision,
pattern recognition, and multimedia computing [5]. Specifically,
based on an over-complete dictionary A  R
bases

y  Ax

*Area Chair: Bernard Merialdo.

where

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
MM’11, November 28–December 1, 2011, Scottsdale, Arizona, USA.
Copyright 2011 ACM 978-1-4503-0616-4/11/11...$10.00.

yR

m1

mn

(m<n) with n
(1)

is the target signal to be approximated,

n1

is the unknown (sparse) coefficient vector. The
xR
sparsest solution can be obtained by solving a L1 optimization
problem in polynomial time by the standard linear programming
method [6]:
(2)
min || Ax  y ||22  || x ||1
x

1505

where

 is a regularization parameter.

in which A '( j )  Rmn ' consisting of patches from all frames
except y(j) and n ' 

However, for cases when group attributes are present (e.g.,
categorical information of images, image sources of sub-image
patches), the Lasso solution is not satisfactory as it only selects
variables without considering the grouping factors. To overcome
this problem, sparse group Lasso [7, 8] was proposed, which
imposes an additional group sparse constraint upon the original
Lasso penalty. Using Moreau-Yosida regularization, we consider
the sparse group Lasso criterion [9]
g
1
min || Ax  y ||22 1 || x ||1 2  wGk || xGk ||2
x
2
k 1

where A  R

mn

r 1

( j)

(3)

2.3 Bi-layer Sparse Group Lasso for Video
Reconstruction

1 || x ||1 indicates traditional
g

w
i 1

Gi

|| xGi ||2

As described in Section 2.2, traditional group sparse
representation can be applied to reconstruct each frame of the
input video separately, so that sparse coefficients are optimized
for each frame independently (i.e., a set of coefficient x(j)
obtained for the entire sequence). Due to the visually continuous
nature of video, the sparse solver turns to select patches from
temporally nearby frames to reconstruct the target frame. Hence,
overall, it is difficult to differentiate the relative contributions of
each frame to reconstruct the entire sequence.

further

imposes group-level sparsity in computing the cost to reconstruct
y. 1 and 2 control the relative contributions of the two terms,
respectively. Intuitively, Eq. (3) computes a sparse coefficient
vector to approximate y, where nonzero coefficients exhibit a
grouping effect.

2.2 Sparse Group Lasso for Frame
Reconstruction

In this work, we propose to introduce another layer of grouping
by accumulating the reconstruction errors of each frame with its
corresponding dictionary, which yields a bi-layer group sparsity
formulation:

Due to the temporal redundancy present in videos, it is possible
to reconstruct each frame of the input video as a sparse linear
combination of the remaining frames. Formally, given an input
video containing n frames y(1), y(2), …, y(n), where y ( j )  Rm1
and m is the dimension of features representing frame

min

( j)

y , we

x

can find a sparse set of coefficients

x( j )  [ x1( j ) , x2( j ) ,..., xn( j ) ]T , j  1,..., n
to reconstruct frame y(j) based on dictionary
consists of all frames from y except y(j)

(5)

where
( j)

 y , i  j , i, j  1,..., n
ai( j )   m1

0 , i  j

n
1 n
|| A '( j ) x ' y ( j ) ||22 1 || x ' ||1 2  wGk || x 'Gk ||2 (9)

2 j 1
k 1

Compared to Eq. (8), Eq. (9) sums reconstruction errors for each
frame y(j) based on the corresponding dictionary A(j) , which
contains patches from all frames except y(j) . Note that, in Eq.
(9), the expected coefficients x’ are shared by all of the n frames,
which requires a global optimization over the entire group (i.e.,
all frames of the input sequence) to solve the sparse
representation problem. By stacking the target frames and their
corresponding dictionaries

(4)

A( j )  Rmn , which

A( j )  [a1( j ) , a2( j ) ,..., an( j ) ]T , j  1,..., n

denotes the number of total patches (for

of patches from frame j in A '
as zeros instead of removing
them from the dictionary) and x '( j )  n '1 are the patch-level
sparse coefficients. All patches belong to n non-overlapping
groups, which corresponds to n input frames. Intuitively, Eq. (8)
seeks a set of sparse coefficients x(j) to reconstruct frame y(j) by
using a small amount of patches from a limited number of frames
except y(j) itself. (Note that we use a quantized histogram of gridbased SIFT features, so the dimensions of features that represent
patches and frames are the same. More details about features
will be described in Section 3.1).

overlapping groups xG , xG , …, xG and wG is the weight
1
2
k
g

element-level sparse penalty and 
2

r

a consistent representation for all frames, we set the dimensions

, y  R m1 , x  R n1 is divided into g non-

for the kth group. In Eq. (3),

n

l

(6)

To exploit the local spatial information present in the input video
frames, we first segment each frame into homogeneous subimage
patches:
(7)
y ( j )  { p1( j ) , p2( j ) ,..., pl( j ) }

mn n '
A '  [ A '(1) , A '(2) ,..., A '( j ) ]T , A '  R

(10)

Y  [ y (1) , y (2) ,..., y ( n ) ], Y  Rmn1

(11)

Eq. (9) can be further written as
n
1
min || A ' x ' Y ||22 1 || x ' ||1 2  wGk || x 'Gk ||2 (12)
x
2
k 1

j

where p(j) are the patches from frame y(j) and the total number of
patches in y(j) is lj. Then, frame reconstructions are performed on
the patch level with both patch-level and frame-level sparsities
imposed by using the sparse group Lasso formulation in Eq. (3):

which can be solved by using the regular group sparse solver
(e.g., [9]). In other words, the multi-task group sparse
representation problem can be viewed as a single-task group
sparse representation problem with dimension-concatenated
target signals and dictionaries.

n
1
( j)
( j)
( j) 2
( j)
min
||
A
'
x
'

y
||


||
x
'
||


wGk || x 'Gk ||2 (8)

2
1
1
2
x '( j ) 2
k 1

With the obtained sparse coefficients x’ on the patch level,
frame-level coefficients x can be further computed as a weighted

1506

Initial an empty set  . Loop for the following steps until 
contains K indices (we used the number of ground-truth key
frames to specify K in our experiments):

summation of patch-level coefficients (weighted by the sizes of
patches) involved in the current frame.
b

x ' j   xr , j = 1,…, n

(13)

(1) Choose the index with the largest coefficient

r a

in which a 

j 1

l
q 1

q

j

 1 and b   lq , which are the number of

i1  arg max{xi | i  1,...,n }

q 1

(2) Check the surrounding region of i1 in s and choose the
index i2 with the largest frame quality score

patches of the first j-1 frames and j frames, respectively.

2.4 Proposed Approach for Key Frame
Extraction

i2  arg max{si | i [i1   , i1   ]}
(3) Set the surrounding region of i2 in both x and s as zero

xi  0, si  0, i [i2   , i2   ]

si( IQ )  exp( BIQI ( fi ))

Frames from the original video with indexes in  are selected
as final key frames.

3. EXPERIMENTS
To verify the effectiveness of the proposed approach, we
conducted experiments on video from actual users and compared
the results to human-labeled ground-truth and results from other
methods. A total of 17 videos provided by the authors of [2] were
originally selected from 3000+ home videos [12] with resolution
640X480 or 320X240, frame rates from 24 to 30, and average
length of 29 seconds. Contents of the videos range from natural
sceneries, trips, sports, and outdoor activities to concerts,
weddings, birthday parties, and card games, and therefore span
reasonably the space of general consumer videos, with each video
clip containing a single shot. Note that this assumption is
generally true for videos captured using digital or phone cameras,
thus removing the need for shot boundary detection.

(14)

Frames with high image quality score (the larger the better image
quality) are preferred as key frames.
Scene complexity score of a single frame is defined as its
Kolmogorov complexity measured by the normalized (to the
frame size) length of its Bzip2 sequence [11].

si( SC )  BZIP2( fi ) / size( fi )

(15)

Frames with high scene complexity scores (i.e., informative
frames) are more likely to be key frames.

3.1 Experimental Settings

Color histogram change score is defined as the maximum
Euclidean distance between the color histograms (we use
concatenated hue histograms in HSV space over a two-level
image pyramid, 1X1 and 2X2) of the current frame and the
( i   )th and the ( i   )th frames, respectively.



( H i  H i  )2 , ( H i  H i  )2



For a given video, we first segment the sequences into frames
according to their original frame rates. For spatio-temporal data
reduction, extracted frames are further down-sampled to
320X240 (if the original resolution is 640X480) spatially and
temporally with a step of 5 frames (i.e., each group of 5
consecutive frames is defined as a snippet and the first frames
are used as representatives for each snippet). Dense SIFT
descriptions [13] are then extracted on an image grid
(GridSpacing=5, PatchSizes=8 and 16 in our experiments) for H,
S, V color channels of each frame and further quantized to 100
dimensions by k-means clustering on randomly selected samples
over the entire video. Histograms of quantized features are then
computed over a three-level spatial pyramid to yield a 2100dimension feature vector for each frame.

(16)

This score captures salient visual scene changes of the input
video. According to an assumption that transition frames are of
less interest, frames with high color histogram change scores
(i.e., frames that appear in the relatively more continuous
portions of the videos) are better candidates for key frames.
The above scores are normalized to [0, 1] for the given video
before further computation. Finally, the frame important score is
defined as a weighted summation of the above three scores

si    [s

( IQ )
i

( SC )
i

,s

( CC ) T
i

,s

]

(20)

(4) Add i2 to set  .

Image quality score computes BIQI image quality score [10]



(19)

i

Using the obtained frame-level sparse coefficients x from Eq.
(13), key frames can be selected according to the relative
importance of each frame to reconstruct the input video.
However, sparse representation is typically appropriate for only
low-level features. To compensate for high-level semantics (e.g.,
high-quality frames preferred to blurry frames with the same
visual contents for key frames), we incorporate three frame
quality scores to regulate the obtained sparse coefficients to
finally select key frames. The three scores are introduced below:

si(CC )  exp  max

(18)

i

Results from the proposed approaches and other competitive
methods are evaluated by comparing them with the ground-truth
key frames that were selected by three third-party human judges
[2]. We follow the same evaluation rules as proposed in [2].
Specifically, by comparing two sets of ordered key frames, two
fames that occurr within a short period of time and similar in
scene content and composition are considered as equivalent,
score=1; otherwise, score=0 (0.5 is used when there is
ambiguity). Accuracies are reported as the number of matched
key frames (i.e., total score for the entire video) over the number
of total ground-truth key frames.

(17)

where  is a vector of constant weights for balancing the three
components. Overall, we expect frames with high frame quality
scores to be selected as key frames. We set   [1/ 3, 1/ 3, 1/3]
in our experiments.
To select final key frames, we combined the obtained frame-level
sparse coefficients x and frame quality score s by following the
strategy below:

1507

is based on the traditional group sparse Lasso with MoreauYosida regularization and models the spatio-temporal correlation
present in the input video for key frame extraction by jointly
imposing the group sparsity at two levels: (i) patch-to-frame, and
(ii) frame-to-sequence. The grouped sparse coefficients are
further combined with high-level semantic-based frame quality
scores to select key frames. The proposed approach does not rely
on any non-visual data or domain knowledge and is more robust
than existing approaches for general consumer videos. Results on
videos from actual end users and comparisons to existing
approaches demonstrate the effectiveness of the proposed
approach.

3.2 Results and Analysis
Results from the proposed approach on the 17 videos (which
were used in [2]) are presented in Table 1. Compared to the
accuracies reported in [2] (we removed video#14-“wedding”
which is missing and recomputed the accuracies for fair
comparisons): Evenly spaced (ESKF) 48%, Motion-based
(MKFE) 55% and Histogram-based (UCF) 48%, our approach
outperforms ESKF and UCF by 7.7% and slightly better than the
performance of MKFE. Visual results from different approaches
on Video-“SoloSurfer” are presented in Figure 1.

5. REFERENCES
[1] Truong, B. T., and Venkatesh, S. 2007. Video abstraction: A
systematic review and classification. ACM Transactions on
Multimedia Computing, Communications and Applications,
3(1).
[2] Luo, J., Papin, C., and Costello, K. 2009. Towards
extracting semantically meaningful key frames from persona
video clips: From humans to computers. IEEE Trans. on
Circuits and Systems for Video Technology, 19(2): 289-301.
[3] Rasheed, Z., and Shah, M. 2005. Detection and
representation of scenes in videos. IEEE Transactions on
Multimedia, 7(6): 1097-1105.
[4] Tibshirani, R. 1996. Regression shrinkage and selection via
the Lasso. Journal of the Royal Statistical Society: Series B
58(1): 267–288.
[5] Mairal, J., Bach, F., Ponce, J., Sapiro, G., and Zisserman,
A. 2009. Non-local sparse models for image restoration.
IEEE International Conference on Computer Vision.
[6] Chen, S., Donoho, D., and Saunders, M. 2001. Atomic
decomposition by basis pursuit. Society for Industrial and
Applied Mathematics, 43(1): 129-159.
[7] Yuan, M., and Lin, Y. 2006. Model selection and estimation
in regression with grouped variables. Journal of the Royal
Statistical Society: Series B, 68(1): 49-67.
[8] Meier, L., Geer, S., and Buhlmann, P. 2008. The group
Lasso for logistic regression. Journal of the Royal
Statistical Society: Series B, 70: 53-71.
[9] Liu, J., and Ye, J. 2010. Moreau-Yosida Regularization for
Grouped Tree Structure Learning. Annual Conference on
Neural Information Processing Systems.
[10] Moorthy, A. K., and Bovik, A. C. 2010. A two-step
framework for constructing blind image quality indices.
IEEE Signal Processing Letters, 17(5): 513-516.
[11] Sundaram, H., Xie, L., Chang, S. 2002. A utility framework
for the automatic generation of audio-visual skims. ACM
International Conference on Multimedia.
[12] Loui, A., Luo, J., Chang, S., Ellis, D., Jiang, W., Kennedy,
L., Lee, K., and Yanagawa, A. 2007. Consumer video
benchmark data set: concept definition and annotation.
International Workshop on Multimedia Information
Retrieval.
[13] Lazebnik, S., Schmid, and C. Ponce, J. 2006. Beyond bags
of features: Spatial pyramid matching for recognizing
natural scene categories. IEEE Conference on Computer
Vision and Pattern Recognition.

Figure 1. “SoloSurfer” (Red solid borders indicate match
score=1, dashed borders indicate score=0.5): Comparison is
between ground truth (1st row), MKFE (2nd row), ESKF (3rd
row) and the proposed approach (4th row). As reported in [2],
the UCF method resulted in a total match score=2. Visual
results from the UCF method are not available.
Table 1. Results from the proposed approach.
Video Name

Score of matches over
number of total key frames

SchoolBand

3/6

Asian Exhibit

3/6

CardsAroundTable

5.5/7

ReporterTour

2/9

SoloSurfer

5/6

SkylineFromOverlook

4/6

FireworkAndBoat

1/4

EatingSardines

4/6

BusTour

1.5/5

LiquidChocolate

5/6

OrnateChurch

4/4

LawnchairDance

1.5/6

KnockKnockPrincess

2.5/7

HappyDog

4/4

MuseumExhibit

3/4

PoolAcrobat

2 /7

BoatUpsideDown

7.5/12

Total

58.5/105 = 55.7%

4. CONCLUSION
In this work, we propose a bi-layer group sparse representation
approach to extract key frames from consumer videos by relying
solely on visual content. The bi-layer group sparse representation

1508

2016 23rd International Conference on Pattern Recognition (ICPR)
Cancún Center, Cancún, México, December 4-8, 2016

A Computational Approach to Relative Aesthetics
Vijetha Gattupalli∗ , Parag S. Chandakkar∗ and Baoxin Li
School of Computing, Informatics and Decision Systems Engineering, Arizona State University
{jgattupa,pchandak,baoxin.li}@asu.edu

Abstract—Computational visual aesthetics has recently become
an active research area. Existing state-of-art methods formulate
this as a binary classification task where a given image is predicted to be beautiful or not. In many applications such as image
retrieval and enhancement, it is more important to rank images
based on their aesthetic quality instead of binary-categorizing
them. Furthermore, in such applications, it may be possible that
all images belong to the same category. Hence determining the
aesthetic ranking of the images is more appropriate. To this end,
we formulate a novel problem of ranking images with respect to
their aesthetic quality. We construct a new dataset of image pairs
with relative labels by carefully selecting images from the popular
AVA dataset. Unlike in aesthetics classification, there is no single
threshold which would determine the ranking order of the images
across our entire dataset. We propose a deep neural network
based approach that is trained on image pairs by incorporating
principles from relative learning. Results show that such relative
training procedure allows our network to rank the images with a
higher accuracy than a state-of-art network trained on the same
set of images using binary labels.

I. I NTRODUCTION
Automatic assessment of image aesthetics is an active area
of research due to its wide-spread applications. Most of
the existing state-of-art methods treat this as a classification
problem where an image is categorized as either beautiful
(having high aestheticism) or non-beautiful (having low aestheticism)1 . In [1], [2], this problem has been formulated
as a classification/regression problem by mapping an image
to a rating value. Various approaches such as [1], [2], [3],
[4], [5], [6], [7], [8], [9], [10] have been proposed which
either use photographic rules or hand-crafted features to assess
the aesthetics of an image. Due to the recent success of
deep convolutional networks, approaches such as [11], [12]
claim to have learned the feature representations necessary to
categorize the given image as either beautiful or non-beautiful.
The approaches based on photographic rules have certain
limitations. For example, the implementations of these rules
may be an approximation, thus affecting the accuracy of
aesthetic assessment. Also, the rules may not sufficiently
govern the process of how we decide the aesthetic quality of an
image. It is possible that some of the important rules have been
left out or some erroneous ones have been included. These
1 We

use this terminology throughout the paper.
indicates equal contribution by authors.
Acknowledgement: The work was supported in part by ONR grant N0001415-1-2344 and ARO grant W911NF1410371. Any opinions expressed in this
material are those of the authors and do not necessarily reflect the views of
ONR or ARO.
∗

978-1-5090-4847-2/16/$31.00 ©2016 IEEE

rules are mostly accompanied by generic image descriptors
or task-specific hand-crafted features. Such approaches suffer
from the disadvantages of generic/hand-crafted features that
they may not be suited for a special task such as aesthetic
assessment or the feature space does not fully represent
the key characteristics which make an image aesthetic. The
deep neural network based approaches may overcome these
disadvantages by learning the feature representations.
While deep learning approaches have advanced the state-ofart for this task, we observe that classifying a given image as
beautiful or non-beautiful may not always be the natural choice
for some applications. It may also be more intuitive for humans
to compare two images rather than giving an absolute rating to
an image based on its aesthetic quality. Moreover, all images
in a set could belong to the beautiful or non-beautiful category
according to a classification model. In such cases, it may often
be necessary to rank the images according to their aesthetic
quality. For example, a machine-learned enhancement system
[13] has to provide an enhanced version of the query image
to the user. To do so, it needs to compare two images with
respect to their aesthetics to determine which enhancement
results in a more beautiful image. In an image retrieval engine,
it would be desirable to have an option to retrieve images
having low/similar/high aesthetic quality as compared to the
query image.
Motivated by these observations, we introduce a novel
problem of picking a more beautiful image from a pair. We
term this problem as “Relative Aesthetics”. We build a new
dataset of image pairs for this task by carefully choosing
images from the popular AVA dataset [14] to satisfy certain
constraints. For example, we observed that comparing images
from unrelated categories (for example, a close-up of a car
and a wedding scene) does not make sense and hence such
pairs are avoided. There exists no single threshold which can
binary-classify the pairs correctly across the entire dataset. In
other words, if images were categorized into beautiful and nonbeautiful, then some of the pairs in our data could contain both
beautiful or both non-beautiful images. The details of dataset
creation and its statistical analysis are provided in Section IV.
Our problem draws certain parallels with “relative attributes” [15], where it was observed that training on
relatively-labeled data leads to models that capture more
general semantic relationships. They also mention that by
using attributes as a semantic bridge, their model can relate to
an unseen object category quite well. On the other hand, our
problem presents different challenges. In [15], they compare

2446

two images with respect to attributes (for example, more
natural, furrier, narrower etc.), which are better defined than
the aesthetics of two images. Thus even though it is trivial to
use models trained on categorical data to solve these ranking
tasks, we found that using relative learning principles allows
us outperform previous state-of-art classification models by
gaining a more general and a semantic-level understanding of
the proposed problem.
Our contributions are as follows:
1) We propose a novel problem termed as “relative aesthetics”, which involves picking a more beautiful image
from a given pair of images. We create a new dataset
which has such relative labels from the popular AVA
dataset by careful and constrained selection of image
pairs. We will make our dataset and model source code
publicly available upon the decision of the paper.
2) We build a deep network incorporating the relative learning paradigm and train it end-to-end. To the best of our
knowledge, there is no prior work on studying aesthetics
in a relative manner using deep neural networks.
3) We show that our model trained on relatively-labeled
data is able to outperform a recent state-of-art method
[11] trained on a similar sized, categorically labeled
dataset for the proposed task.
Section II discusses the relevant literature. Section III
describes our relative, deep neural network based approach.
Section IV and V describe the data-creation, experimental
setup, results and analysis. We conclude in Section VI.
II. R ELATED W ORK
Computational aesthetics research in the earlier years was
focused on employing photographic rules, hand-crafted features or generic image descriptors. Intuitive and common
properties such as color [1], [7], [8], texture [1], [2], content
[6], [5], combination of photographic rules, picture composition and hand-crafted features [5], [4], [6] have been used.
The most commonly used photographic rules include Rule of
Thirds used in [5], [4], [1]. Other compositional rules include
low depth of field, opposing colors etc. [5]. Common color
features such as lightness, color harmony and distribution,
colorfulness have been quantified for aesthetics assessment
by computational models [1], [7], [8]. Texture features based
on wavelets edge distribution, low depth of field, amount of
blur have also been used [2], [5]. Approaches specifically
trying to model content in the image by detecting people [6],
[5], [4], generic image descriptors such as SIFT [16] have
been proposed in [5]. Inspired by the then success of deep
neural network on various tasks such as image classification
[17], [18], object segmentation [19], facial point detection
[20], Decaf features [21] for style classification [22] etc.,
[11] proposed a deep-learning-based approach to aesthetics
assessment. This approach classifies a given image as beautiful
or non-beautiful depending on the entire image as well as its
local patches. Another such approach was presented in [12]
where the authors aggregate the information from multiple
patches in the multiple-instance-learning manner to improve

the result of aesthetics assessment. Most of these approaches
treat aesthetics assessment as a binary classification task,
which may not always be the best choice for many applications, as discussed before.
The concept of training on relatively-labeled data to improve model performance and provide it with certain semantic
understanding of the problem has been well-explored. The
work on relative attributes [15] predicts the relative strength
of individual property in images. It allows for comparison
with an unseen object category in the attribute space. Models
learned in such a way enable richer text descriptions of
images. Relative attribute feedback was used in conjunction
with semantic language queries to improve the image search
capability in [23]. There are many such applications where
relative learning has explored a new dimension of the problem
and improved the overall understanding of the model of a given
task.
In this work, we propose to employ the relative learning principles for the task of image aesthetics assessment.
This task is extremely subjective and have vaguely-defined
properties than other general attributes like size, being more
natural etc. To allow for learning using hand-crafted features, various datasets have been proposed such as Photo.net,
DpChallenge.com, AVA dataset. The first two datasets contain
20,278 and 16,509 images respectively2 , whereas the AVA
dataset [14] contains 250,000 images. Thus we use AVA to
form image pairs which in turn will facilitate the learning
of our approach. We propose a Siamese deep neural network
architecture [24] with a relative ranking loss, which takes
an image pair as input and ranks them with respect to their
aesthetic quality. The back-propagation happens with the loss
obtained from the ranking function, which, we believe, helps
the network explore the attributes of certain images that make
them more beautiful than others.
III. P ROPOSED A PPROACH
The comparison of the aesthetics of two images is dependent
on many factors and people’s visual preferences. Some of the
factors include color harmony [7], colorfulness [1], inclusion
of opposing colors [5], composition [25], visual balance [26]
etc. They are also affected by the content in the picture
[4], [6]. Though determination of aesthetics is a subjective
process, there are some well-established rules in the photography community such as low depth-of-field, rule of thirds,
golden ratio [27]. However, making hand-crafted features for
such rules is difficult and often will lead to approximation
or misrepresentation of those rules. Therefore, we take a
deep neural network based approach in which we incorporate
relative ranking by designing a suitable loss function. Most of
the rules or aesthetic criteria can be defined using either an
entire image or a part of it. Therefore, for each image in the
pair, our network is trained on two views of an image as also
done in [11]: the entire image and a local patch. This enables
the network to see different aspects of the input. For example,

2447

2 Datasets

hosted on ritendra.weebly.com/aesthetics-datasets.html

TABLE I
A RCHITECTURE OF A COLUMN IN OUR PROPOSED NETWORK . C ONVOLUTION IS REPRESENTED AS ( PADDING , # FILTERS , RECEPTIVE FIELD , STRIDE )
Padded Input

Conv

Max-pooling

Conv

Max-pooling

Conv

Conv

Dropout

Dense

Dropout

Dense

Dropout

3 × 230 × 230

2, 64, 11, 2

2×2

1, 64, 5, 1

2×2

1, 64, 3, 1

−, 64, 3, 1

0.5

1000

0.5

256

0.5

512-D

512-D

256-D
1st image

Channel 1 - Column 1: C11

_
Local patch
from the
1st image

C1

Channel 1 - Column 2: C12

512-D

256-D
256-D
2nd image

d

d=

Channel 2 - Column 1: C21

_
Local patch
from the
2nd image

−
w2T

·

(f (w1T

· (C1 − C2 )))

C2

Channel 2 - Column 2: C22

512-D

256-D

C1 − C2

f (w1T ·(C1 −C2 ))

Fig. 1. Architecture of the proposed network; Weights are shared between the columns C11 and C21 (shown in green), C12 and C22 (shown in red); The
features obtained from C11 and C12 are concatenated (represented by _ symbol) to get C1 and C21 and C22 are concatenated to get C2 ; The vector
C1 − C2 is passed through two dense layers to obtain a score d comparing the aesthetics of two images. f (·) denotes an ReLU non-linearity. Please refer
to the text for further details.

a view of the entire image may provide the network with the
knowledge of color composition while the local patch may
help with resolution, depth-of-field etc. We now describe our
network architecture and its training procedure in detail.
A. Network Architecture
Our deep convolutional neural network (DCNN) takes an
image pair as input. For each image in the pair, it takes as input
that image itself and its local patch. Since all images have to
be of the same size, they are warped to be 224 × 224 × 3.
A same-sized local patch is cropped from the original image.
We choose to warp the image based on the findings in [11],
which shows that local patches along with warped image gives
the best result. Our network has two “channels” as shown in
Fig. 1, corresponding to the input pair of images. A channel
is defined as the part of our CNN which takes an image along
with its local patch as input. Each channel has two “columns”.
One column takes the warped image and the other one takes
its local patch as input.
Our architecture is a Siamese network where each channel
shares weights in a certain way, which is shown in Fig. 1 by
means of color coding. The columns with the same color (i.e.
either red or green) share the same weights. This is because
the ranking produced by the network should be invariant to
the order of the images in the pair. Both channels have exactly
the same architecture until they are merged at the final dense
layer of 512 − D. We now describe the architecture of the
upper channel (channel 1). This channel has two columns
which takes the image and its local patch as input. Since
these two inputs are on a different spatial scale and trying to
convey different aesthetic properties as discussed earlier, we

do not set constraints on the weights of both the columns in a
channel. The upper column in channel 1 (C11 ) takes the entire
image as input which is of size 224 × 224 × 3, zero-padded
with 3 pixels on all sides. The column has five convolutional
layers. The first convolutional layer has 64 filters each of size
11 × 11 × 3 with stride 2. Second convolutional layer has
64 filters of size 5 × 5 with stride 1. Third and fourth layer
have 64 filters of size 3 × 3 with stride 1. These are followed
by two dense layers of size 1000 and 256 respectively. We
apply 50% Dropout at these two dense layers. Max-pooling is
applied after first two convolutional layers. Each max-pooling
operation halves the input in both the directions. We use ReLU
activation throughout. The architecture of C11 is also detailed
in Table I. The lower column of channel 1 (C12 ) and both
the columns of channel 2 (i.e. C21 and C22 ) have the same
architecture as C11 including dropout, max-pooling and zeropadding operations.
The key thing to note here is that the weights are shared
for (i) the two columns which take the entire image as input
i.e. C11 and C21 , and (ii) the remaining two columns which
take the local patches as input i.e. C12 and C22 . C11 and
C21 each generate a 256 − D representation (i.e. of the entire
image). Similarly, C12 and C22 also generate 256−D features
(i.e. of the local patch). We concatenate the two 256 − D
representations from (C11 , C12 ) as well as (C21 , C22 ) to form
two 512 − D representations. Fig. 1 shows this architecture
and the sharing of weights.
Now we explain our ranking loss function which takes the
above two 512 − D representations and gives a quantitative
measure comparing the aesthetics of the two images in a pair.

2448

B. Ranking Loss Layer
Our network aims at correctly ranking two input images
based on their underlying aesthetic quality. Formally, given
two input images I1 and I2 , we decide that I1 is more beautiful
than I2 (also denoted as I1 > I2 here onward) if a positive
value is obtained for d(I1 , I2 ) and vice versa. In other words,
d(I1 , I2 ) is a measure comparing aesthetics of two images.
d(I1 , I2 ) = wT · (g(I1 ) − g(I2 ))

(1)

Here, g(I1 ) and g(I2 ) are the CNN representations. In our
network, g(I1 ) and g(I2 ) are represented by C1 and C2 respectively, as shown in Fig. 1. To increase the representational
power, we pass (C1 − C2 ) through two dense layers separated
by a ReLU non-linearity. Thus for our network, Equation 1
takes a slightly modified form as follows:
d(I1 , I2 ) = w2T · (f (w1T · (C1 − C2 )),

(2)

where f (·) denotes an ReLU non-linearity.
Keeping this in mind, we can now design our final loss
function with the following properties:
1) It should propagate zero loss when all image pairs are
ranked “correctly” (i.e. the representations of the images
in these pairs are separated by a margin δ).
2) It should only be able to produce a non-negative loss.
Hence the loss function is designed as follows:
L = max(0, δ − y · d(I1 , I2 )),

(3)

where y is a ground-truth label which takes value 1 if the
first image in the pair is more beautiful than the second (i.e.
I1 > I2 ) and it equals -1 if I1 < I2 . The term max(0, ·)
is necessary to ensure that only non-negative loss gets backpropagated. The δ is a user-defined parameter which serves
two purposes. First, it defines a required separation to declare
I1 > I2 (or I1 < I2 ). That means if y · d(I1 , I2 ) > δ, then
no loss should be back-propagated for such pairs. Secondly,
and more importantly, δ > 0 avoids a trivial solution to our
optimization objective. To clarify further, if δ = 0, then for
y = 1 and y = −1, a common trivial solution exists which
makes either w1 = 0 or w2 = 0. We set δ = 3 as we
do not find any performance boost by further increasing the
separation between CNN feature representations of I1 and I2 .
In the further subsections, we explain the training and
testing procedures of our network. Then we compare the
aesthetic ranking results of our network against a state-of-art
network that is trained on a categorical data.
C. Training Our Architecture
This architecture is trained using mini-batch SGD with a
learning rate of 0.001, momentum = 0.9, weight decay of
10−6 and by employing Nesterov momentum. The learning
rate is reduced by 15% after every 10 epochs. The batch size
is set to 50. Apart from warping and cropping out the local
patch, we only subtract the mean RGB value computed on
the training set from each pixel of the image. During training,

when the network makes a wrong decision, it is forced to learn
by exploiting the difference between some other characteristics
of the image in the next iteration. Over a number of epochs,
it manages to discover the relevant image properties which
better define image aesthetics.
We have 23, 000 image pairs containing all unique images
(i.e. total 46, 000 images). We use subsets of 20, 000 and 3, 000
pairs for training and validation respectively. We stop our
training when the accuracy on the validation set does not show
significant improvement for 10 consecutive epochs. We train
using relative labels i.e. a pair is labeled as 1 if r1 − r2 > 1,
otherwise it is labeled as −1. Here, ri is the average rating of
Ii in AVA dataset. More details on the data creation are given
in Section IV.
D. Testing Our Architecture
Given a new pair of images, we first subtract the mean of
the training data from each pixel of both the images. We would
like to point out that our test set does not share any pairs or
any individual images with the training and validation set. We
first pass both the images and their patches into our network
and get the value of d(I1 , I2 ) from Equation 2. I1 is then
predicted as a more beautiful image than I2 if d(I1 , I2 ) > 0
and vice versa. Our test set contains 20, 000 image pairs. We
use the weights of the epoch where we achieve highest ranking
accuracy with the least amount of loss on the validation set.
E. Ranking using a Network Trained on Categorical Labels
We train a network on categorically-labeled data using our
own implementation of the RAPID approach [11], which is
a recent state-of-art method for aesthetics assessment. It is
trained on the same set of 40,000 images that is used to train
our network. However, in this case, these images have been
categorized as either beautiful or non-beautiful depending on
the average ratings obtained directly from the AVA dataset. We
set the threshold that determines the class of an image equal to
5.5, since the ratings in the AVA dataset range from 1-10. This
network consists of stacks of convolutional layers, followed by
dense layers and finally a sigmoid to convert the raw scores
into a probability measure, p(y = 1|I), i.e. probability of an
image I belonging to the beautiful class. We point the reader
to [11] for more details about the RAPID network architecture.
While testing for a pair of input images, we pass first image
through the network and get the probability measure - p(y =
1|I1 ). Passing the second image gives us p(y = 1|I2 ). We
decide that first image is more beautiful than the second one
if p(y = 1|I1 ) > p(y = 1|I2 ). This test set contains 20, 000
image pairs and is identical to the test set used for our approach
as mentioned in Section III-D. Despite RAPID network being
similar in size to our network, it gets a significantly lower
accuracy on this relative ranking problem, which suggests that
a network trained on categorically-labeled data fails to learn
the complex, relative ranking order in the data.
IV. DATASET
Our task is to determine the more beautiful image in a
pair. To the best of our knowledge, there exists no such

2449

dataset containing relatively-labeled pairs with respect to their
aesthetic rating. We created a dataset containing 43, 000 image
pairs. The individual images in these pairs belong to the
AVA dataset [14]. We use 20, 000 pairs for training, 3, 000
for validation and the rest for testing. We now describe the
protocol used to form the pairs out of the images from the AVA
dataset. The protocol can be defined by these three constraints:
1. The difference between the average ratings of images in
a pair should be ≥ 1. Constraining this difference ensures
that the training/test pairs are more likely to be aesthetically
different.
2. Each image in the AVA dataset has 210 ratings on an
average. We computed variance of all the ratings for each
image. We observed that the distribution of all these variances
over the entire the AVA dataset takes the form of a Gaussian
with a mean of 2.08 and a standard deviation of 0.6. The minimum and maximum variance in the image ratings are 0.8 and
4.5 respectively. As mentioned in [14], high variances among
the image ratings are a result of the collective disagreement
between the raters, which suggests that such images may have
certain abstract/novel content or photographic style, preferred
only by certain group of people. We avoid the images which
cause such significant disagreements among the raters by only
considering the images having rating-variance less than 2.6.
3. We avoid including pairs from different categories since the
characteristics which make an image aesthetic may vary with
the category. For example, a beautiful picture of a car may
have bright colors whereas a beautiful picture of a human face
may have low-depth of field and better details. Additionally,
since the ratings in the AVA dataset are crowd-sourced ratings,
the opinions may exhibit a preference towards some category.
We can mitigate the effect of these two factors by using
pictures from the same category to form pairs.
After such selection of pairs, we can form the relative labels.
We label a pair as 1 if the average rating of the first image
is greater than that of the second image and −1 otherwise.
The majority of the pairs in our dataset have the ratingdifference ≈ 1. To quantify, the rating-difference for about
85% of the training and test data is between 1 and 1.5. As
the rating difference between the images of a pair decreases,
choosing the more beautiful image in that pair gets difficult.
Also, to ensure that our network is not biased towards our
dataset, we replicate our experiments on another reference
test-set provided by the creators of the AVA dataset [14]. This
reference test-set contains 20, 000 images and has also been
used by [11]. By following the aforementioned protocol, these
20, 000 images yield us 7, 670 pairs. We call these set of pairs
as the standard test set. We now describe the experiments and
give analysis of results.
V. E XPERIMENTS AND R ESULTS
We run our network on our test set and the standard test
set containing 20, 000 and 7, 670 image pairs respectively. We
achieve a ranking accuracy of 70.51% and 76.77% on our
test-set and on the standard test-set respectively. Here, ranking

TABLE II
R ESULTS FOR RANKING AND BINARY CLASSIFICATION

RAPID [11]
Proposed

Ranking
on our
test-set

Ranking on
the pairs from
standard test-set

Classification
on our test-set

Classification
on standard
test-set

62.21
70.51

65.87
76.77

59.92
59.41

69.18
71.60

accuracy is defined as the fraction of pairs for which the model
correctly picks the more beautiful image according to the
ground-truth labels. We compare our approach with a state-ofart aesthetics classification network called RAPID [11], trained
as described in Section III-E: we pass both the images oneby-one to the RAPID network and choose the more beautiful
image. RAPID produces a ranking accuracy of 62.21% and
65.87% on ours and the standard test-set respectively. Since
each channel of our architecture is a replica of [11] with
the modified ranking loss, we compare our architecture only
with [11]. However, we believe that we will obtain similar
performance improvements if a different state-of-art model
(e.g. [12]) was used for each of our channels.
Due to our relative-learning-based approach, we believe that
our network has gained a semantic-level understanding of the
properties which make an image highly aesthetic. To verify
this, we attempted binary classification on our dataset as well
as the standard test-set. For this purpose, we extracted the
top channel of our network i.e. C11 and C12 (see Fig. 1).
We use the best weights learned from the ranking task for
this channel. After the last node, we just append a sigmoid
layer to convert the values into decision values. The input
image is passed through the network to obtain the probability
of that image being beautiful. We compute our results on a
subset of 10, 000 images taken from our test set and the entire
standard test set [14] containing 20, 000 images. On our test
set, proposed approach obtains 59.41% classification accuracy
as compared to 59.92% obtained by RAPID. On the standard
test set, we obtain an accuracy of 71.60% as compared to
69.18% obtained by RAPID. Note that we do not perform any
training to adopt our network for classification, which shows
that the learned features may be capturing the characteristics
that are responsible for making an image aesthetic. Our
network outperforms RAPID on the ranking task and produces
competitive performance on the classification task without any
additional training. We note that the performance of both the
networks is significantly lower on our test-set as compared to
that of on the standard test-set. This performance difference
could be attributed to the fact that all images in the standard
test-set are distributed only over 8 categories, whereas the
images in our test-set are distributed over all 65 categories.
The results of all the experiments are summarized in Table II
Fig. 2 illustrates some ranking results obtained by our network. The wrong predictions in the bottom row show that the
network lacks semantic knowledge about objects and natural
phenomena. For example, even though the picture containing

2450

Fig. 2. Rankings produced by our network are shown above. Top and bottom rows show correct and wrong predictions respectively for a total of 4 pairs.
Each of them are enclosed in either red/green boxes. For every pair, our network ranks the right image higher than the left image. Please view in color.

two birds has better color harmony/contrast, the lightning
phenomena is a rare capture, making it more picturesque.
VI. C ONCLUSION
We introduced a novel problem of relative aesthetics which
could have widespread applications in image search, enhancement, retrieval etc. We created a dataset with a careful and
constrained selection of 43, 000 pairs of images from the AVA
dataset where one image is always more beautiful than the
other. We showed that a deep neural network trained with an
appropriate loss function which accounts for such relativelylabeled data, significantly outperforms a state-of-art network
trained on same data with categorical labels. The proposed
network is also able to achieve a competitive performance on
an aesthetics classification problem with trivial modifications
to its architecture and no fine-tuning at all. This shows that
it has gained a certain semantic-level understanding of the
factors involved in making an image aesthetic.
R EFERENCES
[1] R. Datta, D. Joshi, J. Li, and J. Z. Wang, “Studying aesthetics in photographic images using a computational approach,” in ECCV. Springer,
2006, pp. 288–301.
[2] Y. Ke, X. Tang, and F. Jing, “The design of high-level features for photo
quality assessment,” in IEEE CVPR, vol. 1, 2006, pp. 419–426.
[3] S. Bhattacharya, R. Sukthankar, and M. Shah, “A framework for photoquality assessment and enhancement based on visual aesthetics,” in The
18th ACM international conference on Multimedia, 2010, pp. 271–280.
[4] Y. Luo and X. Tang, “Photo and video quality evaluation: Focusing on
the subject,” in ECCV. Springer, 2008, pp. 386–399.
[5] S. Dhar, V. Ordonez, and T. L. Berg, “High level describable attributes
for predicting aesthetics and interestingness,” in IEEE CVPR.
[6] W. Luo, X. Wang, and X. Tang, “Content-based photo quality assessment,” in IEEE ICCV, 2011, pp. 2206–2213.
[7] M. Nishiyama, T. Okabe, I. Sato, and Y. Sato, “Aesthetic quality
classification of photographs based on color harmony,” in CVPR.
[8] P. O’Donovan, A. Agarwala, and A. Hertzmann, “Color compatibility
from large datasets,” in ACM Transactions on Graphics (TOG), vol. 30.

[9] H.-H. Su, T.-W. Chen, C.-C. Kao, W. H. Hsu, and S.-Y. Chien, “Scenic
photo quality assessment with bag of aesthetics-preserving features,” in
The 19th ACM international conference on Multimedia.
[10] L. Marchesotti, F. Perronnin, D. Larlus, and G. Csurka, “Assessing the
aesthetic quality of photographs using generic image descriptors,” in
IEEE ICCV, 2011, pp. 1784–1791.
[11] X. Lu, Z. Lin, H. Jin, J. Yang, and J. Z. Wang, “Rapid: Rating pictorial
aesthetics using deep learning,” in The ACM International Conference
on Multimedia, 2014, pp. 457–466.
[12] X. Lu, Z. Lin, X. Shen, R. Mech, and J. Z. Wang, “Deep multi-patch
aggregation network for image style, aesthetics, and quality estimation,”
in IEEE ICCV, 2015, pp. 990–998.
[13] J. Yan, S. Lin, S. B. Kang, and X. Tang, “A learning-to-rank approach
for image color enhancement,” in IEEE CVPR, 2014, pp. 2987–2994.
[14] N. Murray, L. Marchesotti, and F. Perronnin, “AVA: A large-scale
database for aesthetic visual analysis,” in IEEE CVPR.
[15] D. Parikh and K. Grauman, “Relative attributes,” in IEEE ICCV.
[16] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
International journal of computer vision, vol. 60, no. 2.
[17] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
with deep convolutional neural networks,” in Advances in neural information processing systems, 2012, pp. 1097–1105.
[18] D. Ciresan, U. Meier, and J. Schmidhuber, “Multi-column deep neural
networks for image classification,” in IEEE CVPR, 2012, pp. 3642–3649.
[19] F. Chen, H. Yu, R. Hu, and X. Zeng, “Deep learning shape priors for
object segmentation,” in IEEE CVPR, June 2013.
[20] Y. Sun, X. Wang, and X. Tang, “Deep convolutional network cascade
for facial point detection,” in IEEE CVPR, June 2013.
[21] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and
T. Darrell, “Decaf: A deep convolutional activation feature for generic
visual recognition,” arXiv preprint arXiv:1310.1531, 2013.
[22] S. Karayev, M. Trentacoste, H. Han, A. Agarwala, T. Darrell, A. Hertzmann, and H. Winnemoeller, “Recognizing image style,” in Proceedings
of the British Machine Vision Conference., 2014.
[23] A. Kovashka, D. Parikh, and K. Grauman, “Whittlesearch: Image search
with relative attribute feedback,” in IEEE CVPR, 2012, pp. 2973–2980.
[24] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun, C. Moore,
E. Säckinger, and R. Shah, “Signature verification using a siamese time
delay neural network,” International Journal of Pattern Recognition and
Artificial Intelligence, vol. 7, no. 04, pp. 669–688, 1993.
[25] O. Litzel, On Photographic Composition. Amphoto, 1975.
[26] W. Niekamp, “An exploratory investigation into factors affecting visual
balance,” ECTJ, vol. 29, no. 1, pp. 37–48, 1981.
[27] D. Joshi, R. Datta, E. Fedorovskaya, Q.-T. Luong, J. Z. Wang, J. Li, and
J. Luo, “Aesthetics and emotions in images,” IEEE Signal Processing
Magazine, vol. 28, no. 5, pp. 94–115, 2011.

2451

2016 23rd International Conference on Pattern Recognition (ICPR)
Cancún Center, Cancún, México, December 4-8, 2016

MSR-CNN: Applying Motion Salient Region Based
Descriptors for Action Recognition
Zhigang Tu

Jun Cao

Yikang Li and Baoxin Li

School of Computing, Informatics,
Decision System Engineering
Arizona State University
Tempe, USA
Email: Zhigang.Tu@asu.edu

Intel Corp.
Tempe, USA
Email: jun.cao@intel.com

School of Computing, Informatics,
Decision System Engineering
Arizona State University
Tempe, USA
Email: YikangLi,Baoxin.Li@asu.edu

Abstract—In recent years the most popular video-based human
action recognition methods rely on extracting feature representations using Convolutional Neural Networks (CNN) and then
using these representations to classify actions. In this work, we
propose a fast and accurate video representation that is derived
from the motion-salient region (MSR), which represents features
most useful for action labeling. By improving a well-performed
foreground detection technique, the region of interest (ROI)
corresponding to actors in the foreground in both the appearance
and the motion field can be detected under various realistic
challenges. Furthermore, we propose a complementary motion
salient measure to select a secondary ROI – the major moving
part of the human. Accordingly, a MSR-based CNN descriptor
(MSR-CNN) is formulated to recognize human action, where
the descriptor incorporates appearance and motion features
along with tracks of MSR. The computation can be efficiently
implemented due to two characteristics: 1) only part of the RGB
image and the motion field need to be processed; 2) less data
is used as input for the CNN feature extraction. Comparative
evaluation on JHMDB and UCF Sports datasets shows that our
method outperforms the state-of-the-art in both efficiency and
accuracy.
Index Terms—Action recognition; Motion salient regions; Convolutional Neural Networks

I. I NTRODUCTION
The amount of video data available is experiencing explosive growth due to ubiquity of digital recording devices and
popularity of video sharing web sites. Human action recognition in video, which is one of the long-standing research
topics in computer vision, has been extensively investigated
in recent years [1], [4]. In general, action recognition can
be considered as a two-step procedure: feature extraction and
subsequent classification using these features.
Video-based action recognition is a challenging problem,
with many difficulties yet to be resolved. The challenges come
from three main aspects [4]: (1) Intra-class (variations within
action classes) and inter-class (ambiguities between action
classes) variations; (2) Environment and recording settings;
and (3) Temporal variations. In this work, we concern how
to accurately detect the person and his/her primary moving
body part under various complicated conditions, and subsequently localize them. This is a promising way to improve
video representation, which eventually determines the result of
978-1-5090-4847-2/16/$31.00 ©2016 IEEE

action recognition, since the performance of action recognition
heavily depends on video representation.
Noticeably, after the deep convolutional neural networks
(CNN) was applied in [8] to achieve remarkable success in
static image classification, extending CNN to extract features
for video representations has been widely studied for action
recognition [1], [2], [3], [12]. Human actions in video can
naturally be viewed as 3D spatio-temporal signals, which
are characterized by the temporal evolution of visual appearance governed by motion [10]. In consistence with this
characteristic, the approaches used to learn spatiotemporal
features to represent spatially and temporally coupled action
patterns are exploited. One representative work is [11], which
presents two CNNs: one spatial CNN, in which the appearance
representations are learned from RGB inputs, and one motion
CNN, in which the motion representations are learned from
pre-computed optical flow. These two representations are
complementary, and better performance was obtained when
combining them. We adopt the two-stream network, and
improve it by proposing a technique based on motion-salient
region (MSR).
In general, features used for human detection belong to
global representations, which encode the region of interest
(ROI) of a human as a whole. The ROI is normally extracted
by leveraging background subtraction or tracking [4]. The
global representations depend on the performance of localization, background subtraction or tracking. Furthermore, they are
sensitive to variations in viewpoint, background motion, noise
and illumination changes. Recently, [2] applied the selective
search scheme [7] to produce approximately 2K regions in
each frame, and discard the regions that are void of motion
according to a motion salient measure (MSM) based on optical
flow. However, this method has three drawbacks. First, there
is no good method to select the motion salient threshold α,
which directly affects the selected regions that are salient in
shape and motion, and hence affecting the final accuracy and
efficiency of the approach. Second, some subtle actions with
small motion could be missed. Third, the selected regions are
not necessarily spatially coherent.
On the other hand, Cheron et al. [1] obtained the representations derived from human pose. In particular, they used

3524

positions of the estimated body joints to define informative
regions. The regions corresponding to four body parts – right
hand, left hand, upper body and full body, and plus the full
image in both the RGB image and the flow field are cropped.
This method faces two main problems though. First, humanpose estimation is a difficult task. Pose-estimators should be
avoided for action recognition, at least until the performance
of the pose estimation being enhanced [5]. Second, using
five inputs for CNN feature extraction leads to extensive
computation that may not be completely necessary.
Inspired by the above analysis and the current advances
in the domain of moving object detection, we formulate an
action descriptor based on the identified motion-salient regions (MSRs). The Block-sparse Robust Principal Component
Analysis (B-RPCA) technique [13] is employed to detect
the human. The B-RPCA method addresses various realistic
challenges, e.g., background motions, illumination changes,
poor image quality under low light, noise and camouflage, in a
unified framework. Not only the foreground individuals can be
accurately extracted, but also the implementation is efficient.
In addition, since a motion saliency estimation step is applied
to compute the support of the foreground regions, spatial
coherence is imposed on these target regions. To improve the
performance of the B-RPCA technique, we add a velocity
angle measure to reduce errors on the consistency of the
motion direction. Normally, for the current widely-used action
recognition video datasets, the detected MSR in each frame is
the full human. According to the obtained motion information
of the whole human, we propose another MSM to extract
one primary part of the human body, where the movement
is most distinctive. The secondary MSR can convey highly
discriminative information, which is complementary to the
first detected MSR of the whole human. Replacing the four
body parts of [1] with our two MSRs, the proposed MSRbased CNN (MSR-CNN) outperforms the closely related stateof-the-art methods: the pose-based CNN (P-CNN), and the
regions of interest based spatial- and motion-CNN [2] on both
evaluation datasets.

Fig. 1. Input data processing: MSRs extraction and resizing. (a) operation on
the RGB input image; (b) operation on the motion field. (P1 (patch1) denotes
one MSR – the human body; P2 (patch2) denotes the secondary MSR –
motion salient body part)

Fig. 2. The framework of our MSR-CNN. From left to right: The resized
extracted three patches of each frame. The appearance descriptor of the spatialCNN and the flow descriptor of the motion-CNN ftr is respectively captured
per frame t and per region r. Static frame descriptors ftr are aggregated across
r .
all the frames according to min and max to get the video descriptor vsta
r
Temporal differences of ftr are aggregated to vdyn
in the same way. Video
descriptors are normalized and concatenated over patches r and aggregation
schemes into appearance features vapp and flow features vof . The final MSRCNN feature representation is the concatenation of vapp and vof . At last, a
linear SVM classifier is carried out for action classification.

II. MSR-CNN: CNN F EATURES E XTRACTION FROM
MSR S
From the state-of-the-art work [1], [2], [16] and our analysis,
it is clear that selecting informative regions for CNN features
extraction is a effective way to modify the accuracy of action
recognition in video. Fig. 1 and Fig. 2 outline the framework of
our MSR-CNN schematically. We detect two complementary
MSRs in terms of the improved B-RPCA technique and a
MSM separately. As shown in Fig. 1, P1 and P2 are two
extracted MSRs. P3 (see Fig. 2) is a 224 × 224 patch which
is obtained by resizing the input full image or the full flow
field. This method significant decreases the number of regions
need to be processed and allows for faster computation. The
two CNNs of [1] are introduced to operate on the MSRs of
the RGB image and the optical flow respectively, and correspondingly producing two representations – the appearancebased CNN representation and the motion-based representa-

tion. These two representations are captured at each frame and
then concatenated over time to form a video representation. At
last, the action classification is performed with a linear SVM
on the extracted video representation (see Fig. 2).
A. CNN Descriptors
To capture MSR-CNN features, we adopt the same architecture and training procedure as [1]. We apply the MatConvNet
toolbox [17] for the convolutional networks. Below, a brief
description of our training process is given.
1) Step 1: Processing the input data: To construct a
motion-CNN, the optical flow is first calculated for each
successive pair of frames according to the method of [18].
Optical flow [19], [20], which describes the pattern of apparent
motion of objects in a scene, is of critical importance for

3525

action recognition in video. The x-component (i.e. u), the ycomponent (i.e. v) and the magnitude of the flow are rescaled
to the range of [0, 255] like the input RGB images in the
following way: [b
u, vb] = γ[u, v] + 128, where γ = 16 is the
rescale factor. The values smaller than 0 and larger than 255
are discarded. Then, the three components of every flow are
stacked to form a 3D image as the input for the motionCNN. During training, for each selected MSR, we resize it to
224 × 224 to fit the CNN input layer. To construct a spatialCNN, for each selected MSR in the RGB image, we also resize
it to 224 × 224.
2) Step 2: Selecting/Training CNN model: Two different
CNNs with an identical architecture (similar to [8], with
5 convolutional and 3 fully-connected layers) are employed
to obtain the representations of the MSRs on appearance
and motion field respectively. The public available model
”VGG-f” [21], which is a pre-trained model on the ImageNet
challenge database [22], is chosen for spatial-CNN. The stateof-the-art motion network [2], which has been pre-trained on
the UCF101 dataset [23], is selected for motion-CNN.
3) Step 3: Aggregation: (1) Formulating a video descriptor
by aggregating all frame descriptor ftr (r represents the MSR, t
denotes the frame at time t). In particular, the frame descriptor
ftr contains n = 4096 values which is the output of the second
fully-connected layer.
(2) Formulating the min and max aggregation by calculating the minimum and maximum values for each descriptor
dimension i over T frames:
mi = min ftr (i)
1≤t≤T

Mi = max ftr (i)

(1)

1≤t≤T

(3) Formulating the static video descriptor vrsta by concatenating the time-aggregated frame descriptors:
vrsta = [m1 , . . . , mn , M1 , . . . , Mn ]T

(2)

(4) Formulating the dynamic video descriptor vrdyn by
concatenating the minimum 4mi and maximum 4mi aggregations of 4ftr
vrdyn = [4m1 , . . . , 4mn , 4M1 , . . . , 4Mn ]T

(3)

r
− ftr , 4t = 4 is the time interval.
where 4ftr = ft+4t
(5) Formulating a spatio-temporal MSR-CNN descriptor
by aggregating all the normalized video descriptors for both
appearance and motion of all MSRs and different aggregation
strategies.
4) Step 4: Classification: The actions are categorized by
using a linear SVM classifier trained on the spatio-temporal
representations produced by our MSR-CNN.

III. D ETECTION OF M OTION - SALIENT R EGIONS
Detecting moving objects is an extensively investigated
subject [24] and significant progresses have been achieved in
the past few years. Most existing techniques may still face
some challenges with real data from complicated conditions.
In this work, the B-RPCA technique [13] is employed for its

overall good performance. Furthermore, an improved B-RPCA
is presented to detect the foreground human in the input image.
Besides, a MSM is exploited to extract one MSR in the human
body detected from the previous step.
A. The B-RPCA Method
To deal with the difficulties in detecting foreground moving objects, Gao et al. [13] imposed few constraints to the
background. The background can be identified according to
a low-rank conditional matrix. Mathematically, the observed
video frames can be considered as a matrix M, which is a
sum of a low-rank matrix L that denotes the background, and
a sparse outlier matrix S that consists of the moving objects.
Besides, [13] introduced a feedback scheme, and proposed
a B-RPCA technique which consists a hierarchical two-pass
process to handle the decomposition problem. Three major
steps are carried out, which are summarized below to facilitate
the discussion of our improvement later (Sect. III-B)
1) Step 1: First-pass RPCA: In this step, a first-pass RPCA
in a sub-sampled resolution is applied to fast detect the likely
regions of foreground:
min k L k∗ +λ k S k1 ,
L,S

s.t. M = L + S

(4)

where k L k∗ denotes the nuclear norm of the background
matrix L, λ is a regularizing parameter which constraints
no foreground
regions will be missed. The appropriate value
p
λ = 1/ max(m, n). Equation (4) is a convex optimization
problem, and it can be solved by applying the inexact augmented Lagrange multiplier (ALM) [27]. Through this firstpass RPCA, all outliers can be identified and stored in the
outlier matrix S.
2) Step 2: Motion Saliency Estimation: A motion consistency strategy is used to assess the motion saliency of
the detected foreground regions and the probability of a
block containing the moving objects. Pixels within the blocks
captured in the first round of RPCA are tracked by optical flow.
After tracking, dense point trajectories are extracted. Firstly,
the short trajectories, like k − j <= 10 (j, k represent the
frame index, j, k ∈ [1, n] rely on the trajectory l), are removed.
Secondly, [26] is applied to estimate the motion saliency of
the remaining trajectories according to the consistency of the
motion direction. Two benefits are achieved due to the motion
saliency estimation: (1) the foreground objects moving in a
slow but consistent manner can be better identified; (2) the
small local motion comes from inconsistent motions of the
background can be further discard. Most of the non-stationary
background motions identified and stored in the outlier matrix
S in the first step, are filtered off or suppressed.
3) Step 3: Second-pass RPCA: In this step, the λ value
is reset according to the motion saliency, which ensures the
changes derived from the foreground motion can be completely transferred to the outlier matrix S and avoids to leave
any bad presence in the background. The second pass RPCA
is implemented as:
X
min k L k∗ +
λi k Pi (S) kF , s.t. M = L + S (5)

3526

L,S

i

where k · kF denotes the Frobenius norm of a matrix. Pi is
an operator which unstacks every column of S and returns a
matrix that represents block i. The inexact ALM algorithm is
emplyed again to solve the equation (5).
B. The Improved B-RPCA Method
The motion saliency estimation in [13] utilizes the trajectory
length and the motion direction of the point trajectories to
remove the non-stationary background motion. This strategy
is effective to detect the foreground moving objects that keep
moving constantly in the scene. If the object stops occasionally, the foreground object cannot be detected via the BRPCA technique due to the Step 2 operation: motion saliency
estimation. Especially for the action recognition datasets, such
as JHMDB, the actors may have little motion for some
intervals of the actions. To overcome such difficulties for the
B-RPCA approach, we propose the following improvements:
1) Relaxing the constraint of the trajectory length to k −
j <= 5. In this way, falsely-removed foreground due to
trajectory length (e.g., when the actor suddenly stops for short
moment) will be significantly reduced. Meanwhile, to avoid
noise arising from the background, we add the motion derivative constraint similar to MBH [15]. By calculating derivatives
of the optical flow components u and v, the background motion
due to locally-constant camera motion will be excluded.
2) Enhancing the consistency measure of the motion direction. Not only the negative direction or positive direction of
u and v along the trajectory, but also the direction variation
should be considered. Hence, we add a velocity angle measure
as follows:
4θ = arctan(ut+1 /vt+1 )−arctan(ut /vt ) ∈ [−π/4, π/4] (6)
where [u, v] 6= 0. Same as the motion direction consistency
operation, this velocity angle measure is also conducted at
positions where the velocity is no-zero along the trajectory.
3) If ut and vt satisfy either of the following conditions
(Equation 7 or Equation 8), we consider the actor is static
between frame t and frame t + 1. Then, we only perform the
first Step 1 to detect the actor in the RGB images.
range(ut ) < 0.5 ∧ range(mGf low) < 0.5

(7)

range(vt ) < 0.5 ∧ range(mGf low) < 0.5

(8)

or
where range(ut ) denotes the difference between the maximum value of ut and the minimum value of ut . mGflow is
the magnitudeqof the gradients of the optical flow (ut , vt ),
mGf low = (ut )2x + (ut )2y + (vt )2x + (vt )2x , 0.5 is an empirically selected threshold and denotes a half pixel distance.

Fig. 3. An outline of our method to select one MSR in the human body.
From Left to Right: the result of step 1 – extracting MSR candidates, the
result of step 2 – discarding the small MSR candidates, and the result of step
3 – selecting the most salient motion region (the red rectangle).

introduce a MSM to select the MSR of the human body, where
the motion is most distinguishable.
The region of the foreground actor body is detected and
localized via the improved B-RPCA. Accordingly, the location
information is useful for identifying other informative body
parts which are discriminative. We employ the following steps.
(See Fig. 3)
1) Extracting MSR candidates from the detected human
body according to a conditional measure defined as:
LabH ∧(mGf low > AmGf low)∧(mf low > Amf low) (9)
and
(|u| > Au) ∨ (|v| > Av)

(10)

where LabH is the already obtained human body from the last
step. AmGflow is the mean of mGf low. mf low is the
√ magnitude of the optical flow f low = (u, v), mf low = u2 + v 2 .
Amflow is the mean of mf low. Au and Av is the mean of the
horizontal flow u and the vertical flow v.
2) Discarding the small MSR candidates. Different body
parts have different motion patterns. In addition, some background motion around the human body may be inaccurately
identified by the B-RPCA technique. Due to this implementation, the incorrectly captured background motions could be
removed once again. The 3th subfigure in Fig. 3 displays that
most of the outliers are suppressed.
M SR(i) > τ

(11)

where i is the index of MSR candidates. τ is a threshold. If
the area of one candidate M SR(i) is smaller than τ , it will
be removed. In this paper we set τ = 10 × 10 experimentally.
3) Capturing the first two largest MSR candidates. We adopt
the simple MSM as [2] to select the final MSR by comparing
the normalized magnitude of the optical flow between these
two candidates:
f lowm (Ri ) =

C. Selecting the MSR of the human

1 X
f low(j)
|Ri |

(12)

j∈Ri

As suggested in [1], [2], [16], selecting suitable MSRs of the
actor body is essential, as these body parts are complementary
and are potentially helpful for improving action recognition
when combined in an appropriate manner. Based on the
captured human information of the improved B-RPCA, we

where f lowm (Ri ) is the normalized magnitude of the optical
flow in the i-th MSR candidate, j is the index of the optical
flow. The MSR candidate with larger f lowm (Ri ) will be
finally selected.

3527

TABLE III
R ESULTS (% ACCURACY ) OF THE SPATIAL - MOTION MSR BASED CNN ON
THE JHMDB DATASET.

TABLE I
R ESULTS (% MEAN AVERAGE PRECISION ( M AP)) OF THE
SPATIAL - MOTION MSR BASED CNN ON THE UCF S PORT DATASET.
Patches Div. Golf Kick. Lift. Rid. Run S.Board. Swing1 Swing2 Walk mAP
P1
P2
P3
P1+P3
P2+P3
All

100
100
100
100
100
100

100
100
100
100
100
100

100
52.50
52.50
100
100
100

100
100
100
100
100
100

100
100
100
100
100
100

63.89
0
63.89
0
63.89 63.89
29.17 52.50
29.17 25.0
63.89 100

87.67
100
63.89
63.89
100
87.67

100
100
100
100
100
100

100
100
100
100
100
100

P1

85.16
81.64
84.42
84.56
85.42
96.39

P2

P3 P1 + P3 P2 + P3 All
65.88 66.02

Accuracy(%) 59.79 58.92 60.78 63.76

TABLE IV
P ERFORMANCE OF OUR MSR-CNN ON THE JHMDB DATASET. W E
COMPARE THE MSR-CNN WITH THE STATE - OF - THE - ART RELATED
METHODS : ACTION T UBES [2] AND P OSE -CNN [1].
Methods
Accuracy(%)

IV. E XPERIMENTS
In this section, we evaluate our MSR-CNN method by
testing it on two challenging datasets – UCF Sports [14] and
JHMDB [6], and compare it with the state-of-the-art algorithms. In particular, in each dataset, we assess our method in
two aspects: 1) whether the improved B-RPCA is effective in
detecting the foreground human under complicated situations,
and the proposed MSM can extract the MSR of the body part;
2) whether the extracted secondary MSR is complementary to
the first one, and if it can further enhance the performance.
A. Evaluation on UCF Sports
Fig. 4 shows the two detected MSRs on 6 different action
categories. These actions are operated in various challenging
conditions, such as multiple actors and the displacements are
larger than the object scale (the 1th and 3th subfigures), the
moving area is textureless (the 2th subfigure), occlusion (the
4th subfigure), motion blur (the 5th subfigure) and illumination
changes (the 6th subfigure), our two detectors can successfully
deal with these difficulties.
Table I shows the results of our proposed MSR based
spatial-temporal CNN technique on using different patches.
Comparing the first row and the second row, we can find that
for some sequences, the extracted CNN features from these
two MSRs are different and complementary. Consequently,
extracting these two MSRs are necessary as they have different contributions for action recognition. Integrating the three
patches together, significant gain is achieved, where the mAP
is increased from 84.02% (P3) to 96.39% (All).
B. Evaluation on JHMDB
Fig. 5 shows the action detection and localization performance of our improved B-RPCA as well as the MSM.
It is clear that our detectors perform well in complex and
realistic situations. For example, in the 5th subfigure, even
encountering with the extremely motion blur, the human body
and one of his body part are accurately captured.
Table II demonstrates again that different patches play
different significant roles in action recognition, and incorporating them can further increase the performance to recognize
actions. The results of 71.1% from All outperforms other
approach more than 4% (comparing with the second best result
68.2% of P2+P3).
Table III shows the results of different patches based MSRCNN in the accuracy manner. The best result still comes from

P-CNN (Without GT) [1] Action Tubes [2] MSR-CNN
61.1

62.5

66.02

All (our proposed MSR-CNN), where the accuracy achieves
to 66.2%.
C. Comparison with the state-of-the-art
In Table IV, we compare our MSR-CNN approach with the
two state-of-the-art algorithms. The accuracy of our method
is about 5% better than Pose-CNN (66.02 vs 61.1), and about
3.5% more accurate than Action Tubes (66.02 vs 62.5). As
we have analyzed in Introduction, compared with Pose-CNN,
which relies on pose estimation, our detectors handle the
difficulties in realistic data without requiring accurate pose
estimation. Our method can extract the moving actor body
as well as one of its primary moving body part precisely.
Compared with Action Tubes, our method outperforms it
due to its three drawbacks that degrade its performance. In
particular, the empirically-selected motion salient threshold α
in Action Tubes is a fixed constant, which is not optimal for
all videos. Not only some fine-scale moving objects would
be removed, but also some large-scale moving objects in
challenging conditions would be incorrectly captured.
Since we focus on extracting complementary motion regions
of the human body and its body parts, we do not experiment
on integrating the hand-crafted IDT features [15] with our
deep-learned MSR-CNN features. The combination can be
easily conducted via fusion, and that can further boost the
performance (Refer to Pose-CNN [1] for more details).
V. C ONCLUSIONS
We propose a motion-salient region based convolutional
neural networks (MSR-CNN) for action recognition and localization. The idea is derived from the intrinsic characteristic that
only local motion features in the video contribute to the action
label. By employing the B-RPCA method and improving its
performance in three aspects, the foreground actor can be accurately detected under complex realistic situations. Additionally,
based on the motion information obtained from the improved
B-RPCA, a simple MSM can be used to efficiently extract a
complementary MSR of the human body, which corresponds
to the most discriminative motion part of the body. Therefore,
it should contribute more to the action recognition. Evaluation
on two challenging datasets and comparison with the related
state-of-the-art algorithms demonstrate our method achieves
superior performance on both the task of action recognition

3528

Fig. 4. Results on UCF Sports. Each column represents an action class. The big rectangle corresponds to the extracted foreground human body via the
Improved B-RPCA method, the small one corresponds to the extracted secondary MSR via the proposed MSM.
TABLE II
R ESULTS (% MEAN AVERAGE PRECISION ( M AP)) OF THE SPATIAL - MOTION MSR BASED CNN ON THE JHMDB DATASET. T HE RESPECTIVE
PERFORMANCE OF P1, P2 AND P3, AND THE COMBINATION PERFORMANCE BY INTEGRATING THEM IN DIFFERENT WAYS ARE SHOWN .
Patches brushhair catch clap climbstairs golf jump kickball pick pour pullup push run shootball shootbow shootgun sit stand wingbaseball throw walk wave mAP
P1
P2
P3
P1+P3
P2+P3
All

76.6
93.6
66.9
76.4
86.0
89.1

54.6
54.0
53.1
56.5
51.0
47.3

63.3
71.9
51.2
52.1
65.5
61.3

58.5
45.9
65.9
53.4
53.4
54.1

88.8
80.8
91.3
91.3
91.3
91.3

43.4
54.1
47.3
56.6
57.0
60.1

48.0
54.6
55.2
59.8
52.3
59.5

57.7
61.8
56.3
56.3
61.8
69.4

87.4
80.8
97.9
92.6
92.6
97.6

98.8
97.1
100
100
100
100

82.0
93.0
85.6
86.8
87.3
96.0

Fig. 5. Results on JHMDB. The big rectangle corresponds to the extracted
foreground human body via the Improved B-RPCA method, the small one
corresponds to the extracted secondary MSR via the proposed MSM.

and localization. In the future, we plan to design more robust
and efficient approaches to extract MSRs, and analyze the
type and number of MSRs that can bring the most significant
contribution to action recognition.
ACKNOWLEDGMENT
The work was supported in part by ONR grant N0001415-1-2344 and ARO grant W911NF1410371. Any opinions
expressed in this material are those of the authors and do not
necessarily reflect the views of ONR or ARO.
R EFERENCES
[1] G. Cheron, I. Laptev, and C. Schmid, ”P-CNN: Pose-based CNN features
for action recognition,” in CVPR, 2015.
[2] G. Gkioxari and J. Malik, ”Finding action tubes,” in CVPR, 2015.
[3] L. Wang, Y. Qiao, and X. Tang, ”Action Recognition with TrajectoryPooled Deep-Convolutional Descriptors,” in CVPR, 2015.
[4] R. Poppe, ”A survey on vision-based human action recognition,” Image
Vis. Comput., vol.28, no.6, pp.976–990, 2010.
[5] W. Chen and J. Corso, ”Action detection by implicit intentional motion
clustering,” in ICCV, 2015.
[6] H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. J. Black, ”Towards
understanding action recognition,” in ICCV, 2013.
[7] J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders, ”Selective
search for object recognition,” Int. J. Comput. Vis., 2013.

55.3
65.6
52.8
52.2
67.0
71.8

38.6
48.4
42.4
47.5
50.0
50.8

80.1
70.1
91.4
93.1
91.8
92.1

60.1
63.7
72.2
67.7
68.0
71.0

74.8
65.9
61.4
59.7
64.0
65.5

72.9
69.5
66.3
66.4
65.6
72.9

63.4
60.6
32.7
49.0
55.5
60.1

8.9
22.5
29.2
16.8
20.9
40.3

85.8
64.7
86.4
88.0
88.0
86.9

57.3
39.2
46.9
65.5
62.9
55.2

64.6
64.7
64.4
66.1
68.2
71.1

[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ”ImageNet classification
with deep convolutional neural networks,” in NIPS, 2012.
[9] J. Ng, M. Hausknecht, S. Vijayanarasimhan, O. Vinyals, R. Monga,
and G. Toderici, ”Beyond Short Snippets: Deep Networks for Video
Classification,” in CVPR, 2015.
[10] G. Varol, I. Laptev, and C. Schmid, ”Long-term Temporal Convolutions
for Action Recognition,” in CVPR, 2016.
[11] K. Simonyan and A. Zisserman, ”Two-stream convolutional networks
for action recognition in videos,” in NIPS, 2014.
[12] Y. Wang, S. Wang, J. Tang, N. Hare, Y. Chang, and B. Li, ”Hierarchical Attention Network for Action Recognition in Videos,” in CoRR
abs/1607.06416, 2016.
[13] Z. Gao, L. F. Cheong, and Y. X. Wang, ”Block-Sparse RPCA for Salient
Motion Detection,” Trans. Pattern Anal. Mach. Intell., vol.36, no.10,
pp.1975–1987, 2014.
[14] M. D. Rodriguez, J. Ahmed, and M. Shah, ”Action mach: a spatiotemporal maximum average correlation height filter for action recognition,” in CVPR, 2008.
[15] H. Wang and C. Schmid, ”Action recognition with improved trajectories,” in ICCV, 2013.
[16] P. Weinzaepfel, Z. Harchaoui, and C. Schmid, ”Learning to track for
spatio-temporal action localization,” in CVPR, 2015.
[17] A. Vedaldi and K. Lenc, ”MatConvNet: Convolutional Neural Networks
for MATLAB,” in ACM Int. Conf. Multimedia, 2015.
[18] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert, ”High accuracy
optical flow estimation based on a theory for warping,” in ECCV, 2004.
[19] Z. Tu, N. Aa, C. V. Gemeren, and R. C. Veltkamp, ”A combined
post-filtering method to improve accuracy of variational optical flow
estimation,” Pattern Recognit., vol.47, no.5, pp.1926–1940, 2014.
[20] Z. Tu, R. Poppe, and R. C. Veltkamp, ”Weighted local intensity fusion
method for variational optical flow estimation,” Pattern Recognit., vol.50,
pp.223–232, 2016.
[21] K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman, ”Return of
the devil in the details: Delving deep into convolutional nets,” in BMVC,
2014.
[22] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and F. Li, ”ImageNet: A
large-scale hierarchical image database,” in CVPR, 2009.
[23] K. Soomro, A. R. Zamir, and M. Shah, ”Ucf101: A dataset of 101 human
action classes from videos in the wild,” in CRCVTR-12-01, 2012.
[24] A. Bugeau and P. Perez, ”Detection and segmentation of moving objects
in complex scenes,” Comput. Vis. Image Understand., 2009.
[25] E. Candes, X. Li, Y. Ma, and J. Wright, ”Robust principal component
analysis?,” J. ACM, vol.58, no.3, pp.1–37, 2011.
[26] L. Wixson, ”Detecting salient motion by accumulating directionallyconsistent flow,” Trans. Pattern Anal. Mach. Intell., 2000.
[27] Z. Lin, M. Chen, and Y. Ma, ”The augmented Lagrange multiplier
method for exact recovery of corrupted low-rank matrix,” Mathematical
Programming, 2010.

3529

Robust Two-View External Calibration by Combining
Lines and Scale Invariant Point Features
Xiaolong Zhang, Jin Zhou, and Baoxin Li
Department of Computer Science and Engineering
Arizona State University, Tempe, USA
{xiaolong.zhang.1,jinzhou,baoxin.li}@asu.edu

Abstract. In this paper we present a new approach for automatic external calibration for two camera views under general motion based on both line and point
features. Detected lines are classified into two classes: either vertical or horizontal. We make use of these lines extensively to determine the camera pose.
First, the rotation is estimated directly from line features using a novel algorithm. Then normalized point features are used to compute the translation based
on epipolar constraint. Compared with point-feature-based approaches, the proposed method can handle well images with little texture. Also, our method bypasses sophisticated post-processing stage that is typically employed by other
line-feature-based approaches. Experiments show that, although our approach is
simple to implement, the performance is reliable in practice.

1 Introduction
Multi-view-based camera external calibration is a well-studied computer vision task,
which is of importance to applications such as vision-based localization in robotics.
We consider the following problem of external calibration: given two views under
general motion, to recover the relative camera pose and translation. The state of art
solution to determine the geometry for two views is to first estimate the essential
matrix and then compute the relative pose and translation from the essential matrix.
Usually, the essential matrix is computed from the fundamental matrix, which can be
estimated from 8 point correspondences [2]. In practice, RANSAC is used for
processing much more points to gain robustness. Still, such methods suffer from two
degenerated cases: 1) lack of texture in the images and thus lack of corresponded
features; and 2) the existence of coplanar feature points. When the number of correctly-corresponded features in general positions is too small, the computation of
fundamental matrix will have difficult. In the case that the number of good feature
correspondences is less than 8, the algorithm will not work. To address these problems, we propose a novel method for two-view external calibration by combining
lines and points features. Specifically, we utilize the fact that, in images captured in
most indoor or outdoor urban environments, there exist a lot of line-like structures
that are either vertical or horizontal in the original physical world. These include, for
example, the vertical contours of buildings, door frames, etc. Our method employs a
two-step approach by estimating the camera orientation and translation respectively,
G. Bebis et al. (Eds.): ISVC 2008, Part I, LNCS 5358, pp. 825–835, 2008.
© Springer-Verlag Berlin Heidelberg 2008

826

X. Zhang, J. Zhou, and B. Li

heavily using such line-like structures from the images. We first estimate the orientation using detected lines then the camera can be rotated such that only the translation
needs to be estimated. In theory, only two points are sufficient to determine the new
essential matrix which is determined only by the translation.
Earlier work on stereo matching based on line segments have been reported, verifying the robustness of line-based approaches in general and especially when texture
information is insufficient. However, such approaches often require sophisticated post
processing stages after line detection, either by iteratively reintroducing mismatches
[5] or by merging and growing operations [7]. These methods yield fairly good results
but rely on a large number of correct correspondences and thus require such extra
steps that boost the computational cost. Our approach focuses on very simple manipulation based on geometric principles to provide a closed-form solution rapidly. Compared with the approach reported in [8], in which and are first determined, our
approach does not require two vanishing points in orthogonal direction, so it works
with weaker assumptions and thus it may be able to handle more general situations.
The proposed approach has three main steps:
1. Line detection and matching.
2. Line-based estimation of the rotation matrix .
3. Point-based estimation of the translation vector .
In the next section we briefly describe our line matching method, and then in Sections 3 and 4 we discuss the estimation of and . In Section 5, we provide experimental results using both synthetic and real data. We conclude with brief discussion
on future work in the last section.

2 Line Detection and Matching
The fundamental problem we are trying to solve is the estimation of the relationship
between different views of the same scene, as shown in Fig.1, which illustrates an
outdoor environment from which a fair amount of line features can be observed. The
first component of our algorithm deals with reliable line detection and matching.

Fig. 1. Multiple views of the same scene taken by cameras with different pose containing rotation and translation

Robust Two-View External Calibration

827

For line detection, we use the line detector utility HoughLines2 in OpenCV [12],
which is based on Canny edge detector [11] and Hough transform [13]. We control the
thresholds in the Canny edge detector so as to guarantee at least 10 lines being generated. Note that, in theory our method requires only two pairs of corresponding vertical
lines and one pair of horizontal lines to carry out the estimation. In practice, experiments show that using about 10 lines is sufficient to guarantee reliable performance.

Fig. 2. Building the profile vector from adjacent parallel lines for each single line feature

Fig. 3. Above: edges and lines returned by the Canny detector and the line detector. Below:
vertical lines plotted in green. This office scene contains 7 vertical line matches containing 1
mismatch and 7 non vertical matches containing 2 mismatches.

We had three line matching criteria: color histogram with sidedness constraint, line
slope value, and centroid distance. First, our algorithm extracts a profile vector to
represent the color histogram on both sides of each line. For computing the color
histogram, we use pixels that lie on lines that are parallel to and are within a reasonable distance to the given line. The distance should not be too close (in which case the
pixels may fall onto the given line) or too far (in which case the pixels may be too far
from an actual edge). We experimented with the distance range of 3 to 10 pixels.
Bilinear interpolation is used in obtaining the lines of pixels. This is illustrated in

828

X. Zhang, J. Zhou, and B. Li

Fig. 2. This color histogram is computed for both sides of the given line and then
saved as a profile vector for that line.
Then we match each line on the other image for potential correspondences by measuring the difference between the histograms on both sides with the cost function
defined in [5]. The search region here is a window centered at the middle of the original line. After we found the potential candidates, we check the slope of each candidate and disqualify correspondences with tilt angle difference greater than 45 degrees.
Furthermore, we require mutual agreement (line in view 1 and line b in view 2
should both choose each other as the best match) to confirm a correspondence. During
this process, lines with the largest slope were labeled as vertical lines and others as
horizontal ones (as illustrated in Fig.3).

3 Estimating the Rotation
After we achieve reliable line matching we start the next step of estimating the rotation matrix. We assume the camera matrix K is known:
0
0
0

(1)
0

1

and ,
The rotation matrix can be decomposed as the multiplication of ,
each standing for the rotation along each axis in the 3D space respectively. We estiand
based on the vertical lines, rectify the image coordinate based on
mate
and , and then estimate
based on the horizontal lines.
estimated

Fig. 4. Illustration of
and
estimation: two vertical lines in an image defines an vertical
intersection P, rotating it twice we get

Robust Two-View External Calibration

3.1 Estimating

829

and

We now discuss our method for estimating
and . With the vertical lines in each
view, we calculate the vertical vanishing point from those lines. By rotating the image
coordinate along the z axis, we make this vanishing point fall on the y-z plane, and
then we further rotate the image along the x axis so that the vanishing point falls on
the y-axis. In practice, we rotate the original image along the z axis to guarantee the
vertical vanishing point falls onto the y axis in the imaging plane in the first step, and
then rotate the image along the x axis to transform the vertical lines to be parallel to
each other. This is illustrated in Fig. 4. It can be proven that the two rotation angles in
the previous steps are
and
respectively. Since in this step only one pair of vertical lines from each view is needed, we pick the pair from the set of corresponded
vertical lines using the following criterion:
We pick the pair whose intersection point is closest to the centroid of the intersections of all vertical line pairs.
3.2 Estimating
So far we have focused on the vertical lines. Next the horizontal lines will play the
and
known, we normalize line coordinates
main role in estimating . With
(2)
and
have been calculated in the previous
We assume that is known, and
steps. Now we normalize the image coordinate as shown in (2). In theory, if two cameras have the same internal parameters as well as orientation, any vanishing point will
by transform a
have same position in both views. From this property, we can find
vanishing point on one image to its corresponding position on the other image.
Vanishing points can be obtained by intersecting horizontal lines with the horizontal vanishing plane. In the standard view, the axis 0,1,0 is the vanishing line of
the horizontal plane, and any point on it has the coordinates , 0,1 . After rotating
the camera around the axis, one point , 1,0 is transferred to another point
, 1,0 on the axis (illustrated in Fig.5). So we have
0
1

0
1

(3)

where
cos
0
sin

0
1
0

sin
0
cos

(4)

The solution of
can be obtained from one pair of vanishing point ,
which is
determined by a pair of horizontal edge correspondence , normalized in (2),
0,1,0

and

0,1,0

(5)

830

X. Zhang, J. Zhou, and B. Li

Multiple correspondences yield an over-determined problem, for which we define
the following cost function of geometric distance to search for a solution:
(6)
stands for a pair of corresponding vanishing points and
is the esHere ,
based on the jth correspondence. This cost function value represents the
timation of
total cost associated with the jth pair of correspondence, and we choose the j that minimizes this value. Up to here we have accomplished our estimation of the rotation
matrix and we can normalize the original image coordinate:
(7)
The normalized image will be used to estimate the camera translation.

Fig. 5. Computing θ from corresponding intersection sets between two views. Blue dots stand
for intersection sets containing all point candidates, the red one is the optimal matching.

4 Estimating the Translation
Since we have already estimated the orientation of both cameras, after normalizing
both image coordinates, we have removed the rotation component and the relationship
I|0
between two views become pure translation. The new camera matrices are P
I| . The essential matrix is E
, which has only two degrees of freeand P
dom. According to the definition of the essential matrix, the following property holds
true for any corresponding point pair x and x’

Robust Two-View External Calibration

E

0

831

(8)

Based on this constrain, two point correspondences are enough to determine the
translation . Suppose
, ,
, then
0
(9)
E
0
0
Substituting a pair of corresponding points in homogeneous coordinates,
, ,1
, , 1 , into (8), we have
0

(10)

Given a set of correspondences with k matches, we obtain a set of linear equations
of the form
0

(11)

The solution is the right null-space of . For any point correspondences more than
two pairs, this became an over determined problem. We adopt the RANSAC [10]
framework to eliminate inaccurate point measurements and correspondence outliers
and to ensure robustness of our system. Our point correspondence method is based on
the SIFT[4] features which provides scale invariant point matching across views.
Matching outliers is not a big concern here because RANSAC optimization would
always select points that yield comparatively good results. Nevertheless, even if the
point correspondence fails completely, we could always fall back to the baseline system using intersections of corresponding edges we detected.

5 Experimental Results
We carried out two types of experiments assessing the effectiveness of the proposed
method. The first category based on synthetic data contains three experiments, covering pure rotation along each of the three axis respectively. The second category contains four experiments based on real data from both indoor and outdoor environments
containing both rotation and translation.
For synthetic data experiments, we constructed a virtual scene using ray tracer
software Mega-Pov[14], then we adjusted the camera angle and took a series of pictures. Here the camera location is fixed and we intend to separate the rotation on each
of the three axis, and thus three types of movement were recorded and three sequences were generated corresponding to rotation along the , and axis respectively.Then we pick a pair of images from each sequence as our test samples (as
shown in Fig. 6).
We quantitatively analyze the performance by comparing the ground truth with estimates generated by a point based system using SIFT feature detector implementation
[9] as well as the output of our system.
It can be observed in Table1 that few effective point correspondences were established due to the lack of texture in the scenes and the accuracy is lower than 50%. In
comparison line correspondences have a higher accuracy at 77%. In addition, since we

832

X. Zhang, J. Zhou, and B. Li

require only a few correct correspondences for the estimation, the number of correct
matches returned is sufficient for the calibration. This is further proven by the comparison of the actual estimation result: on the fourth row, estimation by point based methods failed whereas on the fifth row line based method provide fairly good estimation.

Fig. 6. Results for synthetic data. Column 1,2 and 3 correspond to experiment on the , and
axis respectively. In each column the top two rows are two input views and the last row is the
stitched image based on resulting estimation.
Table 1. Experimental results for synthetic data. Top two rows: correct/total correspondence
detected by each method. Bottom three rows: comparison of rotation angle estimation.

Point correspondences
Line correspondences

14/32
18/21

17/40
14/20

16/35
12/16

Ground truth
Point based estimate
Line based estimate

0.3683
0.1291
0.3665

0.1459
0.5005
0.1189

0.3579
0.7850
0.3142

For real data, we carried out four experiments covering the case of different rotations and translations in Fig. 7. These experiments covered both indoor and outdoor
cases. Due to the lack of ground truth information in these experiments, here we only
visualize the stitched images to demonstrate the performance of camera pose estimation: camera poses were determined and rectification was carried out accordingly, and
then the rectified images were stitched together; a good match of the two stitched
image implies that the two views have been calibrated accurately.

Robust Two-View External Calibration

833

Fig. 7. Four experiments based on real data. On each triplet, two smaller images in each group
stands for original inputs and the larger one is the stitched image.

In Fig.7, shown on the top row is the indoor examples, and the bottom is the outdoor ones. Smaller image pair containing epipolar lines is the original input and the
stitched result based on the estimation is shown as well. The disparity in stitched
images is caused by camera translation across the views. As long as the scene contains more than one vertical lines, which holds true in most man-made environments,
our algorithm would recover the camera pose with satisfying performance. Since our
method mainly based on lines especially the vertical lines, the correspondence in
poorly textured scenes like the second one is not a problem.

834

X. Zhang, J. Zhou, and B. Li

6 Conclusion and Discussion
We have proposed a novel approach for two-view camera calibration based on line
features. The experimental results demonstrate the advantage of our method over
point based methods in poorly textured region where good point correspondence is
hard to obtain. Basic geometric constrains enabled us to achieve satisfying result
bypassing any complicated extra process in comparison with other line feature based
methods, thus simplifying the process. By combining both line and point features, our
system can estimate rotation and translation matrices with robust performance. Preliminary results validated our method on both natural scenes and synthetic data.
We now discuss potential limitations of the proposed approach. The approach may
encounter difficulties when vertical lines are occluded or broken into segments that
are shorter than our threshold. Though our adaptive module guarantees 10 lines are
always detected, it cannot guarantee how many of them are actually vertical. If it
turned out to be no vertical lines among them, the algorithm would fail. There are two
problems yet to be solved: 1) broken vertical lines are important structural information but could not be used effectively; and 2) input image could have been taken with
very large rotation angle which would confuse the system in judging vertical lines
from horizontal ones. For the first problem, existing works adopt iterative process to
merge line segments as mentioned earlier, but the computational cost is high. On the
other hand, human eyes are “very sensitive to spatial continuity”, as summarized by
David Marr [3]. One potential strategy is to look for higher level structures in noisy
scene with perceptual oriented methods such as tensor voting. As for the second question, since it is hard to make any assumption on the basic camera pose for pictures in
general, we are working on developing preprocessing techniques based on natural
constrains such as shadows to determine if the coordinate system of an input picture
should be reversed before processing.

References
1. Kosecka, J., Yang, X.: Global Localization and Relative Pose Estimation Based on ScaleInvariant Feature. In: Proc. ICPR 2004 (2004)
2. Hartley, R., Zisserman, A.: Multiple View Geometry in Computer Vision, 2nd edn. Cambridge (2006)
3. Marr, D.: Vision: A Computational Investigation into the Human Representation and
Processing of Visual Information. Freeman, San Francisco (1982)
4. Lowe, D.G.: Object Recognition from local scale-invariant features. In: Proc. ICCV (1999)
5. Bay, H., Ferrari, V., Van Gool, L.: Wide-Baseline Stereo Matching with Line Segments.
In: Proc. CVPR 2005 (2005)
6. Rother, C., Carlsson, S.: Linear Multi View Reconstruction and Camera Recovery Using
Reference Plane. IJCV 49(2/3), 117–141 (2002)
7. Baillard, C., Schmid, C., Zisserman, A., Fitzgibhon, A.: Automatic Line Matching And 3D
Reconstruction of Buildings From Multiple Views. In: Proc. ISPRS Conference on Automatic Extraction of GIS Objects from Digital Imagery (September 1999)
8. Zhou, J., Li, B.: Exploiting vertical lines in vision-based navigation for mobile robot platforms. In: ICASSP 2007 (2007)

Robust Two-View External Calibration

835

9. Lowe, D.G.: Demo Software: U. of British Columbia, http://www.cs.ubc.ca/~
lowe/keypoints/
10. Fischler, M.A., Bolles, R.C.: Random sample consensus: A paradigm for model fitting
with applications to image analysis and automated cartography. CACM 24(6), 381–395
(1981)
11. Canny, J.: A computational approach to edge detection. IEEE PAMI 8(6) (November
1986)
12. Open Source Computer Vision Library, Copyright © 2000, Intel Corporation (2000)
13. Hough, P.V.C.: Method and means for recognizing complex patterns. U.S. Patent
3,069,654
14. MegaPov: unofficial POV-Ray extentions collection, http://megapov.inetart.
net/index.html

Rectification with Intersecting Optical Axes for Stereoscopic Visualization
Jin Zhou and Baoxin Li
Dept. of Computer Science & Engineering
Arizona State University, Tempe, AZ 85287, USA
{jin.zhou,baoxin.li}@asu.edu

Abstract
There exist various methods for stereoscopic
viewing of images, most requiring some special glasses
for controlling what goes to the left and the right eyes
of the viewer. Recent technology developments have
resulted in displays that enable 3D viewing without
glasses. However, these displays demand a true stereo
pair as the input, which greatly limits their practical
use, as true stereoscopic media are scarce. In our
recent work [6], we developed a systematic approach
to automatic rectification of two images of the same
scene captured by cameras at general positions, so
that the results can be viewed on a 3D display.
However, the approach cannot work well for large
camera displacement (i.e., very wide baseline). In this
paper, we propose a new rectification scheme to
address this wise baseline rectification problem, with
the basic idea of using a special stereo setup with
intersecting optical axes,. In a sense, the idea mimics
human vision when viewing objects close to the eyes.
Experiments with a 3D display demonstrate the
feasibility and effectiveness of the proposed approach.

1. Introduction
Recent display technologies have led to various lowcost 3D displays that enable stereoscopic viewing
without inconvenient 3D glasses (see [1,7] for
examples). While the underlying technologies may
vary from one manufacture to another, the basic
principle of many 3D displays can be illustrated by
Fig. 1, where a parallax barrier is used in LCD to direct
the light rays from the pixels to the right and left eyes,
respectively. Such technologies have made 3D viewing
very convenient. There are many potential applications
of this type of displays, including scientific
visualization, gaming, and personal entertainment.
From Fig. 1, it is obvious that, to enable the
stereoscopic viewing function on a 3D display, one
must have the “correct” left and right eye image pair. If
the image pair is captured by a standard stereo rig
giving proper disparities, then this problem is solved.
However, true stereo media is scarce, and general
consumers rarely use stereo cameras. These

The 18th International Conference on Pattern Recognition (ICPR'06)
0-7695-2521-0/06 $20.00 © 2006

unfortunate facts limit the otherwise great potential of
the 3D displays.

Fig. 1. An illustration of how a parallax-barrierbased 3D display works. Both eyes need to fall into
the correct “viewing diamond” to sense 3D.

While stereo rectification is a well-studied topic,
most existing efforts (e.g., [2,3,4]) are mainly focused
on rectifying images as preprocessing for stereo
matching or 3D reconstruction, rather than for direct
3D visualization (which imposes additional
requirements on the rectification). In our recent work
[6], we proposed an approach that can rectify two
images of the same scene from general viewpoints so
that they look like a true stereo pair from a standard
stereo rig matching the requirements of a 3D display.
Unfortunately, if the displacement between the two
cameras/viewpoints is large, the resultant stereo rig
may have too wide a baseline to support 3D viewing
on a display. Also, a wide baseline may render a large
part (or even all) of the rectified images fall outside of
the new image planes of the rectified cameras. Both
problems were handled to certain degree in [6] by
translating the image plane, which works
approximately only if the baseline is not too wide.
In this paper, in addressing the difficulty due to a
wide baseline, we propose a new approach, i.e., stereo
through two views with intersecting optical axes. The
proposed method is built on top of our previous work.
That is, for two given uncalibrated images of the same
scene, we first rectify them into a standard stereo pair
(which is the subject of [6]), and then we make use of
the method proposed in this paper to solve the wide
baseline problem by furthering “rectifying” the pair
into one that looks like images captured by a special
stereo rig with intersecting optical axes. Accordingly,
in this paper, we only address the problem of

intersecting-axis rectification from a standard stereo
pair and analyze the effect of such an approach.

2. Problem Statement
Consider the standard stereo setup as shown in Fig.
2(a). For a 3D point with depth z, the disparity d of this
point in the image domain is given as
d = fk / z
(1)
where f represents the focal length and k represents the
baseline. From this equation, d is directly proportional
to k, which means that when the baseline is large, the
disparity is also large. A large disparity may exceed the
capacity of human eyes in making proper
correspondence. In the 3D display application, a
viewer may see two dizzy images instead of a fused 3D
image.
Optical Axis

Optical Axis

0 −tg (θ )º
ª 1
«

R2 = R (−θ ) = « 0
1
0 »»
«¬tg (θ ) 0
1 »¼

θ
z0
C1

k
Baseline

(a)

C2

C1

k
Baseline

C2

(b)

Fig. 2: (a) A standard stereo setup. (b) Stereo with
intersecting optical axes.

From Eqn. (1), for a given f, what really matters is
the ratio k/z. That is, a large disparity may be due to a
wide baseline or a short depth (or both), as long as the
ratio is the same. So we can simply consider the short
depth situation. Humans can still perceive 3D objects
at a short depth, and Fig. 2(b) illustrates how human
eyes act when viewing a nearby object. That is, the
optical axes (the line of sight) intersect at the object.
This inspires us to design a rectification scheme to
address the wide baseline difficulty based on the same
principle. Our method in this paper is based on this
simple idea of using intersecting optical axes for
addressing the wide baseline stereo rectification
problem. The core task is then how to rectify a pair of
images from a setup of Fig.2 (a) into a setup of Fig.
2(b). (As mentioned above, we assume that two
general images have been rectified into a standard
stereo pair of Fig. 2(a) by the method of [6].)

3. Rectification with Intersecting Optical Axes
Similar to the rectification of two images into a
stereo pair as in [6], we can just “rotate” the cameras to
obtain the new image pair from a setup with
intersecting optical axes. Pure camera rotation results
in a new image that is linked to the original one by a
homography transformation. Therefore, the problem
boils down to the computation of the transformation.
We first formalize the camera setups. A camera’s

The 18th International Conference on Pattern Recognition (ICPR'06)
0-7695-2521-0/06 $20.00 © 2006

calibration matrix can be written as:
P = KR[ I | −C ]
where K represents the internal camera matrix, R the
camera rotation matrix, and C the camera center. For
the standard stereo setup, the two camera matrices are:
P1 = KR[ I | −C1 ]
P2 = KR[ I | −C2 ]
Without losing generality, we can set:
R = I C1 = ( x0 , 0, 0)T C2 = (− x0 , 0, 0)T
For the new camera setup of Fig. 2(b), suppose that
the intersection point is (0, 0, z0 )T , then tg (θ ) = x0 / z0 .
Now we can compute the rotation matrix for the new
cameras as
0 tg (θ ) º
ª cos(θ ) 0 sin(θ ) º ª 1
«
»
«

R1 = R(θ ) = « 0
1
0 »≅« 0
1
0 »»
«¬ − sin(θ ) 0 cos(θ ) »¼ «¬−tg (θ ) 0
1 »¼

With the new rotation matrix, we can easily get the
required homography as
  )( KR )−1
H = ( KR

where K and R represent the new internal matrix and
the rotation matrix, respectively. For pure rotation,
K = K , therefore
(2)
H i = KRi K −1
In uncalibrated case, K is unknown. However, as
shown in [6], it is possible to use a simple approximate
for K to obtain reasonable results, such as using the
following
ª f 0 px º
­ px = w / 2
w: image width
°
«
»
K = « 0 − f py » with ® p y = h / 2 ,
h: imageheight
°
«¬ 0 0
1 »¼
¯ f = ( w + h) / 2
With this approximation, the only remaining variable is
θ that determines the amount of rotation.

3.1 Determine the rotation angle
From Fig. 2 (b), given the intersection point of two
optical axes, the rotation angle satisfies tg (θ ) = x0 / z0 .
This can be further linked to the image domain
disparity in the configuration of Fig. 2(a) since we
have
d 0 = fk / z0 = 2 fx0 / z0

tg (θ ) = x0 / z0 = d 0 / 2 f

where d0 represents the disparity of the intersection
point in the original stereo pair. Note that, however, for
two given images, x0 and z0 , and thus d0, all vary
depending on which part of the images is the focus of
the sight (the intersection point of the optical axes).
Our strategy is to handle this as follows. In the images

from the setup with intersecting optical axes, the
intersection point lies at the center for both images, and
thus the new disparity for this point is zero. On the
other hand, when viewing with the 3D display,
typically the ideal case is that the background (or the
scene at the infinity) has zero disparity. Therefore,
from the given two images, we can set the
background’s disparity as d 0 for computing ș.
Practically speaking, without explicitly determining the
background, a simple strategy is to select the minimal
disparity in the two images as d0 . With d0
determined, we can compute tg (θ ) and further the
transformation matrix H i by Eqn. (2).
3.2 Analysis of the effect of intersecting optical axes
Realistic 3D perception is due to proper disparities
of the objects in two images. In this section, we
analyze numerically how rectification with intersecting
optical axes affects the disparities. Specifically, we
show that, at and around the center of the rectified
images, the proposed method results in the same
difference of disparities for any two given points as the
stand stereo setup does. This implies that the proposed
method is able to keep the relative depth unchanged, at
least for the center region of the image.
After the proposed rectification, for a 3D point
X = ( x, y, z )T , its image point in the first camera is
0 tg (θ ) º ª x − x0 º
ª 1
1
0 »» «« y »»
x1 = P1 X = KR1 [ I | −C1 ] X = K «« 0
1 ¼» «¬ z ¼»
¬« −tg (θ ) 0
−
+
(
θ
)
x
x
ztg
ª
º
0
« f −( x − x )tg (θ ) + z »
0
»
ª x − x0 + ztg (θ ) º «
»
y
»≅ «f
= K ««
y
» « −( x − x )tg (θ ) + z »
0
«
»
¬« −( x − x0 )tg (θ ) + z ¼» «
»
1
«
»
¬
¼
Let (u1 , v1 )T be an image point in the first camera and

in the second camera. We have
x − x0 + ztg (θ )
y
u1 = f
v1 = f
−( x − x0 )tg (θ ) + z
−( x − x0 )tg (θ ) + z
x + x0 − ztg (θ )
y
u2 = f
v2 = f
( x + x0 )tg (θ ) + z
( x + x0 )tg (θ ) + z

(u2 , v2 )

T

v2 − v1 = fy (
= fy

1
1
−
)
( x + x0 )tg (θ ) + z −( x − x0 )tg (θ ) + z

−2 xtg (θ )
( z + x0 tg (θ )) 2 − ( xtg (θ ))2

Suppose z = z0 − Δz , then we have
x − x0 + ( z0 − Δz )tg (θ )
x − Δztg (θ )
u1 = f
= f
−( x − x0 )tg (θ ) + z
−( x − x0 )tg (θ ) + z

The 18th International Conference on Pattern Recognition (ICPR'06)
0-7695-2521-0/06 $20.00 © 2006

u2 = f

x + Δztg (θ )
( x + x0 )tg (θ ) + z

u2 − u1 = f

−2 x 2 tg (θ ) + 2 Δztg (θ )( z + x0 tg (θ ))
( z + x0 tg (θ )) 2 − ( xtg (θ )) 2

For the intersection point, x = 0, Δz = 0 , so v2 − v1 = 0
and u 2 − u1 = 0 , which means the disparity of the
intersection point is 0. For any other point, if
tg (θ ) ≈ 0 and x ≈ 0 , we have
f
(3)
2Δztg (θ )
z
This also gives the disparity difference between a point
with depth z and the intersection point (for which the
disparity is 0, as shown above).
On the other hand, for a standard stereo setup
(Fig.2(a)), there is only horizontal disparities, given by
d = u2 − u1 = fk / z = 2 fx / z
and the disparity for the intersection point is:
d0 = 2 fx0 / z0
(4)
v2 − v1 ≈ 0

u2 − u1 ≈

For a point with z = z0 − Δz
d ' = 2 fx0 /( z0 − Δz )
The disparity difference between the intersection point
and this point is given by
x
f
1 1
d '− d 0 = 2 fx0 ( − ) = 2 f Δz 0 = 2 Δztg (θ )
z z0
zz0
z

(5)

which is exactly the same as in (3). This shows that,
when x is small, the disparity difference for any two
points is the same in both of the setups, and hence the
relative depth is the same for both setups.
The above analysis also suggests that, when θ is
small, the proposed method is equivalent to translation
of the images. When θ is large, the intersection point
is close to the camera and thus rotating the cameras can
no longer be approximated by translation.

4. Experiments
We designed two types of experiments to validate
the proposed algorithms. In our experiments, the
rectified pairs were further validated by visualizing on
a SHARP 3D display ([7]).
Experiment I. In the first experiment, we use the
publicly available data from [5], which contains
accurate camera calibration information and thus
allows a more accurate rectification for verifying the
basic idea of this paper. The data set contains videos
captured from 8 different cameras. In the experiments,
we select a pair of images from any two cameras, and
then plot some corresponding epipolar lines in the
images (Fig. 3 (a1) and (a2)). We first rectify the
original image to a standard stereo pair, resulting in the
images as in Fig. 3 (b1) and (b2), where the epipolar
lines become horizontal and aligned. However, it is

obvious that the disparities between the images are too
large, even for the background. Then we apply the
proposed method to make the optical axes intersect,
and obtain the results as in Fig. 3 (c1) and (c2). It can
be seen that now the disparity of the background is
roughly 0 while the front person still has some
disparity. In this example, the actual θ is very small
(~ 1.5° ), and thus the “rotation” transformation appears
to be just like translation, as discussed above. When
viewed on the 3D display, (c1) and (c2) produce good
depth perception while (b1) and (b2) fail to do so.
Experiment II. In the second experiment, we use
images captured by a hand-held camera from different
positions to test our algorithms (and thus we do not
have accurate camera calibration data). The results are
illustrated in Fig. 4. After rectification into a standard
stereo pair, we got (b1) and (b2). Obviously, the
disparity for the white cup is very large. Moreover, the
cup in the left image is distorted (the right hand side is
stretched. Since the object is close to the camera, after
rotating the camera, the re-projected image may show
obvious perspective effect). After rectified with
intersecting optical axes, we obtained (c1) and (c2),
where the cup in both images “moves” towards the
center, and the distortion of the cup is removed. In this
case, θ is about 5° , so the epipolar lines are no longer
parallel. Note that this pair does not look like a
standard stereo pair, especially for the background
region. When viewing on the 3D display, the cup and
its surroundings produce good 3D perception. See also
the next section for further discussion.

the cursor as the center. This will allow 3D viewing of
a large image part by part.
Acknowledgement: Some of the experiments were based
on data made public on the Internet by the Interactive Visual
Media Group at Microsoft Research [5].

6. References
[1] P. May, “A Survey of 3-D Display Technologies”,
Information Display, Vol. 32, 28-33, 2005.
[2] A. Fusiello, E. Trucco, and A. Verri, “A compact
algorithm for rectification of stereo pairs”, In Machine
Vision and Applications, Vol. 12, pp. 16-22, July 2000.
[3] R. Hartley. Theory and practice of projective
rectification. IJCV, 35(2):1-16, Nov. 1999.
[4] M. Pollefeys, R. Koch and L. Van Gool, A simple and
efficient rectification method for general motion, In
ICCV, pp. 496-501, Sep. 1999.
[5] C. L. Zitnick, S. B. Kang, M. Uyttendaele, S. A. J.
Winder, and R. Szeliski, “High-quality Video View
Interpolation Using a Layered Representation”,
SIGGRAPH, Vol. 23, pp. 600-608, Aug, 2004.
[6] J. Zhou and B. Li, “Image Rectification fro Stereoscopic
Visualization without 3D Glasses”, Proc. Int. Conf. on
Image & Video Retrieval,2006.(See also a Technical
Report from cubic.asu.edu)
[7] http://www.sharp3d.com/

(a1)

(b1)

(c1)

(a2)

(b2)

(c2)

5. Conclusion and Discussion
In this paper we proposed a novel method to
address the problem of stereo rectification for 3D
visualization under wide baseline. In a sense, the
proposed method handle wide baseline by mimicking
the behavior of human eyes when viewing an object at
a close range. We developed the algorithm and
validated its performance using a 3D LCD display.
It is apparent from the discussion in Section 3 that,
when the baseline is too large, we can only rectify the
center part of the images, and thus when viewed on 3D
displays (or through any other means) only that part of
the image can produce 3D perception. This is also in
fact natural in human vision, which can only fixate on
a particular scene object in each instant. Of course,
human eyes can quickly move to other parts of the
scene. For the specific 3D display application, the
proposed method can be used in the following fashion
to mimic this: assuming that a mouse cursor is used to
indicate the viewer’s focus of attention, we can then
perform rectification using the position specified by

The 18th International Conference on Pattern Recognition (ICPR'06)
0-7695-2521-0/06 $20.00 © 2006

Fig 3. Rectification with MS data. (a1) and (a2) are the
original pair; (b1) and (b2) are rectification results of
stereo pair; (c1) and (c2) are results of intersecting
optical axes.

(a1)

(b1)

(c1)

(a2)

(b2)

(c2)

Fig. 4. Experiments with data from a hand-held
camera.

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

Weakly Hierarchical Lasso based Learning to Rank
in Best Answer Prediction
Qiongjie Tian

Baoxin Li

Computer Science and Engineering
Arizona State University
Email: qiongjie.tian@asu.edu

Computer Science and Engineering
Arizona State University
Email: baoxin.li@asu.edu

Abstract—In community question and answering sites, pairs
of questions and their high-quality answers (like best answers
selected by askers) can be valuable knowledge available to others.
However lots of questions receive multiple answers but askers
do not label either one as the accepted or best one even when
some replies answer their questions. To solve this problem, highquality answer prediction or best answer prediction has been
one of important topics in social media. These user-generated
answers often consist of multiple “views”, each capturing different (albeit related) information (e.g., expertise of the asker,
length of the answer, etc.). Such views interact with each other
in complex manners that should carry a lot of information for
distinguishing a potential best answer from others. Little existing
work has exploited such interactions for better prediction. To
explicitly model these information, we propose a new learningto-rank method, ranking support vector machine (RankSVM)
with weakly hierarchical lasso in this paper. The evaluation
of the approach was done using data from Stack Overflow.
Experimental results demonstrate that the proposed approach
has superior performance compared with approaches in state-ofthe-art.

I. I NTRODUCTION
In the era of Internet and social media, community question and answering (CQA) sites, like Baidu Zhidao1 , Yahoo!
Answers2 and StackOverflow3 , are seeing phenomenal growth.
As one form of user-generate content, data from CQA sites are
typically very noisy, which does not lead to ready usage either
by humans or by computers. Consequently, how to extract
useful information from the noisy CQA data to form valuable
knowledge base has become an important research task [1].
One popular task on this regard is best answer prediction, on
which our paper focuses.
Given a question with multiple answers, one way to solve
best answer prediction is to reformulate it into a binary
classification problem which is whether, in a question-answer
pair, the answer is the best one or not. There have been some
research efforts in this setting like [2], [3]. In these efforts,
features were extracted from different views of the data to
generate a good representation for the question-answer pairs,
and the final feature vector was formed by concatenating
them together. As a result, each feature dimension carries
some information of the CQA data. But there are a couple of
1 http://zhidao.baidu.com/
2 https://answers.yahoo.com/
3 http://stackoverflow.com/

limitations inherent to these existing techniques. First, a binary
classifier is not natural to this research problem, which often
involves multiple answers for one given question. It is possible
for a trained classifier to declare many or even all answers
are the best ones (if they happen to lead to feature vectors
lying on the positive side of the decision boundary). Also it is
counter-intuitive as a human user would normally compare all
received answers and decide on a single best one. The binary
classification does not model directly on the difference of
multiple answers, compared with learning-to-rank techniques.
Second, the interaction between features from different views
may carry a lot of information for distinguishing a potential
best answer from others, however current existing methods do
not readily support incorporation of such interactions, which
by itself is a challenging task.
In anther setting, best answer prediction is modeled as one
ranking problem, which is conceptually more intuitive. This
kind of modeling results from the fact that the best answer
to one question is defined/discovered relatively by comparing
it with all the other given answers. A ranking-based setting
may benefit even more from considering the latent interactions
between features designed from different views of the CQA
data. Unfortunately, similar to the binary-classification cases,
the existing learning-to-rank techniques have not attempted to
explicitly to model such interactions among different views of
the data [4][5][6].
In this paper, we focus on how to incorporate the interaction
structure of features into one existing algorithm framework to
improve the performance of best answer prediction. Similar to
[5][7], we adopt the learning-to-rank formulation for its natural
match to the prediction problem. Considering the interaction
structure (or the hierarchical structure of feature dimensions
in our study) and the ranking framework, we propose a
new learning-to-rank formulation based on weakly hierarchical
lasso.
The contributions of our work are summarized as follows:
Firstly, we propose a new RankSVM model by constructing the weakly hierarchical structure between features from
different views. Secondly, to solve the new formulation, we
propose an efficient algorithm and evaluate via experiments
its efficiency and effectiveness with comparisons with other
existing methods.

IEEE/ACM ASONAM 2016, August 18-21 2016, San Francisco, CA, USA
978-1-5090-2846-7/16/$31.00 ©2016 IEEE

307

II. R ELATED W ORK
In this section, we review briefly related research on community question and answering, and discuss the difference
between the reviewed work and our proposed method.
A. Content Quality Analysis
Compared with traditional on-line search, as one supplementary approach to solving our daily problems, CQA sites
contain a lot of valuable knowledge. Thus, since the first CQA
site was launched, finding high quality content from these sites
has become important. For example some early work was done
in [8] where Jiwoon Jeon et al. crawled data from Naver Q&A
site and manually labeled each pair of questions and their
corresponding answers as bad, medium, good. They proposed
to use non-textual features to represent each question-answer
pair and used kernel density estimation and the maximum
entropy approach to model the problem of answer quality.
To have a better representation of questions and answers on
CQA sites, more sources of information were used to extract
new features like interactions between questions and answers
and users, as studied in [2], where Eugene Agichtein et al.
proposed to use non-content information to model question
and answer pairs on CQA sites including the interaction
features. Then different classifiers like support vector machine,
log-linear classifier and stochastic gradient boosted trees were
applied to learn the prediction model, whose efficiency and
effectiveness were evaluated using data from Yahoo! Answers.
The importance of social information for predicting answer
quality was studied in [3], where Chirag Shah et al. found the
importance of user information by studying the quality labeled
manually. Besides research on the answer quality, question
quality is also studied. In [9], Baichuan Li et al. worked
on the question quality prediction problem. They first studied
what factors may affect question quality and then proposed a
model termed Mutual Reinforcement-based Label Propagation
to predict question quality. In [10], it was found that the voting
scores of questions have a strong positive correlation with that
of the corresponding answers and they proposed a set of coprediction algorithms to predict the voting scores of questions
and answers.
The above work focused on content quality prediction
(question quality and answer quality), which is modeled as one
classification problem. These existing efforts mainly focused
on finding a better representation of the data by introducing
various features to facilitate the prediction problem.
B. Best Answer Prediction and Answer Ranking
Pairs of questions and their best answers can be easily
used to answer similar questions, as the research in [11]
shows. With the fast growth of CQA sites, there are a lot of
questions which have high quality answers but no best ones
eventually marked. To this end, a lot of research efforts have
been devoted to best answer prediction and answer ranking.
In [12], Lada Adamic et al. analyzed Yahoo! Answers for
best answer prediction. They used simple four-dimensional
features and reported that the length of answers is the most

important factor of answer quality. The problem they are
worked on is to predict whether a given answer is the best
one of the given question. They did not consider interaction
information like relationship between questions and answers
and users. It is not natural to model best answer prediction
as a classification problem since the best answer is relatively
defined. Thus there have been a lot of efforts on modeling
best answer prediction as a ranking problem. In [13], Mihar
Surdeanu et al. proposed a ranking model for non-factoid
questions and studied whether ranking algorithms can be
used to rank answers for given questions. They also showed
the importance of different features in the answer ranking
problem. This work was further extended in [14]. Instead of
simply applying learning to rank algorithms, some researchers
worked on improving the performance by using piggybacking
and ranking aggregation techniques. In [7], Felix Hieber et
al. applied RankSVM algorithms to best answer prediction
with piggybacking being used to improve the performance. In
their work, interaction features were used, like the similarity
between questions and answers. Piggybacking is used to for
obtaining a better representation of the questions so that
similarity between the questions and answers can help improve
the ranking performance of RankSVM. One example work
to use ranking aggregation is [15], where Arvind Agarwal
et al. made a comparison between different learning to rank
algorithms and proposed to use ranking aggregation techniques
to improve them. But that work focused on the factoid question
and answers instead of CQA. In contrast, our work employs
hierarchical interactions in the feature space.
There are also some efforts on studying the influence of
different combinations of features on the prediction accuracy
and also comparison across different CQA sites [16]. Pointwise ranking techniques were also used to rank answers to
each question. In [4], Daniel Dalip et al. assumed that the
voting scores to be the quality scores of answers. Then random
forest was used to model the relationship between the scores
and features. The final predicted rating scores were used to
rank each questions. To evaluate the performance, normalized
discounted cumulative gain at top k (NDCG@K) is used.
However, there is noise in the rating scores as shown in
[17], and thus in our work we do not use this assumption.
The information between answers to each question may help
capture the relative information for better prediction, as shown
in [18], where Tian et al. proposed to extract features from the
context information between answers to each question. There
are many other efforts on finding/defining new features for
best answer prediction. For example, temporal features are
proposed in [5].
One common observation in the most of the existing work
is that, when new features are derived, all of them are
concatenated to one vector to be the final feature vector. For
example, in [12], these features are used: reply length, thread
length, the total number of best answers of one user, the
total number of replies one user has. They can be denoted
as x1 , x2 , x3 , x4 . Then the final feature vectors are the simple
concatenation of these features which are (x1 , x2 , x3 , x4 ). In

308

our work, we focus on proposing a new model which can
capture the feature interactions based on hierarchical lasso.

The RankSVM formulation is given below (Eqn. 3):
X
1
kwk22 + C
ξi,j1 ,j2
(3)
min
2
w∈Rd×1
s.t. S1 (i, j1 ) ≥ S1 (i, j2 ) + 1 − ξi,j1 ,j2 , ∀(i, j1 , j2 )
ξi,j1 ,j2 ≥ 0, ∀(i, j1 , j2 )

III. P ROBLEM D ESCRIPTION AND F ORMULATION
The research problem in this paper is formally defined as
follows: given a question with all of its received answers, to
predict which one is the best one. To select the best answer,
one has to compare it with the others, so that the best answer
is relatively defined. Thus instead of using the classification
framework, we employ the learning-to-rank strategy. The basis
of our proposed approach is RankSVM [6]. While existing work focuses on designing new features, we study this
prediction problem from the following angle: modeling the
interaction of features from different views of data beyond
simple concatenation of them. To achieve this goal, we employ
weakly hierarchical lasso [19] in constructing a new ranking
model.
Notations of this paper are described in the following.
Denote a dataset with N questions as {qi , i ∈ {1, · · · , N }}.
For each question qi , it receives a group of answers which
are {Ai,j , j ∈ {1, · · · , Mi }} where Mi is the total number
of answers to qi . The feature vector xi,j ∈ R1×d is used to
represent the j th answer to the ith question. Moreover, the
k th dimension of one feature vector xi,j is defined as xi,j,k
where k ∈ {1, · · · , d}. xi,j is the simple concatenation of
features extracted from different views of our problem, as
done in the existing work. It is named as the main effect.
Then for each xi,j , we compute the second-order interaction
2
which is denoted as zi,j ∈ R1×d , which is called the
second-order interaction term. The final feature vector by
considering the main effect and the interaction term is denoted
2
as x̂(i, j) = [xi,j , zi,j ] ∈ R1×(d+d ) . The interaction term is
defined as follows (see Eqn.1):
(1)

(2)

(d)

zi,j = [zi,j , zi,j , · · · , zi,j ]
(m)
zi,j

(1)

= [xi,j,m · xi,j,1 , xi,j,m · xi,j,2 , · · · , xi,j,m · xi,j,d ]

where i ∈ {1, · · · , N }, j ∈ {1, · · · , Mi } and m ∈ {1, · · · , d}.
In our work, instead of classification methods, learning-torank techniques are used to model the relativeness of the
best answers. Each relatively ranked pair is represented as
(qi , Ai,j1 , Ai,j2 ) where the quality of Ai,j1 is higher than
that of Ai,j2 . For simplicity, we may use (i, j1 , j2 ) as the short
version of (qi , Ai,j1 , Ai,j2 ) in the following equations. The
set Pi contains all these pairs of answers to the question qi .
Furthermore, the entire set of these relatively ranked pairs is
denoted as P in Eqn.2.
P =

[

Pi

(2)

i∈{1,··· ,N }

RankSVM, as one state-of-the-art pair-wise learning-to-rank
algorithm used in best answer prediction [5][7], is used as the
basic building block of our new ranking model.

where (i, j1 , j2 ) is one ranked QA pair in P and S(i, j) is the
quality score function of the j th answer to qi and defined in
Eqn.4.
S1 (i, j) = xi,j w + w0

(4)

where w0 ∈ R.
To improve the performance of RankSVM, our model
involves the second-order interactions via constructing one
weakly hierarchical structure in the feature space. The formulation of the new ranking model is shown in Eqn.5. Compared with the existing work, we model the latent interaction
structure between features from different views of the data,
instead of simple concatenation. The hierarchical structure of
the feature space is constructed through the first group of
constraints (a.k.a kQ.,j k1 ≤ |wj |, j ∈ {1, · · · , d}) in Eqn.5.
X
1
ξi,j1 ,j2
(5)
min
kwk1 + kQk1 + C
2
w∈Rd×1 ,
Q∈Rd×d

(i,j1 ,j2 )∈P

s.t. kQ.,j k1 ≤ |wj |, j ∈ {1, · · · , d}
ξi,j1 ,j2 ≥ 0, ∀(i, j1 , j2 ) ∈ P
S(i, j1 ) > S(i, j2 ) + 1 − ξi,j1 ,j2 , ∀(i, j1 , j2 ) ∈ P
P P
where Q.,j is the j th column of Q, kQk1 = i j |Qi,j | and
S(·, ·) is the ranking score for each answer to one question
defined in Eqn.6. For example S(i, j) is the ranking score for
answer Ai,j to qi .
1
(6)
S(i, j) = xi,j w + zi,j vec(Q) + w0
2
where vec(Q) is the vectorized version of Q and zi,j is shown
in Eqn.1 and w0 ∈ R.
To help illustrating the proposed model, we depict the
hierarchical structure based on one example shown in Figure 1,
in which we only show three features: the length of the answer
(Alen ), the number of URLs in the answer (Nurl ), the number
of pictures used in the answer (Npic ). In this illustration, we
can see that the upper layer contains all main effects (a.k.a xi,j
) while the second layer shows the interaction terms (a.k.a zi,j
in Eqn.1) excluding the square values of themselves. When
one term contributes to the objective function, no matter it
belongs to main effects or interaction terms, its corresponding
coefficient is set to be non-zero. For each interaction term,
if it contributes to the objective function, then at least one
of its corresponding main effects contributes to the objective
function. Satisfying these hierarchical constraints, it is easy for
us to conclude that the interaction terms contribute less than
their corresponding main effects. Specifically, in this figure, if
the coefficient of Alen · Nurl is non-zero, then the coefficient
of Alen is non-zero but that of Nurl can be zero.

309

From Eqn. 5, the weakly hierarchical lasso is involved
via the first group of constraints (a.k.a kQ.,j k1 ≤ |wj |, j ∈
{1, · · · , d}).

where L(w, Q) is given in the following:
L(w, Q) =
Set λ =

1
C,

|P |
X

1
max(0, 1 − (x̃m w + z̃m vec(Q)))2 (13)
2
m=1

the final model is obtain as given in Eqn.14
λ
kQk1
2
j ∈ {1, · · · , d}

min L(w, Q) + λ · kwk1 +
w,Q

s.t. kQ.,j k1 ≤ |wj |,
Fig. 1: One illustration to show hierarchical structure in the
feature space, where “·” represents the scalar multiplication.
The first layer contains the main effect, while the second layer
consists of the 2nd order of interaction.
IV. S OLVING THE P ROPOSED M ODEL
To develop a solution to our proposed model in Eqn. 5, we
first reformulate the problem as follows. Consider this group
of constraints (Eqn.7) in the proposed model in Eqn. 5.
Si,j1 > Si,j2 + 1 − ξi,j1 ,j2

(7)

Together with Eqn.6, we have the following computation:
Si,j1 > Si,j2 + 1 − ξi,j1 ,j2
(8)
1
Si,j1 = xi,j1 w + zi,j1 vec(Q) + w0
2
1
Si,j2 = xi,j2 w + zi,j2 vec(Q) + w0
2
If we assume the relatively ranked pair (qi , Ai,j1 , Ai,j2 ) is
the mth element in the set P of Eqn.2, then Eqn.8 can be
simplified and the following is obtained:
1
(9)
x̃m w + z̃m · vec(Q) > 1 − ξ˜m
2
where x̃m , z̃m should satisfy the following constraints in
Eqn.10.
x̃m = xi,j1 − xi,j2
z̃m = zi,j1 − zi,j2
As a result, Eqn.5 is converted to the following:
X
1
min kwk1 + kQk1 + C
ξ˜m
w,Q
2

(10)

To this point, our objective function has been reformulated into
the standard form as in the weakly hierarchical lasso problem
defined in [19] and [20].
To solve Eqn. 14, the scheme in [20] can be applied since
it can directly solve the weakly hierarchical lasso without
adding more penalty compared with approach in [19]. Since
the optimization process in [20] is based on a general iterative
shrinkage and thresholding algorithm (GIST) in [21], before
we use the method in [20], we need to prove that L(w, Q) in
Eqn. 14 is continuously differentiable with Lipschitz continuous gradient.
Before proceeding with the proof, we introduce following
notations:
x̂ = (x̃, z̃)


w
ŵ =
1
2 vec(Q)

(11)

L̂(ŵ) =

1
min kwk1 + kQk1 + C · L(w, Q)
w,Q
2
s.t. kQ.,j k1 ≤ |wj |, j ∈ {1, · · · , d}

X

max(0, 1 − x̂m · ŵ)2

To show L̂(ŵ) is differentiable with Lipschitz continuous
gradient, this requirement needs to be satisfied: there exists
a positive constant β such that
dL̂
dL̂
(w1 ) −
(w2 )k2 ≤ βkw1 − w2 k2
dŵ
dŵ

(17)

Let us first consider one additive component of L̂(ŵ). The
point-wise maximum function can be written as Eqn.18.
l(ŵ) = max(0, 1 − x̂m · ŵ)2
(
0
if 1 − x̂m · ŵ < 0
=
2
(1 − x̂m · ŵ) if 1 − x̂m · ŵ ≥ 0

(12)

(16)

m∈{1,··· ,|P |}

1
s.t. x̃m w + z̃m · vec(Q) > 1 − ξ˜m , m ∈ {1, · · · , |P |}
2
kQ.,j k1 ≤ |wj |, j ∈ {1, · · · , d}
ξ˜m ≥ 0, m ∈ {1, · · · , |P |}
where |P | is the size of the set P .
Now we can reformulate Eqn.11 into Eqn.12:

(15)

As a consequence, x̂ ∈ R1×(d+d·d) and ŵ ∈ R(d+d·d)×1 .
L(w, Q) is converted from Eqn.13 as Eqn.16.

k

m∈{1,··· ,|P |}

(14)

(18)

It is easy to see that when w1 , w2 ∈ {w|1 − x̂m · w < 0}
and w1 , w2 ∈ {w|1 − x̂m · w ≥ 0}, Eqn.17 is satisfied. Now
considering w1 ∈ {w|1 − x̂m · w < 0}, w2 ∈ {w|1 − x̂m ·
w > 0}, it is easy to see that the left part of Eqn.17 becomes
k(1 − x̂ · w2 )x̂m k. Moreover, define ŵ∗ as 1 − x̂m · w∗ = 0 and

310

this inequality is satisfied: kw1 − w2 k ≥ kw∗ − w2 k. Now to
obtain the constant β, the following induction is performed:
k(1 − x̂m · w2 )x̂m k ≤ βkw1 − w2 k
k(1 − x̂m · w2 )x̂m k
⇐
≤β
kw1 − w2 k
k(1 − x̂m · w2 )kkx̂m k
⇐
≤β
kw∗ − w2 k
k(1 − x̂m · w2 )kkx̂m k2
⇐
≤β
kw∗ − w2 kkx̂m k
k(1 − x̂m · w2 )kkx̂m k2
⇐
≤β
k1 − x̂m · w2 k
⇐β ≥ kx̂m k2

Now it is feasible to apply the algorithm in [20] to solve
Eqn.14 which is equivalent to solving this proximal operator
problem of Eqn.22.
1
1
(w(k+1) , Q(k+1) ) = arg min kw − v (k) k22 + kQ − U (k) k22
w,Q 2
2
1
λ
+ (k) (λkwk1 + kQk1 )
2
t
s.t. kQ.,j k1 ≤ |wj | ∀j ∈ {1, · · · , d}
(22)
where v (k) , U (k) are defined as follows:
v (k) = w(k) −
U (k) = U (k) −

(19)

Similarly, it is easy to obtain that β ≥ kx̂m k2 also satisfies
the case where w2 ∈ {w|1 − x̂m · w < 0}, w1 ∈ {w|1 −
x̂m · w > 0}. Thus, there exists a proper positive constant β
so that l(ŵ) meets the requirement Eqn. 17. In conclusion,
l(ŵ) is continuously differentiable with Lipschitz continuous
gradient. With this result, we will further introduce and prove
the following lemma, together with which we will able to show
the desired property for L(w, Q) is satisfied.
Lemma IV.1. For each function f (w)i , i ∈ {1, · · · , N }
which is continuously differentiablePwith Lipschitz continuous
N
gradient, their summation f (w) = i=1 fi (w) is continuously
differentiable with Lipschitz continuous gradient.

1

· 5w L(w(k) , Q(k) )

(23)

1
· 5Q L(w(k) , Q(k) )
t(k)

(24)

t(k)

where t(k) > 0 which is the step size.
Considering w, Q are products of their signs and also
absolute values, Eqn.22 can be re-written into Eqn.25.
1
1
(w(k+1) , Q(k+1) ) = arg min kw − v (k) k22 + kQ − U (k) k22
w,Q 2
2
λ
1
+ (k) (λkwk1 + kQk1 )
2
t
˜
s.t. Q.,j ≤ w̃j ∀j
(25)
where Q.,j = sign(Q.,j ) Q˜.,j and wj = sign(wj ) w̃j . The
above equation can be solved in a closed form as proved in
[20]. The pseudocode of our entire algorithm is shown in the
following. which is summarized in Algorithm 1.

Proof.
Algorithm 1 The pseudo-code to solve our model

d
d
f (w1 ) −
f (w2 )k
k
dw
dw
N
N
X
X
d
d
= k
fi (w1 ) −
fi (w2 )k
dw
dw
i=1
i=1
= k
≤

1:
2:
3:
4:
5:

N
X
d
d
(
fi (w1 ) −
fi (w2 ))k
dw
dw
i=1

N
X
i=1

k

6:
7:
8:
9:

d
d
fi (w1 ) −
fi (w2 )k
dw
dw

≤ βkw1 − w2 k

(20)

10:
11:
12:

Denote that there exists positive constant βi such that fi (w)
satisfies Eqn.17 where i ∈ {1, · · · , N }. Thus Eqn.20 is valid
when β meets this requirement:
β = max βi
i

13:

(21)

2
Eqn. 17 and L̂(ŵ) =
PSince max(0, 1 − x̂m · ŵ) satisfies
2
max(0,
1−
x̂
·
ŵ)
,
according
to Lemma IV.1,
m
m∈{1,··· ,|P |}
L̂(ŵ) satisfies Eqn. 17, same as L(w, Q) defined in Eqn. 13.
Thus, L(w, Q) is continuously differentiable with Lipschitz
continuous gradient.

14:
15:
16:

INPUT: data matrix X and ranking information of all data
OUTPUT: model parameters w and Q
BEGIN:
compute the set P based on Eqn.2.
compute the data difference {x̃m , m ∈ {1, · · · , |P |}}
and {z̃m , m ∈ {1, · · · , |P |}} as Eqn.10.
provide initial values for w and Q.
choose one t via BB Rule [22].
while w, Q satisfy the stop criteria do
while tk does not satisfy the stop criteria do
update v k according to Eqn.23.
update U k according to Eqn.24.
obtain new w(k+1) and Q(k+1) based on Eqn.25,
which can be in the closed form as [20].
update the step size t(k) = α ∗ t(k) where α is the
constant update ratio.
end while
k = k + 1;
end while

V. E XPERIMENTS
In this section, we present experimental results on Stack
Overflow to show the performance of our proposed model and
the comparison with existing methods.

311

number of users
number of votes
number of comments
number of questions
number of answers

A. Data Description
Founded in 2008, StackOverflow is active and well maintained. On this site, users can post questions and everyone can
provide answers even including the askers. For each question
and each answer, users can comment on it. For one question or
answer, users can vote up or down based on its quality except
the user who posts it. For one comment, users can only vote
up if they think the comment is useful, but cannot vote down.
Same as one question or one answer, the one cannot vote up
his or her own comments. For one question or one answer, it
can receive up-votes and also down-votes. Then the number
of up-votes minus the number of down-votes is the vote score.
It is easy to see that the vote score are integers and can be
negative.
Each question can receive multiple answers and only the
asker can decide which one can be marked as the accepted
answer which we call the best answer. This choice is not
permanent, which means the asker can change his or her mind
at any time and mark another answer as the best answer.
There is one fact we need to point out. One question may
receive multiple correct answers but only one of them can
be marked as the best answer. So the best answer has the
relatively best quality instead of absolutely best one. This is the
reason why we use the learning to rank techniques instead of
the classification methods. For users, they can earn reputations
if their posts (e.g. questions ,answers, and comments) obtain
upvotes or answers are accepted or suggestions on editing
others’ posts are accepted. Otherwise, they lose reputations
if their posts receive downvotes or are reported as spam or
offensive. Figure 2 shows one sample of one question with its
answers from StackOverflow. Till May 8, 2015, the statistics

Fig. 2: Illustration of one sample question from Stack Overflow.
of this site are as in Table. I.
B. Experiment Settings
In our experiment, part of StackOverflow dataset is used.
We downloaded all questions posted from October 1, 2012 to
December 31, 2012 and all related information like answers

4,232,639
62,357,544
44,557,809
9,365,722
15,632,696

TABLE I: The information of Stack Overflow till May 8, 2015.

was tracked until January 2014. This time period was chosen
because of these reasons: First, questions and answers in this
time period are not very out-dated; Second, few user activities
on posts in this period are active. Thus, we assume that the
best answer to one question is the final one. The dataset we
use was dumped on January 2014 4 . Before feature extraction,
posts without users’ IDs are removed. Then, only questions
which have best answers and at least two more answers are
considered. The final processed dataset has 52,104 questions
and 190,165 answers. On average, there are 3.65 answers per
question. During the experiments, our data set is randomly
split into two parts evenly: training and testing.
To be specific, details as follows show how to generate
relatively ranked pairs. For each question, only its best answer
is considered as the high quality answer while others are
treated as low-quality answers. Then each pair is generated
in this way: one best answer and one of other answers to the
same question. After all pairs are generated, feature extraction
is performed based on information from three main aspects
of each pair of questions and answers: content, interactions,
users. These are briefly described below.
The First group of features are extracted based on the
content of the answer in each pair of questions and answers.
Part of these features are based on comments to the answers
like average score of comments, variance of the comments’
scores, number of comments. Comment-based features at least
show that the corresponding answer is interesting and incur
a good discussion towards problem solving. Besides these,
whether one answer has pictures, URL or codes are also factors
to show that the current answer has a high quality, since these
components are able to show more information than text.
Moreover, the length of answers [12][2] and its readability
[18] also play an important role on answer quality.
Apart from the content information, features based on
interaction are also considered, for example, the interaction
between questions and answers, and that between different
answers to one question. The first one is easy to understand
since one answer has to be similar to its corresponding question, and thus the similarity between questions and answers
is used as one feature. The second one is designed based on
the assumption that users prefer the answers which is easy to
understand. Computation of these features are shown in [18].
This is different from the feature interaction in our model. This
one is on the feature-design level which focuses on exploring
new information sources to design new features, while our
case focuses on the model-design level.

312

4 http://blog.stackoverflow.com/category/cc-wiki-dump/

User information also has an impact on the quality of
answers. One answer is likely to have a high quality if the
answerer is one expert. To represent the expertise of one user,
these features are extracted based on users’ previous activities,
for example the number of answers one provides, how many
questions one asks, the number of best answers he or she posts.
Our experiment is conducted by considering different
groups of features and then results are presented respectively.
In this way, it is easy to see the performance of different
algorithms when we only consider informations from different
aspects of our research problem (i.e. different groups of
features). Finally, the experiment is conducted on the entire
feature set we have. The three groups of features we consider
in this experiment are: content, interactions and user information.
C. Experiment Results & Discussion
To show the performance of our proposed algorithm, we
compare our model with approaches used in state-of-the-art.
As mentioned in Section Introduction, there are two main
trends in best answer prediction: one is to use classification
techniques and then decision values are used as quality scores
while the other one is to use ranking approaches directly.
For the former case, linear support Vector Machine (SVM)
is common used because data in social media is in large scale
so that nonlinear algorithms are not computational efficient.
In our experiment, linear SVM is the first baseline we choose.
For the latter case, RankSVM [6] is used which is one main
ranking algorithm used in the area of best answer prediction
[5]. The code for RankSVM is from Microsoft Research 5 .
On CQA sites, there are no direct information we can use
as the metric to measure answer quality without manually
labeling. For example, scores of each answer might be one
proper metric. But this metric is not accurate. It is easy to
see that it is easy for the answer which is posted early to
have the high score. In fact, on Stack Overflow, there are a lot
of answers having the higher scores than the corresponding
best answers6 . Thus in our experiments, we only treat the best
answers as the high-quality ones and others as low-quality. As
a result, in our experiment, it is the pairwise ranking problem
so we do not compare with listwise ranking algorithms.
To make comparison between different models, two evaluation metrics are used: one is defined in Eqn. 26 and the other
one is defined in Eqn. 27.
P
∀(qi ,Ai,j1 ,Ai,j2 )∈P I(si,j1 > si,j2 )
(26)
e1 =
|P |
where si,j1 , si,j2 are predicted scores of Ai,j1 , Ai,j2 respectively. The relatively ranking set P is defined in Eqn. 2 and
the function I(·) is shown in Eqn. 28.
5 http://research.microsoft.com/en-us/um/beijing/projects/letor/baselines/
ranksvm-primal.html
6 https://data.stackexchange.com/stackoverflow/query/380215/whereaccepted-answer-does-not-have-the-highest-score

g(i) = arg max{si,j , j ∈ {1, · · · , Mi }}
j
P
I(ji,0 == g(i))
(27)
e2 = i
N
where ji,0 is the index of the best answer of the ith question,
si,j is the predicted score of the j th answer of the ith question
and the function g(·) returns the index of the best answer of
one given question and the function I(·) is given by Eqn.28.
(
1 if x is true
(28)
I(x) =
0 otherwise
From the definitions, it is easy to see this fact: e1 shows
how good one algorithm is when it considers the pairwise
ranking regardless of whether one algorithm can find the best
answer to one question or not, while e2 shows the performance
of each algorithm when applied to best answer prediction. In
other words, e1 measures what percentage of relatively ranked
pairs are predicted correctly, which focuses on the answerlevel comparison. However e2 measures what percentage of
questions have the correctly predicted best answers.
To show the performance of different models on the pairwise ranking in best answer prediction, experiments were
conducted to collect the metric e1 . The experimental results
are shown in Table. II. Table. II presents the performance of
SVM
RankSVM
Ours

fc
0.671
0.411
0.689

fi
0.541
0.534
0.552

fu
0.480
0.543
0.570

all
0.544
0.476
0.693

TABLE II: This table shows the results of different algorithms
on Stack Overflow when considering the measurement metric
e1 . Three groups of features: fc content, fi interactions, fu
user information.
algorithms used as learning to rank. From the results, we can
see that our model performs best not only when only individual
feature groups are considered but also when all features are
considered. This shows that our model can be one good
pairwise ranking algorithm in the area of community question
and answering. From the results of SVM, we can see that when
only fc is considered, the performance is best. However, when
simple concatenation of all features from different views is
applied, the final one gives worse performance instead of better
one. Similarly, for RankSVM, its performance is best when
only fu is considered. However after considering all features,
the performance drops. For our approach, because we consider
the interaction structure of features from different views, the
final performance is best. This shows that there exists on latent
interaction structure in the feature space. Incorporating weakly
hierarchical lasso, we can capture this interaction structure.
This shows the effectiveness of our proposed model.
To show comparison of performance on best answer prediction, experiments were run to collect metric e2 . Table. III
presents the performance of different models. From the results,
it is easy to see that our model performs best in the problem

313

SVM
RankSVM
Ours

fc
0.479
0.223
0.494

fi
0.331
0.321
0.334

fu
0.294
0.361
0.377

all
0.349
0.286
0.498

TABLE III: Experiment results (e2 ) of different algorithms’
performance. Three groups of features: fc content, fi interactions, fu user information.

of best answer prediction not only when considering different
groups of features independently but also when considering all
features jointly. Similar to Table.II, the performance of SVM
and RankSVM drop a lot when all features are considered
by simple concatenation. For our model, it does not have this
problem because of the fact that we incorporate the information from the latent interaction of features from different
views.
Consequently, we conclude that the proposed models perform better than those in the state-of-the-art. Performance
of experiments using both metrics shows the effectiveness
of hierarchical interactions between different views in the
problem of best answer prediction.
VI. C ONCLUSION & F UTURE W ORK
We present a new learning-to-rank approach to best answer
prediction on CQA sites. Incorporating the weakly hierarchical
lasso, our proposed model is able to effectively exploit the
interactions of features from different views of the data. To find
a solution under this new model, we reformulate it into one existing optimization framework. Experiments on Stack overflow
are used to evaluate the proposed approach, with comparison
to other methods in state-of-the-art. The experimental results
demonstrate the effectiveness and superior performance of
our approach. Although our algorithm is designed originally
for best answer prediction, it can be treated as one ranking
algorithm and used in most ranking situations. Thus the
application of our algorithm in different areas can be one piece
of future work. Moreover, in our algorithm, one limitation
is that we study the interaction structure of different feature
dimensions, instead of different groups of feature dimensions.
Another interesting future work is to extending our algorithm
by considering the hierarchical structure of different groups of
feature dimensions.
ACKNOWLEDGMENT
This work was supported in part by a grant (#1135616) from
National Science Foundation. Any opinions expressed in this
material are those of the authors and do not necessarily reflect
the views of the NSF.
R EFERENCES
[1] A. Anderson, D. Huttenlocher, J. Kleinberg, and J. Leskovec, “Discovering value from community activity on focused question answering
sites: a case study of stack overflow,” in Proceedings of the 18th ACM
SIGKDD international conference on Knowledge discovery and data
mining. ACM, 2012, pp. 850–858.

[2] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G. Mishne, “Finding
high-quality content in social media,” in Proceedings of the 2008
International Conference on Web Search and Data Mining. ACM,
2008, pp. 183–194.
[3] C. Shah and J. Pomerantz, “Evaluating and predicting answer quality
in community qa,” in Proceedings of the 33rd international ACM
SIGIR conference on Research and development in information retrieval.
ACM, 2010, pp. 411–418.
[4] D. H. Dalip, M. A. Gonçalves, M. Cristo, and P. Calado, “Exploiting
user feedback to learn to rank answers in q&a forums: a case study with
stack overflow,” in Proceedings of the 36th international ACM SIGIR
conference on research and development in information retrieval. ACM,
2013, pp. 543–552.
[5] Y. Cai and S. Chakravarthy, “Answer quality prediction in Q/A social
networks by leveraging temporal features,” Proceedings of International
Journal of Next-Generation Computing, vol. 4, no. 1, 2013.
[6] O. Chapelle and S. S. Keerthi, “Efficient algorithms for ranking with
svms,” Information Retrieval, vol. 13, no. 3, pp. 201–215, 2010.
[7] F. Hieber and S. Riezler, “Improved answer ranking in social questionanswering portals,” in Proceedings of the 3rd international workshop on
Search and mining user-generated contents. ACM, 2011, pp. 19–26.
[8] J. Jeon, W. B. Croft, J. H. Lee, and S. Park, “A framework to predict
the quality of answers with non-textual features,” in Proceedings of
the 29th annual international ACM SIGIR conference on Research and
development in information retrieval. ACM, 2006, pp. 228–235.
[9] B. Li, T. Jin, M. R. Lyu, I. King, and B. Mak, “Analyzing and
predicting question quality in community question answering services,”
in Proceedings of the 21st international conference companion on World
Wide Web. ACM, 2012, pp. 775–782.
[10] Y. Yao, H. Tong, T. Xie, L. Akoglu, F. Xu, and J. Lu, “Detecting
high-quality posts in community question answering sites,” Information
Sciences, 2015.
[11] A. Shtok, G. Dror, Y. Maarek, and I. Szpektor, “Learning from the past:
answering new questions with past answers,” in Proceedings of the 21st
international conference on World Wide Web. ACM, 2012, pp. 759–768.
[12] L. A. Adamic, J. Zhang, E. Bakshy, and M. S. Ackerman, “Knowledge
sharing and yahoo answers: everyone knows something,” in Proceedings
of the 17th international conference on World Wide Web. ACM, 2008,
pp. 665–674.
[13] M. Surdeanu, M. Ciaramita, and H. Zaragoza, “Learning to rank answers
on large online qa collections.” in ACL, 2008, pp. 719–727.
[14] ——, “Learning to rank answers to non-factoid questions from web
collections,” Computational Linguistics, vol. 37, no. 2, pp. 351–383,
2011.
[15] A. Agarwal, H. Raghavan, K. Subbian, P. Melville, R. D. Lawrence,
D. C. Gondek, and J. Fan, “Learning to rank for robust question
answering,” in Proceedings of the 21st ACM international conference
on Information and knowledge management. ACM, 2012, pp. 833–
842.
[16] G. Burel, Y. He, and H. Alani, “Automatic identification of best answers
in online enquiry communities,” in The Semantic Web: Research and
Applications. Springer, 2012, pp. 514–529.
[17] S. Ravi, B. Pang, V. Rastogi, and R. Kumar, “Great question! question
quality in community q&a,” in Eighth International AAAI Conference
on Weblogs and Social Media, 2014.
[18] Q. Tian, P. Zhang, and B. Li, “Towards predicting the best answers in
community-based question-answering services,” in Seventh International
AAAI Conference on Weblogs and Social Media, 2013.
[19] J. Bien, J. Taylor, R. Tibshirani et al., “A lasso for hierarchical
interactions,” The Annals of Statistics, vol. 41, no. 3, pp. 1111–1141,
2013.
[20] Y. Liu, J. Wang, and J. Ye, “An efficient algorithm for weak hierarchical lasso,” in Proceedings of the 20th ACM SIGKDD international
conference on Knowledge discovery and data mining. ACM, 2014, pp.
283–292.
[21] P. Gong, C. Zhang, Z. Lu, J. Z. Huang, and J. Ye, “A general iterative shrinkage and thresholding algorithm for non-convex regularized
optimization problems,” in Machine learning: proceedings of the International Conference. International Conference on Machine Learning,
vol. 28, no. 2. NIH Public Access, 2013, p. 37.
[22] J. Barzilai and J. M. Borwein, “Two-point step size gradient methods,”
IMA Journal of Numerical Analysis, vol. 8, no. 1, pp. 141–148, 1988.

314

AUTOMATIC GENERATION OF PENCIL-SKETCH LIKE DRAWINGS
FROM PERSONAL PHOTOS
Jin Zhou and Baoxin Li
Center for Cognitive Ubiquitous Computing
Department of Computer Science and Engineering
Arizona State University, Tempe, AZ, U.S.A.
{jinzhou, baoxin.li}@asu.edu
ABSTRACT
In this paper, we present an algorithm for automatically
generating pencil-sketch like drawings from personal photos. On
top of the core step of gradient computation, some proper
transformations are introduced to achieve the best visual effects.
It is found that the popular Sobel operator and Laplacian operator
are both not desirable for this task, and thus we propose a
computationally simple algorithm for gradient estimation.
Experimental results show that the proposed method can generate
visually appealing pencil-sketch like images from personal
photos.

1. INTRODUCTION
Pencil sketch drawings are a very popular form of art. In a typical
pencil sketch image, only the most characteristic lines of the
underlying subject are drawn, using a dark color (pencil) on a
white background (paper). Also, certain degree of variation in the
darkness of the pencil is typically used to depict various types of
transitional boundaries (e.g., edges) and shadows in the original
scene. Pencil sketch drawings are in some sense similar to penand-ink drawings. In computer graphics, many pen-and-ink
rendering techniques have been proposed [1-5], which generate
pen-an-ink images by 3D geometry or through human interaction.
Recently, an automatic method [6] was reported, which generates
pen-and-ink drawings directly from photos. Advantages of this
type of automatic approaches, such as no need for a 3D model
and no requirement for human interaction, make them appealing
in various applications targeting at the general public.
Unfortunately, directly applying the method of [6] to the
generation of pencil sketches is not practical. First of all, a
fundamental assumption in [6] is that the pen-and-ink drawings
are binary. And thus the major focus there is to represent the
texture information via binary pen strokes. This is not necessarily
true and useful in the case of pencil sketch, which may allow
(ideally) some degree of variation in the darkness of the pencil
color (we confined ourselves to black pencils only). In particular,
there are no examples of human photos given in [6]. It is very
likely that in the case of human photos, such as portraits, binary
texture information alone is unlikely to capture the essence of the
photo. In addition, the method of [6] does not seem to keep the
continuity of edges, as evident in the scenery-only examples
listed in the paper, which may be important in processing
portrait-type photos.
In this paper, we focus on photos with human subjects. Unlike in
the case of scenery images used in [6], where people may simply
judge if the generated image is beautiful, the generation of a

0-7803-9332-5/05/$20.00 ©2005 IEEE

human photo demands the maintaining of the key features of the
image, otherwise the obtained image may not resemble the
original, rendering the generated sketch undesirable. We also
believe that processing human photos may be deemed more
appealing to a user as people may be more interested in
converting a photo contain themselves.
In Section 2, we introduce a general method for converting a
photo into a pencil sketch drawing, illustrated with examples.
Then, in Section 3, we experiment the proposed method using
different gradient estimation methods, and then propose a
computationally simple algorithm, which generates the most
appealing visual results. Experiments with various images are
then presented in Section 4.

2. PROPOSED METHOD
2.1 Problem Description
The essential properties of a pencil sketch image include:
1. Objects are depicted by contours, which are long and
significant edges.
2. Edges are drawn in dark, with the darkness roughly
proportional to the local gradient, while the background is kept
purely white.
3. Textured regions can be depicted by a collection of short
lines/edges.
Therefore, if we want to generate pencil sketch image from
a photo, first we need to extract edges from the image,
and then draw the edges in black stroke, with the darkness
roughly proportional to the local gradient. If the edge detector is
so designed that it can extract short edges in textured regions,
then the above point 3 will be handled naturally.
Note that, while the above task looks simple and similar to edge
detection, it is not a pure edge detection problem, as illustrated in
Fig. 1, where we apply the well-known Canny’s edge detector [7]
to an input image (a) to get the edge map (b). Obviously, the
image in (b) does not constitute what people would normally call
a pencil sketch drawing.
2.2 The Proposed Four-step Approach
From the example in Fig. 1, it is obvious that, to achieve the
desired visual effects of an actual pencil sketch as summarized
above, more processing is needed. In the following, we propose a
four-step method for this purpose, as outlined in Fig.2. The
method is first described in a general setting with the Laplacian
operator as an example. We will then explore more for a better
gradient estimation algorithm in Sect. 3.

the input, Steps 1 and 2 will generate the image given in Fig. 3
(a).

(a)
(b)
Figure 1. Edge detection does not generate a pencil-sketch
type of drawing. (a) Original image. (b) Detected edges.
Input
image

(a)
(b)
Figure 3. Intermediate results of the steps.

Sketch

Smoothing

Gradient
estimation

Gradient
transform

Smoothing

Figure 2. Outline of the steps in the proposed method.
Step 1: Smooth the input picture
In general, an unprocessed image may contain excessive noise,
which reacts strongly to the subsequent gradient estimation.
Therefore, as the first step, the input image is smoothed by a
Gaussian low-pass filter, just as the pre-processing of the Canny’s
edge detector.
Step 2: Gradient estimation
With the noise subdued by the smoothing step, the next task is to
detect points of significant gradient (roughly speaking, the
edges). As noted above, not only the locations of the edges but
also the gradient at those locations should be kept. For illustration
purpose, we use the following Laplacian operator for gradient
estimation. (The performance will be discussed in Sect. 3.)

(c)

Note that, even with the smoothing operation in Step 1, one can
still find that, from Fig. 3(a), there are excessive noisy details still
visible in the image, making it dissimilar to a pencil sketch. This
is especially the case since we used the Laplacian operator, which
is very sensitive to noise. To get a succinct pencil sketch image,
we need to eliminate most of the details. To this end, in practice
we threshold the gradient image before applying the
transformation of Eqn. (2). Thus in Eqn. (1), g will be set to zero
if |g| is less than a threshold. Using a threshold 4 gives us the
image of Fig. 3 (b).
Step 4: Final smoothing for enhanced visual effects
The results from the previous three steps typically give the visual
effect of a pencil sketch, except that there may be many broken
contours that appear to be unnatural. Also, the thresholding step
may make the difference between the contours and the
background too abrupt, rendering the contours less similar to a
pencil stroke. To alleviate these problems, we adopt another
smoothing step to further blend the contours with the background
and to link the broken contours. The effect of this step is
illustrated in Fig. 3(c).

3. A BETTER GRADIENT ESTIMATION METHOD
That is, the output after applying the mask (left) to an image
block
shown
on
the
right
is
given
as g = 4 z5 − ( z 2 + z 4 + z6 + z8 ) .
Since the Laplacian operator produces both positive gradient and
negative gradient, to roughly detect the edge, we can simply keep
either of the two gradients (a more accurate way is to detect the
zero-crossing). We choose the negative gradient. To this end, the
following simple thresholding is used:

⎧| g |
g=⎨
⎩0

if g < 0
otherwise

(1)

Step 3: A proper gradient transform
To achieve the objective of linking darker pencil color to edges of
larger gradient, we apply the following transform,

⎧120 − g if g > 0
g=⎨
otherwise
⎩255

(2)

where 120 is an empirically-chosen parameter, which can be
user-adjustable in the software. With the image of Fig. 1(a) as

The core step of the proposed method in Sect. 2 is the gradient
estimation. In this section, we show, with experiments, that both
of the commonly-used Sobel operator and Laplacian operator are
not very effective for our purpose. Then we discuss what consists
of a good gradient estimator for the purpose, and propose an
algorithm for this task.
The examples shown in Fig. 3 are based on the Laplacian
operator. Here we give an example of using the Sobel operator to
perform the same task. The results are given in Fig. 4. Note that,
although Fig. 4 (a) seems very pencil-drawing like, the details of
the original image (e.g. the eyes and the mouth) are damaged,
rendering the final results less appealing than that of Fig. 3(c).
3.1 The Proposed Gradient Estimator
In estimating the gradient from the original image, it would be
desired to consider a larger support in order to extract true
contours rather than noisy segments. One of the drawbacks of the
Laplacian operator as well as other conventional edge detectors is
that only a small neighborhood is used to compute the gradient.
To address this issue, we propose a simple algorithm which

makes use of a larger support in gradient computation. For a
given point, along the horizontal axis, this method first searches
for a local maximum in the 1-D gradient in both directions
(towards the left and towards the right of the current point). The
two gradients from the above are added to get one horizontal
gradient for that point. The process is repeated for the vertical
axis, and the larger of the two gradients will be assigned as the
final gradient of the current point. With the input image smoothed
first, this search effectively utilizes a larger support in gradient
computation.

(a)
(b)
(c)
Figure 5. (a) Gradient estimation using the proposed
method. (b) After thresholding. (c) Final results; Compare
with Fig. 3(b) and Fig. 3(c).

4. EXPERIMENTAL RESULTS
4.1 Comparison of Different Gradient Estimation Methods

(a)

(b)

Figure 4. (a) Results of the Sobel operator, after inversion
and thresholding.. (b) After applying the transformation of
Eqn. (2).
Formally, we define the gradient between two points p1 and p2 as

g ( p1 , p2 ) = (v1 − v2 ) / Distance( p1 , p2 )
For each point p, we define four search directions, to its left,
right, up, and down, respectively. For each direction, we use a
greedy search algorithm to search for the local maximum in
gradient for point p, as illustrated in the following pseudo code:
Function g_d( p ) // d ∈ {left, right, up, down}
maxg = 0;
For each point pi in the direction d ,from near to far:
g = g( p , pi);
if (|g| > |maxg|) maxg = g;
else return maxg;
On each of the horizontal (x) and vertical (y) axes, the gradient
for point p is obtained by a simple addition:
g_x(p) = g_left(p) + g_right(p)
g_y(p) = g_up(p) + g_down(p)
and the final gradient for point p is given by

⎧ g _ x ( p ) if |g_x(p)| ≥ |g_y(p)|

g ( p) = ⎨

⎩ g _ y ( p ) otherwise

With the above simple algorithm, we process the image of Fig.
1(a) again, and Fig. 5 shows the obtained results, corresponding
to those of Fig. 3. A close look reveals that, in the final results in
Fig. 5, the lines are darker and thicker by the proposed gradient
computation; also more true details are detected without adding
too much noise. Overall, it was found that the simple gradient
estimation method proposed above generates outputs that are
visually more appealing. Additional examples of comparison
between the different gradient estimation methods will be
presented in the next section.

In Fig. 6, we present another example for the comparison
between results of the proposed method (Sect.2) in combination
with different gradient estimation methods. Apparently, this test
image is more complex, which contains not only human faces,
but also entire human bodies with some complex background.
From Fig. 6, it can be seen that the proposed gradient estimation
method produces the most visually appealing results, with more
details. For example, the face and elbows of the person on the left
is very obvious in (b), while it is unclear or partially missing in
(c).
4.2 More Examples from the Proposed Method
We tested our method with more than 100 images containing
human faces plus some other types of images. Some examples
and the corresponding pencil sketch drawings are shown in Fig.
7. It can be seen that our method is good at not only human face
images but also other types of images such as the architecture
image. For a full-sized version of the examples presented here
and for more examples of other images, please visit the following
Web site:

http://www.public.asu.edu/~jzhou19/pencil.htm
5. CONCLUSION

This paper presents a method for automatically generating pencil
sketch drawings from photos. A gradient estimation method has
also been proposed, which was found to serve the purpose better
than the conventional methods. Experimental results show that
the proposed methods are suitable for face images as well as other
types of images.

6. REFERENCES
[1] Georges Winkenbach and David H. Salesin. “Computergenerated pen-and-ink illustration”. In Andrew Glassner, editor,
Proceedings of SIGGRAPH’94, pp. 91-100, Orlando, Florida,
July 1994.
[2] John Lansdown and Simon Schofield. “Expressive rendering:
A review of nonphotorealistic techniques”, IEEE Computer
Graphics and Applications, 15(3), pp. 29-37, May 1995.

[3] Micheal P. Salisbury, Sean E. Andersion, Ronen Barzel, et al.
“Interactive pen-and-ink illustration”. In Andrew glassner, editor,
Proceedings of SIGGRPATH’94, pp. 101-108, Orlando, Florida,
July 1994.
[4] Michael P. Salisbury, Micheal T. Wong, John F. Hughes, et
al. “Orientable textures for image-based pen-and-ink illustration”.
In T. Whitted, Editor, Proceedings of SIGGRAPH’97, pp. 401406, Los Angeles, California, 1997.
[5] Oliver Deussen and Thomas Strothotte. “Computer-generated
pen-and-ink illustration of trees”. Proc. SIGGRPATH 2000, pp.
13-18, New Orleans, Louisana, July 2000.

(a)

[6] Jiatao Song, Zheru Chi, Jilin Liu and Hong Fu, “Automatic
generation of pen-and-ink drawings from photos”, Proc. IEEE
International Conf. on Image Processing, 2004.
[7] J. Canny, A Computational Approach to Edge Detection,
IEEE Transactions on Pattern Analysis and Machine
Intelligence, Vol 8, No. 6, Nov 1986.

(b)

(c)

(d)

Figure 7. Other examples from the proposed method.

Figure 6. (a) Original image; (b) Results using the
proposed gradient estimation method;. (c) Results based
on the Laplacian operator; (d) Results based on the Sobel
operator. Note that all the results are generated by the
method proposed in Sect. 2, except that the gradient
estimation is computed using different approaches.

FAST GPU IMPLEMENTATION OF LARGE SCALE DICTIONARY AND SPARSE
REPRESENTATION BASED VISION PROBLEMS
Pradeep Nagesh, Rahul Gowda and Baoxin Li
Arizona State University
ABSTRACT

2.1. GPU Architecture and Software Framework

Recently, Computer Vision problems like Face Recognition and
Super-Resolution solved using sparse representation based
methods with large dictionaries have shown state-of-the-art results.
However such methods are computationally prohibitive for typical
CPUs, especially for a large dictionary size. We present fast
implementation of these methods by exploiting the massively
parallel processing capabilities of a GPU within a CUDA
framework, owing to its easy off-the-shelf availability and
programmer friendliness. We provide details of system level
design, memory management and implementation strategies.
Further, we integrate the solution to the preferred scientific
computational platform - MATLAB.

Index Terms— Sparse representation, GPU-based
computing, CUDA, face recognition, super-resolution.
1. INTRODUCTION
A recent trend in parallel computation has been the increased
adoption of GPUs (Graphical Processing Units) for massively
multithreaded scientific computing. They feature many-core
architectures with very high memory bandwidth that maximizes
parallel computing performance and hence are used as compute
coprocessors in tandem with host CPUs. The spurt in recent
activity in this area began with the opening-up of parallel
functionalities of the GPU by manufacturers. NVIDIA has released
CUDA (Compute Unified Device Architecture) as a programming
model to harness the GPU for general purpose computation [3]. It
is a software platform with extensions to the C language allowing
users to manage device memory, write multi-threaded code and
perform synchronization. Such a framework provides an excellent
platform for fast implementation of algorithms with high
arithmetic intensity, provided they possess structures friendly
enough for parallelizing the computations [3,4,5]. Many image
processing and pattern recognition algorithms fall into this
category and have been implemented in CUDA [1, 2].
The recent theory of Compressive Sensing (CS) and sparse
representation [6, 7] has shown state-of-the-art performance in
computer vision applications like Face Recognition (FR) [8] and
Super Resolution (SR) [9]. However they are computationally
prohibitive for practical applications because of (i) the use of largesize dictionary of dense data and (ii) the enormous computation
involved in CS or sparse solvers. In this paper we focus on a
CUDA implementation of such FR and SR problems in a largescale CS framework and show results of the speedup obtained with
respect to a CPU.

2. BACKGROUND AND PROBLEM DEFINITION

978-1-4244-4296-6/10/$25.00 ©2010 IEEE

First we briefly describe the GeForce 9800 GT which is a midrange NVIDIA GPU used in our implementation. It has 112 scalar
processors (SP), grouped in units of 8 together into a streaming
multiprocessor (SMP). Each SMP has 8192 registers, 16 KB of
local memory shared by all SPs and caches. The SMPs
communicate via a high bandwidth global device memory and the
GPU communicates with the host through the PCI express bus.
Higher-end GPUs differ mainly in the number of SMP units but
possess the same architecture.
The CUDA architecture is based on a scalable SIMT (Single
Instruction Multiple Thread) model [3]. In brief, the task is split
into a grid of blocks which in turn consists of multiple threads.
Applications conducive to porting onto CUDA are those that are
computationally intensive and have high arithmetic intensity.
Substantial computation has to be performed on any data read from
memory to amortize memory access costs. CUDA provides
accelerated libraries for dense matrix multiplications and other
basic linear algebra operations (CUBLAS [4]) which are very
useful for vision applications.

2.2. Compressive Sensing and Sparse Solvers
We now provide some background on CS, CS Solvers and
their implementations. CS is a recent approach which has been
proven powerful in solving under-determined inverse problems [6,
7]. According to CS, if ࢞ ‫ א‬Թ୒ is a real-valued signal that has
sparse representation in a basisࢸ ‫  א‬Թ୒ൈ୏ and we have a sensing
systemࢶ ‫  א‬Թ୑ൈ୒, with ‫ ܯ‬൏ ܰ such that ࢶ and ࢸ are
incoherent, then from a measurement of ࢞ defined as
࢟ ൌ ࢶ࢞ǡ࢟ ‫  א‬Թ୑

(1)

one can recover ࢞ from ࢟ as long as ‫ ܯ‬ൌ ܱሺܵሺܰȀܵሻሻ. Here ܵis
the sparsity of ࢞ in the basisࢸ [6]. Let࡭ ൌ ࢶࢸ, then the
reconstruction is based on any of the l1 optimization:
ෝ ൌ ܽ‫݊݅݉݃ݎ‬ԡࢻԡଵ ‫ݏ‬Ǥ ‫ݐ‬Ǥ࢟ ൌ ࢶࢸࢻ ൌ ࡭ࢻ (2)
(BP/LP)
ࢻ
ෝ ൌ ܽ‫߬݊݅݉݃ݎ‬ԡࢻԡଵ ൅ ͲǤͷԡ࢟ െ ࡭ࢻԡଶଶ
(BPDN/QP) ࢻ
(3)
where ߬ is a regularization parameter. There are several methods in
the literature to solve BP or BPDN. Generally, cast as a linear
programming (LP) problem, BP is very expensive for practical
purposes. A package like l1-magic [11], is certainly not feasible to
be used when ࡭ is large, one bottleneck being matrix-matrix
multiplication (࡭் ࡭). Recent interests have been on solving the
unconstrained version of BPDN in (3) which is cast as a quadratic
program (QP) and is much faster than BP as it involves only
matrix-vector multiplications (࡭࢞, ࡭் ࢟). Examples of solvers
include GPSR [12] and SpaRSA [10], with SpaRSA being the
recent algorithm that is reported to perform faster than others.

1570

ICASSP 2010

CPU

GPU
if(INIT) {
1.Read Arg’s

A

MEX
Call

2. cuda
Malloc(A)
Mex(i)
3. Memcpy
CPU Æ GPU
4.Return }

GPU
if(INIT) {
1.Read Args
2.Do All
CUDA
Mallocs
3. CPUÆ GPU

else {…}
RET
Persistent,
But, Integrity
Destroyed

A

B

4. Register
Call-back
mexAtExit()
Clean all
Persistent,
CUDA
+ Integrity
Mallocs
Maintained
5.Return }

if(INIT)
else {
1.Compute

A

2. cuda
Malloc(B)

B

3.Kernel
<<< >>>
4.Return }

Left

MEX
Call

if(INIT)

A
o=
Mex()

else {
1.Compute
2.Kernel
<<< >>>

RET

B

3.Return }

Right

Fig 1: Division of flow into Initialize and Compute phases. In
Compute, GPU memory is persistent. Left: integrity is destroyed
(prone to memory leaks). Right (the proposed scheme):
integrity is maintained; Further, it is freed through call-back
function only on termination of the MEX process.

2.3. FR and SR: Large Scale CS Problems
The FR and SR algorithms implemented in this paper are based
on a CS/Sparse Representation framework assuming overcomplete
redundant dictionaries. In the below, we outline the FR algorithm
from [8] and the SR algorithm from [9].
FR: Consider ‫ ܮ‬distinct face classes with each having ݊ training
samples. Let ࢞௜ǡ௟ ‫ א‬Թ୒ be the vectored ith sample of lth class. A
matrix ࡭௟ is formed corresponding to lth class as ࡭௟ ൌ
ൣ࢞ଵǡ௟ ࢞ଶǡ௟ ǥ࢞௡ǡ௟ ൧ ‫ א‬Թ୒ൈ୬ and the matrix for the entire training set
is࡭ ൌ ሾ࡭ଵ ࡭ଶ ǥ࡭௅ ሿ ‫ א‬Թ୒ൈ୬୐ . Let ࢝ ‫ א‬Թ୒ be a test image whose
class is unknown. The classification is done in three steps:
1.

2.
3.

ෝ ൌ ܽ‫߬݊݅݉݃ݎ‬ԡࢻԡଵ ൅ ͲǤͷԡ࢝ െ ࡭ࢻԡଶଶ
Solve: ࢻ
ෝ ሻԡ૛
Compute Residuals: ‫ݎ‬௟ ൌ ԡ࢝ െ ࡭ࢾ௟ ሺࢻ

‫ݕݐ݅ݐ݊݁݀ܫ‬ሺ࢝ሻ =ܽ‫݊݅݉݃ݎ‬௟ ‫ݎۃ‬௟ ሺ࢝ሻ‫ۄ‬

In 2, ࢾ௟ ǣ Թ୬୐ ՜ Թ୬୐ selects n coefficients belonging to class݈.
Note that the ࡭ matrix is equivalent to the “basis” ࢸ in equation
(3) and measurement is ignored.).
SR: SR deals with the inverse problem of obtaining a high
resolution image (ࢄ) from a given low resolution image (ࢅ). They
are related by a blurring and decimation operation asࢅ ൌ ࡸࢄ. In
[9] ࢄ is found in small patches, by casting the inverse problem in a
CS framework so that࢟ ൌ ࡸ࢞, where ࢞ ‫ א‬Թࡺ ǡ ࢟ ‫ א‬Թࡹ are pairs of
vectorized image patches respectively. Here‫ ܯ‬ൌ ܷܰ, where
ܷ ൐ ͳ is the magnification factor. Hence ࢟ ൌ ࡸ࢞ is very similar to

1571

Eqn (1) except that ࡸ is a deterministic matrix. The “sparsifying
basis” ࢸ of eqn (3) isࡰࡴ , an overcomplete dictionary of high
resolution image patches and ࡰࡸ ൌ ࡸࡰࡴ is corresponding low
resolution dictionary. The algorithm first estimates the high෡ by looping for all ࢟ patches:
resolution imageࢄ
1. Extract a patch ࢟ from ࢅ with one pixel overlap at top
and bottom. Scan: left to right, top to bottom.
ෝ ൌ ܽ‫߬݊݅݉݃ݎ‬ԡࢻԡଵ ൅ ͲǤͷԡ࢟
෥ െ ࡭ࢻԡଶଶ
2. Solve: ࢻ
෡.
ෝ ൌ ࡰு ࢻ and updateࢄ
3. Compute estimate of࢞:࢞
ࡲ࢟
ࡲࡰࡸ
෥ ൌ ൤ ൨ and࡭ ൌ ൤
In 2, ࢟
൨, where ࡲ operator extracts
ࡼࡰு
෡
ࡼࢄ
෡ . Then,ࢄ
෡ is
feature, ࡼ extracts overlap between current patch andࢄ
refined using a back-projection algorithm [9].

2.4.Need for CUDA Implementation- Problem Definition
In both the above algorithms, the most computationally expensive
portion is the CS solver since ࡭ is a large dense matrix. Recently, a
GPU implementation of SpaRSA was done in [13], but is suitable
for only certain class of ࡭ matrices obtained by randomly choosing
rows of a DCT or DFT matrix. Such an implementation obviates
the need to transfer the entire ࡭ matrix to the GPU as only the
indices of the chosen rows would suffice. This reduces magnitude
of data transfer from ܱሺܰ ଶ ሻ toܱሺܰሻ. Further fast implementation
of ࡭࢞ and ࡭் ࢟ can be done using FFTs in CUFFT [5] library.
However this will not generalize for FR/SR (where ࡭ is a generic
large matrix). Further, it involves greater memory access/transfer
and also matrix-vector multiplications of complexityܱሺܰ ଶ ሻ. In
this regard, we propose and implement a full FR/SR system, with
enhanced memory management and multithreading scheme for the
large-scale solvers by building on top of GPU implementation of
[13] and integrate with the preferred MATLAB platform (for its
highly-optimized linear algebra subroutines).

3. SYSTEM DESIGN
With regard to the CS solver, our goal is to (i) harness the power of
matrix-vector multipliers like cublasSgemv (CUBLAS level-2
routines [4]) for multiplications involving ࡭ and (ii) minimize the
effect of ܱሺܰ ଶ ሻ CPU-GPU transfer time and amortize its effect
with greater computation inside GPU. Further, we also provide
novel implementation strategies to optimize specific steps involved
in FR/SR, like (i) calculation of residual in Step 2 for FR, and (ii)
better memory management for matrix ࡭ in SR.

3.1. Memory Schematic for Dense Matrix in CS Solvers
We implement the computationally intensive part of the
algorithms in CUDA (GPU) and interface it using a MEX
(MATLAB) framework using CUDA-MEX tools. In this regard,
even though the BPDN decoder is offloaded from the CPU to the
GPU, significant speedup will not be observed, as repeated
invocations involves the transfer of the large ࡭ over a much slower
bus to the GPU. Thus we partition the tasks to minimize such
transfers to achieve speedup. We note that in FR, ࡭ is the training
matrix that is constant across different test images. Similarly, in
SR, the ࡲࡰࡸ part of ࡭ matrix is the same for all patches andࡼࡰு
depends on the patch position. Thus one solution is to organize the
flow into ‘Initialize’ and ‘Compute’ phases, where Initialize is
done once and includes the complete transfer of large ࡭ matrix to
GPU. The Compute phase may consist of repeated GPU kernel
invocations or computations, and possibly small CPU-GPU data
transfers. However, we need a mechanism to keep ࡭ matrix
persistent inside GPU memory. This is not a straightforward task

for implementing on MEX as in every invocation, access to
memory regions of previous invocations is undefined on the GPU.
In case of CPU, persistence can be handled using routines like
mexMakeArrayPersistent(),but for GPU there is no such
API. Fortunately, the GPU device memory contents are not
destroyed once allocated unless overwritten. Hence we allocate all
necessary memory on the device only during Initialize phase with
handles to these locations in global variables. This ensures both
memory persistence and integrity over repeated calls as long as no
additional GPU memory is allocated during the Compute phase. To
prevent memory leaks the GPU memory should be freed only at
the time the MEX process is terminated. This is achieved by
registering the GPU memory free functions to the
mexAtExit()call back function. Figure 1 illustrates this system
level design concept. Note that we only need to be concerned with
memory integrity within the CUDA programs.

3.2. CUDA: FR Specific Optimizations
In case of FR, ࡭ is always constant. Thus we implement Step 1
of the algorithm in GPU as discussed earlier. Step 2 for calculating
residuals also consumes significant time if implemented in CPU
alone. The residuals are calculated as a series of l2 norms between
the test image ࢝ and the matrix-vector product involving the subෝ ሻ of Step 2). With
matrices ࡭࢒ and the corresponding ࢻ࢒ (ࢾ௟ ሺࢻ
regard to the sub-matrix-vector multiplication we note that (i) ࡭࢒ ࢻ࢒
can be done in parallel for different ࢒ƍ‫(ݏ‬subjects) and (ii)ࢻ࢒ is
sparse and thus multiplication can be further optimized. We design
a kernel taking advantage of these two aspects, as shown in Figure
2. The submatrix-vector multiplication is performed as,
(4)
࢓࢒ ൌ σ࢔࢐ୀ૚ ࡭࢒ ሺ݆ሻࢻ࢒ ሺ݆ሻǡ ࢓࢒ ‫ א‬Թ୒
where ࡭࢒ ሺ݆ሻis the ݆th column vector of ࡭࢒ (݈ th training image) and
ࢻ࢒ ሺ݆ሻ is ݆th scalar coefficient. However the key is to perform the
multiplication only if ࢻ࢒ ሺ݆ሻ is non-zero. As shown in Figure 2(a),
multiple BLOCKS take over multiple-parts of one column and
each thread in a block runs over corresponding pixels of ࢔
columns. A thread computes an element of ࢓࢒ ‫ א‬Թ୒ as per Eqn
(4). It has a branch condition depending on zero condition ofࢻ࢒ ሺ݆ሻ.
Since ࢻ࢒ ሺ݆ሻ is either zero or not for one-column࡭࢒ ሺ݆ሻ, there is no
room for branch divergence within a thread-warp inside a block
and hence the scheme is still efficient. See Figure 2(b) for details.
The residual can be calculated as࢘࢒ ൌ ࢓࢒ െ ࢝, and then stored in
the global memory within the same kernel. Finally, ‫ݎ‬௟ ൌ ԡ࢘࢒ ԡ૛ is
computed separately using norm2 kernel of [13].

࡭ଵ

ࢻ଴

࡭ଶ

ࢻଵ

Transpose of ࡭࢒ Matrix
࢔
Image Vector Length (N)

ࢻ௅

࡭ࡸ

BLOCK_LEN
BLOCK

1

__shared__ float partial_prod[BLOCK_SIZE];
__shared__ float cache[BLOCK_SIZE];
partial_prod[tx] = 0.0f;
__syncthreads();
j = __mul24(blockIdx.y,n);//cur image in Al
while(j < __mul24(blockIdx.y,(n+1)))
alp = alpha[j];
if(0 != alp){

}
}

rl_Blk[tx] Å w_Block[tx] -partial_prod[tx];
syncthreads();
(b)

Fig 2: In FR (a) shows the schematics of kernel to calculate
residuals (step 2 in sec II.C). This calculates only the ࡭࢒ ࢻ࢒ െ ࢝
part of it and norm2 is done separately (b) pseudo-code of kernel
(note: storage of ࡭ is in column major format)
other storages. To circumvent this, we allocate the maximum
possible memory for one ࡭ matrix and pre-copy the unchanging
ࡲࡰࡸ matrix using memory-copy routines. The appropriate ࡼࡰு
matrix is appended in the Compute phase according to the region
(see Figure 3(b)). However, to minimize such copies we reorder
the scan direction as shown in Figure 3(c). This reduces the
number of such memory copies to a fixed value of 3 fromʹ ൈ ܴ,
whereܴ is the number of rows of patches in the image. Another
important strategy to discuss is about the storage of ࡭ and
multiplications ࡭࢞ and࡭் ࢟. Since MATLAB has column major
storages (and CUDA, C/C++ assumes row-major), the two

2

1572

{

cache[tx]Å AlCURRENT_BLOCK[tx];
partial_prod[tx]+= cache[tx]* alp;
j++; // j covers “n” images in Al

3.3. CUDA: SR Implementation

In SR the ࡭ matrix is a concatenated version of ࡲࡰࡸ and ࡼࡰு
matrices where ࡼࡰு changes depending on patch overlap location.
In this regard, (refer to Figure 3(a), (c)) we may identify four
regions of operation (for patches) on the (1) top-left corner, (2) left
edge, (3) top edge, and (4) elsewhere. Patch extraction and
corresponding overlap for these regions are depicted in Figure 3(a).
Hence, within each region, the ࡭ matrix remains the same. This
means that if we were to process patches in normal raster scan
order (left to right, top to bottom), the ࡭ matrix changes at least
twice every row. One naïve method could be to store different ࡭
matrices for each region, but this is redundant and prohibitive. As a
typical example for 3x3 patch size (low-resolution), ࡲ being
gradient feature extraction (in four directions) and dictionary size
of 100,000 atoms, ࡭ requires 148MB of storage. In case of
RGB/YUV color images, we need three such components and use
over 1 GB for all regions which leaves little if any memory for

Non-Zeros

(a)

3

4

(a)
1 2
3 4

ሺࡲࡰ௅ ሻ்
ሺࡼࡲࡰு ሻ்
࡭்

(b)

(c)

Fig 3: In SR (a) image 3x3 patch extraction and regions of
operation based on overlap (b) The allocation of ࡭ as a
combination of constant and overlap parts; its update depending on
region. (c) Scan direction resulting in 3 updates in ࡭for an image.

matrices cannot be simply concatenated as a columns of ࡲࡰ௅ and
ࡼࡲࡰு would be sandwiched between two consecutive columns
of࡭. Instead,࡭,ࡲࡰ௅ and ࡼࡲࡰு are transposed and stored (using
a transpose kernel) in which case the stride-length will be always
equal to the width of ࡭ which is constant across all regions (see
Figure 2 (b)). So the multiplications ࡭࢞ and ࡭ࢀ ࢟ are done
with࡭ࢀ storage but using ‘Transpose’ and ‘Normal’ options in
cublasSgemv()respectively[4]

only 112 cores, the setup readily scales with higher-end CPUs and
GPUs. This makes such computationally intensive algorithms
viable for practical use.

4. SIMULATION RESULTS
We use a mid-range CPU (Pentium IV, 2.8 GHz, 1GB RAM) and
mid-range GPU (GeForce 9800 GT). We implement the CPU and
GPU code in ‘single’ precision float and compute the average
times required by the CPU and GPU for different sizes of࡭. The
CPU time corresponds to the total execution time on MATLAB,
while the GPU time is measured based on MATLAB execution
times on the CPU, but with the BPDN decoder and other parallel
tasks offloaded to the GPU. We omit the time taken in Initialize
phase: time to transfer the ࡭ matrix to the GPU in GPU code and
time for preparation of ࡭ in CPU code. The timing results are
obtained by averaging over 10 repeated runs for consistency.
For FR we fix 10 training images per subject, but increase the
number of subjects (hence face samples in the training set) and
ascertain the time required to identify a match. For lesser number
of samples the performances are almost similar. However, the GPU
execution times are almost constant, which is because the time for
transfer of ࡭ predominates over the computations. We see a
speedup in the range of 30x for cases involving 1500 samples as
shown in Figure 4. The size of the face images is 32x32.
In SR, the average execution time for a 3x3 patch and an
upscaling or magnification factor of 3 is depicted in Figure 5 for
varying sizes of the dictionary. We observe an average speed-up of
4x. On a direct comparison with FR performance speed-up, this is
seems inferior, but for an application like SR this can be
considered significant even though both employ the same BPDN
decoders as explained in the following discussion.
It is important to note some important differences in the two
applications. First, the choice of ߬ differs (Eqn 3) since the goals
are entirely different. For FR, fidelity in reconstruction is not
important (i.e accuracy in value of non-zero coefficients in ࢻ of
Eqn (2) is less important than the support). Hence a high value of ߬
can be chosen which results in lesser iterations. However for SR
since fidelity of reconstruction is important, ߬ is small, the solver
requires more iterations and CPU-GPU communication, leading to
lesser speed-up. Second, the base atoms of ࡭ (columns) are
normalized in terms of l2 norm or energy. Due so such
homogeneity, the BPDN decoder converges much faster to the
sparse solutions which not only require fewer iterations, but also
lesser computations within an iteration [10]. On the other hand in
SR, the ࡭ matrix is a dictionary of raw training image patches
hence the base atoms are non-homogeneous in energy. Hence
convergence is much harder and may require more computation
within iteration.

5. CONCLUSIONS
We provided a system level design to achieve speed-up in
implementation of large-scale dictionary-based CS problems.
Specifically we demonstrated that the speed-up for FR and SR
applications is about 30x and 4x respectively. Although we only
made use of a mid-range CPU and a mid-range GPU which has

1573

Fig 4: FR Execution times for increasing number of training images.

Fig 5: SR: Average patch processing times (sec) for a 3x3 patch and
magnification 3 for varying dictionary sizes (lengths).

6. REFERENCES
[1] V. Garcia, E. Debreuve, and M. Barlaud, “Fast k nearest neighbor
search using gpu,” in Proceedings of CVPR Workshops, June
2008, pp. 1–6.
[2] B. Catanzaro, N. Sundaram, and K. C. Keutzer, “Fast support
vector machine training and classification on graphics
processors,” in International Conf on Machine Learning, 2008.
[3] CUDA Programming Guide, Version 2.0, NVIDIA, June 2008.
[4] CUDA CUBLAS Library, Version 2.0, NVIDIA, March 2008.
[5] CUDA CUFFT Library, Version 2.0, NVIDIA, April 2008.
[6] E J. Candès and M B. Wakin “An Introduction to Compressive
Sampling”, IEEE Sig Proc. Vol. 25, Issue 2, Mar 08, pp. 21-31.
[7] D. L. Donoho, “Compressed sensing,” IEEE Trans. Inform.
Theory, vol. 52, no. 4, pp. 1289–1306, Apr. 2006.
[8] J. Wright, A. Yang, A. Ganesh, S. Sastry, and Y. Ma. “Robust
face recognition via sparse representation”. IEEE Trans. PAMI.
[9] J. Yang, J. Wright, T. Huang, Y. Ma, “Image Super-Resolution as
Sparse Representation of Raw Image Patches”, IEEE, CVPR’08,
pp. 1-8, Anchorage, AK ,23-28 June ’08.
[10] S. J. Wright, R. D. Nowak, and M. A. T. Figueiredo, “Sparse
reconstruction by separable approximation,” in Proceedings of the
ICASSP, March-April 2008.
[11] E. Cand`es and J. Romberg, “`1-MAGIC: Recovery of sparse
signals viaconvex programming,” Cal Tech, Rep.October 2005.
[12] M. Figueiredo, R. Nowak, and S. Wright. “Gradient projection
for sparse reconstruction: application to compressed sensing and
other inverse problems”. IEEE Journal on Selected Topics in
Signal Processing, 2007, Vol. 1, Issue 4, pp. 586-597.
[13] S. Lee and S. J. Wright, “Implementing Algorithms for Signal
and Image Reconstruction on Graphical Processing Units”
Accepted, Mathematical Programming Society and by
the Optimization Technology Center, Nov. 11, 2008.

Power Reduction via Macroblock Prioritization for
Power Aware H.264 Video Applications
Michael A. Baker, Viswesh Parameswaran, Karam S. Chatha, and Baoxin Li
Department of Computer Science and Engineering
Arizona State University
Tempe, Arizona 85281

{mike.baker, vparames, karam.chatha, baoxin.li}@asu.edu
ABSTRACT

1. INTRODUCTION

As the importance of multimedia applications in hand-held
devices increases, the computational strain and corresponding demand for energy in such devices continues to grow.
Portable multimedia devices with inherently limited energy
supplies face tight energy constraints and require optimization for energy conservation. Power-aware applications give
their users flexibility to prioritize and trade between performance and battery-life.
This paper introduces a power-aware technique for user
selectable power reduction in exchange for controlled reductions in video quality for H.264 video streams. The technique uses an encoder-decoder pair. The encoder characterizes video streams and provides information to the decoder
via Flexible Macroblock Ordering (FMO) by generating prioritized slice groups. The decoder selectively ignores low
priority slice groups based on user selected preference effectively reducing the decoder workload. With a reduced
computational requirement, processor voltage and frequency
scaling (DVFS) significantly improve decoder power performance within timing constraints. Our PXA270 system implementation resulted in power savings of as much as 53%
with an average PSNR per frame of 24dB compared to the
unmodified video.

Multimedia applications are among the most computationally expensive commonly found applications in portable
devices. They are also among the most power hungry. Dedicated portable multimedia devices such as MP3 players are
increasingly expected to deliver video content as well as the
traditional audio. Cell phones and handheld computers have
begun to merge into a new class of devices facing the same
demands. Consumers find devices which can provide all of
these services from telephone to personal digital assistant
to internet browsing and watching video-on-demand more
convenient than having the same services provided by multiple devices. A portable device struggling with a myriad of
tasks and a limited power budget faces numerous opportunities for power conservation by focusing effort and limiting
resources on certain tasks. The concept of Power Aware devices as presented in [12] asserts that the device should be
able to choose an appropriate power saving mode by direct
user input, based on the user’s history, or automatically by
sensing the environment.
Consider a situation where a portable multimedia device
user wants to watch a video program for the duration of airline flight, but remaining battery power only affords video
for half the duration. Under these circumstances users are
willing to sacrifice Quality of Service (QoS) in exchange for
increasing the time service is available. Power Aware applications provide users the flexibility to exercise this type of
trade-off.
Our Power Aware H.264 system provides user selectable
degrees of power saving effort in exchange for controlled QoS
reduction. Our goal is to reduce the amount of computation
required to decode the H.264 stream. We achieve this goal
by skipping carefully selected blocks during video stream decoding. Based on the number of skipped blocks, we can predict the resulting speed-up in the decoder and use Dynamic
Voltage and Frequency Scaling (DVFS) to reduce power consumption. Our H.264 encoder-decoder system classifies macroblocks in each frame into slice groups using Flexible Macroblock Ordering (FMO) described in the H.264 standard
[15]. Our prioritization algorithm determines which blocks
in each frame are the most expendable and organizes them
into slices accordingly. Slice groups corresponding to different QoS measures are then selectively omitted from the
decoding process based on user preference. We compare the
effect on QoS and power savings using two types of error concealment. Simple Copy Forward Error Concealment (CFERC) provides replacement data from the previous frame
with a very inexpensive copy based solely on macroblock

Categories and Subject Descriptors
C.4 [Performance of Systems]: [measurement techniques,
performance attributes]; K.8.1 [Personal Computing]: Application Packages—graphics

General Terms
Performance, Measurement

Keywords
H.264, MPEG4, Video, Low Power, Power Aware, Voltage
Scaling, Frequency Scaling

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
CODES+ISSS’08, October 19–24, 2008, Atlanta, Georgia, USA.
Copyright 2008 ACM 978-1-60558-470-6/08/10 ...$5.00.

261

location. Motion Vector Error Concealment (MV-ERC) is
more computationally expensive, but provides substantially
improved QoS.
The experimental results in Section 5 demonstrate that
both techniques are capable of enabling significant power
savings of up to 53% and 29% compared to the fully decoded video stream. In some cases, particularly using Motion Vector error concealment, the power savings are significant, while degradation to QoS is minimal. Significantly,
our encoded stream is also compatible with standard H.264
decoders without support for our slice dropping scheme.
Next we present some relevant background on H.264 and
QoS concepts. Previous work will be discussed in Section 2,
in Section 3 we introduce the encoder-decoder system, and
discuss modifications to the JM version 12.4 reference encoder and decoder [2]. Our experimental setup is explained
in Section 4 followed by our results and conclusion.

to improve video streams’ robustness against packet loss. In
[11], a priority drop scheme manages prioritized data flow
on a network for frame dropping, and dynamically variable
coefficient quantization enabling graceful QoS degradation
under variable network conditions. Our encoded stream is
suited for similar packet prioritization, but without necessary modifications to the video coding standard. Additionally, reducing quantization levels in the decoder does not
provide significant computational savings relative to block
or frame dropping, and slice dropping provides better resolution version of frame dropping. [6] analyzes the video
stream in the compressed domain, pruning the compressed
data based on a model which estimates the effect on distortion. This scheme is aimed at mobile devices on a network
where each device has limited computational ability considered by the video server which appropriately modifies the
compressed data stream. The transcoding scheme focuses on
computational constraints while minimizing the introduced
distortion, but it relies heavily on frame dropping and does
not provide measured distortion injection in contrast to our
approach.
Video decoding workload is highly variable. Significant
effort has gone toward predicting workload for the purpose
of DVFS for power optimization. [14] gives two methods for
predicting workload in MPEG streams in order to effectively
scale frequency and voltage. [3] describes a method for dynamically choosing IDCT algorithms in order to exchange
quality for energy. In [5] three techniques are introduced
for taking advantage of multimedia applications tolerant to
deadline misses enabling opportunities for DVFS. Here, data
points are dropped from the stream exchanging quality for
power savings; however, unlike our scheme, the impact of
individual data points on QoS is not considered.

1.1 H.264
The H.264/Advanced Video Coding (AVC) codec is part
of the MPEG-4 suite of multimedia standards. The first
revision of the standard, completed in May 2003, sought to
address increasing demand on video bandwidth associated
with growing services like High Definition Television and
internet based streaming video applications [15].
H.264 implements a number of improvements over previous video codecs which collectively result in reductions of up
to 50% in bandwidth requirements for the same level of image quality. The savings result from extensive analysis and
optimization in the encoder which serves to minimize redundancy in the video stream, but comes at a significant cost
in computational complexity. The H.264 decoder’s complexity is about 2.4 times that of a comparable H.263/MPEG2
decoder [4].
Reduced bandwidth makes H.264 bitstreams particularly
useful for hand-held applications using video streams transmitted over a wireless network where bitrates are limited and
bandwidth comes at an absolute premium. In addition, storage capacity in a mobile device may also be limited. Unfortunately the increased computational complexity that comes
with reduced bandwidth has obvious negative consequences
for portable devices relying on battery power supplies.

3. POWER AWARE H.264 APPLICATION
We implemented our encoder-decoder pair by modifying
the JM H.264/AVC reference encoder and decoder version
12.4. Our goal was to free the decoder from the maximum amount of computation while minimizing the degradation introduced into the decoded video stream. We accomplish this by selectively skipping or dropping from the video
stream blocks which introduce the least amount of distortion into the video output. The decoder need not perform
dequantization or inverse integer transform operations on
the dropped blocks, effectively reducing the time required to
decode a given video stream. The resulting speedup enables
application of Frequency and Voltage Scaling for potentially
significant reductions in power consumption.
Our encoder generates H.264 video streams inserting an
I frame every twelfth frame from which no blocks will be
dropped followed by eleven P frames with prioritized blocks.
We use the Group of Pictures (GOP) sequence:

1.2 Quality of Service
For our purposes, QoS is objectively defined as the level
of distortion introduced into a video at the individual frame
level and across several frames as a result of discarding residual data from macroblocks. We use Peak Signal to Noise
Ratio (PSNR) calculated between the unmodified video and
the same video with discarded data. We consider a PSNR
value around 25dB to be acceptable for a low power mode.
Values close to 35dB and higher are considered acceptable
for more expensive computations. This range corresponds
with the expected PSNR performance of a lower quality low
bit-rate video stream vs. a high bit-rate [3]. More details are
provided in Section 3.3. The Video Quality Measurement
Tool from MSU is used to collect PSNR data for individual
video frames [13].

2.

{I, P, P, P, P, P, P, P, P, P, P, P }
The I frame uses intra-coding exclusively so that no error introduced during block dropping will propagate beyond this
frame. The P frames in the sequence use inter-coding in
which blocks from each P frame may be reconstructed using data from the previous I or P frame in the video sequence. These inter-coded frames will propagate errors from
one frame to the next when a motion vector points to areas
in the previous frame impacted by dropped blocks.

PREVIOUS WORK

Several publications have focused on the effects of network
packet loss on streaming video quality, methods for recovering from packet loss and encoder optimizations designed

262

At this point we avoid introducing B frames which use
motion vectors in both the forward and reverse direction
into the data stream in order to simplify analysis of the
decoded video stream. However, adding B frames to the
encoded stream in the future will improve bandwidth efficiency, and reduce the propagation of distortion introduced
by dropped macroblocks. Since B frames are not normally
used as reference frames for Copy Forward or Motion Vector Copy operations, blocks dropped from B frames will not
propagate at all. Additionally, inserting a B frame between
two P frames means that the effect of distortion spreading
to multiple blocks through multiple motion vectors is limited to one stage over three frames instead of two as is the
case with a sequence of three P frames. Adding B frames to
the stream does; however, increase the computational load
on the decoder, potentially reducing power efficiency.

and the block in the same location from the previous frame,
MBnm−1 is given by

=

1
162

MSE (MB m
n )
m
m−1
||M
B
(i,
j) − M B n
(i, j)||2
n
j=0

P15 P15
i=0

(1)

where A(i, j) is the Y component of the pixel at position
(i, j ) in the 16 × 16 pixel block A. We derive the frame
distortion from macroblock dropping, Dm from the sum of
MSE (MBnm ) values across all n macroblocks in the frame
Dm =

n−1
X

M SE(M B m
i )

(2)

i=0

The total distortion of frame m, D̂m can be estimated as
the sum of the distortion due to dropped macroblocks added
to the distortion already present and propagated forward
from the previous frame

3.1 Error Concealment
H.264 includes provisions for error concealment to minimize video quality degradation in the event of lost data
such as dropped network packets or missed decoding deadlines. Error concealment functions typically replace an entire missing frame or block with buffered data from a previously decoded frame. In the JM reference software, “conceal
by copy” replaces a missing block with its predecessor from
the previous frame. A “conceal by trial” replaces the missing
macroblock after evaluating the surrounding blocks to find
one whose motion vector points to data minimizing the distortion at the edge of the missing block. These two actions
loosely correspond to the error concealment actions taken
by our two decoder schemes. We use “conceal by copy” for
error correction in the simple CF-ERC decoder, and a modified version of “conceal by trial” for the MV-ERC decoder.
Since we selectively drop available blocks in the decoder, any
motion vector data calculated for a given macroblock is still
available to us. This provides the opportunity to replace the
macroblock using its motion vector data rather than a simple copy forward. This method is guaranteed to introduce
the minimum amount of distortion into the video stream
as the unmodified H.264 motion compensation algorithm in
the encoder has carefully selected the motion vector data for
each block to do exactly that.

D̂m = Dm + D̂m−1

(3)

Since our system does not drop macroblocks from I frames,
and I frames are decoded independently without inter-coding,
the distortion in any I frame is considered to be zero. Thus,
the total distortion, D̂ present in the first P frame, m following an I frame is D̂m = Dm . We can adjust for the attenuation of distortion from the previous frame, D̂m by multiplying by an empirically determined scaling factor 0 ≤ α ≤ 1
before adding Dm giving
D̂m = Dm + α · D̂m−1

(4)

3.3 Encoder Modification
The modified JM12.4 reference encoder prioritizes and orders blocks within each frame according the amount of distortion, MSE (MB ) we would introduce into the stream if the
block were dropped by the decoder. Blocks with the smallest
MSE are considered more expendable, and are placed at the
beginning of the list. Blocks with larger MSE are considered
more important and are placed at the end of the list.
After prioritizing the macroblocks, the encoder begins dividing the blocks into 6 slice groups numbered from 0 to
5. The most expendable blocks are placed in slice group 5
which will be the first slice dropped in the decoder. The
most important blocks are placed in slice group 0 which is
never dropped in the decoder. Each slice group has associated with it an acceptable level of total distortion, D̂m
which acts as a threshold when assigning macroblocks to
slice groups. The threshold is estimated from user selected
Y -component PSNR values for each slice group. PSNR is
given by

3.2 Error Propagation
Error introduced in one video frame decreases as it propagates forward due to leakage in the prediction loop [9]. Although distortion introduced through macroblock dropping
need only be considered over the course of a window of several frames [9], errors for a given frame can be approximated
as the sum of the propagation errors since the last I frame
[10].
Errors introduced through macroblock dropping propagate to the next frame each time a macroblock in the following frame references the dropped block via motion vector.
All or part of the distortion caused by the dropped macroblock is carried forward when each dependent block in
the following frame is reconstructed from its correct residual data added to the distorted data present in the previous
(reference) frame.
We define the amount of error or distortion introduced
into the video stream by a single macroblock, MB, in terms
of Mean Squared Error (MSE ). For our purpose, MSE defined between the nth block in the current frame m, MBnm


MAX 2
PSNR = 10 · log10
(5)
MSE
where MAX is the maximum pixel value, in our case 255.
Given a desired value for PSNR, we can calculate the associated MSE by


MAX 2
(6)
10PSNR/10
Since this value is an average across all macroblocks in
one frame, we multiply MSE by the number of macroblocks
MSE =

263

Glencoe Idle Power Consumption
With Frequency Scaling, 2N = 3
Power (mW)

140
130
120

550
500
450

110
100
90
80
70

400
350
300
250
200

Frequency
Power

Frequency (MHz)

in the frame to find the allowable distortion or distortion
threshold for slice group s, Ds . For CIF video, 352 × 288
gives us 22 × 18 blocks, or 396 macroblocks per frame, and
Ds = 396 · M SE.
The encoder builds slice groups for each frame starting
at the low distortion end of the prioritized macroblock list
and adding macroblocks to the lowest priority slice group as
long as the sum of their distortions does not exceed the previously calculated distortion threshold for that slice group.
The result is a slice group for each distortion threshold such
that dropping any slice and all higher numbered (more expendable) slices results in a level of distortion described by
the distortion threshold for the dropped slice. During this
process, the encoder also checks each macroblock’s distortion in all three components (YUV ) against an individual
macroblock threshold and immediately adds failing blocks to
slice 0. This check prevents the encoder from adding blocks
when plenty of room for additional distortion is available to
an expendable slice group, but adding it would introduce
obvious artifacts into the video stream.

150

60
25

24

23

22

21

20

19

18

17

16

15

14

L value

Figure 1: Glencoe DVFM Characteristics
Table 1: Speedup and DVFM frequency for decoder
in 1-slice mode for CF and MV decoders.
CF
MV
Video
Speedup f(MHz) Speedup f(MHz)
akiyo cif
25%
351
9%
429
container cif
57%
312
16%
409.5
highway cif
43%
331.5
6%
448.5
soccer cif
30%
370.5
5%
448.5
tempete cif
65%
292.5
14%
429
waterfall cif
57%
312
17%
409.5

3.4 Decoder Modification
We modified the JM12.4 reference decoder to drop slices
from the modified H.264 stream in accordance with a user
selected mode. The user chooses a desired level of QoS performance in order to reduce power consumption in the decoder by selecting the number of slice groups to keep in the
stream. For example, placing the decoder in 6 slice mode
will not drop any slices. On the other hand, placing the decoder in 1 slice mode will drop slice 1 and any higher numbered slices, decoding only the most important slice, slice
0. Based on the user selected mode, the decoder performs
error concealment to replace data in the skipped slices. We
implemented two decoders—one using macroblock CF-ERC
and the other using MV-ERC as described in section 3.1.
The decoding loop treats I frames and slice groups numbered smaller than the mode number normally, reading and
decoding each macroblock. Slice groups with identification
numbers equal to or greater than the mode number are
skipped. The CF-ERC decoder does not read or decode
blocks in these slices so that they are treated as lost by the
error concealment function. The error concealment function is modified to ensure that conceal by copy is the only
method used for error concealment, avoiding the conceal by
trial function to minimize computational complexity for performance reasons. The MV-ERC decoder does read each
macroblock in a dropped slice in order to obtain the associated motion vector data, but the coefficient data is not read,
and the block is never decoded. Each block from a dropped
slice is marked as lost in order to trigger error concealment,
and a modified version of the conceal by trial function is
used to recover the motion vector data for each marked macroblock copying data from the previous (reference) frame in
accordance with the motion vector information.
In our testbed, preliminary results indicated that using
motion vectors for error concealment in all three video components (YUV ) was so computationally expensive that it
wiped out any savings achieved through avoiding integer
transform and dequantization operations. The process of
rebuilding blocks by performing motion compensation can
take 55% of the decoder’s effort [1]. As a result, we use
motion vector error concealment only for the Y component,
and simple copy forward to replace dropped UV data.

The CF-ERC decoder provides better power performance
than the MV-ERC alternative. In addition to skipping block
decoding, it avoids the expenses of recovering motion vector
information from the video stream and later dereferencing
it to perform error concealment. However, the improved
power performance implies reduced QoS. Although the encoder classifies blocks based on their distortion with respect
to the same block (same position) used in CF-ERC, the MVERC method will regularly reconstruct a replacement block
with less distortion resulting in improved QoS as discussed
in Section 3.1.

4. EXPERIMENTAL SETUP
We generated H.264 video streams for 16 benchmark videos
using our modified JM12.4 encoder. These streams were
then decoded through the two modified JM12.4 decoders on
a Linux server, with an Intel(R) Xeon(TM) CPU running at
2.80GHz with 4GB RAM and on the Intel Glencoe Development Platform running Linux 2.6.9 on the XScale-PXA270
rev 4, with 64MB RAM.
Power measurements were obtained for a subset of the
16 benchmark videos using the PXA270 Vcore voltage touch
point on the Glencoe Development Platform with a Keithley 2000 Multimeter sampling at 100Hz with the decoder
running each video in each mode. Voltage samples were collected and analyzed in Labview to obtain an average power
reading over several iterations of 1024 samples each. These
results are presented in the next section.
The Linux port for the Glencoe Development board includes a utility for Dynamic Voltage and Frequency Management (DVFM) [7]. Frequencies are set by giving the utility
a clock multiplier, L, and a turbo mode multiplier, 2N. The
base clock frequency of 13MHz is multiplied by L · N to
obtain the desired frequency [8].
In order to obtain a piecewise-linear power function in
terms of L and 2N, it was necessary to fix the 2N value.

264

6
5
4
3
2
1

500
450
400
350
300
250
200
150
100
50
0

slice
slice
slice
slice
slice
slice

y
aki

cif
o_

500
450
400
350
300
250
200
150
100
50
0

f
f
cif
cif
cif
_ci
_ ci
te_
er_
ay_
fall
cer
pe
hw
ter
tain
soc
tem
hig
wa
con
Video Title

y
aki

(a) Copy Forward Error Concealment

12

18

24

f
f
cif
cif
cif
_ci
_ci
te_
er_
ay_
fall
cer
pe
hw
ter
tain
soc
tem
hig
wa
con
Video Title

PSNR by Slice Mode, MV-ERC: tempete_cif

5 slice
4 slice
3 slice
2 slice
1 slice

30

PSNR (dB)

PSNR (dB)

PSNR by Slice Mode, CF-ERC: tempete_cif

6

cif
o_

slice
slice
slice
slice
slice
slice

(b) Motion Vector Error Concealment

50
45
40
35
30
25
20
0

6
5
4
3
2
1

Power Consumption by Slice Mode
Motion Vector ERC

Power (mW)

Power (mW)

Power Consumption by Slice Mode
Copy Forward ERC

36

50
45
40
35
30
25
20
0

6

12

18

24

30

5 slice
4 slice
3 slice
2 slice
1 slice

36

Frame Number

Frame Number

(c) Copy Forward Error Concealment

(d) Motion Vector Error Concealment

Figure 2: Power figures by slice dropping mode for Copy Forward (a), and Motion Vector ERC (b). The
PSNR charts show QoS performance for each decoder mode on tempete cif for Copy Forward (c) and Motion
Vector ERC (d).

Table 2: Power and PSNR for
mode for CF and MV decoders.
CF
Video
P
PSNR(dB)
akiyo cif
29%
43.0
container cif 50%
34.9
highway cif 33%
34.5
soccer cif
27%
28.7
tempete cif 53%
24.1
waterfall cif 51%
30.5

speedup determines our ability to slow the decoder with frequency scaling in order to reduce power consumption. To
collect decoder timing data, a 100 frame modified H.264
video stream was generated for each test video. Decoder
frame rate data was then collected over several iterations
for each video in each slice dropping mode on the Linux
server and the Glencoe board in order to verify the Glencoe’s performance.
In our analysis, we take the frame rate the Glencoe decoder achieves for the full six slice video as the nominal frame
rate against which improved frame rates for slice dropping
modes are compared for frequency scaling purposes. For instance, if the decoder in 1 slice mode decoding akiyo cif finishes 25% earlier than the same video fully decoded, then we
can slow the clock by 25% and still decode at the same frame
rate. The adjusted frequency is given by f ′ = t′ · fdef ault /t
where t′ is the improved decoding time, and fdef ault is the
default frequency 468MHz. This value must be adjusted up
to the next available frequency in our DVFS scheme. For
akiyo cif, the calculated frequency, f ′ = 348.7MHz must
be adjusted up to the next available frequency, 351MHz at
VDD = 1.4V .

decoder in 1-slice

P
7%
29%
2%
0%
5%
29%

MV
PSNR(dB)
43.8
37.6
35.7
32.7
30.1
33.8

We chose a value of 2N = 3 giving the suitable although
non-ideal function of power in terms of frequency for system
idle seen in Figure 1. We use frequencies ranging from the
default value of 468Mhz to 273MHz indicated in the chart
with L values from 24 down to 14. This range of frequencies
crosses three available voltage levels as can be seen in the
figure. The high step on the left corresponds to VDD = 1.5V ,
the middle step corresponds to VDD = 1.4V , and the low
step on the right corresponds to VDD = 1.3V . The power
performance chart for the idle Glencoe board gives an idea of
the power savings we should expect at various frequencies
after implementing DVFM in the decoder. Reductions in
VDD provide significant power savings as expected from the
quadratic relationship between power and supply voltage,
P = Cswitching · V 2 · f .
Our goal is to characterize the decoders’ power performance given H.264 streams generated by the modified encoder. In the first step, we find the relative speedup experienced by the decoder when dropping slices and compare
with the speed of the decoder decoding all slices. Relative

5. RESULTS
Timing results for the CF-ERC and MV-ERC decoders in
1-slice mode are given in Table 1 with the computed DVFM
frequency used for each video on the Glencoe board. The
associated power savings for a given video and frequency
are listed in Table 2 along with average P frame PSNR.
Significant power savings were obtained from the CF-ERC
decoder, in some cases without substantial impact on PSNR.
The MV-ERC decoder achieves much smaller power savings
due to the significant complexity of handling motion vectors

265

ularly focusing on distinguishing foreground objects from
background. Tuning a number of other factors may also improve performance. Some examples are reducing the number of slice groups which decreases decoding complexity and
bandwidth, considering motion vector data in addition to
distortion data when prioritizing macroblocks, using motion
vectors less aggressively in the decoder for MV-ERC, considering B frames, and varying the GOP size between I frames.

7. ACKNOWLEDGMENT
(a) akiyo cif

V. Parameswaran and B. Li are partially supported by
ARO grant W911NF-06-1-0354.

8. REFERENCES
[1] Y.-K. Chen, E. Q. Li, X. Zhou, and S. Ge. Implementation
of h.264 encoder and decoder on personal computers.
Journal of Visual Communication and Image
Representation, 17(2):509–532, April 2006.
[2] H.264/AVC. Reference software.
http://iphome.hhi.de/suehring/tml/.
[3] R. Henning and C. Chakrabarti. A quality/energy tradeoff
approach for idct computation in mpeg-2 video decoding.
In IEEE Workshop on Signal Processing Systems. 2000.
[4] M. Horowitz, A. Joch, F. Kossentini, and A. Hallapuro.
H.264/avc baseline profile decoder complexity analysis.
IEEE Transactions on Circuits and Systems for Video
Technology, 13(7):704–716, July 2003.
[5] S. Hua, G. Qu, and S. Bhattacharyya. An energy reduction
technique for multimedia application with tolerance to
deadline misses. In DAC Proceedings, pages 131–136. June
2003.
[6] Y. Huang, A. V. Tran, and Y. Wang. A workload
prediction model for decoding mpeg video and its
application to workload-scalable transcoding. In
Proceedings of the 15th international conference on
Multimedia, pages 952–961. 2007.
[7] M. Ihmig. Porting linux 2.6.9 to the pxa270 based
development platform. research collaboration between Intel
and CMU, May 2005.
R Intel
pxa27x
R
[8] Intel
.
processor family developer’s manual.
Intel Order Number: 280000-001, Apr. 2004.
[9] J. G. Kim, J. W. Kim, and C. C. J. Ku. Corruption model
of loss propagation for relative prioritized packet video. In
SPIE Proceedings. July 2000.
[10] J. S. Kim, J. G. Kim, K. O. Kang, and J. Kim. A distortion
control scheme for allocating constant distortion in fd-cd
video transcoder. In IEEE International Conference on
Multimedia and Expo, pages 161–164. June 2004.
[11] C. Krasic, J. Walpole, and W. Feng. Quality-adaptive
media streaming by priority drop. In Proceedings of the
13th International Workshop on Network and Operating
Systems Support for Digital Audio and Video, pages
112–121. 2003.
[12] C. Lian, S. Chien, C. ping Lin, P. Tseng, and L. Chen.
Power-aware multimedia: Concepts and design
perspectives. IEEE Circuits and Systems Magazine,
7(2):26–34, Second Quarter 2007.
[13] MSU. Video quality measurement tool v. 1.52.
http://compression.ru/video/.
[14] D. Son, C. Yu, and H.-N. Kim. Dynamic voltage scaling on
mpeg decoding. In Proceedings of the Eighth International
Conference on Parallel and Distributed Systems. June
2001.
[15] T. Wiegand, G. J. Sullivan, G. Bjntegaard, and A. Luthra.
Overview of the h.264/avc video coding standard. IEEE
Transactions on Circuits and Systems for Video
Technology, 13(7):560–576, July 2003.

(b) soccer cif

Figure 3: Example of last P frame decoded before
next GOP demonstrating image quality for the Copy
Forward (middle) and Motion Vector Error Concealment (right) vs. the fully decoded video (left).
during decoding, but it does a much better job of preserving image quality. The two videos with the lowest PSNR
values decoded using the CF-ERC decoder see the largest
QoS improvement when the MV-ERC decoder is used. An
example of QoS performance of both encoders is presented
over all slide dropping modes for tempete cif in Figures 2(c)
and 2(d). The controlled introduction of distortion into the
video can be seen as well as the QoS improvement introduced through MV-ERC.
The power measurements obtained for each video and decoder slice dropping mode are shown for both decoders in
Figures 2(a) and 2(b). The impact of voltage scaling is evident as power drops significantly each time reducing frequency makes a lower VDD available as predicted in Figure
1. Figure 2(b) illustrates unpredictable performance when
using MV-ERC. Performance varies for both CF and MV
schemes with variations in the transmitted image. There
may be significant variation in MB inter-dependence and
motion-vector densities from one scene or even one frame
to the next. The MV-ERC scheme is particularly susceptible to these variations due to the expense of decoding with
motion estimation.

6.

CONCLUSION

Our H.264 block prioritization scheme successfully enabled
significant power savings in concert with DVFM. Our objective QoS measurements indicate acceptable quality performance, but there are several opportunities for improvement. The subjective quality of the decoded videos can be
improved through further tuning of the parameters in the
encoder to reduce artifacts in the decoded image, partic-

266

arXiv:1605.04369v1 [cs.CV] 14 May 2016

Neural Dataset Generality
Ragav Venkatesan
Arizona State University

Vijetha Gattupalli
Arizona State University

Baoxin Li
Arizona State University

ragav.venkatesan@asu.edu

jgattupa@asu.edu

baoxin.li@asu.edu

Abstract
Often the filters learned by Convolutional Neural Networks (CNNs) from different datasets appear similar. This
is prominent in the first few layers. This similarity of filters
is being exploited for the purposes of transfer learning and
some studies have been made to analyse such transferability of features. This is also being used as an initialization
technique for different tasks in the same dataset or for the
same task in similar datasets. Off-the-shelf CNN features
have capitalized on this idea to promote their networks as
best transferable and most general and are used in a cavalier manner in day-to-day computer vision tasks.
It is curious that while the filters learned by these CNNs
are related to the atomic structures of the images from which
they are learnt, all datasets learn similar looking low-level
filters. With the understanding that a dataset that contains many such atomic structures learn general filters and
are therefore useful to initialize other networks with, we
propose a way to analyse and quantify generality among
datasets from their accuracies on transferred filters. We applied this metric on several popular character recognition,
natural image and a medical image dataset, and arrived
at some interesting conclusions. On further experimentation we also discovered that particular classes in a dataset
themselves are more general than others.

1. Introduction
Neural networks, particularly CNNs have broken all
records recently in the computer vision research area. The
growth of CNNs focused initially on the recognition of
characters. Fukushima and LeCun were the initial pioneers.
Independently they developed CNN based systems, some
of which are still being used widely [7, 16]. Large networks are often trained with large number of data samples
to achieve good accuracies [25, 14]. Still, scepticism over
CNNs among the modern day computer vision scientists
stems from the fact that one does not have a clear understanding of its inner working. Some studies show that a
few (< 1%) nodes are all that are actively contributing to

𝑆
𝐷1

𝐷2

𝐷4

𝐷3

𝐷5

Figure 1. Thought experiment to describe the dataset generality.
S is the space of all possible atomic structures, D1 − D5 are the
atomic structures present in respective datasets.

classification [4]. They also suggest that large networks often overfit, but since the data is too large over-fitting often
works as an advantage [20].
While it is reasonable to expect edge detectors and
Gabor-like features in the lower-level filters and more sophisticated concepts at the higher levels, it is not clear as to
why these filters adapt themselves in this manner. What is
fairly clear though is that different datasets result in different sets of filters that are similar if the datasets are similar.
It is only natural to ask, what role does the data itself play
in such filters being learnt and how they compare with filters learnt from another dataset. In this paper we take the
view that the filters learnt by networks when trained using
a particular dataset represent the detectors for some atomic
structure in the data itself. In which case each layer is a
mapping form the previous layer to the next layer that is
constructed using combinations of these atomic structures
in the first layer in order to minimize a cost.
Let us first define atomic structures to be the forms that
CNN filters take by virtue of the entropy of the dataset it
is learning on, analogous to dictionary atoms. Complex

datasets have more and varied atomic structures. Consider
the following thought experiment: Let’s assume that all possible atomic structures reside in an universe S. Suppose we
have a set of three datasets D = {D1 , D2 , D3 } and D ∈ S.
Consider the system in figure 1. The figure describes the
configuration of the elements of D. One would now recognize that D1 is a more general dataset with respect to
D2 and D3 . It is so because, while D1 contains most of
the atomic structures of D2 and D3 , the latter do not contain as many atomic structures of D1 . While this analysis
is simplified for one layer, in typical CNNs, co-adaptation
plays a major role in the learning of these atomic structures.
Therefore, generality as defined by the overlap of areas in a
layer-wise Venn diagram is impractical to obtain.
In this paper we postulate that, the generalization performances of CNNs on one dataset re-trained on a network
initialized by training using another, could be used to derive generality. We call this process of pre-training as prejudicing. By prejudicing on the first dataset, we froze and
unfroze layers and retrained the networks on the second
dataset. By freezing layers we are making a network more
obstinate and we call this process obstination1 . The more
the layers are frozen, the more obstinate the feature extractor is, therefore the harder the classifier has to work. If the
prejudice was general enough, the classifier shall still generalize fairly well enough. What this means is that if the
prejudicing dataset is more general than the re-train dataset,
the classifier can generalize better than vice versa.
We developed a generality metric by comparing the gain
in performances of networks of various obstination. Using
a generality such as the one proposed, it becomes clearer as
to what kind of datasets are to be used to prejudice CNNs
with during transfer learning. We even discovered that samples with particular labels within a dataset alone are general
enough. So, if we begin by prejudicing the network on only
those and then moved on to the rest of the labels, we were
able to learn the rest of the dataset with considerably less
training samples while achieving comparable generalization
performances.
Off-the-shelf networks such as VGG, overfeat and various published Caffe model weights are trained on large
scale image datasets such as Imagenet or PASCAL [23, 12,
8, 22, 5]. For instance, while these may work on applications such as human pose recognition or vehicle detection, they do not necessarily work on tasks involving medical images. This is because the datasets on which they are
trained are not general enough to adapt to the representational requirements of medical images, which is on a manifold unique and disjoint form the manifolds of natural images. This is visualized in D4 and D5 from figure 1. Even
a large collection of natural images is not general enough
1 Obstinate layer or freezing implies that the weights were not changed
during backprop. The layer remains prejudiced.

to have networks trained that are suitable to medical images. In these cases, the prejudiced network often fails. For
instance, on the Colonoscopy dataset discussed later a 22
layer deep overfeat features, trained with a logistic regression performs poorer than a 3 layer deep CNN trained from
random initialization, which is in turn outperformed when
initialized by a network trained on an endoscopy dataset.
In this article we considered popular offline character
recognition datasets and arrived at some interesting analysis and generalities. We also show that within the MNIST
dataset, classes [4, 5, 8] are general enough that we could
learn the other classes with very few (even just one) samples, when prejudiced with networks trained on [4, 5, 8]. We
also considered more sophisticated datasets such as Cifar 10
and Caltech 101 against some medical image datasets for
colonoscopy video quality [13]. This study led us to two
major research insights:
1. If one has very few data to learn from, which other
dataset is better to prejudice the network with? The
answer is particularly helpful when dealing with medical image datasets where data is very scarce and one
can’t simply use a network trained on VOC datasets as
feature extractors as discussed above.
2. Among the various classes during the training procedure, if we prejudice with a certain general set of
classes first and then move on to others later, generalization to all classes, even for those with few samples
is better. This is particularly significant if the dataset
has a lot of samples in certain classes and not as much
of others.
The rest of the paper is organized as follows: section 2
discusses related works, section 3 presents the design of
our experiments, section 4 shows some results on the coreexperiment and section 5 provides concluding remarks.

2. Related work
One related work that this article shares with is the work
by Yosinski et al [26]. In that article, the authors considered
two tasks A and B that were essentially 500 classes each
from the Imagenet dataset [22]. They trained an 8 layer network on one of the tasks (say A). They then initialized a
new network carrying over the first n layers from the previous job while randomly initializing the others. This new
network was used to retrain task B. Such a network was
AnB + . They experimented by obstination of the carried
over layers. Such a network was AnB. They also studied the specificity of each layer and their contributions to
the overall performance. They also showed that networks
working on similar tasks had a high memorability and that
co-adaptation of layers increased the generalization performance.

While this analysis is interesting, it was performed on
only one dataset: Imagenet. By design, the networks were
forced to learn very general filters, so as to be best transferable. Since the images were all natural images, one would
expect the layers to be more Gabor-like at earlier layers and
have more label specific features at later layers, which was
what was observed. Also, the paper analysed the transferability of the feature extractors from the perspective of
the networks in terms of their fall in generalization performance. This analysis was not catered to the dataset’s perspective, which is that the filters learned are a property of
the dataset being trained on. This was not a problem for the
authors as their datasets for tasks A and B occupied similar
manifolds. This analysis also didn’t explore re-training using the same network but rather went with re-initializing so
that they could learn new co-adaptations. This is not interesting to the study of generality as we want to observe the
effect of filters transferred from one dataset on another. The
more general a dataset, the more variety of atomic structures it offers to the network to learn. We used this idea to
define a generality metric between two datasets. To do so,
we cannot follow the techniques used by Yosinski et al.
Another closely related work is the work on dark knowledge by Hinton et al, [10]. Here the authors suggest that
among the various classes in a dataset, there exists some
amount of generalization knowledge that could be transferred. The authors construct a large network that learns all
its classes. They then go on to train a smaller network with
the same dataset (or with a dataset that is missing some of
the classes altogether). While training this smaller network
though, instead of using the the hard labels, they also use
the softmax output from the large network also for backprop. This creates an effect of the larger network guiding
the smaller network to not just generalize to the dataset,
but also to generalize to unseen classes. This is because,
as the argument goes, ”the network learns the relationship
between the classes” and ”all the knowledge is among the
relative probabilities or softmaxes that the network is almost
certain is wrong” [10].
Although the author retrains an entire network that is
randomly initialized using the softmax outputs from a
trained network and uses this as prejudice, no information
is actually being transferred in terms of actual filters. Ergo,
this work, while interesting, also doesn’t help in understanding generality of the data itself in a more direct manner. Some of the claims made by this article though were
indirectly and independently verified by us through our generality results. The basic claim of their work is that among
only a handful of classes, there is enough knowledge to generalize to other classes. Unless there exists some generality
between classes, training on particular classes will not have
been representational enough for the other classes to learn
on. We directly verify this by showing that some classes

Figure 3. Samples of some of the datasets that we used in
this analysis. From top to bottom: MNIST [17], MNISTrotated [15], MNIST-random-background [15], MNIST-rotatedbackground [15], Google street view house numbers [19], Char
74k English [3], Char 74k Kannada [3]. Last two rows, first five
from left are CIFAR 10 and the rest are Caltech101 [13, 6]. The
bottom row is the colonoscopy dataset.

alone have a high generalization to the rest of the dataset
and make a similar conclusion from an entirely independent
direction of research.

3. Design of experiments
Consider figure 3. Among the various datasets shown,
it is natural to expect any network trained on MNIST to
contain simpler filters than MNIST-rotated. This is because, while MNIST-rotated contains many structures from
MNIST, due to the rotations, MNIST-rotated will contain
additional structures that require the learning of more complicated filters. A network trained on MNIST-rotated on
its first layers will be expected to additionally have filters
for detecting sophisticated oriented edges than for MNIST.
This would mean that prejudicing a network with MNIST to
then re-train MNIST-rotated is much less helpful than vice
versa. A network prejudiced with a general enough dataset
is better to be retrained for it generalizes easily. A prejudice must come from a more general dataset if a prejudice
transfers positive knowledge as shown in their generaliza-

0

0

0

0

1

1

1

1

9

9

9

9

Figure 2. Protocol of obstination: From left to right, all layers frozen, one, two and three layers unfrozen. Green represent unfrozen and
red represent frozen. Note that the layers are always unfrozen from the end and that the softmax layer is always unfrozen and randomly
initialized. This should be generalized similarly for more than three layers also.

tion performances. We use this simple intuition to argue
that MNIST-rotated is a more general dataset with respect
to MNIST.
Our basic experiment is conducted between pairs of
datasets Di and Dj . Firstly, we train (prejudice) a randomly
initialized network with dataset Di . We call this network
n(Di |r) or the base network (r implies random initialization). We then proceed to retrain n(Di |r) as per any of the
setup shown in figure 2. nk (Dj |Di ) would imply that there
are k degrees of freedom, or to be precise, k layers of filters
that are allowed to learn by dataset Dj that is prejudiced
by the filters of n(Di |r). nk (Dj |Di ) has N − k obstinate
layers that carries the prejudice of dataset Di , where N is
the total number of layers. Note that more degrees of freedom implies that the network is less obstinate to learn. Also
note that these layers can be both convolutional or fully connected neural layers. Any idea expressed here can be extended to any type of parametrized layers. In fact while we
perform operations such as batch normalizations, we even
freeze and unfreeze the α and β of batch norm [11]. Obstination also includes the bias parameters.
Layers learn in two facets. They learn some components
that are purely their own and some that are co-adapted from
previous layers that are allowed to learn as well. By freezing some layers we are making those layers a fixed functional transformation. Note that the performance gain from
nk (Dj |Di ) and nk+1 (Dj |Di ) is not because of just the new
layer k + 1 being allowed to learn, but of the combination
of all k + 1 layers allowed to learn.
Figure 2 shows the setup of our experiments and explains
degrees of freedom. These are our obstination protocols.
Notice that in all the various setup, the softmax layer remains non-obstinate. In fact the softmax layer is always
randomly re-initialized because not all dataset pairs have
the same number of labels. Also notice that the unfreezing
of layers happen from the rear. We cannot unfreeze a layer
that feeds into a frozen layer. This is because, while the
unfrozen layer learns a new filter and therefore represents
the image on new distributed domains, the latter layer is
not adapting to such a transformation. When there are two

layers unfrozen, the two layers should be able to co-adapt
together and must finally feed into an unfrozen classifier
layer.

3.1. Dataset generality
Suppose the generalization performance of n(Dj |r) is
Ψ(Dj |r) and the generalization performance of nk (Dj |Di )
is Ψk (Dj |Di ). First order dataset generality or simply
dataset generality of Di with respect to Dj at the layer k
is given by,
gk (Di , Dj ) =

Ψk (Dj |Di )
Ψ(Dj |r)

(1)

This indicates the level of performance that is achieved
by Dj using N − k layers worth of prejudice from Di and
k layers worth of features from Di combined with k layers
of novel knowledge from Dj together. Note that the generality is calculated for the base dataset as a measure of how
the re-train performs with the prejudice of the base dataset.
gk (Di , Dj ) > gk (Di , Dl ) indicates that at k layers, Di provides more general features to Dj than to Dl . Conversely,
when initialized by n(Di |r), Dj has an advantage in learning than Dl .
Note that, gk (Di , Di ) ≥ 1 ∀k. gk (Di , Dj ) for i 6= j
might or might not be greater than 1. If gk (Di , Dj ) ≥ 1
for i 6= j, it indicates that Dj is at least very similar to Di
(such as the case considered by Yosinski et al.) and at most
a perfect generalizer of Di [26].

3.2. Class generality
Di and Dj need not be entire datasets but can also be
just disjoint class instances of the same dataset that is split
in two. These generalities will tell us if particular classes
are themselves more general than others. For instance, we
divided the MNIST dataset into two parts. The first part
contained the classes [4, 5, 8], the rest were contained by
the second part2 . We performed the generality experiments
2 We chose this combination of classes strategically after trail and error
as these are the most general among the classes and exaggerate the effect.

with MNIST[4, 5, 8] as base, which was trained over a random initialization. We re-trained this prejudiced network
using the second part with the same experiment design as
above. We defined class generality as the generality, of a
class or a set of classes, retrained on the prejudice of the
other mutually exclusive classes.
We repeated this experiment several times with decreasing number of training samples per-class in the retrain
dataset of MNIST [0, 1, 2, 3, 6, 7, 9]. All the while, the
testing set remained the same size. This implies that the
prejudiced network retrains on a much smaller dataset and
tests on a much larger dataset. The re-train dataset had
7 classes. We created seven such datasets with 7p, p ∈
[1, 3, 5, 10, 20, 30, 50] samples each. We now define subclass generality as the generality of these sub-sampled
datasets (in each class we only consider a small random
sample), retrained on the base of other mutually exclusive classes (MNIST[4, 5, 8]). . Initializing a network that
was trained on only a small sub-set of well-chosen classes
can significantly improve generalization performance on all
classes, even if trained with arbitrarily few samples, even at
the extreme case of one-shot learning.

3.3. Datasets Used
We designed these experiments across three board
categories of datasets: 1. Character datasets that included MNIST [17], MNIST-rotated [15], MNIST-randombackground [15], MNIST-rotated-background [15], Google
street view house numbers [19], Char 74k English [3] and
Char 74k Kannada [3] 2. Natural image datasets that includes Cifar 10 and Caltech 101 [13, 6] and 3. Natural
images against medical images that included in addition
to Caltech 101 a Colonoscopy video qualitty dataset. We
leave it to the reader to find for themselves details about
the datasets from the original articles, but the setup we have
used can be found in table 1. Although we chose only a
handful of datasets, the intention of this article was only to
show that such generality measures could be made. The
scope of this article was not to benchmark various publicly
available popular datasets. Neither was it to make suggestions specific to types of datasets.

3.4. Network architecture and learning
We used one standard network architecture for all character datasets and experiments, one for Cifar 10 vs. Caltech
101 and another standard for Caltech 101 vs. Colonoscopy.
The setup we have used can be found in table 1.
The network architectures, learning rates and other details are provided below. The experiments were conducted
on a Macbook Pro Laptop using an Nvidia GT 750M GPU,
for character datasets and on an Nvidia Tesla K40 GPU for
the others, with cuDNN v3 and Nvidia CUDA v7.
Table 1 shows the train-test-validation splits and the

batch sizes used in stochastic gradient descent of all the
datasets used. No pre-processing were done on the images themselves except for cropping, resizing, normalizing. The images were all normalized to lie in [0, 1]. The
character recognition datasets were all of a constant 28X28
grayscale, the Caltech 101 vs. Cifar 10 experiments were
performed ar 32X32, RGB and the Caltech 101 vs. Colonsoscopy were at 128X128, RGB. It is to be noted that the
aim of the authors was not to set up the networks to achieve
state-of-the-art. The authors did although try to achieve satisfactory performances on all base datasets involved before
proceeding with the experimentation.
Character Datasets.
Our networks had three convolutional layers with 20, 20
and 50 kernels respectively. All the filters were 5 X 5
and were all stride 1 convolutions. The first layer didn’t
have any pooling. The second and the third layer maxpool by 2 subsampled. All the layers used rectified linear
units (ReLU ) activations [18]. The classifier layer was a
softmax layer and we didn’t use any fully connected layers.
We used a dropout of 0.5 only from the last convolutional
layer to the softmax layer [24]. We optimized a categorical cross-entropy loss using an rmsprop gradient descent
algorithm [2]. For acceleration we used Polyak Momentum that linearly increases in range [0.5, 1] from start to 100
epochs [21]. Unless early terminated, we ran 200 epochs.
We also used a constant L1 and L2 regularizer co-efficients
of 0.0001. Our learning rate was a 0.01 with a multiplicative decay of 0.0998.
CIFAR10 Vs.
Colonoscopy.

Caltech101 and Caltech 101 vs

For this task, the networks had five convolutional layers
with 20, 20, 50, 50 and 50 kernels respectively. We also
had a last fully connected layer of 1800 nodes, which also
had a dropout of 0.5. All the filters were 5 X 5 and were all
stride 1 convolutions. Only the last layer maxpool by 2 subsampled. All the layers used rectified linear units (ReLU )
activations [18]. All CNN and MLP layers were also batch
normalized [11].The classifier layer was a softmax layer and
we didn’t use any fully connected layers. We used a dropout
of 0.5 only from the last convolutional layer to the softmax
layer [24]. We optimized a categorical cross-entropy loss
using an rmsprop gradient descent algorithm [2]. For acceleration we used Polyak Momentum that linearly increases
in range [0.5, 0.85] from start to 100 epochs [21]. We use
a learning rate of 0.001 for the first 150 epochs and then
fine tune with a learning rate of 0.0001 for an additional 50
epochs unless early-terminated. Our learning decay of was
subtractive 0.0005. Figure 4 shows more generality curves.

Dataset
MNIST [17]
MNIST-random-background [15]
MNIST-rotated-background [15]
NIST Special Dataset-19 [9]
Google Street View House Numbers [19]
Char 74k English [3]
Char 74k Kannada [3]
MNIST [4, 5, 8]
MNIST [0, 1, 2, 3, 6, 7, 9] − p per-class
CIFAR 10 [13]
Caltech 101 [6]
Colonoscopy3

Training
50,000
40,000
40,000
271,220
63,042
9,300
5,694
14,000
7p
40,000
5,080
2,700

Testing
10,000
12,000
12,000
271,220
63,042
3,355
1,314
2,500
7,000
10,000
3,048
900

Validation
10,000
10,000
10,000
271,220
63,042
305
1,752
2,500
7,000
10,000
1,016
100

Classes
10
10
10
62
10
62
100
3
7
10
102
2

Training Batch Size
500
500
500
191
399
305
438
500
500
500
254
100

Table 1. Datasets used and their properties.

4. Results and observations

Error Curve - Base Network:mnist-rotated-bg ; Retrained onmnist

300

Error Curve - Base Network:mnist-rotated-bg ; Retrained onmnist-bg-rand

1200

1000

4.1. Character Datasets

800

250

600

400

Figure 4 shows the generalities of MNIST-rotated-bg
and Kannada prejudiced by all other the character datasets.
For reference each plot also shows the generalization performance of a randomly initialized base convolutional network. The following are some observations of interest:
While no dataset is qualitatively the most general, it is
quite clear that MNIST dataset is the most specific. Rather,
MNIST dataset is one that is generalized by all datasets very
highly at all layers. Surprisingly, MNIST dataset actually
gives better accuracy when prejudiced with other datasets,
rather than when initialized with random, if all layers were
allowed to learn. This is a strong indicator that all datasets
contain all atomic structures of MNIST.
NIST, Char74-English and Char74-Kannada follow similar generalization trends with almost all the datasets. With
no degrees of freedom they all generalize rather poorly, but
their generalities shoot up once one or many layers of the
base networks are unfrozen. This indicates two properties:
Firstly, these three datasets have similar manifolds. Secondly this also indicates that the last layers of the base
datasets are extracting some particular quality of atomic
structures that are present in the these datasets alone. Similarly, SVHN does not generalize in the first layer to most
datasets, it generalizes much better in the latter layers. This
is particularly noticeable in MNIST and Kannada. This further exemplifies the results.
While initially one would have assumed that Kannada
would be a general dataset, we observed the contrary.
SVHN, Char74-English and Nist generalizes better to Kannada than even Kannada itself does. English characters
seem to be a more general set than Kananda. While
counter-intuitive, this result is immediately obvious when
one pays close attention to the filers that are learnt and

Base Network -mnist-bg-rand
1-unfrozen
2-unfrozen
3-unfrozen
all-frozen

200

0

200

0

20

40

60

80

100

120

140

160

180

200

150
Base Network -mnist
1-unfrozen
2-unfrozen
3-unfrozen
all-frozen

100

50

0

50

100

150

200

Figure 5. Validation errors vs Epoch number for base-MNISTrotated-bg retrained on MNIST

the dataset itself. Kannada is dominated by predominantly
curved edges only, whereas even MNIST has a multitude of
unique atomic structures.
Figure 5 demonstrates some interesting phenomenon
that we discovered often. The gain in performance
achieved, constantly decreases with increase in degrees of
freedom. Through the epochs, unfreezing only the classifier layer, quickly converges. But while unfreezing, all
layers converge at about the same number of epochs. We
also observe, that MNIST retrained over MNIST-rotatedbackground, with the last degree of freedom does not learn
antything at all. The error rate is within the statistical margin of error. This is a testament to the generality of MNISTrotated-background among the MNIST datasets. One might
expect this because MNIST-rotated-background contains
smooth background images (similar to natural image set)
and MNIST characters that are rotated. These conditions
provide for a good generality.
For the intra-class experiment described in subsection 3.2 above, table 2 shows the accuracies. From the table one can observe that even with one-sample per class,

Retrainedmnist-bg-rand for different bases

Retrainedmnist for different bases
1.1

1.02

1

1

0.9

mnist
MNIST random background
MNIST
MNIST rotated background
NIST Special Dataset-19
Google Street View House Numbers
Char 74k English
Char 74k Kannada

0.94
0.92
0.9

0

1

2

0.8

0.5
0.4
3

0.9

0.8

0.8

0.7

0.7
mnist-rotated-bg
MNIST random background
MNIST
MNIST rotated background
NIST Special Dataset-19
Google Street View House Numbers
Char 74k English
Char 74k Kannada

0.4
0.3

0

1

1

2

3

Retrainedchar74-english for different bases

1
0.9

0.5

0

Number of Layers Unfrozen

1

0.6

mnist-bg-rand
MNIST random background
MNIST
MNIST rotated background
NIST Special Dataset-19
Google Street View House Numbers
Char 74k English
Char 74k Kannada

0.7
0.6

Number of Layers Unfrozen
Retrainedmnist-rotated-bg for different bases

1.1

Generality

Generality

0.96

Generality

Generality

0.98

2

0.6

char74-english
MNIST random background
MNIST
MNIST rotated background
NIST Special Dataset-19
Google Street View House Numbers
Char 74k English
Char 74k Kannada

0.5
0.4
0.3
0.2

3

0

1

2

3

Number of Layers Unfrozen

Number of Layers Unfrozen
Retrainedchar74-kannada for different bases

Retrainedchar74-english for different bases

1

1.2

0.9

1

0.8

0.6

char74-kannada
MNIST random background
MNIST
MNIST rotated background
NIST Special Dataset-19
Google Street View House Numbers
Char 74k English
Char 74k Kannada

0.4
0.2
0

Generality

Generality

0.8

0

1

2

0.7
0.6

char74-english
MNIST random background
MNIST
MNIST rotated background
NIST Special Dataset-19
Google Street View House Numbers
Char 74k English
Char 74k Kannada

0.5
0.4
0.3
0.2

3

0

1

Number of Layers Unfrozen

2

3

Number of Layers Unfrozen

Retrained CALTECH101 for different bases
1.1

1.6

1

1.4

0.9

1.2

Generalities

0.8
0.7
0.6

Caltech 101 vs. Colonoscopy

1
0.8
0.6

0.5

0.2

0.3
0

1

2

3

4

5

caltech 101 on caltech 101
caltech 101 on colonoscopy
colonsoopy on colonoscopy
colonoscopy on caltech101

0.4

CALTECH101-base
CALTECH101
CIFAR10

0.4

6

0

1

2

3

4

5

6

Figure 4. Generalities of datasets not shown in the actual paper. The dark line represents the accuracy of n(D|r). Please zoom on a
computer monitor for closer inspection.

a 7-way classifier could achieve 22% more accuracy than
a randomly initialized network. It is note worthy that the

last row of table 2 still has 100 times less data than the full
dataset and it already achieves close to state-of-the-art ac-

p
1
3
5
10
20
30
50

base
Random
MNIST[458]
Random
MNIST[458]
Random
MNIST[458]
Random
MNIST[458]
Random
MNIST[458]
Random
MNIST[458]
Random
MNIST[458]

k=0
73.07
83.61
90.98
91.55
95.52
96.5
96.38

k=1
73.91
87.2
92.98
93.71
95.52
97.34
97.40

k=2
76.37
85.7
92.6
93.82
97.07
97.35
97.71

k=3
55.61
77.52
73.34
87.6
83.32
92.07
81.31
95.08
87.77
96.78
88.62
97.45
90.78
97.38

Table 2. Sub-sample experiment and its generalization accuracies for different layers of freezing. The re-train network was
MNIST[0, 1, 2, 3, 6, 7, 9]. For obvious reasons random initializations are trained only with all layers unfrozen, hence the missing
values.

Figure 6. Sub-class generalities for MNIST [4, 5, 8]

curacy even when no layer is allowed to change. This is a
remarkably strong indicator that the classes [4, 5, 8] generalizes the entire dataset.
Figure 6 mimics the same. We also observed that once
initialized with a general enough subset of classes from
within the same dataset, the generalities didn’t vary among
the layers like it did when we initialized with data from outside the mother dataset. We also observed that the more the
data we used, more stable the generalities remained. Point
of take away from this experiment is that if the classes are
general enough, one may now initialize the network with
only those classes and then learn the rest of the dataset even
with very small number of samples.

4.2. CIFAR 10 vs. Caltech 101
From figure 4 we observe that Caltech 101 doesn’t generalize to Cifar 10, which is surprising because Caltech 101
has a lot more classes. One would expect it to be more general. Its quite the opposite because Caltech 101 although
has a lot of classes, the variability of each class is not as
much as the variability in the Cifar 10 dataset. But it is
altogether a serendipitous result that Cifar 10 is more general than Caltech 101 on the lower layers. However after
three layers of obstination, we find that when the generalities crosses 1, the effect nullifies and reverses slightly. Even
though the low-level features are more general in Cifar 10,
Caltech 101 generalizes more on higher layers.

4.3. Caltech 101 vs. Colonoscopy
The colonoscopy dataset’s labels identify if a image is
deemed to be of a quality that is good enough so as to make
a diagnosis on the pathology of that particular image. Figure 7 show the filters learnt by Caltech 101 base network
and Colonoscopy base network for the exact same architecture from random initialization. Two things are immediately
apparent from the learnt filters that while Caltech 101 learns
more structured and organized shape features, Colonoscopy
dataset learns at first sight what appears to be unstructured
blob detectors and detectors for dark colors. These features
still produce state-of-the-art accuracy on the dataset. On
observation of the activations produced after the first layer,
and from observations of images and their labels, one can
immediately recognize that what the network is learning is
indeed changes in brightness patterns.
Most often the video quality in colonoscopy is affected
because of saturation when too much light is thrown at a
scene. The quality is also affected due to light reflection
from bodily fluids that is also noticeable in the activations.
As also can be noticed that most of the filter colors are yellowish or blueish. On an colonoscopy video most often the
video is also labelled poor quality when these colors are
present, as these colors are often present mostly because
of scattering and reflections. Having made these observations one would arrive at the obvious conclusion that neither dataset generalizes the other. This was indeed the result observed from figure 4. Although, Caltech 101 seem
to generalize a bit better for even though it predominantly
learns shapes, it learns some color features also.

4.4. Summary of results
From all these results and observations, we could summarize that one should prefer to initialize with a general
dataset that might have a lot of variability or rather generality in data, when attempting to train with very few number of samples. Whenever possible one must initialize the
network trained by a general dataset as this always boosts
generalization performance. When there are biased datasets

Figure 7. From left to right, separated by a line are filters learnt by a base Caltech101 base colonoscopy, sample images from the
colonoscopy dataset and their first activation for a filter that detects smooth areas of brightness.

with large number of samples in some classes and fewer in
others, one should train the most general classes first. Once
the network is well-prejudiced one should start introducing
the classes with fewer number of and less general samples,
provided the general class is general enough.

5. Conclusions
In this paper, we used the performance of CNNs on a
dataset when initialized with the filters from other datasets
as a tool to measure generality. We proposed a generality
metric using these generalization performances. We used
the proposed metric to compare popular character recognition datasets and found some interesting patterns and generality assumptions that add to the knowledge-base of these
datasets. In particular, we noticed that MNIST data is one
of the most specific dataset. We also found that Char74k
Kannada is less general than English datasets. We also calculated generality on class-level within a dataset and conclude that a few well-chosen classes used as pre-training
could build a network that is well-initialized that even with
100 times less samples, we could learn the other classes. We
also provided some practical guidelines for a CNN engineer
to adopt. After performing similar experiments on popular imaging datasets and medical datasets, we made similar
serendipitous observations.

References
[1] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow, A. Bergeron, N. Bouchard, and Y. Bengio. Theano: new
features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.
[2] Y. N. Dauphin, H. de Vries, J. Chung, and Y. Bengio. Rmsprop and equilibrated adaptive learning rates for non-convex
optimization. arXiv preprint arXiv:1502.04390, 2015. 5
[3] T. E. de Campos, B. R. Babu, and M. Varma. Character
recognition in natural images. In Proceedings of the International Conference on Computer Vision Theory and Applications, Lisbon, Portugal, February 2009. 3, 5, 6

[4] V. Escorcia, J. C. Niebles, and B. Ghanem. On the relationship between visual attributes and convolutional networks.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 1256–1264, 2015. 1
[5] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and
A. Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303–
338, 2010. 2
[6] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative
visual models from few training examples: An incremental
bayesian approach tested on 101 object categories. Computer
Vision and Image Understanding, 106(1):59–70, 2007. 3, 5,
6
[7] K. Fukushima and N. Wake. Handwritten alphanumeric
character recognition by the neocognitron. Neural Networks,
IEEE Transactions on, 2(3):355–365, 1991. 1
[8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic
segmentation. In Computer Vision and Pattern Recognition
(CVPR), 2014 IEEE Conference on, pages 580–587. IEEE,
2014. 2
[9] P. J. Grother. NIST Special Database 19 Handprinted Forms
and Characters Database. 1995. 6
[10] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
in a neural network. arXiv preprint arXiv:1503.02531, 2015.
3
[11] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
arXiv preprint arXiv:1502.03167, 2015. 4, 5
[12] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. In Proceedings of
the ACM International Conference on Multimedia, pages
675–678. ACM, 2014. 2
[13] A. Krizhevsky and G. Hinton. Learning multiple layers of
features from tiny images, 2009. 2, 3, 5, 6
[14] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classification with deep convolutional neural networks. In
Advances in neural information processing systems, pages
1097–1105, 2012. 1
[15] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and
Y. Bengio. An empirical evaluation of deep architectures

[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

on problems with many factors of variation. In Proceedings
of the 24th international conference on Machine learning,
pages 473–480. ACM, 2007. 3, 5, 6
Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.
Howard, W. Hubbard, and L. D. Jackel. Backpropagation
applied to handwritten zip code recognition. Neural computation, 1(4):541–551, 1989. 1
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. 3, 5, 6
V. Nair and G. E. Hinton. Rectified linear units improve
restricted boltzmann machines. In Proceedings of the 27th
International Conference on Machine Learning (ICML-10),
pages 807–814, 2010. 5
Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 5. Granada,
Spain, 2011. 3, 5, 6
A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks
are easily fooled: High confidence predictions for unrecognizable images. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 427–436,
2015. 1
B. T. Polyak. Some methods of speeding up the convergence
of iteration methods. USSR Computational Mathematics and
Mathematical Physics, 4(5):1–17, 1964. 5
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer
Vision (IJCV), pages 1–42, April 2015. 2
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014. 2
N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: A simple way to prevent neural
networks from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958, 2014. 5
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. arXiv preprint
arXiv:1409.4842, 2014. 1
J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural networks? In Advances in
Neural Information Processing Systems, pages 3320–3328,
2014. 2, 4

AUTOMATIC DETECTION OF REPLAY SEGMENTS IN BROADCAST SPORTS
PROGRAMS BY DETECTION OF LOGOS IN SCENE TRANSITIONS

M. Ibrahim Sezan

Baoxin Li

Bao Pan

Sharp Laboratories of America Inc.
5750 NW Pacific Rim Blvd., Camas, WA 98607, USA

{hpan,bli,sezanj@Sharp/abs.com

slow-motion replays in content whose fields are sub-sampled

during encoding. Further, the algorithm treats boundaries of

ABSTRACT

replays as ordinary gradual transitions, and hence the boundary

In broadcast sports, replays provide viewers another look at

detection may be inaccurate

interesting events. We propose an automatic algorithm for

In this paper, we propose a new algorithm for detection of
replays by automatic detection of the special gradual scene

replay segment detection by detecting frames containing logos
in the special scene transitions that sandwich replays. Detected
replays are. utilized in efficient

navigation,

transitions

indexing, and

automatically determines the logo template from frames
surrounding slow motion segments, where slow motion

[4].

immediately

precede

and

follow

replay

transitions". Logo transitions remain the same during the entire

Then, it locates

game, so we detect the logo transitions and the associated replay
segments. In contrast to our previous slow-motion detection

all the similar frames in the video using the logo template.
Finally the algorithm identifies the replay segments by grouping
the detected logo frames and slow-motion segments.

that

segments. Because these special gradual transitions usually
contain logos of the broadcaster, and with special editing effects
applied to the logos, we call these transitions as "logo

summarization of sports programs. The proposed algorithm first

segments are automatically detected using

[5].

algorithm, the image features of logo transitions are not affected

Our

by high-speed cameras, sub-sampled fields, and boundaries of
replays are detected with much higher accuracy.

algorithm accurately detects replays, with or without slow
motion.

Babaguchi et al. [6] also detect replays by detecting special
transition effects where features characterizing the transition

1

effects are known a

able to have a simpler and more efficient detection approach
when the logo is known a

over the world. It is desirable for the fans to watch a time­
compressed summary composed of interesting events of the

giune,

algorithm

especially those games that they have nUssed.

such

as

MPEG-7

[1]

and

TV-Anytime

[4], when the logo is not known a priori.

Treating the

is forced to use complex and non-robust features such as motion
vectors and segmented regions, and it is not fully automatic. A

fully automatic algorithm is required, especially for consumer

[2].

platforms such as personal video recorder

for

(PVR).

Next, we discuss the proposed method in three major parts:

transitions; (2) detecting all logo transitions; and (3) combing
the results of logo transition detection and slow motion detection

segments in a sports broadcast video. Detected replay segments,
representing interesting events, can be used in forming compact
summaries and efficient navigation, or in conjunction with other

to identity replay segment boundaries. We then present our

specialized event detection algorithms. For example, in [3], we
provide algorithms that identity every important event in

results obtained using actual sports broadcast content.

American football and baseball (e.g., plays). These algorithms

:z

redundantly detect replays of events that are already detected

during the regular coverage of the game. It is desirable to

THE PROPOSED METHOD

A logo transition, usually less than I second long, is a set of

determine and exclude the redundant replay segments in the

consecutive frames that contain logo images of the broadcaster

program summary in order to form the most compact summary.

or a special event. For example, in Figure I, a logo transition in

In [4], we have proposed and successfully applied an algorithm
for detection of replay segments where events are rendered in
slow motion. The algorithm, however, cannot accurately detect

the 2000 Super Bowl broadcast contains the broadcaster and the
Super Bowl logo.

those slow-motion replays generated by a high-speed camera, or

IV

(I)

automatic, unsupervised learning the beginning and ending logo

We address the problem of automatic detection of replay

0-7803-7402-9/02/$17.00 ©20021EEE

Moreover, we provide a fully

transitions as a special type of wipe, the Babaguchi et al. method

and efficient navigation of a lengthy game video where
descriptions of these segments are compliant with emerging
standards

priori.

automatic approach that uses our previous slow motion detection

Segments containing interesting events are also used in indexing

Broadcasters use interesting segments of the game
generating short highlights, promotions, and for indexing.

While we characterize a transition

the logo, which is mostly invariant to the scene content, we are

Sports broadcast is one of the most desirable types of content
that is enjoyed and followed by a large number of fans in all

broadcast

priori.

segment using a logo, they characterize it by a special, a priori
known manipulation of a natural scene from the game. By using

INTRODUCTION

- 3385

Figure 1. A starting logo transition In the XXXV Super

Figure 3. The logarithm of

Bowl broadcast In year 2000.

Logos in many frames of a logo transition are so significant that
it is unnecessary to locate all the consecutive frames (as done in
[6]) in order to detect a transition. Our algorithm locates only
one frame with a significant logo in one logo transition by
evaluating similarities between frames and a logo template. The
logo template is automatically determined based on our previous
slow motion detection algorithm [4].

The distance d(k)(i,j)of the search area k (k=1. 2) between
frame j and} is defined as

d(kl(i,})

=

{Jd(k)P (i,j)

+

(1- P)d(k)H (i,j),

where 0< p < 1 is a weight, d(klP (i, j) is the pixelwise intensity­
based mean-square difference (MSD) , and d(k)H (i,j) is the

The algorithm consists of three phases: (1) automatic detection
of logo template, (2) detection of all logo transitions in the video
program using the logo template, and (3) identification of the
replay segments. Since the starting and end logo transitions of
replays may not be the same, they need to be processed
independently. Our discussion below for starting logo transitions
also applies for ending logo transitions.
2.1

d(kl(i,j)' Two horizontal axes

represent i andj, respectively.

color histogram-based MSD in the search area k. The color
histogram uses 16 evenly distributed bins on the normalized R
component, RI(R+G+B), and 16 evenly distributed bins on the
normalized G component, G/(R+G+B). The weight P is chosen
so that both MSDs have contributions of the same numerical
range. Figure 3 shows an example of log(d(kl (i, j». The spike
indicates the most similar frames in the two logo transitions.

Automatic detection of the logo template

To get a reliable logo template, we use six SLO-MO segments.
Ideally, each SLO-MO segment has five detected logo frames by
pairing with the other five SLO-MO segments. This "5 to 1"
redundancy will reduce the error detection rate from Po.. of two

SLO-MOs to
Figure 2. The corresponding frames in different logo
transitions contain the same logos.

ofsix SLO-MOs.

In practice, we consider the detection as a success when at least
one SLO-MO has more than 3 successfully detected logo
frames, and they are temporally in the range of 1 second
(because logo transitions are shorter than I second). If we could
not find any SLO-MO that satisfies the above condition, then we
will go back and replace those SLO-MOs that have less than 3
lo go frames with new SLO-MOs and recalculate logo frames
and logo area. The actual error detection rate is p!"..

Automatic detection of the logo template is based on that
different starting (or ending) logo transitions, apart by at least
several hundreds of frames, contain one common image
structure, namely the logo, at a certain spatial area of
corresponding frames. On the contrary, the frames, apart by at
least several hundreds of frames but not in logo transitions,
usually are arbitrarily different, as depicted in Figure 2.

2.2

a lengthy sports program and finding the
frames containing the same logo are impossible. Nevertheless,
using our SLO-MO detection algorithm [1], we are able to
detect SLO-MO segments within some of the replays and
therefore restrict the search for logo frames to a few hundred of
frames that surround the SLO-MO segments.

Directly searching

Logo frame detection

After we detect the logo template, we use it to locate all logo
frames in the video program by a probabilistic approach.
First, we calculate pixelwise intensity-based MSDs dP (i) and
color histogram-based MSDs dH (i), between the detected logo
template with every frame i in th e video. An example of
10g(dP (i» of one-hour sports video clip is shown in Figure 4.

Specifically, first, we use our SLO-MO detection algorithm to
locate two SLO-MO segments. Two rectangular search areas are
pre-determined at the center of frames where logo images
usually appear (shown as the two boxes in one frame in Figure
1). Then, we find the two frames most similar at one of the two
search areas from two sets of 300 frames preceding the two
SLO-MO segments. Finally, we check the distance between the
two most similar areas, if the distance is below an empirically
chosen threshold, then we consider them as the logo frames.

IV

P!.

Th ose spikes in the Figure 4 (a) indicate the logo frames.
Second, we create probability models for the two MSDs. To
model d P (i), we use two classes: A represents the class of nonlogo frames and n represents the class of logo frames, and two
probability densities associated with the two classes: p P (x I A)

-

3386

and

pl'(xlil),

logarithm of

where

dl' (i)

x=log(dl'(i».

(We found that the

is easy to model.)

frames are unavoidably identified as ending logo frames, and
vice versa. On the other hand, because of the difference between

starting and ending logo transitions in sports programs, if we use

only one logo template for both starting and logo transitions,
some logo frames will be missed.

Replay segment identification

2.3

Because the logo detection algorithm could not differentiate
between starting and ending logo transitions, to identifY the
replay segments, we develop a probabilistic method based on the

a

<a)

(b)

Probability

pP (x I A) ,

density

distances of non-logo

frames,

log(dl' (i».

a

describing

pP ( x I A)

logo frames. Because the number of logo frames is much

reasonable. The histogram of x of one-hour sports video clip is

Probability

(b), which has a rough shape of a Gaussian

pP (x I IT),

density

replays are

a

probability

describing

distances of logo frames, is difficult to estimate because logo

10-20

we

assume

distribution between

0

that

pp(x I TI)

L

is

a

information on the

-

(L _LO)2 � (L -L )
20'2
�
0
L

)

L represents the duration
L. and u(x) is the unit step
X<O). Because the duration of

(u(x)=l

when

x�; u(x)=O

when

replays is always longer than three seconds, the origin of the

Rayleigh distribution is offset by Lo

=

3 seconds. Figure S (a)

shows the normalized histogram of the durations of 122 replays
and the Rayleigh function with 0 estimated from them.
L

1\

and xo' where Xo denotes the threshold

pI' (xl TI)= l/(Pp - 20'p).

{

ex

where r represent the class of replays,

uniform

value is set to ILP - 20 p' as the two lines shown in Figure

,
�
t, .. M .• t
J
, ..

�
'OI

�
�I

4.

.

�

�
'"

'JI

dH (i), we pH (y I A) and pH (y I IT),
y = 1 0g(dH (i». The probability pH (y I A) is Gaussian

Similarly, to model
where

distribution with parameters /J.H and 0 H estimated in the same
fashion of

ILP and

0

distribution between

p' The probability pH(y I IT)
0

is a uniform

and Yo' where Yo denotes the threshold

that separates classes of logo and non-logo frames. The value of
the threshold Yo is set to be ILH - 20 • Therefore,

H

pH(yIIT)=lI(P -20'H)'
H

10g(dH (i)) < Yo' then frame i is a
potential
logo
frame
with
a
posteriori probability
P
p(IT I dl'(j),dH (j». The probability p(IT I d (J),dH (j»
Ifboth l og(d l' ( i» < Xo and

describes the reliability of the detected logo frame j.

is

calculated by the Bayes' rule in the probability theory with
assumption of independence between

dP (i)

and

the

a replay, Oi. is the covariance of

that separates the classes of logo and non-logo frames and its
Therefore,

and

SLO-MO

seconds long and the temporal distances

p(LID= L-Lo
0'2

frames are unknown and they are in small number. For the sake

of simplicity,

priori

and

well modeled by a Rayleigh distribution:

are

smaller than the number of non-logo frames, the estimation is

distribution.

In identifYing the replay segments, a

replays

frames

lengths of replays provides very useful information. Usually

is modeled by a Gaussian

estimated by x of all the frames including both logo and non­

4

the lengths of

between potential logo

between replays are much longer. Duration of replays can be

probability

distribution. The mean ILl' and covariance 0 I' in

shown in Figure

information on

segments.

Figure 4 Pixelwise intensity-based MSDs of one-hour sports
video clip. (a) The x-axis is the frame index i, and the y-axis

is log(dl' (i». (b) The histogram of

priori

relationship

dH (i).

.,
Figure S. <a) The duration of replays. The solid eurve is the

histogram of duration of 122 replays, and the dashed eurve
is the Rayleigh distribution with 0 estimated from the
L

data. (b) Five patterns on relationships of detected SLO­
MOs and logo frames, where the arrows indicate the
locations of detected logo, and the thick bars indicate SLO­
Mos.
Nevertheless, when several replays are positioned back-to-back,

i.e., the intervals between replays is comparable to the duration

of the replays, using the information on the lengths of replays

alone is not enough.
To attack this problem, in addition to the duration information,
we further use the five possible patterns of relationships between

the detected logo frames and SLO-MOs to guide the grouping of

them into replays, as shown in Figure 5 (b).

The potential logo frames could not be identified as starting or
ending logo transitions, although we use different starting and
end logo templates. Because of the similarities between the
starting and the ending logo transitions, some starting logo

IV - 3387

The five patterns could not be identified simultaneously. The
priority is from 1 to 5. For example, pattern 2 has to wait until
pattern 1 has been examined, because pattern 1 could be

grouped into pattern 2 while pattern 2 does not have the risk of
being grouped into pattern 1 .

the new approach is higher than that of SLO-Mq as the new
approach is not affected by missing fields. Further, the new
algorithm achieves higher recalJ performance since' it is capable
of detecting replays even when they are rendered in regular
speed or effects of slow motion are produced by using high
speed cameras. The boundary accuracy is defined as the ratio of
the miss-detected fields over the total number of fields in the
detected replays. Since the new algorithm characterizes
boundaries of replays as logo transitions and directly detects
them, it has a higher boundary accuracy than our previous SLO­
MO detection algorithm that treats boundaries as ordinary
gradual transitions.

Therefore, the algorithm scans the detected SLO-MOs and logo
frames by 5 times, and each time only groups some SLO-MOs
and logo frames according to one pattern. The order of scanning
is from pattern 1 to 5. Each time the algorithm evaluates a
probability associated with the pattern, which describes the
possible replays based on a posteriori probabilities of detected
logo frames and the probability of replay durationp(LI D:
Different patterns have different probabilities to evaluate. For
patterns 1 and 4 that have two logo frames i and j, the

P

probability is p (L , qd (i),d

H (i); dP j ) ,dH (j», which is
(

interpreted as "given (dP (i),dH (i» and (dP {j),dH{j» of two
detected logo frames i and j, the probability that this segment
from
frame
i
to j is a
replay (L=j-i) ".
If

P
P
H
p(L,qd (i),d (i); d (j),dH (j» exceeds an empirically
chosen threshold, then the segment from frame i to j is a replay.

Table 1 Replay detection by logo detection versus slow
motion detection

We have observed that the misses and inaccuracy of the new
algorithm are due to the fact that (1) some replays have no logo
transitions, or they start with the usual starting logo transition
but end with a unique ending logo transition showing the game
statistics and (2) occasionally, a "banner" overlay appears at the
bottom of the screen for several minutes providing information
about another game. The banners resize the original frames and
significantly change the positions of the logo area.

This probability is calcu lated by

=

p(L,q dP(i),dH (i);dP (j),dH (j»
�(n I dP(i),dH (i»p(n I dP(j),dH (j»p(L I r)

where a is a normalization factor. The probability is
proportional to the two aposteriori probabilities of two detected
logo frames and the duration of the segment.
For patterns 2 and 3 that only have one logo frame: i, the
probability is p(L,r I dP(i),dH(i» which is interpreted as
"given

(dP(i),dH (i»

of the detected logo frames i,

We have presented a new algorithm for automatic detection of
replay segments in broadcast sports and demonstrated its
successful perfonnance in the case of real broadcast video. The
algorithm is fully automatic and is capable of detecting replay
segments that render events in slow motion or in regular speed.
Its performance is also insensitive to slow-motion segments that
are captured with high-speed cameras and to content whose
fields are sub-sampled during compression. The algorithm
dete cts the replay segment boundaries with high accuracy.
During the conference, we will demonstrate automatic football
and basebalJ broadcast summarization utilizing the proposed
replay detection algorithm in conjunction with the event
detection algorithms given in [3].

probability that this segment is a replay and the length is L ".
Similarly,

we

use

an

empirically

chosen

p(L,fi dP(i),dH (i» to make decisions. We have
p(L,r I dP(i),dH(i»

=

threshold

on

pen I dP(i),dH(i»p(L I n

that does not have logo frame, the probability is
Similarly, a threshold is further chosen to make

For pattern 5

pel I n·

decisions.
Finally, after five times of scanning, the algorithm marks
remaining ungrouped potential logo frames as non-logo frames.

2.4

Experimental results

We have evaluated the performance of our algorithm using

4

14

football and baseball broadcast clips recorded from several
different cable television stations, such as ESPN, FOX, TBS and
CBS, over the period of one year. The length of clips ranges
from 20 to 8S minu tes , and th e total length of 14 clips is 730
minutes 29 seconds. The data were captured using a
commercialJy available PC board in MPEG-l format at 320x240
picture size and at 30 pictures per sec. where one out of every
two fields of the input video was dropped by the encoder. In
total, the test suite has six different broadcast logos and 187
replay segments. On a regular PC with an AMD 1 G Hz CPU,
our algorithm processes an input video program in one third of
its normal playback time.

REFERENCES

[1]
[2]

http://www.cselt.itlmpeg!standardslmpeg- 7/mpeg. 7 .httn

[3]

B.

[4]

H.

[5]

R. Lienhart and A. Zaccarin, "A system for reliable dissolve
detection in videos," (CIP 2001, Vancouver BC, Septe�ber 2001.

[6]

N.Babaguchi, Y. Kawai, Y. Yasugi and T.Kitahashi,

http://www.tv-anvtime.org!

Li

and M.

J.

Sezan, "Event Detection and Summarization Sports

Video", to be presented in IEEE Workshop on Content-based
Access of Image and Video Libraries, Kauai, HI, December, 200!.

Pan, P. van Beek, and M. J. Sezan, "Detection of slow-motion
replay segments in sports video for highlights generation,"
ICASSP

In Table I, we compare the results of our proposed algorithm
with our previous SLO-MO detection. The detection accuracy of

IV

SUMMARY AND CONCLUSION

3

the

2001, Salt

Lake City,

UT, May 2001.

"Linking
Proc.

Live and Replay Scenes in Broadcasted Sports Video," in

ACM Multimedia Workshop on Multimedia Infonnation Retrieval
(MIR2000), Los Angeles, CA, November '2000.

-

3388

Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)

CLARE: A Joint Approach to Label
Classiﬁcation and Tag Recommendation
Yilin Wang,1 Suhang Wang,1 Jiliang Tang,2 Guojun Qi,3 Huan Liu,1 Baoxin Li1
1

Arizona State University
Michigan State University
3
University of Central Florida
{yilinwang,suhang.wang,huan.liu,baoxin.li}@asu.edu,tangjili@msu.edu,guojun.qi@ucf.edu
2

and Van Zwol 2008; Wang et al. 2015a; Zhang et al. 2014;
Wang et al. 2015b; Lian et al. 2015). For social media posts,
labels in classiﬁcation often capture their high-level content, while tags in tag recommendation are likely to describe their attributes such as objects in images, actions in
videos and keywords in blogs. Therefore classiﬁcation and
tag recommendation are generally considered as two independent tasks and the majority of efforts are made to study
them separately. However, these two tasks should be connected since labels and tags of social media posts are often related(Wang, Blei, and Li 2009). For example, posts
annotated with NCAA, stadium, pac12 are likely to be labeled as football, while posts with the class label of president campaign are likely to be tagged with election, polling,
democratic and republic. In other words, tags could provide evidence for class labels, which could in turn serve as
useful contextual information for tags. However, in (Wang,
Blei, and Li 2009), there is no explicit relations between
tags and labels, which results a relative poor performance on
each task. Aforementioned intuitions motivate us to develop
a more robust joint classiﬁcation and tag recommendation
framework for social media posts.
In this paper, we investigate the problem of predicting
class labels and tags simultaneously for social media posts
by exploiting the relations between labels and tags. The
differences between traditional methods and the proposed
method are illustrated in Figure 1. As shown in Figure 1(a),
traditional methods treat these two problems separately –
classiﬁcation uses data and its labels to learn a classiﬁer (or
a label predictor), while tag recommendation uses data and
its tags to learn a tag predictor. In contrast, the proposed
framework performs classiﬁcation and tag recommendation
jointly by leveraging data, labels, tags and relations between
labels and tags as demonstrated in Figure 1(b). Since the
current methods cannot take advantage of relations between
labels and tags, we proceed to study two fundamental problems: (1) how to capture relations between labels and tags
mathematically; and (2) how to make use of it for joint classiﬁcation and tag recommendation. These two problems are
tackled by the proposed framework CLARE and our contributions are summarized as follows:

Abstract
Data classiﬁcation and tag recommendation are both important and challenging tasks in social media. These two tasks
are often considered independently and most efforts have
been made to tackle them separately. However, labels in data
classiﬁcation and tags in tag recommendation are inherently
related. For example, a Youtube video annotated with NCAA,
stadium, pac12 is likely to be labeled as football, while a
video/image with the class label of coast is likely to be tagged
with beach, sea, water and sand. The existence of relations
between labels and tags motivates us to jointly perform classiﬁcation and tag recommendation for social media data in
this paper. In particular, we provide a principled way to capture the relations between labels and tags, and propose a novel
framework CLARE, which fuses data CLAssiﬁcation and tag
REcommendation into a coherent model. With experiments
on three social media datasets, we demonstrate that the proposed framework CLARE achieves superior performance on
both tasks compared to the state-of-the-art methods.

Introduction
The increasing popularity of social media generates massive data at an unprecedented rate. For example, on average for every minute, 300 hours of video are uploaded
to YouTube 1 , 54.9 million posts are shared on Reddit2
and 30 billion photos are posted on Instagram 3 . Therefore
many techniques (or tasks) have been proposed to help organize and access social media data, among which classiﬁcation and tag recommendation are two popular ones. For
social media posts4 , classiﬁcation is to assign them class
labels (Kaplan and Haenlein 2010; Agichtein et al. 2008;
Li and Zaiane 2015), while tag recommendation aims to suggest tags to annotate them automatically (Wang et al. 2016;
Sigurbjörnsson and Van Zwol 2008).
For each task of classiﬁcation and tag recommendation,
we have witnessed a large body of literature in recent
years (Chen, Zheng, and Weinberger 2013; Sigurbjörnsson
c 2017, Association for the Advancement of Artiﬁcial
Copyright 
Intelligence (www.aaai.org). All rights reserved.
1
https://www.youtube.com/yt/press/statistics.html
2
www.redditblog.com/2014 12 01 archive.html
3
https://instagram.com/press/
4
In this paper, we use posts in a loose sense to cover blogs,
microblogs, images and videos.

• We provide a principled approach to model relations between labels and tags, which bridges the tasks of classiﬁcation and recommendation;

210

(a) Traditional Methods

(b) The Proposed Method

Figure 1: Differences between existing methods and the proposed method.
• We propose a novel joint framework CLARE, which can
predict class labels and tags for social media posts simultaneously and achieve better performance than the current
state-of-the-art methods on both tasks. It is worth noting
that the proposed algorithm explores the joint relations
between labels and tags, however, it do NOT require a test
example come with labels or tags to annotate one another;
• We conduct experiments on various social media datasets
to analyze and understand the inter-working of the proposed framework CLARE.

For tag recommendation, we also assume that there is a
linear function Wt ∈ Rd×c1 which captures the relation between X and Yt as Yt = XWt . Similarly the optimization
problem to learn Wt is:

The framework CLARE

where Y = [Yt , Yc ] ∈ Rn×(c1 +c2 ) and W = [Wt , Wc ] ∈
Rd×(c1 +c2 ) . Note that though we rewrite the basic models of
classiﬁcation and tag recommendation into a uniﬁed formulation, we still consider them as two independent tasks since
we do not capture any relations between these two tasks.

min Ω(Wt ) + L(XWt , Yt )
Wt

Combining Eq. (1) and Eq. (2), we can obtain a uniﬁed
basic model for classiﬁcation and tag recommendation as:
min Ω(W) + L(XW, Y)
W

Before detailing the proposed framework, we ﬁrst introduce
notations we used in this paper. We use X ∈ Rn×d to denote
a set of social media posts where n is the number of posts
and d is the number of features. Note that there are various
ways to extract features to represent social media posts such
as raw features and features learned via deep learning techniques (Jia et al. 2014). Let Yt ∈ Rn×c1 be the post-tag
matrix where c1 is the number of tags. Yt (i, j) = 1 if the
i-th post is annotated the j-th tag and Yt (i, j) = 0 otherwise. Similarly we use Yc ∈ Rn×c2 to represent the class
label afﬁliation matrix where c2 is the number of class labels. Yc (i, j) = 1 if the i-th post is labeled as the j-th class
and Yc (i, j) = 0 otherwise. In the following subsections,
we ﬁrst introduce basic models for classiﬁcation and tag recommendation, then detail the model component to capture
relations between labels and tags and ﬁnally discuss the proposed framework.

In the previous subsection, we deﬁned a uniﬁed formulation
for classiﬁcation and tag recommendation. Capturing relations between labels and tags can further pave a way for us
to develop a joint framework that enables simultaneous classiﬁcation and tag recommendation.
The relations between labels and tags can be denoted as
a bipartite graph as shown in Figure 1(b). We assume that
B ∈ Rc2 ×c1 is the adjacency matrix of the graph where
B(i, j) = 1 if both the i-th label and the j-th tag co-occur
in the same posts and B(i, j) = 0 otherwise. Note that in
this paper, we do not consider the concurrence frequencies
of tags and labels and we would like to leave it as one future
work. From the bipartite graph, we can identify groups of
labels and tags that share similar properties such as semantical meanings. A feature X(:, j) should be either relevant
or irrelevant to labels and tags in the same group. In W,
Wc (i, j) indicates the effect of the i-th feature on predicting the j-th label; while Wt (i, k) denotes the impact of the
i-th feature on the k-th tag. Therefore we can impose constraints on W, which are derived from group information on
the bipartite graph, to capture relations between labels and
tags.

For classiﬁcation, we assume that there is a linear classiﬁer
Wc ∈ Rd×c2 to map X to Yc as Yc = XWc . Wc can be
obtained by solving the following optimization problem:
Wc

(3)

Capturing Relations between Labels and Tags

Basic Models

min Ω(Wc ) + L(XWc , Yc )

(2)

(1)

where L() is a loss function and Ω is a regularization penalty
to avoid overﬁtting. Popular choices of L include square,
logistic and hinge loss functions.

211

In this paper, we use overlapped group lasso to extract
groups from the bipartite graph – for the i-th label, we consider the label and tags that connect to that label in the bipartite graph as a group, i.e., B(i, j) = 1. Note that a tag may
connect to several labels thus groups formed via the aforementioned process have overlaps. Assume that G is the set
of groups we detect from the label-tag bipartite graph and we
propose to minimize the following term to capture relations
between labels and tags as:

min XW − Y2F + β W2F +

W,V

c2
d 


αj V(i, (c1 + c2 ) · (j − 1) + 1 : (c1 + c2 ) · j)2

i=1 j=1

s.t.V = WM
(7)

where αg is the conﬁdence of the group g and wgi is a vector
concatenating {W(i, j)}j∈g . For example, if g = {1, 5, 9},
wgi = [W(i, 1), W(i, 5), W(i, 9)].
Next we discuss the inner workings of Eq. (4). Let us
check the 
terms in Eq. (4) related to a speciﬁc group g,
d
wgi  , which is equal to adding a 1 norm on the veci=1
2
tor g = [wg1 , wg2 , . . . , wgd ], i.e., g1 . That ensures a sparse
solution of g; in other words, some elements of g could be
zeros. If gi = 0 or wgi 2 = 0, the effects of the i-th feature on both the label and tags in the group g are eliminated
simultaneously.

where αj means the conﬁdence of the j-th group and M ∈
{0, 1}(c1 +c2 )×c2 (c1 +c2 ) is deﬁned as: if the i-th tag connects
to the j-th label, then M (i, (c1 + c2 )(j − 1) + i) = 1, otherwise it is zero. Here we do not assume any prior knowledge
available on the group weight. In the following, for simplicity and without loss of generality, we assume αj = α, ∀j.
2
For brevity, we denote L(X, Y, W) = XW − YF ,
Ωgroup (V) to be the group regularizer, and Ωreg to be the
F-norm regularizer. To apply ADMM, we use augmented
Lagrangian of Equation 7:
Ωgroup (V)+Ωreg (W) + L(X, Y, W)+
ρ
2
T r(μT (V − WM)) + V − WMF
2
(8)
where μ is the Lagrange variables, and ρ > 0 is the parameter that controls the quadratic penalty.

The Proposed Framework

Updating W

Incorporating the component to capture relations between
labels and tags leads us to the following joint framework for
classiﬁcation and tag recommendation:

If V and μ are ﬁxed, the objective function is decoupled and
the constraints are independent of W. Thus we can optimize
W separately and ignore the terms without W, leading to
the following:

d 


 
αg wgi 2

(4)

i=1 g∈G

min Ω(W) + L(XW, Y) + α
W

d 


 
αg wgi 2

(5)

min J(W) = Ωreg (W) − T r(μT WM) + L(X, Y, W)+
W

i=1 g∈G

ρ
V − WM2F
2

where the ﬁrst and second terms are the basic models for
classiﬁcation and tag recommendation; and the third term
captures the relations of these two tasks. The parameter α
controls the contribution of the third term.
In this paper, we choose square loss as the loss function
L and the ridge regularization as the regularization penalty
Ω. With these choices, the optimization problem for the proposed framework CLARE can be rewritten as:
min
W

2

XW − YF + α

d 



2
ρ
μ 
∼

WM
−
(V
+
)
= min Ωreg (W) + L(X, Y, W) + 
W
2
ρ F

= T r((XW − Y)T (XW − Y)) + βT r(WT W)+
ρ
μ
μ
T r([WM − (V + )]T [WM − (V + )])
2
ρ
ρ

(9)

Taking the derivation of J(W) and setting it to zero, we
obtain the following form:
1
ρ
1
( βI + XT X)W − XT Y + W( MMT + βI)
2
2
2
(10)
ρ
μ
T
− (V + )M = 0
2
ρ
In Equation 10, solving W is intractable. On the other hand,
XT X + 12 βI and ρ2 MMT + 12 βI are symmetric and positive
deﬁnite. Thus we employ eigen decomposition for each of
them:
1
βI + XT X = U1 Λ1 UT1
2
(11)
ρ
1
MMT + βI = U2 Λ2 UT2
2
2
where U1 , U2 are eigen vectors and Λ1 , Λ2 are diagonal matrices with eigen values on the diagonal. Substituting
XT X + 12 βI and ρ2 MMT + 12 βI in Eq 10:

 
2
αg wgi 2 + β WF

i=1 g∈G

(6)

An Optimization Algorithm for CLARE
Since the group structures are overlapped, directly optimizing the objective function is difﬁcult. We propose to use alternating directions method of multipliers (ADMM)(Boyd
et al. 2011; Yogatama and Smith 2014) to optimize it. The
central idea in ADMM is to break the optimization problem down into subproblems, each depending on a subset of
the dimensions of W. Specially, we introduce an auxiliary
variable V ∈ Rd×c2 (c1 +c2 ) , and for each subproblem i, we
encode the group constrains to Vi which has the same dimension of the group size. Therefore the objective function
to be minimized by ADMM is:

212

Algorithm 1 An Optimization Algorithm for CLARE
U1 Λ1 UT1 W − XT Y + WU2 Λ2 UT2 −

ρ
μ
(V + )MT = 0
2
ρ
(12)

1: Input: {X, Y, M}α, β, μ
2: Output: c1 tags label and c2 data labels for each data

instance.

Multiplying UT1 and U2 from left to right on both sides,
 = UT WU2 and Q = UT [XT Y + ρ (V +
and letting W
μ
T
ρ )M ]U2 ,

1

1

Eq 12 becomes:
 + WΛ
 2=Q
Λ1 W

3: Initialization: W = 0
4: Precompute Eigen vectors U1 , U2 eigen values

2

Λ 1 , Λ2

5: while Not Converge do
6:
Calculate Q = UT1 [XT Y + ρ2 (V +

(13)

 use Eq 14 and update W = U1 WU
 T
7:
Compute W
2
8:
parfor i ← 1, d do (computed in parallel)
9:
for j ← 1, c2 do
10:
Vi,j = proxΩgroup , (Zi,j )
11:
end for
12:
end parfor
13:
Update μ = μ + ρ(V − WM)
14: end whileEnd
15: Using max-pooling for XW to predict tags and labels.

 and W as:
Then, we can obtain W
 l) = Q(s, l)
W(s,
λs1 + λl2

(14)

 T
W = U1 WU
2

(15)

where λs1 is the s-th eigen value of XT X + 12 βI and λl2 is
l-th eigen value of ρ2 MMT + 12 βI.

Convergence Analysis
Since the sub-problems are convex for W and V, respectively, Algorithm 1 is guaranteed to converge because they
satisfy the two assumptions required by ADMM. The proof
of the convergence can be found in (Boyd et al. 2011).
Specially, Algorithm 1 has dual variable convergence. Our
empirical results show that our algorithm often converges
within 100 iterations for all the datasets we used for evaluation.

Updating for V
If W and μ are ﬁxed, the V can be obtained by the following
optimization problem:
ρ
2
Ωgroup (V) + T r(μT V) + V − WMF
2

2
μ 
ρ
∼

+ const
= min Ωgroup (V) + V − (WM − )
V
2
ρ F
(16)
When applied to the collection of groups for the parameters, V, Ωgroup (V) no longer have overlapping groups. We
denote the j-th group in i-th row as Vi,j = V(i, (c1 + c2 ) ·
(j − 1) + 1 : (c1 + c2 ) · j), similarly for Mi,j , which is
deﬁned as the columns of M corresponding to the groups.
Hence we can solve the problem separately for each row of
V within one group:

F
ρ
μi,j 

(17)
minα Vi,j 2 + Vi,j − ((WM)i,j −
)
Vi,j
2
ρ 2
min
V

Time Complexity Analysis
The main computation cost for W involves the eigen decomposition on XT X + 12 βI and 12 ρMMT + 12 βI; and the
computation of Q = UT1 [XT Y + ρ2 (V + μρ )MT ]U2 . The
time complexity for Eigen decomposition is O(d3 ). However, in Algorithm 1 the Eigen decomposition is only computed once before the loop. The computation cost for Q is
O(nd2 ) due to the sparsity of M. The computation of V
depends on the proximal method within each group. Since
there are c2 groups which have the group size c1 + c2 for
each feature dimension, the total computation cost for V is
O(dc2 (c1 + c2 )). It is worth noting that V can be computed
in parallel for each feature dimension. Similarly, the computational cost of μ depends on the computation of WM,
which is O(dc21 ).

Note that Eq 17 is the proximal operator (Yuan, Liu, and Ye
μ
2011) of ρ1 (V)i,j applied to (WM)i,j − ρi,j . Let Zi,j =
μi,j
(WM)i,j − ρ . The solution by applying the proximal
operator used in non-overlapping group lasso to each subvector is:
Vi,j = proxΩgroup , (Zi,j )

0
if Zi,j 2  αρ
= Zi,j 2 − αρ
Zi,j
otherwise.
Zi,j 

μ
T
ρ )M ]U2

Experimental Analysis
In this section, we conduct experiments to evaluate the effectiveness of CLARE. After introducing datasets and experimental settings, we compare CLARE with the state-of-the
art methods of tag recommendation and classiﬁcation. Further experiments are conducted to investigate the effects of
important parameters in CLARE.

(18)

2

Updating for μ
Updating μ is simple and is deﬁned as μ = μ+ρ(V−WM).
With the updating rules, the proposed algorithm for
CLARE is summarized in Algorithm 1.

Experiments Settings
The experiments are conducted on 3 publicly available social media datasets.

213

USAA-YouTube dataset (Fu et al. 2014): All 1500
videos in the dataset are unstructured and unedited which
are either taken by digital camera or mobile phones and they
are originally uploaded on the social media website Youtube.
It has 8 class labels with totally 69 manually annotated tags.
These eight classes are birthday party, graduation party, music performance, non-music performance, parade, wedding
ceremony, wedding dance and wedding reception. The size
of tag vocabulary is 69 and example tags are: dancing, eating, fast-moving, garden, living room, conversation, presentation, bride, ﬂag, and candles. We use three types of features
including: scale invariant feature transform (SIFT), Melfrequency cepstral coefﬁcients (MFCC), and spatial temporal interest points (STIP).
NUS-Flickr dataset (Chua et al. 2009): It contains
269,648 images crawled from Flickr. The images are linked
to 5,000 different user tags, which are annotated by users
registered on Flickr. Beyond these images and user tags, the
images are labeled with 81 concepts. We use the most common 1k tags in our experiment. We also ﬁlter out those images with less than 7 tags, resulting in 110k images. 500dimensional visual features are extracted using a bag-of
visual-word model with local SIFT descriptor.
Shoe-Zappo dataset (Yu and Grauman 2014): It is a large
shoe dataset consisting of 50,025 catalog images collected
from Zappos.com. The images are divided into 4 major categories: shoes, sandals, slippers, and boots. The tags are functional types and individual brands such as high-heel, oxford,
leather, lace up, and pointed toe. The number of tags is 147
and we extract LAB color features and GIST features (Oliva
and Torralba ) as (Yu and Grauman 2014).
We use accuracy as the metric to assess classiﬁcation performance since all datasets are relatively balanced. To evaluate the performance of tag recommendation, we rank all
tags based on their relevant scores and return the top K
ranked tags. We use the average precision AP @K as the
evaluation metric which has been widely used in the literature (Chen, Zheng, and Weinberger 2013; Lin et al. 2013;
Wang, Blei, and Li 2009). Although, the proposed model
can incorporate any advanced features, e.g., CNN feature,
we use the features provided by (Fu et al. 2014),(Chua et
al. 2009), and (Yu and Grauman 2014), respectively, for fair
comparison. 5 .

also consider tags as a kind of labels and apply GLasso to
learn the mapping of features to labels and tags. Note that
it does not make use of the label-tag bipartite graph. We
use the implementation in (Liu et al. 2009);
• sLDA (Wang, Blei, and Li 2009): It is a joint framework
based on topic models, which learns both class labels and
annotations given latent topics;
• LS (Ji et al. 2008): A multi label classiﬁcation method that
exploits the label correlation information. To apply LS for
joint classiﬁcation and tag recommendation, we consider
tags as a kind of labels and use tag and label relations to
replace the label correlation in the original model; and
• FT (Chen, Zheng, and Weinberger 2013): It is one of the
state-of-the art annotation method which is based on linear mapping and co-regularized joint optimization. To apply it for classiﬁcation, we consider labels as tags to annotate; and
• RD: It predicts labels and tags by randomly guessing.
For all baseline methods with parameters, we use crossvalidation to determine their values. Each time we choose
60% of a dataset as training data and the remaining as testing. Since only sLDA and GLasso are the baselines for joint
label prediction, we present the result separately for demonstration.
From the tables, we make the following observations:
• The proposed method considers classiﬁcation and tag
recommendation jointly in a bipartite graph structure
tends to outperform the methods which treat them separately. These results support that (1) tags can provide evidence for the classiﬁcation; especially for the NUS-Flickr
dataset that contains 81 label classes, those methods utilize information from tags signiﬁcantly improve the classiﬁcation performance. (2) The performance of tag recommendation AP @K indicates that the class label contains important information for tag recommendation;
• The proposed method with model components to capture relations between labels and tags outperform those
without. For example, compared to GLasso and sLDA,
the proposed framework, modeling the label-tag bipartite graph, gain remarkable performance improvement for
both classiﬁcation and tag recommendation; and
• Most of the time, the proposed framework CLARE performs the best among all the baselines, which demonstrates the effectiveness of the proposed algorithm. There
are two major reasons. First, CLARE jointly performs
classiﬁcation and recommendation. Second, CLARE captures relations between labels and tags by extracting
group information from the label-tag bipartite group,
which works as the bridge between classiﬁcation and tag
recommendation. More details about the effect of the relations between labels and tags on CLARE will be discussed in the following subsection.

Performance Comparison
We compare CLARE with the following representative algorithms:
• SVM (Chang and Lin 2011): It uses the state of the art
classiﬁer SVM for classiﬁcation with linear kernel; We
also apply it to tag prediction by considering tags as a
kind of labels;
• GLasso (Yuan and Lin 2006): The original framework is
to handle high-dimensional and multi-class data. To extend it for joint classiﬁcation and tag recommendation, we

Parameter Analysis

5

If we use advanced features, it would be hard to tell whether
the performance gain comes from the feature or the model we proposed.

There are two important parameters for the proposed framework CLARE – α controlling the contribution from the

214

Table 1: Performance comparison in terms of classiﬁcation.
Method
SVM
GLasso
sLDA
LS
FT
RD
CLARE

USAA-YouTube(8 class)
36.15%
38.25%
32.28%
39.39%
39.26%
12.49%
53.07%

NUS-Flickr (81 class)
18.92%
27.51%
26.12%
34.39%
35.67%
1.23 %
40.32%

Shoe-Zappo (4 class)
75.57 %
76.31%
74.32%
86.03%
85.39%
25.01%
89.39%

Table 2: Performance comparison in terms of tag recommendation.
Method
SVM
GLasso
sLDA
LS
FT
RD
CLARE

USAA-YouTube (69 tags)
AP @3
AP@5
AP @10
58.45% 53.53% 48.95%
59.32% 55.12% 47.31%
37.32% 31.12% 17.31%
61.96% 57.77 % 50.94%
62.42% 57.52% 51.78%
1.44%
1.43%
1.44%
77.10% 71.08% 62.95%

NUS-Flikr (1k tags)
AP @3
AP@5
AP @10
18.71%
13.12% 10.92%
17.50%
13.43% 10.11%
18.95%
14.26% 11.78%
22.69% % 17.21% 13.35%
21.35%
16.77% 13.43%
0.10%
0.11%
0.11%
21.22%
16.18% 13.94%

model component of capturing relations between labels and
tags and β controlling the regularization penalty. In this subsection, we investigate the impact of these parameters on the
performance of the proposed framework.
To
study
the
impact
of
α,
we
ﬁx
β
=
0.1 and vary the value of α as
{10−6 , 0.01, 0.1, 0.2, 0.3, 0.5, 0.7, 1.5, 2, 10, 100}.
The
performance variance w.r.t α is shown in Figure 2. Due to
the page limitation, we only show results from Shoe-Zappo
since we have similar observations on other datasets. In
general, with the increase of α, the CLARE performance
increases signiﬁcantly ﬁrst, reaches its peak and then drops
with larger values. Especially, when α increases from 10−6
to 0.01, the CLARE performance increases almost 8%,
which suggests the importance of the model component
to capture relations between labels and tags. When α is
in [0.3, 1], the performance is relatively stable. When α
increases from 10 to 100, the performance decreases dramatically. Indicating the model component will dominate
the learning process and the learned parameters could
overﬁt.

Shoe-Zappo (147 tags)
AP @3
AP@5
AP@10
52.29% 46.17% 38.07%
58.32% 49.22% 42.31%
61.32% 57.12% 49.31%
73.56% 66.42% 61.49%
69.01% 60.77% 57.85%
0.67%
0.67%
0.68%
76.74% 69.47% 63.71%

Figure 3: Performance variance w.r.t. β. Note that the Y axis
denotes the performance and X axis is the value of β.
{10−6 , 10−4 , 10−2 , 10−1 , 10, 102 , 104 , 106 , } and ﬁx α =
0.3. The performance of CLARE with the changes of β is
demonstrated in Figure 3. When β is small, CLAER may
overﬁt, which leads to poor performance. When β becomes
larger, the learned W will have larger amount of shrinkage
and values in W become more robust to collinearity. However, if the β becomes extremely large, the regularization
penalty dominates the learning process and elements of W
tend to be zeros, which results in poor performance as well.

Conclusion and Future Work
Due to the relations between labels and tags, we study
the problem of joint classiﬁcation and tag recommendation in this paper. We extract group information from the
label-tag bipartite graph as constraints to bridge classiﬁcation and tag recommendation and propose a novel framework CLARE that performs classiﬁcation and tag recommendation simultaneously. Experiments on three social media datasets demonstrate that: (1) joint classiﬁcation and recommendation can improve performance for each task; and
(2) the importance to consider relations between tags and
labels. In the future, we will include robust community de-

Figure 2: Performance variance w.r.t. α. Note that the Y axis
is the performance and X axis is the value of α.
To study the impact of β, we vary the values of β as

215

Liu, J.; Ji, S.; Ye, J.; et al. 2009. Slep: Sparse learning with
efﬁcient projections. Arizona State University 6:491.
Oliva, A., and Torralba, A. Modeling the shape of the scene:
A holistic representation of the spatial envelope. IJCV.
Sigurbjörnsson, B., and Van Zwol, R. 2008. Flickr tag
recommendation based on collective knowledge. In WWW,
327–336. ACM.
Wang, S.; Tang, J.; Wang, Y.; and Liu, H. 2015a. Exploring
implicit hierarchical structures for recommender systems. In
Proceedings of the 24th International Conference on Artiﬁcial Intelligence, 1813–1819. AAAI Press.
Wang, Y.; Wang, S.; Tang, J.; Liu, H.; and Li, B. 2015b.
Unsupervised sentiment analysis for social media images.
In International Joint Conferences on Artiﬁcial Intelligence.
Wang, Y.; Wang, S.; Tang, J.; Liu, H.; and Li, B. 2016.
PPP: Joint pointwise and pairwise image label prediction.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition.
Wang, C.; Blei, D.; and Li, F.-F. 2009. Simultaneous image
classiﬁcation and annotation. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on,
1903–1910. IEEE.
Yogatama, D., and Smith, N. 2014. Making the most of
bag of words: Sentence regularization with alternating direction method of multipliers. In Proceedings of the 31st
International Conference on Machine Learning (ICML-14),
656–664.
Yu, A., and Grauman, K. 2014. Fine-Grained Visual Comparisons with Local Learning. In Computer Vision and Pattern Recognition (CVPR).
Yuan, M., and Lin, Y. 2006. Model selection and estimation in regression with grouped variables. Journal of the
Royal Statistical Society: Series B (Statistical Methodology)
68(1):49–67.
Yuan, L.; Liu, J.; and Ye, J. 2011. Efﬁcient methods for
overlapping group lasso. In Advances in Neural Information
Processing Systems, 352–360.
Zhang, Q.; Zhou, J.; Wang, Y.; Ye, J.; and Li, B. 2014. Image
cosegmentation via multi-task learning. In BMVC.

tection to extract the group in order to avoid noise tags.

Acknowledgment
Yilin Wang and Baoxin Li were supported in part by an ARO
grant (#W911NF1410371) and an ONR grant (#N0001415-1-2722). G.-J. Qi is partly sponsored by NSF Grant
1560302.Any opinions expressed in this material are those
of the authors and do not necessarily reﬂect the views of
ARO, ONR or NSF.

References
Agichtein, E.; Castillo, C.; Donato, D.; Gionis, A.; and
Mishne, G. 2008. Finding high-quality content in social media. In Proceedings of the 2008 International Conference on
Web Search and Data Mining, 183–194. ACM.
Boyd, S.; Parikh, N.; Chu, E.; Peleato, B.; and Eckstein, J.
2011. Distributed optimization and statistical learning via
the alternating direction method of multipliers. Foundations
R in Machine Learning 3(1):1–122.
and Trends
Chang, C.-C., and Lin, C.-J. 2011. Libsvm: A library for
support vector machines. ACM Transactions on Intelligent
Systems and Technology (TIST) 2(3):27.
Chen, M.; Zheng, A.; and Weinberger, K. 2013. Fast image
tagging. In Proceedings of the 30th international conference
on Machine Learning, 1274–1282.
Chua, T.-S.; Tang, J.; Hong, R.; Li, H.; Luo, Z.; and Zheng,
Y. 2009. Nus-wide: a real-world web image database from
national university of singapore. In Proceedings of the ACM
international conference on image and video retrieval, 48.
ACM.
Fu, Y.; Hospedales, T. M.; Xiang, T.; and Gong, S. 2014.
Learning multimodal latent attributes. Pattern Analysis and
Machine Intelligence, IEEE Transactions on 36(2):303–316.
Ji, S.; Tang, L.; Yu, S.; and Ye, J. 2008. Extracting shared
subspace for multi-label classiﬁcation. In Proceedings of the
14th ACM SIGKDD international conference on Knowledge
discovery and data mining, 381–389. ACM.
Jia, Y.; Shelhamer, E.; Donahue, J.; Karayev, S.; Long, J.;
Girshick, R.; Guadarrama, S.; and Darrell, T. 2014. Caffe:
Convolutional architecture for fast feature embedding. In
ACM MM, 675–678. ACM.
Kaplan, A. M., and Haenlein, M. 2010. Users of the
world, unite! the challenges and opportunities of social media. Business horizons 53(1):59–68.
Li, J., and Zaiane, O. 2015. Associative classiﬁcation with
statistically signiﬁcant positive and negative rules. In Proceedings of the 24th ACM International on Conference on
Information and Knowledge Management. ACM.
Lian, W.; Rai, P.; Salazar, E.; and Carin, L. 2015. Integrating
features and similarities: Flexible models for heterogeneous
multiview data.
Lin, Z.; Ding, G.; Hu, M.; Wang, J.; and Ye, X. 2013. Image tag completion via image-speciﬁc and tag-speciﬁc linear sparse reconstructions. In Computer Vision and Pattern
Recognition (CVPR), 2013 IEEE Conference on. IEEE.

216

Discriminative K-SVD for Dictionary Learning in Face Recognition
Qiang Zhang and Baoxin Li
Computer Science and Engineering
Arizona State University, Tempe, AZ
{qzhang3,baoxin.li}@asu.edu

Abstract
In a sparse-representation-based face recognition
scheme, the desired dictionary should have good representational power (i.e., being able to span the subspace
of all faces) while supporting optimal discrimination of
the classes (i.e., different human subjects). We propose a
method to learn an over-complete dictionary that attempts
to simultaneously achieve the above two goals. The proposed method, discriminative K-SVD (D-KSVD), is based
on extending the K-SVD algorithm by incorporating the
classification error into the objective function, thus allowing the performance of a linear classifier and the representational power of the dictionary being considered at the
same time by the same optimization procedure. The DKSVD algorithm finds the dictionary and solves for the classifier using a procedure derived from the K-SVD algorithm,
which has proven efficiency and performance. This is in
contrast to most existing work that relies on iteratively solving sub-problems with the hope of achieving the global optimal through iterative approximation. We evaluate the proposed method using two commonly-used face databases, the
Extended YaleB database and the AR database, with detailed comparison to 3 alternative approaches, including
the leading state-of-the-art in the literature. The experiments show that the proposed method outperforms these
competing methods in most of the cases. Further, using
Fisher criterion and dictionary incoherence, we also show
that the learned dictionary and the corresponding classifier
are indeed better-posed to support sparse-representationbased recognition.

1. Introduction
Face recognition is a challenging computer vision task
that has seen active research for many years [23]. Wellknown, conventional approaches include Eigenface [21]
and Fisherface [3], among others. These methods usually involve two stages: feature extraction and classification. Recently, a lot of attention has been given to applying

978-1-4244-6985-7/10/$26.00 ©2010 IEEE

sparse-representation-based techniques to computer vision
and image processing problems, such as image denoising
[10], image inpainting [16], image compression [4][5]. In
particular, the SRC algorithm proposed in [22] uses sparse
representation for face recognition: training face images are
used to form a dictionary, and classifying a new face image
is achieved through finding its sparse coefficients with respect to this dictionary. Unlike conventional methods such
as Eigenface and Fisherface, SRC does not need an explicit
feature extraction stage. The superior performance reported
in [22] suggests that this is a promising direction for face
recognition.
The basic way of forming the dictionary by using all the
training images may result in a huge size for the dictionary,
which is detrimental to the subsequent sparse solver. For
example, we may have 32 images for each person (e.g., as
in the Extended YaleB database [12]). Then the number
of atoms in the dictionary will be 32 times the number of
people. Thus for a large face database with thousands of
people, the sheer size of the dictionary becomes a practical
concern. One may manually select a subset of the training images to be used in the dictionary, as done in [22]. But
this is not only tedious but also sub-optimal since there is no
guarantee that the manually-selected images form the best
dictionary. Methods for learning a small-sized dictionary
for sparse-coding from the training data have been proposed
recently. For example, the K-SVD algorithm [1] learns an
over-complete dictionary from a set of signals. The algorithm has been shown to work well in image compression
and denoising. K-SVD focuses on only the representational
power of the dictionary (or the efficiency of sparse coding
under the dictionary) without considering its capability for
discrimination. Another recent work [18] attempts to address this issue by further iteratively updating the K-SVDtrained dictionary based on the outcome of a linear classifier, hence obtaining a dictionary that may be also good
for classification in addition to having the representational
power. Other efforts along similar direction include [14]
and [15], which use more sophisticated objective functions
in dictionary optimization in the training stage in order to

2691

[vi,1 , vi,2 , . . . , vi,ni ] ∈ Rm∗ni , any test sample y ∈ Rm
from the same class will approximately lie in the subspace
spanned by the training samples associated with this class:

gain some discriminative power for the dictionary.
In this paper, we propose to extend the K-SVD algorithm to learn an over-complete dictionary from a set of labeled training face images. By directly incorporating the
labels in the dictionary-learning stage (as opposed to relying on iteratively updating the dictionary using feedback
from the classification stage as in [18]), we can efficiently
obtain a dictionary that retains the representational power
while making the dictionary discriminative (i.e., supporting sparse-coding-based classification). We also propose a
corresponding classification algorithm based on the learned
dictionary. Incorporating the classification stage directly
into the dictionary-learning procedure has the potential of
avoiding the local minima that may be encountered more often in the approach of [18], which computes the sub-optimal
solution by alternating between solving subset of parameters while fixing others. Furthermore, the complexity of the
proposed method is bounded by that of the K-SVD, while
the approach of [18] involves multiple additional optimization procedures.
To demonstrate the effectiveness and the advantage of
the proposed method for face recognition, extensive experiments have been carried out using two commonly-used face
databases: the extended YaleB database [13] and the AR
database [17]. In addition to comparing the recognition
rates of our method with those from existing state-of-the-art
approaches, we also analyze and compare the performance
of the classifiers based on Fisher criterion. The learned
dictionaries are also compared in terms of dictionary incoherence [7][9]. The experimental results show that the
proposed method has some clear advantages. In particular,
the experiments show that with the same dictionary size or
with dictionaries of randomly-chosen training images, our
method can obtain better recognition rate than the SRC algorithm.
The rest of paper is organized as follows. We first briefly
describe in Section 2 the basic formulation of the problem
of face recognition based on spare-representation using an
over-complete dictionary. Then we present the proposed algorithm in Section 3. The experiments and analysis of the
results are reported in Section 4. We conclude with discussion in Section 5.

3. Proposed Method

2. Basic Formulation of the Problem

3.1. Adding Discrimination Ability to K-SVD

It has been observed that images of human face under varying illumination conditions and expressions lie
on a special low-dimensional space [3][2]. In a sparserepresentation-based face recognition scheme like the SRC
algorithm, this observation is exploited for recognition
through sparse-coding of a testing face image using an overcomplete dictionary of the training faces. Our method follows this scheme, which we briefly outline in the below.
Given sufficient samples of the i-th person, Ai =

The above drawbacks associated with the SRC algorithm
may be overcome if we can learn a smaller-sized dictionary from the given training images while maintaining the
representational power of the dictionary. For example, the
K-SVD algorithm [1] may be employed for this purpose,
which finds a solution for the following problem:

y = ai,1 ∗ vi,1 + ai,2 ∗ vi,2 + · · · + ai,ni ∗ vi,ni

(1)

where ai,j is a scalar.
By grouping samples from all the classes, we form a dictionary D:
D = [A1 , A2 , . . . , Ak ] = [v1,1 , v1,2 , . . . , vk,nk ]

(2)

where k is the number of classes. Then the linear representation of y can be written in terms of all samples as:
y = a1,1 ∗v1,1 +a1,2 ∗v1,2 +· · ·+ak,nk ∗vk,nk = D∗x0 (3)
where x0 = [0, , . . . , 0, ai,1 , ai,2 , . . . , ai,ni , 0, . . . , 0] ∈ Rn
is a vector of coefficients whose entries are all zero except
for those associated with the i-th class.
If we extract the coefficient α0 (j) associated with the jth person and reconstruct the image as
y(j) = D ∗ α0 (j)

(4)

we can expect that the reconstruction error err(j) = ky −
y(j)k2 will be large for any general j 6= i except for err(i).
We can use this idea to recognize the test sample. While
such a scheme has been shown to be able to generate the
state-of-art results in [22], there are a few practical drawbacks associated with the method. For example,
1. In order to improve the representational power of the
dictionary, we need to use a large number of training
samples for each person. But a large dictionary is detrimental for the subsequent sparse solver.
2. In order to ensure that the dictionary atoms can span
the underlying subspace reasonably well, we need to
carefully choose the training images. For example, in
[22], for the AR database, the authors manually chose
7 normal images (without artificial disguise) from Section 1 for each person.

< D, α >= argminkY − D ∗ αk2 subject to kαk0 ≤ T
D,α

(5)

2692

where Y is the matrix of all input signals (the training face
images in our case), and T is a parameter to impose the
sparsity prior. In Eqn. 5, each column of D is normalized to have unit norm. This K-SVD formulation has been
found to work well for real images in applications such as
image denoising and face image compression. However,
since the objective function in Eqn. 5 considers only the
reconstruction error and the sparsity of the coefficient, the
learned dictionary is not optimized for a classification task.
In other words, the learned dictionary may not have the best
discriminative power despite its representational power.
Efforts have been reported on improving a dictionarylearning procedure for classification tasks. In [14] and [15],
an extra term was introduced to consider the classifier performance in dictionary learning. For a binary classifier, this
term can be represented by
X
C(hi ∗ f (αi , θ)) + λ1 ∗ kθk2 (6)
< θ >= argmin
θ

by a K-SVD-like algorithm. This is in contrast with the the
sophisticated (and computationally involving) optimization
procedures used in [14], [15], and [18]. To better illustrated
this point, we describe the following iterative procedure for
solving the problem of Eqn. 8 (which we will refer to as the
Baseline Algorithm later):
1. Initialize D and α with K-SVD by Eqn. 5;
2. Calculate W in Eqn. 7 when D and α are fixed;
3. Calculate α when D and W are fixed;
4. Calculate D when α and W are fixed;
5. Iterate Steps 2 to 4 until some criterion are met.
Essentially, the above procedure is effectively the algorithm
of [18], except that here we only consider labeled data.
Hence this Baseline Algorithm will be used in our comparison of the proposed method with that of [18].

i

where θ is the parameter of the classifier, hi is the label
and C(x) = log(1 + e−x ) is a logistic loss function . The
resultant problem is very complex and thus there is no direct method to find the solution. Instead, projected gradient
descent was used in finding approximate solutions in the
paper. Another example is [18], which uses a simpler formulation for considering the classifier performance:
< W, b >= argminkH − W ∗ α − bk2 + β 0 kW k2 (7)
W,b

where W , b are parameters for a linear classifier H =
W ∗ α + b. Each column of H is a vector: hi =
[0, 0, . . . , 1 . . . , 0, 0], where the position of non-zero element indicates the class. So kH − W ∗ α + bk2 is the classification error and kW k2 is the regularization penalty term.
We can set b to zero for simplicity.
Considering Eqn. 5 and Eqn. 7 at the same time, we can
define the following problem for learning a dictionary with
both discriminative power and representative power:

3.2. A New Algorithm: Discriminative K-SVD
The Baseline Algorithm described above can only find
an approximate solution to the problem of Eqn. 8, since
in each step of the method, it only finds the solution for a
sub-problem of Eqn. 8. While practically speaking, the final solution may converge to the real solution, the method
has big potential of getting stuck at local minima of the subproblems. Additionally, as is obvious from the Baseline Algorithm, in each iteration there are three optimization problems involved and thus the convergence, if it happens, will
be slow to reach. To get around these issues, and to leverage the proven performance of the K-SVD algorithm, we
propose the following Discriminative K-SVD (D-KSVD)
algorithm, which uses K-SVD to find the globally optimal
solution for all the parameters simultaneously. The task is
formulated as solving the following problem
< D, W, α >= argmin
D,W,α


 

Y
D
k √
− √
∗ αk2 + β ∗ kW k2
γ∗H
γ∗W

< D, W, α >= argmin

subject to kαk0 ≤ T

D,W,α

kY − D ∗ αk2 + γ ∗ kH − W ∗ αk2 + β ∗ kW k2
subject tokαk0 ≤ T
(8)
where Y is the set of input signals, D the dictionary, α the
coefficient, H the label of the training images, W the parameter of the classifier, and γ and β are scalars controlling
the relative contribution of the corresponding terms.
The above formulation may be viewed as a special case
of [18] without considering the unlabeled data therein.
However, our emphasis is on viewing the formulation of
Eqn. 8 as an extended K-SVD problem and thus the solution
(to be presented in subsequent subsections) will be solved

(9)

We adopt the
 protocol
 in the original K-SVD algorithm:
D
the matrix √
is always normalized column-wise.
γ∗W
Therefore, we can further drop the regularization penalty
term kW k2 , and thus the final formulation of the problem
can be written as:
< D, W, α >= argmin
D,W,α

 


D
Y
− √
∗ αk2
k √
γ∗H
γ∗W
subject to kαk0 ≤ T

2693

(10)

Proof: If y is a vectorized image, then
X
α i ∗ di
y = D∗α=

Now, the problem of Eqn. 10 can be efficiently solved by
updating the dictionary atom by atom with the following
method: For each atom dk and the corresponding coefficient
αk , we solve the following problem
< dk , αk >= argminkEk − dk ∗ αk kF

i

(11)

αi ∗ kdi k2 ∗

=

X

αi0 ∗ d0i = D0 ∗ α0

i

dk ,αk

P

where Ek = Y − i6=k di ∗ αi and Y is the training data.
kkF denotes the Frobenius norm. This is essentially the
same problem that K-SVD has solved and thus the the solution to Eqn. 11 is given by
U ∗Σ∗V
d˜k
α˜k

= SV D(Ek )
= U (:, 1)
= Σ(1, 1) ∗ V (1, :)

i

label

=

W ∗α=

3.3. The Algorithm for Classification
Upon the completion of training with the labeled data in
the previous D-KSVD algorithm, we obtain an learned dictionary D and a classifier W . However, the dictionary D
does not readily support a sparse-coding based representation of a new test image, since D and W are normalized
jointly in the previous learning algorithm, i.e,

i

αi ∗ di
wi
kdi k2

X

αi ∗ kdi k2 ∗

=

X

αi0 ∗ wi0 = W 0 ∗ α0

i

where U (:, 1) denotes the first column of U and V (1, :) for
the first row of V .

X

=

i

(12)

di
kdi k2

=

X

where d0i = kddiik2 and wi0 =
and W respectively.

wi
kdi k2

(15)

are the i-th column of D

With the normalized D0 , we can find the sparse coefficients for a given test image y by solving the following
problem
< α0 >= argminky − D0 ∗ α0 k2 + σ ∗ kα0 k0

(16)

α0

This is the typical sparse-coding problem and in practice we
often resort to the following convex optimization problem
< α0 >= argminky − D0 ∗ α0 k2 + σ ∗ kα0 k1

(17)

α0



d
k =1
k √ i
γ ∗ wi 2

(13)

Note that we cannot simply re-normalize D column-wise
by itself, since in the training stage W is obtained with the
original, un-normalized D. Hence, we need to figure out
a way of obtaining a valid (normalized) dictionary and the
corresponding classifier, based on the learning results, D
and W . To this end, we prove the following lemma which
establishes the relationship between the desired (D0 , W 0 )
and the learned (D, W ).
Lemma: The normalized dictionary D0 and the corresponding classifier W 0 can be computed as
D0

=
=

W0

=
=

{d01 , d02 , . . . , d0k }
d1
d2
dk
{
,
,...,
}
kd1 k2 kd2 k2
kdk k2
{w10 , w20 , . . . , wk0 }
w1
w2
wk
{
,
,...,
}
kd1 k2 kd2 k2
kdk k2

which can be solved by many L1 optimization methods,
such as GPSR [11], L1 magic [6] and so on. The stability of
the solution depends on the incoherence of D0 and sparsity
of α0 [20]. When α0 is sparse and D0 is sufficiently incoherent, Orthonormal Matching Pursuit [8] can also find the
sparse coefficient [11]. According to our experiments with
large face databases, OMP works well and run faster than
other L1-optimization methods mentioned above. Thus the
results reported in this paper are based on the OMP method.
The final classification of a test image is based on its
sparse coefficient α0 , which carries most discriminative information. We can simply apply the linear classifier W 0 to
α0 and obtain the label of the image:
l = W 0 ∗ α0

(14)

where di and wi denote the i-th column of D and W , respectively.

(18)

where l is a vector.
Note that the coefficient α0 can be viewed as the weight
of each atom in reconstructing the test image. Thus we can
view each column wk0 of W 0 as a factor for measuring the
similarity of atom d0k to each class. Therefore, l = W 0 ∗ α0
is the weighted similarity of the test image y to each class.
In this sense, the label of test image y is decided by the
index i where li is the largest among all elements of the l

2694

computed in Eqn. 18. Obviously, in the ideal case, l will be
of the form l = {0, 0, . . . , 1, . . . , 0, 0} (i.e, with only one
non-zero entry, which equals to 1).

Table 1. The result for fisher criterion in simulation experiments.

Method
Fisher Criterion

D-KSVD
1.2431

Baseline
1.0924

4. Experiments and Analysis
In this section, we first use a simulation experiment (still
based real face images) to compare the proposed D-KSVD
method with the method of [18]. (As there is no code publicly available for the method in [18], our comparison is
based on our implementation of the Baseline Algorithm discussed in Sect. 3.1, which is essentially the same as that of
[18].) Then we evaluate our method on two commonly-used
face databases: the Extended YaleB database and the AR
database. For comparison purpose, we also implemented
the SRC algorithm. To gain more insights into how the proposed D-KSVD method may gain over a plain K-SVD technique, we also implemented an algorithm that directly uses
the dictionary learned by the original K-SVD algorithm for
face recognition. The training stage of this algorithm runs
as follows:

Figure 1. Visualizing the computed l. The plot on the left is from
the Baseline Algorithm (or [18]). The plot on the right is from the
proposed D-KSVD method.
Table 2. The maximal pair-wised correlation for dictionary learned
by D-KSVD and Baseline in simulation experiment.

Method
Max R value

1. Train D with K-SVD according to Eqn. 5;

D-KSVD
0.7633

Baseline
0.7830

2. Train W with equation:
W = (αT α + β 0 ∗ I)−1 ∗ α ∗ H T

(19)

In this algorithm, D and W are trained independently. The
test stage is done similarly to what described in Sect. 3.3.
For simplicity, we will refer to this method simply as KSVD thereafter.
All the experiments were run on Matlab 2008a. The PC
we used has an Intel P4 2.8GHz CPU and 1 GB RAM.

4.1. Simulation Experiments
We used 52 images from 2 random persons in the
AR database (26 images each person) for this simulation.
These images contain all the possible conditions in the AR
database: varying expressions, varying illumination, and
different occlusion conditions. We used the same parameters in running the two competing methods: the proposed
and the Baseline Algorithm (or the method of [18]).
First, we compare the methods based on the Fisher criterion, which is commonly used to evaluate the performance of classifiers. Fisher criterion measures the ratio
of between-class variance and in-class variance. A bigger
value usually means a better classification result. For a twoclass problem, the Fisher criterion can be computed as follows:
S=

C

(µ1 − µ2 )2

C

1
2
1 X
1 X
∗
∗
(x1 (i) − µ1 )2 +
(x2 (i) − µ2 )2
C1 i=1
C2 i=1
(20)

where, µ is the mean of the data, the subscripts are the class
labels, and x is the data, which is the l computed in Eqn.
5 in this analysis (since we wanted to see how well the l’s
computed by the two methods are). We have visualized l
for all 52 images in Fig. 1. The computed Fisher criteria
of the two methods are listed in Table 1. It shows that our
method get a bigger value for fisher criterion, which means
that our method can do better classification than the method
in [18] does.
Second, we measure the incoherence of the dictionary
which is critical for sparse representation. Y. Sharon et
al. [19] proposed Equivalence Breakdown Point (EBP)
for measuring the incoherence of the dictionary. However,
computing EBP is computationally prohibitive for large dictionaries (e.g., of the size 600*500 as in our experiments).
Thus we used the correlation coefficients from pairs of the
atoms in the dictionary instead. This is calculated as
cov(x, y)
R(x, y) = p
cov(x, x) ∗ cov(y, y)

(21)

where x and y are two atoms in the dictionary, and cov
computes the covariance. A smaller coefficient R between
two atoms means that they are more incoherent. Ideally,
we want to have a small R for all possible pairs from the
learned dictionary. We computed the largest R from all
pairs in the two dictionaries learned from the Baseline Algorithm and the proposed D-KSVD algorithm, as reported
in Table 2, which shows that the proposed method learns a
better dictionary.

2695

4.2. Results with the Extended YaleB Database
The Extended YaleB database contains about 2414
frontal face images of 38 individuals. Following [22],
we used the cropped and normalized face images of
192*168 pixels, which were taken under varying illumination conditions[13]. We randomly split the database into
two halves. One half, which contains 32 images for each
person, was used for training the dictionary. The other half
was used for testing. Further, we projected the face image
∈ R192∗168 into a vector ∈ R504 with a randomly generated
matrix, which is called Randomface [22]. The learned dictionary contains 304 atoms, which corresponds to, on average, roughly 8 atoms for each person (but we must point out
that, unlike in the SRC algorithm, in our method there is no
explicit correspondence between the atoms and the labels
of the people, since all the information is encapsulated into
the discriminative dictionary and the corresponding classifier). The sparsity prior assumed in the learning was set to
T = 16.
With this database, we tested 4 methods: SRC, K-SVD
(as defined earlier in the beginning of Section 4.1), the
Baseline Algorithm (or equivalently the method of [18]),
and the proposed D-KSVD method. The best result reported
for SRC is 98.26% when there are 32 images per person in
the dictionary. We also tested the performance of SRC when
the dictionary is smaller (8 atoms per people). This set of
results are denoted by SRC† in the subsequent tables. In
[18], the authors only used a few images (at most 4) per
person for training and the recognition result was very poor
(about 66.4%). For a fair comparison, we tested the Baseline Algorithm (essentially the method of [18]) with more
training images. In short, the key learning parameters used
in the four methods were kept to be the same in our experiments.
All the results are summarized in Table 3. In the experiments, the scalar β and γ were set to 1. From the experiments, we found that most of the failure cases (about 46 out
of 54) are from images under extreme illumination conditions. Some examples of these cases are given in Fig. 2.
Thus, we performed another set of experiments with these
”bad” images excluded (13 for each person). This was intended to show the true performance of the competing methods without the interference of images of extremely bad
quality. The results of this new round experiments are listed
in the last row of the table. From the table, it is clear that
the proposed D-KSVD method always obtains better results
than the K-SVD method and the Baseline (or the method
of [18]); In addition, for dictionaries of the same size, our
method performs better than the SRC method.
We also evaluated the incoherence of the learned dictionaries in the experiments by calculating the correlation
coefficient for each pair of atoms in the dictionary. Since
the experiments involve multiple classes, the correlation of

Table 3. The performance of the algorithms (recognition rate in %)
for the Extended YaleB database. The 2nd row is the result when
we used all 64 images for each people. The 3rd row is the result
when we excluded 13 poor-quality images for each person. The
images for training the dictionary were randomly selected.

D-KSVD
95.56
99.58

SRC
99.31
99.72

SRC†
80.76
93.85

K-SVD
93.17
99.30

Baseline
93.17
98.89

Table 4. The time for classifying one test image using the SRC
method and the D-KSVD method on the extended YaleB database.
We record the time for all the test images and then divide it by the
number of images. The value is the average over 4 rounds. The
unit is millisecond. The 2nd row is the result when we use all 64
images for each people. The 3rd row is the result when we use 51
images for each people.

Method
Case 1
Case 2

D-KSVD
84
78

SRC
120
121

SRC†
83
82

Figure 2. Sample images under extreme illumination conditions.
The left two are from one person and the right two from another
person.
0.16
SRC
K−SVD
Baseline
D−KSVD

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Figure 3. The histogram of pairwise correlation coefficients of the
atoms in the learned dictionary. The dictionaries were trained by
4 different methods with the extended YaleB database.

the atoms may exhibit more complex patterns. To avoid
the situation that a big R from a single pair of atoms overshadows the correlation of all other pairs, in this case, we

2696

0.25
SRC
K−SVD
Baseline
D−KSVD

0.2

0.15

Table 5. The result reported for SRC. SRC‡ means first breaking
the images into blocks, then classifying each block, finally using
voting policy to decide image’s label.

Test images
SRC
SRC‡

0.1

0.05

0
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Figure 4. The histogram of pairwise correlation coefficients of the
atoms in learned dictionary. The dictionaries were trained by 4
different methods with the extended YaleB database with 13 poorquality images excluded.

plot the histogram of the correlation coefficients. The results are given in Fig. 3 and Fig. 4 respectively. From
these plots, it was found that the proposed D-KSVD method
was able to generate a dictionary that contains more lesscorrelated atom pairs. That is, in the plots, the bars from the
proposed method are on average slightly taller towards the
left side of the axis of the correlation coefficients. (Probably one cannot expect to see dramatic difference in these
plots, given that the performance of the algorithms are already very close and the improvement is at most a couple of
percents. However, these few last percents are the hardest
to obtain.)
In addition to the classification performance of the DKSVD method and the SRC method, we also compared
their speed performance for classifying one test image. We
recorded the total time for classifying all the test images,
then divided it by the number of the test images, hence obtaining the average processing time for each testing image.
We ran this for 4 rounds and calculated the average result,
as shown in Table 4. From the results in Table 4, we can see
that, with a smaller dictionary (304 atoms in the dictionary
for D-KSVD and SRC†, and 1216 atoms in the dictionary
for SRC), we can save about 1/3 of the time in testing. With
a database involving more people, we can expect a smaller
dictionary can save even more time (see the results below
for the AR database too).

4.3. Results with the AR database
The AR database contains over 4000 color images for
126 people. For each person, there are 26 images taken in
two different sections. These images contains 3 different illumination conditions, 3 different expressions and 2 different facial disguises (with sunglasses and scarf respectively).
Thus this is a more challenging dataset. In our experiments,
we used 2600 images from 50 male and 50 female. For each
person, we randomly selected 20 images for training and the
other 6 for testing, which generally contain all the possible
variations in the database. The results reported in the subse-

Without disguise
94.7%
NA

Sunglasses
87.0%
97.5%

Scarves
59.5%
93.5%

Table 6. The performance of the algorithms (recognition rate in %)
for the AR database. SRCn means there are n atoms per person in
the dictionary learned by SRC. All other 3 methods use 500 atoms
(roughly 5 per person). The images for training the dictionary
were randomly selected.

D-KSVD
95.0

SRC20
90.50

SRC5
68.14

K-SVD
88.17

Baseline
93.0

Table 7. The time for classifying one test image using the SRC
method and the D-KSVD method on AR database. We record
the time for all the test images and then divide it by the number
of images. The value is the average over 4 rounds. The unit is
millisecond.

Method
Result

D-KSVD
62

SRC20
131

SRC5
76

quent table are from the average of three such random spits
of the training and testing images. The learned dictionary
contains 500 atoms, i.e., roughly 5 atoms per person (but
again, as discussed earlier, in our method there is no explicit correspondence between the atoms and the people).
The sparsity prior was set to T = 10.
For direct comparison, we quoted the performance of the
SRC algorithm on this dataset from [22], as listed in Table
5. It is worth noting that, in their experiments, they manually selected 7 images without facial disguise for each people from first section to build the dictionary. In our experiments, we tested the SRC algorithm with randomly-selected
images for building the dictionary and experimented with
two different dictionary size. We also tested K-SVD and
the method of [18] (the Baseline) on the AR database. In
the experiments, all four methods used the same parameters. We also did the random projection as described earlier
and in this case, this was from face images ∈ R165∗120 into
vectors ∈ R540 .
The final results from all the methods are listed in Table
6. The experiments show that the performance of the SRC
algorithm degraded dramatically when the training of the
dictionary was based on randomly-selected images: when
there are 5 images per person in the dictionary, the result
is merely 68.14%. From the table, the proposed method
outperforms all the competing methods.
As in the experiments with the Extended YaleB database,
we also compared the speed performance of the D-KSVD
method and the SRC method for classifying one test image

2697

on the AR database. The same method was used here. The
result is shown in Table 7. For SRC5 and D-KSVD, the
dictionary has 500 atoms, and the size is 2000 atoms for
SRC20 . As expected, for a database involving more people,
a smaller dictionary can save more time, which is about 1/2
from the table.

5. Conclusion
We proposed a dictionary-learning approach, Discriminative K-SVD (D-KSVD), for face recognition. By adding
a discriminative term into the objective function of the original K-SVD algorithm, we can ensure that the learned overcomplete dictionary is both representative and discriminative. The solution of the new formulation follows a procedure derived from the original K-SVD algorithm and thus
can be efficiently solved. Unlike existing approaches that
iteratively solve sub-problems in order to approximate a
global solution, our method directly finds all the parameters
(the dictionary and the classifier) simultaneously. With experiments on two large, commonly-used face databases, we
demonstrated the advantages of the proposed method. The
experimental results shows that: under the same learning
condition, our method always outperforms K-SVD and the
method of [18]; with the same dictionary size or with randomly chosen training images, our method outperforms the
SRC algorithm. Our future work includes exploring both
theoretically and empirically the structure of the learned
dictionaries from our method and the competing methods,
so as to reveal deeper insights on how to incorporate label
information into dictionary learning. More extensive analysis on the speed performance of the algorithms is also another direction of interest and of practical importance.

References
[1] M. Aharon, M. Elad, and A. Bruckstein. K-SVD: An algorithm for designing overcomplete dictionaries for sparse
representation. IEEE Transactions on signal processing,
54(11):4311, 2006. 1, 2
[2] R. Basri and D. Jacobs. Lambertian reflectance and linear
subspaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 218–233, 2003. 2
[3] P. Belhumeur, J. Hespanha, and D. Kriegman. Eigenfaces
vs. Fisherfaces: recognition using class specific linearprojection. IEEE Transactions on pattern analysis and machine
intelligence, 19(7):711–720, 1997. 1, 2
[4] O. Bryt and M. Elad. Compression of facial images using
the K-SVD algorithm. Journal of Visual Communication and
Image Representation, 19(4):270–282, 2008. 1
[5] O. Bryt and M. Elad. Improving the k-svd facial image compression using a linear deblocking method. In IEEE 25th
Convention of Electrical and Electronics Engineers in Israel,
2008. IEEEI 2008, pages 533–537, 2008. 1

[6] E. Candes and J. Romberg. l1-magic: Recovery of sparse
signals via convex programming. URL: www. acm. caltech.
edu/l1magic/downloads/l1magic. pdf. 4
[7] E. Candes, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction from highly incomplete
frequency information. IEEE Transactions on information
theory, 52(2):489–509, 2006. 2
[8] S. Chen, C. Cowan, P. Grant, et al. Orthogonal least squares
learning algorithm for radial basis function networks. IEEE
Transactions on neural networks, 2(2):302–309, 1991. 4
[9] D. Donoho. Compressed sensing. IEEE Transactions on
Information Theory, 52(4):1289–1306, 2006. 2
[10] M. Elad and M. Aharon. Image denoising via sparse
and redundant representations over learned dictionaries.
IEEE Transactions on Image Processing, 15(12):3736–3745,
2006. 1
[11] M. Figueiredo, R. Nowak, and S. Wright. Gradient projection for sparse reconstruction: Application to compressed
sensing and other inverse problems. IEEE Journal on selected topics in Signal Processing, 1(4):586–597, 2007. 4
[12] A. Georghiades, P. Belhumeur, and D. Kriegman. From Few
to Many: Illumination Cone Models for Face Recognition
under Variable Lighting and Pose. IEEE Trans. Pattern Anal.
Mach. Intelligence, 23(6):643–660, 2001. 1
[13] K. Lee, J. Ho, and D. Kriegman. Acquiring Linear Subspaces
for Face Recognition under Variable Lighting . IEEE Trans.
Pattern Anal. Mach. Intelligence, 27(5):684–698, 2005. 2, 6
[14] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman.
Discriminative learned dictionaries for local image analysis.
In IEEE Conference on Computer Vision and Pattern Recognition, 2008. CVPR 2008, pages 1–8, 2008. 1, 3
[15] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman.
Supervised dictionary learning. Adv. NIPS, 21, 2009. 1, 3
[16] J. Mairal, M. Elad, G. Sapiro, et al. Sparse representation
for color image restoration. IEEE Transactions on Image
Processing, 17(1):53, 2008. 1
[17] A. Martinez and R. Benavente. The AR face database. Univ.
Purdue, CVC Tech. Rep, 24, 1998. 2
[18] D. Pham and S. Venkatesh. Joint learning and dictionary
construction for pattern recognition. In IEEE Conference
on Computer Vision and Pattern Recognition, 2008. CVPR
2008, pages 1–8, 2008. 1, 2, 3, 5, 6, 7, 8
[19] Y. Sharon, J. Wright, and Y. Ma. Computation and relaxation of conditions for equivalence between l1 and l0 minimization. submitted to IEEE Transactions on Information
Theory, 2007. 5
[20] J. Tropp. Just relax: Convex programming methods for identifying sparse signals in noise. IEEE Transactions on Information Theory, 52(3):1030–1051, 2006. 4
[21] M. Turk and A. Pentland. Eigenfaces for recognition. Journal of cognitive neuroscience, 3(1):71–86, 1991. 1
[22] J. Wright, A. Yang, A. Ganesh, S. Sastry, and Y. Ma. Robust
face recognition via sparse representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages
210–227, 2009. 1, 2, 6, 7
[23] W. Zhao, R. Chellappa, P. Phillips, and A. Rosenfeld. Face
recognition: A literature survey. Acm Computing Surveys
(CSUR), 35(4):399–458, 2003. 1

2698

Discriminative Affine Sparse Codes for Image Classification
Naveen Kulkarni and Baoxin Li
Computer Science and Engineering
Arizona State University, Tempe, AZ 85287, USA
{naveen.kulkarni,baoxin.li}@asu.edu

Abstract

work well for objects with similar poses or in cases where
similar features for an object class can be generated by normalizing the pose. But these features may not be discriminative enough when the images involve a wide range of pose
variation.

Images in general are captured under a diverse set of
conditions. An image of the same object can be captured
with varied poses, illuminations, scales, backgrounds and
probably different camera parameters. The task of image
classification then lies in forming features of the input images in a representational space where classifiers can be
better supported in spite of the above variations. Existing
methods have mostly focused on obtaining features which
are invariant to scale and translation, and thus they generally suffer from performance degradation on datasets which
consist of images with varied poses or camera orientations.
In this paper we present a new framework for image classification, which is built upon a novel way of feature extraction that generates largely affine-invariant features called
affine sparse codes. This is achieved through learning a
compact dictionary of features from affine-transformed input images. Analysis and experiments indicate that this
novel feature is highly discriminative in addition to being
largely affine-invariant. A classifier using AdaBoost is then
designed using the affine sparse codes as the input. Extensive experiments with standard databases demonstrate that
the proposed approach can obtain the state-of-the-art results, outperforming existing leading approaches in the literature.

The SPM method [8] formulates the image classification
problem in terms of the global non-invariant representation
by aggregating local features over different subregions at
different scales. This method is effective only when objects involved undergo spatial translation. A non-parametric
nearest neighbor classifier [1] obtained good classification
performance based on nearest neighbor distances on local
image descriptors. But this method is only scale-invariant.
Recently sparse-coding-based SPM method was found to
be effective in obtaining promising results on the Caltech
datasets [17]. The main idea was the use of sparse codes
to obtain discriminative features which could be classified
by a classifier such as a linear SVM. The same authors further improved on the performance through the use of LLC,
reporting state-of-the-art classification performance on the
Caltech 101, the Caltech 256 and the PASCAL datasets
[16]. Again the features used in this method were only scale
and translation invariant and features would lose their discriminative ability under large pose variations.
Various image categorization datasets such as the Caltech and the Visual Object Class (VOC) datasets have
widely varied poses/orientations. This poses a challenging
task of obtaining unique features which are discriminative
in nature and also largely invariant to common variations
including scale, translation and (both in-plane and out-ofplane) rotation. Assuming the commonly-used affine model
for image transformation, the problem is then one of finding affine-invariant features. Techniques for image matching using affine transform (e.g., [14]) can be used to generate affine-invariant descriptors. However, such descriptors
directly generated from raw image patches are often not discriminative enough on their own. This demands new ways
of extracting discriminative features from the raw affineinvariant descriptors. Further, images from multiple classes
may have similar appearance, and hence the features, even

1. Introduction
Image classification has seen significant development in
recent years, with new approaches ranging from bag-offeatures-based visual vocabulary generation [2] and spatial
pyramid matching (SPM) [8] to the most recent localityconstrained linear coding (LLC) [16]. In general, naturallycaptured images from various sources are not restricted to
fixed acquisition condition. This poses a challenge in terms
of associating invariant features to images of the same object under diverse acquisition conditions. Many of the current state-of-the-art image classification framework rely on
a set of features which are largely scale and translation invariant. Scale and translation invariant features generally
1609

if being discriminative, may not be sufficient to clearly distinguish the images beyond reasonable doubt.
Aiming at addressing the above challenges, in this paper
we present a new framework for image classification, which
is built upon a novel way of feature extraction that generates largely affine-invariant features called affine sparse
codes. This is achieved through learning a compact dictionary of features from the set of raw affine-invariant descriptors computed from the input images. Then a classifier
using AdaBoost is designed using the affine sparse codes as
the input, further improving the separability of the classes
by assigning different set of weights to each of the classes
adaptively. We evaluated the proposed framework and algorithms based on two commonly-used datasets: Caltech
101 and Caltech 256. Comparative study of the experimental results has shown that the proposed method is able to
outperform existing state-of-the-art in the literature. In the
section below, we first present the details of the proposed
approach, and then report experimental results in Section
3. We then conclude in Section 4 with brief discussion on
future work.

Figure 1. A few examples of Caltech 101 and Caltech 256 dataset
showing different poses and orientations in images.

Figure 2. A few examples of Caltech 101 and Caltech 256 dataset
showing similar appearance among objects belonging to different
classes.

2. Proposed Approach
original image, it cannot be normalized. SIFT obtains invariant features by simulating zoom across different scales.
The translation and spin parameters are normalized. In general a camera model involves 6 parameters namely scale,
translation (vertical and horizontal), rotation, latitudinal and
longitudinal camera axis parameters. Any affine map (without translation) involves transformation through the matrix
given by




cos ψ − sin ψ t 0 cos φ − sin φ
A=λ
(1)
sin ψ cos ψ 0 1
sin φ cos φ

In this section, we present the proposed approach towards image classification. The proposed method relies on
a combination of three key techniques to achieve the desired
invariance and accuracy: (1) Exacting affine-invariant raw
descriptors from the input images using a simplified AffineScale invariant feature transform (ASIFT) algorithm [14];
(2) Developing a novel way of extracting discriminative features through first learning a compact dictionary from the
raw descriptors and then perform sparse coding with the
dictionary; (3) Building a classifier using AdaBoost to maximally exploit the compact affine sparse codes in final classification. The implementation of the proposed method involves the following logical steps:
1. Obtain ASIFT features for the given input images;
2. Obtain a compact codebook from the dense ASIFT descriptors;
3. Use sparse coding for extracting coefficients from the
ASIFT descriptors under the codebook;
4. Select the best descriptor for each spatial region on the
basis of minimum error sparse codes;
5. Max pooling of the sparse feature codes across finer
subregions;
6. Use a classifier based on AdaBoost for training and
testing the affine sparse codes.

where λ > 0, factor t is responsible for the tilt involved,
and ψ represents camera spin and φ ∈ [0,π). As with SIFT,
ASIFT also normalizes translations and spin but it also involves simulation of camera axis parameters and the scale
(zoom) parameter.
A smaller dataset like Caltech 101 has large inter-class
variations while a much larger dataset like Caltech 256 has
large intra-class variations in addition to the inter-class variations. Large intra-class variations along with many similar
inter classes put a constraint on how features can be obtained which can be separated in high-dimensional space
such that objects belonging to the same class are easily differentiated from other objects of similar classes. A simple example is shown in Fig. 1 where an object belonging
to the same class has widely varied poses/orientations and
scales. These images have different discriminative features
and they need to be mapped onto a uniquely representable
discriminative feature space. This example illustrates the
necessity of a feature transform which is invariant not only

We describe the different steps of the algorithm in detail in
the following sub-sections

2.1. ASIFT: An Overview
SIFT method combines the idea of simulation and normalization [11]. Since scale changes result in blurring of the
1610

to scale but also to varied poses and orientations. While another example in Fig. 2 shows objects belonging to different
classes which have similar appearances. This makes it extremely difficult to obtain good classification performance
on classes with similar features. This example indicates the
necessity of a classifier which can discriminate classes with
similar features by assigning different weights to them and
generating multiple hypothesis.
Images undergo affine distortion caused by change in the
optical axis orientation as viewed from a frontal position.
These distortions can be charaterized by the latitude and
longitude camera parameters φ and θ. The longitude parameter also known as φ can be simulated by rotating an image
about the horizontal axis viewed from the frontal position.
The latitude parameter also known as tilt which is inversely
related to cosine of the angle θ can be simulated by performing directional t-subsampling defined in [14][18]. The
ASIFT framework defined in [14] experimentally provides
a set of 6 different tilts performed on a finite number of rotational angles φ. Since the image datasets considered in
this paper comprise of data where there are no images rotated greater than 90 degrees in the horizontal and vertical
axes, we restrict ourselves to a maximum of 4 tilts and corresponding different rotations. So the algorithm in simple
terms can be explained as follows:
√ i
1. Obtain tilt factor t = 2 where i = 1,2,3,4
2. Obtain φ for each tilt factor t given by 1t k ∗ 72 where
◦
k = 1,2,3,... such that k∗72
t < 180 .
3. Calculate the affine transform of the input image for
all tilts t and rotations φ .
The tilts t = cos1 θ correspond to the latitude angle θ
and the sampling range follows a geometric series given by
1, a, a2 , · √
· · an . Experimentally it has been found that setting a = 2 provides a good range for performing various
tilts [14]. The longitude angle φ for each tilt is sampled
accordingly so as to follow an arithmetic series given by
◦
0, bt , · · · kb
t where b = 72 is a good choice and k is such
kb
◦
that t < 180 . A set of affine transformed images are obtained using the above method. Dense SIFT descriptors are
obtained for each affine transformed image. These dense
ASIFT descriptors form the input to the dictionary learning
algorithm as well as for the formation of sparse descriptors.

through sparse representation in this paper. This necessitates the need for a prior learned dictionary for which we
propose an online learning algorithm. Consider a dictionary
D of K basis atoms and dense features F, then the dense features can be uniquely represented in a dictionary D through
sparse representation given by
α,argmin
α∈<K

1
k F − Dα k22 +λ k α k1
2

(2)

Under mild conditions the solution to the system is
unique. With this background we will consider the codebook formation step.
The ASIFT descriptors obtained are of the order of 106 .
A batch processing based scheme like [9] would require
huge amount of memory and also would require lot of computations to obtain accurate representation of the large data
features. Thus we resort to an efficient online dictionary
learning mechanism. Recently an online dictionary learning scheme of [12][13] details the efficiency of stochastic
gradient approximations. For large datasets the speed and
memory requirements would be huge and it would be impractical to use a batch processing based optimization technique.
The codebook generation algorithm involves two important steps. The first step is the sparse coding step which
involves finding the coefficients which can approximately
represent the input features through a dictionary. The second step is dictionary updating which involves updating the
base atoms of the dictionary through coordinate descent
method with warm restarts. Once the compact dictionary is
obtained, the dense ASIFT descriptors can be represented in
a dictionary basis through sparse coefficients. The l1 sparse
coding problem can be cast as Eqn. 2. This problem also
known as basis pursuit or Lasso has been quite successful in
l1 -decomposition problems. Since there are two parts in the
equation, namely the least squares part and the l1 penalty
part, they can be individually optimized keeping the other
one fixed. It is well known that a penalty such as l1 will
lead to a sparse set of coefficients α. We also performed
experiments on the sparse coding problem with separable
constraints. In this method we write the equation with separable positive and negative constraints. It is given by
1
T
T
1 + λα−
1
α,argmin k F − D+ α+ + D− α− k22 +λα+
2
K
α∈<
s.t α+ , α− ≥ 0
(3)

2.2. Codebook formation and sparse descriptor generation
The features extracted from ASIFT correspond to a large
set of dense descriptors. There exists a lot of redundancy
in the descriptors obtained. The most relevant descriptors among them need to be picked. In order to achieve
good classification performance we need to generate similar
codes for descriptors belonging to the same class and they
also should be able to distinguish themselves from descriptors belonging to other classes. Such codes are obtained

Eqn. 3 is again a convex optimization problem which
can be solved using coordinate descent method. Coordinate
descent methods are fast and has been shown to converge to
a stationary point of the cost function with probability one
[5].
Experiments have been conducted on features using the
K-nearest neighbours based LLC method, the Lasso method
1611

volves selecting a subset of features from all the representative features. We use sparse coding to obtain the best feature
for the given spatial location among all ASIFT descriptors.
Let Ak be the descriptor for the k th affine transformed image and let fk be the descriptor obtained by sparse representation of Ak , we select the best descriptor given by
L

Figure 3. Plot of error between original and reconstructed features
for a few classes

Ak ,min k Ak − fk k22
k=1

(4)

where L is the number of affine transformed images on
which SIFT descriptors are formed. Thus among all the
dense ASIFT descriptors for each spatial location, only one
of the sparse code gets selected. The assumption is that
the low error sparse codes are more likely to lead to informative and discriminative codes than the ones with higher
error. There are two advantages of picking the code with
the lowest error. First, the codes are the best representations of the input feature; Second, when the error is small,
the codes are sparser, resulting in larger coefficient values.
Larger coefficients inherently lead to selection of the closest
basis from the dictionary for the input feature during maxpooling. This method thus plays an important role in spatial
pooling where sparse codes are max-pooled. Spatial maxpooling involves dividing the image into finer sub-regions
and picking the largest coefficient among the sparse coefficients obtained from the ASIFT dictionary. The largest coefficient represents the weightage associated with the dictionary element and uniquely represent the feature for the
spatial region. Codes formed across different sub-regions
are now concatenated to obtain the final feature descriptors.
These feature descriptors form input to the classifier.

and the coordinate descent method. Fig. 3 shows the average squared error over all dimensions of the input features.
In the plot, only LLC and Lasso methods have been shown
and the errors have been plotted for 30 of the 257 classes
of the Caltech 256 dataset. The errors obtained using coordinate descent method (not shown in plot) are comparable with the Lasso method and both of these methods have
considerable gain over the K-nearest neighbor based LLC
method. One of the reasons why coordinate descent method
performs better than others is because of the nature of dictionary updates in the online learning mechanism. Since
similar mechanisms are used in both the cases, the codes
obtained are much closer to the input features.
The aforementioned algorithm for online dictionary
learning is summarized below:
Algorithm 2.1
Online codebook generation for obtaining sparse codes
Input : F eatures F ∈ <MxN , Initial Dictionary
D0 ∈ <MxK , Iterations R, λ ∈ <
(regularization/sparsity parameter)
Output: Dictionary D ∈ <MxK
1 : P0 ←0, Q0 ←0
2 : for i = 1 to R do
3 : Draw samples f ∈ <M from F
Sparse coding step using LASSO
4 : αi ,argmin 12 k fi − Di−1 α k22 +λ k α k1

2.4. AdaBoost-based Classification
Feature extraction, representation and selection are necessary for formation of the training and test sets for a classification algorithm. An efficient classifier would make the
best usage of the given training data set to learn the model
and generalize it over the test data. Recognizing that boosting is one such general method for improving the accuracy
of any given learning algorithm [3][4], in this work, we propose to use AdaBoost [20] in building the desired classifier.
For the multi-class case, the AdaBoost algorithm takes input
features for all different classes with different labels. It calls
a weak learning algorithm repeatedly for a different distribution set over different classes. The distribution for all
classes represents the weights associated with each sample
belonging to each class. Initially the distribution is uniform,
and after each iteration the weak classifier returns a hypothesis. The distribution is modified so as to give more weightage to misclassified samples of each class. The error of
the weak learner’s hypothesis is measured by its misclassified samples on the distribution on which the samples were
trained. The weak hypothesis outputs the classification accuracy based on the distribution of the samples. In case of

α∈<K

5 : Pi = Pi−1 + αi αiT
6 : Qi = Qi−1 + fi αiT
7 : Calculate D using coordinate descent updates from
Di−1 and also Pi , Qi
Pi
Di ,argmin j=1 1i k fj − Dαj k22 +λ k αj k1
D∈C

8 : end for
9 : Return DR

2.3. Feature selection via sparse coding
ASIFT descriptors are obtained for various rotations and
tilts. Thus we have a multitude of dense feature descriptors
for each spatial position of the image. Feature selection in1612

binary class, even if the error is greater than 12 the hypothesis ‘h(xi )’can be replaced by ‘1 − h(xi )’[3]. Hence theoretically we can minimize the classification error as small
as possible until overfitting occurs. However, in the multiclass case this cannot be done because there cannot be an
equivalent of hypothesis ‘1 − h(xi )’in the multiclass case
and hence we need to stop continuing with generating the
hypothesis once classification accuracy is less than 12 .
With these, we summarize the actual implementation of
the AdaBoost algorithm used in this paper: Algorithm 2.2
Implementation of Multiclass AdaBoost Algorithm of [3]
Input : Sequence of training and testing features
ftrain, ftest ∈ F with labels yi ∈ Y
1 : Initialize weights D1 , D2 , · · · DN = N1
2 : for j = 1,2,...T
3 : Call weaklearning algorithm such as SVM with
distribution D; get back the model and hypothesis hj
PN
4 : Error over D : j = i=1 Dij [hj (xi ) 6= yi ]
5 : If j > 12 terminate loop
6 : Using model obtain testing hypothesis Hj
j
7 : Calculate βj = 1−
j

Gerenuk
Acc-100

Accordion
Acc-100

Skate

Sunflower

Umbrella

Acc-100

Acc-98.8

Acc-98.3

Figure 4. Results of Caltech 101 dataset showing some selected
classes with high accuracy.

Background
Acc-13

Beaver

Mayfly

crocodile

Crab

Acc-18.9

Acc-22

Acc-23

Acc-23.5

Figure 5. Results of Caltech 101 dataset showing some selected
classes with low accuracy.
Table 1. Caltech 101 dataset classification results

Training size 5
10
15
20
Zhang[19]
46.6 55.8 59.1 62
Lazebnik[8]
56.4 Griffin[6]
44.2 54.5 59.0 63.3
Boiman[1]
65.0 Jain[7]
61.0 Gemert[15]
Yang[16]
51.15 59.77 65.43 67.74
Ours
66.13 73.09 78.38 78.50
Note: ’-’ indicates unavailability of results

1−[hj (xi )6=yi ]

8 : Calculate weights Dij+1 = Dij βj

9 : end for
10 : Output final train hypothesis
PT
hT (F) = argmax j=1 log( β1j )[hj (F ) = y]
y∈<Y

11 : Output final test hypothesis
PT
HT (F) = argmax j=1 log( β1j )[Hj (F ) = y]
y∈<Y

25
65.8
70.16
82.36

30
66.2
64.6
67.6
70.4
69.1
64.16
73.44
83.28

are divided among 101 object classes and 1 background
class. As we can see from Table 1, even for a small training size the classification accuracy is comparatively higher
than other methods. The classification performance without the background class for train size of 30 is 87.72%.
The percentage accuracy for various classes is illustrated
in Fig.4 and Fig.5. As we can see from Fig.4, a few of the
classes achieved 100% accuracy. In fact a total of 8 classes
achieved 100% accuracy. We also provide a few examples
with accuracy less than 25%, shown in Fig. 5. As expected,
the background class is one among them since there are no
specific features which are discriminative and hence leading to misclassification. The other cases includes cougar
body which was in majority classified as leopard, and crab
as crayfish. These are typical examples of classes which
are extremely similar in nature and are hard to classify even
with the most discriminative features. Other factors include
the camouflaging of images with the background and occlusion. Over 70 classes achieve an accuracy of 50% or higher.
Only 5 classes had low accuracy of 25% or less.
Experiments on classification performance with and
without AdaBoost was also carried out. This is illustrated

3. Experimental Results
The experiments were performed on the Caltech 101 and
Caltech 256 datasets. We used only ASIFT descriptor for
all the experiments. The dimension of each ASIFT descriptor is 128. The set of descriptors of the order of 106 are
trained using the online dictionary learning mechanism to
obtain a dictionary of size 1024. ASIFT descriptors generated from images taken only from Caltech 256 dataset were
used for training a common dictionary which was used for
sparse descriptor generation for both Caltech 101 and Caltech 256 dataset. The best affine sparse descriptors obtained
after feature selection are max-pooled across 4x4,2x2 and
1x1 scales to obtain the final feature descriptors. The max
pooling is obtained by selecting the max of the sparse codes
obtained across different sub regions. These codes are now
concatenated to obtain a final feature vector which is sparse.

3.1. Results with Caltech 101
Table 1 shows the results obtained for the Caltech 101
dataset. Caltech 101 dataset consists of 9144 images which
1613

Table 2. Performance of SVM and AdaBoost on the Caltech-101
dataset

Training
size
SVM
AdaBoost

5

10

15

20

25

30

63.4 70.1 73.6 73.9 77.3 78.9
66.13 73.09 78.38 78.50 82.36 83.2

through the use of another classifier such as SVM. Table 2
illustrates the performance of a classifier such as SVM with
linear kernel on Caltech 101. Similarly, Table 4 illustrates
the same for the Caltech 256 dataset. Although [10] emphasizes on the effectiveness of radial basis functions as a kernel, we used a linear SVM kernel because of low computational complexity involved in training. Using an SVM with
linear kernel as a weak learner, we obtained a classification
accuracy of 79%. The training involved in case of AdaBoost
was not intensive. Only three iterations were required to
train the weak classifier and obtain a hypothesis for each
case. It is obvious that, without involving intense training,
there has been considerable performance gain achieved by
AdaBoost. The classes for which the performance was improved in each of the hypothesis were the ones whose images were largely similar. The distribution change was able
to convert the misclassified samples to their respective class
without affecting the appropriately classified samples. We
shall see how error bounds affect the classification performance of AdaBoost in a later sub-section.

Galaxy

Motorbikes

Car-side

Faces

Acc-95.23

Acc-98.9

Acc-100

Acc-98.67

Brain

Deskglobe

Saturn

Acc-91.3

Acc-86.3

Acc-88.46

Acc-91.66

Homer

Mbike

Guitar

Chandelier

Acc-83.3

Acc-73.9

Acc-51.3

Kayak

Acc-63.6

Figure 6. Results of Caltech 256 dataset showing classes with different accuracies.
Table 3. Image classification results for Caltech-256 dataset

3.2. Results with Caltech 256

Training size
Griffin[6]
Gemert[15]
Yang[16]
Ours

Table 3 shows the results for Caltech 256. This is a
harder dataset with much larger inter as well as intra class
variations. There are a total of 30607 images which are
divided among 256 object classes and 1 background class.
Fig. 6 provides accuracies for a few of the classes in Caltech
256. The dictionary used in the sparse descriptor generation
consists entirely of images only from Caltech 256 dataset.
Experiments were carried out on online dictionary training
using 40%, 80% and 100% of the images from Caltech 256
dataset. A common dictionary trained from such images
was used for feature descriptor generation in both Caltech
101 and 256 datasets. There was no significant difference
in the performance obtained when the number of images
used were reduced from 100% to 80% and to 40% for Caltech 256 dataset. In fact, in case of Caltech 101 there was
slight increase in the performance when 80% and 40% images were used, which may be because of overfitting issues
when more number of images are involved. Table 5 shows
some of the results obtained for Caltech 256 and Caltech
101 datasets when different percentage of the images were
selected for dictionary learning. For Caltech 256 dataset
in cases when 80% and 40% of images were used in dictionary learning, it was made sure that the remaining 20%
and 60% images would be part of the test set. For the Cal-

15
28.3
34.36
39.42

30
34.1
27.17
41.19
45.83

45
45.31
49.3

60
47.68
51.36

Table 4. Performance of SVM and AdaBoost on Caltech-256
dataset

Training
size
SVM
AdaBoost

15

30

45

37.67 43.1 46.9
39.42 45.83 49.3

60
49.84
51.36

tech 101 case no such restrictions were involved for training
and testing. This is a clear indicator that a single dictionary
generated from a larger dataset would result in discriminative codes for both Caltech 101 and Caltech 256. This again
substantiates the discriminative power of the dictionary for
generating sparse codes which are largely affine-invariant.

3.3. Analysis of affine sparse codes
The affine sparse descriptors are discriminative in nature.
The reason is attributed to the sparse coefficients obtained
which can be termed as features with minimum intra class
variance and maximum inter class variance. This comparison was made with the SIFT LLC codes. Correlation statistics for affine sparse codes are shown in Fig.8 and SIFT
1614

Table 5. Performance comparison on images selected for dictionary learning

Training size
Caltech256(40%)
Caltech256(80%)
Caltech256(100%)
Caltech101(40%)
Caltech101(80%)
Caltech101(100%)

15
37.3
38.51
39.42
79.98
79.2
78.38

30
44.11
45.24
45.8
84.1
83.8
83.2

60
49.2
51.13
51.36
-

codes are shown in Fig.7. Fig.9 shows the sum of correlations obtained for each class. The intra-class correlations
obtained for the same class of features represent within class
correlations among feature vectors. The inter class correlations represent the correlations between feature vectors belonging to different classes. A random set of feature vectors were correlated with a random set of vectors from all
other classes. The number of random vectors picked for
each class was 30. The number of random classes picked
to correlate with the current class was 25. The four different colors shown in Fig. 9 shows four different correlation
statistic of the two different codes. As can be seen from
Fig. 9, the red and green labels clearly indicate that affine
sparse codes have higher intra class correlations and lower
inter class correlations than SIFT LLC codes shown in blue
and black labels respectively. This is also evident from the
scatter matrix plots of fig 7 and fig 8. The scatter matrix is a
representation of the pearson correlation coefficient statistic. The points represent the scatter of each class with respect to every other class.
Correlations are divided into three different ranges as can
be seen in Fig. 7 and Fig. 8. High correlation values, mid
and low correlation values are represented by black dots,
red dots and green dots respectively. Black dots clearly
seen on the diagonal indicate the correlation among class
features of the same class. Red and green dots indicate correlations of each class feature with features of other classes.
Both sparse codes and LLC codes exhibit higher correlations among features of same class. But sparse codes gain
an upper hand in terms of inter-class correlations. We can
see denser red dots in case of LLC codes indicating higher
inter-class correlations than in case of affine sparse codes.
Sparser red dots lead to lower inter-class correlations and
hence the features are discriminative with respect to each
other. Dense green dots obviously imply sparse red dots and
hence lower inter-class correlations. Thus the classification
performance is improved by the high intra class correlation
and low inter class correlation between features. This is
quite evident from Table 1 and Table 3 for both Caltech 101
and Caltech 256 datasets.

Figure 7. Plot of scatter matrix of all classes for LLC codes belonging to Caltech 101 dataset

Figure 8. Plot of scatter matrix of all classes for Sparse codes belonging to Caltech 101 dataset

Figure 9. Plot of averaged correlations for LLC and Sparse codes

Algorithm 2.2 and assuming j ≤ 1/2, then error
X
=
[hf (xi ) 6= yi ]
i ˜D

(5)

defined in [3] of the final hypothesis hf is bounded by
q
 ≤ 2T ΠTj=1 j (1 − j )
(6)
We obtained error bounds for the Caltech 101 and Caltech
256 datasets as shown in Table 6. These error bounds also
illustrate the fact that beyond a certain number of iterations
the error of the final hypothesis would not accurately represent the training error because it would be less than 12 and
that would be the point to stop generating hypothesis.

3.4. Analysis of error bounds of AdaBoost
Suppose that the weak learning algorithm such as SVM
generates errors 1 , 2 ...T where j is defined as shown in
1615

[4] Y. Freund, R. Schapire, and N. Abe. A short introduction
to boosting. JOURNAL-JAPANESE SOCIETY FOR ARTIFICIAL INTELLIGENCE, 14:771–780, 1999. 1612
[5] J. Friedman, T. Hastie, and R. Tibshirani. Regularization
paths for generalized linear models via coordinate descent.
Journal of statistical software, 33(1):1, 2010. 1611
[6] G. Griffin, A. Holub, and P. Perona. Caltech-256 object category dataset. 2007. 1613, 1614
[7] P. Jain, B. Kulis, and K. Grauman. Fast image search for
learned metrics. 2008. 1613
[8] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of
features: Spatial pyramid matching for recognizing natural
scene categories. In Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on, volume 2,
pages 2169–2178. IEEE, 2006. 1609, 1613
[9] H. Lee, A. Battle, R. Raina, and A. Ng. Efficient sparse coding algorithms. Advances in neural information processing
systems, 19:801, 2007. 1611
[10] X. Li, L. Wang, and E. Sung. A study of AdaBoost with
SVM based weak learners. In Neural Networks, 2005.
IJCNN’05. Proceedings. 2005 IEEE International Joint Conference on, volume 1, pages 196–201. IEEE, 2005. 1614
[11] D. Lowe. Distinctive image features from scale-invariant
keypoints.
International journal of computer vision,
60(2):91–110, 2004. 1610
[12] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online dictionary
learning for sparse coding. In Proceedings of the 26th Annual
International Conference on Machine Learning, pages 689–
696. ACM, 2009. 1611
[13] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning
for matrix factorization and sparse coding. The Journal of
Machine Learning Research, 11:19–60, 2010. 1611
[14] J. Morel and G. Yu. ASIFT: A new framework for fully affine
invariant image comparison. SIAM Journal on Imaging Sciences, 2(2):438–469, 2009. 1609, 1610, 1611
[15] J. van Gemert, J. Geusebroek, C. Veenman, and A. Smeulders. Kernel codebooks for scene categorization. Computer
Vision–ECCV 2008, pages 696–709, 2008. 1613, 1614
[16] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong.
Locality-constrained linear coding for image classification.
In Computer Vision and Pattern Recognition (CVPR), 2010
IEEE Conference on, pages 3360–3367. IEEE, 2010. 1609,
1613, 1614
[17] J. Yang, K. Yu, Y. Gong, and T. Huang. Linear spatial pyramid matching using sparse coding for image classification.
2009. 1609
[18] G. Yu and J. Morel. A fully affine invariant image comparison method. In Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on,
pages 1597–1600. IEEE, 2009. 1611
[19] H. Zhang, A. Berg, M. Maire, and J. Malik. SVM-KNN:
Discriminative nearest neighbor classification for visual category recognition. In Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on, volume 2,
pages 2126–2136. IEEE, 2006. 1613
[20] J. Zhu, S. Rosset, H. Zou, and T. Hastie. Multi-class adaboost. Ann Arbor, 1001:48109, 2006. 1612

Table 6. Error bounds of AdaBoost algorithm on Caltech-101 and
Caltech-256 datasets

Dataset
Caltech101
Caltech256

1
2
3
f
0.12 0.024 0.0024 1.4.10−3
0.263 0.081 0.0259 3.13.10−3

4. Conclusion and Discussion
We proposed the affine sparse codes for providing compact and discriminative features, which is then used in an
AdaBoost-based classifier for the image classification task.
Detailed analysis has been performed on the proposed approach, using two standard test sets. The discriminative nature of the proposed feature is due to the affine-invariance
and sparsity-based learning. Sparsity allows us to pick different number of basis atoms from the dictionary and hence
leading to low-error high-energy codes. Affine invariance
is responsible for low intra-class variance, thus making features of the same class clustered tightly around its mean.
With the proposed method, we have seen considerable gain
in classification performance over leading existing methods.
One of the drawbacks of the current method is the use
of large number of raw descriptors. A new method for efficiently discarding the dense feature points before online
dictionary learning needs to be incorporated. This will considerably reduce the amount of space required to extract
each dense descriptor and storing it before sparse coding.
Also the existing method may not achieve good performance on datasets involving multiple class labels in a single
image. Thus features extracted from the images should be
such that multiple labels can be assigned to it by a classifier.
Thus we aim at addressing following issues in the future:
Obtaining considerably less number of ASIFT descriptors
to reduce space requirements and also better feature selection mechanisms to generate unique sparse features of low
dimensionality. Combining these with a classifier which has
the ability to assign multiple class labels to certain features
would lead to much better classification system.

References
[1] O. Boiman, E. Shechtman, and M. Irani. In defense of
nearest-neighbor based image classification. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1–8. IEEE, 2008. 1609, 1613
[2] G. Csurka, C. Dance, L. Fan, J. Willamowski, and C. Bray.
Visual categorization with bags of keypoints. In Workshop
on statistical learning in computer vision, ECCV, volume 1,
page 22. Citeseer, 2004. 1609
[3] Y. Freund and R. Schapire. A desicion-theoretic generalization of on-line learning and an application to boosting.
In Computational learning theory, pages 23–37. Springer,
1995. 1612, 1613, 1615

1616

Mining Discriminative Components With Low-Rank And
Sparsity Constraints for Face Recognition
Qiang Zhang, Baoxin Li
Computer Science and Engineering
Arizona State University
Tempe, AZ, 85281

qzhang53, baoxin.li@asu.edu
ABSTRACT

1. INTRODUCTION

This paper introduces a novel image decomposition approach
for an ensemble of correlated images, using low-rank and
sparsity constraints. Each image is decomposed as a combination of three components: one common component, one
condition component, which is assumed to be a low-rank
matrix, and a sparse residual. For a set of face images of N
subjects, the decomposition ﬁnds N common components,
one for each subject, K low-rank components, each capturing a diﬀerent global condition of the set (e.g., diﬀerent
illumination conditions), and a sparse residual for each input image. Through this decomposition, the proposed approach recovers a clean face image (the common component)
for each subject and discovers the conditions (the condition
components and the sparse residuals) of the images in the
set. The set of N + K images containing only the common
and the low-rank components form a compact and discriminative representation for the original images. We design
a classiﬁer using only these N + K images. Experiments
on commonly-used face data sets demonstrate the eﬀectiveness of the approach for face recognition through comparing with the leading state-of-the-art in the literature. The
experiments further show good accuracy in classifying the
condition of an input image, suggesting that the components
from the proposed decomposition indeed capture physically
meaningful features of the input.

Face recognition has been an active research ﬁeld for a
few decades, and its challenges and importance continue to
attract eﬀorts from many researchers, resulting in many new
approaches in recent years. The most recent literature may
be divided into roughly two groups, where methods in the
ﬁrst group try to model the physical processes of image formation under diﬀerent conditions (e.g., illumination, expression, pose etc.). For example, the approach of [10] models
the face image under varying illumination conditions to be
a linear combination of images of the same subject captured
at 9 specially designed illumination conditions; the SRC algorithm of [19] further assumes that face images with illumination and expression conditions can be represented as a
sparse linear combination of the training instances (i.e., the
dictionary atoms). On the other hand, the second group
of approaches utilizes mathematical/statistical tools to capture the latent relations among face images for classiﬁcation.
E.g., the SUN approach [7] uses the statistics of the human
ﬁxation of the images to recognize the face images, Volterrafaces [9] ﬁnds a latent space for face recognition, where the
ratio of intra-class distance over inter-class distance is minimized. One major advantage of the techniques in the ﬁrst
class comes from their being generative in nature, which allows these methods to accomplish tasks like face relighting or
novel pose generation in addition to recognition. The second
group of methods in a sense ignores the physical property of
the faces images and treats them as ordinary 2D signals.
Although the methods in the ﬁrst group have the above
nice property, a baseline implementation usually requires
dictionaries with training images as atoms and thus may
face the scalability issue in real-world applications with a
huge number of subjects. Hence eﬀorts have also been devoted to reducing the size of the dictionary while attempting
to retain the level of performance of the original dictionary.
Examples include those that generate more compact dictionaries through some learning procedure (e.g., [13]) and those
that attempt to extract subject-speciﬁc features that are effectively used as dictionary atoms (e.g., [15]). Our approach
belongs to the second group. Since the expressive power of
the original dictionary-based techniques comes from largely
the number of training images for each subject, a compact
dictionary may suﬀer from degraded performance unless the
reduced dictionary properly captures the conditions of the
original data that are critical for a recognition task. For
example, the method of [15], while shown to be eﬀective for
expression-invariant recognition, is diﬃculty to generalize to
handle global conditions such as illumination change, which

Categories and Subject Descriptors
H.4 [Information Systems Applications]: Miscellaneous

General Terms
Feature extraction and preprocessing

Keywords
Subspace learning, Low-rank matrix, Sparse matrix, Face
Recognition, Component Decomposition
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
KDD’12, August 12–16, 2012, Beijing, China.
Copyright 2012 ACM 978-1-4503-1462-6/12/08... $15.00.

1469

age of the ith subject. When there is no confusion, we also
use X to denote the set {Xi,j }N,M
i,j=1 .

often introduce to the data non-sparse conditions that cannot be captured by the sparsity model proposed therein.
Recognizing that non-sparse conditions such as illumination change and large occlusion are critical for face recognition, and that for a typical application we may assume only
a ﬁnite number of such conditions (e.g., a relatively small
number of illumination conditions or other conditions), in
this paper, we propose a model for representing a set of
face images by decomposing them into three components: a
common component shared by images of the same subject,
a low-rank component capturing non-sparse global changes,
and a sparse residual component. Such a decomposition is
partially inspired by the observation that the reconstruction
of the image with the top few singular values and the corresponding singular vectors often capture the global information of the image, which can be represented by a low-rank
matrix. To this end, a generic algorithm is proposed, with
theoretic analysis on the convergence and parameter selection. The learned common and low-rank components form
a compact and discriminative representation of the original
set of images. A classiﬁer is then built based on comparison
of subspaces spanned by these components and by a novel
image to be classiﬁed. This is very compact compared with
the number of atoms in an over-determined dictionary such
as that in [19]. Further, by explicitly modeling non-sparse
conditions, the proposed approach is able to handle both
illumination changes and large occlusions, which would fail
methods like [15].
To demonstrate the eﬀectiveness of the proposed method,
we ﬁrst design synthetic experiments with known ground
truth to verify its key capability in recovering the underlying
common, low-rank and sparse components. Then we report
results on three commonly-used data sets of real face images: the Extended YaleB dataset [4], the CMU PIE dataset
[17] and the AR dataset [14].The experiments show that,
the proposed approach obtained better performance than
the SRC algorithm [19], which utilizes a much larger dictionary, and the SUN approach [7]. The proposed approach
also achieves comparable result to Volterrafaces, which is the
current state-of-the-art in the literature for a few commonlyused data sets. In addition, the proposed approach can explicitly model the most important feature of the subject and
the conditions in the dataset. Experiments also show that
the proposed method is robust to situations where a nontrivial percentage of the training images is unavailable. Further, the capability of the proposed approach for classifying
the type of condition that an input image is subject to is
also demonstrated by extensive experiments. This suggests
that the proposed decomposition is able to obtain physically
meaningful and thus potentially discriminative components.
We introduce the proposed method in Section 2, including
the proposed model, the learning algorithm and the classiﬁcation method. The experiments are reported and analyzed
in Section 3. We conclude in Section 4 with a summary of
the work and brief discussion on future work.
In the presentation, we use upper case bold font for matrices, e.g., X, lower case bold font for vectors, e.g., x and
normal font for scalars, e.g., x. {Xi,j }N,M
i=1,j=1 denotes a set
of N × M matrices, with Xi,j as its (i, j)th member. We
assume that N is the number of the subjects, and M the
number of images per subject1 . Thus Xi,j refers to jth im1

2. PROPOSED METHOD
In this section, we ﬁrst present the general formulation
of the proposed model in Section 2.1, and then present our
algorithm for obtaining the desired decomposition in Section
2.2 and analysis of its convergence in Sec. 2.3. With these,
a face recognition algorithm is then designed in Section 2.4.

2.1 Decomposing a Face Image Set
In many applications of image and signal processing, we
often consider a set of correlated signals as an ensemble. For
eﬃcient representation, a signal in the ensemble can often be
viewed as a combination of a common component, which is
shared among all the signals in the ensemble, and an innovation component, which is unique to this signal. Many beneﬁts can be drawn from this decomposition of the ensemble,
such as obtaining better compression rate and being able to
extract more relevant features. In face recognition, all the
face images, especially the subset corresponding to a subject, may be naturally viewed as forming such an ensemble
of correlated signals. In a sense, a sparse-coding approach
like SRC implicitly ﬁgures out the correlation of the images
in the ensemble via the sparse coeﬃcients under the dictionary of the training images.
In this work, we aim at developing a new representation
of this ensemble so that the face recognition task can be
better supported. In particular, considering the common
challenges such as illumination conditions and large occlusions, we want to have a representation that can explicitly
model such conditions. To this end, we propose the following
decomposition of face images Xi,j in the ensemble X as:
Xi,j = Ci + Aj + Ei,j , ∀Xi,j ∈ X

(1)

where Ci is the common part for Subject i, Aj is a low-rank
matrix, and Ei,j is a sparse residual.
One essential diﬀerence between the proposed method and
Robust PCA (RPCA [18]), is that RPCA assumes the signals are linearly dependent, with some sparsely corrupted
entries in the signals. As a result, they build a big matrix
with each signal as a vector. The big matrix would naturally be low-rank (because of the assumed inter-image correlation), in addition to having a sparse set of entries. On the
other hand, the proposed decomposition is partially inspired
by the observation that the reconstruction of the image with
ﬁrst few singular values and the corresponding singular vectors often capture the global information of the image [12],
e.g., illumination conditions, structured patterns, which can
be represented by a low-rank matrix. Here the low-rank constraint arises from certain physical conditions (rather than
due to inter-image correlation), and it is imposed on each
individual image. Accordingly, we represent images by matrices rather than vectors, unlike other methods like [19, 18].
With this, we can expect that:
Ci is a matrix representing the common information of images for Subject i, i.e., the common components;
number of images, which can always be achieved by using
some blank images, a situation the proposed method can
handle.

For simplicity, we assume that each subject has the same

1470

that, unlike [18] where a set of images are stacked as vectors
of a low-rank matrix, we do not convert the image to a vector
in the decomposition stage.
To absorb the constraints into the objective function, we
can reformulate Eqn. 5 with augmented Lagrange multiplier
as:

C, A, E = argmin
Aj ∗ + λi,j Ei,j 1

Aj is a low-rank matrix capturing the global information of
the image Xi,j , e.g., illumination conditions (Fig. 3),
structured patterns (Fig. 1); and
Ei,j is a sparse matrix pertaining to image-speciﬁc details
such as expression conditions or noise with sparse support in the images.
In this modeling, we have assumed M diﬀerent low-rank
matrices, which are responsible for M diﬀerent global conditions such as illumination conditions or large occlusions,
and they are shared among the images of diﬀerent subjects.
However, images of each subject do not necessarily contain
all the M conditions, as we will show in Sec. 2.2.
We can also obtain a variant of the above model by considering the Retinex theory, in which image I can be represented as:
I(p, q) = R(p, q) · L(p, q)

C,A,E

(2)

(3)

The above equation indicates that we can represent the intensity of the face image as follows:
log(Xi,j ) = Ci + Aj + Ei,j , ∀Xi,j ∈ X

(4)

where Ci = log(R) captures the common property of the images for Subject i, Aj = log(L) captures the lighting conditions, and Ei,j captures the residual. This is a variant of the
model in Eqn. 1, and is especially suitable for illuminationdominated datasets such as the extended YaleB dataset and
the CMU-PIE dataset.
With the above decomposition, the entire dataset containing N × M images can be compactly represented by N
common components and K low-rank components. If we
extract the common component Ci for face images of Subject i under diﬀerent conditions, we expect that this common component Ci represents the most signiﬁcant feature
of that subject. The set of all the learned low-rank components A = {Aj }M
j=1 represents all possible global conditions
of the images in the set. Hence we may use A and Ci to
span the subspace of the face images for Subject i, where, in
the ideal case, any face images of this subject should lie in,
barring a sparse residual. This suggests that we can utilize
the subspaces for face recognition by identifying which subspace a test image is more likely to lie in, which is detailed
in Sec. 2.4.

Ei,j

= argminλi,j Ei,j 1

(7)

Ei,j

+

μi,j
2
E
XE
i,j − Ei,j F + < Yi,j , Xi,j − Ei,j >
2

with XE
i,j = Xi,j − Ci − Aj . So we do the following update
[6]:
Ei,j = S

λ
μi,j

(XE
i,j +

1
Yi,j )
μi,j

(8)

where Sτ (X) = sign(X) · max(0, |X| − τ ).
Sub-solution 2: For ﬁnding an optimal Ak in the t-th
iteration, where the problem can be written as

Aj = argmin
Aj ∗
(9)
Aj

+

i

μi,j
2
A
XA
i,j − Aj F + < Yi,j , Xi,j − Aj >
2

We use the singular value thresholding algorithm [2, 5]:

A
T
i μi,j Xi,j + Yi,j

←
UΣV
i μi,j

2.2 An Algorithm for the Decomposition
Based on Eqn. 1, we formulate the decomposition task
as the following constrained optimization problem, with an
objective function derived from the requirement of decomposing a set of images into some common components, some
low-rank matrices and the sparse residuals:

C, A, E = argmin
Aj ∗ + λi,j Ei,j 1
C,A,E

(6)

where Yi,j is the Lagrange multiplier, λi,j and μi,j are
scalars controlling the weight of sparsity and reconstruction
error accordingly. When μ is suﬃciently large, Eqn. 6 is
equivalent to Eqn. 5. It is worth pointing out that, while
for clarity we have written only the expression for Subject i,
the optimization is actually done for the entire set of images,
since the low-rank components are deemed as been shared
by all images.
To solve the problem of Eqn. 6, a block coordinate descent algorithm may be designed, with each iterative step
solving a convex optimization problem [3][18] for one of the
unknowns. To this end, we ﬁrst describe the following three
sub-solutions that are needed in each iteration of such an
algorithm, which correspond to solving only one of the unknowns (blocks) while ﬁxing others.
Sub-solution 1: For ﬁnding an optimal Ei,j in the t-th
iteration, where the problem can be written as

where R(x, y) is the reﬂectance at location (x, y), which depends on the surface property, L(x, y) is the illumination,
and · is element-wise product. Converting this into the logarithm domain, we have
log(I) = log(R) + log(L)

i,j

μi,j
Xi,j − Ci − Aj − Ei,j 2F
+
2
+ < Yi,j , Xi,j − Ci − Aj − Ei,j >

Aj

=

USτ (Σ)VT

N
with XA
.
i,j = Xi,j − Ci − Ei,j and τ =
i μi,j
Sub-solution 3: The solution to the problem of ﬁnding
optimal Ci
μi,j 
2
C
argmin
XC
(10)
i,j − Ci F + < Yi,j , Xi,j − Ci >
2 j
Ci

i,j

s.t. Xi,j = Ci + Aj + Ei,j , ∀Xi,j ∈ X (5)

where Aj ∗ = i σi (Aj ) is the nuclear norm, Ei,j 1 =

N,M
p,q |Ei,j (p, q)| is the 1 norm and E = {Ei,j }i,j=1 . Note

where XC
i,j = Xi,j − Aj − Ei,j , can be obtained directly (by
taking derivatives of the objective function and setting to

1471

Ideally a face image from Subject i should lie in a subspace spanned by its common component Ci and the lowrank components A. Therefore, we propose the following
classiﬁcation scheme based on comparing the distance between subspaces spanned by the training components and
those spanned by replacing the training common by the test
image y. We ﬁrst build the subspace Si for subject i, which
contains all the linear combinations of the images of Subject
i under all conditions, i.e.,

wk × (ci + aj ) ∀w ∈ RM }
(12)
Si = {x|x =

zero) as

Ci =

j

Yi,j + μi,j XC
i,j

μ
i,j
j

(11)

As alluded earlier, the images of any given subject may
not range over all possible M conditions. This may be equivalently viewed as a problem of some images are missing for
the subject. We now show how this can be addressed in
a principled way. Assume that Ω is the set of (i, j) where
Xi,j is available and Ω̄ is the complement of Ω. To deal
with those missing entries, we only need to set Yi,j , μi,j
and Xi,j to 0 for (i, j) ∈ Ω̄ in the initialization stage. In
each iteration, we do not update Ei,j for (i, j) ∈ Ω̄. The
proposed decomposition algorithm will automatically infer
the missing images.
With the above preparation, we now propose the following Algorithm 1 to solve Eqn. 6:

k

where ci and aj is the vectorized form of Ci and Aj respectively. Subspace Si can be suﬃciently represented by a set
of “basis”, i.e., {ci + aj }M
j=1 . Accordingly, we can build the
subspace Sy for the test image y as the follows:

Sy = {x|x =
wk × (y + aj ) ∀w ∈ RM }
(13)
k

Algorithm 1: Learning the Decomposition

Then we use the principal angles [8] between these subspace
to measure their similarities. In this paper, the principal
angles measure the cosine distancebetween the subspaces,
which is calculated as s(Si , Sy ) = k cos2 (θk ), where θk is
the kth principal angle between Si and Sy . The assign i as
the label of f, for which s(Si , Sy ) is maximal.

Input: X, Ω, N , M , ρ, λ and τ ;
N,M
K
Output: {Ci }N
i=1 , {Aj }j=1 and {Ei,j }i,j=1 ;
% Initialization
t = 0, C0i = A0j = E0i,j = 0;
Xi,j
τ
Y0i,j = Xi,j
, μ0i,j = Xi,j
for (i, j) ∈ Ω;
F
F

Y0i,j = 0, μ0i,j = 0 for (i, j) ∈
/ Ω;
while not converged do
Solve Ei,j for (i, j) ∈ Ω by Sub-solution 1:
Solve Aj for j = 1, 2, ..., M with Sub-solution 2;
Solve Ci for i = 1, 2, ..., N using Sub-solution 3;
%Update Yi,j and μi,j for (i, j) ∈ Ω:
t+1
t
t
Yt+1
− At+1
− Et+1
i,j = Yi,j + μi,j (Xi,j − Ci
j
i,j );
t+1
μi,j = μti,j ρ;
t = t + 1;
end


where for convergence, we check

i,j

3. EXPERIMENTAL RESULTS
Experiments have been done to evaluate the proposed
model and algorithms. In this section, we report several sets
of results from such experiments. First, simulations (Sec.
3.1) are employed to demonstrate the convergence and parameter selection of the proposed decomposition algorithm.
Then, we show the decomposition of the images from extended YaleB dataset and also how the learned components
can be used to reconstruct new images in Sec. 3.2. Finally,
we demonstrate the application of the proposed method and
algorithms in classiﬁcation tasks, including face recognition
(Sec. 3.3) and identifying the conditions of the images (Sec.
3.4). The performance of the proposed method in face recognition task is compared with that of SRC [19], Volterrafaces
[9] and SUN [7] on 2 commonly used datasets, i.e., extended
YaleB [10] and CMU-PIE [17].

Xi,j −Ci −Aj −Ei,j 2
F

i,jXi,j 2
F

and if it is small enough (e.g., 10−6 ), we terminate the algorithm. λ, τ and ρ are three parameters speciﬁed in input,
which are discussed in Sec. 3.1.

2.3 Convergence of the Algorithm
The convergence property of an iterative optimization procedure like the algorithm proposed above is critical to its usefulness. The Algorithm 1 has similar convergence property
as the methods described in [11], which are also augmented
Lagrange multiplier based approaches. We can draw the following theorem:

t+1
t
−2
Theorem 1 If ∞
< ∞ and limt→∞ μti,j (Et+1
i,j −
t=1 μi,j (μi,j )
t
Ei,j ) = 0 ∀i, j, then Algorithm 1 will converge to the optimal solution for the problem of Eqn. 5.
The proof of Theorem 1 is included in the appendix.

2.4 Face Recognition Using the Decomposition
With the components in Eqn. 1 estimated from the previous algorithm, we now discuss how to classify a test image.
Recognizing that the sparse residual captures only imagespeciﬁc details that have not been absorbed by the common
or the global condition, we discard the sparse residuals from
the decomposition (training) stage and keep only the common and the low-rank components.

1472

3.1 Simulation-based Experiments
In this subsection, we use synthetic data to demonstrate
the convergence of the algorithm and selection of the parameters. The common components and condition components
used in this experiment are shown in Fig. 1 (b,c), where the
condition components are from [16] and both components
are rescaled to range [0, 1]. The sparse components are sampled from a uniform distribution in the range of [0, 1]. We
use those components to generate 25 images, which are used
in this experiment, as Eqn. 1.
Algorithm 1 in Sec. 2.2 requires three parameters, ρ controls the convergence speed; λ controls the sparsity of the
sparse residuals; and τ is a scalar. In [11], they suggest
ρ = 1.5, τ = 1.25 and λ = √1m for Robust PCA, where
m is the width of Xi,j . We have also found that λ = √1m
is optimal from the experiments, thus we adopt this selection in our paper. From the experiment, we found that
τ ∈ [0.125, 2] and ρ = 1.25 would be an optimal choice. Fig.
1 shows an example of the recovered common components

(a)

(a)

(b)

(c)

(d)

(e)

(b)

(c)

Figure 2: (a) the input data with 10 image manually removed, (b,c) is the common components and
condition components decomposed from (a) accordingly.

Figure 1: (a) shows the 25 images generated in the
experiment, where the sparse part has 20% support
of each image. (b,c) shows the ground truth of the
common components and condition components accordingly. We also show the common components
(d) and condition components (e) decomposed from
(b) when ρ = 1.25 and τ = 2.

commons are largely clean pictures of the subjects, while
the condition components align well with the given illumination conditions. This experiment shows the capability of
the proposed method with the Retinex model to discover
the illumination conditions and the subject commons from
a set of real images.
Next, we randomly pick 32 illumination conditions out of
the decomposed 64 conditions and the common components
of Subject 1 to form a subspace as described in Eqn. 14.
Then we use the proposed method to identify whether an
new image is in this subspace, by reconstructing this image
as the linear combination of the “basis” of this subspace, i.e.,
c1 + aj . Fig. 4 shows an example, where the new image is
also picked from Subject 1; and Fig. 5 shows another example, where the new image is picked from Subject 2. These
examples suggest that the learned components can be used
for identifying which subject an new image belongs to. Similarly, the learned components can also be used for identifying which conditions the new image is associated with.
These two scenarios are further evaluated in the following
two subsections, with real face images.

(d) and condition components (e) when the sparse part has
20% support of the image.2
To demonstrate the robustness of the algorithm, when
only part of data is available, we randomly remove 10 images
from the 25 images (Fig. 2(a)) and run the algorithm with
the same set of parameters. The results are shown in Fig.
2, where (b) is the recovered common components and (c) is
the recovered condition components. These results suggest
that the algorithm is still able to produce reasonable results
even with 40% of the images missing.

3.2 Decomposing a Set of Images
In this subsection, we ﬁrst demonstrate the decomposition
of the set of images from Extended YaleB dataset[4]. All the
2432 images from 38 subjects under 64 illumination conditions were used. The common components and the condition components are illustrated in in Fig. 3. Comparing
these with the original data, it is evident that the recovered

3.3 Recognizing the Face Images
In this subsection, we demonstrate the performance of the
proposed method in face recognition task, with the comparison to SRC, Volterraface and SUN on the extended YaleB
dataset and CMU PIE dataset. As these two datasets are
dominated by illumination conditions, we use the Retinex
model for the proposed method, i.e., the image is converted

2
The recovered parts are subject to a linear shift and scaling.
We identify the parameters for this linear shift and scaling
then map them back with those parameters.

1473

(a)

(b)
Figure 5: (a) the coeﬃcient for the linear combination, (b) the input image and (c) the reconstructed
image.

Figure 3: The decomposition of the extended YaleB
dataset. We use all the 2432 images which contain
38 subjects (b) and 64 illumination conditions (a).

ments are set to 0 and the corresponding images won’t be
used for training.
The Extended YaleB dataset [4] contains N = 38 subjects with 64 images for each subject, which correspond to
64 illumination conditions in the dataset. The images are resized to 48 × 42. The results on the extended YaleB dataset
are summarized in Tab. 1. From this table, we ﬁnd that
the proposed approach and Volterrafaces achieve the best
results; and SUN get obviously the lowest accuracy. The
performance of SRC degrade dramatically as the size of dictionary (i.e., number of training instances) reduced.
The CMU PIE dataset [17] contains N = 68 subjects with
varying poses, illuminations and expressions etc.. For all the
images, we manually crop the face region, according to the
eye position, then resize them to 50 × 35. The results are
summarized in Tab. 2. In Experiment 1, all 4 methods get
similar results; in Experiment 2, the proposed method and
Volterrafaces get the best result; and in Experiment 3, the
proposed approach gets the best result. In addition, the
proposed method is more robust to the missing of training
images. The performance of SRC degrades obviously as the
size of dictionary reduced.
To illustrate the speed performance of the proposed approach, we compared the time required to classify one image in our approach and the SRC approach. This time was
about 0.84 seconds in our method, and about 1.59 seconds
in SRC. The time for the decomposition (i.e., Algorithm 1)
is less than 5 minutes. The most time consuming part for
the proposed approach is the singular value decomposition
(SVD), which is used in computing the principle angle, so
an eﬃcient implementation of SVD can make the proposed
algorithm even faster.

Figure 4: (a) the coeﬃcient for the linear combination, (b) the input image, which is not observed
in the images for training the 32 illumination conditions, and (c) the reconstructed image.

to logarithm. In the SRC method, we build the dictionary
by containing all the training images as its columns. Since
there is no code publicly available for SRC, we build our own
implementation. For 1 optimization used by SRC, we used
Orthonormal Matching Pursuit (OMP)[1] as the solver. We
set the number of non-zero elements in the sparse coeﬃcient
(refer as K later) to be twice the number of conditions in
the training data. In addition, each image is normalized to
have zero mean and unit l2 norm for SRC. For Volterrafaces
and SUN, we use the author’s original implementation and
the provided parameters. For all the results, we present the
both mean and standard deviation of the accuracies of 3
rounds of experiments.
To examine the robustness of the approaches with respect
to the amount of training data, we use the following scheme.
In the experiment, we only pick “#train per subject” images
for each subject as the training instances, according to the
randomly generated sample matrix, where some of the ele-

3.4 Identifying the Conditions
Finally, we use an experiment to show how the proposed
method can be applied to to identifying the conditions the
testing images are associated with. The AR dataset [14]
contains N = 100 subjects and 26 images for each subjects.
The dataset contains 2 sessions, which are taken at diﬀerent
times. Each session contains 13 conditions: 4 for expressions, 3 for illuminations, 3 for sun glasses and 3 for scarves.

1474

(a) Experiment 1
#train per subject
Proposed
SRC
Volterrafaces
SUN

32
24
99.78±0.24%
99.54±0.04%
96.48±0.44%
95.29±0.52%
99.95±0.06% 99.80±0.26%
89.61±1.85%
87.64±2.80%
(b) Experiment 2

16
99.18±0.14%
91.90±0.94%
99.48±0.49%
76.91±3.71%

8
95.15±1.03%
78.65±1.81%
90.22±11.84%
60.17±2.09%

#train per subject
Proposed
SRC
Volterrafaces
SUN

16
99.56±0.00%
89.14±0.00%
99.25±0.34%
79.22±0.00%

8
98.32±0.03%
81.02±0.13%
96.27±4.03%
68.86±0.00%

4
80.03±2.17%
58.54±1.26%
91.03±2.43%
51.60±0.00%

12
99.33±0.23%
87.88±0.44%
99.17±0.39%
76.75±0.00%

Table 1: The results on extended YaleB dataset. Experiment 1: we randomly pick M = 32 illumination
conditions for training and the remaining for testing, i.e., we will obtain N = 38 common components and
M = 32 conditions by the proposed method. Experiment 2: we manually pick M = 16 illumination conditions
for training and the remaining for testing.

4. CONCLUSIONS AND FUTURE WORK
In this paper, we proposed a novel decomposition of a
set of face images of multiple subjects, each with multiple
images. The decomposition ﬁnds a common image and a
low-rank image for each of the subjects in the set. All the
low-rank images form a set that is used to capture all possible global conditions existing in the set of images. This facilitates explicit modeling of typical challenges in face recognition, such as illumination conditions and large occlusion.
Based on the decomposition, a face classiﬁer was designed,
using the decomposed components for subspace reconstruction and comparison. The classiﬁcation performance shows
that the proposed approach can achieve state-of-the-art performance. Experiments also showed that the proposed method
is robust with missing training images, which can be an important factor to consider in a practical system. We also
demonstrated with experiments that the decomposition indeed captures physically meaningful conditions, with both
synthetic data and real data.
There are a few possible directions for further development of the work. In particular, the current algorithm assumes that the low-rank conditions of the training images
are known and given for each of them. In practice, if the
data do not have such image-level label (but still with a
ﬁnite set of low-rank conditions), it is possible to expand
the current algorithm by incorporating another step that
attempts to estimate a mapping matrix for assigning a condition label to each image, during the optimization iteration. For example, we may deﬁne a mapping matrix Φ
with Φi,j = k deﬁning that training image Xi,j is associated with condition Ak . Eqn. 1 suggests a constraint that
we may use to solve for Φ: the optimal mapping matrix
should result in the most sparsity for Ei,j or the lowest rank
for Ak , given the same reconstruction error. If we use the
ﬁrst criterion, the
 problem of ﬁnding Φ can be formulated
as Φ = argmin
Xi,j − Ci − AΦi,j 1 .

Figure 6: The confusion matrix (in percentage)
of condition recognition result from the proposed
method, where both axes are the condition index.
The axis is index of the conditions.

In our experiments, we use one session for training and the
other session for testing. The images are converted to gray
scale and resized to 55 × 40. To recognized the associated
condition, we slightly changes the formulation of the subspace:

Si = {x|x =
wj × (ai + cj ) ∀w ∈ RN } (14)
j

Sy

= {x|x =



wj × (y + cj ) ∀w ∈ RN }

(15)

j

where Si is the subspace for condition i and Sy the subspace
for the test image. The other settings were the same as those
of previous face recognition experiments.
The proposed method achieves an accuracy of 91.77% in
recognizing the conditions, with the confusion matrix given
in Fig. 6, where we achieved over 96% accuracy for all but
conditions 1, 2, 3 (3 expressions) and 12. This experiment
again demonstrates the eﬀectiveness of the proposed method
in capturing the physical conditions in the form of low-rank
components.

Φ

i,j

5. ACKNOWLEDGMENT
The work was supported in part by a grant (Grant No.
0845469) from the National Science Foundation. Any opinions, ﬁndings, and conclusions or recommendations expressed

1475

(a) Experiment 1
#train per subject
Proposed
SRC
Volterrafaces
SUN
#train per subject
Proposed
SRC
Volterrafaces
SUN
#train per subject
Proposed
SRC
Volterrafaces
SUN

20
100±0.00%
99.88±0.07%
100±0.00%
100%
(b)

15
100±0.00%
99.88±0.07%
100±0.00%
99.84±0.11%
Experiment 2

10
99.65±0.37%
99.73±0.14%
100±0.00%
99.45±0.43%

12
100±0.00%
99.91±0.16%
100±0.00%
100±0.00%
(c)

9
99.960.08%
98.89±1.74%
100±0.00%
99.84±0.05%
Experiment 3

6
99.17±0.15%
96.90±3.73%
99.54±0.31%
98.53±0.29%

40
99.98±0.03%
99.98±0.03%
99.60±0.22%
99.93±0.05%

30
99.92±0.06%
99.45±0.03%
98.37±0.47%
99.38±0.14%

20
99.24±0.06%
96.79±0.28%
97.63±0.28%
97.89±0.30%

5
97.49±0.21%
97.73±0.54%
95.83±4.16%
95.75±0.49%
3
94.70±0.20%
87.18±1.78%
94.30±4.72%
88.75±4.72%
10
90.95±0.70%
86.98±0.16%
89.72±1.45%
88.29±0.02%

Table 2: The result on CMU-PIE dataset. Experiment 1: we pick the images with frontal pose (C27), which
include 43 illumination conditions for each subject. We randomly pick M = 20 conditions for training and the
remaining for testing. Experiment 2: we again only pick the image with frontal pose, but we randomly pick
M = 12 conditions for training and the remaining for testing. Experiment 3: we use all the images from 5
near frontal poses (C05, C07, C09, C27, C29), which includes 153 conditions for each subject. We randomly
pick M = 40 conditions for training and the remaining for testing..
[9] R. Kumar, A. Banerjee, and B. Vemuri. Volterrafaces:
Discriminant analysis using volterra kernels. In
Computer Vision and Pattern Recognition, 2009.
CVPR 2009. IEEE Conference on, pages 150 –155,
june 2009.
[10] K. Lee, J. Ho, and D. Kriegman. Acquiring linear
subspaces for face recognition under variable lighting.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, pages 684–698, 2005.
[11] Z. Lin, M. Chen, L. Wu, and Y. Ma. The augmented
lagrange multiplier method for exact recovery of
corrupted low-rank matrices. Arxiv preprint
arXiv:1009.5055, 2010.
[12] J. Liu, S. Chen, and X. Tan. Fractional order singular
value decomposition representation for face
recognition. Pattern Recogn., 41:378–395, January
2008.
[13] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and
A. Zisserman. Discriminative learned dictionaries for
local image analysis. In Computer Vision and Pattern
Recognition, 2008. CVPR 2008. IEEE Conference on,
pages 1–8. IEEE, 2008.
[14] A. Martinez and R. Benavente. The AR face database.
Technical report, CVC Technical report, 1998.
[15] P. Nagesh and B. Li. A compressive sensing approach
for expression-invariant face recognition. In Computer
Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on, pages 1518 –1525, 2009.
[16] J. Portilla and E. Simoncelli. A parametric texture
model based on joint statistics of complex wavelet
coeﬃcients. International Journal of Computer
Vision, 40(1):49–70, 2000.
[17] T. Sim, S. Baker, and M. Bsat. The CMU pose,
illumination, and expression (PIE) database. In

in this material are those of the authors and do not necessarily reﬂect the views of the National Science Foundation.

6.

REFERENCES

[1] M. Aharon, M. Elad, and A. Bruckstein. K-SVD:
Design of dictionaries for sparse representation.
Proceedings of SPARS, 5, 2005.
[2] J. F. Cai, E. J. Candes, and Z. Shen. A singular value
thresholding algorithm for matrix completion.
preprint, 2008.
[3] E. Candes and Y. Plan. Matrix completion with noise.
Proceedings of the IEEE, 2009.
[4] A. Georghiades, P. Belhumeur, and D. Kriegman.
From few to many: Illumination cone models for face
recognition under variable lighting and pose. IEEE
Transactions on Pattern Analysis and Machine
Intelligence, 23(6):643–660, 2001.
[5] D. Goldfarb and S. Ma. Convergence of ﬁxed-point
continuation algorithms for matrix rank minimization.
Foundations of Computational Mathematics, pages
1–28, 2011.
[6] E. T. Hale, W. Yin, and Y. Zhang. Fixed-point
continuation for 1 -minimization: Methodology and
convergence. SIAM Journal on Optimization,
19(3):1107–1130, 2008.
[7] C. Kanan and G. Cottrell. Robust classiﬁcation of
objects, faces, and ﬂowers using natural image
statistics. In Computer Vision and Pattern
Recognition (CVPR), 2010 IEEE Conference on,
pages 2472 –2479, june 2010.
[8] A. Knyazev and M. Argentati. Principal angles
between subspaces in an A-based scalar product:
algorithms and perturbation estimates. SIAM Journal
on Scientific Computing, 23(6):2008–2040, 2002.

1476

Proceedings of the 5th International Conference on
Automatic Face and Gesture Recognition, 2002.
[18] J. Wright, A. Ganesh, S. Rao, Y. Peng, and Y. Ma.
Robust Principal Component Analysis: Exact
Recovery of Corrupted Low-Rank Matrices via Convex
Optimization. In Advances in Neural Information
Processing Systems 22.
[19] J. Wright, A. Yang, A. Ganesh, S. Sastry, and Y. Ma.
Robust face recognition via sparse representation.
Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 31(2):210–227, 2008.

Eqn. 5.
Proof For (Ċ t+1 , Ȧt+1 , Ė t+1 ), we have the following:
L(Ċ t+1 , Ȧt+1 , Ė t+1 , Ẏ t , μt ) = min L(C, A, E, Ẏ t , μt )
C,A,E

≤
≤

PROOF OF THEOREM 1

t+1
Proposition 1 The sequences of Ỹi,j
,
t+1
and Ẏi,j are all bounded ∀i, j, where

t+1
i Ŷi,j ,



t
μti,j (Xi,j − Ct+1
− At+1
− Et+1
i
j
i,j ) + Yi,j

t+1
Ŷi,j

=

t
μti,j (Xi,j − Cti − At+1
− Et+1
j
i,j + Yi,j

t+1
Ỹi,j

=

t
μti,j (Xi,j − Cti − Atj − Et+1
i,j ) + Yi,j

t+1
Ẏi,j

=

t+1
t
μti,j (Xi,j − Ċit+1 − Ȧt+1
− Ėi,j
) + Ẏi,j
j

Aj ∗ + λi,j Ei,j 1

i,j

=

L(Ċ t+1 , Ȧt+1 , Ė t+1 , Ẏ t , μt ) −

≤

f∗ −

t−1 2
t
 Ẏi,j
− Ẏi,j
F
t
2μi,j
i,j

t−1 2
t
 Ẏi,j
 t −1
− Ẏi,j
F
= f ∗ + O(
(μi,j ) )
t
2μi,j
i,j
i,j

t+1
is bounded ∀i, j.
where we use the knowledge that Ẏi,j

∗
∗
Take t → ∞, we have i,j Ȧj ∗ + λi,j Ėi,j
1 = f ∗ . Using
t−1
t−1
t−1
t
− Ẏi,j
) = μt−1
− Ȧt−1
− Ėi,j
) and bound(Ẏi,j
i,j (Ẋi,j − Ċi
j
t+1
∗
∗
edness of Ẏi,j ∀i, j, we also have Xi,j − Ċi − Ȧ∗j − Ėi,j
=0
∗
∗
∗
∀i, j. Thus (Ċ , Ȧ , Ė ) is the optimal solution for Eqn. 5.
t+1
t
t
By Xi,j − Ct+1
− At+1
− Et+1
i
j
i,j = μi,j (Yi,j − Yi,j ) and
t+1
t+1
t
boundedness of Yi,j , we have limt→∞ Ci + Aj + Et+1
i,j =
Xi,j ∀i, j, i.e., (Ct+1 , At+1 , Et+1 ) approaches to a feasible solution. In addition, we have
 t −1 t+1
 t+1
t+1
Aj − Atj F = 
(μi,j ) (Ŷi,j − Ỹi,j
)F


and (Ċ t+1 , Ȧt+1 , Ė t+1 ) is the optimal solution to the probt N,M
lem minC,A,E L(C, A, E, Ẏ t , μt ) with Ẏ t = {Ẏi,j
}i,j=1 .
Proof Let’s write the Lagrange function in 6 as:
L({Cti }i , {Atj }j , {Eti,j }i,j , {Yti,j }i,j , {μt }i,j )
 t
Aj ∗ + λi,j Eti,j 1
i,j

+



We also have:
 t+1
t+1
Ȧj ∗ + λi,j Ėi,j
1

t+1
j Yi,j

=

+

min

Ci +Aj +Ei,j =Xi,j ,∀(i,j)

i,j



Yt+1
i,j

=

Ci +Aj +Ei,j =Xi,j ,∀(i,j)

L(C, A, E, Ẏ t , μt )

= f∗

APPENDIX
A.

min

μti,j
Xi,j − Cti − Atj − Eti,j 2F
2
< Yti,j , Xi,j − Cti − Atj − Eti,j >

i

∞

i

t
−1
With the assumption
< ∞, boundedness of
t=1 (μi,j )
 t+1
t+1
t
∗
Ŷ
and
Ỹ
,
A
has
a
limit
A
i,j
j . Similarly:
j
i i,j

 t+1
t+1
Aj − Atj + Ct+1
− Cti F = 
(μti,j )−1 (Yt+1

i
i,j − Ỹi,j )F

For simplicity, we will use L(Ct , At , Et+1 , Yt , μt ) instead of
t
t
L({Cti }i , {Atj }j , {Et+1
i,j }i,j , {Yi,j }i,j , {μ }i,j ). The subgrat
t
t
dient of L(C , A , E, Y , μt ) over Ei,j is

j

λi,j ∂Ei,j 1 − μti,j (Xi,j − Cti − Atj − Ei,j ) − Yti,j



j

Thus limt→∞ j At+1
− Atj + Ct+1
− Cti = 0. Since At+1
j
i
j
t+1
∗
has limit Aj , then Ci has limit C∗i , then Et+1
i,j has limit
Xi,j − A∗j − C∗i . So (C∗ , A∗ , E∗ ) is a feasible solution.
Considering the subgradients and the optimality of Et+1
i,j
 t+1
t+1
t+1
t+1
Ŷ
and At+1
,
we
have
Ỹ
∈
∂E

and
∈
∂A
∗ .
1
j
i,j
i,j
j
i i,j

t
t
t
As Et+1
i,j is optimal for the problem argmin L(C , A , E, Y , μt )
Ei,j

t+1
0 ∈ λi,j ∂Eti,j 1 − Ỹi,j

t+1
i.e., Ỹi,j
∈ λi,j Et+1
According to the property of subgradients:
i,j 1 ; and according to the Theorem 3 of
t+1

 t+1
[11], Ỹi,j is bounded ∀i, j. Similarly, we can also show that
t+1
(
At+1
∗ + λi,j Et+1
Ȧj ∗ + λi,j Ėi,j
1 )
 t+1  t+1
j
i,j 1 ) − (
t+1
Ŷ
,
Y
and
Ẏ
are
bounded
∀i,
j.
i,j
i,j
i i,j
j
i,j
i,j

Proposition 2 The sequences of (Ct+1 , At+1 , Et+1 ) is bounded.
t+1
t+1
t+1
− < Ŷi,j
, Ȧt+1
− At+1
> − < Ỹi,j
, Ėi,j
− Et+1
≤
j
j
i,j >
Proof For Algorithm 1, we can ﬁnd that:
i,j

L(Ct+1 , At+1 , Et+1 , Yt , μt ) ≤ L(Ct , At+1 , Et+1 , Yt , μt )
t
t+1
=
−μti,j < Et+1
− At+1
>
i,j − Ei,j , Ȧj
j
t
t
t+1
t
t
t
t
t
t
t
≤ L(C , A , E , Y , μ ) ≤ L(C , A , E , Y , μ )
i,j
t
 μt−1
t+1
t+1
t+1
t+1
t
t
i,j + μi,j
< Ỹi,j
, Ỹi,j
− Ỹi,j
>
, Ẏi,j
− Ẏi,j
>
< Ỹi,j
2
Yti,j − Yt−1
= L(Ct , At , Et , Yt−1 , μt−1 ) +
i,j F
−
−
t
2
t
t
(μ
)
i,j
μi,j
μi,j
i,j
∞ t+1 t −2
t+1
t+1
By boundedness of assumption that
<
By Proposition 1 and 2 that Yi,j , Ẏi,j are bounded; by
t=1 μi,j (μi,j )


∗
∞ and j Yti,j ∀i, j, we have L(Ct+1 , At+1 , Et+1 , Yt , μt ) is
1 = f ∗ ; and by asProposition 3 that i,j Ȧ∗j ∗ + λi,j Ėi,j


t
t
t+1
t
t
upper bounded. Thus i,j Aj ∗ + λi,j Ei,j 1 is bounded.
sumption limt→∞ μi,j (Ei,j − Ei,j ) = 0, we have i,j A∗j ∗ +
λi,j E∗i,j 1 = f ∗ . That is (C∗ , A∗ , E∗ ) is optimal for the
Proposition 3 The accumulation point (Ċ ∗ , Ȧ∗ , Ė ∗ ) for
t+1
t+1
t+1
problem in Eqn. 5. This completes the proof of Theorem 1.
sequences (Ċ , Ȧ , Ė ) is optimal for the problem in

1477

Synthesis of Stereoscopic Views from Monocular Endoscopic Videos
Jin Zhou, Qiang Zhang, Baoxin Li
Computer Science and Engineering
Arizona State University, Tempe, AZ

Ananya Das
Mayo Clinic Arizona
Scottsdale, AZ

{jinzhou, qzhang53, baoxin.li}@asu.edu

das.ananya@maya.edu

Abstract
Recent studies have shown that 3D imaging provides
some unique advantages over traditional 2D imaging for
minimal invasive surgery. However, most existing endoscopes still use single-lens cameras, and the use of duallens 3D imaging techniques is still limited. This paper proposes an approach to enabling 3D imaging from a singlelens endoscope by automatically synthesizing stereoscopic
views from monocular images captured by the endoscope.
We first formulate the problem by introducing the notion of
normalized disparity, based on which we show that affine
reconstruction is sufficient for stereoscopic view synthesis.
With this formulation and exploiting other domain-specific
constraints, we then propose a robust structure-from-motion
algorithm for a sparse set of feature points and a fast, linear interpretation algorithm for creating a dense disparity field for synthesizing stereoscopic views from original
monocular video. Both synthetic images and real endoscopic videos are used to evaluate the proposed method.
The results demonstrate the feasibility and effectiveness of
the proposed method.

1. Introduction
Endoscopy using flexible video-endoscopes is a universally used procedure for the diagnosis and therapy of various pathologies of the gastrointestinal tract. In addition to a
compact video camera, an endoscope is also equipped with
a light source and a manipulator that can be controlled by
the physician to remove some tissues or to perform other
operations. For this reason, it is considered as the vehicle for minimal invasive surgery [3]. Since a monocular
video camera can provide only 2D images, which are different from what the physician can see from the actual sites
of the body, efforts have been spent on the development of
3D imaging systems for endoscopy [3][11]. Recent studies [16][10][7][1] reported that stereoscopic vision provides
significant advantages over traditional 2D imaging methods
for minimal invasive surgery. For example, the benefits in-

978-1-4244-7030-3/10/$26.00 ©2010 IEEE

clude faster and safer surgical operations [7] and shorter
learning curve [16].
Stereoscopic vision in endoscopy can be achieved by using 3D imaging with stereo cameras and special display
systems [11]. However, it has been shown that the current stereo-endoscopy still has some limitations [11]. For
instance, the baseline between the lenses of the stereo cameras is fixed (and usually very small) in a stereo endoscope
and cannot be adjusted, resulting inflexible and weak depth
perception; the images captured by the two lenses may suffer from different lighting due to the difference in viewing
angles and/or positions of the two lenses. Consequently,
stereo endoscopic system has yet to gain wide adoption.
In this paper, we present a systematic approach to enabling stereoscopic vision in widely-adopted monocular endoscopes. We first formulate the problem as one of synthesizing stereo views from monocular endoscopic images,
using the notion of normalized disparity. Based on this,
we show that affine reconstruction is sufficient for this purpose. Then an algorithm is designed to recover the depth
of a set of feature points in each frame based on structurefrom-motion, which is combined with another disparity interpolation step for obtaining the dense disparity field for
synthesizing the stereoscopic views. We prove that a linear
interpolation of the disparity field corresponds to a linear
interpolation in the 3D space, hence verifying the correctness of the algorithm. The synthesized views can then be
used with a proper 3D viewing scheme such as a glassesfree 3D display as illustrated in Fig. 1. Since the proposed
approach does not rely on stereo cameras, it can avoid many
of the limitations of existing stereo-endoscopy systems as
discussed above. In particular, with the proposed method,
the disparities may be relatively easily adjusted to achieve
desired perceived depth.
The remaining of the paper is organized as follows. We
first briefly review related work in Sec. 2. Sec. 3 presents
the details of the proposed approach. Experimental results
are presented in Sec. 4. Discussion and conclusions are
summarized in Sec. 5.

55

Figure 1. Illustrating the concept: Given a monocular image sequence, the system synthesizes the corresponding stereo images;
the data can then be viewed on a 3D display.

2. RELATED WORK
To assist gaining 3D perception from monocular endoscopic images, various techniques have been proposed.
The work of [6][2] combines endoscopic images with other
types of 3D data, which are obtained from Computed Tomography (CT), or Magnetic Resonance (MR), or Laser
Range Finder (LSR). These methods are not only expensive
but also difficult to use in practical endoscopy due to many
factors such as the sheer size of the required equipments.
Another track of research is to create 3D models purely
from monocular images sequences, which are known as
shape-from-X in computer vision. For example, shapefrom-shading method was used in [14] to obtain the 3D
model from a single frame. The model obtained from this
method is typically very limited in complexity due to the
limited 3D information from a single frame. Moreover,
such a method is typically sensitive to lighting condition
variation, and thus is not suitable for endoscopic images,
in which glares occur quite often, among other lighting irregularities. Shape-from-motion (also known as structurefrom-motion) was used in [15][17] to obtain 3D models.
The work of [15] assumes that cameras only have translational movement and no rotation, which is too restrictive for
an endoscope. The approaches of [17] rely on factorization
methods to estimate 3D structure. Factorization methods
work for only a weak perspective camera model and usually require complete point correspondences (i.e. each feature point should appear in every frame), which are difficult
to guarantee for endoscopic images due to fast backwardforward motion of the camera and the lack of distinctive textures, non-rigid motion, noise and glare, etc. Another common problem with the above structure-from-motion methods is that the resultant model is usually spiky due to the
sparsity of the feature points, which is not good for 3D
visualization. A method was proposed in [19] to create a
smooth 3D surface by fitting a circular generalized cylinder
with Markov Random Fields. However, this method can
only work with tube-like organs.
There are also some other related efforts for creating
stereo images from monocular image sequences. In [9],

two frames from the monocular video are chosen and rectified into a stereo pair. Such approaches typically require the
camera to move laterally, which is not natural in endoscopic
imaging. Homography-based transforms are used in [8][18]
to create stereo views. However, the methods cannot produce true stereo images, since pure image transformation
does not introduce depth parallax and thus cannot simulate
camera re-positioning. The method of [4] first computes
the depth for a set of feature points based on a plausible
Euclidean reconstruction, then propagates the depth to all
pixels and uses the dense depth map to create stereo views.
The practical challenge is that accurate dense depth map is
still difficult to obtain and the process is typically computationally costly.

3. STEREOSCOPIC VIEW SYNTHESIS
FROM A MONOCULAR IMAGE SEQUENCE
In this section, we present the proposed approach for
synthesis of stereoscopic views from a monocular endoscopic video. We first formulate the problem as one of recovering the normalized depth (or disparity) for every pixel
in every frame of the original video, in Sec. 3.1. Then, in
Sec. 3.2 and Sec.3.3 respectively, we present two core algorithms to recover the normalized depth, i.e. sparse depth
recovery via structure-from-motion and dense depth recovery via linear interpolation. The complete approach is then
summarized in Sec. 3.4.

3.1. Formulating the Problem with Normalized Disparities
Stereo images are usually captured by a stereo camera
which consists of two individual imaging planes, as Fig. 2
shows. In general, the configuration of a stereo camera can
be either of the two types: parallel configuration or cross
configuration. In parallel configuration, the principal axes
of the two cameras are parallel to each other and perpendicular to the baseline, as Fig. 2(a) shows. In this case, the
image planes are aligned. On the other hand, for cross configuration, the principal axes intersect at a finite point, as
Fig. 2(b) shows. These two different configurations lead to
different properties of the stereo images. In the parallel configuration, there are only horizontal disparities in the stereo
images, while in the cross configuration, there are both horizontal and vertical disparities. In practice, stereo images are
usually captured by the stereo camera with the parallel configuration. If we have the stereo images obtained from the
parallel configuration, it is possible to obtain the stereo images from the cross configuration by applying some transformations [20]. Thus we will focus on only the parallel
configuration in the subsequent discussion.
From Fig. 2(a), it is easy to derive the relationship be-

56

depth L̂ can be calculated as

(a)

(b)

Figure 2. Stereo camera configurations. (a) Parallel configuration:
the principal axes are parallel and perpendicular to the baseline.
(b) Cross configuration: the principal axes intersect at some finite
point.

tween the depth L of an object and its disparity d:
d=

fk
L

(1)

where k is the baseline and f is the focal length. We can
rewrite Eqn. 1 as d = λL−1 , which suggests that if the
depth of every pixel is known, we can synthesize a stereo
pair from a single image by choosing an appropriate scaling
factor λ:
Ir (x0 , y) = Il (x, y) with x0 = x+d(x, y) = x+λL−1 (x, y)
where Il is the original (left) image and Ir the corresponding stereo image. Il (x, y) denotes the pixel value of the
point (x, y) on the original image.
During the acquisition of an image sequence, the focal
length of the camera may change. If we assume that the
baseline of the stereo camera does not change, i.e., k is constant, then we need to scale the depth according to the focal
length: L̂ = L/f . We call L̂ the normalized depth, which
corresponds to a camera whose focal length is equal to 1.
Thus we modify the correspondence relation in Eqn. 2 as

L̂ = X̂(3) with X̂ = KR(X−C) = (u, v, L̂)T = L̂(x, y, 1)T
(4)
where K is the camera internal matrix, R the camera orientation, and C the camera center. X(3) denotes the third element of X. X and X̂ are both inhomogeneous coordinates
which are 3 × 1 vectors. (x, y, 1)T is the homogeneous coordinates of the image point, which is equal to (u, v, L̂)T up
to a scale factor. X̂ is the new coordinates of the 3D point
in which the world coordinates system is the same as the
camera coordinates system and the camera internal matrix
is equal to identity (i.e. with a focal length 1). We call the
3D space of X̂ normalized 3D space. In the normalized 3D
space, the z coordinate of X̂ corresponds to the normalized
depth.
The problem of recovering the 3D information of image
points as well as the camera parameters from multiple views
is the well-known structure-from-motion (SfM) problem in
computer vision. Formally, the problem can be stated as:
Given a set of point correspondences {xji } , where xji is the
2D projection of i-th point on j-th frame, recover the 3D
coordinates of the points Xj and camera information {Pj },
with x̃ji ← Pj X̃i , where x̃ji is the homogeneous coordinates
of xji and X̃i is the homogenous coordinates of Xi . Pj =
Kj Rj [I| − Cj ] is a 3 × 4 matrix.
Fig. 3 illustrates the typical process of structure-frommotion. We simulate a camera moving inside a torus (mimicking the endoscope inside the human intestine).
Without camera calibration information (i.e., unknown
), we can only obtain a projective reconstruction for the initial two cameras from the fundamental matrix F . Thus the
whole reconstruction is also up to a projective transformation, since
x̃ji ← Pj H −1 H X̃i
(5)

We further define the normalized disparity as the inverse of
the normalized depth, i.e.,

where H is a 4 × 4 non-singular matrix. Eqn 5 means
that if {Pj , X̃i } is a valid 3D reconstruction for xji , then
{Pj H −1 , H X̃i } is also a valid 3D reconstruction for xji . In
a projective reconstruction, the depths of points may not be
uniquely determined and thus cannot be directly used for
stereo rendering. This can be seen from the following analysis. We first rewrite Eqn. 4 as

dˆ = L̂−1

X̂ = P (X T , 1)T = (u, v, L̂)T

x0 = x + λL̂−1 (x, y)

(2)

(3)

ˆ y). Therefore, the problem of synThus x = x + λd(x,
thesizing stereo images from a monocular image sequence
is equivalent to estimating the normalized depth or normalized disparity for every pixel in every frame.

(6)

0

3.2. Depth Recovery via Structure-from-Motion

This can be further rewritten as (assuming that the homogeneous coordinates of X is X̃ ← (X T , 1)T :
X̂ = P X̃/X̃(4) = (u, v, LT )T

(7)

where X̃(4) is the fourth coordinate of X̃. Thus we have

Given an image point, if we know its corresponding 3D
position X and also the camera parameters, the normalized

L̂ = M (3)/X̃(4)

57

(8)

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

(j)

(k)

(l)

Figure 3. Structure from motion. (a) is the outside view of a torus.
The camera is moving inside the torus and takes 200 views. (b) to
(d) are three different views in the image sequence. (e) is the point
correspondences between (b) and (c). (f) and (g) draw the epipolar
lines for (b) and (c) respectively. (h) and (i) draw the reprojected
points (in cross) and original points (in circle) for (b) and (c). (j)
draws the reprojected points and original points for (d). (k) and (l)
show two views of the reconstructed points and cameras.

where M = P X̃ and M (3) is the third coordinate of M . If
we apply a projective transformation to the reconstruction
such that P 0 = P H −1 and X̃ 0 = H X̃, the new normalized
depth is
L̂0 = (P 0 X̃ 0 )(3)/X̃ 0 (4) = (P X)(3)/X̃ 0 (4) = M (3)/X̃ 0 (4)
(9)
Comparing Eqn. 9 with Eqn. 8, we can see that
the only difference is in the last coordinate value of the
homogeneous coordinates of the 3D point. After applying
a projective transformation, this value may be changed.
Thus applying a projective transformation will alter the
normalized depth and then the projective reconstruction
cannot be used for stereo view synthesis. However, affine
transformation does not change this value, since the last
row of any affine transformation matrix has the form
(0, 0, 0, 1)T . To formally state this result, we introduce the
following lemma:
Lemma 1: The normalized depth is invariant under affine
transformation.
Lemma 1 suggests that affine 3D reconstruction is good
enough for stereo view synthesis. In other words, stereo
view synthesis based on affine 3D reconstruction is as good
as that based on Euclidean 3D reconstruction.
There are various techniques to rectify a projective re-

construction to an affine reconstruction and further to a Euclidean reconstruction ([5]). Essentially, to obtain a Euclidean reconstruction from a projective reconstruction is
equivalent to performing camera calibration, either manually or automatically; conversely, once we have camera
calibration information, we can directly obtain a Euclidean
reconstruction. However, to obtain an affine reconstruction is only equivalent to identifying the plane at infinity
or the infinity homography (the homography of the plane
at infinity), which is a much weaker requirement. In particular, it is shown in [5] that, for pure camera translation without rotation and change in the internal parameters,
F = [e]× = [e0 ]× , and one may choose the two cameras as
P = [I|0] and P 0 = [I|e0 ] for affine reconstruction.
In our application, since the dominant motion of the
endoscope is forward-backward translation, within a very
short period of time the camera motion will be mostly translational. Therefore, we can utilize the above result for initial
affine reconstruction by picking two initial frames that exhibit no camera rotation. (This can be done automatically
by assessing the goodness of fit of the simplified fundamental matrix with the data.) Then for other frames, we simply
use the affine reconstructed points to estimate the camera
information. The estimated new cameras and new image
points will be used to further reconstruct new 3D points. After we have an affine 3D reconstruction of the cameras and
the feature points, we can compute the normalized depth for
each point in each view based on Eqn. 8. Note that we do
not assume that the camera does not change orientation all
the time. The only assumption is that there is no relative
rotation for the chosen frames for the initial reconstruction.
Assuming mostly pure translational movement in initial
reconstruction also leads to another advantage: both the
fundamental matrix computation and the 3D reconstruction
are much more robust, due to the reduced degrees of freedom from 8 (for the original 3 × 3 fundamental matrix) to
2 (for the new fundamental matrix that can be represented
by the epipole, which is a 2D image point). The advantage is significant because the endoscopic data are usually
near degenerate for general fundamental matrix computation, i.e. the depth variance is small and the motion is also
small, which makes the computation very unstable. Fig. 4
illustrates the difference of the results from two different
approaches: reconstruction by general fundamental matrix
computation and reconstruction by assuming only translational movement. In the figure, the two frames were chosen
so that there is mainly only translational motion between
them. We might find many good planes. However, a numerically good model does not necessarily match the real
model. As a comparison, Fig. 4(e) (h) give the results
under the assumption of only translational movement. In
this case, we compute a special fundamental matrix model
which has only 2 degrees of freedom. The new epipolar

58

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

Figure 4. Comparison of the reconstruction results without the assumption (first row) and with the assumption of pure translation
(second row). Green lines are the epipolar lines, which correspond to the computed fundamental matrix. The first two columns
are the original image pairs, last two columns are the views of the
reconstructed point clouds of the organ walls from different perspectives.

Figure 5. Mapping between a 3D plane and the corresponding disparity plane.

lines are matched, and the reconstructed model is much better since a well-defined structure can be derived from the
point clouds.

To synthesize a stereo view from the sparse disparities, we
design a two step approach: 1) interpolate the disparities for
points on a regular grid overlaid onto the image (e.g., each
grid block being of 8 × 8 pixels); 2) synthesize the stereo
view based on the original image and the disparities of the
grid points.
The problem of predicting the values on unknown sites
based on the known values on a sparse set of sites is known
as regression or interpolation. Two popular approaches
for this problem are radial basis function interpolation and
Gaussian Process (GP) regression [13]. However, they are
both computationally expensive when the number of points
is large. In this paper, we propose the following four-step
approach to computing the disparities of the grid points.
First, we do Delaunay triangulation from the sparse 2D
points. After we have a set of triangles, we linearly interpolate every point inside the triangle based on the vertices.
In the second step, the disparities of the grid points are determined by those of the underlying pixels (which were interpolated in the previous step). Since the triangles may
not cover all points in the image, there may be some holes.
Thus in the third step we fill the holes. Finally, we perform
a smoothing operation on the disparities of the grid.
The first two steps and the final step of smoothing are
straightforward. For the third step of hole-filling, we use
a modified version of the Laplace interpolation [12] (also
called Laplace/Poisson interpolation). The basic idea of the
algorithm is to construct a linear constraint for each grid
point based on its neighbors. Fig. 6 gives an example illustrating the interpolation of the disparities for the grid points
from the original sparse disparities.
After we have the disparities for the grid points, we can
synthesize stereo views from the original images. In practice, we do a backward mapping for stereo view synthesis.

3.3. Disparity Interpolation and Stereoscopic View
Synthesis
The outcome of the SfM process is the camera parameters for each frame and a set of sparse 3D points, which
project to each of the frames on a set of sparse 2D points.
Therefore, from the SfM process, we only have the depth
for a sparse set of image points. However, to synthesize
a stereo image, we need the depth for every pixel in the
image. To this end, we first introduce the following lemma
that enables us to perform a linear interpolation on the
ˆ y).
normalized disparity field, i.e. d(x,
Lemma 2: Linear interpolation on the normalized disparity
field corresponds to linear interpolation in the normalized
3D space.
We can further show that a plane in the normalized 3D
space corresponds to a plane in the normalized disparity
space, which is illustrated in Fig. 5. The above analysis
shows that we do not need to fit a surface in the normalized
3D space to obtain depth for every pixel. Rather, we only
need to operate on the normalized disparity field. Directly
operating on the normalized disparity field has several advantages:
1. The disparities can be directly used for stereo image
rendering. If we compute a surface in the 3D space,
we still need to compute the depth for every pixel for
each image and this can be a computationally costly
task;

2. The disparity space is much smaller than 3D space and
thus more robust for interpolation;
3. The 3D space structure is more complex than the disparity field since the data point has one more dimension.

59

The procedure is as follows. Firstly, we map the disparities
of grid points in source image into target images. Secondly,
the disparities of pixels other than grid points in the target
image are interpolated from disparities of those grid points.
One practical issue with stereo view synthesis is the
choice of the disparity scale, which corresponds to the
choice of the baseline of a stereo camera. On one hand,
too large a baseline will lead to too large disparities, which
cause difficulty for human to fuse the stereo images in gaining the 3D perception. On the other hand, too small a baseline does not give rise to good 3D perception. To this end,
we compute a scaling factor from the grid disparities of the
first frame such that after scaling with this factor, the average disparity of the frame equals to a predefined value (e.g.
10 pixels). Then for each of other frames, the same scaling
factor is applied.

3.4. The Complete Algorithm
We now summarize the previous discussion and processing steps into the following complete algorithm.
Algorithm: Stereo Video Synthesis
Input: A monocular endoscopic video
Output: A stereoscopic video with one channel being the
input video
1. Do point tracking to obtain point correspondences
{xji };
2. Do structure-from-motion to obtain an affine reconstruction of cameras {Pj } and points {X̂i };
3. Extract the normalized disparities for points {xji }
based on Eqn. 9 and 3.
4. Interpolate the normalized disparities on a regular grid;
Compute a scaling factor λ such that the average of
grid point disparities in the first frame equals to a predefined value; Scale the grid point disparities with λ
for other frames.
5. Synthesize stereo views using the original views and
the grid disparities.

4. EXPERIMENTS AND RESULTS
We present 3 experiments in this section to show the effectiveness of the proposed approach. In the first experiment, we verify our algorithm using simulated data for
which we have the ground truth model. Experimental results from 2 different sets of real endoscopic images are
then provided to illustrate the performance of the algorithm
with real images. In our current implementation, the point
tracking module is based on KLT method. Since no code
optimization has been done, the speed performance is not

evaluated. In all the 3 cases, we have created video clips
to facilitate the visualization of the final results. The videos
are available from supplemental materials and viewed with
a simple red-cyan glasses.

4.1. Results from Synthetic Data
We have already seen the synthetic data in Fig. 3, where
we illustrated the results of the SfM process. Here we
present the results of disparity interpolation and the synthesized stereo views, which are shown in Fig. 6. Due to space
limit, only four frames are picked to illustrate the results
at different stages. Comprehensive demos can be found in
the supplemental video. The frame indices are 0, 2, 11 and
29 respectively. In the beginning frames, the triangles obtained from the projected 3D points cannot cover the whole
image, as shown in the first three images of the 1st row of
Fig. 6. As the camera moving forward, the uncovered region becomes smaller and smaller, since the moving directions of feature points are outward. The 2nd row shows the
final dense disparity map after grid disparity interpolation,
hole-filling by Laplace interpolation and smoothing. While
frame 0 still contains black regions (since the disparities of
all the boundary points in frame 0 are zero), the holes are
successfully filled in frame 2 and frame 11. From the constraints of the Laplace interpolation algorithm, we can see
that the disparity of the boundary points depends only on
that of the boundary points. Once there is one non-zero
boundary point, all the black region will be filled. The result
of frame 2 shows that even large black regions can be reasonably interpolated. The 3rd row shows the ground truth
disparity map for each frame, which are exactly the same
due to the experiment setup. For comparison, we scale the
disparity map such that the mean value is the same as that
of the result of frame 29. Comparing the 2nd row and 3rd
row, we can see that the result of frame 29 is very close to
the ground truth, except that it is more blurred, due to the
smoothing operation. The result of frame 11 is also close
to the ground truth, except some artifacts in the lower left
corner, which are interpolated black region. Other black regions of frame 11 are gracefully interpolated. The artifacts
in frame 2 are more obvious, but still tolerable. In our visual
tests, the synthesized stereo view of frame 2 can still provides good 3D experience. The stereo results of all frames
can be examined from the video supplement materials.
Since the stereo images are synthesized based on the disparity map, we can measure the quality of the final results
by evaluating the precision of the reconstructed disparity
map. To do this, we calculate the average difference between the ground truth disparity map (the fourth row in Fig.
6) and the reconstructed one (the third row in Fig. 6) for
each frame. Because the disparities are up to a scaling, we
normalize all the average disparity differences by dividing
them with a constant value, i.e. the average value of the

60

(0)

(12)

(19)

(45)

Figure 8. Results of real endoscopic data (CREEL).

(0)

(2)

(11)

(29)

Figure 6. Results of synthetic data. Intermediate results of four
frames (0, 2, 11 and 29) are shown. The 1st row is the dense
disparity after triangulation and interpolation. The 2nd row shows
the final dense disparity after grid point sampling, holes filling and
smooth. The 3rd row shows the ground truth disparity image (after
properly scaled). The 4th row shows the synthesized stereo views
in red-cyan format. The last row is the frame index.

Figure 7. Precision of the disparities.

ground truth image (In our simulation, the ground truth disparity map is the same for every frame and the average value
is 122.7 in Fig. 6). Fig. 6 shows the precision results. We
can found that only the first two frames have large errors.
The reason is that there are large black regions even after
interpolation. From the third frame, the error quickly drops
to 5 percent (of the average disparity). Note that in Fig. 6,
frame 11 still has some black regions. This shows that the
precision of our disparity map is high and our method is
robust to incomplete information.

4.2. Real Data Experiments
2 real monocular endoscopic videos were used to test the
method. From Fig. 8 and Fig. 9, we present the results on
these 2 different datasets, with the name CREEL and GRAY
respectively. For each dataset, the dense disparity maps and

the synthesized stereo views from four samples frames are
presented (see 2nd row and 3rd row of each figure). The
last row of each figure shows the frame indices and the 1st
row shows the original frames. Unlike the synthetic data,
all of the real datasets are very challenging. For instance,
the white glare points change as the camera moves; the surface tissue is non-rigid; some fluids may flow on the lens
and etc. All these challenges make the point correspondences calculation difficult, or simply fail, especially for a
long sequence. In implementation, we cut a long sequence
into small segments and we process each segment individually. For CREEL and GRAY datasets, a segment contains
20 frames. Another practical challenge is that most of the
feature points are nearly coplanar, which means a near degenerated situation for 3D reconstruction. As described in
Sec. 3.2, we exploit the translation movement of the endoscope and thus simplify the fundamental matrix model significantly, which makes the reconstruction algorithm more
robust for near degenerated cases. Such simplification also
directly leads to affine reconstruction from uncalibrated images. Other robust techniques we used include RANSAC
and bundle adjustment. As a result, despite the above challenges of real endoscopic data, our method can still successfully recover the camera motions and many 3D points. The
dense disparity maps correctly reveal the general relative
depth, although there are also some inconsistencies. We
visually inspected the synthesized videos using red-cyan
glasses and strong 3D experiences were experienced.

5. CONCLUSION AND DISCUSSION
This paper presents an approach to synthesize stereoscopic views from monocular endoscopic videos. A general framework as well as the detailed implementation were
introduced. The framework consists of two major steps:
structure from motion and disparity interpolation. We proposed the concept of normalized disparity, which can be
computed from the SfM results and used for stereoscopic

61

(0)

(7)

(14)

(19)

Figure 9. Results of real endoscopic data (GRAY).

view synthesis. We proved that affine reconstruction is
enough for stereoscopic view synthesis, although a good 3D
model usually requires Euclidean reconstruction. To obtain
an affine reconstruction from uncalibrated videos, we exploit the fact that the endoscopy camera has nearly translational motion for much of the acquisition time. By assuming two initial frames with no relative rotation, the fundamental matrix computation becomes much more robust
due to significant reduction of the degrees of freedom. As
a result, the 3D reconstruction is also much more robust
than the approach which assumes general motion. We also
proved that linear interpolation in the normalized disparity
field is equivalent to linear interpolation in the 3D space.
This result justifies our approach of linear disparity interpolation. Experiments demonstrate the effectiveness of the
proposed approach.
Currently, the dense disparity map still contains inconsistencies, due to the errors from the SfM process. We
plan to improve the robustness of dense disparity map calculation by exploring more sophisticated interpolation and
filtering algorithms. Another possible direction is to estimate the disparities for more feature points (in addition to
the initial tracked feature points), which will make the final disparity map more accurate. Among others, another
future task is to tackle the issue of real-time implementation of a complete system based on the proposed algorithms.

References
[1] J. Bittner et al. Three-dimensional visualisation and articulating instrumentation: Impact on simulated laparoscopic
tasks. Journal of Minimal Access Surgery, vol, 4:33–38,
2008. 1
[2] D. Burschka, M. Li, M. Ishii, R. Taylor, and G. Hager. Scaleinvariant registration of monocular endoscopic images to CTscans for sinus surgery. Medical Image Analysis, 9(5):413–
426, 2005. 2

[3] J. Gibbs. Surgeons and the Scope. JAMA, 291(12):1507,
2004. 1
[4] P. Harman, J. Flack, S. Fox, and M. Dowley. Rapid 2D to
3D conversion. In Proc. SPIE, volume 4660, pages 78–86.
Citeseer, 2002. 2
[5] R. Hartley and A. Zisserman. Multiple view geometry in
computer vision. Cambridge Univ Pr, 2003. 4
[6] M. Hayashibe, N. Suzuki, and Y. Nakamura. Laser-scan endoscope system for intraoperative geometry acquisition and
surgical robot safety management. Medical Image Analysis,
10(4):509–519, 2006. 2
[7] I. Jourdan, E. Dutson, A. Garcia, T. Vleugels, J. Leroy,
D. Mutter, and J. Marescaux. Stereoscopic vision provides
a significant advantage for precision robotic laparoscopy.
British Journal of Surgery, 91(7):879–885, 2004. 1
[8] S. Knorr, M. Kunter, and T. Sikora. Stereoscopic 3D from 2D
video with super-resolution capability. Signal Processing:
Image Communication, 23(9):665–676, 2008. 2
[9] Q. Liu, R. Sclabassi, N. Yao, and M. Sun. 3D construction
of endoscopic images based on computational stereo. In Bioengineering Conference, pages 69–70, 2006. 2
[10] J. Luursema, W. Verwey, P. Kommers, and J. Annema. The
role of stereopsis in virtual anatomical learning. Interacting
with Computers, 20(4-5):455–460, 2008. 1
[11] U. Mueller-Richter, A. Limberger, P. Weber, K. Ruprecht,
W. Spitzer, and M. Schilling. Possibilities and limitations of
current stereo-endoscopy. Surgical endoscopy, 18(6):942–
947, 2004. 1
[12] A. Noma and M. Misulia. Programming topographic maps
for automatic terrain model construction. Surveying and
Mapping, 19:355, 1959. 5
[13] C. Rasmussen and C. Williams. Gaussian processes for machine learning. Springer, 2006. 5
[14] A. Tankus, N. Sochen, and Y. Yeshurun. Perspective shapefrom-shading by fast marching. In Proc. CVPR, volume 1,
2004. 2
[15] T. Thormahlen, H. Broszio, and P. Meier. Three-dimensional
endoscopy. Medical Imaging in Gastroenterology and Hepatology, page 199, 2002. 2
[16] K. Votanopoulos, F. Brunicardi, J. Thornby, and C. Bellows.
Impact of Three-Dimensional Vision in Laparoscopic Training. World Journal of Surgery, 32(1):110–118, 2008. 1
[17] C. Wu, Y. Sun, and C. Chang. Three-Dimensional Modeling From Endoscopic Video Using Geometric Constraints
Via Feature Positioning. IEEE Transactions on Biomedical
Engineering, 54(7):1199–1211, 2007. 2
[18] G. Zhang, W. Hua, X. Qin, T. Wong, and H. Bao. Stereoscopic Video Synthesis from a Monocular Video’. In Visualization and Computer Graphics, IEEE Transactions on,
13(4):686–696, 2007. 2
[19] J. Zhou, A. Das, F. Li, and B. Li. Circular generalized
cylinder fitting for 3D reconstruction in endoscopic imaging
based on MRF. In Proc. CVPR MMBIA Workshop, pages
1–8, 2008. 2
[20] J. Zhou and B. Li. Rectification with intersecting optical
axes for stereoscopic visualization. In Proc. ICPR, volume 2,
pages 17–20, 2006. 2

62

arXiv:1704.01235v1 [cs.CV] 5 Apr 2017

Joint Regression and Ranking for Image Enhancement
Parag Shridhar Chandakkar
Arizona State University

Baoxin Li
Arizona State University

pchandak@asu.edu

baoxin.li@asu.edu

Abstract
Research on automated image enhancement has gained
momentum in recent years, partially due to the need for
easy-to-use tools for enhancing pictures captured by ubiquitous cameras on mobile devices. Many of the existing leading methods employ machine-learning-based techniques, by
which some enhancement parameters for a given image are
found by relating the image to the training images with
known enhancement parameters. While knowing the structure of the parameter space can facilitate search for the optimal solution, none of the existing methods has explicitly
modeled and learned that structure. This paper presents an
end-to-end, novel joint regression and ranking approach to
model the interaction between desired enhancement parameters and images to be processed, employing a Gaussian
process (GP). GP allows searching for ideal parameters using only the image features. The model naturally leads to a
ranking technique for comparing images in the induced feature space. Comparative evaluation using the ground-truth
based on the MIT-Adobe FiveK dataset plus subjective tests
on an additional data-set were used to demonstrate the effectiveness of the proposed approach.

1. Introduction
The corpus of images on the Web is exponentially increasing in size with close to two billion photos being added
or circulated each day1 . Image sharing has become an integral part of daily life for many people, and they want
their photos to look good without doing too much manual editing. Some tools for easy image enhancement have
already been deployed on popular social networking platforms, such as Instagram, or mobile devices such as iPhone.
However, most such tools are essentially based on some
pre-defined image filters for obtaining certain visual effects.
Recent research efforts on automated image enhancement
employing machine learning techniques for improved functionalities such as content adaptivity and personalization.
1 http://www.kpcb.com/internet-trends

Such solutions range from learning a tone mapping between the spaces of low-quality and high-quality images2
to building a ranking relation between these two spaces
[2, 5, 13, 9, 12, 14, 6], although we are yet to see such techniques being deployed on a popular platform.
Many recent approaches follow the pipeline shown in
Fig. 1. During training, these approaches learn a model
to assign a score for a given image quantifying its visual
appeal. To enhance a new image, a nearest training image
is found. Then a dense sampling around the parameters3 of
the high-quality counterpart of that training image gives the
candidate set of enhancement parameters for the new image. A set of candidate images is then generated by applying these enhancement parameters to the new image. The
next step is extracting features of all candidate images and
use the learned model to select the highest-quality image.
There are two major drawbacks in the above processing flow. First, it is computationally expensive at the testing phase since a search through the entire training data is
needed. The training data for such applications could be
of huge size and is usually hosted on a server. Thus hundreds of thousands people querying the server per second
is undesirable and uncalled-for. Second, the set of candidate parameters which would enhance the original image is
found in a sub-optimal manner by doing a kNN search. It
does not provide any structured way to search for the optimal parameters and thus it becomes necessary to search
the entire training set and create a lot of candidate images,
resulting a computational bottleneck for the testing phase.
In this paper, we develop a joint regression and ranking
approach to address the above drawbacks. Our approach
employs GP regression to predict the mean and variance of
the candidate parameters only from the feature vector of a
low-quality image. We also simultaneously train a ranking
model on the GP-covariance-kernel-induced feature space.
To achieve this, we derive and use the dual-form of ranking SVM [10] with the GP kernel integrated into it. Thus
2 We call the images before enhancement as low-quality and those after
the enhancement as high-quality in the rest of this article. We also refer to
the process of enhancing a new picture as “the testing stage”.
3 The brightness, saturation and contrast are referred to as “parameters”
of an image in this article

c

2017
IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers
or lists, or reuse of any copyrighted component of this work in other works.

Figure 1. Pipelines of image enhancement approaches.

the kernel builds a relation between the image feature space
and its corresponding enhancement parameter space. Along
with that, the kernel learns to give more weight to the image
features which are highly responsible for making an image
to be of higher quality. Finally, we learn the GP kernel in
such a way that all the high-quality counterparts of a lowquality image form a cluster. This allows exploration of the
image parameter space in a structured manner for obtaining
the optimal solution.
In the testing stage (i.e., while enhancing a new image),
the model provides the expected value and variances of
the enhancement parameters, drastically reducing required
computation since there is no need to sift through the training set. We can generate/show some enhanced images by
applying parameters that are k standard deviations away
from the expected values of the parameters, where k is a
user-defined and can be changed on-the-fly, so that user can
choose from some good candidate images. The same kernel
can be used to rank images if the user wants to see a single
image. Through extensive experiments, we show that our
approach is computationally efficient at the testing phase,
and that it predicts parameters correctly for new images and
that it also predicts the ranking relations between the new
images and its enhanced counterparts.

2. Related Work
Automated image enhancement has recently been an active research area. Various solutions have been proposed
for this task. We review those works which aim to improve
the visual appeal of an image using automated techniques.
A novel tone-operator was proposed to solve the tone reproduction problem [18]. A database named MIT-Adobe
FiveK of corresponding low and high-quality images was
published in [5]. They also proposed algorithm to solve the

problem of global tonal adjustment. The tone adjustment
problem only manipulates the luminance channel. In [11],
an approach was presented, focusing on correcting images
containing faces. They built a system to align faces between
a “good” and a “bad” photo and then use the good faces to
correct the bad ones.
Content-aware enhancement approaches have been developed which aim to improve a specific image region.
Some examples of such approaches are [2, 14]. A drawback
of these is the reliance on obtaining segmented regions that
are to be enhanced, which itself may prove difficult. Pixellevel enhancement was performed by using local scene descriptors. First, images similar to the input are retrieved
from the training set. Then for each pixel in the input, a
set of pixels was retrieved from the training set and they
were used to improve the input pixel. Finally, Gaussian
random fields are used to maintain the spatial smoothness
in the enhanced image. This approach does not take the
global information of an image into account and hence the
local adjustments may not look right when viewed globally.
A deep-learning based approach was presented in [26]. In
[12], users were required to enhance a small amount of images to augment the current training data.
Two closely related and recent works involve training
a ranking model from low and high-quality image pairs
[25, 6]. In a recent state-of-art method [25], a dataset of
1300 corresponding image pairs was reported, where even
the intermediate enhancement steps are recorded. A ranking
model trained with this information can quantify the (enhancement) quality of an image. In [6], non-corresponding
low and high-quality image pairs were used to train a ranking model. Both the approaches use kNN search at the test
time to create a pool of candidate images first. After extracting features and ranking all of them, the best image is

presented to the user.
Now we briefly review Gaussian process based methods which are relevant in this context. GP has been effectively used to obtain good performance for applications
where complex relationships have to be learned using a
small amount of data (in the order of several hundreds) [23].
In [7], it was used for view-invariant facial recognition. A
GP latent variable model was used to learn a discriminative feature space using LDA prior where examples from
similar classes are project nearby. In [20], GP regression
was used to map the non-frontal facial points to the frontal
view. Then facial expression methods can be used using
these projected frontal view points. Coupled GP have been
used to capture dependencies between the mappings learned
between non-frontal and frontal poses, which improves the
facial expression recognition performance [19].
Our effort in this paper deals with enhancement considering contrast, saturation and brightness of an image. We
attempt to explicitly model interactions between parameters controlling these factors and features extracted from the
underlying image, employing GP. Our approach of joint regression and ranking allows us to learn the complex mapping from the image features to the regions corresponding
to desired enhancement in the parameters space, without actually generating several hundreds of enhanced candidate
images. The expected value of the parameters and their
standard deviations provide us with a way to systematically
explore the parameters space. In the next section, we detail our proposed approach. To the best of our knowledge,
this is the first attempt on incorporating GP regression and
ranking for image enhancement.

3. Proposed Approach
Our problem is to predict the set of image parameters
which would enhance a given image. Our proposed approach consists of two objectives: 1. Given a low-quality
image feature, probabilistically estimate the parameters that
could generate the enhanced counterpart. 2. Produce a
ranking in the GP-kernel-induced feature space and thereby
discover the features responsible for making an image of
higher-quality.
We have pairs of low and high-quality images along
with their parameters for training. The feature representation of an image will be discussed in detail in section
3.6. Features of N low-quality images are represented by
F = {f1 , f2 , . . . , fN }4 . We have p high-quality versions
for each low-quality image. Its features are represented by
+
+
F + = {F1+ , . . . , FN+ }, where Fi+ = {fi1
, . . . , fip
}, and
+
D×1
fi , fij ∈ R
∀ i, j. We also have p sets of high-quality
parameters for a low-quality image. However, for illus4 We represent vectors by lower-case bold letters. Matrices are represented by upper-case bold letters. Scalars are denoted by non-bold letters.

tration, we predict parameters only for the first set. Note
that we still use all the p sets of high-quality images to
train a ranking model. The parameters of low and highquality images are represented by Y = {y1 , . . . , yN } and
+
Y + = {y1+ , . . . , yN
} respectively. We use three image
parameters, namely, brightness, contrast and saturation and
hence yi , yi+ ∈ R3×1 ∀ i. Our task is to obtain yi+ using
only fi and yi . We predict each parameter using a separate
GP. To that end, we collect the mth parameter of all low
and high-quality images into, ȳm = (y1m , . . . , yN m )T and
+
+
+
T
ȳm
= (y1m
, . . . , yN
m ) , respectively and train a separate
GP model to predict each parameter. We now outline the
proposed joint GP regression and ranking.

3.1. GP Regression
GPs define a prior distribution over functions which becomes a posterior over functions after observing the data.
GPs assume that this distribution over functions is jointly
Gaussian with a mean and a positive definite covariance kernel function. GPs provide well-calibrated, probabilistic outputs which are particularly useful and necessary in our application [15]. If we let the prior on regression function be
a GP, then it can be denoted as GP (m(f ), κ(f , f 0 )) where
f and f 0 are image features ∈ RD×1 as defined previously,
m(f ) is a mean function and κ(f , f 0 ) is a covariance function. It is well-known that GPs are flexible enough to model
an arbitrary mean. It can be shown that the posterior predictive density for a single test input is:
+
+
+
p(ȳ∗m
|f∗ , F , Y ) = N (ȳ∗m
|k∗T Ky−1 ȳm
, k∗∗ −k∗T Ky−1 k∗ )
(1)
where k∗ = [κ(f∗ , f1 ), . . . , κ(f∗ , fN )], N is the number
of samples, k∗∗ = κ(f∗ , f∗ ) and Ky = K + σy2 IN . K
is a kernel function between all training inputs f , and (·)∗
denotes a new data sample. The noise or uncertainty in the
output is modeled by the noise variance, σy2 .
It can be further shown that the log-likelihood function
for a GP regression model is easily obtained by using a standard multivariate Gaussian distribution as follows,

+
+ T
+
log p(ȳm
|F ) = −0.5 ȳm
Ky−1 ȳm
− 0.5log|Ky |−

0.5N log(2π)
(2)
We choose a standard squared exponential kernel for our
application. It is as follows,
1
κ(fi , fj ) = σf2 exp(− (fi − fj )T · Λ · (fi − fj )) + σy2 δij
2
(3)
Here, σf2 controls the vertical scale of the regression function, σy2 models uncertainty, Λ is a diagonal matrix with
entries {θ1 , . . . , θD } and δpq is a Kronecker delta function,
which takes the value 1 if p = q, it is zero everywhere else.
We call h = {σf2 , Λ, σy2 } as hyper-parameters. It is easy

to see that the prediction in Equation 1 is dependent on the
kernel and in turn on the hyper-parameters: σf , Λ and σy2 .
We will see later how to obtain optimal hyper-parameters.
Now, we explain inclusion of ranking into our formulation.

3.2. GP Ranking
We build a ranking relation in the GP-kernel-induced
feature space. Thus the GP kernel discovers the features responsible for making an image of higher-quality and assigns
higher weight to them by adjusting the hyper-parameters.
The primal form of rank SVM [10] is given by:

form as follows:
min
w,ξi

X
1 T
w w+C
ξi ,
2
i

subject to: wT Di + 1 − ξi ≤ 0, ξi ≥ 0, ∀i = {1, . . . , N 0 }.
(6)
We use Lagrangian multipliers to convert the above equation into an unconstrained optimization problem.
X
1 T
w w+C
ξi +
2
i
X
X
αi (wT Di + 1 − ξi ) −
βi ξ i

L(w, α, β) =

i

X
1
ξij , subject to: ui  uj ∀ (i, j)
min wT w + C
w,ξij 2
i,j
(4)
where ui  uj indicates that ui is ranked higher than uj .
For ranking, we observe that only building a relation between low and high-quality images does not provide good
ranking accuracy on new images. The reason is that the
enhanced images often possess high saturation, brightness
and/or contrast. Thus the ranking model sometimes assigns
a higher score to over-saturated and over-exposed images.
This would not be a problem if one had intermediate information about the enhancement steps being performed
[25]. Thus we deteriorate our original low-quality images
by shifting the image parameters to both extremes. The
amount of shifting for an image is decided by first deteriorating 20 images manually and then heuristically defining a relation between existing image parameters and the
amount of shifting needed to significantly deteriorate the
image. Let’s call these images as poor-quality images. We
also generate p poor-quality images for every low-quality
image. We now have features for poor, low and high quality images, denoted by F − , F and F + respectively. Primal
form for our ranking model can be written as follows,
X
X
1
0
min wT w + C1
ξij + C2
ξik
,
w,ξij 2
i,j
i,k

subject to: w

T

+
fij

T

≥ w fi + 1 − ξij ,

−
0
subject to: w fi ≥ wT fik
+ 1 − ξik
,
T

+
−
00
0
00
subject to: wT fij
≥ wT fik
+ 1 − ξik
, ξij , ξik
, ξik
≥0

∀ i = {1, . . . , N }, ∀j = {1, · · · , p}, ∀k = {1, · · · , p}.
(5)
Now we derive the dual form to incorporate the GP-kernel,
κ. It would be cumbersome to derive the dual of Equation
5 as it stands. Instead, we can define a new set of data D
+
−
−
+
consisting of fi − fij
, fik
− fi and fik
− fij
∀ i, j, k. Now,
0
2
D has N = N (2p + p ) elements, we can write the primal

(7)

i

Differentiating with respect to w and ξ and equating them
to zero, we get,
X
∇w L(w, α, β) = 0 ⇒ w = −
αi Di
(8)
i
∇ξ L(w, α, β) = C − αi − βi = 0 ⇒ αi ≤ C.
Substituting w back into Equation 6 and doing some algebraic manipulation, we get a dual maximization problem as,
max
α

X
i

αi −

1 XX
αi αj DiT Dj , subj. to: 0 ≤ αi ≤ C.
2 i j

(9)
The inner product in the above equation can be replaced
with GP kernel by employing the kernel trick. Thus the
final optimization problem to get α becomes,
1
max 1T α − αT Ky α.
α
2

(10)

Here, 1 is a column vector of ones. The length of both α
and 1 is N (2p + p2 ). The dimensions of Ky are N (2p +
p2 )×N (2p+p2 ). The (i, j)th element of Ky is κ(Di , Dj ).

3.3. Clustering high-quality images together
We turn our attention to the third constraint. Given a
low-quality image: 1. it forces all its high-quality counterparts to form a cluster and 2. it tries to maximize the distance between poor-quality and high-quality images in the
GP-kernel-induced feature space. The intuition behind this
is as follows. Ultimately, given a new image, we would not
only like to predict the parameters for its enhanced counterpart, but we also wish to traverse the parameter space
and explore more of such enhancement parameters. The
traversing of the parameter space is, in our opinion, essential since the choices of people vary by a great amount and
no model would do justice with just one set of predicted parameters. Note that this constraint tries to minimize distance
+
between fi and fij
∀j, so by definition of GP, the distance
+
between the corresponding output parameters, yij
∀j, will

be reduced, which in turn achieves the aforementioned effective traversal. The second part of the constraint tries to
push the predicted parameters away from the parameters of
the poor-quality images. The details of traversing the parameter space after getting the GP predictions are discussed
later. The constraint can be formulated as follow.


∂Z
1
1
1
+
+ T
= − Ky−1 ym
ym
Ky−1 + Ky−1 + ααT +
∂Ky
2
2
2
X  F+
+
−
F
,F
2
Ky i − Ky i i ,
i


X

min
h

F+
||Ky i ||2F

−

F + ,F −
||Ky i i ||2F

!
,

(11)

F + ,Fi−

is a p × p matrix defined as follows,

F + ,Fi−

Ky i


ij



1 2
1
T
= − σf exp − (fi − fj ) Λ(fi − fj ) ·
2
2
(q)

(fi

i

where || · ||2F indicates squared Frobenius norm. The term
Ky i

∂Ky
∂θq


+
−
κ(fi1
, fi1
) ···
+
−
κ(fi2
,
f
i1 ) · · ·

=
..
..

.
.
+
−
κ(fip , fi1 ) · · ·

+
− 
κ(fi1
, fip
)
+
− 
κ(fi2
, fip
)

..

.
+
−
κ(fip , fip )

(12)

(q)

− fj )2 ,

1
∂Ky
= σf2 exp(− (fi − fj )T · Λ · (fi − fj )),
∂σf2
2


∂Ky
= δij ,
∂σy2 ij
"
T 
#
∂Z
∂Z
∂Ky
= tr
∀q ∈ {1, . . . , D},
∂θq
∂Ky
∂θq
(14)
∂Z
∂Z
and
where tr denotes matrix trace. Similarly, ∂σ
2
∂σ 2 are
f

F+

F + ,F +

The term Ky i is equal to Ky i i .
We combine Equations 2, 10 and 11 to form our objective function. It is as follows,

1 + T −1 + 1
ȳ
Ky ȳm + log |Ky | − 1T α+
h
2 m
2

X
1 T
F + ,F −
F+
α Ky α +
||Ky i ||2F − ||Ky i i ||2F
2
i
(13)
Note that we have removed the constant term. We now focus on how to solve Equation 10 and 13 to get α and h.

y

D+2
computed to construct ∂Z
. This derivative can
∂h ∈ R
now be used to obtain the optimal set of hyper-parameters,
h. In practice, all the matrix inverses are implemented using Cholesky decomposition. We alternately optimize for
α and h till Equation 13 converges or the maximum cycles
are reached. We set the convergence criterion to be 10−3
and the maximum cycles to 20.

min Z =

3.4. Optimization
Our optimization problem is separable in α and h. First
we optimize α, which can be done by using a standard rankSVM solver. It could also be solved by using quadratic programming, however, that would be memory inefficient. In
particular, we use a rank-SVM implementation which uses
the LASVM algorithm proposed in [4]. LASVM employs
active example selection to significantly reduce the accuracy after just one pass over the training examples.
After optimizing α, we turn our attention to Equation 13.
We find its local minimizer, h∗ , by using scaled conjugate
gradient descent (SCG) algorithm. SCG is chosen due to
its ability to handle tens of thousands of variables. SCG has
also been widely used in previous approaches involving GPs
[17, 7, 19]. We use chain rule to compute ∂Z
∂h by evaluating
∂Ky
∂Z
first ∂Ky and then ∂h . The matrix calculus identities from
[16] are used while computing the following expressions.

3.5. Testing
Once we get the optimal α and h, we can predict the
+
+
+
parameters, {ȳ∗1
, ȳ∗2
, ȳ∗3
}, for the enhanced counterpart by
using three trained GP models in Equation 1. Let us call
the mean and variances of the predicted parameters as m =
{m1 , m2 , m3 } and s = {s1 , s2 , s3 } respectively. With their
availability, we now explain our parameter space traversal.
As mentioned before, people’s choices vary a lot in such
applications. Thus, it is essential to explore the parameter
space to generate additional enhancement parameters. The
first advantage of our approach is that we can generate such
parameters without referring to the training set. Since we
explore the parameter space in a structured manner (with a
certain mean and variance), we can afford to generate only
32 parameters per image instead of hundreds as done in conventional kNN-based heuristic methods.
The First step in parameter space traversal is to determine lower and upper bounds. Those can be decided heuristically. For example, we decrease the saturation, brightness
and contrast at most by an amount of {15%, 15%, 5%} and
increase it at most by {35%, 35%, 20%} of the original image parameter values. We observed that these limits are not
absolutely critical to the quality since the generated images
will be ranked later using the learned α and the images with
extreme parameter settings will usually be filtered out.

Now, we change (increase and decrease) the mean value
of the parameters by µs till it reaches the pre-specified
thresholds. Intuitively, we think that s gives us the direction of our stride in the parameter space and µ gives us the
length of that stride. The value of µ is determined by the
number of enhanced counterparts the user wants to generate
for each low-quality image. We set that value to be 30. This
value could be decreased if the user is on a mobile device
with a smaller screen and similarly increased when operating on a desktop. These settings can be changed on-the-fly.

3.6. Image feature representation
We extract 432-D color histogram with 12 bins for hue,
6 bins each for saturation and value, which acts as a global
feature. We then divide the image into a 12 × 12 grid. For
each grid, we calculate its saturation, value by taking the
mean values of those image blocks in the HSV color space.
We also calculate RMS contrast on that grid. These act as
localized features of 144-D each. We finally append the
image parameters, which are average saturation, value and
RMS contrast. Appending the image parameters allows GP
to express the parameters of the enhanced counterparts as a
function of both, the low-quality parameters and its feature
vector. We finally get a 867-D (= 432 + 3 × 144 + 3)
representation for every image.

3.7. Implementation Details and Efficiency
GPs are known to be computationally-intensive. They
take about O(N 3 ) time for training, where N are the number of training examples. The matrix inversion of an N × N
matrix and the computation of the derivative of the kernel are the bottlenecks in the GP training procedure. We
train a GP model using about 1200 low-quality images and
six counterparts per image in about 18 hours on an Intel
Xeon @2.4 GHz × 16. The computational efficiency can
be improved by using GP regression techniques proposed
for large data [8, 1] or using efficient data-structures such
as kd-trees [22]. During testing, our approach is extremely
fast. We tested it on two systems, Intel Xeon and a modern
desktop system with Intel i7 @3.7GHz. It can predict all
the three parameters for 3150 and 1287 images per second
using Intel Xeon and i7 systems respectively. A built-in
kNN-search function processes only 224 images per second when asked to find one nearest neighbor in 5000 image
data-set on the Intel Xeon system. All the implementations
are done in MATLAB. Since our approach need not query
the training database, it could be portable and potentially allow for enhancements being performed on mobile devices.

4. Data-sets and Experimental Setup
In this section, we present describe the data and experimental setup. Results of these experiments are presented
in the Section 5. We perform four kinds of experiments.

The first experiment provides a weak quantitative measure
of the accuracy of our approach. We use the MIT-Adobe
FiveK [5] data-set for this experiment. This data-set has
5000 low-quality images with 5 expert-enhanced counterparts for each image. This is the largest such data-set available. We use 1200 images and 6 counterparts (three each
for poor and high-quality) per low-quality image to train our
GP models. We use 1500 and 800 images for validation and
testing respectively. We predict the parameters (i.e. brightness, contrast and saturation) for the first enhanced counterpart of all the images in the test set. Then we calculate
the root mean square error (RMSE) and a more stringent
criterion - Pearson’s correlation - between the ground-truth
parameters computed from expert-enhanced image and our
predicted parameters. We compare our quantitative results
against twin Gaussian processes (TGP) [3]. TGP is a strutured prediction method which considers correlation between both input and output to produce predictions. Though
a low RMSE between ground truth and predicted parameters does not guarantee that the enhancement will be visually appealing (unless the RMSE tends to zero), it gives us
a confirmation that the prediction is lying near the groundtruth in the parameter space. Also, this experiment validates
the effectiveness of the GP regressor.
Second experiment is a qualitative measure of the image
quality produced by the proposed and the competing algorithms - kNN, Picasa and [25]. The metric of L2 error in the
L*ab space was adopted in [25]. We believe that it is a poor
indicator of the enhancement quality and instead opt for Visual Information Fidelity (VIF) metric [21]. This metric has
the ability to predict whether the visual quality of the other
image has been enhanced in comparison with the reference
image by producing a value greater than one. This is unlike
other quality metrics such as SSIM [24], FSIM [28], VSI
[27] etc. We use the publicly available implementation of
VIF5 . We calculate the VIF between the proposed enhancement (reference image) and the enhancement by 1. kNN
2. Picasa and 3. the approach of [25] (other image). Thus
VIF < 1 implies that the proposed enhancement is better
than the one produced by the competing algorithm and viceversa. This comparison is done for 60 pairs where 15 images each are enhanced using Picasa and [25], whereas the
remaining 30 images are enhanced using kNN approach.
Third experiment is aimed towards evaluating the effectiveness of GP ranking. For each image we generate only
32 enhanced versions. Our GP ranker selects the highest
ranked image out of those 32 and presents it to the user.
The highest ranked image is supposed to have the best quality. We compute the VIF metric between the best image
selected by the ranker (reference image) and the other 31
images (other images). Ideally, for all these 31 images, we
should get values less than one indicating that GP ranker
5 available

at live.ece.utexas.edu/research/quality/

5. Results
We present results of our quantitative analysis first.
We train three GP models to predict saturation, brightness and contrast for 800 images from the test set of
MIT-Adobe data-set.
When compared with the parameters of expert-enhanced counterparts, we achieve
RMSEs of 0.0057, 0.0022, 0.0037 and correlations of
0.5359, 0.5553, 0.8023 respectively, for the above three parameters. TGP gets an average RMSE of 0.0022 but it suf-

600

Votes obtained

has indeed selected the best image.
We also carry out a subjective evaluation test to assess if
people prefer the enhanced counterparts generated by our
approach. We compare our approach against three other
methods. First one is the kNN-based approach. Given a
low-quality image, we search for the nearest non-duplicate
image from the 5000 images of MIT-Adobe dataset. The
parameters of the expert-enhanced counterparts of the nearest image are applied to the given low-quality image. In
this manner, we generate 5 enhanced counterparts per lowquality image. Note that, kNN utilizes all other 4999 images whereas we only use the model trained on 1200 images for prediction. Then we compare against Picasa’s
one-touch-enhance tool. The third approach is from [25],
which also is a learning-to-rank based image enhancement
approach that uses the pipeline shown at the top in Fig. 1.
We use 60 images for the subjective test which was performed by 15 people. Thirty images are selected from our
testing set of the MIT-Adobe data-set. The rest of the images are from the data-set used in the paper [25]. Since we
only have access to their testing set, we use that data-set
solely for subjective test purposes. It contains 124 images
out of which we randomly select 30 images. We enhance all
the 60 images using our approach. The comparison against
other methods is done as follows.
The first 30 images from the MIT-Adobe data is split into
two halves. The first half is enhanced using the kNN approach and the second half is enhanced using Picasa. The
remaining 30 images from [25] are split into two halves.
The first half is enhanced using the kNN approach and for
the second half, we directly use the high-resolution results
of the test data-set of [25]. Thus each person compares 60
image pairs. One of the image in that pair has been enhanced using our approach and the other image has been enhanced using either kNN approach, Picasa or the approach
of [25]. The subject has to choose the image which he/she
finds “visually-appealing”. If the subject feels that both images have almost the same visual appeal, a third option of
preferring neither image is provided. The order in which
the images appear in front of a subject is always randomized. The pairing order is also randomized. The subjects do
the evaluation test in standard lighting conditions and at a
comfortable and constant distance from the screen.

36.3,4.9

500

Proposed Approach
Competing Approach

400
300
200
100
0
Ours vs. all
approaches

Ours vs. kNN, Ours vs. kNN, Ours vs. [11], Ours vs. Picasa,
MIT-Adobe
data of [11]
data of [11]
MIT-Adobe

Figure 2. Subjective evaluation test metrics.

fers while producing an average correlation of only 0.3326.
We can see that it is relatively easier for a GP to relate
the contrast to the image quality, which is intuitive since
contrast variation changes the image drastically and it also
makes the image look vibrant or dull. This in turn contributes most to the visual appeal of an image.
The left bar chart in Fig. 3 shows the results of second experiment. VIF between our enhancement and competing enhancements produces values which are, in most
cases, less than one. Thus according to VIF metric, our
approach produces better enhancements than Picasa, kNNbased heuristics and [25]. For third experiment, we get
32 VIF values for each image, which correspond to 32 enhanced versions generated by our approach. The GP ranker
selects one, as mentioned earlier. We compute the average
VIF value and its standard deviation over 31 other images.
This process is repeated for all the 60 images and the VIF
values are shown in the right bar chart of Fig. 3.
We now analyze the results of our subjective tests. We
provide the following five metrics about our subjective test
in Fig. 2. 1. we count votes gathered by our approach and
by all other competing approaches bundled into one. This
is a coarse measure of how much preference people have
towards enhancements generated by our approach. 2. We
count votes gathered by our approach and by kNN approach
on the MIT-Adobe data-set. 3. comparison of votes gathered by our approach and by the kNN approach on the dataset of [25]. 4. comparing our approach against the results
of [25] on their data. 5. Lastly, we compare our approach
versus Picasa on the MIT-Adobe data. Fig. 2 shows all
these metrics. On top of each bar, we indicate the mean and
standard deviation for that particular approach and metric.
For example, the second set of bars denote that for the MITAdobe data, our approach gathered 133 votes against 104
votes gathered by the kNN approach. The average number
of votes obtained per user for our and the kNN approach
were 8.9 and 6.9 with the standard deviations of 2 and 1.6,
respectively. Fig. 2 shows that people consistently prefer
our approach over other state-of-art approaches.
Fig. 4 shows some of the results obtained by ours and the
approach of [25], kNN and Picasa’s auto-enhance tool. The
first and the third row illustrate that the kNN approach is

1.2

1

1

VIF Value

VIF value

1.2

0.8
0.6
0.4
0.2
0

0.8
0.6
0.4

0.2
0
1

Enhancement Type

6

11

16

21

26

31

36

41

46

51

56

Image Index

Figure 3. Left plot shows VIF values comparing proposed enhancement and enhancements produced by competing algorithms. The right
plot shows the mean and standard deviation of the VIF values between the best enhancements and 31 other enhancements “rejected” by
GP ranker. VIF values < 1 are desirable in both the cases.

Figure 4. The left column always contains an original low-quality image. Row 1 and 3: Columns 2-4 contain images enhanced by kNN,
Picasa and GP respectively. Row 2: The right three columns contain enhanced versions generated by GP. Please read text for details.

not always effective and sometimes may give over(under)exposed results due to its dependence on the nearest training
image parameters. The second row shows three representative versions generated by GP. We can see that the image in
the fourth column is over-exposed. However, our ranking
model successfully filters out that image and selects the one
in the third column. In general, we observed that kNN can
only get comparable results to Picasa and our approach if it
finds a good match in the training set. Thus kNN is unlikely
to scale to large-scale enhancement tasks.

6. Conclusion
We presented a novel approach to image enhancement
using joint regression and ranking by employing GPs. We
train our GP models on the pairs formed from poor, low and
high-quality images. The learned GP models predict the desired parameters for a low-quality image from its features,
which may produce its enhanced counterparts. We also described a strategy to traverse the parameter space without
referring to the training images, which makes our approach

efficient during testing. The GP prediction is defined by
the covariance kernel, on which we impose two constraints.
The first one enables the kernel to learn the feature dimensions responsible for making an image of higher-quality.
The other constraint clusters all the enhancement parameters corresponding to a low-quality image, thereby allowing
for effective parameter traversal. We perform quantitative
and subjective evaluation experiments on two-data sets to
assess the effectiveness of our approach, first one being the
MIT-Adobe data [5] and the another one proposed in [25].
Quantitative experiments show that our predictions produce
a low RMSE when compared with the ground-truth parameters of the MIT-Adobe data. The results show that people
consistently prefer the enhancements produced by the proposed approach over the other state-of-art approaches.
Acknowledgement: The work was supported in
part by ONR grant N00014-15-1-2344 and ARO grant
W911NF1410371, and a gift from Nokia. Any opinions
expressed in this material are those of the authors and do
not necessarily reflect the views of the sponsors.

References
[1] S. Ambikasaran, D. Foreman-Mackey, L. Greengard, D. W.
Hogg, and M. O’Neil. Fast direct methods for gaussian processes and the analysis of nasa kepler mission data. arXiv
preprint arXiv:1403.6015, 2014.
[2] F. Berthouzoz, W. Li, M. Dontcheva, and M. Agrawala. A
framework for content-adaptive photo manipulation macros:
Application to face, landscape, and global manipulations.
ACM Trans. Graph., 30(5):120, 2011.
[3] L. Bo and C. Sminchisescu. Twin gaussian processes for
structured prediction. International Journal of Computer Vision, 87(1-2):28–52, 2010.
[4] A. Bordes, S. Ertekin, J. Weston, and L. Bottou. Fast kernel
classifiers with online and active learning. The Journal of
Machine Learning Research, 6:1579–1619, 2005.
[5] V. Bychkovsky, S. Paris, E. Chan, and F. Durand. Learning photographic global tonal adjustment with a database of
input/output image pairs. In Computer Vision and Pattern
Recognition (CVPR), 2011 IEEE Conference on, pages 97–
104. IEEE, 2011.
[6] P. S. Chandakkar, Q. Tian, and B. Li. Relative learning from
web images for content-adaptive enhancement. In Multimedia and Expo (ICME), 2015 IEEE International Conference
on, pages 1–6. IEEE, 2015.
[7] S. Eleftheriadis, O. Rudovic, and M. Pantic. Discriminative shared gaussian processes for multiview and viewinvariant facial expression recognition. Image Processing,
IEEE Transactions on, 24(1):189–204, 2015.
[8] J. Hensman, N. Fusi, and N. D. Lawrence. Gaussian processes for big data. arXiv preprint arXiv:1309.6835, 2013.
[9] S. J. Hwang, A. Kapoor, and S. B. Kang. Context-based
automatic local image enhancement. In Computer Vision–
ECCV 2012, pages 569–582. Springer, 2012.
[10] T. Joachims. Optimizing search engines using clickthrough
data. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,
pages 133–142. ACM, 2002.
[11] N. Joshi, W. Matusik, E. H. Adelson, and D. J. Kriegman.
Personal photo enhancement using example images. ACM
Trans. Graph, 29(2):12, 2010.
[12] S. B. Kang, A. Kapoor, and D. Lischinski. Personalization of
image enhancement. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 1799–1806.
IEEE, 2010.
[13] A. Kapoor, J. C. Caicedo, D. Lischinski, and S. B. Kang.
Collaborative personalization of image enhancement. International Journal of Computer Vision, 108(1-2):148–164,
2014.
[14] L. Kaufman, D. Lischinski, and M. Werman. Content-aware
automatic photo enhancement. In Computer Graphics Forum, volume 31, pages 2528–2540. Wiley Online Library,
2012.
[15] K. P. Murphy. Machine learning: a probabilistic perspective.
MIT press, 2012.
[16] K. B. Petersen, M. S. Pedersen, et al. The matrix cookbook.
Technical University of Denmark, 7:15, 2008.

[17] C. E. Rasmussen. Gaussian processes for machine learning.
2006.
[18] E. Reinhard, M. Stark, P. Shirley, and J. Ferwerda. Photographic tone reproduction for digital images. In ACM
Transactions on Graphics (TOG), volume 21, pages 267–
276. ACM, 2002.
[19] O. Rudovic, M. Pantic, and I. Patras. Coupled gaussian processes for pose-invariant facial expression recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions
on, 35(6):1357–1369, 2013.
[20] O. Rudovic, I. Patras, and M. Pantic. Regression-based
multi-view facial expression recognition. In Pattern Recognition (ICPR), 2010 20th International Conference on, pages
4121–4124. IEEE, 2010.
[21] H. R. Sheikh and A. C. Bovik. Image information and
visual quality. Image Processing, IEEE Transactions on,
15(2):430–444, 2006.
[22] Y. Shen, A. Ng, and M. Seeger. Fast gaussian process regression using kd-trees. In Proceedings of the 19th Annual Conference on Neural Information Processing Systems, number
EPFL-CONF-161316, 2006.
[23] R. Urtasun and T. Darrell. Discriminative gaussian process
latent variable model for classification. In Proceedings of the
24th international conference on Machine learning, pages
927–934. ACM, 2007.
[24] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.
Image quality assessment: from error visibility to structural similarity. Image Processing, IEEE Transactions on,
13(4):600–612, 2004.
[25] J. Yan, S. Lin, S. B. Kang, and X. Tang. A learning-to-rank
approach for image color enhancement. In Computer Vision
and Pattern Recognition (CVPR), 2014 IEEE Conference on,
pages 2987–2994. IEEE, 2014.
[26] Z. Yan, H. Zhang, B. Wang, S. Paris, and Y. Yu. Automatic
photo adjustment using deep neural networks. arXiv preprint
arXiv:1412.7725, 2014.
[27] L. Zhang, Y. Shen, and H. Li. Vsi: A visual saliency-induced
index for perceptual image quality assessment. Image Processing, IEEE Transactions on, 23(10):4270–4281, 2014.
[28] L. Zhang, L. Zhang, X. Mou, and D. Zhang. Fsim: a feature similarity index for image quality assessment. Image
Processing, IEEE Transactions on, 20(8):2378–2386, 2011.

EXPLOITING VERTICAL LINES IN VISION-BASED NAVIGATION FOR MOBILE ROBOT
PLATFORMS
Jin Zhou and Baoxin Li
Department of Computer Science & Engineering
Arizona State University, Tempe, AZ 85287-8809, USA
ABSTRACT
Vertical line-like structures are abundant in man-made
environments, such as the vertical edges of buildings in an
outdoor environment and door frames and vertical edges of
furniture in an indoors environment. These ubiquitous line
or line-like structures may provide useful information for
mobile robot navigation. In this paper, we propose an
approach to estimating the camera orientation based on
detected vertical lines. Further we use the estimated camera
orientation in one key problem of mobile robot navigation:
ground/obstacle detection. Our method is theoretically wellgrounded and yet simple and practical in implementation.
Index Terms— Robot vision systems, homography
transform
1. INTRODUCTION
For vision-based mobile robot navigation, two essential
tasks are obstacle detection and localization. In many cases,
obstacle detection is also directly related to the problem of
ground plane detection. Various vision-based approaches
have been proposed for the problem of obstacle/ground
plane detection (e.g., [1-6]), using various visual cues such
as motion, disparity, and color, etc. In man-made
environments, there exist a lot of vertical line-like
geometrical entities that arise from buildings, boxes,
bookshelves, cubicle walls, door frames, etc. These vertical
line-like entities (which will be simply called “vertical lines”
thereafter) should provide very useful cues in guiding robot
navigation. In fact, there have been some attempts on using
this type of information. However, in prior work such as [9,
10], the vertical lines were simply used as an ordinary
feature, and usually the image plane is assumed to be
vertical.
In this paper, we employ vertical lines for vision-based
robot navigation. Specifically, we design an algorithm that
uses vertical lines for estimating the camera orientation,
which can be used to eliminate the assumption of a vertical
image plane mentioned above. Furthermore, with the
estimated camera orientation, the ground-plane coordinates
in the image domain can be directly transformed to the real
world ground plane coordinates, which is useful for
localization and path planning in robotics.

1­4244­0728­1/07/$20.00 ©2007 IEEE

As a direct application of the proposed algorithm, we
apply it to the ground plane detection task, expanding our
prior work in [8], where a normalized-homography-based
approach was proposed for robust ground plane detection. In
this work, we show that the proposed algorithm enables us to
by-pass a manual calibration stage that is required in [8].
2. MODELING THE IMAGING SYSTEM
We assume that the mobile robot is navigating on a plane.
Without loss of generality, we use the model of Fig. 1 for the
imaging system. We define the world coordinates system
such that the y axis is perpendicular to the ground plane and
the origin is at the same height as the robot’s camera center.
The square box represents the robot. Initially, it is at
position C1, with a rotation matrix R1. After moving to a new
location C2, the rotation matrix becomes R2. (Ri is defined in
the camera’s coordinates system by convention).
y

z

o

R1 C1
x
R2 C2
π0
Fig 1. Illustration of the coordinate systems in relation
to the ground plane.
Based on the above world coordinates system, we have
Ci = ( xi , 0, zi )T and the ground plane has coordinates

π 0 = (n0T , d )T and thus for points on the plane we have
n0T X + d = 0 , where n0 = (0,1, 0)T .
Suppose that the internal matrix of the camera is K and
that it does not change while navigating, we have the camera
matrices as:
P1 = KR1[ I | −C1 ], P2 = KR2 [ I | −C2 ]
(1)
where Pi is a 3 × 4 matrix, which maps a point in the 3D
space with homogeneous coordinates X = ( x, y, z ,1)T to an

image point with homogeneous coordinates x = ( x ', y ',1)T
x  Pi X
(2)
If we know K and Ri, we can compute the normalized
image coordinates as
x = ( KRi )-1 x  [ I | −Ci ] X
(3)

I ­ 465

ICASSP 2007

If we assume Ci = 0, i.e. assign the origin to be the
camera center, then
x  ( x, y, z )T
(4)
For the ground plane points, y is constant, i.e. –d. Therefore,
we can easily map them between the image coordinates and
the real 3D coordinates with (4). However, we need to first
know K and R. K may be easily calibrated and can be
assumed to be fixed, while R depends on the real pose of the
camera, which is consistently changing for a moving robot.
This will be our primary concern in subsequent discussion.
3. CAMERA ORIENTATION ESTIMATION FROM
VERTICAL LINES

The camera orientation is basically defined by R, i.e. the
camera rotation matrix in the world coordinates system. We
now show how to use vertical lines to estimate R. Ideally, if
the camera’s skew is zero and R = I, all vertical lines in the
physical world will be also vertical in the image. This is
intuitive and can be proved as follows.
Suppose a vertical line in the 3D space is represented as
L = ( x0 , t , z0 ,1)T , t ∈ (−∞, +∞) . Using (4), we can project it
to the image domain as
⎡ f x 0 px ⎤ ⎡ x0 '⎤ ⎡ x0 ''⎤
l  KR[ I | −C ]L = ⎢⎢ 0 f y p y ⎥⎥ ⎢⎢ t ' ⎥⎥ = ⎢⎢ t '' ⎥⎥
⎢⎣ 0 0 1 ⎥⎦ ⎢⎣ z0 ' ⎥⎦ ⎢⎣ z0 ''⎥⎦

where x0 '' and z0 '' are fixed and t '' ∈ (−∞, +∞) unless
fy = 0 .
The above observations lead to the following heuristics:
find an R such that vertical lines in the 3D space become
also vertical in the image upon rotating back with R.
Mathematically, if a camera only rotates, there exists a
homography between the old and the new images. In (1), let
C1 = C2 (i.e., rotation only), then we can have
I 2 = HI1 , where H = KR2 ( KR1 ) −1
(5)
where Ii stands for homogeneous coordinates of the image
pixels.
In our problem, we can set R2 = I . That is, the camera
is rotated back to the world coordinates system. Thus we
have
H = KR1−1 K −1
(6)
Since we assume a known K (as in [8]), we can further
simplify H by first normalizing the image coordinates as


I 2 = HIˆ1 , where Iˆi = K −1 I i
(7)
After the normalization, the orientation of a line will not
change since only the origin is translated and the coordinates
are scaled. Thus the earlier heuristics still apply, and now we
only need to find the normalized homography

H = R1−1
(8)
so that after transformation, all vertical lines in the 3D space
become vertical in the image. To find such transformation,
we need to solve two problems: (i) to detect lines in the

image that are vertical in the 3D space; (ii) find a transform
so that all those lines become vertical in the image. We now
present our methods for solving these problems.
3.1 Find Vertical Lines
The first problem is to find what lines in the image are
vertical lines in the 3D space. While this is in general an illposed problem, we simplify the task by making reasonable
assumptions based on our application. Suppose that the
angle of the vertical lines in the 2D image is zero, then those
lines whose absolute angle is less than a threshold is the
projection of 3D lines that are nearly vertical. If the rotation
around z and x axes is not much, which is usually the case
for a fixed camera on a mobile platform, we can use Hough
transformation to search for all lines and then filter out
vertical lines using the above criteria.
3.2 Find R by Transforming Vanishing Point

Theoretically, all parallel lines in the 3D space intersect
at same vanishing point. After projection to a 2D image, the
projected vanishing point maybe not at infinity any more,
which means the parallel lines are no longer parallel in the
2D image. To make them parallel in the image, we can
simply transform the vanishing point to the infinity. For our
problem, we need to transform the vanishing point to
(0,1, 0)T , which is the coordinates of the vanishing point of
all parallel vertical lines in the 2D image. Therefore, we can
formalize our approach to finding R as: Suppose that the
vanishing point in the normalized 2D image is

v  ( xv , yv , zv )T , find a rotation matrix H = Rv = R1−1 such
that Rv v  (0,1, 0)T . In our approach, Rv is computed by
rotating the camera around two axes independently. First, we
rotate the camera around its z axis, which is equal to rotating
the image coordinates, such that the vanishing point is
transformed to the y axis (that is, x = 0). Second, we rotate
the camera around the x axis to transform the vanishing
point to the infinity. Finally, we have
Rv = Rx Rz , where
⎡ cos(θ z ) sin(θ z ) 0⎤
⎡ xv ⎤ ⎡ 0 ⎤
Rz = ⎢⎢ − sin(θ z ) cos(θ z ) 0⎥⎥ such that Rz ⎢⎢ yv ⎥⎥ = ⎢⎢ yv '⎥⎥ (9)
⎢⎣ 0
⎢⎣ zv ⎥⎦ ⎢⎣ zv ' ⎥⎦
0
1 ⎥⎦
0
0 ⎤
⎡1
⎡0⎤ ⎡ 0 ⎤
⎢
⎥
Rx = ⎢ 0 cos(θ x ) sin(θ x ) ⎥ such that Rx ⎢⎢ yv '⎥⎥ = ⎢⎢ yv ''⎥⎥
⎢⎣ 0 − sin(θ x ) cos(θ x ) ⎥⎦
⎢⎣ zv ' ⎥⎦ ⎢⎣ 0 ⎥⎦

From (9), we have:
θ z = − arctan( xv / yv ), θ x = arctan( zv '/ yv ')
(10)
3.3 Iterative Rv Estimation
Before estimating Rv, we need to transform the lines to
the normalized image space as (7) defines. Suppose that a
line is represented as a 3x1 vector l = ( a, b, c)T , after
transformation, l ' = ( K −1 ) −T l = K T l ([7]). Due to noise and

I ­ 466

inaccuracies in vertical line detection, we need a robust
estimation algorithm for Rv. We first define an error
measurement for Rv: Given a set of normalized vertical lines
{li} and an estimate Rv , the error of Rv is defined as
E ( R ) = ∑ E (l | R ) = ∑ E (l ' = R l ) = ∑ θ (l ') (11)
v

i

i

v

i

i

v i

i

i

where li’ represents transformed line of li under Rv , and
θ (li ') is the angle of li’ (assuming that the vertical line’s
angle is zero). Intuitively, the error function is defined so as
to make all transformed lines to be vertical.
We have proposed the method for estimating Rv in
section 3.2, which starts from the vanishing point. Basically,
vanishing point is the intersection of the vertical lines.
Ideally, all detected vanishing lines should have a common
intersection points. But in practice, there may be multiple
intersections. Here we propose the following algorithm for
searching for the best Rv in the rotation space:
Input: An image I, camera internal matrix K
Output: camera orientation Rv
Algorithm:
1. Use Hough transformation to detect all lines
2. Choose all lines whose absolute angle is less than T
3. Compute all intersection points from any two lines.
4. Determine Rv from intersection point.
5. Choose a best Rv that has the minimum error.
6. Start with Rv, adjust θ x and θ z in a range, and find a
better Rv.
4. GROUND PLANE DETECTION WITH
ORIENTATION ESTIMATION

In ([8]), we proposed a normalized-homography-based
approach to detect ground plane from motion. In general, the
homography of the plane can be computed as:
H = KR2 ( I + ΔCnT / d ) R1−1 K −1
(12)
where π = (nT , d )T is the coordinates of a plane so that for
points X on the plane we have nT X + d = 0 . If we assume
that the camera only rotates around the y-axis,
or R2 = R1ΔRy . We can use the normalized homography:
(13)
Hˆ = ( KR1 ) −1 H ( KR1 ) = ΔR( I + ΔCnT / d )
Since the normal vector of the ground plane is determined,
i.e. n0 = (0,1, 0)T , normalized ground plane homography has
only 3 degree of freedom(DOF), which is much less than a
general homography with 8 DOFs. Moreover, ΔR and
ΔC can be directly used for localization.
However, to achieve such DOF reduction, camera
orientation (R1) is required. [8] presents a manual way to
calibrate the orientation. With automatic camera orientation
estimation, not only can we eliminate initial R1 calibration,
but also we can remove the constraint that the camera can
only rotate around the y-axis. We now show how to achieve
this.

Given the world coordinates system in Fig. 1. R can be
decomposed as:
R = Rz Rx Ry
(14)
This means that, from the world coordinates system to R1,
we first rotate around the y-axis, and then around x and z
axes. From (9), since Rv = R1−1 , we can recover Rz and Rx.
Although Ry is left unspecified, it is reasonable since in the
world coordinates system, we only require the y axis is
perpendicular to the ground plane, meaning that there is a
freedom to rotate around the y axis. Thus we can write R1
and R2 as R1 = R1v −1 R1 y and R1 = R1v −1 R2 y . So (12) becomes
H = KR2 v −1 R2 y ( I + ΔCnT / d ) R1 y −1 R1v K −1

(15)

Without losing generality, we can assume R1y = I , now we
can write the normalized homography as
Hˆ = ( KR2v −1 )−1 H ( KR1v −1 ) = R2 y ( I + ΔCnT / d ) (16)
Note R2y is a rotation matrix around the y-axis, which
has exactly the same form and meaning as ΔR in (13). In
(16), R1v and R2v are automatically estimated from images,
thus no R calibration is required and the constraint of only
rotating around the y-axis is removed.
5. EXPERIMENTS

Experiments have been carried out to assess the proposed
algorithm. In the following, sample results are presented.
The first example illustrates the process of estimating
camera orientation with the detection of vertical lines. Fig. 2
is based an indoor sequence captured by a robot platform.
Fig. 2 (a) shows the original image with detected vertical
lines in green color. (b) shows the rectified view with
rotation around the z axis so that the vanishing points lie on
the horizontal center of image. (c) shows the rectified view
with rotation around the x axis so that the vanishing points
are transformed to infinity, i.e. all lines become parallel. (d)
shows the results by applying both rotations so that all
vertical lines become vertical in the image.
The second sample illustrates the application of the
proposed method to normalized-homography-based ground
plane detection. Fig. 3 (a) and (b) show the point
correspondences. Fig. 3 (c) shows the rectified first view
with orientation calibrated with a check board pattern ([8]).
Fig. 3 (d) shows the rectified view with automatically
estimated orientation from vertical lines in (e). (f) shows the
detected ground plane points in green (red ones are thus
obstacle points). (g) shows the difference between the
second view and the transformed first view using the
computed normalized homography. Note the difference
between (c) and (d). This is partially due to the inaccuracy in
the detection of vertical lines. For example, as (e) shows, the
cabinet’s edge is not detected (due to a specific parameter
setting in line detection) whereas an edge on the backpack is
detected although that may not be an actual vertical line.
However, this example serves to illustrate that the proposed

I ­ 467

method works reasonably well despite the lack of prior
calibration and in the presence of inaccuracies of the vertical
line detection.
6. CONCLUSIONS
In this paper, we introduced a method to automatically
estimate the orientation of the camera on a robot platform,
using detected image lines that correspond to vertical line
structures in the physical world, which are abundant in most
man-made environments. We have showed how this
estimated orientation can be used in rectifying a view so that
the image plane becomes vertical, which is a common
assumption in many vision-guided mobile robot systems. We
further applied the technique to normalized-homographybased ground plane detection, which previously would
demand a fixed camera and an initial orientation calibration.
Both theoretical analysis and experimental evaluation have
demonstrated the advantages of the proposed method.

(a)

[5] S. Lenser and M. Veloso, “Visual sonar fast obstacle avoidance
using monocular vision,” In Proc. of the IEEE/RSJ IROS 2003.
[6] Y. Kim and H. Kim, “Layered ground floor detection for
vision-based mobile robot navigation,” Proc. IEEE Int. Conf. on
Robotics and Automation, vol. 1, pp. 13-18, 2004.
[7] R. Hartley and A. Zisserman. Multiple view geometry in
computer vision. Cambridge University Press, 2nd edition, 2003.
[8] J. Zhou and B. Li, “Robust Ground Plane Detection with
Normalized Homography in Monocular Sequences from a Robot
Platform” In ICIP 2006.
[9] J.J. Guerrero, C. Sagues, “Navigation from uncalibrated
monocular vision”, IFAC, Intelligent Autonomous Vehicles, pp.
210-215, 1998.
[10] C. Sagues, A. C. Murillo, J. J. Guerrero, T. Goedeme, T.
Tuytelaars, L. Van Gool, “Localization with omnidirectional
images using the radial trifocal tensor”, in ICRA’06, pp. 551-556.

(a)

(b)

(c)

(d)

(e)

(f)

(b)

(c)
(d)
Fig. 2. (a) The original image and detected vertical lines. (b)
Rotation around z axis. (c) Rotation around x axis. (d) The results
combing (b) and (c), where the detected lines are all vertical and
parallel in the image.

REFERENCES
[1] J. Hoffmann, M. Jungel and M. Lotzsch, “Vision Based System
for Goal-Directed Obstacle Avoidance”,8th Int. Workshop on
RoboCup 2004.
[2] K. Sabe, M. Fukuchi, J.-S. Gutmann, T. Ohashi, K. Kawamoto,
and T. Yoshigahara, “Obstacle Avoidance and Path Planning for
Humanoid Robots using Stereo Vision,” Proc. ICRA 2004, New
Orleans, pp. 592 – 597, Vol.1 April 2004.
[3] L. M. Lorigo, R. A. Brooks, and W. E. L.Grimsou, “Visuallyguided obstacle avoidance in unstructured environments,” Proc.
IEEE/RSJ Int. Conf. Intel. Robot & Systems, France, vol. 1, pp.
373-379, Sept. 1997.
[4] N. Pears and B. Liang, “Ground plane segmentation for mobile
robot visual navigation,” In IROS 2001, vol. 3, pp. 1513–1518.
[5] B. Liang, N. Pears and Z. Chen, "Affine Height Landscapes for
Monocular Mobile Robot Obstacle Avoidance" Proc. Intelligent
Autonomous Systems, pp. 863-872, August 2004.

(g)
(h)
Fig. 3. (a)-(b): two views with feature points and their
correspondence (indicated by red line segments). (c): rectified first
view with R1 calibrated by checkboard patterns [8]. (d): rectified
view with R1 automatically estimated from vertical lines in (e). (f):
the result of detected ground plane points with R1 in (d). (g): the
difference between the second view and the transformed first view
(h): difference using the homography estimated from inliers in (f).

I ­ 468

Simultaneous Semantic Segmentation of a Set of Partially Labeled Images
Qiongjie Tian
Computer Science and Engineering
Arizona State University

Baoxin Li
Computer Science and Engineering
Arizona State University

qtian5@asu.edu

baoxin.li@asu.edu

Abstract
Semantic segmentation, by which an image is decomposed into regions with their respective semantic labels, is
often the first step towards image understanding. Existing
research on this regard is mainly performed under two conditions: the fully-supervised setting that relies on a set of
images with pixel-level labels and the weakly-supervised
one that uses only image-level labels. In both cases, the
labeling task is time-consuming and laborious, and thus
training data are always limited. In practice, there are voluminous on-line images, which unfortunately often have only
incomplete image-level labels (tags) but would otherwise be
potentially useful for a learning-based algorithm. Only limited efforts have been attempted on using such coarsely and
incompletely labelled data for semantic segmentation. This
paper proposes a new approach to semantic segmentation
of a set of partially-labelled images, using a formulation
considering information from multiple visual similar images. Experiments on several popular datasets, with comparison with existing methods, demonstrate evident performance improvement of the proposed approach.

mation to help improve the local superpixel-level labeling.
Limited availability of fully-labeled data is a practical constraint for such approach. In [20][21], region-based cues
are used to build exemplar-SVMs to gain the final labeling.
However, there is one obvious disadvantage: users have to
label each pixel in the dataset, which is time-consuming and
involves a lot of manual work. In the weakly-supervised setting, data with only image-level labels are assumed. Most
existing work further assumes that the labels are “complete”
in the sense that the image-level label set for a given image
contains all possible labels we may assign to any pixel in
that image. This setting has been used in [29][11][25].

Figure 1. Two images with partially and fully image-level labels

1. Introduction
In the era of Internet and social media, there are more and
more images posted on-line. Often, such on-line data lack
sufficient textual annotation desired by learning-based algorithms. To make such data more useful, efforts have been
devoted towards tasks like image sentiment analysis [27],
image tagging[5][7][28] and image classification[35][26],
targeting at producing labels for the images. In the labeling
effort the finest granularity one could achieve is to perform
semantic segmentation [3], which may classify each pixel in
one image into a proper class/label. Both fully-supervised
and weakly-supervised approaches exist.
In the fully-supervised setting, a set of images with pixellevel labels are available. In [21], all pixels in one superpixel are assumed to have the same label and Markov Random Field (MRF) was used to capture the context infor-

Figure 2. Illustrating the problem studied in this paper: the left
panel represents the input to our algorithm, which are a set of images with partial image-level labels (one demo shown in Figure 1),
and the right panel is the output of segmented images with labeled
pixels. A formal problem definition is shown in Section 3.

The abundance of images with tags on social media platforms provides the opportunity for obtaining large-scale
training sets without laborious manual labelling. However,
in reality, even if we may be able to obtain a lot of images

with a desired set of semantic tags (and use the tags as semantic labels for simplicity), the majority of on-line images
would still have only incomplete image-level labels, especially for user-generated images. That is, it is unrealistic
to expect tags associated with an on-line image would happen to cover all semantic concepts we need to employ for
segmentation. Therefore, in order to utilize the vast on-line
images, we face the task of how to label each pixel in each
image (i.e., semantic segmentation), given a set of images
with partial image-level labels. Figure shows a demo of one
image with partially image-level labels, while our task is illustrated in Figure 2. One similar work is [34], which only
considers using information from one image only and does
not consider the fact that visually similar superpixels across
different images also are likely to have the same labels. In
this paper, we work on this problem from one new aspect by
proposing an approach based on conditional random fields
(CRFs), which attempts to employ all possible sources of
information in the dataset to deal with the challenge of incomplete labels.
The contributions of the paper are as follows. First, we
propose a novel formulation for a new problem of semantic
segmentation with partial image-level labels. Second, under
the proposed multi-image model, we propose an efficient
solution and demonstrate with comparative experiments its
effectiveness.
The organization of the remainder of the paper is as follows. We first give a brief literature review on related works
in Section 2. Then, a detailed description of the problem
and our proposed approach are provided in Section 3. To
show the performance of our proposed method, experiments
are reported in Section 4. We conclude our work and present
our future work in Section 5.

2. Related Works
We briefly review below two classes of related research on semantic segmentation: those relying on fullysupervised learning and those utilizing only weaklysupervised learning. As is evident from the following discussion, the distinction between these two classes of approaches is mainly on the granularity of labelling for the
training data.

2.1. Fully-supervised Semantic Segmentation
As described in Section 1, in fully-supervised semantic segmentation, labels of each pixel or superpixel in the
training set are known. There are a lot of existing efforts
on this regard. In [19], Jamie Shotton et al. proposed semantic texton forests to do semantic segmentation using a
bag-of-semantic-textons model, where only simple features
of superpixels were used. To improve the performance,
some other approaches attempt to consider neighboring information of different superpixels. In [9], Pushmeet Kohli

et al. proposed to use higher order CRFs to capture such
information of a set of pixels. Since high-order CRF models do not model the relevance of semantic labels, in [13],
Heesoo Myeong et al. proposed to use high-order semantic relations to capture the context information in images
and then transfer semantic labels from a labeled image to
another unlabeled image. Besides tree-structure algorithms
and graphical models (like CRF, MRF), active learning and
deep learning are also applied to semantic segmentation recently. In [16], Gemma Roig et al. proposed a MAP inference method based on active learning, which is in fact one
semi-supervised method. In [18], to improve the Recursive
Context Propagation Network (RCPN), two revisions were
made: one is to solve the potential problem because of the
special structure of RCPN, which can help reduce the complexity of the network structure; the other is to consider the
context information by building a Markov Random Field on
the modified structure. This is one recent work on applying
deep network to capture the context information of different
superpixels for semantic segmentation.
Obviously, one key limitation of the fully-supervised approaches is the requirement of a set of images with pixellevel (or superpixel-level) labels. Due to the cost associated
labeling, generally speaking one cannot assume the availability of high-quality and large-scale training data.

2.2. Weakly-supervised Semantic Segmentation
Because of the strong requirement of fully-supervised
semantic segmentation, research on finding new techniques
to solve weakly semantic segmentation becomes popular.
Liu et al. worked on dual clustering for semantic segmentation by constructing two clusterings on smoothness and also
the relation between image features and superpixel-level labels [11]. Besides the dual clustering method, many other
approaches are also proposed to solve weakly supervised
semantic segmentation. For example, Vezhnevets et al proposed to use active learning in [23], and multiple instance
multi-task learning to solve weakly semantic segmentation
in [22]. It may be difficult to learn superpixel-level labels
from only one image. In [24], a multi-image model was proposed, which builds a graphical model on the entire dataset.
More recently, a graphical model was also proposed in [4],
where multiple instance learning and CRF are combined.
Besides CRF-based methods, structural information from
different superpixels was also considered in [32][33][31],
using the concept of graphlets. Recently, semantic relevance has also been studied in the weakly-supervised cases.
For example, in [30], hypergraphs were used to capture the
high-order semantic relevance, instead of only the secondorder relevance in [29], and in [14], deep learning techniques are used to find the pixel-level labeling. In [34], Wei
Zhang et al. studied one new practical case in which each
image is assumed to have part of image-level labels and also

maybe some incorrect labels.
While apparently less stringent than the fully-supervised
cases, the image-level labels in existing methods of weaklysupervised semantic segmentation are still assumed to be
complete, i.e., the set of labels of a given image captures
all possible semantic labels that can be assigned to pixels of
that image. As discussed previously, this limitation makes it
difficult to utilize vast amount of on-line pictures that would
otherwise be useful for the learning task. Our study in this
paper is intended to address this issue by considering using
information from the entire dataset instead of only one image. We will formally define the problem and present our
solution in the next section.

[24] may be employed except that complete labelling was
assumed therein. Our basic strategy in modeling the problem with incomplete labels is to construct a conditional random field (CRF) for capturing these types of probabilistic
associations: visually-similar superpixels are likely to have
the same labels (but two similar superpixels may have different likelihoods belonging to the same label, depending
on if they are from the same image or from different images), nearby superpixels tend to share labels, and the final
label set of an image is a superset of the given (incomplete)
label set. Graphically, a basic component of the overall CRF
model may be illustrated by Figure 3.

3. Proposed Approach
Based on the previous discussion, we formally define the
following problem of this study: Given a set of images with
incomplete image-level labels, to predict all pixel-level labels for each image in the set. The image-level labels indicate possible objects in one image, while the pixel-level
labels are the final desired segmentation and classification.
The incompleteness of labels for an image means that this
image may contain some objects/regions which cannot be
assigned to any of the given classes in its label set. For
example, an image with four objects, car, street, sky, and
grass, may have only a set of image-level labels, say car
and sky. Still, in the final segmentation, the correct results
should properly label those regions corresponding to the
missing labels (street and grass). Apparently, the missing
information needs to be figured out by considering the entire set of images. This is schematically illustrated in Figure
2. In this work, we employ the concept of superpixel [15],
and assume that pixels within the same superpixel share the
same label. This helps simplify the problem to some extent
for better tractability.
We use the following notations in the rest of the presentation. Denote one image set with N images by A = {Ii , i ∈
{1, · · · , N }}, which has corresponding partial image-level
labels L = {Li , i ∈ {1, · · · , N }}. Pixels are denoted
by pi,j , j ∈ {1, · · · , Mi }, i ∈ {1, · · · , N } where pi,j is
the j th pixel in the image Ii which has Mi pixels in total. Similarly, superpixels of the image Ii are denoted by
xi = {xi,j , j ∈ {1, · · · , ni }} where xi,j is the j th superpixel in the image Ii which has ni superpixels in total. Also
we use Li,j to denote the label of the j th superpixel’s label
in the image Ii .

Figure 3. Illustrating the basic component of the proposed CRF
model. Each superpixel is related to others via the shown connections. See text for definitions of the symbols. The entire set of
image forms an overall CRF by combining all the basic components corresponding all superpixels.

In Figure 3, xi,j is the j th superpixel of the image Ii
in the dataset. Si,j is the set of spatial neighbors of xi,j ,
defined as the superpixels which are located next to xi,j in
the image Ii . Mi,j is the set of visually-similar neighbors
of xi,j , defined as superpixels which are located in those
images sharing common image-level labels as Ii . Vi,j is
the set of visually-similar neighbors of xi,j , defined as superpixels which are located in the images without common
image-level labels with Ii . To help illustrate how the nodes
and connections on the final CRF link the entire image set
together, we depict in Figure 4 a visual example with exemplar images and their superpixels explicitly shown.
Based on the structure described above, we can have
the complete energy function for our CRF-based model as
given in Eqn.1:
E({Li,j ,j ∈ {1, · · · , Mi }, i ∈ {1, · · · , N }}, θ, α) =
X
(φ(xi,j , Li,j , θ) + λ(Li,j , Ii ))+
xi,j ,∀i,j
0

ϕ(Li,j , Li,j )+

0

(xi,j ,xi,j )∈Si,j ,∀i,j

3.1. Formulating the Problem
In our problem, the input images do not have superpixellevel labels. Further, the images do not have a complete
set of semantic labels. Evidently, in general the full information needed for labelling an image needs to be inferred
from other images. The multi-image model introduced in

X

α1

0

X

α2

ϕ(Li,j , Li,j )+

0

(xi,j ,xi,j )∈Mi,j ,∀i,j

X

α3
0

(xi,j ,xi,j )∈Vi,j ,∀i,j

0

ϕ(Li,j , Li,j )

(1)

Li,j is computed by (Eqn.3):
Li,j = arg min φ(xi,j , l, θ) + λ(l, Ii )+
l
X
0
ϕ(l, Li,j )+
α1
0

(xi,j ,xi,j )∈Si,j

α2

0

X

ϕ(l, Li,j )+

0

(xi,j ,xi,j )∈Mi,j

α3

X

0

ϕ(l, Li,j )

(3)

0
(xi,j ,xi,j )∈Vi,j

Figure 4. Illustrating basic components of the proposed CRF
model with sample images. Shown are some superpixels of three
images I1 , I2 , I3 . These superpixels are separated by red boundaries and their positions in their corresponding images are marked
by the black rectangles. I1 and I2 have one common image-level
label, while I1 and I3 have no common image-level labels. A
basic CRF component is shown in light green color and is built
on xi,j . Each circle represents one node in CRF. In this example, we only set Mi,j = {mi,j } and Vi,j = {vi,j } and their size
is one. It is easy to see there are six elements in Si,j , which is
{ski,j , k ∈ {1, 2, · · · , 6}}.

where α = [α1 , α2 , α3 ] controls the contributions of each
potential terms, φ(xi,j , Li,j , θ) is the unary potential which
gives the energy caused by the fact that the label Li,j is
assigned to the superpixel xi,j . λ(Li,j , Ii ) relates to how
likely Ii has the label Li,j . It can be the negative of the
possibility that the image Ii has the label Li,j , computed
by [5]. For the pairwise potential, we use the Potts model,
where the function ϕ(·) is given as Eqn.2.
(
1
ϕ(Li,j , Li,j ) =
0
0

0

if Li,j 6= Li,j
otherwise

(2)

The entire algorithm based on the above core ICM iteration
is given in Algorithm 1.
Algorithm 1 An Algorithm Based On ICM
1: Input: Energy function (Eqn.1), one potential label set
L̃ of each superpixel xi,j
2: Output: the label Li,j of each superpixel xi,j , j ∈
{1, · · · , Mi }, i ∈ {1, · · · , N }
3: BEGIN:
4: initialize each xi,j using random element from L̃ and
store initialized labels of each superpixel in Y1 , Y2 .
5: while check the stop-condition do
6:
for each superpixel xi,j , j ∈ {1, · · · , Mi }, i ∈
{1, · · · , N } do
7:
tmp = ∅ and Consider Si,j , Mi,j and Vi,j of xi,j .
8:
for each l in L do
9:
compute the local energy (denoted as e) by
assuming each superpixel has the label as that in Y1 except that xi,j has the label Li,j = l
10:
tmp = tmp ∪e.
11:
end for
0
12:
Set the label of xi,j in Y2 as l which has the
smallest local energy.
13:
end for
14:
Y1 = Y2 .
15: end while

3.2. An Inference Algorithm
Exact solutions for achieving the extrema of Eqn.1
would require exponential complexity and thus cannot be
obtained unless it is for datasets of trivial complexity. Approximate approaches to inference under similar graphical
models have been developed over the years. Examples include Loopy Belief Propagation [12], Graph cut [6], Simulated Annealing [1], and etc. In this work, we adopt Iterated
Conditional Modes (ICM) [8] in developing an inference
algorithm, owing to its simplicity and in turn efficiency in
dealing with a large model like ours. The key idea of the
ICM-based algorithm is based on the iterative update: when
computing the label of one superpixel, labels of the others
are assumed to be fixed. For each superpixel xi,j , its label

3.3. Key Implementation Details
We now present a few key technical details that are necessary to fully implement the proposed solution. We use
the SLIC algorithm proposed in [2] to obtain superpixels for
images in our experiments and also compute the histogrambased features for superpixels and images, following the
method of [21]. Before constructing the entire energy function of Eqn.1, we first train one SVM classifier using a very
small image set. In this small image set, there are about two
images per label and full pixel-level labels of each image are
provided. Labeling this subset requires less manual work.
More details are shown in Section 4. This pre-trained SVM
classifier supplies a measurement for the unary potential in

the proposed model, i.e., the function φ(·) given in Eqn. 4.
(
0
ρ if Li,j 6= Li,j (θ)
φ(xi,j , Li,j , θ) =
(4)
0 otherwise
0

where Li,j (θ) is the predicted label of xi,j by the pretrained SVM with model parameters θ, and ρ is the penalty.
For the term λ(Li,j , Ii ), we compute it using the method
proposed in [5], which does image-tagging and can provide
a ranked list of all possible image-level labels which are
likely to be shown in the corresponding image. λ(Li,j , Ii )
is the negative value of the likelihood that the image Ii has
the label Li,j .
For pairwise potentials, we need to consider different
neighboring relations. For one superpixel xi,j , there are
three sets of neighbors we need to compute: Si,j , Mi,j
and Vi,j . For one given superpixel xi,j , the spatial neighbor set Si,j can be estimated using image erosion/dilation
(note that typically superpixels are irregular in shape). This
is illustrated in Figure 5. For the other two sets of neigh-

Algorithm 2 Algorithm to compute Mi,j and Vi,j
1: Input: {Ii , Li }, {xi,j }, j ∈ {1, · · · , Mi }, i ∈
{1, · · · , N }, D1 (·) which is the function to compute
the distance between two images and D2 (·) which is to
compute the distance between two superpixels.
2: Output: Mi,j , Vi,j , j ∈ {1, · · · , Mi }, i ∈ {1, · · · , N }
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:

24:
25:

Figure 5. Illustrating how to find the spatial neighbors of one given
superpixel xi,j shown in (a). First we need to get the image (b)
which is the mask of xi,j . Then we can apply the image dilation
to (b) to get the image (c). By computing the difference of images
(b) and (c), the final mask (d) is obtained. Comparing (d) and
the original image (a), we can easily get Si,j which consists of
super-pixels which overlap with the final mask (d).

26:
27:
28:
29:
30:

bors, we can obtain them by Algorithm 2, in which the
normalized Euclidean distance is used to compute the similarity between different images and superpixels, based on
the image/superpixel features defined above. We emphasize
that such neighboring relations are defined based on the proposed CRF model and thus they reflect physical constraints
imposed by the given labels (and their interaction) and geometrical proximity, in addition to visual similarity.

BEGIN:
// To compute SMi , SVi .
for i = 1,· · · , N do
for j = 1, · · · , N, i 6= j and Li ∩ Lj 6= ∅ do
Compute the similarity D1 (Ii , Ij ).
end for
Find the top q most similar images, denoted as SMi .
for j = 1, · · · , N, i 6= j and Li ∩ Lj == ∅ do
Compute the similarity D1 (Ii , Ij ).
end for
Find the top q most similar images to Ii , denoted as
SVi .
end for
for each superpixel xi,j , j ∈ {1, · · · , Mi }, i ∈
{1, · · · , N } do
// we have SMi and SVi of Ii
// and will construct SPMi,j and SPVi,j
SPMi,j = ∅, MSSi,j = ∅, ∀i, j.
0
for each superpixel xi,j in each image I 0 ∈ SMi do
Find the top p most similar superpixels to xi,j based
0
on D2 (xi,j , xi,j )
Denote these p superpixels as MSSi,j and also we
set SPMi,j = SPMi,j ∪ MSSi,j
end for
Find top k most similar superpixels to xi,j from
SPMi,j , which are Mi,j of xi,j .
SPVi,j = ∅, MSSi,j = ∅, ∀i, j.
0
for each superpixel xi,j in each image I 0 ∈ SVi do
Find the top p most similar superpixels to xi,j based
0
on D2 (xi,j , xi,j )
Denote these p superpixels as MSSi,j and SPVi,j =
SPVi,j ∪ MSSi,j
end for
Find top k most similar superpixels to xi,j from
SPVi,j , which are Vi,j of xi,j
end for

3.4. Comparison With MIM
The proposed method bears some similarity to the MultiImage Model (MIM) of [24], since both consider a set of
images simultaneously. To appreciate the key difference
easily, we provide the energy function of the MIM below

size of each superpixel xi,j , which is denoted by size(xi,j ).
P
0
i,j δ(Li,j − Li,j )size(xi,j )
P
pp =
(7)
i,j size(xi,j )
P
0
i,j δ(Li,j − l)δ(Li,j − Li,j )size(xi,j )
P
pcl =
(8)
i,j δ(Li,j − l)size(xi,j )
1 X
¯ = S
pc
pcl
(9)
| Li |

(Eqn.5):
E({Li,j , j ∈ {1, · · · , Mi }, i ∈ {1, · · · , N }}, θ) =
X
(ψ1 (xi,j , Li,j , θ) + π(Li,j , Ii ))+
xi,j ,∀i,j

X

0

0

ϕ1 (Li,j , Li,j , xi,j , xi,j )+

0

(xi,j ,xi,j )∈Si,j ,∀i,j

X

0

0

ϕ1 (Li,j , Li,j , xi,j , xi,j )

(5)

0

0

(xi,j ,xi,j )∈Mi,j ,∀i,j

where π(Li,j , Ii ) is zero if the label Li,j is one image-level
label of the image Ii and it is set to infinity otherwise. Moreover, ϕ1 (·) is given as follows:
0

l

0

ϕ1 (Li,j , Li,j , xi,j , xi,j ) =
(
0
0
1 − D(xi,j , xi,j ) if xi,j , xi,j are different
0
otherwise

(6)

where D(·) is one similarity metric.
Eqn.5 clearly indicates one strong requirement on the
labels, imposed by the choice of π(·). Because of that
function, MIM cannot be used to solve the general problem defined in this paper. In our formulation, to solve the
more general and practical problem, we relaxed the strong
requirement in MIM by introducing a new π(·) function
plus one additional pairwise potential to better capture visual similarity of superfixels (those across images and do
not have common image-level labels). These resulted in
the new model of Eqn.1. In fact, compared with both formulations, we can see that MIM is one special case of our
approach, which is used to deal with the less challenging
situation where images have completely image-level labels.

4. Experiments
In this section, we demonstrate the effectiveness of the
proposed approach based on comparative experiments using the following three datasets: one synthetic dataset, the
MSRC-21 dataset [19] and the Siftflow dataset [21]. For the
synthetic dataset and the MSRC-21 dataset, we make comparison with the approach in [24], which is among the stateof-art methods in the literature. For the Siftflow dataset, we
provide our experimental results and compare with existing approaches in the fully-supervised case and the ordinary weakly-supervised case. The comparison is based on
two metrics: per-pixel accuracy (denoted as pp and shown
¯ and
in Eqn.7) and average per-class accuracy (denoted as pc
shown in Eqn.9). To compute these measures, we need the

In the above definitions, Li,j is the predicted label and Li,j
is the ground truth of the label of xi,j , and pcl is the pixellevel
S accuracy for all the pixels whose label is l. Also
| Li | is the total number of potential labels .

4.1. Synthetic Dataset
The simulation is designed as follows. First, we generate
one synthetic dataset that has 30 pairs of observation images
and labelmaps. An observation image is a 200×200 grayscale image while its labelmap is a 200×200 image whose
pixel values are the labels of its corresponding observation.
For each observation image, we split it into 20×20 superpixels, each of which has 10×10 pixels. Moreover, we assume that all pixels in one superpixel have the same label
and labels are from this set: {1, 2, 3, 4, 5}.
To generate each pair of one observation image and its
labelmap, we run the following procedure:
1. We first generate one labelmap randomly and make
sure that labels of pixels in the same superpixel are the
same.
2. The corresponding observation image is generated
based on the new labelmap.
3. The inference algorithm runs for 200 iterations to obtain the final pair of observation image and labelmap.
(a) For each iteration, we use the current labelmap
and the observation image to generate a better labelmap whose energy is smaller. Then based on
the new generated labelmap, we generate the new
observation image.
During the above procedure, we set the total number of iterations to be 200 since at this iteration the observationlabelmap pair is already stable. Besides the number of iterations, we set the relationship between one observation
image and its labelmap as the Gaussian distribution whose
standard variation is set to be 10. Samples of the constructed dataset are shown in Figure 6. The average size
of the complete image-level labels is 3.46. To generate partial image-level labels, we randomly remove one label from
the complete image-level labels. The parameters k, q and p
we set in this simulation are 21, 3, 5, respectively.

The per-class accuracies from the proposed and the MIM
method for Set one, Set two, and the Entire Set are plotted
respectively in Figure 7, Figure 8 and Figure 9. Overall, the
performance gains of the proposed method over MIM are
5%, 3% and 2% respectively for Set one, Set two, and the
Entire Set.
100
MIM
ours

90
80

50
40
30
20
10
0

at
bo
dy
bo
g
do
t
ca
ad
ro
r
ai
ch
ok
bo
rd
bi
gn
si r
e
w
flo le
c

cy

bi

w

r
ca
ce
fa
er e
at
w lan
p
ro
ae
y
sk p
ee

sh

co

e
tre
s
as
gr ng
i
ild
e
ag
er

bu

MIM [24]
Proposed

pp
51.15%
76.74%

60

Av

The synthetic dataset was then used to compare the performance of the proposed approach and the MIM method.
The MIM method would simply assume whatever labels
given for an image is complete. The final results are summarized in Table 1. From these results, it is obvious that the
MIM method lags the proposed approach by a large margin.
We also note the difficulty of the task (even if the dataset is
synthetic), since a lot of source of uncertainties were introduced in the process of creating the data. This explains why
the overall accuracy numbers are not very high for either
approach.

70

Accuracy (in %)

Figure 6. This figure shows some pairs of the observation and the
labelmap generated in the synthetic dataset. The first row consists
of labelmaps while the second one consists of observation images.
For each column, it is a pair of one labelmap and its observation.

Figure 7. Comparison of per-class accuracies for Set one. The first
column is the average performance of two algorithms. The left 21
columns are for each object.

100

¯
pc
29.81%
42.72%

MIM
ours

90

80

Table 1. Comparing with the MIM model on the synthetic dataset.

1

50

40

30

20

10

0

at
bo
dy
bo
g
do
t
ca
ad
ro
r
ai
ch
ok
bo
rd

e

er

cl

w

gn

bi

si

flo

r

cy

bi

ca

ce
fa
er
e
at
w lan
p
ro
ae

y
sk
p
ee
sh
w
co
e
tre
s
as
gr g
in
ild
bu ge
a
er

1 There are 23 objects in total, but 2 of them are not considered by
Microsoft research. So we only use 21 objects. Details are shown in the
dataset which is available on Microsoft research.

60

Av

In this dataset, there are 591 images and 21 objects
in total. We split the dataset into two parts: Set one and
Set two, both are the same as those used in [19]. As a result, there are 276 images in Set one, 256 in Set two. Also
we call the union of Set one and Set two as the Entire Set.
To get the pre-trained SVM classifier, we randomly choose
42 images out of 59 images which consist of the validation set as in [19]. The average numbers of the complete
image-level labels for Set one, Set two and the Entire Set
are 2.4710, 2.4492 and 2.4605, respectively. To generate
partial image-level labels, we randomly remove one label
from each complete image-level label set. So the average
sizes of Set one, Set two and Entire Set decrease by 40.4 %,
40.8% and 40.6%, respectively. In this experiment, parameters k, p and q are set to be 10, 3 and 8, respectively.

Accuracy (in %)

4.2. MSRC-21 Dataset

70

Figure 8. Comparison of per-class accuracies for Set two. The first
column is the average performance of two algorithms. The left 21
columns are for each object.

In addition to per-class accuracy, we also provide the perpixel accuracy in Table 2, where it is clear that the proposed
approach was able to outperform MIM by large margins on
all the sets of data.
The above results demonstrated the effectiveness of the
proposed approach in dealing with incomplete image-level

100

¯ = 22.34%. Since the related work do not report
and pc
the per-pixel accuracy (pp) on this dataset, we only report
the per-class accuracy (by quoting) in Table 3, including
the results from some fully-supervised methods ([19][10])
and weakly-supervised methods assuming complete imagelevel labels ([24][25][11][31]). From the table, we see that
our approach was able to deliver nearly comparable performance, although we subject our approach to the heavy loss
of information, while the competing methods either utilize
pixel-level labels or assume and use complete image-level
labels.

MIM
ours

90

80

Accuracy (in %)

70

60

50

40

30

20

[24]
14%

10

0

e

er

cl

at
bo
dy
bo
g
do
t
ca
ad
ro
r
ai
ch
ok
bo
rd

w

gn

bi

si

flo

r

cy

bi

ca

an

pl

p

er

ce

fa

ro

at

w

ae

s

e

g

in

ag

ee

y

sk

e

w

sh

co

tre

as

gr

er

ild

bu

Av

e

Figure 9. Comparison of per-class accuracies for the Entire Set.
The first column is the average performance of two algorithms.
The left 21 columns are for each object.

MIM in [24]
Proposed

Set one
43.33%
56.69%

Set two
39.44%
52.80%

Entire Set
41.82%
53.08%

Table 2. The per-pixel accuracies pp of our approach and MIM in
[24].

labels. It is worth pointing out that the MIM method reported higher performance numbers in [24], where it was
studied as an ordinary weakly-supervised approach with
complete image-level labels for training. Our experimental setting is more realistic for simulating the scenario of
learning with Web images. In this experiment, considering
the dropped label per image, the label set suffers a loss of
around 40% labeling information compared with the case
where images have complete image-level labels. The proposed approach, even if with only a very simple ICM-based
inference algorithm, was shown to be able to better deal
with the incomplete label data.

4.3. Siftflow Dataset
In this experiment, we show the performance of our algorithm on the Siftflow dataset [21]. This dataset consists
of 2688 images and 33 labels. We use the entire training
set which has 2488 images, as defined in [17]. The average number of image-level labels for each image in the
entire Siftflow dataset is 4.4297 and for the part we use,
on average, there are 4.3881 labels per image. To simulate
incomplete image-level labeling, we create partial imagelevel labels for each image by randomly removing one label from the original label set. This means we remove
22.79% label information on average for each image. During the experiment, parameters k, p and q are set to be
10, 3 and 8, respectively. Our results are: pp = 57.09%

[25]
21%

[19]
24%

[10]
24%

[11]
26%

[31]
27.73%

Ours
22.34%

¯ from our approach and
Table 3. Average per-class accuracy pc
those from a set of competing approaches, either fully-supervised
or weakly-supervised with complete image-level labels. The results above are in percentage.

5. Conclusion & Future Work
We identified a key limitation in existing methods for semantic segmentation and proposed a new multi-image formulation for addressing the limitation. An inference algorithm was designed for finding a solution under the proposed multi-image model. To demonstrate the effectiveness
of our algorithm, we performed experiments on both synthetic data and real datasets including MSRC-21 and Siftflow. While current results have shown advantages of the
proposed method, there are still a few leads for future exploration. In particular, current results indict that some classes
have low per-class accuracy, possibly due to their rare presence in the images. Such information (some classes being
rare), if known a priori, may be explicitly factored into the
formulation so that rare classes do not get overshadowed by
other more common classes.

Acknowledgments
The work was supported in part by an ARO grant
(#W911NF1410371) and an ONR grant (#N00014-15-12344). Any opinions expressed in this material are those
of the authors and do not necessarily reflect the views of
ARO or ONR.

References
[1] AARTS/KORST. Simulated annealing and boltzmann machines. A stochastic approach to combinatorial optimization
and neural computing. John Wiley., 1990.
[2] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and
S. Susstrunk. Slic superpixels compared to state-of-the-art
superpixel methods. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34(11):2274–2282, 2012.

[3] P. Arbeláez, B. Hariharan, C. Gu, S. Gupta, L. Bourdev, and
J. Malik. Semantic segmentation using regions and parts.
In Computer Vision and Pattern Recognition (CVPR), 2012
IEEE Conference on, pages 3378–3385. IEEE, 2012.
[4] F.-J. Chang, Y.-Y. Lin, and K.-J. Hsu. Multiple structuredinstance learning for semantic segmentation with uncertain
training data. In Computer Vision and Pattern Recognition,
2014 IEEE Conference on. IEEE, 2014.
[5] M. Chen, A. Zheng, and K. Weinberger. Fast image tagging.
In Proceedings of the 30th International Conference on Machine Learning, pages 1274–1282, 2013.
[6] A. Delong, A. Osokin, H. N. Isack, and Y. Boykov. Fast
approximate energy minimization with label costs. International journal of computer vision, 96(1):1–27, 2012.
[7] X. He, X. Li, G. Yang, J. Xu, and Q. Jin. Adaptive tag selection for image annotation. In Advances in Multimedia Information Processing–PCM 2014. Springer, 2014.
[8] J. Kittler and J. Föglein. Contextual classification of multispectral pixel data. Image and Vision Computing, 2(1), 1984.
[9] P. Kohli, P. H. Torr, et al. Robust higher order potentials for
enforcing label consistency. International Journal of Computer Vision, 82(3):302–324, 2009.
[10] C. Liu, J. Yuen, and A. Torralba. Nonparametric scene parsing: Label transfer via dense scene alignment. In Computer
Vision and Pattern Recognition, 2009 IEEE Conference on,
pages 1972–1979. IEEE, 2009.
[11] Y. Liu, J. Liu, Z. Li, J. Tang, and H. Lu. Weakly-supervised
dual clustering for image semantic segmentation. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE
Conference on, pages 2075–2082. IEEE, 2013.
[12] K. P. Murphy, Y. Weiss, and M. I. Jordan. Loopy belief propagation for approximate inference: An empirical study. In
Proceedings of the 5th conference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers Inc., 1999.
[13] H. Myeong and K. M. Lee. Tensor-based high-order semantic relation transfer for semantic scene segmentation. In
Computer Vision and Pattern Recognition, 2013 IEEE Conference on, pages 3073–3080. IEEE, 2013.
[14] P. O. Pinheiro and R. Collobert. From image-level to pixellevel labeling with convolutional networks. In CVPR, pages
1713–1721, 2015.
[15] X. Ren and J. Malik. Learning a classification model for
segmentation. In Computer Vision. Proceedings. Ninth IEEE
International Conference on, pages 10–17. IEEE, 2003.
[16] G. Roig, X. Boix, R. D. Nijs, S. Ramos, K. Kuhnlenz, and
L. V. Gool. Active map inference in crfs for efficient semantic segmentation. In Computer Vision (ICCV), 2013 IEEE
International Conference on, pages 2312–2319. IEEE, 2013.
[17] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman.
Labelme: a database and web-based tool for image annotation. IJCV, 77(1-3):157–173, 2008.
[18] A. Sharma, O. Tuzel, and D. W. Jacobs. Deep hierarchical parsing for semantic segmentation. arXiv preprint
arXiv:1503.02725, 2015.
[19] J. Shotton, J. Winn, C. Rother, and A. Criminisi. Textonboost
for image understanding: Multi-class object recognition and
segmentation by jointly modeling texture, layout, and context. International Journal of Computer Vision, 81(1), 2009.

[20] J. Tighe and S. Lazebnik. Finding things: Image parsing
with regions and per-exemplar detectors. In Computer Vision
and Pattern Recognition (CVPR), 2013 IEEE Conference on,
pages 3001–3008. IEEE, 2013.
[21] J. Tighe and S. Lazebnik. Superparsing. International Journal of Computer Vision, 101(2):329–349, 2013.
[22] A. Vezhnevets and J. M. Buhmann. Towards weakly supervised semantic segmentation by means of multiple instance
and multitask learning. In Computer Vision and Pattern
Recognition, 2010 IEEE Conference on, pages 3249–3256.
IEEE, 2010.
[23] A. Vezhnevets, J. M. Buhmann, and V. Ferrari. Active learning for semantic segmentation with expected change. In
Computer Vision and Pattern Recognition, 2012 IEEE Conference on, pages 3162–3169. IEEE, 2012.
[24] A. Vezhnevets, V. Ferrari, and J. M. Buhmann. Weakly supervised semantic segmentation with a multi-image model.
In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 643–650. IEEE, 2011.
[25] A. Vezhnevets, V. Ferrari, and J. M. Buhmann. Weakly supervised structured output learning for semantic segmentation. In Computer Vision and Pattern Recognition (CVPR),
2012 IEEE Conference on, pages 845–852. IEEE, 2012.
[26] W. Voravuthikunchai, B. Crémilleux, and F. Jurie. Histograms of pattern sets for image classification and object
recognition. In Computer Vision and Pattern Recognition,
2014 IEEE Conference on. IEEE, 2014.
[27] Y. Wang, S. Wang, J. Tang, H. Liu, and B. Li. Unsupervised
sentiment analysis for social media images. In 24th International Joint Conference on Artificial Intelligence, 2015.
[28] Z. Wang and B. Li. Learning to recommend tags for online photos. In Social Computing and Behavioral Modeling,
pages 1–9. Springer, 2009.
[29] W. Xie, Y. Peng, and J. Xiao. Semantic graph construction for weakly supervised image parsing. In Twenty-Eighth
AAAI Conference on Artificial Intelligence, 2014.
[30] W. Xie, Y. Peng, and J. Xiao. Weakly-supervised image parsing via constructing semantic graphs and hypergraphs. In
Proceedings of the ACM International Conference on Multimedia, pages 277–286. ACM, 2014.
[31] L. Zhang, Y. Gao, Y. Xia, K. Lu, J. Shen, and R. Ji. Representative discovery of structure cues for weakly-supervised image segmentation. Multimedia, IEEE Transactions on, 2014.
[32] L. Zhang, M. Song, Z. Liu, X. Liu, J. Bu, and C. Chen. Probabilistic graphlet cut: Exploiting spatial structure cue for
weakly supervised image segmentation. In Computer Vision
and Pattern Recognition, 2013 IEEE Conference on, pages
1908–1915. IEEE, 2013.
[33] L. Zhang, Y. Yang, Y. Gao, Y. Yu, C. Wang, and X. Li.
A probabilistic associative model for segmenting weaklysupervised images. 2014.
[34] W. Zhang, S. Zeng, D. Wang, and X. Xue. Weakly supervised semantic segmentation for social images. In CVPR,
pages 2718–2726, 2015.
[35] Y. Zhang, J. Wu, and J. Cai. Compact representation for
image classification: To choose or to compress? In CVPR
2014 IEEE Conference on, pages 907–914. IEEE, 2014.

RELATIVE LEARNING FROM WEB IMAGES FOR CONTENT-ADAPTIVE ENHANCEMENT
Parag Shridhar Chandakkar, Qiongjie Tian and Baoxin Li
School of Computing, Informatics and Decision Systems Engineering, Arizona State University
{pchandak,qtian5,baoxin.li}@asu.edu

ABSTRACT
Personalized and content-adaptive image enhancement can
find many applications in the age of social media and mobile computing. This paper presents a relative-learning-based
approach, which, unlike previous methods, does not require
matching original and enhanced images for training. This
allows the use of massive online photo collections to train
a ranking model for improved enhancement. We first propose a multi-level ranking model, which is learned from only
relatively-labeled inputs that are automatically crawled. Then
we design a novel parameter sampling scheme under this
model to generate the desired enhancement parameters for a
new image. For evaluation, we first verify the effectiveness
and the generalization abilities of our approach, using images
that have been enhanced/labeled by experts. Then we carry
out subjective tests, which show that users prefer images enhanced by our approach over other existing methods.
Index Terms— Content-adaptive image enhancement,
learning-to-rank, subjective evaluation testing.
1. INTRODUCTION
In today’s age of social media, it is becoming more important to capture good-looking photos. Due to the outreach of
social media sites, the photos get spread around quickly. It
is common to retouch the photo after capturing to improve
its appearance. The photo-retouching tools have made significant progress in recent years. There exist sophisticated
tools such as Adobe Photoshop as well as one-touch enhancement tools such as Picasa, Windows Live Gallery and Apple’s
auto-enhance. However, one-touch enhancement tools neither
offer personalization nor content-based image enhancement.
For example, an indoor image may need a different style of
enhancement than an outdoor image. Adobe Photoshop offers large variety of enhancement operations but can be complex and time-consuming for an amateur photographer. This
underlines the need for better and automated image enhancement tools. Enhancement operations are performed on various aspects of an image, e.g. saturation, contrast, brightness,
sharpness, etc. Hence the space of possible combinations
of enhancement parameters is huge. This work focuses on
content-based image enhancement by using content-similar

high-quality images as reference.
Training-based methods have recently been explored for
image enhancement, where pairs consisting of a low-quality
image and its enhanced counterpart are used for training
[1, 2, 3, 4]. The enhancements are done by expert users. Such
a training set allows learning of a regression/ranking function which maps the input feature to the optimal enhancement
parameters. For a regression function, it learns a mapping
between the input parameters (could be pixel values) to the
parameters in the corresponding enhanced image. The ranking function assigns a score to each feature vector. The enhancement parameter which gets the highest score is selected
as the best enhanced version of the input image. However,
such schemes do not scale well, owing to the need of expertenhanced training images. Per our knowledge, the largest
such publicly available training database is MIT-Adobe 5K
[5], consisting of 5 enhanced versions per image and 5000
images. In reality, we have millions of high-quality images
available on the Web which, if properly utilized, can improve
the performance significantly. Further, since the previous approaches need low-quality and its enhanced counterparts, it is
difficult to customize the system according to the individual’s
preferences. Our approach can handle a non-corresponding
pair of a low-quality image and a high-quality reference image. We can possibly retrieve popular images from a user’s
Flickr/Instagram account to customize the enhancement preferences. To the best of our knowledge, only our approach
considers both of the above aspects simultaneously.
It is a challenge to find optimal enhancement parameters
with non-corresponding pair of input and output images. The
visual features are not corresponding to each other to build a
regression or a simple ranking function. Usually, the optimal
parameters of low-quality image are explored near the parameter space of its enhanced counterpart. In this case, the search
for possible space of enhancement parameters is extremely
difficult due to non-correspondence of input and output. To
remedy this, we define a novel parameter sampling scheme
and a multi-level ranking model which uses simple visual features along with derived image parameter features (such as
brightness, contrast and saturation). We build a multi-level
ranking relation from the partial ordering available between
the visual feature and parameter vectors of low-quality input and high-quality reference images. A learning-to-rank

Feature
extraction

Low-quality
photo database

Relative ranking model
learning phase.
Enhancing a new image
using the learned model

Top K retrieved
images

High-quality
photo database

A new lowquality photo

Retrieval
module

Top K retrieved
images

Relative ranking
model learning

Learned model

Ranking score
generation

Output the version
having highest score

Generate enhanced
versions

Parameter
sampling

Fig. 1: Overview of our image enhancement algorithm.

approach has already been proposed in [1]. Unlike us, they
need the corresponding pairs of input and output images generated by an expert user along with a record of the intermediate enhancement steps. This limits the possible applications
of their approach as discussed before. We show superiority of
our method over one-touch enhancement tools and the stateof-art ranking-based enhancement approach [1].
2. RELATED WORKS
Purely image-processing-based and learning-based approaches have been developed for enhancing an image. In
this section, we mainly focus on learning-based approaches.
Common faces across the images have been used in [6] for
personal photo enhancement. They built a system to detect
examples of good and bad faces. Then the good faces were
used for enhancing the bad ones. However, their approach
lacks generality. A novel tone-operator was proposed in [7] to
solve the tone reproduction problem. In [2], a preliminary solution was proposed to enhance an image according to a user’s
profile. The result shows that the users’ preference can be
classified into different groups and also improved enhanced
results can be obtained by enhancing images according to
users’ choices. Local image enhancement was performed in
[3] by using local scene descriptors along with context. For
different scenes in the input image, similar image pairs are
retrieved. Then for each pixel in the input image, a set of pixels were retrieved from the data-set and used to improve the
given pixel. Then Gaussian random fields are used to maintain spatial smoothness in the enhanced image and improve
the perceptual quality of the image. MIT and Adobe collaborated to generated a large reference data-set which has 5000
input images and each has five enhanced versions, created by
five experts [5]. Using this database, they apply supervised
learning to predict a user’s adjustment and the preference for
a new user. They also analyze the difference in users’ preferences. In [4], the user preference was modeled based on the
image database. Users have to enhance some images to effectively train the model. The learned model was then applied to
obtain multiple enhanced versions for an image, according to
the user’s preferences. Image enhancement based on content

and scene semantics was done in [8]. Regions containing different objects were first detected. Then customized enhancement operators were applied in these regions. They concentrated more on personal photo collections and their content
detectors were limited to some objects such as people, sky
etc. The intermediate enhancement steps carried out by an
expert were recorded and used to train a ranking model in
[1]. They generated multiple enhanced versions by sampling
in the enhancement parameter space. The enhanced version
which obtained the highest ranking score was selected as the
final output. However, they require a lot of work from the
expert which is undesirable due to aforementioned reasons.
3. PROPOSED APPROACH
Our approach enhances an image depending on its content
and color composition. Fig. 1 shows the flow of our algorithm. We first perform CBIR to retrieve high-quality images similar to the low-quality input image. We then create
a ranking between the input image and the retrieved highquality images using extracted image features. To overcome
the shortcoming of not having one-to-one correspondence between the query and the retrieved high-quality images, we introduce an additional level in the ranking problem (detailed
later) to create a three-level ranking problem. For a new lowquality image, we generate multiple enhanced versions of the
input image by using a novel parammeter sampling scheme.
We select the version which gets highest rank from the model.
The individual steps of our approach are detailed below.
3.1. Data Collection and Processing
A good collection of high-quality images is essential for our
approach. We choose high-quality images from the database
published in [9]. It consists of photos from DpChallenge.com
and Photo.net among others. We select top 10% photos which
have more than 10 ratings. We then remove grayscale photos. DpChallenge.com has photos based on 66 themes ranging from “animals”, “food” to “portrait”, “water” etc. Our
database contains 1, 822 and 9, 467 photos from Photo.net
and DpChallenge.com respectively. We use bottom 10% rated
photos on DpChallenge.com as low-quality photos. We also
select photos from Flickr taken by old camera-phones such as
iPhone 3G, Samsung Galaxy II for low-quality photos.
We use an open-source CBIR engine called LIRE [10] to
retrieve K (=100) high-quality images for each low-quality
image. In the CBIR framework, we use opponent histogram
and auto color-correlogram features for color, PHOG descriptor and JPEG coefficient histogram to represent shape and image quality respectively. Before retrieval, we introduce a simple but effective step of auto-enhancement. It increases the
contrast of an image by saturating 1% of its data at low and
high intensities. Low-quality images usually have less contrast. Some of those images look either hazy or dark. There-

fore, the retrieved images do not correspond to the true colors in the low-quality image. This is avoided by first autoenhancing each low-quality image before retrieval.
3.2. Relative Ranking Model Learning
We formulate the problem of enhancing a low-quality image using other retrieved high-quality images as a multi-level
ranking problem. We now explain the general formulation of
our ranking approach. It is followed by the proposed multilevel ranking and the formation of feature vectors.
Suppose Q and D are the sets of query images and all
high-quality images respectively. The retrieved images of the
given image, Ii ∈ Q, are denoted as Ri ⊆ D. We assume
that images in Ri are of better quality than Ii . We construct
relative ranking pairs, say (Ii , Ri,j ) where Ri,j is the j th image in Ri . These pairs are now used for building a simple
two-level ranking model. We use visual features along with
the parameter feature vectors in our ranking model, while [1]
uses only visual features. The parameters help us to capture
hidden characteristics of an image. For example, the contrast
of an image depends on the spatial arrangement of the various colors. Saturation represents the purity of the color in
a different dimension. The combination of features and parameters helps compare seemingly unrelated input and output
images in a similar feature space. Let us denote the visual features extracted for Ii as Vi and the feature vector for the k th
parameter as Pik . Then the ranking model is given as follows:
min

1 2 X
ω +
ξi,j
2
i,j

s.t.

ω T fi ≤ ω T gi,j − ξi,j

∀ i, j

(1)

ξi,j ≥ 0
[Vi , Pi1 , . . . , Pik ]

where fi =
and gi,j = [VRi,j , PR1 i,j , . . . ,
k
PRi,j ].
The above ranking model concatenates all the feature and
parameter vectors to get a score for an image. However, assume a situation where we have an image with a slightly
higher level of saturation and brightness but with less contrast. Since the above ranking model considers all the elements together, the image may obtain a high score. Thus we
propose a modified ranking model in which we concatenate
visual feature and one parameter at a time.
1 2 X
min
ω +
ξi,j
2
i,j
s.t.

n
n
ω T fin ≤ ω T gi,j
− ξi,j
n
ξi,j

∀ i, j, n and n = {1, . . . , k}

≥0

(2)
n
where fin = [VIi , Pin ] and gi,j
= [VRi,j , PRni,j ].
In Equation 2, we would get as many constraints as the
cardinality of n. In other words, it will be equal to the total number of parameters. The final score depends on the

combination of visual feature and each parameter. An image gets a high score only if all of its parameters are in balanced amounts. Moreover, the visual feature stays common
in all of the inequality constraints since the parameters are
dependent and changing one of them affects all of them. A
common visual feature ensures that only a balanced featureparameter combination defines a high-quality image. All parameter feature vectors, [Pi1 , . . . , Pik ], are required to have the
same length in order to construct such a model.
3.2.1. Multi-level ranking
We enhance the low-quality images using the high-quality
ones stored on photo-sharing websites. Unlike in [1], we
neither possess corresponding pairs of original and enhanced
images nor the record of intermediate steps carried out by an
expert during an enhancement process. The external highquality images have high contrast, brightness and saturation.
Therefore, the ranking model generated using Equation 1 or 2
has only learned that high values can generate a high-quality
photograph. The model lacks the knowledge that extremely
less or high values of parameters (e.g. brightness, saturation
and contrast) can degrade the image quality significantly.
To incorporate this knowledge, we propose multi-level
ranking. We manually vary brightness, contrast and saturation for 20 images in our database (one parameter at a time)
till their quality degrades significantly. The variation happens
on both the extremes. For the rest of the images, we automatically vary the parameters to generate 8 degraded versions for
each low-quality image. The amount of variation in the parameters is determined empirically using these 20 images.
Let us denote the mth corresponding degraded image for
a low-quality image, Ii , by Bim . The visual feature vector and
the k th parameter vector of Bim is denoted by VBkm and PBk m
i
i
respectively. We use Bim , Ii and Ri,j to build a three-level
ranking model such that Bim < Ii < Ri,j ∀ {i, j, m}.
The ranking model can be formulated as follows:
X
1 2 X
0
ω +
ξi,j +
ξi,m
min
2
i,j
i,m
(3)
T m
0
s.t.
ω hi − ξi,m ≤ ω T fi ≤ ω T gi,j − ξi,j
0
ξi,j ≥ 0, ξi,m
≥0

∀ i, j, m , m = {1, . . . , 8}

where fi = [Vi , Pi1 , . . . , Pik ] and gi,j = [VRi,j , PR1 i,j , . . . ,
1
k
m
PRk i,j ] and hm
i = [VBi , PBim , . . . , PBim ]. The approach using above ranking model is termed as approach-3176 since
the feature space generated using this approach is 3176-D .
Similarly, Equation 2 can be converted into a three-level ranking model and we call that approach as approach-2744. All
the ranking formulations are solved using ranking-SVM [11].
3.2.2. Feature and parameter vectors for ranking
The entire feature vector includes a 2600-D HSV histogram
(visual feature) and four 144-D parameter vectors represent-

ing brightness, contrast, saturation and sharpness.
HSV histogram: We divide saturation and value uniformly
into 10-bins each. Hue is divided non-uniformly into 27 bins,
based on the distance between the hues in the CIELAB color
space. That distance is given by the CIEDE2000 metric which
considers perceptual uniformity. The reader is pointed to [12]
for the representation of the formula and its implementation
details. We form a 2-D grid where saturation and brightness
varies from 0 to 1 in a grid and hue varies across different
grids, in steps of 5. We measure the CIELAB distance between the corresponding points on two grids. The maximum
distance between two grids gives us a rough measure of the
distance between all possible shades of these two hues. Thus,
dmn = max ∆E(GHm (i, j), GHn (i, j))
ij

(4)

where GHm and GHn are the grids for the mth and nth hue
respectively. ∆E is the CIEDE2000 color difference metric.
dmn provides the largest possible distance between all shades
of hue m and n. Our aim is to make a separate bin for a
hue which is significantly different from the previous hue. To
achieve this, we keep m constant and vary n till d crosses a
pre-determined threshold (=7). Once this threshold has been
reached, we assign the value of n to m and repeat the process
till m reaches 360. We obtain 27 bins for hue. Due to the proposed binning, the HSV histogram captures more details in
an image by using relatively less number of bins. Separation
between two hues ranges from 6◦ to 40◦ in this binning.
Contrast: We propose to measure local RMS contrast. The
RMS contrast of an M × N image I is defined as the standard
deviation of pixel intensities as follows,

CRM S

v
u
−1
−1 M
X
u 1 NX
(I − I¯ij )
=t
M N i=0 j=0

(5)

We first divide the image into 12 × 12 grid to capture the
local characteristics of an image. Each grid is subdivided into
blocks of 8 × 8 pixels. We measure the RMS contrast of these
blocks and average the contrast values inside a grid. This
gives a 144-D vector (12 × 12 = 144) for each image.
Brightness and Saturation: We divide the image into 3 × 3
grid. For each grid, we calculate a 16-bin histogram of V
and S-channel. This gives us two 144-D (16 × 9 = 144)
histograms as brightness and saturation features.
Sharpness: We adopt the approach mentioned in [13] to measure sharpness. Instead of calculating the sharpness metric for
the entire image, we divide the image into 3 × 3 grid and for
each grid, we calculate its sharpness as the ratio of area of
high-frequency components to the total area of the grid,
msharp =

||H||
,
||I||

(6)

where ||H|| = {(u, v)| |F (u, v)| > θ} and F is the FFT of
image I. θ is a pre-defined threshold. By varying θ, one can
decide the sharpness level of an image, at which the metric
should start responding. For example, msharp will produce
large values even for a blurred image when θ is kept small
and vice-versa. We define θ as a monotonically-increasing
16-dimensional vector. θ increases on a log-scale from 0 to 1.
The ith bin of the sharpness histogram is defined as,
msharpi =

||Hi ||
||I||

for i = [1, ..., 16],

(7)

where ||Hi || = {(u, v)| |F (u, v)| > θ(i)}. msharp is calculated for each grid to produce a 144-D sharpness feature.
3.3. Algorithm for enhancing a new image
The relative ranking model is now capable of assigning a
score to an enhanced version of an image depending on its
color composition and content. However, generating these enhanced versions is not straightforward due to the large number
of possible enhancement parameter combinations. We employ a novel parameter sampling scheme based on the ranking
scores to reduce the search space as follows.
We retrieve 100 most similar images to the new image by
using the CBIR module defined in Section 3.1. We then calculate scalar values for saturation, brightness and contrast of
the retrieved images, denoted by sR , bR and cR respectively.
We also define the terms sN , bN and cN for the new image
equivalently. The average saturation and brightness is calculated as the mean of S and V channel of an image in HSV
color space, respectively. The average contrast is the standard
deviation of the RGB pixel values. Next step is to calculate
the ranking scores of those retrieved images by multiplying
the learned model w with the feature vectors of all the retrieved images. The ranking score for the ith retrieved image
is denoted by scri . The procedure of calculating scr for the
approach-3176 and 2744 is slightly different as follows:
scr3176 = wT ∗ f,

scr2744 =

k
1 X T
w ∗ fj
k j=1

(8)

where f and k are the feature vector of a retrieved image and
number of parameters respectively. Note that since the features for both the above approaches are different, the learned
models have a different structure (3176-D vs. 2744-D).
We non-uniformly sample around the values of sR , bR and
cR using the obtained ranking scores. Dense (sparse) sampling is performed around those values of sR , bR and cR for
which we have obtained high (low) scores. A high score indicates that the image is visually appealing and similar to the
new image. After parameter sampling, we try to steer sN , bN
and cN towards the sampled values. However, the low and
high-quality images are not counterparts of each other and
hence we stop steering sN , bN and cN towards the sampled

values if the percentage change is more than a certain threshold. It is determined empirically to be ±20% for average
brightness and saturation and ±4% for average contrast. Using this procedure, we generate anywhere from 150 to 250 enhanced versions of an image. Ranking scores are calculated
for all these enhanced versions as well as the original one.
The image with the highest score is presented to the user.

Fig. 2: Average result of our subjective evaluation study.

4. EXPERIMENTS AND RESULTS
We carried out two-fold assessment of our algorithm. Firstly,
as a verification step, we evaluated our ranking model on
manually enhanced and degraded images. We selected 500
images from MIT-Adobe FiveK dataset [5]. Each image in the
data-set has been enhanced by five experts. We also generated
8 degraded versions for every image. We calculated ranking
scores for original images, its enhanced and degraded counterparts. For 90.19% (451/500) images, at least one enhanced
version got a higher score than the original image. For 100%
of the images, at least one bad version obtained less score than
the input image. For 80.27% (401/500) of the images, at least
7 out of 8 bad versions got a lower score than the input image.
For a more robust performance assessment, we carried out
a subjective evaluation test. To this end, we formed a testing
set with 127 images which is disjoint with the training set.
Our low-quality image data-set contains 77 images. The remaining 50 images were selected at random from the 94 image database provided to us by the authors of [1]. Our subjective evaluation test involved 33 users. Each user was shown
a pair of images and was asked to choose the better photograph. In case a user preferred both photos equally, a third
option of voting to both of them was made available. There
was no time-limit and users could take breaks in between if
they were fatigued. The tests were done on the same type
of monitor and the lighting conditions as well as the sitting
arrangements were identical for all the tests. The order between different pairs and between the images of a pair was
kept random. No indication regarding the type of enhancement method used was given to the user.
The subjective evaluation test consisted of five sessions.
Users were asked to compare approach-2744 and 3176 to Picasa in the first two sessions. In the next two sessions, we enhanced the 50 images using these two approaches and asked
users to compare them with the approach of [1]. Finally, we
skipped the auto-enhancing step mentioned in the Section 3.1
and also employed a simple two-level ranking (instead of the
three-level ranking) as described in Equation 1 to formulate
a new but presumably inferior approach. It is now used to
enhance 25/77 images, chosen at random. In the fifth session, users compared these 25 images with Picasa. The last
session aims to explain the need of multi-level ranking and
pre-processing step of auto-enhancement before retrieval. In
total, each user compared 279 (= 77 ∗ 2 + 50 ∗ 2 + 25) pairs
of images. Each user took anywhere from 30-45 minutes to

complete the test.
Fig. 2 shows the cumulative votes given to each approach
in every session. The votes are accumulated from all users
over all the images. The method which was selected more
number of times in a session was favored by the users for that
session. Users consistently pick photos enhanced by our approach over Picasa and the approach of [1]. Interestingly, the
results of the fifth session favor Picasa, implying the need for
multi-level ranking. The numbers above each bar represent
the average rank obtained for each approach, lower rank implying better quality for the image. It is calculated as the average of all the vote ratings (from all users) to that approach.
Fig. 3 shows example results from session 1, 2 (row 1,
2, 5), 3 (row 3, 4) and 5 (row 6). Similar to [1], even we
observed that Picasa is conservative while enhancing photos.
Picasa concentrates on conservatively adjusting the brightness
and contrast. However, images in the second row illustrate
that the conservative adjustment does not suit all the images.
Our approach converges on better enhancement parameters
by performing content-adaptive enhancement. For example,
outdoor images may need significant changes in their composition while quality of indoor images may be harmed by
doing so. On the other hand, the approach of [1] significantly
changes the original photo as shown in the middle two images
in row 3 and 4. However, in this process, sometimes large
variation happens in the amount of contrast and saturation,
which harms the quality of the photo. We avoid this by introducing multi-level ranking and the novel parameter-sampling
strategy. Approach of [1] enhances the original photo based
on the parameters of K-nearest high-quality images (in terms
of L2 distance) alone. We sample our parameters based on Knearest images and more importantly using the ranking score.
We also set an upper limit to the amount of variation in the
enhancement parameters. Thus our parameter sampling takes
into account the color composition, content and quality of the
retrieved images. The importance of these steps is clearly
seen in the images in the last row. The rightmost image is
over-saturated since the model lacks any knowledge about the
bad effects of setting extreme values for image parameters.
Our algorithm prefers more significant adjustments than
Picasa. The reason for such a preference stems from our training database. We have total 11, 289 images from DPChallenge.com and Photo.net. Most of the images are vibrant in
colors, with high-contrast and saturation. Though many participants in our study tend to choose high-contrast and satura-

Acknowledgement: The work was supported in part by a
grant (1135616) from the National Science Foundation. Any
opinions expressed in this material are those of the authors
and do not necessarily reflect the views of the NSF.
6. REFERENCES
[1] Jianzhou Yan, Stephen Lin, Sing Bing Kang, and Xiaoou Tang, “A learning-to-rank approach for image
color enhancement,” in IEEE CVPR, 2014.
[2] Juan C Caicedo, Ashish Kapoor, and Sing Bing Kang,
“Collaborative personalization of image enhancement,”
in IEEE CVPR, 2011, pp. 249–256.
[3] Sung Ju Hwang, Ashish Kapoor, and Sing Bing Kang,
“Context-based automatic local image enhancement,” in
ECCV, pp. 569–582. Springer, 2012.
[4] Sing Bing Kang, Ashish Kapoor, and Dani Lischinski, “Personalization of image enhancement,” in IEEE
CVPR, 2010, pp. 1799–1806.
Fig. 3: Comparison of image enhancement methods. Top two
rows, our data: left: original image, middle: enhancement
by Picasa, right: approach-2744. Row 3 and 4, data of [1]:
left:original image, middle: enhancement result of [1], right:
approach-2744. Row 5, our data: From L to R, 1. original image, 2. enhancement by Picasa, 3. approach-3176, 4.
approach-2744. Last row, our data: From L to R, 1. original image, 2. enhancement by Picasa 3. multi-level ranking,
approach-3176 4. two-level ranking, approach-3176.
tion photos, it is possible that some users prefer washed-out
or dark photos. Such personalized preferences can be learned
by our model by training on users’ Flickr or Instagram feed.
5. CONCLUSION
We presented a novel learning-based framework for image
enhancement. It uses CBIR to perform content-adaptive enhancement of low-quality images. A multi-level relative ranking model is trained with the help of high-quality images on
the web. We show that instead of concatenating all features,
considering them pairwise in the ranking model creates better enhanced images with all of its parameters in balanced
amounts. We propose a novel parameter sampling scheme
to reduce the huge search space and converge onto better
enhancement parameters. We verified the effectiveness of
our framework by checking its performance on MIT-Adobe
FiveK dataset. For a more robust comparison, we carry out
subjective evaluation tests and show that users prefer photos
enhanced by our framework over others. Our framework offers scalablity and personalization since it directly uses highquality image databases from the web.

[5] Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and
Frédo Durand, “Learning photographic global tonal adjustment with a database of input / output image pairs,”
in IEEE CVPR, 2011.
[6] Neel Joshi, Wojciech Matusik, Edward H Adelson, and
David J Kriegman, “Personal photo enhancement using
example images,” ACM Trans. Graph, 2010.
[7] Erik Reinhard, Michael Stark, Peter Shirley, and James
Ferwerda, “Photographic tone reproduction for digital
images,” in ACM Trans. Graph, 2002.
[8] Liad Kaufman, Dani Lischinski, and Michael Werman, “Content-aware automatic photo enhancement,” in
Computer Graphics Forum. Wiley Online Library, 2012.
[9] Ritendra Datta, Jia Li, and James Ze Wang, “Algorithmic inferencing of aesthetics and emotion in natural images: An exposition,” in IEEE ICIP, 2008, pp. 105–108.
[10] Mathias Lux, “Content based image retrieval with lire,”
in 19th ACM Multimedia, 2011, pp. 735–738.
[11] Thorsten Joachims, “Training linear svms in linear
time,” in Proceedings of the 12th ACM SIGKDD, 2006.
[12] Gaurav Sharma, Wencheng Wu, and Edul N Dalal, “The
CIEDE2000 color-difference formula: Implementation
notes, supplementary test data, and mathematical observations,” Color Research & Application, 2005.
[13] Yiwen Luo and Xiaoou Tang, “Photo and video quality evaluation: Focusing on the subject,” in Computer
Vision–ECCV 2008, pp. 386–399. Springer, 2008.

Robust Integration of Multiple Information Sources by View Completion
Shankara Subramanya, Baoxin Li and Huan Liu
Department of Computer Science and Engineering
Arizona State University, Tempe AZ-85287 USA
{shankarbs, baoxin.li, huan.liu}@asu.edu

Abstract

performance.
In all the above examples, the object being observed is
characterized by multiple sets of features. In the webpage
classification example, the terms from the body text yield
one set of features and the terms from the anchor text
yield another set of features. Even though the features
from each source are different, the feature sets represent
the same underlying object instances and hence would be
semantically related. For example, features derived from
the body text and the anchor text of a webpage will most
likely be semantically related since both sets of features
are representing the same webpage. The anchor text would
most likely contain terms which describe the webpage,
which in turn is defined by the body text present in it. Each
of the multiple sets of features can be considered a ’View’
and each object is characterized by multiple independent
views.

There are many applications where multiple data
sources, each with its own features, are integrated in order
to perform an inference task in an optimal way. Researchers
have shown that for many tasks like webpage classification,
image classification, and pattern recognition, combining
data from multiple information sources yields significantly
better results than using a single source. In these tasks each
of the multiple data sources can be thought of as providing
one view of the underlying object. However in many domains not all of the views are available for the available
instances; some of the views would be missing. This problem of missing views affects the performance of the machine
learning task. In this paper we provide a method of view
completion to heuristically predict the missing views. We
show that with view completion we are able to achieve significantly better results. We also show that by considering
the information at a higher level in terms of views rather
than considering them at a lower level in terms of features
we are able to achieve better results. We demonstrate this
by comparing our method with existing methods which consider the missing views problem as a missing value problem.

1

Researchers have devised various ways of combining
multiple views to achieve higher accuracy. In [8], text
and image views were combined using a fusion SVM
classifier. In [2], multiple views of a webpage were
combined using a density based method. [11] combined
multiple text and image views of a webpage using LSI. All
these methods use the fusion of multiple views to achieve
higher accuracy. Multiple views have also been used to
improve classification performance using co-training [3].
Co-training improves classification learning by enforcing
internal consistency among predicted classes of unlabeled
objects based on different views. The idea behind these
methods of combining multiple views is that the views
though semantically related and possibly overlapping,
provide useful complementary information.

Introduction

Combining data from multiple information sources
has received significant attention in recent years. Many
researchers have shown that using multiple information
sources is significantly better than using a single information source. In an application of battle field surveillance,
for example, information from infrared sensors, video
feeds, and Laser range finder can be combined for object
recognition. For webpage classification, anchor text,
images, and body text of a webpage can be combined and
used. For classifying images, images as well as the text
occurring with each image can be combined. Multiple
information sources yield complementary information
about the underlying object which results in improved

IEEE IRI 2008, July 13-15, 2008, Las Vegas, Nevada, USA
978-1-4244-2660-7/08/$25.00 ©2008 IEEE

2

Missing Views

One problem in combining multiple views is that in
many domains not all the views are available for the

398

data integration scenario is different from the missing value
problem. In the missing view problem, a complete feature
set from a particular source would be unavailable. Hence,
it would be not only possible but also desirable to deal with
higher-level “features” in terms of views. We compare the
view completion method with the missing value methods
later in Section 7.
The problem of missing views has been considered previously by [6] in an active learning setting. They discussed
the problem of deciding which additional features need
to be acquired for an incompletely characterized object in
order to improve the performance of the classifier. This
approach of active learning is applicable only in the cases
where active acquisition of features is possible. However in
many real world scenarios it would be impossible to acquire
additional features for some incompletely characterized
objects. For example in an application using sensors, a
missing view due to a failed or obstructed sensor would be
impossible to acquire at a later stage as all the incoming
data would be real time data. If a webpage does not have
body text, anchor text, or images, then active learning
would be of little use since there is no data in the source
itself to acquire. So in data fusion such missed views are
usually ignored though the missing views affect the final
classification accuracy.

Figure 1. Schematic description - Missing
view problem
available instances. Some of the instances will be incompletely characterized and only a subset of the views will be
available for them. In the webpage classification example
some of the webpages may not have any body text in them
(for example, webpages containing images only). In this
case the particular webpage will have the ’body text view’
missing. Another example of missing views occurs in the
domain of battle field surveillance. In many surveillance
applications a large number of cheap sensors are usually
employed to avoid high cost. These sensors could be prone
to frequent failures. When a sensor fails, the view from that
particular sensor would become unavailable. The problem
of missing views is represented schematically in Figure 1.

To overcome these problems, in the following sections
we propose a method for completing the missing views
based on canonical correlation analysis. We first define
formally the problem of View Completion. We then
propose an algorithm to select the closest matching value
for the missing view using the views which are not missing
and observed. We next show experimentally that with this
view completion method we are able to achieve significant
improvement in the classification accuracy.

This missing view problem is common to many domains. In all these scenarios the missing view will affect
classification performance. In this work we address the
problem of heuristically predicting missing views to obtain
a complete characterization of the object. We then show
experimentally that with this complete characterization we
are able to achieve better classification performance.

4
3

Related Work

View Completion

We define the multi-source model as follows. Each
object is represented by two or more views. For any
given object zero or more views may be missing. But for
every object at least one view will be present. We also
assume that typically there is a set of objects which have
the class labels and there is a larger set of unlabeled data.
Each view is represented by a set of feature vectors. This
model is similar to the co-training model with the additional detail that some missing views of objects are missing.

The analysis of data with missing values has for long
been a well studied problem in statistics [7]. Imputation
methods like maximum likelihood imputation and median
imputation have been recommended to deal with the
missing value problem. Schafer and Graham [9] provide a
good survey of the methods for dealing with the missing
value problem. The authors in [1] evaluate and compare
the effect of different imputation methods which deal
with missing values on classification accuracy. All these
methods for handling missing values however work with
the lower level features. The problem of missing views in a

An object with n views can be represented as an (n+1)tuple. If f1 , f2 , ....fn represent the n views and c the corresponding class label, then the instance in is represented

399

where Fx and Fy are the two sets of variables, and
Wx and Wy are the basis vectors onto which Fx and Fy are
projected, respectively. The equation for ρ can be rewritten
as
(Fx ·Wx ,Fy ·Wy )
ρ = maxWx ,Wy (||Fx ·W
x ||·||Fy ·Wy ||)

as
in = (f1 , f2 , ..., fn , c)
Specifically, an object instance with two views can be
represented as a 3-tuple. If X and Y represent feature sets
corresponding to two different views of an object. Each
instance i is defined as follows.

The problem of finding ρ is therefore an optimization
problem with respect to Wx and Wy . This optimization
problem can be formulated as a standard Eigen problem [4]
which can be easily solved. Since Wx and Wy are always
calculated to maximize the correlation of the projections,
CCA is independent of the original coordinate system
unlike other correlation analysis techniques. There may
be more than one canonical correlation, each representing
orthogonally separate pattern of relationship between the
two sets of variables. The correlation for the successively
extracted canonical variates are smaller and smaller. When
extracting the canonical correlation the eigen values are
calculated. The square root of the eigen values can be
interpreted as the canonical coefficients. Corresponding
to each canonical correlation the canonical weights for
each of the variable in the data set is calculated. The
canonical weights represent the unique positive or negative
contribution of each variable to the total correlation.

i = (x, y, c), where x ∈ X, y ∈ Y
Here x ∈ X is a vector corresponding to features from first
source, y ∈ Y is a vector corresponding to features from
second source and c is the class label. Either one of x or y
can be ∅.
Let Xp and Yp represent the feature sets corresponding
to those instances which have features from both the views
present. Let Xym ∈ X be the set of features corresponding
to instances which have the other view missing, i.e.,
the corresponding Yym = ∅. Let Iym be the set of all
such instances. Our goal is now to find for each of such
instances, the view yym ∈ Yym using the available view
xym ∈ Xym and the paired views from other instances
Xp and Yp . Similarly let Ixm represent the instances with
the first view missing. Let Xxm = ∅ and Yxm ∈ Y
represent the two views of these instances. We can then
find xxm ∈ Xxm using yxm ∈ Yxm , Xp and Yp .

CCA has been used previously by researchers to find the
semantic relationship between two multimodal inputs. In
[4] kernel CCA was used to find correlation between image and text features obtained from a webpage and used
it for content based image retrieval. [10] used CCA to
find the language independent semantic representation of a
text by using English text and its French translation as two
views. When two multidimensional variables represent the
two views of the same object, then the projections found
by CCA can be thought of as capturing the underlying semantics of the object. In other words we can say that in the
semantic feature space, the different views of the object are
highly correlated. So to acquire a missing view of an object
we can select the closest match from the observed views of
other objects, such that it has the maximum correlation with
the non-missing views of the current object, in the semantic
feature space. In the next section we present the procedure
for view completion using CCA.

To accomplish this, we develop a method of view
completion which heuristically predicts the missing view(s)
of the objects. Since this method uses only the available
views and not the class label, it can be used on both the
labeled and unlabeled data. To predict the missing view
from the view which is available we first need to find
the semantic relationship between the views. To find this
semantic relationship we use the statistical technique of
Canonical Correlation Analysis.

5

Canonical Correlation Analysis (CCA)

CCA attempts to find basis vectors for the two sets of
variables such that the correlation between the projections
of the variables onto these basis vectors is mutually
maximized [5]. The correlation between the two sets of
variables may not be visible in their original coordinate
system. CCA tries to find a linear transformations for two
sets variables such that in the transformed space they are
maximally correlated.

6

View Completion Procedure

Let CCA(p, q) denote the canonical correlation analysis
of vectors p and q which gives the basis vectors and the projections of p and q on the basis vectors. The basis vectors
can be considered as representing the lower dimensional
semantic feature space which captures the underlying
semantics of the object. Therefore to find yym we can
select yp ∈ Yp , which has the highest correlation with xym

The canonical correlation between any two data sets is
defined as
ρ = maxWx ,Wy corr(Fx · Wx , Fy · Wy )

400

Data sources
used
Without View
completion
With View
completion

in the lower dimensional semantic feature space. Using
these notations the procedure for the view completion is as
follows.
1. Perform the canonical correlation analysis between
Xp and Yp and find the basis vectors.
[A, B, U, V ] = CCA(Xp , Yp ) ,
U and V are the matrices where the columns represent
the basis vectors corresponding to Xp and Yp respectively
A and B are the projection of Xp and Yp onto U and
V respectively

Anchor

0.38+0.02
0.29+0.027

0.135+0.017
0.135+0.017

Body +
Anchor
0.085+0.013
0.07+0.016

All 7
sources
0.065+0.017
0.055+0.016

Table 1. Classification error rate on web dataset

7.1

Experiment with different numbers of features

We evaluated the classification accuracy on the original
dataset with and without view completion. The experiments
were performed on the following four cases.

2. For each instance i ∈ Iym , i = (xym , yym , c), where
xym ∈ Xym and yym ∈ Yym ,
Project each yp ∈ Yp onto V and the feature set xym
onto U
p = yp ∗ Vk
q = xym ∗ Uk
where Uk and Vk are obtained by selecting top k basis
vectors from U and V respectively.

1. Body text view alone: In this setting only body text
view was used for classification. For the completion of
body text view, the anchor text view was used.
2. Anchor text view alone: In this setting only anchor text
view was used for classification. For the completion of
anchor text view, the body text view was used.

3. The Pearson correlation cor between p and q is then
calculated
cor = correlation(p, q)

3. Anchor text and body text features: In this setting both
body and anchor text views were used for classification. And each of those views was used for the completion of the other.

4. Select yp with the maximum value for cor as
ymax . Set yym = ymax and update the instance
i = (xym , ymax , c)

4. All seven views:In this setting all the seven views were
used for classification. The anchor view was used for
the completion of all the other views except URL.
URL was present for all the webpages and hence did
not need to be completed). The completion for anchor
view was done using body view.

5. Repeat the procedure to find missing features xxm for
instances i ∈ Ixm , i = (xxm , yxm , c)
Though the above mentioned procedure is for object instances which have two views, it can be easily extended to
instances with n views, n > 2 by doing a pairwise view
completion. For example, in an instance of n views in =
(f1 , f2 , ..., fn , c), if, say, f1 is missing in the least number
of object instances, then all the other views f2 , ..., fn could
be completed using the above method by performing pairwise comparison with f1 .

7

Body

Body and anchor views were selected among the seven
views for the first three experiments, since these two are the
most commonly used among the seven views for webpage
classification and the most intuitive. The anchor view
was used for view completion of all the other views since
anchor view was missing among least number of object
instances except for URL and it is more semantically
related to all the other views compared to the URL view.
The integration of the multiple views were done using the
density based method proposed by [2]. The classifications
were done using the SVM classifier with a linear kernel.

Experiments and Results

The above procedure for view completion was run on
the adult website classification data set used by [2]. The
data set used contains seven different sources of data for
classifying a webpage. The seven sources obtained based
on the HTML tags of the webpages are BODY, Anchor Text
and HREF(A), Image and ALT(Img), TITLE, METADATA,
TABLE, Webpage URL(URL).

Table 1 gives the results of the experiment. This experiment is used to show the possibilities of view completion:
using only a single view for classification (as given in first
two columns) does not give good classification results, but
completing them can reduce error rates significantly. Also

401

7.3

In the third part of the experiment, the number of instances with missing views in Anchor text view was incrementally increased and the classification accuracy was evaluated. Since the anchor text and body text features are represented by term counts, if any of the view is missing, the
term counts for that particular instance would be zero. So
to get an additional mi instances with missing anchor text
view, we randomly select mi instances which have anchor
text view and then set the vector corresponding to anchor
text view to zero. Table 3 and the Figure 3 give the results of the experiment. From the results we that over the
whole range of instances with missing views applying view
completion gives a consistently better classification accuracy compared to having no view completion at all. The
figure also shows the results of KNN Imputation, Mean Imputation and EM Imputation. These imputation methods
consider each missing value individually and substituting a
value for it. From the figure it is clear that view completion
which addresses the problem at a higher level in terms of
views rather than at a lower level in terms of values, performs much better than the other methods.

Figure 2. Effect of number of basis vectors
Number of
basis vector
10
20
30
40
50
60
70
83

Error rates
0.07+-0.0186
0.075+-0.0112
0.05+-0.0129
0.055+-0.0138
0.045+-0.0117
0.055+-0.0138
0.05+-0.0129
0.06+-0.0145

Table 2. Effect of number of basis vectors

8

there may be scenarios where the multiple views are present
only for a small amount of data used for training, but only
one single view is present for the rest of the unlabeled data.

Conclusions

We identify the need for studying the problem of missing
views in the domain of multi-source data integration. We
formally define the problem of missing views and we
propose and efficient algorithm for view completion. By
dealing with the missing view problem at a higher level in
terms of views instead of a lower level representation of
features we are able to achieve better results. By using the
underlying existing semantic relationship between multiple
views we propose a heuristic algorithm for view completion
using CCA. Experiments on the web classification dataset
demonstrates the advantages of our method.

In the first column we see that after view completion the
error rate for classification using body view has reduced by
9% with view completion. When anchor view was used,
there was no change in the error rates. This was not surprising as the number of instances with anchor view missing
was less in the beginning itself. So there was no additional
gain with view completion. When both body and anchor
views were used for classification a gain of 1.5% in accuracy was observed. Finally when all the seven sources were
used for classification we observed that the error rate reduced by 1% and the accuracy increased from 93% to 94%.
In all the above cases, the number of basis vectors was fixed
to get the least error rates as described below.

7.2

Effect of missing views

The view completion method can be applied whenever
the multiple views representing an instance are correlated.
However in some application domains where the views are
completely complementary and orthogonal, or when the
views are completely uncorrelated, view completion would
not be of much help. Our future work includes extending
the algorithm using kernel methods to handle nonlinear correlation and developing algorithms to handle non-numeric
attributes. We also plan to devise formal tests to decide
when view completion procedure would be useful.

Effect of numbers of basis vectors

In this experiment the number of basis vectors was varied
as mentioned in Step 2 of Section 6. The effect of numbers
of basis vectors on view completion was tested with different numbers of basis vectors. The experiment was carried
out using the Body view + Anchor view setting. The results
are presented in the table 2 and the figure 2. We see that the
least error rate was obtained with k = 50.

Acknowlegments
This work is in part sponsored by AFOSR via a grant
(No. FA9550-08-1-0132).

402

Figure 3. Effect of missing views
No. of
instances missing
anchor view
13
20
30
40
50
60
70
80
90
105
150

Error rate
No view completion

Error rate
With view completion

Error rate
KNN Imputation

Error rate
Mean Imputation

Error rate
EM Imputation

0.085+-0.013
0.105+-0.019
0.12+-0.019
0.16+-0.014
0.185+-0.026
0.21+-0.026
0.405+-0.033
0.31+-0.047
0.37+-0.03
0.37+-0.05
0.385+-0.049

0.045+-0.012
0.07+-0.0152
0.075+-0.017
0.145+-0.028
0.125+-0.029
0.185+-0.018
1.0.265+-0.039
0.235+-0.029
0.275+-0.038
0.29+-0.026
0.31+-0.029

0.08+-0.015
0.085+-0.015
0.455+-0.051
0.465+-0.044
0.46+-0.052
0.415+-0.05
0.465+-0.044
0.465+-0.044
0.405+-0.051
0.375+-0.052
0.36+-0.043

0.175+-0.028
0.26+-0.058
0.235+-0.056
0.22+-0.027
0.225+-0.054
0.24+-0.031
0.25+-0.035
0.325+-0.039
0.315+-0.04
0.305+-0.037
0.32+-0.034

0.49+-0.013
0.49+-0.013
0.49+-0.013
0.49+-0.013
0.5+-0.033
0.5+-0.033
0.5+-0.033
0.5+-0.033
0.5+-0.033
0.5+-0.033
0.5+-0.033

Table 3. Webpage classification error rate - Varying number of instances with missing views

References
[7]

[1] E. Acuna and C. Rodriguez. The treatment of missing values and its effect in the classifier accuracy. Classification,
Clustering and Data Mining Applications, pages 639–648,
2004.
[2] N. Agarwal, H. Liu, and J. Zhang. Blocking objectionable
web content by leveraging multiple information sources.
SIGKDD Explor. Newsl., 8(1):17–26, 2006.
[3] A. Blum and T. Mitchell. Combining labeled and unlabeled
data with co-training. In COLT: Proceedings of the Workshop on Computational Learning Theory, Morgan Kaufmann Publishers, 1998.
[4] D. R. Hardoon, S. R. Szedmak, and J. R. Shawe-taylor.
Canonical correlation analysis: An overview with application to learning methods.
Neural Computation,
16(12):2639–2664, 2004.
[5] H. Hotelling. Relations between two sets of variates.
Biometrika, 28(312-377), 1936.
[6] B. Krishnapuram, D. Williams, Y. Xue, L. Carin, M. A. T.
Figueiredo, and A. J. Hartemink. Active learning of features and labels. Workshop on learning with multiple views

[8]

[9]

[10]

[11]

403

at the 22nd International Conference on Machine Learning
(ICML-05), pages 43–50, 2005.
R. J. A. Little and D. B. Rubin. Statistical analysis with
missing data. John Wiley & Sons, Inc., New York, NY,
USA, 1986.
B. Rafkind, M. Lee, S.-F. Chang, and H. Yu. Exploring text
and image features to classify images in bioscience literature. In Proceedings of the HLT-NAACL BioNLP Workshop
on Linking Natural Language and Biology, pages 73–80,
New York, New York, June 2006. Association for Computational Linguistics.
J. L. Schafer and J. W. Graham. Missing data: our view of
the state of the art. Psychological methods, 7:147–77, June
2002. PMID: 12090408.
A. Vinokourov, J. Shawe-Taylor, and N. Cristianini. Inferring a semantic representation of text via cross-language correlation analysis, 2002.
R. Zhao and W. Grosky. Narrowing the semantic gap - improved text-based web document retrieval using visual features. Multimedia, IEEE Transactions on, 4(2):189–200, Jun
2002.

Improving Vision-based Self-positioning in Intelligent Transportation Systems
via Integrated Lane and Vehicle Detection

arXiv:1704.01256v1 [cs.CV] 5 Apr 2017

Parag S. Chandakkar, Yilin Wang and Baoxin Li, Senior Member, IEEE
School of Computing, Informatics and Decision Systems Engineering
Arizons State University, Tempe.
{pchandak,ywang370,baoxin.li}@asu.edu

Abstract
Traffic congestion is a widespread problem. Dynamic
traffic routing systems and congestion pricing are getting
importance in recent research. Lane prediction and vehicle density estimation is an important component of such
systems. We introduce a novel problem of vehicle selfpositioning which involves predicting the number of lanes
on the road and vehicle’s position in those lanes using
videos captured by a dashboard camera. We propose an
integrated closed-loop approach where we use the presence of vehicles to aid the task of self-positioning and viceversa. To incorporate multiple factors and high-level semantic knowledge into the solution, we formulate this problem as a Bayesian framework. In the framework, the number of lanes, the vehicle’s position in those lanes and the
presence of other vehicles are considered as parameters.
We also propose a bounding box selection scheme to reduce
the number of false detections and increase the computational efficiency. We show that the number of box proposals
decreases by a factor of 6 using the selection approach. It
also results in large reduction in the number of false detections. The entire approach is tested on real-world videos
and is found to give acceptable results.

1. Introduction
The United States has 786 motor vehicles per 1000 people, which ranks as third highest in the world 1 . It has an estimated total of 253.1 million registered vehicles as of 2011
according to Bureau of Transportation Statistics 2 . The vehicle ownership in the United States has been on constant
rise with some occasional fluctuations. Traffic congestion
has always been a severe problem. There are various measures developed to quantify the problem of traffic congestion, e.g. congestion cost and yearly hours of delay per com1 Statistics
2 Statistics

taken from http://data.worldbank.org/
taken from http://www.rita.dot.gov/bts/

muter, freeway travel time index (FTTI) etc. Large urban
areas with more than 3 million population suffer an average of 52 hours of delay per year per auto commuter. Each
commuter also has to bear the congestion cost of $1128 per
year. Since freeway travel is a large part of our daily commute, experts have developed measures such as FTTI. The
average value of FTTI in 2011 for the freeways in large urban cities was 1.31, which implies the freeway travel duration was increased by factor of 1.31 per auto commuter 3 .
The overall congestion cost and delay for 498 metropolitan
areas in the United States was $121 billion and 5.5 billion
hours ??. The problem of traffic congestion is also slowly
spreading to small cities as well as rural areas.
To remedy this situation, Federal Highway Administration (FHWA) is trying to implement various policies such as
dynamic traffic signal timings and varying tolls and pricing
for roads with different levels of activity 4 . To apply these
strategies, it is essential to know the state of the traffic on
the freeway at any given instant of time. Currently, stationary loop detectors carry out the task of estimating the traffic flow at certain checkpoints with certain accuracy. However, they have reliability issues and they cannot estimate
the flow of traffic on a finer resolution level, e.g. lane-level
traffic flow. Deploying loop detectors is expensive too [9].
In recent years, there has been a tendency to rely on smart
ubiquitous devices such as mobile sensors [9] or GPS data.
Though these techniques increase the overall accuracy, they
cannot provide enough resolution of the traffic density.
Intelligent transportation system (ITS) research analyses the traffic flow and provides necessary feedback, mainly
through varying congestion pricing and toll. The envisioned
future for ITS systems is that they should be able to collect
the information on lane-level and price lanes accordingly
for platoons of cars entering and exiting the freeways at a
given time [26]. This requires estimation of the number
3 Congestion statistics taken from http://mobility.tamu.
edu/ums/national-congestion-tables/
4 Nationwide congestion information taken from http://www.
fhwa.dot.gov/congestion/

c

2015
IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers
or lists, or reuse of any copyrighted component of this work in other works. DOI: 10.1109/WACV.2015.60

of lanes on the road and the position of the vehicle within
those lanes. We call the process of determining these two
parameters as vehicle self-positioning. To calculate the (approximate) density of the vehicles, a detection module also
needs to be in place. Hardware-dependent systems such as
LIDAR and those using GPS are present in literature, but
they are costly and are not usually preferred [13, 11].
Video-based lane and vehicle detection is popular and
well-researched [10, 22]. Though even simple solutions
such as Hough transform can extract the lane markers, inferring the road structure from them has proven difficult.
The stand-alone problem of self-positioning was studied in
[2] and initial findings were reported. In this paper, we extend the problem so as to perform self-positioning and vehicle detection in a closed-loop system such that each process
aids the other one and in-turn increases efficiency of the entire system. The primary purpose of this system is not to
extract lane markers or vehicle detection, though we consider them as sub-problems, but to perform accurate selfpositioning with the aid of vehicles present on the road. We
propose a Bayesian model which takes three parameters:
number of lanes on the road, the lane in which vehicle is
being driven and the presence of a vehicle. To construct
the likelihoods of this model, we use a bottom-up approach
which uses guided-filter, lane-model generation and a vehicle detection module. To allow further exploration of this
problem as well as to improve and verify current techniques,
we have made the database publicly available.
The organization of rest of the paper is as follows. Section 2 discusses recent relevant literature on lane and vehicle
detection and self-positioning. Section 3 describes the proposed approach in detail. Section 4 explains the experimental setup and results. Section 5 gives concluding remarks
and outlines the future scope for the current system.

2. Related Work
Lane and vehicle detection are sub-tasks within our current problem formulation. Very few attempts have been
made in the past to perform self-positioning, however there
are many algorithms reported in the literature which handle
the problem of lane and vehicle detection separately. Both
software and hardware-dependent approaches have been developed for the said tasks. Here we focus only on visionbased approaches.
Lane detection includes multiple stages, namely, image
pre-processing, feature extraction, model-fitting for further
verification and tracking to maintain spatio-temporal consistency. One of the most popular pre-processing methods
is inverse perspective mapping (IPM) which maps an image
to a bird’s eye-view. This makes the lane markers appear
straight and parallel to each other. Also, other fine details in
the image get suppressed. Next step is usually simple morphological operation followed by parametric line fitting to

extract the lane markers [14, 1, 25]. Kalman filter is also
used for tracking to minimize the effect of false detections
[18]. In another tracking-based method, the lane model
generated from previous frame was enforced onto the new
frame and only newly appearing lanes were considered [16].
Since roads are not straight, some methods have tried to fit
splines or higher-order curves to get an accurate representation of lane markers [23, 20]. However, fitting higher-order
curves or B-snakes may be computationally inefficient. Another interesting method which can be used to detect more
than one lane (host-lane) is the hierarchical bi-partite graphbased road modeling [19]. This approach outputs multiple lanes and tries to understand the road structure. This
approach also assigns a confidence measure for each lane.
However, this method does not attempt to position the vehicle in the lanes.
A similar problem to self-positioning has been handled
in [15] using spatial RAY features. It tries to predict the position of the vehicle on the road from an input video stream.
However, they predict at most 3 lanes at a time either on
the left or right hand side, whereas we have upto 6 lanes in
our database with upto 3 lanes being on each side. In addition, there experiments are spread over only 2 days with
similar road conditions. Our experiments are more rigorous, spreading over 5 days and the data includes 6 different
road conditions. They have low traffic density whereas we
have moderate-to-high traffic density. Thus direct comparison between these two approaches may not be possible.
Due to recent progress in computer vision research and
computational abilities, patch-learning based approaches
classify image patches depending on the presence of lanemarkers. Gradients and steerable Gaussian filters act as
good features for such patch-based methods [17]. Attempts
are also made to learn the road structure by collecting
knowledge about the type and structure of lane-markers [4].
Vehicle detection is a subset of a widely-studied problem of object detection. There are a plethora of approaches for general object detection, e.g. [6], or approaches dedicated to vehicle detection which use opticalflow and hidden-markov-model based classification to interpret motion-based clues [12]. HOG and HAAR features
have also been used with different classifiers and learning frameworks such as adaboost, SVM and active learning
[21, 24]. Though there are innumerable methods developed
for general object detection and specifically for vehicle detection, listing them all is beyond the scope of this paper.
A good review of vehicle detection methods can be found
in [22]. In spite of continuous efforts, there are no reliable
vision-based solutions to predicting number of lanes and vehicle self-positioning given a front-facing view.

3. Proposed approach

problem as,

In this section, we propose an integrated approach to
solve the problem of vehicle self-positioning. Apart from
external factors such as bad weather and conditions of lane
markings, lane-occlusion by passing vehicles is one of the
biggest hurdles to reliable vehicle self-positioning. Our
approach works in a closed loop. We utilize information
from the positions of other vehicles to improve our selfpositioning. The self-positioning information gives us information about the road structure, which we use to generate the detection proposals for vehicles. We also employ
temporal smoothing of results across frames to counter the
effect of unexpected events, e.g. a car stopped on the shoulder or faint/invisible lane markings.
We incorporate this problem in a Bayesian framework
to add the additional factors such as presence of vehicles, vehicle dynamics, scene
etc. Assume we
 semantics

F1 Θ1
 F2 Θ2 


have labeled data D =  .
..  where Fi is the ith
 ..
. 
Fn Θn
video clip and Θi is a 2 − D label vector [θ1 , θ2 ] where
θ1 ∈ [1, nlanes ] is the number of lanes present in Fi . The
other parameter θ2 ∈ [1, θ1 ] is the lane in which the car
is currently driving (assuming the leftmost lane is the first
lane). The process of determining the above label vector for each frame is called self-positioning. Each video
clip Fi contains m frames - [fi1 , fi2 , . . . , fim ]. We include
the vehicle presence in the Bayesian framework in order
to aid self-positioning as follows. The joint probability
density for any frame fkj such that the video clip Fk belongs to the test data is P (fkj , Θ|D, fkj−1 , . . . , fkj−p , Vi )
where [fkj−1 , fkj−2 , . . . , fkj−p ]] is a set of p frames occurring before fkj and Vi denotes the vehicle presence in
lane i. We need to predict the label vector Θ̂ for current
frame fkj given the set of previous p frames, the training
data D and vehicle presence in a lane i can be written as,
P (Θ|D, fkj−1 , . . . , fkj−p , Vi ). By Bayes’ rule it can be further decomposed into,

Θ

≈ argmax ψ T [P (fkj |Θ, D, Vi ) . . . P (fkj−p |Θ, D, Vi )]
Θ

The temporal smoothing kernel takes the form of a slowly
increasing exponential function as shown in the following
equation.
ψ(i) = initial value × (1 + rate-of-increase)i
∀ i ∈ [1, p + 1]

(2)

where i is the frame index and p is the number of frames
we have buffered. The values are normalized so that the
(p + 1)th frame (which is also the current frame) gets a
unit weight. Now, expanding only the first term from the
previous step (for readability purposes), we get,
Θ̂ = argmax ψ T P (fkj |Θ, D, Vi ) P (Θ|Vi , D)
Θ

= argmax ψ T P (fkj |Θ, D, Vi ) P (θ1 , θ2 |Vi , D)
Θ

The determination of our current lane does not depend on
the vehicle presence in a lane since we do not incorporate
view-point information of a detected object. Thus we take
P (θ2 |Vi , D) as constant and remove it from the equation.
Θ̂ = argmax ψ T P (fkj |Θ, D, Vi ) P (θ1 |Vi , θ2 , D)
Θ

The term P (fkj |Θ, D, Vi ) involves calculating features of
the current frame given the label vector and the vehicle detection result. However, the detection result only affects
the label vector, and in particular θ1 . The feature extraction process and the vehicle detection process run parallely
and so they are independent of each other. By definition of
θ1 and θ2 , we see that though θ2 is weakly conditioned on
θ1 as θ2 ∈ [1, θ1 ], vice versa does not hold (i.e. the current
lane in which we are driving does not determine the number
of lanes on the road). Therefore we remove the condition
of θ2 from the last term. We also re-introduce the previously skipped terms containing frames [fkj−1 , . . . , fkj−p ] in
the following equation.
Θ̂ = argmax ψ T [P (fkj |Θ, D) P (θ1 |Vi , D) · . . . ·

P (Θ|D, fkj−1 , . . . , fkj−p , Vi ) ∝
P (fkj−1 , . . . , fkj−p |Θ, D, Vi )P (Vi |Θ, D).

Θ̂ = argmax P (fkj , . . . , fkj−p |Θ, D, Vi )

Θ

(1)

P (fkj−p |Θ, D) P (θ1 |Vi , D)]. (3)

We assume a uniform prior on presence of vehicles.
Also, we introduce a temporal smoothing kernel which considers the results of previous frames. This helps to minimize
the effect of sudden external factors as explained before.
The above maximum-a-posteriori (MAP) problem can be
formulated into a maximum-likelihood estimation (MLE)

Equation ?? shows a general formulation to obtain the
correct value of the label vector given a set of buffered
frames and the vehicle detection results. The structured
Bayesian formulation allows easy modification of the formulation if we were to add more information to the model
in the future. In the next paragraphs, we show how to calculate the two quantities - P (fkj |Θ, D) and P (θ1 |Vi , D).

3.1. Frame likelihood computation
The frame likelihood computation involves extracting
features from the current frame and then calculating an initial estimate of the label vector, which is refined later by
using vehicle detection results. It includes the following
stages:
1. Image pre-processing.
2. Lane model generation.
3. Feature extraction.
4. Initial estimation of frame likelihood.
Image pre-processing: Our aim is to detect all the lanes
present in the frame in order to compute the label vector Θ.
Pre-processing an image removes the unnecessary details in
it and keeps all the lanes. We choose Guided filter for this
task. It is a edge-preserving smoothing filter which works
in linear time [8]. It takes a pair of images as input. One
of them acts as a reference image and “guides” the filtering
process of the other image. When both images are same,
the filter performs edge-preserving smoothing. The filtering
operation is defined as:
GF (I) = ᾱI + β̄,

(4)

where I is a gray-scale video frame.
ᾱ and β̄ are obtained through block-wise averaging using
σ2
and β = (1 − α)µ, where µ is the mean, σ is
α= 2
σ +
the standard deviation of a block in the image and  is a
small constant. For a flat patch in an image, σ = 0 =⇒
α = 0 and β = µ. Thus each pixel in that flat region is
averaged with its neighbors. Similarly it can be proved that
if σ  , sharp edges are preserved. However, by choosing
 appropriately, we can force the guided filter to consider
almost all the pixels in an image belonging to a flat patch.
We choose  high enough such that lanes are considered
belonging to a flat patch. The lane markings are always
surrounded by the road pixels which have much lower value
and therefore, the value of lane pixels decreases by a large
amount as compared to anywhere else in the image. Now a
post-processing operation such as over-subtracting followed
by saturation returns an image similar to the one shown in
Fig. 1. The post-processing operation we use is as follows:
BW (I) = [(I − δ · GF (I)) ∗ 255] ≥ 0,

(5)

where δ is a constant just greater than 1 (here, δ =
1.06). Since value of lane marking pixels has reduced by
a large extent, they get preserved in the post-processed image whereas many other regions disappear. Advantage of
guided filter is that it can also work in bad weather conditions such as rain, low-sunlight or in the night. It can also
detect extreme lanes which are very thin as shown in Fig. 1.

Lane model generation: The pre-processed image preserves all the lanes and rejects a lot of unwanted regions.
Yet, there are many spurious objects which may prevent us
from performing reliable detection of all the lanes on the
road. Additionally, as mentioned before, our aim is to perform self-positioning only on the freeways which have a
defined road structure such as constant-width lanes. Taking advantage of these constraints, we define a lane model
in which there are a maximum of seven lanes, three being
on each side of the center lane. Due to fixed-width of the
lanes, they can be represented as a function of y-coordinate
of the center lane markers and the camera parameters. We
first detect the center lane markers in an image using simple
thresholding techniques and other heuristics depending on
their possible position, shape and color. We then perform
a linear fit for the left and right center lane marker denoted
respectively by cl and cr . Then the remaining three lane
markers on each side can be obtained as:
li = dli (cl ) = mli ycl + kil , ∀i ∈ [1, 2, 3]

(6)

ri = dri (cr ) = mri ycr + kir , ∀i ∈ [1, 2, 3]

(7)

and,

Above two equations represent the other lanes in the
form of offsets from the center lane - dli (cl ) and dri (cr ).
These offsets are in turn represented as a linear function
of the y-coordinate the center lane markers - ycl and ycr .
{mli , mri , kil , kir } are the slope and intercept parameters of
cl and cr respectively. Once these offsets have been calculated, then obtaining (x, y) coordinates of lane-markers is
a straight-forward task. We assume camera parameters are
fixed, but they can also be included in equation ?? and ??.
The generated lane model is shown in Fig. 2.
Feature extraction: Once we have found the probable lane
regions as shown in Fig. 2, we find lane pixels by simple
thresholding. By following the gradient directions at those
lane pixels, we can get the road pixels as shown in Fig. 3.
We form a 40-dimensional feature vector consisting of:
1. Mean and variance of lane and road pixels (4-D).
2. 36-bin histogram of gradients at lane pixels as shown.
The lane markers will have low mean and variance for
road pixels and the majority of the gradients at lane pixels lie in a specific range of angles (shown in Fig. 3). We
consider at most 7 lanes (or 8 lane markers) at a time, upto
3 lanes being on either side. Presence of middle lane is assumed. Thus our feature vector is 40∗6 = 240 dimensional.
Estimation of P (f |Θ, D): Though there are many methods to implement the general formulation presented in equation ??, we choose to implement it using a linear SVM. It
is trained using the features extracted from D. For a video

Figure 1: Captured frame and pre-processing using guided filter.
frame in the test data, we extract its features and then apply the linear SVM. The likelihood estimate L(Θ|f, D) is
the probability estimate of the linear SVM for each Θ. We
repeat the same procedure with random forest too.

3.2. Refinement of frame likelihood
We use the presence of vehicles in the adjacent lanes to
our advantage. The term P (θ1 |Vi , D) shows that the vehicle
presence in lane i affects the probability of the number of
total lanes on the road. Assume an initial estimate of the
label vector Θ̂init = [θˆ1init , θˆ2init ]. Here, θˆ1init and θˆ2init
are the initial estimates for the number of lanes and the host
lane respectively. From Θ̂init we can obtain indices of lanes
which are present, e.g. if Θ̂init = [5, 2] then the lane indices
are Lind = {l3 , cl , cr , r3 , r2 , r1 } or {3, 4, 5, 6, 7, 8} (refer
Fig. 2). Depending on the vehicle presence in lane i, we

Figure 2: Lane width modeling

Figure 3: Obtaining road pixels from lane pixels

refine Θ̂init in the following manner:

ˆ

if min(Lind ) ≤ θˆ1 ≤ max(Lind )
θ1init
ˆ
ˆ
θ1new = θ1init + (min(Lind ) − i)
if 1 ≤ i < 4

ˆ
θ1init + (i − max(Lind ))
if 5 < i ≤ 8
(8)
This requires a reliable vehicle detector with high quality
object proposals to start with. In the next section we outline
a method to generate high-quality object proposals by using
the lane structure shown in Fig. 2.

3.3. Efficient vehicle detection
Our proposed approach works in a closed loop such that
it first detects the vehicles present in the frame which helps
us to perform self-positioning task. Then the feedback from
the inferred lane structure is used to generate high-quality
object proposals. The obtained detections from these proposals then traverse to the beginning of the loop.
We make a key observation from Fig. 2 that the span
of lower edge of a bounding box enclosing a vehicle is always contained with the two corresponding lane markers.
As shown in Fig. 2, bounding boxes having a span too small
(Box-1) or too large (Box-4) than their corresponding lane
markers are invalid and are rejected in our proposal generation process. We now show a sample process of calculating
the span between the lane markers and also focus on bounding box selection process.
Consider
a
bounding
box
represented
by
[xmin , ymin , xmax , ymax ].
To find the span between
its corresponding lane markers, we first need to find
their (x, y) coordinates. Since any lane marker can be
represented in as a function of the center lane markers, their
coordinates (xc , yc ) can be found by setting yc = ymax and
xc can be found using the linear fit. The nearest lane marker
to (xmin , ymax ) is then found. The coordinates of that lane
marker (xnear , ynear ) are found by setting ynear = ymax
and xnear = xcr + rj . If the nearest lane marker is to the
left then it is similarly obtained as, xnear = xcl − lj (please
refer to equation ?? and ??). Once the coordinates for the

Figure 4: From left to right, row 1: Correct predictions for categories [5, 3], [5, 4] and [6, 4]. Category of the middle image is
predicted correctly in spite of a vehicle occluding the view of first lane. Row 2: Wrong predictions for categories [4, 1], [5, 2]
and [5, 4]. Occlusion, bad road conditions and bad lighting conditions are the plausible causes for wrong detections.
nearest lane marker are found, then the span between that
and the next marker can be easily calculated.
The advantage of the above bounding box selection technique is that it involves only simple computations and it can
be applied to exhaustively generated bounding boxes or to
the proposals generated through techniques such as BING
[3]. However, we observe that in our data-set, the video
frames are not of high-quality and many vehicles are small.
BING does not capture all the cars and thus we have used
exhaustively generated bounding box along with the selection technique. The exhaustive boxes are generated by applying three naive constraints:
1. Aspect ratio of any box cannot be more than three and
smaller than one-third.
2. The boxes lie only in the lower

2 rd
3

part of an image.

3. Maximum size of boxes is pre-determined.

average length of each video clip is 11.8 seconds. Though
L = 7 yields 28 categories, we consider a subset of those
categories which occur frequently in real-life. The list of
label configurations in our database is as follows: Θ =
{[4, 1], [4, 2], [4, 3], [4, 4], [5, 2], [5, 3], [5, 4], [6, 4]}. We assume a uniform prior for all these categories and zero prior
for the rest. Our training data-set contains 27 videos while
the rest 26 are used for testing. The number of frames used
in training and testing are 9018 and 9036 respectively. The
distribution of training and test data over various categories
is shown in Fig. 5. As mentioned before, each category has
videos of varying road, traffic and weather conditions.
We show the accuracy of self-positioning before and after temporal smoothing in Table 1. Table 2 shows the confusion matrix for the initial estimation of self-positioning
and the final estimate after temporal smoothing. We can see
that there is an improvement of 5.04% over all 8 categories,

Even after applying these constraints, the selection technique reduces the no. of boxes by a factor of 6 on an average
and eliminates many false detections. We use a cascaded
deformable parts model [7, 6] trained on our data which
consists of 833 vehicles and on “car” subset of Pascal VOC
2010 data [5]. The negative data is randomly chosen. These
detections can again be used to refine the frame likelihood.

4. Experimental Setup and Results
We have formed the data-set using the videos captured
from a typical camera mounted inside the wind-shield. Our
data-set has 53 videos with a frame rate of 30 fps 5 . The
5 The

data-set is available at http://www.public.asu.edu/
˜bli24/CodeSoftwareDatasets.html

Figure 5: Distribution of Video Frames in Train and Test
Set

Table 1: Self-positioning accuracy.

Class = [θ1 , θ2 ]
[4, 1]
[4, 2]
[4, 3]
[4, 4]
[5, 2]
[5, 3]
[5, 4]
[6, 4]
Overall Accuracy

Without temporal smoothing
Linear SVM
Random forest
33.55%
34.94%
58.84%
80.28%
69.74%
73.50%
72.45%
90.48%
54.93%
47.00%
48.18%
47.94%
44.06%
56.82%
95.45%
94.55%
54.77%
61.41%

With temporal smoothing
Linear SVM
Random forest
32.90%
32.81%
62.67%
89.32%
73.33%
74.88%
73.47%
96.94%
63.44%
63.06%
49.91%
53.78%
46.27%
64.27%
100%
96.36%
57.49%
66.45%

Table 2: Confusion matrix for self-positioning without temporal smoothing (using Random forest). Results of self-positioning
after temporal smoothing are shown in brackets.
Class =
[θ1 , θ2 ]
[4, 1]
[4, 2]
[4, 3]
[4, 4]
[5, 2]
[5, 3]
[5, 4]
[6, 4]

[4, 1]

[4, 2]

[4, 3]

[4, 4]

[5, 2]

[5, 3]

[5, 4]

[6, 4]

378 (355)
24 (3)
10 (0)
0 (0)
19 (5)
10 (0)
21 (0)
0 (0)

300 (302)
985 (1096)
12 (5)
9 (0)
218 (184)
515 (627)
481 (506)
0 (0)

47 (10)
72 (32)
1659 (1690)
0 (9)
23 (0)
235 (98)
213 (141)
0 (0)

5 (0)
6 (10)
37 (35)
266 (285)
2 (2)
3 (0)
6 (0)
0 (0)

348 (415)
89 (86)
2 (0)
0 (0)
243 (326)
1 (3)
31 (0)
0 (0)

2 (0)
49 (0)
295 (328)
0 (0)
11 (0)
779 (874)
7 (0)
1 (0)

2 (0)
0 (0)
239 (199)
17 (0)
1 (0)
46 (0)
1029 (1164)
11 (8)

0 (0)
2 (0)
3 (0)
2 (0)
0 (0)
36 (23)
23 (0)
208 (212)

which indicates that temporal smoothing is able to remove
noisy results persisting only for a few frames. As we increase the value of p in equation ??, the algorithm averages
over larger set of frames but it increases the memory and
makes the algorithm computationally inefficient. Thus we
have set the value of p to 15 while using temporal smoothing, otherwise we make it 0. Temporal smoothing improves
the initial estimation in all but one category -[4, 1]. The decrease may occur when the algorithm wrongly predicts for
more than p/2 frames often. Also, due to slowly increasing exponential kernel, first p/2 frames may be wrongly
predicted when the category changes. Low prediction accuracy is observed in the category -[4, 1]. It should be accounted that there are 3 lanes on one of the sides of the
host-lane. Since its extremely thin (about 4 pixels), its reliable detection becomes difficult. In addition to this, the road
conditions in that category are relatively worse and there is
moderate traffic. Interestingly, the category [6, 4] achieves
96.36% accuracy in spite of having most number of lanes.
We would like to point out the fact that this category has the
best weather and road conditions. Thus our approach can
achieve high accuracy even in the presence of more number
of lanes provided the weather, road and traffic conditions
permit. This is an eight-class classification problem an it

has variable weather and road conditions. Another important fact is that the training data itself may be noisy. We
have divided the video clips such that each clip has a single
label. Therefore, one of the training video clips in category
- [5, 3] (say) may have a faint/invisible second lane for a
few frames. Results using our approach are shown in Fig.
4. The proposed approach is able to handle partial vehicle
occlusions, varying road and weather conditions. However,
it may fail when there are multiple vehicles totally blocking
the view or if other conditions are worse.

Bounding box selection technique reduces the generated
proposals for vehicles by a factor of 6 and in turn this reduces number of false detections. This technique also allows use of other faster but less accurate methods. We
would like to point out that though vehicle detection is an
important component in our system, our final goal remains
to achieve accurate self-positioning in all possible conditions. We do not yet have annotated database of vehicles
for such a scenario and thus we do not list comprehensive
results for vehicle detection.

5. Conclusion and Future Scope
A novel problem of self-positioning is introduced and
an integrated approach is proposed which performs selfpositioning with the aid of vehicle detection in a closed-loop
system. A high-level Bayesian formulation is developed to
allow easy modifications in the future should anyone try to
introduce additional factors such as scene or viewpoint information. Our approach enables a system to perform dynamic traffic routing on lane-level due to its knowledge of
vehicle positions and lane-structure. This promises larger
reduction in congestion cost and travel delays. We also develop a bounding box selection criteria which can be applied to exhaustive set of boxes or to the boxes obtained
from other box-proposal methods. Testing this framework
on real-world videos yielded acceptable results.
The system presently uses just video data to make predictions. In future, we can use GPS, accelerometer data and
other information obtained from vehicle dynamics. It is also
possible to employ this system on cloud and integrate the
results from all the vehicles to understand the underlying
true structure of the road. Additional information such as
position of lanes traveling in opposite direction may be included so that detection of those vehicles could be avoided.
We also aim to generate an annotated database of vehicles
in order to perform vehicle density estimation along with
self-positioning and evaluate the same.

References
[1] Y. Alon, A. Ferencz, and A. Shashua. Off-road path following using region classification and geometric projection
constraints. In IEEE CVPR, volume 1, pages 689–696, 2006.
[2] P. Chandakkar, R. Venkatesan, and B. Li. Video-based selfpositioning for intelligent transport systems applications. In
Proceedings of the 10th International Symposium on Visual
Computing, Las Vegas, NV, 2014.
[3] M.-M. Cheng, Z. Zhang, W.-Y. Lin, and P. H. S. Torr.
BING: Binarized normed gradients for objectness estimation
at 300fps. In IEEE CVPR, 2014.
[4] J. M. Collado, C. Hilario, A. De La Escalera, and J. M.
Armingol. Adaptative road lanes detection and classification. In Advanced Concepts for Intelligent Vision Systems.
Springer, 2006.
[5] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
and A. Zisserman. The PASCAL Visual Object Classes
Challenge 2010 (VOC2010) Results. http://www.pascalnetwork.org/challenges/VOC/voc2010/workshop/index.html.
[6] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part
based models. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 32(9):1627–1645, 2010.
[7] R. B. Girshick, P. F. Felzenszwalb, and D. McAllester.
Discriminatively trained deformable part models, release 5.
http://people.cs.uchicago.edu/ rbg/latent-release5/.

[8] K. He, J. Sun, and X. Tang. Guided image filtering. IEEE
PAMI, 35(6):1397–1409, 2013.
[9] J. C. Herrera and A. M. Bayen. Traffic flow reconstruction
using mobile sensors and loop detector data. University of
California Transportation Center, 2007.
[10] A. B. Hillel, R. Lerner, D. Levi, and G. Raz. Recent progress
in road and lane detection: a survey. Machine Vision and
Applications, pages 1–19, 2012.
[11] A. S. Huang, D. Moore, M. Antone, E. Olson, and S. Teller.
Finding multiple lanes in urban road networks with vision
and lidar. Autonomous Robots, 26(2-3):103–122, 2009.
[12] A. Jazayeri, H. Cai, J. Y. Zheng, and M. Tuceryan. Vehicle detection and tracking in car video based on motion
model. IEEE Transactions on Intelligent Transportation Systems, 12(2):583–595, 2011.
[13] S. Kammel and B. Pitzer. Lidar-based lane marker detection
and mapping. In IEEE Intelligent Vehicles Symposium, 2008.
[14] H. Kong, J.-Y. Audibert, and J. Ponce. Vanishing point detection for road detection. In IEEE Conference on Computer
Vision and Pattern Recognition, pages 96–103, 2009.
[15] T. Kuhnl, F. Kummert, and J. Fritsch. Visual ego-vehicle lane
assignment using spatial ray features. In IEEE Intelligent
Vehicles Symposium (IV), pages 1101–1106, 2013.
[16] C. Lipski, B. Scholz, K. Berger, C. Linz, T. Stich, and
M. Magnor. A fast and robust approach to lane marking detection and lane tracking. In IEEE Southwest Symposium on
Image Analysis and Interpretation., pages 57–60, 2008.
[17] J. C. McCall and M. M. Trivedi. Video-based lane estimation and tracking for driver assistance: survey, system, and
evaluation. IEEE Transactions on Intelligent Transportation
Systems, 7(1), 2006.
[18] M. Nieto, J. A. Laborda, and L. Salgado. Road environment modeling using robust perspective analysis and recursive bayesian segmentation. Machine Vision and Applications, 22(6):927–945, 2011.
[19] M. Nieto, L. Salgado, and F. Jaureguizar. Robust road modeling based on a hierarchical bipartite graph. In IEEE Intelligent Vehicles Symposium, pages 61–66, 2008.
[20] H. Sawano and M. Okada. A road extraction method by an
active contour model with inertia and differential features.
IEICE transactions on information and systems, 89(7):2257–
2267, 2006.
[21] S. Sivaraman and M. M. Trivedi. A general active-learning
framework for on-road vehicle recognition and tracking.
IEEE Transactions on Intelligent Transportation Systems,
11(2):267–276, 2010.
[22] S. Sivaraman and M. M. Trivedi. Looking at vehicles on
the road: A survey of vision-based vehicle detection, tracking, and behavior analysis. IEEE Transactions on Intelligent
Transportation Systems, 14(4):1773, 2013.
[23] Y. Wang, E. K. Teoh, and D. Shen. Lane detection and tracking using b-snake. Image and Vision computing, 22(4), 2004.
[24] Q. Yuan, A. Thangali, V. Ablavsky, and S. Sclaroff. Learning
a family of detectors via multiplicative kernels. IEEE PAMI,
33(3):514–530, 2011.
[25] G. Zhang, N. Zheng, C. Cui, Y. Yan, and Z. Yuan. An efficient road detection method in noisy urban environment. In
IEEE Intelligent Vehicles Symposium, pages 556–561, 2009.

[26] H. Zheng, Y.-C. Chiu, and P. B. Mirchandani. On the system
optimum dynamic traffic assignment and earliest arrival flow
problems. Transportation Science, 2013.

J. Vis. Commun. Image R. 20 (2009) 491–503

Contents lists available at ScienceDirect

J. Vis. Commun. Image R.
journal homepage: www.elsevier.com/locate/jvci

Adapting quantization offset in multiple description coding for error resilient
video transmission q
Viswesh Parameswaran, Avin Kannur, Baoxin Li *
Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85281, USA

a r t i c l e

i n f o

Article history:
Received 21 August 2008
Accepted 4 July 2009
Available online 12 July 2009
Keywords:
Multiple description coding (MDC)
Video streaming
H.264/AVC encoder
Error resilient video transmission
Wireless ad hoc networks
Video quality
Video over wireless
Quantization offset

a b s t r a c t
Multiple description coding (MDC) provides an excellent error resilient approach for transmitting video
over wireless ad hoc networks. In this paper, we propose an improvement to this scheme by jointly
selecting the quantization offsets in different paths to achieve the best overall video quality at the decoder. The statistical distribution of the transform coefﬁcients in the encoded video sequence is parameterized using a Laplacian model. The optimal offset values are computed by solving a multi-level
optimization problem based on the statistics of the transform coefﬁcients and the individual path failure
probabilities. In order to reduce the computational complexity, the encoding modes for the motion vectors and transform coefﬁcients are collected in the ﬁrst step. In the second step, the model parameter is
calculated from the transform coefﬁcients and the offset search is performed when the model parameter
between frames deviates beyond the pre-set threshold. In the ﬁnal step, the stored modes and the respective offset values are directly used for encoding the two bit streams. The simulation results using H.264/
AVC system conﬁrm the advantages of the proposed approach under different packet loss conditions.
Ó 2009 Elsevier Inc. All rights reserved.

1. Introduction
The past decade has witnessed rapid growth in the development and deployment of computationally intensive multimedia
applications on wireless devices. This can be attributed to the following reasons: – (1) the improvements in semiconductor technologies resulting in the development of 45 nm integrated chip
fabrication process, (2) the developments in wireless technologies
enabling information transfer at speeds ranging 2 Mb/s in the case
of 3G networks, and (3) the evolution of video compression standards enabling high quality video transmissions at low bit rates.
Some of the cell phones available in market are already equipped
with digital video streaming and conferencing functionalities along
with the fundamental voice communication feature. The anticipated growth in mobile social networking will further increase
the utility of multimedia transfer over cell phones. The typical video applications running on these devices can be categorized on
the basis of their underlying network, delay requirements and
the desired video quality. The network used for video transfer
could be ﬁxed wire line, ﬁxed wireless, ad hoc wireless or a combi-

q
This research was supported in part by ARO Grant W911NF-06-1-0354. The
information reported here does not reﬂect the position or the policy of the federal
government.
* Corresponding author.
E-mail addresses: vparames@asu.edu (V. Parameswaran), akannur@asu.edu (A.
Kannur), baoxin.li@asu.edu (B. Li).

1047-3203/$ - see front matter Ó 2009 Elsevier Inc. All rights reserved.
doi:10.1016/j.jvcir.2009.07.003

nation of wired and wireless networks. The delay parameters that
need to be considered for system design are the average end-toend delay (known as latency) and the delay variation (known as jitter). The video quality can be speciﬁed in terms of the required spatial and temporal resolutions.
For instance, consider the case of one-way video streaming over
internet. In this application, the video is compressed ofﬂine and is
stored in a streaming server. The user connects to the server using
a suitable protocol such as real-time streaming protocol (RTSP).
Since the video is encoded ofﬂine, there is no delay constraint at
the encoder. The decoder stores the received frames in a buffer
and starts playing them after an initial play-out delay. In this case
the latency can be fairly large, but jitter should be limited so that
the video can be played out smoothly. The maximum jitter that
can be tolerated is determined by the size of the playout buffer.
The desired video quality in the case of this application is determined by the end user terminal and the speed of the linking
network.
Another application scenario that can be envisioned is interactive video conferencing over cell phones. In this case we need to
capture and deliver video in real time. In order for the two-way
communication to be effective, we need to impose stringent delay
constraints both at the encoder and the decoder. The system has to
be designed in such a way so as to minimize latency and jitter. The
buffers at the encoder and decoder should be of limited size to
minimize latency. The underlying network in this case is wireless
and hence the application is constrained by limited band-width

492

V. Parameswaran et al. / J. Vis. Commun. Image R. 20 (2009) 491–503

and high error rates. The error characteristics of the channel
ﬂuctuate because of the multipath fading effects. The video quality
requirements are relaxed in this case since we can tolerate some
degradation in terms of spatial and temporal resolution.
Similar constraints are applicable in case of mobile imaging sensors as well. Recently there has been a lot of focus in developing
miniature low-power imaging sensors by companies such as SONY,
CANON, Cypress semiconductors etc. These sensors are ﬁnding
increasing number of applications in military and commercial
environments. For example, in a typical military battle ﬁeld scenario, the imaging sensors can be mounted on a soldier’s helmet
and the captured video could be relayed to the base station at a remote location. Alternatively, the sensors could be mounted on unmanned ground/air vehicles (UGV/UAVs) and the captured video
could be transmitted to the soldiers at a different location. This
shared vision approach helps in minimizing the risk of injury and
enhances the tactical options available to the soldiers.
The basic problem in all the above applications is video transmission over wireless networks. The wireless channel is inherently
error prone because of omnipresent noise and multi-path fading.
The use of forward error correcting codes (FEC) is a popular way
of providing error resilience in these conditions as demonstrated
in articles [2,9,15,25,29] etc. The channel codes work well as long
as the packet drop probability is low. In case of burst errors or path
outages, the use of FEC alone is not sufﬁcient to provide adequate
error resilience. This is especially true in case of low-power mobile
imaging sensors, where the transported video stream is very likely
to use compression techniques such as H.264 thereby having inter
frame dependencies which cannot be handled well using FEC. FEC
based on popular channel codes do not seem to provide the best
performance at high packet drop rates. In this case, video from
the source goes over multiple hops before reaching the destination.
Since each of these nodes is mobile, any node can move out of the
transmission range resulting in path outage. In some cases, the best
available path might change over a period of time and switching
from one path to another will become necessary. In this scenario,
it is advisable to transmit multiple correlated descriptions of the
same video source over disjoint paths. Such a coding scheme,
termed as multiple description coding (MDC) [5,17], provides error
resilience through path redundancy. A good comparison between
MDC and single description coding with FEC is presented in [3].
In MDC, the individual descriptions can be decoded independently
of each other. In case we receive both descriptions the information
from them can be combined together to generate a better quality
video at the decoder. This is in contrast to a layered coding system,
where the base layer and the enhancement layer information are
transmitted over different paths. In such a system, the enhancement layer information cannot be decoded independently from
the base layer. Typically the layered coding is suited for scenarios
where one path is highly reliable compared with the other path.
If both paths are equally good or equally worse, it is better to use
MDC for achieving error resilience. The history and development
of MDC for audio and still image applications was covered in detail
by Goyal in [5].
Some of the popular approaches for multiple description video
coding were explored by Brian in [7]. The MDC scheme for encoded
video transmission gets complicated because of drift error propagation between frames. The latest video coding standard H.264/
AVC, [30,18,28], employs both spatial intra-prediction as well as
motion compensated inter-prediction for achieving high compression ratios. In these cases the loss of one packet can cause unpredictable impact in the current frame and future dependent
frames. In order to better understand the drift effects, we will take
two popular approaches for MDC – temporal sub-sampling (MDTS)
and repetitive coding (MDRC) and compare them in terms of their
coding efﬁciency and error resilience. In MDTS, the original video

sequence is partitioned temporally into even and odd frames.
These two descriptions are encoded independently and transmitted over two different paths. In case both descriptions are received
at the decoder, the decoder interleaves the received frames and
reconstructs the original video. The problem happens when only
one description is received. In this case we cannot use the information from the other description to reconstruct the lost frame. The
only option available is to do error concealment as in case of single
description decoder and this results in propagation of errors in future frames. The coding efﬁciency is also reduced in this scheme
because of poor temporal correlation. In MDRC, identical descriptions are transmitted over both paths. This scheme is highly error
resilient since the loss of one description can be compensated directly with information from the other path. In case both descriptions are received, the information from one path is discarded. This
leads to a overall loss of performance especially at low packet drop
rates. The MDC scheme, in general, is suited for high packet drop
networks or networks with path outages. In addition to this highly
efﬁcient error concealment techniques, similar to the ones covered
by Wang in [24,26], can be implemented at the decoder with
MDRC scheme since both descriptions contain information about
the same frame.
Hence in this paper we propose a modiﬁed RC based MDC
scheme that relies on the optimal selection of the quantization offsets for the transform coefﬁcients to achieve the best overall video
quality at the decoder [27]. The design of multiple description scalar quantizers (MDSQ) was ﬁrst done by Vaishampayan in [21–23].
In this scheme, two separate indices are generated for each quantization level in each path. Such a scheme can be implemented in
practice by shifting the quantization levels by half in one of the
paths. If both descriptions are received at the decoder, termed as
the central description, a ﬁner quantized video can be generated.
But in case only one side description is received the quality deteriorates because of the additional offset. An improvement of the
scheme was proposed by Lee in [12], where the spatial and motion
information is used to reconstruct the video in case only one
description is received. The scheme reported above shows good
performance for intra-predicted MBs, but does not show signiﬁcant
gains in case of inter-predicted MBs. Jafarkhani and Tarokh describe a MDC scheme based on trellis-coded quantization in [8].
In our approach, we address the above problem by optimally
selecting the quantization offsets depending on both the statistics
of the input data source as well as their respective path failure
probabilities. This is done because the overall video quality at the
decoder does not depend solely on the individual side description
or the central description, but instead is a function of the side and
central distortion weighed by the probabilities of receiving each of
them. The side and the central distortion terms in turn depends
upon the statistics of the input data source. We will do a thorough
analysis on the variation of model parameter for luminance and
chrominance transform coefﬁcients. We will also demonstrate that
the model parameter needs to be computed separately for the intra
and the inter predicted frames and also for different transform domain block sizes. The motion vector and the transform coefﬁcients
prediction modes are collected in the ﬁrst pass and are used for
subsequent encoding of the two bit streams. The offset search is reﬁned when the model parameter between frames varies beyond a
pre-set threshold.
The rest of the paper is structured as follows. In Section 2 we
will formulate the optimization problem to minimize the overall
decoder side distortion based on the transform coefﬁcients and
path failure probabilities. Section 3 will cover the implementation
details speciﬁc to H.264/AVC system. In Section 4 we will illustrate
the block schematic of the proposed system. The simulation results
under different packet loss conditions are presented in Section 5
with concluding remarks in Section 6.

V. Parameswaran et al. / J. Vis. Commun. Image R. 20 (2009) 491–503

2. Problem formulation
The total distortion of any frame in a video sequence at the decoder can be modeled as sum of distortions of two separate components – (1) distortion when the frame is received from at least
one path Dreceiv ed ðf Þ, (2) distortion when the frame is lost in both
paths and error concealment is performed Dloss ðf Þ. We will assume
a simple frame copy error concealment scheme where the lost
frame is concealed by the blocks in the previous frame. This is
mathematically represented in Eq. (1).

Dtotal ðf Þ ¼ P receiv ed  Dreceiv ed ðf Þ þ Ploss  Dloss ðf Þ

ð1Þ

In this equation Preceiv ed and Ploss are the respective packet receive
and lose probabilities. We will analyze each of these terms one by
one. The distortion Dreceiv ed ðf Þ is the sum of the quantization error
of the predicted error coefﬁcient in the current frame and the error
introduced by incorrect predictions because of errors in the reference frame. In H.264/AVC, we have multiple reference frame prediction, so in this case the reference frame need not, in general, be the
previous frame.
Let us denote the actual pixel value at a location i in frame f as
xðiÞ, the decoded pixel value in the reference frame as 
xref ðiÞ and the
^ðiÞ. Then the
quantized predicted error coefﬁcient for the pixel as p
distortion Dreceiv ed ðiÞ can be represented as in Eq. (2).

^ðiÞÞÞ2
Dreceiv ed ðiÞ ¼ ðxðiÞ  ðxref ðiÞ þ p

ð2Þ

Now 
xref ðiÞ is the sum of the quantized error coefﬁcient ^xref ðiÞ and
the error in the reference frame eðiÞ. Then Eq. (2) can be rewritten
as follows.
2

2

^ðiÞÞ þ eðiÞ  2  eðiÞ
Dreceiv ed ðiÞ ¼ ðxðiÞ  ^xref ðiÞ  p
^ðiÞÞ
 ðxðiÞ  ^xref ðiÞ  p

ð3Þ

The ﬁrst term in this expression is the quantization error Dquant ðiÞ
and the remaining terms are due to error propagation from previous
frames Dpropagate ðiÞ and they are represented in Eqs. (4) and (5),
respectively.

^ðiÞÞ2
Dquant ðiÞ ¼ ðxðiÞ  ^xref ðiÞ  p
2

^ðiÞÞ
Dpropagate ðiÞ ¼ eðiÞ  2  eðiÞ  ðxðiÞ  ^xref ðiÞ  p

ð4Þ
ð5Þ

Now let us look at the second term Dloss ðf Þ in Eq. (1). If a pixel i in
frame f is lost, we will replace it with the pixel in the previous
frame. Let us denote ^
xprev ðiÞ as the actual quantized value of the pixel in the previous frame and eprev ðiÞ is the error in the previous pixel
value. Then proceeding similarly as above we can write Dloss ðiÞ as
shown in Eq. (6).

Dloss ðiÞ ¼ ðxðiÞ  ^xprev ðiÞÞ2 þ eprev ðiÞ2  2  eprev ðiÞ
 ðxðiÞ  ^xprev ðiÞÞ

quence was set to have I and P-frames. The intra-frame refresh
period (IDR period) was set to 10. The number of reference frames
was set to 1 so that prediction happens with respect to the previous frame. The P-frames in a group of picture (GOP) sequence were
randomly selected and dropped. The tests were performed to measure the Dloss ðf Þ and Dpropagate ðf Þ distortions. The quantization
parameter was set to 28 for all P-frames. The experiment was conducted as follows.
Assume that the random number generator picked 25 as the
frame to be dropped. In this case the encoder will encode the
24th frame with a different QP (22, 28, 34). This will adjust the
quantizer distortion of the reference frame. The decoder will drop
the 25th frame and do error concealment by replacing it with the
24th frame and the future frames will be predicted from this incorrect frame at the decoder. The simulation is repeated by dropping
different frames in the sequence. The average PSNR obtained for
the decoded sequence (excluding the frames for which QP was
changed) against the original sequence is plotted in Fig. 1.
From Fig. 1, it can be seen that the average PSNR is slightly better when the reference frame is ﬁne quantized. This means that we
can minimize the total distortion by minimizing the quantizer distortion Dquant ðf Þ without affecting the other distortion components.
We will now focus on minimizing the quantizer distortion. Different approaches have been proposed in literature [14] to determine
disjoint paths (no common links) between source and destination
in a wireless ad hoc network. We assume that two disjoint paths
between source and destination have been determined. The quantizer distortion can be represented as in Eq. (8).

Dquant ðf Þ ¼ P0;1  Df ;0;1 þ P 1;0  Df ;1;0 þ P1;1  Df ;1;1

X

Preceiv ed  ðDquant ðiÞ þ Dpropagate ðiÞÞ þ Ploss  Dloss ðiÞ

Dquant ðf Þ ¼ pd1  ð1  pd2 Þ  Df ;0;1 þ pd2  ð1  pd1 Þ  Df ;1;0
þ ð1  pd1 Þ  ð1  pd2 Þ  Df ;1;1

ð6Þ

ð7Þ

i2f

The distortion terms Dpropagate ðiÞ and Dloss ðiÞ have been previously
modeled using recursive optimal per-pixel estimate (ROPE) model
by Zhang in [19]. In our case we are proposing the adaptive selection of quantization offsets to minimize the quantizer distortion
Dquant ðiÞ. To ensure that the total distortion is minimized, we should
make sure that the other components of the distortion should decrease or remain approximately the same.
The following experiment is designed to measure the effects of
changing quantization parameter (QP) of the reference frame on
loss and propagation errors. The tests were conducted with JM
13.1 version of H.264/AVC on different frames of three different video sequences. The proﬁle was set in baseline mode. The GOP se-

ð8Þ

In Eq. (8) P0;1 is the probability that the packet is dropped in ﬁrst
path and is delivered in the second path, P1;0 is the probability that
the packet is dropped in second path and is delivered in the ﬁrst
path, P 1;1 is the probability that the packet reaches in both paths
and the Df ;0;1 ; Df ;1;0 and Df ;1;1 are the associated distortion values
for each of these events. If the interference between the links is
small as in the case of disjoint paths, the individual packet drop
probabilities pd1 and pd2 in the 2 paths can be assumed independent of each other. In this case the Bernoulli assumption for packet
drops across paths is valid. Assuming Bernoulli model, the quantizer
distortion, Dquant ðf Þ can be rewritten as in Eq. (9).

Using Eqs. (4)–(6), we can rewrite Eq. (1) as shown in Eq. (7).

Dtotal ðf Þ ¼

493

Fig. 1. Effect of quantization step on propagation and loss errors.

ð9Þ

494

V. Parameswaran et al. / J. Vis. Commun. Image R. 20 (2009) 491–503

We tested the case with pd1=0 and pd2=0(no packet drop in both
paths) for different frames of different video sequences. For Iframes it was found that minimizing the central component distortion did achieve the best PSNR values as predicted by Eq. (9). In this
case the side distortion components did not have any effect on the
total distortion. But for P-frames it was observed that the same
equation did not achieve the best results. The side distortions in this
case seems to play a more important role in determining the ﬁnal
distortion value even though both paths are completely lossless.
This is possible because the P-frames are encoded as residual error
components predicted with reference to the previous frames and
therefore the side distortions cannot be increased arbitrarily because they have an adverse impact on the ﬁnal reconstructed values.
So we modiﬁed Eq. (9) by adding some portion of side distortion
components even in cases where both paths are lossless. The a value
was used to adjust the weight of side distortion components. For different values of alpha we ﬁgured out the PSNR values for different
frames of the video sequences. The value of optimal a was observed
to vary slightly between different frames of different video sequences. Eq. (9) was thus modiﬁed for P-frames as in Eq. (10).

Dquant ðf Þ ¼ a  ð1 þ pd1  ð1  pd2 ÞÞ  Df ;0;1 þ a  ð1 þ pd2  ð1  pd1 ÞÞ
ð10Þ
 Df ;1;0 þ 2  a  ð1  pd1 Þ  ð1  pd2 Þ  Df ;1;1
In Eq. (10) it should be noted that the weights are still functions
of the packet drop probabilities, but are weighted by the factor a.
Also it should be noted that under no loss condition, the side
description distortions have no impact on the total distortion of
I-frames, but have a certain impact on the distortion of the Pframes determined by the factor a. The actual weight for the Pframes is dependent on the video sequence. To achieve optimal value of a, we must solve the optimization problem of Eq. (10) with a
as a tuning parameter [0, 1]. In our experiments a value of 1 for a
was observed to yield good results and hence retained for complexity reduction of the optimization procedure. The problem can
now be formally stated as follows: determine the quantization offsets for the AC transform coefﬁcients in each path so as to minimize the objective functions given in Eqs. (9) and (10).
3. Proposed approach based on adaptive quantization offsets
We now propose an approach for solving the problem deﬁned
in Section 2. The key idea of our approach is to adaptively adjust
the quantization offsets in the two paths depending on the input
source statistics and path failure probabilities. For clarity of presentation, we will discuss the approach based on the implementation using H.264/AVC encoder. For this purpose, we will brieﬂy
review some relevant components of the codec as explained below.
H.264/AVC supports both intra-prediction and inter-prediction
modes. In the intra-prediction mode (I-slices), the macroblocks
are predicted from the previously predicted macroblocks in the
current slice. In case of inter-prediction(P-slices or B-slices) motion
compensated inter-prediction is performed with the blocks in the
past or future frames. The prediction output is subtracted from
the actual value to get a residual error which is transformed, quantized and encoded. The transforms used in H.264/AVC are approximate integer versions of DCT. The DCT transfoms have been
previously modeled using well known distributions such as Gaussian, Laplacian, Cauchy etc. Some of the related works can be found
in articles by Lam [11], Lakhani [10] and Altunbasak [1]. The modeling gets complicated in H.264/AVC because the mode selection
algorithm determines the best block size to minimize the rate-distortion cost. In addition to this, the encoder can select 4  4 or
8  8 transforms adaptively depending on the error content. Also
the variation of luminance and chrominance components in an image are typically different for natural video sequences. To accu-

rately compute the offsets, we need to analyze the transform
coefﬁcients for each of these cases separately as illustrated in the
following sub-section.
3.1. Modeling of DCT coefﬁcients
The transform coefﬁcients can be categorized into following 4
types: – (1) 4  4 AC luminance transform coefﬁcients of intra16  16 block size mode (2) 4  4 luminance transform coefﬁcients
from all other modes (3) 8  8 luminance transform coefﬁcients (4)
AC chrominance transform coefﬁcients. The distribution of AC
4  4 mode luminance coefﬁcients and their absolute value in
the selected I-frame of the Foreman sequence is shown in Fig. 2.
As can be noted from Fig. 2. the coefﬁcient distribution can be
approximated as a Laplacian distribution of the form given in Eq.
(11).

pðxÞ ¼

k
 expðk  jxjÞ; x 2 R
2

ð11Þ

The model parameter k can be estimated in multiple ways. One way
is to calculate the mean of all positive transform coefﬁcients and
calculate the model parameter as one half of the inverse of this
mean. Another way to compute the model parameter is from the
standard deviation of the distribution. In our case we use an estimator based on the absolute value of the transform coefﬁcients to
compute the model parameter. This is because in the H.264/AVC encoder, the absolute value of the transform coefﬁcient is used for
quantization and the sign value is appended at a later stage. Also
the computation of the mean of the absolute value is much simpler
than the computation of standard deviation of the transform coefﬁcients. This can be represented as shown in Eq. (12).

k¼

1
jxj

ð12Þ

In Eq. (12), jxj is the mean of the absolute value of the transform
coefﬁcients. Table 1 tabulates the model parameters obtained for
different categories of transform coefﬁcients for the selected I and
P-frames in the Akiyo and Foreman sequences. The high values of
model parameter indicate that variance of the transform coefﬁcients is small. It can be noted that the chrominance components
have small variances compared with the luminance components
because of high correlation between the pixel values. Also it can
be seen that the model parameters for the Akiyo sequence is larger
compared with Foreman sequence. This is because the Akiyo sequence has less motion and is spatially more smooth compared
with the Foreman sequence.
In the next sub-section we will use the transform coefﬁcients,
path failure probabilities and the model parameter to compute
the path offsets and use the computed offsets to quantize the
transform coefﬁcients.
3.2. Offset computation and differential quantization
The objective function minimized here depends both on the
transform coefﬁcients as well as the path failure probabilities.
The transform and quantization process is coupled in H.264/AVC
so as to avoid division and ﬂoating point arithmetic. We will explain the algorithm with a more compact ﬂoating point representation. The integer equivalents of the offsets will be found out by
multiplying with suitable multiplying factors. The quantization
and inverse quantization process for standard codec is described
in Eq. (13),

Z i;j ¼



ðjW i;j j þ bÞ
 signðW i;j Þ
D

W 0 i; j ¼ Z i;j  D

ð13Þ

V. Parameswaran et al. / J. Vis. Commun. Image R. 20 (2009) 491–503

495

Fig. 2. 4  4 mode luminance coefﬁcients distribution in selected I-frame of the Foreman sequence.

Table 1
Model parameter for different video sequences.
Video sequence

Frame type

k1616

k88

k44

kcr

Foreman
Foreman
Akiyo
Akiyo

I
P
I
P

0.03
—
0.03
—

0.03
0.03
0.02
0.05

0.03
0.03
0.02
0.05

0.05
0.06
0.04
0.12

the reconstruction point. The equivalent decoder offsets are calculated by the following process.
Assuming that the transform coefﬁcients within a quantization
interval are approximately modeled by a continuous exponential
distribution, the reconstruction level will point to the mean if Eq.
(15) is satisﬁed.

Z

b

ðx  ^xÞ  ekx dx ¼ 0

ð15Þ

a

In the above equation, W i;j is the transform value at ði; jÞ location, Z i;j
is the equivalent quantized level and W 0i;j is the reconstructed coefﬁcient value. D is the quantization step size and b is a ﬁxed offset,
D
 
for intra-predicted frames and D6 for inter-predicted frames.
3
The modiﬁed quantization process for a 2-path MDC is given by
Eq. (14),



ðjW i;j j þ b þ De1 Þ
 signðW i;j Þ
D


ðjW i;j j þ b þ De2 Þ
¼
 signðW i;j Þ
D

Z 1;i;j ¼
Z 2;i;j

ð14Þ

In the above equation Z 1;i;j ; Z 2;i;j are the quantized levels in paths 1
and 2 and ðDe1 ; De2 Þ are the corresponding encoder offsets.
The working of the algorithm is described as follows. The encoder side offsets ðDe1 ; De2 Þ changes the number of coefﬁcients getting mapped to a particular quantization level. Since we are
adding different offsets in each path, the coefﬁcients that will be
getting mapped to a particular level will be different. This disparity
in quantization mapping creates an additional quantization level in
case both descriptions are received at the decoder. The descriptions are averaged to generate the ﬁnal output, in case both
descriptions are received at the decoder. In case only one description is received, an equivalent offset is subtracted out at the decoder side. This offset is computed at the encoder and is encoded in
the bit stream, which is selectively used by the decoder. This is
required because the additional offset at the encoder shifts the
mean of the coefﬁcients getting mapped to a particular quantization level. This needs to be compensated at the decoder by shifting

Here x is the actual transform coefﬁcient and ^x is the reconstructed
level. a and b are the limits of coefﬁcients getting mapped to the
ﬁrst quantization level D. In our case ða ¼ D  b  ðDe1 ÞÞ and
ðb ¼ 2:D  b  ðDe1 Þ  1Þ for path1 and ða ¼ D  b  ðDe2 ÞÞ and
ðb ¼ 2:D  b  ðDe2 Þ  1Þ for path2, respectively. We need to change
the new reconstruction level to ðD  ðDd1 ÞÞ in path1 or ðD  ðDd2 ÞÞ in
path2, where ðDd1 ; Dd2 Þ are the corresponding decoder offsets and
are given by Eq. (16).

1 ða  eka  b  ekb Þ
Ddx ¼ D  þ
k
ðeka  ekb Þ

ð16Þ

In Eq. (16), Ddx , stands for Dd1 or Dd2 depending on the description
which was received at the decoder. The inverse quantization process is performed differently depending on the number of descriptions received at the decoder. The average central reconstructed
coefﬁcient is determined as shown in Eq. (17).

W 01;i;j ¼ ðjZ 1;i;j j:DÞ  signðZ 1;i;j Þ
W 02;i;j ¼ ðjZ 2;i;j j:DÞ  signðZ 2;i;j Þ
W 0c;i;j

W 01;i;j þ W 02;i;j
¼
2

ð17Þ

In the above equation W 01;i;j ; W 02;i;j are the respective reconstructed
coefﬁcients in paths 1 and 2 and W 0c;i;j is the combined reconstructed coefﬁcient of the two paths. In case we receive description
from only path1, the side description coefﬁcient of path1 is determined by subtracting the equivalent decoder offset Dd1 as shown
in Eq. (18).

496

V. Parameswaran et al. / J. Vis. Commun. Image R. 20 (2009) 491–503

W 01;i;j ¼ ðjZ 1;i;j j:D  Dd1 Þ  signðZ 1;i;j Þ

ð18Þ

Similarly the side description coefﬁcient of path2 is determined by
subtracting the equivalent decoder offset Dd2 as shown in Eq. (19).

W 02;i;j ¼ ðjZ 2;i;j j:D  Dd2 Þ  signðZ 2;i;j Þ

ð19Þ

The encoder offsets are computed by minimizing the quantizer distortion of the transform coefﬁcients given in Eqs. (9) and (10) and is
given below in Eqs. (20) and (21) for I- and P-frames, respectively.

Dquant ðf Þ ¼

X


2
ðpd1  ð1  pd2ÞÞ  W i;j  W 02;i;j

ði;jÞ2f


2
þ pd2  ð1  pd1Þ  W i;j  W 01;i;j

2
þ ð1  pd1Þ  ð1  pd2Þ  W i;j  W 0c;i;j

	
D
subject to ðb þ 1Þ 6 ðDe1 Þ 6
b
2

	
D
b
and to ðb þ 1Þ 6 ðDe2 Þ 6
2

2
X
a  ð1 þ pd1  ð1  pd2ÞÞ  W i;j  W 02;i;j
Dquant ðf Þ ¼

ð20Þ

ði;jÞ2f


2
þ a  ð1 þ pd2  ð1  pd1ÞÞ  W i;j  W 01;i;j

2
þ 2  a  ð1  pd1Þ  ð1  pd2Þ  W i;j  W 0c;i;j

	
D
subject to ðb þ 1Þ 6 ðDe1 Þ 6
b
2

	
D
b
and to ðb þ 1Þ 6 ðDe2 Þ 6
2

ð21Þ

The integer equivalents ðDIe1 ; DIe2 Þ of the encoder offsets ðDe1 ; De2 Þ
can be obtained by Eq. (22).

ðDe1 << qbitsÞ
D
ðDe2 << qbitsÞ
¼
D

DIe1 ¼
DIe2

ð22Þ



QP
In

QPEq.
 (22), qbits ¼ 15 þ 6 for 4  4 transform and ðqbits ¼ 16þ
Þ for 8  8 transform is used, where QP is the quantization
6
parameter.

4. Proposed system design
The block schematic of the proposed system is shown in Fig. 3.
In standard wireless protocols there are multiple ways of estimating the packet drop probability. For instance, the standard IEEE
802.11 can be conﬁgured to send ‘‘HELLO” packets at periodic
intervals of time. Depending on the number of lost packets, the
drop probability of any path can be estimated. The path failure
probability estimator block performs this function. The MDC encoder is identical to conventional encoder in most aspects with the
following differences. After the transform block, the ac coefﬁcients
are used by the model parameter estimation block to calculate the
Laplacian model parameter. This, along with the channel packet
drop, is used to solve the optimization equation by the offset calculator block, which generates the optimal encoder side offsets minimizing the overall quantization error distortion. The transformed
coefﬁcients are then quantized using the two side encoder offsets
to generate two streams, which are entropy encoded and packetized. The reconstructed frame is obtained by combining the two
quantized coefﬁcients and then performing the inverse quantization and inverse transformation. The decoder offsets are encoded
in the PPS header of the encoded bit stream, which is used during
the decoding process in case we receive bit stream from only one
path. In general, the proposed MDC scheme, like other MDC
schemes, can be extended to more than two descriptions as long
as the equivalent number of disjoint paths can be determined.
With each new descriptor, the error resilience of the scheme increases since we have more redundancy in the side descriptions.
However the incremental quality enhancement of the combined
descriptor tends to saturate as the number of descriptions is increased. Also, additional complexity is introduced in the optimization procedure to solve jointly for multiple offsets.
4.1. Computational complexity reduction
For applications running ofﬂine, as in the case of streaming
stored video, the optimization procedure explained above can be
directly applied. But for real-time applications the search procedure is computationally expensive, especially when the quantization step size is large. In order to reduce the computational
complexity, the following steps were done. The mode selection
algorithm in standard H.264/AVC encoder determines the optimum mode, which minimizes the rate-distortion cost for all the

Fig. 3. Block schematic of the proposed MDC system.

V. Parameswaran et al. / J. Vis. Commun. Image R. 20 (2009) 491–503

497

These quantization error coefﬁcients for the optimum mode are
grouped in several bins so that the later computation of the histogram can be avoided. The luminance and chrominance offsets were
coarsely determined by subtracting the ﬁxed offset from the offset

points corresponding to ð13Þ and 23 values of their respective histograms. The ﬁne level reﬁnement was performed by deﬁning a small
search range around the coarse offset values. The computation of
coarse level offsets was performed at periodic intervals of time
and the ﬁne level reﬁnement was performed only when there
was a change in the calculated coarse offset values. The procedural
ﬂow diagram of this computation is shown in Fig. 4. The transform
coefﬁcients and the best prediction modes were calculated in the
ﬁrst pass and were stored for subsequent encoding. The second
and third passes were designed to use the already calculated
modes from the ﬁrst pass. The overall system ﬂow is shown in
Fig. 5.
5. Experimental results
The performance of the proposed scheme was evaluated by
simulating the quantization process in MATLAB and also on
H.264/AVC encoder.
5.1. Simulations in MATLAB

Fig. 4. Procedural ﬂow diagram of complexity reduction in real-time MDC coding.

macroblocks in the frame. As part of this algorithm, the quantization error is calculated for all modes including the optimum mode.

A set of still images with different spatial contents was transformed into frequency domain using 8  8 DCT transform. The
transformed coefﬁcients were quantized with different offset values in each path. The packet drop probability in path2 was ﬁxed
at 15% and the packet drop probability in path1 was varied from
0%-35%. The variation of offset values for Lena and Bear image
for different packet drop values is shown in Fig. 6.

Fig. 5. System ﬂow for proposed MDC encoding.

498

V. Parameswaran et al. / J. Vis. Commun. Image R. 20 (2009) 491–503

As can be seen from the Fig. 6, the offset values in the two paths
for the same path failure probability pair is different for both
images. This indicates the dependence of offset values on the model parameter, which in turn is determined by the spatial frequency
content in the particular image. Also it can be seen that the optimal
offset values are different for different path failure probabilities.
This is because the probability of receiving the central and side
descriptions varies depending on the path failure probability,
which in turn changes the parameters of the objective function.
The results validate our hypothesis that the optimal offset values
depend both on the model parameter and the packet drop probabilities in each path.
5.2. Simulations using H.264/AVC encoder
The simulations were carried out with JM 13.1 version of H.264/
AVC software. The GOP sequence was set to contain I- and Pframes. The intra-frame refresh frequency can be varied depending
upon the path failure probabilities as suggested by Stuhlmuller in
[20]. But in our experiments we ﬁxed the frequency as 20. The per-

formance of any video encoding system, in general, can be compared under constant bit rate (CBR) or variable bit rate (VBR)
conditions. In our simulations we have used both VBR and CBR
modes for comparing the performance of different schemes. The
objective parameter, peak signal to noise ratio (PSNR), was used
for comparing the quality of the received video frames. The models
used in the paper for testing are Bernoulli and Gilbert model. The
Bernoulli model has been used extensively in literature to model
independent loss wireless channels. The Gilbert–Elliot model has
been used in literature to model channels with burst errors and
congestion like Internet [7,20]. The schemes used in the paper for
comparisons were selected based on our knowledge of the existing
MDC systems, schemes popular in network community working on
video transmissions and the reference differential quantization
scheme that is popular in the video processing research community and have been used by people for comparative purposes
[7,12,14]. The results of our simulations are summarized in the
next sub-sections.
5.3. Experiment No. 1: performance of the system under no loss
conditions
The objective of this experiment is to estimate the relation between the optimal offsets and the model parameter. In order to
achieve this, the following MDC schemes were tested under zero
packet drop conditions (1) MDC repetitive coding scheme (MDRC)
where identical descriptions were transmitted over
 both paths (2)
MDC scheme with ﬁxed offsets (MDFO) fixedoffset þ 1; D2
fixedoffsetÞ and (3) the proposed MDC scheme with adaptive offsets. The QP value was ﬁxed at 34 to simulate VBR conditions. The
results obtained are summarized in Table 2.
The addition of quantization offsets has an impact on the
encoding efﬁciency. The reason for this can be understood by having a closer look at the implementation of context adaptive variable length coder (CAVLC) [6]. Consider the situation where we
add a signiﬁcant positive offset greater than half the quantization
step size. This has two effects. The ﬁrst one is the expected increase
in the side level distortion. The second effect is the decrease in coding efﬁciency because of the fact that some of the coefﬁcients
belonging to the ﬁrst two quantization levels (level 0 and 1) in
the original codec get transformed to higher quantization levels
in the modiﬁed codec scheme. The CAVLC tables are designed so
as to encode the runs of zeroes and sequences of ±1 coefﬁcients

Table 2
PSNR comparisons for different video sequences under no loss conditions, QP = 34.
Sequence

MDC-RC based
scheme

MDC-ﬁxed offsets
based scheme

Proposed MDC
approach

Foreman

Y = 32.6 dB
U = 38.8 dB
V = 40.1 dB
(R1 + R2) = 293 kbps
Y = 36.4 dB
U = 39.8 dB
V = 41.6 dB
(R1 + R2) = 69 kbps
Y = 29.0 dB
U = 33.0 dB
V = 32.8 dB
(R1 + R2) = 854 kbps
Y = 30.5 dB
U = 38.6 dB
V = 40.0 dB
(R1 + R2) = 573 kbps
Y = 32.46 dB
U = 40.0 dB
V = 41.5 dB
(R1 + R2) = 450 kbps

Y = 33.2 dB
U = 39.7 dB
V = 41.2 dB
(R1 + R2) = 314 kbps
Y = 37.8 dB
U = 41.5 dB
V = 42.8 dB
(R1 + R2) = 81 kbps
Y = 29.7 dB
U = 35.2 dB
V = 35.0 dB
(R1 + R2) = 891 kbps
Y = 31.2 dB
U = 39.5 dB
V = 40.9 dB
(R1 + R2) = 590 kbps
Y = 32.9 dB
U = 40.9 dB
V = 42.3 dB
(R1 + R2) = 480 kbps

Y = 34.5 dB
U = 40.1 dB
V = 41.6 dB
(R1 + R2) = 386 kbps
Y = 38.4 dB
U = 42.1 dB
V = 43.5 dB
(R1 + R2) = 88 kbps
Y = 31.7 dB
U = 35.2 dB
V = 35.0 dB
(R1 + R2) = 1147 kbps
Y = 33.0 dB
U = 39.8 dB
V = 41.5 dB
(R1 + R2) = 741 kbps
Y = 34.3 dB
U = 41.4 dB
V = 42.9 dB
(R1 + R2) = 596 kbps

Akiyo

Mobile

Bus

Soccer

Fig. 6. Variation of offset values for (a) Lena image (b) Bear image, Pdpath2 = 15%,
Quantization step size = 16.

V. Parameswaran et al. / J. Vis. Commun. Image R. 20 (2009) 491–503

in an efﬁcient compact manner. Since the modiﬁed codec will have
more number of higher level coefﬁcients, the bit rate increases for
the particular description. This means that there is an upper limit
on the offset values that can be added without increasing the bit
rate signiﬁcantly.
the offsets to
 In the simulations we restricted

vary between fixedoffset þ 1; D2  fixedoffset where D is the
quantization step size.
In order to compare the rate-distortion (RD) performance, we
did some experiments with the standard H.264/AVC encoder at different QPs. The statistical results show that the bit rate increases
by around 14% with a PSNR improvement of 0.7 dB under zero
packet drop conditions. This is in conformance with the general
RD characteristics obtained for H.264/AVC in [13]. In addition to
this, the beneﬁt in PSNR for the same increase in rate decreases
to 0.5 dB at 5% drop rate, 0.3 dB at 15% drop rate and 0.2 dB for
higher packet drop rates.
The following observations can be made by referring to Table 2.
The proposed approach yields around 1.9–2.7 dB advantage over
the MDC repetitive coding scheme with around 18–30% increase
in bit rate. Using the approximate thumb rule stated above, we
can see that for achieving the same advantage in PSNR (1.9–
2.7 dB) the bit rate needs to be increased by 45–68% for the MDC
repetitive coding scheme. This implies that the proposed MDC
scheme is around 50–60% more efﬁcient than the MDC scheme
at zero packet loss conditions. Most of the redundancy in the proposed scheme is because of the replicated motion vector information in the two streams and the additional bits required for
transmitting the macroblock, slice and frame headers. The proposed approach also shows improvements when compared against
the scheme with ﬁxed offsets. This beneﬁt was dependent upon the
content in the video sequence. The advantage was around 0.6 dB
for Akiyo sequence, but it was much more pronounced (>1.4 dB)
for all other video sequences. This shows that the offsets calculated
for Akiyo sequence for the proposed scheme is close to the pre-selected ﬁxed offsets, but vary appreciably for other video sequences.
We will compare the CBR and VBR performance of some other MDC
schemes at different packet drop rates in detail in the next
experiment.
5.4. Experiment No. 2: balanced paths Bernoulli packet loss model
In this experiment we tested the performance of the proposed
system under Bernoulli packet loss model. In this case, the packet
drop probability in a particular iteration is independent of the state
of the packet in the previous iteration. The proposed system was
compared against the following coding schemes (1) Single description multiple path coding (SDMP) (2) Temporally sampled multiple
description multiple path coding (MDTS) (3) MDC scheme with
repetitive coding (MDRC) and (4) MDC scheme with ﬁxed offsets
(MDFO).
In SDMP, the encoding is done in the same way as the standard
single description single path encoder, but the odd frames are
transmitted over one path and even frames on the other path. It
should be noted that this splitting happens in the network layer
and not on the application layer. In case of MDTS, the splitting is
done on the application layer. In this case the even frame predicted
from the previous even frames is transmitted over one path and
the odd frame predicted from the previous odd frames is transmitted over the other path. At this point it is important to note the difference between MDTS and SDMP. In SDMP, the inter-predicted
frame depends on the preceding reference frames, but in MDTS
the inter-predicted frame depends on the even or odd set of previous reference frames as the case may be. This increases the bit rate
required for MDTS because of the reduced temporal correlation.
The number of I-frames required in MDTS is also more since the
two paths need to be independently decodable. The error conceal-

499

ment is also tricky in case of MDTS. This is because if say a particular frame is lost, we should replace that frame with the
immediately preceding frame from the other stream for subsequent predictions. This is indeed essential for achieving good PSNR
values. The MDTS scheme is popularly known as the Video Redundancy Coding scheme (VRC) and is one of the accepted standards
for H.263 MDC video transmissions. This solution is currently deployed in I-Share TriblerP2P software for transmitting video over
peer-to-peer networks. The implementation of MDRC, MDFO and
the proposed schemes were similar to the previous experiment.
The tests were carried out under balanced packet drop conditions, that is both paths are equally reliable. In this experiment
we considered that each frame is encoded in one packet at an
application level. This means that in case the packet is lost, we assume that entire frame is lost. The tests were performed at CBR
conditions for all the MDC schemes. The results obtained for 150
frames of the Bus and the Mobile sequence are shown in Figs. 7
and 8, respectively.
The following inferences can be made by referring to Figs. 7, and
8.
 The SDMP scheme is the best performing MDC scheme at zero
packet drop conditions. This is because there is no redundancy
in this scheme and its performance is exactly the same as the
standard single path single description encoder. The performance of the SDMP scheme deteriorates rapidly with increase
in packet drop rates.
 The MDTS or the VRC scheme performs second best at zero
packet drop rate, but it starts performing substantially better
than the SDMP scheme at around 3% packet drop rate. This is
because in MDTS scheme the descriptions in a particular path
are unaffected by the packet drops in the other path which
reduces the drift error propagation for the MDTS scheme.
 The MDRC scheme has even higher error resilience than the
MDTS scheme, but performs the poorest at low packet drop
rates. This is because one description is discarded in MDRC in
case both descriptions arrive at the decoder, which is very likely
at low packet drop rates.
 The MDFO scheme performs better than the MDRC scheme for
chrominance components. The advantage for luminance component is around 0.5 dB at zero packet drop rate and progressively
decreases at higher packet drop rates.
 The proposed MDC approach performs third best at zero packet
drop conditions but starts performing signiﬁcantly better than
the other MDC schemes as the packet drop probability is
increased. For the Bus sequence, the proposed scheme gives
the best Y-PSNR and V-PSNR starting around 3% packet drop
rate. The scheme starts giving the best U-PSNR starting around
12% packet drop rate. For the Mobile sequence, the proposed
scheme gives the best results for all components at even 3 %
packet drop rate. It can also be observed that the proposed
scheme gives signiﬁcant beneﬁt for luminance component when
compared with MDFO scheme, but has almost similar performance for the chrominance components. This indicates that
the offsets determined by the proposed approach is closer to
the ﬁxed offsets used in MDFO for the chrominance component,
but differs substantially for the luminance component. The content dependent gains in reconstructed video quality for the proposed scheme over MDFO indicates that the statistics of the
video content also effects the offset determination and must
be considered in the obtaining the optimal offset as done in
the proposed scheme.
In order to compare the performance of the MDC schemes under VBR conditions, the packet drop probability in both paths
was set at ﬁxed values. The quantization parameter in each path

500

V. Parameswaran et al. / J. Vis. Commun. Image R. 20 (2009) 491–503

Fig. 7. PSNR comparisons under balanced packet drop conditions (a) Y-PSNR Bus
sequence (b) U-PSNR Bus sequence (c) V-PSNR Bus sequence, Total bit rate
(R1 + R2): 0.41 BPP (bits per pixel).

was varied to get videos encoded at different quality. The R–D results obtained for the luminance component for Bus sequence is
plotted in Fig. 9.
The following inferences can be made from Fig. 9. At 0% packet
drop rate, the SDMP scheme performs the best followed by MDTS,
the proposed MDC scheme, MDFO and MDRC. This is expected
since the redundancy is minimal in case of SDMP scheme. At higher
packet drop rates 5% and 25%, the performance of the SDMP
scheme degrades rapidly and it has the worst performance among
all MDC schemes. The proposed approach gives the best perfor-

Fig. 8. PSNR comparisons under balanced packet drop conditions (a) Y-PSNR
Mobile sequence (b) U-PSNR Mobile sequence (c) V-PSNR Mobile sequence, Total
bit rate (R1 + R2): 1.36 BPP.

mance at 5% and 25% packet drop rates. The advantage of the proposed scheme over the MDRC scheme degrades slightly at high
packet drop rates. This is because at high packet drop rates, the errors due to packet loss and propagation starts dominating compared with the quantization errors. The MDRC scheme has the
least propagation error since the descriptions in two paths are exactly identical. It can also be noted that MDRC scheme performs
signiﬁcantly better than SDMP and MDTS schemes at high packet
drop rates. The above results indicate that the proposed MDC
scheme provides the best trade off between redundancy and error
resilience for video transmissions over paths with varying packet
drop rates.

V. Parameswaran et al. / J. Vis. Commun. Image R. 20 (2009) 491–503

5.5. Experiment No. 3: unbalanced paths Bernoulli packet loss model
In this experiment we ﬁxed the packet drop probability in path1
as 5% and varied the drop probability in the other path from [0%,
35%]. The simulations were carried out under CBR conditions.
The packet drop probability in each path for all iterations was kept
independent as in the case of the Experiment No. 2. The Y-PSNR
values obtained for 150 frames of the Bus and the Mobile sequence
is shown in Fig.10. The PSNR obtained for the standard codec by
transmitting in path1 alone is shown in dotted lines for comparison purposes. The following observations can be made from the

501

ﬁgure. The SDMP scheme performs slightly better than the standard single path scheme when the packet drop probability in path2
is 0%, but its performance quickly degrades as the packet drop
probability is increased. The MDTS scheme is more error resilient
and its performance degrades only when the packet drop probability in path1 exceeds 15%. These two results are quite intuitive because the number of dropped packets increases as the packet
failure probability in path2 is increased. On the contrary, the
MDRC, MDFO and the proposed MDC schemes always perform signiﬁcantly better than the case where the description is transmitted
over only one path. This clearly indicates the advantage of these
MDC schemes owing to path redundancy. The proposed MDC
scheme shows better performance than the MDC with repetitive
coding and the MDC scheme with ﬁxed offsets as in Experiment
No. 2. This is because the proposed scheme adapts the offsets
depending on the statistics of the input source and the channel
path failure probabilities. Similar improvements in PSNR were
noted for the chrominance components as well.
5.6. Experiment No. 4: comparisons under burst error conditions
The earlier experiments assumed that the packet drop in a particular path at any time instance t is independent of the state of the
packet at the previous time instances ðt  t prev Þ or alternatively the
channel is memoryless. This is not generally true in case of wireless
networks since the errors tend to be bursty. A lot of models have
been previously proposed to estimate the packet drop probability

Fig. 9. R–D results under balanced packet drop conditions, X-axis contains the total
bit rate (R1 + R2) measured in BPP (a) Y-PSNR at 0% drop (b) Y-PSNR at 5% drop (c)
Y-PSNR at 25% drop.

Fig. 10. Obtained Y-PSNR values under unbalanced packet drop conditions,
pd1 = 5% (a) Y-PSNR for Bus sequence, Total bit rate (R1 + R2):0.42 BPP (b) Y-PSNR
for Mobile sequence, total bit rate (R1 + R2):1.36 BPP.

502

V. Parameswaran et al. / J. Vis. Commun. Image R. 20 (2009) 491–503

under burst error conditions. The most widely used model is the
Gilbert–Elliott model described in [4]. This is a 2-state Markov
model that transitions between the good state 0 and bad state 1
with state transition probabilities p and k. In the original Gilbert–
Elliott model the state 0 is the low loss state with an independent
packet drop probability pe0 and the state 1 is a high loss state with
a different independent packet drop probability pe1 . In the simpliﬁed version of this model, termed as the simple Gilbert model, the
state 0 is assumed to be a zero loss state while the state 1 is assumed to be completely faulty. The simpliﬁed Gilbert model is
shown in Fig. 11.
The important parameters of this model are the average packet
loss rate and the mean burst length. The Y-PSNR values obtained
for 150 frames of the Bus and Mobile sequence for different burst
lengths at CBR conditions with average packet drop probability 5%
is shown in Fig. 12. For SDMP and MDTS schemes, the PSNR values
were highly dependent on the relative position of the dropped
frame in the group of pictures sequence (GOP IPPPPP . . . IP . . .). This
can be understood in the following manner. If we assume that the
GOP length is 20 and we assume that the frame drop happens at
the ﬁfth frame, then all the frames following Frame 5 will be affected because of drift errors. The impact of drift error will be lesser
if instead of Frame 5 say Frame 19 is dropped because it does not
produce any impact on the following frame. The PSNR values
shown in Fig. 12 for MDTS and SDMP schemes were averaged over
multiple iterations by dropping frames at different locations. The
MDRC, MDFO and the proposed MDC approach have less dependence on the starting location of the dropped frame. This is because
even though the packet drop in a particular is correlated, the packet drop between paths is independent. Since MDRC, MDFO and the
proposed MDC schemes contain information about every frame in
each path, the probability that the same frames are dropped in
both paths is still small and hence the PSNR values are better
and more stable for different burst lengths as can be seen from
the Fig. 12. The proposed approach still gives around 1.2 dB beneﬁt
compared with MDRC scheme.

Fig. 12. Obtained Y-PSNR values under Gilbert packet drop conditions,
pd1 = pd2 = 5% (a) Y-PSNR for Bus sequence, Total bit rate (R1 + R2):0.42 BPP (b) YPSNR for Mobile sequence total bit rate (R1 + R2):1.36 BPP.

5.7. Experiment No. 5: effect of slice level partitioning
In this section we exploit the hierarchical frame-macroblock
structure in H.264/AVC to conceal errors in a better manner in case
of packet drops. The hierarchical frame-macroblock structure of
H.264/AVC system allows a frame to be encoded with one or more
slice groups. Each slice group contains one or more slices and each
slice, in turn, can contain multiple macroblocks. The slice can be
considered as the network equivalent of a packet. The frame can
be partitioned into slices on an application level by specifying
the slice size, in terms of number of macroblocks or number of bytes.The maximum size of the packet is, in general, determined by
the type of physical network interfaces. For instance in the case
of 802.11, the maximum packet size is limited to 2346 bytes. In
case the frame size exceeds the maximum packet size, the network
layer splits the frame into multiple packets. The problem of split-

Fig. 11. Simple Gilbert model.

ting the packets on the network layer for video transmission is that
the information in each packet is not necessarily self contained because of the inter-macroblock prediction dependencies. In this
case, even if one constituent packet of a frame is lost, the entire
frame becomes non-decodable and the error concealment will
have to be performed on a frame level.
In order to reduce this effect, it is better to do packetization on
an application level. The application layer analog of the packet is
termed as slice. The size of a slice can be speciﬁed in terms of
the number of macroblocks or number of bytes. The advantage of
having slice is that the macroblocks belonging to a particular slice
are completely independent of the macroblocks present in a different slice. In this case even if one packet is lost, the frame can be
partially decoded and the error concealment can be performed
on a slice level. The drawback of this approach is the additional
overhead because of the slice headers and the reduction in coding
efﬁciency because of the slice classiﬁcation which translates into
an increase in data rate.
In the case of MDC, the slice classiﬁcation in both paths needs to
match with each other to perform effective error concealment. The
methodology of MDC slice level error concealment used for simulations was as follows. In case the description from path1 was lost,
the information from the path2 description was used to reconstruct the particular slice and vice versa. In order for this to happen
effectively, the same slice structure needs to be followed in both
paths. Hence it is better to specify the slice in terms of number
of macroblocks. In case both descriptions were lost, the slice copy
was done from the respective macroblocks in the previous frame.

V. Parameswaran et al. / J. Vis. Commun. Image R. 20 (2009) 491–503

[3]

[4]
[5]
[6]

[7]

[8]
[9]
Fig. 13. Obtained Y-PSNR values with and without MDC slice level error concealment for different packet sizes, QP = 34.

[10]
[11]

The results obtained with and without slice level error concealment for two different packetization modes is shown in Fig. 13.
It is evident from results, encoding a frame in multiple slices provides good error resiliency and can signiﬁcantly enhance the performance of the proposed MDC scheme which also adds to the
gains through path diversity and redundancy in the transmitted
streams.

[12]

[13]

[14]

6. Conclusion
[15]

In this paper, we proposed to use adaptive quantization offsets
in multiple description coding for achieving better video quality. In
the proposed approach, optimal quantization offsets for the two
channels of MDC are determined by considering not only the statistics of the input stream but also the path failure probabilities.
Experiments using an implementation of the proposed scheme
based on the JM 13.1 version of H.264/AVC codec clearly demonstrated the advantages of the proposed scheme compared with
other MDC schemes.
This method can be extended to the region of interest based
MDC scheme proposed in [16]. In the referenced paper, the foreground information is segmented out based on the motion vectors
and the distortion cost and is ﬁne quantized compared with rest of
the background. The foreground constitutes the important slice
group and the background forms the less important slice group.
We can apply the adaptive offset scheme by determining the optimum offsets on a slice group level instead of a frame level. This
information could be encoded in the PPS header information for
further processing by the decoder. The channel error correcting
codes could also be employed to further enhance the error resilient
capability of the proposed MDC scheme. The design ﬂexibility of
the proposed approach appears to be promising and we believe
that the scheme will serve as a reliable framework for transmitting
video over wireless ad hoc networks.
References
[1] Y. Altunbasak, N. Kamaci, An analysis of the DCT coefﬁcient distribution with
the H.264 video coder, in: Proceedings of IEEE International conference on
Acoustics, Speech, and Signal Processing, May 2004, Montreal, Canada, pp.
177–180.
[2] E. Ayanoglu, P. Pancha, A. Reibman, S. Talwar, Forward error control for MPEG2 video transport in a wireless ATMlan, in: Proceedings of International

[16]

[17]

[18]
[19]

[20]

[21]

[22]

[23]
[24]

[25]
[26]
[27]
[28]

[29]

[30]

503

Conference on Image Processing, September 1996, Lausanne, Switzerland, pp.
833–836.
R. Bernardini, M. Durigon, R. Rinaldo, A. Vitali, Comparison between multiple
description and single description video coding with forward error correction,
in: Proceedings of IEEE 7th Workshop on Multimedia Signal Processing,
October 2005, Shanghai, China, pp. 1–4.
E.O. Elliott, A model of the switched telephone network for data
communications, in: Proceedings of BSTJ, January 1965, p. 44.
V. Goyal, Multiple description coding: compression meets the network, IEEE
Signal processing magazine 18 (5) (2001) 74–93.
V.H.S Ha, S. Woo-Sung, J. Kim, Real-time MPEG-4 AVC/H.264 CABAC entropy
coder, in: Proceedings of International Conference on Consumer Electronics,
January 2005, pp. 255–256.
B. Heng, J. Apostolopoulos, J. Lim, End-to-end rate-distortion optimized md
mode selection for multiple description video coding, EURASIP Journal on
Applied Signal Processing 5 (2006) 12.
H. Jafarkhani, V. Tarokh, Multiple description trellis-coded quantization, IEEE
Transactions on Communications 47 (6) (1999) 799–803.
L. Kondi, F. Ishtiaq, A.K. Katsaggelos, Joint source-channel coding for scalable
video, in: Proceedings of SPIE Conference On Visual Communications and
Image Processing, January 2000, San Jose, CA, pp. 108–109.
G. Lakhani, Distribution-based restoration of dct coefﬁcients, IEEE Transactions
on Circuits and Systems for Video Technology 10 (5) (2000) 819–823.
E.Y. Lam, J.W. Goodman, A mathematical analysis of the DCT coefﬁcient
distributions for images, IEEE Transactions on Image Processing 9 (10) (2000)
1661–1666.
Y.C. Lee, Y. Altunbasak, R.M. Mersereau, Coordinated application of multiple
description scalar quantization and error concealment for error-resilient
MPEG video streaming, IEEE Transactions on Circuits and Systems for Video
Technology 15 (4) (2005) 457–468.
S. Ma, Wen Gao, Yan Lu, Rate-distortion analysis for h264/avc video coding and
its application to rate control, IEEE Transactions on Circuits and Systems for
Video Technology 15 (12) (2005) 533–1544.
Sudheendra Murthy, Prasad Hegde, Viswesh Parameswaran, Baoxin Li,
Arunabha Sen, Improved path selection algorithms for multi-path video
streaming in wireless ad hoc networks, in: Proceedings of IEEE Military
Communications Conference, October 2007, Orlando, FL, pp. 1–7.
N.C. Oguz, E. Ayanoglu, Performance analysis of level forward error correction
for lost cell recovery in ATM networks, in: Proceedings of IEEE INFOCOM, April
1995, Boston, Massachusetts, pp. 728–737.
Viswesh Parameswaran, Sudheendra Murthy, Arunabha Sen, Baoxin Li, An
adaptive slice group multiple description coding technique for real-time video
transmission over wireless networks, in: Proceedings of IEEE Military
Communications Conference, October 2007, Orlando, FL, pp. 1–7.
A.R. Reibman, Optimizing multiple description coders in a packet loss
environment, in: Proceedings of 12th International Packet Video Workshop,
April 2002, Pittsburgh, PA.
I. Richardson, vocdex: H.264 tutorial white papers, 2003.
R. Zhang, S.L. Regunathan, K. Rose, Video coding with optimal inter/intra mode
switching for packet loss resilience, IEEE Journal on Selected Areas in
Communication 18 (6) (2000) 966–976.
K. Stuhlmuller, N. Farber, M. Link, B. Girod, Analysis of video transmission over
lossy channels, IEEE Journal on Selected Areas in Communications 18 (6)
(2000) 1012–1032.
V.A. Vaishampayan, J. Domaszewicz, Design of entropy-constrained multipledescription scalar quantizers, IEEE Transactions on Information Theory 40 (1)
(1994) 245–250.
V.A. Vaishampayan, N.J.A. Sloane, S.D. Servetto, Multiple-description vector
quantization with lattice codebook: design and analysis, IEEE Transactions on
Information Theory 47 (5) (2001) 718–1734.
V. Vaishampayan, Design of multiple description scalar quantizers, IEEE
Transactions on Information Theory 39 (5) (1993) 821–834.
Y. Wang, M.T. Orchard, V.A. Vaishampayan, A.R. Reibman, Multiple description
coding using pairwise correlating transforms, IEEE Transactions on Image
Processing 10 (3) (2001) 351–366.
Y. Wang, J. Ostermann, Y. Zhang, Book on Video processing and
Communications, second ed., Prentice Hall, 2002.
Y. Wang, J.WenS. Wenger, A.K. Katsaggelos, Error resilient video coding
techniques, IEEE Signal Processing Magazine 17 (4) (2000) 61–82. July.
T. Wedi, S. Wittmann, Quantization offsets for video coding, in: Proceedings of
IEEE International Symposium on Circuits and Systems, 23–26 May 2005.
T. Wiegand, G. Bjntegaard, G. Sullivan, A. Luthra, Overview of the H264/AVC
video coding standard, IEEE Transactions on Circuits and Systems for Video
Technology 13 (7) (2003) 560–576. July.
C.W. Yap, K.N. Ngan, R. Liyanapathirana, A combined source-channel video
coding scheme for mobile channels, Signal Processing: Image Communication
14 (6) (1999) 559–574.
ITU-T Rec.H.264, Advanced video coding for generic audio-visual services,
March 2003.

J. Vis. Commun. Image R. 33 (2015) 286–300

Contents lists available at ScienceDirect

J. Vis. Commun. Image R.
journal homepage: www.elsevier.com/locate/jvci

Joint modeling and reconstruction of a compressively-sensed set
of correlated images
Kan Chang a,⇑, Baoxin Li b
a
b

School of Computer and Electronic Information, Guangxi University, Nanning, Guangxi 530004, China
Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85287, USA

a r t i c l e

i n f o

Article history:
Received 5 October 2014
Accepted 28 September 2015
Available online 9 October 2015
Keywords:
Compressive sensing
Correlated images
Intra-image correlation
Inter-image correlation
Inter-channel correlation
Total variation
Non-local means
Group sparsity

a b s t r a c t
Employing correlation among images for improved reconstruction in compressive sensing is a conceptually attractive idea, although developing efficient modeling strategies and reconstruction algorithms are
often the key to achieve any potential benefit. This paper presents a novel modeling strategy and an efficient reconstruction algorithm for processing a set of correlated images, jointly taking into consideration
inter-image correlation, intra-image correlation and inter-channel correlation. The approach starts with
joint modeling of the entire image set in the gradient domain, which supports simultaneous representation of local smoothness, nonlocal self-similarity of every single image, and inter-image correlation. Then
an efficient algorithm is proposed to solve the joint formulation, using a Split-Bregman-based technique.
Furthermore, to support color image reconstruction, the proposed algorithm is extended by using the
concept of group sparsity to explore inter-channel correlation. The effectiveness of the proposed
approach is demonstrated with extensive experiments on both grayscale and color image sets. Results
are also compared with recently proposed compressive sensing recovery algorithms.
Ó 2015 Elsevier Inc. All rights reserved.

1. Introduction
The theory of compressive sensing (CS) [1,2], which enables
sub-Nyquist sampling rates, has attracted considerable research
interests from signal processing communities in recent years. By
incorporating signal acquisition with compression, it can significantly improve the energy efficiency of sensors. Based on the CS
theory, several real CS-based imaging systems have been built,
including the single-pixel camera [3], the compressive spectral
imaging system [4], and the high-speed video camera [5], etc.
However, perfectly reconstructing the images from a small number
of random measurements is still a big challenge due to the illposed nature of inverse problems.
In this paper, we study the problem of reconstructing a set of
correlated images, each of which is independently acquired by
the CS technique. Such a set of images could be multi-view images
which represent a scene from different view points, or a series of
video frames which are taken at different time points. The former
situation occurs in scenarios where the cameras are densely
deployed but with strong power consumption constraints, e.g.,
wireless multimedia sensor networks (WMSN), while the latter
⇑ Corresponding author.
E-mail addresses: changkan0@gmail.com (K. Chang), baoxin.li@asu.edu (B. Li).
http://dx.doi.org/10.1016/j.jvcir.2015.09.020
1047-3203/Ó 2015 Elsevier Inc. All rights reserved.

one could appear in scenarios like commercial video processing
cameras, video surveillance or dynamic magnetic resonance imaging (MRI) for medical imaging. It is no doubt that one can treat
each image independently and use the reconstruction algorithms
designed for single CS-imaging systems, e.g., [6–9]. However, if
we can take advantage of the high correlation between images,
the quality of the reconstructed images could be potentially
improved.
There have been many studies focussing on the problem of CS
reconstruction of correlated images. We roughly review three
kinds of methods as follows:
 Techniques of the first kind rely on joint sparsity of the whole
image set. For examples, Ma et al. [10] applied a modification
of the approximate message passing algorithm and proposed
to incorporate three-dimensional (3-D) dual-tree complex
wavelet transform during reconstruction. Li et al. [11] recovered
video cubes by minimizing total variation (TV) of the pixelwise
discrete cosine transform (DCT) coefficients along the temporal
direction. Hosseini et al. [12] proposed an alternative TV
regularization model that utilizes high-order-of-accuracy differential finite impulse response (FIR) filter. Then the proposed
TV was extended to tensorial representation to encode video
features by decomposing each space–time dimension.

K. Chang, B. Li / J. Vis. Commun. Image R. 33 (2015) 286–300

 The second kind of techniques relies on the estimation of geometric transformations between images. For instance, Park
and Wakin [13] used a manifold lifting algorithm to obtain a
first estimation of the transformation parameters and a standard CS algorithm for the estimation of the images. In order
to reduce computational complexity, Thirumalai and Frossard
[14] estimated correlation between neighboring images directly
from the linear measurements by the graph cut algorithm. Puy
et al. [15] modeled each observed image as the sum of a geometrically transformed background image and a foreground
image. Then a reconstruction method was proposed to jointly
estimate the background and foreground images and the transformation parameters by minimizing a nonconvex functional.
 Techniques of the third kind use the neighboring images
(usually spatially or temporally close to the current one) to
represent the current image. The most direct ways to do representation are disparity estimation (DE)/disparity compensation
(DC) for multi-view imaging and motion estimation (ME)/
motion compensation (MC) for video sequences. Mun et al.
[16] performed block-based ME between reconstructed
neighboring frames. To guarantee a more precise prediction,
iteratively processing estimation–reconstruction was needed.
Trocan et al. [17] applied the optical flow method to perform
DE instead of the simple block matching method, and proposed
to reconstruct the residual value in TV domain. Similar method
was also proposed in [18], where the prediction from a neighboring image was applied as an adaptive transform to form a
joint optimization problem for all images. Other ways to do
representation include Karhunen–Loève transform (KLT)-based
method [19], dictionary learning-based method [20] and
multi-hypothesis (MH) prediction-based method [21], etc.
Among the three kinds of techniques discussed above, the last
one usually leads to a better quality of reconstruction. The reason
lies in that the correlation within image set is explored in
image–image pattern, providing guaranteed robustness. Although
inter-image correlation is extensively studied for CS reconstruction
of correlated images, intra-image correlation, i.e., structured prior
knowledge of images, is not explored well or is neglected in most
existing methods. When inter-image correlation is not reliable,
the quality of reconstruction would drop (this situation happens
frequently, e.g., large disparity/motion occurs in the image sets,
or the images are sampled at low subrates, making accurate
estimation of inter-image correlation impossible). On the other
hand, if color images are considered, there also exists strong
inter-channel correlation which can be taken advantage of.
Unfortunately, as far as we know, none of the methods
designed for CS reconstruction of correlated images considers
inter-channel correlation, while works like [22–24] only focuss
on single color image reconstruction.
In this paper, we propose a novel strategy for high-fidelity CS
reconstruction of correlated image set by jointly characterizing
inter-image, intra-image and inter-channel correlation. Our main
contributions are listed as follows. Firstly, we establish a joint
modeling (JM) for correlated images, where inter-image correlation and intra-image structured prior knowledge including local
smoothness and nonlocal self-similarity are explored simultaneously to ensure a more reliable and robust estimation. Then a
new Split-Bregman-based algorithm is developed to efficiently
solve the inverse problem where the minimization functional is
formulated by using JM under regularization-based framework.
After that, for CS reconstruction of color images, by utilizing the
concept of group-sparsity, inter-channel correlation is additionally
taken into consideration, and the proposed algorithm is further
extended as a joint-channel reconstruction manner. Here we need
to point out that the goal of joint modeling in this paper is to

287

explore different kinds of correlation within correlated image sets,
so that the compressively-sensed image sets can be accurately
recovered; while the modeling in [25], the sparse representationbased method in [26], and the learning methods in [27,28] aim
to extract multiple features for applications such as image annotation and human motion recognition.
The remainder of the paper is organized as follows. Section 2
briefly reviews the related background knowledge. Section 3 elaborates the design of JM for correlated images. The new functional
which contains regularization term formed by JM, and the details
of solving inverse problem are provided in Section 4. Section 5
extends the proposed CS reconstruction algorithm to support
reconstruction of color images. Extensive experimental results
and discussion are given in Section 6. Finally, we summarize this
paper in Section 7.
2. Background
Consider an imaging system which independently takes measurements for each image in a correlated image set. The sensing
procedure for the ith image can be written as

y i ¼ Ui u i

ð1Þ

where ui is an N-length vector, which denotes the original ith
image; yi contains M measurements of ui and Ui is the measurement matrix, whose size is M  N. We call M=N the subrate of CS,
and decreasing the subrate causes the system to become highly
ill-conditioned and eventually underdetermined.
To reconstruct the underlying images from such an underdetermined system, one common way is to employ image prior knowledge for regularizing the solution to the following minimization
problem

minWðui Þ
ui

s:t: yi ¼ Ui ui

ð2Þ

where W is the regularization term denoting image prior. Usually,
this problem is further transformed to the following unconstrained
optimization problem

k
minWðui Þ þ kyi  Ui ui k22
ui
2

ð3Þ

where k is the regularization parameter that controls the tradeoff
between the data fidelity term and the regularization term.
The most current CS reconstruction algorithms explore the prior
knowledge that a natural image is sparse in some domains, such as
DCT, wavelets and the gradient domain utilized by the TV norm
[29]. Particularly, TV-based CS reconstruction algorithms, which
explore image local structural information and achieve state-ofthe-arts results, turn (3) to

k
minkDui k1 þ kyi  Ui ui k22
ui
2

ð4Þ

T

where D ¼ ½DTh ; DTv  and Dh ; Dv denote horizontal and vertical finite
difference operators, respectively.
In natural images, there might exist many pixels which are
very similar to each other in values but spatially far away from
a current pixel. This phenomenon is named as the nonlocal
structure [30], and it has been successfully utilized to enhance
the performance of TV-based CS reconstruction by applying the
nonlocal means (NLM) filter via regularization, which is represented as

Wðui Þ ¼ kui  wi ui kpp

ð5Þ

where wi is the weight matrix. Assume that uiðj1 Þ ; uiðj2 Þ are two
pixels at index j1 and j2 in image ui , respectively, the weight at
ðj1 ; j2 Þ in wi is calculated by

288

K. Chang, B. Li / J. Vis. Commun. Image R. 33 (2015) 286–300
2

2

wiðj1 ;j2 Þ ¼ ekuiðj1 Þ uiðj2 Þ k2 =h =cj1

ð6Þ

where h is a controlling factor of the weight, cj1 is the normalization
factor, uiðj1 Þ and uiðj2 Þ are bs  bs blocks with uiðj1 Þ and uiðj2 Þ as central
pixels, respectively. Note that p in (5) is set as 2 in [9,7], while p ¼ 1
in [6,8].
Besides the local and the nonlocal prior information, there
exists strong redundance among correlated images. One common
way to explore the inter-image structured sparsity is applying
estimation and compensation among correlated images. Let the
compensated result for the ith image be si , the residual of the ith
image is reconstructed, i.e., problem (2) is changed to

minWðri Þ
ri

s:t: yi ¼ Ui ðri þ si Þ

ð7Þ

where ri ¼ ui  si .
Solving problem (7) usually can achieve much better result than
(2). The fundamental insight is that if the estimation is accurate
enough, residual will be much more sparse than the original image.
Note that the residual reconstruction can be operated either image
by image [17,16,21] or in a joint reconstruction way [18].
However, every prior information discussed above only
characterizes one aspect of prior knowledge of the correlated
image set. In this paper, we try to take advantage of all of them
in an efficient way, so that the optimum reconstruction quality
can be achieved.
3. Joint modeling for reconstruction of correlated images
As mentioned before, to obtain optimal reconstruction
quality, both inter-image correlation and intra-image correlation
should be considered for a correlated image set. For every image,
two types of intra-image correlation including the local and the
nonlocal image properties can be applied. The former type
describes smoothness within local regions, while the latter
depicts the repetitiveness of textures or structures in natural
images.
In this study, we propose a joint modeling (JM) strategy for
high-fidelity reconstruction of correlated images by characterizing
the above three properties of a correlated image set. Specifically,
the proposed JM is established by merging three complementary
models: intra-image local modeling (ILM), intra-image nonlocal
modeling (INM), and inter-image correlation modeling (ICM). For
the ith image, that is

WJM ðui Þ ¼ WILM ðui Þ þ aWICM ðui ; ui1 ; uiþ1 Þ þ bWINM ðui Þ

ð9Þ

The reason lies in that the first derivative is sensitive to noise.
Thus minimizing (9) would result in very ‘‘clean” image.
When modeling inter-image correlation, we use the two adjacent images of the ith image as references to form its inter-image
prediction, and require the prediction error to be small in the
gradient domain, i.e.,

WICM ðui ; ui1 ; uiþ1 Þ ¼ kDðui  si Þk1
where

ð10Þ

1
ðPðui1 ; ui Þ þ Pðuiþ1 ; ui ÞÞ
2

ð11Þ

Pðui1 ; ui Þ means calculating the prediction for the ith image by
using the ði  1Þth image as reference. Such procedure can be
interpreted as DE/DC for multi-view images or ME/MC for video
sequences. Since references from both direction are used, Eq. (11)
can obtain a more precise prediction than the single-direction prediction. Similar to (7), here we want the prediction error to be small.
Measuring the prediction error in the gradient domain would
provide a more sparse representation than in the pixel domain
and also increase its sensitivity to noise.
Now we discuss more about why measuring ui ; ui  wi ui and
ui  si in the gradient domain. Typically, in image restoration problems, a major concern is to preserve important image features,
such as those most easily detected by the human visual system,
while removing noise. One such important feature is the edges,
which happen for instance at object boundaries. To preserve sharp
edges in images while removing noise and other unwanted fine
scale detail, TV-based image restoration model, which would
succeed when the gradient of the underlying signal or image is
sparse, is an useful tool and has long history [31]. Recent research
has also confirmed that using TV regularization in CS problems
makes the recovered image quality sharp by preserving the edges
or boundaries more accurately [2,32], which is essential to characterize images. Compared with TV-based method, wavelet-based
approach is relatively new for image restoration problems and
came from a different school. Although research demonstrated that
analysis-based wavelet frame can be viewed as a discrete approximation at a given resolution to variational methods (e.g., the TVbased method) [33], in practice, for CS reconstruction problems,
works such as [8,34] presented that requiring either the images
or the residual images to be sparse in the gradient domain usually
outperforms both in wavelet and in DCT domain. Besides, the
experimental results of this work also show that the gradient
domain-based single color image reconstruction algorithm
performs much better than the wavelet domain-based methods
(in Table 2, TV-MMV greatly outperforms IHT-MMV and SL20).
Therefore, in this work, the gradient domain is chosen in local, nonlocal and inter-image modeling.
To facilitate uniform representation, inspired by [18], assuming
given disparity vectors (DV) or motion vectors (MV), we define two
types of transformation for the compensation operations from both
directions, with which (11) can be re-written as

ð8Þ

where a and b are regularization parameters, which control the
trade-off among three competing regularization terms. WILM ðui Þ represents the local smoothness prior within a image and keeps image
local consistency; WINM ðui Þ corresponds to nonlocal modeling of
every image; WICM ðui ; ui1 ; uiþ1 Þ explores inter-image correlation
between the ith image and its two adjacent images.
We use the TV regularization as WILM ðui Þ. However, for WINM ðui Þ,
different from (5), we require the difference between ui and its filtered result wi ui to be small in the gradient domain, i.e.,

WINM ðui Þ ¼ kDðui  wi ui Þk1

si ¼

si ¼

1
ðFi1 ui1 þ Biþ1 uiþ1 Þ
2

ð12Þ

where Fi1 and Biþ1 denote forward and backward compensation
operators by using the DV/MV from the ði  1Þth image and the
ði þ 1Þth image, respectively.
Thus, the JM for the ith image can be summarized as

1
WJM ðui Þ ¼ kDui k1 þ akDðui  ðFi1 ui1 þ Biþ1 uiþ1 ÞÞk1
2
þ bkDðui  wi ui Þk1

ð13Þ

To better illustrate how the proposed JM describes the properties of a correlated image set, Fig. 1 visualizes ui ; ui  si and
ui  wi ui in the horizontal gradient domain for the 2th view of
image set Monopoly, where black dots represent high gradient
magnitude. Note that because only a few measurements are taken
for reconstruction, the CS-recovered image is noise-corrupted.
Therefore here we add different levels of Gaussian noise to the
original images to simulate the images recovered from CS measurements. From Fig. 1, one can observe that

289

K. Chang, B. Li / J. Vis. Commun. Image R. 33 (2015) 286–300

Fig. 1. Visualization of u2 ; Dh u2 ; Dh ðu2  s2 Þ and Dh ðu2  w2 u2 Þ with different levels of noise for Monopoly dataset. From left to right: u2 ; Dh u2 ; Dh ðu2  s2 Þ; Dh ðu2  w2 u2 Þ.
From top to bottom: noise standard deviation = 0, 10, and 20.

1. Dh ui is sparse and very sensitive to both noise and edges in the
images. So if the regularization term only takes ILM, the
resulted image would be oversmoothed.
2. The result of Dh ðui  si Þ depends on image contents and the differences between adjacent images. It is less sensitive to edges
and usually more sparse than Dh ui . However, if the accuracy
of si cannot be guaranteed, only minimizing ICM would not
result in satisfactory quality of the reconstructed image.
3. Dh ðui  wi ui Þ looks like white noise when compared with Dh ui
and Dh ðui  si Þ. The reason lies in that wi ui is the denoised version of ui , and transforming ui  wi ui to the gradient domain
makes it more sensitive to noise. Since Dh ðui  wi ui Þ reveals a
different aspect of image structure, apart from ILM and ICM,
minimizing INM is also necessary.
While the above modeling has the desired property of capturing
all types of correlations that may exist in a set of correlated image,
one challenge remain to be addressed: developing an efficient
solution under this framework. In the next section, to make JM
tractable and robust, a new Split-Bregman-based iterative algorithm is developed to efficiently solve the minimization problem
with JM as regularization term. Then the evaluation of the model
and the algorithm will be reported in Section 6.
4. Proposed algorithm for reconstruction of correlated images
4.1. Joint modeling-driven Split-Bregman iteration
By incorporating (13) into (3), we have the following minimization problem

 

X


1
min kDui k1 þ a
D ui  ðFi1 ui1 þ Biþ1 uiþ1 Þ 


ui
2
1
n
k
þ bkDðui  wi ui Þk1 þ kyi  Ui ui k22
2

ð14Þ

Note that, for the first image and the nth image, only one direction
prediction is available.
Problem (14) is a joint reconstruction problem and it is not easy
to solve, so we change its form to

k
minkDd Uk1 þ akDd ðU  CUÞk1 þ bkDd ðU  WUÞk1 þ kY  HUk22
U
2
ð15Þ
where

3
2 3
2
y1
U1
u1
6u 7
6y 7
6 0
6 27
6 27
6
7
6 7
6
U¼6
6 .. 7; Y ¼ 6 .. 7; H ¼ 6 ..
4 . 5
4 . 5
4 .
2

0
U2
..
.

un
0
0
yn
2
3
2
w1 0
D 0  0
6 0 w
6 0 D  0 7
2
6
7
6
6 .
7
Dd ¼ 6
;
W
¼
.7
..
6 .
6 .. .. . .
4 .
. .. 5
4. .
.
0 0  D
0
0
3
2
0
B2
0

0
6 F =2 0
B3 =2

0 7
7
6 1
7
6
6 ..
.
.
.
.
..
..
..
. 7
C¼6 .
7:
.
7
6
7
6
0
Bn =2 5
   Fn2 =2
4 0
0

0
Fn1
0



..
.

0
0
..
.

3
7
7
7;
7
5

   Un
3
 0
 0 7
7
..
.. 7
7; and
.
. 5
   wn

However, this problem is still difficult to solve since U exists in
every l1 norm and the l2 norm. To simplify the problem, we
introduce two new variables and turn the original problem (15)
into

k
min kDd Uk1 þ akDd Ek1 þ bkDd Xk1 þ kY  HUk22
U;E;X
2
s:t: E ¼ U  CU;
X ¼ U  WU

ð16Þ

290

K. Chang, B. Li / J. Vis. Commun. Image R. 33 (2015) 286–300

Then we turn the constrained problem (16) into the unconstrained problem

k
minkDd Uk1 þ akDd Ek1 þ bkDd Xk1 þ kY  HUk22
U;E;X
2
þ

r
2

kE  ðU 

CUÞk22

c

þ kX  ðU 
2

WUÞk22

ð17Þ

Since kDd Uk1 ; kDd Ek1 and kDd Xk1 are non-separable, variable
splitting in Split-Bregman iteration [35] is used to address
this non-separability by defining the following constrained
optimization formulation

k
kd1 k1 þ akd2 k1 þ bkd3 k1 þ kY  HUk22
2

min

U;E;X;d1 ;d2 ;d3

r

c

kE  ðU  CUÞk22 þ kX  ðU  WUÞk22
2
2
s:t: d1 ¼ Dd U; d2 ¼ Dd E; d3 ¼ Dd X
þ

ð18Þ

ð19Þ
4.2. Solving sub-problems

8 kþ1
>
U
¼ argminU 2k kY  HUk22 þ r2 kEk  ðU  CUÞk22
>
>
>
>
>
k
k
>
>
þ 2c kXk  ðU  WUÞk22 þ v2 kd1  Dd U  b1 k22
>
>
>
>
>
k
k
>
Ekþ1 ¼ argminE r2 kE  ðUkþ1  CUkþ1 Þk22 þ g2 kd2  Dd E  b2 k22
>
>
>
>
>
>
k
k
>
Xkþ1 ¼ argminX 2c kX  ðUkþ1  WUkþ1 Þk22 þ l2 kd3  Dd X  b3 k22
>
>
>
>
>
>
< dkþ1 ¼ argmind kd1 k þ v kd1  Dd Ukþ1  bk k2
1

1 2

2

g

¼ argmind2 akd2 k1 þ 2 kd2  Dd E

kþ1

k

 b2 k22

¼ argmind3 bkd3 k1 þ l2 kd3  Dd Xkþ1  b3 k22
k

k

kþ1

k

kþ1

k

kþ1

ð21Þ

Ekþ1 ¼

rðUkþ1  CUkþ1 Þ þ gDTd ðdk2  bk2 Þ
rI þ gDTd Dd

ð22Þ

Xkþ1 ¼

cðUkþ1  WUkþ1 Þ þ lDTd ðdk3  bk3 Þ
cI þ lDTd Dd

ð23Þ

Here I is the identity matrix with size of Nn  Nn. To guarantee the
accuracy of W and C, both matrixes should be updated during iterðI  WÞ þ vDTd Dd Þ1 at every iteration is too costly. In order to
efficiently solve the ‘‘U” subproblem, here we use the
results WUk and CUk , and then we simplify the original ‘‘U”
subproblem to

Ukþ1 ¼ argmin
U

k
r
c
kY  HUk22 þ kEk  ðU  CUk Þk22 þ kXk
2
2
2

 ðU  WUk Þk22 þ

v
2

¼ b1 þ ðDd Ukþ1  d1 Þ
¼ b2 þ ðDd Ekþ1  d2 Þ
¼ b3 þ ðDd Xkþ1  d3 Þ
ð20Þ

Since the ‘‘U”, ‘‘E” and ‘‘X” subproblems contain strictly convex
quadratic functions, there are actually closed form solutions for
U; E and X

k

k

kd1  Dd U  b1 k22

ð24Þ

whose closed-form solution is

kHT Y þ rðCUk þ Ek Þ þ cðWUk þ Xk Þ þ vDTd ðd1  b1 Þ
k

Ukþ1 ¼

kH H þ ðr þ cÞI þ v
T

DTd Dd

k

ð25Þ

Unfortunately, the burdens of directly calculating (25), (22) and
(23) are still very heavy. Therefore, an iterative method is highly
desirable. Here, the conjugate gradients squared (CGS) method
[37] is applied.
kþ1

In practice, it is hard to find the exact solution of the first subproblem of (19). One commonly used method is the alternative
minimization. As pointed out in [35,36], it is not desirable to solve
every subproblem in Split-Bregman iteration to full convergence.
In fact, it was found empirically in [35] that optimal efficiency is
obtained when only one iteration of the inner loop is performed,
so we have

1

T

previously updated Uk to replace Ukþ1 , yielding two approximate

8
kþ1
kþ1
kþ1
>
ðUkþ1 ; Ekþ1 ; Xkþ1 ; d1 ; d2 ; d3 Þ ¼
>
>
>
>
>
>
argminU;E;X;d1 ;d2 ;d3 kd1 k1 þ akd2 k1 þ bkd3 k1
>
>
>
>
>
>
>
þ 2k kY  HUk22 þ r2 kE  ðU  CUÞk22 þ 2c kX  ðU  WUÞk22
>
>
<
k
k
k
þ v2 kd1  Dd U  b1 k22 þ g2 kd2  Dd E  b2 k22 þ l2 kd3  Dd X  b3 k22
>
>
>
>
kþ1
k
kþ1
>
b1 ¼ b1 þ ðDd Ukþ1  d1 Þ
>
>
>
>
>
kþ1
k
kþ1
>
>
b2 ¼ b2 þ ðDd Ekþ1  d2 Þ
>
>
>
>
: kþ1
k
kþ1
b3 ¼ b3 þ ðDd Xkþ1  d3 Þ

1

k

kHT H þ rðI  CÞ ðI  CÞ þ cðI  WÞ ðI  WÞ þ vDTd Dd
T

ation. However, calculating ðkHT H þ rðI  CÞT ðI  CÞ þ cðI  WÞT

By converting problem (18) into an unconstrained problem, our
JM-driven Split-Bregman iteration can be written as follows

kþ1
>
>
d2
>
>
>
>
>
kþ1
>
>
d3
>
>
>
>
>
>
kþ1
>
> b1
>
>
>
>
> bkþ1
>
>
2
>
>
>
: kþ1
b3

kHT Y þ rðI  CÞT Ek þ cðI  WÞT Xk þ vDTd ðd1  b1 Þ
k

Ukþ1 ¼

kþ1

kþ1

When updating d1 ; d2
and d3
in (20), closed form
solutions by the shrinkage (or soft thresholding) formula [38] are
given as
kþ1

¼ shrinkðDd U þ b1 ; 1=vÞ

ð26Þ

kþ1

¼ shrinkðDd E þ b2 ; a=gÞ

ð27Þ

kþ1

¼ shrinkðDd X þ b3 ; b=lÞ

ð28Þ

d1
d2
d3

k

k

k

For a vector A and a constant c, the shrinkage operation is
defined as

shrinkðA; cÞ ¼ maxðjAj  c; 0Þ  sgnðAÞ

ð29Þ

4.3. Summary of the proposed algorithm
So far, all issues in the process of handing the sub-problems in
(20) have been solved efficiently. In light of all derivations above, a
detailed description of the proposed algorithm, whose name is
JM-RCI (Joint Modeling-based Reconstruction of Correlated
Images), is provided in Algorithm 1.
In every inner iteration of JM-RCI, we can use the updated U to
compute a new W. Nevertheless, due to high complexity of DE/ME,
the updating of C is executed only once every T iterations. The
algorithm will stop if the relative change of U is smaller than a
predefined threshold, or the maximum number of iterations is
reached.

291

K. Chang, B. Li / J. Vis. Commun. Image R. 33 (2015) 286–300

a group sparsity-based TV minimization problem can be established
as

Algorithm 1. JM-RCI
Input: U0 ; H; Y; a; b; k;
0

0

0

0

r; c; g; v; l

0

~u
~ i k2;1
minkD
~i
u

0

d1 ¼ d2 ¼ d3 ¼ 0, b1 ¼ b2 ¼ b3 ¼ 0, E0 ¼ 0, X0 ¼ 0
Use U0 to update W and C.
while inner stopping criteria unsatisfied do

kxk2;1 ¼

Use Ukþ1 to update W.

Update Xkþ1 by using CGS method to solve (23).
via (26).

kþ1

via (27).

kþ1

via (28).

Update d3
kþ1

¼ b1 þ ðDUkþ1  d1 Þ

kþ1

¼ b2 þ ðDEkþ1  d2 Þ

b1
b2

k

kþ1

k

kþ1
b3

kþ1

k
b3

ð32Þ

where x½g j  denotes the jth group in x, and m is the number
~u
~ i contains R, G, and B
of groups. For (31), every group of D
signals at the same index in the horizontal or the vertical gradient
domain.
Since the l2;1 norm facilitates group sparsity, to support CS
reconstruction for color version of correlated images, we extend
the original problem (14) by substituting l1 norm with l2;1 norm
~u
~ i term, yielding a new group sparsity-based minimization
for D
problem as follows

Update Ekþ1 by using CGS method to solve (22).

Update d2

m
X
kx½g j k2
j¼1

If modðk þ 1; TÞ ¼¼ 0, use Ukþ1 to update C.

kþ1

ð31Þ

where k  k2;1 stands for l2;1 norm [39], and it is defined as

Update Ukþ1 by using CGS method to solve (25).

Update d1

~ iu
~i
~i ¼ U
s:t: y

kþ1

kþ1

 

X


1 ~
~ u
~u
~ iþ1 u

~
~ i k2;1 þ a
~
~
D
ð
F
min kD

þ
B
Þ
u
i
i1
i1
iþ1


~
ui
2
1
n

¼
þ ðDX
 d3 Þ
k¼kþ1
end while
return Final Reconstructed Images U

k
~ u
~ iu
~i  w
~ i Þk1 þ ky
~ i k22
~i  U
~ iu
þ bkDð
2

ð33Þ

where

2

5. Group sparsity extension of JM-RCI
For color version of images, the CS procedure (1) can be rewritten as

2

yiR

3

2

UiR

6 i 7 6
4 yG 5 ¼ 4 0
yiB
0
uiR ;

uiG ;

0
UiG
0

0

32

uiR

3

76 7
0 54 uiG 5
uiB
UiB

ð30Þ

uiB

where
are the original contents of red (R), green (G) and
blue (B) channels of the ith image, respectively, and yiR ; yiG ; yiB represent their corresponding CS measurements.
Generally speaking, the signals in R, G and B color channels are
highly correlated. That means, if we consider the values of
uiR ; uiG ; uiB at an index j as a group, the components within a group
are likely to be either all zeros or all nonzeros [23,24]. This prior
knowledge still holds when images are transformed to the gradient
domain. In Fig. 2, we visualize the horizontal gradients of R, G and
B color channels of one view of Tsukuba dataset. One can see that
except the slight differences on the statue and the lamp, the gradient values at the same positions in different color channels are
almost the same. According to the above analysis, for single color
image reconstruction, if we define

2

UiR

0

~i ¼ 6
U
4 0

UiG

0

0

3
2
0
wi 0
Fi1
6
7
~i1 ¼ 6
~ i ¼ 4 0 wi 0 5; F
w
4 0
0
0 wi
0
2
3
0
0
Biþ1
7
~ iþ1 ¼ 6
B
Biþ1
0 5;
4 0
0
0
Biþ1

0
Fi1
0

0

3

7
0 5; and
Fi1

which means both the NLM filtering coefficient matrix and DV/MV
are the same for all color channels of the ith image (i.e., NLM filtering and DE/ME techniques work in 3D manner).
~u
~ u
~ iu
~iÞ
~ i , we can also requires that Dð
~i  W
In addition to D
~
~
admits group sparsity. The reason is that Wui is a denoised

~ i after using NLM filter. When the noise is not
version of u
~W
~u
~ i should admit group sparsity. Thus, we can
significant, D
~ u
~ iu
~i  W
~ i Þ to further improve the
also apply l2;1 norm for Dð
performance at a high subrate. However, we have to keep


~ u
~i1 u
~ iþ1 u
~ i1 þ B
~ iþ1 Þ . On one
~ i  1 ðF
on using l1 norm for D
2

3

hand, the accuracy of DE/ME is sensitive to noise; on the
~i1 and B
~ iþ1
other hand, we do not want to update both F
too frequently due to complexity consideration. Hence, we
cannot guarantee that the inter-image prediction value
~i1 u
~ iþ1 u
~ i1 þ B
~ iþ1 Þ would satisfy group sparsity in the
~si ¼ 12 ðF

7 ~
7 ~ 6
6 i 7 ~
6 i 7
¼ 4 0 D 0 5; y
0 5; D
i ¼ 4 y G 5; u
i ¼ 4 uG 5 ;
i
0 0 D
yB
uiB
UiB

gradient domain.
According to the above analysis, when subrate >¼ 0:15, we
resort to the following problem

0

3

2

D 0

0

3

2

yiR

3

2

uiR

Fig. 2. Horizontal gradients of different color channels of Tsukuba, from left to right: R channel, G channel, B channel.

292

K. Chang, B. Li / J. Vis. Commun. Image R. 33 (2015) 286–300

 

X


1 ~
~u
~ iþ1 u
~ u

~ i k2;1 þ a
~
~
~
ð
F
min kD

þ
B
Þ
D
u
i
i1
i1
iþ1


~
2
u
i

1

n

k
~ u
~ iu
~i  w
~ i Þk2;1 þ ky
~ i k22
~i  U
~ iu
þ bkDð
2

ð34Þ

To solve the minimization problem (34), we convert it to

k ~ ~~ 2
~
~ d Uk
~ ~
~
~ ~
~~
minkD
2;1 þ akDd ðU  CUÞk1 þ bkDd ðU  WUÞk2;1 þ kY  HUk2
~
2
U
ð35Þ
where

3
2 3
2~
~1
~1
y
u
U1 0
7
7
6u
6y
6 0 U
~2
~
~
6 27
6 27
6
~ ¼ 6 7; H
~ ¼6
~ ¼ 6 7; Y
U
..
6 .. 7
6 .. 7
6 ..
4 . 5
4 . 5
4 .
.
~n
~n
u
y
0
0
3
2
2
~ 0  0
~1 0
w
D
7
6
6 0 w
~
~2
6 0 D  0 7
6
~
~d ¼ 6
6
D
..
.. 7
7; W ¼ 6 ..
6 .. .. . .
4. .
4 .
.
. .5
~
0
0
0 0  D
3
2
~2
0
B
0

0
7
6~
~ 3 =2
B

0 7
6 F1 =2 0
7
6
..
..
..
.. 7
~¼6
C
7:
6 ...
.
.
.
.
7
6
7
6
~
~
4 0
0
Bn =2 5
   Fn2 =2
~n1
0
0

0
F
2



..
.

0

3
7
7
7;
7
5

0
..
.
~n
 U
3
 0
 0 7
7
..
.. 7
7; and
.
. 5
~n
 w

The non-smoothness and the mixed-norm structure make problem (35) difficult to solve. However, similar as (20), by introducing
~1 ¼ D
~U
~
~¼U
~ C
~ and X
~ ¼U
~ W
~ U,
~ and letting d
~ d U,
new variables E
~2 ¼ D
~3 ¼ D
~ d E,
~ d X,
~ d
~ the extended version of JM-driven Splitd
Bregman iteration is written as

8
~ UÞk
~ kþ1 ¼ argmin ~ k kY
~ H
~ Uk
~ 2 þ r kE
~ k  ðU
~ C
~ 2
>
U
>
U2
2
2
2
>
>
>
>
c ~k
~k  D
~ k k2
~ W
~ UÞk
~ 2 þ v kd
~U
~ b
>
þ
k
X

ð
U
>
1
1 2
2
2
2
>
>
>
>
~k  D
~ k k2
~U
~ kþ1 ¼ argmin~ r kE
~  ðU
~ kþ1  C
~ kþ1 Þk2 þ g kd
~E
~b
>
E
>
2
2 2
2
E
2
2
>
>
> kþ1
>
2
l
c ~
kþ1
kþ1
k
~
~ k k2
~
~
~
~
~
~
>
X
¼
argmin
k
X

ð
U

W
U
Þk
þ
k
d

D
X

b
>
~ 2
3
3 2
2
X
2
>
>
>
>
~ 1 k þ v kd
~1  D
~ k k2
~ kþ1 ¼ argmin ~ kd
~U
~ kþ1  b
<d
2;1
1 2
1
d1
2
~ kþ1
>
>
d
>
2
>
>
> ~ kþ1
>
>
d
>
>
> 3
>
>
~ kþ1
>
>
> b1
>
>
>
~ kþ1
>
>b
2
>
>
>
:b
~ kþ1
3

~ k þ g kd
~ D
~ k k2
~E
~ kþ1  b
¼ argmind~ 2 akd
2 1
2
2 2
2
~ 3 k þ l kd
~3  D
~ k k2
~X
~ kþ1  b
¼ argmind~ 3 bkd
2;1
3 2
2
~ k þ ðD
~ kþ1 Þ
~U
~ kþ1  d
¼b
1
1
~ k þ ðD
~ kþ1 Þ
~E
~ kþ1  d
¼b
2

2

~ k þ ðD
~ kþ1 Þ
~X
~ kþ1  d
¼b
3
3
ð36Þ
~k

~ W
~ UÞk
~ 2
Here we also replace the l2 -norm term kX  ðU
2
2
~ UÞk
~ k  ðU
~ C
~
~
~k
~
~ ~k 2
and kE
2 in ‘‘U” subproblem with kX  ðU  WU Þk2
~U
~ k  ðU
~ C
~ k Þk2 , respectively, leading to a closed-form
and kE
2

solution
k
~ ~k
~ T ~k ~k
~T ~
~ ~k ~k
~ kþ1 ¼ kH Y þ rðCU þ E Þ þ cðWU þ X Þ þ vDd ðd1  b1 Þ
U
T~
~
~
~
T
kH H þ ðr þ cÞI þ vDd Dd

ð37Þ

But note that the size of the identity matrix in (37) is
3Nn  3Nn. The solutions of ‘‘E” and ‘‘X” subproblems are given by

~ T ~k ~k
~ kþ1 ~ ~ kþ1
~ kþ1 ¼ rðU  CU Þ þ gDd ðd2  b2 Þ
E
T~
~
rI þ gDd Dd

ð38Þ

~ T ~k ~k
~ kþ1
~ ~ kþ1
~ kþ1 ¼ cðU  WU Þ þ lDd ðd3  b3 Þ
X
T~
~
cI þ lDd Dd

ð39Þ

~ kþ1 , its closed form solution is
When updating d
2

~ kþ1 ¼ shrinkðD
~ k ; a=gÞ
~ dE
~ kþ1 þ b
d
2
2

ð40Þ

Fig. 3. Image samples of test datasets, from left to right and from top to bottom: Monopoly, Venus, Tsukuba, Breakdancer, Ballroom, Art, Foreman, Carphone, Container, Football,
Crew, and Harbour.

293

K. Chang, B. Li / J. Vis. Commun. Image R. 33 (2015) 286–300

38

43
42

PSNR

PSNR

36

34

32

41
40
39

30
4

38
4
3

3

4

α

4

3

2

2

1
0

α

β

1
0

3

2

2

1
0

(a) Foreman dataset, subrate =0.2

β

1
0

(b) Foreman dataset, subrate =0.4

35

42

34

PSNR

PSNR

40
33
32

38

31
36
30
29
4

34
4
3

3

4

α

4

3

2
1

α

β

1
0

0

3

2

2

2

1

(c) Tsukuba dataset, subrate =0.2

β

1
0

0

(d) Tsukuba dataset, subrate =0.4

Fig. 4. PSNR evolution with respect to ða; bÞ.

~ kþ1 , when solving the l2;1  l1
Different from the way to update d
2
kþ1
~
~ kþ1 , the ith group of d
~ kþ1
mixed problems for updating d1 and d
3
1
kþ1
~
and the ith group d
are respectively calculated by group-wise
3

shrinkage [39] formula as follows

~ kþ1 ½g  ¼ shrinkG ððD
~ k ½g ; 1=vÞ
~ kþ1 Þ½g  þ b
~ dU
d
j
j
1
1 j

ð41Þ

~ kþ1 ½g  ¼ shrink ððD
~ k ½g ; b=lÞ
~ kþ1 Þ½g  þ b
~ dX
d
G
j
j
3
3 j

ð42Þ

where group-wise shrinkage operation is defined as

shrinkG ðA; cÞ ¼

A
maxðkAk2  c; 0Þ
kAk2

ð43Þ

When sampling at a low subrate, problem (33) is considered.
The only difference between solving (33) and (34) is that when
~ kþ1 , (42) should be changed to
updating d
3

~ kþ1 ¼ shrinkðD
~ k ; b=lÞ
~ kþ1 þ b
~ dX
d
3
3

ð44Þ

The whole algorithm, which is called JM-RCI-GSE (Joint Modelingbased Reconstruction of Correlated Images-Group Sparsity Extension) is showed in Algorithm 2.

Algorithm 2. JM-RCI-GSE
~ 0 ; H;
~ Y;
~ a; b; k; r; c; g; v;
Input: U
0
0
0
~
~
~
~0 ¼ b
~0 ¼ b
~ 0 ¼ 0,
d1 ¼ d2 ¼ d3 ¼ 0; b
1
2
3
0
~
~ and C.
~ to update W
Use U

l
~ 0 ¼ 0;
E

~0 ¼ 0
X

while inner stopping criteria unsatisfied do
~ kþ1 by using CGS method to solve (37).
Update U
kþ1
~
~
Use U
to update W.
~
~ kþ1 to update C.
If modðk þ 1; TÞ == 0, use U
kþ1
~
Update E
by using CGS method to solve (38).
~ kþ1 by using CGS method to solve (39).
Update X
~ kþ1 via (41).
Update d
1
~ kþ1 via (40).
Update d
2

~ kþ1 via
~ kþ1 via (42), else update d
If SubRate P 0:15, update d
3
3
(44).
~ k þ ðD
~ kþ1 Þ
~ kþ1 ¼ b
~ kþ1  d
~ E
b
1

1

d

1

~ kþ1 ¼ b
~ k þ ðD
~ kþ1 Þ
~ kþ1  d
~ dU
b
2
2
2
kþ1
~
~ k þ ðD
~ kþ1 Þ
~ kþ1  d
~ dX
b
¼b
3
3
3
k¼kþ1
end while
~
return Final Reconstructed Images U

294

K. Chang, B. Li / J. Vis. Commun. Image R. 33 (2015) 286–300
34

38

33

40

42

37

44

41.5

43.5

39
32
36

41
43

31

38

34

37

PSNR (dB)

29

PSNR (dB)

30

40.5

PSNR (dB)

PSNR (dB)

PSNR (dB)

35

40

42.5

42
33

39.5

28

36
41.5
32

39

27
35
31

26

25

30

100 200 300 400 500

iteration number

41

38.5

34

100 200 300 400 500

iteration number

38

100 200 300 400 500

iteration number

40.5

100 200 300 400 500

iteration number

100 200 300 400 500

iteration number

Fig. 5. Progression of PSNR(dB) results achieved by JM-RCI for Foreman, from left to right: subrate = 0.1, subrate = 0.2, subrate = 0.3, subrate = 0.4, subrate = 0.5.

29.5

36

29

35

39

42

45

38

41

44

37

40

43

36

39

42

28.5
34
28

32

35

PSNR (dB)

27

PSNR (dB)

27.5

PSNR (dB)

PSNR (dB)

PSNR (dB)

33

38

41

31
26.5
34

37

40

33

36

39

30
26

29

25.5

25

100 200 300 400 500

iteration number

28

100 200 300 400 500

iteration number

32

100 200 300 400 500

iteration number

35

100 200 300 400 500

iteration number

38

100 200 300 400 500

iteration number

Fig. 6. Progression of PSNR (dB) results achieved by JM-RCI for Tsukuba, from left to right: subrate = 0.1, subrate = 0.2, subrate = 0.3, subrate = 0.4, subrate = 0.5.

6. Experimental results
In this section, extensive experimental results are presented to
evaluate the performance of the proposed algorithms. All experiments are performed in Matlab 7.14 on a Dell computer with
Intel(R) Core(TM) i5-2500 processor, 8.00 G memory, and 64-bit
Windows 7 operating system. To facilitate evaluation and further

exploration of the method proposed above, we have also published
the source code, see footnote.1
We apply our algorithm to both grayscale version and color version of datasets, including 6 multi-view image sets and 6 video
sequences. For multi-view images, Monopoly (443  370), Tsukuba
1

http://www.public.asu.edu/bli24/SourceCodeForJM-RCI-GSE.html.

295

K. Chang, B. Li / J. Vis. Commun. Image R. 33 (2015) 286–300
Table 1
Average values of different IQA criterions for gray images reconstruction.
SR

Methods

Multi-view image sets

Video sequences

PSNR

FSIM

VIF

BRIS.

PSNR

FSIM

VIF

BRIS.

0.1

JM-RCI
WATV [34]
MASTeR [18]
DC-TV [17]
RIRMM [15]
TVNLR [9]
TVAL3 [41]

32.58
30.70
29.54
29.40
28.64
28.47
26.90

0.9266
0.8986
0.8930
0.8706
0.8661
0.8578
0.8307

0.4315
0.3617
0.3463
0.3233
0.2737
0.2678
0.2280

32.99
36.33
33.62
33.94
38.57
38.21
45.88

28.53
26.52
24.37
24.42
25.05
24.79
23.59

0.8635
0.8251
0.8028
0.7925
0.7931
0.7984
0.7745

0.3139
0.2499
0.2038
0.2003
0.2055
0.1947
0.1705

30.71
37.86
31.94
36.39
36.20
33.81
40.64

0.2

JM-RCI
WATV [34]
MASTeR [18]
DC-TV [17]
RIRMM [15]
TVNLR [9]
TVAL3 [41]

37.42
35.13
34.31
33.77
32.23
33.90
30.79

0.9731
0.9585
0.9575
0.9355
0.9274
0.9461
0.9004

0.6500
0.5725
0.5611
0.5348
0.4588
0.5070
0.3922

28.77
33.92
29.75
30.12
35.47
34.91
39.51

32.88
30.50
28.32
28.74
29.76
30.28
27.10

0.9337
0.9035
0.8889
0.8839
0.8969
0.8966
0.8576

0.5051
0.4196
0.3630
0.3725
0.3778
0.4018
0.3008

25.37
33.96
27.16
30.17
33.01
30.74
36.18

0.3

JM-RCI
WATV [34]
MASTeR [18]
DC-TV [17]
RIRMM [15]
TVNLR [9]
TVAL3 [41]

40.58
38.44
37.66
37.11
34.78
36.89
33.94

0.9865
0.9786
0.9785
0.9705
0.9548
0.9688
0.9413

0.7821
0.7087
0.7114
0.6886
0.5706
0.6452
0.5365

27.35
31.41
27.88
28.08
34.35
32.93
35.34

35.84
33.45
31.42
32.28
32.38
32.99
30.25

0.9627
0.9419
0.9310
0.9341
0.9345
0.9352
0.9109

0.6416
0.5533
0.4957
0.5243
0.5070
0.5271
0.4332

22.05
30.51
24.45
26.35
30.87
28.85
31.72

0.4

JM-RCI
WATV [34]
MASTeR [18]
DC-TV [17]
RIRMM [15]
TVNLR [9]
TVAL3 [41]

43.06
41.10
40.50
40.06
36.97
39.22
36.85

0.9924
0.9883
0.9876
0.9856
0.9707
0.9813
0.9673

0.8573
0.8036
0.8052
0.7969
0.6717
0.7412
0.6682

26.06
30.29
26.75
27.37
33.05
30.21
32.22

38.41
36.05
34.11
35.34
34.47
35.07
33.10

0.9781
0.9641
0.9559
0.9617
0.9565
0.9566
0.9440

0.7449
0.6635
0.6107
0.6560
0.5981
0.6204
0.5591

20.27
26.89
22.93
23.53
29.28
26.88
27.27

0.5

JM-RCI
WATV [34]
MASTeR [18]
DC-TV [17]
RIRMM [15]
TVNLR [9]
TVAL3 [41]

45.18
43.49
42.10
42.79
38.80
41.42
39.74

0.9954
0.9931
0.9922
0.9917
0.9799
0.9889
0.9827

0.9048
0.8714
0.8629
0.8748
0.7472
0.8178
0.7815

24.83
30.33
25.57
26.93
32.64
29.10
30.18

40.70
38.50
36.59
38.03
36.75
37.20
35.74

0.9868
0.9778
0.9721
0.9774
0.9705
0.9716
0.9653

0.8237
0.7536
0.7092
0.7509
0.6960
0.7066
0.6734

19.22
25.27
21.70
23.14
28.27
25.90
26.29

The bold values indicate the best numerical results obtained among different methods.

(384  288), Venus (434  383) and Art (463  370) come from the
middlebury multi-view database,2 Ballroom (640  480) is provided
by Mitsubishi Electric Research Laboratories,3 Breakdancer is generated by Interactive Visual Media group at Microsoft Research4
(resampled to a resolution of 512  384 by using bilinear filters).
For video sequences, Foreman Football, Container, Harbour, Crew are
of size 352  288, while Carphone is 176  144. 5 When testing the
multi-view image sets, the first 5 views of Monopoly, Tsukuba, Venus
and Art are used; in Breakdancer and Ballroom, we also consider the
first 5 views, but only the first frame of each view is selected. When
testing the video sequences, we take the first 20 frames of each
sequence. Fig. 3 shows samples of all the 12 test datasets.
To generate U for every image, structurally random matrices
(SRM) [40] is applied, where block Walsh–Hadamard transform
with pre-randomization is selected with a block size of 32. U0 in
~ 0 in JM-RCI-GSE are generated by independently
JM-RCI and U
reconstructing every image with TVAL3 [41] software.6 For
~ optiDE/ME to generate DV/MV and construct the matrix of C and C,
cal flow implementation7 of [42] is used. We update DV/MV once
every 30 iterations for both JM-RCI and JM-RCI-GSE algorithms,
i.e., T ¼ 30.
2
3
4
5
6
7

http://vision.middlebury.edu/stereo/data.
ftp://ftp.merl.com/pub/avetro/mvc-testseq/orig-yuv/.
http://research.microsoft.com/en-us/um/people/sbkang/3dvideodownload/.
http://trace.eas.asu.edu/yuv/index.html/.
http://www.caam.rice.edu/optimization/L1/TVAL3/.
http://people.csail.mit.edu/celiu/OpticalFlow/.

6.1. Parameters selection for the proposed algorithms
In this part, we will explain how to decide parameters for JMRCI algorithm, and all the parameters in JM-RCI-GSE are set to be
the same as JM-RCI. In JM-RCI, we have 8 parameters to determine,
i.e., a; b; k; r; c; v; g; l. To make it tractable, we simplify the
optimization of 8 parameters as the optimization of the two most
important parameters a and b. To do that, we should first analyze
the relationship among them.
First have a look at r and c. These two parameters are trading
off the data fidelity terms related to E and X, respectively. Therefore, they should be relatively large, and we empirically set
r ¼ 3  a and c ¼ 3  b. Since k is the weight of data fidelity term
related to Y, a larger k may guarantee reliable results. Thus,
k ¼ 50 is investigated. The parameters v; g, and l are related to
variable splitting for DU; DE, and DX. We observe that a good
rule of thumb is 1 ¼ 20  v; a ¼ 20  g, and b ¼ 20  l. So far,
the relationships among all parameters are established. Except k
and v, all the rest parameters r; c; g and l are controlled by a
and b.
a and b represent the weight of inter-image correlation and the
weight of nonlocal correlation within every image, respectively. To
evaluate the influence of a and b, we search over different combinations of ða; bÞ for different datasets at different subrates. Fig. 4
provides peak signal to noise ratio (PSNR) evolution with respect
to different ða; bÞ at subrate 0.2 and 0.4 for 2 datasets.
From Fig. 4, we can observe that: firstly, ignoring either interimage correlation (i.e., set a ¼ 0) or nonlocal correlation within

296

K. Chang, B. Li / J. Vis. Commun. Image R. 33 (2015) 286–300
WATV

MASTeR

DC-TV

RIRMM

TVNLR

JM-RCI
1

45

0.9

40

0.8

35

0.7

30

0.6

FSIM

PSNR (dB)

JM-RCI
50

25

0.4

15

0.3

10

0.2

5

0.1
0.1

0.2

0.3

0.4

MASTeR

0.1

MASTeR

DC-TV

RIRMM

0.2

0.3

TVNLR

JM-RCI

0.9

45

0.8

40

0.7

35

BRISQUE

50

VIF

0.6
0.5
0.4

10

0.1

5

0.4

0.5

0

0.1

DC-TV

0.2

RIRMM

TVNLR

0.3

0.4

0.5

subrate

subrate

(c) VIF comparison

MASTeR

20
15

0.3

WATV

25

0.2

0.2

0.5

30

0.3

0.1

0.4

(b) FSIM comparison

1

0

TVNLR

subrate

(a) PSNR comparison
WATV

RIRMM

0

0.5

subrate

JM-RCI

DC-TV

0.5

20

0

WATV

(d) BRISQUE comparison

Fig. 7. Comparison of error bars and average values for gray images reconstruction.

Fig. 8. Visual quality comparison for Monopoly (subrate = 0.1). From left to right and from top to bottom: Original, TVAL3 [41], TVNL [9], RIRMM [15], DC-TV [17], MASTeR
[18], WATV [34], JM-RCI.

every image (i.e., set b ¼ 0) leads to inferior results, which demonstrates the effectiveness of the proposed JM; secondly, the best
ða; bÞ varies from one dataset to another since different datasets
have different kinds of contents and different levels of interimage correlation; finally, for a given dataset, the optimal ða; bÞ is
also different at different subrates. Fortunately, we find that
around the optimal ða; bÞ in each case, there exists a large acceptable region of values for ða; bÞ where relatively high PSNR can be
achieved, so we can still find a empirical fixed values of ða; bÞ for
all the cases. In the following experiment, we fix ða; bÞ as (1.75,
2.50).

Using the parameters mentioned above, Figs. 5 and 6 plot the
evolutions of average PSNR versus iteration numbers for datasets
Foreman and Tsukuba at different subrates. It is observed that with
the growth of iteration number, all the PSNR curves increase
monotonically and finally converge. Besides, according to the
results in Figs. 5, and 6, we find that the best maximum iteration
number is dependent on subrates. Thus, the maximum iteration
number is set to be 400, 300, 250, 200, 150 for subrates 0.1, 0.2,
0.3, 0.4, 0.5, respectively. In addition to reaching maximum iteration number, we also stop the algorithm if the relative change of
U is smaller than 104 .

297

50

1

45

0.95

40

0.9

35

0.85

FSIMc

PSNR (dB)

K. Chang, B. Li / J. Vis. Commun. Image R. 33 (2015) 286–300

30

25

0.75
JM−RCI−GSE
JM−RCI
TV−MMV
SL20
IHT−MMV

20

15

0.8

0.1

0.2

0.3

0.4

JM−RCI−GSE
JM−RCI
TV−MMV
SL20
IHT−MMV

0.7

0.65

0.5

0.1

0.2

0.3

0.4

subrate

subrate

(a) PSNR comparison

(b) FSIMc comparison

1

0.5

90

0.9

JM−RCI−GSE
JM−RCI
TV−MMV
SL20
IHT−MMV

80

0.8
70
0.7
60

BRISQUE

VIF

0.6
0.5
0.4

50
40

0.3

0.1
0

30

JM−RCI−GSE
JM−RCI
TV−MMV
SL20
IHT−MMV

0.2

0.1

0.2

0.3

0.4

20
10

0.5

subrate

0.1

0.2

0.3

0.4

0.5

subrate

(c) VIF comparison

(d) BRISQUE comparison

Fig. 9. Comparison of error bars and average values for color images reconstruction. (For interpretation of the references to color in this figure legend, the reader is referred to
the web version of this article.)

6.2. Grayscale images reconstruction
With the parameters discussed in Section 6.1, we now test the
performance of JM-RCI on grayscale image sets. 6 representative
methods are compared, including TV minimization by augmented
lagrangian and alternating direction algorithms (TVAL3) [41], TVbased image CS recovery algorithm by nonlocal regularization
(TVNLR) [9],8 DC-TV [17], motion-adaptive spatio-temporal regularization (MASTeR) [18],9 weighted anisotropy total variation (WATV)
[34], and robust image reconstruction from multi-view measurements (RIRMM) [15].10
TVAL3 [41] solves the TV-based reconstruction problem, which
only explores image local structural information. TVNLR [9] introduces NLM filtering-based nonlocal regularization in addition to
TV-based regularization. Since TVNLR and TVAL3 does not consider
inter-image correlation, they are applied to each image independently. DC-TV performs estimation and compensation among the
images, and reconstructs the prediction error of each image in gradient domain [17]. MASTeR [18] formulates the compensation as
forward and backward interpolation operators, and reconstruct
8
9
10

http://idm.pku.edu.cn/staff/zhangjian/Block-TVNLR.rar.
http://users.ece.gatech.edu/sasif/dynamicMRI/index.html.
http://lts2www.epfl.ch/people/gilles/softwares.

the residual of the whole image set. WATV is our previously proposed algorithm which calculates TV norm of each image and each
residual image [34]. Note that DC-TV and WATV algorithms perform reconstruction image by image. To recover the whole image
set, the sliding window-based reconstruction framework in [34]
is applied here to work with both algorithms. RIRMM models each
observed image as the sum of a background image and a foreground one, and assumes that the background image is common
to all observed images but undergoes geometric transformations
[15].
Recently, many full reference image quality assessment (IQA)
methods have been proposed, such as information fidelity criterion
(IFC) [43], visual information fidelity (VIF) [44] and feature similarity (FSIM) [45]. According to Zhang’s report [46], FSIM11 and VIF12
are two of the best IQA models. Therefore, apart from PSNR, FSIM
and VIF are also used to evaluate the performance of JM-RCI. Considering the cases where reference image is not available, no reference
IQA methods are also necessary. Here blind/referenceless image
spatial quality evaluator (BRISQUE) [47] is chosen.
Due to the length limitation of this paper, we only present average IQA results of all multi-view image sets and of all video
11
12

http://www4.comp.polyu.edu.hk/cslzhang/IQA/FSIM/FSIM.htm.
http://live.ece.utexas.edu/research/quality/.

298

K. Chang, B. Li / J. Vis. Commun. Image R. 33 (2015) 286–300

sequences in Table 1, where bold values indicate the best method
(note that the lower BRISQUE value indicates the better quality).
Recall that we have a total of 12 test datasets, and each dataset
consists of 5–20 correlated images. Thus, we also plot the error
bars along with the average values of different IQA criterions in
Fig. 7 (because of the poor performance, the results of TVAL3 are
excluded). One can observe that no matter which criterion is used,
our proposed method achieves the best performance, which suggests that JM-RCI derives large benefits from fully utilizing the
prior information within correlated image sets.
Fig. 8 shows visual quality comparison for the 3rd view of image
sets Monopoly in the case of subrate = 0.1 (please enlarge the figures for better comparison). The proposed JM-RCI algorithm not
only generates the cleanest results but also reproduces the sharpest edges. On the contrary, results generated by other competing
methods are either noisy or tend to lose fine details.
6.3. Color images reconstruction
In this subsection, experimental results of JM-RIC-GSE algorithm on color image sets are provided, and other 4 color image
reconstruction methods are selected for comparison, including
smoothed l2;0 -norm minimization (SL20) [24], iterative hard
thresholding algorithm-based on multiple measurement vector
(IHT-MMV) [24], TV minimization based on multiple measurement
vector (TV-MMV), and JM-RCI algorithm.
SL20 and IHT-MMV, which formulate the color images reconstruction problems as MMV recovery problems, are based on the
fact that coefficients of orthogonal transformations in different
color channels satisfy group-sparsity [24]. In our experiment, 9/7
wavelet is used for both SL20 and IHT-MMV. TV-MMV reconstructs
color images by solving the problem (31), it is the group sparsityextension of TV-based CS reconstruction method. For the reason
that SL20, IHT-MMV and TV-MMV only explore inter-channel
and intra-image correlation, they are applied to each image

separately. For the reason that JM-RCI does not explore interchannel correlation, it is applied to the whole image set channel
by channel.
Table 2 lists average PSNR/FSIMc/VIF/BRISQUE values of all the
multi-view image sets and of all the video sequences, where FSIMc
is the color images extension of FSIM [45]. Additionally, error bars
and average results of different IQA criterions are also shown in
Fig. 9. From Table 2 and Fig. 9, we can conclude that compared with
channel by channel executing JM-RCI, JM-RCI-GSE has even better
performance since it fully explores inter-image correlation, intraimage correlation and inter-channel correlation.
Visual results by different algorithms for Tsukuba are shown in
Fig. 10. Only parts of the reconstructed images are enlarged and
displayed here since the differences between JM-RCI results and
JM-RCI-GSE results are not obvious. From both figures, we can
observe that the methods which only consider inter-channel
correlation (i.e., IHT-MMV, SL20, and TV-MMV) yield much worse
results than JM-RCI and JM-RCI-GSE. Although the results generated by JM-RCI and by JM-RCI-GSE are similar, one can still notice
the differences in the details, e.g., the characters on the book and
the reflection of light on the lamb.
6.4. Computational complexity
Now let us discuss the computational complexity of each iteration in JM-RCI. In Algorithm 1, CGS method is used to solve (25),
(22) and (23). In every iteration of CGS, 2 matrix–vector products,
2 inner products, 6 SAXPY (adds a vector, which is multiplied by a
scalar, to another vector) are contained [48]. Since the dominating
part of the computation is the matrix–vector product, the computational complexity of one iteration in CGS is OðN 2  n2 Þ, where N is
the pixels in an image and n is the number of images to be
reconstructed. If an average of K iterations is needed for CGS
method, the total computational complexity of solving U; E, and
X subproblems becomes 3  K  OðN 2  n2 Þ. When updating W,

Table 2
Average values of different IQA criterions for color images reconstruction.
SR

Methods

Multi-view image sets

Video sequences

PSNR

FSIMc

VIF

BRIS.

PSNR

FSIMc

VIF

BRIS.

0.1

JM-RCI-GSE
JM-RCI
TV-MMV
SL20 [24]
IHT-MMV [24]

33.02
32.20
29.40
24.04
24.33

0.9397
0.9316
0.8780
0.7578
0.7712

0.4719
0.4454
0.3183
0.1246
0.1302

32.09
33.22
42.87
70.96
62.97

28.87
28.31
26.18
21.07
21.18

0.8760
0.8659
0.8090
0.7118
0.7207

0.3325
0.3190
0.2357
0.0887
0.0942

28.61
30.23
42.84
72.21
68.65

0.2

JM-RCI-GSE
JM-RCI
TV-MMV
SL20 [24]
IHT-MMV [24]

37.72
36.71
33.18
28.08
27.90

0.9797
0.9756
0.9405
0.8461
0.8511

0.7062
0.6724
0.5066
0.2812
0.2665

28.53
30.13
37.21
57.46
49.81

33.16
32.39
29.69
24.78
24.60

0.9412
0.9331
0.8904
0.8069
0.8076

0.5281
0.5093
0.3908
0.2051
0.2019

24.78
25.71
36.16
61.63
56.98

0.3

JM-RCI-GSE
JM-RCI
TV-MMV
SL20 [24]
IHT-MMV [24]

40.46
39.63
36.22
31.25
30.83

0.9895
0.9876
0.9682
0.9030
0.9030

0.8130
0.7904
0.6519
0.4376
0.4004

27.04
28.47
33.47
46.09
41.28

36.22
35.51
32.34
27.67
27.35

0.9681
0.9629
0.9311
0.8675
0.8656

0.6661
0.6459
0.5160
0.3243
0.3097

22.06
23.62
31.42
48.59
45.82

0.4

JM-RCI-GSE
JM-RCI
TV-MMV
SL20 [24]
IHT-MMV [24]

42.64
41.91
38.87
34.34
33.61

0.9940
0.9929
0.9813
0.9451
0.9398

0.8762
0.8612
0.7609
0.5920
0.5338

25.83
27.00
31.84
36.02
34.04

38.73
38.03
34.79
30.43
29.95

0.9812
0.9781
0.9562
0.9131
0.9085

0.7653
0.7474
0.6246
0.4538
0.4231

20.45
21.88
27.79
38.12
37.11

0.5

JM-RCI-GSE
JM-RCI
TV-MMV
SL20 [24]
IHT-MMV [24]

44.55
43.89
41.24
37.44
36.41

0.9963
0.9957
0.9892
0.9724
0.9649

0.9160
0.8993
0.8403
0.7352
0.6623

24.48
25.70
29.56
28.27
29.02

40.99
40.31
37.09
33.19
32.56

0.9887
0.9870
0.9722
0.9455
0.9406

0.8396
0.8253
0.7197
0.5843
0.5843

19.12
20.26
25.52
29.22
28.88

The bold values indicate the best numerical results obtained among different methods.

K. Chang, B. Li / J. Vis. Commun. Image R. 33 (2015) 286–300

299

Fig. 10. Visual quality comparison for Tsukuba (subrate = 0.1). From left to right and from top to bottom: Original, IHT-MMV [24], SL20 [24], TV-MMV, JM-RCI, JM-RCI-GSE.

assuming that the search window for each image has a size of w2s
2

and the size of the similarity window is bs , the complexity for each
2

up JM-RCI and JM-RCI-GSE, possible solutions include using parallelization techniques such as GPU hardware.

2

image is OðN 2  w2s  bs Þ [30], leading to n  OðN 2  w2s  bs Þ for
the whole image set. To update C, every image needs both forward
and backward DE except the images at edges. Hence it needs to
execute the optical flow method 2ðn  2Þ þ 2 times for the whole
image set. Although it is time consuming, updating C once every
30 iterations in our experiment reduces the average computational
time to be 1/30 for every iteration in JM-RCI. The rest parts in one
JM-RCI iteration (i.e., updating d1 ; d2 ; d3 ; b1 ; b2 and b3 ) are very
easy to compute, where only 3 times of matrix–vector products
and 3 times of shrinkage operations are needed.
The average computational times for reconstructing one grayscale image (443  370 Monopoly) and for reconstructing one color
image are given in Tables 3 and 4. We can observe that for grayscale images reconstruction, JM-RCI has the second highest computational complexity among all the competing algorithms. Usually,
the lower the subrate is, the more computational time is needed.
For color images reconstruction, we can see that the computational
time of JM-RCI is largely increased due to being applied channel by
channel. However, JM-RCI-GSE is less complex than executing JMRCI channel by channel. The reason is that in JM-RCI-GSE, both
optical flow and NLM filtering are processed in 3D mode, leading
to a significant reduction in computational complexity. To speed

7. Conclusion
This paper proposes joint modeling of a set of correlated images,
considering all possible types of correlations. Based on the key idea
of joint modeling, an algorithm named JM-RCI was first proposed
to reconstruct the entire image set from CS measurements. Then,
to support color image reconstruction, inter-channel correlation
is explicitly explored by enforcing group-sparsity over different
color channels, resulting an extension of the JM-RCI algorithm to
JM-RCI-GSE. Experimental results demonstrated that the proposed
method outperforms many state-of-the-arts CS reconstruction
methods in terms of PSNR, FSIM/FSIMc, VIF and BRISQUE. Note that,
within the same image sets, there are similar contents in different
images. Therefore, it is possible to extend the intra-image nonlocal
modeling to inter images, so that more effective results could be
obtained. We list this potential way of improvement as one of
our future works. Other possible future works include the investigation of the proposed algorithms on other similar applications
such as multiple-frame super-resolution, and video deblurring.
Acknowledgments

Table 3
Average computational time (s) for reconstructing one grayscale image.
SR

TVAL3

TVNLR

RIRMM

DC-TV

MASTeR

WATV

JM-RCI

0.1
0.2
0.3
0.4
0.5

3.18
2.90
2.76
2.77
2.68

84.39
84.21
76.80
71.10
63.43

2074.41
1755.86
1541.26
1443.54
1513.04

23.47
23.60
23.79
23.66
23.85

81.91
76.23
68.80
61.28
56.20

70.64
69.32
64.83
59.14
53.00

296.29
227.26
189.04
150.96
115.94

Table 4
Average computational time (s) for reconstructing one color image.
SR

IHT-MMV

SL20

TV-MMV

JM-RCI

JM-RCI-GSE

0.1
0.2
0.3
0.4
0.5

44.79
45.46
46.12
46.79
47.80

8.05
8.16
8.25
8.35
8.52

155.08
96.54
72.21
57.34
45.07

890.38
682.37
568.91
453.73
353.78

681.41
519.41
433.96
330.92
243.60

The authors would like to thank anonymous reviewers for giving valuable suggestions to improve the quality of this manuscript.
This work was partially supported by the Natural Science Foundation of China via Grants 61401108 and 61261023, the Natural
Science Foundation of Guangxi Zhuang Autonomous Region via
Grant 2013GXNSFBA019272. The support provided by China
Scholarship Council (No. 1402170001) during a visit of K. Chang
to Arizona State University is also acknowledged. B. Li was partially
supported by the Natural Science Foundation via Grants 1135616
and 0845469.
References
[1] D.L. Donoho, Compressed sensing, IEEE Trans. Inform. Theory 52 (4) (2006)
1289–1306.
[2] E.J. Candès, J. Romberg, T. Tao, Robust uncertainty principles: exact signal
reconstruction from highly incomplete frequency information, IEEE Trans.
Inform. Theory 52 (2) (2006) 489–509.

300

K. Chang, B. Li / J. Vis. Commun. Image R. 33 (2015) 286–300

[3] M.F. Duarte, M.A. Davenport, D. Takhar, J.N. Laska, T. Sun, K.F. Kelly, R.G.
Baraniuk, Single-pixel imaging via compressive sampling, IEEE Signal Process.
Mag. 25 (2) (2008) 83–91.
[4] A. Wagadarikar, R. John, R. Willett, D.J. Brady, Single disperser design for coded
aperture snapshot spectral imaging, Appl. Opt. 47 (10) (2008) B44–B51.
[5] Y. Hitomi, J. Gu, M. Gupta, T. Mitsunaga, S. Nayar, Video from a single coded
exposure photograph using a learned over-complete dictionary, in: IEEE
International Conference on Computer Vision (ICCV), 2011, pp. 1–8.
[6] W. Dong, X. Yang, G. Shi, Compressive sensing via reweighted tv and nonlocal
sparsity regularisation, Electron. Lett. 49 (3) (2013) 184–186.
[7] W. Dong, G. Shi, X. Li, L. Zhang, X. Wu, Image reconstruction with locally
adaptive sparsity and nonlocal robust regularization, Signal Process.: Image
Commun. 27 (10) (2012) 1109–1122.
[8] X. Wu, W. Dong, X. Zhang, G. Shi, Model-assisted adaptive recovery of
compressed sensing with imaging applications, IEEE Trans. Image Process. 21
(2) (2012) 451–458.
[9] J. Zhang, S. Liu, D. Zhao, R. Xiong, S. Ma, Improved total variation based image
compressive sensing recovery by nonlocal regularization, in: IEEE International
Symposium on Circuits and Systems (ISCAS), IEEE, Beijing, China, 2013, pp.
2836–2839.
[10] J. Ma, G. Plonka, M.Y. Hussaini, Compressive video sampling with approximate
message passing decoding, IEEE Trans. Circ. Syst. Video Technol. 22 (9) (2012)
1354–1364.
[11] C. Li, H. Jiang, W. Paul, Y. Zhang, A new compressive video sensing framework
for mobile broadcast, IEEE Trans. Broadcast. 59 (1) (2013) 197–205.
[12] M. Hosseini, K.N. Plataniotis, High-accuracy total variation with application to
compressed video sensing, IEEE Trans. Image Process. 23 (9) (2014) 3869–
3884.
[13] J. Park, M.B. Wakin, A geometric approach to multi-view compressive imaging,
EURASIP J. Adv. Signal Process. 2012 (37) (2012) 1–15.
[14] V. Thirumalai, P. Frossard, Correlation estimation from compressed images, J.
Vis. Commun. Image Represent. 24 (6) (2013) 649–660.
[15] G. Puy, P. Vandergheynst, Robust image reconstruction from multiview
measurements, SIAM J. Imag. Sci. 7 (1) (2014) 128–156.
[16] S. Mun, J.E. Fowler, Residual reconstruction for block-based compressed
sensing of video, in: Data Compression Conference (DCC), IEEE, Snowbird,
Utah, USA, 2011, pp. 183–192.
[17] M. Trocan, E.W. Tramel, J.E. Fowler, B. Pesquet, Compressed-sensing recovery
of multiview image and video sequences using signal prediction, Multimedia
Tools Appl. 72 (1) (2014) 95–121.
[18] M.S. Asif, L. Hamilton, M. Brummer, J. Romberg, Motion-adaptive spatiotemporal regularization (master) for accelerated dynamic MRI, Magn. Reson.
Med. 70 (3) (2013) 800–812.
[19] Y. Liu, M. Li, A. Dimitris, Motion-aware decoding of compressed-sensed video,
IEEE Trans. Circ. Syst. Video Technol. 23 (3) (2013) 438–444.
[20] H. Chen, L. Kang, C. Lu, Dictionary learning-based distributed compressive
video sensing, in: Picture Coding Symposium (PCS), IEEE, Nagoya, Japan, 2010,
pp. 210–213.
[21] E.W. Tramel, J.E. Fowler, Video compressed sensing with multihypothesis, in:
Data Compression Conference (DCC), IEEE, Snowbird, Utah, USA, 2011, pp.
193–202.
[22] P. Nagesh, B. Li, Compressive imaging of color images, in: IEEE International
Conference on Acoustics, Speech, and Signal Processing (ICASSP), IEEE, Taipei,
Taiwan, 2009, pp. 1261–1264.
[23] A. Majumdar, R.K. Ward, Compressed sensing of color images, Signal Process.
90 (12) (2010) 3122–3127.
[24] A. Majumdar, R.K. Ward, T. Aboulnasr, Algorithms to approximatly solve NP
hard row-sparse MMV recovery problem: application to compressive color
imaging, IEEE J. Emerg. Sel. Top. Circ. Syst. 2 (3) (2012) 362–369.

[25] W. Liu, D. Tao, Multiview Hessian regularization for image annotation, IEEE
Trans. Image Process. 22 (7) (2013) 2676–2687.
[26] W. Liu, D. Tao, J. Cheng, Y. Tang, Multiview Hessian discriminative sparse
coding for image annotation, Comput. Vis. Image Understan. 118 (2014) 50–
60.
[27] C. Xu, T. Dacheng, C. Xu, Large-margin multi-view information bottleneck, IEEE
Trans. Pattern Anal. Mach. Intell. 36 (8) (2014) 1559–1572.
[28] C. Xu, T. Dacheng, C. Xu, Multi-view intact space learning, IEEE Trans.
Pattern Anal. Mach. Intell. (2015), http://dx.doi.org/10.1109/TPAMI.2015.
2417578.
[29] L. Rudin, S. Osher, E. Fatemi, Nonlinear total variation based noise removal
algorithms, Phys. D: Nonlinear Phenom. 60 (1) (1992) 259–268.
[30] A. Buades, B. Coll, J.M. Morel, A review of image denoising algorithms, with a
new one, SIAM Multiscale Model. Simul. 4 (2) (2005) 490–530.
[31] T. Chan, S. Esedoglu, F. Park, A. Yip, Recent developments in total variation
image restoration, Math. Models Comput. Vis. 17 (2005) 1–18.
[32] J. Romberg, Imaging via compressive sampling, IEEE Signal Process. Mag. 25 (2)
(2008) 14–20.
[33] J. Cai, B. Dong, S. Osher, Z. Shen, Image restoration: total variation, wavelet
frames, and beyond, J. Am. Math. Soc. 25 (4) (2012) 1033–1089.
[34] K. Chang, T. Qin, W. Xu, Z. Tang, Reconstruction of multi-view compressed
imaging using weighted total variation, Multimedia Syst. 20 (4) (2014) 363–
378.
[35] T. Goldstein, S. Osher, The split Bregman method for l1 regularized problems,
SIAM J. Imag. Sci. 2 (2) (2009) 323–343.
[36] J. Cai, S. Osher, Z. Shen, Split Bregman methods and frame based image
restoration, SIAM J. Multiscale Model. Simul. 8 (2) (2010) 337–369.
[37] P. Sonneveld, CGS: a fast Lanczos-type solver for nonsymmetric linear systems,
SIAM J. Sci. Stat. Comput. 10 (1) (1989) 36–52.
[38] E.T. Hale, W. Yin, Y. Zhang, Fixed-point continuation for l1-minimization:
methodology and convergence, SIAM J. Optim. 19 (3) (2008) 1107–1130.
[39] W. Deng, W. Yin, Y. Zhang, Group Sparse Optimization by Alternating Direction
Method, Tech. rep. TR11-06, Rice University, Department of Computational
and Applied Mathematics, April 2011.
[40] T.T. Do, L. Gan, N.H. Nguyen, T.D. Tran, Fast and efficient compressive sensing
using structurally random matrices, IEEE Trans. Signal Process. 60 (1) (2012)
139–154.
[41] C. Li, W. Yin, H. Jiang, Y. Zhang, An efficient augmented lagrangian method
with applications to total variation minimization, Comput. Optim. Appl. 56 (3)
(2013) 507–530.
[42] C. Liu, Beyond Pixels: Exploring New Representations and Applications for
Motion Analysis, Ph.D. Thesis, Massachusetts Institute of Technology, 2009.
[43] H.R. Sheikh, A.C. Bovik, G. De Veciana, An information fidelity criterion for
image quality assessment using natural scene statistics, IEEE Trans. Image
Process. 14 (12) (2005) 2117–2128.
[44] H.R. Sheikh, A.C. Bovik, Image information and visual quality, IEEE Trans.
Image Process. 15 (2) (2006) 430–444.
[45] L. Zhang, L. Zhang, X. Mou, D. Zhang, FSIM: a feature similarity index for
image quality assessment, IEEE Trans. Image Process. 20 (9) (2011) 2378–
2386.
[46] L. Zhang, L. Zhang, X. Mou, D. Zhang, A comprehensive evaluation of full
reference image quality assessment algorithms, in: IEEE International
Conference on Image Processing (ICIP), IEEE, 2012, pp. 1477–1480.
[47] A. Mittal, A.K. Moorthy, A.C. Bovik, No-reference image quality assessment in
the spatial domain, IEEE Trans. Image Process. 21 (12) (2012) 4695–4708.
[48] R. Barrett, M. Berry, T.F. Chan, J. Demmel, J. Donato, J. Dongarra, V. Eijkhout, R.
Pozo, C. Romine, H.V. der Vorst, Templates for the Solution of Linear
Systems: Building Blocks for Iterative Methods, second ed., SIAM,
Philadelphia, PA, 1994.

CHI 2010: Work-in-Progress (Spotlight on Posters Days 3 & 4)

April 14–15, 2010, Atlanta, GA, USA

On Presenting Audio-Tactile Maps
to Visually Impaired Users for Getting
Directions
Devi Archana Paladugu, Zheshen Wang, Baoxin Li
Arizona State University
Tempe, AZ 85281 USA
Archana.Paladugu@asu.edu
Zheshen.Wang@asu.edu
Baoxin.Li@asu.edu

goal of the research is to establish comprehensive
design guidelines for building technologies that truly
serve the needs of the users in the application of
accessible maps. The results of our current study
suggest that the proposed designs are effective for
supporting a blind user in obtaining directions from
online maps.

Keywords
Abstract
Recent years have witnessed significant efforts on
developing computer-based technologies for making
maps accessible to people who are blind. Existing work
has largely focused on the technological aspects of the
problem without adequate attention to the humancomputer interaction issues. Using an audio-tactile
system as the platform, we present a focused study on
such HCI issues for supporting a blind user’s effective
navigation of a map in getting directions. The ultimate

Copyright is held by the author/owner(s).
CHI 2010, April 10–15, 2010, Atlanta, Georgia, USA.
ACM 978-1-60558-930-5/10/04.

Evaluation, Tactile-audio Map, Accessibility, Guidelines,
Assistive Technology, Blind Users, Visual Impairment.

ACM Classification Keywords
H.5.2. User Interfaces.

General Terms
Design, Experimentation, Human Factors.

Introduction
Maps are very useful for helping a person to grasp a
sense of direction and topography of a location of
interest. With the advent of online map services like
Google Maps [6] and MapQuest [8], sighted people are
increasingly enjoying the instant availability of services
like detailed directions for getting around.

3955

CHI 2010: Work-in-Progress (Spotlight on Posters Days 3 & 4)

(a)

(b)

(c)

(d)
Figure 1. Illustration of direction using

Unfortunately, a blind computer user has been largely
deprived of such benefits. The lack of accessible maps
is among the significant impediments to many visuallyimpaired individuals as they strive to lead independent
and active lives.
There have been some efforts that attempt to automate
the production of maps so as to support on-demand
accessibility of maps for people with visual impairment
[1, 3-5]. Most of such technologies rely on audio and
tactile channels for conveying the information in a map.
If such technologies are to be integrated with an online
map service for supporting instant access by a blind
user to getting directions, the usefulness of such
services will be heavily discounted unless effective
ways of repurposing the map contents can be designed.
For example, it is difficult for a blind user to correlate
the text instructions (for directions) with a tactile map,
if the system simply reads out the instructions and
presents to the user a simple tactile version of the
underlying map.
In this study, using an audio-tactile system as the
platform, we investigate various design issues that are
deemed essential for building a practical system
supporting a blind user’s online map experiences. We
first present the proposed design considerations, and
then evaluate the design methods by conducting
experiments with six visually-impaired participants. The
results validate the effectiveness of the design and
provide useful insights for future development.

MapQuest: (a) Input Query; (b) Returned

Related Work

map image with suggested route; (c)

Paper-based tactile maps has been discussed in depth
in the book Tactile Graphics by Edman [2]. While the
general principles remain helpful for a computer-based

Returned textual directions; (d) A user
explores the created tactile-audio map on a

April 14–15, 2010, Atlanta, GA, USA

approach, there are many new issues that need to be
addressed when it comes to making online maps
independently accessible. For example, most existing
computer-based approaches use a touch-sensitive
tablet to support navigation of the map by both touch
and hearing. More importantly, a computer-based
approach has the potential of enabling interactivity.
Supporting interactivity certainly brings about new HCI
issues that do not need to be considered in creating a
simple paper map.
Early work addressing such issues includes [1] and [3],
where the design issues were discussed mostly only at
the conceptual level. Recently, real prototype systems
have been reported [4, 5]. In [4], a system, Talking
TMAPS, was described, which automatically produces
audio-enabled tactile street maps. In [5], an automatic
approach was reported for creating interactive tactileaudio map based on online maps. In preparation for the
current study, we built a similar system [9] which can
produce tactile-audio maps based on the queries to
MapQuest. Our focus of study in the current paper is on
the HCI issues in using such tactile-audio maps for
supporting effective and efficient exploration of the
maps by a blind user. We consider primarily maps for
directions: two addresses are supplied by a user and
the routing information (both graphical and textual) is
returned. The problem and the system setup are
illustrated in Fig. 1.

Supporting Interactive tactile-audio map
In this section, we propose our design considerations
that we believe are essential for effective rendering of
tactile-audio maps for providing directions in a system
illustrated in Fig. 1. These include guidelines based on

laptop with a touchpad.

3956

CHI 2010: Work-in-Progress (Spotlight on Posters Days 3 & 4)

general knowledge on tactile perception and new
design suggestions based on our field study and the
specificity of our platform for supporting interactivity
with online maps. The details of the design are
presented below.
Adaptive Rendering of the Map Region

Figure 2. Positioning of the start/end
points.

For a sighted person, it is easy to glance at a map of
Fig. 1(b) and immediately determine the big-picture of
the routing information (e.g., where to start and where
to end). However, it would take considerably much
more efforts for a blind user to find the start/end points
(presumablly rendered as special patterns) by touch. In
route tracing, this issue may be exacerbated since the
blind user may have to periodically go back to the start
end for reference. (These are among the complaints
from blind users during our field study.) To solve this
problem, we propose an adaptive rendering scheme,
which adjusts the rendered region based on the spatial
relationship between the input start/end points. Fig. 2
illustrates possible positions for setting the start and
end points: each black-white pair connected by a line
represents a possible positioning of the start/end points
for defining the map region. Essentially, this scheme
allows us to maximally use the rendering region while
being able to maintain a protocol that tells the user the
start/end points will be on fixed locations. (The points
may need to have a margin from the edge to
accommodate the case where the routes take a detour
from the initial direction, but this can be determined
automatically from the coordinates of the route
returned by the map server).
Utilizing an Information Button
To further help the user to orient him/herself right
away, we suggest putting a start information button on

April 14–15, 2010, Atlanta, GA, USA

the top right corner of the map. The user is asked to
use/press the information button before any other
action, and audio guidance will read out some essential
start information (e.g. “The start location is at the
bottom-left corner of the map.”). We have used a large
round filled circle to represent this information button.

Design of Patterns
Tactile rendering relies primarily on texture and shape
for distinction among different objects. While a
manually-crafted tactile map may be even made of
different materials (e.g. cloth, wood, etc.) to increase
the represenational power, a paper-based tactile map
(which can be automatically created) is constrained by
the almost binary (flat or raised) nature of the
representation. Hence the careful design of the patterns
is critical.We proposed and evaluated a set of patterns
for map rendering. We first conducted an initial round
of field study in which a large set of candidate patterns
was reduced to a couple of desirable patterns. These
chosen patterns were then used for map rendering to
test and compare their effectiveness in the final
expriments. (Details of evaluations are presented in the
next section.) The designed patterns are presented
below.
Regular Streets, Landmarks and Audio Labels:
Our field study revealed that using a bold solid line for
rendering streets is effective (an alternative of using
two parallel lines was deemed not as good by the
users). Also, it was suggested by the users that land
marks (parks etc) be highlighted on a map using a
filled regions. The audio labels that enable the readout
of verbal descriptions when the user touches them on a

3957

CHI 2010: Work-in-Progress (Spotlight on Posters Days 3 & 4)

map (on a touchpad) were determined to be a raised
dot of certain size. The size should be small enough to
fit them in good numbers on the map and large enough
to be easily detected by touch with the fingers.
Figure 3. Patterns of start (top) and
end (bottom) points. The first 2 pairs
were chosen and named Pattern 1, 2
respectively.

Start/End Points
With the adaptive rendering region defined earlier, the
user can search for a smaller area for the start/end
locations. Still, patterns for the start/end points need to
be easy to find by touch. We experimented with eight
pairs of start/end patterns as illustrated in Fig. 3, and
settled down on the first two pairs based on user
inputs.
Density of the Audio Labels
One could insert as many as possible audio buttons,
e.g., on a long road to remind the user where he/she
is. We evaluated two different densities for placing the
audio labels, corresponding to 1 inch apart and 0.5 inch
apart respectively (referred to as Density 1 and Density
2).
Patterns for Suggested Route
The route suggested by the map service needs to be
distinctive. We considered four patterns (Fig. 4) in the
field study and chose the first two based on user inputs
for further evaluation.

Figure 4. Patterns for the suggested
route. The first two were chosen based
on user inputs and named as Route 1,
2 respectively.

Interlacing Audio Labels
The online map service typically provides instructions
corresponding to each turn of the journey. We put an
interactive audio label at every turn and use the
information from the online map service to create audio
tags.

April 14–15, 2010, Atlanta, GA, USA

Utilizing a Legend
The patterns that are used should be included in a
legend that is provided to the user for quick reference
so that they know the shape of the pattern that they
are looking for.

Evaluation
Six visually-impaired participants were recruited to
evaluate our design solutions. The users have varied
levels of experience with tactile maps and different
background: one professional Braille proofreader, two
college students and three blind working professionals.
Three of them were born blind and three of them lost
vision later in their lives.
Experimental Setup
Six tactile maps of varying complexity were
automatically built from real on-line maps with
directions using the system in [9]. The proposed
patterns were then manually added to the maps
(through editing the SVG files). Each of the six maps
was rendered with different combinations of the chosen
patterns defined in the previous section (i.e., two
start/end pairs, two rendering patterns for the
suggested route, and two audio label densities). Before
a user started using the map, he/she was given a
legend containing a description of what patterns are
used for the map. The users could go back and refer to
it during the task. For each user, we started off with a
map that did not use the proposed adaptive rendering
scheme with the information button (everything else is
the same as with the other 5 maps). Then the user was
introduced to the rendering scheme and the starting
information button, which were used on the subsequent

3958

CHI 2010: Work-in-Progress (Spotlight on Posters Days 3 & 4)

5 maps. The created tactile maps are illustrated in Fig.
5. The users were asked to give a rating on the scale of
zero to ten about each of the patterns used. We also
recorded the time taken to find the start information
button, the start location, the end location, and to trace
the route. This was done silently without informing the
participant to avoid rushing them in exploring the map.
User Interface
A Laptop computer with the IVEO viewer software was
connected to an IVEO touchpad [7], as shown in Fig. 1.
The subject was asked to place the tactile map on the
touchpad and calibrate the device using step by step
instructions provided to him/her. After calibration, the
users can explore the map on the touchpad, and when
he/she presses an interactive button, the computer will
read out audio associated with that button.
Figure 5. Six maps used in the
experiments. Note that the first map does
not have the start information button while
all the other maps have the button on the
upper-right corner.

Results and Analysis
We evaluate our rendering scheme and make
comparison based on primarily three metrics: the time
taken to complete the tasks, the accuracy in grasping
the geographic information from the map, and the user
rating. The accuracy is simply defined by whether a
user is able to traverse all the legs of a route specified
by the online map and name the street names
correctly. For all the 6 maps, a 100% accuracy was
observed for all users (i.e., none of them missed any
legs of the journey and they all successfully traced the
route suggested by the online map while being able to
name the street names on the journey). In the
following, we present the results and analysis on the
times and ratings.
The information button could be located in 3.5 seconds
on average, with the maximum time taken being 10

April 14–15, 2010, Atlanta, GA, USA

seconds and the minimum being about 1 second.
Younger users turned to be quicker in a majority of the
tasks. The users expressed a strong liking for the
convenience provided by the information button as it
gave them a quick and good idea where to start looking
for the start/end locations.
On the Adaptive Rendering Scheme
This was evaluated by comparing the time to find the
start location for the first map and the subsequent 5
maps. On average, there was a time reduction from
112 seconds to 16 seconds in finding the start location
with the adaptive rendering scheme plus the start
information button. This is an impressive improvement
which suggests that the proposed scheme should be
adopted for such an application.

Start/End point Locating
Three of the maps use Pattern 1 and the other three
use Pattern 2. The times taken by the 6 users
(including the average) are shown in Fig. 6. The
average time taken for the two patterns is close and
the difference is not statistically significant. But 5 of the
6 users stated that Pattern 1 is more to their liking.
(The average user rating for Pattern 1 was 6.4 which is
slightly higher than that of Pattern 2 (5.3)).

Route Tracing
After the users located the start and then end location,
they were asked to trace out one possible way to travel
from start to end. Then they were asked to trace out
the suggested route that uses the route pattern. After
they finish tracing the route, they were asked to name
some of the streets they traveled on and if they could

3959

CHI 2010: Work-in-Progress (Spotlight on Posters Days 3 & 4)

identify the longest route. All of them could identify the
direction in which they were travelling and the longest
street. For the sake of simplicity, we have used less
than 6 legs and an average of 3 legs in each map.
Density of interactive buttons
We have used two different densities on the selected
route that read out the labels associated with locations
on the map. The users could not distinguish the
usefulness between the two densities on the map. But
when asked for their preference from the legend, all of
them liked the sparser density (Density 1) better.
Figure 6. Times taken for locating the
start/end addresses.

Other Preferences and Comments
All six users picked Pattern 1 for start/end location; 4
of the 6 users picked Route 1; 5 of the 6 users thought
Density 1 would suffice and anything denser would be
redundant; 5 out of 6 users wanted a separate symbol
for the start location and the end location. One of the
user wanted grids on the bounding box indicating a
scale so that he could correlate distances better on the
map. He also wanted more information in the start
button about the number of legs in the journey. He also
suggested having one symbol for start and the end
location. Another user wanted more information about
landmarks on the route like Starbucks, food joints etc.

Conclusions and Future work
We presented several design solutions and evaluated
their effectiveness for supporting a blind user in getting
routing information from online map services through a
tactile-audio map. The user study suggests that the
proposed design is easy to learn and use. And the users
affirmed the usefulness of a system that generates
such maps for everyday use. The study also led to
answers to some key design questions like what

April 14–15, 2010, Atlanta, GA, USA

patterns should be used and how audio should be
interlaced. These methods should be incorporated in
making automated tactile maps. Our future work
includes factoring the results of this study in building
the fully-automatic system of [9] and performing a
larger-scale study with more users and more varieties
of maps.

References

[1] Campin, B., McCurdy, W., Brunet, L., Siekierska, E.
SVG Maps for People with Visual Impairment, SVG
OPEN Conference, 2003.
[2] Edman, P.K., Tactile Graphics. AFB Press, New
York: American Foundation for the Blind, 1992.
[3] Hudson, M. J., Michigan State Univ. Talking Tactile
Map Project: Advancement Through Collaboration, 14th
Annual International Technology and Persons with
Disabilities Conference (CSUN), 1998.
[4] Miele, J. A., Landau, S., Gilden, D. Talking TMAP:
Automated generation of audio-tactile maps using
Smith-Kettlewell’s TMAP software. British Journal of
Visual Impairment, 2006.
[5] Wang, Z., Li, B., "Enable Interactive Access to Map
Images for People Who are Visually-Impaired", 24th
Annual International Technology and Persons with
Disabilities Conference (CSUN), 2008.
[6]

Google Map. http://maps.google.com/.

[7] IVEO Software/Touchpad:
http://www.viewplus.com.
[8]

MapQuest. http://www.mapquest.com/.

[9] Wang, Z., Li, B., Hedgpeth, T., Haven, T., "Instant
Tactile-Audio Map: Enabling Access to Digital Maps for
People with Visual Impairment", ACM Conference on
Computers and Accessibility (ASSETS), 2009.

3960

A Computational Approach to Relative Aesthetics
Vijetha Gattupalli∗ , Parag S. Chandakkar∗ and Baoxin Li
School of Computing, Informatics and Decision Systems Engineering, Arizona State University

arXiv:1704.01248v1 [cs.CV] 5 Apr 2017

{jgattupa,pchandak,baoxin.li}@asu.edu

Abstract—Computational visual aesthetics has recently become
an active research area. Existing state-of-art methods formulate
this as a binary classification task where a given image is predicted to be beautiful or not. In many applications such as image
retrieval and enhancement, it is more important to rank images
based on their aesthetic quality instead of binary-categorizing
them. Furthermore, in such applications, it may be possible that
all images belong to the same category. Hence determining the
aesthetic ranking of the images is more appropriate. To this end,
we formulate a novel problem of ranking images with respect to
their aesthetic quality. We construct a new dataset of image pairs
with relative labels by carefully selecting images from the popular
AVA dataset. Unlike in aesthetics classification, there is no single
threshold which would determine the ranking order of the images
across our entire dataset. We propose a deep neural network
based approach that is trained on image pairs by incorporating
principles from relative learning. Results show that such relative
training procedure allows our network to rank the images with a
higher accuracy than a state-of-art network trained on the same
set of images using binary labels.

I. I NTRODUCTION
Automatic assessment of image aesthetics is an active area
of research due to its wide-spread applications. Most of
the existing state-of-art methods treat this as a classification
problem where an image is categorized as either beautiful
(having high aestheticism) or non-beautiful (having low aestheticism)1 . In [1], [2], this problem has been formulated
as a classification/regression problem by mapping an image
to a rating value. Various approaches such as [1], [2], [3],
[4], [5], [6], [7], [8], [9], [10] have been proposed which
either use photographic rules or hand-crafted features to assess
the aesthetics of an image. Due to the recent success of
deep convolutional networks, approaches such as [11], [12]
claim to have learned the feature representations necessary to
categorize the given image as either beautiful or non-beautiful.
The approaches based on photographic rules have certain
limitations. For example, the implementations of these rules
may be an approximation, thus affecting the accuracy of
aesthetic assessment. Also, the rules may not sufficiently
govern the process of how we decide the aesthetic quality of an
image. It is possible that some of the important rules have been
1 We

use this terminology throughout the paper.
indicates equal contribution by authors.
Acknowledgement: The work was supported in part by ONR grant N0001415-1-2344 and ARO grant W911NF1410371. Any opinions expressed in this
material are those of the authors and do not necessarily reflect the views of
ONR or ARO.
∗

c
978-1-4799-7492-4/15/$31.00 
2016
IEEE

left out or some erroneous ones have been included. These
rules are mostly accompanied by generic image descriptors
or task-specific hand-crafted features. Such approaches suffer
from the disadvantages of generic/hand-crafted features that
they may not be suited for a special task such as aesthetic
assessment or the feature space does not fully represent
the key characteristics which make an image aesthetic. The
deep neural network based approaches may overcome these
disadvantages by learning the feature representations.
While deep learning approaches have advanced the state-ofart for this task, we observe that classifying a given image as
beautiful or non-beautiful may not always be the natural choice
for some applications. It may also be more intuitive for humans
to compare two images rather than giving an absolute rating to
an image based on its aesthetic quality. Moreover, all images
in a set could belong to the beautiful or non-beautiful category
according to a classification model. In such cases, it may often
be necessary to rank the images according to their aesthetic
quality. For example, a machine-learned enhancement system
[13] has to provide an enhanced version of the query image
to the user. To do so, it needs to compare two images with
respect to their aesthetics to determine which enhancement
results in a more beautiful image. In an image retrieval engine,
it would be desirable to have an option to retrieve images
having low/similar/high aesthetic quality as compared to the
query image.
Motivated by these observations, we introduce a novel
problem of picking a more beautiful image from a pair. We
term this problem as “Relative Aesthetics”. We build a new
dataset of image pairs for this task by carefully choosing
images from the popular AVA dataset [14] to satisfy certain
constraints. For example, we observed that comparing images
from unrelated categories (for example, a close-up of a car
and a wedding scene) does not make sense and hence such
pairs are avoided. There exists no single threshold which can
binary-classify the pairs correctly across the entire dataset. In
other words, if images were categorized into beautiful and nonbeautiful, then some of the pairs in our data could contain both
beautiful or both non-beautiful images. The details of dataset
creation and its statistical analysis are provided in Section IV.
Our problem draws certain parallels with “relative attributes” [15], where it was observed that training on
relatively-labeled data leads to models that capture more
general semantic relationships. They also mention that by

using attributes as a semantic bridge, their model can relate to
an unseen object category quite well. On the other hand, our
problem presents different challenges. In [15], they compare
two images with respect to attributes (for example, more
natural, furrier, narrower etc.), which are better defined than
the aesthetics of two images. Thus even though it is trivial to
use models trained on categorical data to solve these ranking
tasks, we found that using relative learning principles allows
us outperform previous state-of-art classification models by
gaining a more general and a semantic-level understanding of
the proposed problem.
Our contributions are as follows:
1) We propose a novel problem termed as “relative aesthetics”, which involves picking a more beautiful image
from a given pair of images. We create a new dataset
which has such relative labels from the popular AVA
dataset by careful and constrained selection of image
pairs. We will make our dataset and model source code
publicly available upon the decision of the paper.
2) We build a deep network incorporating the relative learning paradigm and train it end-to-end. To the best of our
knowledge, there is no prior work on studying aesthetics
in a relative manner using deep neural networks.
3) We show that our model trained on relatively-labeled
data is able to outperform a recent state-of-art method
[11] trained on a similar sized, categorically labeled
dataset for the proposed task.
Section II discusses the relevant literature. Section III
describes our relative, deep neural network based approach.
Section IV and V describe the data-creation, experimental
setup, results and analysis. We conclude in Section VI.
II. R ELATED W ORK
Computational aesthetics research in the earlier years was
focused on employing photographic rules, hand-crafted features or generic image descriptors. Intuitive and common
properties such as color [1], [7], [8], texture [1], [2], content
[6], [5], combination of photographic rules, picture composition and hand-crafted features [5], [4], [6] have been used.
The most commonly used photographic rules include Rule of
Thirds used in [5], [4], [1]. Other compositional rules include
low depth of field, opposing colors etc. [5]. Common color
features such as lightness, color harmony and distribution,
colorfulness have been quantified for aesthetics assessment
by computational models [1], [7], [8]. Texture features based
on wavelets edge distribution, low depth of field, amount of
blur have also been used [2], [5]. Approaches specifically
trying to model content in the image by detecting people [6],
[5], [4], generic image descriptors such as SIFT [16] have
been proposed in [5]. Inspired by the then success of deep
neural network on various tasks such as image classification
[17], [18], object segmentation [19], facial point detection
[20], Decaf features [21] for style classification [22] etc.,
[11] proposed a deep-learning-based approach to aesthetics
assessment. This approach classifies a given image as beautiful
or non-beautiful depending on the entire image as well as its

local patches. Another such approach was presented in [12]
where the authors aggregate the information from multiple
patches in the multiple-instance-learning manner to improve
the result of aesthetics assessment. Most of these approaches
treat aesthetics assessment as a binary classification task,
which may not always be the best choice for many applications, as discussed before.
The concept of training on relatively-labeled data to improve model performance and provide it with certain semantic
understanding of the problem has been well-explored. The
work on relative attributes [15] predicts the relative strength
of individual property in images. It allows for comparison
with an unseen object category in the attribute space. Models
learned in such a way enable richer text descriptions of
images. Relative attribute feedback was used in conjunction
with semantic language queries to improve the image search
capability in [23]. There are many such applications where
relative learning has explored a new dimension of the problem
and improved the overall understanding of the model of a given
task.
In this work, we propose to employ the relative learning principles for the task of image aesthetics assessment.
This task is extremely subjective and have vaguely-defined
properties than other general attributes like size, being more
natural etc. To allow for learning using hand-crafted features, various datasets have been proposed such as Photo.net,
DpChallenge.com, AVA dataset. The first two datasets contain
20,278 and 16,509 images respectively2 , whereas the AVA
dataset [14] contains 250,000 images. Thus we use AVA to
form image pairs which in turn will facilitate the learning
of our approach. We propose a Siamese deep neural network
architecture [24] with a relative ranking loss, which takes
an image pair as input and ranks them with respect to their
aesthetic quality. The back-propagation happens with the loss
obtained from the ranking function, which, we believe, helps
the network explore the attributes of certain images that make
them more beautiful than others.
III. P ROPOSED A PPROACH
The comparison of the aesthetics of two images is dependent
on many factors and people’s visual preferences. Some of the
factors include color harmony [7], colorfulness [1], inclusion
of opposing colors [5], composition [25], visual balance [26]
etc. They are also affected by the content in the picture
[4], [6]. Though determination of aesthetics is a subjective
process, there are some well-established rules in the photography community such as low depth-of-field, rule of thirds,
golden ratio [27]. However, making hand-crafted features for
such rules is difficult and often will lead to approximation
or misrepresentation of those rules. Therefore, we take a
deep neural network based approach in which we incorporate
relative ranking by designing a suitable loss function. Most of
the rules or aesthetic criteria can be defined using either an
entire image or a part of it. Therefore, for each image in the
2 Datasets

hosted on ritendra.weebly.com/aesthetics-datasets.html

TABLE I
A RCHITECTURE OF A COLUMN IN OUR PROPOSED NETWORK . C ONVOLUTION IS REPRESENTED AS ( PADDING , # FILTERS , RECEPTIVE FIELD , STRIDE )
Padded Input

Conv

Max-pooling

Conv

Max-pooling

Conv

Conv

Dropout

Dense

Dropout

Dense

Dropout

3 × 230 × 230

2, 64, 11, 2

2×2

1, 64, 5, 1

2×2

1, 64, 3, 1

−, 64, 3, 1

0.5

1000

0.5

256

0.5

512-D

512-D

256-D
1st image

Channel 1 - Column 1: C11

_
Local patch
from the
1st image

C1

Channel 1 - Column 2: C12

512-D

256-D
256-D
2nd image

d

d=

Channel 2 - Column 1: C21

_
Local patch
from the
2nd image

−
w2T

·

(f (w1T

· (C1 − C2 )))

C2

Channel 2 - Column 2: C22

256-D

512-D

C1 − C2

f (w1T ·(C1 −C2 ))

Fig. 1. Architecture of the proposed network; Weights are shared between the columns C11 and C21 (shown in green), C12 and C22 (shown in red); The
features obtained from C11 and C12 are concatenated (represented by _ symbol) to get C1 and C21 and C22 are concatenated to get C2 ; The vector
C1 − C2 is passed through two dense layers to obtain a score d comparing the aesthetics of two images. f (·) denotes an ReLU non-linearity. Please refer
to the text for further details.

pair, our network is trained on two views of an image as also
done in [11]: the entire image and a local patch. This enables
the network to see different aspects of the input. For example,
a view of the entire image may provide the network with the
knowledge of color composition while the local patch may
help with resolution, depth-of-field etc. We now describe our
network architecture and its training procedure in detail.
A. Network Architecture
Our deep convolutional neural network (DCNN) takes an
image pair as input. For each image in the pair, it takes as input
that image itself and its local patch. Since all images have to
be of the same size, they are warped to be 224 × 224 × 3.
A same-sized local patch is cropped from the original image.
We choose to warp the image based on the findings in [11],
which shows that local patches along with warped image gives
the best result. Our network has two “channels” as shown in
Fig. 1, corresponding to the input pair of images. A channel
is defined as the part of our CNN which takes an image along
with its local patch as input. Each channel has two “columns”.
One column takes the warped image and the other one takes
its local patch as input.
Our architecture is a Siamese network where each channel
shares weights in a certain way, which is shown in Fig. 1 by
means of color coding. The columns with the same color (i.e.
either red or green) share the same weights. This is because
the ranking produced by the network should be invariant to
the order of the images in the pair. Both channels have exactly
the same architecture until they are merged at the final dense
layer of 512 − D. We now describe the architecture of the
upper channel (channel 1). This channel has two columns

which takes the image and its local patch as input. Since
these two inputs are on a different spatial scale and trying to
convey different aesthetic properties as discussed earlier, we
do not set constraints on the weights of both the columns in a
channel. The upper column in channel 1 (C11 ) takes the entire
image as input which is of size 224 × 224 × 3, zero-padded
with 3 pixels on all sides. The column has five convolutional
layers. The first convolutional layer has 64 filters each of size
11 × 11 × 3 with stride 2. Second convolutional layer has
64 filters of size 5 × 5 with stride 1. Third and fourth layer
have 64 filters of size 3 × 3 with stride 1. These are followed
by two dense layers of size 1000 and 256 respectively. We
apply 50% Dropout at these two dense layers. Max-pooling is
applied after first two convolutional layers. Each max-pooling
operation halves the input in both the directions. We use ReLU
activation throughout. The architecture of C11 is also detailed
in Table I. The lower column of channel 1 (C12 ) and both
the columns of channel 2 (i.e. C21 and C22 ) have the same
architecture as C11 including dropout, max-pooling and zeropadding operations.
The key thing to note here is that the weights are shared
for (i) the two columns which take the entire image as input
i.e. C11 and C21 , and (ii) the remaining two columns which
take the local patches as input i.e. C12 and C22 . C11 and
C21 each generate a 256 − D representation (i.e. of the entire
image). Similarly, C12 and C22 also generate 256−D features
(i.e. of the local patch). We concatenate the two 256 − D
representations from (C11 , C12 ) as well as (C21 , C22 ) to form
two 512 − D representations. Fig. 1 shows this architecture
and the sharing of weights.

Now we explain our ranking loss function which takes the
above two 512 − D representations and gives a quantitative
measure comparing the aesthetics of the two images in a pair.
B. Ranking Loss Layer
Our network aims at correctly ranking two input images
based on their underlying aesthetic quality. Formally, given
two input images I1 and I2 , we decide that I1 is more beautiful
than I2 (also denoted as I1 > I2 here onward) if a positive
value is obtained for d(I1 , I2 ) and vice versa. In other words,
d(I1 , I2 ) is a measure comparing aesthetics of two images.
d(I1 , I2 ) = wT · (g(I1 ) − g(I2 ))

(1)

Here, g(I1 ) and g(I2 ) are the CNN representations. In our
network, g(I1 ) and g(I2 ) are represented by C1 and C2 respectively, as shown in Fig. 1. To increase the representational
power, we pass (C1 − C2 ) through two dense layers separated
by a ReLU non-linearity. Thus for our network, Equation 1
takes a slightly modified form as follows:
d(I1 , I2 ) = w2T · (f (w1T · (C1 − C2 )),

(2)

where f (·) denotes an ReLU non-linearity.
Keeping this in mind, we can now design our final loss
function with the following properties:
1) It should propagate zero loss when all image pairs are
ranked “correctly” (i.e. the representations of the images
in these pairs are separated by a margin δ).
2) It should only be able to produce a non-negative loss.
Hence the loss function is designed as follows:

C. Training Our Architecture
This architecture is trained using mini-batch SGD with a
learning rate of 0.001, momentum = 0.9, weight decay of
10−6 and by employing Nesterov momentum. The learning
rate is reduced by 15% after every 10 epochs. The batch size
is set to 50. Apart from warping and cropping out the local
patch, we only subtract the mean RGB value computed on
the training set from each pixel of the image. During training,
when the network makes a wrong decision, it is forced to learn
by exploiting the difference between some other characteristics
of the image in the next iteration. Over a number of epochs,
it manages to discover the relevant image properties which
better define image aesthetics.
We have 23, 000 image pairs containing all unique images
(i.e. total 46, 000 images). We use subsets of 20, 000 and 3, 000
pairs for training and validation respectively. We stop our
training when the accuracy on the validation set does not show
significant improvement for 10 consecutive epochs. We train
using relative labels i.e. a pair is labeled as 1 if r1 − r2 > 1,
otherwise it is labeled as −1. Here, ri is the average rating of
Ii in AVA dataset. More details on the data creation are given
in Section IV.
D. Testing Our Architecture
Given a new pair of images, we first subtract the mean of
the training data from each pixel of both the images. We would
like to point out that our test set does not share any pairs or
any individual images with the training and validation set. We
first pass both the images and their patches into our network
and get the value of d(I1 , I2 ) from Equation 2. I1 is then
predicted as a more beautiful image than I2 if d(I1 , I2 ) > 0
and vice versa. Our test set contains 20, 000 image pairs. We
use the weights of the epoch where we achieve highest ranking
accuracy with the least amount of loss on the validation set.
E. Ranking using a Network Trained on Categorical Labels

L = max(0, δ − y · d(I1 , I2 )),

(3)

where y is a ground-truth label which takes value 1 if the
first image in the pair is more beautiful than the second (i.e.
I1 > I2 ) and it equals -1 if I1 < I2 . The term max(0, ·)
is necessary to ensure that only non-negative loss gets backpropagated. The δ is a user-defined parameter which serves
two purposes. First, it defines a required separation to declare
I1 > I2 (or I1 < I2 ). That means if y · d(I1 , I2 ) > δ, then
no loss should be back-propagated for such pairs. Secondly,
and more importantly, δ > 0 avoids a trivial solution to our
optimization objective. To clarify further, if δ = 0, then for
y = 1 and y = −1, a common trivial solution exists which
makes either w1 = 0 or w2 = 0. We set δ = 3 as we
do not find any performance boost by further increasing the
separation between CNN feature representations of I1 and I2 .
In the further subsections, we explain the training and
testing procedures of our network. Then we compare the
aesthetic ranking results of our network against a state-of-art
network that is trained on a categorical data.

We train a network on categorically-labeled data using our
own implementation of the RAPID approach [11], which is
a recent state-of-art method for aesthetics assessment. It is
trained on the same set of 40,000 images that is used to train
our network. However, in this case, these images have been
categorized as either beautiful or non-beautiful depending on
the average ratings obtained directly from the AVA dataset. We
set the threshold that determines the class of an image equal to
5.5, since the ratings in the AVA dataset range from 1-10. This
network consists of stacks of convolutional layers, followed by
dense layers and finally a sigmoid to convert the raw scores
into a probability measure, p(y = 1|I), i.e. probability of an
image I belonging to the beautiful class. We point the reader
to [11] for more details about the RAPID network architecture.
While testing for a pair of input images, we pass first image
through the network and get the probability measure - p(y =
1|I1 ). Passing the second image gives us p(y = 1|I2 ). We
decide that first image is more beautiful than the second one
if p(y = 1|I1 ) > p(y = 1|I2 ). This test set contains 20, 000
image pairs and is identical to the test set used for our approach

as mentioned in Section III-D. Despite RAPID network being
similar in size to our network, it gets a significantly lower
accuracy on this relative ranking problem, which suggests that
a network trained on categorically-labeled data fails to learn
the complex, relative ranking order in the data.
IV. DATASET
Our task is to determine the more beautiful image in a
pair. To the best of our knowledge, there exists no such
dataset containing relatively-labeled pairs with respect to their
aesthetic rating. We created a dataset containing 43, 000 image
pairs. The individual images in these pairs belong to the
AVA dataset [14]. We use 20, 000 pairs for training, 3, 000
for validation and the rest for testing. We now describe the
protocol used to form the pairs out of the images from the AVA
dataset. The protocol can be defined by these three constraints:
1. The difference between the average ratings of images in
a pair should be ≥ 1. Constraining this difference ensures
that the training/test pairs are more likely to be aesthetically
different.
2. Each image in the AVA dataset has 210 ratings on an
average. We computed variance of all the ratings for each
image. We observed that the distribution of all these variances
over the entire the AVA dataset takes the form of a Gaussian
with a mean of 2.08 and a standard deviation of 0.6. The minimum and maximum variance in the image ratings are 0.8 and
4.5 respectively. As mentioned in [14], high variances among
the image ratings are a result of the collective disagreement
between the raters, which suggests that such images may have
certain abstract/novel content or photographic style, preferred
only by certain group of people. We avoid the images which
cause such significant disagreements among the raters by only
considering the images having rating-variance less than 2.6.
3. We avoid including pairs from different categories since the
characteristics which make an image aesthetic may vary with
the category. For example, a beautiful picture of a car may
have bright colors whereas a beautiful picture of a human face
may have low-depth of field and better details. Additionally,
since the ratings in the AVA dataset are crowd-sourced ratings,
the opinions may exhibit a preference towards some category.
We can mitigate the effect of these two factors by using
pictures from the same category to form pairs.
After such selection of pairs, we can form the relative labels.
We label a pair as 1 if the average rating of the first image
is greater than that of the second image and −1 otherwise.
The majority of the pairs in our dataset have the ratingdifference ≈ 1. To quantify, the rating-difference for about
85% of the training and test data is between 1 and 1.5. As
the rating difference between the images of a pair decreases,
choosing the more beautiful image in that pair gets difficult.
Also, to ensure that our network is not biased towards our
dataset, we replicate our experiments on another reference
test-set provided by the creators of the AVA dataset [14]. This
reference test-set contains 20, 000 images and has also been
used by [11]. By following the aforementioned protocol, these
20, 000 images yield us 7, 670 pairs. We call these set of pairs

TABLE II
R ESULTS FOR RANKING AND BINARY CLASSIFICATION

RAPID [11]
Proposed

Ranking
on our
test-set

Ranking on
the pairs from
standard test-set

Classification
on our test-set

Classification
on standard
test-set

62.21
70.51

65.87
76.77

59.92
59.41

69.18
71.60

as the standard test set. We now describe the experiments and
give analysis of results.
V. E XPERIMENTS AND R ESULTS
We run our network on our test set and the standard test
set containing 20, 000 and 7, 670 image pairs respectively. We
achieve a ranking accuracy of 70.51% and 76.77% on our
test-set and on the standard test-set respectively. Here, ranking
accuracy is defined as the fraction of pairs for which the model
correctly picks the more beautiful image according to the
ground-truth labels. We compare our approach with a state-ofart aesthetics classification network called RAPID [11], trained
as described in Section III-E: we pass both the images oneby-one to the RAPID network and choose the more beautiful
image. RAPID produces a ranking accuracy of 62.21% and
65.87% on ours and the standard test-set respectively. Since
each channel of our architecture is a replica of [11] with
the modified ranking loss, we compare our architecture only
with [11]. However, we believe that we will obtain similar
performance improvements if a different state-of-art model
(e.g. [12]) was used for each of our channels.
Due to our relative-learning-based approach, we believe that
our network has gained a semantic-level understanding of the
properties which make an image highly aesthetic. To verify
this, we attempted binary classification on our dataset as well
as the standard test-set. For this purpose, we extracted the
top channel of our network i.e. C11 and C12 (see Fig. 1).
We use the best weights learned from the ranking task for
this channel. After the last node, we just append a sigmoid
layer to convert the values into decision values. The input
image is passed through the network to obtain the probability
of that image being beautiful. We compute our results on a
subset of 10, 000 images taken from our test set and the entire
standard test set [14] containing 20, 000 images. On our test
set, proposed approach obtains 59.41% classification accuracy
as compared to 59.92% obtained by RAPID. On the standard
test set, we obtain an accuracy of 71.60% as compared to
69.18% obtained by RAPID. Note that we do not perform any
training to adopt our network for classification, which shows
that the learned features may be capturing the characteristics
that are responsible for making an image aesthetic. Our
network outperforms RAPID on the ranking task and produces
competitive performance on the classification task without any
additional training. We note that the performance of both the
networks is significantly lower on our test-set as compared to
that of on the standard test-set. This performance difference

Fig. 2. Rankings produced by our network are shown above. Top and bottom rows show correct and wrong predictions respectively for a total of 4 pairs.
Each of them are enclosed in either red/green boxes. For every pair, our network ranks the right image higher than the left image. Please view in color.

could be attributed to the fact that all images in the standard
test-set are distributed only over 8 categories, whereas the
images in our test-set are distributed over all 65 categories.
The results of all the experiments are summarized in Table II
Fig. 2 illustrates some ranking results obtained by our network. The wrong predictions in the bottom row show that the
network lacks semantic knowledge about objects and natural
phenomena. For example, even though the picture containing
two birds has better color harmony/contrast, the lightning
phenomena is a rare capture, making it more picturesque.
VI. C ONCLUSION
We introduced a novel problem of relative aesthetics which
could have widespread applications in image search, enhancement, retrieval etc. We created a dataset with a careful and
constrained selection of 43, 000 pairs of images from the AVA
dataset where one image is always more beautiful than the
other. We showed that a deep neural network trained with an
appropriate loss function which accounts for such relativelylabeled data, significantly outperforms a state-of-art network
trained on same data with categorical labels. The proposed
network is also able to achieve a competitive performance on
an aesthetics classification problem with trivial modifications
to its architecture and no fine-tuning at all. This shows that
it has gained a certain semantic-level understanding of the
factors involved in making an image aesthetic.
R EFERENCES
[1] R. Datta, D. Joshi, J. Li, and J. Z. Wang, “Studying aesthetics in photographic images using a computational approach,” in ECCV. Springer,
2006, pp. 288–301.
[2] Y. Ke, X. Tang, and F. Jing, “The design of high-level features for photo
quality assessment,” in IEEE CVPR, vol. 1, 2006, pp. 419–426.
[3] S. Bhattacharya, R. Sukthankar, and M. Shah, “A framework for photoquality assessment and enhancement based on visual aesthetics,” in The
18th ACM international conference on Multimedia, 2010, pp. 271–280.

[4] Y. Luo and X. Tang, “Photo and video quality evaluation: Focusing on
the subject,” in ECCV. Springer, 2008, pp. 386–399.
[5] S. Dhar, V. Ordonez, and T. L. Berg, “High level describable attributes
for predicting aesthetics and interestingness,” in IEEE CVPR.
[6] W. Luo, X. Wang, and X. Tang, “Content-based photo quality assessment,” in IEEE ICCV, 2011, pp. 2206–2213.
[7] M. Nishiyama, T. Okabe, I. Sato, and Y. Sato, “Aesthetic quality
classification of photographs based on color harmony,” in CVPR.
[8] P. O’Donovan, A. Agarwala, and A. Hertzmann, “Color compatibility
from large datasets,” in ACM Transactions on Graphics (TOG), vol. 30.
[9] H.-H. Su, T.-W. Chen, C.-C. Kao, W. H. Hsu, and S.-Y. Chien, “Scenic
photo quality assessment with bag of aesthetics-preserving features,” in
The 19th ACM international conference on Multimedia.
[10] L. Marchesotti, F. Perronnin, D. Larlus, and G. Csurka, “Assessing the
aesthetic quality of photographs using generic image descriptors,” in
IEEE ICCV, 2011, pp. 1784–1791.
[11] X. Lu, Z. Lin, H. Jin, J. Yang, and J. Z. Wang, “Rapid: Rating pictorial
aesthetics using deep learning,” in The ACM International Conference
on Multimedia, 2014, pp. 457–466.
[12] X. Lu, Z. Lin, X. Shen, R. Mech, and J. Z. Wang, “Deep multi-patch
aggregation network for image style, aesthetics, and quality estimation,”
in IEEE ICCV, 2015, pp. 990–998.
[13] J. Yan, S. Lin, S. B. Kang, and X. Tang, “A learning-to-rank approach
for image color enhancement,” in IEEE CVPR, 2014, pp. 2987–2994.
[14] N. Murray, L. Marchesotti, and F. Perronnin, “AVA: A large-scale
database for aesthetic visual analysis,” in IEEE CVPR.
[15] D. Parikh and K. Grauman, “Relative attributes,” in IEEE ICCV.
[16] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
International journal of computer vision, vol. 60, no. 2.
[17] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
with deep convolutional neural networks,” in Advances in neural information processing systems, 2012, pp. 1097–1105.
[18] D. Ciresan, U. Meier, and J. Schmidhuber, “Multi-column deep neural
networks for image classification,” in IEEE CVPR, 2012, pp. 3642–3649.
[19] F. Chen, H. Yu, R. Hu, and X. Zeng, “Deep learning shape priors for
object segmentation,” in IEEE CVPR, June 2013.
[20] Y. Sun, X. Wang, and X. Tang, “Deep convolutional network cascade
for facial point detection,” in IEEE CVPR, June 2013.
[21] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and
T. Darrell, “Decaf: A deep convolutional activation feature for generic
visual recognition,” arXiv preprint arXiv:1310.1531, 2013.
[22] S. Karayev, M. Trentacoste, H. Han, A. Agarwala, T. Darrell, A. Hertzmann, and H. Winnemoeller, “Recognizing image style,” in Proceedings
of the British Machine Vision Conference., 2014.
[23] A. Kovashka, D. Parikh, and K. Grauman, “Whittlesearch: Image search
with relative attribute feedback,” in IEEE CVPR, 2012, pp. 2973–2980.

[24] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun, C. Moore,
E. Säckinger, and R. Shah, “Signature verification using a siamese time
delay neural network,” International Journal of Pattern Recognition and
Artificial Intelligence, vol. 7, no. 04, pp. 669–688, 1993.
[25] O. Litzel, On Photographic Composition. Amphoto, 1975.
[26] W. Niekamp, “An exploratory investigation into factors affecting visual
balance,” ECTJ, vol. 29, no. 1, pp. 37–48, 1981.
[27] D. Joshi, R. Datta, E. Fedorovskaya, Q.-T. Luong, J. Z. Wang, J. Li, and
J. Luo, “Aesthetics and emotions in images,” IEEE Signal Processing
Magazine, vol. 28, no. 5, pp. 94–115, 2011.

Creating Stereoscopic (3D) Video from a 2D Monocular
Video Stream
Xiaokun Li1, Roger Xu1, Jin Zhou2, and Baoxin Li2
1

Intelligent Automation, Inc, Rockville, MD 20855
2
Arizona State University, Tempe, AZ 85287

Abstract. It is a challenge to generate stereoscopic (3D) video through a single
moving camera under widely varying conditions. We propose an efficient
approach to create true stereoscopic video from a monocular video stream
captured under various moving conditions. The approach contains three major
steps. First, we apply Harris’ corner detector to detect distinctive feature points
from a pair of image frames selected from the incoming video captured by a
moving camera. Second, according to the consecutive property of the video, a
local-window search based algorithm is developed for fast and accurate feature
correspondence between the two image frames. Third, a hierarchical image
rectification technique is designed to guarantee the success in creating a true
and visually-comfortable stereo image for each incoming image frame. Besides,
a software-based video stabilization algorithm is also developed for improved
stereo video generation performance. Extensive tests using real video collected
under various situations were performed for performance evaluation of the
proposed approach.

1 Introduction
In gaming and TV programs, 3D video effects are one of the most attractive features.
3D video techniques have also found wide civilian applications, such as medical
operations, microscopy, scientific data display, and CAD/CAM. Military applications
of 3D techniques include battlefield reconnaissance and surveillance. Conventional
computer-based stereo vision techniques, although studied for many years, still have
many limitations. For example, a conventional stereo vision system requires two
identical cameras, a narrow baseline, fixed parameter settings and positions. It is only
suitable for short-range scenes. However, in real world, camera motion is often
nonstationary and viewpoints of the camera are different from time to time.
Furthermore, parameter settings of the camera are sometimes unknown and variable
during video capturing.
To generate stereo (3D) video captured by a moving camera under widely varying
conditions presents a challenge. The main reason is that the camera is obliquely
mounted on a platform when the platform moves non-linearly or when the camera
parameters vary while the platform is moving. In recent years, many research efforts
have been made on stereo generation with uncalibrated cameras. Fusiello et al. [1]
developed a compact and efficient stereo generation algorithm via image rectification.
G. Bebis et al. (Eds.): ISVC 2007, Part I, LNCS 4841, pp. 258–267, 2007.
© Springer-Verlag Berlin Heidelberg 2007

Creating Stereoscopic (3D) Video from a 2D Monocular Video Stream

259

However, their approach assumes that the stereo rig is calibrated, which means the
intrinsic parameters of the camera pair such as focal length, aspect ratio, and their
relative position are already precisely known. Unfortunately, as mentioned earlier, the
camera parameters are not readily available and the relative position between the two
cameras is difficult to obtain or calibrate in practice. Loop and Zhang at Microsoft
Research [2] developed one method to construct stereo images with uncalibrated
cameras. Their method mainly relies on stereo matching and the residual distortion
may result in poor visualization performance. The method proposed by Hartley &
Zisserman [3], [4] for stereo generation from uncalibrated cameras is the most
advanced one in the literature to our best knowledge. One important distinction of
their method is that it is insensitive to unknown and variable camera parameter
settings during image/video capture. However, the quality of the generated stereo
images cannot be guaranteed. In some cases, the resulting stereo image may even be
corrupted.
Although many efforts have been made on stereo generation with uncalibrated
cameras, stereo video generation from a single moving camera is a fairly new
research topic. In this paper, we focus on the following stereo vision problem: given
two video frames acquired by a moving video camera at two different locations with
different viewing angles, we create a stereo image frame based on the two video
frames by means of feature extraction and correspondence, and image rectification.
The resulting stereo frame gives a viewer a realistic 3D perception, and can be
displayed on any type of display devices. Furthermore, for each video frame in the
video stream, we can successfully construct a stereo image based on the current frame
and a second frame with a predefined constant time delay very rapidly. As a result,
stereo video can be generated. The biggest challenge in generation stereo video from
the 2D video stream is how to generate a true and eye-comfortable stereo image for
each video frame. As can be found in the later part of this paper, our proposed robust
and efficient approach can successfully create stereo video in various situations.

2 Algorithm Description
The proposed approach consists of four algorithms executed in a sequential order. The
first algorithm is software-based video stabilization. The second is feature extraction
which provides an efficient way for image feature extraction by Harris’ corner
detection. The third is a local-window-based search method to find feature
correspondence of input frame pair. The last algorithm is stereo video generation
based on image rectification.
2.1 Software-Based Stabilization
When a camera is in motion during video acquisition, the acquired video may contain
unexpected jitters that will certainly affect the image rectification process of stereo
generation. Such scenario is illustrated in Fig. 1. To show how frame jitters may
affect image rectification, we use the example in Fig. 2 (a) for illustration. Suppose

260

X. Li et al.

Fig. 1. Frame jittering

(a)

(b)

Fig. 2. Conflict and video stabilization

(a)

(b)

Fig. 3. (a) Left image shows original moving path of feature points; (b) Right image shows the
moving path after stabilization

that there are three incoming video frames which are not in a straight line. For the first
two image frames, their stereo result is a clockwise rotation of the original frames. On
the other hand, for the second and third frames, the stereo result is a counterclockwise
rotation of the original frames. The conflict will damage the consistence of the stereo
video and bring the viewpoint varying problem. To alleviate the effect of framejittering, we developed a method to stabilize the video along the vertical direction. We
smooth the scene translation in the video by vertically shifting frames, which makes
the stereo generation easier to be performed. Although there exist some hardware
based stabilization systems, we still found that the software-based stabilization is very
useful due to the following two reasons: i) most existing stabilization systems remove
only high-frequency jitters, and the vide still contain low-frequency vertical shifting

Creating Stereoscopic (3D) Video from a 2D Monocular Video Stream

261

across frames; and ii) those hardware-based stabilization systems are unlikely to be
mounted on a light-weight platform such as a UAV/UGV.
The objective of our method is to make the frames roughly aligned horizontally, as
illustrated in Fig. 2 (b). To stabilize the video, we first subtract the two selected
frames. Then, we analyze the residua of the subtraction. The degree of horizontal shift
of the two image frames can be estimated by counting the number of horizontal edges
and the average distance between the corresponding horizontal edges in the residual
image. Based on the estimated horizontal shift, we align the two frames by shifting
the second frame up or down with the estimated shifting value which is equal to the
average distance value of the corresponding horizontal edges. Fig. 3 (a) shows one
example of the tracked feature trajectories from a real-life video sequence. The green
crosses are feature points and the red curves are the trajectories of the features across
ten frames. Fig. 3 (b) shows the result of the stabilization. As we can see from the
result, the red curves in horizontal direction are reduced a lot after stabilization.
2.2 Feature Extraction
To accurately detect image features for stereo, we use Harris’ corner detector [8]
whose efficiency on corner and edge detection has been proven due to its strong
invariance to object rotation, scale, illumination and noise. The Harris corner detector
is based on the local auto-correlation function of a signal, where the local autocorrelation function measures the local changes of the signal with patches shifted by a
small amount in different directions. The brief introduction of Harris corner detector
is given below:
Given a shift ( Δx, Δy ) and a point (x, y), the auto-correlation function is defined
as,

c( x, y ) = ∑ [ I ( x i , y i ) − I ( x i + Δx, y i + Δy )] 2
W

where I(·, ·) is denoted as the image function and ( xi , y i ) are the points in the
window W (Gaussian) centered on (x, y).
The shifted image is approximated by a Taylor expansion truncated to the first
order terms,

⎡ Δx ⎤
I ( x i + Δx, y i + Δy ) ≈ I ( x i , y i ) + [ I x ( x i , y i ) I y ( x i , y i )]⎢ ⎥
⎣ Δy ⎦
where Ix(·, ) and Iy(·, ) denote the partial derivatives in x and y, respectively.
Substituting the approximation I(·, ·) to c(x,y) yields,
c( x, y ) = ∑ [ I ( x i , y i ) − I ( x i + Δx, y i + Δy )] 2
W

⎡ ∑ (I x (x x , yi ) 2
⎢
W
= [Δx Δy ]⎢
I x ( x i , y i ) I y ( xi , y i )
∑
⎢
⎣W

∑ I x ( xi , y i ) I y ( xi , y i )⎤⎥ ⎡Δx ⎤
W

∑ ( I y ( xi , y i ) 2
W

⎥ ⎢ Δy ⎥
⎥⎣ ⎦
⎦

262

X. Li et al.

⎡ Δx ⎤
= [Δx Δy ]C ( x, y ) ⎢ ⎥
⎣ Δy ⎦
where matrix C(x, y) captures the intensity structure of the local neighborhood. Let
λ1 , λ2 be the eigenvalues of matrix C(x, y). The eigenvalues form a rotationally
invariant description.
There are three possible results after the eigenvalues are obtained:
1. If both λ1 and λ2 are small, which means that the local auto-correlation function is
flat (i.e., little change in c(x, y) in any direction), so the windowed image region is
of approximately constant intensity.
2. If one eigenvalue is large and the other is small, which means the local autocorrelation function is ridge shaped and only local shifts in one direction (along the
ridge) cause a small change in c(x, y) and a significant change in the orthogonal
direction, the point (x,y) is on an edge.
3. If both eigenvalues are large, which means the local auto-correlation function is
sharply peaked and shifts in any direction will result in a significant increase, the
point (x,y) is a corner point.
2.3 Feature Correspondence

According to the consecutive property of video, we developed a fast feature
correspondence method based on local-window search. The key idea of the localwindow based search is summarized as follows:
First, we select some feature points from the first image of a given image pair.
These points are selected according to their significance (edge energy) and
geometrical distribution, and can be expressed as the following:
Pj = { pij | i = 1..n j }

where Pj is the set of the selected feature points of Image j and pij is ith feature point
of Image j.
Second, suppose we need to find feature correspondences between the first image
(t) and the second image (s) of a selected video frame pair, that is, we need to output a
set of feature point pairs which assumed to be point correspondences between the two
images.
PC (t , s ) = Matching ( Pt , Ps ) = {( pit , p js )}

To do this, we need to find the best correspondence in the second image for each
feature point in the first image. Now, the problem becomes how to compute the
matching score for two feature points from an image pair.
Third, since the motions of the scene/objects are continuous in video, for any
feature point in the first image its corresponding feature point in the second image
should be located very close to its position, which means we do not have to search its
corresponding feature point in the whole image space. Based on this observation, we

Creating Stereoscopic (3D) Video from a 2D Monocular Video Stream

263

developed a local-window search method for fast feature correspondence. For each
selected feature point in the first image, we define a local-window which is centered
at this point. Then, we set up a sliding window in the second image, which has the
same size of the local-window in the first image, and shift the window in a region of
the second image. During the sliding, we compare the difference between the localwindow and the sliding window at each position by computing its matching score
which is defined as the average difference of pixel intensity in the two windows.
Diff ( n, i ) =

1
area ( window )

n

∑ I ( n ) − I (i , j )
j =1

where I(n) represents the sum of the intensity values of the local window centered at
the selected feature point n in the first image ; I(i,j) represents the sum of the intensity
values of the sliding window centered at the feature point i in the second image; j is
the pixel located in the current sliding window; i is the feature point on the sliding
trace; area(patch) is the size of the local window. For the feature point n, the feature
point in the second image with the minimum difference will be chosen as its best
match. In this way, we can find the feature correspondence for each feature point in
the first image.
2.4 Hierarchical Rectification for Fast and Robust Stereo Video Generation

In the introduction section, we knew that the
most efficient way on stereo is using
fundamental matrix for stereo generation. It
is known that the fundamental matrix is
computed from point correspondences. As
we discussed before, in some cases an
incorrect fundamental matrix might be
obtained because the fundamental matrix
computation has some rigorous assumptions,
such as no dominant plane is permitted. In a
video steam, there are many cases, where
those assumptions do not hold. Therefore,
although a true stereo image can be
Fig. 4. Hierarchical image rectification
guaranteed with a correct fundamental matrix
we cannot completely rely on the fragile and
time consuming fundamental matrix for
stereo construction. In addition, for many cases we do not need to do any image
rectification for an image pair as the pair is a stereo pair already. According to this
observation, we developed a hierarchical image rectification strategy as illustrated in
Fig. 4 to not only improve the quality and the success rate of stereo but also e save the
processing time. Also, to overcome the viewpoint varying problem during stereo
construction, we only perform image rectification to the second image of a selected
frame pair. In this way, the viewpoint of the generated stereo video will always be the
same as the camera viewpoint.

264

X. Li et al.

(1) Stereo check
One property of a stereo pair is that all disparities are horizontal. If a frame pair
satisfies the constraint that all disparities are horizontal, no rectification is needed. In
our stereo generation algorithm, we will first compute the average absolute vertical
disparities. If the value is less than a predefined threshold and the average value of the
horizontal disparities is closed to the standard baseline value, we assume that the
frame pair is a stereo image pair already.
(2) Image Rectification Method 1: Image Translation
If the stereo check fails for the frame pair, we will shift the second image in the
horizontal direction to minimize horizontal disparities. Afterwards, we will perform
stereo check again. If the result can pass the stereo check, we finish the stereo
generation for this image pair. If it fails again, we apply the second method described
below.
(3) Image Rectification Method 2: Homography
Here, we make the disparities of the frame pair to be horizontal by applying a
homography transformation to the second image of the frame pair. After the
homography transformation, the feature points of the second image should have the
same horizontal line as the feature points of the first image. We can compute a
homography from four correspondences via the following steps:
Algorithm - Homography
Step 1: Select four points from a set of feature points of the first image.
Step 2: Find their correspondence in the second image,
Step 3: Call RANSAC-based model matching method [9] to compute the
Homography transformation matrix, H.

In practice, the point correspondences we found may still include some mismatches.
Therefore, we use RANSAC to find the best Homography transformation matrix in
global optimum. If the stereo construction by Homography fails, we will apply the third
method for stereo.
(4) Image Rectification Method 3: Fundamental Matrix
The fundamental matrix based image rectification method is built on our recent work
[5-7]. That is, for two selected uncalibrated video frames of the same scene, we
rectify them into a standard stereo pair (which is the subject of [5-7]). In this research,
we customize our fundamental-matrix-based stereo imaging algorithm for stereo
video generation. Instead of rectifying two images into a standard stereo pair, we only
calculate H matrix for the second image. In this way, we can solve the “viewpoint
varying” problem. Here, we give a very concise description and the basic steps of the
stereo construction.
Algorithm - Stereo image generation based on fundamental matrix
Step 1: Estimate fundamental matrix F [2] which represents the spatial difference of
the two selected images
Step 2: Compute the rectification matrix H2 for the second image
Step 3: Rectify each pixel of the second image by multiplying H2 to generate a widebaseline or narrow baseline stereo pair

Creating Stereoscopic (3D) Video from a 2D Monocular Video Stream

265

Step 4: Compute the average Z value of the center part of the images and translate
them to configure a proper baseline (standard baseline) to construct a true
stereo pair
Step 5: If the result cannot pass through stereo check, jump to the Homography based
method and output the stereo result of the Homoography based method
without stereo check.

3 Experimental Results
Extensive tests including many side-looking and down-looking scenarios were
performed to improve and optimize our proposed stereo algorithm. We summarized
our results in this section. The statistic results on side-looking and down-looking
video clips are listed in Table 1 and Table 2 separately. Some experimental results are
shown in Fig. 5 to Fig. 7. All tests were performed on an ordinary PC (Intel P-4
2.4GHz and 1G memory) with MS Windows XP. For the incoming video with 25fps
and 640 by 480 resolution, the average processing time on stereo video generation is
6.2 times real-time (about 4 fps). The processing includes video decoding and
encoding, stereo generation and display. According to our calculation, the average
processing speed on pure stereo creation is 4.2 times real-time (about 6 fps). After
algorithm optimization, we expect to improve the processing speed significantly so
that it can run in near real time.
3.1 Experimental Results on Side-Looking Video Clips
Table 1. Statistic results of side-looking video clips
File name

SideLooking-1.avi
Movie1_1.avi
Movie2_1.avi

Length
(hr:min:sec)

Frame
rate

Resolution

00:02:17
00:01:50
00:03:01

30
25
25

640x480
720x480
720x480

(a)

Entire processing
time including
decoding and
encoding time
00:14:50
00:12:18
00:20:31

Processing
time for
stereo
00:09:50
00:07:58
00:13:32

(b)

Fig. 5. Example #1 Side-looking ((a) 2D frame, (b) 3D frame in Red-cyan format)

266

X. Li et al.

3.2 Experimental Results on Down-Looking Video Clips
Table 2 Statistic results of down-looking video clips
File name

Recon_1_all.avi
Side_1_all.avi
VTS_01_1.avi
VTS_01_2.avi
VTS_01_3.avi
1stmsnops.avi
Safe house.avi
Bda bldgs.avi
Kidnapping.avi
KiowaDown.avi
KufaCompressed.avi
Raven Footage.avi

(a)

Length
(hr:min:sec)

Frame
rate

00:00:49
00:05:54
00:04:32
00:34:46
00:05:50
00:00:25
00:03:34
00:00:15
00:03:28
00:00:41
00:00:24
00:00:48

25.0
29.6
29.4
29.1
29.8
23.5
23.7
23.7
23.8
23.9
23.7
22.2

Resolution

720x480
304x224
720x480
720x480
720x480
320x240
320x240
320x240
320x240
320x240
640x480
640x480

Entire
Processing time
processing
for Stereo
time including
decoding and
encoding time
00:04:54
00:03:11
00:39:25
00:28:33
00:52:54
00:40:32
04:53:23
03:54:11
00:03:42
00:02:43
00:02:31
00:01:36
00:23:12
00:15:18
00:01:36
00:01:03
00:22:51
00:15:18
00:04:06
00:02:35
00:02:36
00:01:43
00:05:07
00:03:25

(b)

Fig. 6. Example #2 Down-looking ((a) 2D frame, (b) 3D frame in Red-cyan format)

(a)

(b)

Fig. 7. Example #3 Down-Looking ((a) 2D frame, (b) 3D frame in Red-cyan format)

Creating Stereoscopic (3D) Video from a 2D Monocular Video Stream

267

4 Conclusions
We have presented a systematic approach to stereo video formation. The approach
consists of software-based stabilization, robust feature extraction using Harris’ corner
detection, accurate feature correspondence by a local-window based search, and
reliable and robust video generation based on hierarchical image rectification. Key
advantages of our approach include stereo video formation from a single moving
uncalibrated camera and real-time processing capability. Extensive evaluations using
real-life data collected from a single moving camera with unknown parameters and
unknown relative position and orientation of the camera clearly demonstrated the
efficiency of the approach. The future work of our research includes algorithm
optimization and improvement on stereo video generation, and real-time
implementation.
Acknowledgements

This work was supported by a grant from the U. S. Army Research Lab (ARL). The
authors would like to thank Dr. Barry Vaughan for his helpful suggestions and
comments.

References
1. Fusiello, A., Trucco, E., Verri, A.: A compact algorithm for rectification of stereo pairs.
Journal of Machine Vision and Applications 12, 16–22 (2000)
2. Loop, C., Zhang, Z.: Computing rectifying homographies for stero vision. In: CVPR, pp.
125–131 (1999)
3. Hartley, R., Zisserman, A.: Multiple view geometry in computer vision, 2nd edn.
Cambridge University, Cambridge (2003)
4. Hartley, R.: Theory and practice of projective rectification. IJCV 35(2), 1–16 (1999)
5. Zhou, J., Li, B.: Image Rectification for Stereoscopic Visualization without 3D Glasses. In:
Proc. Int. Conf. on Image & Video Retrieval (July 2006)
6. Li, X., Kwan, C., Li, B.: A generic approach on object matching and tracking. In:
Campilho, A., Kamel, M. (eds.) ICIAR 2006. LNCS, vol. 4141, Springer, Heidelberg
(2006)
7. Li, X., Kwan, C., Li, B.: Stereo imaging with uncalibrated camera. In: ISVC 2006 (2006)
8. Harris, C., Stephens, M.J.: A combined corner and edge detector. In: Alvey vision
conference (1988)
9. Fischler, M.A., Bolles, R.C.: Random Sample Consensus: A Paradigm for Model Fitting
with Applications to Image Analysis and Automated Cartography. Comm. of the ACM 24,
381–395 (1981)

A Machine-learning Approach to Retrieving Diabetic
Retinopathy Images
Parag S. Chandakkar, Ragav Venkatesan,
Baoxin Li
Computer Science & Engineering
Arizona State University, Tempe, AZ, USA

{pchandak, ragav.venkatesan,
Baoxin.Li}@asu.edu
ABSTRACT
Diabetic retinopathy (DR) is a vision-threatening complication
that affects people suffering from diabetes. Diagnosis of DR
during early stages can significantly reduce the risk of severe
vision loss. The process of DR severity grading is prone to human
error and it also depends on the expertise of the ophthalmologist.
As a result, many researchers have started exploring automated
detection and evaluation of diabetic retinal lesions. Unfortunately,
to date there is no automated system that can perform DR lesion
detection with the accuracy that is comparable to a human expert.
In this poster, we present a novel way of employing content-based
image retrieval for providing a clinician with instant reference to
archival and standardized DR images that are used for assisting
the ophthalmologist with the diagnosis of a given DR image. The
focus of the poster is on retrieving DR images with two
significant DR clinical findings, namely, microaneurysm (MA)
and neovascularization (NV). We propose a multi-class multipleinstance DR image retrieval framework that makes use of a
modified color correlogram (CC) and statistics of steerable
Gaussian filter (SGF) responses. Experiments using real DR
images with comparisons to other prior-art methods demonstrate
the improved performance of the proposed approach.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]; I.5 [Pattern
Recognition]

General Terms
Algorithms, Performance

Keywords
Diabetic retinopathy, image retrieval, multiple-instance learning,
Color Correlogram, Steerable Gaussian Filters, Fast Radial
Symmetric Transform

1. INTRODUCTION
Diabetic retinopathy (DR) is one of the leading causes of
blindness among adults. Diabetes is a disease associated with the
blood vessels. Glycosylation reaction occurs between sugar and

The work is support in part by a grant from the Agency for Healthcare
Research and Quality (Grant Number 1R21HS019792-01A1). Any
opinions and findings expressed in this material are those of the authors
and do not necessarily reflect the view of the Agency for Healthcare
Research and Quality.
Copyright is held by author/owner(s).
ACM-BCB'12, October 7-10, 2012, Orlando, FL, USA

ACM 978-1-4503-1670-5/12/10

ACM-BCB 2012

Helen K. Li, MD
Weill Cornel Medical College/The Methodist Hospital, The
University of Texas Health Science Center Houston, and
Thomas Jefferson University.

hli@communityretina.com

the proteins in the vessel walls, owing to a large amount of
glucose coursing through the circulatory system. Organs like the
eyes and the kidney have microvasculature that is more
susceptible to damage because of glycosylation. Studies have
shown that timely DR diagnosis and treatment can significantly
reduce the risk of severe vision loss [1]. By improving the
assesment qualities of DR severiety levels, the quality of
diagnosis of DR can be significantly improved.
Unfortunately, there is a dearth of computer-based systems that
can match the level of performance achieved by ophthalmologists.
Further, an even more difficult task is fast and accurate DR
severity grading. This has not been properly addressed by existing
systems. An ophthalmologist’s painstaking visual examination of
a digital retinal image and physical comparison with standard
images are still the best methods for identifying and assessing DR.
In previously published works, there have been several attempts to
develop content-based image retrieval (CBIR) systems. Structured
Analysis of the Retina (STARE) project aimed at automatic
diagnosis and comparison of images, including annotating image
contents and searching for images similar in content [2]. Though
the retrieved images appear to be largely similar in appearance,
they are not clinically relevant. Thus the method is of limited
clinical use.
The proposed research aims at filling this gap in both technical
capability and clinical practice by developing a computer-based
system with the innovative idea of content-based retrieval and
classification of DR images with clinical relevance.
The remainder of this article is organized as follows: Section 2
presents the algorithm. Section 3 provides the results of the new
algorithm and Section 4 provides concluding remarks.

2. PROPOSED APPROACH
Multi-Class Multiple Instance Learning (McMIL) for DR
classification is already studied in [3]. It can be observed that
McMIL performs best for DR image amongst the other methods
compared against in that article. The authors therefore, propose
McMIL for DR image retrieval also.
Image retrieval, unlike classification requires a distance or density
estimation. It is imperative thereby that for the purposes of
retrieval a McMIL based distance approach has to be developed.
Work on this is already done in [4]. A Haussdorff distance is used
in Citation-KNN. It is defined as the least distance between two
bags. In certain MA and NV images, very few regions of the
image are affected and the other instances resemble that of a
Normal image. The minimal Haussdorff distance thus doesn’t
provide any information and it doesn’t suffice. This calls for a
new way of measuring the distance between two bags. The
authors are currently developing a new distance framework and

588

ranking algorithm called the Rank-KNN. It considers distances
between two bags at an instance level rather than at bag-level.
Suppose there are two bags ‫ ܣ‬and‫ ܤ‬with ݉ and ݊ instances each.
The minimum distance between the ݅ ௧௛ instance of ‫ ܣ‬and all
instances of ‫ ܤ‬is given by,
‫ܦ‬ሺ௜ሻ ሺ‫ܣ‬ǡ ‫ܤ‬ሻ ൌ

݉݅݊
݀൫ܽ௜ ǡ ܾ௝ ൯‫ א ݆׊‬ሼͳǡʹǡ Ǥ Ǥ ݊ሽ
ܾ‫ܤא‬

(1),

where, ݀ሺήǡήሻ represents the Euclidean distance between two
instances.
Let ܳ be the bag of instances for a query image. Equation (1) is
used to calculate the minimum distances between every instance
in ܳ and instances of all the images in the database. Sorting
individually a similarity list (SL) is obtained. The mean of ranks
in the SL belonging to that particular bag is calculated. After
repeating this procedure for each bag, a bag-level aggregated
similarity rank (ASR) is obtained. The rank list thus formed is
called the m-Rank. Since only few instances contribute towards
label, retrieval using only on the nearest neighbors need not
necessarily give optimum results. To overcome this, citer-Rank is
incorporated into M-Rank to make it more robust. The procedure
of calculating citers is is explained in [4]. A final meanRank is
obtained by averaging m-Rank and citer-Rank of ܳ.
The feature space operated upon for this approach is based on a
CC approach described in [5]. This feature space is modified by
spectral tuning described in [3]. These features are extracted for
each instance are can be further augmented with Statistic of SGF
[6] and Fast radial symmetric transform [7]. This provides for a
accurate catching of the proper instance representations.

3. RESULTS

Table 1 presents the results of k hit-rate and also shows mean
accuracy for the retrieval of top 5 images retrieved. The
performance metrics have been adopted from [8]. These are also
compared against some state of the art methods. The experiments
are performed on a database containing 425 images assembled
from well-known public databases. These contain an unbalanced
ensemble of 160 normal images, 181 MA images and 84 NV
images. The retrieved images are considered a hit as long as the
retrieved images and the query images contain the same label.
Some of the state-of-art image retrieval systems used for
comparison is Gabor features [9] and semantics of histogram of
neighborhood moments [10]. Another well-studied feature for
CBIR puposes related to general images is the color correlograms
[11, 5]. CC describe the global correlation of local spatial
correlation of colors. Gabor features is a strong representation of
textures. HNM feature is especially preferred in medical image
retrieval [10]. HNM is considered good color feature because it
takes into account the spatial correlation and the global
distribution of colors.
Table 1. Mean accuracies and k hit rates in percentages.

From table 1, it can be noted that k hit-rate at each rank is much
higher than the other methods. This implies that the retrieved
images have more clinical relevance than the other methods.

4. CONCLUSION
Our preliminary study in this poster suggests that McMIL is a
promising approach for DR classification. Specifically, in the
preliminary results reported in this paper, the McMIL-based
retrieval algorithm retrieves DR images that are of clinical
relevance than otherwise. Further developments are under way,
including more experiments with comparison with other leading
approaches and analysis of the performance of our algorithm.

5. REFERENCES
[1] "Grading diabetic retinopathy from stereoscopic color fundus
photographs - An extension of the modified airlie house
classification.," Early treatment of diabetic retinopathy study
group (ETDRS). Rep. 10 Opthalmology., 1991.
[2] A. Gupta , S. Moezzi, A. Taylor, S. Chatterjee, R. Jain, L.
Goldbaum and S. Burgess, "Content-based retrieval of
opthalmological images.," in ICIP, 1996.
[3] R. Venkatesan, P. S. Chandakkar, B. Li and H. Li,
"Classification of Diabetic Retinopathy Images Using MultiClass Multiple-Instance Learning Based on Color
Correlogram Features," in IEEE conference on Engineering
in Medicine and Biology Society, San Diego, 2012.
[4] J. Wang and J.-D. Zucker, "Solving the multiple-instance
problem: A lazy learning approach," in 17th International
conference of Machine Learning, 2000.
[5] M. Li, "Texture moment content-based image retrieval," in
IEEE ICME, 2007.
[6] W. Freeman and E. Adelson, "The design and use of
steerable filters," IEEE Transactions on pattern analysis and
machine intelligence, vol. 13, pp. 891-906, 1991.
[7] G. Loy and A. Zelinsky, "Fast radial symmetry for detecting
points of interest," IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 25, pp. 959-973, 2003.
[8] Z. Wang and B. Li, "Learning to Recommend Tags for Online Photos," Social Computing and Behavioral Modeling,
pp. 1-9, 2009.
[9] B. Manjunath and W. Ma, "Texture features for browsing
and retrieval of image data," IEEE transactions on pattern
analysis and machine intelligence (PAMI-special issue on
digital libraries), vol. 18, pp. 837-842, August 1996.

42.35
59.52

5
hitrate
23.29
31.76

[10] Q. Chen, X. Tai, Y. Dong, S. Pan, X. Wang and C. Yin,
"Medical image retrieval based on sementic of neighborhood
color moment histogram," in The 2nd international
conference on bioinformatics and biomedical engineering
(ICBBE), 2008.

55.76
68.00

32.00
43.29

[11] J. Huang, S. Kumar, M. Mitra, W. Zhu and R. Zabih, "Image
indexing using color correlograms," in IEEE CVPR, 1997.

Mean
Accuracy

2
hit-rate

3
hit-rate

4
hit-rate

AutoCC
Gabor

60.85
68.61

79.06
83.76

64.47
75.76

HNM
Proposed

68.04
75.48

84.47
88.70

74.58
82.58

ACM-BCB 2012

State-of-art methods like Gabor filters, original CC and HNM fail
to produce satisfactory results as texture or color features, by
themselves cannot be good feature descriptors. The proposed
approach combines color and texture feature and this with McMIL
produces good results.

589

arXiv:1604.08220v1 [cs.LG] 27 Apr 2016

Diving deeper into mentee networks
Ragav Venkatesan
Arizona State University

Baoxin Li
Arizona State University

ragav.venkatesan@asu.edu

baoxin.li@asu.edu

Abstract
Modern computer vision is all about the possession of
powerful image representations. Deeper and deeper convolutional neural networks have been built using larger and
larger datasets and are made publicly available. A large
swath of computer vision scientists use these pre-trained
networks with varying degrees of successes in various tasks.
Even though there is tremendous success in copying these
networks, the representational space is not learnt from the
target dataset in a traditional manner. One of the reasons for opting to use a pre-trained network over a network
learnt from scratch is that small datasets provide less supervision and require meticulous regularization, smaller and
careful tweaking of learning rates to even achieve stable
learning without weight explosion. It is often the case that
large deep networks are not portable, which necessitates the
ability to learn mid-sized networks from scratch.
In this article, we dive deeper into training these midsized networks on small datasets from scratch by drawing additional supervision from a large pre-trained network. Such learning also provides better generalization accuracies than networks trained with common regularization
techniques such as l2 , l1 and dropouts. We show that features learnt thus, are more general than those learnt independently. We studied various characteristics of such networks and found some interesting behaviors.

1. Introduction
With the proliferation of off-the-shelf, downloadable
networks such as VGG-19, overfeat, R-CNN and several
others in the caffe model zoo, it has become common practice in the computer vision community to simply fine-tune
one of these networks for any task [21, 13, 8]. These networks are usually trained on a large dataset such as Imagenet and Pascal [20, 7]. The proponents of these networks
argue that these networks have learnt image representations
that are pertinent for most datasets that deal with natural
images. Under the assumption that all these datasets are
natural images and are derived from a similar distribution

Mentor

Mentee

Figure 1. Mentor mentoring mentee on the second hidden layer.

this might as well be true. Even with such networks, features that are unique to each datasets do matter. While finetuning of an already trained network works to a certain extent, these features are not learnt in a traditional manner on
the target dataset but are simply copied. There is also no
guarantee that these features are the best representations for
the target dataset, although there is some validity in expecting that such a representation might work well, since after
all it was learnt from a large enough dataset.
Most computer vision scientists do not attempt to train
a new architecture from scratch (random initializations).
Training even a mid-sized deep network with a small dataset
is a notoriously difficult task. Training a deep network,
even those with mid-level depth require a lot of supervision in order to avoid weight explosion. On most imaging
datasets, with image sizes being 224X224, the memory insufficiency of a typical GPU restricts the mini-batches to
less than 100. Using small mini-batches and small datasets
lead to very noisy and untrustworthy gradients. This leads
to weight explosions unless the learning rates are made sufficiently smaller. With smaller learning rates, learning is
slowed down. With smaller mini-batches learning is unstable. One way to avoid such problems is by using reg-

ularization. By regularizing we can penalize the gradients
for trying to make the weights go higher and higher. Batch
Normalization is another technique that is quite commonly
used to keep weight explosion under check [12]. Even with
these regularization techniques, the difficulty of training a
deep network from scratch leads most computer vision scientists to use pre-trained networks.
There are several reasons why one might favour a smaller
or a mid-sized network even though there might be a better
solution available using these large pre-trained networks.
Large pre-trained networks are computationally intensive
and often have a depth in excess of 20 layers. The computational requirement of these networks do not make them
easily portable. Most of these networks require state-ofthe-art GPUs to work even in simple feed forward modes.
The impracticality of using pre-trained networks on smaller
computational form factors necessitates the need to learn
smaller network architectures. The quandary now is that
smaller networks architectures cannot produce powerful
enough representations.
Many methods have been recently proposed to draw additional supervision from large well-trained networks to
regularize a new network while learning from scratch [23,
19, 2, 5]. All of these works were inspired from the Dark
Knowledge (DK) approach [11]. All these techniques use
at most one layer of supervision on top of the softmax supervision and try to use this technique to learn more deeper
networks better. Figure 1 shows a conceptualization of this
idea.
In this paper, we try and make a shallower mentee network learn the same representation as a larger, well-trained
mentor network at various depths of its network hierarchy.
Mentorship happens by tagging on to the loss of the mentee
network, a dissimilarity loss for each layer that we want
mentored. To the best of our knowledge, there hasn’t been
any work that has regularized more than one layer this way.
There also hasn’t been any work that has trained a mid-sized
network from a larger and deeper network from scratch. We
study some idiosyncratic properties for some novel configurations of mentee networks. We argue that such mentoring
avoids weight explosion. Even while using smaller minibatches, mentee networks get ample supervision and are
capable of stable learning even at high learning rates. We
show that mentee networks produce a better generalization
performance than an independently learnt baseline network.
We also show that mentee networks are better transferable
than the independently learnt baselines and are also a good
initializer. We also show that mentee networks can learn
good representations from very little data and sometimes
even without supervision from a dataset in an unsupervised
fashion.
The rest of the paper is organized as follows: section 2
discusses related works, section 3 details the mentored

learning, section 4 discusses designs for experiments, section 5 produces results and section 6 provides concluding
remarks.

2. Related Works
Hinton et al., tried to make networks portable by learning
the softmax outputs of a larger well-trained network along
with the label costs [11]. This was previously explored using logits by Caruana et al., [1, 4]. By directly learning
the softmax layers, they were forcing the softmax layer of
a smaller network to mimic the same mapping as that of a
larger network onto the label space. In a way they tried to
learn a better second and third guesses. They called this
dark knowledge, as the knowledge so learnt is only available to the larger network. By attempting to learn the softmax layer, they were able to transfer or distil knowledge
between the two networks. The drawback of this work is
that it only works as long as the larger network is already
well-trained and stable. They relied upon the network’s
predictive softmax layer being learnt perfectly on the target
dataset and propagate that knowledge. This also assumes
that there are relationships between classes to be exploited.
While this may work in cases where this is true, such as
in character recognition or in voice recognition, it doesn’t
work in most object detection datasets where the relationship between classes is not a given in terms of its appearance features1 . They also distil only the softmax labels and
not the representational space itself. This also requires that
the smaller network is capable of training in a stable manner.
Dark knowledge is extended upon by several previous
works [23, 19, 2, 5]. One extension of this work that
we generalize in this article is using layer-wise knowledge
transfer for one layer in the middle of the network. This
was used to show that thinner and deeper network can be
trained with better regularization [19]. Another method
uses a similar one-layer regularizer as knowledge transfer
between a RNN and a CNN [5]. Mentored training has also
been shown to be extremely useful when training LSTMs
and RNNs with an independent mentor supervision [23].
All these methods discussed above are essentially the
same technique as the dark-knowledge method extended beyond just the softmax layer. All of these methods have fixed
one-layer regularizations and although trivial, we generalize
this for many layers. Their mentee networks are typically
much deeper and complex than their mentors and they use
these as a means to build more complex models (albeit thinner as in the case of FitNets [19]). There has been no study
to the best of our knowledge that builds less complex (both
thinner and shallower) models with the same capability as
larger models. Also, neither has there been a study that stud1 We tried this approach on Caltech101 and couldn’t get reliable results.

ies various properties of these networks nor those that show
the transferability and generality of these networks.

3. Generalized mentored learning
Let us first generalize all of the methods that use this
knowledge transfer as follows: Consider a large mentor network with n layers Mn . Suppose we represent the k th neuron activations of the ith layer in the network as Mn (i, k).
Consider a smaller mentee network with m2 layers Sm .
Suppose that M is already well-trained and stable on a general enough dataset D. Now consider that we are using S
to learn classification3 on a newer dataset d1 which is less
general and much smaller than D as determined a priori.
Although this is not a constraint, having a smaller and less
general dataset emphasizes the core need where such mentored learning is most useful.
∀ l ≤ n and j ≤ m, we can define a probe as an error measure between Mn (l) and Sm (j). This error can be
modelled as an RMSE error as follows,
v
u a
u1 X
(Mn (l, i) − Sn (j, i))2 ,
(1)
Ψ(l, j) = t
a i=0
where a is the minimum number of neurons between
Mn (l, .) and S(j, .). If the neurons were convolutional, we
consider element-wise errors between filters. By adding this
cost to the label cost of the network S during back propagation, we learn not just a discriminative enough representation for the samples and labels of d1 , but also for layers
j in a pre-determined set of layers, a representation closer
to the one produced by M. Some implementations of such
loses in literature tend to learn a regressor instead of simply adding the loss, but we concluded from our experiments
that the computational requirements of such regressors do
not justify their contributions. Adding a regressor would
involve embedding the activations of the mentor and the
mentee onto a common space and minimizing the distances
between those embeddings. We quite simply circumvent
that and consider the minimum number of matching neurons. This enables us to have a slimmer, fatter or same
sized mentee. Suppose db1 is the bth mini-batch of data from
the dataset d1 and suppose we have a pre-determined set of
probes B, which is a set of tuples of layers from (M, S).
The overall network cost is,
e = αt Ls (db1 ) + βt

X

Ψ(l, j) + γt Ψ(n, m),

(2)

∀(l,j)∈B
2 Although we adhere strictly to m < n, without losing any generality,
we could have any m or n. In fact m > n with only one probe would be
the special case of FitNets [19].
3 Although we only consider the task of classification, the methods proposed are applicable to many forms of learning.

where L(.)s is the network loss of that mini-batch, αt and βt
weighs the two losses together to enable balanced training
and γt is the weight of the probe between the two (temperature) softmax layers. αt = gα (t), βt = gβ (t) and
γt = gγ (t) are annealing functions parametrized by the
iteration t under progress. Although most methods in the
literature use constants for αt , βt and γt , we found it preferable to retain gα (t) = 1, ∀t throughout and anneal β and γ
linearly. We discuss the value and the need for these parameters in detail further.
Since M is pre-trained and stable, the second and third
terms of equation 2 are penalties for the activations of those
layers in S not resembling the activations of the probed
layer from M respectively. These losses as defined by equation 1 are functions of the weights of those layers from S
only. They restrict the weights within a proximity or region, that produces activations that are known for the mentor to be better activations. This restricting behaviour acts
as a guided regularization process, allowing the weights to
explore in a direction that the mentor thinks is a good direction, while still not letting the gradients to explode or
vanish.
For a particular weight w ∈ S at any layer, a typical
update rule without the probe is,
wt+1 = wt − η

∂
Ls ,
∂w

(3)

where t is some iteration number, η is the learning rate and
assuming αt = 1, ∀t. The update rule with mentored probes
is,
h ∂
wt+1 = wt − η αt
Ls +
∂w
i
X ∂
∂
βt
Ψ(l, j) + γt
Ψ(n, m) . (4)
∂w
∂w
∀(l,j)∈B

The last two terms add a guided version of a noise that decreases with each iteration. While at earlier stages of training, this allows the weights to explore the space, it also
restricts the weights from exploding because the direction
that the weights are allowed to explore is controlled by the
mentor. The freedom to explore tightens up as the as learning proceeds, provided gβ (t) is a monotonically annealing
function with respect to t. Note that even though to calculate these error gradients we need one forward propagation
through M, we do not back propagate through M. This is
a penalty on the weights, even though we are using the activations to penalize the weights indirectly. Although mentee
networks can be further regularized with l2 , l1 , dropouts and
batch normalizations, it is recommended that the mentee
networks imposes additional regularizations mirroring the
mentor networks for better learning.

Obedient Mentee
0.04

Mentoring Phase

Adamant Mentee

Self-study Phase

0.03

0.04
0.03

α*η

0.02

α*η

0.02

β*η

β*η

γ*η

0.01
0
0

η

50

100
Epochs

γ*η

0.01

150

0
0

η

50

100

150

Epochs

Figure 2. Annealing α, β and η while learning for an obedient and
an adamant network.

Different configurations of mentee networks
Different combinations of α, β and γ produces different
characteristics of mentee networks. Equation 4 can be seen
as learning with three different learning rates, α ∗ η, β ∗ η
and γ ∗ η. We can simulate using these three parameters,
two idiosyncratic personalities of mentee networks: an obedient network and an adamant network. An obedient network is a network that focuses on learning the representation more than the label costs at the beginning stages and
once a good representation is learnt, it focuses on learning
the label space. It tends towards being over-regularized and
its regularization relaxes with epochs. An adamant network
is a network that focuses almost immediately on the labels
as much as learning the representation, but its focus is positively towards learning the label only. The learning rates of
these personalities are shown in figure 2.
An independent network can be considered as a special
case of the adamant network where probe weights are ignored (β = 0, γ = 0, ∀t). The other extreme case of an
obedient network is perhaps a gullible network that learns
just the embedding space of the mentor. Gullible networks
are also a good way to initialize a network in an unsupervised mentoring fashion. Consider a dataset d2 , that does
not have any labels. Neither the mentor nor the mentee
could potentially learn any discriminative features. Using
just the probes we could build an error function that could
make the smaller mentee network still learn a good representation for the dataset. We use the information from
the parent network to learn a good representation for d2
by simply back propagating the second term of equation 2
alone. These gullible mentees come in really handy when
the dataset has considerably less samples to be supervised
with. Unsupervised mentoring is also an aggressive way
to initialize a network and is often helpful in learning large
networks in a stable manner with a stable initialization.
Typically the deeper one goes, the more difficult it becomes to learn the activations and the costs saturate quickly.
The softmax layer is the most difficult to learn. To our surprise we find that probe costs converge much sooner than

the label costs, leading us to believe that the representations
being mentored are indeed relevant as long as the datasets
share common characteristics. There is a plethora of such
configurations that could be tried and many unique characteristics discovered. In this article we limit ourselves to only
those that enable us stability during learning and focus on
those that help us with better generalizations.
For learning large networks we prefer the use of obedient networks as obedient networks are heavily regularized at
the beginning leading to careful initialization and stabilization of the network before learning of labels takes over. We
call the stabilization phase as the mentoring phase and the
rest, self-study phase. During the mentoring phase learning
is slow but steady. In most cases, α ∗ η is an increasing
function due to the aggressive climb of α. The annealing
of these rates for a typical obedient mentee and an adamant
mentee are shown in figure 2. We also find that typically the
later layers are more stubborn in being mentored than earlier layers. Although this is typically to be expected, more
obedience may be enforced by choosing higher β values for
layers that are deeper in the network.

4. Design of experiments
We evaluate the effectiveness of mentorship through the
following experiment designs:

4.1. Effectiveness
To demonstrate the effectiveness of learning, we first
train a larger network on a dataset. Using this network as
a mentor, we train the mentee network on the same dataset.
Unlike those in literature, we choose mentee networks that
are generally much smaller than the mentor. We show that
this generalizes at least as well as an independent network
of the exact same architecture regularized not by mentor,
but by batch normalization, l2 and l1 norms and dropouts.
Training mid-sized networks on small datasets are often difficult. To our best knowledge we have provided our best
effort in meticulously learning all the networks. For learning an independent network often we spent additional effort
in adjusting the learning rates at the opportune moments.
We show that mentee networks outperforms the independent networks and even at the worst case performs as well
as the independent networks.

4.2. Generality of the learnt representations
To demonstrate that the network learns a more general
representation, we gather a pair of datasets of seemingly
similar characteristics with one more general or larger than
the other. We train the mentor with the more general dataset
first and then fine tune it on the less general dataset. We
then train both the independent and the mentee nets on the
less general dataset and demonstrate again that at worst the
mentee net performs the same as the independent net.

0.02

Conifguration For Learning VGG-19 using Caltech 101

0.015

Fine Tuning Begins Here

α*η

0.01

β*η
γ*η

0.005
0
0

η

10

20

30

40

50

60

70

80

90

100

Epochs

Figure 3. Annealing α, β and η while learning VGG-19 space for
Caltech-101. We used an obedient network.

We then proceed to fine tune the classifier layer of both
the mentee net and the independent net using the more general dataset but since the other layers are not allowed to
change, the mentee net does not have any additional supervision. This tests the quality of the features learnt by
these networks on a more general and more difficult dataset.
For the sake of our experiments we consider the pairing of (Cifar-10 - Cifar-100) and (Caltech-101 - Caltech256) [14, 10]. We assume that Cifar-100 is more general
than Cifar-10 and Caltech-256 is more general than Caltech101.
Additionally, we conduct another experiment where we
try to learn from a mentor network trained with the full
MNIST dataset, a mentee network that only has supervision from a part of the dataset [15]. The independent network also in this case, learns with the same redacted dataset.
We redact the dataset by only having p samples for each
class in the dataset where p ∈ {500, 250, 100, 50, 10, 1}.
p = 1 is essentially an ambitious goal of 1-shot learning
from scratch using a deep network. We also try this with a
mentee network that is initialized by unsupervised mentoring from the same mentor network. We acknowledge that
the comparison with unsupervised mentoring is unfair because the mentee net is initialized by the mentor with information filtered from data that is unavailable for the independent network. The latter results are to demonstrate that unsupervised mentoring could learn an effective feature space
even without labels and with very less samples.

4.3. Learning the VGG-19 representation
In particular, while learning classification on the Caltech101 dataset, we try to learn the same representation as
the popularly used VGG-19 network at various levels of the
network hierarchy [21]. VGG-19 network’s 4096 dimensional representation is one of the most coveted and iconic
image features in computer vision at the time of the writing
of this article. The VGG-19 network has 16 convolutional
layers and 2 fully-connected layers the last of which produces the 4096 dimensions of features upon which many

other works have been built.
We try to learn the same 4096 dimensional representation of the VGG-19 network using ambitiously less number
of layers. For the (Caltech-101-Caltech256) dataset pairs
in all our experiments, there is no explicit mentor network
that we learnt. We simply set gγ (t) = 0, ∀t and learnt with
probes without retraining the VGG-19 network. In a way
we are attempting to learn VGG-19’s view of the Caltech101 dataset and are probing into the representational frame
of the VGG-19 network. We used a relatively obedient student as shown in figure 3 for this case.

4.4. Implementation details
The independent networks were all regularized with a
l1 and l2 penalties with a weight of 1e−4 , which seems
to give the best results. On all networks we also applied
parametrized batch norm for both fully connected and convolutional layers and dropouts with rate of p = 0.5 for the
fully connected layers [12, 22]. We find that dropout and
bath norm together help in avoiding over-fitting. All our activation functions were rectified linear units [16]. For learning the mentee network we start with learning rates as high
as 0.5, for the larger independent networks we are forced a
learning rate of 0.001, while for the smaller experiments we
were able to go as high as 0.01, since the batch sizes were
larger. During training, if ever we ran into exploding gradients or NaNs, we reduce the learning rate by ten times, reset
the parameters back to one epoch ago and continue training.
We train until 75 epochs after which we reduce the learning
rate by a hundred times and continue fine-tuning until early
stopping. Unless early stopped, we train for 150 epochs.
All our initializations were from a 0-mean Gaussian distribution, except the biases which were initialized with zeros.
The experiment set-up was designed using Theano v0.8
and the programs were written by ourselves4 [3]. The experiments with MNIST datasets were conducted on a Nvidia
GT 750M GPU, the others on an Nvidia Tesla K40c GPU,
with cuDNN 3007 and Nvidia CUDA v7.5. The minibatch sizes for all the MNIST and cifar experiments were
500 (unless forced by small dataset size in which case we
performed batch descent instead of the usual stochastic descent). The mini-batch sizes for all Caltech experiments
were 36, with images resized to 224X224 so as to the fit the
VGG-19 requirement. Apart from normalization and meansubtraction, no other pre-processing were applied to any of
the images. For the Caltech experiments we used Adagrad
with Polyak’s momentum [18, 9]. For the experiments that
were smaller networks we used RMSprop with Nesterov’s
accelerated gradient [6, 17].
It is to be noted that we chose to use vanilla networks
that are as simple as possible so as to enable us to compare
against a baseline which is also vanilla. Since our aim is
4 Code

is available at our GitHub page.

a) VGG-19 M entor

c) Obedient M entee

b) Gullible M entee

d) Adamant M entee

Figure 6. VGG-19 first layer filters and filters probed using Caltech101 for a Gullible, Obedient and an Adamant mentee after
only one epoch of training. We recommend viewing this image on
a computer monitor.

not to achieve state-of-the-art accuracies on any datasets,
we didn’t implement several techniques that are commonly
applied to boost the network performances in modern day
computer vision. The purpose of these experiments is to
unequivocally demonstrate that among networks that learn
from scratch, one that is mentored can perform better and
learn more general features than one that is not.

5. Results
The results are split across two tables based on the network architectures. The smaller experiments on a 5 layer
network are shown in figure 4 and the larger ones in figure 5. The → symbol shows which layers are probed and
from where.
In figure 4, the results clearly demonstrate the strong performance of the mentee networks over the independent networks. In the cifar experiments we under-weighted γ purposely as we didn’t want to propagate the 20% of error from
the mentor network on to the mentee network. The results
on Cifar 10 from scratch seem to indicate that both networks
have reached the best possible performance for that architecture. We believe with the amount of supervision already
provided from the 40,000 training images, mentoring is not
as effective. When there is already ample supervision, men-

toring is ineffective, or rather unwanted, albeit it doesn’t
hurt. While fine-tuning on cifar 100, we find that there are
great gains to be made.
We find a similar trend with the MNIST experiments
also. The less data there is, the higher the gain of the mentee
networks. Note that even though mentee networks are regularized, care was taken to ensure that they both go through
the exact same number of iterations at the exact same learning rate. We also found that unsupervised mentoring always
keeps the learning at a very high standard although as was
discussed in section 4.2 there was additional supervision on
the entire dataset from the unsupervised mentoring, which
is unfair.
In the experiments with the Caltech101 datasets, we find
that the mentee networks perform better than the vanilla network. The mentee network was also able to perform significantly better than the independent network when only
the classifier/mlp sections were allowed to learn the Caltech256 dataset with representation learnt from Caltech101.
This proves the generality of the feature space learnt. With
an even obedient student, we were able to learn the feature space of the VGG-19 network to a remarkable degree.
While with the first convolutional layer we were able to
learn to a minimum rmse or 0.0023 from 6.54 at random.
With the last two layers we were able to learn upto a rmse
of 2.04 from 12.76 at random.
Figure 6 shows the filters learnt after one epoch for a
gullible network, an obedient network and an adamant network. All these networks were initialized with same random values at their inception. We can easily notice that
the gullible network already sway towards the VGG-19 filters. In obedient mentee, we notice that most corner detector features are already swaying towards the mentee network but more complex features are not swaying as much as
the gullible network. To our surprise we notice that even in
an adamant network corner detectors are swaying towards
VGG-19. This shows that even with low weights, the first
layer features are learning the VGG-19’s representation. It
is to be noted that we are not learning the weights directly,
but are learning the activations produced by the VGG-19
network for the Caltech101 dataset that leads us to learn the
same filters as the VGG-19. This implies that corner features are more general among the Imagenet dataset, which
VGG-19 was trained on, and the Caltech101 dataset, which
explains why they are learnt earlier than others.

6. Conclusions
While the use of large pre-trained networks will continue
to remain popular, because of the ease in just copying a network and fine-tuning the last layers, we believe that there
is still a need for learning small and mid-sized networks
from scratch. We also recognize the difficulty involved in
reliably training deep networks with very few data sam-

Network

Architecture

Mentor

Independent

Kernel Size: 5
Neurons: 20

Kernel Size: 5
Neurons: 20

Convolutional Layers
Activation: ReLU
Stride: 1
Max Pooling: 2
Fully Connected Layers
Activation: ReLU
Dropout Input Rate: 0.5

Neurons: 800

Neurons: 800

Neurons: 800

Neurons: 800

Neurons: 800

Neurons: 800

Output Layer

Neurons: 10/100

Neurons: 10/100

Neurons: 10/100

Trained from scratch
Cifar 10

79.36 %

68.5 %

68.58 %

Fine-tuned last layer only
Cifar 100

41.21%

33.2 %

26.67%

Kernel Size: 3
Neurons: 50

97.73%

MNIST - 500

unsupervised mentoring

98.2%

97.47 %

MNIST-250
Accuracies

Mentee

Kernel Size: 5
Neurons: 20

unsupervised mentoring

97.88%

97.42%

MNIST-100
MNIST
99.59%
MNIST-50

MNIST-10

MNIST - 1

unsupervised mentoring

96.01%

92.95%
unsupervised mentoring

96.80%

78.5 %
unsupervised mentoring

96.7%

48.5%
unsupervised mentoring

96.7%

97.71%

96.89 %

95.12%

90.96%

75.3%

41.5%

Figure 4. Architecture and results for the experiments with CIFAR and MNIST datasets.

ples. One way to meet the best of both worlds is by using
a mentored learning approach. In our study, we find that a
shallower mentee network was able to learn a new representation from scratch while being regularized by the mentor network’s activations for the same input samples. We
found that such mentoring provided much stabler training
even at higher learning rates. We noted some special cases
of these networks and recognize some idiosyncratic personalities. We extended one of these to be able to perform as an
unsupervised initialization technique. We showed through
compelling experiments, the strong performance and generality of mentor networks.

References

[3] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow, A. Bergeron, N. Bouchard, and Y. Bengio. Theano:
new features and speed improvements. Deep Learning and
Unsupervised Feature Learning NIPS 2012 Workshop, 2012.
5
[4] C. Bucilu, R. Caruana, and A. Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,
pages 535–541. ACM, 2006. 2
[5] W. Chan, N. R. Ke, and I. Lane. Transferring knowledge
from a rnn to a dnn. arXiv preprint arXiv:1504.01483, 2015.
2
[6] Y. N. Dauphin, H. de Vries, J. Chung, and Y. Bengio. Rmsprop and equilibrated adaptive learning rates for non-convex
optimization. arXiv preprint arXiv:1502.04390, 2015. 5

[1] J. Ba and R. Caruana. Do deep nets really need to be deep?
In Advances in neural information processing systems, pages
2654–2662, 2014. 2

[7] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and
A. Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303–
338, 2010. 1

[2] A. K. Balan, V. Rathod, K. P. Murphy, and M. Welling.
Bayesian dark knowledge. In Advances in Neural Information Processing Systems, pages 3420–3428, 2015. 2

[8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic
segmentation. In Computer Vision and Pattern Recognition

Network

Mentor (VGG-19)

Mentee

Independent

Neurons: 64
Max Pooling: 2

Neurons: 64
Max Pooling: 2

Neurons: 128
Max Pooling: 2

Neurons: 128
Max Pooling: 2

Neurons: 256
Max Pooling: 2

Neurons: 256
Max Pooling: 2

Neurons: 512
Max Pooling: 2

Neurons: 512
Max Pooling: 2

Neurons: 512
Max Pooling: 2

Neurons: 512
Max Pooling: 2

Neurons: 512
Max Pooling: 2

Neurons: 512
Max Pooling: 2

Neurons: 4096

Neurons: 4096

Neurons: 4096

Neurons: 4096

Neurons: 4096

Neurons: 4096

Neurons: 102/256

Neurons: 102/256

Neurons: 64
Max Pooling: 1
Neurons: 64
Max Pooling: 2
Neurons: 128
Max Pooling: 1
Neurons: 128
Max Pooling: 2
Neurons: 256
Max Pooling: 1
Neurons: 256
Max Pooling: 1
Neurons: 256
Max Pooling: 1
Convolutional Layers
Stride: 1
Kernel Size: 3
Activation ReLU
Architecture

Neurons: 256
Max Pooling: 2
Neurons: 512
Max Pooling: 1
Neurons: 512
Max Pooling: 1
Neurons: 512
Max Pooling: 1
Neurons: 512
Max Pooling: 2
Neurons: 512
Max Pooling: 1
Neurons: 512
Max Pooling: 1
Neurons: 512
Max Pooling: 1
Neurons: 512
Max Pooling: 2

Fully Connected Layers
Activation: ReLU
Dropout Input Rate: 0.5
Softmax Layer

Accuracies

Trained from scratch on
Caltech 101

N/A

56.16%

45.46%

Fine-tuned last layer only
for Caltech 256

N/A

66.12 %

55.45%

Figure 5. Architecture and results for the experiments with Caltech datasets.

(CVPR), 2014 IEEE Conference on, pages 580–587. IEEE,
2014. 1
[9] S. Green, S. I. Wang, D. M. Cer, and C. D. Manning. Fast and

adaptive online training of feature-rich translation models. In
ACL (1), pages 311–321, 2013. 5
[10] G. Griffin, A. Holub, and P. Perona. Caltech-256 object cat-

egory dataset. 2007. 5
[11] G. Hinton, O. Vinyals, and J. Dean. Dark knowledge. Presented as the keynote in BayLearn, 2014. 2
[12] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
arXiv preprint arXiv:1502.03167, 2015. 2, 5
[13] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. In Proceedings of
the ACM International Conference on Multimedia, pages
675–678. ACM, 2014. 1
[14] A. Krizhevsky and G. Hinton. Learning multiple layers of
features from tiny images, 2009. 5
[15] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. 5
[16] V. Nair and G. E. Hinton. Rectified linear units improve
restricted boltzmann machines. In Proceedings of the 27th
International Conference on Machine Learning (ICML-10),
pages 807–814, 2010. 5
[17] Y. Nesterov. A method of solving a convex programming
problem with convergence rate o (1/k2). In Soviet Mathematics Doklady, volume 27, pages 372–376, 1983. 5
[18] B. T. Polyak. Some methods of speeding up the convergence
of iteration methods. USSR Computational Mathematics and
Mathematical Physics, 4(5):1–17, 1964. 5
[19] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta,
and Y. Bengio. Fitnets: Hints for thin deep nets. arXiv
preprint arXiv:1412.6550, 2014. 2, 3
[20] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer
Vision (IJCV), pages 1–42, April 2015. 1
[21] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014. 1, 5
[22] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: A simple way to prevent neural
networks from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958, 2014. 5
[23] D. Wang, C. Liu, Z. Tang, Z. Zhang, and M. Zhao. Recurrent
neural network training with dark knowledge transfer. arXiv
preprint arXiv:1505.04630, 2015. 2

Sim ultaneous Tracking and V erication via Sequen tial P osterior
Estimation 
Baoxin Li Rama Chellappa
Center for Automation Researc h
University of Maryland
College Park, MD 20742
fbaoxin,ramag@cfar.umd.edu

Abstract

An approach to simultaneous tracking and verication in video data is presented. The approach is
based on posterior estimation using sequential Monte
Carlo methods. Visual tracking, which is in essence
a temporal correspondence problem, is solved through
probability density propagation, with the density being
dened over a proper state space characterizing the object conguration. Verication is realized through hypothesis testing using the estimated posterior density.
In its most basic form, verication can be performed
as follows. Given measurement Z and two hypothesis
H1 and H0, we rst estimate posterior probabilities
P(H0jZ) and P (H1jZ); and choose the one with the
larger posterior probability as the true hypothesis.
Applications of the approach are illustrated with experiments devised to evaluate the performance. The
idea is rst tested on synthetic data, and then experiments with real video sequences are presented.

1 Introduction

Programming a computer to recognize objects is a
dicult problem, and has been a research topic for
many years. Recently, there has been increasing interests in integrating temporal information available
in video for improved recognition performance. However, in most cases, the temporal information is exploited only in tracking an object, with less emphasis on using temporal information for recognition. In
its most crude form, temporal information can be exploited through voting, i.e, recognition is done on each
frame, then a vote is taken to give the nal decision.
While this ma y be helpful in some cases, there is much
information being left out in this crude approach such
as the coherence in the shape change of an object in
temporally consecutive frames.
 Prepared

through collaborative participation in the Advanced
Sensors Consortium (ASC) sponsored by the U.S. Army Research
Laboratory under the Federated Laboratory Program, Cooperative
Agreement DAAL01-96-2-0001.

1063-6919/00 $10.00 ã 2000 IEEE

With video data, recognition often becomes a v erication problem. That is to say, an algorithm is needed
to answer the question: is this the object seen in previous time? Or, is this the object I was asked to look
for? And often, there are only a small number of
candidates to verify against, with the candidates being templates obtained in the video from an earlier
time. The problem can be better illustrated by the
applications of monitoring a vehicle entering and then
leaving a parking lot, or a person entering and then
leaving, for example, a bank. In this scenario, tracking is needed rst, then the algorithm needs to do
incremental verication while maintaining a track on
the object. Obviously, in these applications, temporal
information from video, if properly exploited, would
facilitate recognition.
In this paper, an approach to simultaneous object
tracking and verication is proposed. The approach
is based on posterior probability density estimation
through sequential Monte Carlo methods. Tracking,
which is in essence a temporal correspondence problem, is formulated as a probability density propagation problem, with the density being dened over a
proper state space characterizing the object conguration. Verication is realized through hypothesis
testing using the estimated posterior probability densities.
The paper is organized as follows. Section 2 gives
a brief introduction to sequential Monte Carlo methods. Section 3 proposes the approach to simultaneous tracking and verication. Applications of the approach are illustrated with experiments in Section 4.
Section 5 relates the approach to other work. W e conclude in Section 6.

2 Sequential Mon te Carlo Methods for
Dynamic Systems

A dynamic system modeled b y a state-space model,
can be eectively analyzed using a Kalman lter. In
fact, when the system is linear and Gaussian, the
Kalman lter gives optimal results. Methods have

also been proposed for non-linear/non-Gaussian systems. For example, the extended Kalman lter [1] is
a popular tool for handling non-linear systems. In recent years, Monte Carlo methods have been proposed
for the analysis of dynamic systems (e.g. [5, 12]).
The analysis of a dynamic system can be form ulated
as the evaluation of the conditional density p(Xt jZt ),
where Xt is the system state and Zt the observation,
and t indicates (in applications) discrete time. In the
Monte Carlo methods, at time t, the dynamic density is approximated by a set of its samples, with
proper weights. This is basically importance sampling
[8] used in a sequential or recursive fashion. Applications can also be found, for example, in target position
tracking [7], Bayesian missing data problem [13], and
contour tracking [11].
We now give a brief description of this approach.
Following [14], one rst characterizes a probabilistic
dynamic system as a sequence of evolving probability distributions t(Xt ), indexed by discrete time t,
where Xt is the state variable at time t. The following denition is needed before introducing importance
sampling:
Denition[14] A random sample X drawn from
a distribution g is said to be properly weighted by a
weighting function w() with respect to the distribution () if for any integrable function h,
Eg fh(X)w(X)g = E fh(X)g:
A set of random draws and weights (x(j ); w(j )); j =
1; 2; :::, is said to be properly weighted with respect to
 if
Pm h(x(j ))w(j )
Pm w(j ) = E fh(X)g
(1)
lim j =1
m!1
j =1
for any integrable function h. In a practical sense we
can think of  as being approximated by the discrete
distribution supported on the x(j ) with probabilities
proportional to the weights w(j ).
A sequential importance sampling (SIS) algorithm
is then designed as follows, summarizing the aforementioned methods by setting t() = p(Xt jZt ). For
a complete treatment, see [14].

SIS Algorithm
Let St = fXt(j ) ; j = 1; :::; mg denote a set of random

draws that are properly weighted by the set of weights
Wt = fwt(j ); j = 1; :::; mg with respect to t. At each
time step t,
step 1. Draw Xt+1 = x(tj+1) from gt+1(xt+1jx(tt));
step 2. Compute
t+1(x(tj+1) )
(
j
)
j ) = u(j ) w(j ) :
ut+1 =
and wt(+1
t+1 t
(
j
)
(
j
)
(
j
)
t(xt )gt+1 (xt+1jxt )
j ) ) is a properly weighted sample of
Then (x(tj+1) ; wt(+1
t+1.

1063-6919/00 $10.00 ã 2000 IEEE

When a sequence of observ ations induced by a dynamic object X is available, such as a sequence of a
moving person, the posterior estimation can be solved
in a sequential way using the SIS algorithms. With
a state-space model, the weight can also be induced
from the likelihood of the state p(Zt jXt(j ) ), which incorporates the current observation, as in [12, 11].

3 Simultaneous Tracking and Identication via Posterior Estimation

With the SIS algorithm, trac king is immediately
solved by setting the state X to some parametrization of the object. Assuming tracking is done, we now
consider verication. In Section 3.1 we rst put the
verication problem into a stochastic setting. Section
3.2 introduces a reparametrization which leads to a
dynamic system of lower dimensionality. Given in Section 3.3 is the algorithm for simultaneous tracking and
verication based on the SIS method.

3.1 Verication via Posterior Probability

Assume that there are C classes f!1;   ; !C g (e.g,
C dierent people). Let X be a parametrization of
the object, which can be, for example, the intensity
image of the object (viewed as a vector). In general,
X is a random vector governed by the a prior density p(X j!i ). Given an observation Z, the Bayesian
maximum-a-posterior-probability rule chooses ! =
maxi fP(!ijZ)g as the solution of the verication
problem, where P(!ijZ) is the posterior probability
of class !i given the observation Z. P(!ijZ) can be
computed through integrating the posterior density as
Z
P (!ijZ) = pi (X jZ)dX
(2)
A

where pi (X jZ) is the posterior density of class !i , with
A being some proper region. Therefore, the verication problem can be solved best (in the Bayesian
sense) if the posterior pi (X jZ) is rst estimated. Let
hypothesis Hi denote the event \class !i causes the
observation Z"; we then use P(!ijZ) and P(HijZ)
interchangeably.
Unfortunately, when X is high dimensional (for example, an image vector), posterior estimation through
empirical approaches is not realistic, even with SIS
methods. Given limited observation data, the estimates would be inaccurate at best, and meaningless
at worst. If, however, the object can be characterized
by a vector of low dimensionality, then the SIS method
would be an eective tool for posterior estimation.

3.2 A Re-parametrization

Consider a rigid object subject to motion which
can be modeled by a transformation f parametrized
by a parameter vector . Let X0 denote an original

parametrization of the object. X0 can be, for example, a face template (intensity image), a set of intensity discontinuity points (edge map), or the parametric contour of an object. Let X = f(; X0 ) denote
the transformation of X0 into X. Under small and
continuous motion assumption, X would be similar to
X0 , meaning  has only a \small" dierence from 0 ,
with 0 being the parameter for the identity transform: X0 = f(0 ; X0 ). Expanding X = f(; X0 ) at 0
gives
X = f(; X0 )
= f(0 ; X0 ) + J (0 )( ; 0 ) + o() (3)
 X0 + J (0 )( ; 0 )
where o() denotes higher order terms, and J () is the
Jacobian matrix with respect to . Above expansion
shows that the transformed object X can be viewed
as the original X0 plus a changing term caused by
a  def
=  ; 0 . From a practical point of view,
only the dierence is important { knowing the difference, temporal correspondence is solved. Therefore, given X0 , the vector  is a good parametrization of all possible X under small motion assumption. Thus we propose to use  as the state vector. The Jacobian matrix J (0 ) is easy to obtain
for 2-D ane or simpler transformations. For example, let X0 = (x0; y0 ) be the location of one edge
point. For a 2-D ane transformation f(; ) with
 def
= (a11; a21; a21; a22; Tx; T
to say
y )T (equivalent

a11 a12
Tx
f(; ) = a21 a22 () + Ty ), the Jacobian
matrix is computed as


@X
x
y
0
0
1
0
0
0
J (0 ) = j @ j0 = 0 0 x0 y0 0 1 (4)
From now on, whenever X is used, it refers to .
That is to say, the dynamic system under consideration will be the one governing the evolution of .
It is worth pointing out that (3) is valid for general
transformations other than 2-D ane transformation;
also, since 2-D ane is linear in each component of ,
the higher order terms in (3) are zero, implying that
the parametrization with  is accurate for a 2-D planar object even under large motion (same is true for
2-D translation and Euclidean similarity groups. To
achieve linearity on the parameters in Euclidean similarity group, we map scale s and rotation  into two
new parameters a = s  cos, b = s  sin).

3.3 The Algorithm

An algorithm for simultaneous tracking and verication is given as follows.

Algorithm
Initialization: Rectify the templates onto the rst
frame of a sequence.

1063-6919/00 $10.00 ã 2000 IEEE

Tracking:

Initialization: draw Ns random samples from 0(X0 ).
Updating: at time t > 0, invoke the SIS algorithm to

obtain an updated set of samples for t(Xt ).

Verication:

At time t > 0, evaluate the mean value E (Xt ) of
Xt according to (1) (dropping the limiting operation);
Compute the posterior probability P (HijZt) according to (2), with A being a hypercube around
E (Xt ): A = [E (Xt ) ; ; E (Xt ) + ];
Choose as true the hypothesis Hj giving the maximum probability (or sum of probabilities up to t).
As shown above, the algorithms is initialized by rectifying the templates to the rst frame of a sequence.
This step corresponds to the object detection process,
which will not be considered in this paper. The rectication refers to registering the template to the scene,
which is usually done approximately by the detection
process. With the abo ve algorithm, this registration
need not be very accurate, since in the second step,
random samples will be drawn around the initial point,
and will capture the real value with a probability depending on the variance 0 of the population and the
sample size Ns . In theory, this probability goes to 1
with large 0 and Ns .
When the state v ector X is some parametrization
of the underlying object shape or appearance, it is not
hard to understand the rationale behind the verication step in the above algorithm. It remains to be
explained why we can still do verication using the
algorithm when X is now . The argument is that
given the observation history (Zt ), the true hypothesis would generate a density of higher peak and more
concentrated shape than a false hypothesis would do,
since the transformed object would be \close" to the
original one under small motion assumption. In ideal
(deterministic) situation the measurement of the object at time t can be related to its model (true hypothesis) by a unique Xt , while no transform can relate a
false hypothesis (a model dierent from the right one)
to the measurement without incurring a large matching error.
From a Bayesian point of view, recall that
p(X jZ) = p(Z jX)p(X)=p(Z)
where p(Z) is the same for all the hypotheses and thus
has no eect on verication, and p(X) is a prior density of X. p(X) is usually assumed to be unimodal
such as a Gaussian. p(Z jX), which should be treated
as a function of X when Z is given, re
ects the likelihood of the event \the observation Z being incurred by
X". Without considering occlusion, in general p(Z jX)
would peak at the true X which causes the observation, and decrease when X deviates from the true

value. It is reasonable to assume that p(Z jX) is unimodal when X re
ects a small change with respect
to certain X0 1. This is especially true in a verication problem where the templates are usually obtained
from earlier frames of a sequence. The single subject
assumption is automatically satised through tracking: only a local region needs to be considered and
within this region the assumption of a single subject
is reasonable.
In summary, with the above assumptions, p(X jZ)
would be unimodal with the peak at the true value of
X. This is the basis for the verication step. Later
in the experiments, it will be observed that p(Z jX)
is indeed shaped as predicted. It will also be shown
that when the single object assumption is violated, the
verication could fail temporarily (e.g. Fig. 9).
In the verication algorithm, a threshold  needs to
be specied to dene the integral region A. To avoid
choosing an ad hoc , one could compute the (e.g.
95%) condence interval around the mean, and then
test the hypotheses based on the length of the interval
in each dimension. However, we will use the probability rather than the length of condence interval since
the former is more intuitive.
Notice that in the above algorithm, multiple hypotheses are kept during tracking and verication process, meaning several dynamic systems are main tained
simultaneously. This is natural in applications { the
actual scenario could be either of the following: the
system is to identify a given object (template) from
multiple objects in the view (such as catching a person who entered the bank a moment ago based on a
sequence containing multiple persons); or the system
is to identify a single object in the scene as one of
several possible candidates (templates) (such as classifying the person caught in a sequence as one of the
candidates). Our experiments in the next section will
concentrate on the latter case. Maintaining multiple
systems increases the computational complexit y. Fortunately, the systems can be processed in parallel.

4 Applications

In this section, we use experiments to illustrate applications of the proposed algorithm. W e rst test
the algorithm using synthetic data, since synthetic
data allows us to compare estimates with the true values. Applications to vehicle and human face tracking
and verication are then demonstrated with real video
data. The sample size Ns is 200 for all the experiments
in this paper.
1 This is also the advantage of using the re-parametrization.
Otherwise, if only from the dimensionality reduction point of
view, one could also use, for example, the six parameters of the
2-D ane group as the state X .

1063-6919/00 $10.00 ã 2000 IEEE

4.1 Test on Synthetic Data

In the experiments using synthetic data, we assume
that the sequence contains a car receding in the eld
of view with the motion specied by 2-D ane transformation. The car (in the form of its intensity edge)
is shown in Fig. 1(a).
While in real applications, the observ ation is obtained from real images through preprocessing techniques, e.g., [4], in this synthetic test, we simply transform the template with some ane transformation at
each time t and then add independent noise to each
edge pixel. The noise is uniformly distributed on a
region [;; ] 
 [;; ] centered at the current edge
pixel, with  controlling the noise magnitude. Besides, let each edge pixel vanish from the observation
with probability Pv , and each background point be
falsely detected as an edge with probability Pf . As an
example, such an articial \observation" sequence is
illustrated in Fig. 2 (see also Movie 12). Although this
may not model the real observation well, it is obvious
that the articial measurements are highly contaminated by non-Gaussian noise.

(a)

(b)

Figure 1: (a) A car parametrized as intensity edges. This

is what we used to synthesize the sequence in Fig. 2. (B)
Another vehicle used as alternative hypothesis.

With X0 being the parametrization of the template,
we rst estimate the posterior p(Xt jZt ) from the synthetic sequences. The true and alternative hypotheses
(both shown in Fig. 1) are then both tested against the
sequence. Fig. 3 shows an example of the estimated
posterior probability densities for x and y translation,
with the ground truth marked by a cross, for the true
and alternative (false) candidates, respectively. The
resemblance of the two densities results from the fact
that the two templates are very similar. Yet, a close
look at the gure will nd that the true template generates more concentrated densities with higher peaks.
To better show the eectiveness of the algorithm,
we plot three of the estimated motion parameters (i.e.
E (Xt )), together with the ground truth, in Fig. 4.
The gure clearly shows that the estimates are closer
to the ground truth when the hypothesis is true.
According to the verication algorithm, we compute the posterior probability around the mean of the
state Xt . Fig. 5 shows the computed P as a function
of time t. From the gure, it is obvious that the true
hypothesis always gives the higher posterior probability.
2 Each sequence referred to in this paper has corresponding
video clip at www.cfar.umd.edu/~baoxin/cvpr2k.html .

Parameter vs Frame Index

Parameter vs Frame Index
200
Ty

Ty

200
100

100

0

0

Figure 2: Sample frames of a synthetic sequence (see

text), simulating a car driving away (see also Movie 1).
Original frame size is 240  320.
Estimated Density for x Translation at Frame 49

Estimated Density for x Translation at Frame 49

0.4

0.4

0.3

0.3

0.2

0.2

0.1
0
−60

0
−60

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0
−60

−10

40

90

140

190

240

a11

0.04
0.02
0
−0.02
−0.04
0

10

20
30
Frame Index

40

50

−0.5

0.04
0.02
0
−0.02
−0.04
0

0
−60

20
30
Frame Index

40

50

vs. frame index. Left: obtained from the true hypothesis.
Right: obtained from the false hypothesis. Note that the
parameters should be interpreted as  ; 0 (see text).
Posterior Probability vs Frame Index

−10
40
90
140
190
240
Estimated Density for y Translation at Frame 49

10

Figure 4: True (dashed) and estimated (solid) parameters

0.1
−10
40
90
140
190
240
Estimated Density for y Translation at Frame 49

0

−0.5

a12

a12

a11

0

Posterior Probability vs Frame Index

1

1
0.9

0.9

0.8
0.8
0.7
−10

40

90

140

190

240

0.7
0.6

Figure 3: The posterior densities for x and y translations

at frame 49. The left is obtained when the template is the
true object appearing in the sequence; the right is from a
false candidate.

The system dynamics, if known, can be incorporated into the the SIS steps (using the state equation
as the g(j)). However, in all the experiments in this
paper, we assume that the system is governed by a
rst order Gauss Markov process (random walk). This
enables the algorithm to work on unknown dynamics.

4.2 Vehicle Tracking and Verication

We now present an application to vehicle tracking
and verication in real video data, using intensity edge
map as a template. With real sequences, one needs
rst to detect the object and rectify the template to
the scene. As mentioned in Section 3, we assume that
this step is done. In the experiments, the algorithm
was initialized manually, before invoking the tracking
and verication algorithm. The sequences used were
acquired by a hand-held video camera. Sample frames
of a sequence are shown in Fig. 6(also Movie 2). In the
sequence, the car is receding from the camera, but the
camera is subject to zooming and panning, resulting in
complex dynamics. This type of sequences may arise
from applications with moving camera.
W e model the motion using the 2-D ane group,
and test the two hypotheses against the cluttered sequence. The two candidates are from the previous example (Fig. 1), one true and one false. As an example,
Fig. 7 shows the estimated posterior probability densities for the two translation components of the state
vector Xt at frame 35. The computed posterior probabilities are plotted in Fig. 8. From the gure, it is

1063-6919/00 $10.00 ã 2000 IEEE

0.6

0.5
0

0.5

10

20

30

40

50

0.4
0

10

20

30

40

50

Figure 5: The posterior probability on a region specied

by  centered at the mean. Solid and dashed lines are
for the true and false hypotheses respectively. Left:  =
[1; 1; 1; 1; 5; 5]T (equivalent to the marginal probability
of translation parameters on an region of size 10  10).
Right:  = [0:1; 0:1; 0:1; 0:1; 5; 5]T .

obvious that the true hypothesis almost always gives
the higher posterior probability. If we also use the
posterior probability in a retrospective way (consider
P up to time t), at any time t there is no diculty
in getting the correct verication result despite the
resemblance between the two candidates.
In Fig. 6, we overlap the templates (warped by
E (Xt )) onto the original sequence. It is observed
that the true hypothesis matches the scene well, while
the false hypothesis starts to have trouble after rst
few frames (see also Mo vies 3; 4). Note that the 2-D
ane transform is only an approximation to the vehicle motion in this real video, therefore even the true
template cannot exactly match the scene.

4.3 Face Tracking and Verication

Verifying face in video has broad applications as
mentioned in previous sections. Unlike in face recognition, in general a face from surveillance cameras typically has low resolution. Therefore, feature extraction
is dicult, and it is desired to use the image in a holistic way. In the following experiments, we use intensity
face image as the cue for verication. The templates
are simply face regions (about 2018 pixels in size) extracted from the sequence (at a time t not overlapping

Posterior Probability vs Frame Index

Posterior Probability vs Frame Index

1

1

0.9
0.8

0.8
0.7

0.6

0.6
0.5

0.4

0.4
0.3

0.2

0.2
0.1
0

10

20

30

40

50

0
0

10

20

30

40

50

Figure 8: The posterior probability P for the true (solid)

and false (dashed) hypotheses respectively. Left: marginal
P on translation components with  = [1;   ; 1; 5; 5]T .
Right:  = [0:1; 0:1; 0:1; 0:1; 5; 5]T .

Figure 6: Left: Sample frames of a sequence (Movie 2).
Middle and Right: Tracking results for true and false hypotheses, respectively (Movies 3,4).
Estimated Density for x Translation at Frame 35

Estimated Density for x Translation at Frame 35

0.4

0.4

0.3

0.3

0.2

0.2

0.1
0
−60

0.1
−10
40
90
140
190
240
Estimated Density for y Translation at Frame 35

0
−60

0.4

0.4

0.3

0.3

0.2

0.2

0.1
0
−60

−10
40
90
140
190
240
Estimated Density for y Translation at Frame 35

0.1
−10

40

90

140

190

240

0
−60

−10

40

90

140

190

240

Figure 7: Estimated density for translation components at
frame 35. Left: true hypothesis. Right: false hypothesis.
with the video clip used for tracking and verication).
The motion is modeled as 2-D ane group.
Fig. 9(left column) (also Mo vie 5) shows sample
frames of a sequence with two persons moving around,
whose face templates are used to verify against the
video. In the middle and right column (also Mo vie
6,7), we overlap the templates on the video. For easy
visualization, a black block is used for the template
corresponding to the face of the man in white shirt
(denoted M1), and a white block for the other template from the second man (denoted M2). The middle column illustrates the situation where the algorithm is correctly initialized, meaning the templates
are correctly put on their respective persons. The gure and movies show that tracking is maintained all
the time for M1, and is able to reco ver from occlusion for M2. Fig. 10(left) shows the computed probability for this case. Note that during the time M2
is occluded, the probability drops sharply, while M1
is condently (higher probability) veried across all
frames.
The right column in Fig. 9 shows an interesting
case: we switch the hypotheses { put the templates
on the wrong persons. It is observed that M2 eventually gets dropped to the cluttered background, while
M1, rst sticks to the wrong person, is attracted to the

1063-6919/00 $10.00 ã 2000 IEEE

right person after the men meet. Fig. 10(right) shows
the computed probability for this situation. The curve
of M2 (low probability) conveys a lack of condence,
while the curve of M1 shows that, after it is attracted
to the right person, the tracker is condent of what it
is verifying since the probability is high. It is worth
pointing out that during the short period before occlusion happens, M1 also gives a high probability even
though the tracker is on the wrong person. The reason
is that during that period, M1 is tracking on the face
on a clean background (the wall), thus the high probability re
ects the notion that the tracker would rather
stick to the wrong face than move to the background
since the wrong face is at least more face-like than the
background does. Note that this does not aect verifying which person is M1, since the probability is still
lower compared with when M1 is on the righ t person
(given in Fig. 10(left)).

Figure 9: Left (Movie 5): Sample frames of a sequence.

Middle (Movie 6): templates overlaid on the video when
hypotheses are true. Right (Movie 7): hypotheses are false.

This sequence could be thought as a benign situation except for the occlusion part since the motion is

Posterior Probability vs Frame Index

Estimated Density for parameter a11 at Frame 24

Posterior Probability vs Frame Index

0.9

0.9

0.8

0.8

0.7

0.7

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

Estimated Density for parameter a11 at Frame 24

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0
0

10

20

30

40

0
0

0
−6

0.1
−3.5
−1
1.5
4
6.5
Estimated Density for parameter a12 at Frame 24

0
−6

9

0.4

0.4

0.3

0.3

0.2

0.2

0.1
0
−6

10

20

30

40

Figure 10: The posterior probability P for the hypotheses
M1 (solid) and M2 (dashed) respectively, with  = [1;  
; 1; 5; 5]T . Left: the templates are veried against the
right persons. Right: the templates are veried against
the wrong persons.

−3.5
−1
1.5
4
6.5
Estimated Density for parameter a12 at Frame 24

9

0.1
−3.5

−1

1.5

4

6.5

0
−6

9

−3.5

−1

1.5

4

6.5

9

Figure 12: The posterior density for a11 , and a12 at
frame 24. Left: true hypothesis. Right: false hypothesis.
Note that, with the false hypothesis, the densities have
not only worse shapes, but also wrong mean values, resulting in the wrong shape and location of the template in
Fig. 11(right).
Posterior Probability vs Frame Index
1

0.8

0.6

0.4

0.2

0
0

5

10

15

20

25

30

Figure 13: The posterior probability

P for the true
(solid) and false (dashed) hypotheses respectively, with
 = [0:3; 0:3; 0:3; 0:3; 4; 4]T .

wrong template), the template eventually gets stuck
at the cluttered background.

Figure 11: Left (Movie 8): Sample frames of a sequence.

Middle and right: tracking results for true and false hypotheses, respectively (Movies 9,10).

mainly translation, and the video quality is good. We
now experiment on a challenging sequence of moderate visual quality where the motion has large scaling
components, Sample frames of the sequence are shown
in Fig. 11(also Movie 8). The task is to verify the
man carrying a suitcase as one of two templates (extracted from the two men walking in the middle at a
later time in the sequence). Fig. 12 shows an example
of estimated densities of a11 and a12. The computed probability is plotted in Fig. 13. Note that although there are several dicult frames, the algorithm
is able to pick the true hypothesis based on the cumulative probability. In the middle and right columns
in Fig. 11(also Movies 9,10), the template (shown as
white block for easy visualization) is overlaid on the
sequence. It is obvious that when the hypothesis is
true, the algorithm approximately estimated the scaling of the template, while under the false hypothesis,
even though the block is deformed so much (indicating
a competition in trying to verify a person against the

1063-6919/00 $10.00 ã 2000 IEEE

5 Related Work

There has been extensive work on visual tracking in
the literature (e.g. [2, 3, 6, 9, 11, 15, 16]), among which
we only mention a few here. [11] proposed a contour
tracking algorithm using a random sampling method
which can be treated as a special realization of the SIS
algorithm. [9] proposed a tracking approach based
on edge matching. Contours are in a sense features
of lower level compared with intensity image, but of
higher level compared with edge. Thus with contours,
ecient computation is easier to achieve. On the other
hand, the approach in [9] has the virtue that virtually
no model is needed, the algorithm simply works on
the sequence (after extracting edges) through matching based on Hausdor measure [10]. An eigenspacebased approach was presented in [3] which can directly
handle intensity image. To achieve tracking, a training process is needed to construct the eigenspace. In
terms of tracking, the proposed approach diers from
above methods in that it can handle edge, contour,
intensity image, or other representations in a uniform
way.
There are, however, fewer reports on verication
from video. In fact, the algorithm presented here is
the only one that we are aware of which explicitly at-

tempts to solve the verication problem in addition
to tracking in video data. In previous face verication examples, it is obvious that none of the mentioned trackers can give the verication results as the
proposed algorithm does, although we believe some of
them can handle the tracking part well. This is the
major feature that makes the proposed algorithm different from other pure tracking methods.
There is no doubt that verication is an important
aspect in video-related applications. When v erication is the concern, in general, contours alone are not
sucient. While edges ma y be enough for distinguishing, for example, dierent type of vehicles, they are
not expected to work well for verifying more complex
objects such as faces. In this case, we may have to resort back to the intensity images (or other equivalent
transformation domain representations). These dierent representations can be handled uniformly by the
proposed algorithm to give tracking plus verication
results.

6 Discussions and Conclusion

This paper presents a generic simultaneous tracking
and verication algorithm based on sequential Monte
Carlo sampling methods. With this algorithm, applications using edge, contour, intensity image, or other
suitable representations can all be united under the
same framework. Experiments on synthetic and real
data have been presented. The results suggest that the
algorithm could provide a promising stochastic tracking and verication approach.
It is worth pointing out that although in examples
in this paper the motion model is assumed to be 2D, it need not be stationary in time, meaning that
the motion can vary from frame to frame. Th us this
model can still describe some complex dynamics, suc h
as the one shown in Fig. 6 (Movie 2).
Problems remaining to be solved include dealing
with more complex motion. Recall that w e deduced
the re-parametrization based on small motion assumption through a Taylor series expansion. Even
though (3) is general, it represents a parametric motion model. It may be dicult to obtain a parametric model for complex motion suc h as those involving 3-D rotation of the subject. Fortunately, the
parametrization is independent of the algorithm in the
sense that the algorithm is applicable as along as a
low-dimensional state space is formed.
The time complexity is another issue that needs to
be addressed, which has not been fully investigated
yet. To give a rough idea about the speed, in the application of face verication illustrated in Fig. 11, the
algorithm operates at 3 frame/per second on an Ultra 5 Sparc workstation for a single hypothesis. But
currently the algorithm has not been integrated into
a real-time system, thus the time consumed includes

1063-6919/00 $10.00 ã 2000 IEEE

overhead such as reading and writing images les from
the hard disk, etc. Besides, no code optimization in
any sense has been performed yet. Further investigation is needed for accurate speed performance characterization, as well as ecient real-time implemen tation
of the algorithm.

References

[1] B. Anderson and J. Moore, \Optimal Filtering," Englewood Clis, Prentice Hall, New Jersy, 1979.
[2] A. Baumberg and D. Hogg, \An Ecient Method for
Contour Tracking Using Active Shape Models," Proc.
IEEE Workshop on Motion of Non-R igid and Articulated Objects, pp.194-199, 1994.
[3] M. Black and A. Jepson, \EigenTracking: Robust
Matching and Tracking of Articulated Objects Using
a View-based Representation," IJCV, Vol. 20, pp.6384, 1998.
[4] J. Canny, \A Computational Approach to Edge Detection," IEEE PAMI, Vol. 8, pp.679-698, 1986.
[5] B. Carlin, N. Polson, and D. Stoer, \A Monte Carlo
Approach to Nonnormal and Nonlinear State-Space
Modeling," Jour. of Amer. Stat. Assoc., Vol. 87,
pp.439-500, 1992.
[6] D. Freedman and M. Brandstein \A Subset Approach
to Contour Tracking in Clutter," ICCV-1999.
[7] N. Gordon, D. Salmond, and A. Smith, \Novel approach to nonliner/non-Gaussian Bayesian state estimation," IEEE Trans. Radar, Signal Processing,
1993, Vol. 140, pp.107-113.
[8] J. Hammersley and D. Handscomb, \Monte Carlo
Methods," John Willey & Sons, New Y ork, 1964.
[9] D. Huttenlocher, J. Noh and W. Rucklidge, \Tracking
Non-Rigid Objects in Complex Scenes," ICCV-1993.
[10] F. Hausdor, \Set Theory," 3rd ed. Chelsea Publishing Co., New York, 1978.
[11] M, Isard and A. Blake, \Contour Tracking by stochastic propagation of conditional density," ECCV-1996.
[12] G. Kitagawa, \Monte Carlo lter and smoother for
non-Gaussian nonlinear state space models," Jour.
Comp. and Graph. Stat., Vol. 5, pp.1-25, 1996.
[13] A. Kong, J. Liu, and W. W ong, \Sequential Imputations and Bayesian Missing Data Problems," Jour. of
Amer. Stat. Assoc., Vol. 89, pp.278-288, 1994.
[14] J. Liu and R. Chen, \Sequential Monte Carlo Methods for Dynamic Systems," Jour. of Amer. Stat. Assoc., Vol. 93, pp.1031-1041, 1998.
[15] C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland, \Pnder: Real-time Tracking of the Human
Body," IEEE PAMI, Vol. 19, pp.780-785, 1997.
[16] Y. Yacoob and Larry Davis, \Tracking Rigid Motion
Using a Compact-Structure Constraint," ICCV-1999.

2015 IEEE International Conference on Computer Vision

Simpler non-parametric methods provide as good or better results to
multiple-instance learning.
Ragav Venkatesan, Parag Shridhar Chandakkar and Baoxin Li
Arizona State University, Tempe, AZ, USA
ragav.venkatesan@asu.edu, pchandak@asu.edu, baoxin.li@asu.edu

Abstract
Multiple-instance learning (MIL) is a unique learning
problem in which training data labels are available only for
collections of objects (called bags) instead of individual objects (called instances). A plethora of approaches have been
developed to solve this problem in the past years. Popular
methods include the diverse density, MILIS and DD-SVM.
While having been widely used, these methods, particularly
those in computer vision have attempted fairly sophisticated
solutions to solve certain unique and particular conﬁgurations of the MIL space.
In this paper, we analyze the MIL feature space using
modiﬁed versions of traditional non-parametric techniques
like the Parzen window and k-nearest-neighbour, and develop a learning approach employing distances to k-nearest
neighbours of a point in the feature space. We show that
these methods work as well, if not better than most recently
published methods on benchmark datasets. We compare
and contrast our analysis with the well-established diversedensity approach and its variants in recent literature, using benchmark datasets including the Musk, Andrews’ and
Corel datasets, along with a diabetic retinopathy pathology
diagnosis dataset. Experimental results demonstrate that,
while enjoying an intuitive interpretation and supporting
fast learning, these method have the potential of delivering
improved performance even for complex data arising from
real-world applications.

Figure 1. DR image classiﬁcation as a MIL problem.

stance level. It is interesting to note however that the labelspace is the same for both at the bag level and at the instance
level. One may attempt to learn instance-level labels during
the training stage, thus reducing the problem to an instancelevel supervised classiﬁcation. Alternatively, one may also
localize and prototype the positive instances in the feature
space and rely on the proximity to these prototypes for subsequent classiﬁcation.
MIL is an ideal set-up for many computer vision tasks
and examples of its application include object tracking [4],
image categorization [9] [26] [28] [12], scene categorization [20] and content-based image retrieval [36]. In particular, MIL can be an especially suitable model for medical
image-based pathology classiﬁcation and lesion detectionlocalization, where an image is labeled pathological just because of one or a few lesions localized to small portions of
the image. Medical images collected in a clinical setting
may readily have an image-level label (either normal or various levels of pathology) while lacking the exact location of
the lesion(s). Figure 1 illustrates such an example: colour
fundus images of eyes affected with different pathologies
of diabetic retinopathy (DR). It is easy to notice that, although majority of the image looks normal, a small retinal
landmark is enough to alter the label of the image from normal to pathological. In a MIL formulation for this problem,
each image can be considered a bag and patches of images
can be considered instances.
Over the years, many methods have been proposed to
solve the MIL problem [10] [29] [8] [2]. The most fun-

1. Introduction
Multiple-instance learning (MIL) is a setting where labels are provided only for a collection of instances called
bags. There are two types of instances: negative instances,
which are found in either negative bags or positive bags, and
positive instances, which are found only in positive bags.
While a positive bag must contain at least one inherently
positive instance, a negative bag must not contain any positive instances. In MIL, labels are not available at the in1550-5499/15 $31.00 © 2015 IEEE
DOI 10.1109/ICCV.2015.299

2605

(marked in green ellipses P 1 through P 4). The centroids
of these clusters would be the ideal positive instance prototypes that a MIL algorithm should identify. With the help of
this illustration, it is not difﬁcult to imagine that, one or few
noisy negative instances coming close to a true positive instance prototype could lower the diverse density drastically
and thus lead to a dramatic decrease in performance. Herein
lies a core argument to the MIL deﬁnition - the strictness
of positive neighbourhood. We show that DD-based algorithms are not tolerant even to a single negative instance in
an arbitrary positive instance neighbourhood. Such strict assumptions are not suitable for real-world (medical imaging)
data wherein the feature space can be noisy.
In this paper, we propose modiﬁcations to traditional
non-parametric methods adapting them to MIL. We demonstrate their effectiveness against DD taking into consideration the complex arrangements of a typical MIL feature
space. In particular, the formulation aims at easing the
dramatic impact of noisy negative instances on instanceprototyping in DD-based approaches. The formulation
draws intuition from k-nearest-neighbour classiﬁcation and
thus leads readily to an efﬁcient learning algorithm. It employs an aggregated and weighted distance measure computed from any point to its neighbouring instances labeled
according to their respective parent bags, conforming to
MIL requirement. Analysis with simulated data and experiments with real data in comparison to existing state-ofthe-art approaches suggest that the proposed method, while
enjoying simplicity in formulation and learning, has the potential of delivering superior performance for challenging
benchmark datasets.
The remainder of the paper is organized as follows. Section 2 cites related works, while Section 3 describes the
proposed method. Section 4 presents the experimental setup and discusses results on the various evaluation datasets.
Section 5 provides concluding remarks.

P2
P1

P3
P4
Figure 2. An illustrative feature space for multiple-instance setting. The ’x’ in red represents all instances from positive bags and
the ’o’ in blue represents all instances from negative bags.

damental one is the diverse density approach [19], which
has been built upon by many variants [35] [24] [9]. Diverse
density is in its basic sense, a function so deﬁned over the
feature space that it is large at any point in the feature space
that is close to instances from positive bags while being
far away from instances from negative bags and vice-versa.
The various local maximas in this function are positive instance prototypes and any instance that is closer to these
prototypes are labeled inherently positive instances. Other
types of methods also exist in this setting [5] [3] [27] [31].
MIL has many different variants and perspective to its
deﬁnition and indeed most MIL solutions are application
centric [1]. This can be easily seen from table 1. Earlier
methods perform as good or better in the MUSK dataset
than the ones published recently although the recent methods perform better on more complex tasks but for certain
exceptions. In this course of research while many particular
and complicated solutions are sought after, MIL has never
been sufﬁciently analyzed using traditional non-parametric
learning methods. Despite the recent advances, MIL remains a challenging task as the feature space may be arbitrarily complex, the ratio of positive to negative instances
can be arbitrarily low in a positive bag, and (by deﬁnition)
no labeling information is directly available for positive instances.
To illustrate these factors, we simulate a typical MIL feature space as depicted in ﬁgure 2. Each instance belonging
to a particular cluster is independently drawn from a normal
distribution that deﬁnes the said cluster. While positive bags
can draw a subset of random cardinality of instances from
negative distributions, negative bags cannot draw any data
from positive distributions. Every positive bag must have
at least one instance sampled from a positive distribution

2. Related Works
MIL was ﬁrst introduced for the problem of drug activity
prediction [10], where axis-parallel hyper-rectangles (APR)
were used to design three variants of enclosure algorithms.
The APR algorithms tried to surround at least one positive
instance from each positive bag while eliminating any negative instances inside it. Any test bag was classiﬁed positive as long as it had at least one instance within the APR.
Conversely a bag was classiﬁed as negative when it had no
instance represented within the APR.
The ﬁrst density-based formulation of MIL was diverse
density (DD) [19]. DD is not a conventional density but is
rather deﬁned as the intersection of the positive bags against
the intersection of the negative bags. It is a measure that is
high at any point on the feature space x if x is closer to positive instances and is farther away from negative instances.
2606

The local maxima of DD would yield a potential concept
for the positive instances. Several local maxima can yield
several prototypes of positive instances that can be far apart
in the feature space. Some of these prototypes can be separated by other negative instances. The concept point of a
diverse density in a MIL feature space was deﬁned as,


P r(x = t|Bi+ )
P r(x = t|Bi− ). (1)
arg max
x

i

i

These local maxima were termed as instance prototypes. A
noisy-or model was used to intuitively maximize the DD
in Equation 1. This was further developed to assume more
complicated and disjoint concepts in EM-DD and further
developed by other methods including DD-SVM and Accio
[35] [9] [24]. The major drawback of the diverse density
arises in a situation where the distribution of negative instances is noisy. In other terms, if one instance prototype
has a negative distribution closer to the prototype than the
others, then its diverse density is largely lower than that of
the others, as DD unfairly favours the distribution of positive instances that is farther away from negative instances
than those that are relatively closer. This makes it hard to
deﬁne that particular prototype in such situations. Even the
presence of one noisy negative instance near the potential
instance prototype can lower the DD drastically as we show
in the later sections. In ﬁgure 2 the prototype P 4 was the
twenty second largest local maxima in the DD of the feature
space. If there were a bag that contained only one positive
instance near P 4 but was still close enough to the negative instances, chances are that this bag will be misclassiﬁed as negative. DD deﬁned in such a formulation provides
a density-like function that is ﬁckle and is easily affected
by introducing even just one negative sample closer to the
positive prototype.
The maximization procedure for DD is started from initial guesses. An idea was put forward by Chen and Wang
that the maximization should start from every instance in
every positive bag (or at least a large sample of positive
bags) so that unique local maxima in DD can be identiﬁed [9]. A plethora of methods still use this DD formulation [9] [8] [24] [35] [18]. The decision boundary of a DD
system is a hyper-ellipsoid in the feature space. A kernel
based maximum-margin approach would construct hyperplanar decision boundaries characterizing complex decision
surfaces. The ﬁrst formulation of a support vector machine
(SVM) for MIL was proposed in 2002 [2]. They devised an
instance-level classiﬁer mi-SVM and a bag-level classiﬁer
MI-SVM. In a way, MI-SVM maximized the margin between the most positive instances and the least negative instances in positive and negative bags respectively. The MISVM framework is now modiﬁed and re-christened as latent-SVM which plays a central role in the deformable-part
models based object recognition algorithms [12]. MILIS

provided a similar SVM-based approach with a feedback
loop to select instances that provided a higher training stage
conﬁdence [13]. This was an idea adapted from a previously
existing related idea, MILES [8].
The ﬁrst distance-based non-parametric, lazy learning
approach to MIL was taken by citation-k-NN [29]. Interbag distances were found using a minimal Hausdorff distance. A k-nearest neighbour approach was used along
with this distance to classify a new bag or to retrieve closer
bags. This did not always work in a MIL setting as k-NN
uses a majority voting scheme. If a positive bag contains
fewer number of inherently positive instances than inherently negative instances, majority of its neighbours are going to be negative and the algorithm was confused by the
false-positives it reported. Therefore the concept of citers
was introduced. If k-NN refereed its neighbours, then its
neighbours are cited by citers. Citers are the backward
propagated references, in the sense that they refer back the
considered instance. Though it was a generalized approach,
citation k-NN did not work as well when positive instances
were clustered and such clusters were separated by negative
instances, in which case the citers and references did not
always compliment each other.
This problem does not apply to all nearest neighbour
based approaches. Nearest neighbour approaches should be
used properly and their smart usage was discussed in [6]. A
novel concept of bag to class (B2C) distance learning was
adopted for the use of k-NN. A complimentary idea was
utilized in a MIL set-up by learning class to bag (C2B) distances by combining all training bags of a particular class
to form a super-bag [26] [31]. A similar instance speciﬁc
distance learning approach was used in [27]. On further
study, this was reformulated as a l2,1 minimax problem and
was solved with some effort [28]. A similar idea was implemented to group faces in an image by considering inter
bag or bag to bag (B2B) distances in [15]. A related bag to
bag approach is used to quantify super-bags in [3].
Most of the MIL algorithms presented above assume that
the bags are independent. Though it is a reasonable assumption in a computer vision context, it might not be a
general idea. Zhang et al., explored the MIL idea for structured data [34]. A data-dependent mixture model approach
was developed in [30]. Another approach designed speciﬁcally for special data space is the fast bundle algorithm for
MIL [5]. One important assumption in the early understanding of MIL is that every positive bag must contain at least
one positive instance. Chen et al. felt this was too restrictive
and developed a feature mapping using instance selection
that projects a MIL problem into a much simpler supervised
learning problem using an instance similarity measure [8].
This counter-assumption was also used in a histopathology
cancer image learning system using a multiple clustered
instance learning approach [33]. Although in a MIL for2607

mulation bag level classiﬁcation is sufﬁcient and instance
level classiﬁcation though clever, is not required, many algorithms attempt to identify positive instances. A SVM was
used to minimize the hinge loss (modeled as slack variables)
to identify positive instances in [32]. The above methods
cater to certain particular conﬁgurations of the MIL space
and are suitable for particular domains.

3. The proposed approach
Consider ﬁgure 2. Though not universal, this ﬁgure illustrates a typical MIL feature space. The instances arising
from regions P 1 to P 4 are potentially inherently positive
instances as they are farther away from negative instances
while being closer to other positive instances. The instances
from positive bags in other regions, along with negative instances are in reality, negative instances as they are close in
proximity to negative instances from negative
⎤
⎡ bags.
X (1) Y (1)
⎢ X (2) Y (2) ⎥
⎥
⎢ (3)
⎢
Y (3) ⎥
Suppose we have labeled data D = ⎢ X
⎥
⎢ .
.. ⎥
⎣ ..
. ⎦

X (n) Y (n)
where X is the i bag in the dataset and Y (i) ∈ {0, 1}
is its label. Internally, each bag X (i) contains mi (often is
a constant m by design, particularly in image classiﬁcation
(i)
(i)
(i)
contexts) instances such that X (i) = {x1 , x2 , . . . xmi }.
Consider a small region R of volume V in this feature space.
The estimate for the density of instances from positive bags
+
is given by (|k V|)/n , where k + is the set of instances from
positive bags in the region R and |k + | its cardinality, and n
is the number of instances in all of the feature space. Similarly the estimate for the density of negative instances is
−
given by (|k V|)/n , where k − is the set of instances from
negative bags in the region R, |k − | is the number of negative instances in the region R.
+
−
Putting them together, (|k V|)/n − (|k V|)/n is a measure
that, will be high if the number of positives exceed the number of negatives in that region, will be low if the number of
negatives exceed the number of positives in that region, and
will be 0 if the number of positives equal the number of
negatives within that region. Alternatively, if one considers
a (rectangular) Parzen window,
	
1, |uj | ≤ h where, j = 1, 2, ...d,
φ(u) =
(2)
0, otherwise
(i)

th

the aforementioned measure can also be formulated as,
+

−

|kn |
|kn |
1 x − ki+
1 x − ki−
1

1

φ(
)−
φ(
)
fparzen (x) =
n i=1 V
h
n i=1 V
h
(3)

Figure 3. Parsing the MIL feature space with a Parzen window
technique. It can be seen that this follows the properties of a MIL
density-like.

where, x is any location on the feature space and ki+ and ki−
are instances from positive and negative bags within that region respectively. Such a parsing of the MIL feature space
of ﬁgure 2 is shown in ﬁgure 3. The properties of the function fP arzen (x) hold similar to that of DD and can be easily
observed in ﬁgure 3. The choice of the size of the region
(analogous to the selection of the variance for the Gaussian
in the DD formulation) and the Parzen window functions
are in line with that of a traditional Parzen window: if the
size becomes too large, the measure will not have sufﬁcient
resolution. Picking a proper region-size would be a practical difﬁculty.
Instead of considering a region R of ﬁxed size, let’s
limit to a ﬁxed number of neighbours k. In this set-up,
we start with a region of zero volume from x and grow
two regions, one for positive instances and one for negative instances, until we just enclose for each of the regions, k points respectively. This enables us to have different sized regions for positive and negative estimates respectively. While it appears to be a simple k-NN approach
to density estimation, we emphasize that we are not using
the nearest-neighbour voting rules. In fact, a direct application of nearest-neighbour voting technique will not work on
a MIL space as was pointed out by Wang et. al, but the idea
of nearest neighbour can still be modiﬁed and used to suit
the MIL needs [29]. The vote contributions of positive and
negative neighbours enclosed by the two regions are their
respective kernelized distances to the point x, instead of a
uniform majority vote. This aggregated vote can be formu2608

This is an indicator function that classiﬁes the bag 1 if it has
at least a instances classiﬁed as positive and 0 other-wise.
Typically in most MIL settings a = 1, although this need
not be the case generally. The aim of this non-parametric
empirical risk minimization formulation is to minimize the
training error,
n



ˆ = 1
1{b(X (i) ) = Y (i) }, ∀(X, Y ) ∈ D.
(b)
n i=1
Figure 4. A region of a typical 2D MIL feature space and its parse
using the k-NN measure. Red represents positive and blue represents negative.

ˆ as,
by estimating T̂ that best minimizes (b)
ˆ
T̂ = arg min (b)

(8)

T

lated as,
|k− |

fkN N (x) =



i=1

Ψ(||x − ki− ||) −

|k+ |



i=1

Ψ(||x − ki+ ||)

such that, |k + | = |k − | = k.

(4)

where, Ψ(.) is a monotonically increasing sub-modular
function, k is the number of neighbours considered, and k +
and k − are now the set of k instances from positive and
negative bags that are the nearest to x respectively. Ψ(.) is
used as a way to scale distances when the feature space is
arbitrarily large. It can be considered as normalization. For
all our experiments, we typically use Ψ(x) = x.
The advantage of ﬁxing the number of neighbours is that
in a region where there are no points or very few number
of points, we will get a block of uniform measure and in a
region where there is a high density of points, we will get a
smoothly varying measure. Such a measure is shown in ﬁgure 4. The impact of the number of neighbours k is similar
to that of the size of the region R in the Parzen window idea.
If k is too small, the measure is going to give information
about a very small local region and is thereby unreliable.
If k is too large , the impact of proximity is going to be
averaged out.

Learning
Learning under this formulation is a straight forward
threshold learning and this is done by maximizing the validation accuracy. An instance-level classiﬁer using this measure can be constructed as,
h(x) = 1{fkN N (x) ≥ T }

(5)

This is an indicator function that outputs 1 if the measure
is above a threshold T and 0 if the measure is below the
threshold T . We can use this instance-level classiﬁer to construct a bag-level classiﬁer.
m


b(X) = 1{
h(xi ) ≥ a}∀x1 , x2 , . . . , xm ∈ X.
i=1

(6)

(7)

Once the threshold is learnt, classiﬁcation is performed
directly by using the bag-level classiﬁer in equation 6 with
the learnt threshold. Note that in MIL, it is not required,
although possible in this case, to label each instance in the
bag. The labeling of instances can be as follows:
y(x) = h(x)|T =T̂

(9)

This process is equivalent to maximizing the equation 4
(or 3) for all points of feature space and considering the
local maximas as instance prototypes, as was described by
Chen et. al, for the DD formulation [9]. This now enables
comparison to prototyping-based methods. Such a formulation can now be re-written as,
x̂ = arg max
x

 |k− |


i=1

Ψ(||x − ki− ||) −

|k+ |



i=1


Ψ(||x − ki+ ||)

such that,|k + | = |k − | = k.

,

(10)

where x̂ is a prototype positive instance. One advantage
of using equation 10 is that once the prototypes are found,
we neither need the entire dataset anymore nor do we need
to calculate distances to all the points in the dataset. The
prototypes easily divide the feature space into probabilistic
Voronoi tessellations such as in ﬁgure 4. We could also
estimate a radius around every prototype to isolate hyperspherical regions that are positive.
We solve this optimization problem by using an idea similar to the one used in [9]. We start a local gradient ascent
from every instance from every positive bag in the training dataset and ﬁnd a local maxima. Since such maximas
can only ever end in a high density region of true positive
instances from positive bags and since we start each gradient ascent from every instance in every positive bag, each
ascent is computationally tractable in small number of iterations. Indeed, often few well-chosen instances from positive bags make this convergence faster and such techniques
can be found for maximizing the DD in various papers previously surveyed in section 2. Similar techniques can be
2609

applied here as well. All the local maximas are sorted (after
non-maximal suppression) and top N are considered as instance prototypes. It is to be noted that for the dataset shown
in ﬁgure 2, while the top 5 maximas were enough to ﬁnd all
four prototypes for our approach, it takes top 24 maximas
for DD to ﬁnd the four prototypes.
The k for k-NN is picked here by a typical elbow
method. Once local maximas (instance prototypes) are
found we can again maximize a validation accuracy jointly
for all instance prototypes to ﬁnd a threshold of classiﬁcation for each prototype in terms of the distance to the prototype, hence creating a hyper-spherical decision regions
around each prototype. Thus the decision boundaries of
this method creates a tessellation of the feature space. The
tessellation is a superposition of hyper-spherical regions
around a positive prototype with varying radii.

4. Experiments and Results
In this section we provide details of our experiments
and the results from those experiments. We evaluated
our method using three standard MIL datasets: the musk
dataset, Andrew’s datasets, the Corel datasets (both 1k and
2k), and our own dataset: the DR dataset. For all the results
shown on all the datasets, we used the most common implementation methodologies, including data splits, cross validations and average over runs that were found in literature.
This enabled us to compare against results that were published in the same. When results were not available or when
the protocol doesn’t match, we evaluated the results using
the codes from CMU MIL toolbox1 . In case of MILES, the
results were obtained by using the author’s original code2 .

4.1. Musk dataset
An accepted benchmarking dataset in the MIL literature
is the musk dataset. The musk dataset is well-described
in [10]. Musk dataset is a benchmark feature space used to
predict drug activity. It contains two sub-datasets: MUSK1
and MUSK2. MUSK 1 contains 92 molecules with 47
musk and 45 non-musk molecules. MUSK 2 contains 102
molecules with 39 musk and 63 non-musk molecules. Each
bag contains variable number of instances with 166 dimensional features and binary labels. We use the standard
implementation speciﬁcations that is used in the original
APR paper and other published literature: ten-fold crossvalidation over the entire dataset, since its easier to compare against a plethora of methods [10]. Table 1 compares
the performance of various algorithms against the proposed
method. It can be seen that the proposed method is best
in MUSK 1 and among the high performing methods in
MUSK 2.
1 CMU

MIL toolbox: http://www.cs.cmu.edu/˜juny/MILL
2 MILES homepage: http://www.cs.olemiss.edu/ ychen/
˜
MILES.html

Methods
DD [19]
EM-DD [35]
citation (k)-NN [29]
mi-SVM [2]
MI-SVM [2]
DD-SVM [9]
MILES [8]
MIforest [31]
MILIS [13]
ISD [27]
ALP-SVM [3]
MIC-Bundle [5]
Ensemble [18]
Proposed

MUSK 1
88.9%
84.8%
92.4%
87.4%
77.9%
85.8%
86.3%
85%
88.6%
85.3%
87.9%
84%
89.22%
92.4%

MUSK 2
82.5%
84.9%
86.3%
83.6%
84.3%
91.3%
87.7%
82%
91.1%
79.0%
86.6%
85.2%
85.04%
86.4%

Table 1. Performance of various MIL algorithms on the musk
dataset.

MUSK datasets are uni-concept datasets. For instance,
in MUSK 1, among a total of 476 unique instances each
with feature values ranging from -348 degrees to 336 degrees, there are only 633 unique feature values. In such a
heavily quantized feature space that is 166 dimensional, detecting one potential instance prototype is easier for density
based algorithms.

4.2. Andrew’s datasets
Andrews et. al, in their mi-SVM paper proposed the use
of three classiﬁcation datasets, elephant, fox and tiger, for
the use of evaluating multiple-instance learning [2]. These
are now popular benchmark datasets in the MIL literature.
We also test our algorithm on these datasets using the same
speciﬁcations mentioned on the said article. Each dataset
has 200 images with 100 positive and 100 negative images.
The number of instances in each category are 1391, 1320
and 1220 respectively with varying number of instances per
bag. Each instance is a 230 dimensional feature vector. We
train on a 2/3 random split of the data and test on the remaining 1/3 of the unseen data. The results are maximized
over 15 runs of validation and are shown in table 2. Our
result while being the best in the Elephant and Fox classes
is almost as good as the best in the Tiger class. It is to be
noted that we are signiﬁcantly higher in the Fox class which
is widely considered to be a notoriously noisy dataset for
MIL. This is a strong indicator of our method’s adaptability.

4.3. Corel dataset
Corel is another well known, image categorization
dataset for MIL benchmarking. The Corel-2k dataset consists of 2000 images. There are 20 classes and each class
consists of 100 images. The Corel-1k dataset is a subset
2610

Methods
citation k-NN [29]
mi-SVM [2]
MILES [8]
MIforest [31]
ISD [27]
ALP-SVM [3]
MIC-Bundle [5]
Ensemble [18]
Proposed

Elephant
79.2%
79.7%
70%
84%
77.9%
84%
80.5%
84.25%
86%

Fox
62.5%
62.9%
56%
64%
63%
69%
58.3%
63.05%
73.94%

Methods
DD [19]
EM-DD [35]
citation k-NN [29]
mi-SVM [2]
MILES [8]
Proposed

Tiger
82.6%
79%
62%
82%
85.3%
86%
79.11%
79.30%
85.7%

Table 4. Performance of various MIL algorithms on DR dataset.

high-quality colour fundus image database of 425 images
comprising 160 normal images, and 265 affected images to
test our algorithm on. This dataset was constructed from
publicly available databases including DiabRetDB0 [11],
DiabRetDB1 [17], STARE [21] and Messidor3 and has been
used in some existing studies [7] [25]. The balance of the
database is more towards the positive bags and this makes it
more challenging for a MIL algorithm. The results were all
evaluated using a 2/3 − 1/3 train-test split.

Table 2. Performance of various MIL algorithms on Andrew’s
dataset.

Methods
mi-SVM [2]
MI-SVM [2]
MILES [8]
DD-SVM [9]
MILIS [13]
Proposed

Corel-1k
76.4%
75.1%
82.3%
81.5%
83.8%
87.3%

Accuracy
61.29%
73.5%
78.7%
70.32%
71%
81.3%

Corel-2k
53.7%
55.1%
68.7%
67.5%
70.1%
71.9%

Prototyping DR instances
Table 3. Performance of various MIL algorithms on Corel dataset.

In the prototyping sense, each prototype of positive instances should roughly correspond to one type of lesion. As
we use colour features this is easily possible. We estimated
a total of about 35 different types of lesion prototypes using our algorithm and veriﬁed it with EM-DD’s prototypes.
EM-DD had its maximum accuracy at about 40 prototypes.
It is reasonable to assume from this information that there
are in the range of, 35-40 different positive prototypes, each
of which in the feature space might correspond to a unique
lesion type or character. In this feature space, the negative instances are of three types: normal skin, nerves and
the optical disk. This is a reasonably noisy datasets and often has only one or two instances among 48 instances that
are positive in a positive bag. Though the distribution of
the optic disc might be noisy, and the number of true positive instances are very low, the proposed algorithm has the
potential to adjust to it. Table 4 shows the results of the
proposed approach on the DR dataset, where the proposed
method stands best.

of this dataset with the ﬁrst 10 difﬁcult categories. Table 3 shows the performance of the proposed approach in
the corel dataset. It is to be noted that we are producing
the best results in the Corel dataset. Training-testing data is
again a 2/3 − 1/3 split.

4.4. A DR dataset
As was brieﬂy discussed in section 1, DR image classiﬁcation is an application especially suitable for MIL. In
practice, the difﬁculty in this problem arises from the fact
that the physical and observable difference between a normal eye and a pathological eye can be very small, localizing
to regions with slightly different characteristics. This can be
seen in ﬁgure 1.
A variety of classiﬁcation and retrieval schemes have
been tried on DR images. Structural Analysis of the Retina
(STARE) is one of the earliest attempts to solve the DR conundrum [21] [14]. STARE performs automated diagnosis and comparison of images to search for images similar
in content. Recently other learning approaches were developed to identify relevant patterns using local relevance
scores [23]. Application of MIL approaches to DR is gaining interest in recent years [22].
In this study, we consider the auto colour correlogram
(AuoCC) as a colour feature, which is well-studied in the
medical imaging literature [16]. A modiﬁed and quantized
64-bin AutoCC feature is extracted for each instance in an
image [25]. We neglect the black regions and sample 48
non-overlapping instances from every image. We use a

4.5. Sensitivity to labeling error
Although not an implicit feature of the proposal, we perform the experiments to demonstrate the proposed method’s
sensitivity to labeling error, exactly similar to the one described in [8]. We deliberately ﬂip the labels for a range of
percentages of labels randomly on our training split and test
the trained model on the original labels in the testing split.
The split was 2/3 − 1/3. The accuracies of the proposed
method on various datasets are shown in ﬁgure 5. After
3 Kindly provided to us by the messidor program partners. Visit http:
//messidor.crihan.fr

2611

simple, yet novel usage of non-parametric learning philosophy to the MIL problem. In particular, we analyzed the
MIL feature space using a k- NN philosophy and proposed
a new formulation based on distances to k-nearest neighbours. The new formulation was compared and contrasted
with the widely used DD formulation. The proposed approach was tested on the musk datasets, Andrews dataset
and the corel datasets, and was found to be effective. The algorithm was used to solve the DR image classiﬁcation problem and was found to be the best among other algorithms.
We therefore conclude that a non-parametric learning philosophy to MIL not only makes intuitive sense but can also
be a powerful tool for most general cases.

Figure 5. Accuracy vs Percentage of labels ﬂipped for the proposed method. Flatter curve is good.

Acknowledgement
The work was supported in part by a grant
(#W911NF1410371) from the Army Research Ofﬁce
(ARO). Any opinions expressed in this material are those
of the authors and do not necessarily reﬂect the views of
the ARO.

References
[1] J. Amores. Multiple instance classiﬁcation: Review, taxonomy and comparative study. Artiﬁcial Intelligence, 201:81–
105, 2013.
[2] S. Andrews, I. Tsochantaridis, and T. Hofmann. Support
vector machines for multiple-instance learning. Advances in
neural information processing systems, 15:561–568, 2002.
[3] B. Antić and B. Ommer. Robust multiple-instance learning with superbags. In Computer Vision–ACCV 2012, pages
242–255. Springer, 2013.
[4] B. Babenko, M. Yang, and S. Belongie. Robust object tracking with online multiple instance learning. IEEE PAMI,
33(8):1619–1632, 2011.
[5] C. Bergeron, G. Moore, J. Zaretzki, C. M. Breneman, and
K. P. Bennett. Fast bundle algorithm for multiple-instance
learning. Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 34(6):1068–1079, 2012.
[6] O. Boiman, E. Shechtman, and M. Irani. In defense of
nearest-neighbor based image classiﬁcation. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1–8. IEEE, 2008.
[7] P. S. Chandakkar, R. Venkatesan, and B. Li. Retrieving
clinically relevant diabetic retinopathy images using a multiclass multiple-instance framework. In SPIE Medical Imaging, pages 86700Q–86700Q. International Society for Optics
and Photonics, 2013.
[8] Y. Chen, J. Bi, and J. Z. Wang.
Miles: Multipleinstance learning via embedded instance selection. Pattern
Analysis and Machine Intelligence, IEEE Transactions on,
28(12):1931–1947, 2006.
[9] Y. Chen and J. Wang. Image categorization by learning and
reasoning with regions. The Journal of Machine Learning
Research, 5:913–939, 2004.

Figure 6. Drop in accuracy at various noise levels for proposed and
MILES on the DR dataset. The lower the value the better.

about 20% of labels are corrupted, the proposed method
still loses only about 5% accuracy and only when about
one-third of the labels are corrupted, the proposed method
loses about 10% accuracy. The average drop in accuracy
for both the proposed method and MILES are compared in
ﬁgure 6. It is clear that MILES and the proposed algorithm
follow the exact same trend. This trend is clearly indicative that the proposed method is as good as MILES and is
often times better, when it comes to sensitivity to labeling
noise. It is noteworthy that MILES is considered the stateof-the-art benchmark for sensitivity to labeling error out of
all MIL methods published and that was one of its core contributions.

5. Conclusion
In this paper, we postulate whether lazy learning ideas
can be carried over from traditional non-parametric methods for supervised learning to a MIL setup. We proposed a
2612

[10] T. Dietterich, R. Lathrop, and T. Lozano-Pérez. Solving the
multiple instance problem with axis-parallel rectangles. Artiﬁcial Intelligence, 89(1):31–71, 1997.
[11] T. et al. Diabretdb0: Evaluation database and methodology
for diabetic retinopathy algorithms. In Technical Report.
[12] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained partbased models. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 32(9):1627–1645, 2010.
[13] Z. Fu, A. Robles-Kelly, and J. Zhou. Milis: Multiple instance
learning with instance selection. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 33(5):958–977,
2011.
[14] M. Goldbaum, N. Katz, S. Chaudhuri, and M. Nelson. Image
understanding for automated retinal diagnosis. In Proceedings of the Annual Symposium on Computer Application in
Medical Care, page 756. American Medical Informatics Association, 1989.
[15] M. Guillaumin, J. Verbeek, and C. Schmid. Multiple instance metric learning from automatically labeled bags of
faces. Computer Vision–ECCV, pages 634–647, 2010.
[16] J. Huang, S. Kumar, M. Mitra, W. Zhu, and R. Zabih. Image
indexing using color correlograms. In Computer Vision and
Pattern Recognition, IEEE Computer Society Conference on,
pages 762–768. IEEE, 1997.
[17] T. Kauppi, V. Kalesnykiene, J. Kamarainen, L. Lensu,
I. Sorri, A. Raninen, R. Voutilainen, H. Uusitalo,
H. Kälviäinen, and J. Pietilä. Diaretdb1 diabetic retinopathy database and evaluation protocol. Proc. Medical Image
Understanding and Analysis (MIUA), pages 61–65, 2007.
[18] Y. Li, D. M. Tax, R. P. Duin, and M. Loog. Multiple-instance
learning as a classiﬁer combining problem. Pattern Recognition, 46(3):865–874, 2013.
[19] O. Maron and T. Lozano-Pérez. A framework for multipleinstance learning. NIPS, pages 570–576, 1998.
[20] O. Maron and A. Ratan. Multiple-instance learning for natural scene classiﬁcation. In IEEE ICML, volume 15, pages
341–349, 1998.
[21] B. McCormick and M. Goldbaum. Stare= structured analysis
of the retina: Image processing of tv fundus image. In del
USA-Japan Workshop on Image Processing, Jet Propulsion
Laboratory, Pasadena, CA, 1975.
[22] G. Quellec, M. Lamard, M. Abràmoff, E. Decencière,
B. Lay, A. Erginay, B. Cochener, and G. Cazuguel. A
multiple-instance learning framework for diabetic retinopathy screening. Medical Image Analysis, 2012.
[23] G. Quellec, M. Lamard, B. Cochener, C. Roux, G. Cazuguel,
E. Decenciere, B. Lay, and P. Massin. A general framework
for detecting diabetic retinopathy lesions in eye fundus images. In Computer-Based Medical Systems (CBMS), 2012
25th International Symposium on, pages 1–6. IEEE, 2012.
[24] R. Rahmani, S. Goldman, H. Zhang, S. Cholleti, and J. Fritts.
Localized content-based image retrieval. IEEE transactions
on pattern analysis and machine intelligence, 30(11):1902,
2008.
[25] R. Venkatesan, P. Chandakkar, B. Li, and H. K. Li. Classiﬁcation of diabetic retinopathy images using multi-class

[26]
[27]
[28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

2613

multiple-instance learning based on color correlogram features. In Engineering in Medicine and Biology Society
(EMBC), 2012 Annual International Conference of the IEEE,
pages 1462–1465. IEEE, 2012.
H. Wang, H. Huang, F. Kamangar, F. Nie, and C. Ding. Maximum margin multi-instance learning. NIPS, 2011.
H. Wang, F. Nie, and H. Huang. Learning instance speciﬁc
distance for multi-instance classiﬁcation. In AAAI, 2011.
H. Wang, F. Nie, and H. Huang. Robust and discriminative
distance for multi-instance learning. In IEEE CVPR, pages
2919–2924. IEEE, 2012.
J. Wang and J. Zucker. Solving the multiple-instance problem: A lazy learning approach. In Proceedings of the Seventeenth International Conference on Machine Learning, pages
1119–1126. Morgan Kaufmann Publishers Inc., 2000.
Q. Wang, L. Si, and D. Zhang. A discriminative datadependent mixture-model approach for multiple instance
learning in image classiﬁcation,. In In Proceedings of the
12th European Conference on Computer Vision (ECCV-12),,
2012.
Z. Wang, S. Gao, and L.-T. Chia. Learning class-to-image
distance via large margin and l1-norm regularization. In
Computer Vision ECCV 2012, pages 230–244. 2012.
D. Wu, J. Bi, and K. Boyer. A min-max framework of cascaded classiﬁer with multiple instance learning for computer
aided diagnosis. In IEEE CVPR, pages 1359–1366. IEEE,
2009.
Y. Xu, J. Zhu, E. Chang, and Z. Tu. Multiple clustered instance learning for histopathology cancer image segmentation, classiﬁcation and clustering. CVPR, IEEE, 2012.
D. Zhang, Y. Liu, L. Si, J. Zhang, and R. Lawrence. Multiple instance learning on structred data. In Twenty-Fifth Annual Conference on Neural Information Processing Systems
(NIPS), 2011.
Q. Zhang and S. Goldman. Em-dd: An improved multipleinstance learning technique. Advances in neural information
processing systems, 14:1073–1080, 2001.
Q. Zhang, S. Goldman, W. Yu, and J. Fritts. Content-based
image retrieval using multiple-instance learning. In Machine
Learning-International Worskshop-Then Conference-, pages
682–689, 2002.

A Four Point Algorithm for Fast Metric Cone
Reconstruction from a Calibrated Image
Jin Zhou and Baoxin Li
Computer Science and Engineering Department
Arizona State University, Tempe, USA
{jinzhou,baoxin.li}@asu.edu

Abstract. This paper presents a four point algorithm for fast metric reconstruction of a cone from a single calibrated image. The algorithm first estimates the
camera’s orientation based on two edge lines and a cross section contour, which
are derived from user-specified four points. With camera’s orientation determined, the cone is reconstructed from the image points. The algorithm enables
fast and accurate modeling of cone objects from images, which is verified by
the experiments.

1 Introduction
With a single image, 3D information might be accurately extracted [5, 7]. As highresolution digital cameras become commodities, the image based modeling techniques have the great potential for commercial applications. A recent example is the
“Photo Match” tool introduced in Google SketchUp 6 [8], which is released in January 2007. The tool supports fast 3D modeling from a single image. However, it does
not support cone modeling and requires two sets of parallel horizontal lines to calibrate the camera first. Cone-like structures, including the special case of cylinder, are
commonly encountered in man-made environments. In this paper, we introduced an
algorithm for metric cone reconstruction from a single calibrated image from just four
points.
Cones are a special kind of Surface of Revolution (SOR) objects. SOR reconstruction from images has been developed in the past. A projective reconstruction algorithm was proposed in [1] for SOR objects. In [2,3], an algorithm purely based on the
silhouettes was proposed, with the result having two ambiguities. The approaches in
[4, 6] can metric reconstruct SOR objects from uncalibrated images. However, both of
them rely on two cross section contours to estimate the camera’s internal matrix as
well as orientation. In reality, cone objects may just have one obvious cross section
contour. In addition, specifying two cross section contours (ellipses in the image) is
tedious. In this paper, we represent an approach for cone reconstruction by using one
cross section contour and two edge lines, which is a weaker requirement.
Although fully automatic tools might be developed based on the edges [6], they are
not appropriate for modeling a complex environment, where the user need the full
control. The intuitive way for the user to model the objects is to specify the control
points or lines. Since a general ellipse has 5 degrees of freedom, a method based on
G. Bebis et al. (Eds.): ISVC 2008, Part II, LNCS 5359, pp. 634–643, 2008.
© Springer-Verlag Berlin Heidelberg 2008

A Four Point Algorithm for Fast Metric Cone Reconstruction from a Calibrated Image

635

two cross sections would need 10 points to specify. It would be very inconvenient for
the user to specify so many points. By exploiting the symmetry property of the SOR
object’s silhouettes, our approach just requires four control points, which makes the
modeling process much faster and easier.
The paper is organized as follows. We first define the problem in Section 2. The
cone reconstruction algorithm is described in Section 3. Section 4 presents the experimental results. At last, conclusions are drawn.

2 Problem Definition
In our method, from the user’s perspective, to create the 3D model of a cone from a
single image, he/she just needs to move 4 control points on the image, as Fig. 1
shows. The first point is the vertex, the second one is on the left edge, the third one is
on the cross section contour that passes the second point, and the last point is anywhere on the other edge.

Fig. 1. Four control points for reconstructing a cone

From these four points, the cone can be reconstructed from the image. To begin
with, we define the coordinate system of the cone as Fig. 2 shows. The cone coordinate system is defined by (O, X, Y, Z), where O is the origin, X, Y and Z are three
axes respectively. The Y axis has the same direction of the cone axis and the Z axis
intersects at the axis. In the coordinate system, the cone model can be represented as

Μ = (c = (0, yc , z0 )T , r , yv )
where c is the center of a cross section circle and r is its radius. yv is the height of the
vertex of the cone. Without losing generality, we can define z0 = 1 which means the
distance of the cone axis is 1. The camera coordinate system is defined by (O, R),
where O is camera’s origin and R is its orientation in the world coordinate system,
which is defined by 3 axes (x, y, z) as shown in Fig. 2. Note the world origin is defined as the camera’s center.
From the definition, the problem can be formally expressed as: to determine the
model parameters (M) and the camera’s orientation (R) from four points (pi) on the
image.

636

J. Zhou and B. Li

Fig. 2. Coordinate system of the camera (O, x, y, z) and the cone (O, X, Y, Z)

3 Cone Reconstruction
If the camera’s coordinate system is the same as the world coordinate system, the
cone can be reconstructed through only one edge line on the image (which will be
shown later). Therefore, the reconstruction problem can be reduced to estimating the
camera’s orientation through the four points on the image. To determine the camera’s
orientation, we exploit two properties of the geometry of the camera and the cone:
Property 1: When the axis of the SOR object lies on the yz plane of the camera, its
apparent contour on the image exhibits symmetry with respect to the center vertical
line. In this case, the camera’s x-axis is the same as the X-axis of the world coordinate system.
Property 2: When the camera’s z-axis is parallel to the axis of the SOR object, the
cross section circle is imaged as a circle. In this case, the camera’s z-axis is on the Yaxis of the world coordinate system.
From these two properties, we designed a two-step algorithm to determine the
camera’s orientation, in which the first step is based on property 1 and the second step
is based on property 2. Before explaining the algorithm, we first introduce the basic
tools that the algorithm will use.
If the camera is rotated by R, a point x’ in the new view and its corresponding point
x in original view is related by
x ' = KRK −1 x

where K is the camera’s calibration matrix [5]. If we normalize the point coordinates
by x̂ = K −1 x and xˆ ' = K −1 x ' , then
xˆ ' = Rxˆ

(1)

Therefore, the orientation can be estimated from normalized coordinates of point
correspondences or other features.
A rotation might be decomposed into three different kinds of basic rotations, which
are rotations around the x, the y and the z-axis respectively. The rotation around the xaxis is

A Four Point Algorithm for Fast Metric Cone Reconstruction from a Calibrated Image

637

0
0 ⎤
⎡1
⎢
Rx (θ ) = ⎢0 cos(θ ) − sin(θ ) ⎥⎥
⎢⎣0 sin(θ ) cos(θ ) ⎥⎦

(2)

The rotation around the y-axis is
⎡cos(θ ) 0 − sin(θ ) ⎤
1
0 ⎥⎥
Ry (θ ) = ⎢⎢ 0
⎢⎣ sin(θ ) 0 cos(θ ) ⎥⎦

(3)

And the rotation around z-axis is
⎡cos(θ ) − sin(θ ) 0 ⎤
Rz (θ ) = ⎢⎢ sin(θ ) cos(θ ) 0 ⎥⎥
⎢⎣ 0
0
1 ⎥⎦

(4)

The algorithm estimates the orientation by sequentially rotating the camera along
different axes.
In Step 1, we virtually rotate the camera such that two edge lines in the new image
are symmetric with respect to the imaged y-axis, as Fig. 4(a) shows. The rotation is
constructed by three steps: 1) rotate around the z-axis so that the vertex is transform to
the imaged y-axis; 2) rotate around the x-axis so that the vertex is transformed to the
origin on the image; 3) rotate around the z-axis so that the two edge lines are symmetric with respect to the imaged y-axis. After rotation, the camera’s coordinates system
is as Fig. 3 (a) shows, where the x-axis is the same as the X-axis and the z-axis passes
through the cone vertex. Formally, the rotation is:
Rs = Rz (θ 3 ) Rx (θ 2 ) Rz (θ1 )

(5)

such that
Rz (θ1 ) K −1 p1 = (0, a, b)T
Rx (θ 2 )( Rz (θ1 ) K −1 p ) = (0, 0, c)T
and θ 3 is determined by computing the average angle of two transformed edge lines.
After the rotation, the points are transformed to pis = Rs K −1 pi , as Fig. 4 (a) shows.
In Step 2, the camera is rotated around the x-axis such that the contour ellipse is
transformed to a circle, as Fig. 4 (a) and (b) shows. Formally,
Rc = Rx (θ )
such that
Rx (θ ) ERx −1 (θ ) = C

(6)

where E is the ellipse of a cross section contour and C is a circle. Since now the ellipse is symmetric with respect to the imaged y-axis, it can be represented as

638

J. Zhou and B. Li

(a)

(b)

Fig. 3. The camera coordinate system after Step 1 (a) and after Step 2 (b)

(a)

(b)

Fig. 4. (a) The resultant image from Step 1. (b) The resultant image from Step 2.

0 ⎤
⎡a 0
E = ⎢⎢ 0 c e / 2 ⎥⎥
⎢⎣ 0 e / 2
f ⎥⎦
which means for any image point (x, y) on the ellipse we have
ax 2 + cy 2 + ey + f = 0

(7)

From equation (6) and (2) we have
0
0 ⎤
⎡a
⎢
C = ⎢ 0 c ' e '/ 2 ⎥⎥
⎣⎢ 0 e '/ 2 f ' ⎦⎥

(8)

c ' = c cos 2 (θ ) − e sin(θ ) cos(θ ) + f sin 2 (θ )

(9)

where

Since a circle has the constraint that c ' = a , we have
c cos 2 (θ ) − e sin(θ ) cos(θ ) + f sin 2 (θ ) = a
⇒ ( f − a)t 2 − et + (c − a) = 0 where t = tan(θ )

(10)

A Four Point Algorithm for Fast Metric Cone Reconstruction from a Calibrated Image

639

Equation 10 provides us two solutions. We can remove the ambiguity by using the
constraint that the circle center lies in front of the camera.
To get the rotation angle, we need to first fit the ellipse, i.e. determine the parameters of E. From Fig. 4 (a), we estimate the ellipse from three points: the vertex p1s ,
two contour points p2s and p3s . A line tangent to a conic on a point gives 2 constraints
l = Ep2s where l = p1s × p2s

(11)

A point on a conic gives one constraint:
p3sT Ep3s = 0

(12)

Since E is homogeneous, it has only 3 degrees of freedom and we can simply set f
= 1. Thus E can be determined from Equation 11 and 12.
After Step 2, the camera coordinate system is as Fig. 3 (b) shows. It is easy to rotate the camera again to the world coordinate system by rotating around the x-axis
with −π / 2 . Combining all the rotations, the camera’s orientation can be determined
as
R −1 = Rx (−π / 2) Rc Rs

(13)

Now we need to determine the model parameters. With R is known, the vertex’s 3d
position is
Pv = λ R −1 K −1 p1
Combined with another constraint that the depth is assumed as 1, the position is
uniquely determined. For a circle, it can be determined from the result of Step 2.
From equation 8, the coordinates of the circle are

pc = λ (0, −e '/ 2a,1)T , r = λ (e '2 − 4af ) / 4a 2

(14)

After applying Rx (−π / 2) , we can get the coordinates in the world coordinate system, thus
Pc = λ (0, −1, −e '/ 2a )T , r = λ (e '2 − 4af ) / 4a 2

Fig. 5. Five points to reconstruct a cone section

640

J. Zhou and B. Li

Combined with the constraint that the depth is 1, we can determine the parameters
in M as
yc = 2a / e ', r = (e '2 − 4af ) / e '2 .
Now all the parameters of a cone are determined. In practice, we use five points
that are able to handle cone sections as Fig. 5 shows.

4 Experiments
We first use a detailed sample to illustrate the performance of the proposed method.
Fig. 6 demonstrates the modeling algorithm step by step. The green box in the images
shows the size of the original image. In Fig. 6 (a), we manually pick five points on the
edge of the cup. The red lines are edges generated by four edge points and they intersect at the vertex of the cone, which is shown as a blue circle. In (b), we compute
Rz (θ1 ) in Equation (5) so that the vertex is transformed to the y-axis. In (c), we compute Rx (θ 2 ) so that the vertex is transformed to the origin. In (d), we compute Rz (θ3 )
so that the edge lines are symmetric with respect to the y-axis. An ellipse is fit to the
bottom and the result is shown in (e). In (f), we compute Rc so that the ellipse is
transformed to a circle. After apply Rx (−π / 2) to (f), we obtained the standard view
of the cone and the vertex coordinates are determined. (h) shows the 3D mesh of the
cone.
We project the image onto the model to get the texture. Fig. 7 shows the textured
model of the cup. To measure the reconstruction error, we compare the modeling
results to the real object. The bottom diameter of the real cup is 5.8cm and the top
diameter is 8.5cm, the height is 11.0cm. In the modeling result, the bottom radius is
0.061, the bottom height -0.40, the top height -0.17, and the top radius 0.087. The real
ratio between the bottom radius and the cup height is 0.264 and that ratio in the modeling results is 0.265. The real ratio between bottom radius and the top radius is 0.682
and that ratio in the modeling results is 0.701. The comparison shows that the modeling is very accurate.
The second example (Fig. 8) shows a more complex case, where two cups are
placed on a table with one being partially occluded by the other. Fig. 8 (b) shows the
resultant model which contains three objects, i.e. two cups and the desktop. The desktop is reconstructed based on the camera’s orientation. The two cups are scaled so that
they are lying on the desktop.
In practice, there may be accuracy issues when a user clicks a point on an image,
and the errors may affect the final reconstructed model. We analyze the modeling
accuracy using Monte Carlo simulation, in which Gaussian noise (with standard deviation of 1 pixel) is added to the ground truth image points before the modeling
process. We test the modeling accuracy under different orientation setup. For each
orientation setup, we run the simulation for 1000 times. There are numerous configurations for cone’s shape, orientation and position in image. In simulation, we put the
cone in the center of the image. Since the world coordinate system is defined to be
free to rotate around the y-axis, we only measure the modeling error as to different

A Four Point Algorithm for Fast Metric Cone Reconstruction from a Calibrated Image

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)
Fig. 6. Cones modeling

641

642

J. Zhou and B. Li

Fig. 7. 3D model of the cup

(a)

(b)
Fig. 8. Two cups example

(a)

(b)

Fig. 9. Accuracy of modeling. (a) Normalized mean values of cone height and cap radius (b)
Normalized standard deviation of cone height and cap radius.

rotations around x axis and z axis. Because the Euclidean reconstruction is up to a
scale, we use to relative size to measure modeling error. We first scale the reconstructed model such that the bottom radius is equal to one, then we measure the error
of cone height and cap radius. Further, we divide the reconstructed parameters by

A Four Point Algorithm for Fast Metric Cone Reconstruction from a Calibrated Image

643

ground truth such that all ideal result is one. We call the results “normalized value”.
To illustrate the significance of errors, we use mean and standard deviation, as Fig. 9
shows. From the figure, we can found that the rotation around z axis doesn’t have
much influence on modeling accuracy, both mean error and standard deviation is
small, say, less than 1 percent. However, rotation around x axis has significant impact
on modeling accuracy: as the rotation is about 50 degrees, the mean error is about 10
percent and the standard deviation is about 15 percent. Actually, an extreme rotation
around x axis leads to the degenerate case:
Degenerate Case: The imaged vertex of a cone lies inside the imaged cross section
contour, and thus it is impossible to find the line passing through the imaged vextex
and tangent to the cross section contour. An example is when an upright cone is imaged from the overhead view.

5 Conclusions
In this paper we presented a four point algorithm for fast modeling of cones from a
single image. Experiments demonstrate the accuracy and simplicity of our approach.

References
1. Utcke, S., Zisserman, A.: Projective Reconstruction of Surfaces of Revolution. In: Michaelis, B., Krell, G. (eds.) DAGM 2003. LNCS, vol. 2781, pp. 265–272. Springer, Heidelberg
(2003)
2. Wong, K.-Y.K., Cipolla, R.: Structure and Motion from Silhouettes. In: Proc. ICCV, pp.
217–222 (2001)
3. Wong, K.-Y.K., Mendonca, P.R.S., Cipolla, R.: Reconstruction of Surfaces of Revolution
from Single Uncalibrated Views. In: Proc. BMVC, pp. 93–102 (2002)
4. Colombo, C., Bimbo, A.D., Pernici, F.: Metric 3D Reconstruction and Texture Acquisition
of Surfaces of Revolution from a Single Un-calibrated View. PAMI 27(1), 99–114 (2005)
5. Hartley, R.I., Zisserman, A.: Multiple View Geometry in Computer Vision. Cambridge
Univ. Press, Cambridge (2003)
6. Wu, Y., Wang, G., Wu, F., Hu, Z.: Euclidean Reconstruction of a Circular Truncated Cone
only From its Uncalibrated Contours. In: IVC, pp. 810–818 (2006)
7. Criminsi, A., Reid, I., Zisserman, A.: Single View Metrology. In: ICCV, pp. 434–442
(1999)
8. Google SketchUp (2007), http://www.sketchup.com/

YouTubeCat: Learning to Categorize Wild Web Videos
Zheshen Wang1 , Ming Zhao2 , Yang Song2 , Sanjiv Kumar3 , and Baoxin Li1
1

Computer Science and Engineering, Arizona State University, Tempe, AZ 85281, USA
2
Google Research, Mountain View, CA 94043, USA
3
Google Research, New York, NY 10011, USA
1

2,3

{zheshen.wang, baoxin.li}@asu.edu
{mingzhao, yangsong, sanjivk}@google.com

Abstract
Automatic categorization of videos in a Web-scale unconstrained collection such as YouTube is a challenging
task. A key issue is how to build an effective training set in
the presence of missing, sparse or noisy labels. We propose
to achieve this by first manually creating a small labeled
set and then extending it using additional sources such as
related videos, searched videos, and text-based webpages.
The data from such disparate sources has different properties and labeling quality, and thus fusing them in a coherent
fashion is another practical challenge. We propose a fusion
framework in which each data source is first combined with
the manually-labeled set independently. Then, using the hierarchical taxonomy of the categories, a Conditional Random Field (CRF) based fusion strategy is designed. Based
on the final fused classifier, category labels are predicted
for the new videos. Extensive experiments on about 80K
videos from 29 most frequent categories in YouTube show
the effectiveness of the proposed method for categorizing
large-scale wild Web videos1 .

1. Introduction
On-line services for archiving and sharing personal
videos such as YouTube have become quite popular in recent years. Automatic categorization of videos is important
for indexing and search purposes. However, it is a very challenging task for such a large corpus of practically unconstrained (wild Web) videos. A lot of efforts have been devoted to video analysis in the past, but most existing works
use very limited number of videos or focus on specific domains such as news, sports etc. Due to practically unbounded diversity of Web videos in both content and quality (as illustrated in Figure 1), analysis of such data is much
1 This

work was performed when the first author interned at Google.

978-1-4244-6985-7/10/$26.00 ©2010 IEEE

Figure 1. Examples of wild YouTube videos showing extremely
diverse visual content.

more challenging than relatively clean videos expected by
most existing techniques. A recent study by Zanetti et al.
showed that most existing algorithms did not perform well
on general Web videos [25]. It also pointed out that one of
the major challenges in Web video categorization is the lack
of sufficient training data. Manually labeling videos is both
time-consuming and labor intensive – on one hand one has
to watch part of a video before (s)he can suggest labels; on
the other, web videos are extremely diverse in nature, thus
even for human experts, summarizing the video content by
using a few keywords is not an easy task.
In this paper, we propose a novel approach that combines multiple data sources for wild YouTube video categorization. Starting from a small number of manually
labeled samples (as few as 50 per category), we expand
the training set by propagating labels to their co-watched
videos, collecting data by using internet video search engines (such as Google video search), and even incorporating data from other domains (e.g., text-based webpages).
These additional data sources are first pairwise combined
with manually-labeled data and a classification model is
trained for each combination. For fusing these trained mod-

879

els, we propose a CRF-based tree-DRF fusion approach,
which views the taxonomy tree as a random field. Each
node (i.e. a category) is associated with a binary label and
the output likelihoods of the trained models (applied on the
training data) are used as local observations for the nodes.
Unlike a traditional fusion strategy that treats each category
independently, tree-DRF makes the final labeling decision
as a whole by explicitly taking the hierarchical relationships
among the categories into consideration. This is crucial to
achieve good performance since the data from additional
sources is usually quite noisy. The hierarchical relationships among categories provides powerful context for alleviating the noise. Results from extensive experiments on
80K YouTube videos demonstrate that the proposed solution outperforms existing methods that either use just a single data source or traditional data fusion strategy.
The main contributions of this work can be summarized
as follows: First, to the best of our knowledge, this is the
first work that deals with categorization of unconstrained
Web videos at such a large scale. Second, we propose a
novel approach for integrating data from multiple disparate
sources for classification given insufficient training data. Finally, we introduce a tree-DRF based fusion strategy that
exploits the hierarchical taxonomy over categories and effectively deals with noise in multiple data sources. It significantly outperforms other commonly used fusion strategies
based on SVM and iterative co-training [2, 3, 8].
The rest of the paper is organized as follows. We first
review the related literature in Section 2 followed by the
description of multiple data sources we use in Section 3.
The proposed solution with pairwise data combination and
tree-DRF based fusion strategy is presented in Section 4.
Extensive experimental results, comparisons and analysis
are reported in Section 5. We conclude in Section 6 with a
brief discussion on future work.

data and categories were not reported in their work.
Zanetti et al. explored existing video classification methods on about 3000 YouTube videos in their recent work
[25]. They pointed out that a major difficulty in Web video
analysis is the lack of enough labeled training data. Semisupervised machine learning approaches [27] are useful for
expanding training data in general. However, graph-based
methods used commonly for semi-supervised learning e.g.,
[28] and semi-supervised SVM [1] are inefficient for large
amounts of data with high-dimensional features. Popular
co-training/self-training approaches [2, 3, 8] are also typically expensive and their performance is quite sensitive to
the amount and quality of the initial training set.
Another possible way of collecting more training data
is to make use of data from other sources including different domains. It is worth noting that combining multiple
data sources is more challenging than combining multiple
views of the same data [2, 3, 8], since properties of different data sources are typically more diverse. Multiple data
sources can be combined with either early fusion or late fusion strategies [22]. Typically, early fusion assumes that
all the features are available for each video, which is not
valid in our case (e.g. webpage data has only text features).
In late fusion, classifier models are first trained separately;
then the trained models are applied to the training set. At
the fusion stage, obtained likelihoods from different models
are concatenated for each sample and used as a feature vector. Another round of training is then carried out on the new
’features’. Traditional fusion methods are based on regular learning algorithms (such as SVM, AdaBoost), which
treat each category independently. On the contrary, given
a hierarchical taxonomy over categories, it is desirable to
exploit such relationships to achieve robust classification.
In this paper, we propose tree-DRF to handle the category
structure while doing late fusion and empirically show the
benefits of such approach.

2. Related Work
Compared to image analysis, research on video analysis has been relatively recent. Most existing approaches
are either limited to some specific domains (e.g. movies
[4, 12], TV videos [5, 21, 24] etc.) or focus on certain
predefined content such as human face [5, 19] and human activities [14]. However, large scale categorization of
wild Web videos still remains an unsolved problem. The
works of Schindler et al. [20], VideoMule [17] and Zanetti
et al. [25] are among the initial efforts in this direction.
Schindler et al. tried video categorization on 1500 user uploaded videos from 15 categories using bag-of-words representation. However, the classification performance is very
poor on this general video set (best classification accuracy
is 26.9%). Ramachandran et al. proposed VideoMule, a
consensus learning approach to multi-label YouTube videos
classification using YouTube categories. Specific amount of

3. Multiple data sources
As mentioned earlier, lack of labeled training data is
a main bottleneck for general Web video categorization.
To alleviate this problem, we first manually labeled 4345
videos from all the 29 categories as initial seeds. This set is
further expanded by including samples from related videos,
searched videos and cross-domain labeled data (i.e. text
webpages), as illustrated in Figure 2. Details of each data
source are given below.

3.1. Manually-labeled data
To collect the initial seeds for training, we first build a
category taxonomy with the help of professional linguists.
About 1000 categories are defined using a hierarchical tree
of 5 vertical levels (Depth-0 to Depth-4 from top to bottom,

880

3.3. Searched data

Figure 2. Multiple data sources for YouTube videos including a
small set of manually labeled data, related (e.g. co-watched video
data), searched data collected by using a video search engine with
categories as queries, and cross-domain data (e.g. webpages)
which are labeled with the same taxonomy structure.

Depth-0 is the root). Randomly selected YouTube videos
that have been viewed more than a certain number of times
are labeled by professionally-trained human experts based
on the established taxonomy. Each video is labeled from
Depth-0 to the deepest depth it can go. For example, if a
video is labeled as Pop Music, it must be associated with label Music & Audio and Art & Entertainment as well. Note
that this is a general taxonomy instead of being designed
for YouTube videos specifically. Thus, it is not surprising
that the distribution of manually-labeled videos over all categories is extremely unbalanced. For example, the Art &
Entertainment category contains close to 90% of all the labeled videos, and categories such as Agriculture & Forestry
have only a few videos. In fact, such imbalance reflects the
real distribution of videos in the entire YouTube corpus. In
this paper, we work on 29 categories that had a reasonable
amount of manually-labeled samples, i.e., more than 200 for
Depth-1 categories and more than 100 for Depth-2 to 4 categories. Manually-labeled samples from these 29 categories
(4345 samples in total) cover close to 80% of all the data we
labeled, roughly implying that the categories we are working with cover ∼80% of all possible videos on YouTube. To
the best of our knowledge, this is the first paper which deals
with general Web video classification on such diverse categories. In our experiments, 50% randomly selected samples
are used as initial seeds for training (denoted as “M”) and
the remaining 50% are used for testing.

3.2. Related (Co-watched) data
To increase the training samples for each category, we
considered co-watched videos, i.e., the next videos that
users watched after watching the current video. We empirically noticed if a video is co-watched more than 100 times
with a certain video, they tend to have the same category.
Of course, such labels can be noisy but our tree-DRF based
late fusion method is able to handle such noise robustly. So,
in our experiments, co-watched videos (denoted as “R”) of
all the initial seed videos with co-watch counts larger than
100 (3277 video in total) are collected to assist training.

Another possibility for expanding the training set is by
searching for videos using online video search engines using a category label as a text query. For example, returned
videos by submitting a query “soccer” may be used as training samples for the “soccer” category. Constrained by the
quality of existing search engines, searched videos may be
noisy. In our work, we keep about top 1000 videos returned
for each category. Since the categories form a hierarchical
structure, the videos returned for categories at lower levels
are included for their ancestors as well. Querying Google
video search gave us a set of about 71,029 videos (denoted
as “S”).

3.4. Cross-domain labeled data
Compared to video labeling, assigning labels to other
types of data (e.g. text-based webpages) is usually easier.
Although such data comes from a completely different domain, it can be helpful for video classification as long as the
samples are labeled using the same taxonomy. This is because we also use text-based features to describe each video
as explained in Section 4.1. We collected 73,375 manuallylabeled webpages (denoted as “W”) as one of the additional
data sources in our experiments.

4. Learning from multiple data sources
In Section 3, in addition to the manually-labeled data, we
introduced several auxiliary sources which may be useful
for boosting the video classification accuracy. The main
challenge is how to make use of such diverse set of data
with different properties (e.g., video content features are not
available for webpages) and labeling quality (e.g., labels of
searched and co-watched data are fairly noisy).
In this paper, we propose a general framework to integrating data from mixed sources. As illustrated in Figure 3, each auxiliary data source is first pairwise combined
with the manually-labeled training set. Initial classifiers
are trained on each such pair. For each pair, two separate classifiers are learned, one with text-based and another with content-based features. For example, in Figure
3, MSc is a content-based and MSt is a text-based model
for the combination of manually-labeled data and searched
data. Trained models are then fused using a tree-DRF fusion
strategy. Different from traditional methods that fuse models for each category independently, the proposed tree-DRF
incorporates the hierarchical taxonomy structure exploring
the category relationships effectively.
Next we introduce the features used for training individual classifiers followed by the description of our tree-DRF
fusion method.

881

Instead of using the wavelet coefficients directly, we take
the maximum, minimum, mean and variance of them as the
features in each scale. This multi-scale feature extraction is
applied to all our audio and video content features except
the histogram of local features [7].
Note that features are not the main contribution of this
work. Due to space limitation, we skip the details of the
features and refer the reader to the respective references.
For fair comparisons, all the experimental results reported
in this work are obtained based on the same set of features.
Figure 3. General framework of the proposed solution: Additional
data sources are first combined with manually-labeled data independently and classifier models are trained based on either text
or content features for each combination. Individual classifier are
further fused to form the final classifier M .

4.1. Features
It is well known that designing good features is perhaps
the most critical part of any successful classification approach. To capture the attributes of wild Web videos as
completely as possible, state-of-the-art text and video content features are utilized in our experiments as briefly summarized below.
Text features: For each video, the text words from title, description and keywords are extracted. Then, all these
words are weighted to generate text clusters. The text clusters are obtained from Noisy-Or Bayesian Networks [16],
where all the words are leaf nodes in the network and all
the clusters are internal nodes. An edge from an internal
node to a leaf node means the word in the leaf node belongs
to that cluster. The weight of the edge means how strongly
the word belongs to that cluster.
Video content features: color histogram computed using
hue and saturation in HSV color space, color motion defined
as cosine distance of color histograms between two consecutive frames, skin color features as defined in [9], edge
features using edges detected by Canny edge detector in regions of interest, line features using lines detected by probabilistic Hough Transform, histogram of local features using
Laplacian-of-Gaussian (LoG) and SIFT [15], histogram of
textons [13], entropy features for each frame using normalized intensity histogram and entropy differences for multiple frames, face features such as number of faces, size and
aspect ratio of largest face region (faces are detected by an
extension of AdaBoost classifier [23]), shot boundary detection based features using difference of color histograms
from consecutive frames [26], audio features such as audio volume and 32-bin spectrogram in a fixed time frame
centered at the corresponding video frame, adult content
features based on a boosting-based classifier in addition to
frame-based adult-content features [18]. We extract the audio and visual features in the same time interval. Then, a 1D
Haar wavelet decomposition is applied to them at 8 scales.

4.2. CRF-based fusion strategy
Conditional Random Fields (CRFs) are graph-based
models that are popularly used for labeling structured data
such as text [11] and were introduced in computer vision by
[10]. In this work, we use outputs of discriminative classifiers to model the potentials in CRFs as suggested in Discriminative Random Field (DRF) formulation in [10]. Following the notation in [10], we denote the observations as y
and the corresponding labels as x. According to CRFs, the
conditional distribution over labels given the observations
is defined as a Gibbs field:
p(x|y) =

XX
1 X
(
Ai (xi , y)+
Iij (xi , xj , y)), (1)
Z
i∈S

i∈S j∈Ni

where S is the set of all the graph nodes, Ni is the set of
neighbors of node i, and Z is a normalizing constant called
partition function. Terms Ai and Iij are the unary and pairwise potentials sometimes referred to as association potential and interaction potential respectively [10].

4.3. Tree-DRF
As discussed earlier, in this work we use multiple data
sources that are combined by a late fusion step. We want a
fusion strategy that can combine the classifier outputs from
different sources while respecting the taxonomy over categories. The DRF framework described above gives a natural way of achieving that. Formally, Ai learns to fuse the
outputs of independent classifiers while Iij enforces the category relationships defined by the hierarchical taxonomy.
In [10], DRF is used for image classification, in which
a graph is built on image entities, i.e., pixels or blocks. On
the contrary, in our case, the graph is defined over the hierarchical taxonomy (i.e., a tree over categories) and a node
represents a category. Each node i is associated with a binary label variable xi , i.e., xi ∈ {−1, 1} implying whether
ith category label should be assigned to the input video or
not. The scores from different classifiers for the ith category on a given video are concatenated in a feature vector,
which serve as the observation yi . Figure 4 illustrates the
proposed tree-DRF.

882

5. Experiments and results
In order to verify the effectiveness of the proposed solution, we performed extensive experiments with about 80K
YouTube videos and about 70K webpages. We first introduce the experimental data and settings in the next section
followed by a brief description of the evaluation metric.
Figure 4. Late fusion strategy based on tree-DRF. For each input
video, a tree-structure over categories is defined. The binary label
at the ith node (xi ) represents whether that video should be assigned the category label Ci . The observation vector (yi ) is simply
the concatenation of classifier scores on the video for that category.

Following [10], association potential is defined as,
Ai (xi , y) = log

1
,
1 + exp(−xi wTi hi (y))

(2)

where wi is a parameter vector and hi (y) is a feature vector
at site i. Following [10], we define hi (y) to include the
classifier scores and their quadratic combinations.
Note that unlike the homogeneous form used in [10], the
association potential in our tree-DRF model is inhomogeneous. There is a separate association parameter w for each
node. The reason is that since a different set of classifiers
is learned for each category (i.e, a node), forcing the weight
vectors defining combinations of such disparate sets of classifiers to be the same for all the nodes is too harsh. Thus,
we allow the model to chose a different weight vector for
each category. Of course, it leads to more parameters in the
model but since our graph is fairly small (just 29 nodes), and
the size of observation vector, i.e., the number of classifiers,
is also small, the computational overhead was negligible.
Moreover, overfitting is also not a concern since we have
enough training data for such small number of parameters.
The interaction potential in tree-DRF is defined as,
Iij (xi , xj , y) = xi xj v T µij (y), j ∈ Ni ,

(3)

where v are the model parameters and µij (y) is a pairwise
feature vector for nodes i and j. In this work, we only explored data-independent smoothing by forcing µij (y) to be
a constant. Similarly, the parameter v was kept to be the
same for all the node pairs. One can easily relax this to
allow directional (anisotropic) interactions between parents
and children which can provide more powerful directional
smoothing. We plan to explore this in the future.
We used the standard maximum likelihood method for
parameter learning in tree-DRF. Since the graph structure is
a tree, exact unary and pairwise marginals were computed
using Belief Propagation (BP). For inference, we used sitewise Maximum Posterior Marginal (MPM), again using BP.
Results of tree-DRF fusion and comparisons to regular fusion strategy based on SVM and Co-training are presented
in Section 5.

5.1. Experimental data and setting
As described in Section 3, four different data sources and
29 major categories are used in our experiments. The categories followed by their path in the taxonomy tree are: “Arts
& Entertainment” (1), “News” (2), “People & Society” (3),
“Sports” (4), “Celebrities & Entertainment News” (1, 5),
“Comics & Animation” (1, 6), “Events and Listings” (1, 7),
“Humor” (1, 8), “Movies” (1, 9), “Music & Audio” (1, 10),
“Offbeat” (1, 11), “Performing Arts” (1, 12), “TV & Video”
(1, 13), “Team Sports” (4, 14), “Anime & Manga” (1, 6, 15),
“Cartoons” (1, 6, 16), “Concerts & Music Festivals” (1, 7,
17), “Dance & Electronic Music” (1, 10, 18), “Music Reference” (1, 10, 19), “Pop Music” (1, 10, 20), “Rock Music”
(1, 10, 21), “Urban & Hip-Hop” (1, 10, 22), “World Music”
(1, 20, 23), “TV Programs” (1, 13, 24), “Soccer” (4, 14, 25),
“Song Lyrics & Tabs” (1, 10, 19, 26), “Rap & Hip-Hop” (1,
10, 22, 27), “Soul & R&B” (1, 10, 22, 28), and “TV Reality
Shows” (1, 13, 24, 29).
In our experiments, binary classifiers are trained for each
category respectively. Content features and text features are
trained separately by using AdaBoost and SVM, respectively. LibLinear [6] is used to train SVMs when training
samples exceed 10K. Trained models are then integrated
using regular SVM based late fusion strategy [22]. Since
webpage data has only text features (no content features),
only a single model is learned for this set. The training data
from two sources (i.e., manually-labeled data plus one additional data source) is combined before training the classifiers. After all the data sources are leveraged, fusion is
performed for content and text features for three pairwise
combinations, represented by five individual classifiers. In
the training process, negative training samples for each category are randomly selected from other categories with a
negative-positive ratio of 3:1.

5.2. Evaluation metrics
While testing, since binary classifiers are trained for each
category, each test sample receives 29 classification decisions (either “yes” or “no”). Multiple labels for a single
sample are allowed. As the category labels form a taxonomy structure, predicted categories/labels are also propagated to their ancestors as done while generating groundtruth labels for the training data. For example, if a test sample has a ground-truth label “Art & Entertainment” / “TV
& Video” / “TV Programs”, it is treated as a true positive

883

Table 1. Classification accuracy of each data source, including
manually labeled data (M), related data (R), searched data (S) and
webpage data (W). Webpage data achieved the best performance
except for Depth-2.

F-score
M
R
S
W

Depth-1
0.80
0.74
0.73
0.84

Depth-2
0.60
0.53
0.51
0.54

Depth-3
0.45
0.37
0.37
0.48

Depth-4
0.41
0.34
0.31
0.45

sample for “Art & Entertainment” category if it is classified
by any of these three classifiers. For the quantitative evaluation, we compute Precision, Recall and F-score. To perform
aggregate assessment of the classification performance, we
also compute F-scores for each depth level of the taxonomy.

Table 2. Classification accuracy of each combination of manuallylabeled data with one additional data source. The combination
with related data achieves significant improvements over just using
the related data and even outperforms using only manually-labeled
data. But the later observation is not true for the other two cases
(i.e. combination with searched data or webpage data).

F-score
M+R
M+S
M+W

Depth-2
0.63
0.57
0.55

Depth-3
0.47
0.43
0.45

Depth-4
0.49
0.37
0.39

Table 3. Classification accuracy of fusing pairwise combinations
of data using different fusion strategies. The proposed tree-DRF
approach outperforms any single data source or their pairwise
combinations. It is also superior to the traditional SVM fusion
strategy with the same features.

5.3. Results and analysis
The objective of the proposed approach is to improve
video classification performance by making use of data
from multiple sources of varied quality. Table 1 lists classification accuracy of each data source (due to space limitation, we only show F-score in all tables and figures). Performance with just the related videos (R) or the searched
videos (S) is much worse than that from manually-labeled
data (M). It shows that neither related videos or searched
videos are sufficient for training a reliable classifier. Webpage data (W) obtained from a completely different domain,
which does not even contain video content, works better
than manually-labeled data for most taxonomy depths. This
is possible since even noisy text based features for videos
are usually more reliable than video content features.
In order to achieve better results, we combine each of
the additional data sources pairwise with manually-labeled
training data. As shown in Table 2, for related video source,
pairwise combination achieves significant improvements
over just using related videos and even better than training
on manually-labeled data. For the searched videos, performance of pairwise combination is also better than that for
just the searched data, but worse than that of the manuallylabeled data. In terms of the webpage data, pairwise combination is not always superior to the single sources. Overall,
there are two observations: 1) Pairwise combination with
manually-labeled data can improve classification accuracy
of any single additional source in most cases; 2) Introducing additional data sources by simply merging them with
the manually-labeled data does not guarantee improvement
for all cases over the baseline configuration, i.e., using just
the manually-labeled data for training.
Next, we fuse the single classifier models trained from
pairwise combinations to further boost the classification
performance. First row of Table 3 shows the results of using regular SVM late fusion strategy. Compared to the best

Depth-1
0.86
0.78
0.84

F-score

Depth-1

Depth-2

Depth-3

Depth-4

All, SVM
All, Tree-DRF
M+R, Tree-DRF

0.84
0.87
0.85

0.65
0.72
0.66

0.46
0.57
0.48

0.49
0.52
0.45

cases in Table 2, fusing all data sources does not achieve any
obvious improvement (for Depth-1 and Depth-3, results are
even worse). It is because, for SVM, when the feature dimension increases but not the amount of training data, the
test performance may degenerate due to overfitting. This
observation underscores our previous assertion that an inappropriate fusion strategy for adding unreliable data sources
may even harm the classification accuracy.
Results of the proposed tree-DRF fusion strategy are reported in Table 3-second row. For all taxonomy depths,
tree-DRF outperforms regular SVM fusion. Especially for
Depth-2 and Depth-3, in which the categories can benefit from both parent categories and child categories, it
achieves 0.07 (11%) and 0.11 (24%) improvements in Fscores. Compared to the baseline performance (Table 1first row), it gains 0.07 (9%), 0.12 (20%), 0.12 (27%), 0.11
(27%) F-score improvements for Depth-1 to Depth-4 respectively. Such significant improvements are due to the
taxonomy tree based learning of tree-DRF. In other words,
since interactions between parent and child nodes are considered, noise in the additional data sources can be largely
filtered. This is because useful information is typically consistent for neighboring nodes and thus can be emphasized
by the interaction potential in tree-DRF.
For analyzing the effectiveness of including additional
data sources, we applied tree-DRF on the pair of manuallylabeled data and related data (which gave the best results
among all pairwise combinations with regular fusion of
content models and text models) in the third row of Table 3.
Compared to tree-DRF on all data (second row in Table 3),
results are worse, which demonstrates the gain from multiple data sources by using tree-DRF. For easy comparison,

884

accuracies from all experiments are summarized in Figure
5.
To analyze the results for individual categories, we illustrate F-scores for the baseline method (i.e., using only
manually-labeled data for training), and SVM and treeDRF based fusion with all data sources in Figure 6. For
most of the categories, tree-DRF outperforms the other two
methods, especially for the categories with small amount of
training samples but relatively large number of neighbors.
In addition to SVM and tree-DRF based fusion, we also
conducted experiments with co-training on different combinations of the four data sources with different settings (e.g.
by varying the number and weights of new training samples added in each iteration, and the stopping criteria). In
the best case, F-scores for Depth-1 to Depth-4 were 0.82,
0.61, 0.44 and 0.40 respectively, which are much lower than
the proposed tree-DRF method and even lower than regular
SVM fusion strategy.
Regarding computational complexity of tree-DRF, since
the graph is built on the taxonomy, it results in a very small
graph having just 29 nodes connected with very sparse
edges. Also, since the outputs of individual classifiers are
used as features, it leads to very low-dimensional features.
Hence, overall the tree-DRF is extremely fast in training as
well as testing.

6. Conclusion and future work
In this paper, we proposed a novel solution to wild web
video categorization on a large-scale dataset (more than 80
thousand YouTube videos). Our approach provides an effective way of integrating data from diverse sources, which
largely alleviates a major problem of lack of labeled training data for general web video classification. Tree-DRF
was proposed for fusing models trained from individual data
sources when combined with small amount of manuallylabeled data in a pairwise fashion. Compared to traditional
fusion strategies, the proposed tree-DRF takes the taxonomy tree of category labels into account, resulting in significant improvement in classification performance. Experimental results on a large-scale YouTube dataset show
that the proposed approach is effective for categorizing wild
videos on the Web.
Currently we only consider undirected relationships between parent and child categories in tree-DRF. More sophisticated anisotropic formulations of interaction potential
for parent or child neighbors, and siblings may further improve the labeling performance. In addition, it is also possible to make use of unsupervised learning methods (e.g.
clustering) for assigning weights to noisy labeled samples
and adjusting their contributions accordingly while training
classifiers. Integrating an iterative co-training framework
of incrementally adding additional unlabeled data is also a
possible way of further expanding the training data set and

improving the classification performance.

References
[1] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework for learning from labeled
and unlabeled examples. Journal of Machine Learning Research, 7:2399–2434, 2006.
[2] A. Blum and T. Mitchell. Combining labeled and unlabeled
data with co-training. In Proc. of Workshop on Computational Learning Theory, 1998.
[3] C. M. Christoudias, R. Urtasun, A. Kapoor, and T. Darrell.
Co-training with noisy perceptual observations. In Proc. of
CVPR, 2009.
[4] O. Duchenne, I. Laptev, J. Sivic, F. Bach, and J. Ponce. Automatic annotation of human actions in video. In Proc. of
ICCV, 2009.
[5] M. Everingham, J. Sivic, and A. Zisserman. Hello! my name
is... buffy automatic naming of characters in tv video. In
Proc. of BMVC, 2006.
[6] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.J. Lin. Liblinear: A library for large linear classification.
Journal of Machine Learning Research, 9:1871–1874, 2008.
[7] U. Gargi and J. Yagnik. Solving the label–resolution problem
in supervised video content classification. In Proc. of ACM
Multimedia Information Retrieval, 2008.
[8] S. Gupta, J. Kim, K. Grauman, and R. Mooney. Watch, listen
& learn: Co-training on captioned images and videos. In
Proc. of ECML, 2008.
[9] M. J. Jones and J. M. Rehg. Statistical color models with
application to skin detection. IJCV, 46(1):81–96, 2002.
[10] S. Kumar and M. Hebert. Discriminative fields for modeling
spatial dependencies in natural images. In Proc. of NIPS,
2003.
[11] J. D. Lafferty, A. McCallum, and F. C. N. Pereira. Conditional random fields: Probabilistic models for segmenting
and labeling sequence data. In Proc. of ICML, 2001.
[12] I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld.
Learning realistic human actions from movies. In Proc. of
CVPR, 2008.
[13] T. Leung and J. Malik. Representing and recognizing the
visual appearance of materials using three-dimensional textons. IJCV, 43(1):29–44, 2001.
[14] J. Liu, J. Luo, and M. Shah. Recognizing realistic actions
from videos. In Proc. of CVPR, 2009.
[15] D. G. Lowe. Distinctive image features from scale-invariant
keypoints. IJCV, 60(2):91–110, 2004.
[16] R. E. Neapolitan. Learning Bayesian Networks. PrenticeHall, Inc., Upper Saddle River, NJ, USA, 2003.
[17] C. Ramachandran, R. Malik, X. Jin, J. Gao, K. Nahrstedt,
and J. Han. Videomule: a consensus learning approach to
multi-label classification from noisy user-generated videos.
In Proc. of ACM MM, 2009.
[18] H. A. Rowley, Y. Jing, and S. Baluja. Large scale imagebased adult-content filtering. In Proc. of VISAPP, 2006.
[19] M. E. Sargin, H. Aradhye, P. J. Moreno, and M. Zhao. Audiovisual celebrity recognition in unconstrained web videos.
In Proc. of ICASSP, 2009.

885

Figure 5. Comparison of classification accuracies from different data sources and combinations. Tree-DRF with all pairwise data combinations achieved the best performance. M: Manually-labeled data, R: Related Videos, S: Searched Videos, W: Webpage data.

Figure 6. F-scores of 29 categories on manually-labeled data (M), all data with SVM fusion and all data with tree-DRF fusion. Tree-DRF
performed better than the other two methods for most categories.
[20] G. Schindler, L. Zitnick, and M. Brown. Internet video category recognition. In Proc. of CVPR Workshop on Internet
Vision, 2008.
[21] A. F. Smeaton, P. Over, and W. Kraaij. Evaluation campaigns
and trecvid. In Proc. of ACM Workshop on Multimedia Information Retrieval, 2006.
[22] C. G. M. Snoek, M. Worring, and A. W. M. Smeulders. Early
versus late fusion in semantic video analysis. In Proc. of
ACM MM, 2005.
[23] P. Viola and M. Jones. Rapid object detection using a boosted
cascade of simple features. In Proc. of CVPR, 2001.

[25] S. Zanetti, L. Zelnik-Manor, and P. Perona. A walk through
the web’s video clips. In Proc. of CVPR Workshop on Internet Vision, 2008.
[26] H. Zhang, A. Kankanhalli, and S. W. Smoliar. Automatic
partitioning of full-motion video. Multimdedia Systems,
1(1):10–28, 1993.
[27] X. Zhu. Semi-supervised learning literature survey. Technical report, University of Wisconsin-Madison, 2008.
[28] X. Zhu and Z. Ghahramani. Learning from labeled and unlabeled data with label propagation. Technical Report CMUCALD-02-107, Carnegie Mellon University, 2002.

[24] J. Yang, R. Yan, and A. G. Hauptmann. Cross-domain video
concept detection using adaptive svms. In Proc. of ACM MM,
2007.

886

Stereo Imaging with Uncalibrated Camera
Xiaokun Li1, Chiman Kwan1, and Baoxin Li2
1

2

Signal/Image Processing, Intelligent Automation Inc., MD 20855, USA
Computer Science and Eng. Dept., Arizona State University, AZ 85287, USA

Abstract. 3D images provide more information to human than their 2D
counterparts and have many applications in entertainment, scientific data
visualization, etc. The ability to generate accurate 3D dynamic scene and 3D
movie from uncalibrated cameras is a challenge. We propose a systematic
approach to stereo image/video generation. With our proposed approach, a
realistic 3D scene can be created via either a single uncalibrated moving camera
or two synchronized cameras. 3D video can also be generated through multiple
synchronized video streams. Our approach first uses a Gabor filter bank to
extract image features. Second, we develop an improved Elastic Graph
Matching method to perform reliable image registration from multi-view
images or video frames. Third, a fast and efficient image rectification method
based on multi-view geometry is presented to create stereo image pairs.
Extensive tests using real images collected from widely separated cameras were
performed to test our proposed approach.

1 Introduction
In entertainment, gaming, and TV programs, one of the major steps towards natural
and easy understanding and perception of image/video is 3D effects. Besides these
applications, 3D image/video techniques are needed in many other civilian
applications, i.e. medical operation, microscope, scientific data display, CAD/CAM,
and surveillance systems. These technologies can also be used directly in target
detection and recognition, precise strike, fly guidance, and some other military related
applications. Actually, computer-based stereo vision system has been studied for
many years, in which the major task is the estimation of 3D depth of the physical
scene from a pair of cameras emulating the left and the right eyes in a human visual
system. The fundamentals of depth estimation from a pair of stereo cameras (a stereo
rig) can be illustrated in the following figure (Fig. 1). But, there are many limitations
of conventional stereo vision systems, including the requirements of two identical
cameras, narrow baseline, fixed parameter setting and position, limited field of view,
and only suitable for short-range scene, etc. All of the above issues seriously limit the
usage of conventional stereo vision systems in many areas. In real life, the
cameras/sensors used for data acquisition are often nonstationary and located at
different viewpoints which have wide distance between each other. Furthermore, the
parameter settings of camera are usually unknown and unfixed during data
acquisition.
To generate a stereo (3D) image through a single moving camera or multiple
synchronized cameras far from each other is a challenging problem. The main
G. Bebis et al. (Eds.): ISVC 2006, LNCS 4291, pp. 112 – 121, 2006.
© Springer-Verlag Berlin Heidelberg 2006

Stereo Imaging with Uncalibrated Camera

113

reason comes from the fact that, in general, the cameras are not calibrated, thus
simply using standard stereoscopic vision algorithms may not generate correct
stereo images. Hence, advanced algorithms that can handle uncalibrated cameras
are needed. In recent years, many research efforts have been made. Fusiello et al.
(UK and Italy) [1] presented a compact and efficient algorithm to generate stereo
image via image rectification. But, their approach assumes that the stereo rig is
calibrated, which means the camera’s intrinsic parameters such as focal length,
aspect ratio, and the relative position to each other are already precisely known or
calculated. Unfortunately, as mentioned earlier, the camera’s parameters are hard to
know and the relative position between cameras are difficult to obtain or calibrate
in practice. Loop and Zhang at Microsoft Research [9] developed one method to
construct stereo image with uncalibrated cameras. Their method is mainly designed
for stereo matching and the residual distortion may prevent stereoscopic
visualization. Hartley (GE Research) & Zisserman (U. of Oxford) [7], [8] proposed
a novel method for stereo generation from uncalibrated cameras, which is the most
advanced method in the literature based on our knowledge. One important
advantage of their method is that the method is robust to many situations. However,
one major limitation of the method is the quality of the stereo images can not be
guaranteed. Undetermined image deformations such as shrink or distortion often
occur in the stereo images.

Fig. 1. Stereoscopic vision system

In this paper, we propose a stereo imaging approach with uncalibrated camera(s),
which is based on Gabor filter, improved Elastic Graph Matching, and multi-view
geometry theories. The diagram of our proposed system is illustrated in Fig. 2.
Compared with the current methods, the advantages of our approach include accurate
feature detection and registration, stereo imaging without image deformation, and low
computational complexity for potential real-time deployment.

114

X. Li, C. Kwan, and B. Li

Fig. 2. Conceptual diagram of our proposed stereo imaging system

2 Algorithm Description
The proposed approach consists of three sequential steps. The first step is Gabor
filter-based feature extraction which provides an efficient way for image feature
extraction. The second step is an improved Elastic Graph Matching (EGM) to find
feature correspondence of input image pair. The last step is stereo image generation
based on the theories of multi-view geometry.
2.1 Gabor Filter
In human visual system (HSV), research has shown that people are sensitive to both
specific orientation and spatial frequencies of the target of interest. For
feature representation and extraction, wavelets are good at modeling both the
orientation and frequency characteristics of object of interest. A Gabor filter bank
can act as a simple form of wavelet filter bank. Because of its simplicity
and optimum joint spatial/spatial-frequency localization, Gabor filter has attracted
many research efforts [2], [3] and has been applied in many image analysis and
computer vision-based applications, e.g. face recognition and fingerprint
verification.
Gabor-filter bank is a group of 2-D filters which capture the optimal jointed
localization properties of region of interest in both spatial and spectral domain.
Typically, an image is filtered with a set of Gabor filters which have different or
K
preferred orientations and spatial frequencies. To be specific, an image I ( x ) is
filtered with a set of Gabor wavelets as follows,
K K
K
K K K
( wI )( k , x0 ) = ∫ φkK (x0 − x ) I ( x ) dx
where φ kK is the Gabor wavelet (filter) defined by
K

φ kK ( x ) =

K
k

σ

2

exp( −

K2 K2
k x

2
KK
σ
)[exp(
)
−
exp(
−
)]
ikx
2
2σ
2

K
iφ
with k = k v e controlling the orientation and the scale of the filters. By varying v
and μ , we can get different Gabor filters with different orientations and scales. In our
μ

Stereo Imaging with Uncalibrated Camera

115

implementation, μ controls the orientation and is assigned by any value of 0, 1, 2, to
7 and v controls the spatial frequency and is assigned from 0, 1, and 2 with
k v = (π / 2) / 2 v and φ μ = (μπ ) / 8 . After filtering with a set of Gabor filters (24 filters
from the above choice of v and μ ), the outputs on each pixel in the image form a 24dimensional vector called “jet”. The amplitude of the jet represents whether a pixel
has significant gradient value in both orientation and frequency. Thus, it can be used
to determine if this pixel is a good feature for matching and tracking. In [6], some
more complicated criteria are given for selecting good feature.
2.2 Feature Correspondence
Given the scenario that stereo images are generated from two videos (image
sequences) acquired at two different views or one video (image sequence) taken by
one single moving camera, we need to solve the feature correspondence problem to
each synchronized image pair of two views or the image pair selected from single
mobile camera, which can be thought as the most important step in stereo imaging
with uncalibrated camera(s). With the Gabor-filter based features, we develop an
improved version of the Elastic Graph Matching (EGM) method to identify the
correspondence to the image pair. EGM [3], [4] has been applied successfully in
many applications. But, due to the possible arbitrary relative position between the
sensors, different camera settings, and different distances to the scene of interest,
the conventional EGM methods may never converge to the correct position
because of the position, orientation, and scale difference between the two images of
the pair, and thus we propose to roughly match the image pair first, and then use
EGM method to tune the matching result further. The matching between two images
with unknown rotation and size can be formulated using a non-orthogonal image
expansion approach [5]. One important issue in stereo matching is that the same
object might have different 2D projections at different viewpoints. Here, we
assume the difference of the projections is not big. The assumption is reasonable to
most cases, which is based on the fact that a single moving camera has high frame
rate (>15frames/sec) or the inspected scene is far away from the two differentview cameras. For small or middle-level shape changes, called shape deformation,
our modified EGM can work efficiently because of the philosophy of EGM
which is designed specially for the deformed-shape matching. Note that to
guarantee the correctness of using this modified EGM, we require the image pair
for stereo imaging must have at least ½ overlap and the EGM is only applied
to overlap.
The main steps of feature correspondence are shown below:
Step 1: Find approximate position: We use the novel template matching with
unknown rotation and size parameter [5] to identify the initial
correspondence/matching between a target and the template of reference
image/database. From the correspondences, some corresponding pairs of pixels from
target and template are selected as features whose magnitudes of the jets are
obviously larger than that of other pixels.

116

X. Li, C. Kwan, and B. Li

Step 2: Verify position: We first average the magnitudes of the jets of each
feature point. The jets to each pixel are termed as “bunch”. Then, we assign the
average value to the processed bunch and compute the similarity function S a without
phase comparison.

∑ a j a 'j
Sa ( J , J ) =

j

'

∑ a 2j ∑ a 'j2
j

j

where a j is the average value of the jth bunch. Alternatively, we can compute the
similarity function Sφ with phase.

∑ a j a 'j cos(φ j − φ 'j ) 2
Sφ ( J , J ' ) ≈

j

∑ a 2j ∑ a 'j2
j

j

If the similarity is larger than a predefined threshold, the result by template
matching is acceptable. Otherwise, error message will be generated and the EGM
process is stopped.
Step 3: Refine position and size: To the current bunch graph, we vary its position
and size to tune the correspondence. For each bunch, we check the four different
pixels ( ±3 , ±3 ) displaced from its corresponded position in the target image. At each
position, we check two different sizes with a factor of 1.2 smaller or larger the bunch
graph.
Step 4: Refine aspect ratio: A similar relaxation process as described in Step 3 is
performed. But at this time, we apply the operation only to x and y dimensions
respectively.
2.3 Stereo Image Creation

It is not a trivial problem to create a pair of stereo images, even after finding the
feature correspondence of the two images from uncalibrated cameras. With the
knowledge of the correspondence of the two images, we need to rectify the two
images of general viewpoints to form a stereo pair so that the stereo images look the
same as if they were taken from a true stereo camera system. Conventional image
rectification research focuses primarily only on making stereo matching easier but
pays little attention to whether the rectified images form a natural stereo pair like
those from a stereo camera (which is critical to the 3D display application). Here we
present a rectification scheme that makes the rectified images look like a true stereo
image that satisfies the constraints of the 3D display, and hence enables stereoscopic
visualization of two general views of the same scene.
2.3.1 Standard Stereo System
We first consider the standard stereo setup, as shown in Fig. 3, which satisfies the
following constraints:

Stereo Imaging with Uncalibrated Camera

1) The two optical axes are parallel and perpendicular
to the baseline.
2) The two cameras have the same intrinsic parameters.
3) Two image planes are aligned, which means that the
x axes of the two images are parallel to the baseline.

117

Optical Axis

k

The stereo imaging for standard stereo system can be
found in many references and text books.

C1

Baseline

C2

Fig. 3. A standard stereo
setup

2.3.2 Solution to Uncalibrated Camera
Here we propose an efficient algorithm for constructing a
stereo image from two different-view images. For any two video frames or two
different-view images, the corresponding camera(s) setup may vary or are different
during data acquisition and the constraints listed above may not be satisfied anymore.
Therefore, our task of stereo imaging can be defined as rectifying the images so that
they appear to come from a standard stereo camera. It can be easily shown that, for
pure rotation or any internal parameter change, there exists a homography
transformation for image rectification, which means that we do not have to translate
the cameras directly to set a stereo camera (it is not feasible in real life), but we can
achieve this by rectifying the images with a homography transformation. That is to
say, we can “rotate” and “change camera matrix” by applying a proper homography
to the two images, so that the two images are transformed to a stereo pair identical to
one captured by a standard stereo camera. In summary, our problem of constructing a
stereo image from a single moving camera can be defined as: Given two video frames
captured at two general viewpoints, called “Camera 1” and “Camera 2” as illustrated
in Fig. 4 (a), we aim at getting a true stereo pair from these two images through image
rectification. The rectification process can be broken down into two steps. First, by
finding and applying a homography to each image, we transform these two images to
the new ones, which are identical to the ones captured by two parallel cameras, as
illustrated in Fig. 4 (b). Second, we adjust the wide or narrow baseline of the two
parallel cameras to a proper value (say standard base line) by translating the new
images with a proper value. Thus, the desired stereo pair is constructed in Fig. 4 (c).

(a) Two general image plane

(b) Rectified image planes

(c) Stereo image pair with proper baseline

Fig. 4. Image Rectification

• Transforming Each Image By Its Homography
In this case, the epipolar constraint [7] is followed. That is, after rectification, we
should have the following properties: (1) Any epipolar line in each image should be
horizontal. This means that the epipole is at (k , 0, 0)T . (2)The corresponding epipolar
lines should be the same. This means that, given two corresponding epipolar
lines {l , l '} such that l ' = F [k ]× l , l ' should equal to l . There are already some

118

X. Li, C. Kwan, and B. Li

methods [7-11] for stereo imaging based on multi-view geometry. Here, we give a
very concise description for stereo imaging which is the summary of the above
methods. To make the stereo images look like standard stereo setup as much as
possible, we give the following major steps for stereo image generation:
Algorithm. Stereo Image Generation from Uncalibrated Camera
Step 1. Estimate fundamental matrix F which represents the difference between the
two images
Step 2. Compute the rectification matrix H2 for the second image
Step 3. Compute the rectification matrix H1 for the first image
Step 4. Rectify each pixel of the two selected images by multiplying H1 and H2
respectively to generate wide-baseline stereo pair
Step 5. Compute the average Z value of the center part of image and translate image
to configure a proper baseline for 3D display
• Improving Stereo Pair Quality By Accurately Computing {a, b, c}
In the stereo imaging via multi-view geometry, matrix F and H2 can be calculated
accurately, according to [7-11]. But, for H1, we need to estimate the matrix H A ,
which is the unknown component of H1 in the form of (a, b, c;0,1,0;0,01) . In
conventional methods, such as [7-11], solving for H A is not clearly addressed.
Sometime, a solution would be given, but is only suitable to stereo matching, and
therefore not applicable to our case. We have to develop some criteria to determine
the vector ( a, b , c ) more accurately while still being suitable to our case.
Conventional methods use a criterion that minimizes the disparity of the selected
point pairs for stereo matching. This criterion cannot be used for our case as we want
the results undistorted with respect to a standard stereo pair. Therefore, we propose to
use a new minimization criterion to estimate vector ( a, b , c ). First, we first segment
the image pair into many regions according to the image homogeneity. Second, we
select some feature points in the first image, such as edge points, from each region
and find their correspondence in the second image. Third, because the feature points
with same depth in the same region have same disparity, we formulize a minimization
equation to compute ( a, b , c ).

mim(∑

∑

p i , j∈Ap

( H 1 xi − H1 x j − H 2 xi' − H 2 x 'j ) 2 )

Here Ap is a set of points such that for i, j ∈ Ap , xi and x j have the same depth and
in the same image. xi and xi' is the corresponding pair in the two image.

3 Experimental Results
Extensive tests including indoor/outdoor scenarios were performed to validate the
efficiency of our proposed algorithm. 3D video generation for moving target
inspection was also performed. Note that all the stereo results illustrated in the
paper are in red-cyan format. Reader can perceive these 3D images with regular
red-cyan 3D glasses.

Stereo Imaging with Uncalibrated Camera

119

3.1 Tests with a Single Moving Camera

A set of multiple-view images was taken by a hand-hold moving camera. The
resolution of the true-color image is 1024x768 and the internal parameters (such as
focal length, aspect ratio, lens, shutter speed, etc.) of camera are unknown. The
relative position and the orientation of the multi-views were not calculated.

• Test in Indoor Environment
Indoor tests were performed with a mobile camera. Fig. 5 shows the process and results.
It can be seen (with 3D glasses) that the stereo image has been successfully created.

Fig. 5. Example of indoor test

• Test in Outdoor Environment
Outdoor images were taken with objects hiding in grasses. Fig. 6 shows that a 3D
image was successfully created. 3D effect of the scene can be easily perceived with
3D glasses.

Fig. 6. Test in out-door environment

• Test on Depth Perception
Two images at two different viewpoints were captured and then a stereo image was
formed. Fig. 7 illustrates the result.

120

X. Li, C. Kwan, and B. Li

Fig. 7. Test on 3D depth perception

3.2 Moving Target Inspection with Two Video Cameras

Several video clips were captured by two ordinary video camcorders set around the
scene of interest. Each video clip is at least 15 seconds long. The resolution of the
color video is set as 640x480 and the internal parameters (such as focal length, aspect
ratio, lens, shutter speed, etc.) of camcorder are unknown. The relative position and
orientation of two cameras are also uncalculated. In the test, a moving target (pen)
was moving forward and backward between two hanging balls. The Z-direction
movement can not be observed in 2D video. But, with 3D video, the Z-direction
movement can be easily perceived. Fig. 8 and Fig. 9 illustrate the 3D results.

(a)

(b)

(c)

Fig. 8. Pen moving toward front: (a), (b), and (c) are red-cyan stereo images for different time
instances

(a)

(b)

(c)

Fig. 9. Pen moving toward back: (a), (b), and (c) are red-cyan stereo images for different time
instances

Stereo Imaging with Uncalibrated Camera

121

4 Conclusions
We have presented a systematic approach for stereo image generation. The approach
consists of robust feature generation via Gabor filters, accurate feature
correspondence by EGM, and reliable stereo image creation. Tests with real-life data
clearly demonstrated the efficacy of our proposed method.

Acknowledgements
This work was supported by a grant from the U. S. Missile Defense Agency. The
authors would like to thank Mr. Richard Felner and Mr. Steve Waugh for their helpful
suggestions and comments.

References
1. Andrea Fusiello, Emanuele Trucco, Alessandro Verri: A compact algorithm for
rectification of stereo pairs. Journal of Machine Vision and Applications, Vol. 12, (2000)
16-22
2. D. A. Clausi and H. Deng: Fusion of Gabor filter and co-occurrence probability features
for texture recognition. IEEE Trans. of Image Processing, Vol. 14, No. 7, (2005) 925-936
3. L. Wiskott, J. Fellous, N. Kruger, C. Malsburg: Face recognition by Elastic Bunch Graph
Matching, Intelligent Biometric Techniques in Fingerprint and Face Recognition, CRC
Press, Chapter 11, (1999) 355-396
4. D. Beymer: Face recognition under varying pose. In Proc. IEEE Conf. Computer Vision
and Pattern Recognition. (1994) 756-761
5. R. M. Dufour, E. L. Miller, N. P. Galatsanos: Template matching based object recognition
with unknown geometric parameters. IEEE Trans. on Image Processing, Vol.11, (2002)
6. J. Shi and C. Tomasi: Good Features to Track. Proc. IEEE Conference on Computer
Vision and Pattern Recognition. (1994)
7. R. Hartley and A. Zisserman: Multiple view geometry in computer vision. Cambridge
University, Cambridge, 2nd edition, (2003)
8. R. Hartley: Theory and practice of projective rectification. IJCV, 35(2) (1999) 1-16
9. C. Loop and Z. Zhang: Computing rectifying homographies for stereo vision. In CVPR,
(1999) 125-131
10. P. May: A Survey of 3-D Display Technologies. Information Display, Vol.32 (2005)
11. Z. Zhang: Determining the epipolar geometry and its uncertainty: a review. In IJCV,
27(2): (1998) 161–195

RECOGNIZING UNSEEN ACTIONS IN A DOMAIN-ADAPTED EMBEDDING SPACE
Yikang Li†

Sheng-hung Hu†

Baoxin Li

Arizona State University
ABSTRACT
With the sustaining bloom of multimedia data, Zero-shot
Learning (ZSL) techniques have attracted much attention in
recent years for its ability to train learning models that can
handle “unseen” categories. Existing ZSL algorithms mainly
take advantages of attribute-based semantic space and only
focus on static image data. Besides, most ZSL studies merely
consider the semantic embedded labels and fail to address
domain shift problem. In this paper, we purpose a deep
two-output model for video ZSL and action recognition tasks
by computing both spatial and temporal features from video
contents through distinct Convolutional Neural Networks
(CNNs) and training a Multi-layer Perceptron (MLP) upon
extracted features to map videos to semantic embedding word
vectors. Moreover, we introduce a domain adaptation strategy
named “ConSSEV” – by combining outputs from two distinct
output layers of our MLP to improve the results of zero-shot
learning. Our experiments on UCF101 dataset demonstrate
the purposed model has more advantages associated with
more complex video embedding schemes, and outperforms
the state-of-the-art zero-shot learning techniques.
Index Terms— zero-shot learning, action recognition,
multi-layer perceptron, convolutional neural network.
1. INTRODUCTION
Video-based action recognition has many applications [1].
Recent rapid growth of action categories makes conducting
video annotation an expensive, challenging and time consuming task. While conventional classiﬁers require sufﬁcient
training data to achieve acceptable results on action recognition tasks, it is difﬁcult and costly to collect satisfactory
amount of annotated spatial-temporal segments of videos.
The zero-shot learning (ZSL) algorithm provide a solution
to mitigate those issues by connecting human-level semantic
descriptions of the action with low-level visual features and
allowing different categories to share their information –thus
enable new categories to be built in terms of their human descriptions rather than extending the size of the training visuallevel data. Three keys are rather important in ZSL algorithm
– selecting of visual descriptors, constructing human-level
†

These two authors have equal contributions.

,(((

semantic descriptors and the mapping function to map visual
to semantic space.
Most existing ZSL algorithms are realised by building
human-level attribute model to bridge the visual features and
their corresponding semantic space. New categories are then
classiﬁed in terms of their attributes [2, 3]. However, it is
rather difﬁcult to obtain reliable attribute-based representation for objects, especially for actions [4], and this kind of
semantic attribute-based ZSL classiﬁers suffer from lacking
distributed representation of each attribute words.
An alternative approach to the attribute-based method is
the Semantic Embedding Space (SES) [5, 6]. SES is trained
by a skip-grim or continuous bag-of-words neural network
to map the text words into a word vector space – therefore
enable new categories be simply annotated by the similarity
and distribution of existing text-string vectors and avoid nonscaleably growth of attribute lists as the emergence of new
categories Among all SES models, Word2Vec model is considered to be the most efﬁcienct model in maintaining semantic meanings while keeping low model complexity [7, 8, 9].
Although semantic embedding space has demonstrated
signiﬁcant advantages, most ZSL studies only focus on static
images semantic embedding since it is particularly difﬁcult to
extract reliable feature descriptors which cover both seen and
unseen action categories from videos to train the mapping
function. Moreover, the presence of amount of neighbour
vectors surrounding the mapped vectors in semantic space
has been proven to be a challenge for word-based vectors
[10] (i.e., “hubness” problem or domain shift problem).
In this paper, we design a deep two-output model for
video ZSL and action recognition purposes by taking advantages of both “soft” SES labels and conventional“hard”
binary labels to train a multi-layer perceptron that map CNN
visual features to their corresponding semantic meanings. A
new strategy called “convex combination of similar semantic embedding vectors” (ConSSEV) is also implemented to
deal with the domain shift problem. The purposed model not
only outperforms state-of-the-art [8] method on UCF101 [11]
video action dataset on zero-shot learning but also achieve
comparative high accuracy with the conventional supervised
action recognition classiﬁer on action recognition task.



,&,3

Input layer
The
Sequence
of video
frames

Multi-task
Output layer

Hidden layer

The Appreance
features
from pretrained
CNN
model

The Pretrained CNN
Model for Appearance
features extraction

Softmax Label
Semantic Label

concatenation
Videos
The
Sequence
of Optical
Flow

The
Optical
Flow
features
from pretrained
CNN
model

The Pretrained CNN
Model for Optical Flow
features extraction


  


  

total backpropagation
error:
 E

 
 i j


Video Spatial-Temporal Feature extraction

Two-Output Multilayer Perceptron

Fig. 1. The framework of our deep two-output model for video ZSL and action recognition purposes. Notice that errors from
both output layers are back-propagated.
2. METHOD
The overall framework of the proposed model is illustrated in
Fig.1. We ﬁrst extract frames and optical ﬂow [12] from video
contents and then pass them into two different pre-trained
CNN models [13]. The appearance features and optical ﬂow
features are collected from the second last fully connected
layer (i.e., “fc7”) from each CNN. Next we aggregate and
concatenate both features to represent one action by using
a sliding window strategy. Then, a two output layer multilayer perceptron is trained by backpropagating errors from
semantic labels and ﬁne-tuned on softmax hard binary labels
to serve as a mapping function from visual to semantic space.
Finally, zero-shot learning (ZSL) is performed by mapping visual features to semantic space vectors through our model and
Convex combination of Similar Semantic Embedding Vectors
(ConSSEV) is implemented as a domain shift method.

diapp =

1
T

to +T
−1

diof =

1
T

to +T
−1

t=t0

t=t0

t
fapp
(i)

(1)

t
fof
(i)

(2)

di = [diapp , diof ].
i

(3)
st

Finally d is the extracted visual features for the i class with
dimension k = 8192.
2.2. Semantic Embedding Space
In this paper, we construct the semantic embedding space
with the help of the word2vec neural network [5, 6] because of
its reliable mapping between word corpses and mathematical
meanings. Through this networks, semantic labels {y i }i=1...n
are assigned to 500-D vectors Z i = g(y i ) and are divided by
N
the amount of unique words in Z i = N1 j=1 g(yji ) for normalization purpose.

2.1. Visual Feature Extraction
Considering the success of Pose-based Convolution Neural
Network [13] on recognizing human-posed and action, we extracted features in a similar way. Videos are ﬁrst sampled to
RGB frames to represent appearances and optical ﬂows are
computed to represent motions [12]. To describe both apt
t
and motion features fof
, we use two
pearance features fapp
different CNNs on RGB and optical ﬂow frames respectively
and the output of the second last fully-connected layer [14]
with dimension k = 4096 is served as our descriptors. For
RGB frames the publicly available “VGG-f” pre-trained network [14] is used while the pre-trained motion network [15]
is applied on optical ﬂows.
A Sliding Window Strategy: We applied a sliding window
t
t
with size T and step size S on both fapp
and fof
. Features
within the same window are avg aggregated to obtain ﬁxedlength video descriptors di over T frames

2.3. Mapping Function
Given visual features di and semantic embedded space labels
Z i , our goal now is to build a projection model: Z ∗ = M (di )
that can best map each video to a vector in the corresponding
semantic embedding space. Inspired by the idea of [16], A
two-output multi-layer perceptron (MLP) is trained to achieve
this goal.
Both the semantic space soft labels Z i and hard binary
labels y i are applied on training the MLP. Two distinct loss
functions are calculated and both errors are backpropagated.
For semantic soft label loss, we use hinge rank loss function
(similar to [7]) to measure the similarity of the semantic output Z ∗ and semantic labels Z. The hinge rank loss function
is deﬁned as:

max[0, m − Z i · Z ∗ + Z j · Z ∗ ]
(4)
Lse = −



j=i

where m represents the margin value.
For hard binary label loss, the softmax loss function is selected because of its robustness for multi-class classiﬁcation.
The probability and loss for the softmax layer input Z ∗ be
classiﬁed to the j th class (i.e., ȳ = j|Z ∗ ) is deﬁned as:
∗

eZ

P (ȳ = j|Z ) = K
losssm = −



∗T

k=1

(5)

Our deep two-output model can also serve as a conventional
video recognition classiﬁer by supervised training the mapping function M with all categories. Then we map each test
visual feature ditest to a vector Z ∗ in semantic space through
Z ∗ = M (ditest ) and matching them with all projected labels
Z i through Eq. (7).

(6)

3. EXPERIMENTS

wj

eZ ∗

Tw
k

1{ȳ = j} log(P (ȳ = j|Z ∗ )).

2.5. Conventional Video Recognition

j

Here 1{ȳ = j} equals to 1 when predicted label ȳ equals to
target label, otherwise it equals to 0.
Both softmax and semantic output leverage visual and semantic similarity to train MLP since two outputs share information in input and hidden layer. Moreover, the summation
of softmax and semantic loss backpropagate to learn weights
in each layer, so that each loss can be serve as a compensation
and ﬁne-tuning method for the other.
2.4. Zero-Shot Learning and ConSSEV strategy
Zero-Shot learning is then performed on a completely disjoint dataset from training set. Utilizing our deep-multi output
model and word2vec, we are able to project both “unseen” vii
sual contents ditest and their corresponding word labels ytest
∗
i
to semantic embedded space vectors (Ztest and Ztest )). Since
both vectors are normalized, a simple cosine similarity is performed to match projected visual contents and labels
∗
i
, Ztest
).
ȳ = arg max cos(Ztest

(7)

ConSSEV strategy: Due to the disjointness of train and test
set in ZSL, the trained mapping function M may not be the
best ﬁt in the case of mapping test set and thus biases the
similarity measurement [7, 8]. In this paper, we introduce a
self-adaptive domain shift method by utilizing both semantic and softmax outputs in our model to adjust both semantic
∗
i
and label prototype Ztest
.
output Ztest
For semantic output, we ﬁrst ﬁnd the top K training labels that have the highest similarities with test semantic label
vector {Zk∗ }k=1...K as that in Eq. (7). Then we form a new semantic vector Z̄ ∗ by weighted sum of all k vectors with their
corresponding softmax output P (ȳ = k|Zk∗ ). The adaptive
semantic output vector performs better since it penalized “ambiguous feature results” by weighting smaller softmax probabilities (“conﬁdences”).
Z̄ ∗ =

K
1 
P (ȳ = k|Zk∗ ) · Zk∗
K

(8)

k=1

For label prototype, we perform the same self − training
techniques as that in [8] on the adaptive data vector obtained
by Eq. (8). The new convex combination of semantic output
i
Z̄ ∗ and test label prototype Z̄test
are more directly comparable by using Eq. (7).

3.1. Dataset
We train and test our model on one of the most challenging video action dataset – UCF101 [11] which contains
13320 videos from 101 cation categories (e.g.“Apply Eye
Make”,“Basketball Dunk”,and “Brest Stroke”). Videos in
each action category are grouped into 25 groups where each
group shares some common features, such as background,
viewpoints, objects,...etc.
For evaluating ZSL, we use the same evaluation protocol
as in [8] – 30 independent splits for UCF101 dataset with each
split contains a completely disjoint 51 categories for training purpose and 50 for testing purpose. For conventional action recognition, on the other hand, we use the standard splits
(“Three Train/Test Splits”) for UCF101 dataset.
3.2. Experiment Setting
Semantic Embedding Space: Word2Vec [5, 6] method is
used to embed the text labels. We trained a skip-gram text
model on a corpus containing 5.4 billion words extracted
from wikipedia.org. Dimension of word vector is set to 500D to trade-off training complexity and maintaining semantic
meanings [7, 8].
Visual Feature Extraction: To further decrease model complexity, only one frame is sampled for each three consecutive
frames from video and optical ﬂows [12] are computed upon
them. Two distinct convolutional neural networks are applied to extract features – both contain 5 convolutional and
3 fully-connected layers. The appearance and optical ﬂow
features are extracted from the second last fully-connected
layer (i.e.,“fc7”) which dimension for each feature is 4096.
Then we use the sliding window strategy to aggregate and
concatenate the appearance and optical ﬂow feature vectors
into one 8192-D vector.
Mapping function training: A Multilayer Peceptron (MLP)
is trained with those aggregated visual features for each video
clip. Target labels for softmax output are so-called “hard binary labels” (i.e., “0” and “1”), and target labels for semantic
embedding output are the 500-D semantic space word vectors. The number of hidden nodes is set to be 1000, learning
rate to be 0.001, the momentum to be 0.9 and margin value
for rank loss function to be 0.9 based on the result of crossvalidation. Moreover, for each splits, we train ﬁve iterations



of all training features with random training order.
Results Comparison: For zero-shot learning, we compared
the following methods on the same split data of UCF101:
(1) Random Guess: the method randomly guesses one label
from the unseen labels. (2) Attribute Based-Indirect Attribute
Prediction (IAP) [2]: the method selects the unseen label by
the video representation attributes. (3) Convex Combination
of Semantic Embeddings (ConSE) [9]: the method use the
conventional neural network classiﬁer output (i.e. softmax
output) to weight the training labels and combine the top K
embedded labels to denote a new semantic embedding word
vectors. (4) Dense Trajectories Based Regression Model with
Nearest Neighbour (DTRM+NN) [8]: the model is trained
a SVM classiﬁer with RBF kernel on the dense trajectory
descriptors [17]. This method is treated as our baseline. (5)
Our deep two-output model with Nearest Neighbour: ﬁnding
the nearest neighbour in terms of maximal cosine similarity. (6) DTRM+NN+ST: Apply Self-training domain shift
method with DTRM. (6) Our deep two-output model with
ConSSEV approach: Apply ConSSEV domain shift strategy
on our model.
For video recognition, our model is validated with the following: (1) Dense Trajectories [17]. (2) Binary SVM classiﬁer with RBF kernel (DTRM) [8]. (3) Our model Semantic
output. (4) Our model Softmax output.
3.3. Evaluations
3.3.1. Performance of Zero-shot Learning
The results of zero-shot learning are presented in Tab.1. All
listed methods are signiﬁcantly better than random guess
which shows successful ZSL. Without applying any kinds
of domain shifting techniques and only consider the Nearest Neighbour (NN), our deep two-output model achieves
a slightly better performance than existing state-of-the-art
semantic-based ZSL (DTRM) – suggesting visual contents
are effectively mapped to semantic space vectors that are near
to its human-level semantic meanings. Although our model
fails to demonstrate better performance than attribute-based
model [2] when evaluating by NN, it does not suffer from lack
of attributes and costly attribute annotation. By applying our
ConSSEV domain shift strategy, our model signiﬁcantly outperforms other domain shift counterpart (DTRM+NN+ST).
Overall, our zero-shot learning technique based on MLP
has a great performance among the existing state-of-the-art
ZSL methods and the ConSSEV domain shift strategy between test and train categories proves to be a signiﬁcant performance boost on ZSL technique.
3.3.2. Performance of Action Recognition
The performance of our model conducting conventional action recognition task is listed in Tab.2. The ﬁnal results reveal
our approach performs comparatively with the state-of-the-art

Table 1. Zero-shot learning performance
Method
Accuracy ± Variation
Random Guess

2.0

IAP [2]

12.8 ± 2.0

ConSE [9]

10.5 ± 2.0

DTRM + NN [8]

10.9 ± 1.5

Ours + NN

11.3 ± 2.1

DTRM + NN + ST [8]

15.8 ± 2.3

Ours + NN + ConSSEV

26.8 ± 4.4

Table 2. Conventional action recognition performance
Method
Accuracy
Dense Trajectories [17]

75.1

DTRM [8]

73.7

Ours(Semantic)

74.1

Ours(Softmax)

72.7

method including Dense Trajectories [17] and Binary SVM
classiﬁer with RBF kernel [8]. Thus demonstrating the ability of our deep two-output model on addressing conventional
action recognition tasks. We can also observe that Dense Trajectory [17] performs slightly better than our approach. This
may due to the sliding window strategy that is used to extract
video features. Even though motion information of optical
ﬂow is utilized in our model, the averaged features within a
sliding window will lose some temporal information of video
sequences compared with HOF and MBH features in Dense
Trajectories [17].

4. CONCLUSION
In this paper we train a deep two-output model to realise the
zero-shot learning paradigm on video recognition. A domain
shift technique, convex combination of similar semantic embedding vectors (ConSSEV), which proves to provide a signiﬁcant improvement in terms of zero-shot learning accuracy
by utilizing the known semantic space to express the unknown
semantic space, is purposed. We show that our zero-shot
learning model with ConSSEV strategy greatly outperforms
state-of-the-art zero-shot video action recognition techniques.
Acknowledgments: The work was supported in part by
ONR grant N000141512344 and ARO grant W911NF1410371.
Any opinions expressed in this material are those of the authors and do not necessarily reﬂect the views of ONR or
ARO.



5. REFERENCES
[1] Zheshen Wang and Baoxin Li, “Human activity encoding and recognition using low-level visual features.,” in
Proceedings of International Joint Conference on Artiﬁcial Intelligence (IJCAI), 2009, pp. 1876–1883.
[2] C.H. Lampert, H. Nickisch, and S. Harmeling,
“Attribute-based classiﬁcation for zero-shot visual object categorization,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 36, no. 3, pp.
453–465, March 2014.
[3] Heng-Tze Cheng, Martin Griss, Paul Davis, Jianguo Li,
and Di You, “Towards zero-shot learning for human
activity recognition using semantic attribute sequence
model,” in Proceedings of the 2013 ACM International
Joint Conference on Pervasive and Ubiquitous Computing, New York, NY, USA, 2013, UbiComp ’13, pp. 355–
358, ACM.
[4] Chuang Gan, Ming Lin, Yi Yang, Yueting Zhuang, and
Alexander G.Hauptmann, “Exploring semantic interclass relationships (sir) for zero-shot action recognition,” 2015.
[5] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean, “Distributed representations of
words and phrases and their compositionality,” in Advances in Neural Information Processing Systems 26,
C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani,
and K.Q. Weinberger, Eds., pp. 3111–3119. Curran Associates, Inc., 2013.

[11] Khurram Soomro, Amir Roshan Zamir, and Mubarak
Shah, “UCF101: A dataset of 101 human actions classes
from videos in the wild,” CoRR, vol. abs/1212.0402,
2012.
[12] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert,
“High accuracy optical ﬂow estimation based on a theory for warping,” in European Conference on Computer
Vision (ECCV). May 2004, vol. 3024 of Lecture Notes
in Computer Science, pp. 25–36, Springer.
[13] Guilhem Chéron, Ivan Laptev, and Cordelia Schmid,
“P-CNN: Pose-based CNN Features for Action Recognition,” in ICCV 2015 - IEEE International Conference
on Computer Vision, Santiago, Chile, Dec. 2015.
[14] Ken Chatﬁeld, Karen Simonyan, Andrea Vedaldi, and
Andrew Zisserman, “Return of the devil in the details: Delving deep into convolutional nets,” CoRR, vol.
abs/1405.3531, 2014.
[15] G. Gkioxari and J. Malik, “Finding action tubes,” in
CVPR, 2015.
[16] Ragav Venkatesan and Baoxin Li, “Diving deeper into
mentee networks,” arXiv preprint arXiv:1604.08220,
2016.
[17] Heng Wang and Cordelia Schmid, “Action recognition
with improved trajectories,” in IEEE International Conference on Computer Vision, Sydney, Australia, 2013.

[6] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean, “Efﬁcient estimation of word representations in
vector space,” CoRR, vol. abs/1301.3781, 2013.
[7] Andrea Frome, Greg Corrado, Jon Shlens, Samy Bengio, Jeffrey Dean, Marc’Aurelio Ranzato, and Tomas
Mikolov, “Devise: A deep visual-semantic embedding
model,” in Advances In Neural Information Processing
Systems, NIPS, 2013.
[8] Xun Xu, T. Hospedales, and Shaogang Gong, “Semantic embedding space for zero-shot action recognition,”
in Image Processing (ICIP), 2015 IEEE International
Conference on, Sept 2015, pp. 63–67.
[9] Mohammad Norouzi, Tomas Mikolov, Samy Bengio,
Yoram Singer, Jonathon Shlens, Andrea Frome, Greg
Corrado, and Jeffrey Dean, “Zero-shot learning by convex combination of semantic embeddings,” CoRR, vol.
abs/1312.5650, 2013.
[10] Georgiana Dinu and Marco Baroni, “Improving zeroshot learning by mitigating the hubness problem,”
CoRR, vol. abs/1412.6568, 2014.



Proceedings of the Ninth International AAAI Conference on Web and Social Media

Inferring Sentiment from Web Images with Joint Inference on
Visual and Social Cues: A Regulated Matrix Factorization Approach
Yilin Wang1 Yuheng Hu2 Subbarao Kambhampati1 Baoxin Li1
1

Department of Computer Science, Arizona State University, Tempe AZ
2
IBM Almaden Research Center, San Jose CA
{ywang370,rao,baoxin.li}@asu.edu yuhenghu@us.ibm.com

Abstract

of their feelings. It also improves one’s self-esteem and resilience, allowing them to bounce back with ease, from poor
emotional health, and physical stress and difficulty. As people are increasingly using photos to record their daily lives1 ,
we can assess a person’s emotional wellness based on the
emotion and sentiment inferred from her photos on social
media platforms (in addition to existing emotion/sentiment
analysis effort, e.g., see (De Choudhury, Counts, and Gamon
2012) on text-based social media).
In this paper, our goal is to automatically infer human sentiments (positive, neutral and negative) from photos shared
on Flickr and Instagram. While sentiment analysis of photos
is still in its infancy, a number of tools have been proposed
during past two years(Yuan et al. 2013; Jia et al. 2012). A
popular approach is to identify visual features from a photo
that are related to human sentiments, such as objects (e.g.,
toys, birthday cakes, gun), human actions (e.g., crying or
laughing), and many other features like color temperature.
However, such an approach is often insufficient because the
same objects/actions may convey different sentiments in different photo contexts. For example, consider Figure 1: one
can easily detect the crying lady and girl (using computer
vision algorithms such as face detection(Zhu and Ramanan
2012) and expression recognition(Song et al. 2010)). However, the same “crying” action conveys two clearly different
sentiments: the “crying” in Figure 1a is obviously positive as
the result of a successful marriage proposal. In contrast, the
tearful girl in Figure 1b looks quite unhappy thus expresses
negative sentiment. In other words, the so-called “visual affective gap” (Machajdik and Hanbury 2010) exists between
rudimentary visual features and human sentiment embedded
in a photo. On the other hand, one may also consider inferring the sentiment of a photo via its textual descriptions (e.g.,
titles) using existing off-the-shelf text-based sentiment analysis tools (Pang, Lee, and Vaithyanathan 2002). Although
these descriptions can provide very helpful context information of the photos, solely relying on them while ignoring the
visual features of the photos can lead to poor performance
as well. Consider Figure 1 again: by analyzing only the text
description, we can conclude that both Figure 1a and 1b
convey negative sentiment as the keyword “crying” is often

In this paper, we study the problem of understanding human sentiments from large scale collection of Internet
images based on both image features and contextual social network information (such as friend comments and
user description). Despite the great strides in analyzing user sentiment based on text information, the analysis of sentiment behind the image content has largely
been ignored. Thus, we extend the significant advances
in text-based sentiment prediction tasks to the higherlevel challenge of predicting the underlying sentiments
behind the images. We show that neither visual features
nor the textual features are by themselves sufficient for
accurate sentiment labeling. Thus, we provide a way of
using both of them. We leverage the low-level visual
features and mid-level attributes of an image, and formulate sentiment prediction problem as a non-negative
matrix tri-factorization framework, which has the flexibility to incorporate multiple modalities of information
and the capability to learn from heterogeneous features
jointly. We develop an optimization algorithm for finding a local-optima solution under the proposed framework. With experiments on two large-scale datasets, we
show that the proposed method improves significantly
over existing state-of-the-art methods.

1

Introduction

A picture is worth a thousand words. It is surely worth
even more when it comes to convey human emotions and
sentiments. Examples that support this are abundant: great
captivating photos often contain rich emotional cues that
help viewers easily connect with those photos. With the advent of social media, an increasing number of people start
to use photos to express their joy, grudge, and boredom
on social media platforms like Flickr and Instagram. Automatic inference of the emotion and sentiment information
from such ever-growing, massive amounts of user-generated
photos is of increasing importance to many applications in
health-care, anthropology, communication studies, marketing, and many sub-areas within computer science such as
computer vision. Think about this: Emotional wellness impacts several aspects of people’s lives. For example, it introduces self-empathy, giving an individual greater awareness

1
http://www.pewinternet.org/2015/01/09/social-media-update2014/

Copyright c 2015, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

473

(a) “Girlfriend crying a lot when
I proposed to her”.

trix into three factors corresponding to image-term, termsentiment and sentiment-features. The ANPs here can be
seen as providing the initial information (“prior knowledge”) on sentiment-feature factors. Similarly, the learnt image context can be used to constrain image-term and termsentiment factors. Last, the availability of labeled sentiment
of the images can be used to regulate the product of imageterm, term-sentiment factors. We pose this factorization as
an optimization problem where, in addition to minimizing
the reconstruction error, we also require that the factors respect the prior knowledge to the extent possible. We derive
a set of multiplicative update rules that efficiently produce
this factorization, and provide empirical comparisons with
several competing methodologies on two real datasets of
photos from Flickr and Instagram. We examine the results
both quantitatively and qualitatively to demonstrate that our
method improves significantly over baseline approaches.
The rest of this paper is organized as follows: first, we
review the related work on sentiment prediction as well as
work which utilizes the nonnegative matrix factorization.
We then present a basic model for the problem and further
improve the model by incorporating prior knowledge. The
experimental results and a comprehensive analysis are presented in the experiment part. Last, we conclude by identifying future work.

(b) Crying baby after her
toy was taken

Figure 1: An example shows affective gap.
classified as negative sentiment in standard sentiment lexicon (Taboada et al. 2011). Last, both visual feature-based
and text-based sentiment analysis approaches require massive amounts of training data in order to learn high quality models. However, manually annotating the sentiment of
a vast amount of photos and/or their textual descriptions is
time consuming and error-prone, presenting a bottleneck in
learning good models.
The weaknesses discussed in the foregoing motivate the
need for a more accurate automated framework to infer the
sentiment of photos, with 1) considering the photo context
to bridge the “visual affective gap”, 2) considering a photo’s
visual features to augment text-based sentiment, and 3) considering the availability of textual information, thus a photo
may have little or no social context (e.g., friend comments,
user description). While such a framework does not exist,
we can leverage some partial solutions. For example, we can
learn the photo context by analyzing the photo’s social context (text features). Similarly, we can extract visual features
from a photo and map them to different sentiment meanings. Last, while manual annotation of all photos and their
descriptions is infeasible, it is often possible to get sentiment
labeling for small sets of photos and descriptions.
Technical Contribution: We propose an efficient and effective framework, named RSAI (Robust Sentiment Analysis
for Images), for inferring human sentiment from photos that
leverages these partial solutions. Figure 2 depicts the procedure of RSAI. Specifically, to fill the visual affective gap,
we first extract visual features from a photo using low-level
visual features (e.g., color histograms) and a large number
of mid-level (e.g., objects) visual attribute/object detectors
(Yuan et al. 2013; Tighe and Lazebnik 2013). Next, to add
sentiment meaning to these extracted non-sentimental features, we construct Adjective Noun Pairs (ANPs)(Borth et
al. 2013). Note that ANP is a visual representation that describes visual features by text pairs, such as “cloudy sky”,
“colorful flowers”. It is formed by merging the low-level
visual features to the detected mid-level objects and mapping them to a dictionary (more details on ANP are presented in Section 3). On the other hand, to learn the image’s
context, we analyze the image’s textual description and capture its sentiment based on sentiment lexicons. Finally, with
the help from ANPs and image context, RSAI infers the image’s sentiment by factorizing an input image-features ma-

2

Related Work

In this section, we review the related work on sentiment
analysis and the methods for matrix factorization.
Sentiment analysis on text and images: Recently, sentiment analysis has shown its success in opinion mining on
textual data, including product review(Liu 2012; Hu and Liu
2004), newspaper articles (Pang, Lee, and Vaithyanathan
2002), and movie rating (Pang and Lee 2004). Besides,
there have been increasing interests in social media data
(Borth et al. 2013; Yang et al. 2014; Jia et al. 2012; Yuan
et al. 2013), such as Twitter and Weibo data. Unlike textbased sentiment prediction approaches, (Borth et al. 2013;
Yuan et al. 2013) employed mid-level attributes of visual
feature to model visual content for sentiment analysis. (Yang
et al. 2014) provides a method based on low-level visual
features and social information via a topic model. While
(Jia et al. 2012) tries to solve the problem by a graphical
model which is based on friend interactions. In contrast to
our approach, all such methods restrict sentiment prediction to the specific data domain. For example, in Figure 1,
we can see that approaches using pure visual information
(Borth et al. 2013; Yuan et al. 2013) may be confused by the
subtle sentiment embedded in the image. e.g., two crying
people convey totally different sentiment. (Jia et al. 2012;
Yang et al. 2014) assume that the images belong to the
same sentiment share the same low-level visual features is
often not true, because positive and negative images may
have similar low-level visual features, e.g., two black-white
images contain smiling and sad faces respectively. Recent,
deep learning has shown its success in feature learning for
many computer vision problem, (You et al. 2015) provides a
transfer deep neutral network structure for sentiment analysis. However, for deep learning framework, millions of im-

474

Figure 2: The framework of our proposed method. Comparing to conventional methods, which focus on single source/feature,
the proposed method learns the heterogeneous features, including text features, low-level features and mid-level visual features,
for sentiment analysis.
Table 1: Notations

ages with associated sentiment labels are needed for network
training. In real world, such label information is not available and how to deal with overfitting for small training data
remains a challenging problem.
Non-negative matrix factorization(NMF): Our proposed framework is also inspired by recent progress in matrix factorization algorithms. NMF has been shown to be
useful in computer vision and data mining applications including face recognition(Wang et al. 2005), object detection
(Lee and Seung 1999) and feature selection (Das Gupta and
Xiao 2011), etc. Specifically, the work in (Lee and Seung
2001) brings more attention to NMF in the research community, where the author proposed a simple multiplicative
rule to solve the problem and showed the factor coherence
of original image data. (Ding, He, and Simon 2005) shows
that if adding orthogonal constrains, the NMF is equivalent
to K-means clustering. Further, (Ding et al. 2006) presents
a work that shows, when incorporating freedom control factors, the non-negative factors will achieve a better performance on classification. In this paper, motivated by previous NMF framework for learning the latent factors, we extend these efforts significantly and propose a comprehensive
formulation which incorporates more physically-meaningful
constraints for regularizing the learning process in order to
find a proper solution. In this respect, our work is similar in
spirit to (Hu, Wang, and Kambhampati 2013) which develops a factorization approach for sentiment analysis of social
media responses to public events.

3

Notation
X
T
S
V
T0
S0
V0
R0

Description
Input data matrix
Data-term matrix
Term-sentiment matrix
Feature-sentiment matrix
Prior knowledge on T
Prior knowledge on S
Prior knowledge on V
Prior knowledge on the labels

tral and negative. However, our framework can be easily extended to handle more fine-grained sentiment.) Our goal is
to model the sentiment for each image based on visual features and available text features. Let n be the number of
images and the size of contextual vocabulary is t. We can
then easily cluster the images with similar word frequencies
and predict the cluster’s sentiment based on its word sentiment. Meanwhile, for each image, which has m-dimensional
visual features (ANPs, see below), we can cluster the images and predict the sentiment based on the feature probability. Accordingly, our basic framework takes these n data
points and decomposes them simultaneously into three factors: photo-text, text-sentiment and visual feature-sentiment.
In other words, our basic model tries to solve the following
optimization problem:

The Proposed RSAI Framework

min

In this section, we first propose the basic model of our
framework. Then we show the details of how to generate
the ANPs. After that, we describe how to obtain and leverage the prior knowledge to extend the basic model. We also
analyze the algorithm in terms of its correctness and convergence. Table 1 lists the mathematical notation used in this
paper.

3.1

Dimension
n⇥m
n⇥t
t⇥k
m⇥k
n⇥t
t⇥k
m⇥k
n⇥k

X

T SV

subject to

T

T SV T
0, S

2
F

0, V

+ kT

2

T 0 kF

(1)

0; ,

where X 2 Rn⇥m represents input data matrix, and T 2
Rn⇥t indicates the text features. That is, the ith row of matrix T corresponds to the posterior probability of the ith image’s contextual social network information referring to the
t text terms (vocabulary). Similarly, S 2 Rt⇥k indicates the
posterior probability of a text belonging to k sentiments. Finally, V 2 Rm⇥k represents the sentiment for each ANP.
The regularization term T0 is the term-frequency matrix for
the whole word vocabulary (which is built based on textual

Basic Model

Assuming that all the images can be partitioned into K sentiment (K = 3 in this paper as we focus on positive, neu-

475

descriptions of all photos). It is worth noting that the nonnegativity makes the latent components easy to interpret.
As a result of this factorization, we can readily predict the
image sentiment whether the contextual information (comments, user descriptions,etc.) is available or not. For example, if there is no social information associated with the
image, then we can directly derive the image sentiment
by applying non-negative matrix factorization for the input
data X, when we characterize the sentiment of each image
through a new matrix R = T ⇥ S. Specifically, our basic
model is similar to the probabilistic latent semantic indexing (PLSI) (Hofmann 1999) and the orthogonal nonnegative
tri-matrix factorization (Ding et al. 2006). In their work, the
factorization means the joint distribution of documents and
words.

veys strong sentiment, consequently, we also adopt one of
state-of-the-art face detection methods proposed in (Zhu and
Ramanan 2012).

3.2

3.3

Adjective Detection: Modeling the adjectives is more difficult than nouns due to the fact that there are no well defined
features to describe them. Following (Borth et al. 2013), we
collect 20,000 images associate with specific adjective tags
from Web. The a set of discriminative global features, including Gist, color histogram and SIFT, are applied for feature extraction. Finally the adjective detection is formulated
as a traditional image classification problem based on Bag
of words(BOW)model. The dictionary size of BOW is 1,000
with the feature dimension size 1,500 after dimension reduction based on PCA.

Extracting and Modeling Visual Features

In (Tighe and Lazebnik 2013; Tu et al. 2005; Yuan et al.
2013), visual content can be described by a set of midlevel visual attributes, however, most of the attributes such
as “car”, “sky”,“grass”, etc., are nouns which make it difficult to represent high level sentiments. Thus, we followed
a more tractable approach (Borth et al. 2013), which models the correlation between visual attributes and visual sentiment with adjectives, such as “beautiful” , “awesome”, etc.
The reason for employing such ANPs is intuitive: the detectable nouns (visual attributes) make the visual sentiment
detection tractable, while the adjectives add the sentiment
strength to these nouns. In (Borth et al. 2013), a large scale
ANPs detectors are trained based on the features extracted
from the images and the labeled tags with SVM. However,
we find that such pre-defined ANPs are very hard to interpret. For example the pairs like “warm pool” , “abandoned
hospital”, and it is very difficult to find appropriate features
to measure them. Moreover, in their work, during the training stage, the SVM is trained on the features extracted from
the image directly, the inability of localizing the objects and
scales bounds the detection accuracy. To address these problems, we have a two stage approach to detect ANPs based on
the Visual Sentiment Ontology (Borth et al. 2013) and train
a one vs all classifier for each ANP.

Constructing Prior Knowledge

So far, our basic matrix factorization framework provides
potential solution to infer the sentiment regarding the combination of social network information and visual features.
However, it largely ignores the sentiment prior knowledge
on the process of learning each component. In this part, we
introduce three types of prior knowledge for model regularization: (1) sentiment-lexicon of textual words, (2) the normalized sentiment strength for each ANP, and (3) sentiment
labels for each image.
Sentiment Lexicon The first prior knowledge is from a
public sentiment lexicon named MPQA corpus 2 . In this sentiment lexicon, there are 7,504 human labeled words which
are commonly used in the daily life. The number of positive words (e.g.“happy”, “terrific”) is 2,721 and the number of negative words (e.g. “gloomy”, “disappointed”) is
4,783. Since this corpus is constructed without respect to any
specific domain, it provides a domain independent prior on
word-sentiment association. It should be noted that the English usage in social network is very casual and irregular, we
employ a stemmer technique proposed in (Han and Baldwin
2011). As a result, the ill-formed words can be detected and
corrected based on morphophonemic similarity, for example
“good” is a correct version of “goooooooooooood”. Besides
some abbreviation of popular words such as “lol”(means
laughing out loud) is also added as prior knowledge. We
encode the prior knowledge in a word sentiment matrix
S0 where if the ith word belongs to jth sentiment, then
S0 (i, j) = 1, otherwise it equals to zero.

Noun Detection: The nouns in ANPs refer to the objects
presented in the image. As one of fundamental tasks in computer vision, object detection has been studied for many
years. One of most successful works is Deformable Part
Model (DPM) (Felzenszwalb et al. 2010) with Histogram of
Oriented Gradient (HOG) (Dalal and Triggs 2005) features.
In (Felzenszwalb et al. 2010), the deformable part model
has shown its capability to detect most common objects with
rigid structure such as: car, bike and non-rigid objects such
as pedestrian, dogs. (Pandey and Lazebnik 2011) further
demonstrates that DPM can be used to detect and recognize
scenes. Hence we adopt DPM to for nouns detection. The
common objects(noun) are trained by the public dataset ImageNet(Deng et al. 2009). The scene detectors are trained on
SUN dataset (Xiao et al. 2010). It is worth noting that selfie
is one of most popular images on the web (Hu, Manikonda,
and Kambhampati 2014) and face expression usually con-

Visual Sentiment In addition to the prior knowledge on
lexicon, our second prior knowledge comes from the Visual Sentiment Ontology (VSO) (Borth et al. 2013), which
is based on the well known previous researches on human
emotions and sentiments (Darwin 1998; Plutchik 1980). It
generates 3000 ANPs using Plutchnik emotion model and
associates the sentiment strength (range in[-2:2] from negative to positive) by a wheel emotion interface3 . The sample
ANP sentiment scores are shown in Table 2. Similar to the
2
3

476

http://mpqa.cs.pitt.edu/
http://visual-sentiment-ontology.appspot.com

Table 2: Sentiment strength score examples
ANP
innocent smile
happy Halloween
delicious food
cloudy mountain
misty forest
...

Sentiment Strength
1.92
1.81
1.52
-0.4
-1.00
...

Tij

Sij

min X

T SV
kT

+

+ kT S
subject to T 0, S

+ ↵ kV

2

T0 kF + kS
2

R0 kF
0, V
0

2

3.5

(2)

2V T V0 + V0T V )

+ T r(T T T

2T T T0 + T0T T0 )

+ T r(S T S

2S T S0 + S0T S0 )

+ T r(S T T T T S

[T T T SV T V + S + T T T S]ij
s
[X T T S + ↵V0 ]ij
Vij
[V S T T T T S + ↵V ]ij

(5)

(6)

Algorithm Correctness and Convergence

4

Empirical Evaluation

We now quantitatively and qualitatively compare the proposed model on image sentiment prediction with other candidate methods. We also evaluate the robustness of the proposed model with respect to various training samples and
different combinations of prior knowledge. Finally, we perform a deeper analysis of our results.

2X T T SV T + V S T T T T SV T )

+ ↵T r(V T V

[T T XV + S0 + T T R0 ]ij

In this part, we prove the guaranteed convergence and correctness for Algorithm 1 by the following two theorems.
Theorem 1. When Algorithm 1 converges, the stationary
point satisfies the Karush-Kuhn-Tuck(KKT) condition, i.e.,
Algorithm 1 converges correctly to a local optima.
Theorem 2. The objective function is nondecreasing under the multiplicative rules of Eq (4), Eq (5) and Eq (6), and
it will converge to a stationary point.
The detailed proof is presented in Appendix.

where ↵ 0,
0,
0 and
0 are parameters controlling the extent to which we enforced the prior knowledge
on the respective components. The model above is generic
and allows flexibility . For example, if there is no social information available for one image, we can simply set the
corresponding row of T0 to zeros. Moreover, the square loss
function leads to an unsupervised problem for finding the
solutions. Here, we re-write Eq (2) as :
L =T r(X T X

s

Input: X, T0 , S0 , V0 , R0 , ↵, , ,
Output: T, S, V
Initialization: T, S, V
while Not Converge do
Update T using Eq(4) with fixed S,V
Update S using Eq(5) with fixed T,V
Update V using Eq(6) with fixed T,S
End

2
V 0 kF

S 0 kF

(4)

Algorithm 1 Multiplicative Updating Algorithm

After defining the three types of prior knowledge, we incorporate them into the basic model as regularization terms in
following optimization problem:
T SV

[T SV T V S T + T + T SS T ]ij

The learning process consists of an iterative procedure using Eq (3), Eq (4) and Eq (5) until convergence. The description of the process is shown in Algorithm 1.

Incorporating Prior Knowledge

2
F

Sij

Vij

Sentiment labels of Photos Our last prior knowledge focuses on the prior knowledge on the sentiment label associated with the image itself. As our framework essentially is
a semi-supervised learning approach, this leads to a domain
adapted model that has the capability to handle some domain
specific data. The partial label is given by the image sentiment matrix R0 where R0 2 Rn⇥k . For example if the ith
image belongs to jth sentiment, the R0 (i, j) = 1 otherwise
R0 (i, j) = 0. The improvement by incorporating these label
data is empirically verified in the experiment section.

T

[XV S T + T0 + R0 S T ]ij

Next, we use the similar update rule to update S and V :

word sentiment matrix S0 , the prior knowledge on ANPs V0
is the sentiment indicator matrix.

3.4

Tij

s

4.1

(3)

Experiment Settings

We perform the evaluation on two large scale image datasets
collected from Flickr and Instagram respectively. The collection of Flickr dataset is based on the image IDs provided by (Yang et al. 2014), which contains 3,504,192 images from 4,807 users. Because some images are unavailable
now, and without loss of generality, we limit the number of
images from each user. Thus, we get 120,221 images from
3921 users. For the collection of the Instagram dataset, we
randomly pick 10 users as seed nodes and collect images by

2S T T T R0 + R0T R0 )

From Eq (3) we can find that it is very difficult to solve
T , S and V simultaneously. Thus we employ the alternating
multiplicative updating scheme shown in (Ding et al. 2006)
to find the optimal solutions. First, we use fixed V and S to
update T as follows:

477

It is worth noting that in the proposed model, tags are not
included as input feature.
The results of comparison are shown in Table 3. We employ 30% data for training and remaining for testing. To verify the reliability of tags labeled images, we also included
20000 labeled Flickr images with primary sentiment label.
Especially, the classifier setting for SentiBank and EL followed the original papers. The classifier of Sentibank is logistic regression and for EL it is SVM. From the results we
can see that, the proposed method performs best in both
datasets. Noting that proposed method improved 10% and
6% over state-of-the-art methods (Borth et al. 2013). Results from proposed method are shown in Figure 4. Noting
that the number we reported in Table 3 is the prediction accuracy for each method.
From the table, we can see that, even though noise exists
in the Flickr and Instagram dataset, the results are similar
to the performance on human labeled dataset. Another interesting observation is that the performance of EL on Instagram is worse than on Flickr, one reason could be that the
wide usage of ”picture filters” lowers discriminative ability
of the low level visual features, while the models based on
the mid level attributes can easily avoid this filter ambiguity.
Another interesting observation is that our basic model performs fairly well even if it does not incorporate the knowledge from sentiment strength of ANPs, which indicates that
the object based ANPs by our method are more robust than
the features used in (Borth et al. 2013).
Fine Grained Sentiment Prediction: Although our
motivation is to predict the sentiment (positive, negative)
on the visual data, to show the robustness and extension
capability of the proposed model, we further evaluate the
proposed model on a more challenging task in social media; predicting human emotions. Based on the definition of
human emotion (Ekman 1992), our fine grained sentiment
study labels the user posts with following human emotion
categories including: happiness, amusement, disgust, anger,
fear and sadness. The results on 20000 manually labeled
flickr post are shown in Figure 5. Compared to sentiment
prediction, fine grained sentiment prediction would give us
more precise user behavior analysis and new insights on the
proposed model.
As Figure 5 shows, compared to SentiBank and EL, the
proposed method has the highest average classification accuracy and the variance of proposed method on these 6 categories is smaller than that of the baseline methods, which
demonstrates the potential social media applications of the
proposed method such as predicting social response. We noticed that the sad images have the highest prediction accuracy, and both disgust and anger are difficult to predict. Another observation is the average performance of positive categories, happiness and amusement, is similar to the negative
categories. Explaining reason for this drives us to dig deeper
into sentiment understanding in the following section.

traversing the social network based on breadth first search.
The total number of images from Instagram is 130,230 from
3,451 users.
Establishing Ground Truth: For training and evaluating
the proposed method, we need to know the sentiment labels. Thus, 20,000 Flickr images are labeled by three human
subjects, the majority voting is employed. However, manually acquiring the labels for these two large scale datasets
is expensive and time consuming. Consequently, the rest of
more than 230,000 images are labeled by the tags, which
was suggested by the previous works (Yang et al. 2014;
Go, Bhayani, and Huang )4 . Since labeling the images based
on the tags may cause noise issue, and for better reliability we only label the images with primary sentiment labels, which include: positive, neutral and negative. It is
worth noting that the human labeled images have both primary sentiment labels and fine grained sentiment labels. The
fine grained labels, including: happiness, amusement, anger,
fear, sad and disgust, are used to for fine grained sentiment
prediction.
The comparison methods include: Senti API5 , SentiBank
(Borth et al. 2013), EL(Yang et al. 2014) and the baseline
method.
• Senti API is a text based sentiment prediction API, it
measures the text sentiment by counting the sentiment
strength for each text term.
• SentiBank is a state-of-the-art visual based sentiment prediction method. The method extracts a large number of visual attributes and associates them with a sentiment score.
Similar to Senti API, the sentiment prediction is based on
the sentiment of each visual attributes.
• EL is a graphical model based approach, it infers the sentiment based on the friend interactions and several low
level visual features.
• Baseline: The baseline method comes from our basic
model. To compare it fairly, we also introduce R0 with
the basic model which makes the baseline method have
the ability to learn from training data.

4.2

Performance Evaluation

Large scale image sentiment prediction: As mentioned
in Sec 3, the proposed model has the flexibility to incorporate the information and capability to jointly learn from the
visual features and text features. For each image, the visual
features are formed by the confidence score of each ANP
detector, the feature dimension is 1200, which is as large as
VSO (prior knowledge V0 ). For the text feature, it is formed
based on the term frequency and the dimension relies on the
input data. To predict the label, the model input is unknown
data X 2 Rn⇥m and its corresponding text feature matrix
T0 2 Rn⇥t , where n is the number of images, m = 1200
and t is the vocabulary size, we decompose it via Aglorithm
1 and get the label based on max pooling each row of X ⇤ V .

4.3

4

More details can be found in(Yang et al. 2014) and (Go,
Bhayani, and Huang )
5
http://sentistrength.wlv.ac.uk/,a text based sentiment prediction API

Analysis and Discussion

In this section, we present an analysis of parameters for the
proposed method and the results of the proposed method.
Specifically, in last section we have studied the performance

478

(a) Negative

(b) Neutral

(c) Positive

Figure 3: Sample tag labeled images from Flickr and Instagram.

(a) Negative

(b) Neutral

(c) Positive

Figure 4: Sample results from our proposed method. Photos with red bounding box are false positive predictions.
• RQ2:Since the proposed method is better than pure visual
feature based method, How does the model gain?
First, we start with RQ1 by extracting the visual features
used in (Borth et al. 2013) and (Yang et al. 2014) for each
image in the Flickr and Instagram datasets. Then we use
k-means clustering to obtain 3 clusters of images for each
dataset, where the image similarity is measured as Euclidean
distance in the feature spaces. Based on each cluster center,
we used the classfier trained in the previous experiment for
cluster labeling. The results are shown in Figure 6. The xaxis is the different class label for each dataset and the y-axis
is the number of images that belong to each cluster. From the
results, we notice that the “visual affective gap” does exist
between human sentiment and visual features. For the stateof-the art method (Borth et al. 2013), the neural images are
largely misclassified based on the visual features. While for
(Yang et al. 2014), we observe t the low level features, e.g.,
color histogram, contrast and brightness, are not closely related to human sentiment as visual attributes.
We further analyze the performance of the proposed
method based on these 40,000 images.
Parameter study: In the proposed model, we incorporate three types of prior knowledge: sentiment lexicon, sentiment labels of photos and visual sentiment for ANPs. It is
important and interesting to explore the impact of each of

Figure 5: Fine grained sentiment prediction results (Y-axis
represents the accuracy for each method).

of different methods. In this part, our objective is to have
deeper understanding on the datasets and the correlation between different features and the sentiments embedded in the
images. Without loss of generality, we collected additional
20k images from Flickr and Instagram respectively (totally
40K) and we address the following research questions:
• RQ1:What is the relationship between visual features and
visual sentiments?

479

Table 3: Sentiment Prediction Results. The number means prediction accuracy, the higher the better.
20000 Flickr
Flickr
Instagram

Senti API
0.32
0.34
0.27

SentiBank
0.42
0.47
0.56

Figure 6: Sentiment distribution based on visual features.
From left to rigth is number of positive, neutral, negative images in Instagram and Flickr, receptively. Y axis represents
the number of images.

Baseline
0.48
0.48
0.54

Proposed method
0.52
0.57
0.62

Figure 7: Performance gain by incorporating training data.

them on the performance of the proposed model. Figure 7
presents the average results (y-axis) of two datasets on sentiment prediction with different amount of training data (xaxis)6 , where the judgment is on the same three sentiment
labels with different combinations respectively. It should be
noted that each combination is optimized by Algorithm 1,
which has similar formulations. Moreover, we set the same
parameter for ↵, , and (0.9, 0.7, 0.8 and 0.7). Results
give us two insights. First, employing more prior knowledge
will make the model more effective than using only one type
of prior knowledge. For our matrix factorization framework,
T and V have independent clustering freedom by introducing S, thus it is natural to add more constraints for desired
decomposed component. Second, when no training data, the
basic model with S0 performs much better than SentiAPI
(refer Table 3), which means incorporating ANPs significantly improves image sentiment prediction. It is worth noting that there is no training stage for the proposed method.
Thus when compared to fully supervised approaches, our
method is more applicable in practice when the label information is unavailable.
Bridging the Visual Affective Gap (RQ2): Figure 1 and
Figure 7 demonstrate that a visual affective gap exists between visual features and human sentiments (i.e., the same
visual feature may correspond to different sentiments in different context). To bridge this gap, we show that one possible solution is to utilize heterogeneous data and features
available in social media to augment the visual feature-based
sentiment. In the previous parameter study, we have studied the importance of the prior knowledge. Furthermore, we
study importance of which contains the degree of contextual social information used in the proposed model. From
6

EL
0.47
0.45
0.37

Figure 8: The value of versus model performance. X axis
is value, y axis is value of model performance.
Figure 8, we can observe that the performance of the proposed model increases along the value of . However, when
is greater than 0.8, the performance drops. This is because
textual information in social media data is usually incomplete. Larger will cause negative effects on the prediction
accuracy where there is none or little information available.

5

Conclusion and Future Work

Can we learn human sentiments from the images on the
web? In this paper, we proposed a novel approach for visual sentiment analysis by leveraging several types of prior
knowledge including: sentiment lexicon, sentiment labels
and visual sentiment strength. To bridge the “affective gap”
between low-level image features and high-level image sentiment, we proposed a two-stage approach to general ANPs
by detecting mid-level attributes. For model inference, we
developed a multiplicative update algorithm to find the optimal solutions and proved the convergence property. Experiments on two large-scale datasets show that the proposed
model is superior to other state-of-the-art models in both inferring sentiment and fine grained sentiment prediction.

The experiments setting is as same as discussed above.

480

In the future, we will employ crowdsourcing tools, such
as AmazonTurk7 , to obtain high-quality, manually-labeled
data to test the proposed method. Furthermore, inspired by
the recent development of advanced deep learning algorithms and their success in image classification and detection tasks, we will follow this research direction to perform the sentiment analysis via deep learning. In order to
have a robust trained architecture and network parameters,
we will focus on the deep learning models that work for
smaller dataset. Moreover, beyond sentiment analysis, we
will study social event and social response (Hu et al. 2012;
Hu, Manikonda, and Kambhampati 2014) via visual data in
the social media.

6

Felzenszwalb, P. F.; Girshick, R. B.; McAllester, D.; and Ramanan, D. 2010. Object detection with discriminatively trained
part-based models. Pattern Analysis and Machine Intelligence,
IEEE Transactions on 32(9):1627–1645.
Go, A.; Bhayani, R.; and Huang, L. Twitter sentiment classification using distant supervision.
Han, B., and Baldwin, T. 2011. Lexical normalisation of short
text messages: Makn sens a# twitter. In Proceedings of the 49th
Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, 368–378. Association for Computational Linguistics.
Hofmann, T. 1999. Probabilistic latent semantic indexing. In
Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,
50–57. ACM.
Hu, M., and Liu, B. 2004. Mining and summarizing customer
reviews. In Proceedings of the tenth ACM SIGKDD international
conference on Knowledge discovery and data mining, 168–177.
ACM.
Hu, Y.; John, A.; Wang, F.; and Kambhampati, S. 2012. Et-lda:
Joint topic modeling for aligning events and their twitter feedback. In Proceedings of the 6th AAAI Conference.
Hu, Y.; Manikonda, L.; and Kambhampati, S. 2014. What we
instagram: A first analysis of instagram photo content and user
types.
Hu, Y.; Wang, F.; and Kambhampati, S. 2013. Listening to the
crowd: automated analysis of events via aggregated twitter sentiment. In Proceedings of the Twenty-Third international joint
conference on Artificial Intelligence, 2640–2646. AAAI Press.
Jia, J.; Wu, S.; Wang, X.; Hu, P.; Cai, L.; and Tang, J. 2012.
Can we understand van gogh’s mood?: learning to infer affects
from images in social networks. In Proceedings of the 20th ACM
international conference on Multimedia, 857–860. ACM.
Lee, D. D., and Seung, H. S. 1999. Learning the parts of objects
by non-negative matrix factorization. Nature 401(6755):788–
791.
Lee, D. D., and Seung, H. S. 2001. Algorithms for non-negative
matrix factorization. In Advances in neural information processing systems, 556–562.
Liu, B. 2012. Sentiment analysis and opinion mining. Synthesis
Lectures on Human Language Technologies 5(1):1–167.
Machajdik, J., and Hanbury, A. 2010. Affective image classification using features inspired by psychology and art theory. In
Proceedings of the international conference on Multimedia, 83–
92. ACM.
Pandey, M., and Lazebnik, S. 2011. Scene recognition and
weakly supervised object localization with deformable partbased models. In Computer Vision (ICCV), 2011 IEEE International Conference on, 1307–1314. IEEE.
Pang, B., and Lee, L. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd annual meeting on Association for Computational Linguistics, 271. Association for
Computational Linguistics.
Pang, B.; Lee, L.; and Vaithyanathan, S. 2002. Thumbs up?:
sentiment classification using machine learning techniques. In
Proceedings of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10, 79–86. Association for
Computational Linguistics.

Acknowledgment

Yilin Wang and Baoxin Li are supported in part by a grant
(#1135616) from the National Science Foundation. Kambhampati’s research is supported in part by the ARO grant
W911NF-13-1- 0023, and the ONR grants N00014-131-0176, N00014-13-1-0519 and N00014-15-1-2027. Any
opinions expressed in this material are those of the authors
and do not necessarily reflect the views of the funding agencies.

References
Borth, D.; Ji, R.; Chen, T.; Breuel, T.; and Chang, S.-F. 2013.
Large-scale visual sentiment ontology and detectors using adjective noun pairs. In Proceedings of the 21st ACM international
conference on Multimedia, 223–232. ACM.
Dalal, N., and Triggs, B. 2005. Histograms of oriented gradients
for human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on,
volume 1, 886–893. IEEE.
Darwin, C. 1998. The expression of the emotions in man and
animals. Oxford University Press.
Das Gupta, M., and Xiao, J. 2011. Non-negative matrix factorization as a feature selection tool for maximum margin classifiers. In Computer Vision and Pattern Recognition (CVPR), 2011
IEEE Conference on, 2841–2848. IEEE.
De Choudhury, M.; Counts, S.; and Gamon, M. 2012. Not all
moods are created equal! exploring human emotional states in
social media. In Sixth International AAAI Conference on Weblogs and Social Media.
Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-Fei,
L. 2009. Imagenet: A large-scale hierarchical image database.
In Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on, 248–255. IEEE.
Ding, C.; Li, T.; Peng, W.; and Park, H. 2006. Orthogonal nonnegative matrix t-factorizations for clustering. In Proceedings of
the 12th ACM SIGKDD international conference on Knowledge
discovery and data mining, 126–135. ACM.
Ding, C. H.; He, X.; and Simon, H. D. 2005. On the equivalence
of nonnegative matrix factorization and spectral clustering. In
SDM, volume 5, 606–610. SIAM.
Ekman, P. 1992. An argument for basic emotions. Cognition &
Emotion 6(3-4):169–200.
7

https://www.mturk.com/mturk/welcome

481

point to the local minima when
s
[X T T S + ↵V0 ]ij
Vij = Vij
[V S T T T T S + ↵V ]ij

Plutchik, R. 1980. Emotion: A psychoevolutionary synthesis.
Harper & Row New York.
Song, M.; Tao, D.; Liu, Z.; Li, X.; and Zhou, M. 2010. Image
ratio features for facial expression recognition application. Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on 40(3):779–788.
Taboada, M.; Brooke, J.; Tofiloski, M.; Voll, K.; and Stede, M.
2011. Lexicon-based methods for sentiment analysis. Computational linguistics 37(2):267–307.
Tighe, J., and Lazebnik, S. 2013. Finding things: Image parsing
with regions and per-exemplar detectors. In Computer Vision and
Pattern Recognition (CVPR), 2013 IEEE Conference on, 3001–
3008. IEEE.
Tu, Z.; Chen, X.; Yuille, A. L.; and Zhu, S.-C. 2005. Image
parsing: Unifying segmentation, detection, and recognition. International Journal of Computer Vision 63(2):113–140.
Wang, Y.; Jia, Y.; Hu, C.; and Turk, M. 2005. Non-negative
matrix factorization framework for face recognition. International Journal of Pattern Recognition and Artificial Intelligence
19(04):495–511.
Xiao, J.; Hays, J.; Ehinger, K. A.; Oliva, A.; and Torralba, A.
2010. Sun database: Large-scale scene recognition from abbey to
zoo. In Computer vision and pattern recognition (CVPR), 2010
IEEE conference on, 3485–3492. IEEE.
Yang, Y.; Jia, J.; Zhang, S.; Wu, B.; Li, J.; and Tang, J. 2014.
How do your friends on social media disclose your emotions?
You, Q.; Luo, J.; Jin, H.; and Yang, J. 2015. Robust image sentiment analysis using progressively trained and domain transferred
deep networks.
Yuan, J.; Mcdonough, S.; You, Q.; and Luo, J. 2013. Sentribute:
image sentiment analysis from a mid-level perspective. In Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining, 10. ACM.
Zhu, X., and Ramanan, D. 2012. Face detection, pose estimation,
and landmark localization in the wild. In Computer Vision and
Pattern Recognition (CVPR), 2012 IEEE Conference on, 2879–
2886. IEEE.

A

Then the Eq (9) is equivalent to
(2(V S T T T T S + ↵V )

Theorem 2. The objective function is nondecreasing
under the multiplicative rules of Eq (4), Eq (5) and Eq (6),
and it will converge to a stationary point.
Proof of Theorem 2. First, let H(V ) be:
H(V ) = T r((V S T T T T S + ↵V )V T

h(V, Ve ) =

ik

Vf
ik

ik

X

(X T T S + ↵V0 + µ)ik Vik (1 + log

H(V 0 ) = h(V 0 , V 0 )

h(V 0 , V 1 ) = H(V 1 )....

(13)

Thus, with the alternate updating rule of V, S and T , we
have the following inequality chain:

µ (7)

L(V 0 , T 0 , S 0 )

L(V 1 , T 1 , S 0 )....
(14)
Since L(V, S, T )
0. Thus L(V, S, T ) is bounded and
the Algorithm 1 converges , which completes the proof.

Where µ is Lagrangian multiplier µij enforces the nonnegativity constraint on Vij . From the complementary slackness condition, we can obtain
(2(V S T T T T S + ↵V )

X (Ve (V S T T T T S + ↵V ))ik V 2

Vik
)
g
V
ik
ik
(12)
Since z (1 + log z), 8z > 0 and similar in (Ding et al.
2006), the first term in h(V, Ve ) is always larger than that in
H(V ), then the inequality holds h(V, Ve ) H(V ). And it is
easy to see h(V, Ve ) = H(V ), thus h(V, Vb ) is an auxiliary
function of H(V ). Then we have the following inequality
chain:

Proof of Theorem 1. We prove the theorem when updating V using Eq (6), similarly, all others can be proved in
the same way. First we form the gradient of L regards V as
Lagrangian form:
2(X T S + ↵V0 )

(X T T S + ↵V0 + µ)V T )
(11)

and it is very easy to verify that H(V ) is the Lagrangian
function of Eq (3) with KKT condition. Moreover, if we
can verify that the update rule of Eq (4) will monotonically decrease the value of H(V ), then it means that the
update rule of Eq (4) will monotonically decrease the value
of L(V )(recall Eq (3)). Here we complete the proof by constructing the following an auxiliary function h(V, Ve ).

Theorem 1. When Algorithm 1 converges, the stationary
point satisfies the Karush-Kuhn-Tuck(KKT) condition, i.e.,
Algorithm 1 converges correctly to a local optima.

T

2(X T T S + ↵V0 ))ij Vij2 = 0 (10)

This is same as the fixed point of Eq (8),i.e., either
Vij = 0 or the left factor is 0. Thus if Eq (10) holds the Eq
(8) must hold and vice versa.

Appendix

@L
= 2(V S T T T T S + ↵V )
@V

(9)

2(X T T S + ↵V0 ))ij Vij = 0 (8)

This is the fixed point relation that local minima for V
must hold. Given the Algorithm 1., we have the convergence

482

L(V 1 , T 0 , S 0 )

Human Activity Encoding and Recognition Using Low-level Visual Features
Zheshen Wang and Baoxin Li
Department of Computer Science and Engineering
Arizona State University
{zheshen.wang, baoxin.li}@asu.edu

Abstract
Automatic recognition of human activities is among
the key capabilities of many intelligent systems with
vision/perception. Most existing approaches to this
problem require sophisticated feature extraction
before classification can be performed. This paper
presents a novel approach for human action recognition using only simple low-level visual features:
motion captured from direct frame differencing. A
codebook of key poses is first created from the
training data through unsupervised clustering.
Videos of actions are then coded as sequences of
super-frames, defined as the key poses augmented
with discriminative attributes. A weighted-sequence
distance is proposed for comparing two super-frame
sequences, which is further wrapped as a kernel
embedded in a SVM classifier for the final classification. Compared with conventional methods, our
approach provides a flexible non-parametric sequential structure with a corresponding distance
measure for human action representation and classification without requiring complex feature extraction. The effectiveness of our approach is
demonstrated with the widely-used KTH human
activity dataset, for which the proposed method
outperforms the existing state-of-the-art.

1

Introduction

Being able to recognize human activities in video is a critical
capability of an intelligent vision system in applications such
as video surveillance, content-based video retrieval, and
human-robot interaction. On this task, human vision still
outperforms any existing automatic techniques. Humans tend
to describe, remember, perform and compare an action as a
sequence of key poses of the body. For example, in both
choreography and sports, body movements are usually described as an ordered sequence of key poses. Most of the
existing vision techniques attempt to mimic this to certain
degree, explicitly or implicitly, by modeling and classifying
human actions based on key poses and their orders. Unfortunately, such techniques typically reply on sophisticated

feature extraction (e.g., explicit detection and tracking of
body parts, or doing so implicitly by complex representation
and detection of the body motion through high-dimensional
spatial-temporal features), which are a challenging task on its
own especially considering the wide variability of acquisition
condition.
One interesting observation is that, humans can casually
take a glimpse of a video clip (even at a critically-downsampled resolution) and recognize the underlying
action correctly without careful thinking. The recognition
can also be done accurately with optical flow only without
the original video clip. This may suggest that only very rudimentary visual features are necessary for action recognition.
In this work, we propose a novel approach to video-based
recognition of human actions using simple visual features:
motion captured in direct frame differencing. In the proposed
approach, a codebook of atom poses is first formed using
unsupervised clustering of difference frames from videos of
various actions. A video is then coded based on its corresponding sequence of atom poses augmented with other discriminative attributes such as the duration of the poses, resulting in a stream of (pose, attributes) couples. Further, we
introduce a novel weighted-sequence distance (WSD)
measure for comparing the similarity between two sequences,
based on not only the constituent poses but also their
attributes plus the global structure of the pose sequences. The
compact metric WSD is further embedded into a Support
Vector Machine (SVM) classifier as a kernel for classifying
the coded video clips. As the result, the proposed approach
does not rely on sophisticated feature extraction but rather
direct frame differencing and thus it is more universally
applicable for various acquisition conditions such as different
imaging sensors or illumination conditions.
The rest of this paper is organized as follows. We first
review related literature in Section 2. The action coding
scheme and the WSD are proposed in Section 3 and 4 respectively. Classification strategies are described in Section
5. In Section 6, experiments with a real-world dataset are
presented and analyzed. We conclude in Section 7 with a
brief discussion on future directions.

1876

2 Related Work
Existing approaches for human action recognition can be
generally classified into two categories: graphical-model-based approaches and bag-of-words approaches.
The former utilizes a graphical structure consisting of states
and models the activities as sequential transitions between
the states. Hidden Markov Model (HMM) [Niu and Abdel-Mottaleb, 2005] is one of the most commonly-used examples in this category. Other sophisticated extensions include Abstract HMM [Bui and Venkatesh, 2002], Hidden
Permutation Model [Bui et al., 2008], Relational Markov
Networks [Liao, 2005], etc. While being capable of modeling
temporal transitions in a sequence, these methods usually
suffer from an excess of parameters and overfitting of the
data with insufficient training samples. In addition, the rigid
structure also constrains its flexibility of dealing with structural variations among different sequences. Bag-of-words
approaches have become popular in recent years. Methods in
this category view the activity recognition problem as a
standard classification problem which involves two stages:
feature extraction and classification. Typically, the feature
extraction step is very complex (such as spatial-temporal
word feature proposed in [Dollar et al., 2005]), and additional
pre-processing steps or extra information are often required
(e.g., single action circle segmentation [Schindler and Gool,
2008], accurate alignment [Wong et al, 2007; Kim et al.,
2007], accurate scale information [Liu and Shah, 2008], etc.).
Often, the feature vectors are treated as independent data
points in the classification stage (e.g., [Niebles et al., 2008]).
Therefore, in general, these approaches simplify temporal
sequences into a set of independent features and thus while
local temporal information is encapsulated in the extracted
spatial-temporal descriptors, global temporal information is
not utilized. Nowozin et al. proposed a discriminative subsequence mining approach [Nowozin et al., 2007] which
employs spatial-temporal features provided by [Dollar et al.,
2005] and further formed a sequential representation of the
original videos. However, it does not achieve noticeable
improvements over previous approaches.
The proposed approach attempts to incorporate both local
spatial-temporal information (through unsupervised clustering of motion-capturing difference frames in forming the key
poses) and global temporal structure (through a novel WSD
metric for comparing sequences of poses and an SVM classifier based on it) into a unified action coding and classification scheme.

3

Action Coding

In this section, we present the action coding scheme based on
the (pose, attributes) couples, which is termed as super-frames. Formally, a super-frame f is defined as a 2-tuple
(c, w) which consists of an atom pose c and its attributes w.
The set of atom poses form a codebook. Each frame of a
video is first assigned a codeword c and then adjacent frames
with the same codeword are merged with the attribute w

denoting the duration of the same codeword. With this
strategy, the coded sequence reflects the local spatial-temporal information through the codewords (which are
based on frame differencing) while retaining the global
temporal order of the original sequence. This results in a
compact yet descriptive representation of the original video
clip. Details of codebook learning and sequence coding are
elaborated in the following sub-sections.

3.1 Codebook Learning
We create a codebook from the training data through clustering difference frames, computed as the difference between
two nearby frames (not necessarily consecutive ones if the
video frame rate is high and thus the motion magnitude is not
large between consecutive frames). The distance measure for
clustering is based on Euclidean distance between two difference frames. For simplicity, we will still call the difference
frames as “frames”. The centroids from the clustering stage
are kept as atom poses in the codebook. It is worth emphasizing that, although we use the term “atom pose” for the
codeword, the centroids are not original video frames but
rather a representation of the difference frames, which capture the local spatial-temporal variation of the underlying
actions. (Another reason we use difference frames instead of
the original frames is that they may naturally be robust to
cluttered background since the difference is taken between
nearby frames, which presumably have similar clutter signatures, assuming a decent frame-rate.) A learning approach
automatically covers variations present in the training set,
such as scale changes. Figure 1 illustrates some sample atom
poses and their corresponding video frames.

A

B

C

D

E

F

G

Figure 1: An illustration of atom poses (the top row) and their corresponding video frames (the bottom row).

Like any unsupervised clustering scheme, the choice of
the number of clusters is an issue. To address this problem,
we introduce a distance matrix which records pair-wise
Euclidean distances (normalized to [0, 1]) among all codewords in the codebook. The distance matrix is then taken into
consideration in computing the weighted-sequence distance
between two super-frame sequences. More details are discussed in Section 4.

3.2 Video Coding Based on Super-frames
With an established codebook, we code a given video clip by
the following three steps: (1) Assign a codeword to each
frame based on similarities between the frame and each codeword; (2) Group adjacent frames with the same codeword

1877

and store the duration of the codeword in the attribute, resulting in a super-frame; (3) Store all the super-frames in
their original temporal order in the video.
Figure 2 illustrates two coded sequences using key poses
in Figure 1.

(ci(1) , wi(1) ) : (A, 4)

(B, 5) (C, 4) (D, 5) (C, 6) (B, 6)
(a) Super-frame sequence s(1) for a “hand clapping” clip

(2)
:
(c (2)
j , wj )

(A, 6) (F, 7)
(G, 8) (F, 5)
(E, 4)
(b) Super-frame sequence s(2) for a “hand waving” clip
Figure 2: Two examples of coded sequences (Weights here are
shown as the number of frames each group includes. In the
experiments, we use normalized frame numbers as weights.)

4

Firstly, “characters” in a super-frame sequence are atom
poses with corresponding weights (which may reflect the
significance of the atom pose). Intuitively, operations on a
crucial atom pose (e.g., one lasting for a longer period)
should cost more than on a less important one. For example,
deleting a super-frame of significant length during comparison should result in a large cost for a relative short video.
Secondly, the similarity between the atom poses varies and
thus in operations, such as Substitution, the cost of the operation relies on what to use for the replacement. For example,
in Figure 3, we assume that the distance between two characters equals the color difference between the corresponding
bars. Obviously, to substitute “B” in the “ABC” sequence,
“D” would cost less than using “E”, since the color of “D” is
much closer to “B” than “E”. As mentioned earlier, such
information is kept in a distance matrix in the codebook
creation step and thus we should be able to systematically
address such issues.
In the following sub-sections, we propose a
weighted-sequence distance (WSD), which is able to address
both of the above practical issues.

4.1 WSD for Super-frames

Weighted-sequence Distance

After a video clip is coded as a super-frame sequence, action
recognition boils down to classification of the super-frame
sequences, for which a measure of the distance between two
data points (super-frame sequences) is needed. In this section, we propose a generalized version of the classical Levenshtein Distance (also known as String Edit Distance
[Levenshtein, 1966]), which takes both the attribute and the
character (codeword) distances into consideration.
String distance/similarity problems widely appear in many
areas of computer science. For example, in Web search,
string matching/text comparison is one of the basic problems
for text-based retrieval (e.g., [Crochemore and Rytter, 1994];
[Cancedda et al., 2003]); in computational biology, string
matching techniques are often used for comparing biological
patterns (e.g., [Leslie et al., 2004]). Edit distance, a basic
string similarity metric, is defined as the minimum number of
operations (including Copy/Substitution, Insertion and Deletion) required for turning one string to the other [Levenshtein, 1966]. Typically, fixed costs are assigned to each
operation respectively in computing the overall cost of a
series of editing operations. This formulation assumes that
characters are of equal importance and that distance between
two characters is binary (either “same” or “different”).
However, in our problem of comparing super-frame sequences, these assumptions are no longer reasonable.

We define a weighted character a (e.g., the super-frame f in
our problem) as a 2-tuple (c, w) which consists of the label c
(e.g., the codeword in our super-frame formulation) and its
weight w. Then a weighted string can be written as
(1)
s {a1 , a2 ,..., an }, ai (ci , wi ), i 1,..., n
where n is the number of characters in s . Assume that we
have two weighted strings s(1) and s(2):

s (1)

{a1(1) , a2(1) ,..., an(1)1 }, ai(1)

( ci(1) , wi(1) ), i 1,..., n1 (2)

s ( 2)

{a1( 2) , a2( 2) ,..., an( 22 ) }, a (j 2)

(c (j 2) , w (j 2) ), j 1,..., n2 (3)

A (n1 n2 ) (n1 n2 ) symmetric matrix Dc (with zero elements on the diagonal) records pair-wise distances of the
vocabulary (range of values in Dc is [0, 1]):
(4)
{c1(1) , c2(1) ,..., cn(1)1 , c1( 2) , c2( 2) ,..., cn( 22) }
Then the weighted-sequence distance between s(1) and s(2) is
defined as the sum of costs caused by operations for turning
s(1) to s(2):
(5)
DWSD ( s(1) , s (2) )
Costl
l 1,..., L

in which L is the number of operations involved; Costl denotes the required cost for the lth operation. Three types of
editing operations: Substitution, Insertion and Deletion and
corresponding costs are defined as follows:
Cost S (a(1) , a(2) )
Cost I (a (1) , a (2) )
Cost D ( a (1) , a (2) )

Figure 3: An illustration of distances between two characters.

S

(1)

1

| w(1) w(2) | min{w(1) , w(2) } Dc (c(1) , c(2) ) (6)
w(2)

2
3

(2)

w(1)

(7)
(8)
(1)

where Cost (a , a ) denotes the cost of substituting a in
s(1) by a(2) from s(2). CostI(a(1), a(2)) indicates inserting a(2) to

1878

s(1). CostD(a(1), a(2)) means deleting a(1) from s(1) when comparing a(1) and a(2). 1 , 2 and 3 are weights for balancing
the components. We use 1
1/ k in our experi2
3
ments, where k is the codebook size.
With the above definitions, we propose an algorithm as
shown in Figure 4 for computing the WSD by extending the
conventional Edit Distance algorithm based on dynamic
programming. Landau and Vishkin [Landau and Vishkin,
1989] have shown that classical edit distance problem can be
solved in O(mn) time using dynamic programming. Since our
generalized version does not change the structure of the
original algorithm, it still maintains the same computational
complexity.
Algorithm: Weighted-Sequence Distance (WSD) computes the weighed-sequence distance between two
weighted-sequences s(1) and s(2) with given distance matrix Dc.
Input: Weighed-sequence s(1) and s(2) and distance
matrix Dc.
1 if n1 = 0
;
2
return
w(2)
j
end
if n2 = 0
return

wi(1) ;
i 1,..., n1

6 end
7 Construct an empty matrix M ;
8 Initial the first row of M as w11,(w11 w12 ),...,

wi1 ;
i 1,...,n1

9 Initial the first column of M as w12 ,(w12 w22 ),...,

w2j ;
j 1,...,n2

10 while i n1 do
11
while j n2 do
12
Compute the following costs respectively:
13

Cost copy / substitution

14

Cost insertion

;
Cost I (ai(1) , a(2)
j )

15

Cost deletion

;
Cost D (ai(1) , a(2)
j )

16

Let M (i 1, j 1)

;
Cost S (ai(1) , a(2)
j )

17

min{M (i, j 1) Cost insertion ,

18

deletion

M (i 1, j ) Cost

19
20
end
21 end
22 DWSD M (n1 1, n2
WSD

23 return D

M (i, j ) Cost

In this sub-section, we analyze the properties of the WSD
defined in the previous subsection, which serve to verify that
WSD is a desired distance measure for comparing two
weighted-sequences.
Property 1 (Zero Copy Cost): Assume that a(1) = (c(1),w(1))
and a(2) = (c(2),w(2)) are two weighted characters from sequence s(1) and s(2), respectively. If Dc(c(1), c(2)) = 0 and
w(1)=w(2), then the substitution cost for a(1) and a(2) in turning
s(1) and s(2) equals zeros:
(9)
Cost S ( a (1) , a (2) ) 0
This property can be easily proved by inserting Dc(c(1),c(2))=0
and w(1)=w(2) into Eq. (6). It means that when the two
weighted characters are equal, Substitution is reduced to
Copy and its cost becomes zero.
Property 2 (Commutative Law): Assume that s(1) and s(2)
are two weighted-sequences and the weights of Insertion and
Deletion costs ( 2 and 3 in Eq. (7) and Eq. (8)) are equal.
The cost of turning s(1) to s(2) equals the cost of turning s(2) to
s(1):

j 1,..., n2

3
4
5

4.2 Properties of WSD

(10)
DWSD ( s (1) , s (2) ) DWSD ( s (2) , s (1) )
Proof: If we can prove that the costs of any operations for
turning s(1) to s(2) equal the cost of those for turning s(2) to s(1),
Property 2 is proved. Substitution cost defined in Eq. (6)
obeys Commutative Law. For Insertion and Deletion, inserting a(2) to s(1) in turning s(1) to s(2) is essentially the same as
deleting a(2) from s(2) in its inversed process—turning s(2) to
s(1). According to Eq. (7) and (8), we have
Cost I ( a (1) , a (2) ) Cost D ( a (2) , a (1) )
Similarly, we can obtain
Cost D ( a (1) , a (2) ) Cost I ( a (2) , a (1) )
Thus Eq. (10) is always held.
For properties 3-6, we assume there are three
weighted-sequences s(1), s(2) and s(3) which are of the same
length.
Property 3 (Character Variation): If all corresponding
weights of three sequences are equal
wi(1) wi(2) wi(3) , i 1,.., n
while there are p characters in s(2) and q characters in s(3)
which are different from the corresponding characters in s(1)
and the corresponding character distances are all equal to 1:

Dc (cx(1)j , cx(2)j ) 1, j 1,..., p, xj [1, n]
,

copy / substitution

Dc (cx(1)k , cx(3)k ) 1, k 1,..., q, xk [1, n]

};

and p>q, then DWSD ( s (1) , s (2) ) DWSD ( s (1) , s (3) ) .

1) ;

.

Figure 4: Weighed-sequence distance (WSD) algorithm.

Property 4 (Order Variation): Assume that s(1), s(2) and s(3)
consist of the same set of elements (tuples of characters and
corresponding weights), but with different order. If there are
p and q elements in s(2) and s(3) respectively, which are different from the corresponding elements in s(1):

1879

weighted-sequence distance, we define a WSD kernel function as
(11)
exp(
DWSD ( s (1) , s (2) , Dc )),
0

ax(1)j ax(2)j , j 1,..., p, xj [1, n]
ax(1)k ax(3)k , k 1,..., q, xk [1, n]
and
(2)
xr

p q, a

(3)
xr

a , r 1,..., q

then DWSD ( s (1) , s (2) ) DWSD ( s (1) , s (3) ) .
Property 5 (Weight Variation): If all corresponding characters of three sequences are equal
ci(1) ci(2) ci(3) , i 1,.., n
while there are p weights in s(2) and s(3) respectively, which
are different from the corresponding ones in s(1):

wx(1)j wx(2)j , wx(1)j wx(3)j , j 1,..., p, xj [1, n]

6

and
(1)
xj

(2)
xj

|w

(1)
xj

(3)
xj

w | |w

w |, j 1,..., p, x j [1, n]

then DWSD ( s (1) , s (2) ) DWSD ( s (1) , s (3) ) .
Property 6 (Character Distance Variation): If all corresponding weights of three sequences are equal
wi(1) wi(2) wi(3) , i 1,.., n
while there are p characters in s(2) and s(3) respectively, which
are different from the corresponding characters in s(3) and

Dc (cx(1) , cx(2)j ) Dc (cx(1) , cx(3)j ), j 1,..., p, x j [1, n]
j

WSD

j

(2)

WSD

then D ( s , s ) D ( s (1) , s (3) ) .
Properties 3-6 can all be proved formally but we skip the
proofs due to the space limitation. These properties show that
when variations involved are increased, WSD between the
reference sequence and the new created sequence raises
monotonically. They coincide with our heuristic requirements in terms of characters, weights of characters, distances
of characters and sequential order of characters for measuring the distance between two weighed-sequences.

5

(1)

in which is a model parameter. In our experiments, we use
LibSVM [Chang and Lin, 2001] with kernels computed from
our kernel function defined in Eq. (11). Model parameter
is selected from an n-fold cross validation on the training set.
Since the kernel matrix is not always positive semi-definite,
to guarantee a global optimum in SVM, we revise the kernel
matrix through shifting all the eigen values by a positive
constant [Roth et al., 2003]. The constant is set as the absolute value of the minimum eigen value in our experiments.

Experiments

In order to verify the effectiveness of our proposed algorithm,
we performed experiments with a real-world human activity
recognition dataset.
The KTH human activity dataset [Schuldt et al., 2004]
contains 6 human actions (walking, jogging, running, boxing,
hand waving and hand clapping) performed by 25 subjects in
4 different scenarios: outdoors, outdoors with scale variation,
outdoors with different clothes and indoors (599 video clips
available in total with a 25 fps frame rate). Figure 5 shows
some sample frames from the 6 actions under 4 scenarios.
We can see that this dataset covers subject variation, appearance variation, scale variation, illumination variation,
and action execution variation, and thus it is deemed as very
challenging.

Weighed-sequence Classification

With the distance measure WSD proposed in Section 4,
classification algorithms can be designed for the weighedsequences (the super-frame sequences). In our experiment,
we use both 1-Nearest Neighbor (1-NN) strategy based on
WSD and an SVM classifier [Cristianini and Shawe-Taylor,
2000] with our WSD kernel.
For non-numerical data, one possible solution of using
SVM is revising non-numerical data to numerical (e.g., using
binary digits to represent a categorical attribute [Chang and
Lin, 2001]). In addition, it can be solved by using sequence
based kernels, such as the string kernel used for text documents [Cancedda et al., 2003], sequence kernels for speaker
recognition [Campbell, 2001] or protein classification [Leslie
et al., 2004]. However, to the best of our knowledge, none of
existing string/sequence kernels is able to deal with
weighted-sequence. In this paper, based on our proposed

Figure 5: Sample frames from KTH dataset: row—action, column—scenario.

1880

In our experiments, we first compute frame differences for
the first 204 frames in each sequence with a step size 4. (200
difference frames obtained for each sequence.) In order to
reduce dimension, we crop out the human body regions by
using an 80 pixel 100 pixel bounding box. For hand motion
sequences without scale variations, we manually set a fixed
cropping region for the first frame of each sequence and use
it for the entire sequence; for the remaining sequences, we
estimate the center of the cropping region by using the center
of the foreground pixels in the difference frames. We further
sub-sample the sequences spatially to 1/ 4 of the original size
and temporally by a factor of 3, resulting in 67 difference
frames for each video clip to be used as input for classification. Compared to the strong assumptions required in some
other work (discussed in the related work and the following
experiment analysis part), our preprocessing steps are by
design very simple and the obtained results are still quite
noisy in reality.

Figure 6: Samples of obtained atom poses.

%
Box
Clap
Wave
Jog
Run
Walk

Box
88
14
6
0
0
0

Clap
9
81
11
0
0
0

Wave
3
3
83
0
0
0

Jog
0
0
0
70
27
8

Run
0
0
0
22
73
2

Walk
0
2
0
8
0
91

of the number of clusters in creating the codebook is not
critical. Figure 6 shows some sample codewords of the atom
actions (cluster centroids) from our unsupervised classification. As we can expect, results of such simple clustering
strategy applied on the inaccurately-cropped regions are very
noisy. However, our weighted-sequence distance naturally
incorporates the global temporal structure of the sequences in
computing the final measure. The noisy, individual results
are actually implicitly filtered during the computation. And
the 87.8% average classification accuracy (see Table 3) further verified that our proposed approach is robust to noise
and works well for data obtained from low-level visual features without using sophisticated feature extraction.
For algorithm training and testing, we use sequences from
9 random selected subjects for creating the codebook and
adopt the leave-one-out (LOO) testing paradigm, which is
employed by all but one of the reference algorithms chosen
for comparison. Each LOO round consists of sequences from
24 subjects for training and those from the remaining one for
testing. Excluding sequences from those 9 subjects which are
used for creating the codebook, 16 LOO rounds were run.
Results are shown in Table 1 and Table 2. It was found that
the best performance was obtained by the proposed WSD
kernel SVM classifier (which is better than the 1-NN classifier with WSD by 6.8% on average).
Several most recent state-of-the-art approaches are chosen
for comparison. These include Niebles et al.’s work [Niebles
et al., 2008], Dollar et al.’s work [Dollar et al., 2005], and
Nowozin et al.’s result [Nowozin et al., 2007]. (Although
some other methods also reported results on the same dataset,
they were not selected for comparisons here since they rely
on excessive manual segmentation or strong assumption of
the availability of accurate preprocessing steps for feature
extraction, which may significantly boost the performance.
For example, [Schindler and Gool, 2008; Fathi and Mori,
2008; Mikolajczyk and Uemura, 2008] require accurate
bounding box for each frame and [Liu and Shah, 2008] assumes accurate scale information for each frame. Both of the
assumptions explicitly avoid the scale variation problem,
which occurs in 1/4 of the sequences in KTH dataset.)
%

Table 1: Results using 1-NN classifier with WSD measurement.

%
Box
Clap
Wave
Jog
Run
Walk

Box
94
9
6
0
0
0

Clap
6
91
8
0
0
0

Wave
0
0
86
0
0
0

Jog
0
0
0
80
16
8

Run
0
0
0
8
84
0

Our (1-NN)

Walk
0
0
0
13
0
92

Our (SVM)
Niebles et al ’08
Dollar et al. (1-NN)
Dollar et al. (SVM)
Nowozin et al ’07

Box Clap Wave Jog Run Walk Ave

88
94
98
80
85
86

81
91
86
82
77
89

83
86
93
84
93
92

70
80
53
63
57
69

73
84
88
73
85
86

91
92
82
89
90
86

81.0
87.8
83.3
78.5
81.2
84.7

Table 3: Comparisons to state-of-the-art results on KTH dataset

Table 2: Results using SVM classifier with WSD kernel.

In the codebook creation step, we set the cluster number as
30 for our experiments. As mentioned in the previous part,
we take the distances among codewords into consideration in
computing the weighted-sequence distance, thus the choice

It was found that the WSD kernel SVM algorithm is able to
outperform each of the reference methods in terms of overall
accuracy, as shown in Table 3. Further, we can also observe
from Table 3 that our algorithm is able to discriminate well
“jogging”, “running” and “walking” classes, which are good
examples where the global temporal structure of the se-

1881

quences are key to correct classification. This suggests that
the proposed super-frame coding strategy is able to retain the
global temporal information as desired. It is worth pointing
out that both versions of the proposed methods (1-NN-based
and SVM-based) outperform the corresponding algorithms
from Dollar et al.

7 Conclusion and Future Work
We proposed an approach for human action recognition
based on a novel super-frame-based coding scheme and a
novel weighted-sequence distance for comparing super-frame sequences. Our approach takes into consideration
both local and global spatial-temporal structures of an action
that are deemed as critical for classification. The approach
has been evaluated on a challenging real database and was
found to be able to outperform many existing state-of-the-art
approaches by a non-trivial margin, even if it uses very simple low-level visual features as the input.
There is still much room for further improving our work.
For example, in our current work, we only use the temporal
length of the atom poses as the attributes. Other information
or features that may contribute to better discrimination of the
actions can also be added to the attributes for each atom pose.
Also, in our experiments, while the chosen dataset is challenging and has been widely-used, it does not include multiple actions/subjects and appearance variations due to view
angle changes. Our future work includes evaluating and
extending the proposed method to cover more complex cases.
In addition, we also plan to explore the application of the
proposed WSD on other problems where comparison between sequences of symbols with attributes is involved.

References
[Bui and Venkatesh, 2002] H. H. Bui and S. Venkatesh. Policy
Recognition in the Abstract Hidden Markov Model. Journal of
Artificial Intelligence Research, vol. 17, pp. 451-499, 2002.
[Bui et al., 2008] H. H. Bui, D. Phung, S. Venkatesh, and H. Phan.
The Hidden Permutation Model and Location-Based Activity
Recognition. National Conference on Artificial Intelligence
(AAAI), 2008.
[Campbell, 2001] W. M. Campbell. A Sequence Kernel and its
Application to Speaker Recognition. Neural Information
Processing Systems, vol. 14, pp. 1157-1163, 2001.
[Cancedda et al., 2003] N. Cancedda, E. Gaussier, C. Goutte, and J.
M. Renders. Word Sequence Kernels. The Journal of Machine
Learning Research, vol. 3, pp. 1059-1082, 2003.
[Chang and Lin, 2001] C.-C. Chang and C.-J. Lin. LIBSVM : a
library for support vector machines. Software available at
http://www.csie.ntu.edu.tw/~cjlin/libsvm.
[Cristianini and Shawe-Taylor, 2000] N. Cristianini and J.
Shawe-Taylor. An Introduction to Support Vector Machines
and Other Kernel-based Learning Methods: Cambridge University Press, 2000.
[Crochemore and Rytter, 1994] M. Crochemore and W. Rytter. Text
Algorithms: Oxford University Press, 1994.
[Dollar et al., 2005] P. Dollar, V. Rabaud, G. Cottrell, and S. Belongie. Behavior Recognition Via Sparse Spatio-temporal

Features. 2nd Joint IEEE International Workshop on Visual
Surveillance and Performance Evaluation of Tracking and
Surveillance, 2005.
[Fathi and Mori, 2008] A. Fathi and G. Mori. Action Recognition by
Learning Mid-level Motion Features. IEEE International
Conference on Computer Vision and Pattern Recognition
(CVPR), 2008.
[Kim et al., 2007] T. Kim, S. Wong and R. Cipolla. Tensor Canonical Correlation Analysis for Action Classification. IEEE
International Conference on Computer Vision and Pattern
Recognition (CVPR), 2007.
[Landau and Vishkin, 1989] G. Landau, and U. Vishkin. Fast parallel and serial approximate string matching. Journal of Algorithms, vol. 10, 157–169, 1989.
[Leslie et al., 2004] C. Leslie, E. Eskin, A. Cohen, J. Weston, and a.
W. S. Nobley. Mismatch String Kernels for Discriminative
Protein Classification. Bioinformatics Advance Access, vol. 20,
pp. 467-476, 2004.
[Levenshtein, 1966] V. I. Levenshtein. Binary Codes Capable of
Correcting Deletions, Insertions and Reversals. Soviet Physic
Doklady, vol. 10, pp. 707-710, 1966.
[Liao et al., 2005] L. Liao, D. Fox, and H. Kautz. Location-based
Activity Recognition Using Relational Markov Networks.
International Joint Conference on Artificial Intelligence
(IJCAI), 2005.
[Liu and Shah, 2008] J. Liu and M. Shah. Learning Human Actions
via Information Maximization. IEEE International Conference
on Computer Vision and Pattern Recognition (CVPR), 2008.
[Mikolajczyk and Uemura, 2008] K. Mikolajczyk and H. Uemura.
Action Recognition with Motion-Appearance Vocabulary
Forest. IEEE International Conference on Computer Vision
and Pattern Recognition (CVPR), 2008.
[Niebles et al., 2008] J. C. Niebles, H. Wang, and L. Fei-Fei. Unsupervised Learning of Human Action Categories Using Spatial-Temporal Words. International Journal of Computer Vision, vol. 79, pp. 299-318, 2008.
[Niu and Abdel-Mottaleb, 2005] F. Niu and M. Abdel-Mottaleb.
HMM-Based Segmentation and Recognition of Human Activities from Video Sequences. IEEE International Conference
on Multimedia and Expo (ICME), 2005.
[Nowozin et al., 2007] S. Nowozin, G. Bakir, and K. Tsuda. Discriminative Subsequence Mining for Action Classification.
IEEE International Conference on Computer Vision (ICCV),
2007.
[Roth et al., 2003] V. Roth, J. Laub, M. Kawanabe, and J. M.
Buhmann. Optimal cluster preserving embedding of nonmetric
proximity data. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 25, 1540–1551.
[Schindler and Gool, 2008] K. Schindler and L. v. Gool. Action
snippets: How many frames does human action recognition
require? IEEE International Conference on Computer Vision
and Pattern Recognition (CVPR), 2008.
[Schuldt et al., 2004] C. Schuldt, I. Laptev, and B. Caputo. Recognizing Human Actions: A Local SVM Approach. International Conference on Pattern Recognition (ICPR), 2004.
[Wong et al., 2007] S. Wong, T. Kim and R. Cipolla. Learning
Motion Categories using both Semantic and Structural Information. IEEE International Conference on Computer Vision
and Pattern Recognition (CVPR), 2007.

1882

Retrieving Images of Similar Geometrical Configuration
Xiaolong Zhang and Baoxin Li
Computer Science & Engineering
Arizona State University
{xiaolong.zhang.1,baoxin.li}@asu.edu

Abstract. Content Based Image Retrieval (CBIR) has been an active research
field for a long time. Existing CBIR approaches are mostly based on low- to
middle-level visual cues such as color or color histograms and possibly semantic relations of image regions, etc. In many applications, it may be of interest to
retrieve images of similar geometrical configurations such as all images of a
hallway-like view. In this paper we present our work on addressing such a task
that seemingly requires 3D reconstruction from a single image. Our approach
avoids explicit 3D reconstruction, which remains to be a challenge, through
coding the potential relationship between the 3D structure of an image and its
low-level features via a grid-based representation. We experimented with a data
set of several thousands of images and obtained promising results.

1 Introduction
Nowadays, online media sharing sites such as Flickr (http://www.flickr.com) from
Yahoo and Picasa (http://picasa.google.com) from Google are flourishing. The
amount of image content online has experienced enormous increase. As this trend
continues, meaningful ways to retrieve images will be very desirable. Currently, the
searching function supported in most media sites are largely based on keywords (tags)
or low level features such as color. Being able to retrieve images based on geometrical composition of the scene would be interesting and useful for certain applications
such as architecture and tourism. In this paper we propose a method for retrieving
images with similar 3D configuration.
The most intuitive solution to this problem is to first perform 3D reconstruction
and then make comparison between the query image and pool images base on structural information. Unfortunately this solution is practically very challenging, since 3D
reconstruction is still a difficult task especially given only a single image. 3D reconstruction methods such as structure-from-X (motion/stereo/defocus) [3] have been
proposed to recover depth information from 2D images based on motion parallax,
stereo disparity and local optical feature. Most of such methods require more than a
single frame. With multiple frames, such methods may still suffer from issues like
having only a sparse set of 3D points reconstructed. Markov Random Field (MRF)
and Conditional Random Field (CRF)are often introduced to provide a smooth output
but cannot ensure true 3D structure.3D reconstruction based on a single image is a
more challenging task due to the lack of depth cues such as stereopsis and motion
parallax.
G. Bebis et al. (Eds.): ISVC 2010, Part II, LNCS 6454, pp. 449–458, 2010.
© Springer-Verlag Berlin Heidelberg 2010

450

X. Zhang and B. Li

Fig. 1. Six sample scene compositions of an image. Earlier work [7] proposed similar stage
types, our work considers six types.

Most single-image-based 3D reconstruction approaches rely on sophisticated
image features and learning mechanisms. Hoiem et al. [4,5] proposed a method to
classify image regions according to their geometrical orientations with respect to the
camera using logistic regression with AdaBoost being used to train the model and
with decision trees as weak learners. Saxena et al. [9,10] performed depth estimation
from single images by supervised learning with MRF. From a different perspective,
Nedović et.al. [7,8] defined several stages based on scene geometry and performed
supervised learning on stage types using Support Vector Machine (SVM). Inspired by
the concept of scene categorization from [7,8], our approach aims at 3D image
retrieval based on six typical scene compositions as shown in Fig. 1. Instead of precise 3D reconstruction from a single image, our objective is to obtain rough description of 3D configuration of the image and then use that for image retrieval.
In Sect. 2 we describe the proposed method, followed by our experiments in
Sect. 3, where we describe the image retrieval engine based on the learned results.
Finally we provide our discussion and an overview of future work.

2 Proposed Method
The proposed approach is illustrated in Fig. 2. The first component is feature extraction on three levels: segment, macro-grid and entire image. We extract geometrical
context features as well as Gabor filter responses and represent the image by concatenating features from the N×N macro-grids. Retrieval is based on similarity between
the query image and pool images using these features.

Retrieving Images of Similar Geometrical Configuration

451

Fig. 2. Illustration of the proposed approach

2.1 Feature Extraction
As mentioned earlier, our approach is based on a three-level image representation:
segment, macro-grid and entire image. Segments are generated by applying the graphcut based algorithm introduced in [2]. Macro-grids represent rectangular regions of
the image obtained by dividing an image into N×N rectangular Mosaics. This representation is a key for the creation of high level label representation of an image,
which stands as a mid-ground between the pixel values and the image label that facilitates abstract representation of local image content. Macro-grid features and three
types of macro-grid interactions are concatenated to represent an entire image.
2.1.1 Segment Features
To obtain segments, over-segmentation of an input image was performed using a
graph-cut based segmentation method originally proposed by Felzenszwalb [2]. Since
this fine level of feature represents the most detailed description of an image, in general segments of smaller size are preferred. On the one hand, coarse segmentation
result might cause loss of information during feature extraction which involves averaging across all pixels in a segment. On the other hand, having abundant segments
does not necessarily harm the performance. The assumption here is that even if a
group of adjacent segments belonging to the same region with high similarity are

452

X. Zhang and B. Li

broken up during the segmentation stage, their locations in the feature space are very
likely to remain close to each other when clustering is performed.
After segmentation, feature extraction is performed on each segment. We employ
the geometrical context features as used in [3,4]. The first 6 features listed in Table 1
illustrate these features.
2.1.2 Macro-grid Features
Several learning algorithms are based on segment-level features [4,5] while others
that further divide the image into artificial blocks [9,10] generally treat the later division more as a means of guaranteeing equal length of image features. We hypothesize
that there may be rich semantic and statistical information on this sort of abstract level
of representation that can be explored for image retrieval. Interestingly, studies in
Cubism [1] suggest that the school of painting pioneered by Pablo Picasso creates an
representation of the world that is abstract in form but powerful in expression.
By overlaying an N×N grid pattern we divide the original image into N2 parts which
is termed as macro-grids in the following sections. For each macro-grid, we further
extract three types of features based on the feature, label and Gabor filter responses.
2.1.2.1 Macro-grid Centroid. For each macro-grid, we calculate the weighted average of every centroid that is associated with a segment within this macro-grid. Let S
be the number of segments in a macro-grid, the weighted centroid can be expressed as
1

where
#
#

2

As we can see in (2), the weight coefficient is determined by the area each segment
occupies in the macro-grid. stands for the centroid of each segment. In this way the
macro-grid centroid is determined jointly by all its members. If all pixels are considered in a block (1) generates the exact centroid of the block but here we eliminate
outlier segments by an intermediate closing (dilation followed by erosion) step. The
result generates different centroids for each block and forms a quadrangle mesh.
2.1.2.2 Macro-grid Label. The second type of feature we extract for the macro-grid
is based on segment labels. To represent an image with a concise N×N grid pattern,
each macro-grid is represented by a single label. Each segment uses its global segment index as its label to represent the homogeneity of local regions, and we take the
majority voting result in each super-grid as its final label. The resulting image label
matrix conveys a certain amount of information regarding coarse structures of the
scene.
2.1.2.3 Gabor Filter Responses. The features presented thus far are mainly from
color and texture of local image areas. As we know, long straight lines in an image
could form strong perspective geometry cues that indicate camera pose and scene
orientation. Instead of performing edge detection and camera calibration, we employ a

Retrieving Images of Similar Geometrical Configuration

453

set of Gabor filters responses to capture this information. In the proposed work Gabor
filters were constructed at six different frequencies in nine different orientations. As a
result, 54 filter responses for each macro-grid is collected. The reason we implement
this feature in this level is twofold. First of all, it is hard to estimate filters responses
given the irregular shapes of segments. Secondly, applying the filter to the entire
image does not provide information regarding local regions.
Table 1. Feature components for an image

Feature Descriptions
Color-RGB
Color-HSV
Location
Area Ratio
Location Mode
Gradient (X & Y)
Gabor Filter Response
Block Interactions

Dimension
3 × N2
3 × N2
2 × N2
N2
2 × N2
2 × N2
54 × N2
3×N

2.1.3 Image Features
One virtue of the adaptation of macro-grid is a uniform representation of image features. As is shown in Table 1, by concatenating feature from each macro-grid sequentially, we obtain a global vector representing the feature vector of all regions from an
entire image. Meanwhile, the main draw back this scheme introduces is the loss of
spatial information. Though strictly speaking, the sequence of concatenation does
represent the spatial information in an implicit way, the interaction among different
regions in the image is certainly not sufficiently represented. We introduce macrogrid interactions to compensate for this problem.
2.2 Learning and Retrieval
In the segment level we perform clustering to simplify the representation of image
segments, which directly leads to a uniform representation of scene composition by an
N×N macro-grid pattern. Having obtained this representation of images the retrieval
engine performs ranking of the similarity measurement between images in the database and any query image and provide results based on this ranking.
2.2.1 Hierarchical Clustering of Segments
As introduced in Section 3.1, we obtain visual features on the segment level after
which clustering is performed. For our dataset, partitional clustering methods such as
K-means have shown unstable convergence due to strong influence of initial centroid
location. In comparison, hierarchical clustering demonstrated more reliable performance. In our case we adopted agglomerative hierarchical clustering scheme, which
takes a “bottom-up” approach by assigning one cluster for each segment and keeping
merging the most similar clusters into one larger cluster until all data points are eventually merged into one big cluster. Another advantage of hierarchical clustering is that

454

X. Zhang and B. Li

this sequential approach provides clustering results of different 1 to K cluster numbers
in just one run.
We choose cosine similarity as the distance metric and average linkage as the linkage criteria.
1

,

3

Here
and
stand for the number of data points in cluster K and cluster L respec,
stands for the distance between two data points.
represents
tively, and
the average distance from one cluster to the another. With this measure we can minimize errors introduced by either outliers or spatial adjacency of two clusters.

Fig. 3. Sample frames from TV recordings [11]. Columns from left to right stands for each
category: Straight background, Sided background, Box, Person+Background, Corner and No
Depth.

The advantage of cosine distance is that it automatically provides normalized distance value. The average linkage criteria are chosen to eliminate the concern of outlier
within segment clusters. Besides the fact that it is less influenced by extreme values,
these criteria also demonstrate the characteristics of favoring clusters with similar
variance in general.
2.2.2 Retrieval Scheme
Performing feature extraction on each image in the database, a pool of image
represented by their feature is obtained. The retrieval engine works in the following

R
Retrieving
Images of Similar Geometrical Configuration

455

way: when a query image is received, feature extraction is performed on the quuery
image and pair-wise distancces between this image and every image in the database are
calculated.
We adopted Levenstein distance for label features which consist of symbolic vvalues. Levenstein distance caaptures the longest similar sequence in two strings, in our
application this improves th
he robustness of matching similar structure in different ppart
of an images.

Fig. 4. Maacro-grid label representation of image content

3 Experiments
To demonstrate the perform
mance of the proposed approach, first we explain the ddata
acquisition and show samp
ple data for each category. Then we present the macro-ggrid
representation of images. Finally
F
we present result for image retrieval based on the
representation.
3.1 Data Acquisition
The main source of experim
mental data is the NIST TRECVID database [11] contaaining a variety of TV recordiings. Out of over 600 hours of video, we obtained 50,0000
sample frames and manuallly labeled 4,000 frames obtaining roughly equal amounnt of
samples per category. Samp
ple frames from each category are illustrated in Figure 33.
3.2 Macro-grid Represen
ntation of Images
First we present some sam
mple output of our macro-grid label representation. As
is shown in Figure 4, imaages were divided into N×N blocks (in this case N = 5)
and labeled according to clustering result represented by different gray scalee. It

456

X. Zhang and B. Li
Table 2. Mean Reciprocal Rank and Precision

Category
Prior (%)
16.3
Straight Background
16.0
Tilted Background
16.3
Box
9.4
Corner
25.7
Person + Background
16.2
No Depth
16.7
Average

MRR
1.0
2.5
1.5
3.0
1.0
1.0
1.67

P@5
.80
.60
.60
.40
1.00
.80
.70

P@10
.80
.70
.50
.30
1.00
.40
.62

P@20
.65
.45
.55
.20
.95
.35
.53

can be observed that despite the low resolution of this representation, a certain
amount of information regarding key component/plane separation is captured by
the labels.
3.3 Retrieval Evaluation
For each query image, the retrieval engine returns a list of similar images following
decreasing similarity ranking values regarding the original image. When a returned
image has the same category type as the query image, this is defined as a hit. The
performance of the retrieval system is measured by the average rank of the first hit,
and the amount of hit given a certain size of returned images. In our experiment we
employ two metrics to evaluate the retrieval performance.
Mean Reciprocal Rank (MRR): Given an input image and a set of retrieved images,
MRR measures the position of the first relevant image in the returned results averaged
across all trials of experiment.
Precision at k (P@k): The average precision when k images are returned as retrieved
result.
In our experiment we measure P@5, P@10 and P@20. The P@K value is visualized
in the following chart.
It can be observed from Table 2 that the MRR measurement varies from class to
class. The average MRR suggests the ranking that on average a “hit” occurs. We can
also see that the P@k value decreases as the number of retrieved images (k) increases.
Note that among all the categories, the “Corner” and “Person+Background” yielded
the lowest and the highest accuracy respectively. Uneven prior distribution might
have been an influencing factor.
The process that a human perceives a pictorial stimuli and determines the spatial
layout involves complex procedures that engage heavy cognitive processing. The
problem our system addresses is technically challenging. By introducing predefined
categories of image scenes we added constraints to the problem and simplified the
scenario. Although the performance of the system can still be improved, our results
have demonstrated that the proposed approach is capable of capturing underlying 3D
information for image retrieval with a relatively simple feature set.

Retrieving Images of Similar Geometrical Configuration

457

Straight
Background
1
0.8
No Depth

0.6
0.4

Tilted
Background
P@5

0.2

P@10

0

P@20
Person +
Background

Box

Corner
Fig. 5. Precision at K distribution across scene categories

4 Conclusion and Future Work
In this paper we presented a new approach for 3D image retrieval. By dividing image
into 5×5 macro-grids and performing tri-layer feature extraction we obtained an image representation that incorporates information on three levels. Given the small
amount of information represented in the feature set, the proposed approach demonstrated effective retrieval of images based on inherent 3D information. We believe
that the proposed could serve as a pre-processing step for other applications such as
image categorization and more precise 3D reconstruction. The results suggest that
bridging high-level semantic representation with low-level statistical representation of
images is a promising direction for further pursuit.
Another area that would be interesting to study is the robustness of this retrieval
framework in terms of spatial invariance. In our experiment, the nature of our data
source (TV recordings) already introduced significant amount of camera translation/rotation, and no rectification was performed to our data before processing. At the
same time images with salient perspective angles would have a stronger response.
Thus it would be interesting to examine whether global or local features dominate the
retrieval result if after we apply a random projective transform and to what extent
would the results be polluted.
We also identified several potential limitations of the proposed approach such as
static macro-grid resolution. The tradeoff between coarse resolution which is associated with greater information loss and fine resolution which leads to increased approximation within each region and dimension increase should be further examined.
Another aspect of the problem which is not sufficiently addressed here is the interaction between image regions. Although our feature vector incorporates this feature to a

458

X. Zhang and B. Li

certain extent, we anticipate more carefully designed distance metric and interaction
scheme to bring the performance to a higher level.

Acknowledgement
The authors were partially supported during this work by an NSF grant (Award #
0845469), which is greatly appreciated.

References
[1]
[2]
[3]
[4]
[5]
[6]
[7]

[8]

[9]

[10]
[11]

Douglas Cooper, The Cubist Epoch, ISBN 0 87587041 4
Felzenszwalb, P., Huttenlocher, D.P.: Efficient graph-based image segmentation.
IJCV 59(2) (2004)
Hartley, R., Zisserman, A.: Multiple View Geometry in Computer Vision. Cambridge
University Press, Cambridge (2003) ISBN 0-521-54051-8
Hoiem, D., Efros, A.A., Hebert, M.: Automatic Photo Pop-up. In: ACM SIGGRAPH
(2005)
Hoiem, D., Efros, A.A., Hebert, M.: Geometric Context from a Single Image. In: International Conference of Computer Vision (ICCV) (October 2005)
McCullagh, P., Nelder, J.A.: Generalized Linear Models, 2nd edn. Chapman &
Hall/CRC Press (1990)
Nedović, V., Smeulders, A.W.M., Redert, A., Geusebroek, J.-M.: Depth Information by
Stage Classification. In: Proc. of the 11th IEEE Int’l Conf. on Computer Vision (ICCV
2007), Rio de Janeiro, Brazil, October 14-20 (2007)
Nedović, V., Smeulders, A.W.M., Redert, A., Geusebroek, J.-M.: Stages as Models of
Scene Geometry. IEEE Trans. on Pattern Analysis and Machine Intelligence 32(9)
(2010)
Saxena, A., Sun, M., Ng, A.Y.: Make3D: Learning 3-D Scene Structure from a Single
Still Image. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI
(2008)
Saxena, A., Sun, M., Ng, A.Y.: Make3D: Depth Perception from a Single Still Image.
In: AAAI (2008)
TRECVID2007-09 Sound and Vision Data, http://www-nlpir.nist.gov/
projects/trecvid/trecvid.data.html

Proceedings of 2010 IEEE 17th International Conference on Image Processing

September 26-29, 2010, Hong Kong

JOINT SPARSITY MODEL WITH MATRIX COMPLETION FOR AN ENSEMBLE OF FACE
IMAGES
Qiang Zhang and Baoxin Li
Compute Science and Engineering, Arizona State University, Tempe, Arizona, 85281
Email: {qzhang53, baoxin.li} @asu.edu
ABSTRACT
An ensemble of correlated signals are often encountered in
many applications of image processing, such as a set of face
images of the same subject. In this paper, we propose a new
model, called Joint Sparsity Model with Matrix Completion
(JSM-MC), which extracts a common component, an innovation component, and a low-rank component from an ensemble of face images. These components have their respective
physical signiﬁcance in terms of representing different types
of information in the original ensemble, hence facilitating an
analysis task such as recognition. An algorithm is proposed
under the model to solve for the components, based on Block
Coordinate Descent and Singular Value Thresholding. Experimental results show that the proposed method has unique
advantages over existing methods in dealing with challenging
face images with extreme illumination conditions or occlusions.
Index Terms— Face image, compressive sensing, joint
sparsity, matrix completion
1. INTRODUCTION
In many applications of image and signal processing, we may
need to consider a set of correlated signals as an ensemble.
For example, in face recognition, the images from the same
subject naturally form such an ensemble of correlated signals.
For efﬁcient representation, a signal in the ensemble can often
be viewed as a combination of a common component, which
is shared among all the signals in the ensemble, and an innovation component, which is unique to this signal. Such
decomposition of the signals is dependent of the constraints
introduced or assumed. Many beneﬁts can be drawn from
this decomposition of the ensemble, such as obtaining better
compression rate [1] and being able to extract more relevant
features [2].
For face images, [3] proposed an approach for expressioninvariant recognition based on Joint Sparsity Model (JSM)
[1]. In JSM, the innovation component is constrained to be
sparse. This constraint is, unfortunately, hard to meet in the
case of face images with illumination variations or large occlusions, as these conditions usually affect the whole (or a

978-1-4244-7994-8/10/$26.00 ©2010 IEEE

1665

large portion of the) face image and they cannot be easily
sparsiﬁed in any basis. In other words, the necessary condition, sparsity of the innovation, cannot be satisﬁed and thus
the decomposition becomes not useful.
Recently, the so-called matrix completion theory [4] has
been proposed, by which a low-rank data matrix can be recovered from only a sparse sampling of its entries by minimizing the nuclear norm [5]. Based on this theory, [6] proposed
the Robust Principal Component Analysis (Robust PCA), in
which a matrix X is decomposed into a low rank part A and
a sparse part E. In [6], the method was used to remove shadows and specularities in face images, which were assumed
to be captured by the sparse part while the clean faces were
assumed to be captured by the low rank part. This does not
solve the issue of non-sparse conditions such as illumination
changes. Also, the assumption of a low-rank clean face image
is in general not valid.
To overcome the above limitations, in this paper, we propose the Joint Sparsity Model with Matrix Completion (JSMMC), which decomposes the images into a common component (i.e., information shared by the whole set), a low rank
part (e.g., illumination or large occlusion), and a sparse part
(e.g., expression or other unique features). We propose an
algorithm to efﬁciently perform the decomposition, given an
ensemble of face images. To demonstrate the effectiveness of
the proposed model and algorithm, we tested with two face
images databases: the extended YaleB database and the AR
database.
In Section 2, we brieﬂy review the relevant theories. Our
method is introduced in Section 3. Section 4 presents experimental results, followed by conclusions in Section 5.
2. THEORETICAL BACKGROUND
We brieﬂy review the theoretical background in this section.
We use uppercase letters to represent matrices or images, such
as X, and lowercase ones for vector signals, such as x.
2.1. Joint Sparsity Model
In the compressive sensing theory [7], a signal x ∈ RN is K
sparse if only K of its elements are non-zero (in some basis).

ICIP 2010

We take a measurement of x as y = Ψx ∈ RM , where
Ψ ∈ RM ∗N is called the measurement matrix with M  N .
It has been proven that with high probability, the sparse signal
x can be exactly recovered from y by l1 optimization.
The idea was extended to a set of signals in the distributed
compressive sensing (DCS) theory [1]. Given a signal ensemble x = {x1 , x2 , ..., xN } ∈ RM ∗N , DCS assumes that each
signal xj is generated as a combination of two parts: the common part zC , and the innovation part zj . That is:
xj = zC + zj ∀xj ∈ x

(1)

Different sparsity assumptions for the common part and
the innovation part lead to different models. In this work, we
consider only the JSM 3 model, in which the common part is
arbitrary while the innovation is constrained to be sparse, i.e.,
xj = zC + zj s.t. zj 0 ≤ t ∀xj ∈ x

2.2. Matrix completion
In the theory of matrix completion [4], a low-rank matrix X
can be recovered from a small subset of its entries by minimizing the nuclear norm:
Y

(3)

(4)

where U , Σ, V is singular value decomposition of X and
Sτ (Σ) is a soft thresholding operator:
⎧
⎪
⎨x − τ if x − τ ≥ 0
(5)
Sτ (x) = x + τ if x + τ ≤ 0
⎪
⎩
0
otherwise
For exact recovery, X should be low-rank. However, the
matrix in real applications, such as image, is not actually low
rank.
3. PROPOSED METHOD
3.1. Joint Sparsity Model with Matrix Completion
Although both DCS and Matrix Completion have found many
applications, they suffer from some shortcomings when applied to problems like face image processing. Consider the
case where X = {X1 , X2 , ..., XN } is a set of face images of

(6)

where C is the common part, Aj is a low-rank matrix, and
Ej is a sparse innovation. This was partially motivated by the
observation that in Eigenfaces [8], the ﬁrst several singular
values usually correspond to the illumination variation of the
face image. Hence the low-rank matrix Aj could capture the
illumination information that a sparse matrix cannot. With
this model, the problem may be formulated as:

< C, {Aj }, {Ej } > =
argmin
Aj ∗ + λj Ej 1
C,{Aj },{Ej }

s.t.

where Ω is the set of sampling coordinates, Y the recovered
low rank matrix, and Y ∗ the the nuclear norm, which is the
sum of singular value of Y . The Singular Value Thresholding
(SVT) method can be used to solve Eqn. 3:
SV T (X) = U Sτ (Σ)V

Xj = C + Aj + Ej ∀Xj ∈ X

(2)

with t being a small constant.

Y = argminY ∗ s.t. Ω(Y ) = Ω(X)

the same person under different illumination/occlusion conditions. With the DSC model, Xj = Zc + Zj , where Zc
represents the shared information of the face images while
Zj is supposed to capture the image-speciﬁc properties such
as a speciﬁc illumination condition. Note that DCS requires
the latter to be sparse. However, illumination (or large occlusion) can affect the entire face image, and is hardly sparse
in any basis. On the other hand, approaches based on matrix
completion (e.g., Robust PCA) would assume that a low-rank
matrix captures the common part of the face images, which
is often too restrictive since a natural, clean face image is in
general not low-rank.
To address these issues, we propose the Joint Sparsity
Model with Matrix Completion (JSM-MC) model. In JSMMC, each image Xj of the ensemble X is represented by:

j

Xj = C + Aj + Ej ∀Xj ∈ X (7)

where λj is a scalar.
If we apply the augmented Lagrange multipliers to Eqn.
7, we can get:
< C, {Aj }, {Ej } >

=
=
+
+

argmin L(C, {Aj }, {Ej })

C,{Aj },{Ej }

argmin

C,{Aj },{Ej }



Aj ∗ + λj Ej 1

j

< Y, Xj − C − Aj − Ej >
μj
Xj − C − Aj − Ej F
2

(8)

where μj is a scalar and XF the Frobenius norm.
It should be noted that, the objective function in Eqn. 7 of
the proposed model appears similar to that of Robust PCA [6],
where a matrix is decomposed into two parts, X = A + E,
with A a low-rank matrix and E a sparse matrix. However, the
signiﬁcant difference is our introduction of the common component as in Eqn. 7. This allows us to explicitly model the
correlation among the images in the ensemble, which should
be among the most critical information in the given input.
3.2. An Algorithm Based on JSM-MC
Directly solving Eqn. 8 is difﬁcult, as the variables depend
on each other. We propose to utilize the block coordinate

1666

descent method for the optimization, i.e. optimizing a set of
the parameters while ﬁxing others. We divide the parameters
into 3 sets: C, {Aj } and {Ej }, and then present an algorithm after stating the following 2 lemmas, whose proof are
straightforward and hence omitted.
Lemma 1: The solution to equation C = argminL(C),
C

μ
where L(C) = j < Y, Xjc − C > + 2j Xjc − CF , is:

4. EXPERIMENTAL RESULTS

In this section, we present experimental results evaluating our
method and comparing with 3 existing approaches. We aim
at illustrating the potential beneﬁt of the proposed model and
algorithm in addressing challenges encountered in common
facial image analysis tasks such as face recognition, for which
illumination, occlusion, and expression are all common challenges. We compare with the following 3 methods: the low
c
pass ﬁltering based method (henceforth referred to as the ”LPj Yj + μj Xj

(9)
C=
based” method)used in [9], JSM-3 used in [3], and the Robust
j μj
PCA method.
In [9], the appearance variation caused by illumination in
Lemma 2: Equation < {Aj }, {Ej } >= argmin L({Aj }, {Ej })a local image patch X is approximated by a low-pass version
{Aj },{Ej }
of itself. So the clean image X  would be:
can be solved by solving< Aj , Ej > for each j ∈ [1, 2, ..., N ]
individually, where:
X
X =
(10)
conv(X, LP )

L({Aj }, {Ej }) =
Aj ∗ + λj Ej 1
where conv() is the convolution operator and LP is a lowj
pass ﬁlter.
+ < Yj , XjC − Aj − Ej >
The experimental results of applying all the competing apμj
C
proaches
on two face databases are illustrated in Fig. 1 and
+
Xj − Aj − Ej F
2
Fig. 2 respectively. As can be seen from the examples given in
the ﬁgures, the LP-based method introduces noise or edge artifacts, which can be detrimental to face recognition. Robust
Algorithm for solving JSM-MC
PCA, though working reasonably for some cases, does not
perform well on images with extreme illumination conditions
Input: ρ, {Xj }, {λj } and {μ0j };
or large occlusions. JSM-3 also has the same problem, as the
Output: C, {Ej } and {Aj };
sparsity constraint is not met. The proposed method success% Initialization
fully eliminates those problems: we obtain a clean common
Xj
0
0
0
0
k = 0, C = 0, {Aj } = {Ej } = 0, {Yj } = { Xj F };
image, a low-rank image that largely captures illumination or
while not converged do
large occlusion (the latter is especially obvious from Fig. 2 in
for j ∈ [1, 2, ..., N ]
the cases with sunglasses and scarf), and an innovation that
% Step 1: solve {Ej } while ﬁxing C and {Aj }
captures the remain image-speciﬁc features.
XjC = Xj − C k ;
All the faces images in the experiments are well aligned.
Ejk+1 = S λj (XjC − Akj + μ1k Yjk );
However,
unlike [6], JSM-MC does not need to stack each
j
μk
j
image into a column of matrix, which means it can capture
% Step 2: solve {Aj } while ﬁxing C and {Ej }
spatial information of the face image better. The parameters
(U, Σ, V ) = svd(XjC − Ejk+1 + μ1k Yjk );
in the JSM-MC algorithm were set as follows, ρ = 1.5, μ0j =
j
k+1
T
1.25
0.5
Aj = U S λj (Σ)V ;
√
∀j, where m is the height of the face
Xj 2 and λj =
m
μk
j
images.
The
parameters
of the Robust PCA method were the
Xjc = Xj − Ejk+1 − Ak+1
;
j
same as those used in [6]. For the LP-based method, we chose
end
the patch size with which the best visual result was observed.
% Step 3: solve C while ﬁxing {Aj } and {Ej }

(Y k+1 )T +μk X c
5. CONCLUSION AND DISCUSSION
C k+1 = j j  μk j j
j

j

% Step 4: Update Yj and μj
for j ∈ [1, 2, ..., N ]
Yjk+1 = Yjk + μkj (Xj − C k+1 − Ak+1
− Ejk+1 );
j
k+1
k
μj = μj ρ;
end
k = k + 1;
end

We proposed a new Joint Sparsity Model with Matrix Completion (JSM-MC), which combines the beneﬁt of joint sparsity and matrix completion in representing an ensemble of
correlated face images. An algorithm was proposed to decompose an ensemble of face images by utilizing the Block Coordinate Descent and SVT. We performed experiments with two
commonly-used face databases to show the effectiveness of
the proposed method. Comparison was made based on results

1667

(a)

(b)

(c)

(d)

(e)

(f)

(g)

Fig. 1. (a) Six sample images from the Extended YaleB Database [10]. The
image size is 192*168. A total of 64 different illumination conditions for a

(a)

(b)

(c)

(d)

(e)

(f)

(g)

single person are used in the experiment. (b) Results of the LP-based method.
The patch size used is 12. Noise is obvious, especially on the dark regions.

Fig. 2.

(c) The innovations in the results of JSM-3. (d) The common part in the results of JSM-3. (e) Results of Robust PCA: the left column is the recovered

and converted to gray scale. A total of 26 images for a single person are
used in the experiments. (b) Results of the LP-based method. The patch size

images and the right column is for specularities or shadows, i.e. sparse matrices. For the images with extreme illumination conditions (i.e. Row 3 to Row

used is 15. Edge artifacts are obvious. (c) The innovations from JSM-3. (d)
The common part from JSM-3. Notice the a ghost effect of glasses around

6), Robust PCA does not work well. (f) Results of the proposed JSM-MC: the
left column is the low-rank matrices, i.e. illumination/occlusion information,

the eye. (e) Results of Robust PCA: the left column is the recovered images
and the right column for the sparse matrices. For the images with occlusions,

and the right column for the innovations. (g) The common part extracted by
the proposed JSM-MC.

such as glasses and scarves (i.e. Row 4 to Row 6), it does not work well.
(f) Results of the proposed JSM-MC. Left column are low rank matrices and

from 3 other existing approaches. Our method was found to
have the unique beneﬁt of being able to capture a clean common image while generating two image-speciﬁc components
that enjoy physically-meaningful interpretations. Potential
applications of model include face recognition and other facial analysis tasks.

right column for sparse matrices. (g) Common part in the results of proposed
JSM-MC.

[6]

[7]

6. REFERENCES
[1] D. Baron, M. F. Duarte, M. B. Wakin, S. Sarvotham, and
R. G. Baraniuk, “Distributed Compressive Sensing,”
Preprint available at http://arxiv.org/abs/0901.3403.
[2] S. Bengio, F. Pereira, Y. Singer and D. Strelow “Group
Sparse coding,” in Advances in Neural Information Processing Systems 22, 2009, pp. 82–89.
[3] Pradeep Nagesh and Baoxin Li, “A compressive sensing
approach for expression-invariant face recognition,” in
Computer Vision and Pattern Recognition Workshops,
2009. CVPR Workshops 2009. IEEE Computer Society
Conference on, June 2009, pp. 1518–1525.
[4] E.J. Candes and B. Recht, “Exact matrix completion via
convex optimization,” Foundations of Computational
Mathematics, vol. 9, no. 6, pp. 717–772, 2009.
[5] J.F. Cai, E.J. Candes, and Z. Shen, “A singular value

1668

(a) Six images of the AR Database [11]. The image size is 165*120

[8]

[9]

[10]

[11]

thresholding algorithm for matrix completion,” preprint
available at http://arxiv.org/pdf/0810.3286, 2008.
J. Wright, A.Y. Yang, A. Ganesh, S.S. Sastry, and
Y. Ma, “Robust face recognition via sparse representation,” IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 210–227, 2009.
EJ Candes, J. Romberg, and T. Tao, “Robust uncertainty
principles: Exact signal reconstruction from highly incomplete frequency information,” IEEE Transactions
on information theory, vol. 52, no. 2, pp. 489–509, 2006.
M. Turk and A. Pentland, “Eigenfaces for recognition,”
Journal of cognitive neuroscience, vol. 3, no. 1, pp. 71–
86, 1991.
Y. Huang, Q. Liu, and D. Metaxas, “A component based
deformable model for generalized face alignment,” in
On Eleventh IEEE International Conference on Computer Vision. Citeseer, 2007.
K.C. Lee, J. Ho, and D. Kriegman, “Acquiring Linear
Subspaces for Face Recognition under Variable Lighting
,” IEEE Trans. Pattern Anal. Mach. Intelligence, vol. 27,
no. 5, pp. 684–698, 2005.
AM Martinez and R. Benavente,
“The AR face
database,” Univ. Purdue, CVC Tech. Rep, vol. 24, 1998.

778

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 22, NO. 5, MAY 2012

Understanding Compressive Sensing and Sparse
Representation-Based Super-Resolution
Naveen Kulkarni, Pradeep Nagesh, Member, IEEE, Rahul Gowda, and Baoxin Li, Senior Member, IEEE

Abstract—Recently, compressive sensing (CS) has emerged as
a powerful tool for solving a class of inverse/underdetermined
problems in computer vision and image processing. In this paper,
we investigate the application of CS paradigms on single image
super-resolution (SR) problems that are considered to be the most
challenging in this class. In light of recent promising results, we
propose novel tools for analyzing sparse representation-based
inverse problems using redundant dictionary basis. Further,
we provide novel results establishing tighter correspondence
between SR and CS. As such, we gain insights into questions
concerning regularizing the solution to the underdetermined
problem, such as follows. 1) Is sparsity prior alone sufficient?
2) What is a good dictionary? 3) What is the practical implication
of noncompliance with theoretical CS hypothesis? Unlike in
other underdetermined problems that assume random downprojections, the low-resolution image formation model employed
in CS-based SR is a deterministic down-projection that may not
necessarily satisfy some critical assumptions of CS. We further
investigate the impact of such projections in concern to the above
questions.
Index Terms—Compressive sensing, Grammian measures,
redundant dictionary, sparse representation, super-resolution.

I. Introduction
UPER-RESOLUTION (SR) is an inverse problem that
deals with the recovery of a high-resolution image from a
single or a sequence of low-resolution images based on either
specific a priori knowledge or just assumed generic notion
about the imaging model. In generation of low-resolution
images, the imaging process normally involves low-pass filtering followed by decimation. Since such a process results
in a loss of entropy, the reconstruction problem is highly
underdetermined. Hence, proper regularization is necessary for
finding an appropriate solution, especially under large magnification factors, due to the large size of the solution space.
Generic edge smoothness priors and/or other visual features

S

Manuscript received March 15, 2011; revised August 29, 2011; accepted
November 3, 2011. Date of publication December 21, 2011; date of current
version May 1, 2012. This paper was recommended by Associate Editor D.
Tao.
N. Kulkarni is with Research in Motion Corporation, Irving, TX 75038
USA (e-mail: naveen.kulkarni@asu.edu).
P. Nagesh is with Digital Media Solutions Laboratory, Samsung Information
Systems America, Irvine, CA 92612 USA (e-mail: pnagesh@asu.edu).
R. Gowda is with Nvidia Corporation, Santa Clara, CA 95050 USA (e-mail:
rahul.gowda@asu.edu).
B. Li is with Arizona State University, Tempe, AZ 85281 USA (e-mail:
baoxin.li@asu.edu).
Color versions of one or more of the figures in this paper are available
online at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TCSVT.2011.2180773

are typically utilized to regularize the solution. Such examples
include gradient prior [1], soft-edge prior [2], Markov random
field [13], primal sketch prior [23], directional-priors [20],
and total variation [3]. The essence of these priors is to
ensure coherence in the local properties of the reconstructed
image. Also, many algorithms extract local features and learn
the local properties via recognition-based priors to obtain
an appropriate high-resolution image [22], [26]. Recognition
and learning-based SR algorithms [22], [24] estimate the
bounds on the super-resolving factor that can be carried out
on natural images. Single-image SR algorithms have been
studied utilizing the patch repetitions across the same scale and
multiple scales in natural images [25] and through supervised
neighbor embedding algorithms [30]. Adaptive domain selection through sparse representation and adaptive regularization
for image SR [31] have also been studied extensively in
recent years. Sparse derivative priors, learning-based image
up-scaling, local-correlation-based SR, and survey of different
techniques used in SR have been compared in [27]. In addition,
classical multi-image-based SR approaches [32] have seen
recent developments, such as the maximum-a-posteriori-based
joint estimation, segmentation, and SR approach in [28] and
the multiframe method based on Zernike moment in [29]. In all
SR problems, a fundamental global reconstruction constraint
is that the “super-resolved” image should yield the original
low-resolution version when the assumed imaging model is
applied. Iterative back projection is one such method widely
employed for this purpose [6], [7].
The recently emerged idea of compressive sensing (CS)
theory provides a different perspective in solving large underdetermined problems, exploiting sparsity as a prior [15]–
[18], [21]. This powerful and promising tool has proven to be
effective for a wide range of problems of this class, including
sub-Nyquist sensing of signals and coding, image denoising,
and de-blurring [11], [15], [16]. Very recently, [7] addressed
the SR problem using a sparse representation-based algorithm, reporting superior results. However, some fundamental
questions are yet to be answered, such as follows. Whether
CS paradigms can address SR problems? Is the theoretical
hypothesis of CS satisfied in the case of SR problems? What
are its implications in practice?
In this paper, our goal is to holistically analyze and understand how effective CS paradigms are with respect to the
SR problem. Since CS has emerged as a powerful tool, it is
of great interest and importance to address the fundamental
questions in CS for underdetermined problems like SR. In

c 2011 IEEE
1051-8215/$26.00 

KULKARNI et al.: UNDERSTANDING COMPRESSIVE SENSING AND SPARSE REPRESENTATION-BASED SR

this paper, we seek to understand and establish a relationship
between CS and SR theories and provide a better understanding of the role of sparsity priors and the properties of the
projection operator and dictionaries.
The remainder of this paper is organized as follows. In Section II, we first provide an overview of CS and then formulate
the SR problem in a CS framework. In subsequent sections,
we provide novel results of analysis on various aspects of
the problem. Section III provides theoretical analysis of the
projection operator and the redundant dictionaries, including
the discussion on the implication of this theoretical analysis
on practical implementation. We also propose a couple of new
metrics that can assist further analysis. In Section IV, experiments are designed and performed to empirically analyze and
evaluate the projection operator and the dictionaries. Extensive
discussion and insightful conclusions are then drawn based on
the evaluation. Section V provides some further visual results
and Section VI summarizes the paper and briefly discusses
future directions beyond this paper.
II. Modeling of SR in a CS Framework
A. Overview: CS Review and Scope of this Study
For completeness, we first briefly review some necessary
background about CS. Suppose that a signal x ∈ RN is Ssparse with respect to a basis ψ ∈ RN×N (i.e., x = ψα, ||α||0 =
S < N), we define its measurement as y = x, y ∈ RM ,
using a projection-operator  ∈ RM×N , M < N. Then, CS
says that x can be recovered from y ∈ RM using a decoder 
that involves solving either of the following l1 minimization
problems:
 = arg min ||α||1
(BP) α
 = arg min ||α||1
(BPDN) α

s.t.
s.t.

y = ψα or

(1)

||y − ψα||2 <∈ .

(2)

Equation (1) is basis pursuit (BP) and (2) is the basis
pursuit denoising (BPDN) approach [15]. Faithful signal recovery is guaranteed by any decoder  provided M ≥
Cμ2 (, ψ)S log N, where C is a constant, μ(, ψ) is the
coherence between the pair of the measurement matrix and the
sparsifying basis (, ψ) [15]–[18], S is the sparsity of signal
x, and N being the dimension of signal x. Here, coherence is
defined as
μ(, ψ) = max |φj , ψk |, j ∈ , ψk ∈ ψ.

(3)

j,k

An important question for optimal reconstruction of an
S-sparse x is what is the best measurement matrix  ∈ RM×N
so that the number of measurements M is least? The answer
is provided by the notion of restricted isometry property (RIP)
by Candes [15]–[17]. We say that ψ satisfies an RIP of order
S, with a constant  ∈ (0, 1) if
(1 − )||x||22 ≤ ||ψα||22 ≤ (1 + )||x||22 , x ∈


s

.

(4)


Here, s is the set of all S-sparse vectors x, (x = ψα). If ψ
satisfies the RIP property of order 3S for some  ∈ (0, 1),

779

then we say that we can reconstruct x from y = x using a
CS decoder  within an error bound given by
σ s (x)1
(5)
||x − (ψα)||2 ≤ C √ .
S

Here, σ s (x)1 := inf ||x − z||1 , z ∈ s is the error of the Sterm approximation to x in l1 norm. For optimal reconstruction
results, ψ has to satisfy RIP of order S given by [14], [15]
S = M/ log(N/M).

(6)

Another notion says that if the sparsity is bounded as
S ≤ M/(Cμ2 (, ψ) log(N/))

(7)

for a given coherence μ(, ψ) and a constant , then a
decoder  can perfectly recover x with probability exceeding
1 − . Thus, for a given pair (, ψ), higher the RIP (order
S) (or equivalently lower the coherence μ(, ψ)), better the
reconstruction (i.e., better reconstruction guarantee and smaller
reconstruction error) for any decoder . In most CS problems,
the basis ψ is generally assumed to be orthonormal (ONB),
and the projection  is usually chosen as a random Gaussian
matrix as it possesses good RIP and is highly incoherent with
most ψ [15].
An SR problem may be viewed similarly, where y can
be construed as the low-resolution image obtained as a measurement of the high-resolution image x. However, there are
subtle differences in this formulation. The projection matrix
 is no longer a designer’s choice but rather an imaging
model (blurring followed by decimation) and is consequently
deterministic. This is contrary to most existing CS work.
Furthermore, the sparsifying basis ψ may not necessarily be
an ONB, but rather an arbitrary redundant dictionary (ARB)
(denoted as D ∈ RN×K , K >> N). Although the dictionary
D may be a powerful basis in terms of keeping structural
information of the images, leading to good reconstruction of
high-resolution images, it may not strictly satisfy certain CS
hypotheses in comparison to ONBs.
The goal of this paper is to understand and analyze SR
as a problem in a CS framework. Specifically, we aim at
better understanding of the following issues: 1) properties and
implication of the SR projection operator; 2) properties and the
role of dictionaries; 3) role of sparsity priors and CS solvers:
gap between theory and practice; and 4) on solution space in
sparse-recovery: importance of uniform sparse-recovery. We
will start with the problem formulation first.
B. Problem Definition: SR Based on CS
Before we address the questions on the SR projection operator, the dictionaries, and CS solvers, it is necessary to formally
formulate the SR problem in a CS framework. The SR problem
is to recover the high-resolution image X back from a single
or multiple low-resolution images Y i , i = 1, . . . , J.
In this analysis, we consider only the case of a single input
image (J = 1). The low-resolution image Y is obtained from
the high-resolution counterpart, through the following imagegeneration model:
Y = RLp X = LX, X ∈ RP×Q ,

Y ∈ RP̃×Q̃

(8)

780

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 22, NO. 5, MAY 2012

Fig. 1. Low-resolution image generation. (a) Simple model: 2-D low-pass
filter and filter + decimator expressed as matrix L. (b) Example on 6×6 pixels
to get 2 × 2. (c) 2-D frequency response of a random projector (left) and
an L.

where Lp is generally a low-pass operator and R is a decimation operator that reduces the pixel resolution of X. And
U = P/P̃(= Q/Q̃) is the decimation factor, but in the context
of SR, we call it magnification factor or simply up-factor.
The entire operation is linear in nature and we represent
it as a matrix operation L = RLp . We refer to L as the
SR projection operator hereafter, in connection with the CS
framework reviewed in Section II-A. An example is shown
in Fig. 1 for P, Q = 6, along with the matrix representation
of L. Now the problem of SR is to perform the inverse
operation, which is challenging since the process of (8) results
in information loss. The problem can be approached locally
by finding the solution on individual image patches [7] with
an additional global reconstruction constraint that the input Y
should be obtainable when the model of (8) is applied on the
estimated X. Now, if x ∈ RN is a 1-D representation of a small
patch of X, we have an overcomplete dictionary D ∈ RN×K
that can sparsely represent x as
x = Dα, ||α||0 = S, S < K

(9)

then, the low-resolution patch is given by
y = Lx

(10)

where y is a low-dimensional projection of x, similar (if not
typical) to a CS measurement. If the CS recovery conditions
in (6) and (7) are satisfied, then the sparse vector α in (9) can
be recovered from the lower-dimensional measurement
y = LDα

(11)

by solving the optimization problem (1) or (2) (replace  by
L and ψ by D). A global reconstruction constraint like backprojection can be employed to make the final solution comply
with the imaging model of (7) [7]. In the next section, we
present theoretical analysis of the projection operator and the
dictionaries.

the context of SR and compare it with (, D). Again, we
emphasize that L is a deterministic projection operator,  is a
random projection operator, and D is an overcomplete dictionary (ARB). Most of the CS theories have been developed for
sparse representations on ONBs, but recently in [8] and [19],
attempts have been made to generalize these theoretical results
on sparsity/recovery constraints to any ARBs. For example,
mutual-coherence μ of (3) is a good measure and can be relied
on for evaluating tighter sparsity bounds of a CS system with
(, ψ) (ONBs). So we will resort to theoretical analysis of
the properties of the projection operator and the redundant
dictionaries to understand the sparsity bounds and its relation
to the mutual coherence.
A. Projection Operator L
As discussed earlier, in contrast to the random projection
operator , the L operator is deterministic in nature. A first
distinguishing property of L in comparison with  is its
frequency discriminative nature. An effect of this is that L
preserves only the low frequency information, whereas 
uniformly preserves information across all frequencies. From
a CS connection, this implies that L exhibits good RIP
characteristics (4) for the class of signals that has low-pass
information only, while  may be consistent for any class
of signals. Fig. 1(c) visualizes the 2-D frequency responses
of the two operators. We draw attention to the fact that the
matrix representation of L := (li,j ) is circulant in nature since
it satisfies the property li+1,j+u = lij [see Fig. 1(a)], where
u := N/M and i, j are row, column indices incremented in
modulo N arithmetic. In this regard, we draw an interesting
connection to the results in [14] obtained for deterministic
CS matrices. Specifically, [14, Theorem 3.4] states that the
circulant matrix constructed from finite fields satisfies the RIP
property of order S given by
√
S < 1 + M log M/(8 log(N/M))
(12)
and since L ∈ RM×N is a similar matrix, we may use
(12) as an upper-bound. Hypothetically, either considering
L independently or in conjunction with an ideal-basis, (12)
indicates a much inferior bound on sparsity compared to the
case of a random operator required for optimal reconstruction.
For example, if we consider an ideal basis and an imaging
model L, image patch y of M = 9 (3 × 3 pixels), and an
original x patch of N = 81 (9 × 9), then, the upper bound
for sparsity is S < 1.4 or S = 1 as opposed to S < 9 for a
random operator  (6). The upper bound on S = 1 confirms
the fact that the image patch itself has to be the basis. But in
reality, such basis may not exist and we may need to resort to
dictionaries D. Thus, the sparsity bounds should be evaluated
using joint properties of the pair (L, D).
B. Redundant Dictionaries in SR

III. Theoretical Analysis of the Projection
Operator and the Redundant Dictionaries
Our goal is to evaluate and understand the nature of a
given pair of projection operator and dictionary, (L, D), in

What is a good dictionary? This is the fundamental question
that has been researched on in the recent years for various
scenarios or goals (sparse-representation/coding, recognition,
etc). A very naive overcomplete dictionary is one whose base
atoms are the “element-type” itself selected from random

KULKARNI et al.: UNDERSTANDING COMPRESSIVE SENSING AND SPARSE REPRESENTATION-BASED SR

781

Fig. 2. Original dictionary D ∈ R81×1024 (9 × 9) trained by [21]. (a) Grammian (15) of D, LD, and D (three different up-factors). (b) GramH (16)
(p = 2, 4 bins).

sampling of some training data. In the case of SR, they are
raw image patches randomly sampled (hence, the name RS
dictionaries). We have also seen recent attention on training
algorithms with a goal to obtain compact dictionaries [10],
[11], [21]. In SR, the goal is not sparse representation, but
sparse recovery. In this section, our objective is to gain insight
on properties and performance of RS and trained dictionaries.
Unlike in ONBs, which provide a unique sparse representation, the first question is if unique single sparsest representation exists for a system of (9). According to [9], if the
condition


1
1
||α||0 = S <
1+
(13)
2
μ{D}
is satisfied, then the sparse representation α is unique and also
the sparsest. From a low-dimensional space with L, perfect
sparse recovery of α requires much stricter criterion to be
satisfied, that is


1
1
||α||0 = S <
1+
.
(14)
2
μ{LD}
In practice, for most D (RS or well-trained), μ{D} and μ{LD}
are almost close to 1 [see Fig. 2(a)], which yields sparsity
bounds no better than S = 1 (as in earlier ideal case of Section
III-A). Thus, theoretically, this means that optimal recovery is
possible only if there is exactly one match in the dictionary.
Although being theoretically correct, this overpessimistic demand does not provide practically useful understanding on
aforementioned questions, and further analysis on the pair (L,
D) is needed to gain practical insights.
C. Proposed Tools for Analysis of L, , and D
As we can see from the above analysis, there is a need to
evaluate the joint properties of the (L, D) pair. Also, the mutual
coherence μ evaluated for different dictionaries may not provide complete information on their properties. Similarly, for an
ARB (D), complete reliance on μ for a stricter sparsity bound
will always be misleading, since D ∈ RN×K has K >> N. So,
one may obtain similar μ for a relatively well-conditioned D
having fewer similar-atoms as well as a totally ill-conditioned
one with a large number of similar atoms. Other options may
include relying on RIP-based on uniform uncertainly principle
[18]. Reasoning similar to the case of coherence, RIP constants
only give the worst-case conditioning of the dictionary, so they

are not completely reliable. Another notion is a geometrical
view point in [17]. Since none of the measures described above
provide a clear description of the properties of the dictionaries,
there is a need for new method of analysis that provides
insights into the nature of the dictionary and its atoms and
its collective influence on signal reconstruction.
In this paper, in addition to coherence, we propose new
methods to evaluate dictionary D or its projection OD matrix,
(O is L or ), based on the Gram-matrix that is defined
T
as G(D) = D̃ D̃, where D̃i = Di /||Di ||2 (i.e., columns
normalized by l2 energy). Then, the coherence (μ) of the
dictionary [see also (3)] is redefined as
μ(D) 

max

1<i,j<K;i=j

G(i, j)

(15)

and takes the values in [0, 1]. A 0 signifies least coherence
(orthogonal columns) and a 1 means the opposite (exactness).
In the remainder of this paper, we will resort to the following
new statistics for analyzing D or OD (O  L or ).
1) Gram-Histogram: This is a histogram of μ defined as
GramH{D, K, p}  hist ({μ(Dp )}, bins ∈ [0, 1]

(16)

where {Dp } is the set of all submatrices of D formed by
choosing p column support from the set {1, . . . , K}. There
are K Cp such possible elements. Thus, this is similar to RIP
evaluation, but it provides additional statistics as to how well
conditioned the base atoms are. For example, if p = 2, then
(16) evaluates the distribution of coherence μ for all K Cp
“pair-wise” combinations of the base atoms. This can be
evaluated over B bins in the range [0, 1]. More entries in
the lower bins (near 0) means that on a pairwise basis, most
atoms are highly uncorrelated. More entries near 1 signify
that many atoms are similar (ill-conditioned). If evaluated for
OD(O  L or ), it gives joint properties of (O, D). For
p = 2, (16) can be easily implemented by simply plotting the
histogram of Gram matrix G with diagonal explicitly made,
say −1.
2) Gram-Member: This is defined as
GramM{D, T, p, B ∈ [0, 1]  K̃.

(17)

Here, T ≤ K is a threshold and B is a bin in the range [0, 1].
K̃ ≤ K gives the number of Gram members for bin B. The ith
base atom (column vector) Di is called the Gram-member of

782

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 22, NO. 5, MAY 2012

Fig. 3. (a) GramM (17): LD and D for various dimensions: 3 × 3, 4 × 4 & 6 × 6 & original D with p = 2 and T = 30. (b) Reconstruction RMSE curves:
L, andR for various up-factors.

bin B under threshold T, if the following is true: one can find
at least T submatrices in the set {Dp } involving Di , for which
μ(Dp ) ∈ B. To explain this better, let us take an example of
p = 2, T = 50, and dictionary D of size K = 1024. Now
{Dp } is the set of all “pair-wise” combination of submatrices
and there are 1023 such pairs for a base atom Di denoted as
{Dp,i }. If there are at least 50 (T ) elements in {Dp,i } for which
μ(Dp,i ) ∈ B, then, we declare Di to be a member of bin B. We
repeat this for all Di , i = 1, . . . , K. The final result of GramM
of (17) is simply the count of the number of members in bin
B. Thus, if B is near zero, like [0,  (for a small ), GramM
would provide the information about the number of base atoms
that maintain ultralow correlation with at least T other base
atoms. Greater this number, better it is. Similarly, for a B near
one [1 − , 1], GramM should be as low as possible. Note
that greater the T, stricter is the measure. If the percentage of
base atoms with ultralow correlation with at least T other base
atoms is close to 100%, then the dictionary exhibits excellent
well conditionedness.
GramM conveys more local information since it provides
information regarding the uncorrelatedness between the base
atoms and the GramH provides global information on well
conditionedness of the dictionary D or the pair (O, D) as a
whole. In our analysis, we typically use p = 2 and classify
the coherence bins as [0, 0.1] (best), (0.1, 0.3] (good), (0.3,
0.8] (mid), and (0.8, 1] (worst) for analysis of GramH. Also,
in the case of GramM, we simply use bins in steps of
0.1. The measure of well conditionedness of a dictionary
directly translates to a significant quality improvement in the
recovered/reconstructed image. With the theoretical analysis
in place and the new tools proposed, we now proceed to the
empirical analysis, which includes the experimental evaluation
of the projection operator and dictionaries in terms of the new
coherence measures GramM and GramH, together with visual
results to corroborate the empirical evaluation.

IV. Empirical Analysis of SR in a CS Framework
A. Evaluation of the L Projection Operator
1) Gram Statistics Validation of L Projection Operator:
We consider an overcomplete dictionary D with 90 000 atoms

Fig. 4. (a)–(c) Visual results. (a) Reconstructed 9 × 9 patches from 3 × 3 for
L (left) and  (right). Reconstruction of 9 × 9 from (b): 5 × 5 for L (left) 
(right), and (c): 6 × 6 dimensions for L (left) and  (right).

obtained by randomly sampling patches from the training
images. This randomly sampled dictionary is trained using the
feature sign search (FSS) algorithm [21] to obtain a dictionary
of size 1024. The Grammian (coherence) of (15) for D (9 × 9
patch size), LD, and D are shown in the table of Fig. 2(a).
Clearly, this worst-case RIP/coherence measure is high for
all cases and shows only marginal superiority for . Thus,
we resort to Gram-statistics measures proposed earlier. In
Fig. 2(b), GramH measures of (16) (with p = 2) are compared
for (L, D) and (, D) pair for M = 9 from original N = 81.
Clearly, D is far well conditioned: 50% pairwise correlations
among 1024 C2 have the lowest coherence [0 − 0.1) and only
0.05% falls in the inferior range. It is also important to note
that with blur introduced, the dictionary is slightly inferior
when compared with the original as seen in Fig. 2(b) for LD
with blur. On the other hand, both L and  projections degrade
the conditioning. However, L degrades D much higher than 
as indicated in the best and worst correlation bin statistics. In
Fig. 3(a), we present the GramM measures of (17) for (L, D),
and (, D) for various projection dimensions (3 × 3, 4 × 4,
6 × 6), evaluated with p = 2 and T = 30. For a fixed up-factor,
 curves are superior compared to L (higher coherence bins
have lesser Gram-members for  than L). This trend is true
for any up-factor. On the other hand, compared to D, both LD

KULKARNI et al.: UNDERSTANDING COMPRESSIVE SENSING AND SPARSE REPRESENTATION-BASED SR

783

Fig. 5. (a) High-pass filtered LD generated using FSS. (b) High-pass filtered D generated using FSS. Noticeable is RMSE for LD is less than D.
(c) Low-pass filtered LD generated using FSS. (d) Low-pass filtered D generated using FSS. Again noticeable is RMSE for LD is less than D.

and D degrade as up-factor increases. Thus, in line with the
theoretical results, these measures also show that from a CS
connection, L is inferior compared to .
2) Performance Evaluation: With this, we are now interested in understanding the practical implications of L in SR.
We evaluate the performance by devising experiments determining the distortion characteristics in “super-resolving” image patches by different up-factors (Us). We selected multiple
9 × 9 patches xi with varied texture information from multiple
high-resolution test images. The corresponding low-resolution
patches yi were created assuming L to be a Gaussian blurring
kernel with cut-off frequency π/U followed by a decimation
U ↓ (or R, see Section II-B). We recover the original patch
by solving for α in (11) using BPDN (2). Fig. 3(b) shows
the results of the experiment—average root mean square error
(RMSE) curves for L and  operators for various up-factors.
Although the  does not have any semantic meaning in SR, we
use it to benchmark and understand L for reasons discussed
earlier. From theoretical perspective and Gram-analysis, as expected, D is better conditioned than LD. However, this does
not translate to superior performance as indicated in Fig. 4(a)–
(c). In fact, from Fig. 3(b), the L curve is better than ,
especially for dimensions lower than M = 7 × 7. To illustrate
this, a frequency domain analysis on the reconstructed images
for both LD and D is obtained. In general, the patches xi
of natural images only occupy a part of frequencies lesser
than the full Nyquist range. We may assume that x is bandlimited to say π/Z, for some Z ≥ 1 (π being the Nyquist
frequency). Suppose y ∈ RM×M andx ∈ RN×N . Thus, the upfactor is U = N/M. We consider two cases in the following.

Case 1: U > Z. If the blurring filter in L has good
transition characteristics, then y = Lx preserves most of the
entropy of the original N pixels in the range (0, π/U) in M
points. It loses information in (π/U, π/Z). On the other hand,
 works to preserve all the energy in (0, π) in M points giving
equal weight to every band in (0, π). As M is lowered (i.e.,
required U is increased), the RMSE of the estimated x̂ with
reference to x is much superior for L than , in the range
(0, π/U), leading to better overall RMSE. In qualitative terms,
L, due to its low-pass discriminating nature, does not waste
any measurements capturing information in the higher end of
the Nyquist range. Even for high-texture patches, it is unlikely
that the power-spectral density is completely concentrated near
higher-frequency band (significant energy will be distributed in
lower bands too). Thus, overall L performs as well as or better
than . But in both cases, the problem is underdetermined:
 faces challenges in recovering all frequency information
equally, but for L it is only in the range (π/U, π/Z).
Case 2: U < Z, then the probability of perfect recovery
for L is high. Visual results in super-resolving Lenna image for
U = 3 is presented in Fig. 4(a), which corroborate these facts.
The left image is for L and the right for . Fig. 4(b) and (c)
further illustrates the SR results for a high-texture region with
other up-factors. This observation is in line with the fact that L
preserves all of the energy within U while  tries to preserve
all of the energy within Z. As the up-factor increases,  closes
in on L in performance.
To further the above discussion, we resort to frequency
domain analysis. The reconstructed images from both L and
 as well as the original image are subjected to low-pass

784

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 22, NO. 5, MAY 2012

and high-pass filtering. Then all these images are subjected
to a Fourier transform for frequency domain analysis. These
images represent the reconstructed frequency characteristics
for each of low-pass [Fig. 5(c) and (d)] and high-pass images
[Fig. 5(a) and (b)]. As can be seen from Fig. 5(c) and
(d), low-pass characteristics are better represented for the Lreconstructed image than the -reconstructed image.  tries
to recover many unnecessary frequency components that are
not seen in Fig. 5(c). Thus, we see a lot of high-frequency
components in Fig. 5(d), which do not represent the actual
frequency components of the original image. Also the correct
low-frequency components are significantly lesser in  than
in L. This argument is also supported by the RMSE generated
for the low-pass reconstructed images for both L and . A
similar argument can be made even for the high-frequency
components case. As can be seen in Fig. 5(b), the unnecessary
low-frequency components are more than the corresponding
counterpart as shown in Fig. 5(a). The RMSE is less for L than
for , which is synonymous with the argument made above
and hence validates the arguments. This again illustrates the
fact that  tries to preserve energy across Z while L tries to
capture only relevant energy within U.
Now that the properties of L are evaluated experimentally,
we focus the next subsection on the properties of D in SR.
B. Experimental Evaluation of Redundant Dictionaries
We resort to Gram statistics for evaluation of dictionaries.
The high-res dictionary D and low-res dictionary LD are
evaluated for dictionaries trained using FSS and KSVD, a
dictionary based on stochastic approximations (SAs), and a
nontrained dictionary (RS).
1) Gram Statistics Validation: We consider the two categories of Ds of size 1024 for N = 81(9 × 9) high-resolution
patches: a) RS (evaluated for various trials of random sampling), and b) two examples of trained dictionaries: FSS [21]
and K-SVD [10], [11]. Fig. 6 provides the GramH measures
for p = 2 and four bin ranges for these types of Ds and their
low-dimensional versions LD. Clearly, for the lower coherence
bin (0 − 0.1) in D, the statistics indicate that training reduces
the correlation among the base atoms. FSS is overall better
conditioned than SA and KSVD with 50% against 41% and
38% of pairwise correlations, respectively, while RS has 30%
in the (0 − 0.1) region. On the other hand, the worst-case
correlations in the region (0.8−1) of FSS is very low (0.05%),
but significant (0.33%) for RS. KSVD dictionary has higher
value in this bin compared to RS. The general conditioning of
LD for all types of D degrades (see Fig. 6). For 3 × 3 size,
the numbers maintain similar trends across FSS, KSVD, SA,
and RS dictionaries. The number of worst-case correlations increases to a quite high of 6.5% in RS, while for the trained they
remain relatively low. Fig. 7(a) compares the GramM measures
of FSS and RS (D and LD for 6 × 6 and 3 × 3). Clearly, the
curves indicate that FSS has superior conditioning than RS
both in high and low-resolution dictionaries D and LD.
2) Performance Evaluation: Fig. 7(b) shows the RSME
curve in super-resolving for different up-factors for the four
types of D. Clearly, we can see trained dictionaries (from FSS,
SA, and K-SVD) perform better than the RS counterpart for a

Fig. 6. GramH (16) (p = 2, 4 bins) for various types of dictionaries D of
length 1024 (9 × 9 patches) and two categories: RS (average for various RS)
and trained (FSS, SA, and KSVD), plus GramH for LD (3 × 3).

fixed dictionary length K. We verified this trend for different
Ks of RS dictionary. Although the GramH indicates that FSS
is superior overall [Fig. 7(b)], it has only small improvement
in performance as compared to a KSVD and SA dictionaries.
But an important observation here is that though  has better
GramM and GramH characteristics than L, the same measures
are useful in analyzing the trained and RS dictionaries. As
indicated by curves from Fig. 7(a) and (b), trained dictionaries
are far better than the RS dictionaries. Thus, GramM and
GramH measures can be useful to estimate the reconstruction
properties of the dictionary. These measures can be used as a
tool to compare the SR performance for different dictionaries.
The implications of these measures can be directly seen in the
RMSE curves shown in Fig. 7(b). Although we can see that D
of Fig. 6 has very good conditioning, it is not the case with LD
since low-pass operator reduces the well conditionedness. In
addition to comparisons with D (original) dictionary, a blurred
version of the original is also experimented with. The blurred
dictionary is evaluated for both FSS and RS dictionaries that
clearly indicate that they are inferior when compared with
the original. But they retain most of the characteristics of the
original image (no down-sampling) and hence have similar
properties as the original. With these results, we can now
discuss the SR solution space and the CS solvers.
C. SR: Solution Space and CS Solvers
We gained insights on the role and properties of dictionaries
in SR in the previous subsection. This subsection bridges
understanding of some important questions related to sparse
solution and recovery in SR: 1) the role and constraints on
sparsity; 2) solution space and CS solver; and 3) is uniform
sparse-recovery possible or important?
1) Theoretical and Practical Connections: For a dictionary
satisfying (13), the BP problem (1) is guaranteed to find
the unique sparsest solution [19]. However, for actual SR
dictionaries discussed in the previous section, a BP solver
like l1 − magic has stability issues due to the size and
poor conditioning properties of the dictionaries (compared to
ONBs). In practice, the unconstrained version of BPDN (2) is

KULKARNI et al.: UNDERSTANDING COMPRESSIVE SENSING AND SPARSE REPRESENTATION-BASED SR

785

Fig. 7. (a) GramM (17) with p = 2 and T = 30 for FSS and RS. (b) RMSE curves for various up-factors for the RS and trained (FSS, KSVD) dictionaries.
A curve is an average evaluated over various patches. Clearly, FSS and KSVD dictionaries perform better than RS.

cast as the following:
 = arg min τ|||α||1 + 0.5 ∗ ||y −
α

LDα||22

(18)

which is a suitable choice for the CS decoder. Here, τ is
a regularizer that controls the tradeoff between sparsity and
fidelity. In this section, we study and provide interesting
insights on the question “how necessary is a sparse solution for
SR?” and “what is a suitable value for the τ ?” Accordingly,
we are interested in the following zones of operation [19],
based on τ, when solving (18).
1) For τ = 0, (18) reduces to an l2 problem.
2) For τ = 0+ (positive but arbitrarily close to 0), the unique
optimum point of (1) or BP coincides with (2) or BPDN
under certain conditions [19].
3) For τ in the interval, (0+ , τmax ), where τmax =
||(LD)T y||∞ the solutions of BP and BPDN diverge.
As τ increases, the sparsity of the solution improves at
the cost of fidelity.
We want to study the behavior of the CS solver in various
ranges of the regularizer τ in terms of sparsity and thus we
define the following two important aspects on sparse solution.
Uniform sparse-representation: The sparse representation problem defined in (9) is recalled here
x = DαH , ||αH ||0 = S, S < N

(19)

and the sparse analysis is achieved by solving (18). We
represent τ values of (18) as follows:
τ = β||(D)T x||∞ , β ∈ (0+ , 1).

(20)

With τ value set with a β as per (20), we call the sparse
coefficient recovered by (18) as αHβ . The support of αHβ is
a set {Tβ } of size Sβ chosen from {1, . . . , K} (where K is
the length of D). We say that the BPDN decoder performs
uniform sparse representation, if {Tβ1 } is a subset of {Tβ0 }
with Sβ1 ≤ Sβ0 , for any β1 > β0 . This is the same best S-term
sparse approximation observed as β or τ increased.
Uniform sparse-recovery: In SR what is important is
sparse recovery. This involves (10) solving for αL
y = LDαL , |||αL ||0 = SL ,

SL < K

(21)

by BPDN and form an SR patch as 
x = DαL . The equivalent
τ values in solving the system of (21) is again defined similar
to (20), except that D is replaced by LD and x by y. Now
uniform sparse recovery happens when the support of αLβ is
a subset of that of αHβ (again best S-term approximation).
We are interested in analyzing such aspects to understand the
sparse SR solution space.
D. Operational Characteristics in SR
First, we perform an experiment to show the optimal zones
of operation for an acceptable reconstruction in SR. As before,
we use a similar set-up as in Sections IV-A and IV-B. We
assume an up-factor of 3 and determine the reconstruction
fidelity and the corresponding sparsity [both in (19) and (21)]
for various τ values for an RS dictionary. Fig. 8 shows the
related results. We find that the best zone of reconstruction is
for a range of τ (from 0+ > 0) (shaded region in Fig. 8).
Surprisingly, in this zone, the fidelity is stable irrespective
of sparsity. We term this region “Relaxed Sparsity Zone”—
where the constraints of sparsity is of reduced significance.
Similar trends are observed even with trained dictionaries and
hence the plots are omitted. Referring to the dotted curve of
Fig. 8 [sparse-representation problem of (19)], we see that as
τ increases, the RMSE degrades, while sparsity increases. The
SR or sparse-recovery of (21) can perform no better than this
dotted RMSE curve (it acts as the lower bound). However,
interestingly, in the relaxed sparsity zone, for a wide range of
τ, the recovery (21) has stable and constant RMSE, indicating
that striving for sparsity is not necessary or significant. A
threshold is set to determine the impact of the coefficients
on sparsity. Hence, only significant coefficients above this
threshold are taken into account, while plotting curves in
Fig. 8(a) threshold is set to eliminate the smaller nonzero
coefficients that might not strongly contribute toward sparsity.
Note that the sparsity for recovery in (21) is higher than that
for representation (19) as can be seen in Fig. 8 and it varies
from 60 to 4 or 5 coefficients. On the other hand, striving for
sparsity as per theoretical bounds (Section III) of S = 1 for
optimal recovery is meaningless as the reconstruction heavily
degrades. Further, it was verified for τ= 0 or an l2 case, the
results are not optimal either.

786

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 22, NO. 5, MAY 2012

Fig. 8. Curve showing reconstruction RMSE and sparsity as a function of β
[τ, see (20)], which is a fraction of the interval ]0+ , (LD)T y||∞ [, as per (20).
Clearly, in the shaded zone the reconstruction is stable across sparsity S. For
the other regions, even though S satisfies optimal reconstruction constraints,
i.e., S = 1, RMSE suffers.

Next, we study the uniform sparse representation and recovery characteristics for the three dictionaries. For the former,
we solve (19) for various values of τ and plot the percentage
common support of αHβ between {Tβ0+ } and {Tβ } for all other
β > 0+ . For the latter, we plot the percentage common support
between αHβ and αLβ as a function of β (τ). Again similar
to the case for determining sparsity, a threshold is set and the
coefficients above this threshold are used for finding indexes of
common supports. Common supports are calculated as indexes
of coefficients that contribute strongly toward sparsity. At
different sparsity levels or with different τ, common indexes
with coefficient values above a specified threshold form the
common supports of αHβ between {Tβ0+ } and {Tβ } for the
sparse representation case and between αHβ and αLβ for the
sparse recovery case. Our major findings are as follows.
1) Uniform sparse representation: This is satisfied for all
four dictionaries to a similar degree [see Fig. 9(a)].
2) Uniform sparse recovery: Interestingly, recovery characteristics are much better and consistent with increase in
τ for RS [see Fig. 9(b)]. The common support forms a
monotonically increasing curve for only RS.
However, despite such clean characteristics (which are
important in CS), we saw that RS performs inferior to
trained counterparts. This along with earlier discussions on
sparsity/relaxed sparsity zones corroborates the fact that in
SR, uniform sparse recovery is not important and does not
guarantee better results unlike in conventional CS using ONBs.
Similar characteristics for all three trained dictionaries in
uniform sparse recovery also signify their correlation toward
RMSE characteristics for the trained dictionaries, which in turn
depend on the GramM and GramH properties of dictionaries.
Thus, GramM and GramH certainly do provide a basis for
measuring the quality of the dictionary not only in terms of
correlation between basis atoms but also in terms of the RMSE
obtained after reconstruction. Finally, from these discussions,

Fig. 9. Evaluations of the percentage common supports for uniform
(a) sparse-representation and (b) uniform sparse-recovery.

we summarize the analysis results on the solution-space in SR
problems in Fig. 11, which consists of concentric regions of
sparse solutions yielding constant mean squared error (MSE)
(relaxed sparsity regions, shown in different colors), with
sparsity being relaxed as we move outward from the central
black region to the outer green region. Points within the same
region may have widely varied sparsities, with or without
common supports (i.e., need not be best S-term subsets), but
yet yield similar reconstruction. For a sparse-recovery case, on
varying τ, the decoder remains in the same region as shown
through the yellow arrow in Fig. 11 and is not promoted to
a superior MSE region. But for a sparse-representation case
(19), the decoder follows the black arrow, traversing across
the constant MSE regions with an increasing τ.
V. Visual Results and Discussion
We now present and discuss on additional visual results
obtained from RS and trained (FSS and K-SVD) dictionaries.

KULKARNI et al.: UNDERSTANDING COMPRESSIVE SENSING AND SPARSE REPRESENTATION-BASED SR

787

Fig. 10. (a)–(d) Visual results for an up-factor = 3. Top left in each of (a)–(d) is the original image, top right in each of them is generated using the FSS
dictionary, bottom right in each of them is generated using the KSVD dictionary, and bottom left in each of them is generated using the RS dictionary. When
we observe closely, we can see how there is slight degradation in image quality as we move clockwise from top left to bottom left.

Fig. 11. Summarizing and illustrating the SR solution space, with concentric
regions representing the relaxed sparsity zones.

Tests were conducted on a wide variety of images using
these dictionaries and a few results are presented here due to
limited space. Fig. 10 shows visual results for four different
images for an up-factor 3. Clearly, we can see FSS and KSVD
(trained) and SA (not shown here) dictionaries outperform RS
(untrained) dictionary. Some important characteristics to note
are the following.
1) Consistency of solution in the whole image (patch
neighbor) is far superior in the trained dictionary cases.

This is due to the fact that the probability of the solver
picking up an unambiguous base atom from a trained
dictionary (FSS, KSVD) is higher compared to that
of a RS dictionary. This is evidently due to the well
conditionedness of a trained dictionary in terms of its
uncorrelated base atoms. Discontinuity does not appear
when an overlap constraint (smoothness constraint [7])
is imposed on the solver while it picks a base atom from
a trained dictionary.
2) In RS, the result shows local patchwise discontinuities. Although these can be alleviated by applying smoothness constraints [7], RS will have artifacts
that cannot be removed completely by any type of
smoothness constraints, because of the reasons explained
above.
3) As we can see from the objective performance metrics
given in Fig. 12 for the four sample images, FSS
performs slightly better than KSVD and SA. The reason
can be attributed to the well conditionedness of the
FSS dictionary when compared to KSVD and SA as
described in Section IV. It can be seen that all three

788

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 22, NO. 5, MAY 2012

Fig. 12. Average MSE over all patches for each of the images shown in
Fig. 10. It can be noticed that the trained dictionaries (FSS, SA, and KSVD)
perform better than the RS dictionary.

trained dictionaries (FSS, SA, and KSVD) perform
much better than the RS dictionary.
The patchwise discontinuities as can be seen in the results
from the RS dictionary is because of the higher percentage
of correlated base atoms in the [0.8 − 1] range tabulated in
Fig. 6. Training reduces the percentage of correlation between
base atoms and minimizes the worst-case correlation [0.8 − 1]
range of Fig. 6.
Also, one more important observation is the convergence
of the MSEs from the trained dictionaries as the patch size
is increased from 3 × 3 to 9 × 9. This is due to the fact that
when the up-factor decreases from 3 toward 1 (i.e., moving
from patch size 3 × 3 to 9 × 9), the ill conditionedness in
terms of the GramH measure of a trained dictionary keeps
decreasing. Then the GramH of LD will approach the GramH
of D of Fig. 6.
VI. Conclusion
We investigated various issues in SR within a CS framework. A strong relationship between CS and SR was established and their underlying properties were analyzed. This
paper, including its discussion and experimental illustrations,
served to bridge some critical gap in the knowledge of CSbased SR problems. We primarily discussed the following
aspects of the problem.
1) Implication of the deterministic operator: The deterministic operator LD (joint properties of L and D) when
compared with random basis like D yielded superior
performance in terms of lower reconstruction error of
high-resolution image. This is due to the fact that LD
tried to preserve all energy within the downsampled
spectral range, while D tried to preserve in the entire
spectral range.
2) Properties and performance of dictionaries: Trained
dictionaries were effective in aiding a solver to pick
an unambiguous base atom for reconstruction than the
untrained counterpart. The reduced redundancy among
the base atoms in a trained dictionary lead to lower
reconstruction errors.
3) Grammian analysis: GramM and GramH, respectively,
brought out local and global properties of the dictionaries. These properties can be analyzed to evaluate

the reconstructive capability of trained and untrained
dictionaries.
4) CS solvers and solution space, with implications on
sparsity, uniform sparse recovery in SR: As we could
observe from the experiments, sparsity was not a necessary criterion unlike in conventional CS methods, and
uniform sparse recovery may not necessarily guarantee
better reconstruction results as discussed in operational
characteristics in SR of Section IV-D.
Future Work: Obviously, these understandings will provide design guidelines in designing an SR system based on the
CS framework. Specifically, here we emphasized the fact that
theoretical study cannot provide tighter bounds or informative
conclusions on sparsity in sparse recovery, unlike in the case
of sparse representation. Thus, sparsity was not a necessary
criterion unlike in formal CS methods. These analyses have
also provided us with some potential future directions to
explore on other aspects in SR. For example, since CS involved
theoretical analysis on sparse representation-based schemes,
new techniques for analysis on sparse recovery methods in
CS need to be investigated. Also, theoretical analysis on
fundamental issues such as the optimal set of measurements
required for sparse recovery for a given up-factor need to
be understood. Based on the study of this paper, such type
of analysis should consider the deterministic down-projection
model L. We note that there are other important aspects of
SR that should be considered in future study. These include:
1) impact of non-CS priors (e.g., feature space, directional
smoothness priors); 2) methods of training the dictionary
explicitly considering the properties of L; and 3) the impact
of the size of the dictionary on the solution space. These will
be among our future efforts along the same direction of this
paper.

References
[1] J. Sun, J. Sun, Z. B. Xu, and H. Y. Shum, “Image super-resolution using
gradient profile prior,” in Proc. CVPR, Jun. 2008, pp. 1–8.
[2] S. Y. Dai, M. Han, W. Xu, Y. Wu, and Y. H. Gong, “Soft edge
smoothness prior for alpha channel super resolution,” in Proc. CVPR,
Jun. 2007, pp. 1–8.
[3] H. A. Aly and E. Dubois, “Image up-sampling using total-variation
regularization with a new observation model,” IEEE Trans. Image
Process., vol. 14, no. 10, pp. 1647–1659, Oct. 2005.
[4] R. Schultz and R. Stevenson, “Extraction of high-resolution frames from
video sequences,” IEEE Trans. Image Process., vol. 5, no. 6, pp. 996–
1011, Jun. 1996.
[5] M. S. Lewicki and T. J. Sejnowski, “ Learning non-linear overcomplete
representations for efficient coding,” Adv. Neural Inform. Process. Syst.,
vol. 10, pp. 556–562, 1998.
[6] M. Irani and S. Peleg, “Motion analysis for image enhancement: Resolution, occlusion and transparency,” J. Vis. Commun. Image Representat.,
vol. 4, no. 4, pp. 324–335, Dec. 1993.
[7] J. Yang, J. Wright, T. S. Huang, and Y. Ma, “Image super-resolution via
sparse representation,” IEEE Trans. Image Process., vol. 19, no. 11, pp.
2861–2873, Nov. 2010.
[8] H. Rauhut, K. Schnass, and P. Vandergheynst, “Compressed sensing and
redundant dictionaries,” IEEE Trans. Inform. Theory, vol. 54, no. 5, pp.
2210–2219, May 2008.
[9] M. Elad, “Optimized projections for compressed sensing,” IEEE Trans.
Sig. Process., vol. 55, no. 12, pp. 5695–5702, Dec. 2007.
[10] M. Aharon, M. Elad, and A. Bruckstein, “K-SVD: An algorithm for
designing overcomplete dictionaries for sparse representation,” IEEE
Trans. Signal Process., vol. 54, no. 11, pp. 4311–4322, Nov. 2006.

KULKARNI et al.: UNDERSTANDING COMPRESSIVE SENSING AND SPARSE REPRESENTATION-BASED SR

[11] M. Aharon and M. Elad, “Image denoising via sparse and redundant
representations over learned dictionaries,” IEEE Trans. Image Process.,
vol. 15, no. 12, pp. 3736–3745, Dec. 2006.
[12] H. Chang, D.-Y. Yeung, and Y. Xiong, “Super-resolution through
neighbor embedding,” in Proc. CVPR, Jun.–Jul. 2004, pp.
275–282.
[13] R. C. Hardie, K. J. Barnard, and E. Armstrong, “Joint map registration
and high-resolution image estimation using a sequence of undersampled
images,” IEEE Trans. Image Process., vol. 6, no. 12, pp. 1621–1633,
Dec. 1997.
[14] R. A. DeVore, “Deterministic constructions of compressed sensing
matrices,” J. Complexity, vol. 23, nos. 4–6, pp. 918–925, 2007.
[15] E. Candès and J. Romberg, “Practical signal recovery from random
projections,” Proc. SPIE Conf. Wavelet Applicat. Signal Image Process.
XI, vol. 5914, pp. 76–86, 2004.
[16] E. Candès, J. Romberg, and T. Tao, “Robust uncertainty principles:
Exact signal reconstruction from highly incomplete frequency information,” IEEE Trans. Inform. Theory, vol. 52, no. 2, pp. 489–509,
Feb. 2006.
[17] L. D. Donoho and J. Tanner, “Counting faces of randomly-projected
polytopes when the projection radically lowers dimension,” J. Adv.
Manuf. Syst., vol. 22, no. 1, pp. 1–53, 2009.
[18] E. Candès, J. Romberg, and T. Tao, “Stable signal recovery from
incomplete and inaccurate measurements,” Commun. Pure Appl. Math,
vol. 59, no. 8, pp. 1207–1223, 2006.
[19] J.-J. Fuchs, “On sparse representations in arbitrary redundant bases,”
IEEE Trans. Inform. Theory, vol. 50, no. 6, pp. 1341–1344, Jun. 2004.
[20] G. Yu and S. Mallat, “Sparse super-resolution with space matching
pursuit,” in Proc. SPARS, 2009.
[21] H. Lee, A. Battle, R. Raina, and A. Y. Ng, “Efficient sparse coding
algorithms,” Adv. Neural Inform. Process. Syst., vol. 19, pp. 801–808,
2007.
[22] S. Baker and T. Kanade, “Limits on super-resolution and how to break
them,” IEEE Patt. Anal. Mach. Intell., vol. 24, no. 9, pp. 1167–1183,
Sep. 2002.
[23] J. Sun, N. N. Zheng, H. Tao, and H. Y. Shum, “Generic image
hallucination with primal sketch prior,” in Proc. CVPR, 2003, pp. 729–
736.
[24] Z. Lin, J. He, X. Tang, and C.-K. Tang, “Limits of learning-based
superresolution algorithms,” in Proc. ICCV, Oct. 2007, pp. 1–8.
[25] D. Glasner, S. Bagon, and M. Irani, “Super-resolution from a single
image,” in Proc. ICCV, 2009, pp. 349–356.
[26] Q. Shan, Z. Li, J. Jia, and C.-K. Tang, “Fast image/video upsampling,”
ACM Trans. Graphics, vol. 27, no. 5, pp. 1–7, 2008.
[27] J. D. van Ouwerkerk, “Image super-resolution survey,” Image Vision
Comput., vol. 24, no. 10, pp. 1039–1052, 2006.
[28] H. Shen, L. Zhang, B. Huang, and P. Li, “A MAP approach for joint
motion estimation, segmentation, and super resolution,” IEEE Trans.
Image Process., vol. 16, no. 2, pp. 479–490, Feb. 2007.
[29] X. Gao, Q. Wang, X. Li, D. Tao, and K. Zhang, “Zernike moment-based
image super resolution,” IEEE Trans. Image Processing, vol. 20, no. 10,
pp. 2738–2747, Oct. 2011.
[30] K. Zhang, X. Gao, X. Li, and D. Tao, “Partially supervised neighbor embedding for example-based image super-resolution,” IEEE
J. Sel. Topics Signal Process., vol. 5, no. 2, pp. 230–239, Apr.
2011.
[31] W. Dong, L. Zhang, G. Shi, and X. Wu, “Image deblurring and superresolution by adaptive sparse domain selection and adaptive regularization,” IEEE Trans. Image Process., vol. 20, no. 7, pp. 1838–1857, Jul.
2011.
[32] D. Capel, Image Mosaicing and Super-resolution. New York: SpringerVerlag, 2004.

789

Naveen Kulkarni received the B.E. degree from
Visvesvaraya Technological University, Karnataka,
India, in 2006, and the M.S. degree in electrical
engineering from Arizona State University (ASU),
Tempe, in 2011.
He is currently a Video Systems Engineer with
Research in Motion Corporation, Waterloo, ON,
Canada. From 2006 to 2009, he was a Video Engineer with PathPartner Technology Pvt. Ltd., Karnataka. From 2009 to 2011, he was a Research
Assistant with the Visual Representation and Processing Group, ASU. His current research interests include image/video
processing, and computer vision and pattern recognition.
Pradeep Nagesh (M’00) received the B.E. degree
from Visvesvaraya Technological University, Karnataka, India, in 2004, and the M.S. degree in
electrical engineering from Arizona State University
(ASU), Tempe, in 2009.
Since 2009, he has been a Senior Research Engineer with Digital Media Solutions Laboratory, Samsung Information Systems America, San Jose, CA.
From 2004 to 2007, he was a Software Engineer with
Delphi, Technical Center India, Karnataka. From
2007 to 2009, he was a Research Assistant with
ASU and an Intern with various corporate laboratories in the U.S. His current
research interests include image/video processing, and computer vision and
multimedia engineering.
Rahul Gowda received the B.E. degree from
Visvesvaraya Technological University, Karnataka,
India, in 2005, and the Masters degree in electrical
engineering from Arizona State University (ASU),
Tempe, in 2009.
He is currently a Video Software Engineer with
Nvidia Corporation, Santa Clara, CA. From 2005 to
2007, he was a Digital Signal Processing Engineer
with Ittiam Systems Pvt. Ltd., Karnataka. From 2007
to 2009, he was a Research Assistant with the Visual
Representation and Processing Group, ASU. His
current research interests include multimedia signal processing and computer
vision.
Baoxin Li (S’97–M’00–SM’04) received the Ph.D.
degree in electrical engineering from the University
of Maryland, College Park, in 2000.
He is currently an Associate Professor of computer science and engineering with Arizona State
University, Tempe. From 2000 to 2004, he was
a Senior Researcher with SHARP Laboratories of
America, Camas, WA, where he was the Technical
Lead in developing SHARP’s HiIMPACT Sports
technologies. From 2003 to 2004, he was also an
Adjunct Professor with the Portland State University,
Portland, OR. He holds eight issued U.S. patents. His current research interests
include computer vision and pattern recognition, image/video processing,
multimedia, medical image processing, and statistical methods in visual
computing.
Dr. Li won the SHARP Laboratories’ President Award twice, in 2001 and
2004. He also won the SHARP Laboratories’ Inventor of the Year Award
in 2002. He was a recipient of the National Science Foundation’s CAREER
Award from 2008 to 2009.

A Compressive Sensing Approach for Expression-Invariant Face Recognition
Pradeep Nagesh and Baoxin Li
Dept. of Computer Science & Engineering
Arizona State University, Tempe, AZ 85287, USA
{pnagesh, baoxin.li}@asu.edu

Abstract
We propose a novel technique based on compressive
sensing for expression-invariant face recognition. We view
the different images of the same subject as an ensemble of
intercorrelated signals and assume that changes due to
variation in expressions are sparse with respect to the
whole image. We exploit this sparsity using distributed
compressive sensing theory, which enables us to grossly
represent the training images of a given subject by only
two feature images: one that captures the holistic
(common) features of the face, and the other that captures
the different expressions in all training samples. We show
that a new test image of a subject can be fairly well
approximated using only the two feature images from the
same subject. Hence we can drastically reduce the storage
space and operational dimensionality by keeping only
these two feature images or their random measurements.
Based on this, we design an efficient expression-invariant
classifier. Furthermore, we show that substantially low
dimensional versions of the training features, such as (i)
ones extracted from critically-downsampled training
images, or (ii) low-dimensional random projection of
original feature images, still have sufficient information
for good classification. Extensive experiments with
publically-available databases show that, on average, our
approach performs better than the state-of-the-art despite
using only such super-compact feature representation.

1. Introduction
Face recognition (FR) has been a highly active research
area for many years. A typical approach involves two
tasks: feature extraction and classification. Commonlyused feature extraction methods include subspace
techniques such as principle component analysis (PCA or
eigenface), independent component analysis (ICA), linear
discriminant analysis (LDA or fisherface) and so on [1, 2].
With features extracted, classifiers based on techniques
such as nearest neighbor and support vector machines can
then be used to perform recognition. The above feature
extraction methods are well-understood and in a sense
have reached their maturity. Researchers are now looking
for different methods and theories to address the persisting

978-1-4244-3991-1/09/$25.00 ©2009 IEEE

1518

challenges in FR like expression, illumination and pose
variation, and dimensionality reduction, etc. Reducing the
space complexity and in particular the operational
dimensionality of the classifier is vital for practical
applications involving large databases.
The recently-emerged Compressive Sensing (CS)
theory [6,10,12-16], while originally intended to address
signal sensing and coding problems, has shown
tremendous potential for other problems like pattern
representation and recognition [3,4], often beating the
conventional techniques. In this paper we propose a new
technique for face feature extraction and classification,
based on the CS theory. We focus on addressing
expression variation in FR. Expression-invariant FR is a
challenging task owing to complex and varied nature of
facial expressions. Some sample face images are shown in
Fig. 1 to illustrate the complexity of the problem. Our
method relies on distributed CS and joint sparsity models
(JSM) [5, 10]. The JSM was originally proposed for
efficient coding of multiple inter-correlated signals. In our
work, we formulate the JSM from a “representation”
perspective so that it can be readily applied to computer
vision problems requiring compact representation of
multiple correlated images such as instances of the same
face in the context of FR, which is our focus of discussion
in this paper. Further, we design feature extraction and
classification algorithms based on the formulation. Unlike
existing FR work based on sparse representation (e.g.,
[3]), the proposed approach has a natural and close knit
with the CS theory and thus many potential benefits of CS
apply (e.g., projecting the input image into ultra-low
dimensions, as discussed in Section 4.2).
Specifically, we consider the training face images of a
single subject as an ensemble of inter-correlated signals
and propose a technique to represent each subject class
with two feature images: (i) one that captures holistic or
gross face features (the common component) and (ii) the
other that captures mostly the unique features (like
expressions) of all images in a single image (the gross
innovation component). Then, we design a CS based
reconstruction algorithm that can produce a close
approximation of a new face image of the subject, using
only the two training features. In particular, the algorithm

first produces an approximation of expressions in the new
face image using the gross innovation feature and then
uses this with the common component to reconstruct the
given face image. A face classifier is designed based on
the same principle, where the class of the test image is
decided based on how well it can be approximated using
the training features of labeled classes. Since we store only
two feature images per subject (or their low dimensional
measurements), we drastically reduce the training set
storage space and the operational dimensionality of the
classifier, compared with the sparse-representation-based
algorithm of [3], while being able to achieve better
performance than the state-of-art results reported therein.
Further, our method is more robust in scenarios where
only a few samples are available for training.
Section 2 reviews the background and related work.
Section 3 presents our method for feature extraction based
on JSM. A new classifier is designed and discussed in
Section 4, followed by experimental results in Section 5.
We conclude with discussion on future work in Section 6.

(a)

(b)

1

(c)
Figure 1: Sample face images with expressions from (a) [9], (b)
[8], and (c) [7].

2. Background and Related Work
In this section, we first briefly review basics of the CS
theory, and then discuss one most recent work on FR
based on sparse representation and CS.
According to the CS theory, if a signal    is Ksparse, with respect to a basis    (i.e in the
expansion  	 
 , there are only K<N non-zero or
significant coefficients), then  can be recovered by its
measurement    , M<N, obtained by projecting 
onto a second basis    , as long as (i)  and 
are incoherent and (ii) M is of the order Klog(N/K)
[6,10,12-16]. Mathematically, if we write the
measurement as y	 ,    , then the signal
recovery can be done by convex l1 optimization:
 	 arg min

or

 	 arg min


. .  	 
. .    

(1)
(2)

Eqn. (1) is the Basis Pursuit problem and Eqn. (2) is the
Basis Pursuit Denoising problem, which is well suited in
cases where the measurements are noisy. A popular

1519

approximation equivalent to (2) is the unconstrained
version given by
 	 arg min  ! 0.5 $    %

(3)
There are efficient algorithms that use interior-point
methods to solve the l1 minimization of (1) and (2). One of
the earlier implementations is l1-magic [18] which recasts
these problems as a second-order cone program and then
applies the primal log-barrier approach. More recent
interests are in sparse recovery algorithms solving the
unconstrained optimization of (3), since it is much faster
than directly solving (1) or (2). Gradient Projection for
Sparse Reconstruction (GPSR) [11] is one such more
recent algorithm, which is reported to outperform prior
approaches [17].
Recently, an FR algorithm (called SRC) based on
ideas of sparse representation and CS has been proposed
[3], which appears to be able to handle changing
expression and illumination. The work was enhanced by
another paper [4] to handle pose variation. In the SRC
algorithm, it is assumed that the whole set of training
samples form a dictionary (each image is a base atom),
and then the recognition problem is cast as one of
discriminatively finding a sparse representation of the test
image as a linear combination of training images by
solving the optimization problem in (1), (2) or (3). While
the SRC model demonstrates the power of harnessing
sparsity in face recognition problem via l1 minimization, it
has some disadvantages. First, for accurate recognition,
sufficiently large training images for each subject are
needed. But in practice, only a few instances might be
available for a few or even all of the subjects. Second, all
training images (or their low dimensional versions) have
to be stored and accessed during testing, and thus for a
large training set, both the space complexity and the speed
performance may pose as practical challenges.
Nevertheless, the comparison with other existing
approaches in [3] suggests that the SRC algorithm is
among the best and thus we treat it as the state-of-the-art
and will use it as a bench mark in our study in this paper.

3. Face Feature Extraction and Training
The problem of recognition of an unknown object is to
correctly identify the class to which it “belongs to”, using
some information derived from labeled training samples
belonging to K distinct classes. Here we refer to feature
extraction as training. In this section, we propose a feature
extraction algorithm based on the JSM CS recovery
scheme [5, 10]. Our algorithm finds the common (holistic)
and innovation components, with the latter corresponding
to expressions, of all training images of class k. Since we
use a sparsifying basis (like DCT), we term this as B-JSM
feature extraction.

(g)
(d)
(b)
(c)
(a)
(e)
(h)
(f)
Figure 2: B-JSM feature extraction with DCT basis. (a), (b) and (c) are images of the same subject with different expressions (mean
is added back); (d) The common component of (a), (b) and (c) or 23 with mean added. Images (e), (f) and (g) are the innovation
components of (a), (b) and (c) respectively (24 , 24 , 24E ); Image (h) is the sum of the innovation components (e), (f) and (g) (or 2 F ). It
serves as a global representation of the unique features of (a), (b) and (c) together. Note that the eye-brow and mouth regions are
blurred in common component 23 in (d), where as these are captured as expression information in 2 F shown in (h).

(e)
(f)
(f)
(d)
(g)
(a)
(b)
(c)
Figure 3: Illustration of common and innovation features using S-JSM. (a), (b) and (c) are the same images with added white patch
(innovations). (d) is the obtained common component, in which even the skin texture at the patches is nearly retained; (e), (f) and (g)
are the innovation components of (a), (b) and (c) respectively, each retaining an innovation as gray patches (white patch subtracted
with the intensity of skin at regions of patches). (h) is the sum of the innovation components. It serves as a global representation of
the innovations in all images. (For visual clarity, means are added back to in (a), (b), (c) and (d).)

3.1. B-JSM Feature Extraction
To present the idea, let us first assume a grayscale
image represented as 1-D column vector    ,
N=N1xN2. The extension of the presented idea to 2-D is
straightforward. Since the features of interest lie in the
textures but not the intensity of an image, we assume that
x has its mean intensity subtracted. We assume that there
are K distinct classes (i.e., subjects), with each class
having Jk training images, k =1,2,…,K. Let the images of
class k be represented as an ensemble &',( ), * 	 1, …,Jk, or
simply &',( ). Jointly, such an ensemble can be
represented as,



- 	 .', ', … .. ',/0 1  /0

(4)

Noting that all signals in ',( % for a given k are highly
intercorrelated, we may represent the j-th training image
of class k as the sum of a common component and an
innovation component as follows,
',( 	 23' ! 24',(

(5)

Further, let    be the matrix representation of
some orthonormal basis (e.g., DCT) that can sparsely
represent the training images, so that coefficients 5',( 	
   of signal  can be written as,
5',( 	 3' ! 4',( 	 23' ! 24',( ; 3' , 4',(   (6)

Here 3' is common to all the Jk training images of class k
and 4',( * 	 1, … Jk, is unique to each image. Under this
model, let the common and innovation components of
class k be jointly represented by the vector



7' 	 .3' 4', 4', … . 4',/0 1  8/0 9:

(7)

1520

Note that there might be more than one value of 23' or 3'
satisfying (5) or (6), but the one we are interested in is the
component 3' that is strictly derived from the common
support in the ensemble &5',( ) such that the vector Wk is
the sparsest representation of &',( ) (Eqn.(4)) under the
basis . For highly correlated signals, naturally 3' would
be strong and relatively denser compared to the very
sparse innovations. From a feature extraction point of
view, for FR with varying expression, this representation
is useful since the common component 23' would retain all
the gross common face features (holistic), while the
innovation components 24',( retains the unique features
owing to changes in facial expressions. An example of
such a representation is shown in Figure 2 and 3 and will
be discussed in more detail later in this subsection.
In the distributed CS theory of [5, 10], the additive
model of (5) was assumed in the sense of “jointly
recovering” correlated signals from measurements, which
would help reduce the number of measurements in coding
of multi-sensor signals. In our case, essentially we are
interested in forming a new representation of ',( % given
in (7) so as to use the common and innovation features for
facilitating the FR task. From (4)-(7), we may write,
< 7'
' 	 ;
(8)
< = >?@ A ?@ AB is formed by concatenating two
where ;
matrices given by @ 	 ?
 
 … 
 A
  8/0 : and
@ 	 diag8@ :  8/0 :8/0 : , with diag8D: being a
diagonal matrix whose diagonal elements are
D , D … D in D 	 ?D D … D A
 . Note that @ and
@ correspond to the common and innovation components
respectively. The 7' vector can be found by solving the
following l1-minimization problem,

or

< 7'
7' 	 arg min 7' 
. . ' 	 ;

<
7' 	 min  7'  ! 0.5 $ G'  ; 7' G % (9)

The spatial domain common and innovation components
can be recovered by the inverse transformation as,
H' 	 I7'
(10)
where I 	 diag8?
 
 … 
 A
 :  8/0 :8/0: and


H' 	 .23' 24', … . 24',/0 1  8/0 9: . For convenience
and future reference, we represent the process described
by the sequence of equations (8)-(10) for class k as
B-JSM O 8',( %, * 	 1, … P' : Q .23' … . 24',/0 1




(11)

The last step in feature extraction is to form the gross
innovation component denoted by 2'F , (the superscript A
standing for “all”) that can be computed as,
/

0
2'F 	 ∑(S
24',(

(12)

For each class k, we store only two feature images: the
common component 23' and the gross innovation
component 2'F and discard the training and other
innovation images. Hence there is a significant reduction
in the total storage space compared with the SRC method
of [3]. Further dimensionality reduction of feature space
can be achieved by storing just sufficient random
measurements of 23' and 2'F instead of the whole feature
images (see Section 4.2 for more on this). Since the
innovations (changes in expressions) are sparse (and
mostly with different support), the gross innovation
component 2'F captures most of the unique features of all
images in one single image of the same size. It is worth
mentioning that there may be some loss of innovation
information in the representation of (12), especially if 3
is very sparse with a small support while the 4, ’s are
relatively dense with significant overlap in their support.
However, for aligned face images of the same subject, we
can expect 3' to be dense with a significant support
compared to the innovations. We will show with examples
that the representation of (12) indeed has sufficient
information about the innovations (or expressions) of all
training image for the purpose of face recognition.
Refer to Fig. 2, where we have three images of a subject
with different expressions. (For visual clarity, we have
added back the mean of individual training images and
also the overall mean to the common component.) It can
be seen that the common component retains all the gross
features like the face structure, nose region etc. The
innovation components retain unique features in respective
images (for example, the raised eye-brows and open
mouth of the second image in (b) are clearly captured in
24 of (f) and so on). It is to be noted that 4, TU are sparse
and corresponding spatial domain version 24, TU in the figure
are not sparse, but have some negative pixel values, due to

1521

which they appear visually dark. The gross innovation
image 2F captures most of the innovation features of all
three images in (a), (b) and (c). We will show later that,
given only these two features, sufficient innovation
information (or expressions) of any image can be
recovered and a good estimation can be done using (5).

3.2. S-JSM: A Special Case of B-JSM
A special case of the B-JSM feature extraction method
described above is when the common and innovations are
directly extracted from spatial image supports (we may
call it S-JSM, with S standing for spatial). However, such
an approach is sensitive to image alignment, while B-JSM
is more robust if a basis like DCT or Wavelet is used.
Nevertheless, we present here this alternative so as to
provide better insights about the common and innovation
features. For S-JSM, we assume that the basis matrix  in
Equations (1)-(10) is an identity matrix of size N. With
these changes, 3' 	 23' and 4' 	 24' in (6) and the
algorithm is expressed as



S-JSM V	 W&',( ), * 	 1, … P' X Q .23' … . 24',/0 1 (13)
Fig. 3 shows an example of S-JSM features where white
patches were intentionally added to the same image to
simulate “innovations”. (Again, for visual clarity we add
back the mean intensity.) We note that the common
component retains almost all the information of the face
(even the skin intensity at locations of the patches are
closely recovered). The innovation component of an image
retains the gray patch (which is the difference of the actual
patch and the skin intensity at those pixel locations).
Hence these effectively carry the information of the
original white patches, given the common component. Fig.
3(f) shows that the gross innovation retains all the three
gray patches which are unique features of all images. This
intuitively illustrates our argument earlier about why the
gross innovation is sufficient as long as the individual
innovations are sparse (with the hope that the overlap of
the innovations should have been captured by the common
component).

4. Face Classification
4.1. Expression Recovery and B-JSM Classifier
With the given training features (the common and gross
innovation images), there can be many different ways to
design a classifier. Let Y   be a test image of unknown
class. One simple way is to assume that Y is highly
correlated with the correct training class (say class k), and
hence it would have the same common component 23' if
we consider the ensemble &',( , Z), * 	 1,2, ..Jk+1. So the
test image Y can be expressed as
Y 	 23' ! Y4'
(14)

(a)

(b)

(c)

(d)

(g)
(f)
(h)
(e)
(i)
Figure 4: Feature extraction, expression recovery and classification illustrated. (a) Five training images of class 1 (from CMU AMP
EXpression database [7] with 13 subjects); (b) & (c) One sample training image of class 2 & 3 respectively; (d) The feature images
computed from (a); (e) A sample test image; (f) The estimate of (e) with recovered expressions using class 1 training features of (d);
(g) & (h) Estimates with class 2 and class 3 training features respectively; (i) Residuals computed from Eqn. (17), which are used to
determine the correct class. (Note: the mean is added back to all images except the gross innovation for better visual clarity).
.

(b)

(a)

(e)

(c)
(d)
Figure 5: Illustration of image recovery and classification under drastic expressions (13 subjects, five training samples each). (a)
Training images of class 2; (b) Training features of (a); (c), (d) and (e) The image on the left is an actual test image of class 2, on the
right is the reconstructed image using class 2 features (in(b)), and at the bottom is the residual of Eqn. (17) for all thirteen classes.
.

where Y4' is the innovation of Y. In reality, we need to
determine the correct class label k, which may be found as
the k for which the energy (or l2 norm) for Y4' is minimum.
Another approach would be to simply consider sparsity or
number of non-zero components of the expansion of Y4' in
basis . However, these methods ignore the information
from the gross innovation component 2'F . A better
approach would be to first ask the question – “If any at all,
what unique feature present in the test innovation Y4' is
also present in 2'F "? In other words, we want to find the
estimate of the innovation component Y4' of (14) (or
expressions) in the test image Y using the training features.
Assuming B-JSM feature extraction, a good way to
estimate Y4' is to extract a common component \' 	 ]' ,
from the support set common between ^4' (^4' 	 Y4' ) and
_'F ( _'F 	 2'F ). This can be achieved using the B-JSM
recovery model in (11) as follows,


B-JSM8Y4' , 2'F % : Q .]' , ]4' , 24F
' 1
4
F
]4' and 24F
' are innovations of Y' and 2' .

(15)

where
We may
form the estimate of the test image for class k features as,
Ỳ' 	 23' ! ]'
(16)
The correct class label can then be determined as,

1522

a 	 argmin- bỲ'  Y c

(17)

Fig. 4 illustrates the results of expression recovery and
the classification algorithm explained above for images
from the CMU AMP EXpression database [7] (thirteen
subjects with five training images chosen per subject). Fig.
4(a) shows all five training images of one subject labeled
as class 1 with different expressions. Note that in this case
the training common shown in (d) is visually closer to the
training images compared to the case in Fig. 2. It is
difficult to visually interpret the gross innovation image
(Fig. 4(d), right) since it contains a lot information.
Nevertheless with the algorithm described above, the
innovation information or expressions of a new test image
of the correct class (e) can be well recovered, as in (f). On
the other hand, for images of the wrong classes (e.g., (b)
and (c)), the reconstruction is poor (as in (g) and (h)).
A more challenging case is shown in Fig. 5, illustrating
the algorithm performance under drastic variation in
expression. Despite the challenge, the expression is fairly
well recovered and the classifier residual is very small for
the correct class compared to the other classes, leading to
correct classification. Note that, in (e), the test image has a
totally different expression that is not present in any of the
training images. However, the classifier still yields the

correct result. This can be attributed to the dominance of
the “common component” over the innovations in terms of
information for discriminative classification. However, if
full or part of expression information is recovered, the
discrimination would be more pronounced (compare the
residuals of all three test images in Fig. 5). Hence, the BJSM classifier is robust even in cases where the expression
information is missing in the training set. One such
practical case is when only a few training images (per
subject) are available.

Fig. 6 illustrates the performance of the above algorithm
in critically low dimensional feature space for the same
setting as in Fig. 5. The original test image is of size
32x32, which is then down-sampled to 16x16. It is
obvious that downsampling does change the residuals
much. A e operator is applied such that only 10% of
linear measurements are retained (102 features for the
32x32 case and merely 25 features for the 16x16 one).
Again, the residuals do not alter much. Thus in all cases,

4.2. Low-Dimensional Feature Subspace
We have presented our CS-based algorithm for feature
extraction and classification, but have not explicitly
considered the underdetermined or ill-poised case
involving reduced measurement as in conventional CS
coding problems [5, 6, 10, 14-16]. With sparsity prior,
(under mild conditions as suggested in CS theory [6, 15,
16]), significant dimensionality reduction in the feature
space can be handled by the B-JSM algorithm. This can be
explained considering (5), (14) and (15). As discussed in
Section 3, the Jk innovations 4' of (5) (for class d) are very
sparse with respect to the whole image. Suppose that the
test image Y belongs to class d, then we may assume that it
is sufficiently correlated with the training images (i.e., the
training common 23' is significant inY), which means that
Y4' in (14) is also very sparse with its sparsity of the order
comparable to any training innovations 24',( . Essentially, in
the B-JSM expression recovery of (15), we estimate a
highly sparse signal and hence the estimate of Y via (16)
can be done in a lower-dimensional feature space than the
original (23' and 2'F ). Furthermore, since our emphasis is
classification alone and not the fidelity of reconstruction,
there is more scope for descending down to extreme lowdimensions.
Let the dimensionality reduction system be e  
(e can be random or any matrix highly incoherent
with ), a low-dimensional projection of the test image is,
Yf 	 eY  
(18)
And the low dimensional versions of the training features
are 2f3' and 2f4F
' given by,
4F

2f3' 	 e23' , 2f4F
' 	 e 2' ,  

(19)

These can be stored right after the training process of
Section 3.1. Then the B-JSM algorithm of (15) is
computed using the modified version of (9) as below,


< 7' G i (20)
h'  e;
7' 	 min g 7'  ! 0.5 $ G





h' 	 . 8Yf  2f3' : 2f4F
where 
and 7' 	
' 1  
j
jk l
?\d , \d , _d A (the transform coefficients of the right hand
side of (15). The estimate of the test image can then be
determined by (16) as before.

1523

e

Using 32x32
= 1024 points

e

Using 16x16
= 256 points

Using 10% measurement
Using 10% measurement
= 102 feature points
= 25 feature points
(b)
(a)
Figure 6: Recognition with critically low-dimensional features.
In (a) and (b), the top-left is the input image, and the bottom is
its 10% measurement, and on the right are the residuals.

correct classification is achieved (more results from entire
databases are to be presented and discussed in Section 5).

5. Experimental Results and Evaluation
5.1. Experimental Setup
The proposed algorithms described in Sections 3 and 4
were implemented for working with 2-D images instead of
vectored images for speed consideration. Further, in all
cases unless specified otherwise, the GPSR algorithm [11]
was used to solve the unconstrained version if l1
minimization in (9). (We obtained similar results with
other algorithms like TV minimization [12, 16]). For
GPSR, we set
=0.009 and used the continuation
< 
 ' p/
approach [11] with first factor m 	 0.88maxWp
X:. We assume DCT as the sparsifying basis  in our
algorithm. Although the sparsifying operation (5 	 ) is
not exactly equivalent to 1-D DCT on vectored image or
2-D DCT on 2-D image (but is actually 1-D DCT on
columns of a 2-D image ), it yields satisfactory results.
We used three face expression databases: (1) CMU
AMP Face EXpression Database [7] (henceforth referred
to as CMU), (2) Japanese Female Expression database [8]
(henceforth JAFFE), and (3) Cohn-Kanade face
expression database [9] (henceforth CK). The CMU
database contains 975 images (13 subjects with 75 images
per subject) with different facial expressions. The JAFFE
database has 213 images with seven different expressions
(10 female subjects). The CK database is the most
challenging of the three, with 97 subjects and a total of

8795 images. Images of each subject were obtained in 5 to
9 sessions, each session having multiple instances of
similar expressions. Since this is a large database, we
created three sub-databases of 2317 images by sampling
the original frames (uniformly for the first two, randomly
for the third) for all subjects. The images were normalized.
Our results are compared with the most recent sparse
representation based face classification algorithm (SRC)
[3], which reported results superior to other methods.

5.2. Experiments, Results, and Comparison
In all the experiments, the training set was formed by
randomly selecting J images per subject, leaving the rest
for testing. The experiments are summarized below.
(i) Validation: We performed validation for various
values of J with multiple repetitions for each J: J = 4 to 10
for CMU, with 10 trials each; 2 to 5 for JAFFE, with 40
trials each; and 5 to 10 for CK with 9 trials, 3 from each
sub-database. The statistics of the recognition rates (High,
Low and Average) are given in Tables 1, 2, and 3 with
comparison with the SRC algorithm. For CMU and
JAFFE, we used 32x32 image size. The results with these
two databases show that at lower number of training
images, our algorithm invariably outperforms the SRC
algorithm and shows better stability. As the number of
training images increase, the performance for both
methods are on par for most trials, but the averages still
indicate that our method is better. For the CK database, we
considered a critically low-dimensional image size of 7x7
(49 features). Invariably all times, our method outperforms
the SRC algorithm in mean, low and high accuracies.
Further, unlike the SRC algorithm, our method exhibits a
clear trend of increase in accuracy with increased J.
(ii) Recognition in Low-Dimensional Feature Space: To
demonstrate the performance of our algorithm in critically
low-dimensional feature space, we apply linear random
measurement on 32x32 database images (1024 features),
retaining only 40% to 10% values (feature space of 409
to102 points) and evaluate the recognition results. We then
downsample the original 32x32 images to 16x16 (256
features) and repeat the process for measurements from
60% to 10%. The effective feature dimensions vary from
153 to as low as just 25 points. Operating in such a low
dimensional space is certainly challenging for any
database, especially for a large database like CK. Table 4
tabulates the results; where the recognition rate is the
average for 3 trials, with J=5, 4, and 11 for CMU, JAFFE
and CK databases respectively. For this simulation, we
used the TV minimization [12]. Clearly, even with 25
feature points, the recognition rate is as high as 94.35%,
97.69% and 97.818% for the three databases respectively.
(iii) Robustness of recognition w.r.t. expressions: We
further designed two types of tests, one where similar

1524

expressions are present in both the training and the test
sets, and the other where there is no common expression
for the training and the test images. We experimented with
three expressions (surprise, happiness and neutral) for
each database and the results (averaging over 3 trials) are
shown in Fig. 7. In all the cases, the performance is still
very good: the worst case is only a loss of around 0.23%,
0.4% and 0.79% for CMU, JAFFE and CK databases
respectively for the “surprise” expression. For the neutral
expression, there is virtually no loss in accuracy (except
for JAFFE where the loss is merely 0.05%).
Table 1: Recognition rate (%) for 10 trials on the CMU
database with 32x32 image size.
Jk
4
5
6
7
8
9
10

Proposed algorithm
High
Low
Avg
100
97.48
98.95
100
99.67
99.91
100
99.69
99.97
100
100
100
100
100
100
100
100
100
100
100
100

SRC
Low
97.68
99.12
98.76
98.30
99.31
100
98.73

High
100
100
100
100
100
100
100

Table 2: Recognition rate (%) for
database with 32x32 image size.
Proposed algorithm
Jk
High
Low
Avg
2
95.89
81.18
89.94
3
98.13
88.13
93.22
4
98.67
90.67
95.12
5
100
93.57
96.12

Avg
98.9
99.8
99.75
99.74
99.87
100
99.49

40 trials on the JAFFE

High
95.11
98.13
98.24
100

SRC
Low
82.1
87.0
90.2
89

Avg
90.1
92.1
95.13
96.01

Table 3: Recognition rate (%) for 5 trials on the CK database
with mere 7x7 image size.
Proposed algorithm
SRC
Jk
High
Low
Avg
High
Low
Avg
5
96.2
94.01
95.47
89.3
93.4
91.41
6
97.43 94.63
95.93
94.04
91.3
93.77
7
97.35 95.21
96.15
91.89
94.9
93.29
8
97.9
95.23
96.49
94.43
81.0
89.78
9
98.01 95.28
96.90
97.73
95.4
96.29
10
98.63 95.69
97.14
98.1
94.1
95.64
Table 4: Recognition rate (%) for databases with lowdimensional features. “%M” gives the percentage of
measurements taken and “ED” refers to “effective dimension”.
Image Size
%M
ED
CMU
JAFFE
CK
10
102
99.23
97.69
98.425
32x32
20
204
99.45
98.46
98.69
=1024
30
307
99.67
98.69
98.91
pixels
40
409
99.78
98.69
99.01

16x16 =
256 pixels

10
20
30
40
50
60

25
51
76
102
128
153

94.35
99.45
99.67
99.67
99.78
99.78

97.69
98.22
98.46
98.46
98.69
99.69

97.818
98.303
98.546
98.546
98.939
98.939

[2] W. Zhao , R. Chellppa , P. J. Phillips , A. Rosenfeld “Face
Recognition: A Literature Survey” ACM Computing Surveys,
Vol. 35, No. 4, December 2003, pp. 399–458.
[3] J. Wright, A. Yang, A. Ganesh, S. Sastry, and Y. Ma.
“Robust face recognition via sparse representation”. IEEE Trans.
PAMI [DOI 10.1109/TPAMI.2008.79].
[4] J. Huang, X. Huang, D. Metaxas, “Simultaneous Image
Transformation and Sparse Representation Recovery” IEEE
Conf. on CVPR, Anchorage,AK, June 2008.
[5] D. Baron, M. Duarte, S. Sarvotham, M. B. Wakin, and R. G.
Baraniuk, “Distributed compressed sensing,” Tech. Rep.
TREE0612, Rice University, Online at: http://dsp.rice.edu/cs/.
[6] E J. Candès and M B. Wakin “An Introduction to
Compressive Sampling”, IEEE Signal Proc. Magazine, Vol. 25,
Issue 2, March 2008, pp. 21-31.
[7] X. Liu, T.Chen and B.V.K. Vijaya Kumar, “Face
Authentication for Multiple Subjects Using Eigenflow” Pattern
Recognition, Volume 36, Issue 2, February 2003, pp. 313-328.
[8] M. J. Lyons, S. Akamatsu, M. Kamachi, J. Gyoba "Coding
Facial Expressions with Gabor Wavelets" IEEE Int Conf. on
Auto. Face and Gesture Recognition, Nara, Japan, April,1998.
[9] Kanade, T., Cohn, J.F., & Tian, Y. “Comprehensive database
for facial expression analysis” IEEE Int. Conf. on Automatic
Face and Gesture Recognition,Grenoble, France, 2000.

Figure 7: The recognition rate with and without the presence of
similar expressions in the training set - (Surprise (Exp1),
Happiness (Exp2) and Neutral (Exp3)). For the CMU (top), CK
(middle), and JAFEE (bottom) databases.

6. Conclusion

[10] M. F. Duarte, S. Sarvotham, D. Baron, M. B. Wakin and R.
G. Baraniuk, “Distributed Compressed Sensing of Jointly Sparse
Signals”, 39th Asilomar Conference on Signals, Systems and
Computer (IEEE Cat. No.05CH37761), 2005, pp. 1537-41.
[11] M. Figueiredo, R. Nowak, and S. Wright. “Gradient
projection for sparse reconstruction: application to compressed
sensing and other inverse problems”. IEEE Journal on Selected
Topics in Signal Processing, 2007, Vol 1, Issue 4, pp. 586-597.
[12] E. Candès and J. Romberg, “Practical signal recovery from
random projections”. Wavelet Applications in Signal and Image
Processing XI, Proc. SPIE Conf. 5914.

and Future Work

We proposed a novel technique based on compressive
sensing for expression-invariant face recognition. The
approach exploits the correlation of images from the same
subject through joint sparsity models in designing novel
algorithms for feature extraction and face recognition.
Thorough analysis of the proposed algorithms and their
performance evaluation, with comparison to the state-ofthe-art, were performed to demonstrate the claimed
advantages. We are currently working towards the
following extension of the proposed method: handling
illumination changes and pose variations. In addition, the
approach is general in nature and thus can be applied to
other problems involving multiple views of the scene,
which is another direction we are pursuing.

[13] E. Candès, J. Romberg, and T. Tao. “Robust uncertainty
principles: Exact signal reconstruction from highly incomplete
frequency information”. IEEE Trans. Inf. Theory, 52:489–509,
2006.

[14] E. Candès and T. Tao. “Near-optimal signal recovery from
randomprojections: Universal encoding strategies?” IEEE Trans.
on Information Theory, 52(12):5406–5425, 2006.
[15] D. L. Donoho, “Compressed sensing,” IEEE Trans. Inform.
Theory, vol. 52,July 2006, pp. 1289–1306.
[16] E. Cand`es, J. Romberg, and T. Tao, “Stable signal recovery
from incomplete and inaccurate measurements,” Comm. on Pure
and Applied Math, vol. 59, no. 8, 2006, pp. 1207–1223.

References

[17] S. Kim, K. Koh, M. Lustig, S. Boyd, and D. Gorinvesky. “A
method for large-scale ℓ1-regularized least squares problems
with applications in signal processing and statistics,” IEEE J.
Selected Topics in Signal Processing, 1(4):606-617, Dec 2007.

[1] P. Belhumeur, J. Hespanha, D. Kriegman, “Eigenfaces vs.
Fisherfaces: recognition using class specific linear projection”,
in: European Conference on Computer Vision, 1996, pp. 45--58.

[18] E. Cand`es and J. Romberg, “11-magic: Recovery of Sparse
Signals via Convex Programming” User Guide, l1-magic
software, Available at http://www.acm.caltech.edu/l1magic/.

1525

RELATIVE LEARNING FROM WEB IMAGES FOR CONTENT-ADAPTIVE ENHANCEMENT
Parag Shridhar Chandakkar, Qiongjie Tian and Baoxin Li
School of Computing, Informatics and Decision Systems Engineering, Arizona State University

arXiv:1704.01250v1 [cs.CV] 5 Apr 2017

{pchandak,qtian5,baoxin.li}@asu.edu

ABSTRACT
Personalized and content-adaptive image enhancement can
find many applications in the age of social media and mobile computing. This paper presents a relative-learning-based
approach, which, unlike previous methods, does not require
matching original and enhanced images for training. This
allows the use of massive online photo collections to train
a ranking model for improved enhancement. We first propose a multi-level ranking model, which is learned from only
relatively-labeled inputs that are automatically crawled. Then
we design a novel parameter sampling scheme under this
model to generate the desired enhancement parameters for a
new image. For evaluation, we first verify the effectiveness
and the generalization abilities of our approach, using images
that have been enhanced/labeled by experts. Then we carry
out subjective tests, which show that users prefer images enhanced by our approach over other existing methods.
Index Terms— Content-adaptive image enhancement,
learning-to-rank, subjective evaluation testing.
1. INTRODUCTION
In today’s age of social media, it is becoming more important to capture good-looking photos. Due to the outreach of
social media sites, the photos get spread around quickly. It
is common to retouch the photo after capturing to improve
its appearance. The photo-retouching tools have made significant progress in recent years. There exist sophisticated
tools such as Adobe Photoshop as well as one-touch enhancement tools such as Picasa, Windows Live Gallery and Apple’s
auto-enhance. However, one-touch enhancement tools neither
offer personalization nor content-based image enhancement.
For example, an indoor image may need a different style of
enhancement than an outdoor image. Adobe Photoshop offers large variety of enhancement operations but can be complex and time-consuming for an amateur photographer. This
underlines the need for better and automated image enhancement tools. Enhancement operations are performed on various aspects of an image, e.g. saturation, contrast, brightness,
sharpness, etc. Hence the space of possible combinations
of enhancement parameters is huge. This work focuses on
content-based image enhancement by using content-similar

high-quality images as reference.
Training-based methods have recently been explored for
image enhancement, where pairs consisting of a low-quality
image and its enhanced counterpart are used for training
[1, 2, 3, 4]. The enhancements are done by expert users. Such
a training set allows learning of a regression/ranking function which maps the input feature to the optimal enhancement
parameters. For a regression function, it learns a mapping
between the input parameters (could be pixel values) to the
parameters in the corresponding enhanced image. The ranking function assigns a score to each feature vector. The enhancement parameter which gets the highest score is selected
as the best enhanced version of the input image. However,
such schemes do not scale well, owing to the need of expertenhanced training images. Per our knowledge, the largest
such publicly available training database is MIT-Adobe 5K
[5], consisting of 5 enhanced versions per image and 5000
images. In reality, we have millions of high-quality images
available on the Web which, if properly utilized, can improve
the performance significantly. Further, since the previous approaches need low-quality and its enhanced counterparts, it is
difficult to customize the system according to the individual’s
preferences. Our approach can handle a non-corresponding
pair of a low-quality image and a high-quality reference image. We can possibly retrieve popular images from a user’s
Flickr/Instagram account to customize the enhancement preferences. To the best of our knowledge, only our approach
considers both of the above aspects simultaneously.
It is a challenge to find optimal enhancement parameters
with non-corresponding pair of input and output images. The
visual features are not corresponding to each other to build a
regression or a simple ranking function. Usually, the optimal
parameters of low-quality image are explored near the parameter space of its enhanced counterpart. In this case, the search
for possible space of enhancement parameters is extremely
difficult due to non-correspondence of input and output. To
remedy this, we define a novel parameter sampling scheme
and a multi-level ranking model which uses simple visual features along with derived image parameter features (such as
brightness, contrast and saturation). We build a multi-level
ranking relation from the partial ordering available between
the visual feature and parameter vectors of low-quality input and high-quality reference images. A learning-to-rank

c

2015
IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to
servers or lists, or reuse of any copyrighted component of this work in other works. DOI: 10.1109/ICME.2015.7177502

Feature
extraction

Low-quality
photo database

Relative ranking model
learning phase.
Enhancing a new image
using the learned model

Top K retrieved
images

High-quality
photo database

A new lowquality photo

Retrieval
module

Top K retrieved
images

Relative ranking
model learning

Learned model

Ranking score
generation

Output the version
having highest score

Generate enhanced
versions

Parameter
sampling

Fig. 1: Overview of our image enhancement algorithm.

approach has already been proposed in [1]. Unlike us, they
need the corresponding pairs of input and output images generated by an expert user along with a record of the intermediate enhancement steps. This limits the possible applications
of their approach as discussed before. We show superiority of
our method over one-touch enhancement tools and the stateof-art ranking-based enhancement approach [1].
2. RELATED WORKS
Purely image-processing-based and learning-based approaches have been developed for enhancing an image. In
this section, we mainly focus on learning-based approaches.
Common faces across the images have been used in [6] for
personal photo enhancement. They built a system to detect
examples of good and bad faces. Then the good faces were
used for enhancing the bad ones. However, their approach
lacks generality. A novel tone-operator was proposed in [7] to
solve the tone reproduction problem. In [2], a preliminary solution was proposed to enhance an image according to a user’s
profile. The result shows that the users’ preference can be
classified into different groups and also improved enhanced
results can be obtained by enhancing images according to
users’ choices. Local image enhancement was performed in
[3] by using local scene descriptors along with context. For
different scenes in the input image, similar image pairs are
retrieved. Then for each pixel in the input image, a set of pixels were retrieved from the data-set and used to improve the
given pixel. Then Gaussian random fields are used to maintain spatial smoothness in the enhanced image and improve
the perceptual quality of the image. MIT and Adobe collaborated to generated a large reference data-set which has 5000
input images and each has five enhanced versions, created by
five experts [5]. Using this database, they apply supervised
learning to predict a user’s adjustment and the preference for
a new user. They also analyze the difference in users’ preferences. In [4], the user preference was modeled based on the
image database. Users have to enhance some images to effectively train the model. The learned model was then applied to
obtain multiple enhanced versions for an image, according to
the user’s preferences. Image enhancement based on content

and scene semantics was done in [8]. Regions containing different objects were first detected. Then customized enhancement operators were applied in these regions. They concentrated more on personal photo collections and their content
detectors were limited to some objects such as people, sky
etc. The intermediate enhancement steps carried out by an
expert were recorded and used to train a ranking model in
[1]. They generated multiple enhanced versions by sampling
in the enhancement parameter space. The enhanced version
which obtained the highest ranking score was selected as the
final output. However, they require a lot of work from the
expert which is undesirable due to aforementioned reasons.
3. PROPOSED APPROACH
Our approach enhances an image depending on its content
and color composition. Fig. 1 shows the flow of our algorithm. We first perform CBIR to retrieve high-quality images similar to the low-quality input image. We then create
a ranking between the input image and the retrieved highquality images using extracted image features. To overcome
the shortcoming of not having one-to-one correspondence between the query and the retrieved high-quality images, we introduce an additional level in the ranking problem (detailed
later) to create a three-level ranking problem. For a new lowquality image, we generate multiple enhanced versions of the
input image by using a novel parammeter sampling scheme.
We select the version which gets highest rank from the model.
The individual steps of our approach are detailed below.
3.1. Data Collection and Processing
A good collection of high-quality images is essential for our
approach. We choose high-quality images from the database
published in [9]. It consists of photos from DpChallenge.com
and Photo.net among others. We select top 10% photos which
have more than 10 ratings. We then remove grayscale photos. DpChallenge.com has photos based on 66 themes ranging from “animals”, “food” to “portrait”, “water” etc. Our
database contains 1, 822 and 9, 467 photos from Photo.net
and DpChallenge.com respectively. We use bottom 10% rated
photos on DpChallenge.com as low-quality photos. We also
select photos from Flickr taken by old camera-phones such as
iPhone 3G, Samsung Galaxy II for low-quality photos.
We use an open-source CBIR engine called LIRE [10] to
retrieve K (=100) high-quality images for each low-quality
image. In the CBIR framework, we use opponent histogram
and auto color-correlogram features for color, PHOG descriptor and JPEG coefficient histogram to represent shape and image quality respectively. Before retrieval, we introduce a simple but effective step of auto-enhancement. It increases the
contrast of an image by saturating 1% of its data at low and
high intensities. Low-quality images usually have less contrast. Some of those images look either hazy or dark. There-

fore, the retrieved images do not correspond to the true colors in the low-quality image. This is avoided by first autoenhancing each low-quality image before retrieval.
3.2. Relative Ranking Model Learning
We formulate the problem of enhancing a low-quality image using other retrieved high-quality images as a multi-level
ranking problem. We now explain the general formulation of
our ranking approach. It is followed by the proposed multilevel ranking and the formation of feature vectors.
Suppose Q and D are the sets of query images and all
high-quality images respectively. The retrieved images of the
given image, Ii ∈ Q, are denoted as Ri ⊆ D. We assume
that images in Ri are of better quality than Ii . We construct
relative ranking pairs, say (Ii , Ri,j ) where Ri,j is the j th image in Ri . These pairs are now used for building a simple
two-level ranking model. We use visual features along with
the parameter feature vectors in our ranking model, while [1]
uses only visual features. The parameters help us to capture
hidden characteristics of an image. For example, the contrast
of an image depends on the spatial arrangement of the various colors. Saturation represents the purity of the color in
a different dimension. The combination of features and parameters helps compare seemingly unrelated input and output
images in a similar feature space. Let us denote the visual features extracted for Ii as Vi and the feature vector for the k th
parameter as Pik . Then the ranking model is given as follows:
min

1 2 X
ω +
ξi,j
2
i,j

s.t.

ω T fi ≤ ω T gi,j − ξi,j

∀ i, j

(1)

ξi,j ≥ 0
[Vi , Pi1 , . . . , Pik ]

where fi =
and gi,j = [VRi,j , PR1 i,j , . . . ,
k
PRi,j ].
The above ranking model concatenates all the feature and
parameter vectors to get a score for an image. However, assume a situation where we have an image with a slightly
higher level of saturation and brightness but with less contrast. Since the above ranking model considers all the elements together, the image may obtain a high score. Thus we
propose a modified ranking model in which we concatenate
visual feature and one parameter at a time.
1 2 X
min
ω +
ξi,j
2
i,j
s.t.

n
n
ω T fin ≤ ω T gi,j
− ξi,j
n
ξi,j

∀ i, j, n and n = {1, . . . , k}

≥0

(2)
n
where fin = [VIi , Pin ] and gi,j
= [VRi,j , PRni,j ].
In Equation 2, we would get as many constraints as the
cardinality of n. In other words, it will be equal to the total number of parameters. The final score depends on the

combination of visual feature and each parameter. An image gets a high score only if all of its parameters are in balanced amounts. Moreover, the visual feature stays common
in all of the inequality constraints since the parameters are
dependent and changing one of them affects all of them. A
common visual feature ensures that only a balanced featureparameter combination defines a high-quality image. All parameter feature vectors, [Pi1 , . . . , Pik ], are required to have the
same length in order to construct such a model.
3.2.1. Multi-level ranking
We enhance the low-quality images using the high-quality
ones stored on photo-sharing websites. Unlike in [1], we
neither possess corresponding pairs of original and enhanced
images nor the record of intermediate steps carried out by an
expert during an enhancement process. The external highquality images have high contrast, brightness and saturation.
Therefore, the ranking model generated using Equation 1 or 2
has only learned that high values can generate a high-quality
photograph. The model lacks the knowledge that extremely
less or high values of parameters (e.g. brightness, saturation
and contrast) can degrade the image quality significantly.
To incorporate this knowledge, we propose multi-level
ranking. We manually vary brightness, contrast and saturation for 20 images in our database (one parameter at a time)
till their quality degrades significantly. The variation happens
on both the extremes. For the rest of the images, we automatically vary the parameters to generate 8 degraded versions for
each low-quality image. The amount of variation in the parameters is determined empirically using these 20 images.
Let us denote the mth corresponding degraded image for
a low-quality image, Ii , by Bim . The visual feature vector and
the k th parameter vector of Bim is denoted by VBkm and PBk m
i
i
respectively. We use Bim , Ii and Ri,j to build a three-level
ranking model such that Bim < Ii < Ri,j ∀ {i, j, m}.
The ranking model can be formulated as follows:
X
1 2 X
0
ω +
ξi,j +
ξi,m
min
2
i,j
i,m
(3)
T m
0
s.t.
ω hi − ξi,m ≤ ω T fi ≤ ω T gi,j − ξi,j
0
ξi,j ≥ 0, ξi,m
≥0

∀ i, j, m , m = {1, . . . , 8}

where fi = [Vi , Pi1 , . . . , Pik ] and gi,j = [VRi,j , PR1 i,j , . . . ,
1
k
m
PRk i,j ] and hm
i = [VBi , PBim , . . . , PBim ]. The approach using above ranking model is termed as approach-3176 since
the feature space generated using this approach is 3176-D .
Similarly, Equation 2 can be converted into a three-level ranking model and we call that approach as approach-2744. All
the ranking formulations are solved using ranking-SVM [11].
3.2.2. Feature and parameter vectors for ranking
The entire feature vector includes a 2600-D HSV histogram
(visual feature) and four 144-D parameter vectors represent-

ing brightness, contrast, saturation and sharpness.
HSV histogram: We divide saturation and value uniformly
into 10-bins each. Hue is divided non-uniformly into 27 bins,
based on the distance between the hues in the CIELAB color
space. That distance is given by the CIEDE2000 metric which
considers perceptual uniformity. The reader is pointed to [12]
for the representation of the formula and its implementation
details. We form a 2-D grid where saturation and brightness
varies from 0 to 1 in a grid and hue varies across different
grids, in steps of 5. We measure the CIELAB distance between the corresponding points on two grids. The maximum
distance between two grids gives us a rough measure of the
distance between all possible shades of these two hues. Thus,
dmn = max ∆E(GHm (i, j), GHn (i, j))
ij

(4)

where GHm and GHn are the grids for the mth and nth hue
respectively. ∆E is the CIEDE2000 color difference metric.
dmn provides the largest possible distance between all shades
of hue m and n. Our aim is to make a separate bin for a
hue which is significantly different from the previous hue. To
achieve this, we keep m constant and vary n till d crosses a
pre-determined threshold (=7). Once this threshold has been
reached, we assign the value of n to m and repeat the process
till m reaches 360. We obtain 27 bins for hue. Due to the proposed binning, the HSV histogram captures more details in
an image by using relatively less number of bins. Separation
between two hues ranges from 6◦ to 40◦ in this binning.
Contrast: We propose to measure local RMS contrast. The
RMS contrast of an M × N image I is defined as the standard
deviation of pixel intensities as follows,

CRM S

v
u
−1
−1 M
X
u 1 NX
(I − I¯ij )
=t
M N i=0 j=0

(5)

We first divide the image into 12 × 12 grid to capture the
local characteristics of an image. Each grid is subdivided into
blocks of 8 × 8 pixels. We measure the RMS contrast of these
blocks and average the contrast values inside a grid. This
gives a 144-D vector (12 × 12 = 144) for each image.
Brightness and Saturation: We divide the image into 3 × 3
grid. For each grid, we calculate a 16-bin histogram of V
and S-channel. This gives us two 144-D (16 × 9 = 144)
histograms as brightness and saturation features.
Sharpness: We adopt the approach mentioned in [13] to measure sharpness. Instead of calculating the sharpness metric for
the entire image, we divide the image into 3 × 3 grid and for
each grid, we calculate its sharpness as the ratio of area of
high-frequency components to the total area of the grid,
msharp =

||H||
,
||I||

(6)

where ||H|| = {(u, v)| |F (u, v)| > θ} and F is the FFT of
image I. θ is a pre-defined threshold. By varying θ, one can
decide the sharpness level of an image, at which the metric
should start responding. For example, msharp will produce
large values even for a blurred image when θ is kept small
and vice-versa. We define θ as a monotonically-increasing
16-dimensional vector. θ increases on a log-scale from 0 to 1.
The ith bin of the sharpness histogram is defined as,
msharpi =

||Hi ||
||I||

for i = [1, ..., 16],

(7)

where ||Hi || = {(u, v)| |F (u, v)| > θ(i)}. msharp is calculated for each grid to produce a 144-D sharpness feature.
3.3. Algorithm for enhancing a new image
The relative ranking model is now capable of assigning a
score to an enhanced version of an image depending on its
color composition and content. However, generating these enhanced versions is not straightforward due to the large number
of possible enhancement parameter combinations. We employ a novel parameter sampling scheme based on the ranking
scores to reduce the search space as follows.
We retrieve 100 most similar images to the new image by
using the CBIR module defined in Section 3.1. We then calculate scalar values for saturation, brightness and contrast of
the retrieved images, denoted by sR , bR and cR respectively.
We also define the terms sN , bN and cN for the new image
equivalently. The average saturation and brightness is calculated as the mean of S and V channel of an image in HSV
color space, respectively. The average contrast is the standard
deviation of the RGB pixel values. Next step is to calculate
the ranking scores of those retrieved images by multiplying
the learned model w with the feature vectors of all the retrieved images. The ranking score for the ith retrieved image
is denoted by scri . The procedure of calculating scr for the
approach-3176 and 2744 is slightly different as follows:
scr3176 = wT ∗ f,

scr2744 =

k
1 X T
w ∗ fj
k j=1

(8)

where f and k are the feature vector of a retrieved image and
number of parameters respectively. Note that since the features for both the above approaches are different, the learned
models have a different structure (3176-D vs. 2744-D).
We non-uniformly sample around the values of sR , bR and
cR using the obtained ranking scores. Dense (sparse) sampling is performed around those values of sR , bR and cR for
which we have obtained high (low) scores. A high score indicates that the image is visually appealing and similar to the
new image. After parameter sampling, we try to steer sN , bN
and cN towards the sampled values. However, the low and
high-quality images are not counterparts of each other and
hence we stop steering sN , bN and cN towards the sampled

values if the percentage change is more than a certain threshold. It is determined empirically to be ±20% for average
brightness and saturation and ±4% for average contrast. Using this procedure, we generate anywhere from 150 to 250 enhanced versions of an image. Ranking scores are calculated
for all these enhanced versions as well as the original one.
The image with the highest score is presented to the user.

Fig. 2: Average result of our subjective evaluation study.

4. EXPERIMENTS AND RESULTS
We carried out two-fold assessment of our algorithm. Firstly,
as a verification step, we evaluated our ranking model on
manually enhanced and degraded images. We selected 500
images from MIT-Adobe FiveK dataset [5]. Each image in the
data-set has been enhanced by five experts. We also generated
8 degraded versions for every image. We calculated ranking
scores for original images, its enhanced and degraded counterparts. For 90.19% (451/500) images, at least one enhanced
version got a higher score than the original image. For 100%
of the images, at least one bad version obtained less score than
the input image. For 80.27% (401/500) of the images, at least
7 out of 8 bad versions got a lower score than the input image.
For a more robust performance assessment, we carried out
a subjective evaluation test. To this end, we formed a testing
set with 127 images which is disjoint with the training set.
Our low-quality image data-set contains 77 images. The remaining 50 images were selected at random from the 94 image database provided to us by the authors of [1]. Our subjective evaluation test involved 33 users. Each user was shown
a pair of images and was asked to choose the better photograph. In case a user preferred both photos equally, a third
option of voting to both of them was made available. There
was no time-limit and users could take breaks in between if
they were fatigued. The tests were done on the same type
of monitor and the lighting conditions as well as the sitting
arrangements were identical for all the tests. The order between different pairs and between the images of a pair was
kept random. No indication regarding the type of enhancement method used was given to the user.
The subjective evaluation test consisted of five sessions.
Users were asked to compare approach-2744 and 3176 to Picasa in the first two sessions. In the next two sessions, we enhanced the 50 images using these two approaches and asked
users to compare them with the approach of [1]. Finally, we
skipped the auto-enhancing step mentioned in the Section 3.1
and also employed a simple two-level ranking (instead of the
three-level ranking) as described in Equation 1 to formulate
a new but presumably inferior approach. It is now used to
enhance 25/77 images, chosen at random. In the fifth session, users compared these 25 images with Picasa. The last
session aims to explain the need of multi-level ranking and
pre-processing step of auto-enhancement before retrieval. In
total, each user compared 279 (= 77 ∗ 2 + 50 ∗ 2 + 25) pairs
of images. Each user took anywhere from 30-45 minutes to

complete the test.
Fig. 2 shows the cumulative votes given to each approach
in every session. The votes are accumulated from all users
over all the images. The method which was selected more
number of times in a session was favored by the users for that
session. Users consistently pick photos enhanced by our approach over Picasa and the approach of [1]. Interestingly, the
results of the fifth session favor Picasa, implying the need for
multi-level ranking. The numbers above each bar represent
the average rank obtained for each approach, lower rank implying better quality for the image. It is calculated as the average of all the vote ratings (from all users) to that approach.
Fig. 3 shows example results from session 1, 2 (row 1,
2, 5), 3 (row 3, 4) and 5 (row 6). Similar to [1], even we
observed that Picasa is conservative while enhancing photos.
Picasa concentrates on conservatively adjusting the brightness
and contrast. However, images in the second row illustrate
that the conservative adjustment does not suit all the images.
Our approach converges on better enhancement parameters
by performing content-adaptive enhancement. For example,
outdoor images may need significant changes in their composition while quality of indoor images may be harmed by
doing so. On the other hand, the approach of [1] significantly
changes the original photo as shown in the middle two images
in row 3 and 4. However, in this process, sometimes large
variation happens in the amount of contrast and saturation,
which harms the quality of the photo. We avoid this by introducing multi-level ranking and the novel parameter-sampling
strategy. Approach of [1] enhances the original photo based
on the parameters of K-nearest high-quality images (in terms
of L2 distance) alone. We sample our parameters based on Knearest images and more importantly using the ranking score.
We also set an upper limit to the amount of variation in the
enhancement parameters. Thus our parameter sampling takes
into account the color composition, content and quality of the
retrieved images. The importance of these steps is clearly
seen in the images in the last row. The rightmost image is
over-saturated since the model lacks any knowledge about the
bad effects of setting extreme values for image parameters.
Our algorithm prefers more significant adjustments than
Picasa. The reason for such a preference stems from our training database. We have total 11, 289 images from DPChallenge.com and Photo.net. Most of the images are vibrant in
colors, with high-contrast and saturation. Though many participants in our study tend to choose high-contrast and satura-

Acknowledgement: The work was supported in part by a
grant (1135616) from the National Science Foundation. Any
opinions expressed in this material are those of the authors
and do not necessarily reflect the views of the NSF.
6. REFERENCES
[1] Jianzhou Yan, Stephen Lin, Sing Bing Kang, and Xiaoou Tang, “A learning-to-rank approach for image
color enhancement,” in IEEE CVPR, 2014.
[2] Juan C Caicedo, Ashish Kapoor, and Sing Bing Kang,
“Collaborative personalization of image enhancement,”
in IEEE CVPR, 2011, pp. 249–256.
[3] Sung Ju Hwang, Ashish Kapoor, and Sing Bing Kang,
“Context-based automatic local image enhancement,” in
ECCV, pp. 569–582. Springer, 2012.
[4] Sing Bing Kang, Ashish Kapoor, and Dani Lischinski, “Personalization of image enhancement,” in IEEE
CVPR, 2010, pp. 1799–1806.
Fig. 3: Comparison of image enhancement methods. Top two
rows, our data: left: original image, middle: enhancement
by Picasa, right: approach-2744. Row 3 and 4, data of [1]:
left:original image, middle: enhancement result of [1], right:
approach-2744. Row 5, our data: From L to R, 1. original image, 2. enhancement by Picasa, 3. approach-3176, 4.
approach-2744. Last row, our data: From L to R, 1. original image, 2. enhancement by Picasa 3. multi-level ranking,
approach-3176 4. two-level ranking, approach-3176.
tion photos, it is possible that some users prefer washed-out
or dark photos. Such personalized preferences can be learned
by our model by training on users’ Flickr or Instagram feed.
5. CONCLUSION
We presented a novel learning-based framework for image
enhancement. It uses CBIR to perform content-adaptive enhancement of low-quality images. A multi-level relative ranking model is trained with the help of high-quality images on
the web. We show that instead of concatenating all features,
considering them pairwise in the ranking model creates better enhanced images with all of its parameters in balanced
amounts. We propose a novel parameter sampling scheme
to reduce the huge search space and converge onto better
enhancement parameters. We verified the effectiveness of
our framework by checking its performance on MIT-Adobe
FiveK dataset. For a more robust comparison, we carry out
subjective evaluation tests and show that users prefer photos
enhanced by our framework over others. Our framework offers scalablity and personalization since it directly uses highquality image databases from the web.

[5] Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and
Frédo Durand, “Learning photographic global tonal adjustment with a database of input / output image pairs,”
in IEEE CVPR, 2011.
[6] Neel Joshi, Wojciech Matusik, Edward H Adelson, and
David J Kriegman, “Personal photo enhancement using
example images,” ACM Trans. Graph, 2010.
[7] Erik Reinhard, Michael Stark, Peter Shirley, and James
Ferwerda, “Photographic tone reproduction for digital
images,” in ACM Trans. Graph, 2002.
[8] Liad Kaufman, Dani Lischinski, and Michael Werman, “Content-aware automatic photo enhancement,” in
Computer Graphics Forum. Wiley Online Library, 2012.
[9] Ritendra Datta, Jia Li, and James Ze Wang, “Algorithmic inferencing of aesthetics and emotion in natural images: An exposition,” in IEEE ICIP, 2008, pp. 105–108.
[10] Mathias Lux, “Content based image retrieval with lire,”
in 19th ACM Multimedia, 2011, pp. 735–738.
[11] Thorsten Joachims, “Training linear svms in linear
time,” in Proceedings of the 12th ACM SIGKDD, 2006.
[12] Gaurav Sharma, Wencheng Wu, and Edul N Dalal, “The
CIEDE2000 color-difference formula: Implementation
notes, supplementary test data, and mathematical observations,” Color Research & Application, 2005.
[13] Yiwen Luo and Xiaoou Tang, “Photo and video quality evaluation: Focusing on the subject,” in Computer
Vision–ECCV 2008, pp. 386–399. Springer, 2008.

Towards Computational Understanding of Skill
Levels in Simulation-Based Surgical Training via
Automatic Video Analysis
Qiang Zhang and Baoxin Li
Computer Science & Engineering, Arizona State University,
Tempe, AZ, 85283
Abstract. Analysis of motion expertise is an important problem in
many domains including sports and surgery. Recent years, surgical simulation has emerged at the forefront of new technologies for improving the
education and training of surgical residents. In simulation-based surgical
training, a key task is to rate the performance of the operator, which is
done currently by senior surgeons. This is deemed as a costly practice
and researchers have been working towards building automated systems
to achieve computational understanding of surgical skills, largely through
analysis of motion data captured by video or data of other modalities.
This paper presents our study on understanding a fundamental issue in
building such automated systems: how visual features computed from
videos capturing surgical actions may be related to the motion expertise
of the operator. Utilizing domain-speciﬃc knowledge, we propose algorithms for detecting visual features that support understanding the skill
of the operator. A set of video streams captured from resident surgeons
in two local hospitals were employed in our analysis. The experiments revealed useful observations on potential correlations between computable
visual features and the motion expertise of the subjects, hence leading
to insights into how to build automatic system for solving the problem
of expertise evaluation.

1

Introduction

For centuries, the marvel of human movement has been a source of signiﬃcant
research. Understanding of human motion is especially important in ﬁelds such
as biomechanics, dance, computer animation, and ergonomics. A key problem
that is of signiﬃcant interest to several domains is the analysis of motion expertise. In domains such as dance, sports, and even surgery, motion of experts
diﬀers considerably from novices. This has been the basis of several video based
motion analysis systems for sports analysis. However using such systems requires
signiﬃcant amount of planning, calibration, and customized development, rendering it diﬃcult to extend the systems to domains such as surgery wherein
medical educationists are not technically trained. The motivation of this paper
is to develop computational algorithms that support the development of an intuitive and simple-to-use video-based system in the domain of simulation-based
surgical training for motion expertise analysis.
G. Bebis et al. (Eds.): ISVC 2010, Part III, LNCS 6455, pp. 249–260, 2010.
c Springer-Verlag Berlin Heidelberg 2010


250

Q. Zhang and B. Li

The traditional process of training resident surgeons has been primarily based
on interactive and direct instruction of supervising surgeons. Surgical motions
have increasingly become complex, requiring signiﬁcant psychomotor and cognitive skills. As the surgeons have moved from open surgery to laparoscopic
surgery to now robotic surgery, the dexterity requirements have grown manifold. This makes surgery education and training even more challenging. And
thus conventional training purely relying on a senior surgeon’s instruction is not
only typically very slow, but also costly and hard to generalize since objectively
quantiﬁable assessment metrics are often missing. Recent years, surgical simulation has emerged at the forefront of new technologies for improving the education
and training of surgical residents (e.g., [3][4][15][16]). In theory, data captured
from the training simulation can be used to evaluate surgical skills. However, in
practice, there has been little substantial advance in this front largely due to the
lack of eﬃcient and eﬀective computational approaches to inference of surgical
skills from such captured raw data. One challenge is to automatically rate the
proﬃciency level of a resident surgeon (([5]). This factor has currently gained
added initiatives as the American college of surgeons seeks to develop a national
skills curriculum (www.acs.org). The state of the art is the work by Lin et al
([8]) that segments the captured actions into some pre-deﬁned units and the
skill evaluation remains unsolved. For a large part, the challenge is due to the
lack of a computational understanding of surgical skills in terms of computable
features. This is the problem that our study set out to address.

(a)

(b, c and d)

Fig. 1. (a) The setup of the system: an FLS trainer and two external cameras capturing
the hand movements. The on-board camera is connected to the display that shows what
happens inside the box. (b) One sample frame captured by the on-board camera. (c)
and (d) Sample frames from two hand-view cameras.

Towards Computational Understanding of Skill Levels

251

Speciﬃcally, we explore the above problem in the context of simulation-based
laparoscopic surgery, which has emerged as an alternative (and in some cases,
a replacement) to traditional open surgery. Based on the FLS (Fundamentals of
Laparoscopic Surgery) training system [1](Fig. 1), which has been widely adopted
by hospitals in the U.S., we employ two additional cameras to capture the hand
motion of the subject. Adding to the video that is captured by the camera of
the FLS system, we have a total of three video streams: two capturing the hand
movements (i.e. Hand view as shown in Fig. 1c and d) and one capturing the
tool movement (i.e. Tool view as shown in Fig. 1b). Our task is to analyze the
videos so as to ﬁnd potential correlation models between the computable visual
features and the motion expertise of the subjects. To this end, we ﬁrst propose a
novel algorithm for segmenting the tool-view video into segments that correspond
to meaningful physical action units. Then we extract motion features from such
segments from videos of the resident surgeons with varying level of surgical skills.
This allows us to study the potential correlation between the features and the
proﬃciency of the subject. As we know, this is the ﬁrst work that tries to evaluate
the proﬁciency level of surgical operation through visual features.
In Section 2, we brieﬂy review relevant studies in the literature. In Section 3,
we present the proposed algorithms for segmenting the video and for computing
the motion features. Then in Section 4, experimental results and analysis with
real videos are reported. Section 5 concludes the paper with a brief discussion
on future work.

2

Related Work

Objective evaluation of surgical skills has been a topic of research for many
years ([6][10][15]). According to [2][18], there is high correlation between the
proﬃciency level and the motion parameters observed, such as duration, number
of movements and length of path. This provides the theoretical foundation for
building the system for objective evaluation of surgical skills, according to the
features collected. A technique proposed by [5] called task deconstruction was
implemented in a recent system by [14]. They use Markov Models to model
a sequence of force patterns or positions of the tools. Specially designed tools
measure hand-tool and tool-tissue interactions through sensors. They showed
that their Markov Models were suitable for decomposing a task (such as suturing)
into basic gestures, and then the proﬃciency of the complex gesture could be
analyzed. While this study oﬀers an intriguing approach to expertise analysis,
it requires an expert surgeon to provide speciﬃcations for building the topology
of the model; hence it cannot be easily generalized to new procedures.
The work of [8] relies on the use of robot-assisted Minimally Invasive Surgical
Systems (MISS), such as Intuitive Surgical’s da Vinci, to provide the quantitative
motion and video data. In that work, automatic techniques for detecting and
segmenting surgical gestures have been developed. Useful features are selected
from the original feature set obtained from the da Vinci system; these features
are then normalized and projected onto a lower-dimensional feature space with

252

Q. Zhang and B. Li

Linear Discriminant Analysis [13], and a Bayesian classiﬃer is then built to
recognize diﬀerent motion gestures in the lower-dimensional feature space.
The aforementioned systems require special devices, such as data gloves, which
require modiﬃcation of the current system. In addition, wearing data gloves
may interfere with the performance of the subjects. For those reasons, a videobased system is preferred, in which one or more cameras are used to capture
the movements of the subjects or objects. The key task is then to extract visual
features from the videos for proﬃciency evaluation.
This problem is related to video based action recognition, which has been under research for many years and some good surveys can be found in [17][11][12].
However, most of those methods are designed for classifying diﬀerent actions,
whose visual motion is apparently diﬀerent. On the other hand, in the above
problem of proﬁciency evaluation, the subjects are required to perform the same
task and the diﬀerence of the visual motion due to their proﬁciency is subtle. In
addition, subjects in the same proﬁciency level may show very diﬀerent movements, due to the variation of personal habit. Thus typical action recognition
methods do no directly apply for proﬃciency evaluation.

3

Proposed Approach

As discussed in Section 1, we assume that we have 3 cameras capturing the
motion of the subject, two capturing the motion of the hands (giving two handview videos) and the third capturing the motion of the tool (giving the tool-view
video). Our analysis will be based on three videos from these cameras. In this
section, we ﬁrst present our approach for segmenting the tool-view video into
segments that correspond to meaningful physical actions of the subject. Then we
propose algorithms for extracting features from the tool views. These features
will be further used in our analysis in the next Section. Our tool-view-centric
approach was motivated by the fact that human experts mainly rely on the toolview in evaluating the performances of a resident surgeon. In current stage, our
approach works oﬄine.
3.1

Segmenting the Tool-View Video

When a subject performs an operation on the FLS system, a task may include
several subtasks. For example, in the “peg transferring” operation, the subject
needs to ﬁrst lift an object with a grasper ﬁrst in one’s non-dominant hand, then
transfer the object midair to the dominant hand, and then place the object on
a peg on the other side of the board. This process needs to be repeated a few
times. Therefore, to facilitate any further analysis of the motion of the subject
from the captured video of the tool and the objects movements, it is desired to
ﬁrst segment the tool-view video into much shorter clips that correspond to some
underlying actions. However, in the videos captured from existing FLS systems,
segmentation of the video based on motion analysis or explicit object tracking
is challenging due to a few factors. For example, multiple objects with similar

Towards Computational Understanding of Skill Levels

253

colors are present in the ﬁeld of view, which easily confuse a feature tracker
that relies on color-based cues. Also, the tool and the object movement will cast
shadows and occlusions in the ﬁeld of view. In addition, the movement of the
objects and the tools are three-dimensional and non-rigid in nature. All adding
uncertainties to a tracking algorithm.
To get around the above problems, we utilize two other videos capturing the
hand movements to achieve the segmentation of the tool-view video. That is, assuming that all the three videos are synchronized, we will attempt to infer the
information for motion-based segmentation in the hand-view videos and then use
the result to segment the synchronized tool-view video. While in theory, using
both hand views may improve the robustness of the approach, in the current study,
we experimented with only one hand view for this purpose. Our basic strategy is
to compute the motion trajectories from the chosen hand view and then analyze
the motion to segment the tool-view video. From the setup presented in Fig. 1, we
can make the following assumptions, which simplify the analysis: (i) the subjects
wear gloves with known, distinct colors (For simplicity, the gloves region is manually marked out for initilization. But in future work, we can build a color model
for the gloves to enable fully automatical initialization); and (ii) the two hands
of the subject are separate from each other during the operation.With these assumptions, we propose the following tracking methods for the hand view:
Algorithm 1. Tracking two hands in the hand view
Input: A surgical video in the hand view.
Output: Two trajectories l and r storing the positions of mass centers of two
hands respectively.
Initialize: Build a 3-component Gaussian
model for pixels of gloves in HSV color



1
1
space: μ = |G| i∈G xi and σ = |G| i∈G (xi − μ), where G means pixels in
gloves area;
1. Convert current frame into HSV space;
2. Calculate the probability of the each pixel classiﬁed as gloves pixel with
following equation:
P (xH,S,V ) = p(xH )p(xS )p(xV )
(x−μ)2

3.
4.
5.
6.

(1)

1
where p(x) = √2πσ
e− 2σ2 ;
1 
Binarize the probability by following equation: where μp = |I|
i∈I P (x(i))
and I is current frame;
Removing the noises and hole by morphological operation (i.e. erode and
dilate);
Clustering the coordinate of pixels where P̂ (x(i)) = 1 with K means (K = 2).
The 2 cluster centers represent the mass centers of the two hands;
Save the 2 cluster centers in l, r respectively and repeat the above steps until
the end of the video;

254

Q. Zhang and B. Li

With Algorithm 1, we can obtain the trajectory of two hands l ∈ R2∗N and
r ∈ R2∗N , where N is the number of frames.
In order to remove the eﬀect of translation, without loss of generality, the
trajectories are centered to their centroid so that their ﬁrst moments are zero.
x(:,i)
, where x =
The remaining scale normalization is achieved by x̃(:, i) = |x(:,i)|
T
T T
4∗N
[l , r ] ∈ R
and x(:, i) means the ith column of x, i.e. the coordinates of
mass centers of two hands in ith frame. After normalization, we calculate a Self
Similarity Matrix (SSM)[7] M with following equations:
M (i, j) = x̃(:, i) − x̃(:, j)2

(2)

SSM represents the similarity between action representation (trajectory here)
of all pairs of frames. M (i, j) being small means the positions of two hands in
frame i and j are similar. Fig. 2a shows the ﬁgure of SSM of an example. The
region S, where M (i, j) is small for all (i, j) ∈ S (i.e. the blue regions near the
diagonal of Fig. 2a), indicates that the positions in this period is similar, which
can be viewed as a segment. So segmentation of video is formulated as detecting
these regions. The method is summarized in the following algorithm:
Algorithm 2. Segment the hand-view video based on SSM
Input: Self Similarity Matrix M .
Output: Segmentation positions W .
Build a Gabor ﬁlter f with θ = 3π
4 , σx = σy = 16 and f req = 33;
Calculate M̂ = M ∗ f , where ∗ is the convolution operation (Fig. 2b and c);
Extract the diagonal elements d from the matrix M̂ ;
˜ = 0 if d(i) < d,
¯ where d¯ is the mean of d;
Thresholding x by d(i)
Detecting the positions of local maximal for thresholded d˜
˜ − d(i
˜ − 1) ≥ 0 and d(i
˜ + 1) − d(i)
˜ ≤ 0};
w = {i|d(i)
6. For each element in w, if wj+1 − wj is small, remove the one with smaller
˜
value in d;
7. The remaining w are the segmentation positions, which are shown as crosses
in Fig. 2(d);
1.
2.
3.
4.
5.

The gabor ﬁlter is desgiend to detect the discoutinuity in the diagnoal of SSM.
3.2

Feature Extraction in the Tool View

Our hypothesis is that, after segmenting the tool-view video into segments of
meaningful action units, it will become easier to analyze the correlation between
computed motion features and the motion expertise of the subject, compared
with considering the original video on the whole. In this subsection, we describe
our method for computing some features for the video segments in the tool view.

Towards Computational Understanding of Skill Levels

(a)

(b)

255

(c)

(d)
Fig. 2. (a) SSM for an example. (b) The convolution result of Gabor ﬁlter and SSM.
(c) Enlarged version of area in highlighted part of (b). The diagonal d of Self Similarity
Matrix M shown in (a). (d) The red crosses indicate the segmentation positions W we
ﬁnd with Algorithm 2. This segmentation will be carried over the tool view, assuming
the views are synchronized. In our dataset, the segments obtained from the above
algorithm are usually 50 to 150 frames long.

We ﬁrst apply the KLT tracker [9] in each segment to extract the trajectories of
points on the tools and the objects. To alleviate the diﬃculties due to confusing
background and shadows etc, we utilize the fact that each segment is relatively
short and the motion is relative homogeneous within a segment and propose the
following method for computing motion trajectories in the tool view:
Algorithm 3. Extract trajectories in the segments of tool view
Input: A tool-view segment; Number of trajectories K.
Output: K trajectories.
For each frame in the video:
1. Convert every frame I to HSV space and represent the frame as I˜ = IS IV ;
ˆ = 0 if I(i)
˜ <= I,
¯ where I¯ is the mean of I;
˜
2. Threshold Iˆ by I(i)
3. Use morphological operation (erode and dilate) to remove the noise and hole
ˆ
in I;
4. Calculate the absolute diﬀerence D between the ﬁrst frame and the last
frame and binarize it with

1 if D(i) >= D̄
D̃(i) =
(3)
0 otherwise
where D̄ is the mean of D;

256

Q. Zhang and B. Li

5. Detect the feature points with KLT tracker and select those in the region
where D̃(i) = 1;
6. Start KLT tracker with the feature points we selected;
7. Remove the trajectory whose average motion magnitude is smaller than 1
pixel/frame;
8. Sort the trajectories according to their length and return the longest K
trajectories, if any;
For every trajectory X ∈ R2∗T of a segment, we consider two kinds of feature:
the average motion magnitude M and the jitter A:


T
T
1 

1
|V (:, i)| and A = 
(V (:, i) − Ṽ (:, i))2
(4)
M=
T i
T i
d

∈ R2∗T is the velocity and Ṽ is a smoothed version of V
where V = X(t)
dt
after moving-average ﬁltering. Finally, each trajectory is represented by a feature
vector f = [M, A] ∈ R2 . Each surgical video is represented as a bag of these
feature vectors.

4

Experiments and Analysis

Videos captured from two local hospitals were used in our experiments to evaluate the segmentation approach and to assess whether the segmentation procedure
and the subsequent feature extraction can help generate numeric features that
may correlate to the motion expertise of the underlying subject. In the following,
we ﬁrst brieﬂy describe the data used our experiments and then present some
analysis results.
We used 12 set of videos for the “peg transfer” operation by 12 diﬀerent subjects with 2 proﬁciency levels: 6 of the resident surgeons are deemed as experts
who are very skilled with the task while the other 6 are considered as novices
who are yet to gain better skills with the task. To give some more detail about
the operation, we describe the task below. The peg transfer task requires the
subjects to lift (i.e. “Pick”) six objects (one by one) with a grasper by the nondominant hand, transfer the object midair to the dominant hand, and then place
(i.e. “Drop”) the object on a peg on the other side of the board. Once all six
objects are transferred, the process is reversed, and the objects are to be transferred back to the original side of the board. The timing for this task starts
when the subjects grasped the ﬁrst object and ends upon the release of the last
peg. Some details of the videos used in our experiments, including the segments
generated by our algorithm, are given in Tab. 1.
It was found that the segments generated by the proposed algorithm indeed
correspond to some physically-meaningful action units. For example, typically, a
segment corresponds to one of the following actions: picking-up, dropping, transferring, or attempts of these actions. Note that, if a subject made several picking

Towards Computational Understanding of Skill Levels

257

Table 1. The detail of the data we used. “duration” means the number of the frames
in the video. “Movements” means the number of movements, i.e. segments obtained by
Algorithm 2. “comment” indicates the proﬁciency level of residents, where “E5” means
high proﬁciency level and “N” for low proﬁciency level. Frame rate is 30 FPS.
ID
Duration
Segments
Comments

1
2240
34
E

2
2190
27
E

3
3637
39
E

4
2759
29
E

5
3791
47
E

6
3722
29
E

7
5313
61
N

8
5354
70
N

9
5468
62
N

10
5747
71
N

11
4833
54
N

12
4735
48
N

attempt before successfully lifting the object, the video may be segmented into
multiple segments. This explains why in the above table the videos from the experts typically contains fewer segments than the novices. Also, an expert may be
able to pick up one object while dropping another object simultaneously, hence
reducing the number of computed segments.
In this paper, instead of building a classiﬁer for proﬁciecency level based on
some features, we analyzed the relationship between the proﬁciency level and
the features we extracted, including: (1) the duration and the number of detection segments which were used in [2][18]; (2) the histogram of average motion
magnitude and motion jitter for each type of the segments (as deﬁned above, the
segments are manually labeled into “picking-up”, “dropping”, or “transferring”.
In the future work, we plan to do it automatically). The deﬁnition of the the
features are elaborated below:
1. Duration: the time to complete the task (2nd column in Tab. 1);
2. Number of segments obtained by Algorithm 2 (3rd column in Tab. 1);
3. Histogram of average motion magnitude: considering the average motion
magnitude M of every trajectory in each type of segments of a video as
a data points and compute a histogram. Fig. 4 shows the histograms for
the “transferring” segments (i.e. only considering those segments which are
determined to be a “transferring” action after visual inspection);
4. Histogram of motion jitterness: considering the jitterness A of every trajectory in each type of segments of a video as a data points and compute a
histogram. Fig. 5 shows the histograms for the “picking” action;
From the features computed from the videos, we had the following observations:
1. According to Tab 1, the proﬁciency level is highly correlated with the duration time and number of the movements that the subjects take to complete
the task, which is also veriﬁed in [2][18]. For subjects who are novices (i.e. 7
to 12), they take more time and movements, since they would typically need
multiple attempts (e.g. taking several tries to pick up an object), and make
errors (e.g. losing the objects during transferring) and thus need corrections.
Note that, while it may appear to be trivial to ﬁnd that an expert needs
shorter time to complete an action, our intention is for explicitly identifying this fact so that it can be correlated with other feature, hence enabling
more profound analysis tasks such as how to provide feedback to a subject

258

Q. Zhang and B. Li

Fig. 3. The histogram of average motion magnitude for the “transferring” segments,
for each of the 12 videos in Table 1 respectively. The histogram is normalized. The X
axis is motion magnitude with the unit pixel/frame.

Fig. 4. The histogram of jitterness for the “picking” action, for each of the 12 videos
in Table 1 respectively. The histogram is normalized. The X axis is jitterness with unit
pixel/frame.

by giving some actionable instructions. For example, telling a subject to act
quick may not be as helpful as tell him/her to try to stabilize his/her hands,
if we do ﬁnd a correlation between the expertise and the motion jitterness.
2. According to Fig. 4, subjects at higher proﬁciency level (i.e. 1 to 6) tends to
move faster than subjects who are novice during the “transferring” action,
since the histograms of the the former group have big masses on the right
side of the histogram (i.e. higher average motion magnitude). We can also
ﬁnd intuitive explanation for this observation: The “transferring” action is
the most diﬃcult part since most of the time is spent here and errors are
most likely to happen during this period. Thus subjects who are novices tend
to move more slowly for the “transferring” action;
3. If we divide the histograms in Fig. 5 by 0:8, we can ﬁnd that the histograms
of most subjects at higher proﬁciency level have less distribution on the
right-hand side (i.e. 0:8) than that of novice subjects. This indicates that,
subjects at higher proﬁciency level tend to move more smoothly during the
“picking” action. Intuitively, this may be explained by the fact that a novice
may be more susceptible to fumbling while trying to pick up the object.

5

Conclusion and Future Work

We presented our approach for processing and analyzing videos captured in
simulation-based surgical training, based on the widely-adopted FLS platform.

Towards Computational Understanding of Skill Levels

259

Automatic algorithms were proposed for segmenting the videos and for computing some important motion features from the videos. The algorithms were
evaluated with real videos capturing actual movements of resident surgeons in
two local hospitals. The analysis on the relationship between the proﬁciency level
and the feature extracted revealed some clear trends which can also ﬁnd physically meaningful interpretations. The current results were based on 12 videos,
and our immediate future work is to expand the analysis to a large data set involving more human subjects. This work contributes to a better computational
understanding of motion expertise in the domain of simulation-based surgical
training, which will in turn be helpful in building a fully automatic system for
evaluating the skills and for providing feedback in such training. This is our
long-term future work.
Acknowledgement. The authors were partially supported during this work by
an NSF grant (Award # 0904778), which is greatly appreciated.

References
1. Fundamentals of Laparoscopic Surgery
2. Aggarwal, R., Grantcharov, T., Moorthy, K., Milland, T., Papasavas, P., Dosis,
A., Bello, F., Darzi, A.: An evaluation of the feasibility, validity, and reliability
of laparoscopic skills assessment in the operating room. Annals of surgery 245(6),
992–999 (2007)
3. Aggarwal, R., Undre, S., Moorthy, K., Vincent, C., Darzi, A.: The simulated operating theatre: comprehensive training for surgical teams. Quality and Safety in
Health Care 13(suppl. 1), i27 (2004)
4. Eversbusch, A., Grantcharov, T.P.: Learning curves and impact of psychomotor
training on performance in simulated colonoscopy: a randomized trial using a virtual reality endoscopy trainer. Surgical endoscopy 18(10), 1514–1518 (2004)
5. Gallagher, A.G., Ritter, E.M., Champion, H., Higgins, G., Fried, M.P., Moses, G.,
Smith, C.D., Satava, R.M.: Virtual Reality Simulation for the Operating Room.
Proﬁciency-Based Training as a Paradigm Shift in Surgical Skills Training. Annals
of Surgery 241, 364–372 (2005)
6. Healey, A.N., Undre, S., Vincent, C.A.: Developing observational measures of performance in surgical teams. Qual. Saf. Health Care 13, 33–40 (2004)
7. Junejo, I., Dexter, E., Laptev, I., Pérez, P.: Cross-view action recognition from
temporal self-similarities. In: Forsyth, D., Torr, P., Zisserman, A. (eds.) ECCV
2008, Part II. LNCS, vol. 5303, pp. 293–306. Springer, Heidelberg (2008)
8. Lin, H.C., Shafran, I., Yuh, D., Hager, G.D.: Towards automatic skill evaluation:
Detection and segmentation of robot-assisted surgical motions. Computer Aided
Surgery 11(5), 220–230 (2006)
9. Lucas, B., Kanade, T.: An iterative image registration technique with an application to stereo vision. In: International Joint Conference on Artiﬁcial Intelligence,
vol. 3, p. 3. Citeseer (1981)
10. Mayes, S., Deka, J., Kahol, K., Smith, M., Mattox, J., Woodwards, A.: Evaluation
Of Cognitive And Psychomotor Skills Of Surgical Residents at Various Stages
in Residency. In: 5th Annual Meeting of American College of Obstetricians and
Gynecologists (2007)

260

Q. Zhang and B. Li

11. Mitra, S., Acharya, T.: Gesture recognition: A survey. IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews 37(3), 311–324
(2007)
12. Poppe, R.: A survey on vision-based human action recognition. Image and Vision
Computing 28(6), 976–990 (2010)
13. Riﬀenburgh, R.H., Clunies-Ross, C.W.: Linear discriminant analysis. PhD thesis,
Virginia Polytechnic Institute (1957)
14. Rosen, J., Brown, J.D., Chang, L., Sinanan, M.N., Hannaford, B.: Generalized
approach for modeling minimally invasive surgery as a stochastic process using
a discrete markov model. IEEE Transactions on Biomedical Engineering 53(3),
399–413 (2006)
15. Satava, R.M., Gallagher, A.G., Pellegrini, C.A.: Surgical competence and surgical
proﬁciency: deﬁnitions, taxonomy, and metrics. Journal of the American College
of Surgeons 196(6), 933–937 (2003)
16. Sutherland, L., Middleton, P., Anthony, A., Hamdorf, J., Cregan, P., Scott, D.,
Maddern, G.J.: Surgical Simulation: A Systematic Review. Annals of Surgery 243,
291–300 (2006)
17. Wang, J.J., Singh, S.: Video analysis of human dynamics–a survey. Real-Time
Imaging 9(5), 321–346 (2003)
18. Wanzel, K.: Visual-spatial ability correlates with eﬃciency of hand motion and
successful surgical performance. Surgery 134(5), 750–757 (2003)

Video-Based Self-positioning
for Intelligent Transportation Systems Applications
Parag S. Chandakkar, Ragav Venkatesan, and Baoxin Li
Computer Science and Engineering, Arizona State University, Tempe, AZ, U.S.A.
{pchandak,ragav.venkatesan,baoxin.li}@asu.edu

Abstract. Many urban areas face traffic congestion. Automatic traffic management systems and congestion pricing are getting prominence in recent research.
An important stage in such systems is lane prediction and on-road self-positioning.
We introduce a novel problem of vehicle self-positioning which involves predicting the number of lanes on the road and localizing the vehicle within those lanes,
using the video captured by a dashboard camera. To overcome the disadvantages
of most existing low-level vision-based techniques while tackling this complex
problem, we formulate a model in which the video is a key observation. The
model consists of the number of lanes and vehicle position in those lanes as parameters, hence allowing the use of high-level semantic knowledge. Under this
formulation, we employ a lane-width-based model and a maximum-likelihoodestimator making the method tolerant to slight viewing angle variation. The overall approach is tested on real-world videos and is found to be effective.

1 Introduction
Globally, per-capita vehicle ownership is on a constant rise. In the US1 alone, in the year
2010, the per-capita vehicle ownership was 0.78 with an average individual covering
over 15528 kilo-meters. There were 1.77 fatalities per-kilo-meter, in the year 2010. In
the year 2011, there were 211.87 million drivers and 311.59 million people traveling on
highways alone, with over 244.77 million vehicles, of which 130.89 million are cars.
Travel on roads in the US, increased by over 1.5% from September 2012 to September
2013, with over 345.5 billion vehicle kilo-meters covered in this time-line.
This situation is not unique to the U.S. Similar trends were found in the UK2 as well.
Cars, vans and taxis traveled about 800 billion passenger kilo-meters. Distance traveled
by cars observed a steep increase from 27% to 87% from 1952 to 2013. In the year
2013, 64% of all transportation was done by car, with over 34 million vehicles used by
over 62 million people everyday. The national transport model center forecast in UK
predicts a 43% increase in vehicle traffic from 2010 to 2040. With such drastic increase
in global road-traffic, intelligent transport system (ITS) research has taken center-stage
prominence in recent years.
A typical ITS involves collecting real-time vehicle and traffic information, estimating flow and congestion, and providing corrective feedback, usually through congestion
1
2

US statistics were from http://www.fhwa.dot.gov
UK statistics were from http://www.gov.uk; Tables TSGB0101 to TSGB0102.

G. Bebis et al. (Eds.): ISVC 2014, Part I, LNCS 8887, pp. 718–729, 2014.
c Springer International Publishing Switzerland 2014


Video-Based Self-positioning for Intelligent Transportation Systems Applications

719

pricing and toll. Today’s state-of-the-art ITS stands at a level where they collect information at a road-level and provide congestion pricing for platoons of cars entering the
road and exiting the road in a connected-pipes topology [1,2]. The envisioned future for
this kind of systems is to predict and model vehicle patterns and traffic at the lane-level,
along with modeling inter-lane and intra-lane traffic changes in addition to road level
traffic changes. This enables the ITS to price different lanes differently and to provide
routing assistance to drivers at a lane-level. Therefore self-positioning for cars is an
important stage in such an ITS model. Although, many hardware dependent modalities
such as lane marker detection using LIDAR and using those using GPS are popular in
literature, they are cost in-efficient and are not usually preferred [3,4]. With the importance of the problem and the multi-modal data streams involved, ITS applications calls
for development of new multi-media processing and analysis techniques.
Video-based lane detection and prediction is a popular modality as digital cameras
have become ubiquitous and portable. Although in the past, many simple solutions such
as the Hough transform, were found to give acceptable results in lane detection, extracting knowledge about lanes from them is a difficult task [5]. In this paper, we develop a
lane modeling and prediction system from a wind-shield mounted front-facing camera.
The aim of this system is not to detect lane markers, although we treat it as a subproblem, but to predict the number of lanes on the road and the lane at which the car is
presently being driven. In this paper, we propose a Bayesian joint model with the number of lanes and lane number as parameters and we maximize the likelihood for every
combination of these parameters. The likelihood function is developed in a bottom-up
approach using guided filter, lane-width modeling and prediction confidence of a classifier. To facilitate the exploration of the vehicle self-positioning problem and to improve
and verify the current techniques, we have made the database publicly available.
The rest of the paper is organized as follows. Section 2 discusses recent literature
on lane detection. Section 3 formalizes the proposed method. Section 4 provides experiments and results. Section 5 provides concluding remarks.

2 Related Work
There have been extremely less attempts at predicting number of lanes and the position of the car on road in literature. There is a plethora of lane detection algorithms
using lane-marker detection. Lane markers are usually detected in a multi-stage approach, typically involving an intra-frame prediction accompanied by an inter-frame
adjustment. The first stage will usually comprise of a pre-processing to select some candidate regions and later stages will involve tracking-based methods to exploit the correlation between consecutive frames, thereby reducing false detections. Pre-processing
stage followed by tracking using Kalman filter was performed for detecting lanes in [6].
Huang et al, proposed a method where, a simple set of filters were used to detect candidate regions and second-order derivatives were used for further filtering [3]. A similar
idea was used in another lane-keeping paper, where the lane model was estimated rowby-row from the bottom of the image to the top. This way, the lower rows provided
reference to detect lane markers on higher rows [7]. The supportive argument for this
method was the fact that probabilities of finding lane-markers on lower end of the image in a front-view is more than the higher end. Many methods enforce road models

720

P.S. Chandakkar, R. Venkatesan, and B. Li

using temporal integration. In one such tracking-based method the existing lane model
was enforced on the new image and only newer or emerging lanes were considered [8].
Similarly, in another method, albeit not as strict, the existing lane model was used as a
constraint to find the lanes in the current frame in a Hough transform based setup [9]. A
simpler way of using a similar idea is to define a region of interest in the current frame
using the the predictions of the previous frames. Such an idea was discussed in [10].
Since lanes are typically lines or curves, the use of line fitting or grid fitting is quite
popular. This is done either in the original view or in a transformed view, usually using inverse perspective mapping (IPM). Parametric line-fitting is used in many methods
[11,12,13]. In these methods, after an initial detection of lane-markers, lines are fit using
various methods to each lane-lines. Although they work well, they are dependent on the
initial prediction. Roads are not typically straight and sometimes require curves to be
fit. Many methods in literature fit either splines or poly-lines to solve this issue [14,15].
The major problem in using active-contour models like b-snakes for lane detection is
that, they are computationally in-efficient in convergence to a final contour. They also
require seed points which are typically provided from edge maps or pre-assumed regions. Further more, even after the contouring of lane-markers, one still has to classify
contours between lanes and non-lanes. Beyond curve and line fitting is a grid-fitting
approach. One interesting method in the IPM view is the hierarchical bipartite graph
(HBPG) [16]. The major advantage of this approach is that the lane model is accompanied by a confidence measure for each lane. Another method finds and tracks multiple
lanes by using computer vision techniques and LIDAR [17]. These both are stable
systems for lane-marker detection. Both of them have their disadvantages. The former
method doesn’t detect lanes beyond a limit and the latter depends on expensive components such as LIDAR. Both methods do not attempt at predicting number of lanes or
lane number at which the car is currently being driven. An approach using Spatial RAY
features has tried to predict the position of the vehicle on road given a video stream
[18]. However, they predict at most 3 lanes either on the left or right hand side, whereas
we have upto 6 lanes in our database. They also have low traffic density and use IPM
for processing the data. We have normal-to-moderate traffic density and do not use IPM
which makes our method tolerant to different positions of the camera. Therefore, direct
comparison between these may not be appropriate.
Due to increase in computational capabilities and developments in fields such as
computer vision, a few learning based approaches have also been developed recently.
Patch-learning based approaches have also been developed to classify patches in images
as containing lane-markers. While most methods uses image gradients as features [19],
steerable filters based features have also proven reliable [20] for such patch based methods. Detection of lanes is further bettered by introducing learning of additional knowledge such as types of lanes by recognition of colors and structure of lane-lines [21].
Texture of the lane-markers were studied and along with a pre-defined lane-model ,and
further adjustments using Hough [22].
While innumerable methods of lane-marker detection, lane detection and lane modeling methods exist in literature, and a survey of all of them are beyond the scope of
this paper, a select few are surveyed to give the reader a general idea as to the directions

Video-Based Self-positioning for Intelligent Transportation Systems Applications

721

that the research in this field have so far taken. Albeit the all the effort in lane detection,
there are no solutions to predicting the number of lanes or self-positioning of vehicles
given a front-facing view.

3 Proposed Method
⎡

X1
⎢ X2
⎢
Assume that we have labeled data D = ⎢ .
⎣ ..

⎤
Θ1
Θ2 ⎥
⎥
.. ⎥ where Xi is the ith video clip
. ⎦

Xn Θn
and Θi is a 2D vector label [θ1 , θ2 ], where θ1 ∈ [1, L] is the number of lanes on the
observed video, L being the maximum number of lanes considered and θ2 ∈ [1, θ1 ] is
the lane at which the car is currently driving, and each video clip in Xi contains frames
[fi1 , fi2 , . . . , fim ]. The joint probability density of the proposed model for any frame frj
such that Xr ∈
/ D is P (frj , Θ|D, frj−1 , frj−2 , . . . , frj−b ), where [frj−1 , frj−2 , . . . , frj−b ]
is a buffered set of b frames immediately before frj .
The posterior density for the prediction of the label Θ̂ at frame frj given the previous
b frames and the training data D is, P (Θ|frj , frj−1 , frj−2 , . . . , frj−b , D). By Bayes rule,
P (Θ|fr , frj−1 , frj−2 , . . . , frj−b , D) ∝
P (fr , frj−1 , frj−2 , . . . , frj−b |Θ, D)P (Θ|D).

(1)

Assuming a uniform prior P (Θ|D), the maximum-a-posteriori problem becomes a
maximum-likelihood estimation problem as
Θ̂ = arg max P (frj , frj−1 , frj−2 , . . . , frj−b |Θ, D).
Θ

(2)

The weighted likelihood for each frame to be maximized is approximated as,
P (frj , frj−1 , . . . , frj−b |Θ, D) ≈
ΦT [P (frj |Θ, D), P (frj−1 |Θ, D), . . . P (frj−b |Θ, D)],

(3)

where Φ is a weighting kernel.
The advantage of modeling this formulation as a Bayesian framework is that it can
be easily expanded to include other parameters in the future, For example car dynamics
and higher-level knowledge about the road structure such as vehicle traveling directions
in various parts of the road. As the problem complexity grows and more data becomes
available, more classifiers/detectors/other small frameworks can be included as a part
of this bigger framework. This will help us in the future to create more accurate lane
assignments for vehicles. This formulation also helps us to update this likelihood for
every incoming frame as shown in equation 4. Although Φ can be any weighting kernel,
we consider an averaging kernel:
Φ=

1
· 1,
b+1

(4)

722

P.S. Chandakkar, R. Venkatesan, and B. Li

where 1 is a column vector of length b + 1. Averaging gives uniform weighting to the
current frame and all the frames in the buffer, thus helping us to compensate for false
predictions. This requires us to create a likelihood function P (f |Θ, D) for every frame
f and every label Θ.
3.1 The Likelihood Function
There are several stages in the estimation of the likelihood function, P (f |Θ, D) for
each frame f . These are:
1.
2.
3.
4.

Pre-processing using guided filter.
Lane-width modeling.
Feature extraction.
Estimation of P (f |Θ, D).

Guided Filtering. Guided filter is a linear-time, edge-preserving, smoothing filter [23].
It works with a pair of images. One of these images is a reference image, with the
guidance of which, the other is filtered. When both images are the same, the guided
filter smooths the entire image while preserving the edges. For the special case where
both the images are identical, the filtering is performed as follows:
G(f ) = ᾱf + β̄,

(5)

where f is a gray-scale video frame. ᾱ and β̄ are block-wise averaged values generated
σ2
and β = (1 − α)μ, where μ is the mean, σ is the standard deviation
from α = 2
σ +
for any block in the image and  is a small constant. If there is a flat patch in the image
then σ = 0 =⇒ α = 0 and β = μ. Each pixel in the flat region is thus averaged
with its neighbors. For a high variance region such as lane edges, σ   and hence
α ≈ 1 and β ≈ 0. This leaves the edges unchanged. A post-processing operation such
as over-subtracting and then saturating returns an image similar to the one shown in the
middle in Fig. 1. The post-processing operation can be represented as:
Binarizedf = [(f − δG(f )) ∗ 255] ≥ 0;

(6)

Fig. 1. From left to right: Captured frame, pre-processing using guided filter, processing using
Marcos’ filter

Video-Based Self-positioning for Intelligent Transportation Systems Applications

723

where δ > 1 (in this case, δ = 1.06) due to which the original image gets oversubtracted from itself. Since only sharp edges have retained their value in the filtered
image, other regions become negative after over-subtraction. This provides us a much
better frame to work with. Fig. 1 shows the application of the guided filter and compares it with Marcos’ filter, specially designed for lane detection [24]. The Marcos’
filtering process can be defined as follows:
yi = 2xi − (xi−τ + xi+τ ) − |xi−τ − xi+τ |

(7)

τ is the width parameter which controls the filtering process. The filtering process will
produce high response if the current pixel has high value and pixels at a distance of τ
on both sides have equal value. The lane pixels satisfy these criteria. However, due to
the width parameter, Marcos’ filter is not as reliable as guided filter in detecting the lane
markers in extreme left/right. Fig. 1 shows the effect of guided and Marcos’ filter on a
frame. The zoomed-in region shows that Marcos’ filter fails to detect the extreme left
lane. Guided filter also works in many weather conditions, for example, rainy day, lowsunlight conditions, low-contrast images etc. Section 4 compares the results obtained
with guided and Marcos’ filter.
Lane-Width Modeling. The pre-processing using guided filter provides us a frame
that contains all the lane markers and some spurious objects. Before extracting features,
these spurious regions may be filtered. Assuming that the lanes on the free-ways and
such are of a fixed width, the width of lanes, and thereby the probable regions for finding
lane markers in the video, can be modeled as a function of camera parameters and the
ordinates in the image plane with respect to an initial lane (center-lane in this case).
We first detect the lane-markers of the lane in which the car is present. The center
lane markers are detected from the pre-processed video using heuristics about its possible locations and low-level color and gradient features. A line is fit for left (cl [xl , yl ])
and right (cr [xr , yr ]) center-lane-lines individually along the detected lane-markers.
(xl , yl ) and (xr , yr ) are the points lying on the left and right lane markers of the center
lane respectively. Three additional lane-lines on the left and right of the center-lane are
modeled as:

Fig. 2. Lane width modeling

724

P.S. Chandakkar, R. Venkatesan, and B. Li

Fig. 3. Pixel types: The lane pixels in white are type-1. The pixels with blue cross mark are type2. Black pixels indicate the boundary of lane pixels. The rest are type-3 pixels. Please zoom in on
a computer monitor for better viewing.

li [xl , yl ] = dli (cl [xl , yl ]) = ψil yl + τil , ∀i ∈ [1, 2, 3],
and,

(8)

ri [xr , yr ] = dri (cr [xr , yr ]) = ψir yr + τir , ∀i ∈ [1, 2, 3].

(9)
th

where li [xl , yl ] and ri [xr , yr ] are the x and y coordinates lying on the i left and
right of the center lane markers respectively (also shown in Fig. 2). dli (cl [xl , yl ]) and
dri (cr [xr , yr ]) are the horizontal distances of the li and ri lane markers from cl and
cr respectively. yl and yr are the y-coordinates of the left and right lane markers.
{ψil , ψir , τil , τir } are the slope and intercept parameters of the left and right lane markers.
Feature Extraction. Fig. 2 shows the lane-lines that are modeled for L = 7. For simplicity, we assume a constant camera position. Equation 8 and 9 model the distance
between all the left (right) lane markers and the center-left (center-right) lane marker as
a function of only the y-coordinate in the image. Other parameters such as the camera
parameters can also be included. Similarly, although it was sufficient for us to model the
displacement as a linear function, other kernel representations may also be used. The
intersection of these regions is the vanishing point, which can act as an additional parameter in this formulation. The vanishing point is the intersection of the lines {li }3i=1 ,
{rj }3j=1 , cl and cr as shown in Fig. 2.
For every region chosen by the lane-width model, we classify every pixel in the
region into type-1, type-2 or type-3 pixels. The pixels given by the guided filter are type1 pixels. The pixels that are just outside the type-1 pixels and along the direction of the
gradient of the nearest type-1 pixel are the type-2 pixels. Every other pixel in the region
is classified as type-3. Type-1 and type-2 pixels are mean centered. We empirically
observed that subtracting the mean from the pixels provides robustness against varying
lighting conditions. Fig. 3 shows the various types of pixels.
For every region now we extract the following 40 dimensional feature:
1. First and second moments of the type-1 pixels (2 − D).
2. First and second moments of the type-2 pixels (2 − D).
3. 36-bin histogram of gradients of type-1 pixels (36 − D).

Video-Based Self-positioning for Intelligent Transportation Systems Applications

725

Fig. 4. Distribution of Video Frames in Train and Test Set

We neglect the type-3 pixels as they usually constitute anomalies with the road, like
shadows, repair and vehicles. This gives us a 40 − D vector for each lane. First four
features act as low-level color features by encoding mean and standard deviation of
lane and road pixels respectively. The remaining features describe the shape and the
color contrast of the contour under consideration. A lane line would have most of the
gradients pointing only in one direction (treating two opposite directions as one) and
with high contrast and thus high magnitudes. We found these low-level statistics enough
to describe lane-lines well. The maximum number of lanes we consider are L = 7. We
model all the lanes except the lane in which the car is present. Therefore our feature
dimensionality is 240.
Estimation of P (f |Θ, D). A linear SVM is constructed using the features extracted
from D. For any test frame, the same set of extracted features are projected onto the
SVM feature space. The likelihood estimate P (f |Θ, D) is the confidence of prediction
for each Θ. It is measured as the distance of the data-point to the hyper-plane. The
likelihood extracted is then fed into the MLE system for prediction.

4 Experiments and Results
The videos for this experiment have been captured using a camera mounted on the inside of the wind-shield. Our data set contains 53 videos, captured at 30 frames/second3.
The average length of each clip is 11.8 seconds. Though there are many possible configurations with L = 7, we consider only a subset of frequently occurring configurations.
The following configurations exist in our database: Θ = {[4, 1], [4, 2], [4, 3], [4, 4], [5, 2],
[5, 3], [5, 4], [6, 4]}. Uniform prior is assumed among all these configurations and 0 prior
for configurations that are not considered. We use 27 out of 53 videos in our database
3

The dataset is available at http://www.public.asu.edu/ bli24/
CodeSoftwareDatasets.html

726

P.S. Chandakkar, R. Venkatesan, and B. Li
Table 1. Per-class accuracy
Guided filter
Class = [θ1 , θ2 ]

RF

Lin. SVM

Model + Lin. SVM
(Proposed method)

[4, 1]
[4, 2]
[4, 3]
[4, 4]
[5, 2]
[5, 3]
[5, 4]
[6, 4]
Overall Accuracy

30.22%
76.94%
55.22%
98.98%
36.94%
63.99%
46.46%
96.80%
56.37%

46.67%
73.27%
68.75%
79.52%
56.48%
53.30%
39.06%
96.80%
58.33%

44.27%
80.03%
71.66%
82.59%
63.25%
51.70%
39.94%
100%
60.15%

Marcos’ filter+
+Model+RF
(Best combination)
30.48%
38.47%
57.29%
21.84%
47.40%
59.32%
59.91%
98.17%
51.76%

for training purposes and the rest for testing purposes. The total number of frames in
training and testing set are 9018 and 9036. The distribution of video frames over all
configurations in training and testing set can be seen in Fig. 4. Each configuration contains videos of varying lighting, weather and surface conditions. In this problem, the
training data has noise embedded in it. Many frames contain vehicles which occlude
the full view of all the lanes. These external factors increase the difficulty.
The results are presented in two-fold. Table 1 compares the per-class accuracy obtained using guided and Marcos’ filter. Two classifiers, Linear SVM and Random Forest
(RF) have been used. Due to the uneven distribution of data in each class, weights in
RF have been adjusted accordingly. For Marcos’ filter, best performance has been reported. Due to Marcos’ filter’s inability to detect extreme lanes, it has poor accuracy
when compared to guided filter. Table 1 also shows the per-class accuracy of detection
for both the prediction from the SVM stage and the improvements seen using the MLE
model. Table 2 shows the confusion matrix for the initial prediction and Table 3 shows
the confusion matrix for the final prediction by MLE. From the confusion matrices, it
can be seen that the MLE estimator betters the initial prediction in all but two configurations. The decrease in the performance occurs when SVM wrongly predicts for at least
Table 2. Confusion matrix for SVM prediction
Class = [θ1 , θ2 ] [4, 1]
[4, 1]
505
[4, 2]
6
[4, 3]
112
[4, 4]
0
[5, 2]
11
[5, 3]
0
[5, 4]
11
[6, 4]
0

[4, 2]
399
899
181
39
127
569
448
3

[4, 3]
96
52
1560
13
69
166
268
0

[4, 4]
0
0
8
233
10
0
14
0

[5, 2]
58
231
1
3
292
0
6
0

[5, 3] [5, 4]
24
0
14
2
308 99
0
4
0
8
863 2
4 707
0
4

[6, 4]
0
23
0
1
0
19
352
212

Video-Based Self-positioning for Intelligent Transportation Systems Applications

727

Table 3. Final Confusion Matrix
Class = [θ1 , θ2 ] [4, 1]
[4, 1]
479
[4, 2]
3
[4, 3]
112
[4, 4]
0
[5, 2]
9
[5, 3]
0
[5, 4]
14
[6, 4]
0

[4, 2]
391
982
170
31
119
614
494
0

[4, 3] [4, 4]
134 0
27
0
1626 0
13 242
45
8
168 0
232 0
0
0

[5, 2]
63
215
0
0
327
0
0
0

[5, 3] [5, 4] [6, 4]
15
0
0
0
0
0
266 95
0
7
0
0
0
9
0
837 0
0
0 723 347
0
0 219

b/2 consecutive frames with high probability. Increasing the value of b increases the accuracy but it also delays the determination of the current frame configuration. We input
previous 10 frames (b = 10) to the MLE estimator. Also due to averaging kernel, there
is a smooth transition in the likelihood when the configuration changes. Therefore, first
b/2 frames may be predicted wrongly which may decrease the final accuracy. While the
configurations [4, 1], [5, 4] have low accuracy, it should be considered that there are 3
lanes on one of the sides of the current lane. The presence of the third lane is difficult
to detect given that the lane is not more than 4 pixels wide in the frame. The approach
also has to deal with broken and faint lane lines on the road in addition to occlusion.
Interestingly, the configuration [6, 4] achieves 100% accuracy despite having the most
number of lanes. We would like to point out the fact that the [6, 4] configuration has the
best weather and relatively less occlusion from vehicles. It shows that our approach can
achieve excellent results if the road conditions, weather and the traffic density permits.
The final confusion matrix in Table 3 shows that two lowest performing configurations are [5, 4] and [4, 1]. Bad weather conditions and the presence of extreme lane
in both the configurations, as shown in Fig. 5, reduces the classification performance.
This is an eight-class classification problem and the variable weather conditions, road

Fig. 5. From left to right, row 1: Correct predictions for configurations [4, 4], [5, 2] and [6, 4]. Row
2: Wrong predictions for configurations [4, 1], [5, 3] and [5, 4]. Occlusion, bad lighting conditions
and bad road conditions are the plausible causes for wrong detections.

728

P.S. Chandakkar, R. Venkatesan, and B. Li

conditions and inherent noise in the training data makes the problem challenging. As
shown in Fig. 5, the proposed approach is able to handle partial occlusions and bad road
conditions. Still many improvements can be made to the current approach. Apart from
the lane-width modeling, it can incorporate other semantic knowledge such as the vehicle movement, vehicle detection to assist lane detection etc. The current approach does
not perform so well in bad road conditions such as broken lanes or disappeared lane
markers. More sophisticated tracking along with accelerometer and a better lane-model
can be used to overcome these problems.

5 Conclusion and Future Scope
A novel problem of predicting the number of lanes from a video and self-positioning of
the car in those lanes was introduced and a high-level model based approach is developed. This video-based self-positioning approach provides the system an ability to perform dynamic traffic routing on a lane-level. This would possibly contribute to greater
reduction of congestion and thus result in faster travel times. This was formulated as
a top-down maximum-likelihood estimator. The likelihood of the two parameters were
modeled using the confidence of a low-level predictor and guided-filtering. Lane-width
modeling and pixel-level statistics were used to choose candidate-lane regions before
the prediction. Testing this framework on real-world videos yielded satisfactory results.
The system presently accommodates just the video data to perform prediction. Our
aim is to expand this model to include vehicle dynamics and GPS information. Vehicle
dynamics which includes (but not limited to) accelerometer data, speed information etc.
can be used as a high-level knowledge to filter out obvious false detections. To increase
reliability of prediction on the roads with large number of lanes, we may obtain the
number of lanes as an input. Our ultimate aim remains to integrate all these systems to
build a lane-level dynamic traffic routing system.
Acknowledgement. The work was supported in part by a grant from the National
Science Foundation. Any opinions expressed in this material are those of the authors
and do not necessarily reflect the views of the NSF.

References
1. Zheng, H., Chiu, Y.C., Mirchandani, P.B.: On the system optimum dynamic traffic assignment and earliest arrival flow problems. Transportation Science (2013)
2. Angel, A., Hickman, M., Mirchandani, P., Chandnani, D.: Methods of analyzing traffic imagery collected from aerial platforms. IEEE Transactions on Intelligent Transportation Systems 4, 99–107 (2003)
3. Kammel, S., Pitzer, B.: Lidar-based lane marker detection and mapping. In: 2008 IEEE Intelligent Vehicles Symposium. IEEE (2008)
4. Huang, A.S., Moore, D., Antone, M., Olson, E., Teller, S.: Finding multiple lanes in urban
road networks with vision and lidar. Autonomous Robots 26, 103–122 (2009)
5. Hillel, A.B., Lerner, R., Levi, D., Raz, G.: Recent progress in road and lane detection: a
survey. Machine Vision and Applications, 1–19 (2012)

Video-Based Self-positioning for Intelligent Transportation Systems Applications

729

6. Nieto, M., Laborda, J.A., Salgado, L.: Road environment modeling using robust perspective
analysis and recursive bayesian segmentation. Machine Vision and Applications 22, 927–945
(2011)
7. Wu, S.J., Chiang, H.H., Perng, J.W., Chen, C.J., Wu, B.F., Lee, T.T.: The heterogeneous systems integration design and implementation for lane keeping on a vehicle. IEEE Transactions
on Intelligent Transportation Systems 9, 246–263 (2008)
8. Lipski, C., Scholz, B., Berger, K., Linz, C., Stich, T., Magnor, M.: A fast and robust approach
to lane marking detection and lane tracking. In: IEEE Southwest Symposium on Image Analysis and Interpretation, SSIAI 2008, pp. 57–60. IEEE (2008)
9. Cheng, H.Y., Jeng, B.S., Tseng, P.T., Fan, K.C.: Lane detection with moving vehicles in the
traffic scenes. IEEE Transactions on Intelligent Transportation Systems 7, 571–582 (2006)
10. Labayrade, R., Douret, J., Laneurit, J., Chapuis, R.: A reliable and robust lane detection
system based on the parallel use of three algorithms for driving safety assistance. IEICE
Transactions on Information and Systems 89, 2092–2100 (2006)
11. Kong, H., Audibert, J.Y., Ponce, J.: Vanishing point detection for road detection. In: IEEE
Conference on Computer Vision and Pattern Recognition, CVPR 2009, pp. 96–103. IEEE
(2009)
12. Alon, Y., Ferencz, A., Shashua, A.: Off-road path following using region classification and
geometric projection constraints. In: IEEE Computer Society Conference on Computer Vision and Pattern Recognition, vol. 1, pp. 689–696. IEEE (2006)
13. Zhang, G., Zheng, N., Cui, C., Yan, Y., Yuan, Z.: An efficient road detection method in
noisy urban environment. In: 2009 IEEE Intelligent Vehicles Symposium, pp. 556–561. IEEE
(2009)
14. Sawano, H., Okada, M.: A road extraction method by an active contour model with inertia
and differential features. IEICE Transactions on Information and Systems 89, 2257–2267
(2006)
15. Wang, Y., Teoh, E.K., Shen, D.: Lane detection and tracking using b-snake. Image and Vision
Computing 22 (2004)
16. Nieto, M., Salgado, L., Jaureguizar, F.: Robust road modeling based on a hierarchical bipartite graph. In: 2008 IEEE Intelligent Vehicles Symposium, pp. 61–66. IEEE (2008)
17. Huang, A.S., Moore, D., Antone, M., Olson, E., Teller, S.: Finding multiple lanes in urban
road networks with vision and lidar. Autonomous Robots 26, 103–122 (2009)
18. Kuhnl, T., Kummert, F., Fritsch, J.: Visual ego-vehicle lane assignment using spatial ray
features. In: 2013 IEEE Intelligent Vehicles Symposium (IV), pp. 1101–1106. IEEE (2013)
19. Samadzadegan, F., Sarafraz, A., Tabibi, M.: Automatic lane detection in image sequences
for vision-based navigation purposes. In: Proceedings of the ISPRS Commission V Symposium’Image Engineering and Vision Metrology (2006)
20. McCall, J.C., Trivedi, M.M.: Video-based lane estimation and tracking for driver assistance:
survey, system, and evaluation. IEEE Transactions on Intelligent Transportation Systems 7
(2006)
21. Collado, J.M., Hilario, C., de la Escalera, A., Armingol, J.M.: Adaptative road lanes detection and classification. In: Blanc-Talon, J., Philips, W., Popescu, D., Scheunders, P. (eds.)
ACIVS 2006. LNCS, vol. 4179, pp. 1151–1162. Springer, Heidelberg (2006)
22. Zhou, S., Jiang, Y., Xi, J., Gong, J., Xiong, G., Chen, H.: A novel lane detection based on
geometrical model and gabor filter. In: 2010 IEEE Intelligent Vehicles Symposium (IV),
pp. 59–64. IEEE (2010)
23. He, K., Sun, J., Tang, X.: Guided image filtering. IEEE Trans. Pattern Anal. Mach. Intell. 35,
1397–1409 (2013)
24. Nieto, M., Laborda, J.A., Salgado, L.: Road environment modeling using robust perspective
analysis and recursive bayesian segmentation. In: Machine Vision and Applications, vol. 22
(2011)

2015 IEEE Winter Conference on Applications of Computer Vision

Improving Vision-based Self-positioning in Intelligent Transportation Systems
via Integrated Lane and Vehicle Detection
Parag S. Chandakkar, Yilin Wang and Baoxin Li, Senior Member, IEEE
School of Computing, Informatics and Decision Systems Engineering
Arizons State University, Tempe.
{pchandak,ywang370,baoxin.li}@asu.edu

Abstract

muter, freeway travel time index (FTTI) etc. Large urban
areas with more than 3 million population suffer an average of 52 hours of delay per year per auto commuter. Each
commuter also has to bear the congestion cost of $1128 per
year. Since freeway travel is a large part of our daily commute, experts have developed measures such as FTTI. The
average value of FTTI in 2011 for the freeways in large urban cities was 1.31, which implies the freeway travel duration was increased by factor of 1.31 per auto commuter 3 .
The overall congestion cost and delay for 498 metropolitan
areas in the United States was $121 billion and 5.5 billion
hours 4 . The problem of trafﬁc congestion is also slowly
spreading to small cities as well as rural areas.
To remedy this situation, Federal Highway Administration (FHWA) is trying to implement various policies such as
dynamic trafﬁc signal timings and varying tolls and pricing
for roads with different levels of activity 4 . To apply these
strategies, it is essential to know the state of the trafﬁc on
the freeway at any given instant of time. Currently, stationary loop detectors carry out the task of estimating the trafﬁc ﬂow at certain checkpoints with certain accuracy. However, they have reliability issues and they cannot estimate
the ﬂow of trafﬁc on a ﬁner resolution level, e.g. lane-level
trafﬁc ﬂow. Deploying loop detectors is expensive too [9].
In recent years, there has been a tendency to rely on smart
ubiquitous devices such as mobile sensors [9] or GPS data.
Though these techniques increase the overall accuracy, they
cannot provide enough resolution of the trafﬁc density.
Intelligent transportation system (ITS) research analyses the trafﬁc ﬂow and provides necessary feedback, mainly
through varying congestion pricing and toll. The envisioned
future for ITS systems is that they should be able to collect
the information on lane-level and price lanes accordingly
for platoons of cars entering and exiting the freeways at a
given time [26]. This requires estimation of the number

Trafﬁc congestion is a widespread problem. Dynamic
trafﬁc routing systems and congestion pricing are getting
importance in recent research. Lane prediction and vehicle density estimation is an important component of such
systems. We introduce a novel problem of vehicle selfpositioning which involves predicting the number of lanes
on the road and vehicle’s position in those lanes using
videos captured by a dashboard camera. We propose an
integrated closed-loop approach where we use the presence of vehicles to aid the task of self-positioning and viceversa. To incorporate multiple factors and high-level semantic knowledge into the solution, we formulate this problem as a Bayesian framework. In the framework, the number of lanes, the vehicle’s position in those lanes and the
presence of other vehicles are considered as parameters.
We also propose a bounding box selection scheme to reduce
the number of false detections and increase the computational efﬁciency. We show that the number of box proposals
decreases by a factor of 6 using the selection approach. It
also results in large reduction in the number of false detections. The entire approach is tested on real-world videos
and is found to give acceptable results.

1. Introduction
The United States has 786 motor vehicles per 1000 people, which ranks as third highest in the world 1 . It has an estimated total of 253.1 million registered vehicles as of 2011
according to Bureau of Transportation Statistics 2 . The vehicle ownership in the United States has been on constant
rise with some occasional ﬂuctuations. Trafﬁc congestion
has always been a severe problem. There are various measures developed to quantify the problem of trafﬁc congestion, e.g. congestion cost and yearly hours of delay per com1 Statistics
2 Statistics

3 Congestion

statistics taken from http://mobility.tamu.
edu/ums/national-congestion-tables/
4 Nationwide congestion information taken from http://www.
fhwa.dot.gov/congestion/

taken from http://data.worldbank.org/
taken from http://www.rita.dot.gov/bts/

978-1-4799-6683-7/15 $31.00 © 2015 IEEE
DOI 10.1109/WACV.2015.60

404

of lanes on the road and the position of the vehicle within
those lanes. We call the process of determining these two
parameters as vehicle self-positioning. To calculate the (approximate) density of the vehicles, a detection module also
needs to be in place. Hardware-dependent systems such as
LIDAR and those using GPS are present in literature, but
they are costly and are not usually preferred [13, 11].
Video-based lane and vehicle detection is popular and
well-researched [10, 22]. Though even simple solutions
such as Hough transform can extract the lane markers, inferring the road structure from them has proven difﬁcult.
The stand-alone problem of self-positioning was studied in
[2] and initial ﬁndings were reported. In this paper, we extend the problem so as to perform self-positioning and vehicle detection in a closed-loop system such that each process
aids the other one and in-turn increases efﬁciency of the entire system. The primary purpose of this system is not to
extract lane markers or vehicle detection, though we consider them as sub-problems, but to perform accurate selfpositioning with the aid of vehicles present on the road. We
propose a Bayesian model which takes three parameters:
number of lanes on the road and the lane in which vehicle is being driven, the presence of a vehicle. To construct
the likelihoods of this model, we use a bottom-up approach
which uses guided-ﬁlter, lane-model generation and a vehicle detection module. To allow further exploration of this
problem as well as to improve and verify current techniques,
we have made the database publicly available.
The organization of rest of the paper is as follows. Section 2 discusses recent relevant literature on lane and vehicle
detection and self-positioning. Section 3 describes the proposed approach in detail. Section 4 explains the experimental setup and results. Section 5 gives concluding remarks
and outlines the future scope for the current system.

extract the lane markers [14, 1, 25]. Kalman ﬁlter is also
used for tracking to minimize the effect of false detections
[18]. In another tracking-based method, the lane model
generated from previous frame was enforced onto the new
frame and only newly appearing lanes were considered [16].
Since roads are not straight, some methods have tried to ﬁt
splines or higher-order curves to get an accurate representation of lane markers [23, 20]. However, ﬁtting higher-order
curves or B-snakes may be computationally inefﬁcient. Another interesting method which can be used to detect more
than one lane (host-lane) is the hierarchical bi-partite graphbased road modeling [19]. This approach outputs multiple lanes and tries to understand the road structure. This
approach also assigns a conﬁdence measure for each lane.
However, this method does not attempt to position the vehicle in the lanes.
A similar problem to self-positioning has been handled
in [15] using spatial RAY features. It tries to predict the position of the vehicle on the road from an input video stream.
However, they predict at most 3 lanes at a time either on
the left or right hand side, whereas we have upto 6 lanes in
our database with upto 3 lanes being on each side. In addition, there experiments are spread over only 2 days with
similar road conditions. Our experiments are more rigorous, spreading over 5 days and the data includes 6 different
road conditions. They have low trafﬁc density whereas we
have moderate-to-high trafﬁc density. Thus direct comparison between these two approaches may not be possible.
Due to recent progress in computer vision research and
computational abilities, patch-learning based approaches
classify image patches depending on the presence of lanemarkers. Gradients and steerable Gaussian ﬁlters act as
good features for such patch-based methods [17]. Attempts
are also made to learn the road structure by collecting
knowledge about the type and structure of lane-markers [4].

2. Related Work
Lane and vehicle detection are sub-tasks within our current problem formulation. Very few attempts have been
made in the past to perform self-positioning, however there
are many algorithms reported in the literature which handle
the problem of lane and vehicle detection separately. Both
software and hardware-dependent approaches have been developed for the said tasks. Here we focus only on visionbased approaches.
Lane detection includes multiple stages, namely, image
pre-processing, feature extraction, model-ﬁtting for further
veriﬁcation and tracking to maintain spatio-temporal consistency. One of the most popular pre-processing methods
is inverse perspective mapping (IPM) which maps an image
to a bird’s eye-view. This makes the lane markers appear
straight and parallel to each other. Also, other ﬁne details in
the image get suppressed. Next step is usually simple morphological operation followed by parametric line ﬁtting to

Vehicle detection is a subset of a widely-studied problem of object detection. There are a plethora of approaches for general object detection, e.g. [6], or approaches dedicated to vehicle detection which use opticalﬂow and hidden-markov-model based classiﬁcation to interpret motion-based clues [12]. HOG and HAAR features
have also been used with different classiﬁers and learning frameworks such as adaboost, SVM and active learning
[21, 24]. Though there are innumerable methods developed
for general object detection and speciﬁcally for vehicle detection, listing them all is out of the scope of this paper.
A good review of vehicle detection methods can be found
in [22]. In spite of continuous efforts, there are no reliable
vision-based solutions to predicting number of lanes and vehicle self-positioning given a front-facing view.
405

3. Proposed approach

problem as,
Θ̂ = argmax P (fkj , . . . , fkj−p |Θ, D, Vi )

In this section, we propose an integrated approach to
solve the problem of vehicle self-positioning. Apart from
external factors such as bad weather and conditions of lane
markings, lane-occlusion by passing vehicles is one of the
biggest hurdles to reliable vehicle self-positioning. Our
approach works in a closed loop. We utilize information
from the positions of other vehicles to improve our selfpositioning. The self-positioning information gives us information about the road structure, which we use to generate the detection proposals for vehicles. We also employ
temporal smoothing of results across frames to counter the
effect of unexpected events, e.g. a car stopped on the shoulder or faint/invisible lane markings.

Θ

≈ argmax ψ T [P (fkj |Θ, D, Vi ) . . . P (fkj−p |Θ, D, Vi )]
Θ

The temporal smoothing kernel takes the form of a slowly
increasing exponential function as shown in the following
equation.
ψ(i) = initial value × (1 + rate-of-increase)i
∀ i ∈ [1, p + 1]

where i is the frame index and p is the number of frames
we have buffered. The values are normalized so that the
(p + 1)th frame (which is also the current frame) gets a
unit weight. Now, expanding only the ﬁrst term from the
previous step (for readability purposes), we get,

We incorporate this problem in a Bayesian framework
to add the additional factors such as presence of vehicles, vehicle dynamics, scene
etc. Assume we
⎤
⎡ semantics
F1 Θ1
⎢ F2 Θ2 ⎥
⎥
⎢
have labeled data D = ⎢ .
.. ⎥ where Fi is the ith
⎣ ..
. ⎦
Fn Θn
video clip and Θi is a 2 − D label vector [θ1 , θ2 ] where
θ1 ∈ [1, nlanes ] is the number of lanes present in Fi . The
other parameter θ2 ∈ [1, θ1 ] is the lane in which the car
is currently driving (assuming the leftmost lane is the ﬁrst
lane). The process of determining the above label vector for each frame is called self-positioning. Each video
clip Fi contains m frames - [fi1 , fi2 , . . . , fim ]. We include
the vehicle presence in the Bayesian framework in order
to aid self-positioning as follows. The joint probability
density for any frame fkj such that the video clip Fk belongs to the test data is P (fkj , Θ|D, fkj−1 , . . . , fkj−p , Vi )
where [fkj−1 , fkj−2 , . . . , fkj−p ]] is a set of p frames occurring before fkj and Vi denotes the vehicle presence in
lane i. We need to predict the label vector Θ̂ for current
frame fkj given the set of previous p frames, the training
data D and vehicle presence in a lane i can be written as,
P (Θ|D, fkj−1 , . . . , fkj−p , Vi ). By Bayes’ rule it can be further decomposed into,

Θ̂ = argmax ψ T P (fkj |Θ, D, Vi ) P (Θ|Vi , D)
Θ

= argmax ψ T P (fkj |Θ, D, Vi ) P (θ1 , θ2 |Vi , D)
Θ

The determination of our current lane does not depend on
the vehicle presence in a lane since we do not incorporate
view-point information of a detected object. Thus we take
P (θ2 |Vi , D) as constant and remove it from the equation.
Θ̂ = argmax ψ T P (fkj |Θ, D, Vi ) P (θ1 |Vi , θ2 , D)
Θ

The term P (fkj |Θ, D, Vi ) involves calculating features of
the current frame given the label vector and the vehicle detection result. However, the detection result only affects
the label vector, and in particular θ1 . The feature extraction process and the vehicle detection process run parallely
and so they are independent of each other. By deﬁnition of
θ1 and θ2 , we see that though θ2 is weakly conditioned on
θ1 as θ2 ∈ [1, θ1 ], vice versa does not hold (i.e. the current
lane in which we are driving does not determine the number
of lanes on the road). Therefore we remove the condition
of θ2 from the last term. We also re-introduce the previously skipped terms containing frames [fkj−1 , . . . , fkj−p ] in
the following equation.
Θ̂ = argmax ψ T [P (fkj |Θ, D) P (θ1 |Vi , D) · . . . ·

P (Θ|D, fkj−1 , . . . , fkj−p , Vi ) ∝
P (fkj−1 , . . . , fkj−p |Θ, D, Vi )P (Vi |Θ, D).

(2)

Θ

(1)

P (fkj−p |Θ, D) P (θ1 |Vi , D)].

(3)

Equation 3 shows a general formulation to obtain the correct value of the label vector given a set of buffered frames
and the vehicle detection results. The structured Bayesian
formulation allows easy modiﬁcation of the formulation if
we were to add more information to the model in the future.
In the next paragraphs, we show how to calculate the two
quantities - P (fkj |Θ, D) and P (θ1 |Vi , D).

We assume a uniform prior on presence of vehicles.
Also, we introduce a temporal smoothing kernel which considers the results of previous frames. This helps to minimize
the effect of sudden external factors as explained before.
The above maximum-a-posteriori (MAP) problem can be
formulated into a maximum-likelihood estimation (MLE)
406

Figure 1: Captured frame and pre-processing using guided ﬁlter.
ᾱ and β̄ are obtained through block-wise averaging using
σ2
and β = (1 − α)μ, where μ is the mean, σ is
α= 2
σ +
the standard deviation of a block in the image and  is a
small constant. For a ﬂat patch in an image, σ = 0 =⇒
α = 0 and β = μ. Thus each pixel in that ﬂat region is
averaged with its neighbors. Similarly it can be proved that
if σ  , sharp edges are preserved. However, by choosing
 appropriately, we can force the guided ﬁlter to consider
almost all the pixels in an image belonging to a ﬂat patch.
We choose  high enough such that lanes are considered
belonging to a ﬂat patch. The lane markings are always
surrounded by the road pixels which have much lower value
and therefore, the value of lane pixels decreases by a large
amount as compared to anywhere else in the image. Now a
post-processing operation such as over-subtracting followed
by saturation returns an image similar to the one shown in
Fig. 1. The post-processing operation we use is as follows:

3.1. Frame likelihood computation
The frame likelihood computation involves extracting
features from the current frame and then calculating an initial estimate of the label vector, which is reﬁned later by
using vehicle detection results. It includes the following
stages:
1. Image pre-processing.
2. Lane model generation.
3. Feature extraction.
4. Initial estimation of frame likelihood.
Image pre-processing: Our aim is to detect all the lanes
present in the frame in order to compute the label vector Θ.
Pre-processing an image removes the unnecessary details in
it and keeps all the lanes. We choose Guided ﬁlter for this
task. It is a edge-preserving smoothing ﬁlter which works
in linear time [8]. It takes a pair of images as input. One
of them acts as a reference image and “guides” the ﬁltering
process of the other image. When both images are same,
the ﬁlter performs edge-preserving smoothing. The ﬁltering
operation is deﬁned as:
GF (I) = ᾱI + β̄,

BW (I) = [(I − δ · GF (I)) ∗ 255] ≥ 0,

(5)

where δ is a constant just greater than 1 (here, δ =
1.06). Since value of lane marking pixels has reduced by
a large extent, they get preserved in the post-processed image whereas many other regions disappear. Advantage of
guided ﬁlter is that it can also work in bad weather conditions such as rain, low-sunlight or in the night. It can also
detect extreme lanes which are very thin as shown in Fig. 1.

(4)

where I is a gray-scale video frame.

Lane model generation: The pre-processed image preserves all the lanes and rejects a lot of unwanted regions.
Yet, there are many spurious objects which may prevent us
from performing reliable detection of all the lanes on the
road. Additionally, as mentioned before, our aim is to perform self-positioning only on the freeways which have a
deﬁned road structure such as constant-width lanes. Taking advantage of these constraints, we deﬁne a lane model
in which there are a maximum of seven lanes, three being
on each side of the center lane. Due to ﬁxed-width of the
lanes, they can be represented as a function of y-coordinate
of the center lane markers and the camera parameters. We
ﬁrst detect the center lane markers in an image using simple

Figure 2: Lane width modeling

407

3.2. Reﬁnement of frame likelihood

thresholding techniques and other heuristics depending on
their possible position, shape and color. We then perform
a linear ﬁt for the left and right center lane marker denoted
respectively by cl and cr . Then the remaining three lane
markers on each side can be obtained as:
li = dli (cl ) = mli ycl + kil , ∀i ∈ [1, 2, 3]

(6)

ri = dri (cr ) = mri ycr + kir , ∀i ∈ [1, 2, 3]

(7)

We use the presence of vehicles in the adjacent lanes to
our advantage. The term P (θ1 |Vi , D) shows that the vehicle
presence in lane i affects the probability of the number of
total lanes on the road. Assume an initial estimate of the
label vector Θ̂init = [θˆ1 init , θˆ2 init ]. Here, θˆ1 init and θˆ2 init
are the initial estimates for the number of lanes and the host
lane respectively. From Θ̂init we can obtain indices of lanes
which are present, e.g. if Θ̂init = [5, 2] then the lane indices
are Lind = {l3 , cl , cr , r3 , r2 , r1 } or {3, 4, 5, 6, 7, 8} (refer
Fig. 2). Depending on the vehicle presence in lane i, we
reﬁne Θ̂init in the following manner:

and,

Above two equations represent the other lanes in the
form of offsets from the center lane - dli (cl ) and dri (cr ).
These offsets are in turn represented as a linear function
of the y-coordinate the center lane markers - ycl and ycr .
{mli , mri , kil , kir } are the slope and intercept parameters of
cl and cr respectively. Once these offsets have been calculated, then obtaining (x, y) coordinates of lane-markers is
a straight-forward task. We assume camera parameters are
ﬁxed, but they can also be included in equation 6 and 7. The
generated lane model is shown in Fig. 2.

⎧
ˆ
⎪
if min(Lind ) ≤ θˆ1 ≤ max(Lind )
⎨θ1 init
ˆ
ˆ
θ1 new = θ1 init + (min(Lind ) − i)
if 1 ≤ i < 4
⎪
⎩ˆ
if 5 < i ≤ 8
θ1 init + (i − max(Lind ))
(8)
This requires a reliable vehicle detector with high quality
object proposals to start with. In the next section we outline
a method to generate high-quality object proposals by using
the lane structure shown in Fig. 2.

Feature extraction: Once we have found the probable lane
regions as shown in Fig. 2, we ﬁnd lane pixels by simple
thresholding. By following the gradient directions at those
lane pixels, we can get the road pixels as shown in Fig. 3.
We form a 40-dimensional feature vector consisting of:

3.3. Efﬁcient vehicle detection
Our proposed approach works in a closed loop such that
it ﬁrst detects the vehicles present in the frame which helps
us to perform self-positioning task. Then the feedback from
the inferred lane structure is used to generate high-quality
object proposals. The obtained detections from these proposals then traverse to the beginning of the loop.
We make a key observation from Fig. 2 that the span
of lower edge of a bounding box enclosing a vehicle is always contained with the two corresponding lane markers.
As shown in Fig. 2, bounding boxes having a span too small
(Box-1) or too large (Box-4) than their corresponding lane
markers are invalid and are rejected in our proposal generation process. We now show a sample process of calculating
the span between the lane markers and also focus on bounding box selection process.
Consider
a
bounding
box
represented
by
To ﬁnd the span between
[xmin , ymin , xmax , ymax ].
its corresponding lane markers, we ﬁrst need to ﬁnd
their (x, y) coordinates. Since any lane marker can be
represented in as a function of the center lane markers, their
coordinates (xc , yc ) can be found by setting yc = ymax and
xc can be found using the linear ﬁt. The nearest lane marker
to (xmin , ymax ) is then found. The coordinates of that lane
marker (xnear , ynear ) are found by setting ynear = ymax
and xnear = xcr + rj . If the nearest lane marker is to the
left then it is similarly obtained as, xnear = xcl − lj (please
refer to equation 6 and 7). Once the coordinates for the
nearest lane marker are found, then the span between that
and the next marker can be easily calculated.

1. Mean and variance of lane and road pixels (4-D).
2. 36-bin histogram of gradients at lane pixels as shown.
The lane markers will have low mean and variance for
road pixels and the majority of the gradients at lane pixels lie in a speciﬁc range of angles (shown in Fig. 3). We
consider at most 7 lanes (or 8 lane markers) at a time, upto
3 lanes being on either side. Presence of middle lane is assumed. Thus our feature vector is 40∗6 = 240 dimensional.
Estimation of P (f |Θ, D): Though there are many methods to implement the general formulation presented in equation 3, we choose to implement it using a linear SVM. It is
trained using the features extracted from D. For a video
frame in the test data, we extract its features and then apply the linear SVM. The likelihood estimate L(Θ|f, D) is
the probability estimate of the linear SVM for each Θ. We
repeat the same procedure with random forest too.

Figure 3: Obtaining road pixels from lane pixels

408

Figure 4: From left to right, row 1: Correct predictions for categories [5, 3], [5, 4] and [6, 4]. Category of the middle image is
predicted correctly in spite of a vehicle occluding the view of ﬁrst lane. Row 2: Wrong predictions for categories [4, 1], [5, 2]
and [5, 4]. Occlusion, bad road conditions and bad lighting conditions are the plausible causes for wrong detections.
categories which occur frequently in real-life. The list of
label conﬁgurations in our database is as follows: Θ =
{[4, 1], [4, 2], [4, 3], [4, 4], [5, 2], [5, 3], [5, 4], [6, 4]}. We assume a uniform prior for all these categories and zero prior
for the rest. Our training data-set contains 27 videos while
the rest 26 are used for testing. The number of frames used
in training and testing are 9018 and 9036 respectively. The
distribution of training and test data over various categories
is shown in Fig. 5. As mentioned before, each category has
videos of varying road, trafﬁc and weather conditions.
We show the accuracy of self-positioning before and after temporal smoothing in Table 1. Table 2 shows the confusion matrix for the initial estimation of self-positioning
and the ﬁnal estimate after temporal smoothing. We can see
that there is an improvement of 5.04% over all 8 categories,
which indicates that temporal smoothing is able to remove
noisy results persisting only for a few frames. As we in-

The advantage of the above bounding box selection technique is that it involves only simple computations and it can
be applied to exhaustively generated bounding boxes or to
the proposals generated through techniques such as BING
[3]. However, we observe that in our data-set, the video
frames are not of high-quality and many vehicles are small.
BING does not capture all the cars and thus we have used
exhaustively generated bounding box along with the selection technique. The exhaustive boxes are generated by applying three naive constraints:
1. Aspect ratio of any box cannot be more than three and
smaller than one-third.
2. The boxes lie only in the lower

2 rd
3

part of an image.

3. Maximum size of boxes is pre-determined.
Even after applying these constraints, the selection technique reduces the no. of boxes by a factor of 6 on an average
and eliminates many false detections. We use a cascaded
deformable parts model [7, 6] trained on our data which
consists of 833 vehicles and on “car” subset of Pascal VOC
2010 data [5]. The negative data is randomly chosen. These
detections can again be used to reﬁne the frame likelihood.

4. Experimental Setup and Results
We have formed the data-set using the videos captured
from a typical camera mounted inside the wind-shield. Our
data-set has 53 videos with a frame rate of 30 fps 5 . The
average length of each video clip is 11.8 seconds. Though
L = 7 yields 28 categories, we consider a subset of those
Figure 5: Distribution of Video Frames in Train and Test
Set

5 The

data-set is available at http://www.public.asu.edu/
˜bli24/CodeSoftwareDatasets.html
409

Table 1: Self-positioning accuracy.

Class = [θ1 , θ2 ]
[4, 1]
[4, 2]
[4, 3]
[4, 4]
[5, 2]
[5, 3]
[5, 4]
[6, 4]
Overall Accuracy

Without temporal smoothing
Linear SVM
Random forest
33.55%
34.94%
58.84%
80.28%
69.74%
73.50%
72.45%
90.48%
54.93%
47.00%
48.18%
47.94%
44.06%
56.82%
95.45%
94.55%
54.77%
61.41%

With temporal smoothing
Linear SVM
Random forest
32.90%
32.81%
62.67%
89.32%
73.33%
74.88%
73.47%
96.94%
63.44%
63.06%
49.91%
53.78%
46.27%
64.27%
100%
96.36%
57.49%
66.45%

Table 2: Confusion matrix for self-positioning without temporal smoothing (using Random forest). Results of self-positioning
after temporal smoothing are shown in brackets.
Class =
[θ1 , θ2 ]
[4, 1]
[4, 2]
[4, 3]
[4, 4]
[5, 2]
[5, 3]
[5, 4]
[6, 4]

[4, 1]

[4, 2]

[4, 3]

[4, 4]

[5, 2]

[5, 3]

[5, 4]

[6, 4]

378 (355)
24 (3)
10 (0)
0 (0)
19 (5)
10 (0)
21 (0)
0 (0)

300 (302)
985 (1096)
12 (5)
9 (0)
218 (184)
515 (627)
481 (506)
0 (0)

47 (10)
72 (32)
1659 (1690)
0 (9)
23 (0)
235 (98)
213 (141)
0 (0)

5 (0)
6 (10)
37 (35)
266 (285)
2 (2)
3 (0)
6 (0)
0 (0)

348 (415)
89 (86)
2 (0)
0 (0)
243 (326)
1 (3)
31 (0)
0 (0)

2 (0)
49 (0)
295 (328)
0 (0)
11 (0)
779 (874)
7 (0)
1 (0)

2 (0)
0 (0)
239 (199)
17 (0)
1 (0)
46 (0)
1029 (1164)
11 (8)

0 (0)
2 (0)
3 (0)
2 (0)
0 (0)
36 (23)
23 (0)
208 (212)

crease the value of p in equation 3, the algorithm averages
over larger set of frames but it increases the memory and
makes the algorithm computationally inefﬁcient. Thus we
have set the value of p to 15 while using temporal smoothing, otherwise we make it 0. Temporal smoothing improves
the initial estimation in all but one category -[4, 1]. The decrease may occur when the algorithm wrongly predicts for
more than p/2 frames often. Also, due to slowly increasing exponential kernel, ﬁrst p/2 frames may be wrongly
predicted when the category changes. Low prediction accuracy is observed in the category -[4, 1]. It should be accounted that there are 3 lanes on one of the sides of the
host-lane. Since its extremely thin (about 4 pixels), its reliable detection becomes difﬁcult. In addition to this, the road
conditions in that category are relatively worse and there is
moderate trafﬁc. Interestingly, the category [6, 4] achieves
96.36% accuracy in spite of having most number of lanes.
We would like to point out the fact that this category has the
best weather and road conditions. Thus our approach can
achieve high accuracy even in the presence of more number
of lanes provided the weather, road and trafﬁc conditions
permit. This is an eight-class classiﬁcation problem an it
has variable weather and road conditions. Another important fact is that the training data itself may be noisy. We

have divided the video clips such that each clip has a single
label. Therefore, one of the training video clips in category
- [5, 3] (say) may have a faint/invisible second lane for a
few frames. Results using our approach are shown in Fig.
4. The proposed approach is able to handle partial vehicle
occlusions, varying road and weather conditions. However,
it may fail when there are multiple vehicles totally blocking
the view or if other conditions are worse.
Bounding box selection technique reduces the generated
proposals for vehicles by a factor of 6 and in turn this reduces number of false detections. This technique also allows use of other faster but less accurate methods. We
would like to point out that though vehicle detection is an
important component in our system, our ﬁnal goal remains
to achieve accurate self-positioning in all possible conditions. We do not yet have annotated database of vehicles
for such a scenario and thus we do not list comprehensive
results for vehicle detection.

5. Conclusion and Future Scope
A novel problem of self-positioning is introduced and
an integrated approach is proposed which performs selfpositioning with the aid of vehicle detection in a closed-loop
410

system. A high-level Bayesian formulation is developed to
allow easy modiﬁcations in the future should anyone try to
introduce additional factors such as scene or viewpoint information. Our approach enables a system to perform dynamic trafﬁc routing on lane-level due to its knowledge of
vehicle positions and lane-structure. This promises larger
reduction in congestion cost and travel delays. We also develop a bounding box selection criteria which can be applied to exhaustive set of boxes or to the boxes obtained
from other box-proposal methods. Testing this framework
on real-world videos yielded acceptable results.
The system presently uses just video data to make predictions. In future, we can use GPS, accelerometer data and
other information obtained from vehicle dynamics. It is also
possible to employ this system on cloud and integrate the
results from all the vehicles to understand the underlying
true structure of the road. Additional information such as
position of lanes traveling in opposite direction may be included so that detection of those vehicles could be avoided.
We also aim to generate an annotated database of vehicles
in order to perform vehicle density estimation along with
self-positioning and evaluate the same.

[10] A. B. Hillel, R. Lerner, D. Levi, and G. Raz. Recent progress
in road and lane detection: a survey. Machine Vision and
Applications, pages 1–19, 2012.
[11] A. S. Huang, D. Moore, M. Antone, E. Olson, and S. Teller.
Finding multiple lanes in urban road networks with vision
and lidar. Autonomous Robots, 26(2-3):103–122, 2009.
[12] A. Jazayeri, H. Cai, J. Y. Zheng, and M. Tuceryan. Vehicle detection and tracking in car video based on motion
model. IEEE Transactions on Intelligent Transportation Systems, 12(2):583–595, 2011.
[13] S. Kammel and B. Pitzer. Lidar-based lane marker detection
and mapping. In IEEE Intelligent Vehicles Symposium, 2008.
[14] H. Kong, J.-Y. Audibert, and J. Ponce. Vanishing point detection for road detection. In IEEE Conference on Computer
Vision and Pattern Recognition, pages 96–103, 2009.
[15] T. Kuhnl, F. Kummert, and J. Fritsch. Visual ego-vehicle lane
assignment using spatial ray features. In IEEE Intelligent
Vehicles Symposium (IV), pages 1101–1106, 2013.
[16] C. Lipski, B. Scholz, K. Berger, C. Linz, T. Stich, and
M. Magnor. A fast and robust approach to lane marking detection and lane tracking. In IEEE Southwest Symposium on
Image Analysis and Interpretation., pages 57–60, 2008.
[17] J. C. McCall and M. M. Trivedi. Video-based lane estimation and tracking for driver assistance: survey, system, and
evaluation. IEEE Transactions on Intelligent Transportation
Systems, 7(1), 2006.
[18] M. Nieto, J. A. Laborda, and L. Salgado. Road environment modeling using robust perspective analysis and recursive bayesian segmentation. Machine Vision and Applications, 22(6):927–945, 2011.
[19] M. Nieto, L. Salgado, and F. Jaureguizar. Robust road modeling based on a hierarchical bipartite graph. In IEEE Intelligent Vehicles Symposium, pages 61–66, 2008.
[20] H. Sawano and M. Okada. A road extraction method by an
active contour model with inertia and differential features.
IEICE transactions on information and systems, 89(7):2257–
2267, 2006.
[21] S. Sivaraman and M. M. Trivedi. A general active-learning
framework for on-road vehicle recognition and tracking.
IEEE Transactions on Intelligent Transportation Systems,
11(2):267–276, 2010.
[22] S. Sivaraman and M. M. Trivedi. Looking at vehicles on
the road: A survey of vision-based vehicle detection, tracking, and behavior analysis. IEEE Transactions on Intelligent
Transportation Systems, 14(4):1773, 2013.
[23] Y. Wang, E. K. Teoh, and D. Shen. Lane detection and tracking using b-snake. Image and Vision computing, 22(4), 2004.
[24] Q. Yuan, A. Thangali, V. Ablavsky, and S. Sclaroff. Learning
a family of detectors via multiplicative kernels. IEEE PAMI,
33(3):514–530, 2011.
[25] G. Zhang, N. Zheng, C. Cui, Y. Yan, and Z. Yuan. An efﬁcient road detection method in noisy urban environment. In
IEEE Intelligent Vehicles Symposium, pages 556–561, 2009.
[26] H. Zheng, Y.-C. Chiu, and P. B. Mirchandani. On the system
optimum dynamic trafﬁc assignment and earliest arrival ﬂow
problems. Transportation Science, 2013.

References
[1] Y. Alon, A. Ferencz, and A. Shashua. Off-road path following using region classiﬁcation and geometric projection
constraints. In IEEE CVPR, volume 1, pages 689–696, 2006.
[2] P. Chandakkar, R. Venkatesan, and B. Li. Video-based selfpositioning for intelligent transport systems applications. In
Proceedings of the 10th International Symposium on Visual
Computing, Las Vegas, NV, 2014.
[3] M.-M. Cheng, Z. Zhang, W.-Y. Lin, and P. H. S. Torr.
BING: Binarized normed gradients for objectness estimation
at 300fps. In IEEE CVPR, 2014.
[4] J. M. Collado, C. Hilario, A. De La Escalera, and J. M.
Armingol. Adaptative road lanes detection and classiﬁcation. In Advanced Concepts for Intelligent Vision Systems.
Springer, 2006.
[5] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
and A. Zisserman. The PASCAL Visual Object Classes
Challenge 2010 (VOC2010) Results. http://www.pascalnetwork.org/challenges/VOC/voc2010/workshop/index.html.
[6] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part
based models. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 32(9):1627–1645, 2010.
[7] R. B. Girshick, P. F. Felzenszwalb, and D. McAllester.
Discriminatively trained deformable part models, release 5.
http://people.cs.uchicago.edu/ rbg/latent-release5/.
[8] K. He, J. Sun, and X. Tang. Guided image ﬁltering. IEEE
PAMI, 35(6):1397–1409, 2013.
[9] J. C. Herrera and A. M. Bayen. Trafﬁc ﬂow reconstruction
using mobile sensors and loop detector data. University of
California Transportation Center, 2007.

411

arXiv:1607.06416v1 [cs.CV] 21 Jul 2016

Hierarchical Attention Network for Action
Recognition in Videos

Yilin Wang
Arizona State University
ywang370@asu.edu

Suhang Wang
Arizona State University
suhang.wang@asu.edu

Jiliang Tang
Yahoo Research
jlt@yahoo-inc.com

Neil O’Hare
Yahoo Research
nohare@yahoo-inc.com

Yi Chang
Yahoo Research
yichang@yahoo-inc.com

Baoxin Li
Arizona State University
baoxin.li@asu.edu

Abstract
Understanding human actions in wild videos is an important task with a broad range
of applications. In this paper we propose a novel approach named Hierarchical
Attention Network (HAN), which enables to incorporate static spatial information,
short-term motion information and long-term video temporal structures for complex
human action understanding. Compared to recent convolutional neural network
based approaches, HAN has following advantages – (1) HAN can efficiently capture
video temporal structures in a longer range; (2) HAN is able to reveal temporal
transitions between frame chunks with different time steps, i.e. it explicitly models
the temporal transitions between frames as well as video segments and (3) with
a multiple step spatial temporal attention mechanism, HAN automatically learns
important regions in video frames and temporal segments in the video. The
proposed model is trained and evaluated on the standard video action benchmarks,
i.e., UCF-101 and HMDB-51, and it significantly outperforms the state-of-the arts.

1

Introduction

Understanding human actions in wild videos can advance many real-world applications such as social
activity analysis, video surveillance and event detection. Earlier works typically rely on hand craft
features to represent videos [14, 19]. They often consist of two steps: motion detection and feature
extraction. First, motion detectors are applied to detect the informative motion regions in the videos
and then, hand craft descriptors such HOG [3], SIFT, or improved Dense Trajectories (iDT) [19]
extract the feature patterns from those motion regions to represent the video. In contrast to hand-craft
shallow video representation, recent efforts try to learn video representation automatically from large
scale labeled video data [10, 16, 4, 20, 8]. For example, In [10], authors stack the video frames as the
input for convolution neural networks (CNN) and two stream CNNs [16] combine optical flow and
RGB video frames to train CNN and achieve comparable results with the state-of-the art hand craft
based methods. Very recently, dense trajectory pooled CNN that combines iDT and two stream CNNs
via the pooling layer achieves the state-of-the-art performance. However, [16] and [20] merely use
short term motions that cannot capture the order of motion segments and semantic meanings.
The challenges of action recognition in wild videos are three-fold. First, there are large intra-class
appearance and motion variances in the same action due to different viewpoints, motion speeds,
backgrounds, etc. Second, wild videos are often collected from movies, TV shows and media
platforms and usually have very low resolutions and noise background clutters, which exacerbate
the difficulty for video understanding. Third, long range temporal dependencies are very difficult to
capture. For example, the Optical Flow, iDT and 3D ConvNets [8] are computed within a short-time
2016 ( May), Arizona State University, AZ.

Figure 1: An Illustration of the Proposed Model. The block LSTM means a LSTM cell, whose
structure is given in Figure 2. The block ATTN indicates the operation to calculate attention weights
by using the encoded features from both LSTMs. The block WA represents the weighted average of
the input features with the weights from ATTN.
window. Long Short Term Memory (LSTM) has been recently applied to video analysis [4] that
provides a memory cell for long temporal information. However, it has been shown that the favorable
time range of LSTM is around 40 frames [15, 22]. In this work, we aim to develop a novel framework
to tackle these obstacles for action recognition in videos.
In this paper, we study the problem of video representation learning for action recognition. In
particular, we investigate – (1) how to utilize the temporal structures in the video to handle intraappearance variances and background clutters by capturing the informative spatial regions; and
(2) how to model the short-term as well as long-term motion dependencies for action recognition.
Providing answers to these two research questions, we propose a novel Hierarchical Attention
Network (HAN) that employs a hierarchical structure with recurrent neural network unit e.g., LSTM
and a soft spatial temporal attention mechanism for video action recognition. Our contributions can
be summarized as below:
• We propose a novel deep learning framework HAN for video action recognition which
can explicitly capture both short-term and long-term motion information in an end to end
process.
• A soft attention is adopted on the spatial-temporal input features with LSTM to learn the
important regions in a frame and the crucial frames in the videos.
• We conduct an extensive set of experiments to demonstrate that the proposed framework
HAN is superior to both state-of-the art shallow video representation based approaches and
deep video representation based approaches on benchmark datasets.
The rest of this paper is organized as follows. Section 2 reviews related works. Section 3 describes
the proposed hierarchical attention deep learning framework in detail. Experimental results and
comparisons are discussed in Section 4, followed by conclusions in Section 5.

2

Related Work

Hand crafted features: Video action recognition is a longstanding topic in computer vision community. Many hand-crafted features are used in still images. For example, [14] extends 2D harri corner
detector to spatial temporal 3D interest points and achieves good performance with SVM classifier.
Later, HOG3D that is based on HOG feature [3] shows its effectiveness by using integral images.
2

Then improved Dense trajectories [19] has dominated the filed of action recognition. It densely
samples interest points and tracks them within a short time period. For each point, local descriptors
such as HOG, MBH and HOF are extracted for representation. Then, all the features are encoded by
the fish vector as the final video representation.
Deep learned features: Deep learning such as convolutional neural network has been shown its
success in object detection and image classification in recent years. Based on image CNN, [8, 10]
extends the CNN framework to videos by stacking video frames. However, the performance is lower
than iDT based approaches. In order to better incorporate temporal information, [16] proposes
two stream CNNs and achieves comparable results with the state-of-the art performance of hand
craft based feature representations [19]. To consider the time dependency, [22] proposes a LSTM
based recurrent neural network to model the video sequences. Different from [22] that uses a stack
based LSTM, the proposed HAN proposes a hierarchical structure to model the video sequences in a
multi-scale fashion.
Recurrent visual attention model: Visual attention model aims to capture the property of human
perception mechanism by identifying the interesting regions in the images. Saliency detection [7]
typically predicts the human eye movement and fixations for a given image. In [21], a recurrent is
proposed to understand where the model focuses on image caption generations. Moreover, recurrent
attention model has been applied to other sequence modeling such as machine translation [12] and
image generation[6].

3

The Proposed Method

The overall architecture of HAN is shown in 1. We describe the three major components of HAN in
this section – the input appearance and motion CNN feature extraction, temporal sequence modeling
and hierarchical attention model.
3.1

Appearance and Motion Feature Extraction

In general, HAN can adopt any deep convolution networks [8, 16, 20] for feature extraction. In this
paper, we use two stream ConvNets [16] to extract both appearance and motion features. Specifically,
we train a VGG net [17] to extract feature map fP and fQ for the t-th frame image P t and the
corresponding optical flow image Qt :
fP t = CN Nvgg (P t )
fQt = CN Nvgg (Qt )

(1)

Unlike two stream ConvNets [16] that employs the last fully connected layer as the input feature, we
use the features for fP and fQ from the last convolutional layer after pooling, which contains the
spatial information of the input appearance and motion images. The input appearance and motion
images are first rescaled to 224 × 224 and the extracted feature maps from the last pooling layer have
the dimension of D ×K ×K (512×14×14 used in VGG net). K ×K is the number of regions in the
input image and D is the number of the feature dimensions. Thus at each time step, we extract K 2 D
dimension feature vectors for both appearance and motion images. We refer these feature vectors as
feature cube shown in Figure 3. Then, the feature maps fP t and fQ can be denoted in matrix forms
2
2
as Pt = [pt1 , pt2 , . . . , ptK 2 ] ∈ RD×K and Qt = [qt1 , qt2 , . . . , qtK 2 ] ∈ RD×K , respectively.
3.2

Recurrent Neural Network

Long-short term memory (LSTM), which has the ability to preserve sequence information over time
and capture long-term dependencies, has become a very popular model for sequential modeling tasks
such as speech recognition [5], machine translation [1] and program execution [24]. Recent advances
in computer vision also suggest that LSTM has potentials to model videos for action recognition [15].
We follow the LSTM implementation in [25], which is given as follows
3

Figure 2: Illustration of One LSTM Cell

it
ft
ot
gt
ct
ht

= σ(Wix xt + Wih ht−1 + bi )
= σ(Wf x xt + Wf h ht−1 + bf )
= σ(Wox xt + Woh ht−1 + b + o)
= tanh(Wgx xt + Wgh ht−1 + bg )
= ft  ct−1 + it  gt
= ot  tanh(ct )

(2)

where it is the input gate, ft is the forget gate, ot is the forget fate, ct is the memory cell state at
t and xt is the input features at t. σ(·) means the sigmoid function and  denotes the Hadmard
product. The main idea of the LSTM model is the memory cell ct , which records the history of the
inputs observed up to t. ct is a summation of – (1) the previous memory cell ct−1 modulated by a
sigmoid gate ft , and (2) gt , a function of previous hidden states and the current input modulated by
another sigmoid gate it . The sigmoid gate ft is to selectively forget its previous memory while it is to
selectively accept the current input. it is the gate controlling the output. The illustration of a cell of
LSTM at the time step t is shown in Figure 2. Next we will introduce how to model both appearance
and motion features using LSTM and how to integrate attention by using encoded features.
3.3

Hierarchical LSTMs to Capture Temporal Structures

One natural way of modeling videos is to feed features Pt and Qt into two LSTMs and then put a
classifier at the output of LSTMs for classification. However, this straightforward method doesn’t
fully utilize the structure of actions. In real world, an action is usually composed of a set of subactions, which means that video temporal structures are intrinsically layered. For example, a video
about long jump consists of three sub-actions – pushing off the board, flying over the pit and landing.
As the three actions take place sequentially, there are strong temporal dependencies among them thus
we need to appropriately model the temporal structure among the three actions. In the meantime,
the temporal structure within each action is composed of multiple actions. For example, pushing
off the board is composed of running and jump. In other words, the actions we want to recognize
are layered and we need to model video temporal structure with multiple granularities. However,
directly applying LSTM cannot capture this property. To fully capture the video temporal structure,
we develop a hierarchical LSTM. An illustration of the hierarchical LSTM is shown in the purple
rectangle in Figure 1. This hierarchical LSTM is composed of two layers – the first layer accepts the
appearance feature of each frame as the input and the output of the first layer LSTM is used as the
input of the second layer LSTM. To capture the dependencies between different sub-actions such
as dependencies between pushing off the board, flying over the pit and landing, we skip every k
4

encoded features from LSTM and use that as the input to the second layer. In addition to capturing the
video temporal structure, another advantage of layered LSTM is to increase the learning capability of
LSTM. By adding another layer in LSTM, we allow LSTM to learn higher level and more complex
features, which is a common practice proven to work well in other deep architectures such as CNN,
DNN and DBM. Thus, as shown in Figure 1, we use two hierarchical LSTMs to model the appearance
and motion features, respectively.
3.4

Attention Model to Capture Spatial Structures

Figure 3: attention
The K 2 vectors in the appearance Pt (or motion features Qt ) correspond to K 2 regions in the t-th
frame, which essentially encode the spatial structures. For action recognition, not every region of
the frames are relevant for the task at hand. Obviously, we want to focus on the regions where the
action is happening. For the action shown in Figure 3, we want to mainly focus on hands and legs
that are useful for identifying the action; while the background is noisy as a person can perform the
same action at different locations. Therefore we could confuse the classifier if we also target on
backgrounds. Thus, it is natural for us to assign different attention weights to different regions of
the frame. Since video frames are sequential, neighboring frames have strong dependencies, which
suggests that we can use the encoded features at time t − 1 to predict the attention weights at time t
and then use the attention weights to refine the input. Specifically, at each time step t − 1, we use a
softmax function over K × K locations to predict the importance of the K 2 locations in the frame,
which is written as:
exp(wiT ht−1 )
lit = PK 2
(3)
T
j=1 exp(wj ht−1 )
where lit is the importance weight of the i-th region of the t-th frame, W = {w1 , w2 , . . . , wK 2 } ∈
2
q,1
R2D×K are the weights of the softmax function and ht−1 is the concatenation of hp,1
t−1 and ht−1 , i.e.,
the encoded appearance and motion features of the (t − 1)-th frame from the first layer LSTM. Note
that we use the encoded appearance feature and motion feature jointly to compute the attention weights
instead of computing two attention weights by using the two features, separately. Its advantages
are two fold. First, the flow and appearance features capture different aspects of the frame but the
5

attention location on the video should be the same, thus we do not need to calculate two sets of
attentions for appearance and optical LSTM separately, which may introduce more computational
cost. Second, appearance and motion features provide complimentary information that may help
predict more accurate attention. With the attention weights given above, the inputs of the two LSTMs
are the weighted average of different locations as:
2

xpt =

K
X

2

lit pti and xqt =

i=1

3.5

K
X

lit qti

(4)

i=1

Action Recognition with HAN

We use hp,i
T to denote the encoding of the the video by the i-the layer LSTM for the appearance
features and hq,i
T for motion features. As mentioned above, the hierarchical LSTM captures multigranularity of video temporal structures, thus, encoded features in different levels (different i) provide
distinct descriptions of different granularity about actions, which are all useful for action recognition.
In addition, the two LSTMs encode complementary information from appearance and motion features,
thus encoded features from appearance and motion are also relevant for action recognition. Therefore,
p,L
q,L
q,L
we concatenate these features as hf = [hp,1
T , . . . , hT , hT , . . . , hT ], where L is the number of
layers in each LSTM. We then use the softmax function to predict the probability that the video vi is
classified into the class c as
p(c|vi , HAN) = softmax(HAN(vi ))
and the loss function is
max

HAN,Ws

N
X

log p(yi |vi , HAN)

(5)

(6)

i=1

where N is number of videos, yi is the label of vi and Ws are the weights of the softmax classifier.

4

Experiments

In this section, we first present the details of datasets and the evaluation protocol. Then, we describe
the details of the implementation of our method. Finally, we present the experimental results with
discussions.
4.1

Datasets and evaluation protocol

The evaluation is conducted on two public benchmark datasets, i.e., UCF-101 [18] and HMDB51[11].
These two datasets are among the largest available annotated video action recognition datasets that
have been used in [10, 15, 16, 19, 20]. Specifically, UCF-101 contains 13, 000 videos annotated
into 101 action classes with each class having at least 100 videos. HMDB51 is composed of 6, 700
videos from 51 action categories and each category has at least 100 video clips. For both datasets,
the evaluation protocol is the same – we follow the train/test splits provided by the corresponding
organizers. The performance is measured by the mean of accuracies across all the splits in each
dataset.
4.2

Experiments Setting

Training two stream CNNs and HANs: Compared to image classification and detection, training
a good deep convolutional neural network for videos understanding is more challenging. Similar to
[20, 16], we use the training data in UCF 101 split to train two stream CNNs. In our implementation,
we use the Caffe toolbox [9] and the layer configuration is the same as [17]. All hidden layers use
the rectification activation functions and max pooling is performed over 2 × 2. Finally, each of the
two networks contains 13 convolutional layers and 3 fully connected layers. The training procedure
is similar to [20, 16], where we use mini-batch stochastic gradient descent with momentum (0.9).
The learning rate is initially set to 10−2 , then changed to 10−3 after 10, 000 iterations and stopped
after 30, 000 iterations and 10, 000 iterations for spatial and temporal nets, respectively. We use the
Theano toolbox for HAN implementation and the model is trained by using Adadelta [26]. The
6

dimension of LSTM is 1, 024 and the batch size is fixed to 128 . Techniques of dropout [2] and BPTT
are used.
Optical flow: The optical flow is computed by the off-the-shelf OpenCV toolbox with GPU
implementation of [23]. Since the computational cost of optical flow is the bottleneck for the two
stream CNN training. We pre-computed all the optical flow images and stored the horizontal and
vertical components. The optical flow is computed by the adjacent two frames. In the testing stage,
we fix the number of frames with the equal temporal window between them.
4.3

Results and Analysis

We compare our models with a set of baselines proposed recently [15, 22, 16, 20, 10, 13] including
shallow video representation methods and deep ConvNets methods. We first evaluate our proposed
Table 1: Average accuracy over three splits on UCF-101 and HMDB51
Model
Full HAN (spatial CNN cube+temporal CNN cube)
HAN without attention1 (spatial CNN cube +temporal CNN cube)
HAN without attention2 (spatial CNN 4096+ temporal CNN 4096)
Spatial HAN (spatial CNN cube)
Temporal HAN (temporal CNN cube)

UCF-101
92.7%
90.6%
91.1%
75.1%
85.4%

HMDB51
64.3%
62.0%
62.7%
47.7%
58.3%

HAN on UCF-101 and HMDB51 datasets by comparing HAN with different settings to show the
importance of each key component in HAN in Table 1. Then, we further compare HAN with
state-of-the art methods and experimental results are reported in Table 2. From the tables, we can
make the following observations:
• The proposed method with hierarchical LSTM outperforms methods without hierarchical
structures [22, 10, 16]. These results support that (1) the usage of LSTM can capture
video sequences by considering the order of the motion transitions; and (2) the proposed
hierarchical structure can effectively model the complex and long time range actions in
videos.
• Compared with methods without the attention components, the proposed HAN encourages
the model to focus on the important regions in frames during the learning process, which
improves the discriminative ability for classification. For example, in Figure 4 (b) and
Figure 4 (e), we can see that our model can learn the important regions for actions more
accurately.
• The temporal and spatial features are complementary. First, by combining them together,
both of them have been improved significantly. Second, compared with [15] that only considers attention in spatial, HAN can predict more motion related regions in the videos. Third,
compared to TDD, the proposed HAN achieves comparable results without considering
the iDT information, which suggests that the learned attention regions can have the similar
ability to dense trajectory points and reduce the negative impact of background noises.
• Compared to state-of-the art methods on UCF and HMDB51, HAN outperforms them
remarkably except [20]. The major reason for the exception is that the dataset HMDB is
relatively small and the content is unconstrained, while the method in [20] incorporates iDT
features that are computationally expensive.

5

Conclusion and Future Work

In this paper, we propose a hybrid deep framework by incorporating a hierarchical structure and joint
attention model to the two stream convnet approach for human action recognition. The experimental
results suggest that the proposed framework outperforms the two stream convnet approach. Despite
using the only optical flow images as input, HAN achieves comparable performance with the state-ofthe art method TDD that is much more computationally expensive. These results further support that
(1) the hierarchical structure in HAN is important because it can model the frame transitions as well
7

(a) The sampled frame

(b) Attention results from HAN (c) Attention results from [15]

(d) The sampled frame

(e) Attention results from HAN (f) Attention results from [15]

Figure 4: Visual attention comparison between HAN and soft attention model in [15], the green and
red circles highlight the most important region learned by HAN and [15] respectively.
Table 2: Comparison with state of the art methods on UCF101 and HMDB51.
Model
Histogram of Oriented Gradient
Improved dense trajectories (iDT) [19]
iDT + Stack Fish Vector [13]
spatial-temporal CNN [10]
two stream CNN [16]
two stream CNN+LSTM [22]
two stream CNN + iDT [20]
Soft Attention +LSTM [15]
Hierarchical Attention Networks

UCF-101
72.4 %
85.9%
N/A
65.4%
88.0%
88.6%
91.5%
84.96%
92.7%

HMDB51
40.2%
57.2%
66.8%
N/A
59.4%
N/A
65.9%
41.3%
64.3%

as long video segments and (2) the joint visual attention can help HAN focus on the important video
regions and reduce the effect of noisy background. HAN is powerful in sequence modeling thus we
would like to explore more applications for HAN in the future such as video event detection since a
video event usually contains many sub-events and these sub-events have high dependencies to each.
References
[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In Proceedings of ICLR, 2015.
[2] George E Dahl, Tara N Sainath, and Geoffrey E Hinton. Improving deep neural networks for
lvcsr using rectified linear units and dropout. In ICASSP, pages 8609–8613. IEEE, 2013.
[3] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In CVPR.
IEEE, 2005.
[4] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini
Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks
for visual recognition and description. In CVPR, pages 2625–2634, 2015.
8

[5] Alan Graves, Navdeep Jaitly, and Abdel-rahman Mohamed. Hybrid speech recognition with
deep bidirectional lstm. In ASRU Workshop, pages 273–278. IEEE, 2013.
[6] Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. Draw: A recurrent neural
network for image generation. arXiv preprint arXiv:1502.04623, 2015.
[7] Laurent Itti, Christof Koch, and Ernst Niebur. A model of saliency-based visual attention for
rapid scene analysis. TPAMI, (11):1254–1259, 1998.
[8] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolutional neural networks for human
action recognition. TPAMI, 35(1):221–231, 2013.
[9] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick,
Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature
embedding. In Multimedia, pages 675–678. ACM, 2014.
[10] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and
Li Fei-Fei. Large-scale video classification with convolutional neural networks. In CVPR, 2014.
[11] Hildegard Kuehne, Hueihan Jhuang, Estíbaliz Garrote, Tomaso Poggio, and Thomas Serre.
Hmdb: a large video database for human motion recognition. In ICCV. IEEE, 2011.
[12] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attentionbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.
[13] Xiaojiang Peng, Changqing Zou, Yu Qiao, and Qiang Peng. Action recognition with stacked
fisher vectors. In Computer Vision–ECCV 2014, pages 581–595. Springer, 2014.
[14] Christian Schüldt, Ivan Laptev, and Barbara Caputo. Recognizing human actions: a local svm
approach. In ICPR, volume 3, pages 32–36. IEEE, 2004.
[15] Shikhar Sharma, Ryan Kiros, and Ruslan Salakhutdinov. Action recognition using visual
attention. In Proceedings Workshop of ICLR, 2015.
[16] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action
recognition in videos. In NIPS, pages 568–576, 2014.
[17] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556, 2014.
[18] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human
actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.
[19] Heng Wang and Cordelia Schmid. Action recognition with improved trajectories. In CVPR,
pages 3551–3558, 2013.
[20] Limin Wang, Yu Qiao, and Xiaoou Tang. Action recognition with trajectory-pooled deepconvolutional descriptors. In CVPR, pages 4305–4314, 2015.
[21] Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and
Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention.
arXiv preprint arXiv:1502.03044, 2015.
[22] Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat
Monga, and George Toderici. Beyond short snippets: Deep networks for video classification. In
CVPR, pages 4694–4702, 2015.
[23] Christopher Zach, Thomas Pock, and Horst Bischof. A duality based approach for realtime tv-l
1 optical flow. In Pattern Recognition, pages 214–223. Springer, 2007.
[24] Wojciech Zaremba and Ilya Sutskever. Learning to execute. arXiv preprint arXiv:1410.4615,
2014.
[25] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.
arXiv preprint arXiv:1409.2329, 2014.
[26] Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
9

REAL-TIME VEHICLE BACK-UP WARNING SYSTEM WITH A SINGLE CAMERA
JunCao∗1 , Y ilinW ang ∗2 , BaoxinLi2
Intel Corp1 . Arizona State University2
4600 S. Dobson Road
Chandler, AZ 85248
ABSTRACT
In this paper, we propose a real-time system using vehicle back-up camera to alert for potential back-up collisions.
We developed a highly efficient algorithm, combining segmenting pedestrians and vehicles from moving background
using local optical flow value, and a scale adaptive method
using Deformable Part Model to detect objects at different
distances. To test out algorithm, we created our own vehicle back-up dataset that contains rich scenes recorded from a
back-up camera on moving/stationary vehicles with unique
and challenging scenarios such as frequent occlusion with
cluttered and moving background, and we made this dataset
available to public for other researchers. Experiments on the
dataset shows that our algorithm achieves high accuracy in
near real-time, and it is about 10 times faster than the comparable state-of-the-art algorithm.
Index Terms— Computer Vision, Vehicle Safety, Optical
Flow, Deformable Part Model, Latent SVM
1. INTRODUCTION
In United States, about one-fifth of all motor vehicle accidents
happen in parking lots, often during back-up. The NHTSA
(National Highway and Traffic Safety Administration) cites
an average of 210 fatalities and 15,000 injuries from backup accidents every year. To improve back-over safety, a rule
passed on March 31, 2014 by the US Congress mandates rearview cameras in all cars built from May 1, 2018. To take
advantage of this new rule, we developed an active warning
system to enhance the otherwise passive dashboard video display and alert for potential collision in real time.
Many works have been done for vehicle and pedestrian
detection. Haar wavelet like features [1] and HOG feature [2]
are among the most widely used features to recognize vehicle and pedestrian. For handling occlusion and deformation,
the current state-of-the-art and one of the best performers is
Grammar model [3], which represents objects with a hierarchy of deformable parts [4]. These algorithms normally need
to be combined with sliding window technique to find the location of the objects in an image.
Jun Cao and Yilin Wang are equal contributors of this paper

978-1-4799-8339-1/15/$31.00 ©2015 IEEE

2275

However, there are about one billion rectangle sub-images
even in a very low resolution 320 × 240 image, and the number of sub-images grows as n4 for images of sizes n × n
[1], thus the search needs to be limited in a small number
of candidate windows for better performance. For still background, such as in surveillance, background subtraction is often used to obtain sub-windows containing moving objects;
however, it can not be used with moving background. Other
algorithms such as Binarized Normed Gradients Model and
Efficient Sub-window Search [5] can also generate a small
set of candidate windows. However, those algorithms are not
optimized in our scenarios, and even with them, it still take
about one second to detect the objects of interest per frame.
We observed that even with moving background, the
pedestrians and vehicles move at different speed than the
background. So we can group areas of similar optical flow
values to form candidate windows and then perform detection in these small candidate windows. Since most vehicles
have only one back-up camera, we take a novel approach and
take advantage of the known camera calibration information
to estimate the distance between the object and the single
camera.
Currently no public datasets designated to pedestrian and
vehicle detection for back-up scenario exists, especially for
the near collision scenarios. We took our own video clips and
created our unique dataset, and simulated and reconstructed
many near collision scenes. We have made the dataset available to public for testing and benchmarking similar scenarios.
The video clips are taken by the authors in Tempe, Mesa, and
Chandler, Arizona, USA.
The contribution of this paper includes, first, we developed a real-time system adaptively combining existing stateof-the-arts and using only existing equipment to provide collision warning with no additional hardware cost to auto manufacturers. No intellectual property or patents are involved in
our algorithm, so the automobile manufacturers can use our
implementation as a reference design for their own systems.
Second, we create a firsthand dataset of video clips and provide it to the public on vehicle back-up life-like scenarios;
currently no other similar dataset is publicly available.

ICIP 2015

Fig. 1. Demonstration of backup camera setup.

Fig. 2. Demonstration of the different positions (3m and 6m)
from backup camera.

2. DATASET
After comprehensive research on the benchmark datasets,
including Caltech Pedestrians dataset [6], INRIA Person
Dataset [2] and MIT Pedestrian Dataset [7] for traffic data,
we did not find datasets designated for the back-up scenarios taken from slow moving or stationary host vehicles, and
there is especially a lack of test data set for the near collision scenarios. Therefore, we created our own dataset with
videos taken by the authors at different locations, scenes, and
lighting conditions. Each video clip lasts from 30 seconds
to 60 seconds. To simulate common back-up scenarios, the
authors took video footage as the host vehicle was stationary
or moving. To simulate near collision scenarios, the authors
drove two vehicles with the camera mounted to the back of
one of the vehicles to each other at slow speed. There are
three categories in our dataset: 1. pedestrians walking at rear
of the vehicle. 2. vehicles backing in the parking lot. 3. two
vehicle near collision simulation.
In our setup shown in Fig. 1, we used a Wi-Fi controlled
GoPro clip-on camera with fisheye lens of 170 ◦ viewing angle, mounted on the license plate at the rear of the car at 110
cm above the ground, and the axis of the camera lens is tilted
10 down to the horizontal line. In our approach, we define two
zones, alarm zone, which is 3 meter radius from the camera,
and alert zone, which is 6 meter radius from the camera.
Due to the camera lens distortion, it is difficult to derive
a closed form formula for the 3 and 6 meter radius curves as
shown in Fig. 2. So we first draw the lines on the ground for
the 3 and 6 meter radius curves, capture them in image, and
use quadratic fitting to derive 2 smooth curves representing
them. Then we determine if the objects are within the 3 and 6
meter zones by judging if the lower boundary of the bounding
box is below the curves.
The dataset contains 45 videos taken in public parking
lots, and each one lasts from 30 seconds to 60 seconds. The
frame rate is set to 45 FPS and video resolution is 848 × 480.
There are minor variation in the camera position due to repeated mounting of the camera. The dataset summary is described in Table. 1.
From 50,000 labeled frames describing if there are objects
in the alarm zone in our dataset, we uniformly sampled 800
frames and annotated 1700 bounding boxes for pedestrians
and vehicles, where only the visible objects are marked by

2276

Table 1. Dataset summary.
Vehicle Backup Dataset
total frames
∼100K
labeled frame
∼50K
#annotated frame
∼800
#bounding boxes
∼1700
#parking lots
∼4
category 1
∼15
category 2
∼16
category 3
∼14
labeling effort
∼ 100h

bounding boxes and occluded ones are marked as ’truncated’.
We group pedestrians and vehicles by the image sizes (height
in pixels) into 3 scales: near ( > 100 for pedestrian and 220
for vehicle), medium (70 ∼ 100 for pedestrian,150 ∼ 220
for vehicle), far ( < 70 for pedestrian and 150 for vehicles).
Around 94% pedestrians and vehicles in the dataset fall into
the near,medium categories, and detecting objects in this scale
is essential for vehicle back up safety.
3. ALGORITHM
3.1. Process Pipeline
The process pipeline is shown in Fig 3. First the optical flow
is evaluated on a 8 × 8 grid and sum of its absolute difference
for every 2 frames is calculated. If over a threshold, the optical flow will be calculated at higher frame rate for real-time
detection, and the candidate windows are generated by grouping areas with similar optical flow values. Then the objects
are detected and recognized in the candidate windows with
scale adaptive deformable part model, and their relevance is
estimated based on the zones that they are located in.
3.2. Scale Adaptive DPM
The deformable part model (DPM) [4] represents the object
as visual grammar using a hierarchical structure. Each part
of the object is defined independently and trained by latent
SVM. The most challenging part of using DPM is in the training stage where the samples only have bounding boxes around

loss is the maximum of the two convex functions when we set
yi = −1. For the optimization part, we use the coordinate
descent method. First for Eq.2, we optimize the LD in term
of z, and then we optimize LD over β by solving the convex
function.
In our application, we use pyramid feature maps to detect
objects in multi-scales. An object detector is a filter window
F (x0 , y 0 ), and the score of confident map is the filter response
of a given scale feature map G, defined as as sub-window of
image I. The matching score is defined as the dot product
of each sub-window in G. For a sub-window whose top-left
coordinate is (x, y), the matching score is computed as:
X
F (x0 , y 0 )Ġ[x : x + x0 , y : y + y 0 ]
(3)
x0 ,y 0

The scores of filter response over the threshold will be considered as the possible object locations, and the pyramid feature
map is a multi-scale feature map in a fixed range.
Since the objects towards the center of the view are more
relevant than those far away, we focus on the center part of the
field of view. We proposed a scale adaptive DPM (SaDPM)
that estimates the scale and location of the objects based on
the distance from the backup camera. In Fig. 2 the field of the
view is divided into 3 zones based on the 3m and 6m lines,
and we using different scale of DPM for these areas. The
distance d can be related to the size of the object s as:

Fig. 3. Work flow of proposed method.
entire object of interest, and the individual parts are not labeled. We have to treat the unlabeled parts as latent variables.
DPM is parameterized by the individual parts and the
structure model maintaining the spatial relationship between
the parts. With latent SVM, when the training images are
completely labeled and each part has its own bounding box,
simple methods, e.g., linear SVM, can be applied [8]. In a
weakly supervised setting, Expectation Maximization [9] can
be used to estimate the location of the unknown parts. In
DPM, score of a sample x is defined as:
fβ (x) = max β φ̇(x, z)

(1)

where β(x) is the model parameters, z ∈ Z(x) is latent value
for object configuration, φ(x, z) is feature vector. Z(x) is a
set of all possible values for a sample of x. In classical SVM,
we train β from binary labeled data D = {< x1 , y1 >, <
x2 , y2 > ..... < xn , yn >} where y is the label and y ∈
(−1, 1). The objective function is defined as the following:
LD (β) =

1
||β||2 + C
2

n
X

max(0, 1 − fβ (xi )yi )

(2)

i=1

where 1 − fβ (xi )yi is the hinge loss. Since the loss function is convex in β for negative examples yi = −1, we can
simplify it into a semi-convex one in the training stage by
specifying the latent values only to positive samples. From
Eq.1 we can see that f is a linear function of β, and the hinge

2277

1
d∝ √
s

(4)

4. EXPERIMENTS
Since there is no existing comparable algorithm for the proposed scenarios, especially with moving background, we
compared the ours with two baseline methods: optical flow
[10] and deformable part model [4]. Optical flow has shown
its success in real time tracking applications [10], and DPM
is one of the most popular methods for object detection and
classification, thus it is reasonable to compare our method
with them directly. The training stage for pedestrian and vehicle of our method employs VOC 2009 dataset which has no
overlap of our proposed dataset. We tested our algorithm on
a mobile PC (Intel coreTM i5, 2.1GHz CPU).
4.1. Performance Metric
We define two metrics, single video metric and single frame
metric, for performance evaluation. The first metric takes
videos as the input to evaluate proposed method and baseline methods. It measures the alarm accuracy of the system and speed performance. The alarm accuracy counts
the right warning in terms of the ground truth. The falsepositive (warning without any object in the alarming zone)
and false-negative(no warning when there are some objects

a
b
c
d
Fig. 4. Sample frames showing how the proposed method handles a. the field of view of backing up vehicles. b. close field of
view of multiple vehicles. c. collision simulation. d. pedestrian detection
in the warning zone) count as failure cases. For [10] and the
proposed method, it takes five frame for optical flow computation, while for DPM [4] it also computes the corresponding
frame. In proposed application, each method was applied 9
times in one second(45 frames), thus the speed performance
is also very important and we count the time cost for one
single frame for each method.
The single frame metric is the same as [6], which using a
modified version of the scheme laid out in the PASCAL object
detection challenges [11]. A detected Bounding Box (BBdt )
and ground truth (BBgt ) T
form a potential matching if their
BBdt S BBgt
Jaccard coefficient ( BBdt BBgt ) is larger than 50%. Specially, the threshold 50% is arbitrary but reasonable[11]. Each
BBdt and each BBgt are matched once, unmatched BBdt
indicates it is a false positive, while unmatched BBgt counts
as false negative. This metric is preferred to precision-recall
curve for many automotive applications which needs an upper bound on acceptable false positive rate, e.g. pedestrian
detection[6].
4.2. Results

Table 2. Comparison of different methods on video level
alarm accuracy speed object category
optical flow
31.21%
0.13s
N/A
DPM
69.52%
8.34s
People/Car
SaDPM
73.33%
0.63s
People/Car
The comparison results are shown in Table 2. The accuracy is computed by averaging all frame level object detection accuracy. For each frame we also show the time cost and
whether the method can distinguish object category. The Table 1. indicates that our method achieves highest accuracy
with low time cost among them. Though optical flow is the
fastest, it has lowest accuracy and can not identify the object category. The DPM has high computation cost on object
searching, more than 10 times slower than ours.
4.3. Analysis
Deciding the scale of unknown object has been addressed as
a key problem in computer vision [11, 6]. In this section, we
discuss the detection result of different scales vehicles and

2278

Fig. 5. False negative versus false positive per frame curve
for subset of the dataset. Lower curve indicates better performance.
pedestrians in proposed dataset. As mentioned in Sec. 2,
we divide the scales into near, medium, far three category.
From the results, we can see that even with the challenging
dataset with moving and cluttered background, the proposed
scale adaptive approach can improve the detection result over
the existing algorithm. The comparison results are shown in
Table 2.
The proposed method has some failure cases when the objects are too close and only part of them are captured by the
video. It also have some mis-classification when the objects
are too close to each other. However we are still able to provide alert in these cases.
5. CONCLUSION AND FUTURE WORK
The results demonstrate that our algorithm achieves very high
object detection rate in near real time. Since our implementation is in Matlab, we expect significant performance gain
when re-implemented in C++. With our set up and algorithm,
we are able to detect and alert the driver in real time with
no additional equipment cost for the auto manufacturers. We
plan to further optimize the implementation so it can be executed on ultra low power embedded devices. We also plan to
add more training samples to improve our detection accuracy
and help other researchers develop and optimize their algorithms.

6. ACKNOWLEDGMENTS
Y. Wang and B. Li were supported in part by a grant from the
National Science Foundation. Any opinions expressed in this
material are those of the authors and do not necessarily reflect
the views of the NSF
7. REFERENCES
[1] M. Bertozzi, E. Binelli, A. Broggi, and M. Del Rose,
“Stereo vision-based approaches for pedestrian detection,” 2012 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition Workshops,
vol. 0, pp. 16, 2005.
[2] N. Dalal and B. Triggs, “Histograms of oriented gradients for human detection,” in Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer
Society Conference on, June 2005, vol. 1, pp. 886–893
vol. 1.
[3] P. Felzenszwalb, “Object detection grammars,” in Computer Vision Workshops (ICCV Workshops), 2011 IEEE
International Conference on, Nov 2011, pp. 691–691.
[4] P.F. Felzenszwalb, R.B. Girshick, D. McAllester, and
D. Ramanan, “Object detection with discriminatively
trained part-based models,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 32, no. 9,
pp. 1627–1645, Sept 2010.
[5] C. H. Lampert, M.B. Blaschko, and T. Hofmann, “Beyond sliding windows: Object localization by efficient
subwindow search,” in Computer Vision and Pattern
Recognition, 2008. CVPR 2008. IEEE Conference on,
June 2008, pp. 1–8.
[6] P. Dollár, C. Wojek, B. Schiele, and P. Perona, “Pedestrian detection: A benchmark,” in CVPR, June 2009.
[7] M. Oren, C.P. Papageorgiou, P. Sinha, E. Osuna, and
T. Poggio, “Pedestrian detection using wavelet templates,” 1997, pp. 193–99.
[8] D. Crandall, P. Felzenszwalb, and D. Huttenlocher,
“Spatial priors for part-based recognition using statistical models,” in Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on. IEEE, 2005, vol. 1, pp. 10–17.
[9] Y. Amit and A. Trouvé, “Pop: Patchwork of parts models for object recognition,” International Journal of
Computer Vision, vol. 75, no. 2, pp. 267–282, 2007.
[10] T. Senst, V. Eiselein, and T. Sikora, “Robust local optical flow for feature tracking,” IEEE Transactions on
Circuits and Systems for Video Technology, vol. 22, no.
9, pp. 1377–1387, sep 2012.

2279

[11] M. Everingham, L. Van Gool, C. K. I.
Williams, J. Winn, and A. Zisserman,
“The
PASCAL
Visual
Object
Classes
Challenge
2007 (VOC2007) Results,”
http://www.pascalnetwork.org/challenges/VOC/voc2007/workshop/index.html.

A Structured Approach to Predicting Image Enhancement Parameters
Parag Shridhar Chandakkar
Baoxin Li
School of Computing, Informatics and Decision Systems Engineering, Arizona State University
{pchandak,baoxin.li}@asu.edu

Abstract
Social networking on mobile devices has become a commonplace of everyday life. In addition, photo capturing process has become trivial due to the advances in mobile imaging. Hence people capture a lot of photos everyday and they
want them to be visually-attractive. This has given rise to
automated, one-touch enhancement tools. However, the inability of those tools to provide personalized and contentadaptive enhancement has paved way for machine-learned
methods to do the same. The existing typical machinelearned methods heuristically (e.g. kNN-search) predict the
enhancement parameters for a new image by relating the
image to a set of similar training images. These heuristic methods need constant interaction with the training images which makes the parameter prediction sub-optimal
and computationally expensive at test time which is undesired. This paper presents a novel approach to predicting
the enhancement parameters given a new image using only
its features, without using any training images. We propose
to model the interaction between the image features and
its corresponding enhancement parameters using the matrix factorization (MF) principles. We also propose a way
to integrate the image features in the MF formulation. We
show that our approach outperforms heuristic approaches
as well as recent approaches in MF and structured prediction on synthetic as well as real-world data of image enhancement.

1. Introduction
The growth of social networking websites such as Facebook, Google+, Instagram etc. along with the ubiquitous
mobile devices has enabled people to generate multimedia content at an exponentially increasing rate. Due to the
easy-to-use photo-capturing process of mobile devices, people are sharing close to two billion photos per day on the
social networking sites 1 . People want their photos to be
visually-attractive which has given rise to automated, one1 http://www.kpcb.com/internet-trends

touch enhancement tools. However, most of these tools
are pre-defined image filters which lack the ability of doing
content-adaptive or personalized enhancement. This has fueled the development of machine-learning based image enhancement algorithms.
Many of the existing machine-learned image enhancement approaches first learn a model to predict a score quantifying the aesthetics of an image. Then given a new lowquality image2 , a widely-followed strategy to generate its
enhanced version is as follows:
• Generate a large number of candidate enhancement parameters3 by densely sampling the entire range of image parameters. Computational complexity may be reduced by applying heuristic criteria such as, densely
sampling only near the parameter space of most similar training images.
• Apply these candidate parameters to the original lowquality image to create a set of candidate images.
• Perform feature extraction on every candidate image
and then compute its aesthetic score by using the
learned model.
• Present the highest-scoring image to the user.
There are two obvious drawbacks for the above strategy. First, generating and applying a large number of candidate parameters to create candidate images may be computationally prohibitive even for low-dimensional parameters. For example, a space of three parameters where each
parameter ∈ {0, ..., 9} produces 103 combinations. Second,
even if creating candidate images is efficient, extracting features from them is always computationally intensive and
is the bottleneck. Also, such heuristic methods need constant interaction with the training database (which might be
2 We call the images before enhancement as low-quality and those after enhancement as high-quality in the rest of this article. The process of
enhancing a new image is called “the testing stage”.
3 The brightness, saturation and contrast are referred to as “parameters”
of an image in this article.

stored on a server) that makes the parameter prediction suboptimal. All these factors contribute to making the testing
stage inefficient.
Our approach assumes that a model quantifying image
aesthetics has already been learned and instead focuses on
finding a structured approach to enhancement parameter
prediction. During training, we learn the inter-relationship
between the low-quality images, its features, its parameters
and the high-quality enhancement parameters. During the
testing stage, we only have access to a new low-quality image, its features, parameters and the learned model and we
have to predict the enhancement parameters. Using these
enhancement parameters, we can generate the candidate images and select the best one using the learned model. The
stringent requirement of not accessing the training images
arises from real-world requirements. For example, to enhance a single image, it would be inefficient to establish a
connection with the training database, generate hundreds of
candidate images, perform feature extraction on them and
then find the best image.
The search space spanned by the parameters is huge.
However, the enhancement parameters are not randomly
scattered. Instead they depend on the parameters and features of the original low-quality image. Thus we hypothesize that the enhancement parameters should have a lowdimensional structure in another latent space. We employ an MF-based approach because it allows us express
the enhancement parameters in terms of three latent variables, which model the interaction across: 1. the lowquality images 2. their corresponding enhancement parameters 3. the low-quality parameters. The latent factors are
learned during inference by Gibbs sampling. Additionally,
we need to incorporate the low-quality image features since
the enhancement parameters also depend on the color composition of the image, which can be characterized by the
features. The feature incorporation in this framework is
achieved by representing the latent variable which models
the interaction across these images as a linear combination
of their features, by solving a convex `2,1 -norm problem.
We review the related work on MF as well as image enhancement in the following section.

2. Related Work
Development of machine-learned image enhancement
systems has recently been an active research area of immense practical significance. Various approaches have been
put forward for this task. We review those works which
improve the visual appearance of an image using automated techniques. To encourage research in this field, a
database named MIT-Adobe FiveK containing corresponding low and high-quality images was proposed in [4]. The
authors also proposed an algorithm to solve the problem of
global tonal adjustment. The tone adjustment problem only

manipulates the luminance channel, where we manipulate
saturation, brightness and contrast of an image.
Content-based enhancement approaches have been developed in the past which try to improve a particular image region [2, 9]. These approaches require segmented regions which are to be enhanced. This itself may prove to
be difficult. Approaches which work on pixels have also
been developed using local scene descriptors. Firstly, similar images from the training set are retrieved. Then for
each pixel in the input, similar pixels were retrieved from
the training set, which were then used to improve the input
pixel. Finally, Gaussian random fields maintain the spatial smoothness in the enhanced image. This approach does
not consider the global information provided by the image
and hence the enhancements may not be visually-appealing
when viewed globally. In [8], a small number of image enhancements were collected from the users which were then
used along with the additional training data.
Two recent works involving training a ranking model
from low and high-quality images are presented in [5, 25].
The authors of [25] create a data-set of 1300 corresponding
low and high-quality image pairs along with a record of the
intermediate enhancement steps. A ranking model trained
on this type of data can quantify the aesthetics of an image.
In [5], non-corresponding low and high-quality image pairs
extracted from the Web are used to train a ranking model.
Both of these approaches use kNN-search during the testing
stage to create candidate images. After extracting features
and ranking them, the best image is presented to the user.
The task of enhancement parameter prediction could be
related to the attribute prediction [17, 18, 11, 7]. However,
the goal of the work on attribute prediction has been to
predict relative strength of an attribute in the data sample
(or image). We are not aware of any work which predicts
parameters of an enhanced version of a low-quality image
given only the parameters and features of that image. Since
our approach is based on MF principles, we review the related recent work on MF.
MF [19, 15, 20, 10, 24] is extensively used in recommender systems [12, 1, 13, 23, 14, 22, 21]. These systems
predict the rating of an item for a user given his/her existing
ratings for other items. For example, in Netflix problem,
the task is to predict favorite movies based on user’s existing ratings. MF-based solutions exploit following two key
properties of such user-item rating matrix data. First, the
preferred items by a user have some similarity to the other
items preferred by that user (or by other similar users, if we
have sufficient knowledge to build a similarity list of users).
Second, though this matrix is very high-dimensional, the
patterns in that that matrix are structured and hence they
must lie on a low-dimensional manifold. For example, there
are 17, 770 movies in Netflix data and ratings range from
1 − 5. Thus, there are 517770 rating combinations possible

per user and there are 480, 189 users. Therefore, the number of actual variations in the rating matrix should be a lot
smaller than the number of all possible rating combinations.
These variations could be modeled by latent variables lying
near a low-dimensional manifold. This principle is formalized in [15] with probabilistic matrix factorization (PMF).
It hypothesizes that the rating matrix can be decomposed
into two latent matrices corresponding to user and movies.
Their dot product should give the user-ratings. This works
fairly well on a large-scale data-set such as Netflix. However, a lot of parameters have to be tuned. This requirement
is alleviated in [20] by developing a Bayesian approach to
MF (BPMF). BPMF has been extended for temporal data
(BPTF) in [24]. MF is used in other domains such as computer vision to predict feature vectors of another viewpoint
of a person given a feature for one viewpoint [6]. We adopt
and modify BPTF since it allows us to model joint interaction across low-quality images, corresponding enhancement
parameters and the low-quality parameters. In the next section, we detail our problem formulation and proposed approach.

3. Problem Formulation and Proposed Approach
We have a training set consisting of N images
{S1 , . . . , SN }4 . Parameters of all images are represented as
A = {A1 , . . . , AN } where Ai ∈ RK×1 ∀ i ∈ {1, . . . , N }.
Each image has M enhanced versions and each version has
the same size as that of its corresponding low-quality image. All versions corresponding to the ith image are represented as {W1i , . . . , WM
i }. All versions are of higher quality as compared to its corresponding image. Parameters of
all M versions of the ith image (also called as candidate pa1
M
rameters) are represented as A0 = {A0i , . . . , A0i }, where
0j
K×1
Ai ∈R
∀ i, j. Features of all low-quality images are
represented as F = {F1 , . . . , FN } where Fi ∈ RL×1 ∀ i.
In practice, we observe that M  N, K < M . Our goal
is to be able to predict the candidate parameters for all the
versions of the ith image by only using the information provided by Ai and Fi . To the best of our knowledge, this is
a novel problem of real significance that has not been addressed in the literature. We now explain our proposed approach.
As mentioned before, our task is to predict the candidate
parameters for all the enhanced versions of a low-quality
image with the help of its parameters and features. The values for all the K parameters corresponding to N images
and their N · M versions (total N + N · M ) can be stored
4 We use bold letters to denote matrices. Non-bold letters denote
scalars/vectors which will either be clear from the context or will be mentioned. X i , Xi , XT , Xij and ||X||p denote row, column, transpose, entry
at row i and column j of a matrix X and pth norm of matrix X respectively.

in three-dimensional matrix R ∈ RN ×(M +1)×K . We need
k
k
k
to predict R̂ij
= Rik + ∆Rij
or in turn just ∆Rij
. Rik deth
notes the k parameter value (k ∈ {1, . . . , K}) of the ith
k
low-quality image and R̂ij
is the k th parameter value of j th
version of the ith image. Given a new nth low-quality imk
age, we only need to predict ∆Rnj
∀ j = {1, . . . , M }, ∀ k.
k
During training, we can compute ∆Rij
from available
k
k
Rij and R̂ij . Following MF principles, we express ∆R as
an inner product of three latent factors, U ∈ RD×N , V ∈
RD×M and T ∈ RD×K [20, 24]. D is the latent factor dimension. These latent factors should presumably model the
underlying low-dimensional subspace corresponding to the
low-quality images, its enhanced versions and its parameters. This can be formulated as:
k
∆Rij
=< Ui , Vj , Tk >≡

D
X

Udi Vdj Tdk ,

(1)

d=1

where Udi denotes the dth feature of the ith column of
U. Presumably, as we increase D, the approximation erk
ror ∆Rij
− < Ui , Vj , Tk > should decrease (or stay constant) if the prior parameters for latent factors U, V and
T are chosen correctly. Following [20], we choose normal distribution (with precision α) for: 1. the conditional distribution ∆R|(U, V, T) and 2. for prior distributions - p(U|ΘU ), p(V|ΘV ) and p(T|ΘT ), where ΘU =
−1
−1
(µU , Λ−1
U ), ΘV = (µV , ΛV ), ΘT = (µT , ΛT ). ΘU , ΘV
and ΘT are hyper-parameters, and µ and Λ are the multivariate precision matrix and the mean respectively. Since
the Wishart distribution is a conjugate prior for multivariate normal distribution (with precision matrix), we put
Gaussian-Wishart priors on all hyper-parameters5 . We
could find the latent factors U, V and T by doing inference
through Gibbs sampling. It will sample each latent variable
from its distribution, conditional on the values of other varik
ables. The predictive distribution for ∆Rij
can be found by
using Monte-Carlo approximation (explained later).
However, it is important to note the following major differences in our problem when compared with the previous
work on MF [20, 24]. In product or movie rating prediction problems, an average (non-personalized) recommendation may be provided to a user who has not provided any
preferences (not necessarily constant for all users). In our
case, each image may require a different kind of parameter
adjustment to create its enhanced version and thus no “average” adjustment exists. As explained before, the adjustment
should depend on the image’s features, which characterize
that image (e.g. bright vs. dull, muted vs. vibrant). In our
problem, it is particularly difficult to get a good generalizing
performance on the testing set as we shall see later. The loss
in performance of existing approaches on the testing set can
5 For

details, see supplementary material on author’s website.

be attributed to the different requirements for parameter adjustments for each image. Thus it becomes necessary to include the information obtained from image features into the
formulation. We show that simply concatenating the parameters and features and applying MF techniques presented in
[20, 24] does not provide good performance, possibly because they lie in different regions of the feature space.
To overcome this problem, we observe that the conditional distribution of each Ui factorizes with respect to the
individual samples. We propose to express U as a linear
function of F by using a convex optimization scheme. We
then integrate it into the inference algorithm to find out the
latent factors. The linear transformation can be expressed
as,
Ui = FiT P + Q, ∀ i ∈ {1, . . . , N },
L×1

D×1

(2)

D×D

where Fi ∈ R
, Ui ∈ R
,P ∈ R
and Q ∈
R1×D . Note that to carry out this decomposition, we have to
set D = L. This is not a severe limitation since L is usually
large (∼ 1000) and as we have mentioned before, increasing D should decrease the approximation error at the cost
of increased computation. Henceforth we assume that our
feature extraction process generates Fi ∈ RD×1 . Also, note
that large L does not mean that the latent space is no longer
low-dimensional, because L is still smaller as compared to
all the possible combinations of parameters (e.g. 517770 ).
We propose an iterative convex optimization process to
determine coefficients P and Q of Equation 2. We propose
the following objective function to determine them: q
min
P,Q

N
X

and Q could be (trivially) obtained by just setting each entry
of P to a very small value and letting a column of Q ≈ Ui
(which makes Fi redundant). Secondly, while testing for a
new image, we would have to devise a strategy to determine
the suitable value for Q. For example, we could take the
column of Q that corresponds to the nearest training image.
This adds unnecessary complexity and reduces generalization. By making Q a row vector, we consider that it may
be possible to arrive to the space of enhancement parameters by linearly transforming the low-quality image features
with a constant offset. In other words, we want P to transform the features into a region in the latent space where all
the other high-quality images lie and Q provides an offset to
avoid over-fitting. This is a joint `2,1 -norm problem which
can be solved efficiently by reformulating it as convex. We
thus reformulate Equation 3 as follows, inspired by [16]:

min
P,Q

The `2,1 -Norm of a matrix X ∈ RM ×N is defined as,
M
P
`2,1 (X) =
||Xi ||2 . Also, for a row vector Q, we have
i=1

||Q||2 = ||Q||2,1 . Thus Equation 4 can be further written
as:

min
P,Q

||FiT P + Q − UiT ||2 + β||P||2,1 + γ||Q||2

N
γ
1X
||F T P + Q − UiT ||2 + ||P||2,1 + ||Q||2 (4)
β i=1 i
β

1 T
||F P + 1N Q − UT ||2,1 + ||P||2,1 + δ||Q||2,1 , (5)
β

(3)

i=1

The objective function tries to reconstruct U using P, Q
and F while controlling the complexity of coefficients.
Let’s concentrate on the structure of P (by neglecting the
effect of Q momentarily). The columns of P act as coefficients for Fi . Ideally, we would want the elements of Ui
to be determined by a sparse set of features, which implies
sparsity in the columns of P. To this end, we impose `2,1 norm on P, which gives us a block-row structure for P.
Let us consider the structure of Q along with P. Equation 2 shows that different columns of Ui depend on different image features Fi . Also, we expect that a different set
of columns of P will get activated (take on large values) for
different Fi . We add an offset Q ∈ R1×D for regularization.
Thus the offset introduced by Q remains constant across all
the images but changes for each Fi,j . Making Q to be a
row vector also forces P to play a major role in Equation
3. This in turn increases the dependence of Ui on Fi . If we
were to define Q as the same size of U (which would mean
different offsets for each image as well as its features), it
would pose two potential disadvantages. Firstly, optimal P

where δ = βγ and 1N is a column vector of ones ∈ RN .
Now, put FT P + 1N Q − βE = UT . Thus Equation 5
becomes:

min ||E||2,1 + ||P||2,1 + δ||Q||2,1 ,

E,P,Q

s.t. FT P + 1N Q − βE = UT ,
 
 
(6)
 E 
E




T
−1
N
min  P  s.t. −βIN F δ 1  P  = UT
E,P,Q 
δQ 
δQ
2,1

Equation 6 is now in the form of: min ||X||2,1 s.t. ZX =
X

B and thus convex. It can be iteratively solved by an efficient algorithm mentioned in [16]. We set β = 0.1 and
δ = 3. Once we have expressed U as a function of F, we
use Gibbs Sampling to determine the latent factors P, Q, V
and T [20]. As mentioned before, the predictive distribution
k
for a new parameter value ∆R̂ij
is given by a multidimensional integral as:

Algorithm 1 Gibbs Sampling for Latent Factor Estimation
Initialize model parameters {P(1) , Q(1) , V(1) , T(1) }.
T
Obtain U(1) = FT P(1) + Q(1)
For y = 1, 2, . . . , Y
• Sample the hyper-parameters according to the
derivations 6 :
α(y) ∼ p(α(y) |U(y) , V(y) , T(y) , ∆R),
(y)
(y)
(y)
(y)
ΘU ∼ p(ΘU |U(y) ), ΘV ∼ p(ΘV |V(y) ),
(y)
(y)
(y)
ΘT ∼ p(ΘT |T )
• For i = 1, ..., N , sample the latent features of an
image (in parallel):
(y+1)
(y)
Ui
∼ p(Ui |V(y) , T(y) , ΘU , α(y) , ∆R)
Determine P(y+1) and Q(y+1) using the iterative
T
optimization by substituting B = U(y+1) .

T
Reconstruct U(y+1) : Û(y+1)
= FT P(y+1) +Q(y+1)
• For j = 1, ..., M , sample the latent features of the
enhanced versions (in parallel):
(y+1)

Vj

(y)

∼ p(Vj |Û(y+1) , T(y) , ΘV , α(y) , ∆R)

• For k = 1, ..., K, sample the latent features of parameter (in parallel):
(y+1)

Tk

(y)

∼ p(Tk |Û(y+1) , V(y+1) , ΘT , α(y) , ∆R)

k
p(∆R̂ij
|∆R) =

Z

k
p(∆R̂ij
|Ui , Vj , Tk , α)·

p(U, V, T, α, ΘU , ΘV , ΘT |∆R)·
d(U, V, T, α, ΘU , ΘV , ΘT ).

(7)

We resort to numerical approximation techniques to
solve the above integral. To sample from the posterior, we
use Markov Chain Monte Carlo (MCMC) sampling. We
use the Gibbs sampling as our MCMC algorithm. We can
approximate the integral by,

k
p(∆R̂ij
|∆R) ≈

Y


X
(y)
(y)
(y)
k
p ∆R̂ij
|Ui , Vj , Tk , α(y)
y=1

(8)
Here we draw Y samples and the value of Y is set by observing the validation error. The sampling from U, V and
T is simple since we use conjugate priors for the hyperparameters. Also, a random variable can be sampled in
parallel while fixing others which reduces the computational complexity. Algorithm 1 shows how to iteratively
6 See supplementary material on author’s website for detailed derivations.

sample U, V, T and obtain P and Q. Note that it is required in the algorithm to reconstruct U(y+1) at every iteration since there will always be a small reconstruction
error ||Û(y+1) − U(y+1) ||. The error occurs because we
force Q to be a row vector, which makes the exact recovery of U(y+1) difficult. The reconstructed error causes
adjustment of V and T. Once we obtain the four latent
factors, our task is to predict the parameter values for M
enhanced versions having K parameters each. Suppose
Ft is the feature vector of a new image, then the paramk
can be simply obtained by computing,
eter values ∆R̂tj
k
∆R̂tj
=< FtT P + Q, Vj , Tk > ∀ j ∈ {1, . . . , M } and
k ∈ {1, . . . , K}. If the parameter value predictions lie beyond a certain range then a thresholding scheme can be used
based on the prior knowledge. For example, to constrain the
predictions between [0, 1], a logistic function may be used.

4. Experiments
We conduct two experiments to show the effectiveness
of our approach. We did the first one on a synthetic data
and compared it with: 1. BPMF 2. our own discrete version of BPTF, called D-BPTF. 3. multivariate linear regression (MLR) 4. twin Gaussian processes (TGP) [3] 5.
Weighted kNN regression (WKNN). For D-BPTF, we make
minor modifications in the original BPTF approach [24] by
removing the temporal constraints on their temporal variable, since there are no temporal constraints in our case.
The inference for their temporal variable is then done in
the exactly same manner as the other non-temporal variables. This gave us a marginal boost in the performance.
For MLR, We use a standard multivariate regression by
maximum likelihood estimation method. Specifically, we
use MATLAB’s mvregress command. TGP is a generic
structured prediction method. It accounts correlation between both input and output resulting in improved performance as compared to MLR or WKNN. The WKNN approach predicts the test sample as a weighted combination
of the k-nearest inputs. The first two algorithms do not allow features inclusion. For MLR, TGP and WKNN, we
j
concatenate Ai and Fi , and use it to predict A0i . Even
for our approach, we concatenate Ai and sample feature to
form Fi . The intuition behind this concatenation is that the
enhancement parameters should be a function of input parameters as well along with the features. We did observe
performance boost after concatenating the features and parameters.
The second experiment demonstrates the usefulness of
this approach in a real-world setting where we have to predict paramters of the enhanced versions of an image (then
generate those versions by applying predicted parameters
to the input low-quality image) without using any information about the versions. We compare our approach with the

Training RMSE for simulation

Testing RMSE for simulation

0.6

0.6

0.5

0.55
0.5

0.4

0.45

0.3

0.4

0.2

0.35
1

2

3

4

5

PMF_Simulation

6

7

8

9

10 11 12 13 14 15 16

D-BPTF_Simulation

1

Proposed_Simulation

2

3

4

Training RMSE for image enhancement

0.18

0.15

0.16

0.12

0.14

0.09

0.12

0.06

0.1

0.03

0.08
1

2

3

4

5

6

7

8

9

10 11 12 13 14 15 16

D-BPTF_Enhancement

6

7

8

9

10 11 12 13 14 15 16

D-BPTF_Simulation

Proposed_Simulation

Testing RMSE for image enhancement

0.18

PMF_Enhancement

5

PMF_Simulation

Proposed_Enhancement

1

2

3

4

5

PMF_Enhancement

6

7

8

9

10 11 12 13 14 15 16

D-BPTF_Enhancement

Proposed_Enhancement

260
195
130
65
0
Image 1 Image 2 Image 3 Image 4 Image 5

KNN

Best
image

Proposed Approach

Figure 1. Top plots: train and test RMSEs for both the experiments. Bottom plot: First 5 sets of bars show votes for version 1 to 5 of
kNN vs. the best image of our approach. The last set of bars shows votes for the best image of both approaches. Please zoom in for better
viewing. See in color.

competing 5 algorithms in addition to kNN-search as it is
also used in [26, 5]. We also analyzed the effect of Q in our
solution by: removing Q i.e. U = FT P.

4.1. Data set description and experiment protocol
The synthetic data is carefully constructed by keeping
the following task in mind. We are given a training set
consisting of: 1. F ∈ RD×N ; 2. A ∈ RK×N ; and
3. only parameters of M versions for each input sample A0 ∈ RK×N ×M . Our aim is to predict parameters for a set
of M versions given a new Fi and Ai . In real-world problems, A and F are interdependent. The parameters of M
versions are dependent on both A, F. Hence we construct
the synthetic data as follows.
Firstly, we generate a set of 3-D input parameters - A drawn from a uniform distribution [0, 1]. Then we generate
a 50-D feature set F, where each element of Fi is related
to all Ak,i ∀ i = {1, . . . , 103 }, k = {1, 2, 3} by a nonlinA
ear function. For example, Fj,i = r1 1,i + 1+e−r12 A2,i +
3
Ar3,i
, ∀ j ∈ {1, . . . , 50} and r1 , r2 , r3 are random numbers.
The parameters of enhanced versions, A0k,i,m , are also non-

linearly related to Ak,i ∀ k, ∀ m ∈ {1, . . . , 4} and Fi . For
A

3
example, A0k,i,m = η r1 1,i + 1+e−r12 A2,i + Ar3,i
+ (1 −
η) · ||Fi ||2 . The contribution of Fi is decided by η. We perform 3-fold cross-validation. We predict the values of A0
in the test set (disjoint from training) using corresponding
A and F. RMSE is computed between the predicted and
actual A0 .

The MIT-Adobe FiveK data-set contains 5000 highquality photographs taken with SLR cameras. Each photo
is then enhanced by five experts to produce 5 enhanced versions. We extract average saturation, brightness and contrast for every image, which are parameters ∈ A. We also
extract 1274-D color histogram with 26 bins for hue, 7 bins
each for saturation and value. We also calculate localized
features of 144-D each for contrast, brightness and saturation. Finally, we append average saturation, brightness and
contrast of the input low-quality image, which are its parameters. Thus we get a 1709-D (= 1274 + 3 × 144 + 3)
representation for every image ∈ F. We train using 4000
images and use 500 images each for validation and testing.
We predict parameters for 5 versions in a 3 × 5 matrix for

Figure 2. Left: Original image, Middle: enhanced image by kNN and Right: proposed approach 7 . See in color.

each image in the testing set. An entry A0i,j denotes the
value for ith parameter of j th enhanced version. To enable comparison with the expert-enhanced images of the
data-set, we also compute parameters for 5 enhanced versions for each image, which we treat as ground-truth. We
evaluate this experiment in two ways. Firstly, we calculate
RMSE between the parameters of 5 expert-enhanced photos and the parameters of the predicted versions using five
aforementioned algorithms. Secondly, we conduct a subjective test under standard test settings (constant lighting, position, distance from the screen). In this case, we compare
our approach with the popular kNN-search-based approach.
It first finds the nearest original image in the training set to
the testing image - im - and then applies the same parameter
transformation to im to generate 5 version. In our approach,
we predict the parameters for enhanced versions using the
proposed formulation. We threshold the parameter values
as:
A0k,i,m = min(A0k,i,m , Ak,i + λk Ak,i ),
A0k,i,m = max(A0k,i,m , Ak,i − ζk Ak,i ),

(9)

where λ and ζ are multipliers for the k th parameter. In
our case, the multipliers for saturation, brightness and con-

trast are: λ = {0.4, 0.4, 0.05}, ζ = {0.3, 0.3, 0.01}. As
mentioned before, the clipping scheme in our formulation
should be set using prior knowledge. Here, we know that
the enhanced images usually have a larger increase (as compared to decrease) associated with their parameters. Also,
changing contrast by a very small amount affects the image
greatly.
The predicted parameters are applied to the input image
to obtain its enhanced versions. The procedure is the same
for both the approaches and is as follows. First we change
contrast till the difference between the updated and the predicted contrast is marginal. We update contrast first since
changing it updates both brightness and saturation. We then
update brightness and saturation till they come significantly
closer to their corresponding predicted values. This gives
us 5 versions for both approaches. To allow comparisons
within a reasonable amount of time, we use a pre-trained
ranking weight vector w (from [5]) to select the best image of our approach (im-proposed) and kNN-approach (imkNN). For the subjective test, people are told to compare improposed with the 5 enhanced versions of kNN-approach as
well as with im-kNN. Thus for every input image, people
7 See supplementary on author’s website for additional full-resolution
results.

perform 6 comparisons. The image order was randomized.
We conducted the test with 11 people and 35 input images.
Thus every person compared 210 pairs of images. They
were told to choose a visually-appealing image. The third
option of simultaneously preferring both images was also
provided. This option has no effect on cumulative votes.

4.2. Results
The parameters for the synthetic data were more accurately predicted by our approach than BPMF, D-BPTF,
MLR, TGP and WKNN. It is worth noting that though
the training error continues to decrease for our approach,
BPMF and D-BPTF, the testing error starts increasing after
only 5 and 8 iterations for BPMF and D-BPTF, respectively.
However, testing error in our approach decreases rapidly for
4 iterations and then it decreases very slowly for the next
12, as shown in Fig. 1. The RMSE on test set for BPMF,
D-BPTF, MLR, TGP, WKNN and the proposed approach
is 0.4933, 0.4865, 0.6293, 0.4947, 0.8014 and 0.3644. The
numbers show that our approach is able to effectively use
the additional information provided by features and the interaction between A, F and all versions to provide better
prediction. On the other hand, BPMF and D-BPTF start
over-fitting quickly due to lack of sample feature information while MLR and WKNN fail to model the complex interaction between variables. TGP performs better because
of its ability to capture correlations between input and output. However, TGP still treats each version independently
and thus its performance still falls short of our approach.
In the second experiment, the RMSE for BPMF,
D-BPTF, MLR, TGP, WKNN and our approach is
0.1251, 0.1328, 1.2420, 0.1268, 0.1518 and 0.0820 respectively. The testing error starts increasing after only 3 and
5 iterations for BPMF and D-BPTF, respectively. It is important to note that we do not use the clipping scheme mentioned in Equation 9 in order to do a fair comparison of
RMSEs between all the five approaches and the proposed
appraoch. For the subjective evaluation, Fig. 1 shows cumulative votes obtained for ours and the kNN-based approach for comparison between 5 images chosen by kNN
and the best image chosen by our approach. Fig. 1 also
shows votes obtained for the best images chosen by both
approaches. Fig. 2 shows two input images enhanced by
both the approaches. The top row of Fig. 2 shows that kNN
reduces the saturation while increasing the brightness. Our
approach balances both of them to obtain a more appealing
image. In the bottom row, however, both approaches fail to
produce aesthetic images as images become too bright. It
is probably due to the portion of the sky in the input image.
For both the images, most people prefer images enhanced
by our approach. Computationally, our approach is superior than kNN. Complexity of our approach is independent
of data-set size at testing time whereas kNN searches the

Table 1. Effect of varying β and δ

Parameter setting
β = 0.001, γ = 6
β = 0.01, γ = 6
β = 0.02, γ = 0.1
β = 0.2, γ = 0.05
β = 0.8, γ = 0.05
β = 0.1, γ = 0.3
β = 0.1, γ = 0.8
β = 0.1, γ = 2

RMSE (lower the better)
0.3162
0.0962
0.0907
0.0930
0.0872
0.0820
0.0821
0.0820

entire data-set for the closet image and then applies its parameters.
We reconstructed U = FT P and observed performance
drop as it overfits. We get RMSE of 0.9305 and 0.3762
on enhancement and simulation data, respectively. We believe the real-world enhancement data has correlations naturally embedded in it unlike in synthetic data. Thus the performance drop is drastic in case of enhancement since the
problem of recovering P only from U and F is ill-posed.
We also analyzed the effect of varying β and δ. Since our
approach uses Bayesian probabilistic inference, small variations in β and δ do not significantly affect the performance.
Table 1 lists the various parameter settings and its effect on
the performance of the second experiment (i.e. image enhancement):

5. Conclusion
In this paper, we introduced a novel problem of predicting parameters of enhanced versions for a low-quality image by using its parameters and features. We developed an
MF-inspired approach to solve this problem. We showed
that by modeling the interactions across low-quality images, its parameters and its versions, we can outperform
five state-of-art models in structured prediction and MF. We
proposed inclusion of feature information into our formulation through a convex `2,1 -norm minimization, which works
in an iterative fashion and is efficient. Thus our approach
utilizes information which helps characterize input image.
This leads to better generalization and prediction performance. Since other approaches do not model interdependence between image features and parameters of their corresponding enhanced versions, they start over-fitting quickly
and produce an inferior prediction performance on the test
set. Experiments on synthetic and real data demonstrated
superiority of our approach over other state-of-art methods.
Acknowledgement: The work was supported in part
by an ARO grant (#W911NF1410371) and an ONR grant
(#N00014-15-1-2344). Any opinions expressed in this material are those of the authors and do not necessarily reflect
the views of ARO or ONR.

References
[1] L. Baltrunas, B. Ludwig, and F. Ricci. Matrix factorization
techniques for context aware recommendation. In Proceedings of the fifth ACM conference on Recommender systems,
pages 301–304. ACM, 2011.
[2] F. Berthouzoz, W. Li, M. Dontcheva, and M. Agrawala. A
framework for content-adaptive photo manipulation macros:
Application to face, landscape, and global manipulations.
ACM Trans. Graph., 30(5):120, 2011.
[3] L. Bo and C. Sminchisescu. Twin gaussian processes for
structured prediction. International Journal of Computer Vision, 87(1-2):28–52, 2010.
[4] V. Bychkovsky, S. Paris, E. Chan, and F. Durand. Learning photographic global tonal adjustment with a database of
input/output image pairs. In Computer Vision and Pattern
Recognition (CVPR), 2011 IEEE Conference on, pages 97–
104. IEEE, 2011.
[5] P. S. Chandakkar, Q. Tian, and B. Li. Relative learning from
web images for content-adaptive enhancement. In Multimedia and Expo (ICME), 2015 IEEE International Conference
on, pages 1–6. IEEE, 2015.
[6] C.-Y. Chen and K. Grauman. Inferring unseen views of people. In Computer Vision and Pattern Recognition (CVPR),
2014 IEEE Conference on, pages 2011–2018. IEEE, 2014.
[7] L. Chen, Q. Zhang, and B. Li. Predicting multiple attributes
via relative multi-task learning. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages
1027–1034. IEEE, 2014.
[8] S. B. Kang, A. Kapoor, and D. Lischinski. Personalization of
image enhancement. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 1799–1806.
IEEE, 2010.
[9] L. Kaufman, D. Lischinski, and M. Werman. Content-aware
automatic photo enhancement. In Computer Graphics Forum, volume 31, pages 2528–2540. Wiley Online Library,
2012.
[10] N. D. Lawrence and R. Urtasun. Non-linear matrix factorization with gaussian processes. In Proceedings of the 26th Annual International Conference on Machine Learning, pages
601–608. ACM, 2009.
[11] S. Li, S. Shan, and X. Chen. Relative forest for attribute
prediction. In Computer Vision–ACCV 2012, pages 316–327.
Springer, 2013.
[12] H. Ma, H. Yang, M. R. Lyu, and I. King. Sorec: social
recommendation using probabilistic matrix factorization. In
Proceedings of the 17th ACM conference on Information and
knowledge management, pages 931–940. ACM, 2008.
[13] H. Ma, D. Zhou, C. Liu, M. R. Lyu, and I. King. Recommender systems with social regularization. In Proceedings
of the fourth ACM international conference on Web search
and data mining, pages 287–296. ACM, 2011.
[14] B. Marlin, R. S. Zemel, S. Roweis, and M. Slaney. Collaborative filtering and the missing at random assumption. arXiv
preprint arXiv:1206.5267, 2012.
[15] A. Mnih and R. Salakhutdinov. Probabilistic matrix factorization. In Advances in neural information processing systems, pages 1257–1264, 2007.

[16] F. Nie, H. Huang, X. Cai, and C. H. Ding. Efficient and
robust feature selection via joint `2,1 -norms minimization.
In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and
A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 1813–1821. Curran Associates, Inc.,
2010.
[17] D. Parikh and K. Grauman. Relative attributes. In Computer Vision (ICCV), 2011 IEEE International Conference
on, pages 503–510. IEEE, 2011.
[18] D. Parikh, A. Kovashka, A. Parkash, and K. Grauman. Relative attributes for enhanced human-machine communication.
In AAAI, 2012.
[19] J. D. Rennie and N. Srebro. Fast maximum margin matrix
factorization for collaborative prediction. In Proceedings
of the 22nd international conference on Machine learning,
pages 713–719. ACM, 2005.
[20] R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using markov chain monte carlo. In Proceedings of the 25th international conference on Machine
learning, pages 880–887. ACM, 2008.
[21] Y. Shi, M. Larson, and A. Hanjalic. Collaborative filtering
beyond the user-item matrix: A survey of the state of the
art and future challenges. ACM Computing Surveys (CSUR),
47(1):3, 2014.
[22] Q. Song, J. Cheng, and H. Lu. Incremental matrix factorization via feature space re-learning for recommender system.
In Proceedings of the 9th ACM Conference on Recommender
Systems, pages 277–280. ACM, 2015.
[23] S. Wang, J. Tang, Y. Wang, and H. Liu. Exploring implicit
hierarchical structures for recommender systems. In International Joint Conference on Artificial Intelligence (IJCAI).
IJCAI, 2015.
[24] L. Xiong, X. Chen, T.-K. Huang, J. G. Schneider, and J. G.
Carbonell. Temporal collaborative filtering with bayesian
probabilistic tensor factorization. In SDM, volume 10, pages
211–222. SIAM, 2010.
[25] J. Yan, S. Lin, S. B. Kang, and X. Tang. A learning-to-rank
approach for image color enhancement. In Computer Vision
and Pattern Recognition (CVPR), 2014 IEEE Conference on,
pages 2987–2994. IEEE, 2014.
[26] J. Yan, S. Lin, S. B. Kang, and X. Tang. A learning-to-rank
approach for image color enhancement. In Computer Vision
and Pattern Recognition (CVPR), 2014 IEEE Conference on,
pages 2987–2994. IEEE, 2014.

2014 IEEE Conference on Computer Vision and Pattern Recognition

Predicting Multiple Attributes via Relative Multi-task Learning
Lin Chen, Qiang Zhang, Baoxin Li
Arizona State University, Tempe, AZ
lchen109, qzhang53, baoxin.li@asu.edu

Abstract
Relative attributes learning aims to learn ranking functions describing the relative strength of attributes. Most
of current learning approaches learn ranking functions for
each attribute independently without considering possible
intrinsic relatedness among the attributes. For a problem
involving multiple attributes, it is reasonable to assume that
utilizing such relatedness among the attributes would beneﬁt learning, especially when the number of labeled training pairs are very limited. In this paper, we proposed a
relative multi-attribute learning framework that integrates
relative attributes into a multi-task learning scheme. The
formulation allows us to exploit the advantages of the stateof-the-art regularization-based multi-task learning for improved attribute learning. In particular, using joint feature
learning as the case studies, we evaluated our framework
with both synthetic data and two real datasets. Experimental results suggest that the proposed framework has clear
performance gain in ranking accuracy and zero-shot learning accuracy over existing methods of independent relative
attributes learning and multi-task learning.

1. Introduction
Recent literature has witnessed fast development of the
new methodology of relative attribute learning, whose goal
is to overcome the limitation of traditional learning approaches based only on binary labels. In general, a traditional learning approach using binary labels can only map
low-level features to one of the two labels, without capturing the “relativeness” of the concepts that the labels are supposed to represent. For example, in Figure 1, we may see
that 1(a) is “natural” and 1(c) is “man-made”, but we may
be less certain on assigning either of the labels to 1(b). Unlike learning with binary labels, relative attributes learning
is to capture the strength of the attributes under consideration. For example, this would allow us to say 1(b) is less
“natural” but more “man-made” than 1(a) while being more
“natural” but less “man-made” than 1(c).
Many practical applications involve multiple attributes
1063-6919/14 $31.00 © 2014 IEEE
DOI 10.1109/CVPR.2014.135

(a)

(b)

(c)

Figure 1. An example of relative attributes. Considering binary label learning, (a) is labeled as “natural” and (c) is labeled as “manmade”, however, it is hard to lable (b) as “natural” or “man-made.”
In relative attributes, (a) is more “natural” and “open” than (b) and
(b) is more “natural” and “open” than (c).

(like the two concepts, “natural” and “man-made”, in the
above image labeling example). Current relative attributes
learning approaches train separate ranking functions independently for each of the attributes under consideration. For
a given problem, if multiple attributes are involved, they
usually exhibit correlation among them. For example, the
more “natural” a scene image is, the more “open” it may be,
where two attributes “being natural” and “being open” often
have positive correlation. In addition, even if two attributes
are disjointed in the high-level semantic space, in a practical algorithm they may be dependent of some common
low-level features, and thus are made to be related to each
other in some sense. Both factors suggest that the correlation among different attributes of the same problem should
be dealt with in a principled way for effective relative attributes learning.
To exploit potential correlation among multiple attributes for learning better ranking functions, in this paper, we employ multi-task learning (MTL) in relative attributes learning and propose a new multi-attribute relative
learning framework. MTL is a machine learning approach
that learns several tasks simultaneously for potential performance gain through utilizing “relatedness” among different
tasks, which provides a principled way for us to model correlation among attributes if we view the attributes as tasks.
In the proposed framework, a new cost function is deﬁned
to capture the joint effect of the individual objective func1021
1027

tions in original relative attribute learning. Further, a regularization term is introduced to model the potential correlation among the attributes. As a result, the proposed framework could learn the relative strength of the attributes simultaneously while utilizing the correlation among the attributes/tasks. Under this framework, we developed an algorithm employing Block Coordinate Descent principles.
Our algorithm solves the learning problem through alternating optimization steps dealing with capturing the relative
ranking information and the attribute correlation information iteratively. The proposed approach has been tested on
both synthetic data and two real datasets, with comparison
with results from the state-of-the-art approaches of relative
attributes learning and MTL.
The key contribution of this work lies in a novel formulation of relative attributes learning that handles multiple
attributes jointly to capture the potential correlation among
them for improved learning performance. Additionally, an
algorithm is developed to ﬁnd a solution under the formulation. As demonstrated by our experiments, the proposed
method is able to deliver good performance even with a
small number of training pairs, owing to its ability to exploit correlation among the attributes.
In the remaining of the paper, we ﬁrst discuss related
work in Section 2. The proposed approach is presented in
Section 3. Experiments and results are demonstrated in Section 4. We concluded the paper in Section 5.
Notations:
In this paper, we represent scalars, vectors,
matrices and sets as lower case letters x, bold face lower
case letters x, capital letters X and calligraphic capital letters O respectively. xi denotes the i-th column of the matrix X. · and ·F represent Euclidean and Frobenius
norms respectively. Xp,q is deﬁned as the p,q norm
r
 
1
1
( i (( j xqij ) q )p ) p . X∗ =
i=1 σi (X) is the trace
norm, with r = rank(X) and σi (X) the i-th non-zero singular value in non-increasing order.

2. Related Work
As the work is mostly related to multi-task learning and
relative attributes learning, we brieﬂy review the literature
on these two approaches in the following.

2.1. Multi-task Learning
Multi-task learning aims to improve generalization performance by training several tasks together to capture their
intrinsic correlation. Various types of MTL approaches
and applications have been proposed. Neural network approaches [4][6][20] utilized a hidden layer with a few nodes
and a set of network weights shared by all tasks. Hierarchical Bayes approach [3][21][22][23] enforced task relatedness through a common prior probability distribution on the

tasks’ parameters.
In recent years, more attention has been paid to
regularization-based multi-task learning, which is what we
mainly considered in this work. The general form of
regularization-based MTL is:
min
W

t 
n


(yti − WtT xti )2 + λΩ(W )


(1)

t=1 i=1

where t denotes the t-th task and i denotes the i-th sample
in task t. Much work has been proposed, often introducing
different cost functions and regularization terms.
Evgeniou and Pontil [16] assumed that the projection
vectors of all tasks are close to each other and proposed the
regularization term using a shared mean vector w0 and a
small perturbation vector v t to represent the projection vector of the t-th task wt = w0 + v t . This idea is intuitive and
easy to implement, but the assumption is too strong to hold
in real applications. Ando et al. [1] proposed Alternating
Structure Optimization (ASO) based on a similar assumption that the projection model is the sum of a task speciﬁc
component and a shared low dimensional subspace.
Processing high-dimensional feature datasets attracts a
lot of research interests. Considering the task relatedness
that different task models share a common set of features,
Jacob et al. [10] and Liu et al. [15] introduced 1 /q norm group lasso penalty as regularization to obtain a sparse
projection matrix for feature selection. Ji and Ye [12] introduced trace norm as regularization and obtained a lowrank structure projection matrix to capture task relatedness.
These approaches make the strong assumption that all tasks
are related.
Considering the existence of outlier tasks, Jalali et
al. [11] and Gong et al. [9] introduced an extra 1 and
1 /q -norm regularization term individually into feature selection; Chen et al. [7] introduced an extra 1 /q -norm regularization term into low-rank subspace learning. These approaches learn a projection matrix as well as detect the outlier tasks.
Other multi-task learning approaches include assumptions that tasks have some special structure. For example, in
[2][24], tasks in the same group are closer to each other than
tasks in a different group; in [13], tasks from the same node
are closer to each other and relatedness among the nodes
depends on the depth in a tree; in [8], task relatedness depends on the edge weight between the two tasks in a graph
representation.
The above MTL approaches are in general for classiﬁcation, and there is little work on extending them for ranking
applications. Note that, conceptually, one may use a MTLbased classiﬁer for a ranking problem, if binary labels are
also provided. This is the MTL method to be compared in
our experiments. Such an approach is obviously unable to

1028
1022

employ the relative information given in the relative labels.
Our proposed work attempts to learn a ranking function capturing multi-attribute/task correlation when only relative labels are available.

lem:
t



1
2
( wt  + ρ1
ξijt + ρ2
γijt )
W,ξ,γ
2
t=1
i,j∈O
i,j∈St

+ μΩ(W )

min

2.2. Relative Attributes Learning
Relative attribute learning is a fairly recent concept,
which has drawn increasing attention. Relative attributes
were ﬁrst used by Parikh and Grauman [14] to learn a ranking function for each human-nameable attribute of an image. The relative “strength” of an attribute is measured by
some distance metrics learned through SVM-like optimization using (relatively) labeled pairs. Relative attribute learning is applicable to zero-shot learning (detecting ‘unseen’
category) and image description in relative terms.
Parkash and Parikh [19] incorporated attribute feedback
into the classiﬁcation process. Employing attributes as the
communication “language” between the human supervisor
and the machine learner, their work allows supervisors to
provide feedback to the learner for improved learning. Kovashka et al. [14] presented a feedback scheme for image
search. Based on pre-trained relative attribute ranking functions, their system demonstrates an initial set of queried
results and asks the user to provide relative attribute feedback. The system then updates the training set based on the
feedback and provides new queried images utilizing newly
trained relative attribute ranking functions.
Most of current relative attribute learning approaches
only consider ranking attributes independently. The proposed work attempts to explicitly model potential correlation among the attributes of interest so as to achieve better
ranking performance, especially when limited training data
are available (and thus each individual attribute may have
even fewer labeled pairs of training samples).

s.t. wTt (xi − xj ) ≥ 1 − ξijt ; ∀(i, j) ∈ Ot ;
 T

wt (xi − xj ) ≤ γijt ; ∀(i, j) ∈ St ;
ξijt ≥ 0; γijt ≥ 0;
t = 1, 2, ..., t .

In this formulation, W is the projection matrix with the tth column wt as the projection vector for the t-th attribute
(task), Ω(W ) is a regularization term, xi is the feature vector of the i-th sample, Ot = {(i, j)} is the set of ordered
pairs (i, j) satisfying wTi xi > wTj xj , St is the set of similar pairs (i, j) satisfying wTi xi ≈ wTj xj , ρ1 , ρ2 and μ are
trade-off constants, ξijt and γijt are slack variables measuring the error of the distance of prior and similar pairs.
By applying appropriate regularization terms, the attribute
projection model W is learned simultaneously.
Some existing MTL frameworks only consider the correlation among the tasks, but ignore potential outliers. They
brutally enforce all tasks to be similar, though they may
be not. In this study, we adopted the same regularization
scheme as in [9] which is more robust to such outliers and
effectively achieves joint feature learning based on the assumption that the same set of essential features may be
shared across different attributes with existence of outlier
tasks. This results in the following specialized problem
t



1
2
min
( wt  + ρ1
ξijt + ρ2
γijt )
W,ξ,γ
2
t=1
i,j∈O
i,j∈St
  
+ μ1 P  + μ2 QT 
1,2

3.1. A Relative Multi-attribute Learning Framework
With the reasonable assumption that multiple attributes
describing the same object should be related in some way
and that only relatively-labelled data pairs are given, we
propose to jointly learn multi-attribute ranking functions in
the following general formulation of an optimization prob-

1,2

s.t. W = P + Q;

3. Proposed Approach
In this section, we ﬁrst present the proposed formulation for relative multi-attribute learning that attempts to capture potential correlation among given attributes through a
multi-task learning framework (Sect. 3.1), and then present
an algorithm for ﬁnding solutions under this formulation
(Sect. 3.2).

(2)

(3)

wTt (xi − xj ) ≥ 1 − ξijt ; ∀(i, j) ∈ Ot ;
 T

wt (xi − xj ) ≤ γijt ; ∀(i, j) ∈ St ;
ξijt ≥ 0; γijt ≥ 0;
t = 1, 2, ..., t .
where the ﬁrst regularization term enforces a group Lasso
penalty on row groups of P in order to capture the shared
features among the attributes. The second term enforces the
same group Lasso penalty, but on column groups of Q to
discover the outlier tasks.
It should be noted that although joint-feature-learning
regularization is adopted in this work as a case study, other
types of regularization term could also be used. For example, one may choose to impose a low-rank constraint or
some graph-based structure on the attributes, if the problem

1029
1023

warrants such assumptions. But the essence of the problem,
to capture the potential correlation among the attributes, remains the same.

3.2. An Optimization Algorithm
We now turn to the problem of ﬁnding an solution under the proposed formulation. Without loss of generality,
our following discussion is in terms of a general regularization term Ω(W ). In general, solving the constrained optimization problem of function (2) is difﬁcult especially since
common multi-task regularization terms are typically nondifferentiable. In this study, we propose an algorithm based
on Block Coordinate Descent (BCD) principles. In this approach, we introduce a slack variable W̃ which is similar to
W so that the original problem may be solved by two alternating processes, focusing on a new cost function and the
regularization term respectively. That is, we ﬁrst convert
the original problem into
t



1
2
min
( wt  + ρ1
ξijt + ρ2
γijt )
2
W,W̃ ,ξ,γ
t=1
i,j∈O
i,j∈St





+ λW − W̃  + μΩ(W̃ )
(4)




in which the norm W − W̃  enforces a similar solution of
W and W̃ .
We divide ranking and task coupling into separate steps
by iteratively updating W and W̃ in the following two separate problems:
Optimization of W For a ﬁxed W̃ , the optimal W can
be obtained via solving:

This problem enforces a joint learning regularization constraints Ω(W̃ ) to the projection weight matrix to capture the
correlation information among the attributes. The ﬁrst term
penalizes the difference to make sure the learned “multitask” weight matrix W̃ is close to the given projection
weight W .
The overall optimization algorithm is summarized in Table 1.
Algorithm 1: Alternating Optimization
Input: Data feature set X, training ranking pairs set E
(prior) and F (similar), parameters ρ1 , ρ2 , λ, λ̃, μ.
Output: Projection matrix M .
1: Initiate W̃ as random matrix, W as zero matrix,
λ = 0.05λ̃;
2
W −W̃ 
2: while W 2 F > 10−10 , do:
F
3: Optimize function (5), update matrix W ;
4: Optimize function (6), update matrix W̃ ;
5: Set λ = λ + 0.05λ̃;
6: end while
Table 1. Projection Matrix Alternating Optimization Algorithem

In implementation, the ﬁrst problem given in (5) can by
solved by ﬁrst converting it to its dual form problem, which
is a typical quadratic optimization problem. While interested readers may ﬁnd the derivation in the supplemental
material, we list the dual form below for completeness:
1

min
xT Y T Y x + (λY T w̃ − (1 + λ)e)T x
ast ,dst 2
s.t. Ax ≤ b
with x = (x1 ; x2 ; · · · ; xt )T ;

t



1
2
min
( wt  + ρ1
ξijt + ρ2
γijt )
W,ξ,γ
2
t=1
i,j∈Ot
i,j∈St
2 
λ


+ W − W̃ 
2
F
(5)
s.t. wTt (xi − xj ) ≥ 1 − ξijt ; ∀(i, j) ∈ Ot ;
 T

wt (xi − xj ) ≤ γijt ; ∀(i, j) ∈ St ;

Y = Σ(Yt );
w̃ = [w̃1 ; w̃2 ; · · · ; w̃t ];
e = [e1 ; e2 ; · · · ; et ];
A = [E|Y |×|Y | ; −E|Y |×|Y | ];
bt = [ρ1 , ρ1 , · · · , ρ1 , ρ2 , ρ2 , · · · , ρ2 ]T .

	

 
	


|Ot |

ξijt ≥ 0; γijt ≥ 0;
t = 1, 2, ...t .





where we used the Frobenius norm on W − W̃  for facilitating the solution. This problem focuses on capturing relative ranking information by encoding multi-attribute information into one quadratic optimization process. The second
term enforces the projection weight matrix W to be close to
the given “multi-task” weight matrix W̃ .
Optimization of W̃ For a ﬁxed W , the optimal W̃ can
be obtained via solving:





min W̃ T − W T  + μΩ(W̃ )
(6)
W̃

(7)

|St |

b̃t = [0, 0, · · · , 0, ρ2 , ρ2 , · · · , ρ2 ]T .
 	
  
	


|Ot |

|St |

b = [b1 , b2 , · · · , bt , b̃1 , b̃2 , · · · , b̃t ];
t = 1, 2, · · · , t .
In essence, the problem of (5) is similar to regular relative attribute learning, and the problem of (6) is similar to MTL, and thus there convergence behavior is wellunderstood. In our implementation, to facilitate convergence, we set a small value for λ in Equation (5) at the
beginning. Then in each iteration afterwards, we increase
λ gradually until it reaches a speciﬁed large threshold.

1030
1024

the normal distribution N (0, 25). The groundtruth projection matrices P ∈ Rd×n and Q ∈ Rd×n are drawn from
N (0, 64). We set the ﬁrst 10 columns of Q non-zero and
they indicate outlier tasks. We also draw a noise vector
δ i ∈ Rn from N (0, 1). Thus, the ﬁnal ranking score for
data set Xi is computed as y i = XiT (P + Q) + δ i .
We run the experiments 4 rounds with the feature dimension d increasing from 50 to 200 with step size 50. In the
ﬁrst round, all 50 dimensions are set as shared intrisic features, which means all 50 rows of P are set non-zeros. Then
50 more zero rows are added into Q in each round afterwards till d reaches to 200. In this setup, the ﬁrst 50 dimensions of feature (ﬁrst 50 rows of P ) represent the selected
joint features among the attributes.
Through cross validation, during each round of our experiment the best ranking performance is always achieved
while the ﬁrst 50 dimensions are selected as joint features
(the ﬁrst 50 rows of learned projection matrix P are nonzeros) and the ﬁrst 10 attributes are detected as outliers (the
ﬁrst 10 columns of learned projection matrix Q are nonzeros). Figure 2 demonstrates the learned projection matrices P and Q when d reaches 200 as the parameters are set
as μ1 = 9.3, μ2 = 20.7, ρ1 = ρ2 = 300, and λ̃ = 500. The
result shows that when d = 200, the ﬁrst 50 rows of P are
selected as the joint features and the ﬁrst 10 columns of Q
are detected as outlier attributes, which are all non-zeros.
This result matches the groundtruth we have constructed
previously, which suggests that our approach is able to capture the inherent relatedness of the projection model.

Therefore, the weight of the second term becomes larger

2


and larger which ensures the cost W − W̃  would deF
crease after each iteration. The algorithm terminates when
W ≈ W̃ is reached.
We have 4 (since λ and λ̃ are correlated) hyper parameters, all having limited search space. λ mainly enforces
the proper convergence and doesnt impact much on ranking. Experiments also showed ρ1 and ρ2 do not inﬂuence
ranking result much. These parameters is selected via crossvalidation. Speciﬁcally, we ﬁrst ﬁnd a suitable parameter search space by binary search or subgradient approach.
For example, μ can be searched in a space ranging from
achieving a desired minimal sparsity to a maximal sparsity. Then we adjust the parameters one by one while ﬁxing
the other parameters according to the performance of crossvalidation.

4. Experiments
In this section, we tested our proposed framework in one
synthetic dataset and two real datasets. We ﬁrst experimented on synthetic dataset to show how well the correlation among the attributes are captured in our new proposed
attribute learning framework. Then we test the framework
on two real datasets including Outdoor Scene Recognition
(OSR) Dataset [17] and Shoes [5]. We compare our framework with two alternative approaches. The ﬁrst approach is
relative attribute [18] which learns a ranking function for
each attribute independently. The second approach is based
on multi-task learning work [9] [7], by which we trained
classiﬁers and used the classiﬁcation score to rank the attributes. We tested both the ranking accuracy of learned
ranking function and classiﬁcation accuracy of zero-shot
learning in the experiments.
We implemented the program on Matlab and employed
the multi-task learning solver package MALSAR developed by Zhou et al. [25]. Hyper parameters μ1 , μ2 , ρ1 , ρ2
and λ̃ are determined by cross validation as we discussed
previously. Let xtij represents the (i, j)-th entry in the data
matrix Xt of the t-th attributes, where i indexes d dimensions and j indexes n data samples, we normalize the experiment data to satisfy:
n 
2

xij = 1; ∀i ∈ Nd

50
Samples

Samples

50
100
150
200

100
150

10 20
Tasks

30

200

(a) P

10 20
Tasks

30

(b) Q

Figure 2. Projection matrices P (a) and Q (b) learned by our
framework on synthetic data. Blue color represents zero entry
while other colors represent non-zero entry. Results show the ﬁrst
50 rows of P are selected as selected shared features and the ﬁrst
10 columns of Q are detected as outlier tasks.

(8)

j=1

4.1. Experiments with Synthetic Data
In order to test whether our framework can capture the
relatedness among the attributes, we construct the synthetic
datasets in the following way. The total attribute (task)
number is t = 30. For the i-th attribute, we generate the
data set Xi ∈ Rd×n containing n = 200 samples and d dimensions for each sample. Each entry of Xi is drawn from

4.2. Experiments with Real Data
We compare our framework with the baseline methods
on the following two data sets:
OSR This dataset includes 2688 color outdoor scene images from 8 categories. There are totally 29,000 objects

1031
1025

with each image contains 256×256 pixels. Image features
are described as the 512-dimensional gist descriptor. We
used the same attributes and labels deﬁned in [18].
Shoes
This dataset includes 14765 images collected
from like.com containing 10 categories of shoes. Same
image features (960-dimensional gist descriptor plus 30dimensional color histogram), attributes and labels are
adopted from [14]. We randomly selected 6000 images (600
images per category) as our experiment data in this paper.

0.88

Average Ranking Accuracy

0.86
0.84
0.82
0.8
0.78
0.76
Our Approach
Relative Attributes
Multi−task Learning

0.74
0.72

4.2.1 Ranking Accuracy

0.7

We computed an average ranking accuracy (the frequency
of correctly ranked pairs) by running 5 rounds of each implemented approach. By cross validation, parameters of our
framework are set as μ1 = 60, μ2 = 20, ρ1 = ρ2 = 300,
λ̃ = 400 on OSR during which the projection matrix is
learned after 17 iterations. On Shoes, parameters are set as
μ1 = 3, μ2 = 50, ρ1 = ρ2 = 300, λ̃ = 500 and the projection model is got through 15 iterations. For the baseline
relative attributes approach, we adopted the same parameter
setup which is reported in [18] as the optimal parameters.
We ﬁrst experimented the approaches on OSR dataset.
Labeled training pairs are randomly left out for each attribute. The number of training pairs of each attribute increased from 50 to 500 with step size 50. For the baseline multi-task classiﬁcation approach, we left 100 to 1000
training samples out for comparison. Since n training pairs
would select at most 2n training samples, the training set
left for multi-task classiﬁcation gains no less information
than the other two ranking approaches. Figure 3(a) illustrates the average ranking accuracies as a function of increased number of training pairs with the similar standard
deviation among three approaches around ±1.2%. The result show that the accuracies of all three approaches increase with growing size of training data. The accuracy
achieved by our framework (blue curve) outperforms the
baseline results by 5%∼11%. The best performance gain is
achieved when the number of training pairs gets to 50. Table
2 details the ranking accuracies of all 6 attributes on OSR
when the number of training pairs is 50. According to the
result, other than “Depth-cloth”, accuracies of all attributes
achieved by our framework are obviously higher than the
competing results and the best performance gain is 18% in
attribute “natural”. We also analyzed on the P and Q matrices, where P includes 150 shared dimensions and the outlier task in Q is shown as the attribute “Size-large”. This
agrees with our observations that object in different sizes
are randomly appeared in pictures of different classes.
The implemented approaches are then tested on Shoes in
which a different training sets selection scheme is applied.
Instead of leaving training pairs out, we left some training samples out (ranging from 10 to 100 in number), and

50

100

150

200
250
300
350
400
Training Pairs (2X for Samples)

450

500

(a) OSR

0.8

Average Ranking Accuracy

0.75

0.7

0.65

Our Approach
Relative Attributes
Multi−task Learning

0.6

0.55

10

20

30

40

50
60
70
Training Samples

80

90

100

(b) Shoes

Figure 3. Average ranking accuracy of OSR and Shoes datasets as
the increased number of training pairs and samples. Our framework (blue) outperforms the compared approaches by more than
5% (450 pairs) to 11% (50 pairs) on OSR and by more than 4%
(100 samples) to 5% (10 samples) on Shoes.

the training pairs are selected merely from the left training
set. Figure 3(b) depicts the average ranking accuracies as a
function of the size of training data with similar standard deviation among three approaches around ±0.6%. This experiment shows similarly that our proposed framework (blue
curve) outperforms the other approaches by 4%∼5%. The
highest performance gain is got when the number of training samples is 10. Table 3 describes the ranking accuracies
on Shoes in all 10 attributes when the 60 samples are left
out for training. In all of the attributes, better ranking accuracies are achieved by our proposed framework. The best
performance gain is 8.5% in attribute “Pointy at the front”.
Both of these two experiments show that the more lim-

1032
1026

work in [18]. We also adopted the same optimal parameters setup used in 4.2.1. We compute the average classiﬁcation accuracies by running the experiment 5 rounds and
in each round we randomly selected 400 training pairs for
each seen categories to learn the projection model. Same as
in [18], we also assumed the data follows Gaussian distribution model and estimated the mean μ and the covariance
matrix Σ through maximum likelihood estimation. Given a
test image i and its corresponding ranking score vector x̃i ,
we assigned the category label according to the maximum
likelihood.
For the estimation of μ and Σ for unseen categories, we
also adopted the similar schemes but added one more rule
(t)
which we believe can better estimate the model: let ai
(t)
and aj represent the t-th attribute value from the unseen
n
(t)
(t)
category i and seen category j, we set μi = n1 j=1 μj
n
(t)
(t)
and Σi = n1 j=1 Σj .
Figure 4 shows the classiﬁcation accuracies of zero-shot
learning on OSR and Shoes. For OSR, the number of unseen categories increases from 0 to 5 while the total category number is 8 and the parameters of seen categories are
estimated by randomly selected 30 samples; for Shoes, the
number of unseen categories increases from 0 to 7 while the
total category number is 10 and the parameters of seen categories are estimated by randomly selected 100 samples.
The unseen categories are also randomly selected during
each test round for both datasets. The result shows that the
classiﬁcation accuracies decrease as the number of unseen
category increasing for both two datasets. On OSR, the
accuracy of our framework outperforms the competing approaches by 4%∼9% and best performance gain got as the
unseen category number is 4. On Shoes, our classiﬁcation
accuracy is 2%∼4% better than the results from the competing approach and the best performance gain is achieved
when the unseen category number gets to 2.

0.75
Our Approach
Relative Attribute

0.7

Classification Accuracy

0.65
0.6
0.55
0.5
0.45
0.4
0.35

0

1

2
3
Unseen category number

4

5

(a) OSR

0.65
Our Approach
Relative Attribute

0.6

Classification Accuracy

0.55
0.5
0.45
0.4
0.35
0.3
0.25

0

1

2

3
4
Unseen category number

5

6

7

(b) Shoes

Figure 4. Classiﬁcation accuracies of zero-shot learning on OSR
and Shoes. The number of unseen categories increases from 0 to 5
for OSR and from 0 to 7 for Shoes. Our framework (blue) outperforms the competing approach (green) by 4% to 9% on OSR and
by 2% to 4% on Shoes.

5. Conclusions
In this paper, we proposed a framework for relative
multi-attribute prediction through multiple task learning.
By employing a multi-task learning framework for learning
multiple attributes with only relative labels, our proposed
framework is able to capture the intrinsic relatedness among
the different attributes. The proposed method was evaluated
on two public datasets OSR and Shoes with the comparison
with the baseline approaches of relative attribute and multitask learning. Through the experiments on image ranking
and zero shot learning, we demonstrated that our method
obviously outperforms the baseline methods in both ranking and classiﬁcation capacities.
Acknowledgement: The work was supported in part by
a grant (#1135616) from the National Science Foundation.
Any opinions expressed in this material are those of the au-

ited size of training dataset it is, the more beneﬁts our proposed framework can gain from the relatedness among the
attritutes.
4.2.2 Zero-shot Learning
Finally, to show that the learned multi-attribute predictor
captures intrinsically useful information for the underlying
problem, we apply it to the task of zero-shot learning. Given
training data from some ‘seen’ categories and some ‘unseen’ categories without any training data, zero-shot learning tries to learn a classiﬁer to predict the category label of a
new sample. We choose relative attribute as the comparing
approach which has been shown to be the state-of-the-art

1033
1027

Attribute Name
Natural
Open
Perspective
Size-large
Diagonal-plane
Depth-cloth
Average

Our Approach
90.42%
88.62%
83.64%
80.15%
84.08%
82.65%
84.93%

Relative Attributes
72.90%
83.18%
77.67%
71.96%
74.21%
76.70%
76.10%

Multi-task Learning
85.33%
79.44%
78.17%
61.05%
71.73%
83.21%
76.49%

Table 2. Ranking accuracies of each attribute on OSR when the number of training pairs are 50 of each attribute for our approach and
relative attributes, 100 samples of each attribute for multi-task learning.

Attribute Name
Pointy at the front
Open
Bright in color
Covered with ornaments
Shiny
High at the heel
Long on the leg
Formal
Sporty
Feminine
Average

Our Approach
82.90%
76.41%
56.55%
67.66%
78.59%
76.23%
74.53%
73.59%
79.88%
81.51%
74.79%

Relative Attributes
74.52%
72.52%
55.24%
65.72%
75.03%
70.67%
71.91%
70.03%
72.39%
76.29%
70.43%

Multi-task Learning
72.10%
65.33%
53.17%
51.15%
72.71%
70.87%
64.60%
60.61%
69.30%
68.45%
64.83%

Table 3. Ranking accuracies of each attribute on Shoes when the 60 training samples of each attributes are left for training for each
approach. Training pairs are generated from these 60 samples for our approach and relative attributes.

thors and do not necessarily reﬂect the views of the NSF.

[14] A. Kovashka, D. Parikh, and K. Grauman. Whittlesearch: Image
search with relative attribute feedback. In Proc., CVPR 2012.
[15] J. Liu, S. Ji, and J. Ye. Multi-task feature learning via efﬁcient l2,1norm minimization, 2009.
[16] C. A. Micchelli and M. Pontil. Regularized multi-task learning.
2004.
[17] A. Oliva and A. Torralba. Modeling the shape of the scene: A holistic representation of the spatial envelope. International Journal of
Computer Vision, 2001.
[18] D. Parikh and K. Grauman. Relative attributes. In Proc., CVPR
2011, 2011.
[19] A. Parkash and D. Parikh. Attributes for classiﬁer feedback. In Proc.,
ECCV 2012. 2012.
[20] D. L. Silver and R. E. Mercer. The task rehearsal method of sequential learning.
[21] Y. Xue, X. Liao, L. Carin, and B. Krishnapuram. Multi-task learning
for classiﬁcation with dirichlet process priors. Journal of Machine
Learning Research, 2007.
[22] K. Yu, V. Tresp, and S. Yu. A nonparametric hierarchical bayesian
framework for information ﬁltering. In Proc., SIGIR ’04, 2004.
[23] T. Zhang and J. S. Liu. Nonparametric hierarchical bayes analysis of
binomial data via bernstein polynomial priors. Canadian Journal of
Statistics, 2012.
[24] J. Zhou, J. Chen, and J. Ye. Clustered multi-task learning via alternating structure optimization. In Advances in Neural Information
Processing Systems 24. 2011.
[25] J. Zhou, J. Chen, and J. Ye. MALSAR: Multi-tAsk Learning via StructurAl Regularization, 2011.

References
[1] R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data. J. Mach. Learn. Res.,
Dec. 2005.
[2] F. R. Bach. Clustered multi-task learning: a convex formulation.
[3] B. Bakker and T. Heskes. Task clustering and gating for bayesian
multitask learning. J. Mach. Learn. Res., Dec. 2003.
[4] D. A. Baxter and J. H. Byrne. Simulator for neural networks and
action potentials. November 2007.
[5] T. L. Berg, A. C. Berg, and J. Shih. Automatic attribute discovery
and characterization from noisy web data. In Proc., ECCV 2010.
[6] R. Caruana. Multitask learning, 1997.
[7] J. Chen, J. Zhou, and J. Ye. Integrating low-rank and group-sparse
structures for robust multi-task learning. In Proc., KDD ’11, 2011.
[8] X. Chen, S. Kim, Q. Lin, J. G. Carbonell, and E. P. Xing. Graphstructured multi-task regression and an efﬁcient optimization method
for general fused lasso, 2010.
[9] P. Gong, J. Ye, and C. Zhang. Robust multi-task feature learning. In
Proc., KDD ’12, 2012.
[10] L. Jacob and G. Obozinski. Group lasso with overlap and graph lasso.
[11] A. Jalali, P. D. Ravikumar, and S. Sanghavi. A dirty model for multiple sparse regression. CoRR, 2011.
[12] S. Ji and J. Ye. An accelerated gradient method for trace norm minimization.
[13] S. Kim and E. P. Xing. Tree-guided group lasso for multi-task regression with structured sparsity.

1034
1028

